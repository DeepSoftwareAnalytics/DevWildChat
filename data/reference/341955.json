[
    {
        "link": "https://neon.tech/postgresql/postgresql-tutorial/postgresql-recursive-query",
        "document": "Summary: in this tutorial, you will learn about the PostgreSQL recursive query using recursive common table expressions or CTEs.\n\nIn PostgreSQL, a common table expression (CTE) is a named temporary result set within a query.\n\nA recursive CTE allows you to perform recursion within a query using the syntax.\n\nA recursive CTE is often referred to as a recursive query.\n• : Specify the name of the CTE. You can reference this CTE name in the subsequent parts of the query.\n• , , … Specify the columns selected in both the anchor and recursive members. These columns define the CTE’s structure.\n• Anchor member: Responsible for forming the base result set of the CTE structure.\n• Recursive member: Refer to the CTE name itself. It combines with the anchor member using the or operator.\n• : Is a condition used in the recursive member that determines how the recursion stops.\n\nPostgreSQL executes a recursive CTE in the following sequence:\n• First, execute the anchor member to create the base result set (R0).\n• Second, execute the recursive member with Ri as an input to return the result set Ri+1 as the output.\n• Third, repeat step 2 until an empty set is returned. (termination check)\n• Finally, return the final result set that is a UNION or of the result sets R0, R1, … Rn.\n\nA recursive CTE can be useful when dealing with hierarchical or nested data structures, such as trees or graphs.\n\nLet’s take an example of using a recursive query.\n\nThe table has three columns: , , and . The column specifies the manager id of an employee.\n\nSecond, insert some rows into the table:\n\nThe following statement uses a recursive CTE to find all subordinates of the manager with the id 2.\n• The recursive CTE defines an anchor member and a recursive member.\n• The anchor member returns the base result set R0 which is the employee with the id 2.\n\nThe recursive member returns the direct subordinate(s) of the employee id 2. This is the result of joining between the table and the CTE. The first iteration of the recursive term returns the following result set:\n\nPostgreSQL executes the recursive member repeatedly. The second iteration of the recursive member uses the result set above step as the input value, and returns this result set:\n\nThe third iteration returns an empty result set because no employee is reporting to the employee with the id 16, 17, 18, 19, and 20.\n\nPostgreSQL returns the final result set which is the union of all result sets in the first and second iterations generated by the non-recursive and recursive members.\n• Use the syntax to define a recursive query.\n• Use a recursive query to deal with hierarchical or nested data structures such as trees or graphs."
    },
    {
        "link": "https://postgresql.org/docs/current/queries-with.html",
        "document": "provides a way to write auxiliary statements for use in a larger query. These statements, which are often referred to as Common Table Expressions or s, can be thought of as defining temporary tables that exist just for one query. Each auxiliary statement in a clause can be a , , , , or ; and the clause itself is attached to a primary statement that can also be a , , , , or .\n\nThe basic value of in is to break down complicated queries into simpler parts. An example is: WITH regional_sales AS ( SELECT region, SUM(amount) AS total_sales FROM orders GROUP BY region ), top_regions AS ( SELECT region FROM regional_sales WHERE total_sales > (SELECT SUM(total_sales)/10 FROM regional_sales) ) SELECT region, product, SUM(quantity) AS product_units, SUM(amount) AS product_sales FROM orders WHERE region IN (SELECT region FROM top_regions) GROUP BY region, product; which displays per-product sales totals in only the top sales regions. The clause defines two auxiliary statements named and , where the output of is used in and the output of is used in the primary query. This example could have been written without , but we'd have needed two levels of nested sub- s. It's a bit easier to follow this way.\n\nThe optional modifier changes from a mere syntactic convenience into a feature that accomplishes things not otherwise possible in standard SQL. Using , a query can refer to its own output. A very simple example is this query to sum the integers from 1 through 100: WITH RECURSIVE t(n) AS ( VALUES (1) UNION ALL SELECT n+1 FROM t WHERE n < 100 ) SELECT sum(n) FROM t; The general form of a recursive query is always a non-recursive term, then (or ), then a recursive term, where only the recursive term can contain a reference to the query's own output. Such a query is executed as follows:\n• Evaluate the non-recursive term. For (but not ), discard duplicate rows. Include all remaining rows in the result of the recursive query, and also place them in a temporary working table.\n• So long as the working table is not empty, repeat these steps:\n• Evaluate the recursive term, substituting the current contents of the working table for the recursive self-reference. For (but not ), discard duplicate rows and rows that duplicate any previous result row. Include all remaining rows in the result of the recursive query, and also place them in a temporary intermediate table.\n• Replace the contents of the working table with the contents of the intermediate table, then empty the intermediate table. While allows queries to be specified recursively, internally such queries are evaluated iteratively. In the example above, the working table has just a single row in each step, and it takes on the values from 1 through 100 in successive steps. In the 100th step, there is no output because of the clause, and so the query terminates. Recursive queries are typically used to deal with hierarchical or tree-structured data. A useful example is this query to find all the direct and indirect sub-parts of a product, given only a table that shows immediate inclusions: WITH RECURSIVE included_parts(sub_part, part, quantity) AS ( SELECT sub_part, part, quantity FROM parts WHERE part = 'our_product' UNION ALL SELECT p.sub_part, p.part, p.quantity * pr.quantity FROM included_parts pr, parts p WHERE p.part = pr.sub_part ) SELECT sub_part, SUM(quantity) as total_quantity FROM included_parts GROUP BY sub_part When computing a tree traversal using a recursive query, you might want to order the results in either depth-first or breadth-first order. This can be done by computing an ordering column alongside the other data columns and using that to sort the results at the end. Note that this does not actually control in which order the query evaluation visits the rows; that is as always in SQL implementation-dependent. This approach merely provides a convenient way to order the results afterwards. To create a depth-first order, we compute for each result row an array of rows that we have visited so far. For example, consider the following query that searches a table using a field: WITH RECURSIVE search_tree(id, link, data) AS ( SELECT t.id, t.link, t.data FROM tree t UNION ALL SELECT t.id, t.link, t.data FROM tree t, search_tree st WHERE t.id = st.link ) SELECT * FROM search_tree; To add depth-first ordering information, you can write this: WITH RECURSIVE search_tree(id, link, data, ) AS ( SELECT t.id, t.link, t.data, FROM tree t UNION ALL SELECT t.id, t.link, t.data, FROM tree t, search_tree st WHERE t.id = st.link ) SELECT * FROM search_tree ; In the general case where more than one field needs to be used to identify a row, use an array of rows. For example, if we needed to track fields and : WITH RECURSIVE search_tree(id, link, data, ) AS ( SELECT t.id, t.link, t.data, FROM tree t UNION ALL SELECT t.id, t.link, t.data, FROM tree t, search_tree st WHERE t.id = st.link ) SELECT * FROM search_tree ; Omit the syntax in the common case where only one field needs to be tracked. This allows a simple array rather than a composite-type array to be used, gaining efficiency. To create a breadth-first order, you can add a column that tracks the depth of the search, for example: WITH RECURSIVE search_tree(id, link, data, ) AS ( SELECT t.id, t.link, t.data, FROM tree t UNION ALL SELECT t.id, t.link, t.data, FROM tree t, search_tree st WHERE t.id = st.link ) SELECT * FROM search_tree ; To get a stable sort, add data columns as secondary sorting columns. The recursive query evaluation algorithm produces its output in breadth-first search order. However, this is an implementation detail and it is perhaps unsound to rely on it. The order of the rows within each level is certainly undefined, so some explicit ordering might be desired in any case. There is built-in syntax to compute a depth- or breadth-first sort column. For example: WITH RECURSIVE search_tree(id, link, data) AS ( SELECT t.id, t.link, t.data FROM tree t UNION ALL SELECT t.id, t.link, t.data FROM tree t, search_tree st WHERE t.id = st.link ) SELECT * FROM search_tree ORDER BY ordercol; WITH RECURSIVE search_tree(id, link, data) AS ( SELECT t.id, t.link, t.data FROM tree t UNION ALL SELECT t.id, t.link, t.data FROM tree t, search_tree st WHERE t.id = st.link ) SELECT * FROM search_tree ORDER BY ordercol; This syntax is internally expanded to something similar to the above hand-written forms. The clause specifies whether depth- or breadth first search is wanted, the list of columns to track for sorting, and a column name that will contain the result data that can be used for sorting. That column will implicitly be added to the output rows of the CTE. When working with recursive queries it is important to be sure that the recursive part of the query will eventually return no tuples, or else the query will loop indefinitely. Sometimes, using instead of can accomplish this by discarding rows that duplicate previous output rows. However, often a cycle does not involve output rows that are completely duplicate: it may be necessary to check just one or a few fields to see if the same point has been reached before. The standard method for handling such situations is to compute an array of the already-visited values. For example, consider again the following query that searches a table using a field: WITH RECURSIVE search_graph(id, link, data, depth) AS ( SELECT g.id, g.link, g.data, 0 FROM graph g UNION ALL SELECT g.id, g.link, g.data, sg.depth + 1 FROM graph g, search_graph sg WHERE g.id = sg.link ) SELECT * FROM search_graph; This query will loop if the relationships contain cycles. Because we require a “depth” output, just changing to would not eliminate the looping. Instead we need to recognize whether we have reached the same row again while following a particular path of links. We add two columns and to the loop-prone query: WITH RECURSIVE search_graph(id, link, data, depth, ) AS ( SELECT g.id, g.link, g.data, 0, FROM graph g UNION ALL SELECT g.id, g.link, g.data, sg.depth + 1, FROM graph g, search_graph sg WHERE g.id = sg.link ) SELECT * FROM search_graph; Aside from preventing cycles, the array value is often useful in its own right as representing the “path” taken to reach any particular row. In the general case where more than one field needs to be checked to recognize a cycle, use an array of rows. For example, if we needed to compare fields and : WITH RECURSIVE search_graph(id, link, data, depth, ) AS ( SELECT g.id, g.link, g.data, 0, FROM graph g UNION ALL SELECT g.id, g.link, g.data, sg.depth + 1, FROM graph g, search_graph sg WHERE g.id = sg.link ) SELECT * FROM search_graph; Omit the syntax in the common case where only one field needs to be checked to recognize a cycle. This allows a simple array rather than a composite-type array to be used, gaining efficiency. There is built-in syntax to simplify cycle detection. The above query can also be written like this: WITH RECURSIVE search_graph(id, link, data, depth) AS ( SELECT g.id, g.link, g.data, 1 FROM graph g UNION ALL SELECT g.id, g.link, g.data, sg.depth + 1 FROM graph g, search_graph sg WHERE g.id = sg.link ) SELECT * FROM search_graph; and it will be internally rewritten to the above form. The clause specifies first the list of columns to track for cycle detection, then a column name that will show whether a cycle has been detected, and finally the name of another column that will track the path. The cycle and path columns will implicitly be added to the output rows of the CTE. The cycle path column is computed in the same way as the depth-first ordering column show in the previous section. A query can have both a and a clause, but a depth-first search specification and a cycle detection specification would create redundant computations, so it's more efficient to just use the clause and order by the path column. If breadth-first ordering is wanted, then specifying both and can be useful. A helpful trick for testing queries when you are not certain if they might loop is to place a in the parent query. For example, this query would loop forever without the : WITH RECURSIVE t(n) AS ( SELECT 1 UNION ALL SELECT n+1 FROM t ) SELECT n FROM t ; This works because PostgreSQL's implementation evaluates only as many rows of a query as are actually fetched by the parent query. Using this trick in production is not recommended, because other systems might work differently. Also, it usually won't work if you make the outer query sort the recursive query's results or join them to some other table, because in such cases the outer query will usually try to fetch all of the query's output anyway.\n\nA useful property of queries is that they are normally evaluated only once per execution of the parent query, even if they are referred to more than once by the parent query or sibling queries. Thus, expensive calculations that are needed in multiple places can be placed within a query to avoid redundant work. Another possible application is to prevent unwanted multiple evaluations of functions with side-effects. However, the other side of this coin is that the optimizer is not able to push restrictions from the parent query down into a multiply-referenced query, since that might affect all uses of the query's output when it should affect only one. The multiply-referenced query will be evaluated as written, without suppression of rows that the parent query might discard afterwards. (But, as mentioned above, evaluation might stop early if the reference(s) to the query demand only a limited number of rows.) However, if a query is non-recursive and side-effect-free (that is, it is a containing no volatile functions) then it can be folded into the parent query, allowing joint optimization of the two query levels. By default, this happens if the parent query references the query just once, but not if it references the query more than once. You can override that decision by specifying to force separate calculation of the query, or by specifying to force it to be merged into the parent query. The latter choice risks duplicate computation of the query, but it can still give a net savings if each usage of the query needs only a small part of the query's full output. A simple example of these rules is WITH w AS ( SELECT * FROM big_table ) SELECT * FROM w WHERE key = 123; This query will be folded, producing the same execution plan as In particular, if there's an index on , it will probably be used to fetch just the rows having . On the other hand, in WITH w AS ( SELECT * FROM big_table ) SELECT * FROM w AS w1 JOIN w AS w2 ON w1.key = w2.ref WHERE w2.key = 123; the query will be materialized, producing a temporary copy of that is then joined with itself — without benefit of any index. This query will be executed much more efficiently if written as WITH w AS NOT MATERIALIZED ( SELECT * FROM big_table ) SELECT * FROM w AS w1 JOIN w AS w2 ON w1.key = w2.ref WHERE w2.key = 123; so that the parent query's restrictions can be applied directly to scans of . An example where could be undesirable is WITH w AS ( SELECT key, very_expensive_function(val) as f FROM some_table ) SELECT * FROM w AS w1 JOIN w AS w2 ON w1.f = w2.f; Here, materialization of the query ensures that is evaluated only once per table row, not twice. The examples above only show being used with , but it can be attached in the same way to , , , or . In each case it effectively provides temporary table(s) that can be referred to in the main command.\n\nYou can use data-modifying statements ( , , , or ) in . This allows you to perform several different operations in the same query. An example is: WITH moved_rows AS ( DELETE FROM products WHERE \"date\" >= '2010-10-01' AND \"date\" < '2010-11-01' RETURNING * ) INSERT INTO products_log SELECT * FROM moved_rows; This query effectively moves rows from to . The in deletes the specified rows from , returning their contents by means of its clause; and then the primary query reads that output and inserts it into . A fine point of the above example is that the clause is attached to the , not the sub- within the . This is necessary because data-modifying statements are only allowed in clauses that are attached to the top-level statement. However, normal visibility rules apply, so it is possible to refer to the statement's output from the sub- . Data-modifying statements in usually have clauses (see Section 6.4), as shown in the example above. It is the output of the clause, not the target table of the data-modifying statement, that forms the temporary table that can be referred to by the rest of the query. If a data-modifying statement in lacks a clause, then it forms no temporary table and cannot be referred to in the rest of the query. Such a statement will be executed nonetheless. A not-particularly-useful example is: WITH t AS ( DELETE FROM foo ) DELETE FROM bar; This example would remove all rows from tables and . The number of affected rows reported to the client would only include rows removed from . Recursive self-references in data-modifying statements are not allowed. In some cases it is possible to work around this limitation by referring to the output of a recursive , for example: WITH RECURSIVE included_parts(sub_part, part) AS ( SELECT sub_part, part FROM parts WHERE part = 'our_product' UNION ALL SELECT p.sub_part, p.part FROM included_parts pr, parts p WHERE p.part = pr.sub_part ) DELETE FROM parts WHERE part IN (SELECT part FROM included_parts); This query would remove all direct and indirect subparts of a product. Data-modifying statements in are executed exactly once, and always to completion, independently of whether the primary query reads all (or indeed any) of their output. Notice that this is different from the rule for in : as stated in the previous section, execution of a is carried only as far as the primary query demands its output. The sub-statements in are executed concurrently with each other and with the main query. Therefore, when using data-modifying statements in , the order in which the specified updates actually happen is unpredictable. All the statements are executed with the same snapshot (see Chapter 13), so they cannot “see” one another's effects on the target tables. This alleviates the effects of the unpredictability of the actual order of row updates, and means that data is the only way to communicate changes between different sub-statements and the main query. An example of this is that in WITH t AS ( UPDATE products SET price = price * 1.05 RETURNING * ) SELECT * FROM products; the outer would return the original prices before the action of the , while in WITH t AS ( UPDATE products SET price = price * 1.05 RETURNING * ) SELECT * FROM t; the outer would return the updated data. Trying to update the same row twice in a single statement is not supported. Only one of the modifications takes place, but it is not easy (and sometimes not possible) to reliably predict which one. This also applies to deleting a row that was already updated in the same statement: only the update is performed. Therefore you should generally avoid trying to modify a single row twice in a single statement. In particular avoid writing sub-statements that could affect the same rows changed by the main statement or a sibling sub-statement. The effects of such a statement will not be predictable. At present, any table used as the target of a data-modifying statement in must not have a conditional rule, nor an rule, nor an rule that expands to multiple statements."
    },
    {
        "link": "https://cybertec-postgresql.com/en/recursive-queries-postgresql",
        "document": "Many people consider recursive queries a difficult topic. Still, they enable you to do things that would otherwise be impossible in SQL.\n\nThis articles gives a simple introduction with examples and shows the differences to Oracle's implementation of recursive queries.\n\nA (CTE) can be seen as a view that is only valid for a single query:\n\nThis could also be written as a subquery in , but there are some advantages to using CTEs:\n• The query becomes more readable.\n• You can reference the CTE several times in the query, and it will be calculated only once.\n• You can use data modifying statements in the CTE (typically with a clause).\n\nNote that before v12, PostgreSQL always materialized CTEs. That means, the CTE was calculated independently from the containing query. From v12 on, CTEs can be “inlined” into the query, which provides further optimization potential.\n\nCTEs (as well as the recursive CTEs mentioned below) are defined in the SQL standard, although PostgreSQL does not implement the and clause.\n\nRecursive queries are written using , that are CTEs containing the keyword:\n\nPostgreSQL internally uses a to process recursive CTEs. This processing is not really recursive, but rather iterative:\n\nFirst, the working table is initialized by executing the non-recursive branch of the CTE. The result of the CTE is also initialized with this result set. If the recursive CTE uses rather than , duplicate rows are removed.\n\nThen, PostgreSQL repeats the following until the working table is empty:\n• Evaluate the recursive branch of the CTE, replacing the reference to the CTE with the working table.\n• Add all resulting rows to the CTE result. If is used to combine the branches, discard duplicate rows.\n• Replace the working table with all new rows from the previous step (excluding any removed duplicates).\n\nNote that the self-referential branch of the CTE is executed using the complete CTE result so far, but only the rows that are new since the previous iteration (the working table).\n\nYou have to be aware of the danger of an endless loop here:\n\nIf the iteration never ends, the query will just keep running until the result table becomes big enough to cause an error. There are two ways to deal with that:\n• Often you can avoid infinite recursion by using , which removes duplicate result rows (but of course requires extra processing effort).\n• Another way is to place a clause on the query that uses the CTE, because PostgreSQL stops processing if the recursive CTE has calculated as many rows as are fetched by the parent query. Note that this technique is not portable to other standard compliant databases.\n\nWe want to find all the subordinates of person 7566, including the person itself.\n\nThe non-recursive branch of the query will be:\n\nThe recursive branch will find all subordinates of all entries in the working table:\n\nWe can assume that the dependencies contain no cycles (nobody is his or her own manager, directly or indirectly). So we can combine the queries with , because no duplicates can occur.\n\nSo our complete query will be:\n\nSometimes you want to add more information, like the hierarchical level. You can do that by adding the starting level as a constant in the non-recursive branch. In the recursive branch you simply add 1 to the level:\n\nIf you use to avoid duplicated rows in the case of circular references, you cannot use this technique. This is because adding will make rows that were identical before different. But in that case a hierarchical level wouldn't make much sense anyway because an entry could appear on infinitely many levels.\n\nAnother frequent requirement is to collect all ancestors in a “path”:\n\nOracle database has a different syntax for recursive queries that does not conform to the SQL standard. The original example would look like this:\n\nThis syntax is more concise, but less powerful than recursive CTEs. For more complicated queries involving joins, it can become difficult and confusing.\n\nIt is always easy to translate an Oracle “hierarchical query” to a recursive CTE:\n• The non-recursive branch is the Oracle query without the clause but including the clause.\n• The recursive branch is the Oracle query without the clause but including the clause. You add a join with the name of the recursive CTE and replace all columns with columns from that joined CTE.\n• If the Oracle query uses , use , otherwise .\n\nApart from that, Oracle also supports standard compliant recursive CTEs. These also support the and clauses that PostgreSQL doesn't implement.\n\nWithout recursive CTEs, many things that can be written in procedural languages cannot be written in SQL. That is usually no problem, because SQL is made to query databases. If you need procedural code, you can always write a database function in one of the many procedural languages available in PostgreSQL.\n\nBut recursive CTEs make SQL turing complete, that is, it can perform the same calculations as any other programming language. Obviously, such a “program” would often be quite complicated and inefficient, and this is a theoretical consideration and not of much practical value. Still, the previous examples showed that recursive CTEs can do useful work that you couldn't otherwise perform in SQL.\n\nAs an example for the power of recursive queries, here is a recursive CTE that computes the first elements of the Fibonacci sequence:\n\nThis example also demonstrates how an endless loop can be avoided with a clause on the parent query.\n\nYou shouldn't be afraid of recursive queries, even if the term “recursive” scares you. They are not that hard to understand, and they provide the simplest way to query databases containing hierarchies and graphs. In many cases they are also much more efficient than equivalent procedural code because they avoid repeated queries to the same tables.\n\nTogether with window functions, recursive queries are among the most powerful tools that SQL has to offer."
    },
    {
        "link": "https://postgrespro.com/docs/postgresql/14/queries-with",
        "document": "provides a way to write auxiliary statements for use in a larger query. These statements, which are often referred to as Common Table Expressions or s, can be thought of as defining temporary tables that exist just for one query. Each auxiliary statement in a clause can be a , , , or ; and the clause itself is attached to a primary statement that can also be a , , , or .\n\nThe basic value of in is to break down complicated queries into simpler parts. An example is: WITH regional_sales AS ( SELECT region, SUM(amount) AS total_sales FROM orders GROUP BY region ), top_regions AS ( SELECT region FROM regional_sales WHERE total_sales > (SELECT SUM(total_sales)/10 FROM regional_sales) ) SELECT region, product, SUM(quantity) AS product_units, SUM(amount) AS product_sales FROM orders WHERE region IN (SELECT region FROM top_regions) GROUP BY region, product; which displays per-product sales totals in only the top sales regions. The clause defines two auxiliary statements named and , where the output of is used in and the output of is used in the primary query. This example could have been written without , but we'd have needed two levels of nested sub- s. It's a bit easier to follow this way.\n\nThe optional modifier changes from a mere syntactic convenience into a feature that accomplishes things not otherwise possible in standard SQL. Using , a query can refer to its own output. A very simple example is this query to sum the integers from 1 through 100: WITH RECURSIVE t(n) AS ( VALUES (1) UNION ALL SELECT n+1 FROM t WHERE n < 100 ) SELECT sum(n) FROM t; The general form of a recursive query is always a non-recursive term, then (or ), then a recursive term, where only the recursive term can contain a reference to the query's own output. Such a query is executed as follows:\n• None Evaluate the non-recursive term. For (but not ), discard duplicate rows. Include all remaining rows in the result of the recursive query, and also place them in a temporary working table.\n• None So long as the working table is not empty, repeat these steps:\n• None Evaluate the recursive term, substituting the current contents of the working table for the recursive self-reference. For (but not ), discard duplicate rows and rows that duplicate any previous result row. Include all remaining rows in the result of the recursive query, and also place them in a temporary intermediate table.\n• None Replace the contents of the working table with the contents of the intermediate table, then empty the intermediate table. While allows queries to be specified recursively, internally such queries are evaluated iteratively. In the example above, the working table has just a single row in each step, and it takes on the values from 1 through 100 in successive steps. In the 100th step, there is no output because of the clause, and so the query terminates. Recursive queries are typically used to deal with hierarchical or tree-structured data. A useful example is this query to find all the direct and indirect sub-parts of a product, given only a table that shows immediate inclusions: WITH RECURSIVE included_parts(sub_part, part, quantity) AS ( SELECT sub_part, part, quantity FROM parts WHERE part = 'our_product' UNION ALL SELECT p.sub_part, p.part, p.quantity * pr.quantity FROM included_parts pr, parts p WHERE p.part = pr.sub_part ) SELECT sub_part, SUM(quantity) as total_quantity FROM included_parts GROUP BY sub_part When computing a tree traversal using a recursive query, you might want to order the results in either depth-first or breadth-first order. This can be done by computing an ordering column alongside the other data columns and using that to sort the results at the end. Note that this does not actually control in which order the query evaluation visits the rows; that is as always in SQL implementation-dependent. This approach merely provides a convenient way to order the results afterwards. To create a depth-first order, we compute for each result row an array of rows that we have visited so far. For example, consider the following query that searches a table using a field: WITH RECURSIVE search_tree(id, link, data) AS ( SELECT t.id, t.link, t.data FROM tree t UNION ALL SELECT t.id, t.link, t.data FROM tree t, search_tree st WHERE t.id = st.link ) SELECT * FROM search_tree; To add depth-first ordering information, you can write this: WITH RECURSIVE search_tree(id, link, data, ) AS ( SELECT t.id, t.link, t.data, FROM tree t UNION ALL SELECT t.id, t.link, t.data, FROM tree t, search_tree st WHERE t.id = st.link ) SELECT * FROM search_tree ; In the general case where more than one field needs to be used to identify a row, use an array of rows. For example, if we needed to track fields and : WITH RECURSIVE search_tree(id, link, data, ) AS ( SELECT t.id, t.link, t.data, FROM tree t UNION ALL SELECT t.id, t.link, t.data, FROM tree t, search_tree st WHERE t.id = st.link ) SELECT * FROM search_tree ; Omit the syntax in the common case where only one field needs to be tracked. This allows a simple array rather than a composite-type array to be used, gaining efficiency. To create a breadth-first order, you can add a column that tracks the depth of the search, for example: WITH RECURSIVE search_tree(id, link, data, ) AS ( SELECT t.id, t.link, t.data, FROM tree t UNION ALL SELECT t.id, t.link, t.data, FROM tree t, search_tree st WHERE t.id = st.link ) SELECT * FROM search_tree ; To get a stable sort, add data columns as secondary sorting columns. The recursive query evaluation algorithm produces its output in breadth-first search order. However, this is an implementation detail and it is perhaps unsound to rely on it. The order of the rows within each level is certainly undefined, so some explicit ordering might be desired in any case. There is built-in syntax to compute a depth- or breadth-first sort column. For example: WITH RECURSIVE search_tree(id, link, data) AS ( SELECT t.id, t.link, t.data FROM tree t UNION ALL SELECT t.id, t.link, t.data FROM tree t, search_tree st WHERE t.id = st.link ) SELECT * FROM search_tree ORDER BY ordercol; WITH RECURSIVE search_tree(id, link, data) AS ( SELECT t.id, t.link, t.data FROM tree t UNION ALL SELECT t.id, t.link, t.data FROM tree t, search_tree st WHERE t.id = st.link ) SELECT * FROM search_tree ORDER BY ordercol; This syntax is internally expanded to something similar to the above hand-written forms. The clause specifies whether depth- or breadth first search is wanted, the list of columns to track for sorting, and a column name that will contain the result data that can be used for sorting. That column will implicitly be added to the output rows of the CTE. When working with recursive queries it is important to be sure that the recursive part of the query will eventually return no tuples, or else the query will loop indefinitely. Sometimes, using instead of can accomplish this by discarding rows that duplicate previous output rows. However, often a cycle does not involve output rows that are completely duplicate: it may be necessary to check just one or a few fields to see if the same point has been reached before. The standard method for handling such situations is to compute an array of the already-visited values. For example, consider again the following query that searches a table using a field: WITH RECURSIVE search_graph(id, link, data, depth) AS ( SELECT g.id, g.link, g.data, 0 FROM graph g UNION ALL SELECT g.id, g.link, g.data, sg.depth + 1 FROM graph g, search_graph sg WHERE g.id = sg.link ) SELECT * FROM search_graph; This query will loop if the relationships contain cycles. Because we require a “depth” output, just changing to would not eliminate the looping. Instead we need to recognize whether we have reached the same row again while following a particular path of links. We add two columns and to the loop-prone query: WITH RECURSIVE search_graph(id, link, data, depth, ) AS ( SELECT g.id, g.link, g.data, 0, FROM graph g UNION ALL SELECT g.id, g.link, g.data, sg.depth + 1, FROM graph g, search_graph sg WHERE g.id = sg.link ) SELECT * FROM search_graph; Aside from preventing cycles, the array value is often useful in its own right as representing the “path” taken to reach any particular row. In the general case where more than one field needs to be checked to recognize a cycle, use an array of rows. For example, if we needed to compare fields and : WITH RECURSIVE search_graph(id, link, data, depth, ) AS ( SELECT g.id, g.link, g.data, 0, FROM graph g UNION ALL SELECT g.id, g.link, g.data, sg.depth + 1, FROM graph g, search_graph sg WHERE g.id = sg.link ) SELECT * FROM search_graph; Omit the syntax in the common case where only one field needs to be checked to recognize a cycle. This allows a simple array rather than a composite-type array to be used, gaining efficiency. There is built-in syntax to simplify cycle detection. The above query can also be written like this: WITH RECURSIVE search_graph(id, link, data, depth) AS ( SELECT g.id, g.link, g.data, 1 FROM graph g UNION ALL SELECT g.id, g.link, g.data, sg.depth + 1 FROM graph g, search_graph sg WHERE g.id = sg.link ) SELECT * FROM search_graph; and it will be internally rewritten to the above form. The clause specifies first the list of columns to track for cycle detection, then a column name that will show whether a cycle has been detected, and finally the name of another column that will track the path. The cycle and path columns will implicitly be added to the output rows of the CTE. The cycle path column is computed in the same way as the depth-first ordering column show in the previous section. A query can have both a and a clause, but a depth-first search specification and a cycle detection specification would create redundant computations, so it's more efficient to just use the clause and order by the path column. If breadth-first ordering is wanted, then specifying both and can be useful. A helpful trick for testing queries when you are not certain if they might loop is to place a in the parent query. For example, this query would loop forever without the : WITH RECURSIVE t(n) AS ( SELECT 1 UNION ALL SELECT n+1 FROM t ) SELECT n FROM t ; This works because PostgreSQL's implementation evaluates only as many rows of a query as are actually fetched by the parent query. Using this trick in production is not recommended, because other systems might work differently. Also, it usually won't work if you make the outer query sort the recursive query's results or join them to some other table, because in such cases the outer query will usually try to fetch all of the query's output anyway.\n\nA useful property of queries is that they are normally evaluated only once per execution of the parent query, even if they are referred to more than once by the parent query or sibling queries. Thus, expensive calculations that are needed in multiple places can be placed within a query to avoid redundant work. Another possible application is to prevent unwanted multiple evaluations of functions with side-effects. However, the other side of this coin is that the optimizer is not able to push restrictions from the parent query down into a multiply-referenced query, since that might affect all uses of the query's output when it should affect only one. The multiply-referenced query will be evaluated as written, without suppression of rows that the parent query might discard afterwards. (But, as mentioned above, evaluation might stop early if the reference(s) to the query demand only a limited number of rows.) However, if a query is non-recursive and side-effect-free (that is, it is a containing no volatile functions) then it can be folded into the parent query, allowing joint optimization of the two query levels. By default, this happens if the parent query references the query just once, but not if it references the query more than once. You can override that decision by specifying to force separate calculation of the query, or by specifying to force it to be merged into the parent query. The latter choice risks duplicate computation of the query, but it can still give a net savings if each usage of the query needs only a small part of the query's full output. A simple example of these rules is WITH w AS ( SELECT * FROM big_table ) SELECT * FROM w WHERE key = 123; This query will be folded, producing the same execution plan as In particular, if there's an index on , it will probably be used to fetch just the rows having . On the other hand, in WITH w AS ( SELECT * FROM big_table ) SELECT * FROM w AS w1 JOIN w AS w2 ON w1.key = w2.ref WHERE w2.key = 123; the query will be materialized, producing a temporary copy of that is then joined with itself — without benefit of any index. This query will be executed much more efficiently if written as WITH w AS NOT MATERIALIZED ( SELECT * FROM big_table ) SELECT * FROM w AS w1 JOIN w AS w2 ON w1.key = w2.ref WHERE w2.key = 123; so that the parent query's restrictions can be applied directly to scans of . An example where could be undesirable is WITH w AS ( SELECT key, very_expensive_function(val) as f FROM some_table ) SELECT * FROM w AS w1 JOIN w AS w2 ON w1.f = w2.f; Here, materialization of the query ensures that is evaluated only once per table row, not twice. The examples above only show being used with , but it can be attached in the same way to , , or . In each case it effectively provides temporary table(s) that can be referred to in the main command.\n\nYou can use data-modifying statements ( , , or ) in . This allows you to perform several different operations in the same query. An example is: WITH moved_rows AS ( DELETE FROM products WHERE \"date\" >= '2010-10-01' AND \"date\" < '2010-11-01' RETURNING * ) INSERT INTO products_log SELECT * FROM moved_rows; This query effectively moves rows from to . The in deletes the specified rows from , returning their contents by means of its clause; and then the primary query reads that output and inserts it into . A fine point of the above example is that the clause is attached to the , not the sub- within the . This is necessary because data-modifying statements are only allowed in clauses that are attached to the top-level statement. However, normal visibility rules apply, so it is possible to refer to the statement's output from the sub- . Data-modifying statements in usually have clauses (see Section 6.4), as shown in the example above. It is the output of the clause, not the target table of the data-modifying statement, that forms the temporary table that can be referred to by the rest of the query. If a data-modifying statement in lacks a clause, then it forms no temporary table and cannot be referred to in the rest of the query. Such a statement will be executed nonetheless. A not-particularly-useful example is: WITH t AS ( DELETE FROM foo ) DELETE FROM bar; This example would remove all rows from tables and . The number of affected rows reported to the client would only include rows removed from . Recursive self-references in data-modifying statements are not allowed. In some cases it is possible to work around this limitation by referring to the output of a recursive , for example: WITH RECURSIVE included_parts(sub_part, part) AS ( SELECT sub_part, part FROM parts WHERE part = 'our_product' UNION ALL SELECT p.sub_part, p.part FROM included_parts pr, parts p WHERE p.part = pr.sub_part ) DELETE FROM parts WHERE part IN (SELECT part FROM included_parts); This query would remove all direct and indirect subparts of a product. Data-modifying statements in are executed exactly once, and always to completion, independently of whether the primary query reads all (or indeed any) of their output. Notice that this is different from the rule for in : as stated in the previous section, execution of a is carried only as far as the primary query demands its output. The sub-statements in are executed concurrently with each other and with the main query. Therefore, when using data-modifying statements in , the order in which the specified updates actually happen is unpredictable. All the statements are executed with the same snapshot (see Chapter 13), so they cannot “see” one another's effects on the target tables. This alleviates the effects of the unpredictability of the actual order of row updates, and means that data is the only way to communicate changes between different sub-statements and the main query. An example of this is that in WITH t AS ( UPDATE products SET price = price * 1.05 RETURNING * ) SELECT * FROM products; the outer would return the original prices before the action of the , while in WITH t AS ( UPDATE products SET price = price * 1.05 RETURNING * ) SELECT * FROM t; the outer would return the updated data. Trying to update the same row twice in a single statement is not supported. Only one of the modifications takes place, but it is not easy (and sometimes not possible) to reliably predict which one. This also applies to deleting a row that was already updated in the same statement: only the update is performed. Therefore you should generally avoid trying to modify a single row twice in a single statement. In particular avoid writing sub-statements that could affect the same rows changed by the main statement or a sibling sub-statement. The effects of such a statement will not be predictable. At present, any table used as the target of a data-modifying statement in must not have a conditional rule, nor an rule, nor an rule that expands to multiple statements."
    },
    {
        "link": "https://dataforgelabs.com/advanced-sql-concepts/sql-recursive-hierarchy-query",
        "document": "SQL recursive hierarchy queries support processing hierarchical or tree-structured data through self-referencing. At the core of these queries lies the Common Table Expression (CTE), a feature that provides a temporary result set within a larger query. Recursion in a CTE allows you to repeatedly reference a result set to traverse levels in the hierarchy until a specific condition is met. This article dives into the mechanics of SQL recursive CTEs and discusses techniques to optimize their performance with practical examples.\n\nRelational databases are commonly used for graph data in applications like social networks and recommendation systems. Graphs consist of vertices and edges, where vertices represent entities or nodes and edges represent the connections or relationships between those nodes. In a database, vertices are typically stored in one table, while edges, which define the relationships between the vertices, are stored in a separate table. This distinction enables the efficient management of large graph structures, where nodes and their relationships are clearly defined and can be easily queried or updated. SQL recursive queries can achieve transitive closure by navigating relationships within graph-like data structures. They can retrieve all descendants or connections from a given node, allowing the traversal of the graph from one node to any other reachable node. You can produce results that reflect the entire set of connected nodes. The example below creates a recursive CTE that identifies the most environmentally friendly route to visit all five cities in the graph below. The path with the shortest total flight distance results in the lowest CO₂ emissions.\n\nA database generally does not automatically stop a recursive query and results in an infinite loop leading to stack overflow, where the system runs out of memory. You can prevent this issue by manually defining a termination check within the query. The terminator acts as a safeguard, ensuring the recursion stops when a termination condition is met, such as reaching a specific value or exceeding a defined threshold. Sometimes, cycles or interlinked data loops in hierarchical or graph-like data can prevent the termination condition from being reached. PostgreSQL (version 14 and above) directly supports the CYCLE clause for detecting such cycles in recursive queries. For example, consider a table called relationships storing organizational hierarchy. Each row represents an employee, and the superior_id indicates which employee the current one reports to. The code below shows a recursive CTE with cycle detection to check if there’s any cycle in the \"path\" hierarchy:\n\nRecursive queries are often used in real-time data systems, For instance, a recursive query could retrieve all replies to a specific comment to ensure that each user's comment actions, such as posting a new comment or replying to an existing one, are handled efficiently and in real time. However, in recursive queries, each step usually relies on the result of the previous recursion. This creates a chain in which the outcome of one step impacts the subsequent step. This dependency presents challenges in concurrent environments. For example, if another query locks the required data or modifies it during execution, it can cause delays, errors, or failures. In databases, concurrency is managed by setting transaction isolation levels, which determine how transactions interact with each other and the database. They create the following challenges."
    },
    {
        "link": "https://docs.sqlalchemy.org/14/orm/loading_relationships.html",
        "document": "A big part of SQLAlchemy is providing a wide range of control over how related objects get loaded when querying. By “related objects” we refer to collections or scalar associations configured on a mapper using . This behavior can be configured at mapper construction time using the parameter to the function, as well as by using options with the object.\n\nThe loading of relationships falls into three categories; lazy loading, eager loading, and no loading. Lazy loading refers to objects are returned from a query without the related objects loaded at first. When the given collection or reference is first accessed on a particular object, an additional SELECT statement is emitted such that the requested collection is loaded.\n\nEager loading refers to objects returned from a query with the related collection or scalar reference already loaded up front. The achieves this either by augmenting the SELECT statement it would normally emit with a JOIN to load in related rows simultaneously, or by emitting additional SELECT statements after the primary one to load collections or scalar references at once.\n\n“No” loading refers to the disabling of loading on a given relationship, either that the attribute is empty and is just never loaded, or that it raises an error when it is accessed, in order to guard against unwanted lazy loads.\n\nThe primary forms of relationship loading are:\n• None lazy loading - available via or the option, this is the form of loading that emits a SELECT statement at attribute access time to lazily load a related reference on a single object at a time. Lazy loading is detailed at Lazy Loading.\n• None joined loading - available via or the option, this form of loading applies a JOIN to the given SELECT statement so that related rows are loaded in the same result set. Joined eager loading is detailed at Joined Eager Loading.\n• None subquery loading - available via or the option, this form of loading emits a second SELECT statement which re-states the original query embedded inside of a subquery, then JOINs that subquery to the related table to be loaded to load all members of related collections / scalar references at once. Subquery eager loading is detailed at Subquery Eager Loading.\n• None select IN loading - available via or the option, this form of loading emits a second (or more) SELECT statement which assembles the primary key identifiers of the parent objects into an IN clause, so that all members of related collections / scalar references are loaded at once by primary key. Select IN loading is detailed at Select IN loading.\n• None raise loading - available via , , or the option, this form of loading is triggered at the same time a lazy load would normally occur, except it raises an ORM exception in order to guard against the application making unwanted lazy loads. An introduction to raise loading is at Preventing unwanted lazy loads using raiseload.\n• None no loading - available via , or the option; this loading style turns the attribute into an empty attribute ( or ) that will never load or have any loading effect. This seldom-used strategy behaves somewhat like an eager loader when objects are loaded in that an empty attribute or collection is placed, but for expired objects relies upon the default value of the attribute being returned on access; the net effect is the same except for whether or not the attribute name appears in the collection. may be useful for implementing a “write-only” attribute but this usage is not currently tested or formally supported.\n\nThe other, and possibly more common way to configure loading strategies is to set them up on a per-query basis against specific attributes using the method. Very detailed control over relationship loading is available using loader options; the most common are , , and . The option accepts either the string name of an attribute against a parent, or for greater specificity can accommodate a class-bound attribute directly: The loader options can also be “chained” using method chaining to specify how loading should occur further levels deep: Chained loader options can be applied against a “lazy” loaded collection. This means that when a collection or association is lazily loaded upon access, the specified option will then take effect: Above, the query will return objects without the collections loaded. When the collection on a particular object is first accessed, it will lazy load the related objects, but additionally apply eager loading to the collection on each member of . The above examples, using , are now referred to as 1.x style queries. The options system is available as well for 2.0 style queries using the method: Under the hood, is ultimately using the above based mechanism. The relationship attributes used to indicate loader options include the ability to add additional filtering criteria to the ON clause of the join that’s created, or to the WHERE criteria involved, depending on the loader strategy. This can be achieved using the method which will pass through an option such that loaded results are limited to the given filter criteria: When using limiting criteria, if a particular collection is already loaded it won’t be refreshed; to ensure the new criteria takes place, apply the option: In order to add filtering criteria to all occurrences of an entity throughout a query, regardless of loader strategy or where it occurs in the loading process, see the function. Using method chaining, the loader style of each link in the path is explicitly stated. To navigate along a path without changing the existing loader style of a particular attribute, the method/function may be used: A similar approach can be used to specify multiple sub-options at once, using the method: Deferred Loading across Multiple Entities - illustrates examples of combining relationship and column-oriented loader options. The loader options applied to an object’s lazy-loaded collections are “sticky” to specific object instances, meaning they will persist upon collections loaded by that specific object for as long as it exists in memory. For example, given the previous example: if the collection on a particular object loaded by the above query is expired (such as when a object’s transaction is committed or rolled back, or is used), when the collection is next accessed in order to re-load it, the collection will again be loaded using subquery eager loading.This stays the case even if the above object is accessed from a subsequent query that specifies a different set of options.To change the options on an existing object without expunging it and re-loading, they must be set explicitly in conjunction with the method: # change the options on Parent objects that were already loaded If the objects loaded above are fully cleared from the , such as due to garbage collection or that were used, the “sticky” options will also be gone and the newly created objects will make use of new options if loaded again. A future SQLAlchemy release may add more alternatives to manipulating the loader options on already-loaded objects.\n\nBy default, all inter-object relationships are lazy loading. The scalar or collection attribute associated with a contains a trigger which fires the first time the attribute is accessed. This trigger typically issues a SQL call at the point of access in order to load the related object or objects: The one case where SQL is not emitted is for a simple many-to-one relationship, when the related object can be identified by its primary key alone and that object is already present in the current . For this reason, while lazy loading can be expensive for related collections, in the case that one is loading lots of objects with simple many-to-ones against a relatively small set of possible target objects, lazy loading may be able to refer to these objects locally without emitting as many SELECT statements as there are parent objects. This default behavior of “load upon attribute access” is known as “lazy” or “select” loading - the name “select” because a “SELECT” statement is typically emitted when the attribute is first accessed. Lazy loading can be enabled for a given attribute that is normally configured in some other way using the loader option: # force lazy loading for an attribute that is set to # load some other way normally The strategy produces an effect that is one of the most common issues referred to in object relational mapping; the N plus one problem, which states that for any N objects loaded, accessing their lazy-loaded attributes means there will be N+1 SELECT statements emitted. In SQLAlchemy, the usual mitigation for the N+1 problem is to make use of its very capable eager load system. However, eager loading requires that the attributes which are to be loaded be specified with the up front. The problem of code that may access other attributes that were not eagerly loaded, where lazy loading is not desired, may be addressed using the strategy; this loader strategy replaces the behavior of lazy loading with an informative error being raised: Above, a object loaded from the above query will not have the collection loaded; if some code later on attempts to access this attribute, an ORM exception is raised. may be used with a so-called “wildcard” specifier to indicate that all relationships should use this strategy. For example, to set up only one attribute as eager loading, and all the rest as raise: The above wildcard will apply to all relationships not just on besides , but all those on the objects as well. To set up for only the objects, specify a full path with : Conversely, to set up the raise for just the objects: The option applies only to relationship attributes. For column-oriented attributes, the option supports the option which works in the same way. Changed in version 1.4.0: The “raiseload” strategies do not take place within the unit of work flush process, as of SQLAlchemy 1.4.0. This means that if the unit of work needs to load a particular attribute in order to complete its work, it will perform the load. It’s not always easy to prevent a particular relationship load from occurring within the UOW process particularly with less common kinds of relationships. The lazy=”raise” case is more intended for explicit attribute access within the application space.\n\nJoined eager loading is the most fundamental style of eager loading in the ORM. It works by connecting a JOIN (by default a LEFT OUTER join) to the SELECT statement emitted by a and populates the target scalar/collection from the same result set as that of the parent. At the mapping level, this looks like: Joined eager loading is usually applied as an option to a query, rather than as a default loading option on the mapping, in particular when used for collections rather than many-to-one-references. This is achieved using the loader option: The JOIN emitted by default is a LEFT OUTER JOIN, to allow for a lead object that does not refer to a related row. For an attribute that is guaranteed to have an element, such as a many-to-one reference to a related object where the referencing foreign key is NOT NULL, the query can be made more efficient by using an inner join; this is available at the mapping level via the flag: At the query option level, via the flag: The JOIN will right-nest itself when applied in a chain that includes an OUTER JOIN: On older versions of SQLite, the above nested right JOIN may be re-rendered as a nested subquery. Older versions of SQLAlchemy would convert right-nested joins into subqueries in all cases. Using in the context of eager loading relationships is not officially supported or recommended by SQLAlchemy and may not work with certain queries on various database backends. When is successfully used with a query that involves , SQLAlchemy will attempt to emit SQL that locks all involved tables. A central concept of joined eager loading when applied to collections is that the object must de-duplicate rows against the leading entity being queried. Such as above, if the object we loaded referred to three objects, the result of the SQL statement would have had three rows; yet the returns only one object. As additional rows are received for a object just loaded in a previous row, the additional columns that refer to new objects are directed into additional results within the collection of that particular object. This process is very transparent, however does imply that joined eager loading is incompatible with “batched” query results, provided by the method, when used for collection loading. Joined eager loading used for scalar references is however compatible with . The method will result in an exception thrown if a collection based joined eager loader is in play. To “batch” queries with arbitrarily large sets of result data while maintaining compatibility with collection-based joined eager loading, emit multiple SELECT statements, each referring to a subset of rows using the WHERE clause, e.g. windowing. Alternatively, consider using “select IN” eager loading which is potentially compatible with , provided that the database driver in use supports multiple, simultaneous cursors (SQLite, PostgreSQL drivers, not MySQL drivers or SQL Server ODBC drivers). Since joined eager loading seems to have many resemblances to the use of , it often produces confusion as to when and how it should be used. It is critical to understand the distinction that while is used to alter the results of a query, goes through great lengths to not alter the results of the query, and instead hide the effects of the rendered join to only allow for related objects to be present. The philosophy behind loader strategies is that any set of loading schemes can be applied to a particular query, and the results don’t change - only the number of SQL statements required to fully load related objects and collections changes. A particular query might start out using all lazy loads. After using it in context, it might be revealed that particular attributes or collections are always accessed, and that it would be more efficient to change the loader strategy for these. The strategy can be changed with no other modifications to the query, the results will remain identical, but fewer SQL statements would be emitted. In theory (and pretty much in practice), nothing you can do to the would make it load a different set of primary or related objects based on a change in loader strategy. How in particular achieves this result of not impacting entity rows returned in any way is that it creates an anonymous alias of the joins it adds to your query, so that they can’t be referenced by other parts of the query. For example, the query below uses to create a LEFT OUTER JOIN from to , however the added against is not valid - the entity is not named in the query: Above, is not valid since is not in the FROM list. The correct way to load the records and order by email address is to use : The statement above is of course not the same as the previous one, in that the columns from are not included in the result at all. We can add back in, so that there are two joins - one is that which we are ordering on, the other is used anonymously to load the contents of the collection: What we see above is that our usage of is to supply JOIN clauses we’d like to use in subsequent query criterion, whereas our usage of only concerns itself with the loading of the collection, for each in the result. In this case, the two joins most probably appear redundant - which they are. If we wanted to use just one JOIN for collection loading as well as ordering, we use the option, described in Routing Explicit Joins/Statements into Eagerly Loaded Collections below. But to see why does what it does, consider if we were filtering on a particular : Above, we can see that the two JOINs have very different roles. One will match exactly one row, that of the join of and where . The other LEFT OUTER JOIN will match all rows related to , and is only used to populate the collection, for those objects that are returned. By changing the usage of to another style of loading, we can change how the collection is loaded completely independently of SQL used to retrieve the actual rows we want. Below we change into : When using joined eager loading, if the query contains a modifier that impacts the rows returned externally to the joins, such as when using DISTINCT, LIMIT, OFFSET or equivalent, the completed statement is first wrapped inside a subquery, and the joins used specifically for joined eager loading are applied to the subquery. SQLAlchemy’s joined eager loading goes the extra mile, and then ten miles further, to absolutely ensure that it does not affect the end result of the query, only the way collections and related objects are loaded, no matter what the format of the query is.\n\nSubqueryload eager loading is configured in the same manner as that of joined eager loading; for the parameter, we would specify rather than , and for the option we use the option rather than the option. The operation of subquery eager loading is to emit a second SELECT statement for each relationship to be loaded, across all result objects at once. This SELECT statement refers to the original SELECT statement, wrapped inside of a subquery, so that we retrieve the same list of primary keys for the primary object being returned, then link that to the sum of all the collection members to load them at once: The subqueryload strategy has many advantages over joined eager loading in the area of loading collections. First, it allows the original query to proceed without changing it at all, not introducing in particular a LEFT OUTER JOIN that may make it less efficient. Secondly, it allows for many collections to be eagerly loaded without producing a single query that has many JOINs in it, which can be even less efficient; each relationship is loaded in a fully separate query. Finally, because the additional query only needs to load the collection items and not the lead object, it can use an inner JOIN in all cases for greater query efficiency. Disadvantages of subqueryload include that the complexity of the original query is transferred to the relationship queries, which when combined with the use of a subquery, can on some backends in some cases (notably MySQL) produce significantly slow queries. Additionally, the subqueryload strategy can only load the full contents of all collections at once, is therefore incompatible with “batched” loading supplied by , both for collection and scalar relationships. The newer style of loading provided by solves these limitations of . A query which makes use of in conjunction with a limiting modifier such as , , or should always include against unique column(s) such as the primary key, so that the additional queries emitted by include the same ordering as used by the parent query. Without it, there is a chance that the inner query could return the wrong rows: # incorrect if User.name is not unique Why is ORDER BY recommended with LIMIT (especially with subqueryload())? - detailed example\n\nSelect IN loading is similar in operation to subquery eager loading, however the SELECT statement which is emitted has a much simpler structure than that of subquery eager loading. In most cases, selectin loading is the most simple and efficient way to eagerly load collections of objects. The only scenario in which selectin eager loading is not feasible is when the model is using composite primary keys, and the backend database does not support tuples with IN, which currently includes SQL Server. “Select IN” eager loading is provided using the argument to or by using the loader option. This style of loading emits a SELECT that refers to the primary key values of the parent object, or in the case of a many-to-one relationship to the those of the child objects, inside of an IN clause, in order to load related associations: Above, the second SELECT refers to , where the “5” and “7” are the primary key values for the previous two objects loaded; after a batch of objects are completely loaded, their primary key values are injected into the clause for the second SELECT. Because the relationship between and has a simple primary join condition and provides that the primary key values for can be derived from , the statement has no joins or subqueries at all. Changed in version 1.3: selectin loading can omit the JOIN for a simple one-to-many collection. For simple many-to-one loads, a JOIN is also not needed as the foreign key value from the parent object is used: Changed in version 1.3.6: selectin loading can also omit the JOIN for a simple many-to-one relationship. Select IN loading also supports many-to-many relationships, where it currently will JOIN across all three tables to match rows from one side to the other. Things to know about this kind of loading include:\n• None The SELECT statement emitted by the “selectin” loader strategy, unlike that of “subquery”, does not require a subquery nor does it inherit any of the performance limitations of the original query; the lookup is a simple primary key lookup and should have high performance.\n• None The special ordering requirements of subqueryload described at The Importance of Ordering also don’t apply to selectin loading; selectin is always linking directly to a parent primary key and can’t really return the wrong result.\n• None “selectin” loading, unlike joined or subquery eager loading, always emits its SELECT in terms of the immediate parent objects just loaded, and not the original type of object at the top of the chain. So if eager loading many levels deep, “selectin” loading still will not require any JOINs for simple one-to-many or many-to-one relationships. In comparison, joined and subquery eager loading always refer to multiple JOINs up to the original parent.\n• None The strategy emits a SELECT for up to 500 parent primary key values at a time, as the primary keys are rendered into a large IN expression in the SQL statement. Some databases like Oracle have a hard limit on how large an IN expression can be, and overall the size of the SQL string shouldn’t be arbitrarily large.\n• None As “selectin” loading relies upon IN, for a mapping with composite primary keys, it must use the “tuple” form of IN, which looks like . This syntax is not currently supported on SQL Server and for SQLite requires at least version 3.15. There is no special logic in SQLAlchemy to check ahead of time which platforms support this syntax or not; if run against a non-supporting platform, the database will return an error immediately. An advantage to SQLAlchemy just running the SQL out for it to fail is that if a particular database does start supporting this syntax, it will work without any changes to SQLAlchemy (as was the case with SQLite). In general, “selectin” loading is probably superior to “subquery” eager loading in most ways, save for the syntax requirement with composite primary keys and possibly that it may emit many SELECT statements for larger result sets. As always, developers should spend time looking at the statements and results generated by their applications in development to check that things are working efficiently.\n\nThe behavior of is such that joins are created automatically, using anonymous aliases as targets, the results of which are routed into collections and scalar references on loaded objects. It is often the case that a query already includes the necessary joins which represent a particular collection or scalar reference, and the joins added by the joinedload feature are redundant - yet you’d still like the collections/references to be populated. For this SQLAlchemy supplies the option. This option is used in the same manner as the option except it is assumed that the will specify the appropriate joins explicitly. Below, we specify a join between and and additionally establish this as the basis for eager loading of : If the “eager” portion of the statement is “aliased”, the path should be specified using , which allows the specific construct to be passed: # use an alias of the Address entity The path given as the argument to needs to be a full path from the starting entity. For example if we were loading , the option would be used as: When we use , we are constructing ourselves the SQL that will be used to populate collections. From this, it naturally follows that we can opt to modify what values the collection is intended to store, by writing our SQL to load a subset of elements for collections or scalar attributes. As an example, we can load a object and eagerly load only particular addresses into its collection by filtering the joined data, routing it using , also using to ensure any already-loaded collections are overwritten: The above query will load only objects which contain at least object that contains the substring in its field; the collection will contain only these entries, and not any other entries that are in fact associated with the collection. In all cases, the SQLAlchemy ORM does not overwrite already loaded attributes and collections unless told to do so. As there is an identity map in use, it is often the case that an ORM query is returning objects that were in fact already present and loaded in memory. Therefore, when using to populate a collection in an alternate way, it is usually a good idea to use as illustrated above so that an already-loaded collection is refreshed with the new data. will reset all attributes that were already present, including pending changes, so make sure all data is flushed before using it. Using the with its default behavior of autoflush is sufficient. The customized collection we load using is not “sticky”; that is, the next time this collection is loaded, it will be loaded with its usual default contents. The collection is subject to being reloaded if the object is expired, which occurs whenever the , methods are used assuming default session settings, or the or methods are used.\n\nThis is an advanced technique! Great care and testing should be applied. The ORM has various edge cases where the value of an attribute is locally available, however the ORM itself doesn’t have awareness of this. There are also cases when a user-defined system of loading attributes is desirable. To support the use case of user-defined loading systems, a key function is provided. This function is basically equivalent to Python’s own function, except that when applied to a target object, SQLAlchemy’s “attribute history” system which is used to determine flush-time changes is bypassed; the attribute is assigned in the same way as if the ORM loaded it that way from the database. The use of can be combined with another key event known as to produce attribute-population behaviors when an object is loaded. One such example is the bi-directional “one-to-one” case, where loading the “many-to-one” side of a one-to-one should also imply the value of the “one-to-many” side. The SQLAlchemy ORM does not consider backrefs when loading related objects, and it views a “one-to-one” as just another “one-to-many”, that just happens to be one row. Given the following mapping: If we query for an row, and then ask it for , we will get an extra SELECT: This SELECT is redundant because is the same value as . We can create an on-load rule to populate this for us: Now when we query for , we will get from the joined eager load, and from our event:"
    },
    {
        "link": "https://digitalocean.com/community/tutorials/how-to-use-one-to-many-database-relationships-with-flask-sqlalchemy",
        "document": "The author selected the Free and Open Source Fund to receive a donation as part of the Write for DOnations program.\n\nFlask is a lightweight Python web framework that provides useful tools and features for creating web applications in the Python Language. SQLAlchemy is an SQL toolkit that provides efficient and high-performing database access for relational databases. It provides ways to interact with several database engines such as SQLite, MySQL, and PostgreSQL. It gives you access to the database’s SQL functionalities. And it also gives you an Object Relational Mapper (ORM), which allows you to make queries and handle data using simple Python objects and methods. Flask-SQLAlchemy is a Flask extension that makes using SQLAlchemy with Flask easier, providing you tools and methods to interact with your database in your Flask applications through SQLAlchemy.\n\nA one-to-many database relationship is a relationship between two database tables where a record in one table can reference several records in another table. For example, in a blogging application, a table for storing posts can have a one-to-many relationship with a table for storing comments. Each post can reference many comments, and each comment references a single post; therefore, one post has a relationship with many comments. The post table is a parent table, while the comments table is a child table — a record in the parent table can reference many records in the child table. This relationship is important to enable access to related data in each table.\n\nIn this tutorial, you’ll build a small blogging system that demonstrates how to build one-to-many relationships using the Flask-SQLAlchemy extension. You’ll create a relationship between posts and comments, where each blog post can have several comments.\n• A local Python 3 programming environment. Follow the tutorial for your distribution in How To Install and Set Up a Local Programming Environment for Python 3 series. In this tutorial we’ll call our project directory .\n• An understanding of basic Flask concepts, such as routes, view functions, and templates. If you are not familiar with Flask, check out How to Create Your First Web Application Using Flask and Python and How to Use Templates in a Flask Application.\n• An understanding of basic HTML concepts. You can review our How To Build a Website with HTML tutorial series for background knowledge.\n\nIn this step, you’ll install the necessary packages for your application.\n\nWith your virtual environment activated, use to install Flask and Flask-SQLAlchemy:\n\nOnce the installation is successfully finished, you’ll see a line similar to the following at the end of the output:\n\nWith the required Python packages installed, you’ll set up the database next.\n\nStep 2 — Setting up the Database and Models\n\nIn this step, you’ll set up your database, and create SQLAlchemy database models — Python classes that represent your database tables. You’ll create a model for your blog posts and a model for comments. You’ll initiate the database, create a table for posts, and add a table for comments based on the models you’ll declare. You’ll also insert a few posts and comments into your database.\n\nOpen a file called in your directory. This file will have code for setting up the database and your Flask routes:\n\nThis file will connect to an SQLite database called , and will have two classes: A class called that represents your database posts table, and a class representing the comments table. This file will also contain your Flask routes. Add the following statements at the top of :\n\nHere, you import the module, which gives you access to miscellaneous operating system interfaces. You’ll use it to construct a file path for your database file.\n\nFrom the package, you then import the necessary helpers you need for your application: the class to create a Flask application instance, the function to render templates, the object to handle requests, the function to construct URLs for routes, and the function for redirecting users. For more information on routes and templates, see How To Use Templates in a Flask Application.\n\nYou then import the class from the Flask-SQLAlchemy extension, which gives you access to all the functions and classes from SQLAlchemy, in addition to helpers, and functionality that integrates Flask with SQLAlchemy. You’ll use it to create a database object that connects to your Flask application, allowing you to create and manipulate tables using Python classes, objects, and functions without needing to use the SQL language.\n\nBelow the imports, you’ll set up a database file path, instantiate your Flask application, and configure and connect your application with SQLAlchemy. Add the following code:\n\nHere, you construct a path for your SQLite database file. You first define a base directory as the current directory. You use the function to get the absolute path of the current file’s directory. The special variable holds the pathname of the current file. You store the absolute path of the base directory in a variable called .\n\nYou then create a Flask application instance called , which you use to configure two Flask-SQLAlchemy configuration keys:\n• : The database URI to specify the database you want to establish a connection with. In this case, the URI follows the format . You use the function to intelligently join the base directory you constructed and stored in the variable, and the file name. This will connect to a database file in your directory. The file will be created once you initiate the database.\n• : A configuration to enable or disable tracking modifications of objects. You set it to to disable tracking and use less memory. For more, see the configuration page in the Flask-SQLAlchemy documentation.\n\nAfter configuring SQLAlchemy by setting a database URI and disabling tracking, you create a database object using the class, passing the application instance to connect your Flask application with SQLAlchemy. You store your database object in a variable called . You’ll use this object to interact with your database.\n\nWith the database connection established and the database object created, you’ll use the database object to create a database table for posts and one for comments. Tables are represented by a model — a Python class that inherits from a base class Flask-SQLAlchemy provides through the database instance you created earlier. To define the posts and comments tables as models, add the following two classes to your file:\n\nHere, you create a model and a model, which inherit from the class.\n\nThe model represents the post table. You use the class to define its columns. The first argument represents the column type, and additional arguments represent the column configuration.\n\nYou define the following columns for the model:\n• : The post ID. You define it as an integer with . defines this column as a primary key, which will assign it a unique value by the database for each entry (that is, each post).\n• : The post’s title. A string with a maximum length of 100 characters.\n• : The post’s content. indicates the column holds long texts.\n\nThe class attribute defines a One-to-Many relationship between the model and the model. You use the method, passing it the name of the comments model ( in this case). You use the parameter to add a back reference that behaves like a column to the model. This way, you can access the post the comment was posted on using a attribute. For example, if you have a comment object in a variable called , you will be able to access the post the comment belongs to using . You’ll see an example demonstrating this later.\n\nSee the SQLAlchemy documentation for column types other than the types you used in the preceding code block.\n\nThe special function allows you to give each object a string representation to recognize it for debugging purposes.\n\nThe model represents the comment table. You define the following columns for it:\n• : The comment ID. You define it as an integer with . defines this column as a primary key, which will assign it a unique value by the database for each entry (that is, each comment).\n• : The comment’s content. indicates the column holds long texts.\n• : An integer foreign key you construct using the class, which is a key that links a table with another, using that table’s primary key. This links a comment to a post using the primary key of the post, which is its ID. Here, the table is a parent table, which indicates that each post has many comments. The table is a child table. Each comment is related to a parent post using the post’s ID. Therefore, each comment has a column that can be used to access the post the comment was posted on.\n\nThe special function in the model shows the first 20 characters of the comment’s content to give a comment object a short string representation.\n\nThe file will now look as follows:\n\nNow that you’ve set the database connection and the post and comment models, you’ll use the Flask shell to create your database and your post and comment tables based on the models you declared.\n\nWith your virtual environment activated, set the file as your Flask application using the environment variable:\n\nThen open the Flask shell using the following command in your directory:\n\nA Python interactive shell will be opened. This special shell runs commands in the context of your Flask application, so that the Flask-SQLAlchemy functions you’ll call are connected to your application.\n\nImport the database object and the post and comment models, and then run the function to create the tables that are associated with your models:\n\nLeave the shell running, open another terminal window and navigate to your directory. You will now see a new file called in .\n\nIf you receive an error, make sure your database URI and your model declaration are correct.\n\nAfter creating the database and the post and comment tables, you’ll create a file in your directory to add some posts and comments to your database.\n\nAdd the following code to it. This file will create three post objects and four comment objects, and add them to the database:\n\nHere, you import the database object, the model, and the model from the file.\n\nYou create a few post objects using the model, passing the post’s title to the parameter and the post’s content to the parameter.\n\nYou then create a few comment objects, passing the comment’s content. You have two methods you can use to associate a comment with the post it belongs to. You can pass the post object to the parameter as demonstrated in the and objects. And you can also pass the post ID to the parameter, as demonstrated in the and objects. So you can just pass the integer ID of the post if you don’t have the post object in your code.\n\nAfter defining the post and comment objects, you use the to add all post and comment objects to the database session, which manages transactions. Then you use the method to commit the transaction and apply the changes to the database. For more on SQLAlchemy database sessions, see step 2 of the How to Use Flask-SQLAlchemy to Interact with Databases in a Flask Application tutorial.\n\nRun the file to execute the code and add the data to the database:\n\nTo take a look at the data you added to your database, open the flask shell to query all posts and display their titles and the content of each post’s comments:\n\nRun the following code. This queries all posts and displays each post title and the comments of each post below it:\n\nHere, you import the model from the file. You query all the posts that exist in the database using the method on the attribute, and save the result in a variable called . Then you use a loop to go through each item in the variable. You print the title and then use another loop to go through each comment belonging to the post. You access the post’s comments using . You print the comment’s content and then print the string to separate between posts.\n\nYou’ll get the following output:\n\nAs you can see, you can access the data of each post and the comments of each post with very little code.\n\nAt this point, you have several posts and comments in your database. Next, you’ll create a Flask route for the index page and display all of the posts in your database on it.\n\nIn this step, you’ll create a route and a template to display all the posts in the database on the index page.\n\nOpen your file to add a route for the index page to it:\n\nAdd the following route at the end of the file:\n\nHere, you create an view function using the decorator. In this function, you query the database and get all the posts like you did in the previous step. You store the query result in a variable called and then you pass it to a template file you render using the helper function.\n\nBefore you create the template file on which you’ll display the existing posts in the database, you’ll first create a base template, which will have all the basic HTML code other templates will also use to avoid code repetition. Then you’ll create the template file you rendered in your function. To learn more about templates, see How to Use Templates in a Flask Application.\n\nAdd the following code inside the file:\n\nThis base template has all the HTML boilerplate you’ll need to reuse in your other templates. The block will be replaced to set a title for each page, and the block will be replaced with the content of each page. The navigation bar has three links: one for the index page, which links to the view function using the helper function, one for a Comments page, and one for an About page if you choose to add one to your application. You’ll edit this file later after you add a page for displaying all the latest comments to make the Comments link functional.\n\nNext, open a new template file. This is the template you referenced in the file:\n\nAdd the following code to it:\n\nHere, you extend the base template and replace the contents of the content block. You use an heading that also serves as a title. You use a Jinja loop in the line to go through each post in the variable that you passed from the view function to this template. You display the post ID, its title, and the post content. The post title will later link to a page that displays the individual post and its comments.\n\nWhile in your directory with your virtual environment activated, tell Flask about the application ( in this case) using the environment variable. Then set the environment variable to to run the application in development mode and get access to the debugger. For more information about the Flask debugger, see How To Handle Errors in a Flask Application. Use the following commands to do this:\n\nWith the development server running, visit the following URL using your browser:\n\nYou’ll see the posts you added to the database in a page similar to the following:\n\nYou’ve displayed the posts you have in your database on the index page. Next, you’ll create a route for a post page, where you will display the details of each post and its comments below it.\n\nIn this step, you’ll create a route and a template to display the details of each post on a dedicated page, and the post’s comments below it.\n\nBy the end of this step, the URL will be a page that displays the first post (because it has the ID ) and its comments. The URL will display the post with the associated number, if it exists.\n\nLeave the development server running and open a new terminal window.\n\nAdd the following route at the end of the file:\n\nHere, you use the route , with being a converter that converts the default string in the URL into an integer. is the URL variable that will determine the post you’ll display on the page.\n\nThe ID is passed from the URL to the view function through the parameter. Inside the function, you query the post table and retrieve a post by its ID using the method. This will save the post data in the variable if it exists, and respond with a HTTP error if no post with the given ID exists in the database.\n\nYou render a template called and pass it the post you retrieved.\n\nType the following code in it. This will be similar to the template, except that it will only display a single post:\n\nHere, you extend the base template, set the post title as a page title, display the post ID, post title, and the post content. Then, you go through the post comments available via . You display the comment ID, and the contents of the comment.\n\nUse your browser to navigate to the URL for the second post:\n\nYou’ll see a page similar to the following:\n\nNext, edit to make the title of the post link to the individual post:\n\nEdit the value of the attribute of the post title’s link inside the loop:\n\nNavigate to your index page or refresh it:\n\nClick on each of the post titles on the index page. You’ll now see that each post links to the proper post page.\n\nYou’ve now created a page for displaying individual posts. Next, you’ll add a web form to the post page to allow users to add new comments.\n\nIn this step, you’ll edit the route and its view function, which handles displaying an individual post. You’ll add a web form below each post to allow users to add comments to that post, then you’ll handle the comment submission and add it to the database.\n\nFirst, open the template file to add a web form consisting of a text area for the comment’s content, and an Add Comment submit button.\n\nEdit the file by adding a form below the H3 heading, and directly above the loop:\n\nHere, you add a tag with the attribute set to to indicate that the form will submit a POST request.\n\nYou have a text area for the comment’s content, and a submit button.\n\nWith the development server running, use your browser to navigate to a post:\n\nYou’ll see a page similar to the following:\n\nThis form sends a POST request to the view function, but because there is no code to handle the form submission, the form currently does not work.\n\nNext, you will add code to the view function to handle the form submission and add the new comment to the database. Open to handle the POST request the user submits:\n\nEdit the route and its view function to look as follows:\n\nYou allow both GET and POST requests using the parameter. GET requests are used to retrieve data from the server. POST requests are used to post data to a specific route. By default, only GET requests are allowed.\n\nInside the condition, you handle the POST request the user will submit via the form. You create a comment object using the model, passing it the content of the submitted comment which you extract from the object. You specify the post the comment belongs to using the parameter, passing it the object you retrieved using the post ID, with the method.\n\nYou add the comment object you constructed to the database session, commit the transaction, and redirect to the post page.\n\nNow refresh the post page on your browser, write a comment, and submit it. You’ll see your new comment below the post.\n\nYou now have a web form that allows users to add comments to a post. For more on web forms, see How To Use Web Forms in a Flask Application. For a more advanced and more secure method of managing web forms, see How To Use and Validate Web Forms with Flask-WTF. Next, you’ll add a page that displays all the comments in the database and the posts they were posted on.\n\nIn this step, you’ll add a Comments page where you will display all the comments in the database, ordering them by displaying the newest comments first. Each comment will have the title and link of the post the comment was posted on.\n\nAdd the following route to the end of the file. This fetches all the comments in the database, ordered by the latest first. It then passes them to a template file called , which you’ll create later:\n\nYou use the method on the attribute to fetch all the comments in a specific order. In this case you use the method on the column to fetch comments in descending order, with the latest comments being first. Then you use the method to get the result and save it to a variable called .\n\nYou render a template called , passing it the object which contains all comments ordered by the latest first.\n\nType the following code inside it. This will display the comments and link to the post they belong to:\n\nHere, you extend the base template, set a title, and go through the comments using a loop. You display the comment’s ID, its content, and a link to the post it belongs to. You access the post data via .\n\nUse your browser to navigate to the comments page:\n\nYou’ll see a page similar to the following:\n\nNow edit the template to make the Comments navbar link point to this Comments page:\n\nEdit the navigation bar to look as follows:\n\nRefresh your comments page, and you’ll see that the Comments navbar link works.\n\nYou now have a page that displays all the comments in the database. Next, you’ll add a button below each comment on the post page to allow users to delete it.\n\nIn this step, you’ll add a Delete Comment button below each comment to allow users to delete unwanted comments.\n\nFirst, you’ll add a new route that accepts POST requests. The view function will receive the ID of the comment you want to delete, fetch it from the database, delete it, and the redirect to post page the deleted comment was on.\n\nAdd the following route to the end of the file.\n\nHere, instead of using the usual decorator, you use the decorator introduced in Flask version 2.0.0, which added shortcuts for common HTTP methods. For example, is a shortcut for . This means that this view function only accepts POST requests, and navigating to the route on your browser will return a error, because web browsers default to GET requests. To delete a comment, the user clicks on a button that sends a POST request to this route.\n\nThis view function receives the ID of the comment to be deleted via the URL variable. You use the method to get a comment and save it in a variable, or respond with a in case the comment doesn’t exist. You save the post ID of the post the comment belongs to in a variable, which you’ll use to redirect to the post after deleting the comment.\n\nYou use the method on the database session in the line , passing it the comment object. This sets up the session to delete the comment whenever the transaction is committed. Because you don’t need to perform any other modifications, you directly commit the transaction using . Lastly, you redirect the user to the post the now-deleted comment was posted on.\n\nNext, edit the template to add a Delete Comment button below each comment:\n\nEdit the loop by adding a new tag directly below the comment content:\n\nHere, you have a web form that submits a POST request to the view function. You pass as an argument for the parameter to specify the comment that will be deleted. You use the method function available in web browsers to display a confirmation message before submitting the request.\n\nNow navigate to a post page on your browser:\n\nYou’ll see a Delete Comment button below each comment. Click on it, and confirm the deletion. You’ll see that the comment has been deleted.\n\nYou now have a way of deleting comments from the database.\n\nYou built a small blogging system that demonstrates how to manage one-to-many relationships using the Flask-SQLAlchemy extension. You learned how to connect a parent table with a child table, associate a child object with its parent and add it to the database, and how to access child data from a parent entry and vise versa.\n\nIf you would like to read more about Flask, check out the other tutorials in the How To Build Web Applications with Flask series."
    },
    {
        "link": "https://stackoverflow.com/questions/45985063/proper-querying-database-relationships-using-sqlalchemy",
        "document": "You're making your query overly complicated. You've given SQLAlchemy relationship information, so don't worry about trying to create a query with a join.\n\nTo get a list of fields, and display the related ball data, I would do something along these lines (apologies if this doesn't totally match your schema, but it should be close):\n\nThe whole point of an ORM is that you get back objects with properties that will link between related tables, so that you can access data as you would any other objects and variables in your code, like , and don't have to mess around with joins or other complex queries too much."
    },
    {
        "link": "http://docs.sqlalchemy.org/en/latest/tutorial/orm_related_objects.html",
        "document": "In this section, we will cover one more essential ORM concept, which is how the ORM interacts with mapped classes that refer to other objects. In the section Declaring Mapped Classes, the mapped class examples made use of a construct called . This construct defines a linkage between two different mapped classes, or from a mapped class to itself, the latter of which is called a self-referential relationship.\n\nTo describe the basic idea of , first we’ll review the mapping in short form, omitting the mappings and other directives:\n\nAbove, the class now has an attribute and the class has an attribute . The construct, in conjunction with the construct to indicate typing behavior, will be used to inspect the table relationships between the objects that are mapped to the and classes. As the object representing the table has a which refers to the table, the can determine unambiguously that there is a one to many relationship from the class to the class, along the relationship; one particular row in the table may be referenced by many rows in the table.\n\nAll one-to-many relationships naturally correspond to a many to one relationship in the other direction, in this case the one noted by . The parameter, seen above configured on both objects referring to the other name, establishes that each of these two constructs should be considered to be complimentary to each other; we will see how this plays out in the next section.\n\nWe can start by illustrating what does to instances of objects. If we make a new object, we can note that there is a Python list when we access the element: This object is a SQLAlchemy-specific version of Python which has the ability to track and respond to changes made to it. The collection also appeared automatically when we accessed the attribute, even though we never assigned it to the object. This is similar to the behavior noted at Inserting Rows using the ORM Unit of Work pattern where it was observed that column-based attributes to which we don’t explicitly assign a value also display as automatically, rather than raising an as would be Python’s usual behavior. As the object is still transient and the that we got from has not been mutated (i.e. appended or extended), it’s not actually associated with the object yet, but as we make changes to it, it will become part of the state of the object. The collection is specific to the class which is the only type of Python object that may be persisted within it. Using the method we may add an object: At this point, the collection as expected contains the new object: As we associated the object with the collection of the instance, another behavior also occurred, which is that the relationship synchronized itself with the relationship, such that we can navigate not only from the object to the object, we can also navigate from the object back to the “parent” object: This synchronization occurred as a result of our use of the parameter between the two objects. This parameter names another for which complementary attribute assignment / list mutation should occur. It will work equally well in the other direction, which is that if we create another object and assign to its attribute, that becomes part of the collection on that object: We actually made use of the parameter as a keyword argument in the constructor, which is accepted just like any other mapped attribute that was declared on the class. It is equivalent to assignment of the attribute after the fact: We now have a and two objects that are associated in a bidirectional structure in memory, but as noted previously in Inserting Rows using the ORM Unit of Work pattern , these objects are said to be in the transient state until they are associated with a object. We make use of the that’s still ongoing, and note that when we apply the method to the lead object, the related object also gets added to that same : The above behavior, where the received a object, and followed along the relationship to locate a related object, is known as the save-update cascade and is discussed in detail in the ORM reference documentation at Cascades. The three objects are now in the pending state; this means they are ready to be the subject of an INSERT operation but this has not yet proceeded; all three objects have no primary key assigned yet, and in addition, the and objects have an attribute called which refers to the that has a referring to the column; these are also as the objects are not yet associated with a real database row: It’s at this stage that we can see the very great utility that the unit of work process provides; recall in the section INSERT usually generates the “values” clause automatically, rows were inserted into the and tables using some elaborate syntaxes in order to automatically associate the columns with those of the rows. Additionally, it was necessary that we emit INSERT for rows first, before those of , since rows in are dependent on their parent row in for a value in their column. When using the , all this tedium is handled for us and even the most die-hard SQL purist can benefit from automation of INSERT, UPDATE and DELETE statements. When we the transaction all steps invoke in the correct order, and furthermore the newly generated primary key of the row is applied to the column appropriately:\n\nIn the last step, we called which emitted a COMMIT for the transaction, and then per expired all objects so that they refresh for the next transaction. When we next access an attribute on these objects, we’ll see the SELECT emitted for the primary attributes of the row, such as when we view the newly generated primary key for the object: The object now has a persistent collection that we may also access. As this collection consists of an additional set of rows from the table, when we access this collection as well we again see a lazy load emitted in order to retrieve the objects: Collections and related attributes in the SQLAlchemy ORM are persistent in memory; once the collection or attribute is populated, SQL is no longer emitted until that collection or attribute is expired. We may access again as well as add or remove items and this will not incur any new SQL calls: While the loading emitted by lazy loading can quickly become expensive if we don’t take explicit steps to optimize it, the network of lazy loading at least is fairly well optimized to not perform redundant work; as the collection was refreshed, per the identity map these are in fact the same instances as the and objects we’ve been dealing with already, so we’re done loading all attributes in this particular object graph: The issue of how relationships load, or not, is an entire subject onto itself. Some additional introduction to these concepts is later in this section at Loader Strategies."
    },
    {
        "link": "https://groups.google.com/g/sqlalchemy/c/1fqhxuVkWlQ",
        "document": "I've checked sqlalchemy.orm.with_parent (Python function, in Query API) documentation entry, however, it's not clear to me howconstruct can fit in the implementation instead of. I guess it would require a major change in how the library (Flask-SQLAlchemy) is currently designed as it functionally extendsand pass the extended class toand other constructs as well.\n\nHi Mike - Thank you for your insights. Actually, this is part of upgrading Flask-SQLAlchemy library dependency to 1.4.0b3 and eventually 2.0. The snippet above is extracted from a test case that didn't pass against 1.4.0b3.\n\nyes so, SQLAlchemy 2.0's approach is frankly at odds with the spirit of Flask-SQLAlchemy. The Query and \"dynamic\" loaders are staying around largely so that Flask can come on board, however the patterns in F-S are pretty much the ones I want to get away from.\n\n\n\n2.0's spirit is one where the act of creating a SELECT statement is a standalone thing that is separate from being attached to any specific class (really all of SQLAlchemy was like this, but F-S has everyone doing the Model.query thing that I've always found to be more misleading than helpful), but SELECT statements are now also disconnected from any kind of \"engine\" or \"Session\" when constructed. \n\n\n\nas for with_parent(), with_parent is what the dynamic loader actually uses to create the query. so this is a matter of code organization.\n\n\n\nF-S would have you say:\n\n\n\nSQLAlchemy 2.0 would have you say instead:\n\n\n\nNoting above, a web framework integration may still wish to provide the \"session\" to data-oriented methods and manage its scope, but IMO it should be an explicit object passed around. The database connection / transaction shouldn't be made to appear to be inside the ORM model object, since that's not what's actually going on.\n\n\n\nIf you look at any commentary anywhere about SQLAlchemy, the top complaints are:\n\n\n\nSQLAlchemy 2.0 seeks to streamline the act of ORMing such that the user *is* writing SQL, they're running it into an execute() method, and they are managing the scope of connectivity and transactions in an obvious way. People don't necessarily want bloat and verbosity but they do want to see explicitness when the computer is being told to do something, especially running a SQL query. We're trying to hit that balance as closely as possible.\n\n\n\nThe above style also has in mind compatibility with asyncio, which we now support. With asyncio, it's very important that the boundary where IO occurs is very obvious. Hence the Session.execute() method now becomes the place where users have to \"yield\". With the older Query interface, the \"yields\" would be all over the place and kind of arbirary, since some Query methods decide to execute at one point or another. \n\n\n\nFlask-SQLAlchemy therefore has to decide where it wants to go with this direction, and there are options, including sticking with the legacy query / dynamic loader, perhaps vendoring a new interface that behaves in the flask-sqlalchemy style but uses 2.0-style patterns under the hood, or it can go along with the 2.0 model for future releases. From SQLAlchemy's point of view, the Query was always not well thought out and was inconsistent with how Core worked, and I've wanted for years to resolve that problem."
    },
    {
        "link": "https://docs.sqlalchemy.org/14/orm/queryguide.html",
        "document": "This section provides an overview of emitting queries with the SQLAlchemy ORM using 2.0 style usage.\n\nReaders of this section should be familiar with the SQLAlchemy overview at SQLAlchemy 1.4 / 2.0 Tutorial, and in particular most of the content here expands upon the content at Selecting Rows with Core or ORM.\n\nSELECT statements are produced by the function which returns a object:\n\nTo invoke a with the ORM, it is passed to :\n\nThe construct accepts ORM entities, including mapped classes as well as class-level attributes representing mapped columns, which are converted into ORM-annotated and elements at construction time. A object that contains ORM-annotated entities is normally executed using a object, and not a object, so that ORM-related features may take effect, including that instances of ORM-mapped objects may be returned. When using the directly, result rows will only contain column-level data. Below we select from the entity, producing a that selects from the mapped to which is mapped: When selecting from ORM entities, the entity itself is returned in the result as a row with a single element, as opposed to a series of individual columns; for example above, the returns objects that have just a single element per row, that element holding onto a object: When selecting a list of single-element rows containing ORM entities, it is typical to skip the generation of objects and instead receive ORM entities directly, which is achieved using the method: ORM Entities are named in the result row based on their class name, such as below where we SELECT from both and at the same time: The attributes on a mapped class, such as and , have a similar behavior as that of the entity class itself such as in that they are automatically converted into ORM-annotated Core objects when passed to . They may be used in the same way as table columns are used: ORM attributes, themselves known as objects, can be used in the same way as any , and are delivered in result rows just the same way, such as below where we refer to their values by column name within each row: The construct is an extensible ORM-only construct that allows sets of column expressions to be grouped in result rows: The is potentially useful for creating lightweight views as well as custom column groupings such as mappings. As discussed in the tutorial at Using Aliases, to create a SQL alias of an ORM entity is achieved using the construct against a mapped class: As is the case when using , the SQL alias is anonymously named. For the case of selecting the entity from a row with an explicit name, the parameter may be passed as well: The construct is also central to making use of subqueries with the ORM; the sections Selecting Entities from Subqueries and Joining to Subqueries discusses this further. Getting ORM Results from Textual and Core Statements¶ The ORM supports loading of entities from SELECT statements that come from other sources. The typical use case is that of a textual SELECT statement, which in SQLAlchemy is represented using the construct. The construct, once constructed, can be augmented with information about the ORM-mapped columns that the statement would load; this can then be associated with the ORM entity itself so that ORM objects can be loaded based on this statement. Given a textual SQL statement we’d like to load from: \"SELECT id, name, fullname FROM user_account ORDER BY id\" We can add column information to the statement by using the method; when this method is invoked, the object is converted into a object, which takes on a role that is comparable to the construct. The method is typically passed objects or equivalent, and in this case we can make use of the ORM-mapped attributes on the class directly: We now have an ORM-configured SQL construct that as given, can load the “id”, “name” and “fullname” columns separately. To use this SELECT statement as a source of complete entities instead, we can link these columns to a regular ORM-enabled construct using the method: The same object can also be converted into a subquery using the method, and linked to the entity to it using the construct, in a similar manner as discussed below in Selecting Entities from Subqueries: # using aliased() to select from a subquery The difference between using the directly with versus making use of is that in the former case, no subquery is produced in the resulting SQL. This can in some scenarios be advantageous from a performance or complexity perspective. Using INSERT, UPDATE and ON CONFLICT (i.e. upsert) to return ORM Objects - The method also works with DML statements that support RETURNING.\n\nSelecting Entities from UNIONs and other set operations¶ The and functions are the most common set operations, which along with other set operations such as , and others deliver an object known as a , which is composed of multiple constructs joined by a set-operation keyword. ORM entities may be selected from simple compound selects using the method illustrated previously at Getting ORM Results from Textual and Core Statements. In this method, the UNION statement is the complete statement that will be rendered, no additional criteria can be added after is used: A construct can be more flexibly used within a query that can be further modified by organizing it into a subquery and linking it to an ORM entity using , as illustrated previously at Selecting Entities from Subqueries. In the example below, we first use to create a subquery of the UNION ALL statement, we then package that into the construct where it can be used like any other mapped entity in a construct, including that we can add filtering and order by criteria based on its exported columns: Selecting ORM Entities from Unions - in the SQLAlchemy 1.4 / 2.0 Tutorial\n\nThe and methods are used to construct SQL JOINs against a SELECT statement. This section will detail ORM use cases for these methods. For a general overview of their use from a Core perspective, see Explicit FROM clauses and JOINs in the SQLAlchemy 1.4 / 2.0 Tutorial. The usage of in an ORM context for 2.0 style queries is mostly equivalent, minus legacy use cases, to the usage of the method in 1.x style queries. Consider a mapping between two classes and , with a relationship representing a collection of objects associated with each . The most common usage of is to create a JOIN along this relationship, using the attribute as an indicator for how this should occur: Where above, the call to along will result in SQL approximately equivalent to: In the above example we refer to as passed to as the “on clause”, that is, it indicates how the “ON” portion of the JOIN should be constructed. To construct a chain of joins, multiple calls may be used. The relationship-bound attribute implies both the left and right side of the join at once. Consider additional entities and , where the relationship refers to the entity, and the relationship refers to the entity, via an association table . Two calls will result in a JOIN first from to , and a second from to . However, since is a many to many relationship, it results in two separate JOIN elements, for a total of three JOIN elements in the resulting SQL: The order in which each call to the method is significant only to the degree that the “left” side of what we would like to join from needs to be present in the list of FROMs before we indicate a new target. would not, for example, know how to join correctly if we were to specify , and would raise an error. In correct practice, the method is invoked in such a way that lines up with how we would want the JOIN clauses in SQL to be rendered, and each call should represent a clear link from what precedes it. All of the elements that we target in the FROM clause remain available as potential points to continue joining FROM. We can continue to add other elements to join FROM the entity above, for example adding on the relationship to our chain of joins: A second form of allows any mapped entity or core selectable construct as a target. In this usage, will attempt to infer the ON clause for the JOIN, using the natural foreign key relationship between two entities: In the above calling form, is called upon to infer the “on clause” automatically. This calling form will ultimately raise an error if either there are no setup between the two mapped constructs, or if there are multiple linakges between them such that the appropriate constraint to use is ambiguous. When making use of or without indicating an ON clause, ORM configured constructs are not taken into account. Only the configured relationships between the entities at the level of the mapped objects are consulted when an attempt is made to infer an ON clause for the JOIN. Joins to a Target with an ON Clause¶ The third calling form allows both the target entity as well as the ON clause to be passed explicitly. A example that includes a SQL expression as the ON clause is as follows: The expression-based ON clause may also be the relationship-bound attribute; this form in fact states the target of twice, however this is accepted: The above syntax has more functionality if we use it in terms of aliased entities. The default target for is the class, however if we pass aliased forms using , the form will be used as the target, as in the example below: When using relationship-bound attributes, the target entity can also be substituted with an aliased entity by using the method. The same example using this method would be: As a substitute for providing a full custom ON condition for an existing relationship, the function may be applied to a relationship attribute to augment additional criteria into the ON clause; the additional criteria will be combined with the default criteria using AND. Below, the ON criteria between and contains two separate elements joined by , the first one being the natural join along the foreign key, and the second being a custom limiting criteria: The method also works with loader strategies. See the section Adding Criteria to loader options for an example. The target of a join may be any “selectable” entity which usefully includes subqueries. When using the ORM, it is typical that these targets are stated in terms of an construct, but this is not strictly required particularly if the joined entity is not being returned in the results. For example, to join from the entity to the entity, where the entity is represented as a row limited subquery, we first construct a object using , which may then be used as the target of the method: The above SELECT statement when invoked via will return rows that contain entities, but not entities. In order to add entities to the set of entities that would be returned in result sets, we construct an object against the entity and the custom subquery. Note we also apply a name to the construct so that we may refer to it by name in the result row: The same subquery may be referred towards by multiple entities as well, for a subquery that represents more than one entity. The subquery itself will remain unique within the statement, while the entities that are linked to it using refer to distinct sets of columns: In cases where the left side of the current state of is not in line with what we want to join from, the method may be used: The method accepts two or three arguments, either in the form , or : To set up the initial FROM clause for a SELECT such that can be used subsequent, the method may also be used: The method does not actually have the final say on the order of tables in the FROM clause. If the statement also refers to a construct that refers to existing tables in a different order, the construct takes precedence. When we use methods like and , these methods are ultimately creating such a object. Therefore we can see the contents of being overridden in a case like this: Where above, we see that the FROM clause is , even though we stated first. Because of the method call, the statement is ultimately equivalent to the following: The construct above is added as another entry in the list which supersedes the previous entry."
    },
    {
        "link": "https://docs.sqlalchemy.org/14/orm/query.html",
        "document": "is the source of all SELECT statements generated by the ORM, both those formulated by end-user query operations as well as by high level internal operations such as related collection loading. It features a generative interface whereby successive calls return a new object, a copy of the former with additional criteria and options associated with it.\n\nobjects are normally initially generated using the method of , and in less common cases by instantiating the directly and associating with a using the method.\n\nFor a full walk through of usage, see the Object Relational Tutorial (1.x API).\n\nreturn a Query that selects from this Query’s SELECT statement. Deprecated since version 1.4: The method is considered legacy as of the 1.x series of SQLAlchemy and will be removed in 2.0. The new approach is to use the construct in conjunction with a subquery. See the section Selecting from the query itself as a subquery in the 2.0 migration notes for an example. (Background on SQLAlchemy 2.0 at: Migrating to SQLAlchemy 2.0) essentially turns the SELECT statement into a SELECT of itself. Given a query such as: There are lots of cases where may be useful. A simple one is where above, we may want to apply a row LIMIT to the set of user objects we query against, and then apply additional joins against that row-limited set: The above query joins to the entity but only against the first five results of the query: Another key behavior of is that it applies automatic aliasing to the entities inside the subquery, when they are referenced on the outside. Above, if we continue to refer to the entity without any additional aliasing applied to it, those references will be in terms of the subquery: The ORDER BY against is aliased to be in terms of the inner subquery: The automatic aliasing feature only works in a limited way, for simple filters and orderings. More ambitious constructions such as referring to the entity in joins should prefer to use explicit subquery objects, typically making use of the method to produce an explicit subquery object. Always test the structure of queries by viewing the SQL to ensure a particular structure does what’s expected! also includes the ability to modify what columns are being queried. In our example, we want to be queried by the inner query, so that we can join to the entity on the outside, but we only wanted the outer query to return the column: Looking out for Inner / Outer Columns Keep in mind that when referring to columns that originate from inside the subquery, we need to ensure they are present in the columns clause of the subquery itself; this is an ordinary aspect of SQL. For example, if we wanted to load from a joined entity inside the subquery using , we need to add those columns. Below illustrates a join of to , then a subquery, and then we’d like to access the columns: We use above before we call so that the columns are present in the inner subquery, so that they are available to the modifier we are using on the outside, producing: If we didn’t call , but still asked to load the entity, it would be forced to add the table on the outside without the correct join criteria - note the phrase at the end: *entities¶ – optional list of entities which will replace those being selected.\n\nCreate a SQL JOIN against this object’s criterion and apply generatively, returning the newly resulting . Consider a mapping between two classes and , with a relationship representing a collection of objects associated with each . The most common usage of is to create a JOIN along this relationship, using the attribute as an indicator for how this should occur: Where above, the call to along will result in SQL approximately equivalent to: In the above example we refer to as passed to as the “on clause”, that is, it indicates how the “ON” portion of the JOIN should be constructed. To construct a chain of joins, multiple calls may be used. The relationship-bound attribute implies both the left and right side of the join at once: as seen in the above example, the order in which each call to the join() method occurs is important. Query would not, for example, know how to join correctly if we were to specify , then , then , in our chain of joins; in such a case, depending on the arguments passed, it may raise an error that it doesn’t know how to join, or it may produce invalid SQL in which case the database will raise an error. In correct practice, the method is invoked in such a way that lines up with how we would want the JOIN clauses in SQL to be rendered, and each call should represent a clear link from what precedes it. A second form of allows any mapped entity or core selectable construct as a target. In this usage, will attempt to create a JOIN along the natural foreign key relationship between two entities: In the above calling form, is called upon to create the “on clause” automatically for us. This calling form will ultimately raise an error if either there are no foreign keys between the two entities, or if there are multiple foreign key linkages between the target entity and the entity or entities already present on the left side such that creating a join requires more information. Note that when indicating a join to a target without any ON clause, ORM configured relationships are not taken into account. Joins to a Target with an ON Clause The third calling form allows both the target entity as well as the ON clause to be passed explicitly. A example that includes a SQL expression as the ON clause is as follows: The above form may also use a relationship-bound attribute as the ON clause as well: The above syntax can be useful for the case where we wish to join to an alias of a particular target entity. If we wanted to join to twice, it could be achieved using two aliases set up using the function: The relationship-bound calling form can also specify a target entity using the method; a query equivalent to the one above would be: As a substitute for providing a full custom ON condition for an existing relationship, the function may be applied to a relationship attribute to augment additional criteria into the ON clause; the additional criteria will be combined with the default criteria using AND: The target of a join may also be any table or SELECT statement, which may be related to a target entity or not. Use the appropriate method in order to make a subquery out of a query: Joining to a subquery in terms of a specific relationship and/or target entity may be achieved by linking the subquery to the entity using : Controlling what to Join From In cases where the left side of the current state of is not in line with what we want to join from, the method may be used: Which will produce SQL similar to: Deprecated since version 1.4: The following features are deprecated and will be removed in SQLAlchemy 2.0. The method currently supports several usage patterns and arguments that are considered to be legacy as of SQLAlchemy 1.3. A deprecation path will follow in the 1.4 series for the following features:\n• None Joining on relationship names rather than attributes: Why it’s legacy: the string name does not provide enough context for to always know what is desired, notably in that there is no indication of what the left side of the join should be. This gives rise to flags like as well as the ability to place several join clauses in a single call which don’t solve the problem fully while also adding new calling styles that are unnecessary and expensive to accommodate internally. Modern calling pattern: Use the actual relationship, e.g. in the above case:\n• Why it’s legacy: the automatic aliasing feature of is intensely complicated, both in its internal implementation as well as in its observed behavior, and is almost never used. It is difficult to know upon inspection where and when its aliasing of a target entity, in the above case, will be applied and when it won’t, and additionally the feature has to use very elaborate heuristics to achieve this implicit behavior.\n• # ... and several more forms actually Why it’s legacy: being able to chain multiple ON clauses in one call to is yet another attempt to solve the problem of being able to specify what entity to join from, and is the source of a large variety of potential calling patterns that are internally expensive and complicated to parse and accommodate. Modern calling pattern: Use relationship-bound attributes or SQL-oriented ON clauses within separate calls, so that each call to knows what the left side should be:\n• None *props¶ – Incoming arguments for , the props collection in modern use should be considered to be a one or two argument form, either as a single “target” entity or ORM attribute-bound relationship, or as a target entity plus an “on clause” which may be a SQL expression or ORM attribute-bound relationship.\n• None isouter=False¶ – If True, the join used will be a left outer join, just as if the method were called.\n• None When using , a setting of True here will cause the join to be from the most recent joined target, rather than starting back from the original FROM clauses of the query.\n• None If True, indicate that the JOIN target should be anonymously aliased. Subsequent calls to and similar will adapt the incoming criterion to the target alias, until is called. Querying with Joins in the ORM tutorial. Mapping Class Inheritance Hierarchies for details on how is used for inheritance relationships. - a standalone ORM-level join function, used internally by , which in previous SQLAlchemy versions was the primary ORM-level joining interface.\n\nSet the FROM clause of this to a core selectable, applying it as a replacement FROM clause for corresponding mapped entities. Deprecated since version 1.4: The method is considered legacy as of the 1.x series of SQLAlchemy and will be removed in 2.0. Use the construct instead (Background on SQLAlchemy 2.0 at: Migrating to SQLAlchemy 2.0) The method supplies an alternative approach to the use case of applying an construct explicitly throughout a query. Instead of referring to the construct explicitly, automatically adapts all occurrences of the entity to the target selectable. Given a case for such as selecting objects from a SELECT statement: Above, we apply the object explicitly throughout the query. When it’s not feasible for to be referenced explicitly in many places, may be used at the start of the query to adapt the existing entity: Above, the generated SQL will show that the entity is adapted to our statement, even in the case of the WHERE clause: The method is similar to the method, in that it sets the FROM clause of the query. The difference is that it additionally applies adaptation to the other parts of the query that refer to the primary entity. If above we had used instead, the SQL generated would have been: To supply textual SQL to the method, we can make use of the construct. However, the construct needs to be aligned with the columns of our entity, which is achieved by making use of the method: itself accepts an object, so that the special options of such as may be used within the scope of the method’s adaptation services. Suppose a view also returns rows from . If we reflect this view into a , this view has no relationship to the to which we are mapped, however we can use name matching to select from it: Changed in version 1.1.7: The method now accepts an object as an alternative to a object. from_obj¶ – a object that will replace the FROM clause of this . It also may be an instance of ."
    },
    {
        "link": "https://stackoverflow.com/questions/68745946/multiple-group-by-counts-on-single-table-using-sqlalchemy",
        "document": "I'm working with SQLAlchemy (1.4/2.0 Transitional style) to access a PostgreSQL database. I am using group_by to count the number of unique names ordered by the count. What I need to do is then apply additional group_by to count the same thing, but with different conditions. For example, if I have this User object\n\nI can get the top 100 most popular User names and the counts of those with that name using:\n\nWhat I need to do is to for those 100 most popular names, find how many have those names where city == \"New York\", \"Chicago\" & \"Los Angeles\". I'd like to get the data back something like:\n\nAny idea how to do this?"
    },
    {
        "link": "https://datacamp.com/tutorial/sqlalchemy-tutorial-examples",
        "document": "In this course, you'll learn the basics of relational databases and how to interact with them."
    },
    {
        "link": "https://stackoverflow.com/questions/77529480/can-i-bundle-sqlalchemys-join-and-where-statements-to-reuse-them-for-a-sele",
        "document": "With sqlalchemy 1.4 I want to make an assertion on my data before changing it. These assertions and filter conditions span multiple tables. For updating I need the same joins and where clauses again. So currently I have something like:\n\nThis does feel cluttered and could lead to easy human mistakes when one changes the select-part but not the update part. Can I bundle and reuse the join()s and where()s somehow?"
    }
]