[
    {
        "link": "https://xlrd.readthedocs.io",
        "document": "xlrd is a library for reading data and formatting information from Excel files in the historical format.\n\nThe following are also not supported but will safely and reliably be ignored:\n• None Formulas, but results of formula calculations are extracted.\n\nPassword-protected files are not supported and cannot be read by this library.\n\nFrom the command line, this will show the first, second and last rows of each sheet in each file:\n\nYou may also wish to consult the tutorial.\n\nFor details of how to get involved in development of this package, and other meta-information, please see the sections below:"
    },
    {
        "link": "https://geeksforgeeks.org/reading-excel-file-using-python",
        "document": "One can retrieve information from a spreadsheet. Reading, writing, or modifying the data can be done in Python can be done in using different methods. Also, the user might have to go through various sheets and retrieve data based on some criteria or modify some rows and columns and do a lot of work. Here, we will see the different methods to read our excel file.\n\nMethod 1: Reading an excel file using Python using Pandas\n\nIn this method, We will first import the Pandas module then we will use Pandas to read our excel file. You can read more operations using the excel file using Pandas in this article. Click here\n\nMethod 2: Reading an excel file using Python using openpyxl\n\nThe load_workbook() function opens the Books.xlsx file for reading. This file is passed as an argument to this function. The object of the dataframe.active has been created in the script to read the values of the max_row and the max_column properties. These values are used in the loops to read the content of the Books2.xlsx file. You can read other operations using openpyxl in this article.\n\nMethod 3: Reading an excel file using Python using Xlwings\n\nXlwings can be used to insert data in an Excel file similarly as it reads from an Excel file. Data can be provided as a list or a single input to a certain cell or a selection of cells. You can read other operations using Xlwings in this article.\n\nRECOMMENDED ARTICLE – How to Automate an Excel Sheet in Python?\n\nHow to read Excel file in Python using path?\n\nHow to open an Excel file from Python?\n\nTo open and manipulate Excel files, you can also use the xlrd library, which supports .xls files (not .xlsx): \n\n\n\n \n\n # Access sheet by index or name \n\n \n\n\n\n \n\n\n\n \n\n\n\n \n\n\n\nHow to read an xlsx file in Python using Pandas?\n\nHow to read xlsx file in Python using CSV?\n\nIf you prefer working with CSV format data extracted from an Excel file, you can convert an Excel file into CSV format using Pandas: This converts the Excel data into a CSV file named output_file.csv in the current directory."
    },
    {
        "link": "https://xlrd.readthedocs.io/en/latest/api.html",
        "document": "descriptions of the file types can . Inspect the content at the supplied path or the content provided and return the file’s type as a , or if it cannot be determined.\n• None path – A path containing the content to inspect. will be expanded. A , or if the format cannot be determined. The return value can always be looked up in to return a human-readable description of the format found.\n• None filename – The path to the spreadsheet file to be opened.\n• None logfile – An open file to which messages and diagnostics are written.\n• None verbosity – Increases the volume of trace material written to the logfile.\n• None Whether to use the mmap module is determined heuristically. Use this arg to override the result. Current heuristic: mmap is used if it exists.\n• None file_contents – A string or an object or some other behave-alike object. If is supplied, will not be used, except (possibly) in messages.\n• None encoding_override – Used to overcome missing or bad codepage information in older-version files. See Handling of Unicode.\n• None The default is , which saves memory. In this case, “Blank” cells, which are those with their own formatting information but no data, are treated as empty by ignoring the file’s and records. This cuts off any bottom or right “margin” of rows of empty or blank cells. Only and are available. When , formatting information will be read from the spreadsheet file. This provides all cells, including empty and blank cells. Formatting information is available for each cell. Note that this will raise a NotImplementedError when used with an xlsx file.\n• None on_demand – Governs whether sheets are all loaded initially or when demanded by the caller. See Loading worksheets on demand.\n• None The default of means all rows are padded out with empty cells so that all rows have the same size as found in . means that there are no empty cells at the ends of rows. This can result in substantial memory savings if rows are of widely varying sizes. See also the method.\n• None ignore_workbook_corruption – This option allows to read corrupted workbooks. When you may face CompDocError: Workbook corruption. When that exception will be ignored. An instance of the class. For debugging: dump an XLS file’s BIFF records in char & hex.\n• None filename – The path to the file to be dumped.\n• None outfile – An open file, to which the dump is written. For debugging and analysis: summarise the file’s BIFF records. ie: produce a sorted file of .\n• None filename – The path to the file to be summarised.\n• None outfile – An open file, to which the summary is written.\n\nName information is not extracted from files older than Excel 5.0 ( ) No examples have been sighted. Function group. Relevant only if macro == 1; see OOo docs for values. No examples have been sighted. The index of this object in book.name_obj_list The name is global (visible in all calculation sheets). The name belongs to a macro sheet or VBA sheet. The name is local to the sheet whose index is scope. This is a convenience method for the frequent use case where the name refers to a single cell. An instance of the class. xlrd.biffh.XLRDError – The name is not a constant absolute reference to a single cell. This is a convenience method for the use case where the name refers to one rectangular area in one worksheet. clipped – If , the default, the returned rectangle is clipped to fit in . it is guaranteed that and that the number of usable rows in the area (which may be zero) is ; likewise for columns. xlrd.biffh.XLRDError – The name is not a constant absolute reference to a single area in a single sheet. You should not instantiate this class yourself. You use the object that was returned when you called . Which date system was in force when this file was last saved. Defaults to 0 in case it’s not specified in the file. Version of BIFF (Binary Interchange File Format) used to create the file. Latest is 8.0 (represented here as 80), introduced with Excel 97. Earliest supported by this module: 2.0 (represented as 20). An integer denoting the character set used for strings in this file. For BIFF 8 and later, this will be 1200, meaning Unicode; more precisely, UTF_16_LE. For earlier versions, this is used to derive the appropriate Python encoding to be used to convert to Unicode. Examples: , The encoding that was derived from the codepage. A tuple containing the telephone country code for: the user-interface setting when the file was created. This information may give a clue to the correct encoding for an unknown codepage. For a long list of observed values, refer to the OpenOffice.org documentation for the record. What (if anything) is recorded as the name of the last user to save the file. A list of class instances, each corresponding to a FONT record. A list of objects, each corresponding to a record, in the order that they appear in the input file. It does not contain builtin formats. If you are creating an output file using (for example) , use this list. The collection to be used for all visual rendering purposes is . The mapping from to object. Time in seconds to extract the XLS image as a contiguous string (or mmap equivalent). Time in seconds to parse the data from the contiguous string (or mmap equivalent). A list of all sheets in the book. All sheets not already loaded will be loaded. sheet_name – Name of the sheet required. A list of the names of all the worksheets in the workbook file. This information is available even when no sheets have yet been loaded. sheet_name_or_index – Name or index of sheet enquired upon if sheet is loaded, otherwise. sheet_name_or_index – Name or index of sheet to be unloaded. This method has a dual purpose. You can call it to release memory-consuming objects and (possibly) a memory-mapped file ( object) when you have finished loading sheets in mode, but still require the object to examine the loaded sheets. It is also called automatically (a) when raises an exception and (b) if you are using a statement, when the block is exited. Calling this method multiple times on the same object has no ill effect. A mapping from to a list of objects. The list is sorted in scope order. Typically there will be one item (of global scope) in the list. The number of worksheets present in the workbook file. This information is available even when no sheets have yet been loaded. List containing a object for each record in the workbook. This provides definitions for colour indexes. Please refer to The Palette; Colour Indexes for an explanation of how colours are represented in Excel. Colour indexes into the palette map into tuples. “Magic” indexes e.g. map to . is what you need if you want to render cells on screen or in a PDF file. If you are writing an output XLS file, use . If the user has changed any of the colours in the standard palette, the XLS file will contain a record with 56 (16 for Excel 4.0 and earlier) RGB values in it, and this list will be e.g. . Otherwise this list will be empty. This is what you need if you are writing an output XLS file. If you want to render cells on screen or in a PDF file, use . A list of class instances, each corresponding to an record. This provides access via name to the extended format information for both built-in styles and user-defined styles. It maps to , where is either the name of a user-defined style, or the name of one of the built-in styles. Known built-in names are Normal, RowLevel_1 to RowLevel_7, ColLevel_1 to ColLevel_7, Comma, Currency, Percent, “Comma [0]”, “Currency [0]”, Hyperlink, and “Followed Hyperlink”. has the following meanings is an index into .\n\nGeneral purpose function. Uses Euclidean distance. So far used only for pre-BIFF8 record. Doesn’t have to be fast. Doesn’t have to be fancy. This mixin class exists solely so that , , and objects can be compared by value of their attributes. An Excel “font” contains the details of not only what is normally considered a font, but also several other display attributes. Items correspond to those in the Excel UI’s Format -> Cells -> Font tab. An explanation of “colour index” is given in The Palette; Colour Indexes. The 0-based index used to refer to this Font() instance. Note that index 4 is never used; xlrd supplies a dummy place-holder. Height of the font (in twips). A twip = 1/20 of a point. The name of the font. Example: . Font weight (100-1000). Standard values are 400 for normal text and 700 for bold text. A classification that has been inferred from the format string. Currently, this is used only to distinguish between numbers and dates. Values: Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl. A collection of the border-related attributes of an record. Items correspond to those in the Excel UI’s Format -> Cells -> Border tab. An explanations of “colour index” is given in The Palette; Colour Indexes. There are five line style attributes; possible values and the associated meanings are: The line styles 8 to 13 appear in BIFF8 files (Excel 97 and later) only. For pictures of the line styles, refer to OOo docs s3.10 (p22) “Line Styles for Cell Borders (BIFF3-BIFF8)”.</p> The colour index for the cell’s top line The colour index for the cell’s bottom line The colour index for the cell’s left line The colour index for the cell’s right line The colour index for the cell’s diagonal lines, if any The line style for the cell’s top line The line style for the cell’s bottom line The line style for the cell’s left line The line style for the cell’s right line The line style for the cell’s diagonal lines, if any 1 = draw a diagonal from top left to bottom right 1 = draw a diagonal from bottom left to top right A collection of the background-related attributes of an record. Items correspond to those in the Excel UI’s Format -> Cells -> Patterns tab. An explanations of “colour index” is given in The Palette; Colour Indexes. See section 3.11 of the OOo docs. See section 3.11 of the OOo docs. See section 3.11 of the OOo docs. A collection of the alignment and similar attributes of an record. Items correspond to those in the Excel UI’s Format -> Cells -> Alignment tab. file versions BIFF7 and earlier use the documented attribute; this will be mapped (without loss) into . 1 = text is wrapped at right margin A collection of the protection-related attributes of an record. Items correspond to those in the Excel UI’s Format -> Cells -> Protection tab. Note the OOo docs include the “cell or style” bit in this bundle of attributes. This is incorrect; the bit is used in determining which bundles to use. 1 = Cell is prevented from being changed, moved, resized, or deleted (only if the sheet is protected). 1 = Hide formula so that it doesn’t appear in the formula bar when the cell is selected (only if the sheet is protected). Each of the 6 flags below describes the validity of a specific group of attributes.\n• None means the attributes of the parent style are used, (but only if the attributes are valid there);\n• None means the attributes of this are used.\n• None means the attribute should be ignored. the API provides both “raw” XFs and “computed” XFs. In the latter case, cell XFs have had the above inheritance mechanism applied. cell XF: Index into Book.xf_list of this XF’s style XF OOo docs on the XF record call this “Index to FORMAT record”. It is not an index in the Python sense. It is a key to a map. It is true only for Excel 4.0 and earlier files that the key into format_map from an XF instance is the same as the index into format_list, and only if the index is less than 164. An instance of an object. An instance of an object. An instance of an object. An instance of an object.\n\nUsed in evaluating formulas. The following table describes the kinds and how their values are represented. None, or an int error code (same as XL_CELL_ERROR in the Cell class). Used by Excel as a placeholder for a missing (not supplied) function argument. Should *not* appear as a final formula result. Value is None. A float. Note that there is no way of distinguishing dates. The value is either None or a non-empty list of absolute Ref3D instances.\n\n The value is None or a non-empty list of fully or partially relative Ref3D instances. The kind is unknown or ambiguous. The value is None oUNK means that the kind of operand is not known unambiguously. None means that the actual value of the operand is a variable (depends on cell data), not a constant. The reconstituted text of the original formula. Function names will be in English irrespective of the original language, which doesn’t seem to be recorded anywhere. The separator is “,”, not “;” or whatever else might be more appropriate for the end-user’s locale; patches welcome. Represents an absolute or relative 3-dimensional reference to a box of one or more cells. The attribute is a tuple of the form: It is quite possible to have ; for example could have and/or irrespective of how many columns/rows are actually used in the worksheet. The caller will need to decide how to handle this situation. Keyword: :-) The components of the coords attribute are also available as individual attributes: , , , , , and . The attribute is a 6-tuple of flags which indicate whether the corresponding (sheet|row|col)(lo|hi) is relative (1) or absolute (0). There is necessarily no information available as to what cell(s) the reference could possibly be relative to. The caller must decide what if any use to make of operands.\n\nContains the data for one worksheet. In the cell access functions, is a row index, counting from zero, and is a column index, counting from zero. Negative values for row/column indexes and slice positions are supported in the expected fashion. For information about cell types and cell values, refer to the documentation of the class. You don’t instantiate this class yourself. You access objects via the object that was returned when you called . Returns a sequence of the objects in the given column. A 256-element tuple corresponding to the contents of the GCW record for this sheet. If no such record, treat as all bits zero. Applies to BIFF4-7 only. See docs of the class for discussion. Number of columns in left pane (frozen panes; for split panes, see comments in code) Number of rows in top pane (frozen panes; for split panes, see comments in code) Index of first visible row in bottom frozen/split pane Index of first visible column in right frozen/split pane Frozen panes: ignore it. Split panes: explanation and diagrams in OOo docs. Boolean specifying if a record was present, ignore unless you’re A reference to the object to which this sheet belongs. Number of rows in sheet. A row index is in . Nominal number of columns in sheet. It is one more than the maximum column index found, ignoring trailing empty cells. See also the parameter to and . Default column width from record, else . From the OOo docs: Column width in characters, using the width of the zero character from default font (first FONT record in the file). Excel adds some extra space to the default width, depending on the default font and default font size. The algorithm how to exactly calculate the resulting column width is not known. Example: The default width of 8 set in this record results in a column width of 8.43 using Arial font with a size of 10 points. For the default hierarchy, refer to the class. Default width of the columns in 1/256 of the width of the zero character, using default font (first FONT record in the file). For the default hierarchy, refer to the class. Default value to be used for a row if there is no record for that row. From the optional record. Default value to be used for a row if there is no record for that row. From the optional record. Default value to be used for a row if there is no record for that row. From the optional record. Default value to be used for a row if there is no record for that row. From the optional record. Default value to be used for a row if there is no record for that row. From the optional record. The map from a column index to a object. Often there is an entry in records for all column indexes in . xlrd ignores the entry for the non-existent 257th column. On the other hand, there may be no entry for unused columns. The map from a row index to a object. It is possible to have missing entries – at least one source of XLS files doesn’t bother writing records. List of address ranges of cells containing column labels. These are set up in Excel by Insert > Name > Labels > Columns. How to deconstruct the list: List of address ranges of cells containing row labels. For more details, see . List of address ranges of cells which have been merged. These are set up in Excel by Format > Cells > Alignment, then ticking the “Merge cells” box. The upper limits are exclusive: i.e. only spans two cells. How to deconstruct the list: # cell (rlo, clo) (the top left one) will carry the data # and formatting info; the remainder will be recorded as # blank cells, but a renderer will apply the formatting info # for the top left cell (e.g. border, pattern) to all cells in Mapping of to list of tuples. The offset defines where in the string the font begins to be used. Offsets are expected to be in ascending order. If the first offset is not zero, the meaning is that the cell’s ’s font should be used from offset 0. This is a sparse mapping. There is no entry for cells that are not formatted with rich text. A list of the horizontal page breaks in this sheet. Breaks are tuples in the form . A list of the vertical page breaks in this sheet. Breaks are tuples in the form . A list of objects corresponding to records found in the worksheet. A sparse mapping from to an item in . Cells not covered by a hyperlink are not mapped. It is possible using the Excel UI to set up a hyperlink that covers a larger-than-1x1 rectangle of cells. Hyperlink rectangles may overlap (Excel doesn’t check). When a multiply-covered cell is clicked on, the hyperlink that is activated (and the one that is mapped here) is the last in . A sparse mapping from to a object. Cells not containing a note (“comment”) are not mapped. object in the given row and column. Value of the cell in the given row and column. Type of the cell in the given row and column. Refer to the documentation of the class. XF index of the cell in the given row and column. This is an index into . Returns the effective number of cells in the given row. For use with which is likely to produce rows with fewer than cells. Returns a sequence of the objects in the given row. Returns a generator for iterating through each row. Returns a slice of the types of the cells in the given row. Returns a slice of the values of the cells in the given row. Returns a slice of the objects in the given row. Returns a slice of the objects in the given column. Returns a slice of the values of the cells in the given column. Returns a slice of the types of the cells in the given column. colx – Index of the queried column, range 0 to 255. Note that it is possible to find out the width that will be used to display columns with no cell information e.g. column IV (colx=255). The column width that will be used for displaying the given column by Excel, in units of 1/256th of the width of a standard character (the digit zero in the first font). Represents a user “comment” or “note”. Note objects are accessible through . if the containing column is hidden List of tuples. Unlike , the first offset should always be 0. True if the containing row is hidden True if note is always shown Contains the attributes of a hyperlink. Hyperlink objects are accessible through and . Type of hyperlink. Unicode string, one of ‘url’, ‘unc’, ‘local file’, ‘workbook’, ‘unknown’ The URL or file-path, depending in the type. Unicode string, except in the rare case of a local but non-existent file with non-ASCII characters in the name, in which case only the “8.3” filename is available, as a (3.x) or (2.x) string, with unknown encoding. Description. This is displayed in the cell, and should be identical to the cell value. Unicode string, or . It seems impossible NOT to have a description created by the Excel UI. No cases of this have been seen in the wild. It seems impossible to create one in the Excel UI. The piece after the “#” in “http://docs.python.org/library#struct_module”, or the part when type is “workbook”. The text of the “quick tip” displayed when the cursor hovers over the hyperlink. Contains the data for one cell. You don’t call this class yourself. You access objects via methods of the object(s) that you found in the object that was returned when you called Cell objects have three attributes: is an int, (which depends on ) and . If is not enabled when the workbook is opened, will be . The following table describes the types of cells and how their values are represented in Python. int representing internal Excel codes; for a text representation, refer to the supplied dictionary error_text_from_code empty string ''. Note: this type will appear only when open_workbook(..., formatting_info=True) is used. Width and default formatting information that applies to one or more columns in a sheet. Derived from records. Here is the default hierarchy for width, according to the OOo docs: In BIFF3, if a record is missing for a column, the width specified in the record is used instead. In BIFF4-BIFF7, the width set in this record is only used, if the corresponding bit for this column is cleared in the record, otherwise the column width set in the record is used (the record is always ignored in this case ). In BIFF8, if a record is missing for a column, the width specified in the record is used. If this record is also missing, the column width of the record is used instead. xlrd goes with the GCW version of the story. Reference to the source may be useful: see . Width of the column in 1/256 of the width of the zero character, using default font (first record in the file). XF index to be used for formatting empty cells. Value of a 1-bit flag whose purpose is unknown but is often seen set to 1 Outline level of the column, in . (0 = no outline) Height and default formatting information that applies to a row in a sheet. Derived from records. Height of the row, in twips. One twip == 1/20 of a point. Outline level of the row (0 to 7) 1 = Outline group starts or ends here (depending on where the outline buttons are located, see record, which is not parsed by xlrd), and is collapsed. 1 = Row is hidden (manually, or by a filter or outline group) 1 = Row height and default font height do not match. 1 = the xf_index attribute is usable; 0 = ignore it. Index to default record for empty cells in this row. Don’t use this if . This flag is set if the upper border of at least one cell in this row or if the lower border of at least one cell in the row above is formatted with a thick line style. Thin and medium line styles are not taken into account. This flag is set if the lower border of at least one cell in this row or if the upper border of at least one cell in the row below is formatted with a medium or thick line style. Thin line styles are not taken into account."
    },
    {
        "link": "https://stackoverflow.com/questions/12705527/reading-excel-files-with-xlrd",
        "document": "Maybe as a last resort you can try and save the .xls file as a .csv file and then try and read it.\n\nObviously you say you can open it after an open and close from Excel so it's the same effort.\n\nIf you realy want your script to open them, then if you are on the windows platform use the pywin32 to open and close Excel from your script, and open and close the file in the same go. Maybe that could work. (silly but could be a work around)\n\nExample (stolen from here)"
    },
    {
        "link": "https://stackoverflow.com/questions/7372716/parsing-excel-documents-with-python",
        "document": "You're best bet for parsing Excel files would be the xlrd library. The python-excel.org site has links and examples for xlrd and related python excel libraries, including a pdf document that has some good examples of using xlrd. Of course, there are also lots of related xlrd questions on StackOverflow that might be of use.\n\nOne caveat with the xlrd library is that it will only work with (Excel 2003 and earlier versions of excel) file formats and not the more recent file format. There is a newer library openpyxl for dealing with the , but I have never used it.\n\nUPDATE: As per John's comment, the xlrd library now supports both and file formats."
    },
    {
        "link": "https://geeksforgeeks.org/face-detection-using-cascade-classifier-using-opencv-python",
        "document": "In this article, we are going to see how to detect faces using a cascade classifier in OpenCV Python. Face detection has much significance in different fields of today’s world. It is a significant step in several applications, face recognition (also used as biometrics), photography (for auto-focus on the face), face analysis (age, gender, emotion recognition), video surveillance, etc.\n\nOne of the popular algorithms for facial detection is “haarcascade”. It is computationally less expensive, a fast algorithm, and gives high accuracy.\n\nHaarcascade file can be download from here: haarcascade_frontalface_default.xml\n\nIt works in four stages:\n• Haar-feature selection: A Haar-like feature consists of dark regions and light regions. It produces a single value by taking the difference of the sum of the intensities of the dark regions and the sum of the intensities of light regions. It is done to extract useful elements necessary for identifying an object. The features proposed by viola and jones are:\n• Creation of Integral Images: A given pixel in the integral image is the sum of all the pixels on the left and all the pixels above it. Since the process of extracting Haar-like features involves calculating the difference of dark and light rectangular regions, the introduction of Integral Images reduces the time needed to complete this task significantly.\n• AdaBoost Training: This algorithm selects the best features from all features. It combines multiple “weak classifiers” (best features) into one “strong classifier”. The generated “strong classifier” is basically the linear combination of all “weak classifiers”.\n• Cascade Classifier: It is a method for combining increasingly more complex classifiers like AdaBoost in a cascade which allows negative input (non-face) to be quickly discarded while spending more computation on promising or positive face-like regions. It significantly reduces the computation time and makes the process more efficient.\n\nOpenCV comes with lots of pre-trained classifiers. Those XML files can be loaded by cascadeClassifier method of the cv2 module. Here we are going to use haarcascade_frontalface_default.xml for detecting faces.\n\nInitially, the image is a three-layer image (i.e., RGB), So It is converted to a one-layer image (i.e., grayscale).\n\nCascadeClassifier method in cv2 module supports the loading of haar-cascade XML files. Here, we need “haarcascade_frontalface_default.xml” for face detection.\n\nStep 4: Applying the face detection method on the grayscale image\n\nThis is done using the cv2::CascadeClassifier::detectMultiScale method, which returns boundary rectangles for the detected faces (i.e., x, y, w, h). It takes two parameters namely, scaleFactor and minNeighbors. ScaleFactor determines the factor of increase in window size which initially starts at size “minSize”, and after testing all windows of that size, the window is scaled up by the “scaleFactor”, and the window size goes up to “maxSize”. If the “scaleFactor” is large, (e.g., 2.0), there will be fewer steps, so detection will be faster, but we may miss objects whose size is between two tested scales. (default scale factor is 1.3). Higher the values of the “minNeighbors”, less will be the number of false positives, and less error will be in terms of false detection of faces. However, there is a chance of missing some unclear face traces as well.\n\nRectangles are drawn around the detected faces by the rectangle method of the cv2 module by iterating over all detected faces.\n\nBelow is the implementation:"
    },
    {
        "link": "https://stackoverflow.com/questions/45655699/attributeerror-module-cv2-face-has-no-attribute-createlbphfacerecognizer",
        "document": "So i'm doing a little personal project but i keep getting this error when I try to create the recognizer. i have opencv-contrib and everything. Does anyone know whats going on? code posted below it gets caught on that last line. I've tried reinstalling all modules already. Not really sure what else to do. The weird thing is it works on my laptop but not my desktop. They both have the same modules, same python release and running the exact same code."
    },
    {
        "link": "https://fahizkp.medium.com/real-time-face-detection-and-recognition-with-opencv-b57b3dcbbc63",
        "document": "Face detection and recognition are crucial aspects of modern security systems, user authentication, and human-computer interaction. In this tutorial, we’ll walk through a simple yet powerful implementation of real-time face detection and recognition using OpenCV in Python. We will be using the Local Binary Patterns Histograms (LBPH) algorithm, a popular technique for face recognition.\n\nBefore diving into the code, make sure you have the following installed:\n\nimport cv2\n\nimport os\n\nimport urllib.request\n\n\n\nclass FaceDataset:\n\n \"\"\"\n\n A class to capture face images from the webcam and store them in a dataset directory.\n\n \"\"\"\n\n\n\n def __init__(self, face_id: int, save_path: str='dataset', width: int=640, height: int=480)-> None:\n\n \"\"\"\n\n Initializes the FaceDataset object with the provided parameters.\n\n \"\"\"\n\n\n\n self.face_id = face_id\n\n self.save_path = save_path\n\n self.cam = cv2.VideoCapture(0)\n\n self.cam.set(3, width)\n\n self.cam.set(4, height)\n\n self._download_haarcascade() # Download Haarcascade if not available\n\n self.face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\n self.count = 0\n\n\n\n def _download_haarcascade(self) -> None:\n\n \"\"\"Downloads the Haarcascade XML file if it does not exist.\"\"\"\n\n\n\n if not os.path.isfile('haarcascade_frontalface_default.xml'):\n\n print(\"[INFO] Downloading Haarcascade XML file...\")\n\n url = \"https://github.com/opencv/opencv/raw/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n\n urllib.request.urlretrieve(url, \"haarcascade_frontalface_default.xml\")\n\n print(\"[INFO] Download complete.\")\n\n\n\n def capture_faces(self, samples: int=30):\n\n \"\"\"\n\n Captures face images from the webcam and stores them in the dataset directory.\n\n \"\"\"\n\n print(f\"\n\n Initializing face capture for User {self.face_id}. Please look at the camera...\")\n\n\n\n while self.count < samples:\n\n ret, img = self.cam.read()\n\n img = cv2.flip(img, -1)\n\n gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n faces = self.face_detector.detectMultiScale(gray, 1.3, 5)\n\n\n\n for (x, y, w, h) in faces:\n\n cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n\n self.count += 1\n\n cv2.imwrite(f\"{self.save_path}/User.{self.face_id}.{self.count}.jpg\", gray[y:y+h, x:x+w])\n\n cv2.imshow('Face Capture', img)\n\n\n\n if cv2.waitKey(100) & 0xff == 27: # Press 'ESC' to exit early\n\n break\n\n\n\n print(f\"\n\n{self.count} face samples captured successfully.\")\n\n self.cam.release()\n\n cv2.destroyAllWindows()\n\n\n\n# Usage Example\n\nif __name__ == \"__main__\":\n\n face_id = input(\"\n\nEnter user ID and press <return>: \")\n\n face_dataset = FaceDataset(face_id)\n\n face_dataset.capture_faces()\n\nimport cv2\n\nimport numpy as np\n\nfrom PIL import Image\n\nimport os\n\nimport urllib.request\n\n\n\nclass FaceTrainer:\n\n \"\"\"\n\n A class to train the face recognizer using images stored in a dataset directory.\n\n\n\n Attributes:\n\n -----------\n\n dataset_path : str\n\n Path to the dataset directory containing face images.\n\n trainer_path : str\n\n Path to save the trained face recognizer model.\n\n recognizer : cv2.face.LBPHFaceRecognizer\n\n LBPH face recognizer object.\n\n detector : cv2.CascadeClassifier\n\n Haarcascade classifier for face detection.\n\n \"\"\"\n\n\n\n def __init__(self, dataset_path: str='dataset', trainer_path: str='trainer', cascade_path: str='haarcascade_frontalface_default.xml'):\n\n \"\"\"\n\n Initializes the FaceTrainer object with the provided parameters.\n\n \"\"\"\n\n\n\n self.dataset_path = dataset_path\n\n self.trainer_path = trainer_path\n\n self.recognizer = cv2.face.LBPHFaceRecognizer_create()\n\n self._download_haarcascade(cascade_path)\n\n self.detector = cv2.CascadeClassifier(cascade_path)\n\n\n\n def _download_haarcascade(self, cascade_path):\n\n \"\"\"Downloads the Haarcascade XML file if it does not exist.\"\"\"\n\n\n\n if not os.path.isfile(cascade_path):\n\n print(\"[INFO] Downloading Haarcascade XML file...\")\n\n url = \"https://github.com/opencv/opencv/raw/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n\n urllib.request.urlretrieve(url, cascade_path)\n\n print(\"[INFO] Download complete.\")\n\n\n\n def _get_images_and_labels(self):\n\n \"\"\"Extracts face samples and corresponding IDs from the dataset.\"\"\"\n\n\n\n image_paths = [os.path.join(self.dataset_path, f) for f in os.listdir(self.dataset_path)]\n\n face_samples = []\n\n ids = []\n\n\n\n for image_path in image_paths:\n\n PIL_img = Image.open(image_path).convert('L')\n\n img_numpy = np.array(PIL_img, 'uint8')\n\n user_id = int(os.path.split(image_path)[-1].split(\".\")[1])\n\n faces = self.detector.detectMultiScale(img_numpy)\n\n\n\n for (x, y, w, h) in faces:\n\n face_samples.append(img_numpy[y:y+h, x:x+w])\n\n ids.append(user_id)\n\n\n\n return face_samples, ids\n\n\n\n def train(self):\n\n \"\"\"Trains the face recognizer and saves the model to the trainer directory.\"\"\"\n\n \n\n print(\"\n\n Training faces. Please wait...\")\n\n faces, ids = self._get_images_and_labels()\n\n self.recognizer.train(faces, np.array(ids))\n\n\n\n if not os.path.exists(self.trainer_path):\n\n os.makedirs(self.trainer_path)\n\n\n\n self.recognizer.write(f'{self.trainer_path}/trainer.yml')\n\n print(f\"\n\n{len(np.unique(ids))} faces trained successfully. Model saved at {self.trainer_path}/trainer.yml\")\n\n\n\n# Usage Example\n\nif __name__ == \"__main__\":\n\n face_trainer = FaceTrainer()\n\n face_trainer.train()\n\nimport cv2\n\nimport numpy as np\n\nimport urllib.request\n\n\n\nclass FaceRecognizer:\n\n \"\"\"\n\n A class to perform real-time face recognition using a trained LBPH model.\n\n \"\"\"\n\n\n\n def __init__(self, trainer_path: str='trainer/trainer.yml', cascade_path: str='haarcascade_frontalface_default.xml', names: str=None):\n\n \"\"\"\n\n Initializes the FaceRecognizer object with the provided parameters.\n\n \"\"\"\n\n if names is None:\n\n names = ['None', 'Marcelo', 'Paula', 'Ilza', 'Z', 'W']\n\n self._download_haarcascade(cascade_path)\n\n self.recognizer = cv2.face.LBPHFaceRecognizer_create()\n\n self.recognizer.read(trainer_path)\n\n self.face_cascade = cv2.CascadeClassifier(cascade_path)\n\n self.names = names\n\n self.font = cv2.FONT_HERSHEY_SIMPLEX\n\n\n\n def _download_haarcascade(self, cascade_path: str):\n\n \"\"\"Downloads the Haarcascade XML file if it does not exist.\"\"\"\n\n\n\n if not os.path.isfile(cascade_path):\n\n print(\"[INFO] Downloading Haarcascade XML file...\")\n\n url = \"https://github.com/opencv/opencv/raw/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n\n urllib.request.urlretrieve(url, cascade_path)\n\n print(\"[INFO] Download complete.\")\n\n\n\n def recognize_faces(self, video_source: int=0, width: int=640, height: int=480):\n\n \"\"\"\n\n Performs real-time face recognition using the webcam.\n\n \"\"\"\n\n cam = cv2.VideoCapture(video_source)\n\n cam.set(3, width)\n\n cam.set(4, height)\n\n\n\n print(\"\n\n Starting face recognition. Look at the camera...\")\n\n\n\n while True:\n\n ret, img = cam.read()\n\n img = cv2.flip(img, -1)\n\n gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)\n\n\n\n for (x, y, w, h) in faces:\n\n cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n id, confidence = self.recognizer.predict(gray[y:y+h, x:x+w])\n\n\n\n if confidence < 100:\n\n name = self.names[id]\n\n confidence_text = f\" {round(100 - confidence)}%\"\n\n else:\n\n name = \"Unknown\"\n\n confidence_text = f\" {round(100 - confidence)}%\"\n\n\n\n cv2.putText(img, name, (x+5, y-5), self.font, 1, (255, 255, 255), 2)\n\n cv2.putText(img, confidence_text, (x+5, y+h-5), self.font, 1, (255, 255, 0), 1)\n\n\n\n cv2.imshow('Face Recognition', img)\n\n\n\n if cv2.waitKey(10) & 0xff == 27: # Press 'ESC' to exit\n\n break\n\n\n\n print(\"\n\n Exiting face recognition.\")\n\n cam.release()\n\n cv2.destroyAllWindows()\n\n\n\n# Usage Example\n\nif __name__ == \"__main__\":\n\n recognizer = FaceRecognizer(names=['None', 'John Doe', 'Jane Doe'])\n\n recognizer.recognize_faces()\n\nThis tutorial has shown you how to build a modular, object-oriented face detection and recognition system using OpenCV. By organizing the code into classes, we’ve made it more maintainable, scalable, and easier to extend for future improvements or additional features.\n\nWhether implementing a security system, developing a user authentication tool, or exploring computer vision, this approach lays a solid foundation. The code provided here can be adapted and enhanced to meet various needs and use cases."
    },
    {
        "link": "https://pyimagesearch.com/2021/05/03/face-recognition-with-local-binary-patterns-lbps-and-opencv",
        "document": "In this tutorial, you will learn how to perform face recognition using Local Binary Patterns (LBPs), OpenCV, and the function.\n\nIn our previous tutorial, we discussed the fundamentals of face recognition, including:\n• The difference between face detection and face recognition\n• The difference between classical face recognition methods and deep learning-based face recognizers\n\nToday we’re going to get our first taste of implementing face recognition through the Local Binary Patterns algorithm. By the end of this tutorial you’ll be able to implement your first face recognition system.\n\nTo learn how to perform face recognition with LBPs and OpenCV, just keep reading.\n\nIn the first part of this tutorial, we’ll discuss the LBPs for face recognition algorithm, including how it works.\n\nWe’ll then configure our development environment and review our project directory structure.\n\nI’ll then show you how to implement LBPs for face recognition using OpenCV.\n\nThe face recognition algorithm we’re covering here today was first presented by Ahonen et al. on their 2004 publication, Face Recognition with Local Binary Patterns.\n\nIn this section, we’ll present an overview of the algorithm. As you’ll see, it’s actually quite simple.\n\nGiven a face in a dataset, the first step of the algorithm is to divide the face into 7×7 equally sized cells:\n\nThen, for each of these cells, we compute a Local Binary Pattern histogram.\n\nBy definition, a histogram throws away all spatial information regarding how the patterns are oriented next to each other. However, by computing a histogram for each of the cells, we actually are able to encode a level of spatial information such as the eyes, nose, mouth, etc., that we would otherwise not have.\n\nThis spatial encoding also allows us to weigh the resulting histograms from each of the cells differently, giving more discriminative power to more distinguishing features of the face:\n\nHere, we can see the original face image divided into 7×7 cells (left). Then, on the right, we can see the weighting scheme for each of the cells:\n• LBP histograms for the white cells (such as the eyes) are weighed 4x more than the other cells. This simply means that we take the LBP histograms from the white cell regions and multiply them by 4 (taking into account any scaling/normalization of the histograms).\n• Dark gray cells (inner cheek and forehead) only contribute 1x.\n• Finally, the black cells, such as the nose and outer cheek, are totally disregarded and weighed 0x.\n\nThese weighting values were experimentally found by Ahonen et al. by running hyperparameter tuning algorithms on top of their training, validation, and testing data splits.\n\nFinally, the weighted 7×7 LBP histograms are concatenated together to form the final feature vector.\n\nPerforming face recognition is done using the distance and a nearest neighbor classifier:\n• A face is presented to the system\n• LBPs are extracted, weighted, and concatenated in the same manner as the training data\n• k-NN (with k=1) is performed with the distance to find the closest face in the training data.\n• The name of the person associated with the face with the smallest distance is chosen as the final classification\n\nAs you can see, the LBPs for face recognition algorithm is quite simple! Extracting Local Binary Patterns isn’t a challenging task — and extending the extraction method to compute histograms for 7×7 = 49 cells is straightforward enough.\n\nBefore we close this section, it’s important to note that the LBPs for face recognition algorithm has the added benefit of being updatable as new faces are introduced to the dataset.\n\nOther popular algorithms, such as Eigenfaces, require that all faces to be identified be present at training time. This implies that if a new face is added to the dataset the entire Eigenfaces classifier has to be re-trained which can be quite computationally intensive.\n\nInstead, the LBPs for face recognition algorithm can simply insert new face samples without having to be re-trained at all — an obvious benefit when working with face datasets where people are being added or removed from the dataset with routine frequency.\n\nTo learn how to perform use Local Binary Patterns for face recognition, you need to have OpenCV installed on your machine:\n\nIf you need help configuring your development environment for OpenCV, I highly recommend that you read my pip install OpenCV guide — it will have you up and running in a matter of minutes.\n\nAll that said, are you:\n• Wanting to skip the hassle of fighting with the command line, package managers, and virtual environments?\n• Ready to run the code right now on your Windows, macOS, or Linux systems?\n\nGain access to Jupyter Notebooks for this tutorial and other PyImageSearch guides that are pre-configured to run on Google Colab’s ecosystem right in your web browser! No installation required.\n\nAnd best of all, these Jupyter Notebooks will run on Windows, macOS, and Linux!\n\nThe CALTECH Faces challenge is a benchmark dataset for face recognition algorithms. Overall, the dataset consists of 450 images of approximately 27 unique people. Each subject was captured under various lighting conditions, background scenes, and facial expressions, as seen in Figure 4.\n\nThe overall goal of this tutorial is to apply the Eigenfaces face recognition algorithm to identify each of the subjects in the CALTECH Faces dataset.\n\nNote: I’ve included a slightly modified version of the CALTECH Faces dataset in the “Downloads” associated with this tutorial. The slightly modified version includes an easier to parse directory structure with faux names assigned to each of the subjects, making it easier to evaluate the accuracy of our face recognition system. Again, you do not need to download the CALTECH Faces dataset from CALTECH’s servers — just use the “Downloads” associated with this guide\n\nBefore we can implement face recognition with Local Binary Patterns, let’s first review our project directory structure.\n\nStart by accessing the “Downloads” section of this tutorial to retrieve the source code, pre-trained face detector, and example CALTECH Faces dataset:\n\nThe directory contains our OpenCV deep learning-based face detector. This detector is both fast and accurate, capable of running in real-time without a GPU.\n\nWe’ll be applying the face detector model to each image in the dataset. Inside this directory is a subdirectory containing images for each of the people we want to recognize:\n\nAs you can see, we have multiple images for each person we want to recognize. These images will serve as our training data such that our LBP face recognizer can learn what each individual looks like.\n\nFrom there, we have two Python scripts to review today.\n\nThe first, , lives in the module. This file contains two functions:\n• : Applies our face detector to a given image, returning the bounding box coordinates of the face(s)\n• : Loops over all images in and applies the function to each\n\nFinally, glues all the pieces together and forms our final Local Binary Patterns face recognition implementation.\n\nAs we learned in our introduction to face recognition guide, prior to performing face recognition we need to:\n• Detect the presence of a face in an image/video stream\n• Extract the region of interest (ROI), which is the face itself\n\nOnce we have the face ROI we can apply our face recognition algorithms to learn discerning patterns from the face of the individual. Once training is complete we can actually recognize people in images and video.\n\nLet’s learn how to apply our OpenCV face detector to detect faces in images. Open up the file in the module and let’s get to work:\n\nWe start on Lines 2-5 with our required Python packages. We’ll need the submodule of to grab the paths to all CALTECH Faces images residing on disk. The import provides our OpenCV bindings.\n• : Our deep neural network used for face detection\n• : The image we are going to apply face detection to\n• : The minimum confidence for a positive face detection — detections with a probability less than this value will be discarded as a false-positive result\n\nFrom there, we grab the spatial dimensions of the input and construct a such that it can be passed through our deep neural network.\n\nWith the created we set it as input to the face detector and perform inference:\n\nWe also initialize a list of to store our bounding box coordinates after applying face detection.\n\nSpeaking of which, let’s loop over our and populate the list now:\n\nLine 21 loops over all , while Line 24 extracts the of the current detection.\n\nLine 28 filters out weak/false-positive detections by throwing out any face detections that have a less than the .\n\nFrom there, we extract the bounding box coordinates of the face detection, scale them, and update our bounding list (Lines 31-35).\n\nThe final bounding boxes are returned to the calling function on Line 38.\n\nNote: If you need a more detailed review of OpenCV’s deep learning face detector, be sure to refer to my guide on Face detection with OpenCV and deep learning. That article goes into far greater detail and will give you a deeper understanding of how the face detector works.\n\nWith our face detection helper function implemented, we can move to implementing a second helper utility, .\n\nThis function is responsible for:\n• Looping over all images in the CALTECH Faces dataset\n• Counting the number of example images we have for each individual\n• Throwing out any individuals who have less than N faces for training data (otherwise we would run into a class imbalance problem)\n• Returning the face ROIs and class labels (i.e., names of the people) to the calling function\n\nLet’s get started implementing now. Again, open the file inside the module and append the following code at the bottom of the file:\n• : The face to the input dataset we want to train our LBP face recognizer on (in this case, the directory)\n• : Minimum probability/confidence of a face detection used to filter out weak/false-positive detections\n\nLine 46 grabs the paths to all images in our . We then extract the names from these on Line 47.\n• First, it determines the set of unique class labels from the (i.e., the names of the people we want to recognize)\n• Secondly, it counts the number of times each individual’s name appears\n\nWe perform this counting operation because we want to discard any individuals who have less than . If we tried to train our LBP face recognizer on individuals with a low number of training examples we would run into a class imbalance problem and accuracy would suffer (a concept that is outside the scope of this tutorial).\n\nLet’s now process each of our images now:\n\nLines 53 and 54 initialize two lists — one to store the extracted face ROIs and the other to store the names of the individual each face ROI contains.\n\nWe then loop over all on Line 57. For each face, we:\n• Extract the name of the individual from the subdirectory structure\n• Check to see if the has less than associated with it\n\nIf the minimum test fails (Lines 65 and 66), meaning there are not sufficient training images for this individual, we throw out the image and do not consider it for training.\n\nOtherwise, we assume the minimum test passed and then proceed to process the image:\n\nA call to on Line 69 performs face detection, resulting in a set of bounding boxes which we loop over on Line 72.\n• Use NumPy array slicing to extract the face ROI\n\nThe resulting and are then returned to the calling function.\n\nWith our helper utilities implemented, we can move on to creating the driver script responsible for extracting LBPs from the face ROIs, training the model, and then finally performing face recognition.\n\nOpen the file in your project directory structure, and let’s get to work:\n• : Used to encode the class labels (i.e., names of the individuals) as integers rather than strings (this is a requirement to utilize OpenCV’s LBP face recognizer)\n• : Constructs a training and testing split from our CALTECH Faces dataset\n\nWe have one required and two optional command line arguments to parse:\n• : The path to our input dataset containing images of the individuals we want to train our LBP face recognizer on\n• : Minimum probability used to filter out weak detections\n\nWith our command line arguments taken care of we can load the face detector from disk:\n\nFrom there we apply the function to load our face data:\n\nLines 33 and 34 load the CALTECH Faces dataset. Here we supply the path to the directory containing the dataset. We also supply the face detector ( and the minimum number of faces required for a person to be included in the training process ( ).\n\nWe then encode the using our (Lines 38 and 39) followed by constructing our training and testing split, using 75% of the data for training and 25% for evaluation (Lines 42 and 43).\n\nWe are now ready to train our face recognizer using LBPs and OpenCV:\n\nThe function accepts a few (optional) arguments that I explicitly define to make this example clear.\n\nThe and parameters are part of the Local Binary Patterns image descriptor. These values control the number of pixels included in the computation of the histogram, along with the radius these pixels lie on. Please see the Local Binary Patterns tutorial if you need a refresher on these parameters.\n\nThe and controls the number of MxN cells in the face recognition algorithm.\n\nWhile the original paper by Ahonen et al. suggested using a 7×7 grid, I prefer using an 8×8 grid which allows for more granularity, resulting in higher accuracy.\n\nHowever, this increased accuracy comes at the expense of (1) longer feature extraction/comparison times (due to the number of LBP histograms to be computed jumping from 49 to 64), and perhaps more importantly, (2) considerably more memory consumption to store the feature vectors.\n\nIn practice, you should tune the and hyperparameters on your own dataset and see which values yield the highest accuracy.\n\nTo train our LBP face recognizer, we simply call the method, passing in our CALTECH Faces training data along with the (integer) labels for each subject.\n\nLet’s now gather predictions using the LBP face recognizer:\n\nWe initialize two lists, and , to store the predicted class label and the confidence/probability of the prediction.\n\nFrom there, we loop over all images in our testing set (Line 61).\n\nFor each of these faces, we call the method of the which returns a 2-tuple of (1) the (i.e., the integer label of the subject) and (2) the (short for confidence) which is simply the distance between the current testing vector and the closest data point in the training data. The lower the distance, more likely the two faces are of the same subject.\n\nFinally, a classification report is displayed on Lines 73 and 74.\n\nOur final step is to visualize a subset of our face recognition results:\n\nWe then loop over each of these indexes on Line 80. For each index, we:\n• Extract the predicted name of the person from our label encoder (Line 82)\n• Grab the ground-truth name of the person (Line 83)\n• Resze the such that we can easily see it on our screen (Lines 87 and 88)\n• Draw the predicted name and actual name on the face (Lines 91-94)\n• Display the final output to our screen (Lines 99-104)\n\nAnd that’s all there is to it! Congratulations on implementing face recognition with Local Binary Patterns and LBPs!\n\nWe are now ready to perform face recognition with Local Binary Patterns and OpenCV!\n\n\n\nBe sure to access the “Downloads” section of this tutorial to retrieve the source code and example CALTECH Faces dataset.\n\nFrom there, open a terminal and execute the following command:\n\nAs our output shows, we first loop over all input images in our dataset, detect faces, and then extract LBPs using the face recognition algorithm. This process takes a bit of time due to LBPs needing to be computed for each cell.\n\nFrom there we perform inference, obtaining 98% accuracy.\n\nThe downside to this method is that it took just over 2 minutes to recognize all faces in our dataset. The reason inference is so slow is because we have to perform a nearest neighbor search across our entire training set.\n\n\n\nTo improve the speed of our algorithm we should consider using specialized approximate nearest neighbor algorithms which can dramatically reduce the amount of time it takes to perform a nearest neighbor search.\n\nNow, let’s apply our LBP face recognizer to individual images:\n\nFigure 5 displays a montage of results from our LBP face recognition algorithm. We’re able to correctly identify each of the individuals using the LBP method.\n\nThis lesson detailed how the Local Binary Patterns for face recognition algorithm works. We started by reviewing the CALTECH Faces dataset, a popular benchmark for evaluating face recognition algorithms.\n\nFrom there, we reviewed the LBPs face recognition algorithm introduced by Ahonen et al. in their 2004 paper, Face Recognition with Local Binary Patterns. This method is quite simple, yet effective. The entire algorithm essentially consists of three steps:\n• Extract Local Binary Patterns from each of the cells; weight them according to how discriminating each cell is for face recognition; and finally concatenate the 7×7 = 49 histograms to form the final feature vector\n• Perform face recognition by using a k-NN classifier with k=1 and the distance metric\n\nWhile the algorithm itself is quite simple to implement, OpenCV comes pre-built with a class dedicated to performing face recognition using LBPs. We used the to train our face recognizer on the CALTECH Faces dataset and obtained 98% accuracy, a good start in our face recognition journey.\n\nTo download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!"
    },
    {
        "link": "https://stackoverflow.com/questions/71313781/module-cv2-has-no-attribute-lbphfacerecognizer-create",
        "document": "I think that you may need to explicitly state \"cv2.face\" not just \"face...\"\n\nbefore doing so...did you confirm that you have a version of Opencv installed that contains the module Face? You can check like this:\n\nand if not...install like this:"
    }
]