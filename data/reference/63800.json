[
    {
        "link": "https://docs.oracle.com/javase/8/docs/api/java/awt/image/BufferedImage.html",
        "document": ". Returns a of objects that are the immediate sources, not the sources of these immediate sources, of image data for this\n\nSets a rectangular region of the image to the contents of the specified , which is assumed to be in the same coordinate space as the ."
    },
    {
        "link": "https://tutorialspoint.com/java_dip/java_buffered_image.htm",
        "document": "Java class is a subclass of Image class. It is used to handle and manipulate the image data. A is made of ColorModel of image data. All objects have an upper left corner coordinate of (0, 0).\n\nThis class supports three types of constructors.\n\nThe first constructor constructs a new with a specified ColorModel and Raster.\n\nThe second constructor constructs a of one of the predefined image types.\n\nThe third constructor constructs a of one of the predefined image types: TYPE_BYTE_BINARY or TYPE_BYTE_INDEXED.\n\nThe following example demonstrates the use of java class that draw some text on the screen using Graphics Object −\n\nWhen you execute the given code, the following output is seen −"
    },
    {
        "link": "https://docs.oracle.com/javase/8/docs/api/index.html?java/awt/image/BufferedImage.html",
        "document": "JavaScript is disabled on your browser.\n\nThis document is designed to be viewed using the frames feature. If you see this message, you are using a non-frame-capable web client. Link to Non-frame version."
    },
    {
        "link": "https://geeksforgeeks.org/image-processing-in-java-read-and-write",
        "document": "Java implements a particular type of object called a BufferedImage for images in Java. A BufferedImage can be read from several distinct image types (i.e., BMP, HEIC, etc.). Not all of these are backed by ImageIO itself, but there are plugins to extend ImageIO and other libraries such as Apache Imaging and JDeli.\n\nIn Java itself, all the complexity of various image types is hidden, and we only work on BufferedImage. Java provides immediate access to the image pixels and color information and allows conversions and image processing.\n\nClasses Required to Perform the Read and Write Operations:\n\n1. java.io.File: To read and write an image file, we must import the File class. This class represents file and directory path names in general.\n\n2. java.io.IOException: To handle errors, we use the IOException class.\n\n3. java.awt.image.BufferedImage: To hold the image, we create the BufferedImage object; we use BufferedImage class. This object is used to store an image in RAM.\n\n4. javax.imageio.ImageIO: To perform the image read-write operation, we will import the ImageIO class. This class has static methods to read and write an image."
    },
    {
        "link": "https://stackoverflow.com/questions/32414617/how-to-decide-which-bufferedimage-image-type-to-use",
        "document": "Java BufferedImage class has a long list of class variables known as the image type which can be used as an argument for the BufferedImage constructor.\n\nHowever, Java docs did a minimal explanation what these image types are used for and how would it affect the BufferedImage to be created.\n• None How would an image type affect the BufferedImage to be created? Does it control the number of bits used to store various colors (Red,Green,Blue) and its transparency?\n• None Which image type should we use if we just want to create\n\nI read the description in the Java Doc many times, but just couldn't figure out how should we use it. For example, this one:\n\nRepresents an image with 8-bit RGB color components, corresponding to a Windows- or Solaris- style BGR color model, with the colors Blue, Green, and Red packed into integer pixels. There is no alpha. The image has a DirectColorModel. When data with non-opaque alpha is stored in an image of this type, the color data must be adjusted to a non-premultiplied form and the alpha discarded, as described in the AlphaComposite documentation."
    },
    {
        "link": "https://stackoverflow.com/questions/64813186/contrast-stretching-transformation-in-image-processing",
        "document": "Here I split the transform function into three distinct regions each with a unique slope. The slopes were evaluated by taking the difference in neighbouring values the and the difference in neighbouring values the . The process of taking if statements depending on where the input intensity falls will dictate which effective slope to use. The last step is to add the offset of each linear line. Finding the is a process of evaluating how far into the region the intensity value has fallen. This can be done by subtracting the lower bound of the region by the intensity value. This will be dependent on the points. For region 2 the offset is and for region 3 the offset is .\n\nIn bracket notation the bounds of each region can described as:\n\nPlease take the time to look over the logic and mathematics to ensure no mistakes in the following code:"
    },
    {
        "link": "https://geeksforgeeks.org/image-processing-in-java-contrast-enhancement",
        "document": "\n• Image Processing In Java – Get and Set Pixels\n\nAt first, we need to set up OpenCV for Java, we recommend using eclipse for the same since it is easy to use and set up. Now let us understand some of the methods required for enhancing the color of an image. Now let us understand some of the methods required for contrast enhancement.\n\nMethod 1: equalizeHist(source, destination): This method resides in Imgproc package of opencv.source parameter is an 8-bit single-channel source image, and the destination parameter is the destination image\n\nMethod 2: Imcodecs.imread() or Imcodecs.imwrite(): These methods are used to read and write images as Mat objects which are rendered by OpenCV.\n\nImplementation: Let us take an arbitrary input image which is as follows:"
    },
    {
        "link": "https://dynamsoft.com/blog/insights/image-processing/image-processing-101-point-operations",
        "document": "This post discusses the basic point operations and the next post will be covering spatial filters.\n\nPoint operations are often used to change the grayscale range and distribution. The concept of point operation is to map every pixel onto a new image with a predefined transformation function.\n• T is an operator of intensity transformation\n\nThe simplest image enhancement method is to use a 1 x 1 neighborhood size. It is a point operation. In this case, the output pixel (‘s’) only depends on the input pixel (‘r’), and the point operation function can be simplified as follows:\n\nWhere T is the point operator of a certain gray-level mapping relationship between the original image and the output image.\n• s,r: denote the gray level of the input pixel and the output pixel.\n\nDifferent transformation functions work for different scenarios.\n\nIn identity transformation, the input image is the same as the output image.\n\nL = Largest gray level in an image. The negative transformation is suited for enhancing white or gray detail embedded in dark areas of an image, for example, analyzing the breast tissue in a digital mammogram.\n\nThe equation of general log transformation is:\n• s,r: denote the gray level of the input pixel and the output pixel.\n• ‘c’ is a constant; to map from [0,255] to [0,255], c = 255/LOG(256)\n• the base of a common logarithm is 10\n\nIn the log transformation, the low-intensity values are mapped into higher intensity values. It maps a narrow range of low gray levels to a much wider range. Generally speaking, the log transformation works the best for dark images.\n\nThe inverse log transform is opposite to log transform. It maps a narrow range of high gray levels to a much wider range. The inverse log transform expands the values of light-level pixels while compressing the darker-level values.\n• s,r: denote the gray level of the input pixel and the output pixel.\n• ‘c’ is a constant; to map from [0,255] to [0,255], c =LOG(256)/255\n\nThe gamma transform, also known as exponential transformation or power transformation, is a commonly used grayscale non-linear transformation. The mathematical expression of the gamma transformation is as follows:\n• s,r: denote the gray level of the input pixel and the output pixel;\n• ‘γ’ is also a constant and is the gamma coefficient.\n\nPlots of the equation [formula] for various values of γ (c = 1 in all cases)\n\nWhen performing the conversion, it is a common practice to first convert the intensity level from the range of 0 to 255 to 0 to 1. Then perform the gamma conversion and at last restore to the original range.\n\nComparing to log transformation, gamma transformation can generate a family of possible transformation curves by varying the gamma value.\n\nHere are the enhanced images output by using different γ (gamma) values.\n\nThe gamma transformation can selectively enhance the contrast of the dark region or the light region depending on the value of γ.\n• When γ > 1, the contrast of the light gray area is enhanced. Take γ = 25 for example, the pixels with the range of 0.8-1 (at the scale of 256, it corresponds to 240-255) are mapped to the range of 0-1\n• When γ < 1, the contrast of the dark gray area is enhanced\n• When γ = 1, this transformation is linear, that is, the original image is not changed\n\nIn mathematics, a piecewise-defined function is a function defined by multiple sub-functions, where each sub-function applies to a different interval in the domain. ※\n\nGrayscale Threshold Transform converts a grayscale image into a black and white binary image. The user specifies a value that acts as a dividing line. If the gray value of a pixel is smaller than the dividing, the intensity of the pixel is set to 0, otherwise it’s set to 255. The value of the dividing line is called the threshold. The grayscale threshold transform is often referred to as thresholding, or binarization.\n\nThe goal of the contrast-stretching transformation is to enhance the contrast between different parts of an image, that is, enhances the gray contrast for areas of interest, and suppresses the gray contrast for areas that are not of interest.\n\nBelow are two possible functions:\n\nIf T(r) has the form as shown in the figure, the effect of applying the transformation to every pixel to generate the corresponding pixels produce higher contrast than the original image, by:\n• Darkening the levels below k in the original image\n• Brightening the levels above k in the original image\n\nPoints (r1, s1) and (r2, s2) control the shape of the transformation. The selection of control points depends upon the types of image and varies from one image to another image. If r1 = s1 and r2 = s2 then the transformation is linear and this doesn’t affect the image. In other case we can calculate the intensity of output pixel, provided intensity of input pixel is x, as follows:\n\nAs discussed previously, each pixel of a grayscale image is stored as a 8-bit byte. The intensity spans from 0 to 255, which is ‘00000000’ to ‘11111111’ in binary.\n\nBit-plane slicing refers to the process of slicing a matrix of 8 bits into 8 planes.\n\nThe most left bit (the 8th bit from right to left) carries the most weight so it is called the Most Signification Bit. The most right bit (the 1st bit from right to left) carries the least weight so it is called the Least Signification Bit.\n\nHistogram equalization, also known as grayscale equalization, is a practical histogram correction technique.\n\nIt refers to the transformation where an output image has approximately the same number of pixels at each gray level, i.e., the histogram of the output is uniformly distributed. In the equalized image, the pixels will occupy as many gray levels as possible and be evenly distributed. Therefore, such image has a higher contrast ratio and a larger dynamic range.\n\nThis method is to boost the global contrast of an image to make it look more visible.\n• L is the maximum intensity value (typically 256)\n• M is the image width and N is the image height\n• h (v) is the equalized value"
    },
    {
        "link": "http://olympusconfocal.com/gfp/primer/java/digitalimaging/processing/histogramstretching/index.html",
        "document": "Contrast modification in digital images is a point process that involves application (addition, subtraction, multiplication, or division) of an identical constant value to every pixel in the image. This tutorial explores how redistributing brightness values through application of contrast stretching and histogram normalization algorithms can rehabilitate digital images having poor contrast.\n\nThe tutorial initializes with a specimen lacking proper contrast being displayed in the Specimen Image window. Each specimen name includes, in parentheses, an abbreviation designating the contrast mechanism employed in obtaining the image. The following nomenclature is used: (FL), fluorescence; (BF), brightfield; (DF), darkfield; (PC), phase contrast; (DIC), differential interference contrast (Nomarski); (HMC), Hoffman modulation contrast; and (POL), polarized light. All of the images utilized in the tutorial have contrast deficiencies and will benefit from proper implementation of the contrast stretching and histogram normalization algorithms. Visitors will note that specimens captured using the various techniques available in optical microscopy behave differently during image processing in the tutorial.\n\nAdjacent to the Specimen Image window is a Intensity Histogram graphical representation of the specimen intensity profile, which plots the number of pixels versus the pixel intensity (or brightness) distribution from 0 (black) to 255 (white). The black histogram is that of the original specimen image, while the gray histogram represents the current tutorial settings for a contrast-enhanced specimen image. To adjust image contrast, use the mouse cursor to translate the Black Level and White Level sliders while observing results appearing in both the image and histogram windows. Alternatively, the blue arrow buttons can be activated with the mouse to incrementally move the sliders, either to the right or left.\n\nCurrent black and white level values are indicated by red arrows in the histogram window. The Black Level slider determines the intensity level below which all pixels will be set to black. Moving this slider to the right shifts the black level red arrow to the right, and the intensity histogram of the specimen image stretches to the left as a result. The White Level slider determines the intensity level above which all pixels will be turned to white. Moving the White Level slider to the left shifts the right-hand red arrow to the left, and the intensity histogram of the specimen image stretches to the right. Together, these sliders can be utilized to restore contrast to digital images. As the sliders are shifted, the Transfer Function graph indicates changes in pixel input and output levels. Visitors can monitor their progress in rehabilitation of the digital images by clicking on the Show Original checkbox, which will toggle the Specimen Image to the original poor-contrast image. Digital images can be changed by selecting a new specimen from the Choose A Specimen pull-down menu. To select between grayscale and color specimen image sets, click on the appropriate (Grayscale Images or Color Images) radio button.\n\nHistograms of digital images provide a graphical representation of image contrast characteristics and are useful in evaluating contrast deficiencies such as low or high contrast, or inadequate dynamic range. Manipulation of the histogram can correct poor contrast and brightness to dramatically improve the quality of digital images.\n\nHistogram stretching involves modifying the brightness (intensity) values of pixels in the image according to some mapping function that specifies an output pixel brightness value for each input pixel brightness value. For a grayscale digital image, this process is straightforward. For an RGB color digital image, histogram manipulation can be accomplished by converting the image to a Hue, Saturation, Intensity (HSI) color space representation of the image and applying the brightness mapping operation to the intensity information alone. The following mapping function is utilized to compute pixel brightness values:\n\nIn the equation above, the intensity range is assumed to lie between 0.0 to 1.0, with 0.0 representing black and 1.0 representing white. The variable B represents the intensity value corresponding to the black level, while the intensity value corresponding to the white level is represented by the variable W. In some instances, it is desirable to apply a nonlinear mapping function (not addressed in this tutorial) to digital images in order to selectively modify portions of the image."
    },
    {
        "link": "https://eprints.uad.ac.id/18404/1/Image_Enhancement_Using_Piecewise_Linear_Contrast_.pdf",
        "document": ""
    }
]