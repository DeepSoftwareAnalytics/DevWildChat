[
    {
        "link": "https://datacamp.com/tutorial/cnn-tensorflow-python",
        "document": "Master the basics of data analysis with Python in just four hours. This online course will introduce the Python interface and explore popular packages."
    },
    {
        "link": "https://tensorflow.org/tutorials/images/cnn",
        "document": "This tutorial demonstrates training a simple Convolutional Neural Network (CNN) to classify CIFAR images. Because this tutorial uses the Keras Sequential API, creating and training your model will take just a few lines of code.\n\nThe CIFAR10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them.\n\nTo verify that the dataset looks correct, let's plot the first 25 images from the training set and display the class name below each image:\n\nThe 6 lines of code below define the convolutional base using a common pattern: a stack of Conv2D and MaxPooling2D layers.\n\nAs input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. If you are new to these dimensions, color_channels refers to (R,G,B). In this example, you will configure your CNN to process inputs of shape (32, 32, 3), which is the format of CIFAR images. You can do this by passing the argument to your first layer.\n\nLet's display the architecture of your model so far:\n\nAbove, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically, as the width and height shrink, you can afford (computationally) to add more output channels in each Conv2D layer.\n\nTo complete the model, you will feed the last output tensor from the convolutional base (of shape (4, 4, 64)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, you will flatten (or unroll) the 3D output to 1D, then add one or more Dense layers on top. CIFAR has 10 output classes, so you use a final Dense layer with 10 outputs.\n\nHere's the complete architecture of your model:\n\nThe network summary shows that (4, 4, 64) outputs were flattened into vectors of shape (1024) before going through two Dense layers.\n\nYour simple CNN has achieved a test accuracy of over 70%. Not bad for a few lines of code! For another CNN style, check out the TensorFlow 2 quickstart for experts example that uses the Keras subclassing API and ."
    },
    {
        "link": "https://github.com/turhancan97/Convolutional-Neural-Network-for-Object-Tracking",
        "document": "The project we prepared for the Vision-based Control lecture, which is one of the Poznan University of Technology Automatic Control and Robotics graduate courses. In this project, you will find couple of projects that can be helpful to learn Image Processing, Computer Vision, Deep Neural Network, Convolutional Neural Network topics and OpenCV, Tensorflow, Keras, YOLO Frameworks.\n• Project Lists\n• Face and Eye Detection with OpenCV and Haar Feature based Cascade Classifiers\n\nIn this repository, the details of image processing and computer vision are discussed. In addition, convolutional neural networks, which are frequently used in deep learning-based computer vision applications, are explained. In addition to these, seven basic level projects were exhibited in this repository to provide reinforcement on the topics discussed.\n\nMankind continues the data processing process, which started by drawing animal figures on the cave walls, with \"chips\" that are too small to be seen by the human eye. Although this development spanned a long period of approximately 4000 years, the real development took place in the last 50 years. It was impossible for human beings to even predict this technology, which has entered every aspect of our lives today and has become a routine of our lives. For example; A manager of IBM, one of the important companies in the computer industry, said, \"No matter how small a computer gets, it cannot be smaller than a room.\" The fact that one of the leading names in the industry is so wrong explains how fast computer technology has developed. In this development process, human beings, who are no longer content with their own intelligence, are also trying to give intelligence to machines; Now the goal is to produce machines that are more intelligent, capable of sampling human behavior, perceiving and interpreting images, characterizing sounds and, as a result, making decisions.\n\nAlthough the foundations of image formation go back centuries, intensive studies on machine imaging systems started with the introduction of special equipment. With the developments in technology, the use of imaging systems; production lines, medicine, weapons systems, criminology and security spanned many areas.\n\nToday, systems with automatic movement capability include a large share of the technological development process. In the progress of robot systems, researchers have to use sensors similar to the ones that humans have, opening up to the outside world, and develop perception principles in similar ways in order to produce systems that can make faster, more dynamic and more accurate decisions. In addition, this way of working should be close to the working speed of humanoid functions and should be produced in real time.\n\nWith the transfer of the image to the computer environment, there have been significant developments in the speed and capacity ratios of image processing devices. With each advancing year, digital image processors that allow obtaining higher resolution and pixelated images have begun to be developed. High resolution and pixel ratio have revealed high data capacity and significant developments have been experienced in recording environments. Many manufacturers have tried to impose their own recording standard, and along with this, image processing devices using very different recording media have been introduced to the market.\n\nMany factors in the field of image processing point to continuous improvement. One of the main reasons is the falling prices of necessary computer equipment. The processing units required are getting cheaper every year. A second factor is the increase in hardware options required for digitizing and displaying images. The indications are that the prices of essential computer hardware will continue to fall. Many new technologies are developing in a way that allows the expansion of studies in this field. Microprocessors, CCDs used for digitization, new memory technologies and high resolution image systems etc. Another acceleration in development is due to the continuous continuation of new applications. The uses of digital imaging in advertising, industrial, medical and scientific research continue to grow. Significant reduction in hardware costs and the ability to make important applications show that image processing techniques will play an important role in the future.\n\nIn addition, Computer Vision-based Robot systems is one of the fields that researchers have studied intensively. This issue, which is in parallel with the development of especially high-tech security solutions, industrial applications that require complex perceptions and defense technologies, has become the main study target for today's practitioners.\n\nThis repository has discussed in detail the concepts of image processing and computer vision, which have recently entered our lives with the developing technology, together with the applications.\n\nThis repository consists of eight main sections. The first one is the introductory part, which deals with computer vision, image processing and their difference, and also talks about neural networks and their algorithms and frameworks. The second, third, fourth and fifth sectionss are considered as image classification, object detection, recognition and tracking. There is a project list section at sixth section that describing the projects done. Finally, you can find future works and references as last two sections.\n\nImage processing is a method that can be identified with different techniques in order to obtain useful information according to the relevant need through the images transferred to the digital media. The image processing method is used to process certain recorded images and modify existing images and graphics to alienate or improve them. For example, it is possible to see the quality declines when scanning photos or documents and transferring them to digital media. This is where the image processing method comes into play during the quality declines. We use image processing method to minimize the degraded image quality and visual distortions. Another example is the maps that we can access via Google Earth. Satellite images are enhanced by image processing techniques. In this way, people are presented with higher quality images. Image processing, which can be used in this and many other places, is among the rapidly developing technologies. It is also one of the main research areas of disciplines such as engineering and computer science [1]. Image processing is basically studied in three steps.\n• Transferring the image with the necessary tools\n• Analyzing the image and processing it in the desired direction\n• Receiving the results of the data report and output that are analyzed and processed\n\nIn addition to the image processing steps, two types of methods are used for image processing. The first is analog image processing and the other is digital image processing. There are a number of basic steps that data must go through for digital and analog image processing. These steps are as follows:\n\nAfter these steps, results can be obtained from the relevant data according to the needs.\n\nImage processing has purposes such as visualization, making partially visible objects visible, image enhancement, removing spots, noise removal, high resolution capture in the image, pattern, and shape recognition.\n\nSome techniques which are used in digital image processing include:\n\nHuman perception of the outside world is formed by perceiving and analyzing images, which is one of the most important perception channels, and interpreting them. All techniques aiming to create visual perception and understanding in humans on the computer fall into the field of computer vision and cognitive learning. Scientists working in the field of computer vision have played the biggest role in the advancement of artificial intelligence, which is the most mentioned deep learning sub-field today.\n\nComputer vision aims to make sense of all kinds of two-dimensional, three-dimensional or higher-dimensional visual digital data, especially with smart algorithms. In solving these problems, the field of computer vision uses the development and implementation of both mathematical and computational theory-based techniques such as geometry, linear algebra, probability and statistics theory, differential equations, graph theory, and in recent years, especially machine learning and deep learning. In addition to standard camera images, medical images, satellite images, and computer modeling of three-dimensional objects and scenes are of interest to computer vision.\n\nVisual data detection and interpretation constitute the most important algorithm steps in many applications, from autonomous systems such as autonomous vehicles, robots, drones, to security and biometric verification areas. The outputs obtained are fed as inputs to the decision support systems in the next step, completing the artificial intelligence systems [2].\n\nComputer vision has emerged as a critical research topic, with commercial applications based on computer vision approaches accounting for a significant share of the market. Over the years, the accuracy and speed with which images acquired by cameras are processed and identified has improved. Deep learning, being the most well-known lad in town, is playing a vital role as a computer vision technology.\n\nIs deep learning the only way to accomplish computer vision? No, no, no! A few years ago, deep learning made its debut in the field of computer vision. Image processing algorithms and approaches were the mainstays of computer vision at the time. The main task of computer vision was to extract the image's features. When doing a computer vision task, the initial stage was to detect color, edges, corners, and objects. These features are human-engineered, and the extracted features, as well as the methodologies employed for feature extraction, have a direct impact on the model's accuracy and reliability. In the traditional vision scope, the algorithms like SIFT (Scale-Invariant Feature Transform), SURF (Speeded-Up Robust Features), BRIEF (Binary Robust Independent Elementary Features) plays the major role of extracting the features from the raw image.\n\nThe difficulty with this approach of feature extraction in image classification is that you have to choose which features to look for in each given image. When the number of classes of the classification goes high or the image clarity goes down it’s really hard to cope up with traditional computer vision algorithms.\n\nIn the field of computer vision, deep learning, which is a subset of machine learning, has shown considerable performance and accuracy gains. In a popular ImageNet computer vision competition in 2012, a neural network with over 60 million parameters greatly outperformed previous state-of-the-art algorithms to picture recognition, arguably one of the most influential studies in bringing deep learning to computer vision.\n\nThe boom started with the convolutional neural networks and the modified architectures of ConvNets. By now it is said that some convNet architectures are so close to 100% accuracy of image classification challenges, sometimes beating the human eye!\n\nThe main difference in deep learning approach of computer vision is the concept of end-to-end learning. There’s no longer need of defining the features and do feature engineering. The neural do that for you. It can simply put in this way. Though deep neural networks has its major drawbacks like, need of having huge amount of training data and need of large computation power, the field of computer vision has already conquered by this amazing tool already!\n\nComputer vision and image processing are both attractive fields of computer science.\n\nIn computer vision, computers or machines are programmed to extract high-level information from digital images or videos as input, with the goal of automating operations that the human visual system can perform. It employs a variety of techniques, including Image Processing.\n\nImage processing is the science of enhancing photographs by adjusting a variety of parameters and attributes. As a result, Image Processing is considered a subset of Computer Vision. In this case, transformations are made to an input image, and the resulting image is returned. Sharpening, smoothing, stretching, and other changes are examples of these transformations.\n\nBoth of the fields is working with visuals, i.e., images and videos. In fact, although it is not very accurate, we can say that if you use artificial intelligence algorithms and image processing methods in a project, that project is probably turning into a Computer Vision project. So Computer Vision is an intersection of Artificial Intelligence and Image Processing that usually aims to simulate intelligent human abilities.\n\nDeep neural networks are a field of research in which researchers show great interest under the science of artificial intelligence. It covers studies on learning computers. In this section, firstly, a general introduction to artificial intelligence will be given, then deep neural networks will be examined, and then the most widely used deep learning frameworks will be examined.\n\nComputers and computer systems have become an indispensable part of life in the contemporary world. Many devices, from mobile phones to refrigerators in kitchens, work with computer systems. It has become commonplace to use computers in almost every field, from the business world to public affairs, from environmental and health organizations to military systems. When the development of technology is followed, it is seen that computers, which were previously developed only for electronic data transfer and performing complex calculations, gain qualifications that can filter and summarize large amounts of data over time and make comments about events using existing information. Today, computers can both make decisions about events and learn the relationships between events.\n\nProblems that cannot be formulated mathematically and cannot be solved can be solved by computers using heuristic methods. Studies that equip computers with these features and enable them to develop these abilities are known as \"artificial intelligence\" studies. Artificial intelligence, in its simplest definition, is the general name of systems that try to imitate a certain part of human intelligence. From this point of view, when it comes to artificial intelligence, we should not think of systems that can completely imitate human intelligence or that have this purpose.\n\nArtificial intelligence can show itself in many different areas: Systems that predict what we write in our daily life, the search engine that allows us to search for an image on Google, Youtube's video recommendation system, and Instagram, which is very curious about how it once worked, is frequently ranked by those who see the story. are the examples we encountered.\n\nDeep learning, a sub-branch of Artificial Intelligence, is simply the name we give to training multi-layer artificial neural networks (Multi Layer Artificial Neural Networks) with an algorithm called \"backpropagation\". Even these two concepts are broad concepts that can be explained by books on their own. Artificial neural networks (ANNs), which are used in deep learning, are computer software that perform basic functions such as learning, remembering, and generating new data from the data it collects by imitating the learning path of the human brain. Artificial neural networks, inspired by the human brain, emerged as a result of the mathematical modeling of the learning process.\n\nBiological neuron and artificial neural network simulations are given in Figure 5. Biological Nervous system elements and their equivalents in the artificial nervous system are given in table below. Here, the biological nervous system is divided into parts and each element is given an equivalent in the artificial neural network system.\n\nAs seen in below gift, dog and cat data enters the network. The data processed in the middle layers is sent from there to the output layer. In short, this process is the conversion of incoming information to the output using the weight values of the network. In order for the network to produce the correct outputs for the inputs, the weights must have the correct values.\n\nIf you want to get more detailed information about artificial neural networks, you can click on this link. Let's take a look at the most widely used deep learning frameworks.\n\nPyTorch is an optimized tensor library for deep learning using GPUs and CPUs. PyTorch was published by Facebook in 2017 as a Python-based and open-source machine learning library connected to Torch. There is also a C/C++ interface. It offers a faster experience by using graphics processing units. This makes it superior and preferable to other libraries. It is preferred because it is more compatible with the structure we call Pythonic, that is, with various libraries in Python (numpy…). It is also easy and simple to understand. It does not act on a single Back-End. It offers different models for GPUs. What we call Back-End is the name given to the server in the background and the work of developing the base software. It uses dynamic computational graphics. The advantage of dynamic computational graphs lies in their ability to adapt to varying amounts in the input data.\n\nKeras is a deep learning library for Python used in machine learning. The biggest advantage of Keras is that it can run on libraries such as TensorFlow, Theano. It is ideal for beginners as it is easily and quickly accessible. It can run on CPU and GPU. You can choose whatever you want to save time. It supports convolutional neural networks (CNN) and iterative neural networks (RNN). There are quite a lot of resources on the internet because large companies have so many users, so when faced with a problem, the solution becomes simpler. TensorFlow is likewise an open source deep learning library. It can be used on CPU and GPU. Although it is based on Python, it supports multiple languages (C++, Java, C#, Javascript, R…).\n\nfastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes:\n• A new type dispatch system for Python along with a semantic type hierarchy for tensors\n• A GPU-optimized computer vision library which can be extended in pure Python\n• An optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4–5 lines of code\n• A novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training\n• And much more...\n\nfastai is organized around two main design goals: to be approachable and rapidly productive, while also being deeply hackable and configurable. It is built on top of a hierarchy of lower-level APIs which provide composable building blocks. This way, a user wanting to rewrite part of the high-level API or add particular behavior to suit their needs does not have to learn how to use the lowest level.\n\nConvolutional neural networks take pictures or videos as input due to their structure. Of course, when taking pictures, they must be translated into the relevant format. For example, if we are giving a picture to a convolutional neural network, we need to export it in matrix format. We see 32x32 and 5x5 matrices in Figure 9. The 3 next to them indicate the RGB value, that is, it is colored. (It is 1 in black and white) Thanks to the filter we apply to the matrix, data is obtained from the picture by comparing certain features on the picture. Let's now go deeper in CNN.\n\nThe symmetry of the filter to be applied to the two-dimensional information is taken according to the x and y axes. All values are multiplied element by element in the matrix and the sum of all values is recorded as the corresponding element of the output matrix. This is also called a cross-correlation relationship. This can be done simply when the input data (eg, image) is single-channel. However, the input data can be in different formats and number of channels.\n\nColor images consist of Red-Green-Blue (RGB) 3 channels. In this condition, the convolution operation is done for 3 channels. The channel number of the output signal is also calculated equally with the applied filter channel/number.\n\nLet's imagine this computation process as a layer in a neural network. The input image and the filter are actually a matrix of weights that are constantly updated by backpropagation. A scalar b (bias) value is added last to the output matrix to which the activation function is applied. You can examine the convolution process flow from the image below.\n\nEdge information is one of the most needed features from the image. It represents the high frequency regions of the input information. Two filters, vertical and horizontal, are used separately to obtain these attributes. In traditional methods - filters such as Sobel, Prewitt, Gabor - the filter is subject to a 'convolution' operation on the image. The resulting output shows the edge information of the image.\n\nWith different edge detection filters, angular edges, transitions from dark to light, and from light to dark are evaluated and calculated separately as an attribute. Generally, edges are computed in the first layers of a convolutional network model. While making all these calculations, there is a difference between the input size and the output size. For example; In case the input image (n): 6x6, edge detection filter (f): 3x3, the output image obtained as a result of the convolution operation becomes: (n-f+1)x(n-f+1)=4x4 dimensional. If it is not desired to reduce the size in this way - if the input and output sizes are desired to be equal - what should be done?\n\nIt is a computation at our disposal to manage the size difference between the input sign and the exit sign after the convolution operation. This is achieved by adding extra pixels to the input matrix.\n\nThis is exactly the job of adding pixels (padding) is called. In case the input matrix is nxn, filter (weight) matrix (fxf), if the output matrix is desired to be the same size as the input;\n\nHere, the value indicated by 'p' is the pixel size added to the input matrix, that is, the padding value. To determine this, the equation p=(f-1)/2 is used.\n\nThis value informs that for the convolution operation, the filter, which is the weight matrix, will shift on the image in one-pixel steps or larger steps. This is another parameter that directly affects the output size. For example, when the padding value is p=1 and the number of steps is s=2, the size of the output matrix\n\nIf it is calculated for n=5 and f=3 values, the output size will be (3)x(3). Pixels added in the padding operation can consist of zeros, as in the example below. Another implementation is to copy the value of the next pixel.\n\nIn this layer, the maximum pooling method is generally used. There are no learned parameters in this layer of the network. It reduces the height and width information by keeping the number of channels of the input matrix constant. It is a step used to reduce computational complexity. However, according to Hinton's capsule theory, it compromises performance as it causes some important information in the data to be lost.\n\nIt gives very good results, especially in problems where location information is not very important. Outputs the largest of the pixels within the selected jointing size. In the example on the right, 2x2 max-commoning is applied by shifting it by 2 steps (pixels). The largest value in the field with the 4 related elements is transferred to the output. At the output, a 1 in 4 dimensional data is obtained.\n\nWe now know the operations performed in the context of a convolutional network. So how is the model created? The easiest answer to this question is to examine classical network models.\n\nLeNet-5: It is the convolutional neural network model that was published in 1998 and gave the first successful result. It was developed by Yann LeCun and his team to read numbers on postal numbers, bank checks. Experiments are shown on the MNIST (Modified National Institute of Standards and Technology) dataset. In this model, unlike other models that will be developed later, average pooling is performed instead of max-pooling in size reduction steps. In addition, sigmoid and hyperbolic tangent are used as activation functions. The number of parameters entering the FC (Fully Connected) layer is 5x5x16=400 and there is a softmax with 10 classes because it classifies the numbers between 0 and 9 at the y output. In this network model, 60 thousand parameters are calculated. While the height and width information of the matrix decreases along the network, the depth (number of channels) value increases.\n\nAlexNet: It is the first study that made convolutional neural network models and deep learning popular again in 2012. It was developed by Alex Krizhevsky, Ilya Sutskever and Geoffrey Hinton. It is basically very similar to the LeNet model in that it has successive layers of convolution and pooling. ReLU (Rectified Linear Unit) is used as an activation function, and max-pooling is used in pooling layers. This larger and deeper network model is a two-part model on a parallel dual GPU (Graphics Processing Unit). Approximately 60 million parameters are calculated. It is a breaking point in the image classification problem, providing a spike in classification accuracy from 74.3% to 83.6% in the ImageNet ILSVRC competition.\n\nVGG-16: It is a simple network model and the most important difference from the previous models is the use of convolution layers in 2 or 3 layers. It is converted into a feature vector with 7x7x512=4096 neurons in the full link (FC) layer. The 1000 class softmax performance is calculated at the output of the two FC layers. Approximately 138 million parameters are calculated. As in other models, while the height and width dimensions of the matrices decrease from the input to the output, the depth value (number of channels) increases. At each convolution layer output of the model, filters with different weights are calculated, and as the number of layers increases, the features formed in the filters represent the 'depths' of the image. Other Common Network Architectures:\n• GoogLeNet - 2014 ILSVRC winner, changing the network architecture and reducing the number of parameters (4 million instead of 60 milion in AlexNet)\n• ResNet-50 - ResNet is one of the early adopters of batch normalisation (the batch norm paper authored by Ioffe and Szegedy was submitted to ICML in 2015) with 26M parameters.\n• Xception - It is an adaptation from Inception, where the Inception modules have been replaced with depthwise separable convolutions. It has also roughly the same number of parameters as Inception-v1.\n\nNow let's look at object recognition algorithms using convolutional neural networks!!!\n\nObject detection is a term related to computer vision and image processing that deals with detecting objects of a particular class (such as people, buildings or cars) in digital images and videos. More detailed explanation will be given in the following sections. Let's dive into R-CNN algorithm. R-CNN architecture has been developed because the CNN algorithm is not easily sufficient for images with multiple objects. The R-CNN algorithm uses the selective search algorithm that produces approximately 2000 possible regions where the object is likely to be found, and applies the CNN (ConvNet) algorithm to each region in turn. The size of the regions is determined and the correct region is placed in the neural network. A selective search algorithm is an algorithm that combines regions that are divided into smaller segments to generate a region recommendation.\n• Since each region in the image applies CNN separately, the training time is quite long and the prediction time is also long, so it takes a lot of time.\n\nIn R-CNN, we said that since the image is divided into 2000 regions, the training will take a lot of time and the cost will be high. Fast R-CNN was developed to solve this problem of R-CNN. The model consists of one stage compared to the 3 stages in R-CNN. It accepts only one image as input and displays the accuracy value of detected objects and bounding boxes. It also combines different parts of architectures (ConvNet, RoI Pooling Layer) into one complete architecture. This also eliminates the need to store a feature map and saves disk space.\n\nSimply put, the ROI Pooling Layer applies maximum pooling to each cell (input) in the grid to generate fixed size feature maps.\n\nFaster R-CNN, the most widely used and most advanced version of the R-CNN family, was first published in 2015. While both R-CNN and Fast R-CNN use CPU-based region bidding algorithms (For example: Selective search algorithm that takes about 2 seconds per image and runs on CPU computation), Faster R-CNN compares to Fast R-CNN to generate region suggestions. uses RPN, which is more convenient. This reduces the region suggest time from 2 seconds per image to 10 ms.\n\nRegion Proposal Network (RPN): The first stage, RPN, is a deep convolutional neural network for suggesting regions. The RPN takes any size of input as input and reveals the boundig box that can belong to a set of objects according to the truth value. It makes this suggestion by shifting a small mesh over the feature map generated by the convolutional layer. When the graph above is checked, you can notice that Faster R-CNN works in a much shorter time and is very fast. For this reason, Faster R-CNN is recommended for real-time object detection.\n\nIt is a deep neural network that aims to solve the instance segmentation problem in computer vision. Mask R-CNN can separate different objects in an image or video.\n\nIt is the task of defining object outlines at the pixel level. Compared to similar computer vision tasks, it is one of the most difficult vision tasks possible. If we consider the following tasks:\n• Classification: There is a balloon in this image. Semantic\n• Segmentation: These are all balloon pixels.\n• Object Detection: There are 7 balloons at these locations in this image. (We started to account for overlapping objects.)\n• Instance Segmentation: There are 7 balloons in these locations and these are the pixels of each. Mask R-CNN (regional convolutional neural network) is a two-stage framework: the first stage scans the image and generates suggestions (areas that are most likely to contain an object). The second stage categorizes the suggestions and creates bounding boxes and masks. Both stages depend on the backbone structure. Here are the results of applications using Mask R-CNN in TensorFlow\n\nNow let's talk about the most popular object detection algorithm of recent times. So why is the YOLO algorithm so popular?\n• It is fast because it passes the image through the neural network in one go.\n• It has learning abilities that enable it to learn objects and apply them in object detection.\n\nIn fact, we aim to predict the bounding box that specifies the class of an object and the object location. Each bounding box can be defined by four attributes:\n\nYOLO uses a single bounding box regression to estimate the height, width, center, and class of objects. The image above represents the accuracy probability (pc value) of an object appearing in the bounding box.\n\nYOLO divides the input image into NxN grids. Each grid checks if there is an object in it and if the object thinks it exists, it checks whether the center point is in its area. Deciding that the object has a center point, the grid finds that object's class, height, and width and draws a bounding box around the object.\n\nSometimes the same object is repeatedly marked with a bounding box because it exists in more than one grid. As a solution, the Non-max Suppression algorithm draws the bounding boxes with the highest accuracy value on the screen for the objects detected on the image. In short, bounding boxes with accuracy values below the specified threshold are deleted.\n\nThe object detection technique Single Shot MultiBox Detector (SSD) is a version of the VGG16 architecture. It was made public at the end of November 2016 and broke previous performance and accuracy records for object detection tasks, earning over 74% mAP (mean Average Precision) at 59 frames per second on benchmark datasets including PascalVOC and COCO.\n\nSSD’s architecture builds on the venerable VGG-16 architecture, but discards the fully connected layers.\n\nThe reason VGG-16 was used as the base network is because of its:\n• popularity for problems where transfer learning helps in improving results\n\nInstead of the original VGG fully connected layers, a set of auxiliary convolutional layers (from conv6 onwards) were added, thus enabling to extract features at multiple scales and progressively decrease the size of the input to each subsequent layer.\n\nRegion-based object detection algorithms such as R-CNN first determine the areas where objects are likely to be found and then apply CNN (Convolutional Neural Network, Convolutional Neural Networks) there separately. Although this method gives good results, since an image is subjected to two separate processes, the number of processing on the image increases and causes us to get a low FPS (Frames Per Second). The reason why the YOLO algorithm is so fast is that it can predict the class and coordinates of all objects in the picture by passing the image through the neural network at once, unlike systems such as R-CNN that require thousands of operations for a single image. So the basis of this estimation process is that they treat object detection as a single regression problem. This makes it extremely fast, 1000 times faster than R-CNN and 100 times faster than Fast R-CNN. If you need to do a project about real-time object detection, give the YOLO algorithm a chance.\n\nThe process of classifying an entire image is known as image classification. It is assumed that each image will only have one class. The class to which an image belongs is predicted by image classification models when they receive an image as input.\n\nObject detection is a computer technology that deals with finding instances of semantic items of a specific class (such as individuals, buildings, or cars) in digital photos and videos. It is related to computer vision and image processing. Face detection and pedestrian detection are two well-studied object detection areas. Object detection can be used in a variety of computer vision applications, such as picture retrieval and video surveillance [11].\n\nIt's used for picture annotation, vehicle counting, activity recognition, face detection, face recognition, and video object co-segmentation, among other things. It's also used to track things, such as a ball during a football game, a cricket bat's movement, or a person in a film.\n\nEvery object class has its own unique characteristics that aid in classification - for example, all circles are round. These particular properties are used in object class detection. When looking for circles, for example, items at a specific distance from a point (i.e. the center) are sought. Similarly, objects that are perpendicular at corners and have equal side lengths are required while seeking for squares. Face identification uses a similar approach, with traits such as skin color and eye distance being detected along with the eyes, nose, and lips.\n\nObject detection methods are classified as either neural network-based or non-neural approaches. Non-neural approaches require first defining features using one of the methods below, followed by classification using a technique such as support vector machine (SVM). On the other hand, neural approaches, which are often based on convolutional neural networks (CNN) , are capable of doing end-to-end object detection without specifying characteristics .\n• Neural network approaches:\n• You Only Look Once (YOLO)\n• a. Automatic detection of earthquake-induced ground failure effects through Faster R-CNN deep learning-based object detection using satellite images [12].\n• c. A deep learning method to detect foreign objects for inspecting power transmission lines [14].\n• d. Automatic Pass Annotation from Soccer VideoStreams Based on Object Detection and LSTM [15].\n\nObject recognition is a very popular computer vision technique used to detect as well as classify objects in images or videos. In short, this method includes object detection and classification.\n• c. Face recognition to identify and verify\n\nIt is aimed to follow the moving objects in the video and to obtain information such as location, speed or direction. Although people in videos are usually followed, animals or cars can also be followed. In order to carry out object tracking, object detection must be done first. Many different methods are used in object detection, such as subtracting two consecutive images from each other and detecting a moving object. Usually, after the objects in the image are detected, they are placed in a box and each box is assigned a number that has not been used before. Objects are tracked by these numbers.\n• The tracked object then goes behind another object and is not visible.\n• Detecting which is which when two objects intersect each other\n• Detection of the same object even if the same object looks different due to the movement of the camera or the object itself.\n• Objects can look very different from different perspectives and we need to consistently describe the same object from all perspectives.\n• Objects in a video can change scale significantly, for example due to camera zoom. In this case, we need to recognize the same object.\n• Light changes in video can have a huge impact on how objects appear and make it difficult to detect them consistently.\n• Face and Eye Detection with OpenCV and Haar Feature based Cascade Classifiers\n• Face Mask Detection for COVID19 with OpenCV, Tensorflow 2 and Keras\n• Object Detection and Tracking in Custom Datasets with Yolov4\n\nThe goal here is fair self-explanatory:\n• Step #1: Detect the presence of a colored object (blue in the code) using computer vision techniques.\n• Step #2: Track the object as it moves around in the video frames, drawing its previous positions as it moves.\n\nThe end product should look similar to the GIF below:\n\nUsing HSV color range which is determined as Lower and Upper, I detected colorful object. Here I prefered blue objects.\n\nWhen I got the color range, I set capture size and then I read the capture.\n\nFirst I apply Gaussian Blurring for decreasing the noises and details in capture.\n\nAfter Gaussian Blurring, I convert that into HSV color format.\n\nAfter mask, I have to clean around of masked object. Therefor I apply first Erosion and then Dilation\n\nAfter removing noises, the Contours have to be found\n\nIf the Contours have been found, I'll get the biggest contour due to be well.\n\nThe Contours which are found have to be turned into rectangle deu to put rectangle their around. This cv2.minAreaRect() function returns a rectangle which is smallest to cover the area of object.\n\nIn the screen, we want to print the information of rectangle, therefore we need to reach its inform.\n\nUsing this rectangle we found, we want to get a Box. In the next, we will use this Box for drawing Rectangle.\n\nImage Moment is a certain particular weighted average (moment) of the image pixels' intensities. To find Momentum, we use Max. Contour named as \"c\". After that, I find Center point.\n\nNow, I will draw the center which is found.\n\nWe want to print coordinators etc. in the screen\n\nFinally, we wrote the code below to track the blue object with past data\n\nThe goal here is to find contours, draw contours and run a motion detection and tracking algorithm by using contour information.\n\nThe end product should look similar to the GIF below:\n\nWe may quickly locate items in a picture by using contour detection to locate their borders. It frequently serves as the starting point for a variety of fascinating applications, including picture-foreground extraction, basic image segmentation, detection, and recognition [17].\n\nA contour is created by connecting every point along an object's boundaries. Typically, boundary pixels with the same color and intensity are referred to as a certain contour. Finding and drawing contours in images is quite simple with OpenCV. It does two straightforward tasks:\n\nAlso, it has two different algorithms for contour detection:\n\nFirstly, we captured the video that we have in our folder. These videos contain some walking people in Poznan, Paris and Rome. You can import any video by writing the name of the city by .\n\nThen, we want to read two frames from the capture.\n\nWe want to find the area that has changed since the last frame, not each pixel. In order to do so, we first need to find an area. This is what does; it retrieves contours or outer limits from each white spot from the part above. In the code below we find and draw all contours. Also, by using this contour information, we draw the rectangle to the object which are moving.\n\nPaul Viola and Michael Jones in their study Rapid Object Detection using a Boosted Cascade of Simple Features [18] described an efficient object detection method that uses Haar feature-based cascade classifiers. We will detect face and eyes in this project by using Haar Cascade Classifiers.\n\nThe end product should look similar to the GIF below:\n\nA cascade function is trained using a large number of both positive and negative images in this machine learning-based approach. The next step is to utilize it to find items in other pictures.\n\nFace and eye detection will be used in this instance. To train the classifier, the algorithm first requires a large number of both positive (pictures of faces (and eyes)) and negative (images without faces (and eyes)). After that, we must draw features from it. The Haar features in the image below are utilized for this. They are just like convolutional kernel. Each feature is a single value that is obtained by deducting the sum of the pixels under the white and black rectangles.\n\nAt first we need to import pretrained models for face and eye detection so that we do not need to find a lot of photo with eyes, faces (positive) and without (negative). This will give us to save time.\n\nIn the following lines of the code we call these pretrained models by , and .\n\nThen we should choose video to capture (can be male or female). Next, we need to convert our image into grayscale because Haar cascades work only on gray images. So, we are going to detect faces and eyes in a grayscale images, but we will draw rectangles around the detected faces on the color images.\n\nIn the first step we will detect the face. To extract coordinates of a rectangle that we are going to draw around the detected face, we need to create object faces. In this object we are going to store our detected faces. With a function we will obtain tuple of four elements: x and y are coordinates of a top left corner, and w and h are width and height of the rectangle. This method requires several arguments. First one is the gray image, the input image on which we will detect faces. Second argument is the scale factor which tells us how much the image size is reduced at each image scale. Third and last argument is the minimal number of neighbors. This parameter specifying how many neighbors each candidate rectangle should have to retain it.\n\nLater we detected the eyes. In order to do that, first we need to create two regions of interest Now we will detect the eyes. To detect the eyes, first we need to create two regions of interest which will be located inside the rectangle. We need first region for the gray image, where we going to detect the eyes, and second region will be used for the color image where we are going to draw rectangles.\n\nIn this project, it is aimed to create a Convolutional Neural Network to classify the digits from 0 to 9. Approximately 10000 images from 10 different classes are trained in the training code. A test script was then created for use with a webcam.\n\nThe end product should look similar to the GIF below:\n\nThe code below is for reading the images as well as the labels.\n\nThen we split our dataset into train, test and validation.\n\nWe can see the distribution of the test, train and validation via code below.\n\nNext, preprocessing function needs to be created to properly train our images with CNN.\n\nLater, we reshape your images and apply data augmentation for creating more variety of data, which helps us to learn better.\n\nBefore training the model, output variable should be implemented one hot encoding and then we can create model and apply CNN algorithm. Finally, we save our model for further real time classification.\n\nFinally, we plot our results and see confusiton matrix.\n\nThe code below is for capturing the video, loading the saved model and classifiying the digit in real time.\n\nIn this project, it is aimed to use MobileNets + Single Shot Detectors along with OpenCV to perform deep learning based real time object recognition.\n\nThe end product should look similar to the GIF below:\n\nThere are two pyton file in this project. One of them ( ) is for real time recognition and the other ( ) is for image based recognion.\n\nSome of the outputs of image based recognition:\n\nSince we learned the basics of computer vision and image processing thanks to this project, we are planning to test them on a mobile robot with a camera and make a project that can detect objects in real time and take different actions as a result of the detections.\n• [10] - Ceren Gulra Melek, Elena Battini Sonmez, and Songul Albayrak, “Object Detection in Shelf Images with YOLO” 978-1-5386-9301-8/19/$31.00 ©2019 IEEE, 2019, doi: 10.1109/EUROCON.2019.8861817\n• [12] - Hacıefendioğlu, K., Başağa, H.B. & Demir, G. Automatic detection of earthquake-induced ground failure effects through Faster R-CNN deep learning-based object detection using satellite images. Nat Hazards 105, 383–403 (2021). https://doi.org/10.1007/s11069-020-04315-y\n• [14] - J. Zhu et al., \"A Deep Learning Method to Detect Foreign Objects for Inspecting Power Transmission Lines,\" in IEEE Access, vol. 8, pp. 94065-94075, 2020, doi: 10.1109/ACCESS.2020.2995608.\n• [15] - Danilo Sorano, Fabio Carrara, Paolo Cintia, Fabrizio Falchi, and Luca Pappalardo. Automatic pass annotation from soccer videostreams based on object detection and lstm. arXiv preprint arXiv:2007.06475, 2020.\n• [18] - P. Viola and M. Jones, \"Rapid object detection using a boosted cascade of simple features,\" Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001, 2001, pp. I-I, doi: 10.1109/CVPR.2001.990517."
    },
    {
        "link": "https://pytorch.org/vision/main/models/keypoint_rcnn.html",
        "document": "The Keypoint R-CNN model is based on the Mask R-CNN paper.\n\nThe following model builders can be used to instantiate a Keypoint R-CNN model, with or without pre-trained weights. All the model builders internally rely on the base class. Please refer to the source code for more details about this class."
    },
    {
        "link": "https://debuggercafe.com/human-pose-detection-using-pytorch-keypoint-rcnn",
        "document": "In this tutorial, we will learn how to carry out human pose detection using PyTorch and the Keypoint RCNN neural network.\n\nWe will use a pre-trained PyTorch KeyPoint RCNN with ResNet50 backbone to detect keypoints in human bodies. The following image will make things much more clear about what we will be doing in this article.\n\nThe above image (figure 1) is from COCO 2018 Keypoint Detection Task dataset. It is one of the largest datasets for keypoint detection that is publicly available.\n\nBut before diving further into the code, let’s gain some more knowledge about human keypoint detection.\n\nWhat will we be learning in this article?\n• What is human body keypoint detection or pose detection?\n• Different models and systems that are available for human pose detection.\n• Detecting human pose in images and videos using PyTorch Keypoint RCNN?\n\nHuman Pose Detection using PyTorch in Deep Learning and Computer Vision\n\nIn general, pose detection or estimation is not a deep learning problem. In fact, it is a computer vision problem. But in recent years, deep learning based computer vision techniques have helped the research community a lot in achieving great results.\n\nSo, what is pose estimation in human body?\n\nHuman pose detection is detecting the important keypoints that can describe the orientation or movement of a person. You can also relate it to facial keypoint detection where we detect the interesting parts of a face and mark them. We can also do this in real-time.\n\nSimilarly, we can also detect the keypoints in a human body which describe the movement of the human body. This is known as human pose detection.\n\nFor example, take a look at the following image.\n\nFigure 2 shows 17 keypoints of a human body while the person is walking. You can see that each of the major joints below the face are all numbered. Similarly, the important points on the head are also numbered.\n\nI hope that the above image gives you a good idea of what we are trying to achieve here.\n\nIn this section, we will take learn a bit more about the Keypoint RCNN deep learning model for pose detection.\n\nBut before that, let’s have a brief look at different deep learning methods available for human pose detection. Although we will not go into the details, still you may wish to explore these methods on your own.\n\nOpenPose is of one of the most famous human keypoints and pose detection systems.\n\nIt is a real-time multi-person keypoint detection library for body, face, hands, and foot estimation. Along with that, it can detect a total of 135 keypoints on a human body.\n\nThe above are only some of the features. There are many more and I insist that you take a look at their GitHub repository which is quite impressive.\n\nAlso, if you wish, you can also read the OpenPose paper. It is authored by Gines Hidalgo, Zhe Cao, Tomas Simon, Shih-En Wei, Hanbyul Joo, and Yaser Sheikh.\n\nThe GitHub repository is also quite actively maintained. Do take a look at it to get some more details.\n\nThe original paper was published under the name RMPE: Regional Multi-person Pose Estimation by Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. The updated and better version has been renamed to AlphaPose.\n\nDeepCut is another such multi-person keypoint detection system. To fully do justice to the method, I am quoting the authors here.\n\nYou can visit the official website to get the complete details. There, you will also find the link to their GitHub repository and pre-trained models as well.\n\nNow, coming to the deep learning model and technique that we will use in this tutorial.\n\nWe will use one of the PyTorch pre-trained models for human pose and keypoint detection. It is the Keypoint RCNN deep learning model with a ResNet-50 base architecture.\n\nThis model has been pre-trained on the COCO Keypoint dataset. It outputs the keypoints for 17 human parts and body joints. They are: ‘nose’, ‘left_eye’, ‘right_eye’, ‘left_ear’, ‘right_ear’, ‘left_shoulder’, ‘right_shoulder’, ‘left_elbow’, ‘right_elbow’, ‘left_wrist’, ‘right_wrist’, ‘left_hip’, ‘right_hip’, ‘left_knee’, ‘right_knee’, ‘left_ankle’, ‘right_ankle’.\n\nDo you remember the keypoints and joints that you saw in figure 2? The following is the complete image with the keypoints and pose shown on the human body.\n\nFigure 3 shows the boy along with all the keypoints and the body pose. It is quite impressive what deep learning models are capable of achieving when trained on the right dataset.\n\nThe Keypoint RCNN model takes an image tensor as input during inference. It is of the format . For inference, the batch size is mostly going to be one.\n\nFor the output, the model returns a list of dictionary which in turn contains the resulting tensors. The fields of the dictionary are as follows:\n• Boxes ( ): the predicted boxes in format, with values of between and and values of between and .\n• Labels ( ): the predicted labels for each image\n• Scores ( ): the scores or each prediction.\n• Keypoints ( ): the locations of the predicted keypoints, in format.\n\nAnd for figure 3, the actual output is the following.\n\nSo, it outputs the bounding boxes, the labels, the scores, and the keypoints. All in all, it is a full fledge deep learning object detection model along with human pose detection capabilities.\n\nThis is all the detail we need about the Keypoint RCNN model for now. We will get to know the rest of the details while coding.\n\nBy now you know that we will be using PyTorch in this tutorial. PyTorch already provides a pre-trained Keypoint RCNN model with ResNet50 base which has been trained on the COCO keypoint dataset.\n\nTo follow along smoothly, I recommend that you download the latest version of PyTorch (PyTorch 1.6 at the time of writing this). This will ensure that you can download all the pre-trained models without any hassle.\n\nNow, coming to the project directory structure. We will follow the following structure.\n\nWe have three folders.\n• The folder contains the images and videos that we will use for inference.\n• The folder will contain all the output images and videos that we will obtain after running them through the Keypoint RCNN model.\n• And the folder contains three Python scripts. We will write the code for each of them shortly.\n\nYou can either use your own images and videos for inference and keypoint detection. Or if you want to use the same data as in this tutorial, then you can download the input files by clicking the button below.\n\nAfter downloading, you can unzip the file and keep them in the same project directory. All these images and videos are taken from Pixabay.\n\nFrom this section onward, we will get into the coding part of this tutorial. We have three Python scripts and we will fill them with code as per our requirements.\n\nFirst, we will write some helper/utility code that will help us along to detect the human poses efficiently. These utility scripts are mostly repetitive scripts that we can re-use many times.\n\nAll of this code will go into the Python file inside the folder.\n\nWe need only two libraries here. One is the OpenCV module to plot the different edges joining the keypoints. The other one is the Matplotlib library to get different sets of colors for different edges.\n\nDefine the Different Edges that We Will Join\n\nRemember how Keypoint RCNN outputs 17 keypoints for each person. To get the skeletal lines that we have seen above, we need to define the pairs of keypoints that we need to join. Let’s define those now.\n\nI have provided detailed documentation in the form of comment in the above code snippet.\n\nSo, how will we use the list that is holding the tuples?\n• Suppose that we have the 17 keypoints that Keypoint RCNN outputs. Those are marked from 0 to 16.\n• Now, each of the tuples pairs in the edges list contains those two points that we will connect. This means that we will connect point 0 with 1, point 0 with 2, point 2 with 4, and so on.\n• Also, we need not connect all the 17 keypoints with another keypoint. We can decide to connect any point that we want to get the skeletal shape that we want.\n\nCode to Join the Keypoints and Draw the Skeletal Lines\n\nWe have the pairs of the edges now. But how do we connect those keypoints to get the skeletal frame. We will write a very simple Python function for that.\n\nThe following block of code does that.\n• We are defining a function which accepts the list and the NumPy image as input parameters.\n• We loop over the in the list from line 15.\n• Then we detach the keypoints from the GPU that is detected for each person and convert them to NumPy array. This happens at line 16.\n• Starting from line 19, we only proceed if the confidence score of the detection is greater than 0.9. Less than 0.9 tends to give a lot of false positives and error-prone results.\n• Then we reshape the to convert them into 17 rows. Starting from line 21, we loop over each row and draw the keypoints on the image. Also, if you want to draw the keypoint numbers, then you can uncomment lines 27 and 28.\n• Now, coming to line 30. Here, we loop over each of the edge pairs defined above.\n• At line 32, first, we get different colors for different lines that we will draw using the Matplotlib library.\n• Then starting from line 37, we join each of the edge pairs to draw the skeletal structure over the image.\n\nActually, most of the heavy work is done. Drawing the keypoints and skeletal structures is one of the most important tasks regarding human pose detection. Now, most of the work will be done done by the pre-trained Keypoint RCNN model.\n\nWe are all set to write the code for human pose detection using deep learning in images. We will start with images and then move over to videos as well.\n\nThe code in this section will go into the Python script.\n\nWe will start with importing all the modules and libraries that we will need.\n\nWe are importing script at line 6 that we have just written. Also, we need the library to read the image so that we have the image pixel values in the proper range for PyTorch pre-trained model.\n\nNext, let’s define the argument parser to parse the command line arguments.\n\nWe will provide just one command line argument, that is the path to the input image.\n\nThe next block of code defines the image transforms.\n\nThe above will transform the PIL image to PyTorch tensors.\n\nInitialize the Model and Set the Computation Device\n\nWe will use the module from PyTorch to initialize the Keypoint RCNN model.\n\nFor the model, we need to provide two arguments. The first one is , so that it returns a pre-trained model. The second one is which will instruct the model to detect 17 of the keypoints in the human body. Here, we are also defininfg the computation device, loading the Keypoint RCNN model onto it and getting the model into mode.\n\nThe following block of code reads the image and prepares it to be fed into the Keypoint RCNN neural network model.\n\nAt line 28, we are keeping a copy of the image by converting it into NumPy array so that we can apply different OpenCV functions to it. Then we are converting it into the OpenCV BGR color format as well.\n\nPredict the Keypoints and Visualize the Result\n\nWe need to perform a forward pass so that the Keypoint RCNN model can detect the keypoints on the image.\n\nWe do not need to calculate the gradients during inference, therefore, we are keeping the forward pass within the block.\n\nAt line 38, we are calling the function from script to draw the keypoints and skeletal structure. Then we are visualizing the image.\n\nFinally, we just need to save our results to disk.\n\nWe are just setting a using the input path and saving the result to folder.\n\nThis is all the code we need to detect human pose and keypoints using deep learning on images.\n\nWe are all set to execute script and take a look at how the Keypoint RCNN model is performing.\n\nWe will use the images from the folder that I have provided above. Let’s start with the first image.\n\nOpen up your command line/terminal and into the folder of the project directory. Then type the following command.\n\nYou should get the following output.\n\nThe PyTorch Keypoint RCNN model is working perfectly in this case. It is correctly detecting all the 17 keypoints and joining the desired keypoint pairs is also giving a good structure. Looks like deep learning has made keypoint detection in humans really effective.\n\nNow, let’s throw a bit more challenging image to the model for keypoint detection.\n\nIn figure 5, we can see that the model is detecting the keypoints for one of the legs and one of the hands wrongly. It is detecting keypoint for one of the legs when there should not be any, and it is detecting the keypoints for the legs as keypoints for hands. Looks like our dancing man photo was a bit too much for the Keypoint RCNN model.\n\nWhat about multi-person detection? Let’s throw a final image challenge at our model.\n\nFrankly, the results are better than I expected. It is detecting all the keypoints correctly, except the boy’s right arm at the extreme left corner. Looks like because of low-lighting, the Keypoint RCNN neural network is finding it difficult to correctly regress the final keypoint for the right arm. Still, it is much better than expected.\n\nOur Keypoint RCNN deep neural network is performing well on images, but what about videos? This is what we are going to find out in the next section, where we will write the code to detect keypoints and human pose in videos using Keypoint RCNN deep neural network.\n\nHuman Pose Detection in Videos using PyTorch and Keypoint RCNN\n\nIn this section, we will write the code to detect keypoints and human pose in videos using PyTorch and Keypoint RCNN neural network.\n\nIt is going to be very similar to what we did for images. For videos, we just need to treat each individual frame as an image and our work is mostly done. The rest of the work will done by our script.\n\nAll of this code will go into the Python script.\n\nAs usual, we will start with the imports.\n\nIn the above block, we are also importing the module to keep track of the time and calculate the average FPS (Frames Per Second) which we will do later.\n\nThe next block of code does some preliminary work. This includes defining the argument parser, the transforms, and preparing the Keypoint RCNN model. It is going to be the same as we did in the case of images.\n\nSetting Up OpenCV for Video Capture and Saving\n\nWe will use OpenCV to capture the videos.\n\nAfter capturing the video, we are getting the video frames’ width and height at lines 31 and 32.\n\nAt line 35, we are defining the name for the of the output video file. The resulting video with the keypoints will save under this name in the folder.\n\nThen we are using the for defining the codec and save format of the video file. We will save the resulting video files in format.\n\nFinally, at lines 40 and 41, we are defining and to keep track of the total number of video frames and the total Frames Per Second as well.\n\nDetecting Keypoints and Pose in Each Video Frame\n\nTo detect pose in human bodies and predict the keypoints, we need to treat each individual frame in a video as one image. We can easily do that by iterating over all the video frames using a loop.\n\nThe next block of code does just that. Let’s write the code for that and then get into the explanation part.\n• We are iterating over the frames until they are present. Else we break out of the loop.\n• If a frame is present, then we convert the frame from OpenCV format to PIL image format at line 48 and convert it into RGB color format. We also keep a copy of the original frame at line 49.\n• Lines 52 and 54 transform the frame and add a batch dimension to it respectively.\n• At line 57, we define just before we feed our frame to the Keypoint RCNN model.\n• After detections happen at line 60, we define so that we can know the time taken for each forward pass.\n• Line 65 calls the function of script to draw the keypoints and skeletal structure.\n• Line 68 calculates the FPS for the current frame, line 70 adds FPS to the total FPS, and line 72 increments the frame count.\n• We show the image on the screen at line 76 and write it to disk at line 77.\n• Finally, when there are no more frames present, then we break out of the loop.\n\nThere are just a few more lines of code. We need to release the object, destroy all windows and calculate the average FPS.\n\nThat is all the code we need for human pose detection in videos using Keypoint RCNN and PyTorch. Now, we can move forward and execute the script to see how well the neural network performs.\n\nWe have two videos in our folder. Let’s begin with the first video.\n\nThe following is the final video that is saved to the disk. You should also get result similar to this.\n\nIn clip 1, we can see that the neural network is predicting the keypoints quite accurately for the most part. Even when the person is lifting his legs and moving his hands, the keypoints are correct. But the detections go wrong when the person is too near to the camera at the end of the clip. This is most probably because the neural network is not getting enough information to predict the keypoints correctly. It is not able to see the whole body in those frames.\n\nAlso, I got around 4.8 FPS on an average on my GTX 1060. It is not real-time, at least not on a mid-range GPU. Yours may vary according to your hardware.\n\nNow, let’s move on to the detect keypoints on the second video.\n\nThe following is the output clip.\n\nWe can clearly see that Keypoint RCNN can easily detect poses and keypoints for multiple people at the same time. There are some wrong detections for sure but they are for those people who are at the far back and appear small. The people who are closer to the camera, for them, the keypoint detections are quite accurate.\n\nI got around 3.6 FPS on an average for this clip. The reduction in FPS might to be due to the more number of detections.\n\nThere are many ways in which you can improve upon this project. You can try:\n• Using a better and bigger network as the backbone. Like ResNet101 pre-trained model.\n• You can also try inferencing over the video frames in batches instead of a single frame. This may also help to increase the FPS.\n\nIn this tutorial, you got to learn about human pose detection using deep learning and PyTorch. We used PyTorch Keypoint RCNN neural network model to detect keypoints and human pose in images and videos. I hope that you learned new things regarding deep learning in this article.\n\nIf you have any doubts, thoughts, or suggestions, then please leave them in the comment section. I will surely address them.\n\nYou can contact me using the Contact section. You can also find me on LinkedIn, and Twitter."
    },
    {
        "link": "https://ir.lib.uwo.ca/etd/9173",
        "document": "The recent pandemic has impeded patients with hand injuries from connecting in person with their therapists. To address this challenge and improve hand telerehabilitation, we propose two computer vision-based technologies, photogrammetry and augmented reality as alternative and affordable solutions for visualization and remote monitoring of hand trauma without costly equipment. In this thesis, we extend the application of 3D rendering and virtual reality-based user interface to hand therapy. We compare the performance of four popular photogrammetry software in reconstructing a 3D model of a synthetic human hand from videos captured through a smartphone. The visual quality, reconstruction time and geometric accuracy of output model meshes are compared. Reality Capture produces the best result, with output mesh having the least error of 1mm and a total reconstruction time of 15 minutes. We developed an augmented reality app using MediaPipe algorithms that extract hand key points, finger joint coordinates and angles in real-time from hand images or live stream media. We conducted a study to investigate its input variability and validity as a reliable tool for remote assessment of finger range of motion. The intraclass correlation coefficient between DIGITS and in-person measurement obtained is 0.767- 0.81 for finger extension and 0.958–0.857 for finger flexion. Finally, we develop and surveyed the usability of a mobile application that collects patient data medical history, self-reported pain levels and hand 3D models and transfer them to therapists. These technologies can improve hand telerehabilitation, aid clinicians in monitoring hand conditions remotely and make decisions on appropriate therapy, medication, and hand orthoses."
    },
    {
        "link": "http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_ICRA_2010/data/papers/1553.pdf",
        "document": ""
    },
    {
        "link": "https://arxiv.org/html/2409.12259v1",
        "document": "Hand detection and reconstruction has been a long-studied problem due to its numerous applications, ranging from virtual reality to sign language and human behaviour recognition . Given the large variations in hand appearance and articulation along with heavy occlusions and motion blur that are usually present in hand interactions, the task of hand pose estimation is a considerably challenging. Over the years, several methods have been proposed to tackle 3D hand pose estimation . However, despite producing credible results, these methods primarily focus on images containing a fixed number of hands and hence cannot generalize to in-the-wild images. In the closely related fields of 3D human body and face reconstruction, state-of-the-art methods employ bottom-up pipelines founded on top of high-performance detection models that initially localize human body and face within the image, enabling their generalization to in-the-wild images. Despite the numerous methods that have been proposed to solve the task of human body and face detection, there has been a notable lack in real-time hand detection methods. The importance of hand detectors is further emphasized considering that current 3D hand pose estimation frameworks operate on tight crops around the hand regions , high fidelity detectors are essential for the generalisation of such methods in in-the-wild scenarios. Popular hand detection and localisation methods , fail significantly to detect multiple hands and challenging poses, while more recent methods albeit producing reasonable results can not operate in real-time. Motivated from the lack of accurate hand detection frameworks, we propose a robust single-state anchor-free detector that can operates in over than 100 frames-per-second (fps). As we experimentally showcase, robust detections can enforce more stable 4D reconstructions and overcome jittering artifacts which is currently one of the main limitations of 3D frame-based pose estimation methods. In contrast to the relatively unexplored hand detection and localization, 3D hand pose estimation has received significantly more attention. Initial 3D pose reconstruction methods have focused on traditional convolution-based backbones to process and extract image features . Following the success of transformers and their ability to consume large amounts of data , several methods have paved the way of utilising transformer architectures scaling up the 3D human body and hand recovery . Recently, Pavlakos et al. showcased the effectiveness of vision transformers (ViT) using a simple yet powerful framework trained on a large-scale dataset. The key to the success of this method lies in the scale of its architecture, which is composed of more than 0.5 billion parameters, enabling it to effectively consume large amount of data. However, as shown in the literature , regressing the hand parameters from a single image results in bad alignment and incorrect poses. Currently, methods that aim to achieve better image alignment rely on a sub-optimal solutions, such as intermediate heatmap representations . To tackle this, we propose a high-fidelity 3D pose estimation method that decomposes 3D hand reconstruction into two stages. In particular, the decoder first predicts a rough hand estimation that is used to extract multi-scale image-aligned features from our refinement module. By leveraging the rough hand estimation, we can extract meaningful spatial features that lead to better image alignment and state-of-the-art performance on FreiHand and HO3D benchmark datasets. Additionally, in contrast to vertex regression methods that directly regress 3D vertices, our method predicts MANO parameters , ensuring both explainable and plausible hand poses. In this paper, we propose a high fidelity full stack method that can reconstruct 3D hands in real-time. Specifically:\n• Based on the limitations of current hand detection benchmark datasets, we collect a large-scale dataset of in-the-wild images that contain multiple hands and introduce a challenging benchmark for hand detection. We make the dataset along with its 2D and 3D publicly available.\n• We propose a real-time hand detection method trained on the aforementioned large-scale dataset that outperforms previous hand detection methods by a large margin in both accuracy and efficiency.\n• We propose a transformer-based method that facilitates high fidelity 3D reconstructions that tackles the architectural limitations of previous method using an novel refinement module. The proposed method, apart from highly efficient, achieves state-of-the-art performance in Freihand and HO3D benchmark datasets.\n\nHand Detection and Tracking. Object detection has been extensively studied in the literature achieving remarkable advancements and setting the foundations for human body and face detection pipelines. In contrast, despite two decades of research efforts , hand detection has not yet achieved comparable breakthroughs. Initial approaches used controlled conditions and depth cameras to detect and track human hands. Several efforts have been made to boost hand detection under different skin tones and backgrounds using multi-stage frameworks , however, they fail to generalize in challenging environments. Following the success in object detection, several methods have adopted fully convolutional architectures for hand detection . Simon et al. introduced a multi-view bootstrapping procedure to annotate in-the-wild data and train a real-time convolutional detector network. Recently, Narasimhaswamy et al. proposed an extension of MaskRCNN network to detect in-the-wild hands and identifying their corresponding contact points and body associations . Nevertheless, despite the extensive efforts in the literature, most methods rely on slow backbones and struggle with challenging images. The primary issue is the lack of large-scale training data featuring multiple levels of occlusions and motion blur from in-the-wild scenes. To tackle such limitation, we propose lightweight hand detector that is 45 faster compared to previous state-of-the-art detectors, trained on 2M in-the-wild images with diverse environments and occlusion. 3D Hand Pose Estimation. Similar to hand detection, initial approaches for hand pose estimation relied on depth cameras to reconstruct 3D hands. Boukhayma et al. introduced the first fully learnable pipeline that directly regresses the parameters of the MANO hand model from RGB images. In a similar manner, several follow-up works used heatmaps and iterative refinement to enforce 2D alignment. Kulon et al. introduced an alternative regression method that regresses 3D vertices instead of MANO pose parameters, which significantly outperformed previous methods. Various approaches have been proposed to improve task-specific challenges of 3D pose estimation, including robustness to occlusions and motion blur and reducing inference speed . Recently, Pavlakos et al. highlighted the importance of scaling up both the training data and the capacity of the model. Specifically, building on the success of the Vision Transformer (ViT) backbones for body pose estimation , they demonstrated that using a simple yet effective large-scale transformer architecture can achieve state-of-the-art performance when trained on a diverse collection of datasets. However, directly regressing MANO parameters from the image in one go may introduce miss-alignments and incorrect poses. To tackle this, we propose a novel refinement layer that deforms hand pose using mesh-aligned multi-scale features.\n\nA vital cause behind the lack of high-fidelity hand detection systems lies in the limited amount of in-the-wild datasets with multiple hand annotations. To build a robust hand detection and reconstruction framework, we collected a large-scale dataset with millions of in-the-wild hands (WHIM) with diverse poses, illuminations, occlusions, and skin tones. To collect the proposed dataset, we devised a pipeline to automatically annotate YouTube videos from diverse and challenging in-the-wild scenarios. In particular, we selected more than 1,500 YouTube videos containing hand activities including sign language, cooking, everyday activities, sports, and games with ego- and exo-centric viewpoints, motion blur, different hand scales, and interactions. To accurately detect and annotate the hands on each frame we used a combination of ensemble networks. Firstly, we used VitPose and AlphaPose to detect all humans in the frame and selected the bounding boxes with confidence bigger than 0.65. We then cropped the bounding boxes and fed them to an ensemble hand detection pipeline that consists of MediaPipe , OpenPose and ContactHands models. To localize the hand, we used a weighted average between the bounding box positions of the three detectors , scaled from their corresponding confidence : In addition to the bounding box, we used the estimated 2D landmarks , to fit a 3D parametric hand model . More specifically, we optimized shape and pose parameters to minimize the re-projection loss between the regressed and the estimated landmarks : where denotes the weak perspective projection transform and the estimated intrinsic camera matrix. Given the degrees of freedom of the human hand, optimizing the hand model using joint terms usually results in unnatural poses. To tackle the ambiguities during the optimization process, we followed and included bio-mechanical losses to constrain the optimization. In particular, apart from the re-projection error, we enhanced the fitting process using loss functions that constrain the bone lengths and the angle rotations to feasible ranges, as defined in : where and denote the bone length and the joint angle loss terms, respectively. For additional details of the bio-mechanical constraints, we refer the reader to . Finally, given that the bio-mechanical prior acts mainly on the joint space, we followed and trained a PCA model on ARCTIC dataset acting as a 3D prior and to model the distribution of feasible hand poses. We formulated the prior loss as the reconstruction error of the 3D mesh projected and reconstructed from the PCA space as: where is the eigenvector basis of components and is the mean mesh. In Fig. 2 \n\n Example of the proposed WHIM in-the-wild dataset.\n\nOver the past years, fully convolutional networks (FCNs) have shown remarkable efficiency in human detection and object detection . Building on their success, we employ an FCN architecture to achieve both accurate and real-time hand localization. Similar to object detection frameworks, given an image our goal is to detect the bounding boxes of the hands present in the image along with their hand side label . We follow the commonly used one-stage backbone-neck-head formulation, where we built upon the powerful and efficient DarkNet backbone . We extract the last three feature maps of the backbone to generate a multi-scale feature pyramid in the neck module. To enable our model to effectively capture multi-scale features using both top-down and bottom-up pathways across different feature maps, we utilized Path Aggregation Network (PANet) , an extension of Feature Pyramid Network that facilitates fine-grained information flow using a bottom-up path augmentation. Finally, we use three detection heads to predict the bounding boxes and hand side labels at different anchor resolutions. Following , we adopt an anchor-free design to enhance the flexibility of our localization method and directly predict bounding box coordinates without relying on predefined anchor boxes. An overview of the proposed detection network is visualized in Fig. 3. Similar to , we observed that joint keypoint supervision significantly improved the performance and the robustness of the detector. The full training objective can be defined as: where is the binary cross entropy loss between the predicted and the ground truth box labels, denotes the distributional focal loss which measures the difference between the predicted and the ground truth bounding box distributions, measures the discrepancy between the predicted and the ground truth bounding box and are weights that balance the losses. For additional details about the detection network training, we refer the reader to the supplementary material. \n\n : The proposed fully convolutional one-stage hand detection method receives an image and extracts multi-resolution feature maps that are then processed by the Path Aggregation Network (PANet). The corresponding features are then fed to three detection heads that predict the hand side, bounding box, and hand joints at different resolutions. We train the network with a multi-task loss for each anchor. : Given an image represented as a series of feature tokens along with a set of learnable camera , pose and shape tokens, we initially predict a rough estimation of the MANO and camera parameters using a ViT backbone ( ). The updated image tokens are then reshaped and upsampled through a series of deconvolutional layers to form a set of multi-resolution feature maps . We then project the estimated 3D hand to the generated feature maps and sample image-aligned multi-scale features through a novel refinement module . The sampled features are used to predict pose and shape residuals that refine the coarse hand estimation. Using this coarse-to-fine pose estimation strategy we facilitate image alignment and achieve better reconstruction performance. : Given an imagerepresented as a series of feature tokensalong with a set of learnable camera, poseand shapetokens, we initially predict a rough estimation of the MANOand cameraparameters using a ViT backbone (). The updated image tokens are then reshaped and upsampled through a series of deconvolutional layers to form a set of multi-resolution feature maps. We then project the estimated 3D hand to the generated feature maps and sample image-aligned multi-scale features through a novel refinement module. The sampled features are used to predict pose and shape residualsthat refine the coarse hand estimation. Using this coarse-to-fine pose estimation strategy we facilitate image alignment and achieve better reconstruction performance. Given an image that contains a human hand, tightly cropped around the hand detectors bounding box, the proposed 3D hand reconstruction method estimates the corresponding hand pose and shape MANO parameters along with the camera to obtain a 3D hand. To build a powerful 3D pose estimation network that can scale on large amounts of data, we follow , and built our backbone using a pre-trained ViT encoder . The image is first split into -size patches and then embedded to high dimensional tokens . To uniquely encode their spatial location, positional embeddings are added to the image tokens . In addition to the image tokens, we explicitly model hand pose, shape, and camera parameters with three distinct tokens . We then feed the concatenated tokens to a ViT transformer encoder to obtain a set of updated feature tokens . Using a set of MLP layers we regress a rough estimation of pose and shape parameters of the MANO model, which will serve as a prior for the refinement network. Similarly, we regress the camera translation and scale parameters from the camera token features. Multi-Scale Pose Refinement Module. In order to get better image alignment and more accurate hand pose, we introduce a fully differentiable refinement module that predicts pose and shape residuals of the rough hand estimation. To do so we leverage the image features extracted from the ViT backbone to serve as the 2D feature cues in our refinement module. In particular, we reshape image feature tokens to form a low resolution feature map and project the rough hand estimation to feature map using the estimated camera parameters. Then, using bilinear interpolation we sample from a feature vector for each projected vertex : Note that we project the whole hand mesh to the feature map, instead of just the hand joints, as we aim to acquire better shape and pose image alignment. The image-aligned vertex features are then aggregated to form a global feature vector that is used to regress pose and shape residuals: where denotes the aggregation function, e.g., mean, max, sum. Given that the initial feature map is very low-dimensional, we use a set of deconvolutional layers to upsample to multiple higher resolution feature maps that will serve as multi-scale features for the proposed refinement module. Intuitively, low-dimensional feature maps will provide global and structural residuals of the hand shape while more high-resolution features could provide finer details of the hand pose. Loss function. The method proposed in this paper is trained with supervision for 3D vertices , 2D joints and MANO parameters , when available. Additionally, following we utilize a discriminator network to enforce plausible hand poses and shapes and penalize irregular articulations. The full loss function can be defined as:\n\nIn this section, we first evaluate the proposed hand detection network using established benchmarks to assess its performance. Subsequently, we conduct an extensive qualitative and quantitative analysis of the proposed 3D hand pose estimation method. Finally, we demonstrate the critical role of precise hand localization in enhancing the accuracy of 4D hand reconstruction. Training. We train the proposed hand detection network using the curated WHIM dataset that consists of over 2M in-the-wild images of multiple hands and scales. To further boost the generalization and robustness of our network, we follow several data augmentations during training. Particularly, we introduce random rotations in the range of and translations in the range of along with random masking and cropping in the image. Additionally, in each training batch, we follow mosaic and mixup augmentation, which significantly affects the robustness to diverse hand scales. Evaluation. To compare our network, we employ popular baselines such as OpenPose and Mediapipe , which are widely used across the community , along with more recent hand detection pipelines such as ContactHands and ViTDet . All methods are evaluated under three criteria: i) the inference speed in terms of frames per second (FPS), ii) the detection performance in terms of average precision (AP) at IoU = 0.5 and mean AP at different IoU=0.5:0.05:0.95 thresholds and iii) the model size measured in Mb. An optimal hand detection system should be lightweight to ensure compatibility with mobile devices, operate in real-time to avoid impacting the runtime of a 3D pose estimation pipeline while achieving precise detections. \n\n Qualitative Evaluation of the proposed hand detection network on in-the-wild images. The proposed model demonstrates robustness across various lighting conditions, resolutions, hand scales, and even in the presence of motion blur. In Tab. 1, we evaluate the proposed and the baseline methods on three datasets: the proposed WHIM dataset along with the benchmark Coco-WholeBody and Oxford-Hands dataset. All experiments were conducted on a NVIDIA RTX 4090 GPU. As can be easily seen, the proposed detector can run in over than 130 FPS while achieving up to 21% improvement on AP compared to previous state-of-the-art models. In addition, compared to previous state-of-the-art model, ContactHands , the proposed detector is faster and has reduced model size which enables the utilization of the proposed detector in mobile applications and heavy pipelines without posing any significant overhead. It is important to note that despite the varying resolutions and hand scales across the three datasets, the proposed model consistently outperforms the baseline methods. This is particularly evident on the COCO-WholeBody dataset , an extension of the COCO dataset that includes full-body images, where the hands are relatively small compared to the overall image size. , Oxford-Hands and the proposed WHIM dataset. For each method we report the average precision (AP) at IoU=0.5 along with the mean average precision (mAP). We also compare the performance of each method in terms of model size, measured in Mb, and speed, measured in frames per second (FPS). Comparison with the state-of-the-art hand detection methods on COCO-Whole, Oxford-Handsand the proposed WHIM dataset. Ablation. The efficiency and accuracy of the proposed hand detection method are mainly attributed to the selection of the backbone architecture and the utilization of a large-scale training dataset. We further evaluate the contribution of each component using an ablation study on OxfordHands and WHIM datasets where we utilized different backbone networks and training datasets. As can be seen from Tab. 2, trained the detection network with different backbones, apart from achieving similar detection performance, significantly degraded the inference speed of the network. The importance of the proposed large-scale in-the-wild WHIM dataset is also validated in Tab. 2, where we can observe a significant performance drop when the model was trained with significantly less data, e.g., Proposed w. 0.25M, Proposed w. 0.5M, Proposed w. 1M. An interesting observation highlighting the versatility of the WHIM dataset is that the proposed model achieves better performance when trained on WHIM compared to the OxfordHands dataset. Finally, we evaluate the contribution of the proposed augmentation strategy and the use of landmark regression loss. The augmentation strategy significantly contributes to cross-dataset generalization, achieving 14% increase on mAP. Similarly, we can observe that incorporating landmark regression loss enhances the detector’s precision, leading to more robust detections. Ablation study : Evaluation of individual components in the proposed detection pipeline on OxfordHands and WHIM datasets. We use to denote identical network architecture and performance. Training. Following , we trained the proposed hand regressor using a combination of datasets to improve robustness to diverse poses, illuminations and occlusions. Particularly, we utilized a set of datasets containing both 2D and 3D annotations namely FreiHAND , HO3D , MTC , RHD , InterHand2.6M , H2O3D , DEX YCB , COCO WholeBody , Halpe MPII NZSL , BEDLAM , ARCTIC , Re:InterHand and Hot3D . In total we utilized 4.2M images, 55% more than previous state-of-the-art. Including egocentric datasets such as ARCTIC and Hot3D significantly improve the reconstruction performance of the proposed method on egocentric views. Evaluation. We compare the proposed method with state-of-the-art methods including METRO , Mesh Graphormer , AMVUR , MobRecon , HaMeR and SimpleHand . In Tab. 3 and Tab. 4 we report the reconstruction results the popular benchmark FreiHAND and HO3Dv2 datasets. Following the common protocol , we measure the reconstruction performance in terms of Procrustes Aligned Mean per Joint and Vertex Error (PA-MPJPE, PA-MPVPE) along with the fraction of poses with less than 5mm and 15mm error (F@5, F@15). Additionally, we report Area Under the Curve for 3D joints and vertices ( , ) for HO3D dataset. As can be observed the proposed method achieves state-of-the-art performance and outperforms previous methods under all metrics on both benchmark datasets, which can be further validated in Fig. 6. . We use the standard protocol and report metrics for evaluation of 3D joint and 3D mesh accuracy. PA-MPVPE and PA-MPJPE numbers are in mm. Comparison with the state-of-the-art on the FreiHAND dataset . We use the HO3Dv2 protocol and report metrics that evaluate accuracy of the estimated 3D joints and 3D mesh. PA-MPVPE and PA-MPJPE numbers are in mm. Comparison with the state-of-the-art on the HO3D dataset . We report ablations on the backbone and the training data used along with the novel refinement module. . WiLoR demonstrates robustness across challenging poses with heavy occlusions, while maintaining precise image alignment. of proposed and the baseline methods on FreiHAND dataset. WiLoR demonstrates robustness across challenging poses with heavy occlusions, while maintaining precise image alignment. Ablation. To further investigate the contributions of each component of the proposed method we conducted an ablation study. In Tab. 5, we assess the contribution of the backbone architecture, the training datasets used along with the refinement module. As can be observed, swapping the ViT backbone with the recent efficient FastViT architecture (Proposed w. FastViT) results in significant degradation of performance despite the runtime efficiency. Similarly, training the backbone from scratch without using the pre-trained weights of ViTPose (Proposed w/o ViTPose) also results in a performance drop. To evaluate the effect of the proposed refinement module we trained a model that directly regresses the MANO and camera parameters from the ViT output tokens without using any refinement module (Proposed w/o Refinement). Additionally, we trained a model with a single-scale refinement module that samples features from a single feature map (Proposed w. Single-Scale). Both architectural choices deteriorate the reconstruction performance of the proposed model which highlights the effect of the proposed multi-scale refinement module. Finally, we examine the effect of the large-scale training set by training two derivatives of the proposed model using only the FreiHAND dataset (Proposed w. FreiHAND ), similar to and a model trained on the datasets used in (Proposed w. Datasets ). A key challenge for 3D pose estimation methods is to achieve stable and robust 4D reconstructions without being trained on a dynamic setting . Traditionally, methods for 3D pose estimation from single image suffer from low temporal coherence and jittering effects across frames and can not generalize well to videos, setting a huge burden on their generalization to real-world reconstruction. To effectively evaluate the temporal coherence of the proposed method, we reconstruct frame-wise the 4D sequence and measure the jittering between frames. In particular, we calculate the mean per frame Euclidean distance of the 3D vertices (MPFVE) and joints (MPFJE) between consecutive frames. Additionally, similar to , we measure the jerk (Jitter) of the 3D hand joints motion along with the global Root Translation Error (RTE) that measures the displacement of the wrist across frames. In Tab. 6, we report the reconstruction results fro the best performing methods on HO3D dataset. WiLoR outperforms baseline methods in temporal coherence without relying on any temporal module based on the robust stability of the detections. Reconstruction of dynamic 3D Hands. We evaluate the temporal coherence and the jittering of the reconstruction for the proposed and the baseline methods on the HO3D dataset. We include for reference the ground truth values.\n• Pushing the envelope for rgb-based dense 3d hand pose estimation via neural rendering. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n• Introducing hot3d: An egocentric dataset for 3d hand and object tracking.\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• 3d hand shape and pose from images in the wild. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of the IEEE/CVF international conference on computer vision\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose.\n• Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\n• Proceedings of the IEEE/CVF international conference on computer vision\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Robust 3d hand pose estimation in single depth images: from single-view cnn to multi-view cnns. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Using k-poselets for detecting people and localizing their keypoints. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Humans in 4D: Reconstructing and tracking humans with transformers.\n• Humans in 4D: Reconstructing and tracking humans with transformers.\n• Ego4d: Around the world in 3,000 hours of egocentric video. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Honnotate: A method for 3d annotation of hand and object poses.\n• Keypoint transformer: Solving joint identification in challenging hands and object interactions for accurate 3d pose estimation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Keypoint transformer: Solving joint identification in challenging hands and object interactions for accurate 3d pose estimation.\n• Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\n• Proceedings of the IEEE international conference on computer vision\n• Multiple scale faster-rcnn approach to driver’s cell-phone usage and hands on steering wheel detection. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition workshops\n• Proceedings of the IEEE/CVF international conference on computer vision\n• A probabilistic attention model with occlusion-aware texture regression for 3d hand reconstruction from a single rgb image. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of the IEEE conference on computer vision and pattern recognition\n• Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\n• Proceedings of the IEEE/CVF international conference on computer vision\n• Proceedings of the IEEE conference on computer vision and pattern recognition\n• Proceedings of the IEEE international conference on computer vision\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image.\n• Interhand2.6m: A dataset and baseline for 3d interacting hand pose estimation from a single rgb image.\n• Contextual attention for hand detection in the wild. Proceedings of the IEEE/CVF international conference on computer vision\n• Detecting hands and recognizing physical contact in the wild.\n• Whose hands are these? hand detection and hand-body association in the wild. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Recovering 3d hand mesh sequence from a single blurry image: A new dataset and temporal unfolding. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Towards accurate multi-person pose estimation in the wild. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\n• Attention based detection and recognition of hand postures against complex backgrounds.\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n• You only look once: Unified, real-time object detection. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of 1994 IEEE workshop on motion of non-rigid and articulated objects\n• Embodied hands: Modeling and capturing hands and bodies together.\n• Proceedings of the IEEE international conference on computer vision workshops\n• Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of the IEEE international conference on computer vision\n• Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\n• Proceedings of the IEEE/CVF International Conference on Computer Vision\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings of the IEEE/CVF International Conference on Computer Vision\n• Attention is all you need.\n• An adaptive self-organizing color segmentation algorithm with application to robust real-time human hand localization.\n• Monocular total capture: Posing face, body, and hands in the wild. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\n• Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\n• Proceedings of the IEEE/CVF International Conference on Computer Vision\n• Enhancing geometric factors in model learning and inference for object detection and instance segmentation.\n• Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n• Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)\n• Proceedings of the IEEE international conference on computer vision\n• Freihand: A dataset for markerless capture of hand pose and shape from single rgb images. Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
        "link": "https://cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf",
        "document": ""
    },
    {
        "link": "https://sciencedirect.com/science/article/pii/S009784932500041X",
        "document": ""
    }
]