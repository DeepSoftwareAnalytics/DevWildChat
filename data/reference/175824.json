[
    {
        "link": "https://dev.epicgames.com/documentation/en-us/unreal-engine/custom-material-expressions-in-unreal-engine",
        "document": "The Custom Material Expression enables you to write custom HLSL shader code operating on an arbitrary amount of inputs and outputting the result of the operation.\n\nYou can insert the Custom expression from the Custom category in the Material palette or from the right-click search menu in the Material Graph.\n\nWhen the Custom node is selected, the following properties display in the Details Panel.\n\nAdd as many inputs as you need to the Inputs array, and name them. Then type or paste your HLSL code in the Code property field. You can type either a full function body with return statements as shown in the example, or a simple expression such as Input.bgr. You must also specify the output data type in OutputType.\n\nThe example shown below is a Custom expression that can blur a Texture Object based on the value in a Scalar Parameter.\n\nHere is the code that was used above so that you can try the Custom node for yourself. Copy and paste the following text into the Code field in the Details Panel properties.\n\nThe following video shows the creation and result of a Custom expression using the above HLSL code.\n\nThe following sections detail some common pitfalls to be aware of when writing custom Material expressions.\n\nNote that Custom expressions always have a return value because they are wrapped inside another function. All input arguments (like Tex, UV, r, and dist in the above example) are declared as parameters of that outer function, so their scope of visibility is function-local.\n\nSometimes developers write structures with member functions and expect these parameters to be accessible inside that structure, but they cannot access function-local parameters. Therefore, you must copy those parameters by hand when member functions are used inside the Custom expression.\n\nThe example below would fail to compile (assuming the same example as above).\n\nSome shader backends that use the older HLSLcc compiler report syntax errors on HLSL types such as vector and matrix. Try to be explicit with your data types and use float4 or float4x4 respectively. We keep fixing those bugs but at the same time we are moving our efforts to the new DirectXShaderCompiler (DXC)."
    },
    {
        "link": "https://dev.epicgames.com/documentation/en-us/unreal-engine/custom-expressions?application_version=4.27",
        "document": "The Custom expression allows you to write custom HLSL shader code operating on an arbitrary amount of inputs and outputting the result of the operation.\n\nContains the shader code the expression will execute. (See warnings below) Specifies the type of the value output by the expression. Specifies the text to display in the title bar of the expression in the Material Editor. The array of inputs used by the expression. Specifies the name of the input. This is the name displayed on the expression in the Material Editor as well as the name used within the HLSL code to reference the input's value. [/REGION]\n\nAdd as many inputs as you need to the Inputs array, and name them. You can then write code in the Code property. You can type either a full function body with return statements as shown in the example, or a simple expression such as Input.bgr. You must also specify the output data type in OutputType.\n\nHere is the code that was used above so that you can try out the Custom node for yourself."
    },
    {
        "link": "https://forums.unrealengine.com/t/call-material-function-from-material-custom-node/238323",
        "document": "An overview of adding and using your own Global Shaders to Unreal Engine."
    },
    {
        "link": "https://forums.unrealengine.com/t/material-expressions-custom-node-how-to-use/287238",
        "document": "Material expressions that allow the use of custom, plain shader code."
    },
    {
        "link": "https://forums.unrealengine.com/t/extending-custom-hlsl-custom-expressions/88820",
        "document": "Until we see some more love for fast, custom HLSL in Unreal, here is a discussion of my recently discovered tricks to go beyond the current limits.\n\nModifying the Engine’s USF files to define custom functions triggers an overhaul shader recompilation of thousands of materials. This is unusable for creative iteration. Also #include-ing files outsides the Engine’s Shaders directory crashes the Editor at startup. CustomExpression nodes wrap your code inside CustomExpression#() functions, and that normally prohibits defining your own functions.\n\nHowever, there seems to be a barely documented feature in HLSL that allows defining functions (methods) inside struct definitions. struct definitions can be nested inside functions (like the wrapper CustomExpression#).\n\nSo in your CustomExpression Code you can do:\n\nCreate and connect an input pin “InColor” of float3 on the CustomExpression node. Any Node inputs passed into CustomExpression#(), like InColor above is available inside nested function definitions.\n\nThe cool part is, this is all happening inside your own effective namespace, not interfering with Unreal’s USF files, and the compilation is Fast for iteration. So now, you can start to build a library of custom functions, and more complex shaders. It seems HLSL is prohibiting defining a struct nested inside a struct, so make sure to define your custom structs above and outside struct Functions.\n\nInstead of editing intricate code and custom libraries inside the little primitive textbox of CustomExpression, you can edit them in a better external editor with syntax highlighting, code navigation etc, and #include that file. So if you put the above code in a file named Test.hlsl, you can:\n\nThe dummy “return 0;” is to tell CustomExpression node that this not a single line expression but a full function body. The spaces will be required to signal the CustomExpression textbox that it changed, and pressing enter will compile your externally changed and saved Test.hlsl. Of course, you can split the external file and the dynamic code portion, if you prefer to make quick changes and compiles inside the textbox.\n\nThe CustomExpression node limits the output to a float4. I have some ideas using CustomExpression#() chains and injecting macros to redefine their definitions and calls later to create additional outputs or complete Material outputs later at CalcPixelMaterialInputs() etc. But let’s see how the above tricks work for people, before we delve into that.\n\nSome image processes like convolution are far more efficient when broken into multiple passes by storing intermediate results in a texture. Currently, if you have a process that comes before convolution, a previous function has to be called redundantly for all the neighborhood kernel samples. There is some hope in Unreal’s render targets, but whether they can render multiple passes per frame synced, etc, and how much more spaghetti they will create with additional pass materials and control blueprints, we should discuss… Ideally, texture writing should be available completely inside the Material Editor, there should be variables, true branching, and custom #includes at the global scope, all features requested for years to take the already excellent default PBR system in Unreal to the next level with industry leading oceans, volumetric clouds, professional chroma key, etc."
    },
    {
        "link": "https://dev.epicgames.com/documentation/en-us/unreal-engine/math-material-expressions-in-unreal-engine",
        "document": "Abs is an abbreviation for the mathematical term \"absolute value\". The Abs expression outputs the absolute, or unsigned, value of the input it receives. Essentially, this means it turns negative numbers into positive numbers by dropping the minus sign, while positive numbers and zero remain unchanged.\n\nExamples: Abs of -0.7 is 0.7; Abs of -1.0 is 1.0; Abs of 1.0 is also 1.0\n\nExample Usage: Abs is commonly used with Dot Product to determine the angular relationship between two vectors: whether they are parallel, perpendicular, or somewhere in between. Normally, when you get the dot product of two vectors, the value is interpolated between 1.0 (for two parallel vectors) and -1.0 (for two exactly opposite vectors), with the midpoint of 0 indicating that the two vectors are perpendicular. When you take the absolute value of this dot product, the positive values remain unchanged, but the negative values are converted into positive numbers by dropping the minus sign. The result is therefore interpolated between 0 (for perpendicular vectors) and 1.0 (for parallel vectors, regardless of whether the vectors point in the same direction or opposite directions). This tells you simply how far the two vectors are from being orthogonal.\n\nThe Add expression takes two inputs, adds them together, and outputs the result.\n\nIf you pass values with multiple channels, each channel is added separately. For example, if you pass RGB color values to each input, the R channel of the first input is added to the R channel of the second input and the result is stored in the R channel of the output; the G channel of the first input is added to the G channel of the second input and the result is stored in the G channel of the output, and so on.\n\nBoth inputs must have the same number of values, unless one of the values is a single Constant value. In that case, each channel of the multi-channeled input is added to the single float value, and the result is stored in a separate channel of the output value.\n\nExamples: Add of 0.2 and 0.4 is 0.6; Add of (0.2,-0.4,0.6) and (0.1,0.5,1.0) is (0.3,0.1,1.6); Add of (0.2,-0.4,0.6) and 1.0 is (1.2,0.6,1.6)\n\nExample Usage: Add is often used to brighten/darken colors or to offset UV texture coordinates.\n\nThe AppendVector expression enables you to combine channels together to create a vector with more channels than the original. For example, you can take two individual Constant values and append them to make a two-channel Constant2 Vector value. This can be useful for reordering the channels within a single texture or for combining multiple grayscale textures into one RGB color texture.\n\nExamples: Append of 0.2 and 0.4 is (0.2,0.4); Append of (0.2,0.4) and (1.0) is (0.2,0.4,1.0).\n\nThe image above shows two different visualizations of the result of applying this expression:\n• The top bar shows the result as an output color. The left end of the bar shows the color that results from using this expression on an input value of -1, and the right end of the bar shows the results for a value of 1.\n• In the graph, the X axis represents input values ranging from -1 to 1. The Y axis shows the results of using this expression on those input values, again ranging from -1 to 1.\n\nThe ArccosineFast expression outputs an approximation of the inverse cosine function that is faster to calculate than the more accurate Arccosine expression. Input must be between -1 and 1.\n\nSee the Arccosine expression above for a visualization of the output values.\n\nThe image above shows two different visualizations of the result of applying this expression:\n• The top bar shows the result as an output color. The left end of the bar shows the color that results from using this expression on an input value of -1, and the right end of the bar shows the results for a value of 1.\n• In the graph, the X axis represents input values ranging from -1 to 1. The Y axis shows the results of using this expression on those input values, again ranging from -1 to 1.\n\nThe ArcsineFast expression outputs an approximation of the inverse sine function that is faster to calculate than the more accurate Arcsine expression. Input must be between -1 and 1.\n\nSee the Arcsine expression above for a visualization of the output values.\n\nThe image above shows two different visualizations of the result of applying this expression:\n• The top bar shows the result as an output color. The left end of the bar shows the color that results from using this expression on an input value of -1, and the right end of the bar shows the results for a value of 1.\n• In the graph, the X axis represents input values ranging from -1 to 1. The Y axis shows the results of using this expression on those input values, again ranging from -1 to 1.\n\nThe Arctangent2 expression outputs the inverse tangent of x / y where input signs are used to determine quadrant.\n\nThe image above shows two different visualizations of the result of applying this expression:\n• The top bar shows the result as an output color. The left end of the bar shows the color that results from using this expression on an input value of -1, and the right end of the bar shows the results for a value of 1.\n• In the graph, the X axis represents input values ranging from -1 to 1. The Y axis shows the results of using this expression on those input values, again ranging from -1 to 1.\n\nThe Arctangent2Fast expression outputs an approximation of the inverse tangent of X / Y where input signs are used to determine quadrant. It is faster to calculate but less accurate than the Arctangent2 expression.\n\nSee the Arctangent2 expression above for a visualization of the output values.\n\nThe ArctangentFast expression outputs an approximation of the inverse tangent function that is faster to calculate than the more accurate Arctangent expression.\n\nSee the Arctangent expression above for a visualization of the output values.\n\nThe Ceil expression takes in value(s), rounds them up to the next integer, and outputs the result. See also Floor and Frac.\n\nExamples: Ceil of 0.2 is 1.0; Ceil of (0.2,1.6) is (1.0,2.0).\n\nThe Clamp expression takes in value(s) and constrains them to a specified range, defined by a minimum and maximum value. A minimum value of 0.0 and maximum value of 0.5 means that the resulting value(s) will never be less than 0.0 and never greater than 0.5.\n\nExamples: Clamping an input range of (0.0) to (1.0) 0.3 with Min 0.0 and Max 1.0 yields 0.3; Clamping 1.3 with Min 0.0 and Max 1.0 yields 1.0.\n\nThe ComponentMask expression enables you to select a specific subset of channels (R, G, B, and/or A) from the input to pass through to the output. Attempting to pass a channel through that does not exist in the input will cause an error, unless the input is a single constant value. In that case, the single value is passed through to each channel. The current channels selected to be passed through are displayed in the title bar of the expression.\n\nExamples: ComponentMask with an input of (0.2,0.4,1.0) and the G channel will produce an output of (0.4), which appears as a 40% bright grayscale value when used as a color vector.\n\nThe Cosine expression outputs the value of a cosine wave over the input range of [0, 1] and the output range of [-1, 1], both repeating. Most commonly, this is used to output a continuous oscillating waveform by connecting a Time expression to its input, but it can also be used to create ripples in worldspace or screenspace, or any other application where a continuous, smooth cycle is needed. A visual representation of the wave is shown below, scaled to the [0, 1] output range:\n\nExample Usage: This expression is useful anytime an oscillating effect is needed. The speed and amplitude of the oscillation can easily be controlled dynamically by multiplying the time input (speed) or the output (amplitude).\n\nIn the example above, the color would oscillate with co-sinusoidal frequency.\n\nThe Divide expression takes two inputs, divides the first input by the second, and outputs the result.\n\nIf you pass values with multiple channels, each channel is divided separately. For example, if you pass RGB color values to each input, the R channel of the first input is divided by the R channel of the second input and the result is stored in the R channel of the output; the G channel of the first input is divided by the G channel of the second input and the result is stored in the G channel of the output, and so on.\n\nBoth inputs must have the same number of values, unless one of the values is a single float value. In that case, each channel of the multi-channeled input is divided by the single float value, and the result is stored in a separate channel of the output value.\n\nExamples: Using Divide with A=(1.0) and B=(5.0) outputs (0.2), which appears as a dark gray.\n\nThe Floor expression takes in value(s), rounds them down to the previous integer, and outputs the result. See also Ceil and Frac.\n\nExamples: Floor of 0.2 is 0.0; Floor of (0.2,1.6) is (0.0, 1.0).\n\nThe Fmod expression returns the floating-point remainder of the division operation of the two inputs. The Dividend (input \"A\") can be any value, but negative Dividends will result in negative results. The Divisor (second input) should not be zero as this implies a division by zero, but whether the Divisor is negative or positive will not affect the result. A common use case is to make a material that brightens up to a maximum value, then immediately drops back to the minimum value on the next frame, only to begin climbing toward the maximum again.\n\nIn this example, FMod takes a zero-to-one UV progression and converts it into a repeating cycle every 0.2 UV units on the X axis in the green channel.\n\nThe Frac expression takes in values and outputs the fractional portion of those values. In other words, for an input value \"X\", the result is \"X minus Floor of X\". The output value will range from zero to one, inclusive on the low end, but not the high end. See also Ceil and Floor.\n\nExamples: Frac of (0.2) is (0.2). Frac of (-0.2) is (0.8). Frac of (0.0,1.6,1.0) is (0.0,0.6,0.0).\n\nIn this example, the Frac node is converting time into a series of repeating 0 - 1 progressions, causing the color to fade (via the Lerp) from green to red, then snap back to green, repeating indefinitely.\n\nThe LinearInterpolate expression blends between two input value(s) based on a third input value used as a mask. This can be thought of as a mask to define transitions between two textures, like a layer mask in Photoshop. The intensity of the mask Alpha determines the ratio of color to take from the two input values. If Alpha is 0.0, the first input is used. If Alpha is 1.0, the second input is used. If Alpha is between 0.0 and 1.0, the output is a blend between the two inputs. Keep in mind that the blend happens per channel. So, if Alpha is an RGB color, Alpha's red channel value defines the blend between A and B's red channels independently of Alpha's green channel, which defines the blend between A and B's green channels.\n\nProgrammers: LinearInterpolate does a per-channel lerp between A and B based on the parametric value Alpha.\n\nThe Logarithm10 node returns the base-10 logarithm, also called the common logarithm, of the input value. That is, if you take a base value of 10 and raise it to the power of the number returned by this expression, you would get the input value.\n\nThe Logarithm2 node returns the base-2 logarithm of the input value. That is, if you take a base value of 2 and raise it to the power of the number returned by this expression, you would get the input value.\n\nThe Max expression takes in two inputs and outputs the higher of the two.\n\nWhen you use this node with color inputs, the result is similar to the Lighten layer blending mode in Photoshop.\n\nIn the example above, A is \"0\" and B is \"1\"; therefore, \"1\" (white) is the resulting base color.\n\nThe Min expression takes in two inputs and outputs the lower of the two.\n\nWhen you use this node with color inputs, the result is similar to using the Darken layer blending mode in Photoshop.\n\nIn the example above, A is \"0\" and B is \"1\"; therefore, \"0\" (black) is the resulting base color.\n\nThe Multiply expression takes two inputs, multiplies them together, and outputs the result. When you pass color values as input, the results are similar to the results of the Multiply layer blending mode in Photoshop.\n\nIf you pass values with multiple channels, each channel is multiplied separately. For example, if you pass RGB color values to each input, the R channel of the first input is multiplied by the R channel of the second input and the result is stored in the R channel of the output; the G channel of the first input is multiplied by the G channel of the second input and the result is stored in the G channel of the output, and so on.\n\nBoth inputs must have the same number of values, unless one of the values is a single float value. In that case, each channel of the multi-channeled input is multiplied by the single float value, and stored in a separate channel of the output value.\n\nDo not forget that Materials in Unreal Engine are not limited to [0,1]. If colors/values are greater than 1, Multiply will actually brighten colors.\n\nExamples: Multiply of 0.4 and 0.5 is 0.2; Multiply of (0.2,-0.4,0.6) and (0.0,2.0,1.0) is (0.0,-0.8,0.6); Multiply of (0.2,-0.4,0.6) and 0.5 is (0.1,-0.2,0.3).\n\nExample Usage: Multiply is often used to brighten or darken colors/textures, or manipulate the UVs of a texture.\n\nThe OneMinus expression takes an input value \"X\" and outputs \"One Minus X\". This operation is performed per channel.\n\nExamples: OneMinus of 0.4 is 0.6; OneMinus of (0.2,0.5,1.0) is (0.8,0.5,0.0); OneMinus of (0.0,-0.4,1.6) is (1.0,1.4,-0.6).\n\nExample Usage: When the input colors are in the range [0,1], OneMinus has the same effect as what is commonly called \"invert\" -- that is, OneMinus returns the complementary color that when added to the input will produce white.\n\nThe Power expression takes two inputs: a base value (Base) and an exponent (Exp). It raises the base value to the power of the exponent and outputs the result. In other words, it returns Base multiplied by itself Exp times.\n\nExample Usage: If the colors you pass to Power are in [0,1], Power can behave as a sort of contrast adjustment, where very bright values are dimmed slightly, but darker values are decreased drastically.\n\nThe Round expression rounds the input value to the nearest whole number. If the fractional part of the input value is 0.5 or greater, then the output value is rounded up. Otherwise, the output value is rounded down.\n• A value of 1.1 will be rounded down to a value of 1.\n• A value of 1.4 will be rounded down to a value of 1.\n• A value of 1.5 will be rounded up to a value of 2.\n• A value of 1.85 will be rounded up to a value of 2.\n\nThe Saturate node clamps the value between 0 and 1. Values less than 0 are raised to 0; values greater than 1 are lowered to 1; values between 0 and 1 inclusive remain unchanged. The instruction cost of Saturate is almost free on most modern graphics hardware, so you can use this node any time you need to clamp an input or output value between 0 and 1 without effect on your material's performance.\n\nExample Usage: This node should be used whenever you need to clamp an output or input value between 0 and 1.\n\nThe Step Material Expression returns 0 or 1 for every value of X, depending on whether it is greater than or less than the reference value in Y.\n\nBelow, a linear gradient (0 to 1) is plugged into the X input. The value in Y (0.25) acts as the reference value. Every value on the gradient below 0.25 on the returns a 0 (black), every value greater than or equal to 0.25 returns a 1 (white).\n\nThe Step expression is useful when an abrupt on/off effect is needed. For example, you could also use a Step expression to simplify a grayscale texture into a black and white mask.\n\nSmoothStep lets you interpolate between 0 and 1 for values between a Min and Max threshold. SmoothStep takes three arguments:\n• Min defines the lower edge of the interpolation. SmoothStep returns 0 (black) for values less than or equal to Min.\n• Max defines the upper edge of the interpolation. Smoothstep returns 1 (white) for values greater than or equal to Max.\n• Value defines the source value(s) for interpolation. For example, a gradient or a grayscale texture map.\n\nThis expression is useful for transitions where you want the edge to have some degree of smoothness.\n\nBelow a LinearGradient is passed into the Value input, and the Min and Max values are set at 0.1 and 0.9. This results in a relatively smooth gradient where 0.1 is black and 0.9 is white.\n\nOn the second slide, the Min and Max are set to 0.6 and 0.8, resulting in a much more abrupt transition. Everything below 0.6 is black, everything above 0.8 is white, with a smooth gradient in between.\n\nThe Sign node indicates whether a numeric input is negative, positive, or exactly 0.\n• If the input is negative, this node outputs -1.\n• If the input is exactly 0, this node outputs 0.\n• If the input is positive, this node outputs 1.\n\nThe Sine expression outputs the value of a Sine wave over the input range of [0, 1] and the output range of [-1, 1], both repeating. The difference between this and the output of the Cosine expression is the output waveform is offset by one-quarter of the period, meaning that is equal to . Most commonly, this is used to output a continuous oscillating waveform by connecting a Timeexpression to its input, but it can also be used to create ripples in worldspace or screenspace, or any other application where a continuous, smooth cycle is needed. A visual representation of the wave is shown below, scaled to the [0, 1] output range:\n\nExample Usage: This expression is useful anytime an oscillating effect is needed. The speed and amplitude of the oscillation can easily be controlled dynamically by multiplying the time input (speed) or the output (amplitude).\n\nThe SquareRoot expression outputs the square root of the input value. If applied to a vector, each component will be handled individually.\n\nFor textures in the 0 to 1 range, this reduces the apparent contrast of the image. For example, on the calibration texture below, dark values become brighter and white values shift toward gray.\n\nThe Subtract node takes in two inputs, subtracts the second input from the first, and outputs the difference.\n\nIf you pass values with multiple channels, each channel is subtracted separately. For example, if you pass RGB color values to each input, the R channel of the second input is subtracted from the R channel of the first input and the result is stored in the R channel of the output; the G channel of the second input is subtracted from the G channel of the first input and the result is stored in the G channel of the output, and so on.\n\nBoth inputs must have the same number of values, unless one of the values is a single Constant value. In that case, each channel of the multi-channel input has the Constant value subtracted from it, and the result stored in a separate channel of the output value.\n\nExamples: Subtract of 0.5 and 0.2 is 0.3; Subtract of (0.2,-0.4,0.6) and (0.1,0.1,1.0) is (0.1,-0.5,-0.4); Subtract of (0.2,0.4,1.0) and 0.2 is (0.0,0.2,0.8).\n\nExample Usage: Subtract can be used to darken colors and offset UVs.\n\nThe Tangent node outputs the tangent of the specified value.\n\nThe image above shows two different visualizations of the result of applying this expression:\n• The top bar shows the result as an output color. The left end of the bar shows the color that results from using this expression on an input value of -1, and the right end of the bar shows the results for a value of 1.\n• In the graph, the X axis represents input values ranging from -1 to 1. The Y axis shows the results of using this expression on those input values, again ranging from -1 to 1.\n\nThe Truncate node truncates a value by discarding the fractional part while leaving the whole number untouched.\n• An input value of 1.1 will be truncated to 1.\n• An input value of 1.4 will be truncated to 1.\n• An input value of 2.5 will be truncated to 2.\n• An input value of 3.1 will be truncated to 3."
    },
    {
        "link": "https://dev.epicgames.com/documentation/en-us/unreal-engine/utility-material-expressions-in-unreal-engine",
        "document": "Utility Material Expressions are nodes that can affect Materials in a number of different ways than one might be used to. For example the GIReplace node will replace an object's indirect bounce color with a given value you input, while the Linear Interpolate node will help blend between two textures based on an Alpha input. On the following page you will find detailed descriptions for all of the Utility expressions are available in the Material Editor.\n\nThe AntialiasedTextureMask expression allows you to create a Material using a soft (anti-aliased) transition mask. The mask can be used to blend between two complex Material properties or to fade out an alpha blended Material (works well with SoftMasked). Specify a texture with the mask specified in one channel (red, green, blue, or alpha), set the used channel in the expression and specify the comparison value. Assuming the channel stores a grayscale value in the range 0 = black to 1 = white the comparison function defines if the resulting mask should be 0 or 1. This expression is a parameter, allowing the Texture property to be overridden by child MaterialInstances. Specifies the value used as the cutoff point in pixel coverage. Pixel coverage values less than this become black; values greater become white. Specifies the channel of the Texture to use as the mask. Specifies the mask texture to use. Takes in texture coordinates to apply to the texture mask. The actual implementation is quite a bit more complicated as it tries to return values between 0 and 1 depending on the actual pixel coverage to avoid aliasing. Example (this tiny 128x128 texture, uncompressed for best quality): Was used as a normal texture (left top) and used with the described material expression (bottom right): The technique works best in magnification and with blurred input content. Compression hurts the quality a lot so try to use uncompressed low resolution textures.\n\nThe BlackBody expression simulates the effects of black body radiation within your Material. The user inputs a temperature (in Kelvin) and the resulting color and intensity can be used to drive Base Color and Emissive values to get a physically accurate result.\n\nThe ConstantBiasScale expression takes an input value, adds a bias value to it, and then multiplies it by a scaling factor outputting the result. So for example, to convert input data from [-1,1] to [0,1] you would use a bias of 1.0 and a scale of 0.5.\n\nThe DDX expression exposes DDX derivative calculation, a GPU hardware feature used in pixel shader calculation.\n\nThe DDY expression exposes DDX derivative calculation, a GPU hardware feature used in pixel shader calculation.\n\nThe DepthFade expression is used to hide unsightly seams that take place when translucent objects intersect with opaque ones.\n\nThe Material network for this example is pictured below.\n\nThe Depth of Field Function expression is designed to give artists control over what happens to a Material when it is being blurred by Depth of Field. It outputs a value between 0-1 such that 0 represents \"in focus\" and 1 represents \"completely blurred.\" This is useful for interpolating between sharp and blurry versions of a texture, for instance. The Depth input allows for the existing results from the scene's Depth of Field calculations to be overridden by other calculations.\n\nThe DistanceFieldGradient Material Expression node, when normalized, outputs the X,Y,Z direction an object would move with in the distance field. This makes the Distance Field Gradient Material Expression node well-suited for Materials that need to simulate the flow of liquids.\n\nHere is an example of how to use the DistanceFieldGradient Material Expression in your Materials. In this example below make sure to note that the DistanceFieldGradient was first normalized and then input into a Mask Channel node. The reason for this is because without normalizing the DistanceFieldGradient first you can not get directional data. The Mask Channel Parameter was added to allow for easier RGB channel switching with in the Material Instance.\n\nHere is an example of the DistanceFieldGradient in action. The image below shows what data the DistanceFieldGradient will use when the various RGB are enabled.\n\nThe Distance To Nearest Surface Material Expression node allows Materials to sample any point in the levels Global Distance Field. This Material Expression works by outputting the signed distance in world space units from the distance field to the nearest occluders in the scene.\n\nHere is an example of the Distance To Nearest Surface Material Expression in action.\n\nIn this example the Distance To Nearest Surface was fed into the Opacity input on a Material and that Material was applied to a Static Mesh plane that was placed just above the levels floor. What the Distance To Nearest Surface is doing is telling the Material to only color areas red were the Static Meshes plane will start to intersect other Static Meshes placed in the scene.\n\nThe Fresnel expression calculates a falloff based on the dot product of the surface normal and the direction to the camera. When the surface normal points directly at the camera, a value of 0 is output. When the surface normal is perpendicular to the camera, a value of 1 is output. The result is clamped to [0,1] so you do not have any negative color in the center.\n\nThe LightmassReplace expression simply passes through the Realtime input when compiling the material for normal rendering purposes, and passes through the Lightmass input when exporting the material to Lightmass for global illumination. This is useful to work around material expressions that the exported version cannot handle correctly, for example WorldPosition.\n\nThe Noise expression creates a procedural noise field, giving you control over how it is generated.\n\nThe Previous Frame Switch Material Expression assists with the implementation of complex vertex animations in Materials by providing a way to generate correct motion vectors which work correctly with Temporal AA and Motion Blur.\n\nMaterials that are only a function of time already work without modification, however, they cannot account for other variables, such as Material Parameters, that can affect the animation at runtime. The Previous Frame Switch Material Expression provides a means to solve these problems manually by tracking how these parameters change. For example, in Blueprints they could manually provide expressions for motion vector generation that is caused by changes in World Position Offset between frames.\n\nHere is an example using Previous Frame Switch Material Expression in a Material.\n\nIn this example, the Previous Frame Switch is using a constant value to control the directional blur through a Multiply node.\n\nIn this example, you can see how this is being used in Epic's own games, like Fortnite, to control the motion blur with a Vertex Animation that assembles on screen. The animation on the right is using Previous Frame Switch to add some motion blur, while the animation on the left is not.\n\nThere is a show flag in the Editor viewport under Show > Visualize > Previous Frame's Reprojection that you can use with the Previous Frame Switch to diagnose and correct discrepancies in the directional vectors of the current and previous frame.\n\nWhen enabled, this visualizer compares the current frame color with the previous, and returns the difference between the two frames. When the difference is zero, the Material appears gray in the viewport (pictured left). When the directional vectors do not match, the Material displays a colored overlay (pictured right).\n\nThe QualitySwitch expression allows for the use of different expression networks based on the engine is switched between quality levels, such as using lower quality on lower-end devices.\n\nThe SphereMask expression outputs a mask value based on a distance calculation. If one input is the position of a point and the other input is the center of a sphere with some radius, the mask value is 0 outside and 1 inside with some transition area. This works on one, two, three, and four component vectors\n\nThe Thin Translucent Material Output expression accurately represents physically based transparent materials in a single pass. This enables you to create true tinted or colored transparent materials that accurately respond to lighting and shading.\n\nWhen creating a tinted glass material, a white specular highlight and tinted background are needed. These are rendered in a single pass with a physically based shader that accounts for light bounces from the air into the glass and the glass into the air.\n\nThe Vector Noise Material expression adds several more 3D or 4D vector noise results to use in your Materials. Due to the run-time expense of these functions, it is recommended that once a look is developed with them, all or part of the computation be baked into a Texture using the Render Targets feature. These Material graph Expressions allow procedural looks to be developed in the engine on final assets, providing an alternative to creating procedurally generated Textures with an external tool. Inside the Vector Noise Material Expression, you will find the following Vector Noise types. Returns a random color for each cell in a 3D grid (i.e. from the mathematical floor operation applied to the node input). The results are always consistent for a given position, so can provide a reliable way to add randomness to a Material. This Vector Noise function is extremely cheap to compute, so it is not necessary to bake it into a Texture for performance. Returns a random color for each cell in a 3D grid (i.e. from the mathematical floor operation applied to the node input). The results are always consistent for a given position, so can provide a reliable way to add randomness to a Material. This Vector Noise function is extremely cheap to compute, so it is not necessary to bake it into a Texture for performance. Computes the analytical 3D gradient of a scalar Perlin Simplex Noise. The output is four channels, where the first three (RGB) are the gradient, and the fourth (A) is the scalar noise. This noise type is useful for bumps on a surface or for flow maps. Computes the analytical 3D curl of a vector Perlin Simplex Noise (aka Curl Noise). The output is a 3D signed curl vector and is useful for fluid or particle flow. Computes the same Voronoi noise as the scalar Noise material node. The scalar Voronoi noise scatters seed points in 3D space and returns the distance to the closest one. The Vector Noise version returns the location of the closest seed point in RGB, and the distance to it in A. Especially coupled with Cellnoise, this can allow some randomized behavior per Voronoi cell. Below is a simple stone bed Material using the distance component of the Voronoi Vector Noise to modulate some surface bumps and blend moss into the cracks. The seed position together with Vector Noise > Cellnoise is used to change the color and bump height per rock. The derivative-based operations Perlin Curl and Perlin Gradient can be added together in octaves, just as regular Perlin noise can. For derivatives of more complex expressions, it is necessary to compute the gradient of the result of the expression. To help with this, place the expression to compute into a Material Function and use it with the following helper nodes. Uses positions offset in a tetrahedral pattern to compute 3D derivatives. Evaluate the same 3D function at each offset position produced by this function, then feed the resulting values into Compute3DDeriv. Uses positions offset in a tetrahedral pattern to compute 3D derivatives. Use with Prepare3DDeriv. Computes curl of a 3D vector field from result of Prepare3DDeriv/Compute3DDeriv. These helper Material Functions use four evaluations of the base expression spaced in a tetrahedral pattern to approximate these derivative-based operations. Below you will find descriptions of the various noise functions you will find in the Vector Noise Material Expression.\n• Cellnoise: Random color for each integer grid cell in 3D space. About 10 instructions.\n• Perlin 3D Noise: Computational Perlin noise with 3D output, each channel output ranges from -1 to 1. About 83 instructions if only the red channel is used, 125 instructions if all three channels are used.\n• Perlin Gradient: Computes the gradient of a Perlin noise function. RGB output contains the gradient vector, A is the scalar noise. About 106 instructions.\n• Perlin Curl: Computes a 3D curl noise. The output is the mathematical curl of Perlin 3D Noise. About 162 instructions.\n• Voronoi: The same algorithm and instruction counts as the Voronoi function in the Noise expression, but RGB is the location of the closest seed point in each Voronoi cell, and A is the distance to that seed point. A look/performance setting. Lower values are faster but may look worse, higher values are slower but may look better. For noise functions that support it this allows noise to tile. This is more expensive, but useful when baking noise into a seamless wrapping texture. When tiling, how often should the noise repeat. For Perlin noise variants, the Tile Size must be a multiple of three. Allows the texture size to be adjusted via a 3D vector."
    },
    {
        "link": "https://forums.unrealengine.com/t/how-to-build-material-masks-from-an-array-of-coordinates/460988",
        "document": "I have an array of coordinates in a blueprint. At each coordinate I want to create a mask. The amount of coordinates in the array is dynamically variable.\n\nI tried the CRT route, both dynamic and static, of creating a mask in a blueprint and plugging that into the material. And that worked to a degree. But what I’d ideally would like to do is have the masks created in the material itself.\n\nIs something like that possible without CRT? I really just want to pass the array of coordinates to the material and have the material put a mask at each coordinate."
    },
    {
        "link": "https://joyrok.com/BLOG",
        "document": "The grid container here is only as big as the largest thing inside, so it’s 0x0 pixels then the icon gets turned on the grid becomes 256x256 pixels in size, and even when turning on a smaller sized image it still stays at 256x256 because the layout depends on the largest thing inside. I also use grid containers more like smarter overlays I rarely use the row/column features\n\nThe vertical box container here is as big as the sum of the Y heights of all objects inside, so it’s 0x0 pixels then the icon gets turned on and it becomes 256x256 pixels in size, and then turning on a smaller sized image it still stays at 256x256 + 128x128 so now it’s a container that is 256 + 128 in the Y and 256 in the X. \n\n Container is now: 256x384\n\n\n\nTIP 3: Grid Panels are so useful - use them over Canvas\n\nTIP 5: Render Transforms when to use them"
    },
    {
        "link": "https://vfxdoc.readthedocs.io/en/latest/shaders/math",
        "document": "Here is a series of shortform articles that cover common shader cases for use in visual effects and general purpose shader authoring.\n\nThe Add operation used in color math is rather simple, it is simply an addition of the values per component, as does the Linear Lighting blend mode in Photoshop.\n\nThe multiply operation is also really simple, it applies some kind of masking to our color, by darkening the first color by the other. This affirmation is true when using LDR color (between 0.0 and 1.0).\n\nThe linear interpolation is basically the transition from one value to another using a grayscale mask that represents the proportion of B to be blended with A.\n• For each black (0.0) pixel mask. only the X image is output,\n• For each white (1.0) pixel mask. only the Y image is output.\n• When the mask is at 0.5 the output will be a perfect blend of X and Y, each at 50%\n• When the mask is at 0.25, the output will be 25% of Y and 75% of X\n\nFractional part removes then integer part of a number and leaves us only with its fraction of 1. It is roughly the same as a Modulo 1.\n\nOne common use case is to make perdiodic repetition over time, frac or modulo can be used to repeat time.\n\nThe power operation (A exponent n) is an operation that will multiply a value A, N times by itself. A exponent 3 will result in A x A x A.\n\nWhen used in color, it has an effect of altering the hardness of a linear gradient with the following properties:\n• An exponent between 0 and 1 will tend to brighten the darker values.\n• An exponent greater than 1 will tend to darken the darker values.\n• A value of 1 will NEVER change, with any exponent.\n• An exponent of 1 does not alter the gradient\n• Values above 1 will grow exponentially as N is greater than 1\n• Values above 1 will gros logarithmically as N is lower than 1\n• Negative values of the exponent result in root exponents (eg: square root for a exponent of -2)\n\nHere is now, how it applies to color using the same exponent coefficients: we can see that it affects the color in a non-uniform way. In our example, the darker the color will be, the more it will be affected by the exponent.\n\nIn the graph above, it is visible as the green curve will always have f(1) = 1 (the curves cross at (1,1)). However in our graph below, it only displays the [0..1] range.\n\nOne of the purposes of an exponent, applied to color, in a [0..1] range is to manipulate the attenuation of the darker tones.\n\nSine and cosine are operations used for a various range of purposes :\n• Computing coordinates based on an angle, such as rotator.\n• Performing Repeating interpolation back and forth from a value A to a value B, with dampening at source/target.\n\nSine and cosine are basically used to compute a position on an unit circle based on an angle on this circle.\n\nLinear Gradients are pretty straightforward to attain as they can be made using any linear space data, such as UV, or position.\n\nSquare mask is a combination of linear gradients multiplied together, that will perform linear attenuation to the sides of an UV space (or position space)\n\nSphere mask results in a 2D or 3D radial gradient that will correspond to a gradient based on a 2d or 3d distance to a point.\n\nTri-projection is a way of projecting textures on a mesh that either does not possess UVs or on a series of contiguous meshes. Its principle is pretty straightforward : Textures are projected from the 3 axes (X,Y,Z) in local or world space. Then blended accordingly to the mesh normals.\n• A projection from the XY plane will be projected along the Z axis\n• A projection from the YZ plane will be projected along the X axis\n• A projection from the ZX plane will be projected along the Y axis\n\nBlending per-axis is decided by the absolute value of the dot product between the mesh normal and the projection axis.\n\nGradient Sampling is a method to evaluate the direction of a gradient. It involves evaluating the gradient on each dimension axis at least two times to evaluate the direction. It is often used to compute a normal from a bump map.\n\nThe Gradient sampling involves computing the derivative of the gradient for each axis. In 1D, this corresponds to the following:\n\nHowever, if we take a look at the red vector, it corresponds roughly to the curve tangent at the midpoint between the two sampling points.\n\nHowever this is an approximation as the sampling distance discretizes the curve and can lead to approximation errors such as this diagram:\n\nIn this example, the two sample points are too far to catch the actual slope of the curve. That's precisely the reason why sample points should always be as close as possible.\n\nWhen dealing with 2D or 3D gradients, the computation of every point is done per-axis and the tangent value reassembled into a 2D or 3D value. However if we take a look at all values that need to be sampled, we need to compute 4 or 6 Points (2 per dimension) then sample 4 or 6 times to fetch the actual tangent.\n\nHere is an implementation of a gradient sampling to generate a normal map from a height (bump) map. This example uses a user-driven sampling distance and an arbitrary height scale interpretation to adjust the \"bumpiness\" of the output normal.\n\nIn Geometry, points are expressed as 2D or 3D space vectors. Every vector or position is expressed as a tuple of 2 or 3 values (depending if expressed on 2D or 3D). For instance (0,1,3) for a point that would be at X=0, Y=1, Z=3.\n\nA vector value can express either a position in space or a displacement in this space (For instance how to go to a point A to a point B). Regardless of the meaning, all three values stay a vector, and a position can even be considered as a vector from the origin (0,0,0) to the point position.\n\nVectors in shaders are expressed in a reference space, for instance a local position for a 3d mesh. or a position in UV (tangent) space. Really often, we will need to transform vectors from a space to another, for instance, transform a local mesh position to the world position.\n\nTo transform vectors from one space to another, we use a transformation matrix: an array of values that is multiplied by the vector and will output the transformed vector.\n\nGame engines will often provide you these matrices so you can apply transforms yourself. For instance, to transform the local 3d position from local to world we can use the following:\n\nin HLSL we can use the operation to perform the transform.\n\nIn shader graphs, there are nodes you can use to perform transformations on positions or vectors from one space to another, they will gather automatically the needed matrices and perform the multiplication for you\n\nIn the previous example you probably noticed that we performed the operation on a 3d vector, by setting it inside a 4d vector ( ). This is required to be able to perform the multiplication with the 4x4 float matrix. In 3d transformations we can transform either positions or vectors by transformation matrices. Each transformation matrix contains values to translate, rotate and scale the vector. However, in some cases, we do not want to apply the translation to the vector. For example : mesh normals. We only need to rotate them. Scale will be also applied but we will re-normalize them (see Normalization for more info). To control whether we apply translation, we use an additional, 4th component to the vector. If this component has a value of 1.0, the translation will be applied, if it is zero, only the rotation/scale will be applied. This 4D Space is called Homogeneous Coordinates as it is used to be transformed either as a position, or a vector.\n\nSometimes you need to find the vector that binds a point to another, for instance if you want to move a point to a destination, so it fits this equation :\n\nBy reversing the source component, the computation of the delta vector is pretty easy :\n\nIn this schenatic if we follow from the origin to the destination, then move along the -Source vector (going down),\n\nNormalization is the process that ensures that a vector keeps a length of 1 unit. This is highly critical, for example when performing computations of normals, that their length is 1 at all times.\n\nNormalization will take any non-zero vector and will compute a vector of the same direction, but with a length of 1. In the following figure, all three vectors of different lengths (colored dashed ones) are being normalized and become of length one so they are inscribed into a norm circle.\n\nDot Product (often also known as Scalar Product) is a math operation that takes two vectors and will return a value depending on various factors:\n\nOne of the properties of a Dot Product is that it will respond with the cosine of the angle described the two vectors. If the normalized vectors:\n• ... have exactly the same direction (angle is 0 radians), it will respond with a value of 1\n• ... are strictly perpendicular (angle is pi/2 radians), it will respond with a value of 0\n• ... are exactly in opposite directions (angle is pi radians), it will respond with a value of -1\n\nAnother property of the dot product operation is that it can compute the squared length of a vector. When used on the same vector, it shall return the squared length of this vector.\n\nThis can be pretty useful to generate radial gradients by using the following :\n\nAnother property of a dot product is that it can project a point over a line by using a Thales's proportion computation.\n\nThis uses both the property of co-linearity and distance between two vectors. In the example below we want to project a point C on a line segment AB.\n\nTo compute the position, we use :\n• A normalized vector with the same direction of AB : AA'\n\nBy doing a we are able to compute the length ratio that can be reduced to length because AA' is normalized and is of length 1. To compute the position of C' we just have to do the following\n\nwhich leads us to the following code:\n\nalso, this projection can be reduced to this expression:\n\nDepth bias fading is a method that blends the transparent geometry as it gets close to opaque objects behind them. This is often used to avoid hard cuts between large particle sprites and the level's geometry.\n\nIt involves comparing the depth between the currently drawn pixel (the one for the particle sprite) and the depth of the first opaque pixel behind it. Then we apply a ratio comparison to determine if the pixel will not be faded (if the difference of depth is greater of equal to the wanted fade distance), or faded proportionally as the difference gets closer to zero.\n\nThe implementation in most material graph editors is pretty straightforward. Unreal Engine 3 has the node while its name has changed a bit in Unreal Engine 4 to (which is IMO more vague)\n\nIn unity, the legacy renderer implements the feature as a global setting. But in recent render pipelines such as Lightweight Render pipeline or HD Render Pipeline, Depth bias fading can be achieved using the following nodes:"
    },
    {
        "link": "https://forums.unrealengine.com/t/writing-custom-hlsl-shaders/424362",
        "document": "How to Add Global Shaders to UE4\n\nEpic Games Senior Engine Programmer Rolando Caloca Olivares walks through the simple process of creating your own custom global shaders, enabling you to have greater control over the look and feel of your project."
    },
    {
        "link": "https://dev.epicgames.com/documentation/en-us/unreal-engine/shader-development-in-unreal-engine",
        "document": "When working on shaders, be sure to enable r.ShaderDevelopmentMode by setting it to 1. The easiest way is to edit ConsoleVariables.ini so it is done every time you load. This will enable retry-on-error and shader development related logs and warnings.\n\nUse Ctrl+Shift+., which executes the recompileshaders changed command. Executing this command should be performed after you have saved your changes to the Unreal Shader (.usf) file.\n\nGlobal shaders are shaders which operate on fixed geometry (like a full screen quad) and do not need to interface with materials. Examples would be shadow filtering, or post processing. Only one shader of any given global shader type exists in memory.\n\nMaterials are defined by a set of states that control how the material is rendered (blend mode, two sided, etc) and a set of material inputs that control how the material interacts with the various rendering passes (BaseColor, Roughness, Normal, etc).\n\nMaterials have to support being applied to different mesh types, and this is accomplished with vertex factories. A represents a unique mesh type, and a instance stores the per-instance data to support that unique mesh type. For example, stores the bone matrices needed for skinning, as well as references to the various vertex buffers that the GPU skin vertex factory shader code needs as input. The vertex factory shader code is an implicit interface which is used by the various pass shaders to abstract mesh type differences. Vertex factories consist of mainly vertex shader code, but some pixel shader code as well. Some important components of the vertex factory shader code are:\n\nShaders using are pass specific shaders which need access to some of the material's attributes, and therefore must be compiled for each material, but do not need to access any mesh attributes. The light function pass shaders are an example of 's.\n\nShaders using are pass specific shaders which depend on the material's attributes AND the mesh type, and therefore must be compiled for each material/vertex factory combination. For example, / need to evaluate all of the material inputs in a forward rendering pass.\n\nA material's set of required shaders is contained in a . It looks like this:\n\nVertex factories are included in this matrix based on their ShouldCache function, which depends on the material's usage. For example, bUsedWithSkeletalMesh being will include the GPU skin vertex factories. 's are included in this matrix based on their ShouldCache function, which depends on material and vertex factory attributes. This is a sparse matrix approach to caching shaders and it adds up to a large number of shaders pretty quick which takes up memory and increases compile times. The major advantage over storing a list of actually needed shaders is that no list has to be generated, so needed shaders have always already been compiled before run time on consoles. Unreal Engine mitigates the shader memory problem with shader compression, and the compile time problem with multicore shader compilation.\n\nA material shader type is created with the DECLARE_SHADER_TYPE macro:\n\nThis declares the necessary metadata and functions for a material shader type. The material shader type is instantiated with IMPLEMENT_MATERIAL_SHADER_TYPE:\n\nThis generates the material shader type's global metadata, which allows us to do things like iterate through all shaders using a given shader type at runtime.\n\nA typical material pixel shader type will first create a struct by calling the GetMaterialPixelParameters vertex factory function. GetMaterialPixelParameters transforms the vertex factory specific inputs into properties like WorldPosition, TangentNormal, etc that any pass might want to access. Then a material shader will call CalcMaterialParameters, which writes out the rest of the members of , after which is fully initialized. The material shader will then access some of the material's inputs through functions in MaterialTemplate.usf (GetMaterialEmissive for the material's emissive input for example), do some shading and output a final color for that pass.\n\nUMaterial has a setting called bUsedAsSpecialEngineMaterial that allows the material to be used with any vertex factory type. This means all vertex factories are compiled with the material, which will be a very large set. bUsedAsSpecialEngineMaterial is useful for:\n• Materials used with rendering viewmodes like lighting only.\n• Materials used as fallbacks when there is a compilation error (DefaultDecalMaterial, DefaultMaterial, etc).\n• Materials whose shaders are used when rendering other materials in order to cut down on the number of shaders that have to be cached. For example, an opaque material's depth-only shaders will produce the same depth output as the DefaultMaterial, so the DefaultMaterial's shaders are used instead and the opaque material skips caching the depth-only shader.\n\nUnreal Engine compiles shaders asynchronously using a streaming system. Compile requests are enqueued when materials load that do not have a cached shader map, and compile results are applied as they become available, without blocking the engine. This is optimal in terms of load time and compile throughout, but it does mean that there are quite a few layers between the actual platform shader compile and the material that requested it.\n\nThe actual compiling work is done in helper processes called the Shader Compile Workers, because the platform shader compile functions (D3DCompile) often contain critical sections within them that prevent multi-core scaling within a single process.\n\nThere are some settings to control how compilation is done which can simplify debugging of the shader compilers. These can be found in the [DevOptions.Shaders] section of BaseEngine.ini.\n\nIf you want to step into the shader compiler DLL's directly from Unreal Engine (CompileD3D11Shader for example), you should set both of these to false. Compilation will take a long time though, so make sure all other shaders have been cached.\n\nWith r.ShaderDevelopmentMode enabled, you will get the opportunity to retry on shader compile error. This is especially important for global shaders since it is a fatal error if they do not compile successfully.\n\nIn debug, with the debugger attached, you will hit a breakpoint and get the compile error in the Visual Studio output window. You can then Double-click the error log to be taken directly to the offending line.\n\nOtherwise you will get a Yes/No dialog\n\nOnce shaders are compiled, they are stored in the Derived Data Cache. They contain, in their key, a hash of all the inputs to the compile, including shader source files. That means that changes to shader source files are automatically picked up every time you re-launch the engine or do a 'recompileshaders changed'.\n\nWhen you are modifying FShader Serialize functions, there is no need to handle backward compatibility, just add a space to a shader file that is included by that shader.\n\nWhen cooking assets, material shaders are inlined into the material's package, and global shaders are stored seperately in a global shader file which allows them to be loaded early in the engine startup.\n\nThe primary method of debugging shaders is to modify a shader to output an intermediate, then visualize that with the appropriate VisualizeTexture command. This can allow fast iteration since you can compile on the fly without restarting the engine. For example, you can verify that WorldPosition is correct with something like:\n\nThen verify that the scale is right, and the result is not view dependent. However this method does not scale very well to more complex shaders that build data structures.\n\nYou can also use r.DumpShaderDebugInfo=1 to get files saved out to disk for all the shaders that get compiled. It can be useful to set this in ConsoleVariables.ini just like r.ShaderDevelopmentMode. Files are saved to GameName/Saved/ShaderDebugInfo, including\n• A batch file to compile the preprocessed version with equivalent commandline options to the compiler that were used\n\nIf you are working on a global shader, recompileshaders changed or Ctrl+Shift+. is the fastest way to iterate. If the shader takes a long time to compile, you might consider specifying CFLAG_StandardOptimization as a compile flag in the shader's ModifyCompilationEnvironment.\n\nIf you are working on a material shader, like BasePassPixelShader.usf, it is much faster to iterate on a single material. Every time you click the Apply button in the material editor, it will re-read shader files from disk and recompile just that material.\n\nThe HLSL Cross Compiler is used to automatically convert HLSL into GLSL for OpenGL platforms, allowing shaders to be authored only once for all platforms. It is run during offline shader compilation and performs various optimizations to the code that OpenGL drivers are frequently missing.\n\nAsyncCompute is a hardware feature that is available on some APIs that work with certain GPUs. It allows interleaving to better utilize the hardware units in the GPU more efficiently."
    },
    {
        "link": "https://forums.unrealengine.com/t/how-to-create-custom-shader/6288",
        "document": "Hello, I’m a beginner graphics programmer and I’m interesting in all stuff related to rendering. For me it seems that graphical part of UE4 is the half of the engine, but I can’t find any information about it. Yes, I’ve read this 2 pages in documentation about graphics development but: 1. It seems outdated. 2. Well, I can’t understand anything - it’s so brief and dry.\n\n In learning purposes I want to create a shader (a set of shaders) that uses all pipeline stages. Where should I start? Can you provide a step by step guide what classes I need, how to implement this in engine (because I bet I need to modify source code).\n\nI am planning on doing just this, writing some documentation on how to implement your own custom shaders using Vertex Factories, Global Shaders and Drawing Policies. I will be writing the documentation with some examples in mind, so by the end you should have some custom shaders to play with that actually do something. I will be writing the documentation over the weekend.\n\nYes I do still plan on getting that documentation out. It has been pretty low on my list of priorities, things with my project have been pretty hectic over the last month or so. I’ve also learnt a lot more about the rendering side of UE4, so I have more things to cover. But if there are any particular questions you want answered, or urgent help, you can usually find me on irc at #unrealengine\n\nBump been coding shaders for years and as I’m going through the UE4 “materials” video tutorials I’m facepalming so hard, there’s gotta be a way to just code those up instead. A full screen of 6-8 fat wire-connected nodes for what could be 2-3 lines of shader code? This would drive me mad in practice… I’m happy all the artsy code-hating types have a clicky tool but sheesh let me just code shaders close to the metal… , did you ever get around to writing down some notes of your experiences on this?\n\nHas there been any word on the documentation? I read your thread on Metaballs by the way and there you said “Feel free to comment in https://forums.unrealengine.com/show...r&goto=newpost if there is anything specific you would like me to cover, or any certain effects you want to see.” So I hope this isn’t bothering anyone but I have some requests for the shader guide (and I generally would really like to see something like this): glitch-looking effects, or an old vhs-style, kinda like Silent Hill 3 had. Also the deep, dark, grainy black and white look of Tetsuo:\n\n Of course, I don’t expect you to show all of those (or any of those), just saying what I’m interested in seeing.\n\nHas there been any word on the documentation? I read your thread on Metaballs by the way and there you said “Feel free to comment in https://forums.unrealengine.com/show...r&goto=newpost if there is anything specific you would like me to cover, or any certain effects you want to see.” So I hope this isn’t bothering anyone but I have some requests for the shader guide (and I generally would really like to see something like this): glitch-looking effects, or an old vhs-style, kinda like Silent Hill 3 had. Also the deep, dark, grainy black and white look of Tetsuo: Of course, I don’t expect you to show all of those (or any of those), just saying what I’m interested in seeing. You might probably want to look at this [Tutorial] Pixel and Compute Shaders in UE4 - C++ - Epic Developer Community Forums Temaran write a good documentation and for anyone interested to custom shader authoring\n\nI use the Unreal Editor that came with Star Wars: Republic Commando. If I may, could someone please tell me how to export a shader and edit it to the point at which it would best befit these two images?\n\n 501stCommander.bmp and 501stCommanderHelmet.bmp are the names of the images. I cannot seem to post them here, so if at all possible. you can send me a private message requesting my email contact information."
    },
    {
        "link": "https://medium.com/@solaslin/learning-unreal-engine-4-implement-cel-shading-w-outline-using-custom-shading-model-in-ue4-22-1-775bccdb9ffb",
        "document": "Adding a new and customized shading model in UE4 is not something fresh or new. Lots of tutorials can be found, I also learned from many of them, mainly from Matt Hoffman’s Medium: https://medium.com/@lordned\n\nBut when I started to follow those tutorials to work on my own version, I found that lots of shader codes are changed in UE4.22, for example, the class “DrawingPolicy” is totally removed. Maybe the entire rendering flow in shaders is not changed too drastically (still base-pass, and then deferred-pass), but some function are removed, separated, or migrated to different files. So you may still need to spend some time to figure them out. So I decide to share this post which is also a note of my implementation in UE4.22.\n\nIn addition, I also did some extra effort in my version, like cel-shading for reflection, and which I think is more interesting: doing the outline effect in deferred pass instead of post-processing. When I read those great articles, I found the outline effect is usually done in a final post-processing. So I was kind of curious if I can do the outline effect also in this custom shading model? The answer is yes, and I’ll share my method in this series.\n\nIn fact, there are many different ways that can achieve cel-shading and outline, such as post-processing, a material graph which might be faster to implement. Shading model is also an alternative one though it may spend more time tracing the UE source code, but I think it also has different advantages that others can’t do. For example, once you finish the shader code of cel-shading model in the deferred lighting pass, it works for point lights and spot lights without extra effort, which is not easily to do with material graph. And also, you can set different parameters for each of the cel-shading material, give them different cel-levels, outline thickness…etc, and a post-processing is difficult to do this.\n\nMaterial with Your Own Shading Model\n\nFirst, let’s see how it looks like for a custom shading model:\n\nAfter adding a shading model, you can select the new shading model in your material editor in the drop-down list. A shading model means a different shading method, for example, PBR is a shading model, Phong shading is another shading model, they use different shading fundamental (or you can say equation/formula). In this article, I’ll also implement the same one as other tutorials do — cel-shading, since it is a relatively simple to do.\n\nWhen selecting our own custom shading model, we can also define what are the necessary data needed for this shading model? This depends on how you implement your shading. Maybe you need the user to give the input for some threshold, or how many cel bands, or the min/max intensity values to adjust the image contrast. In my practice, I use three values to control my cel-shading and you can see them in the following image.\n\nThis is from the original pin “CustomData 0”. It defines how many “cels” for your cel-shading. For example, if bands = 3, the lighting will be “categorized” into 3 levels. When you have more bands, the shading looks more smooth.\n\nThis is from the original pin “CustomData 1”. It defines the thickness of you edge for the outline effect. Outline effect is not a necessity for a cel-shading. I just combine them together for fun.\n\nThis is from the original pin “Refraction”. I borrow this pin just because in my practice I won’t support a transparent material with cel-shading so that’s safe to use this pin although it will take you some extra time to modify a little more code of shader compiling pipeline.\n\nSensitivity is a threshold value checking if a pixel is on edge. There are many algorithms doing edge detection, I just use a very simple one: calculate the difference of the normal vector and depth value and see if they differ large enough.\n\nTo create a new shading model, you will need to modify both cpp codes and shader codes. Let’s see the cpp part first. If you have read Matt Hoffman’s articles, I believe this part would be very easy for you.\n\nIn EngineTypes.h, add a new UENUM for the new shading model. For example, MSM_CelShading.\n\nAdding an enum only makes it visible in editor menu. We also need to define the behavior of this shading model. First, we setup the environment for compiling this shader. This is done by adding a “SetDefine([_KEYWORD_])” for this shading model. The SetDefine function means to turn on a preprocessor in your shader when compiling. So the following code will make the shader compiler see the “ ” as 1\n\nNext, we need to tell the material editor which “pins” can be used for this shading model? (Pins = material attributes that show in your last material node). Please find the function IsPropertyAction_Internal in Material.cpp, there is a switch block testing if the input property (pin) can be active or not.\n\nLet’s take a look at CustomData0 and 1 first, which are “Cel-Bands” and “Outline Thickness” in our new shading model. To make the two pins active, we just add code at the end to test if the ShadingModel is Cel-Shading as well.\n\nJust to mention some key point here about CustomData, you can leave the name of the CustomData Pin unchanged (so it remains “CustomData0/1”). However, if you want to change the name in editor, you can change it in GetCustomDataPinName() in MaterialGraph.cpp as follow.\n\nAnother important thing is that: the range of CustomData 0/1 is between [0.0, 1.0]. Even if you connect a constant or scalar parameter with value > 1.0 to this pin, it will be clamped to 1.0. And that why you see I actually connect the value 1/Bands to the Cel-Bands pin instead of the actual number in my material graph. (And I have to inverse the value in my shader code)\n\nTo see why, you can take a look at AllocGBufferTargets() function. UE allocates the GBufferD storing CustomData as B8G8R8A8 format. And CustomData0,1 are used to fill-in the R/G channels as 8-bit. So the value will eventually be clamped to 1.0 as its max value. The code snippet below can be found in SceneRenderTarget.cpp.\n\nYou may wonder why I didn’t mention the “Sensitivity” pin? Since this pin is borrowed from “Refraction”, so it is slightly different from CustomData pin. (But still very similar).\n\nFirst, we do the same thing to make this pin active for Cel-Shading model. As shown below, like we just did for CustomData.\n\nBut this is not enough! When I did this and connect some value to the refraction pin, I found it didn’t work. The refraction value in shader is always 0. Why? Because when translating the material graph to shader, UE will check if there are something redundant or not supported, and then the “unsupported” part will be discarded, or be replaced with the default value.\n\nIn the Translate() function in HLSLMaterialTranslator.h. The input of refraction pin will be compiled only when the BlendMode is Translucent. So even we make this pin “active” (connectable in editor), the input value is still ignored since the cel-shading is opaque, so it remains 0 which is the default value.\n\nAfter knowing the reason, it simple to make it work. Just make the refraction pin compiled as well when the MaterialShadingModel is Cel-Shading. It can be done by adding a “||” condition as below.\n\nAnd there are some extra things you can do (but won’t make anything wrong if you don’t do them). You can find the function RebuildGraph() in MaterialGraph.cpp to change the display name of refraction pin by implementing a GetRefractionPinName() function.\n\nOK, now we’ve finally finished all modifications we need for cpp/header files.\n\nThe following steps are almost identical with Matt Hoffman’s articles (again). All the difference mainly come from the code migration in UE4.22. If I don’t make something clear enough, you can go back to read his articles :)\n\nIn files BasePassCommon.ush, at the end of this line (which is cut in the following image), add where “MATERIAL_SHADINGMODEL_CEL_SHADING” is what we set in in cpp file. Without doing this, the GBuffer will not output the value of our input CustomData."
    },
    {
        "link": "https://forums.unrealengine.com/t/make-the-engine-more-accommodating-for-custom-shading-models/544373",
        "document": "Documentation for Strata Materials, released as Experimental in UE 5.1."
    }
]