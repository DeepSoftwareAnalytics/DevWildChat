[
    {
        "link": "https://medium.com/towards-agi/how-to-limit-output-tokens-in-langchain-efficiently-c0df0bb8a85f",
        "document": "Let‚Äôs talk about something that we all face during development: API Testing with Postman for your Development Team.\n\nYeah, I‚Äôve heard of it as well, Postman is getting worse year by year, but, you are working as a team and you need some collaboration tools for your development process, right? So you paid Postman Enterprise for‚Ä¶. $49/month.\n\nNow I am telling you: You Don‚Äôt Have to:\n\nThat‚Äôs right, APIDog gives you all the features that comes with Postman paid version, at a fraction of the cost. Migration has been so easily that you only need to click a few buttons, and APIDog will do everything for you.\n\nAPIDog has a comprehensive, easy to use GUI that makes you spend no time to get started working (If you have migrated from Postman). It‚Äôs elegant, collaborate, easy to use, with Dark Mode too!\n\nWant a Good Alternative to Postman? APIDog is definitely worth a shot. But if you are the Tech Lead of a Dev Team that really want to dump Postman for something Better, and Cheaper, Check out APIDog!"
    },
    {
        "link": "https://huggingface.co/docs/transformers/en/generation_strategies",
        "document": "Text generation is essential to many NLP tasks, such as open-ended text generation, summarization, translation, and more. It also plays a role in a variety of mixed-modality applications that have text as an output like speech-to-text and vision-to-text. Some of the models that can generate text include GPT2, XLNet, OpenAI GPT, CTRL, TransformerXL, XLM, Bart, T5, GIT, Whisper.\n\nCheck out a few examples that use generate() method to produce text outputs for different tasks:\n\nNote that the inputs to the generate method depend on the model‚Äôs modality. They are returned by the model‚Äôs preprocessor class, such as AutoTokenizer or AutoProcessor. If a model‚Äôs preprocessor creates more than one kind of input, pass all the inputs to generate(). You can learn more about the individual model‚Äôs preprocessor in the corresponding model‚Äôs documentation.\n\nThe process of selecting output tokens to generate text is known as decoding, and you can customize the decoding strategy that the method will use. Modifying a decoding strategy does not change the values of any trainable parameters. However, it can have a noticeable impact on the quality of the generated output. It can help reduce repetition in the text and make it more coherent.\n‚Ä¢ saving and sharing custom generation configurations with your fine-tuned model on ü§ó Hub\n\nA decoding strategy for a model is defined in its generation configuration. When using pre-trained models for inference within a pipeline(), the models call the method that applies a default generation configuration under the hood. The default configuration is also used when no custom configuration has been saved with the model.\n\nWhen you load a model explicitly, you can inspect the generation configuration that comes with it through :\n\nPrinting out the reveals only the values that are different from the default generation configuration, and does not list any of the default values.\n\nThe default generation configuration limits the size of the output combined with the input prompt to a maximum of 20 tokens to avoid running into resource limitations. The default decoding strategy is greedy search, which is the simplest decoding strategy that picks a token with the highest probability as the next token. For many tasks and small output sizes this works well. However, when used to generate longer outputs, greedy search can start producing highly repetitive results.\n\nYou can override any by passing the parameters and their values directly to the method:\n\nEven if the default decoding strategy mostly works for your task, you can still tweak a few things. Some of the commonly adjusted parameters include:\n‚Ä¢ : the maximum number of tokens to generate. In other words, the size of the output sequence, not including the tokens in the prompt. As an alternative to using the output‚Äôs length as a stopping criteria, you can choose to stop generation whenever the full generation exceeds some amount of time. To learn more, check StoppingCriteria.\n‚Ä¢ : by specifying a number of beams higher than 1, you are effectively switching from greedy search to beam search. This strategy evaluates several hypotheses at each time step and eventually chooses the hypothesis that has the overall highest probability for the entire sequence. This has the advantage of identifying high-probability sequences that start with a lower probability initial tokens and would‚Äôve been ignored by the greedy search. Visualize how it works here.\n‚Ä¢ : if set to , this parameter enables decoding strategies such as multinomial sampling, beam-search multinomial sampling, Top-K sampling and Top-p sampling. All these strategies select the next token from the probability distribution over the entire vocabulary with various strategy-specific adjustments.\n‚Ä¢ : the number of sequence candidates to return for each input. This option is only available for the decoding strategies that support multiple sequence candidates, e.g. variations of beam search and sampling. Decoding strategies like greedy search and contrastive search return a single output sequence.\n\nIt is also possible to extend with external libraries or handcrafted code. The argument allows you to pass custom LogitsProcessor instances, allowing you to manipulate the next token probability distributions. Likewise, the argument lets you set custom StoppingCriteria to stop text generation. The library contains examples of external -compatible extensions.\n\nIf you would like to share your fine-tuned model with a specific generation configuration, you can:\n‚Ä¢ Save your generation configuration with GenerationConfig.save_pretrained(), making sure to leave its argument empty\n‚Ä¢ Set to to upload your config to the model‚Äôs repo\n\nYou can also store several generation configurations in a single directory, making use of the argument in GenerationConfig.save_pretrained(). You can later instantiate them with GenerationConfig.from_pretrained(). This is useful if you want to store several generation configurations for a single model (e.g. one for creative text generation with sampling, and one for summarization with beam search). You must have the right Hub permissions to add configuration files to a model.\n\nThe supports streaming, through its input. The input is compatible with any instance from a class that has the following methods: and . Internally, is used to push new tokens and is used to flag the end of text generation.\n\nIn practice, you can craft your own streaming class for all sorts of purposes! We also have basic streaming classes ready for you to use. For example, you can use the TextStreamer class to stream the output of into your screen, one word at a time:\n\nThe supports watermarking the generated text by randomly marking a portion of tokens as ‚Äúgreen‚Äù. When generating the ‚Äúgreen‚Äù will have a small ‚Äòbias‚Äô value added to their logits, thus having a higher chance to be generated. The watermarked text can be detected by calculating the proportion of ‚Äúgreen‚Äù tokens in the text and estimating how likely it is statistically to obtain that amount of ‚Äúgreen‚Äù tokens for human-generated text. This watermarking strategy was proposed in the paper ‚ÄúOn the Reliability of Watermarks for Large Language Models‚Äù. For more information on the inner functioning of watermarking, it is recommended to refer to the paper.\n\nThe watermarking can be used with any generative model in and does not require an extra classification model to detect watermarked text. To trigger watermarking, pass in a WatermarkingConfig with needed arguments directly to the method or add it to the GenerationConfig. Watermarked text can be later detected with a WatermarkDetector.\n\nLet‚Äôs generate some text with watermarking. In the below code snippet, we set the bias to 2.5 which is a value that will be added to ‚Äúgreen‚Äù tokens‚Äô logits. After generating watermarked text, we can pass it directly to the to check if the text is machine-generated (outputs for machine-generated and otherwise).\n\nCertain combinations of the parameters, and ultimately , can be used to enable specific decoding strategies. If you are new to this concept, we recommend reading this blog post that illustrates how common decoding strategies work.\n\nHere, we‚Äôll show some of the parameters that control the decoding strategies and illustrate how you can use them.\n\nuses greedy search decoding by default so you don‚Äôt have to pass any parameters to enable it. This means the parameters is set to 1 and .\n\nThe contrastive search decoding strategy was proposed in the 2022 paper A Contrastive Framework for Neural Text Generation. It demonstrates superior results for generating non-repetitive yet coherent long outputs. To learn how contrastive search works, check out this blog post. The two main parameters that enable and control the behavior of contrastive search are and :\n\nAs opposed to greedy search that always chooses a token with the highest probability as the next token, multinomial sampling (also called ancestral sampling) randomly selects the next token based on the probability distribution over the entire vocabulary given by the model. Every token with a non-zero probability has a chance of being selected, thus reducing the risk of repetition.\n\nUnlike greedy search, beam-search decoding keeps several hypotheses at each time step and eventually chooses the hypothesis that has the overall highest probability for the entire sequence. This has the advantage of identifying high-probability sequences that start with lower probability initial tokens and would‚Äôve been ignored by the greedy search.\n\nYou can visualize how beam-search decoding works in this interactive demo: type your input sentence, and play with the parameters to see how the decoding beams change.\n\nTo enable this decoding strategy, specify the (aka number of hypotheses to keep track of) that is greater than 1.\n\nAs the name implies, this decoding strategy combines beam search with multinomial sampling. You need to specify the greater than 1, and set to use this decoding strategy.\n\nThe diverse beam search decoding strategy is an extension of the beam search strategy that allows for generating a more diverse set of beam sequences to choose from. To learn how it works, refer to Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models. This approach has three main parameters: , , and . The diversity penalty ensures the outputs are distinct across groups, and beam search is used within each group.\n\ntransformers AutoTokenizer, AutoModelForSeq2SeqLM checkpoint = prompt = ( \"The Permaculture Design Principles are a set of universal design principles \" \"that can be applied to any location, climate and culture, and they allow us to design \" \"the most efficient and sustainable human habitation and food production systems. \" \"Permaculture is a design system that encompasses a wide variety of disciplines, such \" \"as ecology, landscape design, environmental science and energy conservation, and the \" \"Permaculture design principles are drawn from these various disciplines. Each individual \" \"scientific principles. When we bring all these separate principles together, we can \" \"create a design system that both looks at whole systems, the parts that these systems \" \"consist of, and how those parts interact with each other to create a complex, dynamic, \" \"living system. Each design principle serves as a tool that allows us to integrate all \" \"the separate parts of a design, referred to as elements, into a functional, synergistic, \" \"whole system, where the elements harmoniously interact and work together in the most \" ) tokenizer = AutoTokenizer.from_pretrained(checkpoint) inputs = tokenizer(prompt, return_tensors= ) model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint) outputs = model.generate(**inputs, num_beams= , num_beam_groups= , max_new_tokens= , diversity_penalty= ) tokenizer.decode(outputs[ ], skip_special_tokens= ) 'The Design Principles are a set of universal design principles that can be applied to any location, climate and culture, and they allow us to design the'\n\nThis guide illustrates the main parameters that enable various decoding strategies. More advanced parameters exist for the method, which gives you even further control over the method‚Äôs behavior. For the complete list of the available parameters, refer to the API documentation.\n\nSpeculative decoding (also known as assisted decoding) is a modification of the decoding strategies above, that uses an assistant model (ideally a much smaller one), to generate a few candidate tokens. The main model then validates the candidate tokens in a single forward pass, which speeds up the decoding process. If , then the token validation with resampling introduced in the speculative decoding paper is used. Assisted decoding assumes the main and assistant models have the same tokenizer, otherwise, see Universal Assisted Decoding below.\n\nCurrently, only greedy search and sampling are supported with assisted decoding, and assisted decoding doesn‚Äôt support batched inputs. To learn more about assisted decoding, check this blog post.\n\nTo enable assisted decoding, set the argument with a model.\n\nWhen using assisted decoding with sampling methods, you can use the argument to control the randomness, just like in multinomial sampling. However, in assisted decoding, reducing the temperature may help improve the latency.\n\nWe recommend to install library to enhance the candidate generation strategy and achieve additional speedup.\n\nUniversal Assisted Decoding (UAD) adds support for main and assistant models with different tokenizers. To use it, simply pass the tokenizers using the and arguments (see below). Internally, the main model input tokens are re-encoded into assistant model tokens, then candidate tokens are generated in the assistant encoding, which are in turn re-encoded into main model candidate tokens. Validation then proceeds as explained above. The re-encoding steps involve decoding token ids into text and then encoding the text using a different tokenizer. Since re-encoding the tokens may result in tokenization discrepancies, UAD finds the longest common subsequence between the source and target encodings, to ensure the new tokens include the correct prompt suffix.\n\nAlternatively, you can also set the to trigger n-gram based assisted decoding, as opposed to model based assisted decoding. You can read more about it here.\n\nAn LLM can be trained to also use its language modeling head with earlier hidden states as input, effectively skipping layers to yield a lower-quality output ‚Äî a technique called early exiting. We use the lower-quality early exit output as an assistant output, and apply self-speculation to fix the output using the remaining layers. The final generation of that self-speculative solution is the same (or has the same distribution) as the original model‚Äôs generation. If the model you‚Äôre using was trained to do early exit, you can pass (integer). In this case, the assistant model will be the same model but exiting early, hence the ‚Äúself-speculative‚Äù name. Because the assistant model is a portion of the target model, caches and weights can be shared, which results in lower memory requirements. As in other assisted generation methods, the final generated result has the same quality as if no assistant had been used.\n\nDecoding by Contrasting Layers (DoLa) is a contrastive decoding strategy to improve the factuality and reduce the hallucinations of LLMs, as described in this paper of ICLR 2024 DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models.\n\nDoLa is achieved by contrasting the differences in logits obtained from final layers versus earlier layers, thus amplify the factual knowledge localized to particular part of transformer layers.\n\nDo the following two steps to activate DoLa decoding when calling the function:\n‚Ä¢ Set the argument, which can be either a string or a list of integers.\n‚Ä¢ If set to a string, it can be one of , .\n‚Ä¢ If set to a list of integers, it should be a list of layer indices between 0 and the total number of layers in the model. The 0-th layer is word embedding, and the 1st layer is the first transformer layer, and so on.\n‚Ä¢ Set is suggested to reduce repetition in DoLa decoding.\n\nSee the following examples for DoLa decoding with the 32-layer LLaMA-7B model.\n\nstands for the candidate layers in premature layer selection, as described in the DoLa paper. The selected premature layer will be contrasted with the final layer.\n\nSetting to or will select the lower or higher part of the layers to contrast, respectively.\n‚Ä¢ For -layer models with layers, the layers of and are used for and layers, respectively.\n‚Ä¢ For models with layers, the layers of and are used for and layers, respectively.\n‚Ä¢ If the model has tied word embeddings, we skip the word embeddings (0-th) layer and start from the 2nd layer, as the early exit from word embeddings will become identity function.\n‚Ä¢ Set the to a list of integers for layer indices to contrast manually specified layers. For example, setting will contrast the final layer (32-th layer) with the 28-th and 30-th layers.\n\nThe paper suggested that contrasting layers to improve short-answer tasks like TruthfulQA, and contrasting layers to improve all the other long-answer reasoning tasks, such as GSM8K, StrategyQA, FACTOR, and VicunaQA. Applying DoLa to smaller models like GPT-2 is not recommended, as the results shown in the Appendix N of the paper."
    },
    {
        "link": "https://marco-gonzalez.medium.com/breaking-the-token-limit-how-to-work-with-large-amounts-of-text-in-chatgpt-da18c798d882",
        "document": "Have you ever wanted to use ChatGPT to help you write/review/proofread a large body of text, but were limited by the maximum number of tokens allowed? In this article, I‚Äôm going to show you how I used the OpenAI API and Python to overcome the token limit.\n\nThe inspiration for this solution came when I wanted to scan through a video transcript of a YouTube video for a project I was working on, but I quickly found out that ChatGPT couldn‚Äôt handle the word count, which was over 50,000 words. On average, 4000 tokens is around 8,000 words. This is the token limit for ChatGPT. However, I found a way to work around this limitation.\n\nTo overcome this limitation, I used a technique called ‚Äúbatch processing.‚Äù I broke down the script into smaller chunks of text, and then used the OpenAI API to process each batch separately. I set the batch size to 250 words, while also giving the AI 500 words of context (250 before and 250 after). I also set the max_tokens to 1000 so that GPT 3 wouldn‚Äôt randomly cut off sentences, which was an issue I ran into during early testing.\n\nHere‚Äôs an example of the code we used:\n\nimport openai\n\nopenai.api_key = \"your api key here\"\n\n\n\n# Your large text body here\n\nscript = \"paste your text here\"\n\n\n\n# Setting batch size and context size\n\nbatch_size = 250\n\n\n\n# Tokenize the script\n\nscript_tokens = script.split(\" \")\n\n\n\nfor i in range(0, len(script_tokens), batch_size):\n\n if i < batch_size:\n\n before_context = \"\"\n\n else:\n\n before_context = \" \".join(script_tokens[i-batch_size:i])\n\n text_to_edit = \" \".join(script_tokens[i:i+batch_size])\n\n if i+batch_size*2 >= len(script_tokens):\n\n after_context = \"\"\n\n else:\n\n after_context = \" \".join(script_tokens[i+batch_size:i+batch_size*2])\n\n \n\n prompt = f\"Please proofread, rewrite, and improve of the following text inside the brackets (in the context that it is a youtube script for a narrated video), considering the context given before and after it: before:\\\"{before_context}\\\" text to edit:{text_to_edit} after:\\\"{after_context}\\\" []\"\n\n\n\n response = openai.Completion.create(\n\n model=\"text-davinci-003\",\n\n prompt=prompt,\n\n temperature=0.9,\n\n max_tokens=1000,\n\n top_p=1,\n\n frequency_penalty=0.25,\n\n presence_penalty=0\n\n )\n\n # Print the response from the GPT-3 API\n\n print(response[\"choices\"][0][\"text\"])\n\nIt‚Äôs important to note that this method has its limitations, such as that GPT-3 will not know the context of the entire story, only the small context we feed it of before and after the target text.\n\nAlso, I want to mention that this script cost around 9 dollars to run when I used it on a 50,000 word video transcript. I did use a paid account to accomplish this, but OpenAI gives you an $18 free credit when you sign up."
    },
    {
        "link": "https://reddit.com/r/ChatGPTCoding/comments/1hb2ltk/which_large_language_model_has_the_absolute",
        "document": "I've been experimenting with using a number of different large language models for code generation tasks, i.e. programming.\n\nMy usage is typically asking the LLM to generate full-fledged programs.\n\nTypically these are Python scripts with little utilities.\n\nExamples of programs I commonly develop are backup utilities, cloud sync GUIs, Streamlit apps for data visualization, that sort of thing.\n\nThe program might be easily 400 lines of Python and the most common issue I run into when trying to use LLMs to either generate, debug or edit these isn't actually the abilities of the model so much as it is the continuous output length.\n\nSometimes they use chunking to break up the outputs but frequently I find that chunking is an unreliable method. Sometimes the model will say this output is too long for a continuous output So I'm going to chunk it, but then the chunking isn't accurate And it ends up just being a mess\n\nI'm wondering if anyone is doing something similar and has figured out workarounds to the common EOS and stop commands built into frontends, whether accessing these through the web UI or the API.\n\nI don't even need particularly deep context because usually after the first generation I debug it myself. I just need that it can have a very long first output!"
    },
    {
        "link": "https://deepchecks.com/5-approaches-to-solve-llm-token-limits",
        "document": "Certain techniques can help in overcoming token limitations. Let‚Äôs discuss these in detail.\n\nThe easiest way to bring your text within the max token limit is to clip it from either end. Clipping means we remove words/sentences from the text‚Äôs start or end. It is a simple fix, but it comes at the cost of loss of information. The model will not process the truncated text and might miss the important context.\n\nTruncation can be done on a character or word level, depending on the requirement. The following Python code shows how to truncate the text from the end of the sentence.\n\nAnother method of processing long text bodies is by breaking the text into smaller chunks. Different chunking strategies exist to split texts. Firstly, the strategy that comes to mind is to split on a consistent fixed chunk size according to raw token counts. Chunks can be created on a sentence level by splitting text while respecting the boundaries of sentences. This sentence-splitting technique can be further enhanced by introducing sliding windows with buffering surrounding sentences (thereby adding surrounding context) of the split. Also, chunking can be done with semantics as a focus, such that instead of chunking text with a fixed chunk size, the semantic splitter adaptively picks the breakpoint between sentences using embedding similarity with this concept proposed by Greg Kamradt. Lastly, chunking can be performed on a relation basis such that chunks have several hierarchies of different sizes referencing their parent node.\n\nEach chunk is passed individually as input to the LLM and produces independent results. The results are then combined to form a single output. However, the final result is prone to errors since the individual chunks contain only part of the overall information, and stitching the final results may still leave gaps. These chunks can then be converted into numeric representations (embedding or vectors), encapsulating text semantics and enabling efficient LLM processing. These embeddings can be readily stored in a vector store or database where relevant chunks can be retrieved using similarity search techniques. With Retrieval-augmented Generation pipelines, efficient text chunks can be retrieved and further manage the llm tokenization constraints of the LLM and focus on the most relevant and important information. When combined with chunking, this technique becomes even more powerful, as embeddings for individual chunks can be stored and searched independently, allowing for fine-grained retrieval.\n\nText can convey meaning in multiple formats. A lengthier corpus may not necessarily add value to the text‚Äôs overall meaning. Summarize your text in a way that fits within the token limit of the model and retains its valuable information. By condensing the input text into a concise summary, the core information is processed without exceeding the llm token limits. This is effective but can at times skip minute details in the original text. The following examples portray how intelligently summarizing text can solve LLM problems.\n\nLong_text = ‚ÄúIt‚Äôs such a fine day today, The sun is out, and the sky is blue. Can you tell me what the weather will be like tomorrow?‚Äù\n\nShort_text = ‚ÄúIt‚Äôs sunny today. What will the weather be like tomorrow?‚Äù\n\nShorter_text = ‚ÄúTell me the weather forecast for tomorrow‚Äù\n\nThe three versions of the text each ask the LLM about the weather forecast tomorrow in three different ways. Notice how the `Short_text` and `shorter_text` ask the same question but with significantly fewer tokens. This way, text can be summarized to get relevant outputs.\n\nStop word removal is a common technique in NLP to reduce the corpus size. Stop words include meaningless terms like ‚Äúto‚Äù and ‚Äúthe‚Äù often appearing in the text. These are important for sentence formation, but modern LLMs focus more on key terms. We can simply write the following terms\n\nThe LLM will analyze the terms and pick out actions closest to these entities. It will have enough information to understand that you are asking for tomorrow‚Äôs weather forecast in Texas.\n\nHowever, this method is not reliable with complex sentences. Before moving on with this technique, it should be manually verified that the sentence makes enough sense to convey its true meaning. Otherwise, the corpus will render incorrect results.\n\nThe Python library NLTK provides a helpful collection of stop words for removal.\n\nFine-tuning refers to training a model to perform better on niche tasks. A fine-tuned LLM will produce better results for specific problems with lesser input data; hence it can be used within the token limitation range. Fine-tuning can improve the context window via techniques such as Positional Interpolation to solve the llm token limit.\n\nFine-tuning involves using the existing weights of a model and continuing to train it with specific data. The model develops a richer understanding of the new information and performs well for similar cases. Popular LLMs like ChatGPT provide guides for fine-tuning their models. Moreover, HuggingFace provides an easy, no-code tool called AutoTrain for LLM fine-tuning. The tool allows you to select relevant parameters and the desired open-source model from the Hugging Face Hub."
    },
    {
        "link": "https://analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp",
        "document": "What is Tokenization in NLP? Here‚Äôs All You Need To Know\n\nLanguage is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you‚Äôve ever picked up a language that wasn‚Äôt your mother tongue, you‚Äôll relate to this! There are so many layers to peel off and syntaxes to consider ‚Äì it‚Äôs quite a challenge to learn what us tokenization NLP.\n\nAnd that‚Äôs exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That‚Äôs where the concept of tokenization in Natural Language Processing (NLP) comes in. This process involves breaking down the text into smaller units called tokens. What is tokenization in NLP is essential for various NLP tasks like text classification, named entity recognition, and sentiment analysis.\n\nSimply put, we can‚Äôt work with text data if we don‚Äôt perform tokenization. Yes, it‚Äôs really that important!\n\nAnd here‚Äôs the intriguing thing about tokenization ‚Äì it‚Äôs not just about breaking down the text. Tokenization plays a significant role in dealing with text data. So in this article, we will explore the depths of tokenization in Natural Language Processing and how you can implement it in Python. Also, you will get to know about the what is tokenization and types of tokenization in NLP.\n\nIn this article, you will learn about tokenization in Python, explore a practical tokenization example, and follow a comprehensive tokenization tutorial in NLP. By the end, you‚Äôll have a solid understanding of how to effectively break down text for analysis.\n‚Ä¢ Understand the concept and importance of tokenization in NLP\n\nI recommend taking some time to go through the below resource if you‚Äôre new to NLP:\n\nTokenization is a common task in Natural Language Processing (NLP). It‚Äôs a fundamental step in both traditional NLP methods like Count Vectorizer and Advanced Deep Learning-based architectures like Transformers.\n\nTokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types ‚Äì word, character, and subword (n-gram characters) tokenization.\n\nFor example, consider the sentence: ‚ÄúNever give up‚Äù.\n\nThe most common way of forming tokens is based on space. Assuming space as a delimiter, the tokenization of the sentence results in 3 tokens ‚Äì Never-give-up. As each token is a word, it becomes an example of Word tokenization.\n\nSimilarly, tokens can be either characters or subwords. For example, let us consider ‚Äúsmarter‚Äù:\n\nBut then is this necessary? Do we really need tokenization to do all of this?\n\nNote: If you are new to NLP, check out our NLP Course Online\n\nTokenization is the process of breaking down a piece of text, like a sentence or a paragraph, into individual words or ‚Äútokens.‚Äù These tokens are the basic building blocks of language, and tokenization helps computers understand and process human language by splitting it into manageable units.\n\nFor example, tokenizing the sentence ‚ÄúI love ice cream‚Äù would result in three tokens: ‚ÄúI,‚Äù ‚Äúlove,‚Äù and ‚Äúice cream.‚Äù It‚Äôs a fundamental step in natural language processing and text analysis tasks.\n\nHere is types of tokenization in nlp:\n‚Ä¢ Character Tokenization: Useful for languages without clear separation or for very detailed analysis.\n‚Ä¢ Subwords Tokenization: Smaller than words, but bigger than characters (useful for complex languages or unknown words).\n\nAs tokens are the building blocks of Natural Language, the most common way of processing the raw text happens at the token level.\n\nFor example, Transformer based models ‚Äì the State of The Art (SOTA) Deep Learning architectures in NLP ‚Äì process the raw text at the token level. Similarly, the most popular deep learning architectures for NLP like RNN, GRU, and LSTM also process the raw text at the token level.\n\nAs shown here, RNN receives and processes each token at a particular timestep.\n\nHence, Tokenization is the foremost step while modeling text data. Tokenization is performed on the corpus to obtain tokens. The following tokens are then used to prepare a vocabulary. Vocabulary refers to the set of unique tokens in the corpus. Remember that vocabulary can be constructed by considering each unique token in the corpus or by considering the top K Frequently Occurring Words.\n\nOne of the simplest hacks to boost the performance of the NLP model is to create a vocabulary out of top K frequently occurring words.\n\nNow, let‚Äôs understand the usage of the vocabulary in Traditional and Advanced Deep Learning-based NLP methods.\n‚Ä¢ Traditional NLP approaches such as Count Vectorizer and TF-IDF use vocabulary as features. Each word in the vocabulary is treated as a unique feature:\n‚Ä¢ In Advanced Deep Learning-based NLP architectures, vocabulary is used to create the tokenized input sentences. Finally, the tokens of these sentences are passed as inputs to the model\n\nWhich Tokenization Should you use?\n\nAs discussed earlier, tokenization can be performed on word, character, or subword level. It‚Äôs a common question ‚Äì which Tokenization should we use while solving an NLP task? Let‚Äôs address this question here.\n\nWord Tokenization is the most commonly used tokenization algorithm. It splits a piece of text into individual words based on a certain delimiter. Depending upon delimiters, different word-level tokens are formed. Pretrained Word Embeddings such as Word2Vec and GloVe comes under word tokenization.\n\nBut, there are few drawbacks to this.\n\nOne of the major issues with word tokens is dealing with Out Of Vocabulary (OOV) words. OOV words refer to the new words which are encountered at testing. These new words do not exist in the vocabulary. Hence, these methods fail in handling OOV words.\n\nBut wait ‚Äì don‚Äôt jump to any conclusions yet!\n‚Ä¢ A small trick can rescue word tokenizers from OOV words. The trick is to form the vocabulary with the Top K Frequent Words and replace the rare words in training data with unknown tokens (UNK). This helps the model to learn the representation of OOV words in terms of UNK tokens\n‚Ä¢ So, during test time, any word that is not present in the vocabulary will be mapped to a UNK token. This is how we can tackle the problem of OOV in word tokenizers.\n‚Ä¢ The problem with this approach is that the entire information of the word is lost as we are mapping OOV to UNK tokens. The structure of the word might be helpful in representing the word accurately. And another issue is that every OOV word gets the same representation\n\nAnother issue with word tokens is connected to the size of the vocabulary. Generally, pre-trained models are trained on a large volume of the text corpus. So, just imagine building the vocabulary with all the unique words in such a large corpus. This explodes the vocabulary!\n\nThis opens the door to Character Tokenization.\n\nCharacter Tokenization splits apiece of text into a set of characters. It overcomes the drawbacks we saw above about Word Tokenization.\n‚Ä¢ Character Tokenizers handles OOV words coherently by preserving the information of the word. It breaks down the OOV word into characters and represents the word in terms of these characters\n‚Ä¢ It also limits the size of the vocabulary. Want to talk a guess on the size of the vocabulary? 26 since the vocabulary contains a unique set of characters\n\nCharacter tokens solve the OOV problem but the length of the input and output sentences increases rapidly as we are representing a sentence as a sequence of characters. As a result, it becomes challenging to learn the relationship between the characters to form meaningful words.\n\nThis brings us to another tokenization known as Subword Tokenization which is in between a Word and Character tokenization.\n\nAlso Read- What are Categorical Data Encoding Methods\n\nPython provides several powerful libraries and tools that make it easy to perform tokenization and text preprocessing for natural language processing tasks. Here are some of the most popular ones:\n\nNLTK is a suite of libraries and programs for symbolic and statistical natural language processing. It includes a wide range of tokenizers for different needs:\n\nNLTK tokenizers support different token types like words, punctuation, and provide functionality to filter out stopwords.\n\nspaCy is a popular open-source library for advanced natural language processing in Python. It provides highly efficient tokenization that accounts for linguistic structure and context:\n‚Ä¢ Easy customization to add new rules for tokenizing domain-specific text.\n\nspaCy‚Äôs tokenization forms the base for its advanced NLP capabilities like named entity recognition, part-of-speech tagging, etc.\n\nThe Hugging Face Tokenizers library provides access to tokenizers from popular transformer models used for tasks like text generation, summarization, translation, etc. It includes:\n‚Ä¢ And tokenizers from many other transformers\n\nThis library allows you to use the same tokenization as pre-trained models, ensuring consistency between tokenization during pre-training and fine-tuning.\n\nThere are also tokenization utilities in other Python data science and NLP libraries like:\n‚Ä¢ Gensim: Has basic tokenizers as part of its data preprocessing tools.\n‚Ä¢ Polyglot: Provides word, line, and character tokenizers for over 165 languages.\n\nThe choice of tokenization library depends on the specific NLP task, performance requirements, and whether you need special handling for languages, domains or data types.\n\nSubword Tokenization splits the piece of text into subwords (or n-gram characters). For example, words like lower can be segmented as low-er, smartest as smart-est, and so on.\n\nTransformed based models ‚Äì the SOTA in NLP ‚Äì rely on Subword Tokenization algorithms for preparing vocabulary. Now, I will discuss one of the most popular Subword Tokenization algorithm known as Byte Pair Encoding (BPE).\n\nByte Pair Encoding (BPE) is a widely used tokenization method among transformer-based models. BPE addresses the issues of Word and Character Tokenizers:\n‚Ä¢ BPE tackles OOV effectively. It segments OOV as subwords and represents the word in terms of these subwords\n‚Ä¢ The length of input and output sentences after BPE are shorter compared to character tokenization\n\nBPE is a word segmentation algorithm that merges the most frequently occurring character or character sequences iteratively. Here is a step by step guide to learn BPE.\n‚Ä¢ Split the words in the corpus into characters after appending </w>\n‚Ä¢ Initialize the vocabulary with unique characters in the corpus\n‚Ä¢ Compute the frequency of a pair of characters or character sequences in corpus\n‚Ä¢ Merge the most frequent pair in corpus\n‚Ä¢ Save the best pair to the vocabulary\n‚Ä¢ Repeat steps 3 to 5 for a certain number of iterations\n\nWe will understand the steps with an example.\n\n1a) Append the end of the word (say </w>) symbol to every word in the corpus:\n\nRepeat steps 3-5 for every iteration from now. Let me illustrate for one more iteration.\n\nAfter 10 iterations, BPE merge operations looks like:\n\nBut, how can we represent the OOV word at test time using BPE learned operations? Any ideas? Let‚Äôs answer this question now.\n\nHere is a step by step procedure for representing OOV words:\n‚Ä¢ Split the OOV word into characters after appending </w>\n‚Ä¢ Compute pair of character or character sequences in a word\n‚Ä¢ Select the pairs present in the learned operations\n‚Ä¢ Repeat steps 2 and 3 until merging is possible\n\nLet‚Äôs see all this in action next!\n\nWe are now aware of how BPE works ‚Äì learning and applying to the OOV words. So, its time to implement our knowledge in Python.\n\nThe python code for BPE is already available in the original paper itself (Neural Machine Translation of Rare Words with Subword Units, 2016)\n\nWe‚Äôll consider a simple corpus to illustrate the idea of BPE. Nevertheless, the same idea applies to another corpus as well:\n\nTokenize the words into characters in the corpus and append </w> at the end of every word:\n\nCompute the frequency of each word in the corpus:\n\nLet‚Äôs define a function to compute the frequency of a pair of character or character sequences. It accepts the corpus and returns the pair with its frequency:\n\nNow, the next task is to merge the most frequent pair in the corpus. We will define a function that accepts the corpus, best pair, and returns the modified corpus:\n\nNext, its time to learn BPE operations. As BPE is an iterative procedure, we will carry out and understand the steps for one iteration. Let‚Äôs compute the frequency of bigrams:\n\nFinally, merge the best pair and save to the vocabulary:\n\nWe will follow similar steps for certain iterations:\n\nThe most interesting part is yet to come! That‚Äôs applying BPE to OOV words.\n\nNow, we will see how to segment the OOV word into subwords using learned operations. Consider OOV word to be ‚Äúlowest‚Äù:\n\nApplying BPE to an OOV word is also an iterative process. We will implement the steps discussed earlier in the article:\n\nAs you can see here, the unknown word ‚Äúlowest‚Äù is segmented as low-est.\n\nWhile basic word and character level tokenization are common, there are several advanced tokenization algorithms and methods designed to handle the complexities of natural language:\n\nAn extension of the original BPE, Byte-Level BPE operates on a byte-level rather than character-level. It encodes each token as a sequence of bytes rather than characters. This allows it to:\n‚Ä¢ Achieve open-vocabulary by representing any unseen word as a sequence of subword tokens\n\nByte-Level BPE is used by models like GPT-2 for text generation.\n\nSentencePiece is an advanced tokenization technique that treats text as a sequence of pieces or tokens which can be words, subwords or even characters. It uses language models to dynamically construct a vocabulary based on the input text during training.\n‚Ä¢ Builds vocabularies that minimize the total length of encoded sequences\n\nSentencePiece tokenization is used in models like T5, ALBERT and XLNet.\n\nIntroduced by Google for their BERT model, WordPiece is a subword tokenization technique that iteratively creates a vocabulary of ‚Äúwordpieces‚Äù ‚Äì common words and subwords occurring in the training data.\n\nThe WordPiece algorithm starts with a single wordpiece for each character and iteratively:\n‚Ä¢ Finds two most frequent pairs of wordpieces\n‚Ä¢ Merges them to create a new wordpiece\n\nThis allows representing rare/unknown words as sequences of common wordpieces.\n\nUsed in models like XLNet, this is a data-driven subword tokenization method that creates tokens based on the statistics of the training data. It constructs a vocabulary of tokens (words/subwords) that maximizes the likelihood of the training data.\n\nThese advanced techniques aim to strike the right balance between vocabulary size and handling rare/unknown words for robust language modeling.\n\nTokenization is a powerful way of dealing with text data. We saw a glimpse of that in this article and also implemented tokenization using Python. Go ahead and try this out on any text-based dataset you have. The more you practice, the better your understanding of how tokenization works (and why it‚Äôs such a critical NLP concept). Feel free to reach out to me in the comments below if you have any queries or thoughts on this article. Hope you like this article and get an exact information for about tokenization and types of tokenization in nlp. We have provide an exact informat for the tokenization related topic.\n\nHope you like the article! You will understand what tokenization in NLP is, how tokenization NLP works, and the role of a tokenizer in processing language data effectively.\n‚Ä¢ Tokenization is essential for breaking down text into units machines can process\n‚Ä¢ Different tokenization methods have unique strengths and limitations"
    },
    {
        "link": "https://geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works",
        "document": "Tokenization in natural language processing (NLP) is a technique that involves dividing a sentence or phrase into smaller units known as tokens. These tokens can encompass words, dates, punctuation marks, or even fragments of words. The article aims to cover the fundamentals of tokenization, it‚Äôs types and use case.\n\nWhat is Tokenization in NLP?\n\nNatural Language Processing (NLP) is a subfield of computer science, artificial intelligence, information engineering, and human-computer interaction. This field focuses on how to program computers to process and analyze large amounts of natural language data. It is difficult to perform as the process of reading and understanding languages is far more complex than it seems at first glance. Tokenization is a foundation step in NLP pipeline that shapes the entire workflow.\n\nTokenization is the process of dividing a text into smaller units known as tokens. Tokens are typically words or sub-words in the context of natural language processing. Tokenization is a critical step in many NLP tasks, including text processing, language modelling, and machine translation. The process involves splitting a string, or text into a list of tokens. One can think of tokens as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\n\nTokenization involves using a tokenizer to segment unstructured data and natural language text into distinct chunks of information, treating them as different elements. The tokens within a document can be used as vector, transforming an unstructured text document into a numerical data structure suitable for machine learning. This rapid conversion enables the immediate utilization of these tokenized elements by a computer to initiate practical actions and responses. Alternatively, they may serve as features within a machine learning pipeline, prompting more sophisticated decision-making processes or behaviors.\n\nTokenization can be classified into several types based on how the text is segmented. Here are some types of tokenization:\n\nWord tokenization divides the text into individual words. Many NLP tasks use this approach, in which words are treated as the basic units of meaning.\n\nThe text is segmented into sentences during sentence tokenization. This is useful for tasks requiring individual sentence analysis or processing.\n\nSubword tokenization entails breaking down words into smaller units, which can be especially useful when dealing with morphologically rich languages or rare words.\n\nThis process divides the text into individual characters. This can be useful for modelling character-level language.\n\nTokenization is a crucial step in text processing and natural language processing (NLP) for several reasons.\n‚Ä¢ Effective Text Processing: Tokenization reduces the size of raw text so that it can be handled more easily for processing and analysis.\n‚Ä¢ Feature extraction: Text data can be represented numerically for algorithmic comprehension by using tokens as features in\n‚Ä¢ Language Modelling: Tokenization in NLP facilitates the creation of organized representations of language, which is useful for tasks like text generation and language modelling.\n‚Ä¢ Information Retrieval: Tokenization is essential for indexing and searching in systems that store and retrieve information efficiently based on words or phrases.\n‚Ä¢ Text Analysis: Tokenization is used in many NLP tasks, including , to determine the function and context of individual words in a sentence.\n‚Ä¢ Vocabulary Management: By generating a list of distinct tokens that stand in for words in the dataset, tokenization helps manage a corpus‚Äôs vocabulary.\n‚Ä¢ Task-Specific Adaptation: Tokenization can be customized to meet the needs of particular NLP tasks, meaning that it will work best in applications such as summarization and machine translation.\n‚Ä¢ Preprocessing Step: This essential preprocessing step transforms unprocessed text into a format appropriate for additional statistical and computational analysis.\n\nThe code snippet uses sent_tokenize function from NLTK library. The function is used to segment a given text into a list of sentences.\n\nWhen we have huge chunks of data then it is efficient to use ‚Äò from the NLTK library. The Punkt tokenizer is a data-driven sentence tokenizer that comes with NLTK. It is trained on large corpus of text to identify sentence boundaries.\n\nOne can also tokenize sentence from different languages using different pickle file other than English. In the following code snippet, we have used NLTK library to tokenize a Spanish text into sentences using pre-trained Punkt tokenizer for Spanish. The Punkt tokenizer is a data-driven tokenizer that uses machine learning techniques to identify sentence boundaries.\n\nThe code snipped uses the word_tokenize function from NLTK library to tokenize a given text into individual words. The word_tokenize function is helpful for breaking down a sentence or text into its constituent words, facilitating further analysis or processing at the word level in natural language processing tasks.\n\nThe code snippet uses the from the Natural Language Toolkit (NLTK) to tokenize a given text into individual words.\n\nThese tokenizers work by separating the words using punctuation and spaces. And as mentioned in the code outputs above, it doesn‚Äôt discard the punctuation, allowing a user to decide what to do with the punctuations at the time of pre-processing.\n\nThe is one of the NLTK tokenizers that splits words based on punctuation boundaries. Each punctuation mark is treated as a separate token.\n\nThe code snippet uses the from the Natural Language Toolkit (NLTK) to tokenize a given text based on a regular expression pattern.\n\nUsing regular expressions allows for more fine-grained control over tokenization, and you can customize the pattern based on your specific requirements.\n\nWe have discussed the ways to implement how can we perform tokenization using NLTK library. We can also implement tokenization using following methods and libraries:\n‚Ä¢ BERT tokenizer: uses WordPiece tokenizer is a type of subword tokenizer for tokenizing input text. Using regular expressions allows for more fine-grained control over tokenization, and you can customize the pattern based on your specific requirements.\n‚Ä¢ Byte-Pair Encoding: is a data compression algorithm that has also found applications in the field of natural language processing, specifically for tokenization. It is a technique that works by iteratively merging the most frequent pairs of consecutive bytes (or characters) in a given corpus.\n‚Ä¢ Sentence Piece: SentencePiece is another subword tokenization algorithm commonly used for natural language processing tasks. It is designed to be language-agnostic and works by iteratively merging frequent sequences of characters or subwords in a given corpus.\n‚Ä¢ None Tokenization is unable to capture the meaning of the sentence hence, results in ambiguity\n‚Ä¢ None In certain languages like Chinese, Japanese, Arabic, lack distinct spaces between words. Hence, there is an absence of clear boundaries that complicates the process of tokenization.\n‚Ä¢ None Text may also include more than one word, for example email address, URLs and special symbols , hence it is difficult to decide how to tokenize such elements.\n\nQ. What is Tokenization in NLP?\n\nQ. What is Lemmatization in NLP?\n\nQ. Which are most common types of tokenization?"
    },
    {
        "link": "https://grammarly.com/blog/ai/what-is-tokenization",
        "document": "Tokenization is a critical yet often overlooked component of natural language processing (NLP). In this guide, we‚Äôll explain tokenization, its use cases, pros and cons, and why it‚Äôs involved in almost every large language model (LLM).\n‚Ä¢ What is tokenization in NLP?\n\nWhat is tokenization in NLP?\n\nTokenization is an NLP method that converts text into numerical formats that machine learning (ML) models can use. When you send your prompt to an LLM such as Anthropic‚Äôs Claude, Google‚Äôs Gemini, or a member of OpenAI‚Äôs GPT series, the model does not directly read your text. These models can only take numbers as inputs, so the text must first be converted into a sequence of numbers using a tokenizer.\n\nOne way a tokenizer may tokenize text would be to split it into separate words and assign a number to each unique word:\n\n‚ÄúGrammarly loves grammar and ML and writing‚Äù might become:\n\nEach word (and its associated number) is a token. An ML model can use the sequence of tokens‚Äî[7,102], [37], [564], [2], [9,763], [2], [231]‚Äîto run its operations and produce its output. This output is usually a number, which is converted back into text using the reverse of this same tokenization process. In practice, this word-by-word tokenization is great as an example but is rarely used in industry for reasons we will see later.\n\nOne final thing to note is that tokenizers have vocabularies‚Äîthe complete set of tokens they can handle. A tokenizer that knows basic English words but not company names may not have ‚ÄúGrammarly‚Äù as a token in its vocabulary, leading to tokenization failure.\n\nIn general, tokenization is turning a chunk of text into a sequence of numbers. Though it‚Äôs natural to think of tokenization at the word level, there are many other tokenization methods, one of which‚Äîsubword tokenization‚Äîis the industry standard.\n\nWord tokenization is the example we saw before, where text is split by each word and by punctuation.\n\nWord tokenization‚Äôs main benefit is that it‚Äôs easy to understand and visualize. However, it has a few shortcomings:\n‚Ä¢ Punctuation, if present, is attached to the words, as with ‚Äúwriting.‚Äù\n‚Ä¢ Novel or uncommon words (such as ‚ÄúGrammarly‚Äù) take up a whole token.\n\nAs a result, word tokenization can create vocabularies with hundreds of thousands of tokens. The problem with large vocabularies is they make training and inference much less efficient‚Äîthe matrix needed to convert between text and numbers would need to be huge.\n\nAdditionally, there would be many infrequently used words, and the NLP models wouldn‚Äôt have enough relevant training data to return accurate responses for those infrequent words. If a new word was invented tomorrow, an LLM using word tokenization would need to be retrained to incorporate this word.\n\nSubword tokenization splits text into chunks smaller than or equal to words. There is no fixed size for each token; each token (and its length) is determined by the training process. Subword tokenization is the industry standard for LLMs. Below is an example, with tokenization done by the GPT-4o tokenizer:\n\nHere, the uncommon word ‚ÄúGrammarly‚Äù gets broken down into three tokens: ‚ÄúGr,‚Äù ‚Äúamm,‚Äù and ‚Äúarly.‚Äù Meanwhile, the other words are common enough in text that they form their own tokens.\n\nSubword tokenization allows for smaller vocabularies, meaning more efficient and cheaper training and inference. Subword tokenizers can also break down rare or novel words into combinations of smaller, existing tokens. For these reasons, many NLP models use subword tokenization.\n\nCharacter tokenization splits text into individual characters. Here‚Äôs how our example would look:\n\nEvery single unique character becomes its own token. This actually requires the smallest vocabulary since there are only 52 letters in the alphabet (uppercase and lowercase are regarded as different) and several punctuation marks. Since any English word must be formed from these characters, character tokenization can work with any new or rare word.\n\nHowever, by standard LLM benchmarks, character tokenization doesn‚Äôt perform as well as subword tokenization in practice. The subword token ‚Äúcar‚Äù contains much more information than the character token ‚Äúc,‚Äù so the attention mechanism in transformers has more information to run on.\n\nSentence tokenization turns each sentence in the text into its own token. Our example would look like:\n\nThe benefit is that each token contains a ton of information. However, there are several drawbacks. There are infinite ways to combine words to write sentences. So, the vocabulary would need to be infinite as well.\n\nAdditionally, each sentence itself would be pretty rare since even minute differences (such as ‚Äúas well‚Äù instead of ‚Äúand‚Äù) would mean a different token despite having the same meaning. Training and inference would be a nightmare. Sentence tokenization is used in specialized use cases such as sentence sentiment analysis, but otherwise, it‚Äôs a rare sight.\n\nChoosing the right granularity of tokenization for a model is really a complex relationship between efficiency and performance. With very large tokens (e.g., at the sentence level), the vocabulary becomes massive. The model‚Äôs training efficiency drops because the matrix to hold all these tokens is huge. Performance plummets since there isn‚Äôt enough training data for all the unique tokens to meaningfully learn relationships.\n\nOn the other end, with small tokens, the vocabulary becomes small. Training becomes efficient, but performance may plummet since each token doesn‚Äôt contain enough information for the model to learn token-token relationships.\n\nSubword tokenization is right in the middle. Each token has enough information for models to learn relationships, but the vocabulary is not so large that training becomes inefficient.\n\nTokenization revolves around the training and use of tokenizers. Tokenizers convert text into tokens and tokens back into text. We‚Äôll discuss subword tokenizers here since they are the most popular type.\n\nSubword tokenizers must be trained to split text effectively.\n\nWhy is it that ‚ÄúGrammarly‚Äù gets split into ‚ÄúGr,‚Äù ‚Äúamm,‚Äù and ‚Äúarly‚Äù? Couldn‚Äôt ‚ÄúGram,‚Äù ‚Äúmar,‚Äù and ‚Äúly‚Äù also work? To a human eye, it definitely could, but the tokenizer, which has presumably learned the most efficient representation, thinks differently. A common training algorithm (though not used in GPT-4o) employed to learn this representation is byte-pair encoding (BPE). We‚Äôll explain BPE in the next section.\n\nTo train a good tokenizer, you need a massive corpus of text to train on. Running BPE on this corpus works as follows:\n‚Ä¢ Split all the text in the corpus into individual characters. Set these as the starting tokens in the vocabulary.\n‚Ä¢ Merge the two most frequently adjacent tokens from the text into one new token and add it to the vocabulary (without deleting the old tokens‚Äîthis is important).\n‚Ä¢ Repeat this process until there are no remaining frequently occurring pairs of adjacent tokens, or the maximum vocabulary size has been reached.\n\nAs an example, assume that our entire training corpus consists of the text ‚Äúabc abcd‚Äù:\n‚Ä¢ The text would be split into [‚Äúa‚Äù, ‚Äúb‚Äù, ‚Äúc‚Äù, ‚Äú ‚Äù, ‚Äúa‚Äù, ‚Äúb‚Äù, ‚Äúc‚Äù, ‚Äúd‚Äù]. Note that the fourth entry in that list is a space character. Our vocabulary would then be [‚Äúa‚Äù, ‚Äúb‚Äù, ‚Äúc‚Äù, ‚Äú ‚Äù, ‚Äúd‚Äù].\n‚Ä¢ ‚Äúa‚Äù and ‚Äúb‚Äù most frequently occur next to each other in the text (tied with ‚Äúb‚Äù and ‚Äúc‚Äù but ‚Äúa‚Äù and ‚Äúb‚Äù win alphabetically). So, we combine them into one token, ‚Äúab‚Äù. The vocabulary now looks like [‚Äúa‚Äù, ‚Äúb‚Äù, ‚Äúc‚Äù, ‚Äú ‚Äù, ‚Äúd‚Äù, ‚Äúab‚Äù], and the updated text (with the ‚Äúab‚Äù token merge applied) looks like [‚Äúab‚Äù, ‚Äúc‚Äù, ‚Äú ‚Äù, ‚Äúab‚Äù, ‚Äúc‚Äù, ‚Äúd‚Äù].\n‚Ä¢ Now, ‚Äúab‚Äù and ‚Äúc‚Äù occur most frequently together in the text. We merge them into the token ‚Äúabc‚Äù. The vocabulary then looks like [‚Äúa‚Äù, ‚Äúb‚Äù, ‚Äúc‚Äù, ‚Äú ‚Äù, ‚Äúd‚Äù, ‚Äúab‚Äù, ‚Äúabc‚Äù], and the updated text looks like [‚Äúabc‚Äù, ‚Äú ‚Äù, ‚Äúabc‚Äù, ‚Äúd‚Äù].\n‚Ä¢ We end the process here since each adjacent token pair now only occurs once. Merging tokens further would make the resulting model perform worse on other texts. In practice, the vocabulary size limit is the limiting factor.\n\nWith our new vocabulary set, we can map between text and tokens. Even text that we haven‚Äôt seen before, like ‚Äúcab,‚Äù can be tokenized because we didn‚Äôt discard the single-character tokens. We can also return token numbers by simply seeing the position of the token within the vocabulary.\n\nGood tokenizer training requires extremely high volumes of data and a lot of computing‚Äîmore than most companies can afford. Companies get around this by skipping the training of their own tokenizer. Instead, they just use a pre-trained tokenizer (such as the GPT-4o tokenizer linked above) to save time and money with minimal, if any, loss in model performance.\n\nSo, we have this subword tokenizer trained on a massive corpus using BPE. Now, how do we use it on a new piece of text?\n\nWe apply the merge rules we determined in the tokenizer training process. We first split the input text into characters. Then, we do token merges in the same order as in training.\n\nTo illustrate, we‚Äôll use a slightly different input text of ‚Äúdc abc‚Äù:\n‚Ä¢ We split it into characters [‚Äúd‚Äù, ‚Äúc‚Äù, ‚Äú ‚Äù, ‚Äúa‚Äù, ‚Äúb‚Äù, ‚Äúc‚Äù].\n‚Ä¢ The first merge we did in training was ‚Äúab‚Äù so we do that here: [‚Äúd‚Äù, ‚Äúc‚Äù, ‚Äú ‚Äù, ‚Äúab‚Äù, ‚Äúc‚Äù].\n‚Ä¢ The second merge we did was ‚Äúabc‚Äù so we do that: [‚Äúd‚Äù, ‚Äúc‚Äù, ‚Äú ‚Äù, ‚Äúabc‚Äù].\n‚Ä¢ Those are the only merge rules we have, so we are done tokenizing, and we can return the token IDs.\n\nIf we have a bunch of token IDs and we want to convert this into text, we can simply look up each token ID in the list and return its associated text. LLMs do this to turn the embeddings (vectors of numbers that capture the meaning of tokens by looking at the surrounding tokens) they work with back into human-readable text.\n\nTokenization is always the first step in all NLP. Turning text into forms that ML models (and computers) can work with requires tokenization.\n\nTokenization is usually the first and last part of every LLM call. The text is turned into tokens first, then the tokens are converted to embeddings to capture each token‚Äôs meaning and passed into the main parts of the model (the transformer blocks). After the transformer blocks run, the embeddings are converted back into tokens. Finally, the just-returned token is added to the input and passed back into the model, repeating the process again. LLMs use subword tokenization to balance performance and efficiency.\n\nSearch engines tokenize user queries to standardize them and to better understand user intent. Search engine tokenization might involve splitting text into words, removing filler words (such as ‚Äúthe‚Äù or ‚Äúand‚Äù), turning uppercase into lowercase, and dealing with characters like hyphens. Subword tokenization usually isn‚Äôt necessary here since performance and efficiency are less dependent on vocabulary size.\n\nMachine translation tokenization is interesting since the input and output languages are different. As a result, there will be two tokenizers, one for each language. Subword tokenization usually works best since it balances the trade-off between model efficiency and model performance. But some languages, such as Chinese, don‚Äôt have a linguistic component smaller than a word. There, word tokenization is called for.\n\nTokenization is a must-have for any NLP model. Good tokenization lets ML models work efficiently with text and handle new words well.\n\nInternally, ML models only work with numbers. The algorithm behind ML models relies entirely on computation, which itself requires numbers to compute. So, text must be turned into numbers before ML models can work with them. After tokenization, techniques like attention or embedding can be run on the numbers.\n\nTokenization generalizes to new and rare text\n\nOr more accurately, good tokenization generalizes to new and rare text. With subword and character tokenization, new texts can be decomposed into sequences of existing tokens. So, pasting an article with gibberish words into ChatGPT won‚Äôt cause it to break (though it may not give a very coherent response either). Good generalization also allows models to learn relationships among rare words, based on the relationships in the subtokens.\n\nTokenization depends on the training corpus and the algorithm, so results can vary. This can affect LLMs‚Äô reasoning abilities and their input and output length.\n\nAn easy problem that often stumps LLMs is counting the occurrences of the letter ‚Äúr‚Äù in the word ‚Äústrawberry.‚Äù The model would incorrectly say there were two, though the answer is really three. This error may partially have been because of tokenization. The subword tokenizer split ‚Äústrawberry‚Äù into ‚Äúst,‚Äù ‚Äúraw,‚Äù and ‚Äúberry.‚Äù So, the model may not have been able to connect the one ‚Äúr‚Äù in the middle token to the two ‚Äúr‚Äùs in the last token. The tokenization algorithm chosen directly affects how words get tokenized and how each token relates to the others.\n\nLLMs are mostly built on the transformer architecture, which relies on the attention mechanism to contextualize each token. However, as the number of tokens increases, the time needed for attention goes up quadratically. So, a text with four tokens will take 16 units of time but a text with eight tokens will take 64 units of time. This confines LLMs to input and output limits of a few hundred thousand tokens. With smaller tokens, this can really limit the amount of text you can feed into the model, reducing the number of tasks you can use it for."
    },
    {
        "link": "https://neuralpai.medium.com/the-comprehensive-guide-to-tokenization-concepts-techniques-and-implementation-a5f6958e2b2a",
        "document": "Tokenization is the process of breaking a stream of text into smaller pieces called tokens. These tokens may be words, punctuation marks, numbers, or even subword units, depending on the context and application. Tokenizers serve as the foundational layer for many text processing tasks in both natural language processing (NLP) and compiler design.\n\nIn NLP, tokenization is often the first step in preprocessing raw text data before feeding it into algorithms for analysis, sentiment detection, machine translation, or information retrieval. In compiler design, tokenization (or lexical analysis) is used to convert source code into tokens that are then parsed to create an abstract syntax tree (AST).\n\nThe significance of tokenization cannot be overstated. Whether you are developing a search engine, building a chatbot, or designing a programming language, the ability to effectively and efficiently break down text into manageable units is essential. In this guide, we will explore tokenization from multiple perspectives and provide detailed Python code examples that illustrate how different tokenizers work."
    },
    {
        "link": "https://datacamp.com/blog/what-is-tokenization",
        "document": "Imagine you're trying to teach a child to read. Instead of diving straight into complex paragraphs, you'd start by introducing them to individual letters, then syllables, and finally, whole words. In a similar vein, tokenization breaks down vast stretches of text into more digestible and understandable units for machines.\n\nThe primary goal of tokenization is to represent text in a manner that's meaningful for machines without losing its context. By converting text into tokens, algorithms can more easily identify patterns. This pattern recognition is crucial because it makes it possible for machines to understand and respond to human input. For instance, when a machine encounters the word \"running\", it doesn't see it as a singular entity but rather as a combination of tokens that it can analyze and derive meaning from.\n\nTo delve deeper into the mechanics, consider the sentence, \"Chatbots are helpful.\" When we tokenize this sentence by words, it transforms into an array of individual words:\n\nThis is a straightforward approach where spaces typically dictate the boundaries of tokens. However, if we were to tokenize by characters, the sentence would fragment into:\n\nThis character-level breakdown is more granular and can be especially useful for certain languages or specific NLP tasks.\n\nIn essence, tokenization is akin to dissecting a sentence to understand its anatomy. Just as doctors study individual cells to understand an organ, NLP practitioners use tokenization to dissect and understand the structure and meaning of text.\n\nIt's worth noting that while our discussion centers on tokenization in the context of language processing, the term \"tokenization\" is also used in the realms of security and privacy, particularly in data protection practices like credit card tokenization. In such scenarios, sensitive data elements are replaced with non-sensitive equivalents, called tokens. This distinction is crucial to prevent any confusion between the two contexts.\n\nTokenization methods vary based on the granularity of the text breakdown and the specific requirements of the task at hand. These methods can range from dissecting text into individual words to breaking them down into characters or even smaller units. Here's a closer look at the different types:\n‚Ä¢ Word tokenization. This method breaks text down into individual words. It's the most common approach and is particularly effective for languages with clear word boundaries like English.\n‚Ä¢ Character tokenization. Here, the text is segmented into individual characters. This method is beneficial for languages that lack clear word boundaries or for tasks that require a granular analysis, such as spelling correction.\n‚Ä¢ Subword tokenization. Striking a balance between word and character tokenization, this method breaks text into units that might be larger than a single character but smaller than a full word. For instance, \"Chatbots\" could be tokenized into \"Chat\" and \"bots\". This approach is especially useful for languages that form meaning by combining smaller units or when dealing with out-of-vocabulary words in NLP tasks.\n\nTokenization serves as the backbone for a myriad of applications in the digital realm, enabling machines to process and understand vast amounts of text data. By breaking down text into manageable chunks, tokenization facilitates more efficient and accurate data analysis. Here are some prominent use cases, along with real-world applications:\n\nWhen you type a query into a search engine like Google, it employs tokenization to dissect your input. This breakdown helps the engine sift through billions of documents to present you with the most relevant results.\n\nTools such as Google Translate utilize tokenization to segment sentences in the source language. Once tokenized, these segments can be translated and then reconstructed in the target language, ensuring the translation retains the original context.\n\nVoice-activated assistants like Siri or Alexa rely heavily on tokenization. When you pose a question or command, your spoken words are first converted into text. This text is then tokenized, allowing the system to process and act upon your request.\n\nTokenization plays a crucial role in extracting insights from user-generated content, such as product reviews or social media posts. For instance, a sentiment analysis system for e-commerce platforms might tokenize user reviews to determine whether customers are expressing positive, neutral, or negative sentiments. For example:\n‚Ä¢ The review: \"This product is amazing, but the delivery was late.\"\n‚Ä¢ After tokenization: [\"This\", \"product\", \"is\", \"amazing\", \",\", \"but\", \"the\", \"delivery\", \"was\", \"late\", \".\"]\n\nThe tokens \"amazing\" and \"late\" can then be processed by the sentiment model to assign mixed sentiment labels, providing actionable insights for businesses.\n\nTokenization enables chatbots to understand and respond to user inputs effectively. For example, a customer service chatbot might tokenize the query:\n\n\"I need to reset my password but can't find the link.\"\n\nWhich is tokenized as: [\"I\", \"need\", \"to\", \"reset\", \"my\", \"password\", \"but\", \"can't\", \"find\", \"the\", \"link\"] .\n\nThis breakdown helps the chatbot identify the user's intent (\"reset password\") and respond appropriately, such as by providing a link or instructions.\n\nNavigating the intricacies of human language, with its nuances and ambiguities, presents a set of unique challenges for tokenization. Here's a deeper dive into some of these obstacles, along with recent advancements that address them:\n\nLanguage is inherently ambiguous. Consider the sentence \"Flying planes can be dangerous.\" Depending on how it's tokenized and interpreted, it could mean that the act of piloting planes is risky or that planes in flight pose a danger. Such ambiguities can lead to vastly different interpretations.\n\nSome languages, like Chinese, Japanese, or Thai, lack clear spaces between words, making tokenization more complex. Determining where one word ends and another begins is a significant challenge in these languages.\n\nTo address this, advancements in multilingual tokenization models have made significant strides. For instance:\n‚Ä¢ XLM-R (Cross-lingual Language Model - RoBERTa) uses subword tokenization and large-scale pretraining to handle over 100 languages effectively, including those without clear word boundaries.\n‚Ä¢ mBERT (Multilingual BERT) employs WordPiece tokenization and has shown strong performance across a variety of languages, excelling in understanding syntactic and semantic structures even in low-resource languages.\n\nThese models not only tokenize text effectively but also leverage shared subword vocabularies across languages, improving tokenization for scripts that are typically harder to process.\n\nTexts often contain more than just words. Email addresses, URLs, or special symbols can be tricky to tokenize. For instance, should \"john.doe@email.com\" be treated as a single token or split at the period or the \"@\" symbol? Advanced tokenization models now incorporate rules and learned patterns to ensure consistent handling of such cases.\n\nThe landscape of Natural Language Processing offers many tools, each tailored to specific needs and complexities. Here's a guide to some of the most prominent tools and methodologies available for tokenization:\n‚Ä¢ NLTK (Natural Language Toolkit). A stalwart in the NLP community, NLTK is a comprehensive Python library that caters to a wide range of linguistic needs. It offers both word and sentence tokenization functionalities, making it a versatile choice for beginners and seasoned practitioners alike.\n‚Ä¢ Spacy. A modern and efficient alternative to NLTK, Spacy is another Python-based NLP library. It boasts speed and supports multiple languages, making it a favorite for large-scale applications.\n‚Ä¢ BERT tokenizer. Emerging from the BERT pre-trained model, this tokenizer excels in context-aware tokenization. It's adept at handling the nuances and ambiguities of language, making it a top choice for advanced NLP projects (see this tutorial on NLP with BERT).\n‚Ä¢ Byte-Pair Encoding (BPE). An adaptive tokenization method, BPE tokenizes based on the most frequent byte pairs in a text. It's particularly effective for languages that form meaning by combining smaller units.\n‚Ä¢ SentencePiece. An unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation tasks. It handles multiple languages with a single model and can tokenize text into subwords, making it versatile for various NLP tasks.\n\nOne of the most popular tools for NLP tasks, the Hugging Face Transformers library provides a seamless integration with PyTorch, making it ideal for both research and production. This library includes advanced tokenizers designed to work with state-of-the-art transformer models like BERT, GPT, and RoBERTa. Key features include:\n‚Ä¢ Fast tokenizers: Built using Rust, these tokenizers offer significant speed improvements, enabling faster pre-processing for large datasets.\n‚Ä¢ Support for subword tokenization: The library supports Byte-Pair Encoding (BPE), WordPiece, and Unigram tokenization, ensuring efficient handling of out-of-vocabulary words and complex languages.\n‚Ä¢ Built-in pretrained tokenizers: Each model in the Hugging Face Transformers library comes with a corresponding pretrained tokenizer, ensuring compatibility and ease of use. For instance, the BERT tokenizer splits text into subwords, making it adept at handling language nuances.\n\nYour choice of tool should align with the specific requirements of your project. For those taking their initial steps in NLP, NLTK or Spacy might offer a more approachable learning curve. However, for projects demanding a deeper understanding of context and nuance, the Hugging Face Transformers and BERT tokenizer stand out as robust options.\n\nHow I Used Tokenization for a Rating Classifier Project\n\nI gained my initial experience with text tokenization while working on a portfolio project three years ago. The project involved a dataset containing user reviews and ratings, which I used to develop a deep-learning text classification model. I used `word_tokenize` from NLTK to clean up the text and `Tokenizer` from Keras to preprocess it.\n\nLet's explore how I used tokenizers in the project:\n‚Ä¢ When working with NLP data, tokenizers are commonly used to process and clean the text dataset. The aim is to eliminate stop words, punctuation, and other irrelevant information from the text. Tokenizers transform the text into a list of words, which can be cleaned using a text-cleaning function.\n‚Ä¢ Afterward, I used the Keras Tokenizer method to transform the text into an array for analysis and to prepare the tokens for the deep learning model. In this case, I used the Bidirectional LSTM model, which produced the most favorable outcomes.\n‚Ä¢ Next, I converted tokens into a sequence by using the `texts_to_sequences` function.\n‚Ä¢ Before feeding the sequence to the model, I had to add padding to make the sequence of numbers the same length.\n‚Ä¢ Finally, I split the dataset into training and testing sets, trained the model on the training set, and evaluated it on the testing set.\n\nTokenizer has many benefits in the field of natural language processing where it is used to clean, process, and analyze text data. Focusing on text processing can improve model performance.\n\nI recommend taking the Introduction to Natural Language Processing in Python course to learn more about the preprocessing techniques and dive deep into the world of tokenizers.\n\nWant to learn more about AI and machine learning? Check out these resources:"
    }
]