[
    {
        "link": "https://kubernetes.io/docs/reference/access-authn-authz/rbac",
        "document": "Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization.\n\nRBAC authorization uses the API group to drive authorization decisions, allowing you to dynamically configure policies through the Kubernetes API.\n\nTo enable RBAC, start the API server with the flag set to a file that includes the authorizer; for example:\n\nOr, start the API server with the flag set to a comma-separated list that includes ; for example:\n\nThe RBAC API declares four kinds of Kubernetes object: Role, ClusterRole, RoleBinding and ClusterRoleBinding. You can describe or amend the RBAC objects using tools such as , just like any other Kubernetes object.\n\nAn RBAC Role or ClusterRole contains rules that represent a set of permissions. Permissions are purely additive (there are no \"deny\" rules).\n\nA Role always sets permissions within a particular namespace; when you create a Role, you have to specify the namespace it belongs in.\n\nClusterRole, by contrast, is a non-namespaced resource. The resources have different names (Role and ClusterRole) because a Kubernetes object always has to be either namespaced or not namespaced; it can't be both.\n\nClusterRoles have several uses. You can use a ClusterRole to:\n• define permissions on namespaced resources and be granted access within individual namespace(s)\n• define permissions on namespaced resources and be granted access across all namespaces\n\nIf you want to define a role within a namespace, use a Role; if you want to define a role cluster-wide, use a ClusterRole.\n\nHere's an example Role in the \"default\" namespace that can be used to grant read access to pods:\n\nA ClusterRole can be used to grant the same permissions as a Role. Because ClusterRoles are cluster-scoped, you can also use them to grant access to:\n• None namespaced resources (like Pods), across all namespaces For example: you can use a ClusterRole to allow a particular user to run\n\nHere is an example of a ClusterRole that can be used to grant read access to secrets in any particular namespace, or across all namespaces (depending on how it is bound):\n\nThe name of a Role or a ClusterRole object must be a valid path segment name.\n\nA role binding grants the permissions defined in a role to a user or set of users. It holds a list of subjects (users, groups, or service accounts), and a reference to the role being granted. A RoleBinding grants permissions within a specific namespace whereas a ClusterRoleBinding grants that access cluster-wide.\n\nA RoleBinding may reference any Role in the same namespace. Alternatively, a RoleBinding can reference a ClusterRole and bind that ClusterRole to the namespace of the RoleBinding. If you want to bind a ClusterRole to all the namespaces in your cluster, you use a ClusterRoleBinding.\n\nThe name of a RoleBinding or ClusterRoleBinding object must be a valid path segment name.\n\nHere is an example of a RoleBinding that grants the \"pod-reader\" Role to the user \"jane\" within the \"default\" namespace. This allows \"jane\" to read pods in the \"default\" namespace.\n\nA RoleBinding can also reference a ClusterRole to grant the permissions defined in that ClusterRole to resources inside the RoleBinding's namespace. This kind of reference lets you define a set of common roles across your cluster, then reuse them within multiple namespaces.\n\nFor instance, even though the following RoleBinding refers to a ClusterRole, \"dave\" (the subject, case sensitive) will only be able to read Secrets in the \"development\" namespace, because the RoleBinding's namespace (in its metadata) is \"development\".\n\nTo grant permissions across a whole cluster, you can use a ClusterRoleBinding. The following ClusterRoleBinding allows any user in the group \"manager\" to read secrets in any namespace.\n\nAfter you create a binding, you cannot change the Role or ClusterRole that it refers to. If you try to change a binding's , you get a validation error. If you do want to change the for a binding, you need to remove the binding object and create a replacement.\n\nThere are two reasons for this restriction:\n• Making immutable allows granting someone permission on an existing binding object, so that they can manage the list of subjects, without being able to change the role that is granted to those subjects.\n• A binding to a different role is a fundamentally different binding. Requiring a binding to be deleted/recreated in order to change the ensures the full list of subjects in the binding is intended to be granted the new role (as opposed to enabling or accidentally modifying only the roleRef without verifying all of the existing subjects should be given the new role's permissions).\n\nThe command-line utility creates or updates a manifest file containing RBAC objects, and handles deleting and recreating binding objects if required to change the role they refer to. See command usage and examples for more information.\n\nIn the Kubernetes API, most resources are represented and accessed using a string representation of their object name, such as for a Pod. RBAC refers to resources using exactly the same name that appears in the URL for the relevant API endpoint. Some Kubernetes APIs involve a subresource, such as the logs for a Pod. A request for a Pod's logs looks like:\n\nIn this case, is the namespaced resource for Pod resources, and is a subresource of . To represent this in an RBAC role, use a slash ( ) to delimit the resource and subresource. To allow a subject to read and also access the subresource for each of those Pods, you write:\n\nYou can also refer to resources by name for certain requests through the list. When specified, requests can be restricted to individual instances of a resource. Here is an example that restricts its subject to only or a ConfigMap named :\n\nYou cannot restrict or requests by their resource name. For , this limitation is because the name of the new object may not be known at authorization time. If you restrict or by resourceName, clients must include a field selector in their or request that matches the specified resourceName in order to be authorized. For example,\n\nRather than referring to individual , , and , you can use the wildcard symbol to refer to all such objects. For , you can use the wildcard as a suffix glob match. For , an empty set means that everything is allowed. Here is an example that allows access to perform any current and future action on all current and future resources in the API group. This is similar to the built-in role.\n\nYou can aggregate several ClusterRoles into one combined ClusterRole. A controller, running as part of the cluster control plane, watches for ClusterRole objects with an set. The defines a label selector that the controller uses to match other ClusterRole objects that should be combined into the field of this one.\n\nThe control plane overwrites any values that you manually specify in the field of an aggregate ClusterRole. If you want to change or add rules, do so in the objects that are selected by the .\n\nHere is an example aggregated ClusterRole:\n\nIf you create a new ClusterRole that matches the label selector of an existing aggregated ClusterRole, that change triggers adding the new rules into the aggregated ClusterRole. Here is an example that adds rules to the \"monitoring\" ClusterRole, by creating another ClusterRole labeled .\n\nThe default user-facing roles use ClusterRole aggregation. This lets you, as a cluster administrator, include rules for custom resources, such as those served by CustomResourceDefinitions or aggregated API servers, to extend the default roles.\n\nFor example: the following ClusterRoles let the \"admin\" and \"edit\" default roles manage the custom resource named CronTab, whereas the \"view\" role can perform only read actions on CronTab resources. You can assume that CronTab objects are named in URLs as seen by the API server.\n\nThe following examples are excerpts from Role or ClusterRole objects, showing only the section.\n\nAllow reading resources in the core API Group:\n\nAllow reading/writing Deployments (at the HTTP level: objects with in the resource part of their URL) in the API groups:\n\nAllow reading Pods in the core API group, as well as reading or writing Job resources in the API group:\n\nAllow reading a ConfigMap named \"my-config\" (must be bound with a RoleBinding to limit to a single ConfigMap in a single namespace):\n\nAllow reading the resource in the core group (because a Node is cluster-scoped, this must be in a ClusterRole bound with a ClusterRoleBinding to be effective):\n\nAllow GET and POST requests to the non-resource endpoint and all subpaths (must be in a ClusterRole bound with a ClusterRoleBinding to be effective):\n\nA RoleBinding or ClusterRoleBinding binds a role to subjects. Subjects can be groups, users or ServiceAccounts.\n\nKubernetes represents usernames as strings. These can be: plain names, such as \"alice\"; email-style names, like \"bob@example.com\"; or numeric user IDs represented as a string. It is up to you as a cluster administrator to configure the authentication modules so that authentication produces usernames in the format you want.\n\nThe prefix is reserved for Kubernetes system use, so you should ensure that you don't have users or groups with names that start with by accident. Other than this special prefix, the RBAC authorization system does not require any format for usernames.\n\nIn Kubernetes, Authenticator modules provide group information. Groups, like users, are represented as strings, and that string has no format requirements, other than that the prefix is reserved.\n\nServiceAccounts have names prefixed with , and belong to groups that have names prefixed with .\n\nThe following examples are excerpts that only show the section.\n\nFor the default service account in the \"kube-system\" namespace:\n\nFor all service accounts in the \"qa\" namespace:\n\nFor all service accounts in any namespace:\n\nAPI servers create a set of default ClusterRole and ClusterRoleBinding objects. Many of these are prefixed, which indicates that the resource is directly managed by the cluster control plane. All of the default ClusterRoles and ClusterRoleBindings are labeled with .\n\nTake care when modifying ClusterRoles and ClusterRoleBindings with names that have a prefix. Modifications to these resources can result in non-functional clusters.\n\nAt each start-up, the API server updates default cluster roles with any missing permissions, and updates default cluster role bindings with any missing subjects. This allows the cluster to repair accidental modifications, and helps to keep roles and role bindings up-to-date as permissions and subjects change in new Kubernetes releases.\n\nTo opt out of this reconciliation, set the annotation on a default cluster role or default cluster RoleBinding to . Be aware that missing default permissions and subjects can result in non-functional clusters.\n\nAuto-reconciliation is enabled by default if the RBAC authorizer is active.\n\nDefault cluster role bindings authorize unauthenticated and authenticated users to read API information that is deemed safe to be publicly accessible (including CustomResourceDefinitions). To disable anonymous unauthenticated access, add flag to the API server configuration.\n\nTo view the configuration of these roles via run:\n\nSome of the default ClusterRoles are not prefixed. These are intended to be user-facing roles. They include super-user roles ( ), roles intended to be granted cluster-wide using ClusterRoleBindings, and roles intended to be granted within particular namespaces using RoleBindings ( , , ).\n\nUser-facing ClusterRoles use ClusterRole aggregation to allow admins to include rules for custom resources on these ClusterRoles. To add rules to the , , or roles, create a ClusterRole with one or more of the following labels:\n\nThe Kubernetes controller manager runs controllers that are built in to the Kubernetes control plane. When invoked with , kube-controller-manager starts each controller using a separate service account. Corresponding roles exist for each built-in controller, prefixed with . If the controller manager is not started with , it runs all control loops using its own credential, which must be granted all the relevant roles. These roles include:\n\nThe RBAC API prevents users from escalating privileges by editing roles or role bindings. Because this is enforced at the API level, it applies even when the RBAC authorizer is not in use.\n\nYou can only create/update a role if at least one of the following things is true:\n• You already have all the permissions contained in the role, at the same scope as the object being modified (cluster-wide for a ClusterRole, within the same namespace or cluster-wide for a Role).\n• You are granted explicit permission to perform the verb on the or resource in the API group.\n\nFor example, if does not have the ability to list Secrets cluster-wide, they cannot create a ClusterRole containing that permission. To allow a user to create/update roles:\n• Grant them a role that allows them to create/update Role or ClusterRole objects, as desired.\n• Grant them permission to include specific permissions in the roles they create/update:\n• implicitly, by giving them those permissions (if they attempt to create or modify a Role or ClusterRole with permissions they themselves have not been granted, the API request will be forbidden)\n• or explicitly allow specifying any permission in a or by giving them permission to perform the verb on or resources in the API group\n\nYou can only create/update a role binding if you already have all the permissions contained in the referenced role (at the same scope as the role binding) or if you have been authorized to perform the verb on the referenced role. For example, if does not have the ability to list Secrets cluster-wide, they cannot create a ClusterRoleBinding to a role that grants that permission. To allow a user to create/update role bindings:\n• Grant them a role that allows them to create/update RoleBinding or ClusterRoleBinding objects, as desired.\n• Grant them permissions needed to bind a particular role:\n• implicitly, by giving them the permissions contained in the role.\n• explicitly, by giving them permission to perform the verb on the particular Role (or ClusterRole).\n\nFor example, this ClusterRole and RoleBinding would allow to grant other users the , , and roles in the namespace :\n\nWhen bootstrapping the first roles and role bindings, it is necessary for the initial user to grant permissions they do not yet have. To bootstrap initial roles and role bindings:\n• Use a credential with the \"system:masters\" group, which is bound to the \"cluster-admin\" super-user role by the default bindings.\n• None Create a Role named \"pod-reader\" that allows users to perform , and on pods:\n• None Create a Role named \"my-component-lease-holder\" with permissions to get/update a resource with a specific name:\n• None Create a ClusterRole named \"pod-reader\" that allows user to perform , and on pods:\n• None Create a ClusterRole named \"monitoring\" with an aggregationRule specified:\n• None Within the namespace \"acme\", grant the permissions in the \"admin\" ClusterRole to a user named \"bob\":\n• None Within the namespace \"acme\", grant the permissions in the \"view\" ClusterRole to the service account in the namespace \"acme\" named \"myapp\":\n• None Within the namespace \"acme\", grant the permissions in the \"view\" ClusterRole to a service account in the namespace \"myappnamespace\" named \"myapp\":\n\nGrants a ClusterRole across the entire cluster (all namespaces). Examples:\n• None Across the entire cluster, grant the permissions in the \"cluster-admin\" ClusterRole to a user named \"root\":\n• None Across the entire cluster, grant the permissions in the \"system:node-proxier\" ClusterRole to a user named \"system:kube-proxy\":\n• None Across the entire cluster, grant the permissions in the \"view\" ClusterRole to a service account named \"myapp\" in the namespace \"acme\":\n\nMissing objects are created, and the containing namespace is created for namespaced objects, if required.\n\nExisting roles are updated to include the permissions in the input objects, and remove extra permissions if is specified.\n\nExisting bindings are updated to include the subjects in the input objects, and remove extra subjects if is specified.\n• None Test applying a manifest file of RBAC objects, displaying changes that would be made:\n• None Apply a manifest file of RBAC objects, preserving any extra permissions (in roles) and any extra subjects (in bindings):\n• None Apply a manifest file of RBAC objects, removing any extra permissions (in roles) and any extra subjects (in bindings):\n\nDefault RBAC policies grant scoped permissions to control-plane components, nodes, and controllers, but grant no permissions to service accounts outside the namespace (beyond the permissions given by API discovery roles).\n\nThis allows you to grant particular roles to particular ServiceAccounts as needed. Fine-grained role bindings provide greater security, but require more effort to administrate. Broader grants can give unnecessary (and potentially escalating) API access to ServiceAccounts, but are easier to administrate.\n\nIn order from most secure to least secure, the approaches are:\n• None Grant a role to an application-specific service account (best practice) This requires the application to specify a in its pod spec, and for the service account to be created (via the API, application manifest, , etc.). For example, grant read-only permission within \"my-namespace\" to the \"my-sa\" service account:\n• None Grant a role to the \"default\" service account in a namespace If an application does not specify a , it uses the \"default\" service account. Permissions given to the \"default\" service account are available to any pod in the namespace that does not specify a . For example, grant read-only permission within \"my-namespace\" to the \"default\" service account: Many add-ons run as the \"default\" service account in the namespace. To allow those add-ons to run with super-user access, grant cluster-admin permissions to the \"default\" service account in the namespace. Enabling this means the namespace contains Secrets that grant super-user access to your cluster's API.\n• None Grant a role to all service accounts in a namespace If you want all applications in a namespace to have a role, no matter what service account they use, you can grant a role to the service account group for that namespace. For example, grant read-only permission within \"my-namespace\" to all service accounts in that namespace:\n• None If you don't want to manage permissions per-namespace, you can grant a cluster-wide role to all service accounts. For example, grant read-only permission across all namespaces to all service accounts in the cluster:\n• None If you don't care about partitioning permissions at all, you can grant super-user access to all service accounts. This allows any application full access to your cluster, and also grants any user with read access to Secrets (or the ability to create any pod) full access to your cluster.\n\nKubernetes clusters created before Kubernetes v1.22 include write access to EndpointSlices (and Endpoints) in the aggregated \"edit\" and \"admin\" roles. As a mitigation for CVE-2021-25740, this access is not part of the aggregated roles in clusters that you create using Kubernetes v1.22 or later.\n\nExisting clusters that have been upgraded to Kubernetes v1.22 will not be subject to this change. The CVE announcement includes guidance for restricting this access in existing clusters.\n\nIf you want new clusters to retain this level of access in the aggregated roles, you can create the following ClusterRole:\n\nClusters that originally ran older Kubernetes versions often used permissive ABAC policies, including granting full API access to all service accounts.\n\nDefault RBAC policies grant scoped permissions to control-plane components, nodes, and controllers, but grant no permissions to service accounts outside the namespace (beyond the permissions given by API discovery roles).\n\nWhile far more secure, this can be disruptive to existing workloads expecting to automatically receive API permissions. Here are two approaches for managing this transition:\n\nRun both the RBAC and ABAC authorizers, and specify a policy file that contains the legacy ABAC policy:\n\nTo explain that first command line option in detail: if earlier authorizers, such as Node, deny a request, then the RBAC authorizer attempts to authorize the API request. If RBAC also denies that API request, the ABAC authorizer is then run. This means that any request allowed by either the RBAC or ABAC policies is allowed.\n\nWhen the kube-apiserver is run with a log level of 5 or higher for the RBAC component ( or ), you can see RBAC denials in the API server log (prefixed with ). You can use that information to determine which roles need to be granted to which users, groups, or service accounts.\n\nOnce you have granted roles to service accounts and workloads are running with no RBAC denial messages in the server logs, you can remove the ABAC authorizer.\n\nYou can replicate a permissive ABAC policy using RBAC role bindings.\n\nAfter you have transitioned to use RBAC, you should adjust the access controls for your cluster to ensure that these meet your information security needs."
    },
    {
        "link": "https://kubernetes.io/docs/concepts/security/service-accounts",
        "document": "This page introduces the ServiceAccount object in Kubernetes, providing information about how service accounts work, use cases, limitations, alternatives, and links to resources for additional guidance.\n\nA service account is a type of non-human account that, in Kubernetes, provides a distinct identity in a Kubernetes cluster. Application Pods, system components, and entities inside and outside the cluster can use a specific ServiceAccount's credentials to identify as that ServiceAccount. This identity is useful in various situations, including authenticating to the API server or implementing identity-based security policies.\n\nService accounts exist as ServiceAccount objects in the API server. Service accounts have the following properties:\n• None Namespaced: Each service account is bound to a Kubernetes namespace. Every namespace gets a ServiceAccount upon creation.\n• None Lightweight: Service accounts exist in the cluster and are defined in the Kubernetes API. You can quickly create service accounts to enable specific tasks.\n• None Portable: A configuration bundle for a complex containerized workload might include service account definitions for the system's components. The lightweight nature of service accounts and the namespaced identities make the configurations portable.\n\nService accounts are different from user accounts, which are authenticated human users in the cluster. By default, user accounts don't exist in the Kubernetes API server; instead, the API server treats user identities as opaque data. You can authenticate as a user account using multiple methods. Some Kubernetes distributions might add custom extension APIs to represent user accounts in the API server.\n\nWhen you create a cluster, Kubernetes automatically creates a ServiceAccount object named for every namespace in your cluster. The service accounts in each namespace get no permissions by default other than the default API discovery permissions that Kubernetes grants to all authenticated principals if role-based access control (RBAC) is enabled. If you delete the ServiceAccount object in a namespace, the control plane replaces it with a new one.\n\nIf you deploy a Pod in a namespace, and you don't manually assign a ServiceAccount to the Pod, Kubernetes assigns the ServiceAccount for that namespace to the Pod.\n\nAs a general guideline, you can use service accounts to provide identities in the following scenarios:\n• Your Pods need to communicate with the Kubernetes API server, for example in situations such as the following:\n• Granting cross-namespace access, such as allowing a Pod in namespace to read, list, and watch for Lease objects in the namespace.\n• Your Pods need to communicate with an external service. For example, a workload Pod requires an identity for a commercially available cloud API, and the commercial provider allows configuring a suitable trust relationship.\n• Authenticating to a private image registry using an .\n• An external service needs to communicate with the Kubernetes API server. For example, authenticating to the cluster as part of a CI/CD pipeline.\n• You use third-party security software in your cluster that relies on the ServiceAccount identity of different Pods to group those Pods into different contexts.\n\nHow to use service accounts\n\nTo use a Kubernetes service account, you do the following:\n• None Create a ServiceAccount object using a Kubernetes client like or a manifest that defines the object.\n• None Grant permissions to the ServiceAccount object using an authorization mechanism such as RBAC.\n• None Assign the ServiceAccount object to Pods during Pod creation. If you're using the identity from an external service, retrieve the ServiceAccount token and use it from that service instead.\n\nFor instructions, refer to Configure Service Accounts for Pods.\n\nYou can use the built-in Kubernetes role-based access control (RBAC) mechanism to grant the minimum permissions required by each service account. You create a role, which grants access, and then bind the role to your ServiceAccount. RBAC lets you define a minimum set of permissions so that the service account permissions follow the principle of least privilege. Pods that use that service account don't get more permissions than are required to function correctly.\n\nYou can use RBAC to allow service accounts in one namespace to perform actions on resources in a different namespace in the cluster. For example, consider a scenario where you have a service account and Pod in the namespace and you want your Pod to see Jobs running in the namespace. You could create a Role object that grants permissions to list Job objects. Then, you'd create a RoleBinding object in the namespace to bind the Role to the ServiceAccount object. Now, Pods in the namespace can list Job objects in the namespace using that service account.\n\nTo assign a ServiceAccount to a Pod, you set the field in the Pod specification. Kubernetes then automatically provides the credentials for that ServiceAccount to the Pod. In v1.22 and later, Kubernetes gets a short-lived, automatically rotating token using the API and mounts the token as a projected volume.\n\nBy default, Kubernetes provides the Pod with the credentials for an assigned ServiceAccount, whether that is the ServiceAccount or a custom ServiceAccount that you specify.\n\nTo prevent Kubernetes from automatically injecting credentials for a specified ServiceAccount or the ServiceAccount, set the field in your Pod specification to .\n\nIn versions earlier than 1.22, Kubernetes provides a long-lived, static token to the Pod as a Secret.\n\nIf you need the credentials for a ServiceAccount to mount in a non-standard location, or for an audience that isn't the API server, use one of the following methods:\n• TokenRequest API (recommended): Request a short-lived service account token from within your own application code. The token expires automatically and can rotate upon expiration. If you have a legacy application that is not aware of Kubernetes, you could use a sidecar container within the same pod to fetch these tokens and make them available to the application workload.\n• Token Volume Projection (also recommended): In Kubernetes v1.20 and later, use the Pod specification to tell the kubelet to add the service account token to the Pod as a projected volume. Projected tokens expire automatically, and the kubelet rotates the token before it expires.\n• Service Account Token Secrets (not recommended): You can mount service account tokens as Kubernetes Secrets in Pods. These tokens don't expire and don't rotate. In versions prior to v1.24, a permanent token was automatically created for each service account. This method is not recommended anymore, especially at scale, because of the risks associated with static, long-lived credentials. The LegacyServiceAccountTokenNoAutoGeneration feature gate (which was enabled by default from Kubernetes v1.24 to v1.26), prevented Kubernetes from automatically creating these tokens for ServiceAccounts. The feature gate is removed in v1.27, because it was elevated to GA status; you can still create indefinite service account tokens manually, but should take into account the security implications.\n\nis deprecated since Kubernetes v1.32. Use separate namespaces to isolate access to mounted secrets.\n\nKubernetes provides an annotation called that you can add to your ServiceAccounts. When this annotation is applied, the ServiceAccount's secrets can only be mounted on specified types of resources, enhancing the security posture of your cluster.\n\nYou can add the annotation to a ServiceAccount using a manifest:\n\nWhen this annotation is set to \"true\", the Kubernetes control plane ensures that the Secrets from this ServiceAccount are subject to certain mounting restrictions.\n• The name of each Secret that is mounted as a volume in a Pod must appear in the field of the Pod's ServiceAccount.\n• The name of each Secret referenced using in a Pod must also appear in the field of the Pod's ServiceAccount.\n• The name of each Secret referenced using in a Pod must also appear in the field of the Pod's ServiceAccount.\n\nBy understanding and enforcing these restrictions, cluster administrators can maintain a tighter security profile and ensure that secrets are accessed only by the appropriate resources.\n\nServiceAccounts use signed JSON Web Tokens (JWTs) to authenticate to the Kubernetes API server, and to any other system where a trust relationship exists. Depending on how the token was issued (either time-limited using a or using a legacy mechanism with a Secret), a ServiceAccount token might also have an expiry time, an audience, and a time after which the token starts being valid. When a client that is acting as a ServiceAccount tries to communicate with the Kubernetes API server, the client includes an header with the HTTP request. The API server checks the validity of that bearer token as follows:\n• Checks whether the token has expired.\n• Checks whether object references in the token claims are currently valid.\n• Checks whether the token is currently valid.\n\nThe TokenRequest API produces bound tokens for a ServiceAccount. This binding is linked to the lifetime of the client, such as a Pod, that is acting as that ServiceAccount. See Token Volume Projection for an example of a bound pod service account token's JWT schema and payload.\n\nFor tokens issued using the API, the API server also checks that the specific object reference that is using the ServiceAccount still exists, matching by the unique ID of that object. For legacy tokens that are mounted as Secrets in Pods, the API server checks the token against the Secret.\n\nFor more information about the authentication process, refer to Authentication.\n\nAuthenticating service account credentials in your own code\n\nIf you have services of your own that need to validate Kubernetes service account credentials, you can use the following methods:\n\nThe Kubernetes project recommends that you use the TokenReview API, because this method invalidates tokens that are bound to API objects such as Secrets, ServiceAccounts, Pods or Nodes when those objects are deleted. For example, if you delete the Pod that contains a projected ServiceAccount token, the cluster invalidates that token immediately and a TokenReview immediately fails. If you use OIDC validation instead, your clients continue to treat the token as valid until the token reaches its expiration timestamp.\n\nYour application should always define the audience that it accepts, and should check that the token's audiences match the audiences that the application expects. This helps to minimize the scope of the token so that it can only be used in your application and nowhere else.\n• Issue your own tokens using another mechanism, and then use Webhook Token Authentication to validate bearer tokens using your own validation service.\n• Provide your own identities to Pods.\n• None Use the SPIFFE CSI driver plugin to provide SPIFFE SVIDs as X.509 certificate pairs to Pods. 🛇 This item links to a third party project or product that is not part of Kubernetes itself. More information\n• None Use a service mesh such as Istio to provide certificates to Pods.\n• Authenticate from outside the cluster to the API server without using service account tokens:\n• Configure the API server to accept OpenID Connect (OIDC) tokens from your identity provider.\n• Use service accounts or user accounts created using an external Identity and Access Management (IAM) service, such as from a cloud provider, to authenticate to your cluster.\n• Use the CertificateSigningRequest API with client certificates.\n• Configure the kubelet to retrieve credentials from an image registry.\n• Use a Device Plugin to access a virtual Trusted Platform Module (TPM), which then allows authentication using a private key.\n• Learn how to manage your ServiceAccounts as a cluster administrator.\n• Learn how to assign a ServiceAccount to a Pod."
    },
    {
        "link": "https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin",
        "document": "A ServiceAccount provides an identity for processes that run in a Pod.\n\nA process inside a Pod can use the identity of its associated service account to authenticate to the cluster's API server.\n\nFor an introduction to service accounts, read configure service accounts.\n\nThis task guide explains some of the concepts behind ServiceAccounts. The guide also explains how to obtain or revoke tokens that represent ServiceAccounts, and how to (optionally) bind a ServiceAccount's validity to the lifetime of an API object.\n\nYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:\n\nTo be able to follow these steps exactly, ensure you have a namespace named . If you don't, create one by running:\n\nKubernetes distinguishes between the concept of a user account and a service account for a number of reasons:\n• User accounts are for humans. Service accounts are for application processes, which (for Kubernetes) run in containers that are part of pods.\n• User accounts are intended to be global: names must be unique across all namespaces of a cluster. No matter what namespace you look at, a particular username that represents a user represents the same user. In Kubernetes, service accounts are namespaced: two different namespaces can contain ServiceAccounts that have identical names.\n• Typically, a cluster's user accounts might be synchronised from a corporate database, where new user account creation requires special privileges and is tied to complex business processes. By contrast, service account creation is intended to be more lightweight, allowing cluster users to create service accounts for specific tasks on demand. Separating ServiceAccount creation from the steps to onboard human users makes it easier for workloads to follow the principle of least privilege.\n• Auditing considerations for humans and service accounts may differ; the separation makes that easier to achieve.\n• A configuration bundle for a complex system may include definition of various service accounts for components of that system. Because service accounts can be created without many constraints and have namespaced names, such configuration is usually portable.\n\nServiceAccount tokens can be bound to API objects that exist in the kube-apiserver. This can be used to tie the validity of a token to the existence of another API object. Supported object types are as follows:\n• Pod (used for projected volume mounts, see below)\n• Secret (can be used to allow revoking a token by deleting the Secret)\n• Node (in v1.32, creating new node-bound tokens is beta, using existing node-bound tokens is GA)\n\nWhen a token is bound to an object, the object's and are stored as extra 'private claims' in the issued JWT.\n\nWhen a bound token is presented to the kube-apiserver, the service account authenticator will extract and verify these claims. If the referenced object or the ServiceAccount is pending deletion (for example, due to finalizers), then for any instant that is 60 seconds (or more) after the date, authentication with that token would fail. If the referenced object no longer exists (or its does not match), the request will not be authenticated.\n\nWhen a service account token is bound to a Pod object, additional metadata is also embedded into the token that indicates the value of the bound pod's field, and the uid of that Node, if available.\n\nThis node information is not verified by the kube-apiserver when the token is used for authentication. It is included so integrators do not have to fetch Pod or Node API objects to check the associated Node name and uid when inspecting a JWT.\n\nThe TokenReview API can be used to verify and extract private claims from a token:\n• None First, assume you have a pod named and a service account named .\n• None Create a token that is bound to this Pod:\n• None Copy this token into a new file named :\n• None Submit this resource to the apiserver for review: # use '-o yaml' to inspect the output You should see an output like below: Despite using to create this resource, and defining it similar to other resource types in Kubernetes, TokenReview is a special type and the kube-apiserver does not actually persist the TokenReview object into etcd. Hence is not a valid command.\n\nThe schema for the Kubernetes-specific claims within JWT tokens is not currently documented, however the relevant code area can be found in the serviceaccount package in the Kubernetes codebase.\n\nYou can inspect a JWT using standard JWT decoding tool. Below is an example of a JWT for the ServiceAccount, bound to a Pod object named which is scheduled to the Node , in the namespace:\n\nServices that run outside of Kubernetes and want to perform offline validation of JWTs may use this schema, along with a compliant JWT validator configured with OpenID Discovery information from the API server, to verify presented JWTs without requiring use of the TokenReview API.\n\nServices that verify JWTs in this way do not verify the claims embedded in the JWT token to be current and still valid. This means if the token is bound to an object, and that object no longer exists, the token will still be considered valid (until the configured token expires).\n\nClients that require assurance that a token's bound claims are still valid MUST use the TokenReview API to present the token to the for it to verify and expand the embedded claims, using similar steps to the Verifying and inspecting private claims section above, but with a supported client library. For more information on JWTs and their structure, see the JSON Web Token RFC.\n\nBy default, the Kubernetes control plane (specifically, the ServiceAccount admission controller) adds a projected volume to Pods, and this volume includes a token for Kubernetes API access.\n\nHere's an example of how that looks for a launched Pod:\n\nThat manifest snippet defines a projected volume that consists of three sources. In this case, each source also represents a single path within that volume. The three sources are:\n• A source, that contains a token that the kubelet acquires from kube-apiserver. The kubelet fetches time-bound tokens using the TokenRequest API. A token served for a TokenRequest expires either when the pod is deleted or after a defined lifespan (by default, that is 1 hour). The kubelet also refreshes that token before the token expires. The token is bound to the specific Pod and has the kube-apiserver as its audience. This mechanism superseded an earlier mechanism that added a volume based on a Secret, where the Secret represented the ServiceAccount for the Pod, but did not expire.\n• A source. The ConfigMap contains a bundle of certificate authority data. Pods can use these certificates to make sure that they are connecting to your cluster's kube-apiserver (and not to middlebox or an accidentally misconfigured peer).\n• A source that looks up the name of the namespace containing the Pod, and makes that name information available to application code running inside the Pod.\n\nAny container within the Pod that mounts this particular volume can access the above information.\n\nThere is no specific mechanism to invalidate a token issued via TokenRequest. If you no longer trust a bound service account token for a Pod, you can delete that Pod. Deleting a Pod expires its bound service account tokens.\n\nVersions of Kubernetes before v1.22 automatically created credentials for accessing the Kubernetes API. This older mechanism was based on creating token Secrets that could then be mounted into running Pods.\n\nIn more recent versions, including Kubernetes v1.32, API credentials are obtained directly using the TokenRequest API, and are mounted into Pods using a projected volume. The tokens obtained using this method have bounded lifetimes, and are automatically invalidated when the Pod they are mounted into is deleted.\n\nYou can still manually create a Secret to hold a service account token; for example, if you need a token that never expires.\n\nOnce you manually create a Secret and link it to a ServiceAccount, the Kubernetes control plane automatically populates the token into that Secret.\n\nBefore version 1.24, Kubernetes automatically generated Secret-based tokens for ServiceAccounts. To distinguish between automatically generated tokens and manually created ones, Kubernetes checks for a reference from the ServiceAccount's secrets field. If the Secret is referenced in the field, it is considered an auto-generated legacy token. Otherwise, it is considered a manually created legacy token. For example:\n\nBeginning from version 1.29, legacy ServiceAccount tokens that were generated automatically will be marked as invalid if they remain unused for a certain period of time (set to default at one year). Tokens that continue to be unused for this defined period (again, by default, one year) will subsequently be purged by the control plane.\n\nIf users use an invalidated auto-generated token, the token validator will\n• add an audit annotation for the key-value pair ,\n• update the Secret label with the new date,\n• return an error indicating that the token has been invalidated.\n\nWhen receiving this validation error, users can update the Secret to remove the label to temporarily allow use of this token.\n\nHere's an example of an auto-generated legacy token that has been marked with the and labels:\n\nA ServiceAccount controller manages the ServiceAccounts inside namespaces, and ensures a ServiceAccount named \"default\" exists in every active namespace.\n\nThe service account token controller runs as part of . This controller acts asynchronously. It:\n• watches for ServiceAccount deletion and deletes all corresponding ServiceAccount token Secrets.\n• watches for ServiceAccount token Secret addition, and ensures the referenced ServiceAccount exists, and adds a token to the Secret if needed.\n• watches for Secret deletion and removes a reference from the corresponding ServiceAccount if needed.\n\nYou must pass a service account private key file to the token controller in the using the flag. The private key is used to sign generated service account tokens. Similarly, you must pass the corresponding public key to the using the flag. The public key will be used to verify the tokens during authentication.\n\nAn alternate setup to setting and flags is to configure an external JWT signer for external ServiceAccount token signing and key management. Note that these setups are mutually exclusive and cannot be configured together.\n\nThe modification of pods is implemented via a plugin called an Admission Controller. It is part of the API server. This admission controller acts synchronously to modify pods as they are created. When this plugin is active (and it is by default on most distributions), then it does the following when a Pod is created:\n• If the pod does not have a set, the admission controller sets the name of the ServiceAccount for this incoming Pod to .\n• The admission controller ensures that the ServiceAccount referenced by the incoming Pod exists. If there is no ServiceAccount with a matching name, the admission controller rejects the incoming Pod. That check applies even for the ServiceAccount.\n• Provided that neither the ServiceAccount's field nor the Pod's field is set to :\n• the admission controller mutates the incoming Pod, adding an extra volume that contains a token for API access.\n• the admission controller adds a to each container in the Pod, skipping any containers that already have a volume mount defined for the path . For Linux containers, that volume is mounted at ; on Windows nodes, the mount is at the equivalent path.\n• If the spec of the incoming Pod doesn't already contain any , then the admission controller adds , copying them from the .\n\nThis controller generates a ConfigMap called in the namespace. The ConfigMap records the timestamp when legacy service account tokens began to be monitored by the system.\n\nThe legacy ServiceAccount token cleaner runs as part of the and checks every 24 hours to see if any auto-generated legacy ServiceAccount token has not been used in a specified amount of time. If so, the cleaner marks those tokens as invalid.\n\nThe cleaner works by first checking the ConfigMap created by the control plane (provided that is enabled). If the current time is a specified amount of time after the date in the ConfigMap, the cleaner then loops through the list of Secrets in the cluster and evaluates each Secret that has the type .\n\nIf a Secret meets all of the following conditions, the cleaner marks it as invalid:\n• The Secret is auto-generated, meaning that it is bi-directionally referenced by a ServiceAccount.\n• The Secret is not currently mounted by any pods.\n• The Secret has not been used in a specified amount of time since it was created or since it was last used.\n\nThe cleaner marks a Secret invalid by adding a label called to the Secret, with the current date as the value. If an invalid Secret is not used in a specified amount of time, the cleaner will delete it.\n\nAll the specified amount of time above defaults to one year. The cluster administrator can configure this value through the command line argument for the component.\n\nYou use the TokenRequest subresource of a ServiceAccount to obtain a time-bound token for that ServiceAccount. You don't need to call this to obtain an API token for use within a container, since the kubelet sets this up for you using a projected volume.\n\nIf you want to use the TokenRequest API from , see Manually create an API token for a ServiceAccount.\n\nThe Kubernetes control plane (specifically, the ServiceAccount admission controller) adds a projected volume to Pods, and the kubelet ensures that this volume contains a token that lets containers authenticate as the right ServiceAccount.\n\nHere's an example of how that looks for a launched Pod:\n\nThat manifest snippet defines a projected volume that combines information from three sources:\n• A source, that contains a token that the kubelet acquires from kube-apiserver. The kubelet fetches time-bound tokens using the TokenRequest API. A token served for a TokenRequest expires either when the pod is deleted or after a defined lifespan (by default, that is 1 hour). The token is bound to the specific Pod and has the kube-apiserver as its audience.\n• A source. The ConfigMap contains a bundle of certificate authority data. Pods can use these certificates to make sure that they are connecting to your cluster's kube-apiserver (and not to a middlebox or an accidentally misconfigured peer).\n• A source. This volume makes the name of the namespace containing the Pod available to application code running inside the Pod.\n\nAny container within the Pod that mounts this volume can access the above information.\n\nTo create a non-expiring, persisted API token for a ServiceAccount, create a Secret of type with an annotation referencing the ServiceAccount. The control plane then generates a long-lived token and updates that Secret with that generated token data.\n\nHere is a sample manifest for such a Secret:\n\nTo create a Secret based on this example, run:\n\nTo see the details for that Secret, run:\n\nThe output is similar to:\n\nIf you launch a new Pod into the namespace, it can use the service-account-token Secret that you just created.\n\nIf you know the name of the Secret that contains the token you want to remove:\n\nOtherwise, first find the Secret for the ServiceAccount.\n\nThe output is similar to:\n\nThen, delete the Secret you now know the name of:\n\nThe kube-apiserver can be configured to use external signer for token signing and token verifying key management. This feature enables kubernetes distributions to integrate with key management solutions of their choice (for example, HSMs, cloud KMSes) for service account credential signing and verification. To configure kube-apiserver to use external-jwt-signer set the flag to the location of a Unix domain socket (UDS) on a filesystem, or be prefixed with an @ symbol and name a UDS in the abstract socket namespace. At the configured UDS, shall be an RPC server which implements ExternalJWTSigner. The external-jwt-signer must be healthy and be ready to serve supported service account keys for the kube-apiserver to start.\n\nCheck out KEP-740 for more details on ExternalJWTSigner.\n\nThe kube-apiserver flags and will continue to be used for reading from files unless is set; they are mutually exclusive ways of supporting JWT signing and authentication.\n\nIf you created a namespace to experiment with, you can remove it:"
    },
    {
        "link": "https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands",
        "document": "This section contains the most basic commands for getting a workload running on your cluster.\n• will start running 1 or more instances of a container image on your cluster.\n• will load balance traffic across the running instances, and can create a HA proxy for accessing the containers from outside the cluster.\n\nOnce your workloads are running, you can use the commands in the WORKING WITH APPS section to inspect them.\n\nCreate a resource from a file or from stdin.\n\nCreate a config map based on a file, directory, or specified literal value.\n\nA single config map may package one or more key/value pairs.\n\nWhen creating a config map based on a file, the key will default to the basename of the file, and the value will default to the file content. If the basename is an invalid key, you may specify an alternate key.\n\nWhen creating a config map based on a directory, each file whose basename is a valid key in the directory will be packaged into the config map. Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc).\n\nCreate a cron job with the specified name.\n\nCreate a deployment with the specified name.\n\nCreate an ingress with the specified name.\n\nCreate a job with the specified name.\n\nCreate a namespace with the specified name.\n\nCreate a pod disruption budget with the specified name, selector, and desired minimum available pods.\n\nCreate a priority class with the specified name, value, globalDefault and description.\n\nCreate a resource quota with the specified name, hard limits, and optional scopes.\n\nCreate a role binding for a particular role or cluster role.\n\nCreate a new secret for use with Docker registries.\n\nDockercfg secrets are used to authenticate against Docker registries.\n\nWhen using the Docker command line to push images, you can authenticate to a given registry by running: '$ docker login DOCKER_REGISTRY_SERVER --username=DOCKER_USER --password=DOCKER_PASSWORD --email=DOCKER_EMAIL'.\n\nThat produces a ~/.dockercfg file that is used by subsequent 'docker push' and 'docker pull' commands to authenticate to the registry. The email address is optional.\n\nWhen creating applications, you may have a Docker registry that requires authentication. In order for the nodes to pull images on your behalf, they must have the credentials. You can provide this information by creating a dockercfg secret and attaching it to your service account.\n\nCreate a secret based on a file, directory, or specified literal value.\n\nA single secret may package one or more key/value pairs.\n\nWhen creating a secret based on a file, the key will default to the basename of the file, and the value will default to the file content. If the basename is an invalid key or you wish to chose your own, you may specify an alternate key.\n\nWhen creating a secret based on a directory, each file whose basename is a valid key in the directory will be packaged into the secret. Any directory entries except regular files are ignored (e.g. subdirectories, symlinks, devices, pipes, etc).\n\nCreate a TLS secret from the given public/private key pair.\n\nThe public/private key pair must exist beforehand. The public key certificate must be .PEM encoded and match the given private key.\n\nCreate a ClusterIP service with the specified name.\n\nCreate an ExternalName service with the specified name.\n\nExternalName service references to an external DNS address instead of only pods, which will allow application authors to reference services that exist off platform, on other clusters, or locally.\n\nCreate a LoadBalancer service with the specified name.\n\nCreate a NodePort service with the specified name.\n\nCreate a service account with the specified name.\n\nDisplay one or many resources.\n\nPrints a table of the most important information about the specified resources. You can filter the list using a label selector and the --selector flag. If the desired resource type is namespaced you will only see results in your current namespace unless you pass --all-namespaces.\n\nUninitialized objects are not shown unless --include-uninitialized is passed.\n\nBy specifying the output as 'template' and providing a Go template as the value of the --template flag, you can filter the attributes of the fetched resources.\n\nUse \"kubectl api-resources\" for a complete list of supported resources.\n\nCreate and run a particular image in a pod.\n\nLooks up a deployment, service, replica set, replication controller or pod by name and uses the selector for that resource as the selector for a new service on the specified port. A deployment or replica set will be exposed as a service only if its selector is convertible to a selector that service supports, i.e. when the selector contains only the matchLabels component. Note that if no port is specified via --port and the exposed resource has multiple ports, all will be re-used by the new service. Also if no labels are specified, the new service will re-use the labels from the resource it exposes.\n\nDelete resources by file names, stdin, resources and names, or by resources and label selector.\n\nJSON and YAML formats are accepted. Only one type of argument may be specified: file names, resources and names, or resources and label selector.\n\nSome resources, such as pods, support graceful deletion. These resources define a default period before they are forcibly terminated (the grace period) but you may override that value with the --grace-period flag, or pass --now to set a grace-period of 1. Because these resources often represent entities in the cluster, deletion may not be acknowledged immediately. If the node hosting a pod is down or cannot reach the API server, termination may take significantly longer than the grace period. To force delete a resource, you must specify the --force flag. Note: only a subset of resources support graceful deletion. In absence of the support, the --grace-period flag is ignored.\n\nIMPORTANT: Force deleting pods does not wait for confirmation that the pod's processes have been terminated, which can leave those processes running until the node detects the deletion and completes graceful deletion. If your processes use shared storage or talk to a remote API and depend on the name of the pod to identify themselves, force deleting those pods may result in multiple processes running on different machines using the same identification which may lead to data corruption or inconsistency. Only force delete pods when you are sure the pod is terminated, or if your application can tolerate multiple copies of the same pod running at once. Also, if you force delete pods, the scheduler may place new pods on those nodes before the node has released those resources and causing those pods to be evicted immediately.\n\nNote that the delete command does NOT do resource version checks, so if someone submits an update to a resource right when you submit a delete, their update will be lost along with the rest of the resource.\n\nThis section contains commands for creating, updating, deleting, and viewing your workloads in a Kubernetes cluster.\n\nApply a configuration to a resource by file name or stdin. The resource name must be specified. This resource will be created if it doesn't exist yet. To use 'apply', always create the resource initially with either 'apply' or 'create --save-config'.\n\nAlpha Disclaimer: the --prune functionality is not yet complete. Do not use unless you are aware of what the current state is. See https://issues.k8s.io/34274.\n\nEdit the latest last-applied-configuration annotations of resources from the default editor.\n\nThe edit-last-applied command allows you to directly edit any API resource you can retrieve via the command-line tools. It will open the editor defined by your KUBE_EDITOR, or EDITOR environment variables, or fall back to 'vi' for Linux or 'notepad' for Windows. You can edit multiple objects, although changes are applied one at a time. The command accepts file names as well as command-line arguments, although the files you point to must be previously saved versions of resources.\n\nThe default format is YAML. To edit in JSON, specify \"-o json\".\n\nThe flag --windows-line-endings can be used to force Windows line endings, otherwise the default for your operating system will be used.\n\nIn the event an error occurs while updating, a temporary file will be created on disk that contains your unapplied changes. The most common error when updating a resource is another editor changing the resource on the server. When this occurs, you will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.\n\nSet the latest last-applied-configuration annotations by setting it to match the contents of a file. This results in the last-applied-configuration being updated as though 'kubectl apply -f ' was run, without updating any other parts of the object.\n\nView the latest last-applied-configuration annotations by type/name or file.\n\nThe default output will be printed to stdout in YAML format. You can use the -o option to change the output format.\n\nUpdate the annotations on one or more resources.\n\nAll Kubernetes objects support the ability to store additional data with the object as annotations. Annotations are key/value pairs that can be larger than labels and include arbitrary string values such as structured JSON. Tools and system extensions may use annotations to store their own data.\n\nAttempting to set an annotation that already exists will fail unless --overwrite is set. If --resource-version is specified and does not match the current resource version on the server the command will fail.\n\nUse \"kubectl api-resources\" for a complete list of supported resources.\n\nCreates an autoscaler that automatically chooses and sets the number of pods that run in a Kubernetes cluster.\n\nLooks up a deployment, replica set, stateful set, or replication controller by name and creates an autoscaler that uses the given resource as a reference. An autoscaler can automatically increase or decrease number of pods deployed within the system as needed.\n\n'debug' provides automation for common debugging tasks for cluster objects identified by resource and name. Pods will be used by default if no resource is specified.\n\nThe action taken by 'debug' varies depending on what resource is specified. Supported actions include:\n• Workload: Create a copy of an existing pod with certain attributes changed, for example changing the image tag to a new version.\n• Workload: Add an ephemeral container to an already running pod, for example to add debugging utilities without restarting the pod.\n• Node: Create a new pod that runs in the node's host namespaces and can access the node's filesystem.\n\nDiff configurations specified by file name or stdin between the current online configuration, and the configuration as it would be if applied.\n\nThe output is always YAML.\n\nKUBECTL_EXTERNAL_DIFF environment variable can be used to select your own diff command. Users can use external commands with params too, example: KUBECTL_EXTERNAL_DIFF=\"colordiff -N -u\"\n\nBy default, the \"diff\" command available in your path will be run with the \"-u\" (unified diff) and \"-N\" (treat absent files as empty) options.\n\nExit status: 0 No differences were found. 1 Differences were found. >1 Kubectl or diff failed with an error.\n\nNote: KUBECTL_EXTERNAL_DIFF, if used, is expected to follow that convention.\n\nThe edit command allows you to directly edit any API resource you can retrieve via the command-line tools. It will open the editor defined by your KUBE_EDITOR, or EDITOR environment variables, or fall back to 'vi' for Linux or 'notepad' for Windows. You can edit multiple objects, although changes are applied one at a time. The command accepts file names as well as command-line arguments, although the files you point to must be previously saved versions of resources.\n\nEditing is done with the API version used to fetch the resource. To edit using a specific API version, fully-qualify the resource, version, and group.\n\nThe default format is YAML. To edit in JSON, specify \"-o json\".\n\nThe flag --windows-line-endings can be used to force Windows line endings, otherwise the default for your operating system will be used.\n\nIn the event an error occurs while updating, a temporary file will be created on disk that contains your unapplied changes. The most common error when updating a resource is another editor changing the resource on the server. When this occurs, you will have to apply your changes to the newer version of the resource, or update your temporary saved copy to include the latest resource version.\n\nBuild a set of KRM resources using a 'kustomization.yaml' file. The DIR argument must be a path to a directory containing 'kustomization.yaml', or a git repository URL with a path suffix specifying same with respect to the repository root. If DIR is omitted, '.' is assumed.\n• A label key and value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters each.\n• Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.\n• If --overwrite is true, then existing labels can be overwritten, otherwise attempting to overwrite a label will result in an error.\n• If --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used.\n\nUpdate fields of a resource using strategic merge patch, a JSON merge patch, or a JSON patch.\n\nReplace a resource by file name or stdin.\n\nJSON and YAML formats are accepted. If replacing an existing resource, the complete resource spec must be provided. This can be obtained by\n\nPaused resources will not be reconciled by a controller. Use \"kubectl rollout resume\" to resume a paused resource. Currently only deployments support being paused.\n\nPaused resources will not be reconciled by a controller. By resuming a resource, we allow it to be reconciled again. Currently only deployments support being resumed.\n\nShow the status of the rollout.\n\nBy default 'rollout status' will watch the status of the latest rollout until it's done. If you don't want to wait for the rollout to finish then you can use --watch=false. Note that if a new rollout starts in-between, then 'rollout status' will continue watching the latest revision. If you want to pin to a specific revision and abort if it is rolled over by another revision, use --revision=N where N is the revision you need to watch for.\n\nSet a new size for a deployment, replica set, replication controller, or stateful set.\n\nScale also allows users to specify one or more preconditions for the scale action.\n\nIf --current-replicas or --resource-version is specified, it is validated before the scale is attempted, and it is guaranteed that the precondition holds true when the scale is sent to the server.\n\nThese commands help you make changes to existing application resources.\n\nList environment variable definitions in one or more pods, pod templates. Add, update, or remove container environment variable definitions in one or more pod templates (within replication controllers or deployment configurations). View or modify the environment variable definitions on all containers in the specified pods or pod templates, or just those that match a wildcard.\n\nIf \"--env -\" is passed, environment variables can be read from STDIN using the standard env syntax.\n\nSpecify compute resource requirements (CPU, memory) for any resource that defines a pod template. If a pod is successfully scheduled, it is guaranteed the amount of resource requested, but may burst up to its specified limits.\n\nFor each compute resource, if a limit is specified and a request is omitted, the request will default to the limit.\n\nPossible resources include (case insensitive): Use \"kubectl api-resources\" for a complete list of supported resources..\n\nSet the selector on a resource. Note that the new selector will overwrite the old selector if the resource had one prior to the invocation of 'set selector'.\n\nA selector must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters. If --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used. Note: currently selectors can only be set on Service objects.\n\nPossible resources (case insensitive) can be:\n\nUpdate the user, group, or service account in a role binding or cluster role binding.\n\nExperimental: Wait for a specific condition on one or many resources.\n\nThe command takes multiple resources and waits until the specified condition is seen in the Status field of every given resource.\n\nAlternatively, the command can wait for the given set of resources to be deleted by providing the \"delete\" keyword as the value to the --for flag.\n\nA successful message will be printed to stdout indicating when the specified condition has been met. You can use -o option to change to output destination.\n\nThis section contains commands for inspecting and debugging your applications.\n• will print the logs from the specified pod + container.\n• can be used to get an interactive shell on a pod + container.\n• will print debug information about the given resource.\n\nAttach to a process that is already running inside an existing container.\n\nCheck whether an action is allowed.\n\nVERB is a logical Kubernetes API verb like 'get', 'list', 'watch', 'delete', etc. TYPE is a Kubernetes resource. Shortcuts and groups will be resolved. NONRESOURCEURL is a partial URL that starts with \"/\". NAME is the name of a particular Kubernetes resource.\n\nMissing objects are created, and the containing namespace is created for namespaced objects, if required.\n\nExisting roles are updated to include the permissions in the input objects, and remove extra permissions if --remove-extra-permissions is specified.\n\nExisting bindings are updated to include the subjects in the input objects, and remove extra subjects if --remove-extra-subjects is specified.\n\nThis is preferred to 'apply' for RBAC resources so that semantically-aware merging of rules and subjects is done.\n\nCopy files and directories to and from containers.\n\nShow details of a specific resource or group of resources.\n\nPrint a detailed description of the selected resources, including related resources such as events or controllers. You may select a single object by name, all objects of that type, provide a name prefix, or label selector. For example:\n\nwill first check for an exact match on TYPE and NAME_PREFIX. If no such resource exists, it will output details for every resource that has a name prefixed with NAME_PREFIX.\n\nUse \"kubectl api-resources\" for a complete list of supported resources.\n\nPrint the logs for a container in a pod or specified resource. If the pod has only one container, the container name is optional.\n\nForward one or more local ports to a pod.\n\nUse resource type/name such as deployment/mydeployment to select a pod. Resource type defaults to 'pod' if omitted.\n\nIf there are multiple pods matching the criteria, a pod will be selected automatically. The forwarding session ends when the selected pod terminates, and a rerun of the command is needed to resume forwarding.\n\nCreates a proxy server or application-level gateway between localhost and the Kubernetes API server. It also allows serving static content over specified HTTP path. All incoming data enters through one port and gets forwarded to the remote Kubernetes API server port, except for the path matching the static content path.\n\nThe top command allows you to see the resource consumption for nodes or pods.\n\nThis command requires Metrics Server to be correctly configured and working on the server.\n\nThe top-node command allows you to see the resource consumption of nodes.\n\nThe 'top pod' command allows you to see the resource consumption of pods.\n\nDue to the metrics pipeline delay, they may be unavailable for a few minutes since pod creation.\n\nPrint the supported API versions on the server, in the form of \"group/version\".\n\nkubectl certificate approve allows a cluster admin to approve a certificate signing request (CSR). This action tells a certificate signing controller to issue a certificate to the requestor with the attributes requested in the CSR.\n\nSECURITY NOTICE: Depending on the requested attributes, the issued certificate can potentially grant a requester access to cluster resources or to authenticate as a requested identity. Before approving a CSR, ensure you understand what the signed certificate can do.\n\nkubectl certificate deny allows a cluster admin to deny a certificate signing request (CSR). This action tells a certificate signing controller to not to issue a certificate to the requestor.\n\nDisplay addresses of the control plane and services with label kubernetes.io/cluster-service=true. To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n\nDump cluster information out suitable for debugging and diagnosing cluster problems. By default, dumps everything to stdout. You can optionally specify a directory with --output-directory. If you specify a directory, Kubernetes will build a set of files in that directory. By default, only dumps things in the current namespace and 'kube-system' namespace, but you can switch to a different namespace with the --namespaces flag, or specify --all-namespaces to dump all namespaces.\n\nThe command also dumps the logs of all of the pods in the cluster; these logs are dumped into different directories based on namespace and pod name.\n\nThe given node will be marked unschedulable to prevent new pods from arriving. 'drain' evicts the pods if the API server supports https://kubernetes.io/docs/concepts/workloads/pods/disruptions/ . Otherwise, it will use normal DELETE to delete the pods. The 'drain' evicts or deletes all pods except mirror pods (which cannot be deleted through the API server). If there are daemon set-managed pods, drain will not proceed without --ignore-daemonsets, and regardless it will not delete any daemon set-managed pods, because those pods would be immediately replaced by the daemon set controller, which ignores unschedulable markings. If there are any pods that are neither mirror pods nor managed by a replication controller, replica set, daemon set, stateful set, or job, then drain will not delete any pods unless you use --force. --force will also allow deletion to proceed if the managing resource of one or more pods is missing.\n\n'drain' waits for graceful termination. You should not operate on the machine until the command completes.\n\nWhen you are ready to put the node back into service, use kubectl uncordon, which will make the node schedulable again.\n\nUpdate the taints on one or more nodes.\n• A taint consists of a key, value, and effect. As an argument here, it is expressed as key=value:effect.\n• The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 253 characters.\n• Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.\n• The value is optional. If given, it must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters.\n• The effect must be NoSchedule, PreferNoSchedule or NoExecute.\n• Currently taint can only apply to node.\n\nThese commands correspond to alpha features that are not enabled in Kubernetes clusters by default.\n\nPrint the supported API resources on the server.\n\nOutput shell completion code for the specified shell (bash or zsh). The shell code must be evaluated to provide interactive completion of kubectl commands. This can be done by sourcing it from the .bash_profile.\n\nDetailed instructions on how to do this are available here:\n\nNote for zsh users: [1] zsh completions are only supported in versions of zsh >= 5.2.\n\nThe loading order follows these rules:\n• If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes place.\n• If $KUBECONFIG environment variable is set, then it is used as a list of paths (normal path delimiting rules for your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the last file in the list.\n• Otherwise, ${HOME}/.kube/config is used and no merging takes place.\n\nDelete the specified cluster from the kubeconfig.\n\nDelete the specified context from the kubeconfig.\n\nDelete the specified user from the kubeconfig.\n\nDisplay one or many contexts from the kubeconfig file.\n\nCONTEXT_NAME is the context name that you want to change.\n\nNEW_NAME is the new name you want to set.\n\nNote: If the context being renamed is the 'current-context', this field will also be updated.\n\nSet an individual value in a kubeconfig file.\n\nPROPERTY_NAME is a dot delimited name where each token represents either an attribute name or a map key. Map keys may not contain dots.\n\nPROPERTY_VALUE is the new value you want to set. Binary fields such as 'certificate-authority-data' expect a base64 encoded string unless the --set-raw-bytes flag is used.\n\nSpecifying an attribute name that already exists will merge new fields on top of existing values.\n\nSpecifying a name that already exists will merge new fields on top of existing values for those fields.\n\nSpecifying a name that already exists will merge new fields on top of existing values for those fields.\n\nSpecifying a name that already exists will merge new fields on top of existing values.\n\nUnset an individual value in a kubeconfig file.\n\nPROPERTY_NAME is a dot delimited name where each token represents either an attribute name or a map key. Map keys may not contain dots.\n\nYou can use --output jsonpath={...} to extract specific values using a jsonpath expression.\n\nThis command describes the fields associated with each supported API resource. Fields are identified via a simple JSONPath identifier:\n\nAdd the --recursive flag to display all of the fields at once without descriptions. Information about each field is retrieved from the server in OpenAPI format.\n\nUse \"kubectl api-resources\" for a complete list of supported resources.\n\nPrint the list of flags inherited by all commands\n\nProvides utilities for interacting with plugins.\n\nPlugins provide extended functionality that is not part of the major command-line distribution. Please refer to the documentation and examples for more information about how write your own plugins.\n\nThe easiest way to discover and install plugins is via the kubernetes sub-project krew. To install krew, visit https://krew.sigs.k8s.io/docs/user-guide/setup/install/\n\nList all available plugin files on a user's PATH.\n\nAvailable plugin files are those that are: - executable - anywhere on the user's PATH - begin with \"kubectl-\"\n\nPrint the client and server version information for the current context."
    },
    {
        "link": "https://armosec.io/blog/a-guide-for-using-kubernetes-rbac",
        "document": "What is RBAC in Kubernetes?\n\nKubernetes has revolutionized container orchestration, becoming the go-to platform for managing containerized workloads at scale. However, with its growing popularity, the complexity of managing role-based access control (RBAC) on Day 2 and especially in a multi-cluster environment has become a daunting task. DevOps, SRE, and Platform teams are responsible for multiple clusters and different teams. This article explores the challenges of Kubernetes RBAC implementation, best practices for managing RBAC in Kubernetes, and the innovative solution offered by ARMO Platform to simplify RBAC management and enhance Kubernetes security.\n\nRBAC is a fundamental security mechanism in Kubernetes that governs access to resources within the default namespace or any other namespace in a cluster. RBAC is often compared to attribute-based access control (ABAC), which provides more granular, context-aware access policies. RBAC policies define which users or groups can access specific Kubernetes resources and the actions they are allowed to perform on those resources. The RBAC process involves three steps: authentication, authorization, and admission control.\n\nAuthentication is the process of verifying the identity of a user or entity. There are three types of entities in Kubernetes: a user (human or service accounts), a group (a collection of users), and a service account (used by pods inside the cluster). Users must be created using X.509 certificates signed by the cluster CA, which can be done through CSRs and human access certificate issuing. While this procedure is outside the scope of this document, many online tutorials explain the process.\n\nPods running inside the cluster don’t need a certificate or OAuth tokens, as they can use service accounts and controllers for authentication. Initially, a default service account is automatically created in every namespace when the namespace is created. This service account is named default and is used by pods that do not explicitly specify a service account. User accounts are also available for processes that run inside pods. Creating an X.509 certificate for the accounts is unnecessary; instead, link pod and user accounts are typically done in manifest files. For example:\n\nIn the K8s RBAC process, authentication ensures that the user or entity requesting access to the Kubernetes cluster is who it claims to be. Once the entity is authenticated, the next step is authorization.\n\nIn the Kubernetes cluster, authorization establishes what actions users or entities can allow through granted permissions. Once a user or entity is authenticated, the RBAC system checks their permissions and RBAC roles to determine if they have the necessary privileges to perform the requested actions and access rights. This step ensures that only authorized users, such as those with an admin role, can perform specific operations within the cluster, and is important for maintaining consistent permissions for users. However, please remember the principle of least privilege (PoLP), and that general usage of the cluster admin role is discouraged.\n\nAdmission control is a mechanism in Kubernetes that intercepts and processes API requests before they are persisted in the cluster control plane. The API server handles these requests and the admission control process ensures they comply with the defined authorization policies. Admission control enforces policies and rules to validate and modify the requested resources, including those requiring admin access. In the RBAC process, admission control ensures that the requested actions comply with the defined authorization policies. It can reject or modify requests that do not meet the specified criteria.\n\nIt is important to note that RBAC is unnecessary for Kubernetes resources specified in manifest files. For example, creating a pod and a secret and granting the pod access to the secret. The pod specification in the manifest file specifies that the secret must be “mounted” one way or another (through environment variables or files mounted inside the pod). The API server processes these manifest file requests and enforces the necessary permissions. In this example, the pod can access the secret no matter the RBAC permissions assigned to the pod’s service account. This is because the mounting of such secrets does not depend on RBAC or metadata.\n\nA Kubernetes RBAC rule is made up of three elements which define the API access levels: the API group, a verb (i.e., an action), and a target (either a resource name(s) or an API URL). RBAC rules are specified in roles and cluster roles, and are essential for managing role assignments. The difference between the two is that roles are scoped to a given single namespace, while cluster scoped resources apply to the entire cluster, and often utilize predefined roles or custom roles with different levels of access. When no namespace is specified, resources are created in the default namespace. This is what an RBAC rule looks like, as part of a YAML file specifying a role, including any default roles:\n\nThe API group identifies which API group to target, determining if it is a namespaced resource. This is necessary because different API groups can have the same verbs. Additionally, Kubernetes is highly extensible and allows for adding new APIs with verbs and resource names that clash with other APIs. In the manifest file, this is a list, although usually only one API group is specified here. The API group “” (empty string) represents the core Kubernetes API.\n\nThe verb indicates the action to take. For example: get, list, create, delete, update, etc. Again, this item in the manifest file is a list, which allows for the specification of more than one action, thus avoiding the creation of redundant rules.\n\nThe third part is the verb’s target. It can be an explicitly specified resource, including various types of resources. Examples of resource types are: pod, networkpolicy, service, etc. Additionally, further restrictions can be applied by using resource names or specific fields within those resources.\n\nThe second option for the verb’s target is to specify the URL path. This is the request’s URL path and can contain an ending wildcard, which can be used to give access to certain parts of the API. An RBAC rule must specify a target that is either a namespaced resource or a URL, but not both. Also, note that a URL path can be specified as a target only for cluster roles, as roles are scoped to a single namespace.\n\nThe role in this file has only one RBAC rule, which targets the core API (the empty string in “apiGroups”). The rule allows for the actions “get” and “list” on a secret named “mysupersecret,” providing read only access.\n\nIn this case, the rule gives “get” and “list” access to any request made on the “ ” path. Please note that the role here is a ClusterRole, as only cluster roles can use path-based permissions.\n\nFor a pod access to cloud services, Kubernetes RBAC is not enough because it only manages Kubernetes resources and their credentials. In AWS, for example, service account access must be managed by linking a Kubernetes service account with an IAM role. Achieving this is a somewhat complex process, and AWS has well-documented it. In short, it adds annotations to the service account manifest. These annotations should point to the IAM role the service account should assume for accessing cloud services. This will also help prevent attackers from bypassing RBAC mechanisms through node access. For example:\n\nHowever, a prerequisite is first to create an OIDC provider (one per cluster). Other cloud vendors require similar processes to access their resources.\n\nEffective Kubernetes RBAC management requires adherence to best practices to ensure the security and integrity of the Kubernetes environment, including strong security controls for critical components. Here are some RBAC best practices:\n\nApply the principle of least privilege to accounts to grant the minimum permissions required for each user or group to perform their tasks, which involves assigning roles to users. While applying the same principle to humans is ideal, it can be more challenging to achieve in practice. Reviewing permissions in the kube-system namespace is a key best practice, as this namespace contains critical system components.\n\nCreate a well-defined RBAC strategy, starting with assessing the current state and defining clear goals, including establishing granular roles. Develop a plan to transition from the current state to the desired RBAC policies incorporating clear role definitions, considering the organization’s specific needs.\n\nUse infrastructure-as-code (IAC) or templating tools like Helm charts to manage RBAC policies efficiently. Templating enables use of variables and change tracking, aiding in troubleshooting and auditing processes.\n\nRegularly test RBAC permissions to ensure that entities can perform their necessary actions without being allowed to perform unauthorized actions while also reviewing user permissions. Thorough testing helps identify and rectify vulnerabilities in the system.\n\nPeriodically review existing RBAC and account objects through audit reviews to ensure they are up-to-date and delete unused roles, users, or groups that are no longer needed. This minimizes the attack surface and streamlines RBAC management, especially when considering the kube-system namespace.\n\nKubernetes RBAC policies aren’t static. As mentioned above periodic review and cleanup is a best practice for RBAC. Ensuring no unauthorized access is a key first step in this process.\n\nThe most straightforward way to manage RBAC policies is by using the kubectl command-line tool to view, create, update, and delete them.\n\nHere are a few examples of kubectl commands for viewing RBAC policies and related resources:\n• lists all the roles defined in the cluster, including their associated rules and scope.\n• lists all the role bindings in the entire cluster, detailing how individual users and groups are mapped to specific roles.\n• checks if the current user has the specified permissions (verb) for a specific resource.\n\nIn addition to these commands, there are others to effectively add, remove, and delete RBAC policies across the entire cluster.\n\nAlthough kubectl provides an easy-to-use operational experience, there are some drawbacks, which we discuss below.\n\nKubectl commands are executed for a single cluster only, offering fine-grained controls over access policies within the cluster. This helps to minimize the risk of accidental modifications to other clusters. However, when managing RBAC policies across multiple clusters, tracking which policies have been updated and which ones need to be updated for cluster-wide resources can be challenging.\n\nThe commands mentioned above show the RBAC-related Kubernetes resources in a CLI format, such as a cluster role to watch deployments:\n\nThe output shows the definition of the cluster role with its rules. It lacks contextual information about the level of access granted by each rule, and who is using a role within a cluster. To find that out, it’s necessary to check cluster role bindings and role bindings for cluster-wide permissions and namespace-scoped access.\n\nUsing kubectl commands is insufficient to prevent or detect unauthorized access, as they only provide granular information. What is missing is a broader, contextual understanding of who has access to what resources.\n\nRBAC management becomes more complex as organizations adopt Kubernetes and deploy multiple clusters to manage their containerized workloads. Several challenges arise in managing RBAC across multiple clusters:\n\nManaging granular access control becomes challenging in a dynamic environment with numerous users and roles. Each user or group may require unique permissions, leading to a growing number of roles, making it harder for administrators to keep track of privileges.\n\nAs the number of roles and permissions increases, managing relationships between different roles and permissions within clusters becomes challenging. Without Kubernetes security tools for synchronization, administrators may struggle to comprehend the overall access control system.\n\nCommand-line tools like Kubectl provide granular information about individual roles and bindings but lack a comprehensive view of how different roles are used across the clusters.\n\nImperative RBAC management through kubectl commands lacks continuous synchronization and checks, making it prone to human errors and time-consuming updates.\n\nRBAC management is crucial to securing a Kubernetes cluster and ensuring compliance with regulations and industry standards. However, without the proper RBAC tools in place, managing RBAC can be a complex and error-prone manual process. This is why RBAC visualization is essential. Visualizing RBAC helps administrators keep track of all privileges in a systematic and organized manner. A visual representation of the relationships between roles and bindings allows administrators to understand the policies more easily and thus make informed decisions about policy changes. RBAC investigation is critical to securely managing Kubernetes in production and ensuring security and compliance.\n\nARMO Platform includes an RBAC visualizer allowing administrators to see which privileges are assigned to any given user. With ARMO Platform, administrators can take control of RBAC management and reduce the attack surface by conforming to the principle of least privilege. Try it free today!\n\nQ: What is the difference between role and ClusterRole in Kubernetes?\n\nA: At a high level, roles and RoleBindings are confined to specific namespaces, granting access within those boundaries. In contrast, ClusterRoles and ClusterRoleBindings operate at the cluster-wide level, providing permissions across all namespaces.\n\nQ: What is an example of RBAC?\n\nA: A typical RBAC scenario involves controlling access for various user types such as administrators, specialists, or regular users based on their job roles.\n\nQ: What are the three elements of role-based access control in Kubernetes?\n\nA: The primary components of RBAC include:\n• ClusterRole: Defines permissions that span across the entire cluster or multiple namespaces.\n• Subject: Represents a user, group, or service account to whom permissions are applied.\n\nQ: How can one use Kubernetes RBAC with AKS without leveraging Azure AD?\n\nA: The absence of an identity provider such as Azure AD typically limits certain functionalities. Although alternative approaches might exist, they may not be officially supported or advisable. Collaborating with the team managing identity services to integrate Azure AD for comprehensive RBAC support is recommended.\n\nQ: How can one assign the same RBAC role to two different IAM roles to access a cluster in EKS?\n\nA: To achieve this, specify distinct userNames for each IAM role when configuring the RBAC settings. This approach allows both IAM roles to be associated with the same RBAC role, granting them equivalent permissions within the cluster."
    },
    {
        "link": "https://docs.k8slens.dev/getting-started/add-cluster",
        "document": "Lens Desktop automatically detects clusters from the specified files. You can find available clusters in Navigator.\n\nIn some cases, Lens Desktop might not detect or fail to connect to a . Make sure that your cluster tools, such as , , , , are up-to-date. After updating the tools, re-generate the kubeconfigs that no longer work with Lens Desktop. For example, configure kubectl, so that you can connect to an Amazon EKS cluster:\n\nAs another example, Kubernetes clusters older than version may require updating the field from to client.authentication.k8s.io/v1beta1. Please review your and adjust the configurations accordingly.\n\nTo specify files, click Add Kubeconfigs in Kubernetes Clusters > Local Kubeconfigs in Navigator. Select one of the following options:\n• Paste the cluster configurations into an input field of the modal. Each instance of pasting a cluster creates a file in the application data folder: .\n• Select the file from the system menu.\n\nTo see your current cluster configuration:\n• In the Terminal tab, execute the following command: If successfull, the cluster configuration is displayed in Terminal.\n\nWhen connecting to a cluster, make sure you have a valid and working for the cluster.\n\nRight-click on Local Kubeconfigs and select Manage Kubeconfigs to open Preferences > Kubernetes. In this menu you can configure kubectl, specify kubeconfig files, and manage Helm chart repositories.\n\nTo add a cluster to your team space, see Add a cluster to a team space.\n\nTo connect to a local or a team space cluster, right-click on the cluster and select Connect."
    },
    {
        "link": "https://k8slens.dev/blog/how-to-add-a-cluster-to-lens-a-step-by-step-guide",
        "document": "Adding your Kubernetes cluster to Lens is a straightforward process that enables you to manage and monitor your workloads with ease. This guide will walk you through each step in detail to ensure a smooth setup. Follow these steps to get started:\n\nFirst, open the Lens application on your desktop. If you haven't installed Lens yet, download it from the Lens website. Installation is quick and simple, ensuring you can start managing your clusters without delay.\n\nOn the Lens home screen, you’ll find several options for managing your clusters and resources. Click on \"Browse Your Local Catalog.\" The local catalog is a central hub where you can manage all your local resources and add new clusters. This feature is designed to streamline the process of resource management, making it easier to keep track of your Kubernetes environments.\n\nOnce you are in the local catalog, navigate to the \"Clusters\" tab. This section is specifically dedicated to your Kubernetes clusters. Here, you will see an overview of all clusters that have been added to Lens. This tab is crucial as it provides a centralized view, allowing you to manage multiple clusters from a single interface.\n\n\n\nAdding your Kubernetes cluster to Lens can be done in a few different ways. Lens offers flexibility to suit different user preferences and configurations:\n\nAutomatic Detection: Lens has a built-in feature that automatically detects and syncs kubeconfig files from your local machine. This method is convenient if you have your kubeconfig files saved in the default locations. Lens will scan these locations and automatically add the clusters it finds.\n• Click on the ‘+’ sign located in the Clusters tab. This action will open a dialog box with different options for adding your cluster.\n• Manually Copy and Paste Your Kubeconfig File Content: If you prefer to manually manage your configurations, you can copy the kubeconfig file content and paste it into the provided text area. This method gives you full control over which configurations are being added.\n• Sync Kubeconfig File: Another option is to click \"Sync Kubeconfig File,\" which allows you to browse your local files and select the kubeconfig file manually. This method is useful if your kubeconfig file is stored in a custom directory.\n\nDetailed instructions and screenshots for these methods can be found in the Lens documentation. Each method is designed to be user-friendly and to accommodate different levels of technical expertise.\n\nOnce your cluster is connected, Lens provides a comprehensive overview of your cluster. This includes namespaces, workloads, and resources. The overview screen is designed to give you a snapshot of your cluster's health and status. You can use the sidebar to navigate through different resources, such as Pods, Deployments, Services, ConfigMaps, Secrets, and more. Each section provides detailed information and management options, making it easier to troubleshoot and optimize your Kubernetes environments.\n\n\n\nAdditional Features and Tips for Using Lens\n\nIn the Workload Panel, you can monitor your workloads and deployments and see logs for real-time insights. This panel is specifically designed for managing and troubleshooting workloads, providing a central location to oversee the status and performance of your applications. For troubleshooting your workloads and deployments, the Workload Panel is your go-to tool.\n\nFor advanced users, Lens Pro offers additional premium features that enhance the Kubernetes management experience:\n• Lens Desktop (LDK): Provides a more robust desktop experience with additional functionalities and integrations.\n• Team Collaboration: Share clusters and resources with team members to collaborate effectively.\n\nIf you encounter any issues or have questions, there are several resources available to assist you:\n• Visit our for more detailed instructions and troubleshooting tips. The documentation is comprehensive and regularly updated, ensuring you have access to the latest information and best practices.\n• Join our community on for help from fellow users and our support team. The forum is a great place to ask questions, share experiences, and learn from others who are using Lens.\n\nBy following these steps, you’ll be able to add your Kubernetes cluster to Lens and take full advantage of its powerful management and monitoring features. Lens is designed to simplify Kubernetes management, making it accessible and efficient for users of all levels.\n\nStart exploring the full potential of your workloads with Lens today!"
    },
    {
        "link": "https://kubernetes.io/docs/reference/access-authn-authz/authentication",
        "document": "This page provides an overview of authentication.\n\nAll Kubernetes clusters have two categories of users: service accounts managed by Kubernetes, and normal users.\n\nIt is assumed that a cluster-independent service manages normal users in the following ways:\n• a file with a list of usernames and passwords\n\nIn this regard, Kubernetes does not have objects which represent normal user accounts. Normal users cannot be added to a cluster through an API call.\n\nEven though a normal user cannot be added via an API call, any user that presents a valid certificate signed by the cluster's certificate authority (CA) is considered authenticated. In this configuration, Kubernetes determines the username from the common name field in the 'subject' of the cert (e.g., \"/CN=bob\"). From there, the role based access control (RBAC) sub-system would determine whether the user is authorized to perform a specific operation on a resource.\n\nIn contrast, service accounts are users managed by the Kubernetes API. They are bound to specific namespaces, and created automatically by the API server or manually through API calls. Service accounts are tied to a set of credentials stored as , which are mounted into pods allowing in-cluster processes to talk to the Kubernetes API.\n\nAPI requests are tied to either a normal user or a service account, or are treated as anonymous requests. This means every process inside or outside the cluster, from a human user typing on a workstation, to on nodes, to members of the control plane, must authenticate when making requests to the API server, or be treated as an anonymous user.\n\nKubernetes uses client certificates, bearer tokens, or an authenticating proxy to authenticate API requests through authentication plugins. As HTTP requests are made to the API server, plugins attempt to associate the following attributes with the request:\n• Username: a string which identifies the end user. Common values might be or .\n• UID: a string which identifies the end user and attempts to be more consistent and unique than username.\n• Groups: a set of strings, each of which indicates the user's membership in a named logical collection of users. Common values might be or .\n• Extra fields: a map of strings to list of strings which holds additional information authorizers may find useful.\n\nAll values are opaque to the authentication system and only hold significance when interpreted by an authorizer.\n\nYou can enable multiple authentication methods at once. You should usually use at least two methods:\n• at least one other method for user authentication.\n\nWhen multiple authenticator modules are enabled, the first module to successfully authenticate the request short-circuits evaluation. The API server does not guarantee the order authenticators run in.\n\nThe group is included in the list of groups for all authenticated users.\n\nIntegrations with other authentication protocols (LDAP, SAML, Kerberos, alternate x509 schemes, etc) can be accomplished using an authenticating proxy or the authentication webhook.\n\nClient certificate authentication is enabled by passing the option to API server. The referenced file must contain one or more certificate authorities to use to validate client certificates presented to the API server. If a client certificate is presented and verified, the common name of the subject is used as the user name for the request. As of Kubernetes 1.4, client certificates can also indicate a user's group memberships using the certificate's organization fields. To include multiple group memberships for a user, include multiple organization fields in the certificate.\n\nFor example, using the command line tool to generate a certificate signing request:\n\nThis would create a CSR for the username \"jbeda\", belonging to two groups, \"app1\" and \"app2\".\n\nSee Managing Certificates for how to generate a client cert.\n\nThe API server reads bearer tokens from a file when given the option on the command line. Currently, tokens last indefinitely, and the token list cannot be changed without restarting the API server.\n\nThe token file is a csv file with a minimum of 3 columns: token, user name, user uid, followed by optional group names.\n\nWhen using bearer token authentication from an http client, the API server expects an header with a value of . The bearer token must be a character sequence that can be put in an HTTP header value using no more than the encoding and quoting facilities of HTTP. For example: if the bearer token is then it would appear in an HTTP header as shown below.\n\nTo allow for streamlined bootstrapping for new clusters, Kubernetes includes a dynamically-managed Bearer token type called a Bootstrap Token. These tokens are stored as Secrets in the namespace, where they can be dynamically managed and created. Controller Manager contains a TokenCleaner controller that deletes bootstrap tokens as they expire.\n\nThe tokens are of the form . The first component is a Token ID and the second component is the Token Secret. You specify the token in an HTTP header as follows:\n\nYou must enable the Bootstrap Token Authenticator with the flag on the API Server. You must enable the TokenCleaner controller via the flag on the Controller Manager. This is done with something like . will do this for you if you are using it to bootstrap a cluster.\n\nThe authenticator authenticates as . It is included in the group. The naming and groups are intentionally limited to discourage users from using these tokens past bootstrapping. The user names and group can be used (and are used by ) to craft the appropriate authorization policies to support bootstrapping a cluster.\n\nPlease see Bootstrap Tokens for in depth documentation on the Bootstrap Token authenticator and controllers along with how to manage these tokens with .\n\nA service account is an automatically enabled authenticator that uses signed bearer tokens to verify requests. The plugin takes two optional flags:\n• File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to verify ServiceAccount tokens. The specified file can contain multiple keys, and the flag can be specified multiple times with different files. If unspecified, --tls-private-key-file is used.\n• If enabled, tokens which are deleted from the API will be revoked.\n\nService accounts are usually created automatically by the API server and associated with pods running in the cluster through the Admission Controller. Bearer tokens are mounted into pods at well-known locations, and allow in-cluster processes to talk to the API server. Accounts may be explicitly associated with pods using the field of a .\n\nis usually omitted because this is done automatically.\n\nService account bearer tokens are perfectly valid to use outside the cluster and can be used to create identities for long standing jobs that wish to talk to the Kubernetes API. To manually create a service account, use the command. This creates a service account in the current namespace.\n\nThe signed JWT can be used as a bearer token to authenticate as the given service account. See above for how the token is included in a request. Normally these tokens are mounted into pods for in-cluster access to the API server, but can be used from outside the cluster as well.\n\nService accounts authenticate with the username , and are assigned to the groups and .\n\nBecause service account tokens can also be stored in Secret API objects, any user with write access to Secrets can request a token, and any user with read access to those Secrets can authenticate as the service account. Be cautious when granting permissions to service accounts and read or write capabilities for Secrets.\n\nOpenID Connect is a flavor of OAuth2 supported by some OAuth2 providers, notably Microsoft Entra ID, Salesforce, and Google. The protocol's main extension of OAuth2 is an additional field returned with the access token called an ID Token. This token is a JSON Web Token (JWT) with well known fields, such as a user's email, signed by the server.\n\nTo identify the user, the authenticator uses the (not the ) from the OAuth2 token response as a bearer token. See above for how the token is included in a request.\n• None Log in to your identity provider\n• None Your identity provider will provide you with an , and a\n• None When using , use your with the flag or add it directly to your\n• None sends your in a header called Authorization to the API server\n• None The API server will make sure the JWT signature is valid\n• None Check to make sure the hasn't expired Perform claim and/or user validation if CEL expressions are configured with .\n• None Make sure the user is authorized\n• None Once authorized the API server returns a response to\n• None provides feedback to the user\n\nSince all of the data needed to validate who you are is in the , Kubernetes doesn't need to \"phone home\" to the identity provider. In a model where every request is stateless this provides a very scalable solution for authentication. It does offer a few challenges:\n• Kubernetes has no \"web interface\" to trigger the authentication process. There is no browser or interface to collect credentials which is why you need to authenticate to your identity provider first.\n• The can't be revoked, it's like a certificate so it should be short-lived (only a few minutes) so it can be very annoying to have to get a new token every few minutes.\n• To authenticate to the Kubernetes dashboard, you must use the command or a reverse proxy that injects the .\n\nTo enable the plugin, configure the following flags on the API server:\n\nJWT Authenticator is an authenticator to authenticate Kubernetes users using JWT compliant tokens. The authenticator will attempt to parse a raw ID token, verify it's been signed by the configured issuer. The public key to verify the signature is discovered from the issuer's public endpoint using OIDC discovery.\n\nThe minimum valid JWT payload must contain the following claims:\n\nThe configuration file approach allows you to configure multiple JWT authenticators, each with a unique and . The configuration file even allows you to specify CEL expressions to map claims to user attributes, and to validate claims and user information. The API server also automatically reloads the authenticators when the configuration file is modified. You can use metric to monitor the last time the configuration was reloaded by the API server.\n\nYou must specify the path to the authentication configuration using the flag on the API server. If you want to use command line flags instead of the configuration file, those will continue to work as-is. To access the new capabilities like configuring multiple authenticators, setting multiple audiences for an issuer, switch to using the configuration file.\n\nFor Kubernetes v1.32, the structured authentication configuration file format is beta-level, and the mechanism for using that configuration is also beta. Provided you didn't specifically disable the feature gate for your cluster, you can turn on structured authentication by specifying the command line argument to the kube-apiserver. An example of the structured authentication configuration file is shown below.\n\nIf you specify along with any of the command line arguments, this is a misconfiguration. In this situation, the API server reports an error and then immediately exits. If you want to switch to using structured authentication configuration, you have to remove the command line arguments, and use the configuration file instead.\n\n# CAUTION: this is an example configuration. # Do not use this for your own cluster! # list of authenticators to authenticate Kubernetes users using JWT compliant tokens. # the maximum number of allowed authenticators is 64. # url must be unique across all authenticators. # url must not conflict with issuer configured in --service-account-issuer. # discoveryURL, if specified, overrides the URL used to fetch discovery # information instead of using \"{url}/.well-known/openid-configuration\". # The exact value specified is used, so \"/.well-known/openid-configuration\" # must be included in discoveryURL if needed. # The \"issuer\" field in the fetched discovery information must match the \"issuer.url\" field # in the AuthenticationConfiguration and will be used to validate the \"iss\" claim in the presented JWT. # This is for scenarios where the well-known and jwks endpoints are hosted at a different # location than the issuer (such as locally in the cluster). # discoveryURL must be different from url if specified and must be unique across all authenticators. # PEM encoded CA certificates used to validate the connection when fetching # discovery information. If not set, the system verifier will be used. # Same value as the content of the file referenced by the --oidc-ca-file flag. # audiences is the set of acceptable audiences the JWT must be issued to. # At least one of the entries must match the \"aud\" claim in presented JWTs. # this is required to be set to \"MatchAny\" when multiple audiences are specified. # Instead of claim and requiredValue, you can use expression to validate the claim. # expression is a CEL expression that evaluates to a boolean. # all the expressions must evaluate to true for validation to succeed. # Message customizes the error message seen in the API server logs when the validation fails. : the hd claim must be set to example.com # username represents an option for the username attribute. # This is the only required attribute. # Same as --oidc-username-claim. Mutually exclusive with username.expression. # Same as --oidc-username-prefix. Mutually exclusive with username.expression. # if username.claim is set, username.prefix is required. # Explicitly set it to \"\" if no prefix is desired. # expression is a CEL expression that evaluates to a string. # 1. If username.expression uses 'claims.email', then 'claims.email_verified' must be used in # An example claim validation rule expression that matches the validation automatically # applied when username.claim is set to 'email' is 'claims.?email_verified.orValue(true) == true'. # By explicitly comparing the value to true, we let type-checking see the result will be a boolean, and # to make sure a non-boolean email_verified claim will be caught at runtime. # 2. If the username asserted based on username.expression is the empty string, the authentication # groups represents an option for the groups attribute. # Same as --oidc-groups-claim. Mutually exclusive with groups.expression. # Same as --oidc-groups-prefix. Mutually exclusive with groups.expression. # if groups.claim is set, groups.prefix is required. # Explicitly set it to \"\" if no prefix is desired. # expression is a CEL expression that evaluates to a string or a list of strings. # uid represents an option for the uid attribute. # expression is a CEL expression that evaluates to a string. # extra attributes to be added to the UserInfo object. Keys must be domain-prefix path and must be unique. # key is a string to use as the extra attribute key. # key must be a domain-prefix path (e.g. example.org/foo). All characters before the first \"/\" must be a valid # subdomain as defined by RFC 1123. All characters trailing the first \"/\" must # be valid HTTP Path characters as defined by RFC 3986. # k8s.io, kubernetes.io and their subdomains are reserved for Kubernetes use and cannot be used. # key must be lowercase and unique across all extra attributes. # valueExpression is a CEL expression that evaluates to a string or a list of strings. # expression is a CEL expression that evaluates to a boolean. # all the expressions must evaluate to true for the user to be valid. # Message customizes the error message seen in the API server logs when the validation fails.\n• None represents the expression which will be evaluated by CEL. CEL expressions have access to the contents of the token payload, organized into CEL variable. is a map of claim names (as strings) to claim values (of any type).\n• None represents the expression which will be evaluated by CEL. CEL expressions have access to the contents of , organized into CEL variable. Refer to the UserInfo API documentation for the schema of .\n• None , , represents the expression which will be evaluated by CEL. CEL expressions have access to the contents of the token payload, organized into CEL variable. is a map of claim names (as strings) to claim values (of any type). To learn more, see the Documentation on CEL Here are examples of the with different token payloads. - : # the expression will evaluate to true, so validation will succeed. where the token payload is: The token with the above will produce the following object and successfully authenticate the user. - : # the token below does not have this claim, so validation will fail. : the hd claim must be set to example.com - : # the expression will evaluate to true, so validation will succeed. where the token payload is: The token with the above will fail to authenticate because the claim is not set to . The API server will return error. : the hd claim must be set to example.com : # this will prefix the username with \"system:\" and will fail user validation. - : # the username will be system:foo and expression will evaluate to false, so validation will fail. where the token payload is: The token with the above will produce the following object: which will fail user validation because the username starts with . The API server will return error.\n• Distributed claims do not work via CEL expressions.\n• Egress selector configuration is not supported for calls to and .\n\nKubernetes does not provide an OpenID Connect Identity Provider. You can use an existing public OpenID Connect Identity Provider (such as Google, or others). Or, you can run your own Identity Provider, such as dex, Keycloak, CloudFoundry UAA, or Tremolo Security's OpenUnison.\n\nFor an identity provider to work with Kubernetes it must:\n• None The public key to verify the signature is discovered from the issuer's public endpoint using OIDC discovery. If you're using the authentication configuration file, the identity provider doesn't need to publicly expose the discovery endpoint. You can host the discovery endpoint at a different location than the issuer (such as locally in the cluster) and specify the in the configuration file.\n• None Have a CA signed certificate (even if the CA is not a commercial CA or is self signed)\n\nA note about requirement #3 above, requiring a CA signed certificate. If you deploy your own identity provider (as opposed to one of the cloud providers like Google or Microsoft) you MUST have your identity provider's web server certificate signed by a certificate with the flag set to , even if it is self signed. This is due to GoLang's TLS client implementation being very strict to the standards around certificate validation. If you don't have a CA handy, you can use the gencert script from the Dex team to create a simple CA and a signed certificate and key pair. Or you can use this similar script that generates SHA256 certs with a longer life and larger key size.\n\nThe first option is to use the kubectl authenticator, which sets the as a bearer token for all requests and refreshes the token once it expires. After you've logged into your provider, use kubectl to add your , , , and to configure the plugin.\n\nProviders that don't return an as part of their refresh token response aren't supported by this plugin and should use \"Option 2\" below.\n\nAs an example, running the below command after authenticating to your identity provider:\n\nWhich would produce the below configuration:\n\nOnce your expires, will attempt to refresh your using your and storing the new values for the and in your .\n\nThe command lets you pass in a token using the option. Copy and paste the into this option:\n• a configuration file describing how to access the remote webhook service.\n• how long to cache authentication decisions. Defaults to two minutes.\n• determines whether to use or objects to send/receive information from the webhook. Defaults to .\n\nThe configuration file uses the kubeconfig file format. Within the file, refers to the remote service and refers to the API server webhook. An example would be:\n\nWhen a client attempts to authenticate with the API server using a bearer token as discussed above, the authentication webhook POSTs a JSON-serialized object containing the token to the remote service.\n\nNote that webhook API objects are subject to the same versioning compatibility rules as other Kubernetes API objects. Implementers should check the field of the request to ensure correct deserialization, and must respond with a object of the same version as the request.\n\nThe remote service is expected to fill the field of the request to indicate the success of the login. The response body's field is ignored and may be omitted. The remote service must return a response using the same API version that it received. A successful validation of the bearer token would return:\n\nThe API server can be configured to identify users from request header values, such as . It is designed for use in combination with an authenticating proxy, which sets the request header value.\n• Required, case-insensitive. Header names to check, in order, for the user identity. The first header containing a value is used as the username.\n• 1.6+. Optional, case-insensitive. \"X-Remote-Group\" is suggested. Header names to check, in order, for the user's groups. All values in all specified headers are used as group names.\n• 1.6+. Optional, case-insensitive. \"X-Remote-Extra-\" is suggested. Header prefixes to look for to determine extra information about the user (typically used by the configured authorization plugin). Any headers beginning with any of the specified prefixes have the prefix removed. The remainder of the header name is lowercased and percent-decoded and becomes the extra key, and the header value is the extra value.\n\nFor example, with this configuration:\n\nwould result in this user info:\n\nIn order to prevent header spoofing, the authenticating proxy is required to present a valid client certificate to the API server for validation against the specified CA before the request headers are checked. WARNING: do not reuse a CA that is used in a different context unless you understand the risks and the mechanisms to protect the CA's usage.\n• Required. PEM-encoded certificate bundle. A valid client certificate must be presented and validated against the certificate authorities in the specified file before the request headers are checked for user names.\n• Optional. List of Common Name values (CNs). If set, a valid client certificate with a CN in the specified list must be presented before the request headers are checked for user names. If empty, any CN is allowed.\n\nWhen enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests, and given a username of and a group of .\n\nFor example, on a server with token authentication configured, and anonymous access enabled, a request providing an invalid bearer token would receive a error. A request providing no bearer token would be treated as an anonymous request.\n\nIn 1.5.1-1.5.x, anonymous access is disabled by default, and can be enabled by passing the option to the API server.\n\nIn 1.6+, anonymous access is enabled by default if an authorization mode other than is used, and can be disabled by passing the option to the API server. Starting in 1.6, the ABAC and RBAC authorizers require explicit authorization of the user or the group, so legacy policy rules that grant access to the user or group do not include anonymous users.\n\nThe can be used to configure the anonymous authenticator. If you set the anonymous field in the file then you cannot set the flag.\n\nThe main advantage of configuring anonymous authenticator using the authentication configuration file is that in addition to enabling and disabling anonymous authentication you can also configure which endpoints support anonymous authentication.\n\nIn the configuration above only the , and endpoints are reachable by anonymous requests. Any other endpoints will not be reachable even if it is allowed by RBAC configuration.\n\nA user can act as another user through impersonation headers. These let requests manually override the user info a request authenticates as. For example, an admin could use this feature to debug an authorization policy by temporarily impersonating another user and seeing if a request was denied.\n\nImpersonation requests first authenticate as the requesting user, then switch to the impersonated user info.\n• A user makes an API call with their credentials and impersonation headers.\n\nThe following HTTP headers can be used to performing an impersonation request:\n• : The username to act as.\n• : A group name to act as. Can be provided multiple times to set multiple groups. Optional. Requires \"Impersonate-User\".\n• : A dynamic header used to associate extra fields with the user. Optional. Requires \"Impersonate-User\". In order to be preserved consistently, must be lower-case, and any characters which aren't legal in HTTP header labels MUST be utf8 and percent-encoded.\n• : A unique identifier that represents the user being impersonated. Optional. Requires \"Impersonate-User\". Kubernetes does not impose any format requirements on this string.\n\nis only available in versions 1.22.0 and higher.\n\nAn example of the impersonation headers used when impersonating a user with groups:\n\nAn example of the impersonation headers used when impersonating a user with a UID and extra fields:\n\nWhen using set the flag to configure the header, set the flag to configure the header.\n\nTo impersonate a user, group, user identifier (UID) or extra fields, the impersonating user must have the ability to perform the \"impersonate\" verb on the kind of attribute being impersonated (\"user\", \"group\", \"uid\", etc.). For clusters that enable the RBAC authorization plugin, the following ClusterRole encompasses the rules needed to set user and group impersonation headers:\n\nFor impersonation, extra fields and impersonated UIDs are both under the \"authentication.k8s.io\" . Extra fields are evaluated as sub-resources of the resource \"userextras\". To allow a user to use impersonation headers for the extra field \"scopes\" and for UIDs, a user should be granted the following role:\n\nThe values of impersonation headers can also be restricted by limiting the set of a resource can take.\n\nImpersonating a user or group allows you to perform any action as if you were that user or group; for that reason, impersonation is not namespace scoped. If you want to allow impersonation using Kubernetes RBAC, this requires using a and a , not a and .\n\nand tools using it such as and are able to execute an external command to receive user credentials.\n\nThis feature is intended for client side integrations with authentication protocols not natively supported by (LDAP, Kerberos, OAuth2, SAML, etc.). The plugin implements the protocol specific logic, then returns opaque credentials to use. Almost all credential plugin use cases require a server side component with support for the webhook token authenticator to interpret the credential format produced by the client plugin.\n\nEarlier versions of included built-in support for authenticating to AKS and GKE, but this is no longer present.\n\nIn a hypothetical use case, an organization would run an external service that exchanges LDAP credentials for user specific, signed tokens. The service would also be capable of responding to webhook token authenticator requests to validate the tokens. Users would be required to install a credential plugin on their workstation.\n\nTo authenticate against the API:\n• Credential plugin prompts the user for LDAP credentials, exchanges credentials with external service for a token.\n• Credential plugin returns token to client-go, which uses it as a bearer token against the API server.\n• API server uses the webhook token authenticator to submit a to the external service.\n• External service verifies the signature on the token and returns the user's username and groups.\n\nCredential plugins are configured through kubectl config files as part of the user fields.\n\n# API version to use when decoding the ExecCredentials resource. Required. # The API version returned by the plugin MUST match the version listed here. # To integrate with tools that support multiple versions (such as client.authentication.k8s.io/v1beta1), # set an environment variable, pass an argument to the tool that indicates which version the exec plugin expects, # or read the version from the ExecCredential object in the KUBERNETES_EXEC_INFO environment variable. # Environment variables to set when executing the plugin. Optional. # Arguments to pass when executing the plugin. Optional. # Text shown to the user when the executable doesn't seem to be present. Optional. to the current cluster. It can be installed: # Whether or not to provide cluster information, which could potentially contain # very large CA data, to this exec plugin as a part of the KUBERNETES_EXEC_INFO # The contract between the exec plugin and the standard input I/O stream. If the # contract cannot be satisfied, this plugin will not be run and an error will be # returned. Valid values are \"Never\" (this exec plugin never uses standard input), # \"IfAvailable\" (this exec plugin wants to use standard input if it is available), # or \"Always\" (this exec plugin requires standard input to function). Required. - : client.authentication.k8s.io/exec # reserved extension name for per cluster exec config : can be provided via the KUBERNETES_EXEC_INFO environment variable upon setting provideClusterInfo # API version to use when decoding the ExecCredentials resource. Required. # The API version returned by the plugin MUST match the version listed here. # To integrate with tools that support multiple versions (such as client.authentication.k8s.io/v1), # set an environment variable, pass an argument to the tool that indicates which version the exec plugin expects, # or read the version from the ExecCredential object in the KUBERNETES_EXEC_INFO environment variable. # Environment variables to set when executing the plugin. Optional. # Arguments to pass when executing the plugin. Optional. # Text shown to the user when the executable doesn't seem to be present. Optional. to the current cluster. It can be installed: # Whether or not to provide cluster information, which could potentially contain # very large CA data, to this exec plugin as a part of the KUBERNETES_EXEC_INFO # The contract between the exec plugin and the standard input I/O stream. If the # contract cannot be satisfied, this plugin will not be run and an error will be # returned. Valid values are \"Never\" (this exec plugin never uses standard input), # \"IfAvailable\" (this exec plugin wants to use standard input if it is available), # or \"Always\" (this exec plugin requires standard input to function). Optional. - : client.authentication.k8s.io/exec # reserved extension name for per cluster exec config : can be provided via the KUBERNETES_EXEC_INFO environment variable upon setting provideClusterInfo\n\nRelative command paths are interpreted as relative to the directory of the config file. If KUBECONFIG is set to and the exec command is , the binary is executed.\n\nThe executed command prints an object to . authenticates against the Kubernetes API using the returned credentials in the . The executed command is passed an object as input via the environment variable. This input contains helpful information like the expected API version of the returned object and whether or not the plugin can use to interact with the user.\n\nWhen run from an interactive session (i.e., a terminal), can be exposed directly to the plugin. Plugins should use the field of the input object from the environment variable in order to determine if has been provided. A plugin's requirements (i.e., whether is optional, strictly required, or never used in order for the plugin to run successfully) is declared via the field in the kubeconfig (see table below for valid values). The field is optional in and required in .\n\nTo use bearer token credentials, the plugin returns a token in the status of the\n\nAlternatively, a PEM-encoded client certificate and key can be returned to use TLS client auth. If the plugin returns a different certificate and key on a subsequent call, will close existing connections with the server to force a new TLS handshake.\n\nIf specified, and must both must be present.\n\nmay contain additional intermediate certificates to send to the server.\n\nOptionally, the response can include the expiry of the credential formatted as a RFC 3339 timestamp.\n\nPresence or absence of an expiry has the following impact:\n• If an expiry is included, the bearer token and TLS credentials are cached until the expiry time is reached, or if the server responds with a 401 HTTP status code, or when the process exits.\n• If an expiry is omitted, the bearer token and TLS credentials are cached until the server responds with a 401 HTTP status code or until the process exits.\n\nTo enable the exec plugin to obtain cluster-specific information, set on the field in the kubeconfig. The plugin will then be supplied this cluster-specific information in the environment variable. Information from this environment variable can be used to perform cluster-specific credential acquisition logic. The following manifest describes a cluster information sample.\n\nIf your cluster has the API enabled, you can use the API to find out how your Kubernetes cluster maps your authentication information to identify you as a client. This works whether you are authenticating as a user (typically representing a real person) or as a ServiceAccount.\n\nobjects do not have any configurable fields. On receiving a request, the Kubernetes API server fills the status with the user attributes and returns it to the user.\n\nRequest example (the body would be a ):\n\nFor convenience, the command is present. Executing this command will produce the following output (yet different user attributes will be shown):\n\nBy providing the output flag, it is also possible to print the JSON or YAML representation of the result:\n\nThis feature is extremely useful when a complicated authentication flow is used in a Kubernetes cluster, for example, if you use webhook token authentication or authenticating proxy.\n\nBy default, all authenticated users can create objects when the feature is enabled. It is allowed by the cluster role.\n• To learn about issuing certificates for users, read Issue a Certificate for a Kubernetes API Client Using A CertificateSigningRequest"
    },
    {
        "link": "https://docs.digitalocean.com/products/kubernetes/how-to/connect-to-cluster",
        "document": "DigitalOcean Kubernetes (DOKS) is a managed Kubernetes service. Deploy Kubernetes clusters with a fully managed control plane, high availability, autoscaling, and native integration with DigitalOcean Load Balancers and volumes. You can add node pools using shared and dedicated CPUs, and NVIDIA H100 GPUs in a single GPU or 8 GPU configuration. DOKS clusters are compatible with standard Kubernetes toolchains and the DigitalOcean API and CLI.\n\nDigitalOcean Kubernetes clusters are typically managed from a local machine or sometimes from a remote management server. In either case, the management machine needs two things:\n• , the official Kubernetes command-line tool, to connect to and interact with the cluster. The Kubernetes project provides installation instructions for on a variety of platforms. Use to verify that your installation is working and within one minor version of your cluster.\n• , the official DigitalOcean command-line tool, to manage config files and set context. The GitHub repo has instructions for installing .\n\nGet an Authentication Token or Certificate\n\nAfter creating a cluster, you need to add an authentication token or certificate to your configuration file to connect.\n\nWhen connecting to these Kubernetes versions, generating credentials creates a revocable OAuth token. (If using , as recommended, you must also have version or higher installed to obtain an OAuth token.)\n• Any release of Kubernetes after version .\n\nIf you are not running these versions of Kubernetes, or are using a legacy version of , you will be granted a certificate instead.\n\nTo configure authentication from the command line, use the following command, substituting the name of your cluster.\n\nThis downloads the for the cluster, merges it with any existing configuration from , and automatically handles the authentication token or certificate.\n\nUnder the hood, this automatically generates a revocable OAuth token when using recent versions of Kubernetes and , and automatically renews a certificate with legacy versions:\n• Revocable OAuth token. If you meet the version requirements listed above, you’ll obtain an OAuth token. You can view and revoke this token in the Applications & API section of the control panel.\n• Automatic certificate renewal. With legacy versions of or Kubernetes, this creates a certificate that is valid for seven days, renews automatically, and cannot be revoked. You can upgrade Kubernetes clusters to newer patch versions and minor versions to use tokens instead.\n\nThere is also a cluster configuration file you can download manually from the control panel.\n\nClick the name of the cluster to go to its Overview tab. In the Configuration section, click Download Config File to download its file. The file is named . Put this file in your directory, and pass it to with the flag. For example:\n\nThis generates a revocable OAuth token when using recent versions of Kubernetes and generates a certificate for legacy versions:\n• Revocable OAuth token. If you meet the version requirements listed above, you’ll obtain an OAuth token. You can view and revoke this token in the Applications & API section of the control panel.\n• Expiring certificate. With legacy versions of Kubernetes, this creates a certificate that is valid for 7 days that cannot be revoked. Download the file again every 7 days to retain access to the cluster. You can upgrade Kubernetes clusters to newer patch versions and minor versions to use tokens instead.\n\nOnce the cluster configuration file is in place, you can create, manage, and deploy clusters using . See the official documentation to learn more about its commands and options.\n\nFrom here, you can also add DigitalOcean Load Balancers and add volumes to your cluster.\n\nIn Kubernetes, a context is used to group access parameters under a convenient name. The configuration for every cluster will contain a stanza for contexts with cluster-specific values which look like this:\n\nWhen you use , the commands you run affect the default context unless you specify a different one with the flag (for example, ).\n\nTo check the current default context, use:\n\nIf you get a error, you need to set a default context.\n\nTo list all available contexts, use:\n\nThe terminal returns output that looks like this:\n\nThe default context is specified with an asterisk under “CURRENT”. To set the default context to a different one, use:\n\nIn Kubernetes, namespaces are a way to divide cluster resources between multiple users. They’re useful when you have many users working on the same cluster. You can create multiple namespaces in a cluster, and resources in one namespace are hidden from other namespaces.\n\nLearn more in the Kubernetes namespaces walk-through."
    },
    {
        "link": "https://github.com/lensapp/lens/issues/200",
        "document": "My K8s cluster is on AWS and I use OKTA to login to AWS, so doing any require an additional manual step where I get a one time AWS token via OKTA, I have to export this token before my\n\nIs there any workaround for using Lens with this authentication mechanism ?"
    }
]