[
    {
        "link": "https://nature.com/articles/s41586-023-06337-5",
        "document": "Our experimental results were measured on chips built from 300-mm wafers with a 14-nm complementary metal-oxide-semiconductor front end, fabricated at an external foundry. PCM devices were added in the ‘back-end-of-line’ at the IBM Albany NanoTech Center. Mushroom-cell PCM devices were built with a ring heater with a diameter of approximately 35 nm and a height of around 50 nm (Fig. 1e) as the bottom electrode, a doped Ge Sb Te layer and a top electrode. Wafer characterization before packaging was performed on both 1-resistor macros and 1,024 × 2,048 array diagnostic monitors with on-chip sense amplifiers. After selection of high-yield dies, the wafer was diced and packaged into testable modules at IBM Bromont, as shown in Extended Data Fig. 1a,b.\n\nExperiments were run by mounting the module on a socket connected to a custom-designed board driven by three Xilinx Virtex-7 VC707 field-programmable gate arrays (FPGAs) (Extended Data Fig. 1c). Four Keysight E36312A power supplies were used to power up the boards and the chip. In addition to the 1.5 V, 0.8 V and 1.8 V supplies mentioned in the main text, a 3.0 V power supply was provided but only during PCM device programming (not during inference). Finally, a supply of 0.75 V precharged the peripheral capacitors and set the lower limit for the on-chip digital-to-analog converters (DACs) used in PCM programming, and 0.3 V set the PCM read voltage and the ramp start voltage. These supplies were measured and reported in Fig. 6a as ‘Other’ voltage supplies. The three FPGAs were connected through the custom board and controlled by an x86 machine with a Peripheral Component Interconnect Express connector. All experiments were run using Xilinx MicroBlaze Soft Processor code and x86 MATLAB software wrapper (Extended Data Fig. 1c).\n\nThe off-chip combined transfer bandwidth on our chip is 38.4 Gbps, with a total of 384 input–output pins capable of operating at 100 MHz. Extended Data Fig. 1d shows that routing precision, KWS and RNNT power measurements were run without any additional intermediate data being sent back to the x86 machine. The RNNT accuracy results used the x86 for vector–vector operations and tile calibration. To model such digital operations in terms of performance, we simulated a digital circuitry just outside the ILP–OLP, based on a foundry 14-nm process design kit to implement optimized digital pipelines, control logic and registers. A future chip will eventually include the digital circuitry close to the analog tiles20.\n\nInputs were encoded as 8-bit digital words stored on an SRAM within each ILP. Conversion of 512 such digital words to 512 PWM durations was performed using clock-driven counter circuitry within each ILP. Data were then retrieved from the chip using the OLP, which internally performed the conversion from time to digital using 512 counters plus falling-edge detectors (Extended Data Fig. 2a).\n\nEach analog tile consists of 512 × 512 unit cells (Extended Data Fig. 2b), each containing four PCM devices. Circuitry can implement a significance factor F > 1 but we adopted F = 1, meaning that G+/− and g+/− are the same, apart from intrinsic stochasticity. This enabled the implementation of 2-PCM-per-weight and AB methods, both requiring equal contribution from W and W . Word lines and select lines were controlled by the west circuitry, selecting whether two or four PCM devices were connected to the edge capacitor. During weight programming, signals VSIG1 and 2 were kept at ground. Only one of the four PCM devices was programmed each time, by selecting the word, select and return lines. Weight programming was done in an iterative row-wise fashion4. During inference, VSIG1 and 2 were biased at a read voltage, V , of 0.3 V, while signals RL1 and 2 were at ground.\n\nInference was achieved in two steps (Extended Data Fig. 2c). During the integration phase, PWM pulses activated in each row for a time proportional to the desired input magnitude (unlike ref. 32, these durations were not converted to analog voltages using DACs). V was forced by a per-column operational amplifier, which biased the entire bit line. These pulses were buffered along the row to maintain pulse-width integrity. Although IR drops did occur along columns, the wide wires stopped them being critical to degradation of MAC accuracy, especially when compared with other more-important factors such as peripheral circuit linearity and saturation effects. Current was then mirrored into a per-column capacitor, which could be tuned by the LC by connecting up to 8 parallel metal-oxide-semiconductor capacitors, where each capacitor was 50 fF (we typically chose 250 fF). The choices of capacitor size and range of tunability were based on the available column area, the expected current in the array, the integration time and the mirror ratios achievable. The summation over an entire 512-row tile was performed fully in analog, without the need for partial summation in the digital domain. In the wide-input case involving two vertically neighbouring tiles (Fig. 1i), summation over 1,024 rows (or even 2,048 in the two 2-PCM-per-weight case) was still fully performed in the analog domain, without any intermediate digitization. For layers that used wide input, the read operation during closed-loop tuning used this combined configuration, allowing an individual weight to experience and correct for the same non-idealities that it would experience in the eventual inference MAC. This provided significant mitigation from additional MAC error induced by combining tiles. Depending on the sign of the input, the current could be steered to either charge or discharge the capacitor. After current integration, the tile was disconnected and the output duration was generated. During this step, a tunable ramp circuit, shared among all columns, set a linear voltage ramp that was compared with the voltage on the 512 peripheral capacitors (Extended Data Fig. 2d). For each column, the output voltage started high, and when the comparator switched, the output duration ended, determining the duration of that particular output pulse, which is similar to the approaches in refs. 33,34. Finally, an AND port enabled or disabled the pulse output. With proper enable signal timings controlled from the LC, activation functions such as ReLU or hard sigmoid could be implemented on chip. The 512 durations were produced in parallel, exiting the tile on 512 individual wires. Area-efficient design choices (such as the use of a common ramp generator circuit shared across all the columns, the elimination of a conventional ADC and associated digital registers, as well as optimized full-custom layouts) enabled dedicated per-column circuitry at pitch, without the need for column multiplexers.\n\nThese generated durations left the tile and propagated towards the next tiles or the OLPs using the OUT-from-col path in Extended Data Fig. 2e. Per-column south–north routing circuitry allowed for full parallel duration processing, enabling either N–S or S–N connection (without entering the corresponding tile), collecting durations from the tile (OUT-from-col) or sending durations into the tile columns (IN-to-col) as used during weight programming4. Per-row west–east routing blocks enabled W–E or E–W duration propagation and IN-to-row communication, allowing durations to reach the rows inside an analog tile and/or to move across the tile to implement multi-casting (Extended Data Fig. 2f).\n\nA user-configurable LC on each tile (Fig. 2a) retrieved instructions from a local SRAM. Each very wide instruction word (128 bits) included a few mode bits, as well as the wait duration (in cycles of around 1 ns given the approximately 1-GHz local clock) before retrieving a next instruction. Although some mode-bit configurations allowed JUMP and LOOP statements, most specified which bank of tile control signals to drive. Most of the 128 bits thus represent the next state of the given subset of tile control signals. This approach allowed for highly flexible tests and simplified design verification, with a small area penalty compared with predefined-state machines.\n\nFor example, the LC could configure 2D mesh routing to enable input access to analog tiles through the west circuitry (Fig. 2b) and MAC integration on the peripheral capacitors. The LC then configured the ramp and comparator used to convert the voltage on the capacitor into a PWM duration, avoiding energy-expensive ADCs at the tile periphery. Finally, the LC decided which direction (north, south, west or east) to send the generated durations, configuring the south 2D routing circuits4,33.\n\nThe LC also configured the ‘borderguard’ circuits at the four edges of each tile to enable various routing patterns. For example, Fig. 2c shows how durations from odd columns in the top tile could be merged together with durations from even columns from the bottom tile. This configuration was used on the RNNT Dec chip (Extended Data Fig. 7c).\n\nInputs were transformed into durations in the ILP circuitry. Durations spanned between 0 and 255 ns, encoded using 8-bit words. To verify the reliability of these communication paths across the entire chip (Fig. 2d), we repeatedly multi-cast 512 input PWM durations from the southwest ILP to all six OLPs at the same time. These durations were uniformly randomly distributed between 0 and 50 ns at 1 ns granularity (1 GHz clock), and CDFs of the error between measured and transmitted duration across 2,048 vectors (1 million samples) are shown in Fig. 2d. This experiment was repeated for distributions spanning from 0 to 100, 150, 200 and 250 ns. The maximum error never exceeded 5 ns, with shorter durations exhibiting even smaller worst-case error (±3 ns), showing that durations can be accurately communicated across the chip. Although in this case errors were introduced by the double ILP–OLP conversion and unusually long paths, during conventional inference tasks, the MAC error was always dominated by the analog MAC.\n\nKWS is used in a wide variety of devices, such as personal and home assistants, to perform actions only after specific audio keywords are spoken. Latency and accuracy are important attributes. When used in an ‘always-ON’ configuration, raw power is also an advantage. When gated by a much simpler two-class front end that can detect audio input of potential relevance and wake up the multi-class KWS system, energy per task becomes the relevant figure of merit.\n\nThe KWS network was trained using HWA techniques to make the network more resilient to analog memory noise and circuit-based non-idealities. We trained unitless weights on the interval (−1, 1) using weight clipping. In addition, we added normally distributed noise to these weights during each training mini-batch with a standard deviation of 0.02 (Extended Data Fig. 3a). We also added similarly distributed random noise with a standard deviation of 0.04 to output activations to mimic the imperfections expected from layer-to-layer activation transmission. We find that this simple noise model fits our analog system well and provides effective HWA training. We performed an extensive hyper-parameter search and picked a base learning rate of 0.0005 with a batch size of 250 for training. We found that including bias parameters for this network offered little benefit and therefore eliminated them from the model. We used adaptive moment estimation as the optimizer along with a weight decay (that is, L2 regularization) of zero. Finally, we used cross-entropy loss as our loss metric. The dependence of HWA accuracy for injected noise on weights and activations during training is shown in Extended Data Fig. 3b.\n\nThe KWS network performed several preprocessing steps before feeding the data into the FC layers. Input data (keywords) represented 1-second-interval voice recordings encoded as .wav files at a 16-kHz sampling rate. We computed the audio spectrogram, which is a standard way of representing audio information using the squared magnitudes of fast Fourier transforms taken at multiple time steps, using a window size of 30 ms and a stride of 20 ms. We then computed the Mel-frequency cepstral coefficients (MFCCs), which are a commonly used nonlinear transformation that accurately approximates the human perception of sound. We used 40 cepstral coefficients or bins per time slice. We also clipped the MFCCs to the range (−30, 30) to avoid any potential activation-rescaling problems going into our HW. This preprocessing resulted in a two-dimensional MFCC fingerprint for each keyword with dimensions of 49 × 40 (Extended Data Fig. 3c), and this is then flattened to give a 1,960-input vector. We also randomly shifted keywords by 100 ms and introduced background noise into 80% (the majority) of the training samples to make keyword detection more realistic and resilient.\n\nTo reduce the input size further and fit a 1,024-input-wide layer, we pruned the input data on the basis of the average of the absolute values of the validation input (Extended Data Fig. 3d). Pixels with average input intensity below a certain threshold were pruned, reducing the overall size to 1,024. Interestingly, pruning led to an accuracy improvement, as shown in the summary table in Extended Data Fig. 3e. Although our analog tiles can compute MAC on up to 2,048-element-wide input vectors, the AB method inherently uses both W and W . Thus the maximum input size over which fully analog summation can be supported is reduced to 1,024.\n\nBecause the KWS network is fully on-chip, tile calibration needed to be performed in HW. A per-column slope and offset correction procedure was achieved in three steps. Weights were first programmed using the nominal target values. Next, 1,000 inputs taken from the validation dataset were used as input and the single-tile MAC results were collected to calculate the column-by-column slope scaling factors to be applied to the target weights. The tiles were then reprogrammed with the scaled weights. Finally, experimental MAC was shifted up or down by programming eight additional PCM bias rows available on each tile (Extended Data Fig. 3f). After tile calibration, the ReLU activation function was tuned using the same validation input and comparing the experimental result on validation data with the expected SW ReLU. The inference experiment was then performed on the test dataset. The calibration enabled compensation of column-to-column process variations and input-times-weight column dependencies (such as activation sparsity and residual weight leakage). As shown in the drift results on RNNT, tile weights typically showed good resilience to drift owing to the averaging effect. Bias weights required more-frequent updates, on the scale of days, to compensate for column drift, but this involved merely running a small inference workload and reprogramming the bias weights. Eventually, the tile weights also need to be re-programmed. Although we have not explored temperature-dependent conditions, we believe that the levels of PCM drift exhibited here would be sufficient to allow operation for a few days or even weeks, which is sufficient to keep model reprogramming for the purposes of PCM drift indistinguishable from model refresh for other purposes (such as resource balancing and model updates).\n\nTo encode the MLPerf RNNT weights, we used five chips. Iterative weight programming enabled accurate tuning of the conductances to match the target weights. Heat maps correlating the target and the measured chip-1 weights on each of the 32 tiles are shown for W and W in Extended Data Fig. 4a,b. The corresponding error for each tile, expressed as the fraction of the maximum weight, is shown in Extended Data Fig. 4c,d for W and W . To compare the weight programming in the five chips used for the RNNT experiment, we calculated the CDF on the basis of the data shown in Extended Data Fig. 4c,d and extracted the spread between 1% and 99%. In this way, two data points were extracted for each tile, one for W and one for W . The chip analog yield, measured as the fraction of weights with a programming error of less than 20% of the maximum weight magnitude, is around 99% (Extended Data Fig. 4e). Chip 4 has a slightly lower yield because the corresponding maximum W, defined as the coefficient used to rescale weights from MLPerf (around [−1, 1]) to integers, is larger because more signal was required, causing greater weight saturation. Extended Data Fig. 4e shows the spread distributions for each of the five chips.\n\nThe RNNT encoder weights were mapped using the first four chips, as shown in Extended Data Fig. 5a. The large W and W matrices used for encoder LSTMs all show a size of 1,024 × 4,096 except for the conventional Enc-LSTM0 (W is 960 × 4,096) and Enc-LSTM2 (W is 2,048 × 4,096). Enc-LSTM0, Enc-LSTM1 and the W matrix of Enc-LSTM2 implement AB. In Enc-LSTM0, Enc-LSTM1 and Enc-LSTM2, summation of W and W MACs was performed off-chip at the x86 host, whereas chip 4, implementing Enc-LSTM3 and Enc-LSTM4, performed this entire summation on-chip in analog. Furthermore, blocks 1(−1), 9(−9) and 2(−2), 10(−10) of Enc-LSTM0 W and Enc-LSTM1 W , and blocks 1(9), 17(25) (W (W )) and 2(10), 18(26) were summed in digital after on-chip analog MAC. Finally, Enc-FC was implemented on chip 4. Any spot where tiles were connected by sharing the peripheral capacitor in the analog domain (Fig. 1i) is highlighted with a dark-blue bar. We did not map biases in analog memory but instead incorporated them in the already existing off-chip digital compute, by combining them into the calibration offset with no additional cost. Thus these biases were always applied with FP32 precision. No network retraining was applied.\n\nTo provide input data and collect MAC results in a massively parallel fashion from or to the ILPs–OLPs, complex routing paths were programmed, leveraging the flexibility of the LCs (Extended Data Fig. 5b). In the RNNT encoder, after each MAC, the data needed to go through input–output for off-chip digital processing. Each full operation (including input, MAC, duration generation and output digitization) took 2.1 μs. The input arrows show multi-cast in parallel to one or more analog tiles with MAC operations occurring on those tiles. Output MACs were provided to the OLPs in three time steps owing to the small number of OLPs.\n\nRNNT experiments implemented MAC on-chip, whereas tile affine calibration (shift and scale) and LSTM vector–vector computations were performed in SW (MATLAB SW running on x86). In particular, the first Enc-LSTM0 W required careful input-signal management to maximize the signal-to-noise ratio, owing to the large sensitivity of the WER to any noise on its weights. Extended Data Fig. 6a shows that, in the case of Enc-LSTM0 W , the input data, which naturally exhibits a wide dynamic range, was first shifted to zero-mean, followed by normalization to maximum input amplitude. The preprocessed input was then used for analog MAC. The MAC results were later denormalized back in SW, where the input mean contribution was added (which collapses to the product of one number, the mean value of the input image, and one vector, the sum of weights for every column) and the affine coefficients for calibration were applied.\n\nIn the case of expanded weights (Extended Data Fig. 6b), the input first underwent MAC with the random matrix M (such a matrix has random normal weights but is fixed across all inputs). Because the product of an input with a matrix with zero mean value generates an output with near-zero mean value, there was no need to apply the zero-mean shift, although normalization to maximum amplitude was still performed. After the analog on-chip MAC, the results are denormalized and the usual calibration was applied. For every other layer (Extended Data Fig. 6c) in the RNNT, the inputs were used directly as tile activations and the MAC was calibrated with the usual affine coefficients. All affine coefficients are calculated by comparing experimental and expected SW MAC using 2,000 input frames from the training dataset for each Enc–Dec layer. Data were linearly fitted to obtain the slope and offset coefficients.\n\nExtended Data Fig. 6d shows a detailed description of all data-type conversions. All SW computations were performed in FP32. For transmission to the chip, data were converted into INT9 (UINT8 plus sign) and UINT8 vectors were loaded into the ILP. Here, durations were generated and sent to the tiles where the analog MAC was performed, collecting an analog voltage on a peripheral capacitor. Once the UINT8 vectors were loaded into the ILP, ‘negative’ durations were sent during integration of the second or fourth time step, as shown in Extended Data Figs. 5b and 7d. Finally, charge integrated onto column-wise capacitors was converted by the peripheral circuitry into durations that were sent to other tiles or to the OLP, which converted them back into UINT8. Data were then sent off-chip and transformed back into FP32 during the calibration stage. Extended Data Fig. 6e shows a summary of the equations, highlighting that essentially all MACs were performed on-chip, whereas vector–vector, bias and nonlinear activations were computed in SW. The joint layer was in SW.\n\nExtended Data Fig. 7 shows the details of Dec mapping and signal routing. To account for the Emb layer (Extended Data Fig. 7a), we first collapsed Emb and Dec-LSTM0 W layers into a single Emb × W matrix with size 28 × 1,280, which receives one-hot input vectors. This multiplication is perfectly equivalent in SW, but led to large weights in the Emb × W matrix compared with W , as shown in the first set of CDFs, reporting the maximum weight for each column. Because MAC results from Emb × W and W are summed directly in the analog domain with a shared capacitor, weight values cannot be arbitrarily scaled. To overcome this problem, 9 copies of the 28 × 1,280 Emb × W matrix were programmed and the 28 inputs duplicated onto 9 × 28 rows, leading to a similar amount of signal with W . This allowed us to effectively distribute these large weights over 9 unit cells, while ensuring that the analog summation will aggregate both the Emb × W and the W contributions with the correct scaling.\n\nDec weight mapping used AB (Extended Data Fig. 7b) and signal routing enabled parallel input and output of all signals (Extended Data Fig. 7c). Here, routing concatenation was used to efficiently combine the signal from two different tiles into the same OLP. The full input–MAC–output processing time is 1.5 μs (Extended Data Fig. 7d).\n\nUnlike the KWS experiment, the MLPerf repository mandates that inference be performed with the validation dataset. The RNNT MLPerf inference experiments shown in Fig. 5 were done by inputting the full validation dataset into the first chip, saving the output results on the x86 machine, swapping in the second chip and continuing the experiment, using the previously saved outputs as new inputs. This procedure was repeated for all five chips, ensuring a consistent example-by-example cascading, as in a fully integrated system. Mapping even-larger models, using a weight-stationary configuration, can be supported with improved memory density (including stacking of multiple layers of PCM in the back-end-of-line), multi-chip modules and even multi-module solutions, with careful neural-network partitioning to minimize inter-module communication that would be energy expensive.\n\nExperimental MAC details are shown in Extended Data Fig. 8. The error distributions and MAC correlations are shown for every chip. In all figures, a dashed region highlights the main regions of interest for that MAC. For LSTM layers, the region of interest corresponds to the [−5, 5] range, because outside that range the ensuing sigmoid or tanh function can be expected to fully saturate (for example, the output will always be −1 or +1, being almost completely independent of any variations on the input). Similarly, the regions of interest for the FC layers are mostly the positive MACs because of the ReLU activation function. In this specific case, Enc-FC and Dec-FC are summed before ReLU, so slightly negative contributions could also matter. We plotted the regions of interest to be where MAC > −5. The reported standard deviation σ computes the error for SW MAC in [−5, 5] for LSTMs and [−5, inf] for FC layers. Comparison between the original W and the weight-expanded W for Enc-LSTM0 is also provided. Extended Data Fig. 9 shows examples of transcribed sentence output from the experiments in Fig. 5 that show an almost iso-accuracy WER. Transcription results are in good agreement between the MLPerf RNNT model implemented in analog HW and in SW, indicating that the effective bit-precision of our HW demonstration is n = 4.097 for 9.475% WER and n = 4.153 for 9.258% WER (weight expansion), on the basis of comparison with the full network (no joint FC) curve in Fig. 4c.\n\nThe proposed 5-chip RNNT implementation is not integrated with digital processing, but we can estimate the time needed to process the entire dataset by combining the MAC processing times and energies from the analog chips with the estimated digital processing times and energies that we tabulated previously in our architecture paper20. Extended Data Fig. 10a shows a timing simulation describing the execution of RNNT layers for processing all 2,513 input audio samples, accounting for all pipelining, time stacking, recurrence and Dec steps. We assume times of 2.1 μs and 1.5 μs for the Enc and Dec layers, respectively, which includes all duration generation, and a relatively conservative 300 ns for the digital processing of each layer. Given these assumptions, the entire dataset can be evaluated in 1.2877 s, corresponding to a rate of 1,951.59 samples per second. Combined with the power measurements below, these numbers can be used to extrapolate the analog-AI RNNT system performance.\n\nPower measurements for RNNT were done using a set of 32 exemplar input vectors that filled up the ILP SRAM to capacity. By overflowing the address pointer of the ILP, it is possible to repeat the same set of 32 vectors ad infinitum. Together with JUMP instructions in the LCs resetting the program counters to the start of program execution, this allowed a real-time current measurement from the voltage supplies for the inference tasks. In these measurements, all 7 (or 5) phases of the Enc (or Dec), including 4 integration phases and 3 (or 1 for the Dec) duration generation phases were included. This accounted not just for the MAC integration, but also for the subsequent cost of generating, transporting and digitizing the MAC results. The measured powers are shown in Fig. 6a.\n\nUsing the energy and execution-time models from our architecture study20, the total digital energy (for all the tasks performed off-chip in SW to support the experiments shown in this paper) is estimated to be 0.11 J for nominal Enc-LSTM0 and 0.26 J for weight-expansion Enc-LSTM0. The total number of digital operations and a detailed breakdown are shown in Extended Data Fig. 10c,d.\n\nAlthough several compute-in-memory or near-memory approaches based on SRAMs and digital compute35,36,37,38 have been presented in the literature, most of these do not address the energy and time costs of reloading weights, thus making direct side-by-side comparisons against NVM-based weight-stationary approaches difficult. However, several NVM compute-in-memory studies have focused on the macro-level32,34,39,40,41, without accounting for data transport, control or chip infrastructure (such as clocking) costs. They are also usually at a much smaller scale (sometimes less than 1 million parameters7) than the work here, making a fair assessment of both the accuracy of large models and the associated sustained TOPS/W values difficult.\n\nWe have instead compared our sustained power and performance values against other reported system numbers for the same RNNT task from MLPerf, as shown in Extended Data Fig. 10e. By weighting the sustained power measurements for individual chips with their corresponding activity factors from the timing simulations shown in Extended Data Fig. 10a, the total system energy and corresponding aggregate TOPS/W values for our system are calculated to be 4.44 J and 6.94 TOPS/W, respectively (4.60 J and 6.70 TOPS/W for W ). Although our evaluations in Fig. 6 do not include some external components used in real systems, such as system buses and voltage regulators, this TOPS/W energy efficiency is still more than an order of magnitude better than the best published result for this task.\n\nThe relatively small number of digital operations in the network implies that considerable benefits may yet be obtained by improving the raw analog MAC energy efficiency (currently 20 TOPS/W). This could be enabled by shorter integration times, more-efficient analog opamps and/or lower-conductance devices. Instead, a substantial drop-off in energy efficiency, down to 12.4 TOPS/W for chip 4 (Fig. 6c), occurs as a result of the on-chip infrastructure, such as the landing pads, which need to be exercised at the end of each MAC. This highlights the need for on-chip digital compute cores, potentially in proximity to the same chip, and using the same local 2D mesh for data transport as described in our architecture study20.\n\nMLPerf submissions for RNNT exhibit performance efficiencies ranging between 3.98 and 38.88 samples per second per watt, using system power that ranges from 300 to 3,500 W, assuming the use of large batches to maximize efficiency. Our work inherently assumes a mini-batch size of 1. Although we assume that additional samples are available to keep the pipeline full, our projections are effectively independent of mini-batch size. Under these conditions, an analog-AI system using the chips reported in this paper could achieve 546.6 samples per second per watt (6.704 TOPS/W) at 3.57 W, a 14-fold improvement over the best energy-efficiency results submitted to MLPerf. Reduction in the total integration time through precision reduction, hybrid PWM40 or bit-serial schemes can improve both throughput and energy-efficiency, but these could suffer from error amplification in higher-significance positions. Future efforts will need to address their impact on MAC accuracy for commercially relevant large DNNs."
    },
    {
        "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10447234",
        "document": "The past decade has seen AI techniques spread to a wide range of application areas, from the recognition and classification of images and videos9 to the transcription and generation of speech and text10–16, all driven by a relentless progression towards deep neural network (DNN) models with ever more parameters. In particular, transformer1 and recurrent neural-network transducer (RNNT)12,13,16 models containing up to one billion parameters2 have produced a marked decrease in word error rate (WER) (and therefore much better accuracy) for the automated transcription of spoken English-language sentences, as shown in Fig. 1a for two widely used datasets, Librispeech17 and SwitchBoard18. Unfortunately, hardware (HW) performance has not kept pace, leading to longer training and inference times and greater energy consumption19. Large networks are still trained and implemented using general-purpose processors such as graphics processing units and central processing units, leading to excessive energy consumption when vast amounts of data must move between memory and processor, a problem known as the von Neumann bottleneck. a, Speech recognition has improved markedly over the past 10 years, driving down the WER for both the Librispeech and SwitchBoard (SWB) datasets, thanks to substantial increases in model size and improved networks, such as RNNT or transformer. For comparison with our results, the MLPerf RNNT full-precision WER is shown for two Librispeech datasets (‘test-clean’ and ‘dev-clean’)8, along with this work’s WER, which was computed on Librispeech dev-clean. For model size: B, 1 billion; M, 1 million. b, Inference models are trained using popular frameworks such as PyTorch or TensorFlow. Further optimization for analog AI can be achieved with the IBM analog HW acceleration kit (https://aihwkit.readthedocs.io/en/latest/). c, Trained model weights are then used on a 14-nm chip with 34 analog tiles, two processing elements (PE, not used for this work) and six ILP–OLP pairs. Tiles are labelled as north (N), centre (C) or south (S) followed by west (W) or east (E). d, Each ILP converts 512 8-bit inputs into 512 element vectors of pulse-modulated durations, which are then routed to the analog tiles for integration using a fully parallel 2D mesh that allows multi-casting to multiple tiles. After MAC, the charge on the peripheral capacitors is converted into durations4 and sent either to other tiles, leading to new MACs, or to the OLP, where durations are reconverted into 8-bit representations for off-chip data-processing. e, Transmission Electron Microscopy (TEM) image of one PCM. f, Each tile contains a crossbar array with 512 × 2,048 PCMs, programmed using a parallel row-wise algorithm4. g, PCMs can be organized in a 4-PCM-per-weight configuration, with G+, g+ adding and G−, g− subtracting charge from the peripheral capacitor, with a significance factor F (which is 1 in this paper). h, Alternatively, they can have a 2-PCM-per-weight configuration, which achieves a higher density. By reading different input frames through weights W or W , a single tile can map 1,024 × 512 weight layers. i, Finally, two adjacent tiles can share their banks of 512 peripheral capacitors, enabling integration in the analog domain across 2,048 input rows. Analog-AI HW avoids these inefficiencies by leveraging arrays of non-volatile memory (NVM) to perform the ‘multiply and accumulate computation’ (MAC) operations which dominate these workloads directly in the memory3–7. By moving only neuron-excitation data to the location of the weight data, where the computation is then performed, this technology has the potential to reduce both the time and the energy required. These advantages are further enhanced for DNN models that have many large fully connected (FC) layers, such as the RNNT or transformer models used for state-of-the-art natural language processing (NLP). In conventional digital implementation, such layers require enormous movement of data but provide scant opportunity for amortization over subsequent computing. For analog AI, by contrast, such layers are efficiently mapped onto analog crossbar arrays and computed in parallel using a single integration step. Given the finite endurance and the slow, power-hungry programming of NVM devices, such analog-AI systems must be fully weight stationary, meaning that every weight must be preprogrammed before inference workload execution begins. A highly heterogeneous and programmable accelerator architecture for analog AI has been introduced20 for which system-level performance assessments have predicted energy efficiencies 40–140 times higher than those of cutting-edge graphics processing units. However, this simulation study required several design assumptions that have yet to be demonstrated in HW, two of which are directly addressed below. The first is the use of a dense and efficient circuit-switched 2D mesh to exchange massively parallel vectors of neuron-activation data over short distances. The second is the successful implementation of DNN models that are large enough to be relevant for commercial use and are demonstrated at sufficiently high accuracy levels. In this paper, we present experimental results using a 14-nm inference chip leveraging 34 large arrays of phase-change memory (PCM) devices4, digital to analog input, analog peripheral circuitry, analog to digital output and massively parallel-2D-mesh routing. Our chip does not include on-chip digital computing cores or static random access memory (SRAM) to support the auxiliary operations (and data staging) needed in an eventual, marketable product. However, we can use it to demonstrate the accuracy, performance and energy efficiency of analog AI on NLP inference tasks, either by implementing simple operations such as rectified linear unit (ReLU) non-linear function directly in the analog domain or by performing small amounts of auxiliary computing off-chip. To demonstrate the flexibility of the chip, we chose two neural-network models from the MLPerf standard benchmark8, a suite of industry-relevant use cases. We first targeted the tiny-model task of keyword-spotting network (KWS) on the Google speech-commands dataset. For this we used a HW-aware (HWA) trained network, retrained using a variety of techniques available in the open-source IBM analog HW acceleration kit (https://aihwkit.readthedocs.io/en/latest/) (Fig. 1b). We then implemented the MLPerf version of RNNT, a large data-center network, on Librispeech without any additional HWA retraining. This model has 45 million weights, which we implement using more than 140 million PCM devices across five packaged chip modules, demonstrating near-SW accuracy (ours is 98.1% of that exhibited by the base software (SW)-only model) and executing about 99% of the operations on the analog-AI tiles.\n\nA micrograph of the chip is shown in Fig. 1c, highlighting the 2D grid of 34 analog tiles, each of which has its own 512 × 2,048 PCM crossbar array. Tiles are grouped into six power domains, labelled as north, centre or south followed by west or east. Each power domain contains one input landing pad (ILP) (Fig. 1d) and one output landing pad (OLP), each associated with a large SRAM. The ILP receives digital input vectors (each vector has 8-bits unsigned integer (UINT8) × 512 entries) from off-chip, converting these inputs into pulse-width-modulated (PWM) durations onto 512 wires situated in parallel at the edge of the 2D mesh running over all the tiles4,20. Conversely, the OLP receives PWM durations on 512 wires, digitizing these durations back into UINT8 for off-chip data transport. Analog-tile to analog-tile communication is performed using durations, eliminating the area, power and latency associated with analog-to-digital conversion at the tile periphery4 for situations in which integration on the rows of each destination tile can be performed synchronously with the readout of the columns of one or more source tiles, including FC layers with simple activation functions. When duration vectors are sent from a tile to the OLP, the chip is effectively implementing a ramp-based analog-to-digital converter (ADC), except that the shared ramp circuits and dedicated comparators are located at the tiles and the digital counters are at the OLP. Digitization becomes a necessity for transformer attention and models that require internal data staging. PCM devices are integrated in the back-end wiring above 14-nm front-end circuitry (Fig. 1e) and can encode analog conductance states by tuning, with electrical pulses, the relative volume of crystalline-phase (highly conductive) and amorphous-phase (highly resistive) material at the narrow bottom electrode. To program PCM devices, a parallel programming scheme is used (Fig. 1f) so that all 512 weights in the same row are updated at the same time4. Weights can be encoded using a variable number of PCM devices. Figure 1g shows a 4-PCM-per-weight configuration, where each of the four PCM devices contributes equally to the read current and thus to the charge stored on the peripheral capacitor. A second, denser scheme uses a 2-PCM-per-weight set-up (Fig. 1h), encoding one weight, W = G+ - G−, on the first two PCM devices and a different weight, W = g+ - g−, on the second pair of devices. In this way, two different input vectors can be multiplied with W and W in two separate time steps, on the same capacitor, allowing analog MAC across 1,024 rows. Finally, two analog tiles can share one bank of peripheral capacitors (Fig. 1i), further extending the analog integration up to 2,048 analog input rows across 512 columns per pair of tiles. All weight configurations, MAC operations and routing schemes are defined with a user-configurable local controller (LC) available on each tile (Fig. 2a). A local SRAM stores all the instructions defining the time sequence of several-hundred control signals, allowing for a highly flexible test and simplifying design verification, with a small area penalty when compared with predefined-state machines. a, For maximum test-time flexibility, each tile contains a user-programmable LC that defines all timing sequences. b, LC controls signal routing through the 2D mesh from ILP to the tile, MAC, output duration generation through ramp plus comparator circuitry and borderguard tile routing configuration. c, As an example, LC can implement 2D-mesh concatenation such as merging the durations originating from the even columns on one tile with the durations coming from the odd columns of another tile. d, To test the communication, 1 million random input durations are multi-cast, in parallel, to all 6 OLPs. Durations randomly vary between 0 and 50 ns (dark-blue lines) or between 0 and either 100, 150, 200 or 250 ns (lighter shades of blue) with 1-ns granularity. Cumulative distribution functions (CDFs) reveal that the communication error never exceeds 5 ns, demonstrating high transport accuracy. The 2D mesh comprises 512 east–west wires and 512 north–south wires sitting over each tile, with a diagonal set of 512 metal vias to connect each corresponding pair of wires. ‘Borderguard’ circuits at the four edges of each tile can block or propagate each duration signal using tri-state buffers, mask bits and digital logic. This allows complex routing patterns to be established and changed when required by the LC, including a multi-cast of vectors to multiple destination tiles, and a concatenation of sub-vectors originating from different source tiles20 (Fig. 2c). Finally, Fig. 2d verifies that durations can be reliably transmitted across the entire chip, with a maximum error equal to 5 ns (3 ns for shorter durations).\n\nTo demonstrate the performance of the chip in an end-to-end network, we implemented a multi-class KWS task21. MLPerf classifies KWS as a ‘tiny’ inference model8 and proposes a convolutional-neural-network architecture trained on the Google Speech Commands dataset comprising 12 keywords (Fig. 3a). For this implementation, we instead adopted an FC network22. Both networks require upstream digital preprocessing to convert incoming audio waveforms into suitable input data vectors using a feature-extraction algorithm21,22. The FC model achieves a classification accuracy of 86.75%, compared with 90% for the MLPerf convolutional neural network, but offers a simpler architecture and faster performance (several KWS open submissions to MLPerf use FC-type networks, sometimes reporting even lower accuracy around 82.5%)8. Because an FC network matches our chip topology and exploits our large tiles, our goal is to match the available SW accuracy of 86.75%. a, To classify spoken words into one of the 12 highlighted classes for KWS, an FC baseline is used as a reference. b, The network is then retrained using HWA techniques. c, The end-to-end implementation uses four analog tiles. d, An AB method is used to increase MAC accuracy. Weights W and −W are programmed on W and W respectively. By inferencing the desired input x on W and then −x on W , the MAC is collected twice (xw + (−x) × (−w)), cancelling out any fixed peripheral circuitry asymmetries and improving MAC accuracy. e, A timing diagram shows that a full frame is processed in 2.4 μs. Because the ReLU activation (implemented on-chip in the analog domain) generates positive-only outputs, the second layer requires only two integration steps, rather than the four needed in the first layer. f, Experimental activations after layers L , L and output correlate closely with ideal SW MACs calculated using HW input. PDF, probability distribution function. g, This leads to SW accuracy for this fully end-to-end demonstration. To enable a fully end-to-end implementation on our chip, we first modified the audio-spectrum digital preprocessing to produce 1,960 inputs and increased the size of each hidden layer from 128 to 512 for our tiles (in 4-PCM-per-weight mode). To make the network more resilient to analog noise23–26, we retrained it while including weight and activation noise, weight clipping, L2 regularization and bias removal (https://aihwkit.readthedocs.io/en/latest/). We then pruned this trained network down to 1,024 inputs (Fig. 3b) to fit the first layer into a two-tile mapping configuration (Fig. 3c), using the shared-capacitor-bank approach shown in Fig. 1i. Our end-to-end implementation uses four tiles in total: two for the first weight layer and two for the next two weight layers. To improve the MAC accuracy and compensate for asymmetries in the peripheral circuits, we introduce a MAC asymmetry balance (AB) method (Fig. 3d). Actual weights, W, are programmed on the first PCM pair, W , and opposite-signed weights, −W, are encoded on the second PCM pair, W . By first multiplying the actual input on W = W and then −input on W = −W, we computed the desired MAC (scaled by ×2) while cancelling out fixed asymmetries in the peripheral circuitry for current collection. Each audio frame requires 2.4 μs in total, in the form of 8 time steps of 300 ns each (Fig. 3e); this is 7 times faster than the best-case latency currently reported by MLPerf8. Experimentally measured MAC-plus-Activation function (ReLU for layers L and L , linear for Output) correlations with the expected SW result are shown in Fig. 3f for all three layers. The measured KWS accuracy is 86.14% (Fig. 3g), well within the MLPerf SW ‘iso-accuracy’ limit of 85.88% (defined as 99% of the accuracy of the original SW model).\n\nAlthough KWS represents an excellent benchmark for very small models, we can also use our chip to demonstrate much larger and more-complex networks. As an example, the NLP task of speech-to-text transcription enables applications such as agent assist, media content search, media subtitling, clinical documentation and dictation tools (https://aws.amazon.com/what-is/speech-to-text/). We therefore implemented the MLPerf Datacenter network RNNT as an industry-relevant workload demonstration. To further simplify model use, we programmed the MLPerf weights directly with no additional HWA retraining. The MLPerf RNNT showcases all the important building blocks, such as a multilayer encoder (Enc), decoder (Dec) and joint subnetwork blocks (Fig. 4a). The network is slightly simplified with respect to state-of-the-art RNNTs; the long short-term memory (LSTM) blocks are unidirectional, rather than bidirectional, and the decoding scheme is greedy rather than beam-search, which increases the WER slightly but makes online continuous-streaming use much more straightforward27. a, MLPerf RNNT model, trained on the Librispeech dataset, comprises encoder (Enc), decoder (Dec) and joint blocks. The input signal is digitally preprocessed and stacked to generate the input of Enc-LSTM0 (chip 1) and Enc-LSTM1 (chip 2). The resulting output vectors are again time-stacked before feeding a 2,048-input Enc-LSTM2 (chip 3), followed by two 1,024-input Enc-LSTM3,4 and an Enc-FC linear layer (chip 4). The resulting encoder output is then merged with the vectors received from the Dec (chip 5). Finally, a joint-FC calculates the next-letter probability (in SW), which feeds back to the Dec. This entails greedy decoding in which the highest probability selects the output letter. b, SW-based sensitivity analysis performed by progressively quantizing the FP32 MLPerf weights. c, The WER increases beyond the SW limit when weights are excessively quantized. d, There is a threshold n at which the WER is still SW for the full network, the full network without joint-FC quantization, and for each individual layer. While Dec-LSTM1 is the most resilient to noise, joint-FC exhibits significant sensitivity and is small in size, so it is not mapped in analog to preserve high accuracy. e, All the other layers are mapped to analog tiles (mapping details in Extended Data Figs. 5, 7). All arrows show the input signal routing and are operating at the same time, each performing a simultaneous multi-cast to all tiles that show the same-colour MAC arrow. Note that the borderguard circuits can enable duration data arriving at the west side of a tile to deliver durations onto the rows of that tile, and a completely different duration-vector passes over the centre of that tile on its routing wires at the same time. Small arrows indicate how MACs are aggregated in the analog domain across tile pairs. f, The output duration routing. Each arrow colour requires its own time slot: three for chips 1, 2, 3 and 4, and one for chip 5. Output routing from tiles to OLPs can involve implicit concatenation (chip 5). More details are given in the Methods. The joint block and all LSTM vector–vector operations are computed off-chip. g, More than 45 million weights are mapped using more than 140 million PCMs, with an average of 2.9 (3.1 with W ) PCMs per weight. Coloured bars show PCMs, white bars show weights. As with KWS, digital preprocessing first converts raw audio queries into a sequence of suitable input data vectors. At each sequence time step, the encoder cascades data vectors through five successive LSTMs (Enc-LSTM0, 1, 2, 3, 4) and one FC layer (Enc-FC). At each LSTM, the local input vector for that layer is concatenated with a local ‘hidden’ vector, followed by vector–matrix multiplication through a very large FC weight layer, producing four intermediate sub-vectors. These sub-vectors are then processed and combined using a relatively small amount of vector–vector computing, generating an output vector that is sent forward to become the input to the next LSTM or FC layer for that same time step, and also recursively fed back to become its own hidden vector for the next time step. Time-stacking, performed immediately after preprocessing, as well as between Enc-LSTM1 and Enc-LSTM2 (Fig. 4), scales down the effective number of time steps in the local sequence by concatenating multiple arriving data vectors into one departing data vector. The Dec block, which operates in parallel with the encoder, consists of one embedding FC layer (Dec-Emb), two LSTMs (Dec-LSTM0, 1) and one FC layer (Dec-FC). Finally, the joint layer sums the Enc and Dec signals, applies a ReLU activation function and selects the predicted output letter (including the possibility of a ‘blank’ character) for that time step using a 512 × 29 FC layer with a greedy decoding scheme. The predicted output letter is both the model output and the next input to the Dec block. The joint block alternates between emitting blanks, at which point the next encoder output is consumed, and emitting letters, which then triggers Dec processing. As a result, the number of Dec iterations will not usually match the input sequence length seen by the encoder. When large DNNs such as RNNT are implemented with reduced digital precision, optimal precision choices may vary across the network28–30. Similarly, implementation in analog-AI HW also requires careful layer-specific choices to balance accuracy and performance. Although dense 2-PCM-per-weight mapping (Fig. 1h) can improve energy efficiency (increasing the number of operations per second per watt, OPS/W) or areal efficiency (the number of operations per mm2), higher accuracy can be achieved using techniques such as AB, in exchange for increased area, energy and/or time. Therefore, before mapping RNNT on HW, we need to find out which network layers are particularly sensitive to the presence of weight errors and other analog noise. We perform this initial assessment in SW, not by adding random noise (on either weights or activations) and repeating ad nauseam to obtain stable results through Monte Carlo sampling, but by introducing increasingly stronger weight quantization on the whole, or just a portion, of the RNNT network (Fig. 4b). Any parts of the network outside the portion being stress-tested are evaluated using the original 32-bit floating point (FP32) precision. The resulting degradation in WER can be plotted as a function of the effective precision, n . Layers or entire network blocks that are less susceptible will still deliver a low WER even with aggressive quantization (small values of n ), whereas highly sensitive blocks will exhibit a high WER even for small amounts of weight quantization. Figure 4c shows this simulated WER as a function of n for various cases, using the 99% SW limit (an 8.378% WER) of the network baseline (7.452% WER) to identify a threshold n (arrows). When weights across the full network are all quantized, WER is no longer SW once n < 5.4 (42 levels). Repeating this process for each individual layer identifies the most-sensitive layers (those exhibiting a higher n threshold (Fig. 4d)), such as the joint-FC and Enc-LSTM0, followed by Enc-LSTM1. Given the small size (512 × 29 weights) but large WER impact of the joint-FC, we chose to implement this layer within the digital processing. Again, because the chip does not contain any explicit digital processing, this joint-FC, all vector–vector products and the activation functions are computed off-chip on a host machine. The OLPs (and ILPs) are used to send data from the chip(s) to the host (and back). Now that we have identified which layers are most sensitive, we are ready to map the MLPerf weights onto 142 tiles distributed across 5 chips. Because Enc-LSTM0 and Enc-LSTM1 are sensitive to noise, the AB method is used on these layers, together with a careful treatment of the first matrix, W , of Enc-LSTM0, which helps to improve MAC accuracy and decrease WER (see Methods for details). In summary, of a total of 45,321,309 network weight and bias parameters, 45,261,568 are mapped into analog memory (99.9% of the weights). A single chip can hold only 17,825,792 weights in a 2-PCMs-per-weight scheme, so we used 5 different chips. Specific mapping details are shown in Fig. 4e,f. Coloured tiles encode weights and perform MAC operations; grey tiles are unused. Figure 4e shows how input data reach each tile from an ILP, with fully parallel routing. After all the necessary integrations, duration vectors representing MAC results are sent from tiles to OLPs as shown in Fig. 4f. In total, more than 45 million weights are encoded using more than 140 million PCM devices, with an average of around 3 PCM devices for each weight (Fig. 4g). Figure 5a shows the experimental WER after weight mapping and programming for the full Librispeech validation dataset of 2,513 audio queries. Here a single layer of the RNNT network is mapped on a chip, and everything else is calculated in SW. It is worth noting that individual layers of the network are SW by themselves. As predicted in Fig. 4d, Enc-LSTM0 shows the largest WER, with other layers being more resilient to noise. Finally, the full inference experiment on all five chips is shown in Fig. 5b. From left to right, each bar reports the overall WER obtained by implementing increasingly more layers on chip. The total WER is given by the last Dec bar, 9.475%, with an overall degradation of 2.02% from the 7.452% SW baseline. For this experiment, we inference the full Librispeech validation dataset through chip 1 and save the output results. These are then input into chip 2, and so on across all 5 chips. Even when repeated after more than 1 week of PCM drift31,without any recalibration or weight reprogramming, the RNNT WER has degraded by only 0.4% (Fig. 5c). a, Single-layer WER. The graph shows an experimental sensitivity analysis obtained by implementing one layer on-chip and all the others in SW at FP32 precision. The most critical layer is Enc-LSTM0. b, Cumulative WER. Full RNNT inference using all five chips on the full Librispeech validation dataset. The bars from left to right show the cumulative WER obtained when implementing increasingly more layers on-chip. The full RNNT WER, using the original MLPerf weights, achieved across five chips (right-most bar) is 9.475%. c, After one week of PCM drift, the cumulative WER slightly increases to 9.894%, just 0.4% more than day-0 WER. d, To further improve the accuracy, a weight-expansion technique is introduced for Enc-LSTM0. Given a MAC W × x, the insertion of a random normal matrix M and its pseudoinverse pinv(M) leads to the same MAC output. However, now W = W × pinv(M) contains more rows N, with an increased signal-to-noise ratio. Whereas signal increases linearly with N, the aggregate noise across the larger number of rows increases sub-linearly ( if noise sources are independent Gaussians). e, Simulation results. When quantizing Enc-LSTM0 to n = 3.5 bits, the WER is 42%. Weight expansion greatly improves the resilience, even for only slightly expanded W matrices, with the WER reduced to 7.9%, well below SW . f, Similar accuracy benefits are observed experimentally when implementing weight expansion on Enc-LSTM0 on-chip, revealing stronger WER reduction with respect to weight averaging. M × x is digitally preprocessed. W expansion to 1,024 rows enables a 9.258% WER on the full RNNT, 1.81% from the SW baseline, 0.88% from SW . We observe that the layer-to-layer WER degradation in Fig. 5b is steeper than expected from simple aggregation of the single-layer WER degradations (Fig. 5a). Intuitively, Enc-LSTM0 and other early layers have a bigger cumulative impact owing to error propagation. We can further improve the WER of Enc-LSTM0 with a new weight-expansion method involving a fixed matrix M with normal random values, and its Moore-Penrose pseudo-inverse, pinv(M) (Fig. 5d). The resultant noise-averaging helps to improve the accuracy of the MAC operation and the overall resilience of the network layer, with no additional retraining required. On analog HW, as long as the number of tiles remains unchanged, the additional cost of using more or even all of the rows in each tile is almost negligible. However, more preprocessing is needed to implement M × x in digital, although it is much less than if the entire Enc-LSTM0 layer were implemented in digital. Using our SW-based assessment method from Fig. 4c,d, Fig. 5e shows that quantizing the Enc-LSTM0 weights to 3.5 bits leads to an excessive WER (42%). However, after weight expansion, the WER greatly decreases, even for a small W expansion, saturating at a SW value of 7.9% WER when W contains 1,024 rows. The same behaviour is observed in experiments (Fig. 5f), with the WER for on-chip Enc-LSTM0 decreasing as weight expansion is increased up to a W containing 1,024 rows, exceeding the improvement shown by simply programming multiple weight copies. Figure 5b shows that when the entire RNNT network is run on five chips, starting with expanded W on Enc-LSTM0, WER improves to 9.258%, which is 1.81% from the SW baseline, and only 0.88% from the SW threshold. We also measured the full power consumption for every chip during inference operations. The chip has various power supplies. It uses 1.5 V to drive the row activation and column integration on the tiles during analog computation. All control and communication circuits, including ILP, OLP, LC and 2D mesh, are driven at 0.8 V. As shown in Fig. 6a, the 1.5 V and 0.8 V supplies dominate power consumption. By contrast, the 1.8 V supply that drives the clock phase-locked loop (PLL) and the off-chip drivers and receivers, and some other analog voltage sources, have a negligible impact. The corresponding sustained TOPS/W values are reported in Fig. 6a. Chip 4 has the best power performance (12.40 TOPS/W) because it has the most on-chip weights. In general, the reported TOPS/W values correlate well with the number of weights encoded on-chip: chips 1 and 2 use an AB technique and have 4 PCMs per weight, whereas chip 4 uses a denser mapping of 2 PCMs per weight. Finally, the Dec chip, chip 5, has the lowest TOPS/W value because this chip implements only around 1.8 million weights across only 13 of the 34 tiles, yet the data communication is still extensive, requiring a large number of tiles and ILPs/OLPs to be active to implement the routing network (Fig. 4e,f). a, Measured power and TOPS/W are shown for each chip. TOPS/W (coloured bars) correlate with the number of weights used on each chip (white bars). D, duration; DAC, digital-to-analog converter; PLL, phase-locked loop. b, Reducing the maximum input duration leads to an improvement in TOPS/W with only a small amount of WER degradation (chip 4 is measured, other layers in SW at FP32). c, Energy efficiency at various levels: analog integration only (1.5 V power domain), full chip, all 5 chips for RNNT (analog integration only and full chip), and full system level including estimated digital processing energy20. d, Simulated performance for an integrated system shows that the average processing time for each sample is 500 μs, more than 104 times faster than the input speech sentence, thus enabling real-time transcription. Total processing time = 1.29 s and total real audio = 4 h 20 min, so the real-time factor ≈ 8 × 10−5 ≪ 1. e, Number of operations performed on-chip versus off-chip in the RNNT experiment, with a 325:1 ratio for the original MLPerf weights (W ) and 88:1 with weight expansion (W ) (Fig. 5d). f, Samples per second per watt and TOPS/W performance for comparison with MLPerf submissions, showing a 14-fold improvement for our system. Figure 6b shows that another 25% improvement in TOPS/W (from 12.4 to 15.4 TOPS/W) for chip 4 can be obtained by halving the integration time, albeit with an additional 1% degradation in the WER. Figure 6c shows how the costs of data communication, incomplete tile usage and inefficient digital computing bring the large peak TOPS/W of the analog tile itself (20.0 TOPS/W) down to the final sustained value of 6.94 TOPS/W. Given the actual chip processing times (1.5 μs for chip 5 and 2.1 μs for the other four; see Methods), we can estimate the full processing time for an overall analog–digital system (Fig. 6d). This includes the estimated computation time (and energy) if on-chip digital computing were added at the physical locations of the OLP–ILP pairs. Given the 500-μs average processing time for each audio query, the real-time factor (the ratio between processing and real audio time) is only 8 × 10−5, well below the MLPerf real-time constraint of 1. Although the digital compute is inefficient, the enormous ratio between the number of analog and digital operations (Fig. 6e; 325-fold for conventional weight mapping and 88-fold with the weight-expansion technique, owing to the increased digital preprocessing) makes the analog-only and projected full-system energy efficiencies similar (Fig. 6c; 7.09 TOPS/W and 6.94 TOPS/W using conventional weight mapping). With weight expansion, an analog-AI system using the chips reported in this paper could achieve 546.6 samples per second per watt (6.704 TOPS/W) at 3.57 W, a 14-fold improvement over the best energy-efficiency submitted to MLPerf (Fig. 6f), at 9.258% WER.\n\nOur experimental results were measured on chips built from 300-mm wafers with a 14-nm complementary metal-oxide-semiconductor front end, fabricated at an external foundry. PCM devices were added in the ‘back-end-of-line’ at the IBM Albany NanoTech Center. Mushroom-cell PCM devices were built with a ring heater with a diameter of approximately 35 nm and a height of around 50 nm (Fig. 1e) as the bottom electrode, a doped Ge Sb Te layer and a top electrode. Wafer characterization before packaging was performed on both 1-resistor macros and 1,024 × 2,048 array diagnostic monitors with on-chip sense amplifiers. After selection of high-yield dies, the wafer was diced and packaged into testable modules at IBM Bromont, as shown in Extended Data Fig. 1a,b. (a) Packaged modules used in the experiments. (b) Micrograph of the chip at H3 metal level, showing all analog tiles and the corresponding area breakdown. (c) The test platform is based on a chip socket mounted on a custom board driven by 3 Xilinx Virtex-7 VC707 FPGAs. Power is supplied by four Keysight E36312A. The three FPGAs are connected through the board, and the overall system is controlled by an x86 machine using a PCIe connector. (d) For routing precision, end-to-end KWS, and RNNT power measurements, the chip is run without any need to send intermediate data back to an x86 machine during the experiment. KWS models are trained for chip deployment using HWA methods. For RNNT accuracy measurements, intermediate data moves between the chip and an x86 machine. To predict the RNNT performance, we assume digital circuitry would be integrated on the chip next to the ILP/OLP pair, leading to predictions that should be very close to a fully integrated chip in which both analog tiles and digital compute cores are tightly integrated20. Experiments were run by mounting the module on a socket connected to a custom-designed board driven by three Xilinx Virtex-7 VC707 field-programmable gate arrays (FPGAs) (Extended Data Fig. 1c). Four Keysight E36312A power supplies were used to power up the boards and the chip. In addition to the 1.5 V, 0.8 V and 1.8 V supplies mentioned in the main text, a 3.0 V power supply was provided but only during PCM device programming (not during inference). Finally, a supply of 0.75 V precharged the peripheral capacitors and set the lower limit for the on-chip digital-to-analog converters (DACs) used in PCM programming, and 0.3 V set the PCM read voltage and the ramp start voltage. These supplies were measured and reported in Fig. 6a as ‘Other’ voltage supplies. The three FPGAs were connected through the custom board and controlled by an x86 machine with a Peripheral Component Interconnect Express connector. All experiments were run using Xilinx MicroBlaze Soft Processor code and x86 MATLAB software wrapper (Extended Data Fig. 1c). The off-chip combined transfer bandwidth on our chip is 38.4 Gbps, with a total of 384 input–output pins capable of operating at 100 MHz. Extended Data Fig. 1d shows that routing precision, KWS and RNNT power measurements were run without any additional intermediate data being sent back to the x86 machine. The RNNT accuracy results used the x86 for vector–vector operations and tile calibration. To model such digital operations in terms of performance, we simulated a digital circuitry just outside the ILP–OLP, based on a foundry 14-nm process design kit to implement optimized digital pipelines, control logic and registers. A future chip will eventually include the digital circuitry close to the analog tiles20. Inputs were encoded as 8-bit digital words stored on an SRAM within each ILP. Conversion of 512 such digital words to 512 PWM durations was performed using clock-driven counter circuitry within each ILP. Data were then retrieved from the chip using the OLP, which internally performed the conversion from time to digital using 512 counters plus falling-edge detectors (Extended Data Fig. 2a). (a) Input Landing Pad circuitry receives an 8-bit word, which is converted to a duration using a counter. Output Landing Pad circuitry receives one duration and converts it into an 8-bit word using 512 counters plus falling edge detectors. (b) Each tile has 512x512 unit cells, each consists of 4 PCM devices controlled by Word Lines and Select Lines driven by the tile West circuitry. During weight programming, Return Lines (RL) are driven at higher voltage (max 3V) and VSIG are grounded to enable PCM write. (c) During inference, in the integration phase, RL are grounded and one or both VSIG are biased at read voltage V = 0.3 V using a per-column operational amplifier. Current is then mirrored into a per-column capacitor. (d) For duration generation phase, capacitor voltage is compared against a shared and tunable ramp-voltage. The output of the comparator is a pulse with duration proportional to the voltage on the capacitor, gated by an EN signal using an AND gate. By properly tuning the timing of this EN signal with the Local Controller, activation functions such as ReLU or hard sigmoid can be implemented. (e) 512 durations are then sent into the parallel 2D mesh using a per-column South-North routing circuit. (This same circuit is also used to send pulses onto each column for PCM programming purposes). (f) Finally, West-East Routing enables durations to access another tile and/or to simply move across the tile. Since each borderguard can independently block or pass signals, complex routing patterns including multi-cast and even concatenation are supported. Each analog tile consists of 512 × 512 unit cells (Extended Data Fig. 2b), each containing four PCM devices. Circuitry can implement a significance factor F > 1 but we adopted F = 1, meaning that G+/− and g+/− are the same, apart from intrinsic stochasticity. This enabled the implementation of 2-PCM-per-weight and AB methods, both requiring equal contribution from W and W . Word lines and select lines were controlled by the west circuitry, selecting whether two or four PCM devices were connected to the edge capacitor. During weight programming, signals VSIG1 and 2 were kept at ground. Only one of the four PCM devices was programmed each time, by selecting the word, select and return lines. Weight programming was done in an iterative row-wise fashion4. During inference, VSIG1 and 2 were biased at a read voltage, V , of 0.3 V, while signals RL1 and 2 were at ground. Inference was achieved in two steps (Extended Data Fig. 2c). During the integration phase, PWM pulses activated in each row for a time proportional to the desired input magnitude (unlike ref. 32, these durations were not converted to analog voltages using DACs). V was forced by a per-column operational amplifier, which biased the entire bit line. These pulses were buffered along the row to maintain pulse-width integrity. Although IR drops did occur along columns, the wide wires stopped them being critical to degradation of MAC accuracy, especially when compared with other more-important factors such as peripheral circuit linearity and saturation effects. Current was then mirrored into a per-column capacitor, which could be tuned by the LC by connecting up to 8 parallel metal-oxide-semiconductor capacitors, where each capacitor was 50 fF (we typically chose 250 fF). The choices of capacitor size and range of tunability were based on the available column area, the expected current in the array, the integration time and the mirror ratios achievable. The summation over an entire 512-row tile was performed fully in analog, without the need for partial summation in the digital domain. In the wide-input case involving two vertically neighbouring tiles (Fig. 1i), summation over 1,024 rows (or even 2,048 in the two 2-PCM-per-weight case) was still fully performed in the analog domain, without any intermediate digitization. For layers that used wide input, the read operation during closed-loop tuning used this combined configuration, allowing an individual weight to experience and correct for the same non-idealities that it would experience in the eventual inference MAC. This provided significant mitigation from additional MAC error induced by combining tiles. Depending on the sign of the input, the current could be steered to either charge or discharge the capacitor. After current integration, the tile was disconnected and the output duration was generated. During this step, a tunable ramp circuit, shared among all columns, set a linear voltage ramp that was compared with the voltage on the 512 peripheral capacitors (Extended Data Fig. 2d). For each column, the output voltage started high, and when the comparator switched, the output duration ended, determining the duration of that particular output pulse, which is similar to the approaches in refs. 33,34. Finally, an AND port enabled or disabled the pulse output. With proper enable signal timings controlled from the LC, activation functions such as ReLU or hard sigmoid could be implemented on chip. The 512 durations were produced in parallel, exiting the tile on 512 individual wires. Area-efficient design choices (such as the use of a common ramp generator circuit shared across all the columns, the elimination of a conventional ADC and associated digital registers, as well as optimized full-custom layouts) enabled dedicated per-column circuitry at pitch, without the need for column multiplexers. These generated durations left the tile and propagated towards the next tiles or the OLPs using the OUT-from-col path in Extended Data Fig. 2e. Per-column south–north routing circuitry allowed for full parallel duration processing, enabling either N–S or S–N connection (without entering the corresponding tile), collecting durations from the tile (OUT-from-col) or sending durations into the tile columns (IN-to-col) as used during weight programming4. Per-row west–east routing blocks enabled W–E or E–W duration propagation and IN-to-row communication, allowing durations to reach the rows inside an analog tile and/or to move across the tile to implement multi-casting (Extended Data Fig. 2f). A user-configurable LC on each tile (Fig. 2a) retrieved instructions from a local SRAM. Each very wide instruction word (128 bits) included a few mode bits, as well as the wait duration (in cycles of around 1 ns given the approximately 1-GHz local clock) before retrieving a next instruction. Although some mode-bit configurations allowed JUMP and LOOP statements, most specified which bank of tile control signals to drive. Most of the 128 bits thus represent the next state of the given subset of tile control signals. This approach allowed for highly flexible tests and simplified design verification, with a small area penalty compared with predefined-state machines. For example, the LC could configure 2D mesh routing to enable input access to analog tiles through the west circuitry (Fig. 2b) and MAC integration on the peripheral capacitors. The LC then configured the ramp and comparator used to convert the voltage on the capacitor into a PWM duration, avoiding energy-expensive ADCs at the tile periphery. Finally, the LC decided which direction (north, south, west or east) to send the generated durations, configuring the south 2D routing circuits4,33. The LC also configured the ‘borderguard’ circuits at the four edges of each tile to enable various routing patterns. For example, Fig. 2c shows how durations from odd columns in the top tile could be merged together with durations from even columns from the bottom tile. This configuration was used on the RNNT Dec chip (Extended Data Fig. 7c). Instead of processing Embedding Emb and Dec-LSTM0 W layers separately, we first compress in one single matrix the product Emb * W . At this point, the first Dec-LSTM0 shows Emb * W matrix with a (28*1280) size, contrasting with the (320*1280) W size to sum directly in analog. To balance signal magnitude, nine copies of Emb * (W /9) are programmed, achieving comparable weight absolute magnitudes. (b) Weight mapping and (c) signal routing implementing Asymmetry Balance. Routing from tiles to Output Landing Pads utilizes implicit vector concatenation on the 2D mesh, enabling more efficient data transport. (d) The processing of one full frame requires 5 time steps of 300 ns each. Inputs were transformed into durations in the ILP circuitry. Durations spanned between 0 and 255 ns, encoded using 8-bit words. To verify the reliability of these communication paths across the entire chip (Fig. 2d), we repeatedly multi-cast 512 input PWM durations from the southwest ILP to all six OLPs at the same time. These durations were uniformly randomly distributed between 0 and 50 ns at 1 ns granularity (1 GHz clock), and CDFs of the error between measured and transmitted duration across 2,048 vectors (1 million samples) are shown in Fig. 2d. This experiment was repeated for distributions spanning from 0 to 100, 150, 200 and 250 ns. The maximum error never exceeded 5 ns, with shorter durations exhibiting even smaller worst-case error (±3 ns), showing that durations can be accurately communicated across the chip. Although in this case errors were introduced by the double ILP–OLP conversion and unusually long paths, during conventional inference tasks, the MAC error was always dominated by the analog MAC. KWS is used in a wide variety of devices, such as personal and home assistants, to perform actions only after specific audio keywords are spoken. Latency and accuracy are important attributes. When used in an ‘always-ON’ configuration, raw power is also an advantage. When gated by a much simpler two-class front end that can detect audio input of potential relevance and wake up the multi-class KWS system, energy per task becomes the relevant figure of merit. The KWS network was trained using HWA techniques to make the network more resilient to analog memory noise and circuit-based non-idealities. We trained unitless weights on the interval (−1, 1) using weight clipping. In addition, we added normally distributed noise to these weights during each training mini-batch with a standard deviation of 0.02 (Extended Data Fig. 3a). We also added similarly distributed random noise with a standard deviation of 0.04 to output activations to mimic the imperfections expected from layer-to-layer activation transmission. We find that this simple noise model fits our analog system well and provides effective HWA training. We performed an extensive hyper-parameter search and picked a base learning rate of 0.0005 with a batch size of 250 for training. We found that including bias parameters for this network offered little benefit and therefore eliminated them from the model. We used adaptive moment estimation as the optimizer along with a weight decay (that is, L2 regularization) of zero. Finally, we used cross-entropy loss as our loss metric. The dependence of HWA accuracy for injected noise on weights and activations during training is shown in Extended Data Fig. 3b. Hardware-aware (HWA) training was applied to improve model robustness against hardware imperfections, primarily due to programming errors. (a) Weight programming correlation for the HWA model. (b) Dependence of model accuracy on injected noise on weights and intermediate activations during training. (c) Mel-frequency cepstral coefficients (MFCC), representing the fingerprint for each keyword, are flattened and truncated before input to the fully-connected network. (d) The full input would require 1960 rows, however, to reduce the model to 1024-inputs, pruning is performed by removing all the inputs exhibiting a mean absolute validation input value lower than the indicated threshold. (e) The table shows a comparison of the KWS models and accuracies. (f) Since KWS is fully end-to-end on-chip, an on-chip calibration process is performed at the tile, leveraging 8 additional PCM bias rows to shift the MAC up/down to compensate for any intrinsic column-wise offsets. Slope of the MACs is compensated by re-scaling weights per-column. Calibration is performed using validation input data; inference results are reported for the test dataset. The KWS network performed several preprocessing steps before feeding the data into the FC layers. Input data (keywords) represented 1-second-interval voice recordings encoded as .wav files at a 16-kHz sampling rate. We computed the audio spectrogram, which is a standard way of representing audio information using the squared magnitudes of fast Fourier transforms taken at multiple time steps, using a window size of 30 ms and a stride of 20 ms. We then computed the Mel-frequency cepstral coefficients (MFCCs), which are a commonly used nonlinear transformation that accurately approximates the human perception of sound. We used 40 cepstral coefficients or bins per time slice. We also clipped the MFCCs to the range (−30, 30) to avoid any potential activation-rescaling problems going into our HW. This preprocessing resulted in a two-dimensional MFCC fingerprint for each keyword with dimensions of 49 × 40 (Extended Data Fig. 3c), and this is then flattened to give a 1,960-input vector. We also randomly shifted keywords by 100 ms and introduced background noise into 80% (the majority) of the training samples to make keyword detection more realistic and resilient. To reduce the input size further and fit a 1,024-input-wide layer, we pruned the input data on the basis of the average of the absolute values of the validation input (Extended Data Fig. 3d). Pixels with average input intensity below a certain threshold were pruned, reducing the overall size to 1,024. Interestingly, pruning led to an accuracy improvement, as shown in the summary table in Extended Data Fig. 3e. Although our analog tiles can compute MAC on up to 2,048-element-wide input vectors, the AB method inherently uses both W and W . Thus the maximum input size over which fully analog summation can be supported is reduced to 1,024. Because the KWS network is fully on-chip, tile calibration needed to be performed in HW. A per-column slope and offset correction procedure was achieved in three steps. Weights were first programmed using the nominal target values. Next, 1,000 inputs taken from the validation dataset were used as input and the single-tile MAC results were collected to calculate the column-by-column slope scaling factors to be applied to the target weights. The tiles were then reprogrammed with the scaled weights. Finally, experimental MAC was shifted up or down by programming eight additional PCM bias rows available on each tile (Extended Data Fig. 3f). After tile calibration, the ReLU activation function was tuned using the same validation input and comparing the experimental result on validation data with the expected SW ReLU. The inference experiment was then performed on the test dataset. The calibration enabled compensation of column-to-column process variations and input-times-weight column dependencies (such as activation sparsity and residual weight leakage). As shown in the drift results on RNNT, tile weights typically showed good resilience to drift owing to the averaging effect. Bias weights required more-frequent updates, on the scale of days, to compensate for column drift, but this involved merely running a small inference workload and reprogramming the bias weights. Eventually, the tile weights also need to be re-programmed. Although we have not explored temperature-dependent conditions, we believe that the levels of PCM drift exhibited here would be sufficient to allow operation for a few days or even weeks, which is sufficient to keep model reprogramming for the purposes of PCM drift indistinguishable from model refresh for other purposes (such as resource balancing and model updates). To encode the MLPerf RNNT weights, we used five chips. Iterative weight programming enabled accurate tuning of the conductances to match the target weights. Heat maps correlating the target and the measured chip-1 weights on each of the 32 tiles are shown for W and W in Extended Data Fig. 4a,b. The corresponding error for each tile, expressed as the fraction of the maximum weight, is shown in Extended Data Fig. 4c,d for W and W . To compare the weight programming in the five chips used for the RNNT experiment, we calculated the CDF on the basis of the data shown in Extended Data Fig. 4c,d and extracted the spread between 1% and 99%. In this way, two data points were extracted for each tile, one for W and one for W . The chip analog yield, measured as the fraction of weights with a programming error of less than 20% of the maximum weight magnitude, is around 99% (Extended Data Fig. 4e). Chip 4 has a slightly lower yield because the corresponding maximum W, defined as the coefficient used to rescale weights from MLPerf (around [−1, 1]) to integers, is larger because more signal was required, causing greater weight saturation. Extended Data Fig. 4e shows the spread distributions for each of the five chips. Experimental correlation between target and programmed weights on chip-1 over 32 tiles for both (a) W and (b) W . (c),(d) The corresponding probability distribution functions (PDF) of errors, expressed as percentage of the maximum weight, reveal high-yield chips with very few erroneous weights. (e) Table showing the analog yield, or the fraction of weights with programming error within 20% of the maximum weight. After integration of the PDFs in (c,d), the corresponding cumulative distribution functions (CDFs) are computed and the 1%-99% spread is collected, providing 2 data points (one for W and one for W ) for each tile. The plot shows the corresponding CDFs for each of the five chips used in RNNT experiments. To control the peripheral circuitry saturation, some tiles have weights mapped into a smaller conductance range (Max W equal to 80), leading to a different 1%-99% spread, e.g. the points with increased spread on chip-1 CDF in (e). The RNNT encoder weights were mapped using the first four chips, as shown in Extended Data Fig. 5a. The large W and W matrices used for encoder LSTMs all show a size of 1,024 × 4,096 except for the conventional Enc-LSTM0 (W is 960 × 4,096) and Enc-LSTM2 (W is 2,048 × 4,096). Enc-LSTM0, Enc-LSTM1 and the W matrix of Enc-LSTM2 implement AB. In Enc-LSTM0, Enc-LSTM1 and Enc-LSTM2, summation of W and W MACs was performed off-chip at the x86 host, whereas chip 4, implementing Enc-LSTM3 and Enc-LSTM4, performed this entire summation on-chip in analog. Furthermore, blocks 1(−1), 9(−9) and 2(−2), 10(−10) of Enc-LSTM0 W and Enc-LSTM1 W , and blocks 1(9), 17(25) (W (W )) and 2(10), 18(26) were summed in digital after on-chip analog MAC. Finally, Enc-FC was implemented on chip 4. Any spot where tiles were connected by sharing the peripheral capacitor in the analog domain (Fig. 1i) is highlighted with a dark-blue bar. We did not map biases in analog memory but instead incorporated them in the already existing off-chip digital compute, by combining them into the calibration offset with no additional cost. Thus these biases were always applied with FP32 precision. No network retraining was applied. (a) RNNT Encoder LSTMs weights are represented by two large matrices, W which multiplies the LSTM input, x, and W which multiplies the LSTM recurrent signal, h. Depending on the sizes of x and h, a variable number of tiles is required. Both conventional and weight expansion mappings are shown for Enc-LSTM0. In addition, Enc-LSTM0, Enc-LSTM1 and the W of Enc-LSTM2 implement Asymmetry Balance. Tiles connected with a dark blue line have shared capacitors, enabling 2048-wide analog MAC. In Enc-LSTM0, Enc-LSTM1 and Enc-LSTM2, MACs from tiles 1,9 and 2,10 are summed in digital, while all the other pairs are summed on-chip in analog. (b) Every analog MAC on the encoder chips requires seven 300 ns time steps to process, including digitization of the output. During the first four steps, MAC operations are performed, providing input signals from ILPs as indicated in the figure. In cases where AB is used (Enc LSTM0, Enc LSTM1, W portion of Enc LSTM2), opposite-signed inputs are provided in two of the four MAC time steps. During the last three time steps, MAC results are sent out to the OLPs. To provide input data and collect MAC results in a massively parallel fashion from or to the ILPs–OLPs, complex routing paths were programmed, leveraging the flexibility of the LCs (Extended Data Fig. 5b). In the RNNT encoder, after each MAC, the data needed to go through input–output for off-chip digital processing. Each full operation (including input, MAC, duration generation and output digitization) took 2.1 μs. The input arrows show multi-cast in parallel to one or more analog tiles with MAC operations occurring on those tiles. Output MACs were provided to the OLPs in three time steps owing to the small number of OLPs. RNNT experiments implemented MAC on-chip, whereas tile affine calibration (shift and scale) and LSTM vector–vector computations were performed in SW (MATLAB SW running on x86). In particular, the first Enc-LSTM0 W required careful input-signal management to maximize the signal-to-noise ratio, owing to the large sensitivity of the WER to any noise on its weights. Extended Data Fig. 6a shows that, in the case of Enc-LSTM0 W , the input data, which naturally exhibits a wide dynamic range, was first shifted to zero-mean, followed by normalization to maximum input amplitude. The preprocessed input was then used for analog MAC. The MAC results were later denormalized back in SW, where the input mean contribution was added (which collapses to the product of one number, the mean value of the input image, and one vector, the sum of weights for every column) and the affine coefficients for calibration were applied. Due to the large sensitivity of the first Encoder LSTM W matrix, input data is digitally pre-processed to increase the MAC signal-to-noise ratio. (a) Using the original MLPerf weights, input data is first shifted to zero-mean and normalized to a common maximum input amplitude. Then, on-chip MAC is performed, followed by MAC digital denormalization and addition of mean Input*ΣW. This last operation collapses to a simple single number (mean Input) times one 4096-vector represented by the column-wise sum of weights, not requiring a full digital 1024*4096 MAC operation. Only after these steps the affine coefficients (slope and offset correction) are applied. (b) When using expanded weights, the mean removal is no longer needed since the random matrix M introduced in the main paper has zero mean, leading to naturally near zero-mean input values, thus simplifying input pre-processing. (c) All the other RNNT blocks (rest of Encoder and Decoder) only apply affine coefficients, without any normalization or mean removal. (d) Data-type conversion during the MLPerf RNNT inference: software computation is performed in FP32. Before access to the chip, data is converted into a pair of UINT8 vectors, one for each polarity of the incoming activations. This data is loaded on-chip in the ILP, then converted into time duration and used as MAC input (‘negative’ inputs are durations sent during the second and fourth integration steps, as shown in Extended Data Figs. 5b and 7d). MAC output is represented as analog voltage on a capacitor, then converted into time duration by the peripheral circuitry (details in Extended Data Fig. 2d). Finally, the OLP converts durations into UINT8. Data is sent off-chip and converted in FP32 during the calibration phase. (e) Overall view of the equations solved in the RNNT: essentially, all MACs are performed on chip, while all vector-vector operations, non-linear activations and biases are computed in software. The joint layer is implemented in software, as explained in Fig. 4. In the case of expanded weights (Extended Data Fig. 6b), the input first underwent MAC with the random matrix M (such a matrix has random normal weights but is fixed across all inputs). Because the product of an input with a matrix with zero mean value generates an output with near-zero mean value, there was no need to apply the zero-mean shift, although normalization to maximum amplitude was still performed. After the analog on-chip MAC, the results are denormalized and the usual calibration was applied. For every other layer (Extended Data Fig. 6c) in the RNNT, the inputs were used directly as tile activations and the MAC was calibrated with the usual affine coefficients. All affine coefficients are calculated by comparing experimental and expected SW MAC using 2,000 input frames from the training dataset for each Enc–Dec layer. Data were linearly fitted to obtain the slope and offset coefficients. Extended Data Fig. 6d shows a detailed description of all data-type conversions. All SW computations were performed in FP32. For transmission to the chip, data were converted into INT9 (UINT8 plus sign) and UINT8 vectors were loaded into the ILP. Here, durations were generated and sent to the tiles where the analog MAC was performed, collecting an analog voltage on a peripheral capacitor. Once the UINT8 vectors were loaded into the ILP, ‘negative’ durations were sent during integration of the second or fourth time step, as shown in Extended Data Figs. 5b and 7d. Finally, charge integrated onto column-wise capacitors was converted by the peripheral circuitry into durations that were sent to other tiles or to the OLP, which converted them back into UINT8. Data were then sent off-chip and transformed back into FP32 during the calibration stage. Extended Data Fig. 6e shows a summary of the equations, highlighting that essentially all MACs were performed on-chip, whereas vector–vector, bias and nonlinear activations were computed in SW. The joint layer was in SW. Extended Data Fig. 7 shows the details of Dec mapping and signal routing. To account for the Emb layer (Extended Data Fig. 7a), we first collapsed Emb and Dec-LSTM0 W layers into a single Emb × W matrix with size 28 × 1,280, which receives one-hot input vectors. This multiplication is perfectly equivalent in SW, but led to large weights in the Emb × W matrix compared with W , as shown in the first set of CDFs, reporting the maximum weight for each column. Because MAC results from Emb × W and W are summed directly in the analog domain with a shared capacitor, weight values cannot be arbitrarily scaled. To overcome this problem, 9 copies of the 28 × 1,280 Emb × W matrix were programmed and the 28 inputs duplicated onto 9 × 28 rows, leading to a similar amount of signal with W . This allowed us to effectively distribute these large weights over 9 unit cells, while ensuring that the analog summation will aggregate both the Emb × W and the W contributions with the correct scaling. Dec weight mapping used AB (Extended Data Fig. 7b) and signal routing enabled parallel input and output of all signals (Extended Data Fig. 7c). Here, routing concatenation was used to efficiently combine the signal from two different tiles into the same OLP. The full input–MAC–output processing time is 1.5 μs (Extended Data Fig. 7d). Unlike the KWS experiment, the MLPerf repository mandates that inference be performed with the validation dataset. The RNNT MLPerf inference experiments shown in Fig. 5 were done by inputting the full validation dataset into the first chip, saving the output results on the x86 machine, swapping in the second chip and continuing the experiment, using the previously saved outputs as new inputs. This procedure was repeated for all five chips, ensuring a consistent example-by-example cascading, as in a fully integrated system. Mapping even-larger models, using a weight-stationary configuration, can be supported with improved memory density (including stacking of multiple layers of PCM in the back-end-of-line), multi-chip modules and even multi-module solutions, with careful neural-network partitioning to minimize inter-module communication that would be energy expensive. Experimental MAC details are shown in Extended Data Fig. 8. The error distributions and MAC correlations are shown for every chip. In all figures, a dashed region highlights the main regions of interest for that MAC. For LSTM layers, the region of interest corresponds to the [−5, 5] range, because outside that range the ensuing sigmoid or tanh function can be expected to fully saturate (for example, the output will always be −1 or +1, being almost completely independent of any variations on the input). Similarly, the regions of interest for the FC layers are mostly the positive MACs because of the ReLU activation function. In this specific case, Enc-FC and Dec-FC are summed before ReLU, so slightly negative contributions could also matter. We plotted the regions of interest to be where MAC > −5. The reported standard deviation σ computes the error for SW MAC in [−5, 5] for LSTMs and [−5, inf] for FC layers. Comparison between the original W and the weight-expanded W for Enc-LSTM0 is also provided. Extended Data Fig. 9 shows examples of transcribed sentence output from the experiments in Fig. 5 that show an almost iso-accuracy WER. Transcription results are in good agreement between the MLPerf RNNT model implemented in analog HW and in SW, indicating that the effective bit-precision of our HW demonstration is n = 4.097 for 9.475% WER and n = 4.153 for 9.258% WER (weight expansion), on the basis of comparison with the full network (no joint FC) curve in Fig. 4c. (a) Error histogram and (b) correlation between experimental and target MAC are shown for every chip used during the RNNT inference experiment. On each correlation, white dotted lines highlight the main region of interest, since MACs are followed by sigmoid, tanh or ReLU which naturally filter out portions of MAC. The spread, σ, is calculated only within the highlighted Regions Of Interest (ROI). Data from both original Enc-LSTM0 and weight-expanded Enc-LSTM0 are reported, showing a better sigma for the weight expanded case. Enc-LSTM2, Enc-LSTM3, and Enc-LSTM4 show larger spread due to partial (Enc-LSTM2) or no (Enc-LSTM3, Enc-LSTM4) application of Asymmetry Balance. In addition, Enc-LSTM2 MAC is calculated on larger (3072 instead of 2048) inputs. Finally, decoder layers show larger σ, maybe caused by higher capacitor/Output Landing Pad saturation effects, which however have little impact on the overall WER, as revealed by the accuracy results in the main paper (Fig. 5a,b). First ten transcribed sentences from the Librispeech validation dataset. The first line shows the reference sentence, the second line corresponds to the MLPerf baseline prediction (WER 7.452%), and the third line shows the sentences produced by our five-chips experiment including the weight expansion technique (WER 9.258%). The proposed 5-chip RNNT implementation is not integrated with digital processing, but we can estimate the time needed to process the entire dataset by combining the MAC processing times and energies from the analog chips with the estimated digital processing times and energies that we tabulated previously in our architecture paper20. Extended Data Fig. 10a shows a timing simulation describing the execution of RNNT layers for processing all 2,513 input audio samples, accounting for all pipelining, time stacking, recurrence and Dec steps. We assume times of 2.1 μs and 1.5 μs for the Enc and Dec layers, respectively, which includes all duration generation, and a relatively conservative 300 ns for the digital processing of each layer. Given these assumptions, the entire dataset can be evaluated in 1.2877 s, corresponding to a rate of 1,951.59 samples per second. Combined with the power measurements below, these numbers can be used to extrapolate the analog-AI RNNT system performance. To calculate processing time for RNNT on an integrated system as described in Fig. 6, (a) a simulator based on the MAC runtime on the actual chip and plausible digital processing is considered based on (b) specific timing assumptions stemming from our experiment and prior architectural work20. (c) Detailed breakdown of operations and energy across the 5 chips, including additional digital operations required to process activations from chips. (d) Total on-chip and off-chip number of operations and energy, including measured analog operations (this paper) or estimates for digital ops20. (e) Comparison with MLPerf submissions on RNNT shows a 14 × advantage in energy-efficiency. Power measurements for RNNT were done using a set of 32 exemplar input vectors that filled up the ILP SRAM to capacity. By overflowing the address pointer of the ILP, it is possible to repeat the same set of 32 vectors ad infinitum. Together with JUMP instructions in the LCs resetting the program counters to the start of program execution, this allowed a real-time current measurement from the voltage supplies for the inference tasks. In these measurements, all 7 (or 5) phases of the Enc (or Dec), including 4 integration phases and 3 (or 1 for the Dec) duration generation phases were included. This accounted not just for the MAC integration, but also for the subsequent cost of generating, transporting and digitizing the MAC results. The measured powers are shown in Fig. 6a. Using the energy and execution-time models from our architecture study20, the total digital energy (for all the tasks performed off-chip in SW to support the experiments shown in this paper) is estimated to be 0.11 J for nominal Enc-LSTM0 and 0.26 J for weight-expansion Enc-LSTM0. The total number of digital operations and a detailed breakdown are shown in Extended Data Fig. 10c,d. Although several compute-in-memory or near-memory approaches based on SRAMs and digital compute35–38 have been presented in the literature, most of these do not address the energy and time costs of reloading weights, thus making direct side-by-side comparisons against NVM-based weight-stationary approaches difficult. However, several NVM compute-in-memory studies have focused on the macro-level32,34,39,40,41, without accounting for data transport, control or chip infrastructure (such as clocking) costs. They are also usually at a much smaller scale (sometimes less than 1 million parameters7) than the work here, making a fair assessment of both the accuracy of large models and the associated sustained TOPS/W values difficult. We have instead compared our sustained power and performance values against other reported system numbers for the same RNNT task from MLPerf, as shown in Extended Data Fig. 10e. By weighting the sustained power measurements for individual chips with their corresponding activity factors from the timing simulations shown in Extended Data Fig. 10a, the total system energy and corresponding aggregate TOPS/W values for our system are calculated to be 4.44 J and 6.94 TOPS/W, respectively (4.60 J and 6.70 TOPS/W for W ). Although our evaluations in Fig. 6 do not include some external components used in real systems, such as system buses and voltage regulators, this TOPS/W energy efficiency is still more than an order of magnitude better than the best published result for this task. The relatively small number of digital operations in the network implies that considerable benefits may yet be obtained by improving the raw analog MAC energy efficiency (currently 20 TOPS/W). This could be enabled by shorter integration times, more-efficient analog opamps and/or lower-conductance devices. Instead, a substantial drop-off in energy efficiency, down to 12.4 TOPS/W for chip 4 (Fig. 6c), occurs as a result of the on-chip infrastructure, such as the landing pads, which need to be exercised at the end of each MAC. This highlights the need for on-chip digital compute cores, potentially in proximity to the same chip, and using the same local 2D mesh for data transport as described in our architecture study20. MLPerf submissions for RNNT exhibit performance efficiencies ranging between 3.98 and 38.88 samples per second per watt, using system power that ranges from 300 to 3,500 W, assuming the use of large batches to maximize efficiency. Our work inherently assumes a mini-batch size of 1. Although we assume that additional samples are available to keep the pipeline full, our projections are effectively independent of mini-batch size. Under these conditions, an analog-AI system using the chips reported in this paper could achieve 546.6 samples per second per watt (6.704 TOPS/W) at 3.57 W, a 14-fold improvement over the best energy-efficiency results submitted to MLPerf. Reduction in the total integration time through precision reduction, hybrid PWM40 or bit-serial schemes can improve both throughput and energy-efficiency, but these could suffer from error amplification in higher-significance positions. Future efforts will need to address their impact on MAC accuracy for commercially relevant large DNNs."
    },
    {
        "link": "https://prodigytechno.com/how-ai-edge-voice-chip-company-accelerated-the-post-silicon-bring-up",
        "document": "How did the AI Edge voice chip Company accelerate the Post Silicon bring up?\n\nA startup was working on AI Edge-based voice processor chip. The voice-based AI chip is designed to consume the lowest power as it’s used in consumer headphones or earbuds. The startup was about to tape out the chip and was preparing for a post-silicon bring-up. It faced very significant barriers in the development phase with I/O peripherals like I2C and SPI and using Prodigy protocol analyzer and exerciser it was able to quickly solve some challenges and get ready for the post-silicon validation. The older generation chip was done in 40nm and the latest migration after testing and getting the confidence was to migrate to a 7nm process node.\n\nThe AI processor is one of the first-generation chips that enables complete voice control for consumer products. The consumer devices can be smart speakers, remotes, earbuds, and automotive solutions. The AI processer does the inference and training to enable autonomous speech recognition without sending audio to the cloud for further processing.\n\nThe AI Processor details are as follows:\n\nThe startup was in the tape-out phase of its design when it approached Prodigy Technovations for solutions to help them verify the feasibility of its design. One of the major requirements was to help in testing their standard peripherals which would be used in conjunction with the AI processor using serial communication protocols like I2C, SPI, QSPI, and I2S. The peripherals in the picture were EEPROMs, SDIO’s, and FLASH memory which were being used in applications involving video processing and sensor fusion. The AI processor was to act as the master and the consecutive peripherals as slaves.\n\nHence, Prodigy’s I2C SPI Exerciser and Protocol analyzer proved to be a handy tool in debugging design issues and observing how the slaves behave under different test cases our product was capable of generating. In this case, the prodigy I2C SPI protocol analyzer would help with decoding the protocol bus and reporting and monitoring, and capturing any bus transaction-level issues.\n\nAlso, another requirement put forward was to be able to use the Prodigy I2C SPI Exerciser as a slave and their AI-based processor as a master, to observe different scenarios and do the timing control on bus signals and also improve any development issues on the peripheral during the post-silicon bring up.\n\nSome of the key changes for SPI and I2C Protocol were\n• None Provision for BYTE timing delay and delay between transactions.\n• None Validation of Basic read and write test transactions and integration them via API into the development flow to automate testing\n\nThe slaves used were I2C Atmel and SPI Winbond. The read and write operations were not in a proper sequence. On investigation using the protocol analyzer and exerciser, it was found that the datasheets of the corresponding slaves were not referred to by the design team. The Prodigy protocol analyzer and exerciser were not only able to debug the issue but also gave the flexibility to figure out issues in the development team. Check out our protocol analyzer and exercisers."
    },
    {
        "link": "https://datacamp.com/blog/ai-chips",
        "document": "Discover how to extract business value from AI. Learn to scope opportunities for AI, create POCs, implement solutions, and develop an AI strategy."
    },
    {
        "link": "https://medium.com/@adi.fu7/a-future-of-ai-through-the-semiconductor-looking-glass-ca24451517ae",
        "document": "It has been a while since I sat down to organize my thoughts and wrap them in a blog post. It seems that every time I publish a new post, the AI semiconductor world has drastically changed since my last one, and this time — AI compute demands and costs are off the charts, so it seems like there has never been a more interesting time to talk about AI in the context of compute.\n\nIn this post, I explore the relationship between AI and accelerated processors. I will show why accelerated hardware is advocated as one of the promising drivers of the semiconductor industry in a post-Moore’s law era and how it plays a crucial role in the renaissance of modern AI. I will also show where accelerated processors fall short, how their limitations apply to the AI landscape, and how they will likely affect the future progress of the AI industry. I conclude the post with a discussion on what future directions the AI industry should pursue to achieve an evermore capable AI.\n\nOnce again, as I am still trying to preserve my personal touch in these posts — I still did not use any LLMs to generate the textual content of this blog, so it might be less polished, but (hopefully) with fewer hallucinations. I hope this will be an enjoyable and useful read!\n\nThe Three Pillars of Modern AI\n\nArtificial intelligence has intrigued our imagination for over 50 years. People have been overly excited about the prospect of intelligent machines that interact with us, serve us, and can change many aspects of our daily lives to enhance our human experience. During these years, many science fiction novels have been written on artificial intelligence, and countless scientific experiments explored ways to make AI useful. Through the years, the scientific community’s approach to AI has shifted between optimism and harsh realism when most ideas to transform AI from obscurity to practice have reached a dead end or were deemed impractical. As AI scientists and practitioners failed to live up to the hype, the field has descended into decades of lower interest and experimentation. These time periods were known as “AI winters”.\n\nAround 2010, optimism returned to the AI field with the rise of a new sub-discipline called “Deep Learning” which involved stacking multiple layers of neural networks. Three pillars laid the foundations for deep learning and made the different pieces finally click:\n\nThe Algorithmic Pillar: New algorithmic discoveries enabled new applications and formed a new means of statistical computing, an alternative to traditional “expert systems” which are algorithms and software engineering solutions for precise computing.\n\nIn the past decade, we saw many novel studies that demonstrated algorithmic breakthroughs that use neural architectures in several fields, most notably (but not limited to) computer vision and natural language processing.\n\nThe Data Pillar: At the core of neural architectures as means of statistical computing lies the data-driven approach, also known as “Software 2.0”. In software 2.0, rather than explicitly defining a set of “rules” for the specific computation task, the task is defined and constructed in a “learn by example” fashion; the neural architecture is trained on prior examples of {input → output} solutions to the task, and based on the neural weight, the architecture can later approximate solutions, i.e., provide outputs to new inputs. The ability of a neural architecture to provide good approximations to never-before-seen inputs depends on a sufficiently large set of examples. In other words, neural networks need lots of data to be effective.\n\nAs technology becomes more immersed in our daily lives, more sensory interactions are being recorded, digitized, and stored in cloud entities. As a result, copious amounts of data are now stored in structured and semi-structured datasets, providing more inputs for existing tasks (e.g., more images to train computer vision networks) as well as new input sets to new application domains (e.g., recorded LIDAR data in autonomous driving cars).\n\nThe Compute Pillar (The Topic of This Post): Finally, it would not be possible to experiment with new algorithms on large amounts of data without the availability of powerful compute fabrics (high-end silicon chips, memories, and interconnect); if you cannot meet the computing demands for training your algorithm on your dataset (or inferencing it on new data), it would not be possible to prove that your neural architecture actually works.\n\nOne notable example of how computing has given a much-needed boost to algorithms is the revolutionary AlexNet case. As presented in an earlier blog post, many of AlexNet’s algorithmic foundations were known years before it was published; people have experimented with CNN architectures for image recognition at small scales, most notably LeNet in 1998, which was simpler and was demonstrated on a smaller dataset called MNIST. As AlexNet was one of the first studies to run CNNs on highly parallel processors called graphics processing units (or GPUs) the authors could train on large amounts of data efficiently and within a reasonable timeframe. So these were the processors, or specifically GPUs, that made a radical paradigm shift in the world of deep learning and became the enablers and drivers of revolutions in deep learning, large language models, generative AI, and more. GPUs are a class of highly parallel processors stemming from a broader notion of specialized processors, also known as hardware accelerators (or simply: accelerators). In a past blog post, I did an overview of what accelerators are, and how the semiconductor landscape gravitated towards accelerators (if you are not familiar with the concept of accelerators, feel free to go back and read the post).\n\nTherefore, if accelerators are both the pillar and the driver of AI, it would be worth exploring their past contributions in other fields to assess how they will shape AI for years to come.\n\nNowadays, accelerators (GPUs, FPGAs, and ASICs) are everywhere, and that is by no means a coincidence — in many cases, it is a necessity. In my view, that accelerator premise is as follows: “As semiconductor Scaling Laws (e.g., Moore, Dennard) no longer stand, silicon will stop exponentially improving. Therefore, the way to continuously improve chip gains is by using silicon more efficiently, i.e., by developing specialized hardware accelerators.”\n\nCompared to CPUs, accelerators trade programmability and applicability for efficiency by customizing their compute engines, on-chip memories, and on-chip interconnects to a targeted (narrower) application scope. (VERY) Roughly speaking, much of the gains of acceleration come from narrow data paths and interfaces (for example, compact numeric representations like INT8), customized arithmetical units to both support special numeric representation and the fusion of multiple operations, customized memory banking and management, and importantly — compute parallelism. In contrast to traditional CPU applications, accelerated applications like AI have an abundance of parallelizable computations and therefore scale better on silicon. i.e., parallel applications are more likely to benefit from having more silicon (e.g., more cores) than their CPU-based counterparts that are not parallelizable. As such, accelerated hardware has been advocated as the natural successor to Moore’s law in the silicon world that will be shaped in a post-Moore’s law era.\n\nThe Bad News (Or: why am I skeptical) In contrast to decades of Moore’s law, specialization does not scale exponentially; when you specialize your chip, you do not change the laws of physics; you’re not creating new transistors, you do not make them go faster, nor making them more power efficient. Given an application domain and a silicon budget, a team of computer engineers and scientists searches for solutions in the application/silicon optimization space. For the accelerator approach to match the exponentially improving powers of Moore’s law, one would need to find a solution that is exponentially better than all previously found solutions within the same silicon-application optimization space, every two years or so. At some point, that becomes impossible because you cannot expect to solve the same problem (any problem) exponentially better every time. You will get better at solving it, and at some point, you will exercise all your capabilities. The same goes for designing accelerated hardware; there is a finite number of ways to express a given computation problem and map it to a chip under a given silicon budget (say, 100 million transistors). One can think of it as a problem of lossless compressing of data from a specific domain (e.g., pictures or audio files) to a given storage device; you can explore better compression algorithms and techniques, but ultimately, you will converge on one that compresses data well, and at which point you will not be able to store more data on that device simply because efficiency does NOT scale exponentially.\n\nQuantifying Acceleration: How Efficiently Do We Use Our Silicon?\n\nAccelerators employ specialization to improve silicon efficiency and keep the semiconductor industry going in a post-Moore’s law era. It is, therefore, worth exploring exactly how (and if) that premise empirically holds.\n\nA few years back, I led a research project that quantified how much accelerators improve from the silicon standpoint; in other words, for a given application, how much chip specialization improves over time, empirically. To do that, we used thousands of chip specs and constructed a model that estimates silicon behavior in terms of speed, power, and energy. We used that model to examine hundreds of accelerator chips in a variety of popular accelerator application domains. Our goal is to decouple two factors that contribute to application gains: better silicon and better specialization. To avoid getting too technical, I am showing here the mere principle and not our entire thought process and rigorous formulation. I use the term “performance” broadly, but it could mean throughput, speed, throughput per power / per area / per energy, etc.\n\nThe above equation is mathematically trivial, yet it conveniently separates three factors: chip performance, chip silicon, and performance per silicon. The first two factors are known: (i) Chip performance could be the attained performance number on real applications, like a chip that trains ResNet-50 at a throughput of 3000 samples per second, or it could be a number specified by the vendor, like peak throughput, for example, a chip that can deliver a maximum of 400 BF16 TFLOPs. (ii) Chip silicon is driven mainly by better-grade silicon, e.g., newer CMOS nodes from newer silicon foundries that produce faster transistors, more active transistors per power, etc. We use our constructed model and the chip’s physical properties (e.g., number of transistors, CMOS node, thermal power budget, etc.) to produce a measure that serves as the chip’s CMOS potential. (iii) Finally, the third factor is the most interesting one: performance per silicon, which means “How well does the chip perform compared to its silicon budget?” the idea here is to account for a fair comparison of chips, which surprisingly gets glossed over too often. When benchmarking chips, one should not only account for the attained performance but also how it was attained. For example, one cannot the throughput of a 100 million 32nm transistor chip and a 40 billion 5nm transistor chip, without accounting for the vastly different silicon properties of the two chips. Furthermore, the performance per silicon reflects the quality of the accelerated stack, i.e., better architectural primitives, better compilation frameworks, more appropriate structural sizing of hardware blocks, better chip design disciplines and tools, and a better mixture of compute vs. memory vs. communication. Ultimately, this metric is the performance gained by chip specialization quality, and that is why we refer to it as “chip specialization return” (CSR). While it is generally hard (if not impossible) to quantify the factors that contribute to specialization return, when using the two other known factors of the equation (performance and silicon potential) one can project the trends for all factors given an accelerated application and group of accelerator chips. Finally, in a post-Moore’s law era, specialization might be the only way for silicon to improve, and it is worthwhile to see how specialization design cycles behave, as these would provide a good glimpse into the future of the semiconductor industry.\n\nAcceleration Scaling Trends: Where (And Why) Do Accelerators Fall Short?\n\nAI is not the first application that highly depends on accelerated hardware. There are many specialized processors already around us like signal processors which were first used in the 1970s in TVs, radios, and Radars, and are now used in a wide range of consumer electronic devices like smartphone cameras, speech recognition devices, and so on. However, as noted in the previous section, specialization scaling trends behave differently than their transistor scaling counterparts, and I will demonstrate this using the “transistors vs. CSR” equation presented for two popular applications.\n\nGames are the classic case of extensive use of GPUs; the video gaming industry is pulling many billions of dollars per year. As the graphics engines in games are getting more sophisticated, graphic rendering tasks are getting more compute-hungry and require better-grade GPUs.\n\nThe above example examines several video games. As shown, throughout six years in graphics frame rates have improved by about 4–5x, but when accounting for silicon changes, meaning, newer GPUs also have more transistors, more cores, and faster circuitry (as they were fabricated using newer transistor technology.) the speedup rates from silicon alone were around 4x while the silicon-relative performance (or chip specialization return) improved at much more modest rates of at most 1.27x. The main reason for this behavior is that the domain of graphics is both well-studied (not much to innovate) and massively parallel, which means that applications are likely to scale well as more cores are added — as a result, GPU vendors are incentivized to pack more cores on a chip since that will have a greater impact on performance than most micro-architectural optimizations.\n\nAnother application space that became radically dependent on accelerated hardware was Bitcoin. At a (very) high level, new blocks are added to the Bitcoin network following a process of “mining” which involves discovering a new cryptographic hash via a brute-force computing of many SHA256 functions. Mining is a compute-intensive process that can cost a lot due to wasted power and to cut the cost-to-profit ratio, miners explore various mining hardware solutions. We study the properties of hundreds of chips ranging from general-purpose CPUs to GPUs to FPGAs and the most efficient yet function-specific ASIC-based miners.\n\nEach bright dot in the figure signifies the throughput per (chip) area of a different Bitcoin miner; a dark dot in the same x coordinate indicates the relative chip specialization return of that miner (compared to a baseline CPU miner); the bright and dark arrows show the trend lines for performance and specialization improvements, respectively. The figure encapsulates many details; however, several insights are reflected: 1. ASICs do perform incredibly well, as they outperform CPUs by a rate of about 700,000x 2. Switching to a new architectural platform, i.e., moving from CPU to GPU or GPU to ASIC, gives a boost in specialization (each dark arrows start at a much higher mark than the other), but: 3. after switching to a new platform, specialization does not improve (dark arrows are mostly flat). This experiment highlights a typical life cycle of accelerated hardware: a team discovers new ways to map the computation problem to a new chip architecture and gets a boost in specialization, but following a few chip design iterations specialization returns diminish because there is not much innovation left to drive further architectural gains.\n\nThe study explored other accelerators and applications, and they all demonstrated similar phenomena that are in line with the insights shown here. To summarize, I list the pitfalls that were inherent limiters for chip specialization return, as observed by the study.\n\n(i) Overly Relying on Parallelism: while parallelism is a sure performance win for most accelerated applications, it is not a measure of efficiency or specialization. Accelerators give one the ability “to do more (compute) with the same cost (silicon, power, money, etc,)” While parallelism is the ability “to do more with more cost (more cores → more silicon)” which does not abide with the accelerator premise: if transistor scaling stops you will not be able to pack more cores and you will not be able to drive chip parallelism beyond a certain degree, without increased costs.\n\n(ii) Design Maturity: Chip development is a HARD business. It takes years of engineering effort and potentially several design generations to optimize a line of chips to a targeted application domain. As development moves forward, the domain matures — and so are the hardware/software co-design practices. From this point, the organization and its engineers converge to a single stable architectural solution and move to an “evolutionary and not revolutionary” path simply because it becomes very costly to break out entirely and explore a different hardware/software design optimization space.\n\n(iii) Domain Confinement: In some applications, computation is dominated by a handful of patterns of blocks. For example, Bitcoin mining is an application mostly dominated by a single computation, which is the SHA256 hash calculation. The most efficient ASIC bitcoin miners are simply packing many SHA256 cryptographic engines in parallel with a shared control mechanism for management. As there are not that many ways to devise a computation circuit that does SHA256, the efficiency of a single SHA256 computation cannot improve by much. This last limiting factor brings us back to the world of AI.\n\nMuch like the dominance of SHA256 calculations in Bitcoin mining, modern AI applications are dominated by a specific type of computational block (or pattern) which is matrix multiplication. Convolutional neural network models are lowered to groups of matrix multiplications, and transformers-based models use matrix multiplications that are a part of the multi-head attention mechanism. Hardware optimization of matrix multiplication is more complicated than SHA256; the design space involves not only computing, but also data accessing, scheduling, and communicating. Nevertheless, the presence of a dominating block would eventually result in convergence to specific design points without the ability to make further progress.\n\nMatrix multiplications became prominent in AI (and other domains like image processing and scientific applications) because they can be computed efficiently using a special circuit called a systolic array. I briefly mentioned systolic arrays in a post on architectural foundations. Systolic arrays exploit the regular nature of matrix multiplication, i.e., data gets accessed and deterministically communicated between multiply-and-accumulate nodes. The systolic structure reduces off-chip memory access costs by a factor of N, which is the number of times the data gets reused between multiply-add nodes before it gets written back to the memory.\n\nSince modern AI applications (like state-of-the-art LLMs) require a lot of memory, as reflected in the table above, it is very beneficial to run them on systolic arrays since these arrays convert the most energy-consuming operation, which is off-chip memory access, to a pipelined register access, which is hundreds of times cheaper.\n\nHowever, it seems that the AI hardware industry is moving toward a “systolic lock-in”; a systolic array achieves an unparalleled reduction in memory costs → consequently, most (if not ALL) AI accelerator designs are centered around systolic circuitry → therefore, AI researchers only explore models that heavily rely on matrix multiplication, and cannot pursue other directions of a potentially more general form of artificial intelligence, simply because there is no hardware that can run these efficiently → since no one explores algorithms that are not matrix multiplication based, it is hard to incentivize anyone to build a different type of accelerated hardware since there are no new algorithms to provide clear design assumptions and guidelines.\n\nAs we have seen here for the other accelerated hardware domains, the benefits of having the same architectural primitives (in this case, systolic-based accelerated hardware) are limited. Furthermore, with optimized stacks, TPU and GPU FLOP utilization rates are already over 50%, and therefore, gains are expected to stop improving soon enough. As we have seen in the Bitcoin mining case, the dominance of matrix multiplication produces domain confinement, meaning, a computational building block that we know how to build efficient circuitry for, and that defines the minimal cost of running that deep learning application. If you want to run bigger models that use X times more matrix multiplications it will cost you at least X times more in energy, and this is where things get tricky and interesting."
    },
    {
        "link": "https://milvus.io/ai-quick-reference/what-are-the-best-practices-for-training-speech-recognition-models",
        "document": "Training speech recognition models effectively requires a focus on data quality, model architecture, and iterative refinement. Start by curating a diverse, high-quality dataset that represents real-world scenarios. For example, include audio samples with varying accents, background noises, and speaking speeds. Preprocessing steps like noise reduction, normalization, and segmentation are critical to ensure consistency. Tools like LibriSpeech or Common Voice provide open-source datasets, but augmenting them with domain-specific data (e.g., medical terms for healthcare applications) improves relevance. Avoid over-reliance on synthetic data, as it may not capture natural speech nuances.\n\nNext, choose an appropriate model architecture and training strategy. Convolutional Neural Networks (CNNs) paired with Recurrent Neural Networks (RNNs) or Transformers are common for capturing temporal and spatial audio features. For example, architectures like Wav2Vec 2.0 use self-supervised learning to pretrain on unlabeled audio before fine-tuning on labeled data, reducing reliance on annotated datasets. Use connectionist temporal classification (CTC) loss or attention mechanisms to align audio sequences with text outputs. Optimize hyperparameters like learning rate and batch size through grid search or automated tools like Optuna. Training on GPUs or TPUs accelerates experimentation, but ensure batch sizes fit hardware constraints to avoid memory issues.\n\nFinally, validate and iterate continuously. Measure performance using metrics like Word Error Rate (WER) and Character Error Rate (CER), but also test with real users to uncover edge cases. For instance, a model trained on clean studio recordings might fail in noisy environments like cafes. Deploy A/B testing to compare model versions in production. Regularly retrain the model with new data to adapt to evolving speech patterns or vocabulary. Tools like Kaldi or ESPnet simplify pipeline management, while frameworks like PyTorch or TensorFlow offer flexibility for customization. Address biases by auditing datasets for underrepresentation of certain demographics and adding targeted samples. Iterative improvements, combined with rigorous evaluation, ensure the model remains robust and practical."
    },
    {
        "link": "https://blog.milvus.io/ai-quick-reference/what-are-the-best-practices-for-training-speech-recognition-models",
        "document": "Training speech recognition models effectively requires a focus on data quality, model architecture, and iterative refinement. Start by curating a diverse, high-quality dataset that represents real-world scenarios. For example, include audio samples with varying accents, background noises, and speaking speeds. Preprocessing steps like noise reduction, normalization, and segmentation are critical to ensure consistency. Tools like LibriSpeech or Common Voice provide open-source datasets, but augmenting them with domain-specific data (e.g., medical terms for healthcare applications) improves relevance. Avoid over-reliance on synthetic data, as it may not capture natural speech nuances.\n\nNext, choose an appropriate model architecture and training strategy. Convolutional Neural Networks (CNNs) paired with Recurrent Neural Networks (RNNs) or Transformers are common for capturing temporal and spatial audio features. For example, architectures like Wav2Vec 2.0 use self-supervised learning to pretrain on unlabeled audio before fine-tuning on labeled data, reducing reliance on annotated datasets. Use connectionist temporal classification (CTC) loss or attention mechanisms to align audio sequences with text outputs. Optimize hyperparameters like learning rate and batch size through grid search or automated tools like Optuna. Training on GPUs or TPUs accelerates experimentation, but ensure batch sizes fit hardware constraints to avoid memory issues.\n\nFinally, validate and iterate continuously. Measure performance using metrics like Word Error Rate (WER) and Character Error Rate (CER), but also test with real users to uncover edge cases. For instance, a model trained on clean studio recordings might fail in noisy environments like cafes. Deploy A/B testing to compare model versions in production. Regularly retrain the model with new data to adapt to evolving speech patterns or vocabulary. Tools like Kaldi or ESPnet simplify pipeline management, while frameworks like PyTorch or TensorFlow offer flexibility for customization. Address biases by auditing datasets for underrepresentation of certain demographics and adding targeted samples. Iterative improvements, combined with rigorous evaluation, ensure the model remains robust and practical."
    },
    {
        "link": "https://medium.com/@sujathamudadla1213/how-to-optimize-speech-recognition-algorithms-c0ec441a6563",
        "document": "M.Tech(Computer Science),B.Tech (Computer Science) I scored GATE in Computer Science with 96 percentile.Mobile Developer and Data Scientist."
    },
    {
        "link": "https://restack.io/p/integrating-ai-with-voice-recognition-technology-answer-best-practices-tuning-cat-ai",
        "document": "Systematic Exploration of Dialogue Summarization Approaches for Reproducibility, Comparative Assessment, and Methodological Innovations for Advancing Natural Language Processing in Abstractive Summarization\n\nData preprocessing is crucial to ensure the quality of input for our models. We will start by cleaning up transcriptions, removing non-verbal cues, filler words, and background noise to focus on the dialogue content. Proper speaker attribution will be maintained to preserve the conversational flow and context.\n\nTokenization will then be applied to segment the dialogues appropriately for input into neural network models while preserving sentence boundaries and dialogue structure. This step is essential for ensuring that the model can effectively understand and process the input data.\n\nMoving to the implementation phase, several data preprocessing techniques have been implemented on the images of the words that are detected and passed on to the recognition system. These techniques help bolster the robustness of the system and increase the accuracy of the annotations. Key strategies include:\n• Data Augmentation: Using techniques such as pitch shifting and time stretching to create variations in the training data, which can improve model generalization.\n\nTo optimize the performance of voice recognition models, consider the following best practices:\n• Regular Updates: Continuously update the training dataset with new audio samples to adapt to changing speech patterns.\n• Hyperparameter Tuning: Experiment with different model architectures and hyperparameters to find the optimal configuration for your specific use case.\n• Evaluation Metrics: Utilize metrics such as Word Error Rate (WER) and Character Error Rate (CER) to assess model performance and guide improvements.\n\nBy following these guidelines, you can enhance the effectiveness of your voice recognition systems and ensure they are well-prepared to handle diverse audio inputs."
    },
    {
        "link": "https://theaisummer.com/speech-recognition",
        "document": "Humans communicate preferably through speech using the same language. Speech recognition can be defined as the ability to understand the spoken words of the person speaking.\n\nAutomatic speech recognition (ASR) refers to the task of recognizing human speech and translating it into text. This research field has gained a lot of focus over the last decades. It is an important research area for human-to-machine communication. Early methods focused on manual feature extraction and conventional techniques such as Gaussian Mixture Models (GMM), the Dynamic Time Warping (DTW) algorithm and Hidden Markov Models (HMM).\n\nMore recently, neural networks such as recurrent neural networks (RNNs), convolutional neural networks (CNNs) and in the last years Transformers, have been applied on ASR and have achieved great performance.\n\nThe overall flow of ASR can be represented as shown below:\n\nThe main goal of an ASR system is to transform an audio input signal x=(x1​,x2​,…xT​) with a specific length T into a sequence of words or characters (i.e., labels) y=(y1​,y2​,…,yN​), yn​∈V, where V is the vocabulary. The labels might be character-level labels (i.e., letters) or word-level labels (i.e., words).\n\nThe most probable output sequence is given by:\n\nA typical ASR system has the following processing steps:\n\nThe pre-processing step aims to improve the audio signal by reducing the signal-to-noise ratio, reducing the noise, and filtering the signal.\n\nIn general, the features that are used for ASR, are extracted with a specific number of values or coefficients, which are generated by applying various methods on the input. This step must be robust, concerning various quality factors, such as noise or the echo effect.\n\nThe majority of the ASR methods adopt the following feature extraction techniques:\n\nThe classification model aims to find the spoken text which is contained on the input signal. It takes the extracted features from the pre-processing step and generates the output text.\n\nThe language model (LM) is an important module as it captures the grammatical rules or the semantic information of a language. Language models are important in order to recognize the output token from the classification model as well as to make corrections on the output text.\n\nVarious databases with text from audiobooks, conversations, and talks have been recorded.\n• The CallHome English, Spanish and German databases ( Post et al.) contain conversational data with a high number of words, which are not in the vocabulary. They are challenging databases with foreign words and telephone channel distortion. The English CallHome database has 120 spontaneous English telephone conversations between native English people. The training set has 80 conversations of about 15 hours of speech, while the test and development sets contain 20 conversations, where each set has 1.8 hours of audio files.\n\nMoreover, the CallHome Spanish consists of 120 telephone conversations respectively between native speakers. The training part has 16 hours of speech and its test set has 20 conversations with 2 hours of speech. Finally, the CallHome German consists of 100 telephone conversations between native German speakers with 15 hours of speech in the training set and 3.7 hours of speech in the test set.\n• TIMIT is a large dataset with broadband recordings from American English, where each speaker reads 10 grammatically rich sentences. TIMIT contains audio signals, which have been time-aligned, corrected and can be used for character or word recognition. The audio files are encoded in 16 bits. The training set contains a large number of audios from 462 speakers in total, while the validation set has audios from 50 speakers and the test set audios from 24 speakers.\n\nMel-frequency Cepstral coefficients is the most common method for extracting speech features. The human ear is a nonlinear system concerning how it perceives the audio signal. In order to cope with the change in frequency, the Mel-scale was developed to make a linear model of the human auditory system. Only frequencies in the range of [0,1] kHz can be transformed to the Mel-scale, while the remaining frequencies are considered to be logarithmic. The mel-scale frequency is computed as:\n\nwhere fHz​ is the frequency of the original signal.\n\nThe MFCC feature extraction technique basically includes the following steps:\n\nIn the deep learning era, neural networks have shown significant improvement in the speech recognition task. Various methods have been applied such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), while recently Transformer networks have achieved great performance.\n\nRNNs perform computations on the time sequence since their current hidden state\n\nis dependent on all the previous hidden states. More specifically, they are designed to model time-series signals as well as capture long-term and short-term dependencies between different time-steps of the input.\n\nConcerning speech recognition applications, the input signal x=(x1​,x2​,…xT​) is passed through the RNN to compute the hidden sequences h=(h1​,h2​,…hN​) and the output sequences y=(y1​,y2​,…yN​), respectively. One major drawback of the simple form of RNNs is that it generates the next output based only on the previous context.\n\nRNNs compute the sequence of hidden vectors h as:\n\nwhere W are the weights, b are the bias vectors and H is the nonlinear function.\n\nHowever, in speech recognition, usually the information of the future context is equally significant as the past context (Graves et al.). That’s why instead of using a unidirectional RNN, bidirectional RNNs (BiRNNs) are commonly selected in order to address this shortcoming. BiRNNs process the input vectors in both directions i.e., forward and backward, and keep the hidden state vectors for each direction as shown in the above figure.\n\nThis problem can be addressed using:\n• None Hidden Markov Models (HMMs) to get the alignment between the input audio and its transcribed output.\n• None Connectionist Temporal Classification (CTC) loss, which is the most common technique.\n\nCTC is an objective function that computes the alignment between the input speech signal and the output sequence of the words. CTC uses a blank label that represents the silence time-step i.e., the person doesn't speak, or represents the transition between words or phonemes. Given the input x and the output probability sequence of words or characters y, the probability of an alignment path α is calculated as:\n\nwhere αt​ is a single alignment at time-step t.\n\nFor a given transcription sequence, there are several possible alignments since labels can be separated from blanks in different ways. For example the alignments (a,−,b,c,−,−) and (−,−, a,−, b,c), ( − is the blank symbol) both correspond to the character sequence (a,b,c).\n\nFinally, the total probability of all paths is calculated as:\n\nCTC aims to maximize the total probability of the correct alignments in order to get the correct output word sequence. One main benefit of CTC is that it doesn't require prior segmentation or alignment of the data. DNNs can be used directly to model the features and achieve great performance in speech recognition tasks.\n\nThe decoding process is used to generate predictions from a trained model using CTC. There are several decoding algorithms. The most common step is the best-path decoding algorithm, where the max probabilities are used in every time-step. Since the model assumes that the latent symbols are independent given the network outputs in the frame-wise case, the output with the highest probability is obtained at each time-step as:\n\nBeam search has also been adopted for CTC decoding. The most likely translation is searched using left-to-right time-steps and a small number B of partial hypotheses is maintained. Each hypothesis is actually a prefix of the output sequence, while at each time-step it is extended in the beam with every possible word in the vocabulary.\n\nIn other works (e.g Rao et al.), an architecture commonly known as RNN-Transducer, has also been employed for ASR. This method combines an RNN with CTC and a separate RNN that predicts the next output given the previous one. It determines a separate probability distribution P(yk​∣t,u) for every timestep t of the input and time-step u of the output for the k-th element of the output y.\n\nAn encoder network converts the acoustic feature xt​ at time-step t to a representation het​=fenc​(xt​). Furthermore, a prediction network takes the previous label yu−1​ and generates a new representation hpt​=fp​(yu−1​). The joint network is a fully-connected layer that combines the two representations and generates the posterior probability P(y∣t,u)=fjoint​(het​:hpt​). In this way the RNN-Transducer can generate the next symbols or words by using information both from the encoder and the\n\nprediction network based on if the predicted label is a blank or a non-blank label. The inference procedure stops when a blank label is emitted at the last time-step.\n\nGraves et al. tested regular RNNs with CTC and RNN-Transducers in TIMIT database using different numbers of layers and hidden states.\n\nThe feature extraction is performed with a Fourier transform filter-bank method of 40 coefficients that are distributed on a logarithmic mel-scale concatenated with the first and second temporal derivatives.\n\nIn the table below, it is shown that RNN-T with 3 layers of 250 hidden states each has the best performance of 17.7% phoneme error rate (PER), while simple RNN-CTC models perform worse with PER >18.4%.\n\nRao et al. proposed an encoder-decoder RNN. The proposed method adopts an encoder network consisting of several blocks of LSTM layers, which are pre-trained with CTC using phonemes, graphemes, and words as output. In addition, 1D-CNN reduces the length T of the time sequence by a factor of 3 using specific kernel strides and sizes.\n\nThe decoder network is an RNN-T model trained along with an LSTM language model that also predicts words. The target of the network is the next label in the sequence and is used in the cross-entropy loss to optimize the network. Concerning feature extraction, 80-dimensional mel-scale features are computed every 10 msec and stacked every 30 msec to a single 240-dimensional acoustic feature vector.\n\nThe method is trained on a set of 22 million hand-transcribed audio recordings extracted\n\nfrom Google US English voice traffic, which corresponds to 18,000 hours of training data. These include voice-search as well as voice-dictation utterances. The language model was pretrained on text sentences obtained from the dataset. The method was tested with different configurations. It achieves 5.2% WER on this large dataset when the encoder contains 12 layers of 700 hidden units and the decoder 2 layers of 1000 hidden units each.\n\nRNN-Transducers have also been adopted for real-time speech recognition (He et al.). In this work, the model consists of 8 layers of uni-directional LSTM cells, while a time-reduction layer is used in the encoder to speed up training and inference. Memory caching techniques are also used to avoid redundant computation for identical prediction histories. This saves about 50–60% of the prediction network computations. In addition, different threads are used for the encoder and the prediction network to enable pipe-lining and save a significant amount of time.\n\nThe encoder inference procedure is split over two threads corresponding to the components before and after the time-reduction layer, which balances the computation between the\n\ntwo encoder components and the prediction network, and has a speedup of 28% compared against single-threaded execution. Furthermore, parameters are quantized from 32-bit floating-point precision into 8-bit to reduce memory consumption, both on disk and at run-time, and to optimize the model’s execution in real-time.\n\nThe algorithm was trained on a dataset that consists of 35 million English utterances with a size of 27,500 hours. The training utterances are hand-transcribed and are obtained from Google’s voice search and dictation trafﬁc and it was created by artiﬁcially corrupting clean utterances using a room simulator. The reported results are evaluated on 14800 voice search (VS) samples extracted from Google trafﬁc assistant, as well as 15700 voice command samples, denoted as the IME test set. The feature extraction step creates 80-dimensional mel-scale features computed every 25msec. The results are reported in inference speed divided by audio duration (RT90) and WER. The RNN-T model with symmetric quantization achieves WERs of 7.3% on the voice search set and 4.2% on the IME set.\n\nSak et al. adopt long-short memory (LSTM) networks for large vocabulary speech recognition. Their method extracts high-dimensional features using mel-filter banks using a sliding window technique. In addition, they incorporate context-dependent states and further improve the performance of the model. The method is evaluated on hand-transcribed audio recordings from real Google voice search traffic. The training set has 3 million utterances with an average duration of 4 seconds. The results are shown in the tables below:\n\nOther works have adopted the attention encoder-decoder structure of the RNN that directly computes the conditional probability of the output sequence given the input sequence without assuming a fixed alignment. The encoder-decoder method uses an attention mechanism, which does not require pre-segment alignment of data. An attention-based model uses a single decoder to produce a distribution over the labels conditioned on the full sequence of previous predictions and the input audio. With attention, it can implicitly learn the soft alignment between input and output sequences, which solves a big problem for speech recognition.\n\nThe model can still have a good effect on long input sequences, so it is also possible for such models to handle speech input of various lengths. More specifically, the model computes the output probability density P(y∣x), where the lengths of the input and output are different. The encoder maps the input to the context vector ci​ for each output yi​. The decoder computes:\n\nconditioned on the I previous outputs and the context ci​.\n\nThe posterior probability of symbol yi​ is calculated as:\n\nwhere si​ is the output of the recurrent layer f and g is the softmax function.\n\nThe context is obtained from the weighted average of the hidden states of all time-steps as:\n\nThe attention mechanism selects the temporal locations over the input sequence that should be used to update the hidden state of the RNN and to predict the next output value. It asserts the attention weights ai,t​ to compute the relevance scores between the input and the output.\n\nChorowski et al., adopts an attention-based recurrent sequence generator (ARSG) that generates the output word sequence from speech features h=(h1​,h2​,hT​) that can be modelled by any type of encoder. ARSG generates the output yi​ by focusing on the relevant features:\n\nwhere si​ is the i-th state of the RNN, ai​ are the attention weights.\n\nA new state is generated as:\n\nIn more detail, the scoring mechanism works as:\n\nARSG is evaluated on the TIMIT dataset and achieves WERs of 15.8% and 17.6% on validation and test sets.\n\nIn Chan et al and Chiu et.al the Listen-Attend-Spell (LAS) method was developed. The encoder (i.e., Listen) takes the input audio x and generates the representation h. More specifically, it uses a bidirectional Long Short Term Memory (BLSTM) module with a pyramid structure, where in each layer the time resolution is reduced. The output at the i-th time step, from the j-th layer is computed as:\n\nThe decoder (i.e., Attend-Spell) is an attention-based module that attends the representation h and produces the output probability P(y∣x). In more detail, an attention-based LSTM transducer produces the next character based on the previous outputs as:\n\nwhere si​, ci​ are the decoder state and the context vector, respectively.\n\nLAS was evaluated on 3 million Google voice search utterances with 2000 hours of speech, where 10 hours of utterances were randomly selected for validation. Data augmentation was also performed on the training dataset using a room simulator noise as well as by adding other types of noise and reverberations. It was able to achieve great recognition rates with WERs of 10.3% and 12,0% on clean and noisy environments, respectively.\n\nHori et al., adopt a joint decoder using CTC, attention decoder, and an RNN language model. A CNN encoder network takes the input audio x and outputs the hidden sequence h that is shared between the decoder modules. The decoder network iteratively predicts the 0 label sequence c based on the hidden sequence. The joint decoder utilizes both CTC, attention and the language model to enforce better alignments between the input and the output and find a better output sequence. The network is trained to maximize the following joint function:\n\nDuring inference, to find the most probable word sequence c^ , the decoder finds the most probable words as:\n\nwhere the language model probability is also used. Joint decoder\n\nThe method is evaluated on Wall Street Journal (WSJ) and LibriSpeech datasets.\n\nLibriSpeech is a large data set of reading speech from audiobooks and contains 1000 hours of audio and transcriptions. The experimental results of the proposed method on WSJ and Librispeech are shown in the following table, respectively.\n\nConvolutional neural networks were initially implemented for computer vision (CV) tasks. In recent years, CNNs have also been widely applied in the field of natural language processing (NLP), due to their good generation, and discrimination capability.\n\nA very typical CNN architecture is formed of several convolutional and pooling layers with fully connected layers for classification. A convolutional layer is composed by kernels that are convolved with the input. A convolutional kernel divides the input signal into smaller\n\nparts namely the receptive field of the kernel. Furthermore, the convolution operation is performed by multiplying the kernel with the corresponding parts of the input that are into the receptive field. Convolutional methods can be grouped into 1-dimensional and 2-dimensional networks, respectively.\n\n2D-CNNs construct 2D feature maps from the acoustic signal. Similar to images, they organize acoustic features i.e., MFCC features, in a 2-dimensional feature map, where one axis represents the frequency domain and the other represents the time domain. In contrast, 1D-CNNs accept acoustic features directly as input.\n\nIn 1D-CNN for speech recognition, every input feature map X=(X1​,…,XI​) is connected to many feature maps O=(O1​,…,OJ​). The convolution operation can be written as:\n\nwhere w is the local weight.\n• None In 2D-CNNs they are matrices.\n\nAbdel et al. were the first that applied CNNs to speech recognition. Their method adopts two types of convolutional layers. The first one adopts full weight sharing (FWS), where weights are shared across. This technique is common in CNNs for image recognition since the same characteristics may appear at any location in an image. However, in speech recognition, the signal varies across different frequencies and has distinct feature patterns in different ﬁlters. To tackle this, limited weight sharing (LWS) is used, where only the convolution filters that are attached to the same pooling filters share the same weights.\n\nThe speech input was analyzed with a 25-ms Hamming window with a fixed 10-ms frame rate. More specifically, feature vectors are generated by Fourier-transform-based filter-bank analysis, which includes40 log energy coefficients distributed on a mel scale, along with\n\ntheir first and second temporal derivatives. All speech data were normalized so that each vector dimension has a zero mean and unit variance.\n\nThe building block of their CNN architecture has convolutions and pooling layers. The input features are organized as several feature maps. The size (resolution) of feature maps gets smaller at upper layers as more convolution and pooling operations are applied as shown in the figure below. Usually, one or more fully connected hidden layers are added\n\non top of the final CNN layer to combine the features across all frequency bands before feeding to the output layer. They made a comprehensive study with different CNN configurations and achieved great results on TIMIT, which are shown in the below table. Their best model adopts only LWS layers and achieves a WER of 20.23% .\n\nWang et al. adopted residual 2D-CNN (RCNN) with CTC loss for speech recognition. The residual block uses direct connections between the previous and the next layer as follows:\n\nwhere f is a nonlinear function. This helps the network to converge faster without the use of extra parameters. The proposed architecture is depicted in the figure below. The Residual CNN-CTC method adopts 4 groups of residual blocks with small 3×3 ﬁlters. Each Residual group has N number of convolutional blocks with 2 layers. Each residual group also has different strides to reduce the computational cost and model temporal dependencies with different contexts. Batch normalization and ReLU activation are applied on each layer.\n\nThe RCNN is evaluated on WSJ with the standard configuration (si284 set\n\nfor training, eval92 set for validation, and dev93 set for testing). Furthermore, it is evaluated on the Tencent Chat data set that contains about 1400 hours of speech data for training and an independent 2000 sentences for test. The experimental results demonstrate the effectiveness of residual convolutional neural networks. RCNN can achieve WERs of 4.29%/7.65% on validation and test sets of WSJ and 13.33% on the Tencent Chat dataset.\n\nLi et al. implemented a residual 1D-CNN with dense and residual blocks as shown below. The network extracts mel-filter-bank features and uses residual blocks that contain batch normalization and dropout layers for faster convergence and better generalization. The input is constructed from mel-filter-bank features obtained using 20 msec windows with a 10 msec overlapping. The network has been tested with different types of normalization and activation functions, while each block is optimized to fit on a single GPU kernel for faster inference. Jasper is evaluated on LibriSpeech with different settings of configuration. The best model has 10 blocks of 4 layers and BatchNorm + ReLU and achieves validation WERs of 6.15% and 17.38% on clean and noisy sets, respectively.\n\nZeghidour et al. implement a fully convolutional network (FCN) with 3 main modules. The convolutional front-end is a CNN with low pass filters, convolutional filters similar to filter-banks, and algorithmic function to extract features. The second module is a convolutional acoustic model with several convolutional layers, GELU activation function, dropout, and weight regularization and predicts the letters from the input. Finally, there is a convolutional language model with 14 convolutional residual blocks and bottleneck layers.\n\nThis module is used to evaluate the candidate transcriptions of the acoustic model using a beam search decoder. FCN is evaluated on WSJ and LibriSpeech datasets. Their best configuration adopts a trainable convolutional front-end with 80 filters and a convolutional Language model. FCN achieves WERs of 6.8% on the validation set and 3.5% on the test set of WSJ, while on LibriSpeech it achieves validations WERs of 3.08%/9.94% on clean and noisy sets and testing WERs of 3.26%/10.47% on clean and noisy sets, respectively.\n\nDifferently from other works, Hannum et al. use time-separable convolutional networks with limited number of parameters and because time-separable CNNs generalize better and are more efficient. The encoder uses 2D depth-wise convolutions along with layer normalization. The encoder outputs two vectors, the keys k=k1​,k2​,…kT​ and the values v=v1​,v2​,…vT​ from the input sequence x as:\n\nAs for the decoder, a simple RNN is used and outputs the next token yu​ as:\n\nwhere Su​ is a summary vector and Qu​ is the query vector.\n\nTDS is evaluated on LibriSpeech with different receptive fields and kernel sizes in order to find the best setting for the time-separable convolutional layers. The best option is 11 time-separable blocks, which achieve WERs of 5.04% and 14.46% on dev clean and other sets, respectively.\n\nContextNet is a fully convolutional network that feeds global context information into the layers with squeeze-and-excitation modules. The CNN has K layers and generates the features as:\n\nwhere C is a convolutional block followed by batch normalization and activation functions. Furthermore, the squeeze-and-excitation block generates a global channel-wise weight θ with a global average pooling layer, which is multiplied by the input x as:\n\nContextNet is validated on LibriSpeech with 3 different conﬁgurations of ContextNet, with or without a language model. The 3 configurations are ContextNet(Small), ContextNet(Medium), and ContextNet(Large), which contain different numbers of layers and filters.\n\nResults on LibriSpeech with 3 different conﬁgurations of ContextNet, with or without language model\n\nRecently, with the introduction of Transformer networks, machine translation and speech recognition have seen significant improvements. Transformer models that are designed for speech recognition are usually based on the encoder-decoder architecture similar to seq2seq models. In more detail, they are based on the self-attention mechanism instead of recurrence that is adopted by RNNs. The self-attention can attend to different positions of a sequence and extract meaningful representations. The self-attention mechanism takes three inputs, queries, values, and keys.\n\nLet us denote the queries as Q∈Rtq​×dq​, the values V∈Rtv​×dv​ and the keys K∈Rtk​×dk​, where t∗​ are the corresponding dimensions. The outputs of self-attention is calculated as:\n\nwhere dk​ ​1​ is a scaling factor. However, Transformer adopts the Multi-head attention, which calculates the self-attention h times, one for each head i. In this way, each attention module focuses on different parts and learns different representations. Moreover, the multi-head attention is computed as:\n\nwhere WiQ​∈Rdmodel​×dq​, WiK​∈Rdmodel​×dk​, WiV​∈Rdmodel​×dv​, WO∈Rhdv​×dmodel​ and dmodel​ the dimensionality of the Transformer. Finally, a feed-forward network is used that contains two fully connected networks and ReLU activation functions as:\n\nwhere W1​∈Rdmodel​×dff​,W2​∈Rdff​×dmodel​ are the weights and b1​∈Rdff​,b2​∈Rdmodel​ are the biases. In general, to enable the Transformer to attend relative positions, we adopt a positional encoding which is added to the input. The most common technique is the sinusoidal encoding, described by:\n\nwhere j,i represents the position in the sequence and the i-th dimension, respectively. Finally, normalization layers and residual connections are used to speed up training.\n\nThe Speech-Transformer transforms the speech feature sequence to the corresponding character sequence. The feature sequence which is longer than the output character sequence is constructed from 2-dimensional spectrograms with time and frequency dimensions. More specifically, CNNs are used to exploit the structure locality of spectrograms and mitigate the length mismatch by striding along time.\n\nIn the Speech Transformer, 2D attention is used in order to attend at both the frequency and the time dimensions. The queries, keys, and values are extracted from convolutional neural networks and fed to the two self-attention modules. The Speech Transformer is evaluated on WSJ datasets and achieves competitive recognition results with a WER of 10.9%, while it needs about 80% less training time than conventional RNNs or CNNs.\n\nMohamed et al. adopt an encoder-decoder model formed by CNNs and a Transformer to learn local relationships and context of the speech signal. For the encoder, 2D convolutional modules with layer normalization and ReLU activation are used. In addition, each 2D convolutional module is formed by K convolutional layers with max-pooling. For the decoder, 1D convolutions are performed over embeddings of the past predicted words.\n\nSimilar to RNN-Transducer, a Transformer-Transducer model has also been developed for speech recognition. Compared to RNN-T, this model joint network combines the output of the audio encoder AE at time-step ti​ and the previously predicted label sequence z0i−1​=(z1​,...,zi−1​), which is produced from a feedforward network and a softmax layer, denoted as LE.\n\nThe joint representation is produced as:\n\nThen, the distribution of the alignment at time-step ti​ is computed as:\n\nThe Conformer is a variant of the original Transformer that combines CNNs and transformers in order to model both local and global speech dependencies by using a more efficient architecture and fewer parameters. The module of the Conformer contains two feedforward layers (FFN), one convolutional layer (CNN), and a multi-head attention module (MHA). The output of the Conformer is computed as:\n\nHere, the convolutional module adopts efficient pointwise and depthwise convolutions along with layer normalization.\n\nCTC and language models have also been used with Transformer networks.\n\nWang et al. utilized a semantic mask of the input speech according to corresponding output tokens in order to generate the next word based on the previous context. A VGG-like convolution layer is used in order to generate short-term dependent features from the input spectrogram, which are then modeled by a Transformer. On the decoder network, the position encoding is replaced by a 1D convolutional layer to extract local features.\n\nShi et al. propose a weak attention module to suppress non-informative parts of the speech signal such as during silence. The weak attention module sets the attention probabilities smaller than a threshold to zero and normalizes the rest attention probabilities.\n\nThe threshold is determined based on the following:\n\nThen, softmax is applied again on the new attention probabilities to generate the new attention matrix.\n\nIt is evident that deep architectures have already had a significant impact on automatic speech recognition. Convolutional neural networks, recurrent neural networks, and transformers have all been utilized with great success. Today’s SOTA models are all based on some combination of the aforementioned techniques. You can find some benchmarks on the popular datasets on paperswithcode.\n\nIf you find this article useful, you might also be interested in a previous one where we review the best speech synthesis methods. And as always, feel free to share it with your friends.\n\n* Disclosure: Please note that some of the links above might be affiliate links, and at no additional cost to you, we will earn a commission if you decide to make a purchase after clicking through."
    }
]