[
    {
        "link": "https://w3schools.com/sql/sql_create_table.asp",
        "document": "The statement is used to create a new table in a database.\n\nThe column parameters specify the names of the columns of the table.\n\nThe datatype parameter specifies the type of data the column can hold (e.g. varchar, integer, date, etc.).\n\nTip: For an overview of the available data types, go to our complete Data Types Reference.\n\nThe following example creates a table called \"Persons\" that contains five columns: PersonID, LastName, FirstName, Address, and City:\n\nThe PersonID column is of type int and will hold an integer.\n\nThe LastName, FirstName, Address, and City columns are of type varchar and will hold characters, and the maximum length for these fields is 255 characters.\n\nThe empty \"Persons\" table will now look like this:\n\nTip: The empty \"Persons\" table can now be filled with data with the SQL INSERT INTO statement.\n\nA copy of an existing table can also be created using .\n\nThe new table gets the same column definitions. All columns or specific columns can be selected.\n\nIf you create a new table using an existing table, the new table will be filled with the existing values from the old table.\n\nThe following SQL creates a new table called \"TestTable\" (which is a copy of the \"Customers\" table):"
    },
    {
        "link": "https://stackoverflow.com/questions/3382849/create-sql-table-with-the-data-from-another-table",
        "document": "had a similar question with a bit requiring a detail. So I'll expand on your question and deliver an answer for the expanded version of your question that goes as follows:\n\n\"How do I create a table using partial data which is already present in another table and index it with an added automatic increment column for a primary key?\"\n\nWhen normalizing a database with one table for two (or more) entities with a many to many relationship. For instance, you might have an 'orders' table with multiple item columns (item_1, item_2, item_3). To normalize the database we would\n• create a new TABLE for the 'items' and then\n• (this is were the question becomes relevant) relate the orders TABLE to the items TABLE through a linked TABLE called orders_items. This linked TABLE references the TABLES orders and items using FOREIGN KEYs. If the linking TABLE contains only the FOREIGN KEY to orders and items one problem will arise arise: The original orders table might have records where one order is related to a single item more than once. See below\n\nLooking at record with order_id 2, we see that there are two chairs related to this one order_id. The corresponding entry in the linked table would be\n\nThis contains duplicate columns. To prevent this we can just add an orders_items id as a (syntetic) primary key:\n\nThe new linking table accounts for orders with repeated items, but each record is unique so the table has no duplicates. For the orders_items id just define a new column with an autoincrement.\n\nBelow you find the implementation for PostgreSQL with the ROW_NUMBER() OVER function."
    },
    {
        "link": "https://w3schools.com/sql/sql_select_into.asp",
        "document": "The statement copies data from one table into a new table.\n\nCopy all columns into a new table:\n\nCopy only some columns into a new table:\n\nThe new table will be created with the column-names and types as defined in the old table. You can create new column names using the clause.\n\nThe following SQL statement creates a backup copy of Customers:\n\nThe following SQL statement uses the clause to copy the table into a new table in another database:\n\nThe following SQL statement copies only a few columns into a new table:\n\nThe following SQL statement copies only the German customers into a new table:\n\nThe following SQL statement copies data from more than one table into a new table:\n\nTip: can also be used to create a new, empty table using the schema of another. Just add a clause that causes the query to return no data:"
    },
    {
        "link": "https://tutorialspoint.com/sql/sql-create-table-using-tables.htm",
        "document": "A copy of an existing table can be created using a combination of the CREATE TABLE statement and the SELECT statement. The new table has the same column definitions. All columns or specific columns can be selected. When you will create a new table using the existing table, the new table would be populated using the existing values in the old table.\n\nThe basic syntax for creating a table from another table is as follows −\n\nHere, column1, column2... are the fields of the existing table and the same would be used to create fields of the new table.\n\nFollowing is an example, which would create a table SALARY using the CUSTOMERS table and having the fields customer ID and customer SALARY −\n\nThis would create a new table SALARY which will have the following records −"
    },
    {
        "link": "https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-transact-sql?view=sql-server-ver16",
        "document": "Creates a new table in the database.\n\nSimple CREATE TABLE syntax (common if not using options):\n\nThe name of the database in which the table is created. database_name must specify the name of an existing database. If not specified, database_name defaults to the current database. The login for the current connection must be associated with an existing user ID in the database specified by database_name, and that user ID must have CREATE TABLE permissions.\n\nThe name of the schema to which the new table belongs.\n\nThe name of the new table. Table names must follow the rules for identifiers. table_name can be a maximum of 128 characters, except for local temporary table names (names prefixed with a single number sign ( )) that can't exceed 116 characters.\n\nApplies to: SQL Server 2012 (11.x) and later.\n\nCreates the new table as a FileTable. You don't specify columns because a FileTable has a fixed schema. For more information, see FileTables.\n\nAn expression that defines the value of a computed column. A computed column is a virtual column that isn't physically stored in the table, unless the column is marked PERSISTED. The column is computed from an expression that uses other columns in the same table. For example, a computed column can have the definition: . The expression can be a noncomputed column name, constant, function, variable, and any combination of these connected by one or more operators. The expression can't be a subquery or contain alias data types.\n\nComputed columns can be used in select lists, WHERE clauses, ORDER BY clauses, or any other locations in which regular expressions can be used, with the following exceptions:\n• None Computed columns must be marked PERSISTED to participate in a FOREIGN KEY or CHECK constraint.\n• None A computed column can be used as a key column in an index or as part of any PRIMARY KEY or UNIQUE constraint, if the computed column value is defined by a deterministic expression and the data type of the result is allowed in index columns. For example, if the table has integer columns and , the computed column might be indexed, but computed column can't be indexed because the value might change in subsequent invocations.\n• None A computed column can't be the target of an INSERT or UPDATE statement.\n\nBased on the expressions that are used, the nullability of computed columns is determined automatically by the Database Engine. The result of most expressions is considered nullable even if only nonnullable columns are present, because possible underflows or overflows also produce NULL results. Use the function with the AllowsNull property to investigate the nullability of any computed column in a table. An expression that is nullable can be turned into a nonnullable one by specifying with the check_expression constant, where the constant is a nonnull value substituted for any NULL result. REFERENCES permission on the type is required for computed columns based on common language runtime (CLR) user-defined type expressions.\n\nSpecifies that the SQL Server Database Engine will physically store the computed values in the table, and update the values when any other columns on which the computed column depends are updated. Marking a computed column as lets you create an index on a computed column that is deterministic, but not precise. For more information, see Indexes on Computed Columns. Any computed columns that are used as partitioning columns of a partitioned table must be explicitly marked . computed_column_expression must be deterministic when is specified.\n\nSpecifies the partition scheme or filegroup on which the table is stored. If partition_scheme is specified, the table is to be a partitioned table whose partitions are stored on a set of one or more filegroups specified in partition_scheme. If filegroup is specified, the table is stored in the named filegroup. The filegroup must exist within the database. If is specified, or if ON isn't specified at all, the table is stored on the default filegroup. The storage mechanism of a table as specified in CREATE TABLE can't be subsequently altered.\n\nON { partition_scheme | filegroup | \"default\" } can also be specified in a PRIMARY KEY or UNIQUE constraint. These constraints create indexes. If filegroup is specified, the index is stored in the named filegroup. If is specified, or if ON isn't specified at all, the index is stored in the same filegroup as the table. If the PRIMARY KEY or UNIQUE constraint creates a clustered index, the data pages for the table are stored in the same filegroup as the index. If is specified or the constraint otherwise creates a clustered index, and a partition_scheme is specified that differs from the partition_scheme or filegroup of the table definition, or vice-versa, only the constraint definition will be honored, and the other will be ignored.\n\nIndicates that the text, ntext, image, xml, varchar(max), nvarchar(max), varbinary(max), and CLR user-defined type columns (including geometry and geography) are stored on the specified filegroup.\n\nisn't allowed if there are no large value columns in the table. can't be specified if partition_scheme is specified. If is specified, or if isn't specified at all, the large value columns are stored in the default filegroup. The storage of any large value column data specified in can't be subsequently altered.\n\nApplies to: SQL Server 2008 R2 (10.50.x) and later. Azure SQL Database and Azure SQL Managed Instance do not support .\n\nIf the table contains FILESTREAM data and the table is partitioned, the FILESTREAM_ON clause must be included, and must specify a partition scheme of FILESTREAM filegroups. This partition scheme must use the same partition function and partition columns as the partition scheme for the table; otherwise, an error is raised.\n\nIf the table isn't partitioned, the FILESTREAM column can't be partitioned. FILESTREAM data for the table must be stored in a single filegroup. This filegroup is specified in the FILESTREAM_ON clause.\n\nIf the table isn't partitioned and the clause isn't specified, the FILESTREAM filegroup that has the property set is used. If there is no FILESTREAM filegroup, an error is raised.\n\nAs with ON and , the value set by using for can't be changed, except in the following cases:\n• A CREATE INDEX statement converts a heap into a clustered index. In this case, a different FILESTREAM filegroup, partition scheme, or NULL can be specified.\n• A DROP INDEX statement converts a clustered index into a heap. In this case, a different FILESTREAM filegroup, partition scheme, or can be specified.\n\nThe filegroup in the clause, or each FILESTREAM filegroup that is named in the partition scheme, must have one file defined for the filegroup. This file must be defined by using a CREATE DATABASE or ALTER DATABASE statement; otherwise, an error is raised.\n\nSpecifies the data type of the column, and the schema to which it belongs. For disk-based tables, use one of the following data types:\n• An alias type based on a SQL Server system data type. Alias data types are created with the statement before they can be used in a table definition. The NULL or NOT NULL assignment for an alias data type can be overridden during the statement. However, the length specification can't be changed; the length for an alias data type can't be specified in a statement.\n• A CLR user-defined type. CLR user-defined types are created with the statement before they can be used in a table definition. To create a column on CLR user-defined type, REFERENCES permission is required on the type.\n\nIf type_schema_name isn't specified, the SQL Server Database Engine references type_name in the following order:\n• The default schema of the current user in the current database.\n• The schema in the current database.\n\nFor memory-optimized tables, see Supported Data Types for In-Memory OLTP for a list of supported system types.\n• The precision for the specified data type. For more information about valid precision values, see Precision, Scale, and Length.\n• The scale for the specified data type. For more information about valid scale values, see Precision, Scale, and Length.\n• Applies only to the varchar, nvarchar, and varbinary data types for storing 2^31 bytes of character and binary data, and 2^30 bytes of Unicode data.\n\nSpecifies that each instance of the xml data type in column_name can contain multiple top-level elements. CONTENT applies only to the xml data type and can be specified only if xml_schema_collection is also specified. If not specified, CONTENT is the default behavior.\n\nSpecifies that each instance of the xml data type in column_name can contain only one top-level element. DOCUMENT applies only to the xml data type and can be specified only if xml_schema_collection is also specified.\n\nApplies only to the xml data type for associating an XML schema collection with the type. Before typing an xml column to a schema, the schema must first be created in the database by using CREATE XML SCHEMA COLLECTION.\n\nSpecifies the value provided for the column when a value isn't explicitly supplied during an insert. DEFAULT definitions can be applied to any columns except those defined as timestamp, or those with the property. If a default value is specified for a user-defined type column, the type should support an implicit conversion from constant_expression to the user-defined type. DEFAULT definitions are removed when the table is dropped. Only a constant value, such as a character string; a scalar function (either a system, user-defined, or CLR function); or NULL can be used as a default. To maintain compatibility with earlier versions of SQL Server, a constraint name can be assigned to a DEFAULT.\n• A constant, NULL, or a system function that is used as the default value for the column.\n• A constant, NULL, or a system function that is supported in used as the default value for the column. Must be supported in natively compiled stored procedures. For more information about built-in functions in natively compiled stored procedures, see Supported Features for Natively Compiled T-SQL Modules.\n\nIndicates that the new column is an identity column. When a new row is added to the table, the Database Engine provides a unique, incremental value for the column. Identity columns are typically used with PRIMARY KEY constraints to serve as the unique row identifier for the table. The property can be assigned to tinyint, smallint, int, bigint, decimal(p, 0), or numeric(p, 0) columns. Only one identity column can be created per table. Bound defaults and DEFAULT constraints can't be used with an identity column. Both the seed and increment or neither must be specified. If neither is specified, the default is (1,1).\n• The value used for the first row loaded into the table.\n• The incremental value added to the identity value of the previous row loaded.\n\nIn the statement, the clause can be specified for the IDENTITY property, FOREIGN KEY constraints, and CHECK constraints. If this clause is specified for the property, values aren't incremented in identity columns when replication agents perform inserts. If this clause is specified for a constraint, the constraint isn't enforced when replication agents perform insert, update, or delete operations.\n\nGENERATED ALWAYS AS { ROW | TRANSACTION_ID | SEQUENCE_NUMBER } { START | END } [ HIDDEN ] [ NOT NULL ]\n\nApplies to: SQL Server 2016 (13.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n\nSpecifies a column used by the system to automatically record information about row versions in the table and its history table (if the table is system versioned and has a history table). Use this argument with the parameter to create system-versioned tables: temporal or ledger tables. For more information, see updateable ledger tables and temporal tables.\n\nIf you attempt to specify a column that doesn't meet the above data type or nullability requirements, the system will throw an error. If you don't explicitly specify nullability, the system will define the column as or per the above requirements.\n\nYou can mark one or both period columns with flag to implicitly hide these columns such that doesn't return a value for those columns. By default, period columns aren't hidden. In order to be used, hidden columns must be explicitly included in all queries that directly reference the temporal table. To change the attribute for an existing period column, must be dropped and recreated with a different hidden flag.\n\nApplies to: SQL Server 2014 (12.x) and later, and Azure SQL Database.\n\nSpecifies to create an index on the table. This can be a clustered index, or a nonclustered index. The index will contain the columns listed, and will sort the data in either ascending or descending order.\n\nApplies to: SQL Server 2014 (12.x) and later, and Azure SQL Database.\n\nSpecifies to store the entire table in columnar format with a clustered columnstore index. This always includes all columns in the table. The data isn't sorted in alphabetical or numeric order since the rows are organized to gain columnstore compression benefits.\n\nYou can specify an order for the data in a clustered columnstore index starting with SQL Server 2022 (16.x), in Azure SQL Database, in Azure SQL Managed Instance with the Always-up-to-date update policy, and in Azure Synapse Analytics. For more information, see Performance tuning with ordered columnstore indexes.\n\nApplies to: SQL Server 2014 (12.x) and later versions, Azure SQL Database, and Azure SQL Managed Instance.\n\nSpecifies to create a nonclustered columnstore index on the table. The underlying table can be a rowstore heap or clustered index, or it can be a clustered columnstore index. In all cases, creating a nonclustered columnstore index on a table stores a second copy of the data for the columns in the index.\n\nThe nonclustered columnstore index is stored and managed as a clustered columnstore index. It is called a nonclustered columnstore index to because the columns can be limited and it exists as a secondary index on a table.\n\nYou can specify an order for the data in a nonclustered columnstore index in Azure SQL Database and in Azure SQL Managed Instance with the Always-up-to-date update policy. For more information, see Performance tuning with ordered columnstore indexes.\n\nSpecifies the partition scheme that defines the filegroups onto which the partitions of a partitioned index will be mapped. The partition scheme must exist within the database by executing either CREATE PARTITION SCHEME or ALTER PARTITION SCHEME. column_name specifies the column against which a partitioned index will be partitioned. This column must match the data type, length, and precision of the argument of the partition function that partition_scheme_name is using. column_name isn't restricted to the columns in the index definition. Any column in the base table can be specified, except when partitioning a UNIQUE index, column_name must be chosen from among those used as the unique key. This restriction allows the Database Engine to verify uniqueness of key values within a single partition only.\n\nIf partition_scheme_name or filegroup isn't specified and the table is partitioned, the index is placed in the same partition scheme, using the same partitioning column, as the underlying table.\n\nFor more information about partitioning indexes, Partitioned Tables and Indexes.\n\nCreates the specified index on the specified filegroup. If no location is specified and the table or view isn't partitioned, the index uses the same filegroup as the underlying table or view. The filegroup must already exist.\n\nCreates the specified index on the default filegroup.\n\nApplies to: SQL Server 2008 R2 (10.50.x) and later.\n\nSpecifies the placement of FILESTREAM data for the table when a clustered index is created. The FILESTREAM_ON clause allows FILESTREAM data to be moved to a different FILESTREAM filegroup or partition scheme.\n\nfilestream_filegroup_name is the name of a FILESTREAM filegroup. The filegroup must have one file defined for the filegroup by using a CREATE DATABASE or ALTER DATABASE statement; otherwise, an error is raised.\n\nIf the table is partitioned, the clause must be included, and must specify a partition scheme of FILESTREAM filegroups that uses the same partition function and partition columns as the partition scheme for the table. Otherwise, an error is raised.\n\nIf the table isn't partitioned, the FILESTREAM column can't be partitioned. FILESTREAM data for the table must be stored in a single filegroup that is specified in the clause.\n\ncan be specified in a statement if a clustered index is being created and the table doesn't contain a FILESTREAM column.\n\nFor more information, see FILESTREAM.\n\nIndicates that the new column is a row GUID column. Only one uniqueidentifier column per table can be designated as the ROWGUIDCOL column. Applying the ROWGUIDCOL property enables the column to be referenced using . The ROWGUIDCOL property can be assigned only to a uniqueidentifier column. User-defined data type columns can't be designated with ROWGUIDCOL.\n\nThe ROWGUIDCOL property doesn't enforce uniqueness of the values stored in the column. ROWGUIDCOL also doesn't automatically generate values for new rows inserted into the table. To generate unique values for each column, either use the NEWID or NEWSEQUENTIALID function on INSERT statements or use these functions as the default for the column.\n\nSpecifies encrypting columns by using the Always Encrypted feature.\n• Specifies the column encryption key. For more information, see CREATE COLUMN ENCRYPTION KEY.\n• Deterministic encryption uses a method that always generates the same encrypted value for any given plain text value. Using deterministic encryption allows searching using equality comparison, grouping, and joining tables using equality joins based on encrypted values, but can also allow unauthorized users to guess information about encrypted values by examining patterns in the encrypted column. Joining two tables on columns encrypted deterministically is only possible if both columns are encrypted using the same column encryption key. Deterministic encryption must use a column collation with a binary2 sort order for character columns. Randomized encryption uses a method that encrypts data in a less predictable manner. Randomized encryption is more secure, but it prevents any computations and indexing on encrypted columns, unless your SQL Server instance supports Always Encrypted with secure enclaves. See Always Encrypted with secure enclaves for details. If you are using Always Encrypted (without secure enclaves), use deterministic encryption for columns that will be searched with parameters or grouping parameters, for example a government ID number. Use randomized encryption, for data such as a credit card number, which isn't grouped with other records or used to join tables, and which isn't searched for because you use other columns (such as a transaction number) to find the row that contains the encrypted column of interest. If you are using Always Encrypted with secure enclaves, randomized encryption is a recommended encryption type. Columns must be of a qualifying data type.\n• Applies to: SQL Server 2016 (13.x) and later. For more information including feature constraints, see Always Encrypted.\n\nIndicates that the column is a sparse column. The storage of sparse columns is optimized for null values. Sparse columns can't be designated as NOT NULL. For additional restrictions and more information about sparse columns, see Use Sparse Columns.\n\nApplies to: SQL Server 2016 (13.x) and later.\n\nSpecifies a dynamic data mask. mask_function is the name of the masking function with the appropriate parameters. Four functions are available:\n\nApplies to: SQL Server 2008 R2 (10.50.x) and later.\n\nValid only for varbinary(max) columns. Specifies FILESTREAM storage for the varbinary(max) BLOB data.\n\nThe table must also have a column of the uniqueidentifier data type that has the ROWGUIDCOL attribute. This column must not allow null values and must have either a UNIQUE or PRIMARY KEY single-column constraint. The GUID value for the column must be supplied either by an application when inserting data, or by a DEFAULT constraint that uses the NEWID () function.\n\nThe ROWGUIDCOL column can't be dropped and the related constraints can't be changed while there is a FILESTREAM column defined for the table. The ROWGUIDCOL column can be dropped only after the last FILESTREAM column is dropped.\n\nWhen the FILESTREAM storage attribute is specified for a column, all values for that column are stored in a FILESTREAM data container on the file system.\n\nSpecifies the collation for the column. Collation name can be either a Windows collation name or a SQL collation name. collation_name is applicable only for columns of the char, varchar, text, nchar, nvarchar, and ntext data types. If not specified, the column is assigned either the collation of the user-defined data type, if the column is of a user-defined data type, or the default collation of the database.\n\nFor more information about the Windows and SQL collation names, see Windows Collation Name and SQL Collation Name.\n\nFor more information, see COLLATE.\n\nAn optional keyword that indicates the start of the definition of a PRIMARY KEY, NOT NULL, UNIQUE, FOREIGN KEY, or CHECK constraint.\n• The name of a constraint. Constraint names must be unique within the schema to which the table belongs.\n• Determine whether null values are allowed in the column. NULL isn't strictly a constraint but can be specified just like NOT NULL. NOT NULL can be specified for computed columns only if PERSISTED is also specified.\n• A constraint that enforces entity integrity for a specified column or columns through a unique index. Only one PRIMARY KEY constraint can be created per table.\n• A constraint that provides entity integrity for a specified column or columns through a unique index. A table can have multiple UNIQUE constraints.\n• Indicates that a clustered or a nonclustered index is created for the PRIMARY KEY or UNIQUE constraint. PRIMARY KEY constraints default to CLUSTERED, and UNIQUE constraints default to NONCLUSTERED. In a statement, CLUSTERED can be specified for only one constraint. If CLUSTERED is specified for a UNIQUE constraint and a PRIMARY KEY constraint is also specified, the PRIMARY KEY defaults to NONCLUSTERED.\n• A constraint that provides referential integrity for the data in the column or columns. FOREIGN KEY constraints require that each value in the column exists in the corresponding referenced column or columns in the referenced table. FOREIGN KEY constraints can reference only columns that are PRIMARY KEY or UNIQUE constraints in the referenced table or columns referenced in a UNIQUE INDEX on the referenced table. Foreign keys on computed columns must also be marked PERSISTED.\n• The name of the table referenced by the FOREIGN KEY constraint, and the schema to which it belongs.\n• A column, or list of columns, from the table referenced by the FOREIGN KEY constraint.\n• Specifies what action happens to rows in the table created, if those rows have a referential relationship and the referenced row is deleted from the parent table. The default is NO ACTION.\n• The Database Engine raises an error and the delete action on the row in the parent table is rolled back.\n• Corresponding rows are deleted from the referencing table if that row is deleted from the parent table.\n• All the values that make up the foreign key are set to NULL if the corresponding row in the parent table is deleted. For this constraint to execute, the foreign key columns must be nullable.\n• All the values that make up the foreign key are set to their default values when the corresponding row in the parent table is deleted. For this constraint to execute, all foreign key columns must have default definitions. If a column is nullable, and there is no explicit default value set, NULL becomes the implicit default value of the column. Don't specify if the table will be included in a merge publication that uses logical records. For more information about logical records, see Group Changes to Related Rows with Logical Records. can't be defined if an trigger already exists on the table. For example, in the database, the table has a referential relationship with the table. The foreign key references the primary key. If a statement is executed on a row in the table, and an action is specified for , the Database Engine checks for one or more dependent rows in the table. If any exist, the dependent rows in the table are deleted, and also the row referenced in the table. Conversely, if is specified, the Database Engine raises an error and rolls back the delete action on the row if there is at least one row in the table that references it.\n• Specifies what action happens to rows in the table altered when those rows have a referential relationship and the referenced row is updated in the parent table. The default is NO ACTION.\n• The Database Engine raises an error, and the update action on the row in the parent table is rolled back.\n• Corresponding rows are updated in the referencing table when that row is updated in the parent table.\n• All the values that make up the foreign key are set to NULL when the corresponding row in the parent table is updated. For this constraint to execute, the foreign key columns must be nullable.\n• All the values that make up the foreign key are set to their default values when the corresponding row in the parent table is updated. For this constraint to execute, all foreign key columns must have default definitions. If a column is nullable, and there is no explicit default value set, NULL becomes the implicit default value of the column. Don't specify if the table will be included in a merge publication that uses logical records. For more information about logical records, see Group Changes to Related Rows with Logical Records. , , or can't be defined if an trigger already exists on the table that is being altered. For example, in the database, the table has a referential relationship with the table: foreign key references the primary key. If an UPDATE statement is executed on a row in the table, and an ON UPDATE CASCADE action is specified for , the Database Engine checks for one or more dependent rows in the table. If any exist, the dependent rows in the table are updated, and also the row referenced in the table. Conversely, if NO ACTION is specified, the Database Engine raises an error and rolls back the update action on the row if there is at least one row in the table that references it.\n• A constraint that enforces domain integrity by limiting the possible values that can be entered into a column or columns. CHECK constraints on computed columns must also be marked PERSISTED.\n• A logical expression that returns TRUE or FALSE. Alias data types can't be part of the expression.\n• A column or list of columns, in parentheses, used in table constraints to indicate the columns used in the constraint definition.\n• Specifies the order in which the column or columns participating in table constraints are sorted. The default is ASC.\n• The name of the partition scheme that defines the filegroups onto which the partitions of a partitioned table will be mapped. The partition scheme must exist within the database.\n• Specifies the column against which a partitioned table will be partitioned. The column must match that specified in the partition function that partition_scheme_name is using in terms of data type, length, and precision. A computed column that participates in a partition function must be explicitly marked PERSISTED. We recommend that you specify NOT NULL on the partitioning column of partitioned tables, and also nonpartitioned tables that are sources or targets of ALTER TABLE...SWITCH operations. Doing this makes sure that any CHECK constraints on partitioning columns do not have to check for null values.\n• Specifies how full the Database Engine should make each index page that is used to store the index data. User-specified fillfactor values can be from 1 through 100. If a value isn't specified, the default is 0. Fill factor values 0 and 100 are the same in all respects. Documenting WITH FILLFACTOR = fillfactor as the only index option that applies to PRIMARY KEY or UNIQUE constraints is maintained for backward compatibility, but will not be documented in this manner in future releases.\n\nThe name of the column set. A column set is an untyped XML representation that combines all of the sparse columns of a table into a structured output. For more information about column sets, see Use Column Sets.\n\nApplies to: SQL Server 2016 (13.x) and later, and Azure SQL Database.\n\nSpecifies the names of the columns that the system will use to record the period for which a record is valid. Use this argument with the and arguments to create a temporal table. For more information, see Temporal Tables.\n\nApplies to: SQL Server 2016 (13.x) and later, and Azure SQL Database.\n\nFor a memory-optimized, delay specifies the minimum number of minutes a row must remain in the table, unchanged, before it is eligible for compression into the columnstore index. SQL Server selects specific rows to compress according to their last update time. For example, if rows are changing frequently during a two-hour period of time, you could set to ensure updates are completed before SQL Server compresses the row.\n\nFor a disk-based table, delay specifies the minimum number of minutes a delta rowgroup in the CLOSED state must remain in the delta rowgroup before SQL Server can compress it into the compressed rowgroup. Since disk-based tables don't track insert and update times on individual rows, SQL Server applies the delay to delta rowgroups in the CLOSED state.\n\nFor recommendations on when to use , see Get started with Columnstore for real time operational analytics\n\nSpecifies one or more table options.\n\nSpecifies the data compression option for the specified table, partition number, or range of partitions. The options are as follows:\n• Table or specified partitions are compressed by using row compression.\n• Table or specified partitions are compressed by using page compression.\n• Applies to: SQL Server 2016 (13.x) and later, and Azure SQL Database. Applies only to columnstore indexes, including both nonclustered columnstore and clustered columnstore indexes. COLUMNSTORE specifies to compress with the most performant columnstore compression. This is the typical choice.\n• Applies to: SQL Server 2016 (13.x) and later, and Azure SQL Database. Applies only to columnstore indexes, including both nonclustered columnstore and clustered columnstore indexes. COLUMNSTORE_ARCHIVE will further compress the table or partition to a smaller size. This can be used for archival, or for other situations that require a smaller storage size and can afford more time for storage and retrieval.\n\nFor more information, see Data Compression.\n\nApplies to: SQL Server 2022 (16.x) and later versions, Azure SQL Database, and Azure SQL Managed Instance.\n\nSpecifies the XML compression option for any xml data type columns in the table. The options are as follows:\n• Columns using the xml data type are compressed.\n\nSpecifies the partitions to which the or settings apply. If the table isn't partitioned, the argument will generate an error. If the clause isn't provided, the option will apply to all partitions of a partitioned table.\n\npartition_number_expression can be specified in the following ways:\n• Provide the partition number of a partition, for example:\n• Provide the partition numbers for several individual partitions separated by commas, for example:\n• Provide both ranges and individual partitions, for example:\n\ncan be specified as partition numbers separated by the word TO, for example: .\n\nTo set different types of data compression for different partitions, specify the option more than once, for example:\n\nYou can also specify the option more than once, for example:\n\nSpecifies one or more index options. For a complete description of these options, see CREATE INDEX.\n\nWhen ON, the percentage of free space specified by FILLFACTOR is applied to the intermediate level pages of the index. When OFF or a FILLFACTOR value it not specified, the intermediate level pages are filled to near capacity leaving enough space for at least one row of the maximum size the index can have, considering the set of keys on the intermediate pages. The default is OFF.\n\nSpecifies a percentage that indicates how full the Database Engine should make the leaf level of each index page during index creation or alteration. fillfactor must be an integer value from 1 to 100. The default is 0. Fill factor values 0 and 100 are the same in all respects.\n\nSpecifies the error response when an insert operation attempts to insert duplicate key values into a unique index. The IGNORE_DUP_KEY option applies only to insert operations after the index is created or rebuilt. The option has no effect when executing CREATE INDEX, ALTER INDEX, or UPDATE. The default is OFF.\n• A warning message will occur when duplicate key values are inserted into a unique index. Only the rows violating the uniqueness constraint will fail.\n• An error message will occur when duplicate key values are inserted into a unique index. The entire INSERT operation will be rolled back.\n\ncan't be set to ON for indexes created on a view, non-unique indexes, XML indexes, spatial indexes, and filtered indexes.\n\nIn backward compatible syntax, is equivalent to .\n\nWhen ON, out-of-date index statistics aren't automatically recomputed. When OFF, automatic statistics updating are enabled. The default is OFF.\n\nWhen ON, row locks are allowed when you access the index. The Database Engine determines when row locks are used. When OFF, row locks aren't used. The default is ON.\n\nWhen ON, page locks are allowed when you access the index. The Database Engine determines when page locks are used. When OFF, page locks aren't used. The default is ON.\n\nApplies to: SQL Server 2019 (15.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n\nSpecifies whether or not to optimize for last-page insert contention. The default is OFF. See the Sequential Keys section of the CREATE INDEX page for more information.\n\nApplies to: SQL Server 2012 (11.x) and later.\n\nSpecifies the windows-compatible FileTable directory name. This name should be unique among all the FileTable directory names in the database. Uniqueness comparison is case-insensitive, regardless of collation settings. If this value isn't specified, the name of the FileTable is used.\n\nApplies to: SQL Server 2012 (11.x) and later. Azure SQL Database and Azure SQL Managed Instance do not support .\n\nSpecifies the name of the collation to be applied to the column in the FileTable. The collation must be case-insensitive to comply with Windows operating system file naming semantics. If this value isn't specified, the database default collation is used. If the database default collation is case-sensitive, an error is raised, and the CREATE TABLE operation fails.\n• The name of a case-insensitive collation.\n• Specifies that the default collation for the database should be used. This collation must be case-insensitive.\n\nApplies to: SQL Server 2012 (11.x) and later. Azure SQL Database and Azure SQL Managed Instance do not support .\n\nSpecifies the name to be used for the primary key constraint that is automatically created on the FileTable. If this value isn't specified, the system generates a name for the constraint.\n\nApplies to: SQL Server 2012 (11.x) and later. Azure SQL Database and Azure SQL Managed Instance do not support .\n\nSpecifies the name to be used for the unique constraint that is automatically created on the stream_id column in the FileTable. If this value isn't specified, the system generates a name for the constraint.\n\nApplies to: SQL Server 2012 (11.x) and later. Azure SQL Database and Azure SQL Managed Instance do not support .\n\nSpecifies the name to be used for the unique constraint that is automatically created on the parent_path_locator and name columns in the FileTable. If this value isn't specified, the system generates a name for the constraint.\n\nSYSTEM_VERSIONING = ON [ ( HISTORY_TABLE = schema_name.history_table_name [ , DATA_CONSISTENCY_CHECK = { ON | OFF } ] ) ]\n\nApplies to: SQL Server 2016 (13.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n\nEnables system versioning of the table if the data type, nullability constraint, and primary key constraint requirements are met. The system will record the history of each record in the system-versioned table in a separate history table. If the argument isn't used, the name of this history table will be . If the name of a history table is specified during history table creation, you must specify the schema and table name.\n\nIf the history table doesn't exist, the system generates a new history table matching the schema of the current table in the same filegroup as the current table, creating a link between the two tables and enables the system to record the history of each record in the current table in the history table. By default, the history table is compressed.\n\nIf the argument is used to create a link to and use an existing history table, the link is created between the current table and the specified table. If current table is partitioned, the history table is created on default file group because partitioning configuration isn't replicated automatically from the current table to the history table. When creating a link to an existing history table, you can choose to perform a data consistency check. This data consistency check ensures that existing records don't overlap. Performing the data consistency check is the default.\n\nUse this argument with the and arguments to enable system versioning on a table. For more information, see Temporal Tables. Use this argument with the argument to create an updatable ledger table. Using existing history tables with ledger tables isn't allowed.\n\nApplies to: SQL Server 2016 (13.x) and later.\n\nCreates the new table with Stretch Database enabled or disabled. For more info, see Stretch Database.\n\nWhen you enable Stretch for a table by specifying , you can optionally specify to begin migrating data immediately, or to postpone data migration. The default value is . For more info about enabling Stretch for a table, see Enable Stretch Database for a table.\n\nPrerequisites. Before you enable Stretch for a table, you have to enable Stretch on the server and on the database. For more info, see Enable Stretch Database for a database.\n\nPermissions. Enabling Stretch for a database or a table requires db_owner permissions. Enabling Stretch for a table also requires ALTER permissions on the table.\n\nApplies to: SQL Server 2016 (13.x) and later.\n\nOptionally specifies a filter predicate to select rows to migrate from a table that contains both historical and current data. The predicate must call a deterministic inline table-valued function. For more info, see Enable Stretch Database for a table and Select rows to migrate by using a filter function.\n\nIf you don't specify a filter predicate, the entire table is migrated.\n\nWhen you specify a filter predicate, you also have to specify MIGRATION_STATE.\n\nApplies to: SQL Server 2016 (13.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n• None Specify to migrate data from SQL Server to Azure SQL Database.\n• None Specify to copy the remote data for the table from Azure SQL Database back to SQL Server and to disable Stretch for the table. For more info, see Disable Stretch Database and bring back remote data. This operation incurs data transfer costs, and it can't be canceled.\n• None Specify to pause or postpone data migration. For more info, see Pause and resume data migration -Stretch Database.\n\nEnables retention policy based cleanup of old or aged data from tables within a database. For more information, see Enable and Disable Data Retention. The following parameters must be specified for data retention to be enabled.\n• Specifies the column that should be used to determine if the rows in the table are obsolete or not. The following data types are allowed for the filter column.\n• Specifies the retention period policy for the table. The retention period is specified as a combination of a positive integer value and the date part unit.\n\nApplies to: SQL Server 2014 (12.x) and later, Azure SQL Database, and Azure SQL Managed Instance. Azure SQL Managed Instance does not support memory optimized tables in General Purpose tier.\n\nThe value ON indicates that the table is memory optimized. Memory-optimized tables are part of the In-Memory OLTP feature, which is used to optimize the performance of transaction processing. To get started with In-Memory OLTP see Quickstart 1: In-Memory OLTP Technologies for Faster Transact-SQL Performance. For more in-depth information about memory-optimized tables, see Memory-Optimized Tables.\n\nThe default value OFF indicates that the table is disk-based.\n\nApplies to: SQL Server 2014 (12.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n\nThe value of indicates that the table is durable, meaning that changes are persisted on disk and survive restart or failover. SCHEMA_AND_DATA is the default value.\n\nThe value of indicates that the table is non-durable. The table schema is persisted but any data updates aren't persisted upon a restart or failover of the database. is only allowed with .\n\nApplies to: SQL Server 2014 (12.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n\nIndicates the number of buckets that should be created in the hash index. The maximum value for BUCKET_COUNT in hash indexes is 1,073,741,824. For more information about bucket counts, see Indexes for Memory-Optimized Tables.\n\nApplies to: SQL Server 2014 (12.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n\nColumn and table indexes can be specified as part of the CREATE TABLE statement. For details about adding and removing indexes on memory-optimized tables, see Altering Memory-Optimized Tables\n• Applies to: SQL Server 2014 (12.x) and later, Azure SQL Database, and Azure SQL Managed Instance. Indicates that a HASH index is created. Hash indexes are supported only on memory-optimized tables.\n\nIndicates whether the table being created is a ledger table (ON) or not (OFF). The default is OFF. If the option is specified, the system creates an append-only ledger table allowing only inserting new rows. Otherwise, the system creates an updatable ledger table. An updatable ledger table also requires the argument. An updatable ledger table must also be a system-versioned table. However, an updatable ledger table doesn't have to be a temporal table (it doesn't require the parameter). If the history table is specified with and , it must not reference an existing table.\n\nA ledger database (a database created with the option) only allows the creation of ledger tables. Attempts to create a table with will raise an error. Each new table by default is created as an updatable ledger table, even if you don't specify , and will be created with default values for all other parameters.\n\nAn updatable ledger table must contain four columns, exactly one column defined with each of the following arguments:\n\nAn append-only ledger table must contain exactly one column defined with each of the following arguments:\n\nIf any of the required generated always columns isn't defined in the statement and the statement includes , the system will automatically attempt to add the column using an applicable column definition from the below list. If there is a name conflict with an already defined column, the system will raise an error.\n\nThe <ledger_view_option> specifies the schema and the name of the ledger view the system automatically creates and links to the table. If the option isn't specified, the system generates the ledger view name by appending to the name of the table being created ( ). If a view with the specified or generated name exists, the system will raise an error. If the table is an updatable ledger table, the ledger view is created as a union on the table and its history table.\n\nEach row in the ledger view represents either the creation or deletion of a row version in the ledger table. The ledger view contains all columns of the ledger table, except the generated always columns listed above. The ledger view also contains the following additional columns:\n\nTransactions that include creating ledger table are captured in sys.database_ledger_transactions.\n\nSpecifies the name of the ledger view and the names of additional columns the system adds to the ledger view.\n\nSpecifies whether the ledger table being created is append-only or updatable. The default is .\n\nSpecifies one or more ledger view options. Each of the ledger view option specifies a name of a column, the system will add to the view, in addition to the columns defined in the ledger table.\n\nSpecifies the name of the column storing the ID of the transaction that created or deleted a row version. The default column name is .\n\nSpecifies the name of the columns storing the sequence number of a row-level operation within the transaction on the table. The default column name is .\n\nSpecifies the name of the columns storing the operation type ID. The default column name is ledger_operation_type.\n\nSpecifies the name of the columns storing the operation type description. The default column name is .\n\nFor information about the number of allowed tables, columns, constraints and indexes, see Maximum Capacity Specifications for SQL Server.\n\nSpace is generally allocated to tables and indexes in increments of one extent at a time. When the option of is set to TRUE, or always prior to SQL Server 2016 (13.x), when a table or index is created, it is allocated pages from mixed extents until it has enough pages to fill a uniform extent. After it has enough pages to fill a uniform extent, another extent is allocated every time the currently allocated extents become full. For a report about the amount of space allocated and used by a table, execute .\n\nThe Database Engine doesn't enforce an order in which DEFAULT, IDENTITY, ROWGUIDCOL, or column constraints are specified in a column definition.\n\nWhen a table is created, the QUOTED IDENTIFIER option is always stored as ON in the metadata for the table, even if the option is set to OFF when the table is created.\n\nIn SQL database in Microsoft Fabric, some table features can be created but will not be mirrored into the Fabric OneLake. For more information, see Limitations of Fabric SQL database mirroring.\n\nYou can create local and global temporary tables. Local temporary tables are visible only in the current session, and global temporary tables are visible to all sessions. Temporary tables can't be partitioned.\n\nPrefix local temporary table names with single number sign ( ), and prefix global temporary table names with a double number sign ( ).\n\nTransact-SQL statements reference the temporary table by using the value specified for table_name in the statement, for example:\n\nIf more than one temporary table is created inside a single stored procedure or batch, they must have different names.\n\nIf you include a schema_name when you create or access a temporary table, it is ignored. All temporary tables are created in the schema.\n\nIf a local temporary table is created in a stored procedure or a SQL module that can be executed at the same time by several sessions, the Database Engine must be able to distinguish the tables created by the different sessions. The Database Engine does this by internally appending a unique suffix to each local temporary table name. The full name of a temporary table as stored in the table in is made up of the table name specified in the statement and the system-generated unique suffix. To allow for the suffix, table_name specified for a local temporary name can't exceed 116 characters.\n\nTemporary tables are automatically dropped when they go out of scope, unless explicitly dropped earlier by using :\n• A local temporary table created in a stored procedure is dropped automatically when the stored procedure is finished. The table can be referenced by any nested stored procedures executed by the stored procedure that created the table. The table can't be referenced by the process that called the stored procedure that created the table.\n• All other local temporary tables are dropped automatically at the end of the current session.\n• If the database-scoped configuration is set to ON (default), then global temporary tables are automatically dropped when the session that created the table ends and all other tasks have stopped referencing them. The association between a task and a table is maintained only for the life of a single Transact-SQL statement. This means that a global temporary table is dropped at the completion of the last Transact-SQL statement that was actively referencing the table when the creating session ended.\n• If the database-scoped configuration is set to OFF, then global temporary tables are only dropped using , or when the Database Engine instance restarts. For more information, see GLOBAL_TEMPORARY_TABLE_AUTO_DROP.\n\nA local temporary table created within a stored procedure or trigger can have the same name as a temporary table that was created before the stored procedure or trigger is called. However, if a query references a temporary table and two temporary tables with the same name exist at that time, it isn't defined which table the query is resolved against. Nested stored procedures can also create temporary tables with the same name as a temporary table that was created by the calling stored procedure. However, for modifications to resolve to the table that was created in the nested procedure, the table must have the same structure, with the same column names, as the table created in the calling procedure. This is shown in the following example.\n\nWhen you create local or global temporary tables, the syntax supports constraint definitions except for constraints. If a constraint is specified in a temporary table, the statement returns a warning message that states the constraint was skipped. The table is still created without the constraint. Temporary tables can't be referenced in constraints.\n\nIf a temporary table is created with a named constraint and the temporary table is created within the scope of a user-defined transaction, only one user at a time can execute the statement that creates the temp table. For example, if a stored procedure creates a temporary table with a named primary key constraint, the stored procedure can't be executed simultaneously by multiple users.\n\nGlobal temporary tables in SQL Server (table names prefixed with ) are stored in and shared among all user sessions across the entire SQL Server instance.\n\nAzure SQL Database supports global temporary tables that are also stored in but are scoped to the database level. This means that global temporary tables are shared among all user sessions within the same database. User sessions from other databases can't access global temporary tables. Otherwise, global temporary tables for Azure SQL Database follow the same syntax and semantics that SQL Server uses.\n\nSimilarly, global temporary stored procedures are also scoped to the database level in Azure SQL Database.\n\nLocal temporary tables (table names prefixed with ) are also supported for Azure SQL Database and follow the same syntax and semantics that SQL Server uses. For more information, see Temporary tables.\n\nAny user can create and access temporary objects.\n\nBefore creating a partitioned table by using CREATE TABLE, you must first create a partition function to specify how the table becomes partitioned. A partition function is created by using CREATE PARTITION FUNCTION. Second, you must create a partition scheme to specify the filegroups that will hold the partitions indicated by the partition function. A partition scheme is created by using CREATE PARTITION SCHEME. Placement of PRIMARY KEY or UNIQUE constraints to separate filegroups can't be specified for partitioned tables. For more information, see Partitioned Tables and Indexes.\n• None A table can contain only one PRIMARY KEY constraint.\n• None The index generated by a PRIMARY KEY constraint can't cause the number of indexes on the table to exceed 999 nonclustered indexes and 1 clustered index.\n• None If CLUSTERED or NONCLUSTERED isn't specified for a PRIMARY KEY constraint, CLUSTERED is used if there are no clustered indexes specified for UNIQUE constraints.\n• None All columns defined within a PRIMARY KEY constraint must be defined as NOT NULL. If nullability isn't specified, all columns participating in a PRIMARY KEY constraint have their nullability set to NOT NULL. For memory-optimized tables, the nullable key column is allowed.\n• None If a primary key is defined on a CLR user-defined type column, the implementation of the type must support binary ordering. For more information, see CLR User-Defined Types.\n• If CLUSTERED or NONCLUSTERED isn't specified for a UNIQUE constraint, NONCLUSTERED is used by default.\n• Each UNIQUE constraint generates an index. The number of UNIQUE constraints can't cause the number of indexes on the table to exceed 999 nonclustered indexes and 1 clustered index.\n• If a unique constraint is defined on a CLR user-defined type column, the implementation of the type must support binary or operator-based ordering. For more information, see CLR User-Defined Types.\n• None When a value other than NULL is entered into the column of a FOREIGN KEY constraint, the value must exist in the referenced column; otherwise, a foreign key violation error message is returned.\n• None FOREIGN KEY constraints are applied to the preceding column, unless source columns are specified.\n• None FOREIGN KEY constraints can reference only tables within the same database on the same server. Cross-database referential integrity must be implemented through triggers. For more information, see CREATE TRIGGER.\n• None FOREIGN KEY constraints can reference another column in the same table. This is referred to as a self-reference.\n• None The REFERENCES clause of a column-level FOREIGN KEY constraint can list only one reference column. This column must have the same data type as the column on which the constraint is defined.\n• None The REFERENCES clause of a table-level FOREIGN KEY constraint must have the same number of reference columns as the number of columns in the constraint column list. The data type of each reference column must also be the same as the corresponding column in the column list. The reference columns must be specified in the same order that was used when specifying the columns of the primary key or unique constraint on the referenced table.\n• None CASCADE, SET NULL or SET DEFAULT can't be specified if a column of type timestamp is part of either the foreign key or the referenced key.\n• None CASCADE, SET NULL, SET DEFAULT and NO ACTION can be combined on tables that have referential relationships with each other. If the Database Engine encounters NO ACTION, it stops and rolls back related CASCADE, SET NULL and SET DEFAULT actions. When a DELETE statement causes a combination of CASCADE, SET NULL, SET DEFAULT and NO ACTION actions, all the CASCADE, SET NULL and SET DEFAULT actions are applied before the Database Engine checks for any NO ACTION.\n• None The Database Engine doesn't have a predefined limit on either the number of FOREIGN KEY constraints a table can contain that reference other tables, or the number of FOREIGN KEY constraints that are owned by other tables that reference a specific table. Nevertheless, the actual number of FOREIGN KEY constraints that can be used is limited by the hardware configuration and by the design of the database and application. We recommend that a table contain no more than 253 FOREIGN KEY constraints, and that it be referenced by no more than 253 FOREIGN KEY constraints. The effective limit for you might be more or less depending on the application and hardware. Consider the cost of enforcing FOREIGN KEY constraints when you design your database and applications.\n• None FOREIGN KEY constraints can reference only columns in PRIMARY KEY or UNIQUE constraints in the referenced table or in a UNIQUE INDEX on the referenced table.\n• None If a foreign key is defined on a CLR user-defined type column, the implementation of the type must support binary ordering. For more information, see CLR User-Defined Types.\n• None Columns participating in a foreign key relationship must be defined with the same length and scale.\n• None A column can have only one DEFAULT definition.\n• None A DEFAULT definition can contain constant values, functions, SQL standard niladic functions, or . The following table shows the niladic functions and the values they return for the default during an INSERT statement. Name of user performing an insert. Name of user performing an insert. Name of user performing an insert. Name of user performing an insert.\n• None constant_expression in a DEFAULT definition can't refer to another column in the table, or to other tables, views, or stored procedures.\n• None DEFAULT definitions can't be created on columns with a timestamp data type or columns with an IDENTITY property.\n• None DEFAULT definitions can't be created for columns with alias data types if the alias data type is bound to a default object.\n• None A column can have any number of CHECK constraints, and the condition can include multiple logical expressions combined with AND and OR. Multiple CHECK constraints for a column are validated in the order they are created.\n• None The search condition must evaluate to a Boolean expression and can't reference another table.\n• None A column-level CHECK constraint can reference only the constrained column, and a table-level CHECK constraint can reference only columns in the same table. CHECK CONSTRAINTS and rules serve the same function of validating the data during INSERT and UPDATE statements.\n• None When a rule and one or more CHECK constraints exist for a column or columns, all restrictions are evaluated.\n• None CHECK constraints can't be defined on text, ntext, or image columns.\n• An index created for a constraint can't be dropped by using ; the constraint must be dropped by using . An index created for and used by a constraint can be rebuilt by using . For more information, see Reorganize and Rebuild Indexes.\n• Constraint names must follow the rules for identifiers, except that the name can't start with a number sign (#). If constraint_name isn't supplied, a system-generated name is assigned to the constraint. The constraint name appears in any error message about constraint violations.\n• When a constraint is violated in an , , or statement, the statement is ended. However, when is set to OFF, the transaction, if the statement is part of an explicit transaction, continues to be processed. When is set to ON, the whole transaction is rolled back. You can also use the statement with the transaction definition by checking the system function.\n• When and , row-, page-, and table-level locks are allowed when you access the index. The Database Engine chooses the appropriate lock and can escalate the lock from a row or page lock to a table lock. When and , only a table-level lock is allowed when you access the index.\n• If a table has FOREIGN KEY or CHECK CONSTRAINTS and triggers, the constraint conditions are evaluated before the trigger is executed.\n\nFor a report on a table and its columns, use or . To rename a table, use . For a report on the views and stored procedures that depend on a table, use sys.dm_sql_referenced_entities and sys.dm_sql_referencing_entities.\n\nThe nullability of a column determines whether that column can allow a null value ( ) as the data in that column. isn't zero or blank: means no entry was made or an explicit was supplied, and it typically implies that the value is either unknown or not applicable.\n\nWhen you use or to create or alter a table, database and session settings influence and possibly override the nullability of the data type that is used in a column definition. We recommend that you always explicitly define a column as NULL or NOT NULL for noncomputed columns or, if you use a user-defined data type, that you allow the column to use the default nullability of the data type. Sparse columns must always allow NULL.\n\nWhen column nullability isn't explicitly specified, column nullability follows the rules shown in the following table.\n\nWhen neither of the ANSI_NULL_DFLT options is set for the session and the database is set to the default (ANSI_NULL_DEFAULT is OFF), the default of NOT NULL is assigned.\n\nIf the column is a computed column, its nullability is always automatically determined by the Database Engine. To find out the nullability of this type of column, use the function with the AllowsNull property.\n\nSystem tables can't be enabled for compression. When you are creating a table, data compression is set to NONE, unless specified otherwise. If you specify a list of partitions or a partition that is out of range, an error will be generated. For a more information about data compression, see Data Compression.\n\nTo evaluate how changing the compression state will affect a table, an index, or a partition, use the sp_estimate_data_compression_savings stored procedure.\n\nRequires permission in the database and permission on the schema in which the table is being created.\n\nIf any columns in the statement are defined to be of a user-defined type, permission on the user-defined type is required.\n\nIf any columns in the statement are defined to be of a CLR user-defined type, either ownership of the type or permission on it is required.\n\nIf any columns in the statement have an XML schema collection associated with them, either ownership of the XML schema collection or permission on it is required.\n\nAny user can create temporary tables in .\n\nIf the statement creates a ledger table, permission is required.\n\nThe following example shows the column definition for a PRIMARY KEY constraint with a clustered index on the column of the table. Because a constraint name isn't specified, the system supplies the constraint name.\n\nA FOREIGN KEY constraint is used to reference another table. Foreign keys can be single-column keys or multicolumn keys. This following example shows a single-column FOREIGN KEY constraint on the table that references the table. Only the REFERENCES clause is required for a single-column FOREIGN KEY constraint.\n\nYou can also explicitly use the FOREIGN KEY clause and restate the column attribute. The column name doesn't have to be the same in both tables.\n\nMulticolumn key constraints are created as table constraints. In the database, the table includes a multicolumn PRIMARY KEY. The following example shows how to reference this key from another table; an explicit constraint name is optional.\n\nUNIQUE constraints are used to enforce uniqueness on nonprimary key columns. The following example enforces a restriction that the column of the table must be unique.\n\nDefaults supply a value (with the INSERT and UPDATE statements) when no value is supplied. For example, the database could include a lookup table listing the different jobs employees can fill in the company. Under a column that describes each job, a character string default could supply a description when an actual description isn't entered explicitly.\n\nIn addition to constants, DEFAULT definitions can include functions. Use the following example to get the current date for an entry.\n\nA niladic-function scan can also improve data integrity. To keep track of the user that inserted a row, use the niladic-function for USER. Don't enclose the niladic-functions with parentheses.\n\nThe following example shows a restriction made to values that are entered into the column of the table. The constraint is unnamed.\n\nThis example shows a named constraint with a pattern restriction on the character data entered into a column of a table.\n\nThis example specifies that the values must be within a specific list or follow a specified pattern.\n\nThe following example shows the complete table definitions with all constraint definitions for table created in the database. To run the sample, the table schema is changed to .\n\nG. Create a table with an xml column typed to an XML schema collection\n\nThe following example creates a table with an column that is typed to XML schema collection . The keyword specifies that each instance of the data type in column_name can contain only one top-level element.\n\nThe following example creates a partition function to partition a table or index into four partitions. Then, the example creates a partition scheme that specifies the filegroups in which to hold each of the four partitions. Finally, the example creates a table that uses the partition scheme. This example assumes the filegroups already exist in the database.\n\nBased on the values of column of , the partitions are assigned in the following ways.\n\nI. Use the UNIQUEIDENTIFIER data type in a column\n\nThe following example creates a table with a column. The example uses a PRIMARY KEY constraint to protect the table against users inserting duplicated values, and it uses the function in the constraint to provide values for new rows. The ROWGUIDCOL property is applied to the column so that it can be referenced using the $ROWGUID keyword.\n\nJ. Use an expression for a computed column\n\nThe following example shows the use of an expression ( ) for calculating the computed column.\n\nThe following example creates a table with one column defined as user-defined type , assuming that the type's assembly, and the type itself, have already been created in the current database. A second column is defined based on , and uses method of type(class) to compute a value for the column.\n\nL. Use the USER_NAME function for a computed column\n\nThe following example uses the function in the column.\n\nThe following example creates a table that has a column . If a table has one or more columns, the table must have one column.\n\nThe following example creates a table that uses row compression.\n\nApplies to: SQL Server 2022 (16.x) and later versions, Azure SQL Database, and Azure SQL Managed Instance.\n\nThe following example creates a table that uses XML compression.\n\nP. Create a table that has sparse columns and a column set\n\nThe following examples show to how to create a table that has a sparse column, and a table that has two sparse columns and a column set. The examples use the basic syntax. For more complex examples, see Use Sparse Columns and Use Column Sets.\n\nThis example creates a table that has a sparse column.\n\nThis example creates a table that has two sparse columns and a column set named .\n\nApplies to: SQL Server 2016 (13.x) and later, and Azure SQL Database.\n\nThe following examples show how to create a temporal table linked to a new history table, and how to create a temporal table linked to an existing history table. The temporal table must have a primary key defined to be enabled for the table to be enabled for system versioning. For examples showing how to add or remove system versioning on an existing table, see System Versioning in Examples. For use cases, see Temporal Tables.\n\nThis example creates a new temporal table linked to a new history table.\n\nThis example creates a new temporal table linked to an existing history table.\n\nApplies to: SQL Server 2016 (13.x) and later, and Azure SQL Database.\n\nThe following example shows how to create a system-versioned memory-optimized temporal table linked to a new disk-based history table.\n\nThis example creates a new temporal table linked to a new history table.\n\nThis example creates a new temporal table linked to an existing history table.\n\nThe following example creates a table with two encrypted columns. For more information, see Always Encrypted.\n\nThe following shows how to use NONCLUSTERED inline for disk-based tables:\n\nCreates a table with an anonymously named compound primary key. This is useful to avoid run-time conflicts where two session-scoped temp tables, each in a separate session, use the same name for a constraint.\n\nIf you explicitly name the constraint, the second session will generate an error such as:\n\nThe problem arises from the fact that while the temp table name is unique, the constraint names aren't.\n\nSession A creates a global temp table ##test in Azure SQL Database testdb1 and adds one row\n\nObtain global temp table name for a given object ID 1253579504 in (2)\n\nSession B connects to Azure SQL Database testdb1 and can access table ##test created by session A\n\nSession C connects to another database in Azure SQL Database testdb2 and wants to access ##test created in testdb1. This select fails due to the database scope for the global temp tables\n\nWhich generates the following error:\n\nThe following example creates a table with data retention enabled and a retention period of one week. This example applies to Azure SQL Edge only.\n\nThe following example creates an updatable ledger table that isn't a temporal table with an anonymous history table (the system will generate the name of the history table) and the generated ledger view name. As the names of the required generated always columns and the additional columns in the ledger view aren't specified, the columns will have the default names.\n\nThe following example creates a table that is both a temporal table and an updatable ledger table, with an anonymous history table (with a name generated by the system), the generated ledger view name and the default names of the generated always columns and the additional ledger view columns.\n\nThe following example creates a table that is both a temporal table and an updatable ledger table with the explicitly named history table, the user-specified name of the ledger view, and the user-specified names of generated always columns and additional columns in the ledger view.\n\nThe following example creates an append-only ledger table with the generated names of the ledger view and the columns in the ledger view.\n\nThe following example creates a ledger database in Azure SQL Database and an updatable ledger table using the default settings. Creating an updatable ledger table in a ledger database doesn't require using WITH (SYSTEM_VERSIONING = ON, LEDGER = ON); ."
    },
    {
        "link": "https://stackoverflow.com/questions/43453120/creating-new-table-from-select-statement",
        "document": "I have a select query that I am using in Access to fetch some particular data that I want to achieve. Now I am moving to SQL Server (creating ssrs report) so I want to take this query from Access and use it in SQL Server and create a new table every time when I execute this query.\n\nHere is my current query that I use in Access db:\n\nWhere should I insert/into statement...or is that something else that I need here."
    },
    {
        "link": "https://stackoverflow.com/questions/18702203/select-columns-from-one-table-based-on-the-column-names-from-another-table",
        "document": "I have 2 tables, one that contains the final results I need, and another that contains the list of columns I need to select from based on a set level.\n\nSo if i do the following\n\nThen basically i need to use the column names from this select statement to determine which columns are selected from another statement.\n\nThis is what ive tried and of course i know its wrong but gives you an idea of what im trying to do.\n\nIn a way im trying to dynamically build an sql query which can be altered by whichever columns i put in the table.\n\nThis should in theory act the same as the following sql query"
    },
    {
        "link": "https://geeksforgeeks.org/sql-create-table",
        "document": "The SQL CREATE TABLE statement is a foundational command used to define and structure a new table in a database. By specifying the columns, data types, and constraints such as PRIMARY KEY, NOT NULL, and CHECK, helps you design the database schema.\n\nIn this article, we’ll learn the syntax, best practices, and practical examples of using the CREATE TABLE statement in SQL. We’ll also cover how to create tables from existing data and troubleshoot common errors.\n\nTo create a new table in the database, use the SQL CREATE TABLE statement. A table’s structure, including column names, data types, and constraints like NOT NULL, PRIMARY KEY, and CHECK, are defined when it is created in SQL.\n\nThe CREATE TABLE command is a crucial tool for database administration because of these limitations, which aid in ensuring data integrity. To create a table in SQL, use this CREATE TABLE syntax:\n• table_name : The name you assign to the new table.\n• column1, column2, … : The names of the columns in the table.\n• datatype(size): Defines the data type and size of each column.\n\nLet’s look at examples of CREATE TABLE command in SQL and see how to create table in SQL.\n\nIn this example, we will create a new table and insert data into it. Let us create a table to store data of Customers, so the table name is Customer, Columns are Name, Country, age, phone, and so on.\n\nAfter creating the table, you can use INSERT INTO command to add data to it. Here’s how to add some sample records into the Customer table:\n\nWe can also create a new table based on the structure (and optionally the data) of an existing table. The following query creates a new table called SubTable that contains CustomerID and CustomerName from the existing Customer table.\n\nNote: We can use * instead of column name to copy whole table to another table.\n• CREATE TABLE statement is used to create new table in a database.\n• None It defines the structure of table including name and datatype of columns.\n• None command can be used to display the structure of the created table\n• None We can also add constraint to table like NOT NULL, UNIQUE\n• None If you try to create a table that already exists, MySQL will throw an error. To avoid this, you can use the\n\nThe SQL CREATE TABLE statement is essential for setting up tables and defining data structures within a database. Knowing how to create a table in SQL with the CREATE TABLE SQL syntax helps you establish reliable tables with appropriate constraints. Whether you’re creating an Employee table in SQL with primary keys or duplicating tables with CREATE TABLE AS SELECT, mastering this command is vital for managing data effectively.\n\nIn this guide, we covered SQL CREATE TABLE syntax with various examples, from basic to advanced usage. By applying these practices, you can confidently build tables in SQL for effective data organization and retrieval.\n\nHow do you create a table in SQL?\n\nHow to create a query in SQL?\n\nHow to create a duplicate table in SQL Server?"
    },
    {
        "link": "https://medium.com/@BrandonSouthern/sql-best-practices-e1c61e96ee27",
        "document": "In this post I’m going to share some best practices for formatting SQL that I’ve learned and used over the last 20 years. Many of these tips are things that I’ve collected along the way out of frustration with inheriting code, needing to perform updates, chasing down bugs, and performing code reviews for others. Also, when it comes to training new employees, I have found that it is much more difficult for people to understand the domain and code when dealing with poorly written SQL.\n\nIn each section of this article, I’ll try to focus in on one area of practice. I’ll provide you with an example of a bad practice and example(s) of best practices along with my reasons behind writing code a specific way. As you’re reading you might think, “Well that isn’t as performant of code” and sometimes this might be the case because I’m trying to hone in on a specific point and avoid excess potential for confusion. Also, sometimes I prefer to have slightly less performant code (depending on how often it is used, the purpose of the code, and the performance hit) if it makes the code easier to read and maintain.\n\nI’m guessing that some people reading this might say, “Well my code executes just fine so I don’t care. I understand what it does.” To that (because I’ve heard these comments many times over the years) I would challenge that assertion. It takes significantly more time to find and read sloppy “bad practice” code than it does when working with clean code. These practices will also help to reduce bugs and your code will be appreciated by everyone that reads it.\n\nWhen we’re talking about programming languages, you can pretty much write statements as you see fit and the code will execute. Obviously there are some exceptions to this (such as indentations in Python) but that’s for another day. What this means is that when I’m talking about SQL best practices, one could make the argument that the this is just a matter of opinion and writing style.\n\nHow you write it matters. Think about a book for example. Sure, you could write an entire book without paragraphs, line returns, standard spacing between sentences, and more. Yes, it would still be a book and yes, the reader could probably understand it. However, how easily would it be for you find a specific section if there weren’t paragraphs? How easy would it be to have confidence that you’re actually in the right section if you were asked to edit something? For these reasons any many more, I’m outlining some best practices.\n\nPlease note that while all of these practices could be considered “opinions”, they are practices that I’ve used after 20 years of making mistakes, challenges with efficiently reading, editing, and understanding code, and watching my team members face the same challenges. I’ll try my best to tell you about the practice and the logic behind using them. I’ll leave it up to you to consider these practices, see what works for you, and what works for your team.\n\nThe first thing that I’d like to talk about is formatting. Code should be well formatted and visually appealing, which makes it very easy to read. Having properly formatted code we pay off when it comes to debugging, troubleshooting, and modifying your code.\n\nWhen it comes to formatting, there are a number of things that should be considered such as intentions, alignment, comma positions, and text case. If you made it this far in the reading you’ve probably noticed how well aligned my code is and that most items are found on a single line. This make a tremendous difference in readability.\n\nBelow is an example of code that has been written and is quite unreadable. In this example you’ll notice a number of things go against best practices such as:\n• grouping by number instead of name\n\nIn this article I’ll discuss these practices and more to help you write clean and bug-free code that you can be proud of.\n\nLook at the code below and compare to the code above. Which one is easier to read? Which one provides the best context about what the intent of the code is and what some of the conditional values mean? Which version provides a cleaner UI that will allow you to quickly spot bugs or avoid bugs all together? I’m hoping that you’ve agreed that the code below serves to overcome all of the challenges found in the above code.\n\nThe counter argument that I’ve heard people say is, “Well, you had to write more lines of code.” This is irrelevant. The computer doesn’t care and it’s a couple of key strokes to have a line return and spaces or tabs.\n\nIn the following sections I will discuss each of these issues and provide examples of the good and bad practices.\n\nIf you look at the above “best practice” code, notice how well everything is aligned to the left. All commas, spaces, and indentations make the code very easy to read.\n\nMy general rule is one item per line. This could be one element in your select statement or one condition in a join statement or one case statement. Again, look at the above code on formatting to see the difference of readability when writing with one item per line. The key here is to be consistent. I’ve seen code that is written with one item per line but then every so often there will be a *join* clause that has an and and an or statement that are on the same line. This can be immensely frustrating when reading and debugging because it is very easy to overlook the addition condition because it was written to the same line.\n\nHere we see a case statement that is all in one line. This is a bad practice because it make the code hard to read and quickly pick up on all of the conditions that are being evaluated. Also, it is really challenging if not impossible to properly comment the code. I know that in the example, ‘main_reporting’ isn’t descriptive and doesn’t appear consistent with the other values but hey, sometimes you’re told to output values this way and can’t logic to others.\n\nHere we see a case statement that is written on multiple lines with comments to help provide clarity.\n\nPlease comment your code. I’ll write another post about code comments in the future but comments are important. I feel like I see a post on LinkedIn or some other site on a daily basis where someone says something along the lines of, “You don’t need code comments. The code is a comment. What’s the matter? Don’t you know how to read code?” Seriously, I’ve heard this sort of thing for years. But here’s the reality. While code is a language and if proficient in the language, a reader can understand <strong>what</strong> the code is doing. But the code <strong>never</strong> tells the reader <strong>why</strong> someone wanted to code to function that way. The possibilities are endless as to why someone wanted to code to work a certain way. Sometimes you could be coding around a bug in the back-end data or maybe there is business logic that dictates how the code should function.\n\nWhile it’s true that you could read the code and possibly look up the documentation on certain tables, it’s that a lot more work than typing a few characters. Below are some examples of good and bad commenting practices.\n\nLook at the code below. We can see that the code only wants to return results where u.id > 1000. That’s pretty obvious in this very simple example. But the more important question is why did someone do this?\n\nMaybe they are test users prior to u.id = 1000. Or maybe the code is filtering out all users that are from Michigan, because for some reason someone thought that all users less than u.id 1000 are from the state of Michigan. That might sound like a horrible idea to actually have code that would be written that way but it executes all the same. The point here is that as new users, we don’t know and chances are that in six months you probably won’t know either.\n\nHere we have an in-line comment that tells us a bit more about why we added the u.id > 1000 condition. We evidently have test users that should be scrubbed out of the result set.\n\nLook at the code below. We can see here that the query is going to return users that are considered to be non-test users. The in-line comment helps us to understand that the desire is to scrub out these test users from the result set. But we had to read a few lines of code. You may be saying, well, it isn’t worth a comment block at the top. It’s just 8 lines of code and it’s obvious what is happening. Simple or not, the reader doesn’t know the <strong>why</strong> behind this code. But what if the code wasn’t as simple? You’d surely appreciate some comments. For these reasons and to have better planning before you start writing code (more on that later),\n\nBelow is the same code that we just looked at by now we are telling the user why we\n\nwant to run this code and things to look out for.\n\nCommon table expressions or CTEs are a way of creating an in-memory table of your query results. This table can then be used throughout the rest of your SQL script. The benefit to using a CTE is that you can reduce code duplication, make your code more readable, and increase your ability to perform QA checks on your results.\n\nAlso note the really good block comment.\n\nExample of Code Not Using CTE’s\n\nIn the code below we can see that there are two sub queries that are returning results. These two subqueries are then joined together to produce the final result set. While this code will execute, there are a few concerns:\n\n1. It’s really hard to perform a QA on the sub queries and inspect the results. For example, what if we wanted to run a some counts on the number of users that have multiple records for default screens? We can’t easily just execute some sql against the sub query. We’d have to copy/paste the sub query and then modify it to perform this qa. It would be much better if we could avoid changing code during our QA process.\n\n2. If we need to utilize this users sub-query elsewhere in our code, we’d have to re-write or copy/paste that block of code to other places in our script. This would not be a DRY (don’t repeat yourself)\n\nprocess and exposes more potential to bugs. How so? Assume for a moment that you’ve used the users subquery in 5 locations in your script. Also assume that the code that you are working with is not easy to read because it doesn’t follow best practices. If you are asked to update the code to add another condition to scrub out additional test users, there’s a good chance that you could miss adding this condition\n\nto at least one of the 5 uses of the subquery.\n\n3. More cycles on the database. Each time that the subquery is execute it performs table scans to return results. With our users subquery containing wildcard conditions, the database is going to have a fair amount of work to do. It’s much cheaper (CPU cycles and dollars if you’re using cloud databases) to perform the subquery once, store it in memory and then just re-use the result set as needed in your code.\n\n4. More complex to read the entire block of code and understand what is being performed and why. While it is possible to scroll through the code, it may be hard to easily comprehend what is happening. Generally speaking, if you have to vertically scroll your code on your monitor, your code is too way too long and should be refactored to smaller components.\n\nExample of Using CTEs\n\nBelow we see an example of using CTEs. While CTE’s can be great and help to overcome some of the challenges that we previously pointed out, CTEs typically don’t stay in memory after the final result set has been displayed.\n\nFor example, if you were to run this entire block of code, it would execute. But then if you wanted to select all of the results from the users CTE a few minutes later, this data wouldn’t be available to query. To get around this issue you can use volatile tables or temporary in-memory tables that typically live as long as your session (database connection) remains active. More on this topic later.\n\nYou should never write queries with “select *”. I think the only exception to this rule is if you are trying to inspect a table and in such a case, you should always limit the number of results that are returned. Writing queries this way is a bad idea of many reasons:\n\n1. Database performance. Returning columns that aren’t needed is more expensive than querying only the columns that you care about.\n\n2. Challenges debugging. Assuming that you’re using CTEs as described in the previous section, it can be very challenging to trace the origins of certain attributes.\n\n3. Tables change. Even if you actually need to select all columns, there’s no guarantee that your table won’t change over time. And as the table changes you’ll be querying new data that was never intended which could possibly break code elsewhere, cause confusion, or impact database performance and costs.\n\nAliasing is very important to help readers understand where elements reside and what tables are being used. When aliases aren’t used or poor naming conventions are used, complexity is increased, and the reading/comprehension of code is reduced.\n\nBad Practice — No Alias Used on Fields\n\nBelow you can see that the tables have an alias of ‘u’ and ‘p’ but the selected elements don’t utilize the alias. This can be very frustrating and can cause run-time errors if more than one of the tables contains a field with the same name. Eg. user_id is found in both the ‘users’ table and the ‘preferences’ table.\n\nBest Practice — Alias Used on Fields\n\nBelow you can see that the tables and selected elements utilize the table alias name. This makes the code very readable for the end user. Even if you only have a single table it is a good practice to use an alias on the table and field name. Good habits make for good code.\n\nBelow we have a CTE that has been created but the table name that has been assigned is called ‘cte’. This is a very generic name and tells the end-user absolutely nothing about the data in the table. If you were a user reading the select statement that follows the CTE, you wouldn’t have any indication of what table was being used.\n\nBelow we have a CTE that has been created with a more descriptive name. The name give the user some indication of what data is contained within the able.\n\nIn select statements I prefer to have a leading comma as opposed to a trailing comma and this is one of those cases where I would say my personal opinion comes into play. I’ve seen a lot of people write their selected elements with trailing comma’s and in other languages it is common practice to have a trailing comma. However, with other languages, it isn’t common to have a large number of arguments past into a function whereas in SQL it’s quite common to have a large number of elements being selected (and declared in the code). While you may think that I’m being overly opinionated about this usage, here’s a couple of reasons why I find leading comma’s to be beneficial.\n\n1. Clean looking UI. When you look at the best practice example, look at how nicely the commas are aligned. It is very easy to see that a comma is missing and avoid a run-time error when compared to using trailing commas\n\n2. No confusion when working with longer case statements that wrap lines. Looking at the example below, it’s difficult to tell if the end of the line is the end of an element or statement or if it is specifying the end of an argument that is being passed into a function.\n\nTo add some additional frustration around this bad practice of trailing commas, BigQuery query formatter actually reformats your code to display everything a trailing comma. :(\n\nIn this example we can see that leading commas are not used, making it very hard to spot the missing comma.\n\nIn this example we can see that all of the commas are aligned, making it easy to ensure that no comma is ever missing.\n\nIn this example we can see that leading commas are not used. We have a line break that ends with a comma so it is vary hard to tell if this ‘max’ line is really one single statement or if it is part of a longer statement. One could argue that you shouldn’t use line breaks like this in your code and while I support line breaks in the right places (because it makes code easier to read), you would still encounter the same issue when looking at your word-wrapped code in your editor or in a Git diff comparison.\n\nBest Practice — Leading Comma with Indentation on Wrapped Text\n\nIn the example below it’s easy to see that a comma is missing. You might immediately wonder if someone forgot the comma, but because it’s so easy to read code with leading commas, there’s a higher probability that the missing leading comma is by design and there isn’t actually a comma that should be in front of the ‘cast’ statement. Also, by adding indentation to the cast statement, the code appears to have a more obvious intent as to why a comma isn’t necessary.\n\nHad someone performed a carriage return after “desc)” in line 3, you’d now have a common beginning on line 4. Normally, we want to have all of the commas as leading characters. But that logic only applies if we are talking about an attribute (column) being returned. In our case, that comma at the end of line 3 is part of a case statement so things would get really confusing.\n\nIn a lot of SQL code in the old days, this was pretty common practice and it might have to do with the fact that SQL has been around for very long time, probably longer than most text editors with syntax highlighting. Today, most people are using (or should be using) syntax highlighting in their editors so the capitalization shouldn’t be as necessary to spot the reserve words.\n\nWhile I have a personal opinion to not use uppercase you may disagree and have a different opinion. Here’s my justification:\n\n1. I don’t like my code yelling at me. In social context and written communication, using uppercase is synonymous with yelling so I try to avoid writing this way.\n\n2. The code doesn’t read as fluidly. Psychologic studies have shown that word recognition is easier with lower case words that upper case words. This is because there is more shape variation with lower case words compared to upper case words. Reading speed can decrease by 13–20% when all upper case is used. Also, when you mix upper case and lower case words\n\n3. It’s extra key strokes to hold down the shift key while I type or lock/unlock the caps key. Not a good argument, I know.\n\nIn this example we can see that reserve words are uppercased and all other words are lowercased.\n\nIn this example we can see that all of the commas are aligned, making it easy to ensure that no comma is ever missing.\n\nI almost always perform my group by with explicit field names as opposed to the position number in the select statement. While this doesn’t have any bering on the results I have found that it saves me time when when it comes to typing and debugging. Normally I’d prefer to just use the numbers because it’s less text on the page but it has caused too many issues and time spent chasing down run-time bugs.\n\nHere you can see that we are performing a group-by with position numbers based on the selected items. What I don’t like about this practice is that:\n\n1. If someone puts an aggregate function(s) anywhere but the first or last item in the select statement then you have to skip a position number in the group by. This creates frustrations if you later decide to re-order your selected elements.\n\n2. You have to count out how many elements you have, minus the aggregated items, and then manually type the position number.\n\nBest Practice — Group by field name\n\nHere you can see that we are performing a group-by with explicit field names. While it looks like this would be a lot more typing and work versus using position numbers, it is actually faster to type than using numbers. How so? Because all that you have to do is copy what is in the select statement (minus the aggregated field) and paste those values in the group-by. When you use numbers you actually have to type the numbers."
    },
    {
        "link": "https://community.spiceworks.com/t/sql-select-statement-to-bring-up-all-fields-column-names-in-a-table/627210",
        "document": "Learn how to modify a stored procedure in SQL Server by using SQL Server Management Studio or Transact-SQL."
    }
]