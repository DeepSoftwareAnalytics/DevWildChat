[
    {
        "link": "https://docs.pydantic.dev/latest",
        "document": "Pydantic is the most widely used data validation library for Python.\n\nFast and extensible, Pydantic plays nicely with your linters/IDE/brain. Define how data should be in pure, canonical Python 3.8+; validate it with Pydantic.\n\nBuilt by the same team as Pydantic, Logfire is an application monitoring tool that is as simple to use and powerful as Pydantic itself. Logfire integrates with many popular Python libraries including FastAPI, OpenAI and Pydantic itself, so you can use Logfire to monitor Pydantic validations and understand why some inputs fail validation: # this will record details of a successful validation to logfire\n• Set logfire record all both successful and failed validations, use to only record failed validations, learn more.\n• This will raise a since there are too few , details of the input data and validation errors will be recorded in Logfire. Would give you a view like this in the Logfire platform: This is just a toy example, but hopefully makes clear the potential value of instrumenting a more complex application.\n• Powered by type hints — with Pydantic, schema validation and serialization are controlled by type annotations; less to learn, less code to write, and integration with your IDE and static analysis tools. Learn more…\n• Speed — Pydantic's core validation logic is written in Rust. As a result, Pydantic is among the fastest data validation libraries for Python. Learn more…\n• JSON Schema — Pydantic models can emit JSON Schema, allowing for easy integration with other tools. Learn more…\n• Strict and Lax mode — Pydantic can run in either strict mode (where data is not converted) or lax mode where Pydantic tries to coerce data to the correct type where appropriate. Learn more…\n• Dataclasses, TypedDicts and more — Pydantic supports validation of many standard library types including and . Learn more…\n• Customisation — Pydantic allows custom validators and serializers to alter how data is processed in many powerful ways. Learn more…\n• Ecosystem — around 8,000 packages on PyPI use Pydantic, including massively popular libraries like FastAPI, huggingface, Django Ninja, SQLModel, & LangChain. Learn more…\n• Battle tested — Pydantic is downloaded over 70M times/month and is used by all FAANG companies and 20 of the 25 largest companies on NASDAQ. If you're trying to do something with Pydantic, someone else has probably already done it. Learn more…\n\nInstalling Pydantic is as simple as:\n\nTo see Pydantic at work, let's start with a simple example, creating a custom class that inherits from :\n• is of type ; the annotation-only declaration tells Pydantic that this field is required. Strings, bytes, or floats will be coerced to integers if possible; otherwise an exception will be raised.\n• is a string; because it has a default, it is not required.\n• is a field that is required, but the value may be provided; Pydantic will process either a Unix timestamp integer (e.g. ) or a string representing the date and time.\n• is a dictionary with string keys and positive integer values. The type is shorthand for .\n• The input here is an ISO 8601 formatted datetime, but Pydantic will convert it to a object.\n• The key here is , but Pydantic will take care of coercing it to a string.\n• Similarly, Pydantic will coerce the string to the integer .\n• We create instance of by passing our external data to as keyword arguments.\n• We can access fields as attributes of the model.\n• We can convert the model to a dictionary with .\n\nIf validation fails, Pydantic will raise an error with a breakdown of what was wrong:\n• The input data is wrong here — is not a valid integer, and is missing.\n• Trying to instantiate will raise a with a list of errors.\n\nWho is using Pydantic?¶\n\nHundreds of organisations and packages are using Pydantic. Some of the prominent companies and organizations around the world who are using Pydantic include:\n\nFor a more comprehensive list of open-source projects using Pydantic see the list of dependents on github, or you can find some awesome projects using Pydantic in awesome-pydantic."
    },
    {
        "link": "https://dev.to/devasservice/best-practices-for-using-pydantic-in-python-2021",
        "document": "Pydantic is a Python library that simplifies data validation using type hints. It ensures data integrity and offers an easy way to create data models with automatic type checking and validation.\n\nIn software applications, reliable data validation is crucial to prevent errors, security issues, and unpredictable behavior.\n\nThis guide provides best practices for using Pydantic in Python projects, covering model definition, data validation, error handling, and performance optimization.\n\nTo install Pydantic, use pip, the Python package installer, with the command:\n\n\n\nThis command installs Pydantic and its dependencies.\n\nCreate Pydantic models by making classes that inherit from BaseModel. Use Python type annotations to specify each field's type:\n\n\n\nPydantic supports various field types, including int, str, float, bool, list, and dict. You can also define nested models and custom types:\n\n\n\nOnce you've defined a Pydantic model, create instances by providing the required data. Pydantic will validate the data and raise errors if any field doesn't meet the specified requirements:\n• Optional types: Optional from the typing module for fields that can be None\n• Union types: Union from the typing module to specify a field can be one of several types\n\nIn addition to built-in types, you can define custom types using Pydantic's conint, constr, and other constraint functions.\n\nThese allow you to add additional validation rules, such as length constraints on strings or value ranges for integers.\n\nBy default, fields in a Pydantic model are required unless explicitly marked as optional.\n\nIf a required field is missing during model instantiation, Pydantic will raise a ValidationError.\n\nFields can be made optional by using Optional from the typing module and providing a default value.\n\nIn this example, email is optional and defaults to None if not provided.\n\nPydantic allows models to be nested within each other, enabling complex data structures.\n\nNested models are defined as fields of other models, ensuring data integrity and validation at multiple levels.\n\nWhen working with nested models, it's important to:\n• Validate data at each level: Ensure each nested model has its own validation rules and constraints.\n• Use clear and consistent naming conventions: This makes the structure of your data more readable and maintainable.\n• Keep models simple: Avoid overly complex nested structures. If a model becomes too complex, consider breaking it down into smaller, more manageable components.\n• Range validation: Enforces value ranges and lengths using constraints like conint, constr, confloat.\n• Format validation: Checks specific formats, such as EmailStr for validating email addresses.\n• Collection validation: Ensures elements within collections (e.g., list, dict) conform to specified types and constraints.\n\nThese validators simplify the process of ensuring data integrity and conformity within your models.\n\nHere are some examples demonstrating built-in validators:\n\nIn this example, the User model uses built-in validators to ensure the id is greater than 0, the name is between 2 and 50 characters, the email is a valid email address, and the age is 18 or older.\n\n To be able to use the email validator, you need to install an extension to pydantic:\n\n\n\nPydantic allows you to define custom validators for more complex validation logic.\n\nCustom validators are defined using the @field_validator decorator within your model class.\n\nHere, the price_must_be_positive validator ensures that the price field is a positive number.\n\nCustom validators are registered automatically when you define them within a model using the @field_validator decorator. Validators can be applied to individual fields or across multiple fields.\n\nExample of registering a validator for multiple fields:\n\n\n\nIn this example, the names_cannot_be_empty validator ensures that both the first_name and last_name fields are not empty.\n\nPydantic models can be customized using an inner Config class.\n\nThis class allows you to set various configuration options that affect the model's behavior, such as validation rules, JSON serialization, and more.\n\nIn this example, the Config class is used to strip whitespace from string fields and enforce a minimum length of 1 for any string field.\n• validate_default: Validate all fields, even those with default values.\n• use_enum_values: Use the values of enums directly instead of the enum instances.\n\nWhen Pydantic finds data that doesn't conform to the model's schema, it raises a ValidationError.\n\nThis error provides detailed information about the issue, including the field name, the incorrect value, and a description of the problem.\n\nHere's an example of how default error messages are structured:\n\n\n\nIn this example, the error message will indicate that id must be an integer and email must be a valid email address.\n\nPydantic allows you to customize error messages for specific fields by raising exceptions with custom messages in validators or by setting custom configurations.\n\nHere’s an example of customizing error messages:\n\n\n\nIn this example, the error message for price is customized to indicate that it must be a positive number.\n\nEffective error reporting involves providing clear, concise, and actionable feedback to users or developers.\n\nHere are some best practices:\n• Log errors: Use logging mechanisms to record validation errors for debugging and monitoring purposes.\n• Return user-friendly messages: When exposing errors to end-users, avoid technical jargon. Instead, provide clear instructions on how to correct the data.\n• Aggregate errors: When multiple fields are invalid, aggregate the errors into a single response to help users correct all issues at once.\n• Use consistent formats: Ensure that error messages follow a consistent format across the application for easier processing and understanding.\n\nExamples of best practices in error reporting:\n\n\n\nIn this example, validation errors are logged, and a user-friendly error message is returned, helping maintain application stability and providing useful feedback to the user.\n\nLazy initialization is a technique that postpones the creation of an object until it is needed.\n\nIn Pydantic, this can be useful for models with fields that are costly to compute or fetch. By delaying the initialization of these fields, you can reduce the initial load time and improve performance.\n\nIn this example, the expensive_computation field is computed only when accessed for the first time, reducing unnecessary computations during model initialization.\n\nHowever, if you know that certain data has already been validated or if validation is not necessary in some contexts, you can disable validation to improve performance.\n\nThis can be done using the model_construct method, which bypasses validation:\n\nIn this example, User.model_construct is used to create a User instance without triggering validation, which can be useful in performance-critical sections of your code.\n\nWhen dealing with large datasets or high-throughput systems, efficiently parsing raw data becomes critical.\n\nPydantic provides the model_validate_json method, which can be used to parse JSON or other serialized data formats directly into Pydantic models.\n\nIn this example, model_validate_json is used to parse JSON data into a User model directly, providing a more efficient way to handle serialized data.\n\nPydantic models can be configured to validate data only when necessary.\n\nThe validate_default and validate_assignment options in the Config class control when validation occurs, which can help improve performance:\n• validate_default: When set to False, only fields that are set during initialization are validated.\n• validate_assignment: When set to True, validation is performed on field assignment after the model is created.\n\nIn this example, validate_default is set to False to avoid unnecessary validation during initialization, and validate_assignment is set to True to ensure that fields are validated when they are updated.\n\nPydantic's BaseSettings class is designed for managing application settings, supporting environment variable loading and type validation.\n\nThis helps in configuring applications for different environments (e.g., development, testing, production).\n\nExample of using BaseSettings:\n\n\n\nIn this example, settings are loaded from environment variables, and the Config class specifies that variables can be loaded from a .env file.\n\nFor using BaseSettings you will need to install an additional package:\n• Use environment variables: Store configuration values in environment variables to keep sensitive data out of your codebase.\n• Provide defaults: Define sensible default values for configuration settings to ensure the application runs with minimal configuration.\n• Separate environments: Use different configuration files or environment variables for different environments (e.g., .env.development, .env.production).\n• Validate settings: Use Pydantic's validation features to ensure all settings are correctly typed and within acceptable ranges.\n\nCommon Pitfalls and How to Avoid Them\n\nOne common mistake when using Pydantic is misapplying type annotations, which can lead to validation errors or unexpected behavior.\n\nHere are a few typical mistakes and their solutions:\n• Misusing Union Types: Using Union incorrectly can complicate type validation and handling.\n• Optional Fields without Default Values: Forgetting to provide a default value for optional fields can lead to None values causing errors in your application.\n• Incorrect Type Annotations: Assigning incorrect types to fields can cause validation to fail. For example, using str for a field that should be an int.\n\nIgnoring performance implications when using Pydantic can lead to slow applications, especially when dealing with large datasets or frequent model instantiations.\n\nHere are some strategies to avoid performance bottlenecks:\n• Leverage Configuration Options: Use Pydantic's configuration options like validate_default and validate_assignment to control when validation occurs.\n• Optimize Nested Models: When working with nested models, ensure that you are not over-validating or duplicating validation logic.\n• Use Efficient Parsing Methods: Utilize model_validate_json and model_validate for efficient data parsing.\n• Avoid Unnecessary Validation: Use the model_construct method to create models without validation when the data is already known to be valid.\n\nOvercomplicating Pydantic models can make them difficult to maintain and understand.\n\nHere are some tips to keep models simple and maintainable:\n• Document Your Models: Use docstrings and comments to explain complex validation rules or business logic embedded in models.\n• Encapsulate Logic Appropriately: Keep validation and business logic within appropriate model methods or external utilities to avoid cluttering model definitions.\n• Use Inheritance Sparingly: While inheritance can promote code reuse, excessive use can make the model hierarchy complex and harder to follow.\n• Avoid Excessive Nesting: Deeply nested models can be hard to manage. Aim for a balanced level of nesting.\n\nIn this guide, we have covered various best practices for using Pydantic effectively in your Python projects.\n\nWe began with the basics of getting started with Pydantic, including installation, basic usage, and defining models. We then delved into advanced features like custom types, serialization and deserialization, and settings management.\n\nKey performance considerations, such as optimizing model initialization and efficient data parsing, were highlighted to ensure your applications run smoothly.\n\nWe also discussed common pitfalls, such as misusing type annotations, ignoring performance implications, and overcomplicating models, and provided strategies to avoid them.\n\nApplying these best practices in your real-world projects will help you leverage the full power of Pydantic, making your code more robust, maintainable, and performant."
    },
    {
        "link": "https://docs.pydantic.dev/latest/api/base_model",
        "document": "__class_vars__: The names of the class variables defined on the model. __private_attributes__: Metadata about the private attributes of the model. __signature__: The synthesized `__init__` [`Signature`][inspect.Signature] of the model. __pydantic_complete__: Whether model building is completed, or if there are still undefined fields. __pydantic_core_schema__: The core schema of the model. __pydantic_custom_init__: Whether the model has a custom `__init__` function. __pydantic_decorators__: Metadata containing the decorators defined on the model. This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1. __pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these. __pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models. __pydantic_post_init__: The name of the post-init method for the model, if defined. __pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel]. __pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model. __pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model. __pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects. __pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects. __pydantic_fields_set__: The names of fields explicitly set during instantiation. __pydantic_private__: Values of private attributes set on the model instance. # Note: Many of the below class vars are defined in the metaclass, but we define them here for type checking purposes. Configuration for the model, should be a dictionary conforming to [`ConfigDict`][pydantic.config.ConfigDict]. # Because `dict` is in the local namespace of the `BaseModel` class, we use `Dict` for annotations. # TODO v3 fallback to `dict` when the deprecated `dict` method gets removed. \"\"\"The names of the class variables defined on the model.\"\"\" \"\"\"Metadata about the private attributes of the model.\"\"\" \"\"\"The synthesized `__init__` [`Signature`][inspect.Signature] of the model.\"\"\" \"\"\"Whether model building is completed, or if there are still undefined fields.\"\"\" \"\"\"The core schema of the model.\"\"\" \"\"\"Whether the model has a custom `__init__` method.\"\"\" # Must be set for `GenerateSchema.model_schema` to work for a plain `BaseModel` annotation. \"\"\"Metadata containing the decorators defined on the model. This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\"\"\" \"\"\"Metadata for generic models; contains data used for a similar purpose to __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\"\"\" \"\"\"Parent namespace of the model, used for automatic rebuilding of models.\"\"\" \"\"\"The name of the post-init method for the model, if defined.\"\"\" \"\"\"Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\"\"\" \"\"\"The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\"\"\" \"\"\"The `pydantic-core` `SchemaValidator` used to validate instances of the model.\"\"\" \"\"\"A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects. \"\"\"A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\"\"\" \"\"\"A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra] is set to `'allow'`.\"\"\" \"\"\"The names of fields explicitly set during instantiation.\"\"\" \"\"\"Values of private attributes set on the model instance.\"\"\" # (defined in an `if not TYPE_CHECKING` block for clarity and to avoid type checking errors): 'Pydantic models should inherit from BaseModel, BaseModel cannot be instantiated directly' 'Pydantic models should inherit from BaseModel, BaseModel cannot be instantiated directly' 'Pydantic models should inherit from BaseModel, BaseModel cannot be instantiated directly' \"\"\"Create a new model by parsing and validating input data from keyword arguments. Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be `self` is explicitly positional-only to allow `self` as a field name. # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks 'A custom validator is returning a value other than `self`. \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`. 'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.' # The following line sets a flag that we use to determine when `__init__` gets overridden by the user # TODO: V3 - remove `model_fields` and `model_computed_fields` properties from the `BaseModel` class - they should only # be accessible on the model class, not on instances. We have these purely for backwards compatibility with Pydantic <v2.10. # This is similar to the fact that we have __fields__ defined here (on `BaseModel`) and on `ModelMetaClass`. # Another problem here is that there's no great way to have class properties :( \"\"\"Get metadata about the fields defined on the model. Deprecation warning: you should be getting this information from the model class, not from an instance. In V3, this property will be removed from the `BaseModel` class. # Must be set for `GenerateSchema.model_schema` to work for a plain `BaseModel` annotation, hence the default here. \"\"\"Get metadata about the computed fields defined on the model. Deprecation warning: you should be getting this information from the model class, not from an instance. In V3, this property will be removed from the `BaseModel` class. # Must be set for `GenerateSchema.model_schema` to work for a plain `BaseModel` annotation, hence the default here. A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`. \"\"\"Returns the set of fields that have been explicitly set on this model instance. A set of strings representing the fields that have been set, i.e. that were not filled from defaults. \"\"\"Creates a new instance of the `Model` class with validated data. Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data. Default values are respected, but no other validation is performed. `model_construct()` generally respects the `model_config.extra` setting on the provided model. That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__` and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored. Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in an error if extra values are passed, but they will be ignored. _fields_set: A set of field names that were originally explicitly set during instantiation. If provided, this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute. Otherwise, the field names from the `values` argument will be used. A new instance of the `Model` class with validated data. # Note: if there are any private attributes, cls.__pydantic_post_init__ would exist # Since it doesn't, that means that `__pydantic_private__` should be set to None update: Values to change/add in the new model. Note: the data is not validated before creating the new model. You should trust this data. deep: Set to `True` to make a deep copy of the model. Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. mode: The mode in which `to_python` should run. If mode is 'json', the output will only contain JSON serializable types. If mode is 'python', the output may contain non-JSON-serializable Python objects. include: A set of fields to include in the output. exclude: A set of fields to exclude from the output. context: Additional context to pass to the serializer. by_alias: Whether to use the field's alias in the dictionary key if defined. exclude_unset: Whether to exclude fields that have not been explicitly set. exclude_defaults: Whether to exclude fields that are set to their default value. exclude_none: Whether to exclude fields that have a value of `None`. round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T]. warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors, serialize_as_any: Whether to serialize fields with duck-typing serialization behavior. Generates a JSON representation of the model using Pydantic's `to_json` method. indent: Indentation to use in the JSON output. If None is passed, the output will be compact. include: Field(s) to include in the JSON output. exclude: Field(s) to exclude from the JSON output. context: Additional context to pass to the serializer. by_alias: Whether to serialize using field aliases. exclude_unset: Whether to exclude fields that have not been explicitly set. exclude_defaults: Whether to exclude fields that are set to their default value. exclude_none: Whether to exclude fields that have a value of `None`. round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T]. warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors, serialize_as_any: Whether to serialize fields with duck-typing serialization behavior. by_alias: Whether to use attribute aliases or not. schema_generator: To override the logic used to generate the JSON schema, as a subclass of mode: The mode in which to generate the schema. The JSON schema for the given model class. \"\"\"Compute the class name for parametrizations of generic classes. This method can be overridden to achieve a custom naming scheme for generic BaseModels. params: Tuple of types of the class. Given a generic class the value `(str, int)` would be passed to `params`. String representing the new class where `params` are passed to `cls` as type variables. TypeError: Raised when trying to generate concrete names for non-generic models. 'Concrete names should only be generated for generic models.' # Any strings received should represent forward references, so we handle them specially below. # If we eventually move toward wrapping them in a ForwardRef in __class_getitem__ in the future, # we may be able to remove this special case. \"\"\"Override this method to perform additional initialization after `__init__` and `model_construct`. This is useful if you want to do some validation that requires the entire model to be initialized. \"\"\"Try to rebuild the pydantic-core schema for the model. This may be necessary when one of the annotations is a ForwardRef which could not be resolved during the initial attempt to build the schema, and automatic rebuilding fails. force: Whether to force the rebuilding of the model schema, defaults to `False`. raise_errors: Whether to raise errors, defaults to `True`. _parent_namespace_depth: The depth level of the parent namespace, defaults to 2. _types_namespace: The types namespace, defaults to `None`. Returns `None` if the schema is already \"complete\" and rebuilding was not required. If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`. # delete cached value to ensure full rebuild happens # manually override defer_build so complete_model_class doesn't skip building the model again from_attributes: Whether to extract data from object attributes. context: Additional context to pass to the validator. ValidationError: If the object could not be validated. # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks Validate the given JSON data against the Pydantic model. context: Extra variables to pass to the validator. ValidationError: If `json_data` is not a JSON string or the object could not be validated. # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks \"\"\"Validate the given object with string data against the Pydantic model. obj: The object containing string data to validate. context: Extra variables to pass to the validator. # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks source: The class we are generating a schema for. This will generally be the same as the `cls` argument if this is a classmethod. # Only use the cached value from this _exact_ class; we don't want one from a parent class # This is why we check `cls.__dict__` and don't use `cls.__pydantic_core_schema__` or similar. # Due to the way generic classes are built, it's possible that an invalid schema may be temporarily # set on generic classes. I think we could resolve this to ensure that we get proper schema caching # for generics, but for simplicity for now, we just always rebuild if the class has a generic origin. You can ignore this argument and call the handler with a new CoreSchema, or just call the handler with the original schema. This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema Since this gets called by `BaseModel.model_json_schema` you can override the `schema_generator` argument to that function to change JSON schema generation globally \"\"\"This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass` only after the class is actually fully initialized. In particular, attributes like `model_fields` will be present when this is called. This is necessary because `__init_subclass__` will always be called by `type.__new__`, and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that `type.__new__` was called in such a manner that the class would already be sufficiently initialized. This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely, any kwargs passed to the class definition that aren't used internally by pydantic. **kwargs: Any keyword arguments passed to the class definition that aren't used internally 'Type parameters should be placed on typing.Generic, not BaseModel' cannot be parametrized because it does not inherit from typing.Generic' # if arguments are equal to parameters it's the same object # Attempt to rebuild the origin in case new types have been defined # depth 2 gets you above this __class_getitem__ call. # Note that we explicitly provide the parent ns, otherwise # `model_rebuild` will use the parent ns no matter if it is the ns of a module. # We don't want this here, as this has unexpected effects when a model # is being parametrized during a forward annotation evaluation. # It's okay if it fails, it just means there are still undefined types # that could be evaluated later. # This next line doesn't need a deepcopy because __pydantic_fields_set__ is a set[str], # and attempting a deepcopy would be marginally slower. # We put `__getattr__` in a non-TYPE_CHECKING block because otherwise, mypy allows arbitrary attribute access # The same goes for __setattr__ and __delattr__, see: https://github.com/pydantic/pydantic/issues/8643 # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized. # See `BaseModel.__repr_args__` for more details # this is the current error ` and cannot be set on an instance. ' 'If you want to set a value on the class, use ` # NOTE: We currently special case properties and `cached_property`, but we might need # to generalize this to all data/non-data descriptors at some point. For non-data descriptors # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value # to the instance's `__dict__`, but other non-data descriptors might do things differently. # attribute does not already exist on instance, so put it in extra # attribute _does_ already exist on instance, and was not in extra, so update it # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block: # When comparing instances of generic types for equality, as long as all field values are equal, # only require their generic origin types to be equal, rather than exact type equality. # We only want to compare pydantic fields but ignoring fields is costly. # We'll perform a fast check first, and fallback only when needed # See GH-7444 and GH-7825 for rationale and a performance benchmark # First, do the fast (and sometimes faulty) __dict__ comparison # If the check above passes, then pydantic fields are equal, we can return early # We don't want to trigger unnecessary costly filtering of __dict__ on all unequal objects, so we return # early if there are no keys to ignore (we would just return False later on anyway) # If we reach here, there are non-pydantic-fields keys, mapped to unequal values, that we need to ignore # Resort to costly filtering of the __dict__ objects # We use operator.itemgetter because it is much faster than dict comprehensions # NOTE: Contrary to standard python class and instances, when the Model class has a default value for an # attribute and the model instance doesn't have a corresponding attribute, accessing the missing attribute # raises an error in BaseModel.__getattr__ instead of returning the class attribute # So we can use operator.itemgetter() instead of operator.attrgetter() # In rare cases (such as when using the deprecated BaseModel.copy() method), # the __dict__ may not contain all model fields, which is how we can get here. # getter(self.__dict__) is much faster than any 'safe' method that accounts # for missing keys, and wrapping it in a `try` doesn't slow things down much # other instance is not a BaseModel # delegate to the other item in the comparison # We put `__init_subclass__` in a TYPE_CHECKING block because, even though we want the type-checking benefits # described in the signature of `__init_subclass__` below, we don't want to modify the default behavior of \"\"\"This signature is included purely to help type-checkers check arguments to class declaration, which provides a way to conveniently set model_config key/value pairs. However, this may be deceiving, since the _actual_ calls to `__init_subclass__` will not receive any of the config arguments, and will only receive any keyword arguments passed during class initialization that are _not_ expected keys in ConfigDict. (This is due to the way `ModelMetaclass.__new__` works.) **kwargs: Keyword arguments passed to the class definition, which set model_config You may want to override `__pydantic_init_subclass__` instead, which behaves similarly but is called *after* the class is fully initialized. # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized. # This can happen if a `ValidationError` is raised during initialization and the instance's # repr is generated as part of the exception handling. Therefore, we use `getattr` here # with a fallback, even though the type hints indicate the attribute will always be present. # take logic from `_repr.Representation` without the side effects of inheritance, see #5740 'The `__fields__` attribute is deprecated, use `model_fields` instead.' 'The `__fields__` attribute is deprecated, use `model_fields` instead.' 'The `__fields_set__` attribute is deprecated, use `model_fields_set` instead.' 'The `__fields_set__` attribute is deprecated, use `model_fields_set` instead.' 'The `dict` method is deprecated; use `model_dump` instead.' 'The `dict` method is deprecated; use `model_dump` instead.' 'The `json` method is deprecated; use `model_dump_json` instead.' 'The `json` method is deprecated; use `model_dump_json` instead.' 'The `encoder` argument is no longer supported; use field serializers instead.' 'The `models_as_dict` argument is no longer supported; use a model serializer instead.' 'The `parse_obj` method is deprecated; use `model_validate` instead.' 'The `parse_obj` method is deprecated; use `model_validate` instead.' 'The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, ' 'otherwise load the data then use `model_validate` instead.' 'The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, ' 'otherwise load the data then use `model_validate` instead.' # ctx is missing here, but since we've added `input` to the error, we're not pretending it's the same # The type: ignore on the next line is to ignore the requirement of LiteralString 'The `parse_file` method is deprecated; load the data from file, then if your data is JSON ' 'use `model_validate_json`, otherwise `model_validate` instead.' 'The `parse_file` method is deprecated; load the data from file, then if your data is JSON ' 'use `model_validate_json`, otherwise `model_validate` instead.' \"`model_config['from_attributes']=True` and use `model_validate` instead.\" \"`model_config['from_attributes']=True` and use `model_validate` instead.\" 'You must set the config attribute `from_attributes=True` to use from_orm' 'The `construct` method is deprecated; use `model_construct` instead.' 'The `construct` method is deprecated; use `model_construct` instead.' 'The `copy` method is deprecated; use `model_copy` instead. ' 'See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`.' This method is now deprecated; use `model_copy` instead. If you need `include` or `exclude`, use: include: Optional set or mapping specifying which fields to include in the copied model. exclude: Optional set or mapping specifying which fields to exclude in the copied model. update: Optional dictionary of field-value pairs to override field values in the copied model. deep: If True, the values of fields that are Pydantic models will be deep-copied. A copy of the model with included, excluded and updated fields as specified. 'The `copy` method is deprecated; use `model_copy` instead. ' 'See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`.' # k must have come from extra # new `__pydantic_fields_set__` can have unset optional fields with a set value in `update` kwarg 'The `schema` method is deprecated; use `model_json_schema` instead.' 'The `schema` method is deprecated; use `model_json_schema` instead.' 'The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead.' 'The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead.' 'The `validate` method is deprecated; use `model_validate` instead.' 'The `validate` method is deprecated; use `model_validate` instead.' 'The `update_forward_refs` method is deprecated; use `model_rebuild` instead.' 'The `update_forward_refs` method is deprecated; use `model_rebuild` instead.' 'The private method `_iter` will be removed and should no longer be used.' 'The private method `_iter` will be removed and should no longer be used.' 'The private method `_copy_and_set_values` will be removed and should no longer be used.' 'The private method `_copy_and_set_values` will be removed and should no longer be used.' 'The private method `_get_value` will be removed and should no longer be used.' 'The private method `_get_value` will be removed and should no longer be used.' 'The private method `_calculate_keys` will be removed and should no longer be used.' 'The private method `_calculate_keys` will be removed and should no longer be used.'"
    },
    {
        "link": "https://github.com/zhanymkanov/fastapi-best-practices",
        "document": "Opinionated list of best practices and conventions I use in startups.\n\nFor the last several years in production, we have been making good and bad decisions that impacted our developer experience dramatically. Some of them are worth sharing.\n• Miscellaneous\n• If you must use sync SDK, then run it in a thread pool.\n\nThere are many ways to structure a project, but the best structure is one that is consistent, straightforward, and free of surprises.\n\nMany example projects and tutorials divide the project by file type (e.g., crud, routers, models), which works well for microservices or projects with fewer scopes. However, this approach didn't fit our monolith with many domains and modules.\n\nThe structure I found more scalable and evolvable for these cases is inspired by Netflix's Dispatch, with some minor modifications.\n• Store all domain directories inside folder\n• - highest level of an app, contains common models, configs, and constants, etc.\n• - root of the project, which inits the FastAPI app\n• Each package has its own router, schemas, models, etc.\n• - is a core of each module with all the endpoints\n• When package requires services or dependencies or constants from other packages - import them with an explicit module name\n\nFastAPI is an async framework, in the first place. It is designed to work with async I/O operations and that is the reason it is so fast.\n\nHowever, FastAPI doesn't restrict you to use only routes, and the developer can use routes as well. This might confuse beginner developers into believing that they are the same, but they are not.\n\nUnder the hood, FastAPI can effectively handle both async and sync I/O operations.\n• FastAPI runs routes in the threadpool and blocking I/O operations won't stop the event loop from executing the tasks.\n• If the route is defined then it's called regularly via and FastAPI trusts you to do only non-blocking I/O operations.\n\nThe caveat is if you fail that trust and execute blocking operations within async routes, the event loop will not be able to run the next tasks until that blocking operation is done.\n\nWhat happens when we call:\n• \n• Server's event loop and all the tasks in the queue will be waiting until is finished\n• Server thinks is not an I/O task, so it waits until it is finished\n• Server won't accept any new requests while waiting\n• \n• FastAPI sends the whole route to the threadpool, where a worker thread will run the function\n• While is being executed, event loop selects next tasks from the queue and works on them (e.g. accept new request, call db)\n• Independently of main thread (i.e. our FastAPI app), worker thread will be waiting for to finish.\n• Sync operation blocks only the side thread, not the main one.\n• When finishes its work, server returns a response to the client\n• \n• Event loop selects next tasks from the queue and works on them (e.g. accept new request, call db)\n• When is done, servers finishes the execution of the route and returns a response to the client\n\nThe second caveat is that operations that are non-blocking awaitables or are sent to the thread pool must be I/O intensive tasks (e.g. open file, db call, external API call).\n• Awaiting CPU-intensive tasks (e.g. heavy calculations, data processing, video transcoding) is worthless since the CPU has to work to finish the tasks, while I/O operations are external and server does nothing while waiting for that operations to finish, thus it can go to the next tasks.\n• Running CPU-intensive tasks in other threads also isn't effective, because of GIL. In short, GIL allows only one thread to work at a time, which makes it useless for CPU tasks.\n• If you want to optimize CPU intensive tasks you should send them to workers in another process.\n• https://stackoverflow.com/questions/62976648/architecture-flask-vs-fastapi/70309597#70309597\n• Here you can also check my answer\n\nPydantic has a rich set of features to validate and transform data.\n\nIn addition to regular features like required & non-required fields with default values, Pydantic has built-in comprehensive data processing tools like regex, enums, strings manipulation, emails validation, etc.\n\nHaving a controllable global base model allows us to customize all the models within the app. For instance, we can enforce a standard datetime format or introduce a common method for all subclasses of the base model.\n\nIn the example above, we have decided to create a global base model that:\n• Serializes all datetime fields to a standard format with an explicit timezone\n• Provides a method to return a dict with only serializable fields\n\nBaseSettings was a great innovation for reading environment variables, but having a single BaseSettings for the whole app can become messy over time. To improve maintainability and organization, we have split the BaseSettings across different modules and domains.\n\nPydantic is a great schema validator, but for complex validations that involve calling a database or external services, it is not sufficient.\n\nFastAPI documentation mostly presents dependencies as DI for endpoints, but they are also excellent for request validation.\n\nDependencies can be used to validate data against database constraints (e.g., checking if an email already exists, ensuring a user is found, etc.).\n\nIf we didn't put data validation to dependency, we would have to validate exists for every endpoint and write the same tests for each of them.\n\nDependencies can use other dependencies and avoid code repetition for the similar logic.\n\nDependencies can be reused multiple times, and they won't be recalculated - FastAPI caches dependency's result within a request's scope by default, i.e. if gets called multiple times in one route, it will be called only once.\n\nKnowing this, we can decouple dependencies onto multiple smaller functions that operate on a smaller domain and are easier to reuse in other routes. For example, in the code below we are using three times:\n\nbut is called only once, in the very first call.\n\nFastAPI supports both and dependencies, and there is a temptation to use dependencies, when you don't have to await anything, but that might not be the best choice.\n\nJust as with routes, dependencies are run in the thread pool. And threads here also come with a price and limitations, that are redundant, if you just make a small non-I/O operation.\n\nDeveloping RESTful API makes it easier to reuse dependencies in routes like these:\n\nThe only caveat is to use the same variable names in the path:\n• If you have two endpoints and that both validate whether the given exists, but also checks if the profile is creator, then it's better to rename path variable to and chain those two dependencies.\n\nIf you think you can return Pydantic object that matches your route's to make some optimizations, then it's wrong.\n\nFastAPI firstly converts that pydantic object to dict with its , then validates data with your , and only then serializes your object to JSON.\n\nIf you must use a library to interact with external services, and it's not , then make the HTTP calls in an external worker thread.\n\nWe can use the well-known from starlette.\n\nIf you raise a in a Pydantic schema that is directly faced by the client, it will return a nice detailed response to users.\n• Unless your API is public, hide docs by default. Show it explicitly on the selected envs only.\n• Help FastAPI to generate an easy-to-understand docs\n• If models and statuses vary, use route attribute to add docs for different responses\n\nWill generate docs like this:\n\nExplicitly setting the indexes' namings according to your database's convention is preferable over sqlalchemy's.\n• Migrations must be static and revertable. If your migrations depend on dynamically generated data, then make sure the only thing that is dynamic is the data itself, not its structure.\n• Generate migrations with descriptive names & slugs. Slug is required and should explain the changes.\n• Set human-readable file template for new migrations. We use pattern, e.g.\n\nBeing consistent with names is important. Some rules we followed:\n• stay consistent across tables, but concrete namings are ok, e.g.\n• use in all tables, but if some of them need only profiles that are creators, use\n• use for all abstract tables like , , but use concrete naming in relevant modules like in\n• Usually, database handles data processing much faster and cleaner than CPython will ever do.\n• It's preferable to do all the complex joins and simple data manipulations with SQL.\n• It's preferable to aggregate JSONs in DB for responses with nested objects.\n\nWriting integration tests with DB will most likely lead to messed up event loop errors in the future. Set the async test client immediately, e.g. httpx\n\nUnless you have sync db connections (excuse me?) or aren't planning to write integration tests.\n\nWith linters, you can forget about formatting the code and focus on writing the business logic.\n\nRuff is \"blazingly-fast\" new linter that replaces black, autoflake, isort, and supports more than 600 lint rules.\n\nIt's a popular good practice to use pre-commit hooks, but just using the script was ok for us.\n\nSome very kind people shared their own experience and best practices that are definitely worth reading. Check them out at issues section of the project.\n\nFor instance, lowercase00 has described in details their best practices working with permissions & auth, class-based services & views, task queues, custom response serializers, configuration with dynaconf, etc.\n\nIf you have something to share about your experience working with FastAPI, whether it's good or bad, you are very welcome to create a new issue. It is our pleasure to read it."
    },
    {
        "link": "https://reddit.com/r/Python/comments/16xnhim/what_problems_does_pydantic_solves_and_how_should",
        "document": "Hi Folks,I have been pondering on this question for sometime. But have not been able to get this. Searched internet but didn't find any article or video of help. Most of them talk about syntax and semantics of pydantic and none talked about what I wanted to know.\n\nAsking this question, Because, in the first look pydantic looks helpful. as it helps us know what exact data is flowing through the application, helps us validate data. But it also takes the simplicity and flexibility that python provides us such as suppose, you may assign some intermediatry key in the dictionary pass that dictionary to some function pop that value out and use it.\n\nusing libraries like pydantic will make doing these things to put a little more effort and also the overhead of maintaining pydantic models comes. and in some scenario, some bug may be come up due to incorrect use of these kinds of libraries.\n\nWell, the purpose of my question is not to question the usuability of pydantic. I know it is definitely useful as it is being used widely. but to understand how to use it perfectly and to understand where not to use it."
    },
    {
        "link": "https://github.com/a-luna/fastapi-redis-cache",
        "document": "\n• Lifetime of cached data is configured separately for each API endpoint.\n• Requests with header containing or are handled correctly (all caching behavior is disabled).\n• Requests with header will receive a response with status if for requested resource matches header value.\n\nCreate a instance when your application starts by defining an event handler for the event as shown below:\n\nAfter creating the instance, you must call the method. The only required argument for this method is the URL for the Redis database ( ). All other arguments are optional:\n• ( ) — Cache keys are created (in part) by combining the name and value of each argument used to invoke a path operation function. If any of the arguments have no effect on the response (such as a or object), including their type in this list will ignore those arguments when the key is created. (Optional, defaults to )\n• The example shown here includes the type, if your project uses SQLAlchemy as a dependency (as demonstrated in the FastAPI docs), you should include in in order for cache keys to be created correctly (More info).\n\nDecorating a path function with enables caching for the endpoint. Response data is only cached for operations, decorating path functions for other HTTP method types will have no effect. If no arguments are provided, responses will be set to expire after one year, which, historically, is the correct way to mark data that \"never expires\".\n\nResponse data for the API endpoint at will be cached by the Redis server. Log messages are written to standard output whenever a response is added to or retrieved from the cache:\n\nThe log messages show two successful ( ) responses to the same request ( ). The first request executed the function and stored the result in Redis under key . The second request did not execute the function, instead the cached result was retrieved and sent as the response.\n\nIn most situations, response data must expire in a much shorter period of time than one year. Using the parameter, You can specify the number of seconds before data is deleted:\n\nA response from the endpoint showing all header values is given below:\n• The header field indicates that this response was found in the Redis cache (a.k.a. a ). The only other possible value for this field is .\n• The field and value in the field indicate that this response will be considered fresh for 29 seconds. This is expected since was specified in the decorator.\n• The field is an identifier that is created by converting the response data to a string and applying a hash function. If a request containing the header is received, any value(s) included in the request will be used to determine if the data requested is the same as the data stored in the cache. If they are the same, a response will be sent. If they are not the same, the cached data will be sent with a response.\n\nThese header fields are used by your web browser's cache to avoid sending unnecessary requests. After receiving the response shown above, if a user requested the same resource before the time, the browser wouldn't send a request to the FastAPI server. Instead, the cached response would be served directly from disk.\n\nOf course, this assumes that the browser is configured to perform caching. If the browser sends a request with the header containing or , the , , , and response header fields will not be included and the response data will not be stored in Redis.\n\nThe decorators listed below define several common durations and can be used in place of the decorator:\n\nFor example, instead of , you could use:\n\nIf a duration that you would like to use throughout your project is missing from the list, you can easily create your own:\n\nThen, simply import and use it to decorate your API endpoint path functions:\n\nConsider the API route defined below. This is the first path function we have seen where the response depends on the value of an argument ( ). This is a typical CRUD operation where is used to retrieve a record from a database. The API route also includes a dependency that injects a object ( ) into the function, per the instructions from the FastAPI docs:\n\nIn the Initialize Redis section of this document, the method was called with . Why is it necessary to include in this list?\n\nBefore we can answer that question, we must understand how a cache key is created. If the following request was received: , the cache key generated would be .\n\nThe source of each value used to construct this cache key is given below:\n• The optional value provided as an argument to the method => .\n• The module containing the path function => .\n• The name of the path function => .\n• The name and value of all arguments to the path function EXCEPT for arguments with a type that exists in => .\n\nSince is included in , the argument was not included in the cache key when Step 4 was performed.\n\nIf had not been included in , caching would be completely broken. To understand why this is the case, see if you can figure out what is happening in the log messages below:\n\nThe log messages indicate that three requests were received for the same endpoint, with the same arguments ( ). However, the cache key that is created is different for each request:\n\nThe value of each argument is added to the cache key by calling . The object includes the memory location when converted to a string, causing the same response data to be cached under three different keys! This is obviously not what we want.\n\nThe correct behavior (with included in ) is shown below:\n\nNow, every request for the same generates the same key value ( ). As expected, the first request adds the key/value pair to the cache, and each subsequent request retrieves the value from the cache based on the key.\n\nWhat about this situation? You create a custom dependency for your API that performs input validation, but you can't ignore it because it does have an effect on the response data. There's a simple solution for that, too.\n\nHere is an endpoint from one of my projects:\n\nThe argument is a type. This is a custom type that parses the value from the querystring to a date, and determines if the parsed date is valid by checking if it is within a certain range. The implementation for is given below:\n\nPlease note the method that overrides the default behavior. This way, instead of , the value will be formatted as, for example, . You can use this strategy whenever you have an argument that has en effect on the response data but converting that argument to a string results in a value containing the object's memory location.\n\nIf you have any questions, please open an issue. Any suggestions and contributions are absolutely welcome. This is still a very small and young project, I plan on adding a feature roadmap and further documentation in the near future."
    },
    {
        "link": "https://dev.to/sivakumarmanoharan/caching-in-fastapi-unlocking-high-performance-development-20ej",
        "document": "In today’s digital world, every action—whether it’s swiping on a dating app or completing a purchase—relies on APIs working efficiently behind the scenes. As back-end developers, we know that every millisecond counts. But how can we make APIs respond faster? The answer lies in caching.\n\nCaching is a technique that stores frequently accessed data in memory, allowing APIs to respond instantly instead of querying a slower database every time. Think of it like keeping key ingredients (salt, pepper, oil) on your kitchen countertop instead of fetching them from the pantry each time you cook—this saves time and makes the process more efficient. Similarly, caching reduces API response times by storing commonly requested data in a fast, accessible spot, like Redis.\n\nTo connect with Redis Cache with FastAPI, we require the following libraries to be pre-installed.\n\n\n\nPydantic is for creating database tables and structures. aiocache will perform asynchronous operations on Cache. uvicorn is responsible for the server running.\n\nSetting up Redis directly in a Windows system is not possible at this juncture. Therefore, it has to be setup and run in Windows Subsystem for Linux. Instructions for installing WSL is given below\n\nPost installing WSL, the following commands are required to install Redis\n\n\n\nTo test Redis server connectivity, the following command is used\n\n\n\nAfter this command, it will enter into a virtual terminal of port 6379. In that terminal, the redis commands can be typed and tested.\n\nLet’s create a simple FastAPI app that retrieves user information and caches it for future requests. We will use Redis for storing cached responses.\n\nWe’ll use Pydantic to define our User model, which represents the structure of the API response.\n\n\n\nTo avoid repeating the caching logic for each endpoint, we’ll create a reusable caching decorator using the aiocache library. This decorator will attempt to retrieve the response from Redis before calling the actual function.\n\n\n\nWe’ll now implement a FastAPI route that retrieves user information based on a user ID. The response will be cached using Redis for faster access in subsequent requests.\n\n\n\nNow, you can test the API by fetching user details via:\n\n\n\nThe first request will fetch the data from the users_db, but subsequent requests will retrieve the data from Redis.\n\nYou can verify the cache by inspecting the keys stored in Redis. Open the Redis CLI:\n\n\n\nYou will get all keys that have been stored in the Redis till TTL.\n\nHow Caching Works in This Example\n\n: When the user data is requested for the first time, the API fetches it from the database (users_db) and stores the result in Redis with a time-to-live (TTL) of 120 seconds.\n\nAny subsequent requests for the same user within the TTL period are served directly from Redis, making the response faster and reducing the load on the database.\n\nAfter 120 seconds, the cache entry expires, and the data is fetched from the database again on the next request, refreshing the cache.\n\nIn this tutorial, we’ve demonstrated how to implement Redis caching in a FastAPI application using a simple user details example. By caching API responses, you can significantly improve the performance of your application, particularly for data that doesn't change frequently.\n\nPlease do upvote and share if you find this article useful."
    },
    {
        "link": "https://marbleit.rs/blog/tour-of-fastapi-and-caching",
        "document": "In this guide, we will go through the basics of FastAPI framework and caching by creating a simple fully functioning service from scratch. \n\n\n\nThis guide will reference code from one of the multiple services from the Eventer application which is my personal project meant for learning.\n\n\n\nGeneral Python programming language knowledge is “required” as it won’t be covered in this guide.\n\n\n\nFastAPI is a framework for building APIs using Python. I find it far simpler than Django and easier to work with. You will find various similarities, If you have experience with Express framework for Node.js\n\n\n\nFastAPI states these points in their documentation:\n• Fast: Very high performance, on par with NodeJS and Go (thanks to Starlette and Pydantic).\n• Fast to code: Increase the speed to develop features by about 200% to 300%.\n• Intuitive: Great editor support available Everywhere. Less time debugging.\n• Easy: Designed to be easy to use and learn. Less time reading docs.\n• Standards-based : Based on (and fully compatible with) the open standards for APIs: OpenAPI (previously known as Swagger) and JSON Schema\n\nThe goal is to have a service that retrieves weather data from an external API. Weather data is collected just for cities in Serbia. The map was roughly split up into regions so that if two cities are in the same region we would assume they will have the same forecast. This approach is taken because I want to use the free tier of the external API and precision is not the biggest issue since this is a hobby project.\n\n\n\nResponses will be cached in Redis so that we don’t need to send a request to the external API when the user requests a city in the same region multiple times.\n\n\n\nThe finished code can be found on my repository here.\n\n\n\nIf you want to see more of FastAPI features or go into more depth you can check their official documentation here.\n\n\n\nFirst we’ll set up the virtual environment for the project and install required dependencies. These were used in the project:\n\n\n\nOk, here we instantiate the FastAPI class inside the app variable. That will be our application core. Everything we add, like routing for our app, exception handlers, middleware, or other things will go there.\n\n\n\nIf you are curious to learn more about the parameters you can go to the documentation linked above.\n\n\n\nRight now we have a running application that listens on port 9000. Good, now I would like to add the routing. Let’s create router.py and add this code:\n\n\n\n\n\n\n\nWe instantiated the router and added one endpoint. We are using async functions because FastAPI is designed to work efficiently with async operations.\n\n\n\n@router.{METHOD}({ENDPOINT_PATH}) is a decorator that will tell FastAPI that the next function should respond to the given method (in our case get) on the given path (in our case /api/v1/first). This will also allow our endpoints to appear in the auto-generated documentation.\n\n\n\nIf you want more details on decorators in Python look at this.\n\n\n\nNow how to connect the router to the application?\n\n\n\nTo the app object, we instantiated in main.py. When we add this code:\n\nWe can try and call the endpoint from Swagger. \n\n\n\nOpening Swagger UI\n\n\n\nStart the app and go to url:port/docs\n\n\n\nYou should get this if you “Try it out”\n\n\n\nOk, we’re done with some basic stuff. We should set up the model for our entity, custom exceptions, and neat stuff like that. \n\n\n\nSo let’s go through the simple stuff quickly.\n\n\n\nStatics\n\n\n\n\n\nHere we define some statics that will be used inside the application.\n\n\n\n\n\n\n\nThis will be expanded on when we continue working on the app so update this file while you work.\n\n\n\n\n\nModels\n\n\n\nHere we define the models used in our application. Custom exception and the model that represents the WeatherCondition, our main entity\n\n\n\n\n\n\n\nSince we will “use” this app in multiple environments it will be good to add a .env file and a configuration class.\n\n\n\nI decided to use the decouple library to read the .env file. It has an easy way to set default values and cast to a specific type.\n\n\n\n\n\n\n\nLogging\n\n\n\nHere we instantiate the logger, we will log both to the console and to a file. We can use built-in Python logging to achieve this\n\n\n\nThis is where our main logic will be written. There is a lot to be done here so let’s take it step by step. First, we need a function that will get forecast data, that will be the main one we use in this application.\n\n\n\n\n\n\n\nWe will need coordinates of the city from somewhere and we will check if they are found. After that, we will need to get a region where those coordinates belong. The last thing would be to get the weather data from the API for that region and return it.\n\n\n\nIt would look something like this:\n\n\n\nFor coordinate data, I used this file. It should work for our demo case. I placed it in a folder named data.\n\n\n\nrs.json\n\nCities with coordinates\n\n\n\nSo for our first step, we can read the cities from the file.\n\n\n\nBecause the region boundaries are defined by us I used this function to calculate them.\n\n\n\nHere we need to send the request and parse the data. When developing we don’t need to send a request every time instead we can take the example response from a file. We use information about using real data from the config file.\n\n\n\nFor parsing data, I just take what I need from the result and create WeatherCondition objects.\n\n\n\n\n\n\n\nNow we have everything working and just need to expose it in the router.\n\n\n\nAfter this, we can test the code with swagger and we should have a working application. But what we have now can be improved. There are some things that could be done better, we set up a logger and haven’t yet used it and we can add caching of results.\n\n\n\n\n\nIn this part, we will clean up our application and add caching. For that, we will use Redis inside Docker. This will be covered in short in this guide for readers who don’t want to dive deep just now but want to follow along. Be sure to investigate further if you’re not familiar with these two tools.\n\n\n\nCaching is a technique where we store frequently accessed data in a special, fast-access storage. This makes things faster and easier, especially when you need to get a lot of information quickly.\n\n\n\nAlso if the data is retrieved from an external API we can cache the results and not send a request every time. This will speed up data retrieval and reduce API usage costs.\n\n\n\nRedis is a database that stores data in memory, which means it uses the computer's RAM. This makes it super quick to access data, like getting something from a pocket instead of a backpack. It's mostly used for caching, which means it keeps copies of frequently used information ready to go.\n\n\n\nSince I’m working with a Windows machine and Redis is not supported on Windows I decided to use it in a docker container. The whole app will be dockerized anyway at the end so this was not a hard decision.\n\n\n\nDocker is like a set of containers for your computer programs. It has everything a program needs to run, like its own little computer inside your computer. That means when we set it up on one machine it will work on any machine with Docker.\n\n\n\nYou will need to download and install Docker Desktop on your machine.\n\n\n\nAfter that create folders for the volume so that the path is correct for you. That would be root/tools/docker/redis_data on your machine. :/data will be the volume inside the container.\n\n\n\nAdjust that freely but inside the volume, you will need a redis.conf file. That will be our Redis configuration. This is what I used.\n\n\n\nIn the project, I used a REJSON module for Redis so you will need to download that. We use it to allow Redis to efficiently work with JSON data as if it were any other Redis data type.\n\n\n\nThat should be everything for the setup. When you run the following command in the directory where your docker compose file is located it should start your containers.\n\n\n\nIf everything is correct this should be your Docker Desktop view\n\n\n\nNow we should have the prerequisites set up and can go further.\n\n\n\nWe want to use the asynchronous Redis library in Python so we need to import it correctly and create a function that will create our connections.\n\n\n\nWe'll need functions to cache weather data, retrieve it, and manage coordinates.\n\n\n\nFor caching and getting weather data we are saving a JSON object. That’s why we are using the JSON module for Redis. For coordinates, since it will be a much more simple object we can use classic hset.\n\n\n\nThe get_cache function only returns the connection to our database. The URL is the connection string, encoding is set because we will save coordinates with city names in utf-8 and decode_responses will decode the responses when reading data.\n\n\n\nIn the save_weather function, we use .json() so that Redis can utilize the REJSON module mentioned above.\n\n\n\nf\"{CacheIdentifiers.FORECAST}:{weather.region}@{weather.date}\" is the Redis key under which the JSON document is stored.\n\n\n\n$ refers to the root of the JSON document. In RedisJSON, if you want to replace the entire JSON object stored at a key, you use the root path. This is effectively like saying, \"Replace the entire JSON object at this key with this new data.\"\n\n\n\nweather.serialize() is a method that we need to add to our model so we can serialize it to a JSON-compatible format…a dictionary.\n\n\n\ncache.expire sets how long this key should exist in Redis before expiring (in seconds).\n\n\n\nHere we just get the correct key if it exists, nothing too hard here.\n\n\n\nThese two are simple, just saving and reading the coordinates. They will be saved as city:(lat, lng).\n\n\n\nDon’t forget we need to add a serialize method to our WeatherCondition class.\n\n\n\nWe need to implement the magic method __iter__ in order to convert our object to a dictionary. Serialize method will just replace - with x from the date because using a dash gave me errors in Redis.\n\n\n\nWe have our caching functions ready, now just to edit the service to use them. First, let’s add reading coordinates from the cache. For coordinates, Redis will work as a database more than a cache since they have no time to live.\n\n\n\nThe updated function will look like this:\n\n\n\nDon’t read them from a file, read them from the cache. Nice, now get_conditions should check if there is a forecast for today in the cache. If yes return it right away and if no get the data from the API.\n\n\n\nThe updated function will look like this:\n\n\n\n\n\n\n\nSince the API returns data for more than 1 day I decided to cache all of it in order to reduce the request amount since this is not a real project. Cache every weather data received and return for today.\n\n\n\nThe last thing is to add a way to seed the coordinates data in the database.\n\n\n\nIn the service we can add this function:\n\n\n\nIt will read them from a file, just this one time, then cache all.\n\n\n\nAlso in main, we can add a way to start seeding by arguments provided at the start.\n\n\n\nThe main file should look like this:\n\n\n\nNow if you start with the --seed option it will run seeding.\n\n\n\nNow we have caching done as well as logging, custom exceptions, and reading from a configuration file. To complete the app in full and make it shine we should add response validations, exception handlers, and another cool thing that we will see at the end 😄\n\n\n\n\n\nPydantic models\n\n\n\nPydantic is a Python library used for data parsing and validation. This is a very useful library that can be utilized with FastAPI to validate requests and responses. In models, we can add a WeatherResponse model and define how our response should look like.\n\n\n\nAnd in our route, we configure the endpoint to use it as a response model.\n\n\n\nI like to define how my exceptions are handled so the format is consistent regardless of the type.\n\n\n\nWe can create another file exception_handlers.py and inside add this:\n\n\n\nThe ErrorResponse is a new Pydantic model we can add to models.\n\n\n\nAnd add them to our app in main.\n\n\n\nThis is how errors look without these changes.\n\n\n\nAnd this is how errors look with these changes.\n\n\n\n\n\n\n\nThe requests library is synchronous and we made our whole app asynchronous so this is something we should fix by using the aiohttp library instead.\n\n\n\nInstead of calling the API with requests refactor the function to look like this:\n\n\n\nWe can ignore reading the example response since it’s just for development, but now if we use real data, sending requests to the API will not block our app 😄\n\n\n\nTo summarize, we have created a fully functioning service with route definitions, API calls, a caching mechanism, and set-up exception handlers.\n\n\n\nThe brief tour of this framework is over. Thank you for reading. I hope you had fun and learned something new. The app was simple but some cool concepts were covered."
    },
    {
        "link": "https://stackoverflow.com/questions/76115645/cache-decorator-for-fastapi",
        "document": "The above is simplified but is accurate to what I generally want to do with various functions in my FastAPI project. I've created the following Python decorator, I believe this is what it should be but I'm not sure.\n\nI'm using a Redis instance to cache the URLs from my FastAPI routes. Though, I can't figure out how to get the URL from the decorator and use the parameter from my function. So for example, how can I get the part and the value of and use it my decorator. There might be a way through storing the URL in a variable and maybe passing it to each decorator, but I'm looking for the intended way to do things. In other words, how can I do this most concisely without changing all my functions too much?"
    },
    {
        "link": "https://github.com/a-luna/fastapi-redis-cache/blob/main/README.md",
        "document": "\n• Lifetime of cached data is configured separately for each API endpoint.\n• Requests with header containing or are handled correctly (all caching behavior is disabled).\n• Requests with header will receive a response with status if for requested resource matches header value.\n\nCreate a instance when your application starts by defining an event handler for the event as shown below:\n\nAfter creating the instance, you must call the method. The only required argument for this method is the URL for the Redis database ( ). All other arguments are optional:\n• ( ) — Cache keys are created (in part) by combining the name and value of each argument used to invoke a path operation function. If any of the arguments have no effect on the response (such as a or object), including their type in this list will ignore those arguments when the key is created. (Optional, defaults to )\n• The example shown here includes the type, if your project uses SQLAlchemy as a dependency (as demonstrated in the FastAPI docs), you should include in in order for cache keys to be created correctly (More info).\n\nDecorating a path function with enables caching for the endpoint. Response data is only cached for operations, decorating path functions for other HTTP method types will have no effect. If no arguments are provided, responses will be set to expire after one year, which, historically, is the correct way to mark data that \"never expires\".\n\nResponse data for the API endpoint at will be cached by the Redis server. Log messages are written to standard output whenever a response is added to or retrieved from the cache:\n\nThe log messages show two successful ( ) responses to the same request ( ). The first request executed the function and stored the result in Redis under key . The second request did not execute the function, instead the cached result was retrieved and sent as the response.\n\nIn most situations, response data must expire in a much shorter period of time than one year. Using the parameter, You can specify the number of seconds before data is deleted:\n\nA response from the endpoint showing all header values is given below:\n• The header field indicates that this response was found in the Redis cache (a.k.a. a ). The only other possible value for this field is .\n• The field and value in the field indicate that this response will be considered fresh for 29 seconds. This is expected since was specified in the decorator.\n• The field is an identifier that is created by converting the response data to a string and applying a hash function. If a request containing the header is received, any value(s) included in the request will be used to determine if the data requested is the same as the data stored in the cache. If they are the same, a response will be sent. If they are not the same, the cached data will be sent with a response.\n\nThese header fields are used by your web browser's cache to avoid sending unnecessary requests. After receiving the response shown above, if a user requested the same resource before the time, the browser wouldn't send a request to the FastAPI server. Instead, the cached response would be served directly from disk.\n\nOf course, this assumes that the browser is configured to perform caching. If the browser sends a request with the header containing or , the , , , and response header fields will not be included and the response data will not be stored in Redis.\n\nThe decorators listed below define several common durations and can be used in place of the decorator:\n\nFor example, instead of , you could use:\n\nIf a duration that you would like to use throughout your project is missing from the list, you can easily create your own:\n\nThen, simply import and use it to decorate your API endpoint path functions:\n\nConsider the API route defined below. This is the first path function we have seen where the response depends on the value of an argument ( ). This is a typical CRUD operation where is used to retrieve a record from a database. The API route also includes a dependency that injects a object ( ) into the function, per the instructions from the FastAPI docs:\n\nIn the Initialize Redis section of this document, the method was called with . Why is it necessary to include in this list?\n\nBefore we can answer that question, we must understand how a cache key is created. If the following request was received: , the cache key generated would be .\n\nThe source of each value used to construct this cache key is given below:\n• The optional value provided as an argument to the method => .\n• The module containing the path function => .\n• The name of the path function => .\n• The name and value of all arguments to the path function EXCEPT for arguments with a type that exists in => .\n\nSince is included in , the argument was not included in the cache key when Step 4 was performed.\n\nIf had not been included in , caching would be completely broken. To understand why this is the case, see if you can figure out what is happening in the log messages below:\n\nThe log messages indicate that three requests were received for the same endpoint, with the same arguments ( ). However, the cache key that is created is different for each request:\n\nThe value of each argument is added to the cache key by calling . The object includes the memory location when converted to a string, causing the same response data to be cached under three different keys! This is obviously not what we want.\n\nThe correct behavior (with included in ) is shown below:\n\nNow, every request for the same generates the same key value ( ). As expected, the first request adds the key/value pair to the cache, and each subsequent request retrieves the value from the cache based on the key.\n\nWhat about this situation? You create a custom dependency for your API that performs input validation, but you can't ignore it because it does have an effect on the response data. There's a simple solution for that, too.\n\nHere is an endpoint from one of my projects:\n\nThe argument is a type. This is a custom type that parses the value from the querystring to a date, and determines if the parsed date is valid by checking if it is within a certain range. The implementation for is given below:\n\nPlease note the method that overrides the default behavior. This way, instead of , the value will be formatted as, for example, . You can use this strategy whenever you have an argument that has en effect on the response data but converting that argument to a string results in a value containing the object's memory location.\n\nIf you have any questions, please open an issue. Any suggestions and contributions are absolutely welcome. This is still a very small and young project, I plan on adding a feature roadmap and further documentation in the near future."
    }
]