[
    {
        "link": "https://eigen.tuxfamily.org/dox/group__TutorialSparse.html",
        "document": "In many applications (e.g., finite element methods) it is common to deal with very large matrices where only a few coefficients are different from zero. In such cases, memory consumption can be reduced and performance increased by using a specialized representation storing only the nonzero coefficients. Such a matrix is called a sparse matrix.\n\nThe class SparseMatrix is the main sparse matrix representation of Eigen's sparse module; it offers high performance and low memory usage. It implements a more versatile variant of the widely-used Compressed Column (or Row) Storage scheme. It consists of four compact arrays:\n• stores the coefficient values of the non-zeros.\n• stores the row (resp. column) indices of the non-zeros.\n• stores for each column (resp. row) the index of the first non-zero in the previous two arrays.\n• stores the number of non-zeros of each column (resp. row). The word refers to an inner vector that is a column for a column-major matrix, or a row for a row-major matrix. The word refers to the other direction.\n\nThis storage scheme is better explained on an example. The following matrix\n\nCurrently the elements of a given inner vector are guaranteed to be always sorted by increasing inner indices. The indicates available free space to quickly insert new elements. Assuming no reallocation is needed, the insertion of a random element is therefore in where is the number of nonzeros of the respective inner vector. On the other hand, inserting elements with increasing inner indices in a given inner vector is much more efficient since this only requires to increase the respective entry that is a operation.\n\nThe case where no empty space is available is a special case, and is referred as the compressed mode. It corresponds to the widely used Compressed Column (or Row) Storage schemes (CCS or CRS). Any SparseMatrix can be turned to this form by calling the SparseMatrix::makeCompressed() function. In this case, one can remark that the array is redundant with because we have the equality: . Therefore, in practice a call to SparseMatrix::makeCompressed() frees this buffer.\n\nIt is worth noting that most of our wrappers to external libraries requires compressed matrices as inputs.\n\nThe results of Eigen's operations always produces compressed sparse matrices. On the other hand, the insertion of a new element into a SparseMatrix converts this later to the uncompressed mode.\n\nHere is the previous matrix represented in compressed mode:\n\nA SparseVector is a special case of a SparseMatrix where only the and arrays are stored. There is no notion of compressed/uncompressed mode for a SparseVector.\n\nBefore describing each individual class, let's start with the following typical example: solving the Laplace equation \\( \\Delta u = 0 \\) on a regular 2D grid using a finite difference scheme and Dirichlet boundary conditions. Such problem can be mathematically expressed as a linear problem of the form \\( Ax=b \\) where \\( x \\) is the vector of unknowns (in our case, the values of the pixels), \\( b \\) is the right hand side vector resulting from the boundary conditions, and \\( A \\) is an \\( m \\times m \\) matrix containing only a few non-zero elements resulting from the discretization of the Laplacian operator.\n\nIn this example, we start by defining a column-major sparse matrix type of double , and a triplet list of the same scalar type . A triplet is a simple object representing a non-zero entry as the triplet: index, index, .\n\nIn the main function, we declare a list of triplets (as a std vector) and the right hand side vector \\( b \\) which are filled by the buildProblem function. The raw and flat list of non-zero entries is then converted to a true SparseMatrix object . Note that the elements of the list do not have to be sorted, and possible duplicate entries will be summed up.\n\nThe last step consists of effectively solving the assembled problem. Since the resulting matrix is symmetric by construction, we can perform a direct Cholesky factorization via the SimplicialLDLT class which behaves like its LDLT counterpart for dense objects.\n\nThe resulting vector contains the pixel values as a 1D array which is saved to a jpeg file shown on the right of the code above.\n\nDescribing the buildProblem and save functions is out of the scope of this tutorial. They are given here for the curious and reproducibility purpose.\n\nMatrix and vector properties \n\n The SparseMatrix and SparseVector classes take three template arguments: the scalar type (e.g., double) the storage order (ColMajor or RowMajor, the default is ColMajor) the inner index type (default is ).\n\nAs for dense Matrix objects, constructors takes the size of the object. Here are some examples:\n\nIn the rest of the tutorial, and represent any sparse-matrix and sparse-vector objects, respectively.\n\nThe dimensions of a matrix can be queried using the following functions:\n\nIterating over the nonzero coefficients \n\n Random access to the elements of a sparse object can be done through the function. However, this function involves a quite expensive binary search. In most cases, one only wants to iterate over the non-zeros elements. This is achieved by a standard loop over the outer dimension, and then by iterating over the non-zeros of the current inner vector via an InnerIterator. Thus, the non-zero entries have to be visited in the same order than the storage order. Here is an example:\n\nFor a writable expression, the referenced value can be modified using the valueRef() function. If the type of the sparse matrix or vector depends on a template parameter, then the keyword is required to indicate that denotes a type; see The template and typename keywords in C++ for details.\n\nBecause of the special storage scheme of a SparseMatrix, special care has to be taken when adding new nonzero entries. For instance, the cost of a single purely random insertion into a SparseMatrix is , where is the current number of non-zero coefficients.\n\nThe simplest way to create a sparse matrix while guaranteeing good performance is thus to first build a list of so-called triplets, and then convert it to a SparseMatrix.\n\nThe of triplets might contain the elements in arbitrary order, and might even contain duplicated elements that will be summed up by setFromTriplets(). See the SparseMatrix::setFromTriplets() function and class Triplet for more details.\n\nIn some cases, however, slightly higher performance, and lower memory consumption can be reached by directly inserting the non-zeros into the destination matrix. A typical scenario of this approach is illustrated below:\n• The key ingredient here is the line 2 where we reserve room for 6 non-zeros per column. In many cases, the number of non-zeros per column or row can easily be known in advance. If it varies significantly for each inner vector, then it is possible to specify a reserve size for each inner vector by providing a vector object with an returning the reserve size of the inner vector (e.g., via a or ). If only a rought estimate of the number of nonzeros per inner-vector can be obtained, it is highly recommended to overestimate it rather than the opposite. If this line is omitted, then the first insertion of a new element will reserve room for 2 elements per inner vector.\n• The line 4 performs a sorted insertion. In this example, the ideal case is when the column is not full and contains non-zeros whose inner-indices are smaller than . In this case, this operation boils down to trivial O(1) operation.\n• When calling the element , must not already exists, otherwise use the method that will allow to, e.g., accumulate values. This method first performs a binary search and finally calls if the element does not already exist. It is more flexible than but also more costly.\n• The line 5 suppresses the remaining empty space and transforms the matrix into a compressed column storage.\n\nBecause of their special storage format, sparse matrices cannot offer the same level of flexibility than dense matrices. In Eigen's sparse module we chose to expose only the subset of the dense matrix API which can be efficiently implemented. In the following sm denotes a sparse matrix, sv a sparse vector, dm a dense matrix, and dv a dense vector.\n\nSparse expressions support most of the unary and binary coefficient wise operations:\n\nHowever, a strong restriction is that the storage orders must match. For instance, in the following example:\n\nsm1, sm2, and sm3 must all be row-major or all column-major. On the other hand, there is no restriction on the target matrix sm4. For instance, this means that for computing \\( A^T + A \\), the matrix \\( A^T \\) must be evaluated into a temporary matrix of compatible storage order:\n\nPerformance-wise, the adding/subtracting sparse and dense matrices is better performed in two steps. For instance, instead of doing , better write:\n\nThis version has the advantage to fully exploit the higher performance of dense storage (no indirection, SIMD, etc.), and to pay the cost of slow sparse evaluation on the few non-zeros of the sparse matrix only.\n\nHowever, there is no method.\n• symmetric sparse-dense. The product of a sparse symmetric matrix with a dense matrix (or vector) can also be optimized by specifying the symmetry with :\n• sparse-sparse. For sparse-sparse products, two different algorithms are available. The default one is conservative and preserve the explicit zeros that might appear: The second algorithm prunes on the fly the explicit zeros, or the values smaller than a given threshold. It is enabled and controlled through the functions:\n\nRegarding read-access, sparse matrices expose the same API than for dense matrices to access to sub-matrices such as blocks, columns, and rows. See Block operations for a detailed introduction. However, for performance reasons, writing to a sub-sparse-matrix is much more limited, and currently only contiguous sets of columns (resp. rows) of a column-major (resp. row-major) SparseMatrix are writable. Moreover, this information has to be known at compile-time, leaving out methods such as and . The available API for write-access to a SparseMatrix are summarized below:\n\nIn addition, sparse matrices expose the and methods, which are aliases to the / methods for a column-major storage, and to the / methods for a row-major storage.\n\nJust as with dense matrices, the function can be used to address a triangular part of the matrix, and perform triangular solves with a dense right hand side:\n\nPlease, refer to the Quick Reference guide for the list of supported operations. The list of linear solvers available is here."
    },
    {
        "link": "https://eigen.tuxfamily.org/index.php?title=Main_Page",
        "document": "Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms.\n• Eigen is versatile.\n• It supports all matrix sizes, from small fixed-size matrices to arbitrarily large dense matrices, and even sparse matrices.\n• It supports all standard numeric types, including std::complex, integers, and is easily extensible to custom numeric types.\n• It supports various matrix decompositions and geometry features.\n• Its ecosystem of unsupported modules provides many specialized features such as non-linear optimization, matrix functions, a polynomial solver, FFT, and much more.\n• Eigen is fast.\n• Expression templates allow intelligently removing temporaries and enable lazy evaluation, when that is appropriate.\n• Explicit vectorization is performed for SSE 2/3/4, AVX, AVX2, FMA, AVX512, ARM NEON (32-bit and 64-bit), PowerPC AltiVec/VSX (32-bit and 64-bit), ZVector (s390x/zEC13) SIMD instruction sets, and since 3.4 MIPS MSA with graceful fallback to non-vectorized code.\n• Fixed-size matrices are fully optimized: dynamic memory allocation is avoided, and the loops are unrolled when that makes sense.\n• For large matrices, special attention is paid to cache-friendliness.\n• Eigen is reliable.\n• Algorithms are carefully selected for reliability. Reliability trade-offs are clearly documented and extremely safe decompositions are available.\n• Eigen is thoroughly tested through its own test suite (over 500 executables), the standard BLAS test suite, and parts of the LAPACK test suite.\n• Eigen is elegant.\n• The API is extremely clean and expressive while feeling natural to C++ programmers, thanks to expression templates.\n• Implementing an algorithm on top of Eigen feels like just copying pseudocode.\n• Eigen has good compiler support as we run our test suite against many compilers to guarantee reliability and work around any compiler bugs. Eigen up to version 3.4 is standard C++03 and maintains reasonable compilation times. Versions following 3.4 will be C++14.\n• Eigen 3 documentation: this includes a getting started guide, a long tutorial, a quick reference, and page about porting from Eigen 2 to Eigen 3.\n• Eigen 2 documentation (old): this includes the Eigen 2 Tutorial.\n\nEigen doesn't have any dependencies other than the C++ standard library.\n\nWe use the CMake build system, but only to build the documentation and unit-tests, and to automate installation. If you just want to use Eigen, you can use the header files right away. There is no binary library to link to, and no configured header file. Eigen is a pure template library defined in the headers.\n\nEigen is Free Software. Starting from the 3.1.1 version, it is licensed under the MPL2, which is a simple weak copyleft license. Common questions about the MPL2 are answered in the official MPL2 FAQ.\n\nEarlier versions were licensed under the LGPL3+.\n\nNote that currently, a few features rely on third-party code licensed under the LGPL: constrained_cg. Such features can be explicitly disabled by compiling with the EIGEN_MPL2_ONLY preprocessor symbol defined. Furthermore, Eigen provides interface classes for various third-party libraries (usually recognizable by the <Eigen/*Support> header name). Of course you have to mind the license of the so-included library when using them.\n\nVirtually any software may use Eigen. For example, closed-source software may use Eigen without having to disclose its own source code. Many proprietary and closed-source software projects are using Eigen right now, as well as many BSD-licensed projects.\n\nSee the MPL2 FAQ for more information, and do not hesitate to contact us if you have any questions.\n\nEigen is standard C++98 and so should theoretically be compatible with any compliant compiler. Whenever we use some non-standard feature, that is optional and can be disabled.\n\nEigen is being successfully used with the following compilers:\n• GCC, version 4.8 and newer. Older versions of gcc might work as well but they are not tested anymore.\n• MSVC (Visual Studio), 2012 and newer. Be aware that enabling IntelliSense (/FR flag) is known to trigger some internal compilation errors. The old 3.2 version of Eigen supports MSVC 2010, and the 3.1 version supports MSVC 2008.\n• Intel C++ compiler. Enabling the option is highly recommended.\n• LLVM/CLang++, version 3.4 and newer. (The 2.8 version used to work fine, but it is not tested with up-to-date versions of Eigen)\n\nRegarding performance, Eigen performs best with compilers based on GCC or LLVM/Clang. See this page for some known compilation issues.\n\nNeed help using Eigen? Try this:\n• To get help, stackoverflow is your best resource.\n• Want to discuss something with the developers? Use our mailing list.\n• Want to have an informal chat on Eigen? Use our Discord server.\n\nFor bug reports and feature requests, please use the issue tracker on GitLab.\n• To subscribe, send a mail with subject subscribe to eigen-request@lists.tuxfamily.org\n• To unsubscribe, send a mail with subject unsubscribe to eigen-request@lists.tuxfamily.org\n\nIn both cases, you will get a confirmation mail to which you need to reply. If you have any trouble please ask at the eigen-core-team address for help.\n\nThe Eigen mailing list can be used for discussing general Eigen development topics. End-user questions are often better asked on the Use our Discord server. Development of specific features is best tracked and discussed on our issue tracker on GitLab.\n\nThis mailing list is public and has public archives.\n\nImportant: You must subscribe before you may post. Sorry, this is our only way to prevent spam.\n\nImportant: After you sent your subscription request, you will receive a confirmation e-mail. Check your spam folder, as these confirmation e-mails are often filtered as spam!\n\nThere is also a private mailing list which should only be used if you want to write privately to a few core developers (it is read by Gaël, Christoph, Rasmus, Antonio, David, and Constantino). The address is eigen-core-team at the same lists server as for the Eigen mailing list. You do not need to subscribe (actually, subscription is closed). For all Eigen development discussion, use the public mailing list or the issue tracker on GitLab instead.\n\nEverybody's welcome to discuss Eigen-related topics or just chat. Bugs should still be reported on the issue tracker on GitLab and formal discussions should happen on the mailing list. Discord is an ideal place to ask other users and developers for help.\n\nEigen is written and maintained by volunteers. You can contribute in many ways to help: give support to new users, write and improve documentation, helping with bugs and other issues in the issue tracker on GitLab, discussing the design and the API, running tests and writing code. See our page on Contributing to Eigen for pointers to get you started.\n\nFeel free to add yourself! If you don't have access to the wiki or if you are not sure about the relevance of your project, ask at the #Mailing list.\n• Google's TensorFlow is an Open Source Software Library for Machine Intelligence\n• Google's Ceres solver is a portable C++ library that allows for modeling and solving large complicated nonlinear least squares problems.\n• The Manifold ToolKit MTK provides easy mechanisms to enable arbitrary algorithms to operate on manifolds. It also provides a Sparse Least Squares Solver (SLoM) and an Unscented Kalman Filter (UKFoM).\n• IFOPT is a modern, light-weight, Eigen-based C++ interface to Nonlinear Programming solvers, such as Ipopt and Snopt.\n• CppNumericalSolvers is a lightweight header-only library for non-linear optimization including various solvers: CG, L-BGFS-B, CMAes, Nelder-Mead.\n• GTSAM is a library implementing smoothing and mapping (SAM) in robotics and vision, using factor graphs and Bayes networks.\n• g2o is an open-source C++ framework for optimizing graph-based nonlinear least-square problems.\n• redsvd is a RandomizED Singular Value Decomposition library for sparse or very large dense matrices.\n• trustOptim is a trust-region based non linear solver supporting sparse Hessians (C++ implementation with R binding).\n• StOpt, the STochastic OPTimization library aims at providing tools in C++ for solving some stochastic optimization problems encountered in finance or in the industry.\n• Nelson an open computing environment for engineering and scientific applications using modern C/C++ libraries (Boost, Eigen, FFTW, …) and others state of art numerical libraries. (GPL2)\n• EigenLab is a header only library to parse and evaluate expressions working on Eigen matrices.\n• SpaFEDte a C++ library for discontinuous Galerkin discretizations on general meshes.\n• biicode a C and C++ dependency manager that #includes the most popular and useful C/C++ libs and frameworks.\n• Spectra stands for Sparse Eigenvalue Computation Toolkit as a Redesigned ARPACK. It is a header-only C++ library for large scale eigenvalue problems, built on top of Eigen.\n• preCICE is a coupling library for partitioned multi-physics simulations, including, but not restricted to fluid-structure interaction and conjugate heat transfer simulations. It supports OpenFOAM, CalculiX, SU2, and several other well-known, as well as in-house solvers. It is free/open-source software and its code is available on GitHub under the LGPL3 license. Link to source code and documentation\n• The RcppEigen package provides bindings and more for R.\n• minieigen is small boost::python wrapper for Eigen's core classes (dense fixed and dynamic-sized vectors, matrices, aligned boxes, quaternions; with integer, floating and complex scalars) including operators and subset of Eigen's API methods.\n• Eigency is a Cython interface between the numpy arrays and the Matrix/Array classes of the Eigen C++ library.\n• OCamlEigen, a thin OCaml interface to the Eigen 3 C++ library.\n• eigen-lua, a Lua wrapper around parts of the Eigen numerical library.\n• Eigenpy: Efficient bindings between Numpy and Eigen using Boost.Python with support of the Geometry module\n• GINESTRA, a semiconductor device simulator with a focus on advanced dielectric materials and interfaces.\n• G+Smo, an open-source library for geometric design and numerical simulation with isogeometric analyis.\n• FlexibleSUSY, a spectrum generator which calculates the masses of elementary particles.\n• The ATLAS experiment at the LHC (Large Hadron Collider) at CERN is using Eigen, as reported in this article, noting \"Eigen was chosen since it offered the largest performance improvements for ATLAS use cases of the options investigated.\"\n• The Large Survey Synoptic Telescope (website; trac) is a project to build a 3.2Mpixel camera on an 8.4m telescope and survey the entire visible sky every three days.\n• The 3D astronomical visualization application Celestia is now using Eigen for all orbital and geometric calculation.\n• Yade, platform for dynamic particle models, uses Eigen for geometric computations (switched from the WildMagic package)\n• SLangTNG, an application suite for numerical analysis, linear algebra, advanced statistics, FEM, structural dynamics, data visualization, etc.\n• Clip, an opensource program for the orientation of Laue exposures.\n• Multiprecision Computing Toolbox for MATLAB uses Eigen as core library for matrix computations.\n• Cufflinks, a tool for transcript assembly, differential expression, and differential regulation for RNA-Sequences.\n• NIMBLE, a system for programming statistical algorithms such as Markov chain Monte Carlo from R. NIMBLE includes a compiler for a subset of R to C++ that uses Eigen.\n• ENigMA is a multiphysics numerical library which uses Eigen.\n• iMSTK is an open source software toolkit written in C++ that aids rapid prototyping of interactive multi-modal surgical simulations.\n• mbsolve is an open-source solver tool for the Maxwell-Bloch equations, which are used to model light-matter interaction in nonlinear optics.\n• Quantum++ is a modern C++ general purpose quantum computing library, composed solely of template header files.\n• The Yujin Robot company uses Eigen for the navigation and arm control of their next gen robots. (switched from blitz, ublas and tvmet)\n• The Darmstadt Dribblers autonomous Humanoid Robot Soccer Team and Darmstadt Rescue Robot Team use Eigen for navigation and world modeling.\n• The Mobile Robot Programming Toolkit (MRPT), a set of libraries for SLAM, localization and computer vision, moved to Eigen (switched from home made math classes).\n• RL a self-contained C++ library for robot kinematics, motion planning and control.\n• BTK is a Biomechanical ToolKit, licensed under BSD whose primary goal is to propose a set of tools for the analysis of the human body motion which is independent of any acquisition system. It proposes bindings for Matlab/Octave and Python, and a GUI software called Mokka to visualize/analyze 3D/2D motion capture data.\n• libpointmatcher is a \"Iterative Closest Point\" library for 3D mapping in robotics.\n• RobOptim is a modern, Open-Source, C++ library for numerical optimization applied to robotics.\n• towr is a light-weight and extensible C++ library for trajectory optimization for legged robots.\n• The Humanoid Path Planner: a software for Motion and Manipulation Planning\n• MIRA is a cross-platform framework written in C++ that provides a middleware for the development of complex robotic applications, which consists of distributed software modules.\n• Computational Geometry Algorithms Library (CGAL), a collaborative effort to develop a robust, easy to use, and efficient C++ software library of geometric data structures and algorithms.\n• Point Cloud Library (PCL), a large scale, BSD licensed, open project for point cloud processing. Uses Eigen as their math backend.\n• VcgLib, an opensource C++ template library for the manipulation and processing of triangle and tetrahedral meshes. (switched from home made math classes)\n• MeshLab, an opensource software for the processing and editing of unstructured 3D triangular meshes and point cloud. (switched from vcglib's math classes)\n• eos, an opensource and lightweight 3D Morphable Face Model fitting library in modern C++11/14.\n• The Topology ToolKit (TTK), an open-source library and software collection for topological data analysis in scientific visualization.\n• Theia, an opensource C++ structure from motion library tailored for researchers, BSD licensed.\n• libmv, an opensource structure from motion library. (switched from FLENS)\n• metronome , a simple metric storage/graphing engine using Eigen for SVD least squares interpolation & data consolidation\n• libigl is a simple C++ geometry processing library with wide functionality.\n• ApproxMVBB is a small library to compute fast approximate oriented bounding boxes of 3D point clouds.\n• Madplotlib makes it easier to plot 2D charts on Qt from data created by Eigen::ArrayXf.\n• 3DF Zephyr is a commercial photogrammetry and 3D modeling software, developed by 3Dflow srl\n• Layar, an augmented reality application for IPhone and Android.\n• Red Sword Studios, maker of the iPhone games Gradient, Fortress Luna, Stimulus, and Lustre, uses Eigen extensively. Why roll your own matrix/vector/transformation code when there's Eigen?\n• WhirlyGlobe-Maply an open source geospatial display toolkit for iOS and Android. It implements both a 3D interactive globe and a 2D (slippy) map\n• Calligra Sheets, the spreadsheet module of KDE's office suite uses Eigen for matrix functions such as MINVERSE, MMULT, MDETERM.\n• Kalzium uses Eigen indirectly through the aforementioned Avogadro library.\n• the Mandelbrot wallpaper plugin, some screensavers, kgllib, solidkreator, etc.\n\n\n\n If you are aware of some interesting projects using Eigen, please send us a message (including a link and short description) or directly edit this wiki page!\n\nThe Eigen project was started by Benoît Jacob (founder) and Gaël Guennebaud (guru). Many other people have since contributed their talents to help make Eigen successful. Here's an alphabetical list: (note to contributors: do add yourself!)\n\nEigen is also using code that we copied from other sources. They are acknowledged in our sources and in the Mercurial history, but let's also mention them here:\n\nSpecial thanks to Tuxfamily for the wonderful quality of their services, and the GCC Compile Farm Project that gives us access to many various systems including ARM NEON.\n\nIf you are looking for a BibTeX entry to use to cite Eigen in academic papers, see the BibTeX page."
    },
    {
        "link": "https://eigen.tuxfamily.org/dox/group__TutorialMatrixArithmetic.html",
        "document": "This page aims to provide an overview and some details on how to perform arithmetic between matrices, vectors and scalars with Eigen.\n\nEigen offers matrix/vector arithmetic operations either through overloads of common C++ arithmetic operators such as +, -, *, or through special methods such as dot(), cross(), etc. For the Matrix class (matrices and vectors), operators are only overloaded to support linear-algebraic operations. For example, means matrix-matrix product, and is just not allowed. If you want to perform all kinds of array operations, not linear algebra, see the next page.\n\nThe left hand side and right hand side must, of course, have the same numbers of rows and of columns. They must also have the same type, as Eigen doesn't do automatic type promotion. The operators at hand here are:\n\nMultiplication and division by a scalar is very simple too. The operators at hand here are:\n\nThis is an advanced topic that we explain on this page, but it is useful to just mention it now. In Eigen, arithmetic operators such as don't perform any computation by themselves, they just return an \"expression object\" describing the computation to be performed. The actual computation happens later, when the whole expression is evaluated, typically in . While this might sound heavy, any modern optimizing compiler is able to optimize away that abstraction and the result is perfectly optimized code. For example, when you do:\n\nEigen compiles it to just one for loop, so that the arrays are traversed only once. Simplifying (e.g. ignoring SIMD optimizations), this loop looks like this:\n\nThus, you should not be afraid of using relatively large arithmetic expressions with Eigen: it only gives Eigen more opportunities for optimization.\n\nThe transpose \\( a^T \\), conjugate \\( \\bar{a} \\), and adjoint (i.e., conjugate transpose) \\( a^* \\) of a matrix or vector \\( a \\) are obtained by the member functions transpose(), conjugate(), and adjoint(), respectively.\n\nFor real matrices, is a no-operation, and so is equivalent to .\n\nAs for basic arithmetic operators, and simply return a proxy object without doing the actual transposition. If you do , then the transpose is evaluated at the same time as the result is written into . However, there is a complication here. If you do , then Eigen starts writing the result into before the evaluation of the transpose is finished. Therefore, the instruction does not replace with its transpose, as one would expect:\n\nThis is the so-called aliasing issue. In \"debug mode\", i.e., when assertions have not been disabled, such common pitfalls are automatically detected.\n\nFor in-place transposition, as for instance in , simply use the transposeInPlace() function:\n\nThere is also the adjointInPlace() function for complex matrices.\n\nMatrix-matrix multiplication is again done with . Since vectors are a special case of matrices, they are implicitly handled there too, so matrix-vector product is really just a special case of matrix-matrix product, and so is vector-vector outer product. Thus, all these cases are handled by just two operators:\n• compound operator *= as in (this multiplies on the right: is equivalent to )\n\nNote: if you read the above paragraph on expression templates and are worried that doing might cause aliasing issues, be reassured for now: Eigen treats matrix multiplication as a special case and takes care of introducing a temporary here, so it will compile as:\n\nIf you know your matrix product can be safely evaluated into the destination matrix without aliasing issue, then you can use the noalias() function to avoid the temporary, e.g.:\n\nFor more details on this topic, see the page on aliasing.\n\nNote: for BLAS users worried about performance, expressions such as are fully optimized and trigger a single gemm-like function call.\n\nFor dot product and cross product, you need the dot() and cross() methods. Of course, the dot product can also be obtained as a 1x1 matrix as u.adjoint()*v.\n\nRemember that cross product is only for vectors of size 3. Dot product is for vectors of any sizes. When using complex numbers, Eigen's dot product is conjugate-linear in the first variable and linear in the second variable.\n\nEigen also provides some reduction operations to reduce a given matrix or vector to a single value such as the sum (computed by sum()), product (prod()), or the maximum (maxCoeff()) and minimum (minCoeff()) of all its coefficients.\n\nThe trace of a matrix, as returned by the function trace(), is the sum of the diagonal coefficients and can also be computed as efficiently using , as we will see later on.\n\nThere also exist variants of the and functions returning the coordinates of the respective coefficient via the arguments:\n\nEigen checks the validity of the operations that you perform. When possible, it checks them at compile time, producing compilation errors. These error messages can be long and ugly, but Eigen writes the important message in UPPERCASE_LETTERS_SO_IT_STANDS_OUT. For example:\n\nOf course, in many cases, for example when checking dynamic sizes, the check cannot be performed at compile time. Eigen then uses runtime assertions. This means that the program will abort with an error message when executing an illegal operation if it is run in \"debug mode\", and it will probably crash if assertions are turned off.\n\nFor more details on this topic, see this page."
    },
    {
        "link": "https://stackoverflow.com/questions/57367167/for-eigen-sparsematrix-what-does-innerindexptr-and-outerindexptr-exactly-re",
        "document": "Eigen uses CSC (compressed sparse column) format (see also https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)).\n\nindicates the index into other arrays where i-th column starts. It ends at , where the next column begins. If these thwo indices are equal to each other, the column is empty.\n\nis an array of row indices of elements, and is an array of corresponding values.\n\nSo, to illustrate it, this code iterates through i-th column\n\nAll above is for column-major storage, which is default in Eigen. For row-major, exchange row <-> column in the description above."
    },
    {
        "link": "https://pybind11.readthedocs.io/en/stable/advanced/cast/eigen.html",
        "document": "Eigen is C++ header-based library for dense and sparse linear algebra. Due to its popularity and widespread adoption, pybind11 provides transparent conversion and limited mapping support between Eigen and Scientific Python linear algebra data types.\n\nTo enable the built-in Eigen support you must include the optional header file .\n\nWhen binding a function with ordinary Eigen dense object arguments (for example, ), pybind11 will accept any input value that is already (or convertible to) a with dimensions compatible with the Eigen type, copy its values into a temporary Eigen variable of the appropriate type, then call the function with this temporary variable. Sparse matrices are similarly copied to or from / objects.\n\nOne major limitation of the above is that every data conversion implicitly involves a copy, which can be both expensive (for large matrices) and disallows binding functions that change their (Matrix) arguments. Pybind11 allows you to work around this by using Eigen’s class much as you would when writing a function taking a generic type in Eigen itself (subject to some limitations discussed below). When calling a bound function accepting a type, pybind11 will attempt to avoid copying by using an object that maps into the source data: this requires both that the data types are the same (e.g. and is ); and that the storage is layout compatible. The latter limitation is discussed in detail in the section below, and requires careful consideration: by default, numpy matrices and Eigen matrices are not storage compatible. If the numpy matrix cannot be used as is (either because its types differ, e.g. passing an array of integers to an Eigen parameter requiring doubles, or because the storage is incompatible), pybind11 makes a temporary copy and passes the copy instead. When a bound function parameter is instead (note the lack of ), pybind11 will only allow the function to be called if it can be mapped and if the numpy array is writeable (that is is true). Any access (including modification) made to the passed variable will be transparently carried out directly on the . This means you can write code such as the following and have it work as expected: Note, however, that you will likely run into limitations due to numpy and Eigen’s difference default storage order for data; see the below section on Storage orders for details on how to bind code that won’t run into such limitations. Passing by reference is not supported for sparse types.\n\nWhen returning an ordinary dense Eigen matrix type to numpy (e.g. or ) pybind11 keeps the matrix and returns a numpy array that directly references the Eigen matrix: no copy of the data is performed. The numpy array will have set to to indicate that it does not own the data, and the lifetime of the stored Eigen matrix will be tied to the returned . If you bind a function with a non-reference, return type (e.g. ), the same thing happens except that pybind11 also sets the numpy array’s flag to false. If you return an lvalue reference or pointer, the usual pybind11 rules apply, as dictated by the binding function’s return value policy (see the documentation on Return value policies for full details). That means, without an explicit return value policy, lvalue references will be copied and pointers will be managed by pybind11. In order to avoid copying, you should explicitly specify an appropriate return value policy, as in the following example: # m[5,6] and v[5,6] refer to the same element, c[5,6] does not. Note in this example that is used to tie the life of the MyClass object to the life of the returned arrays. You may also return an , or other map-like Eigen object (for example, the return value of and related methods) that map into a dense Eigen type. When doing so, the default behaviour of pybind11 is to simply reference the returned data: you must take care to ensure that this data remains valid! You may ask pybind11 to explicitly copy such a return value by using the policy when binding the function. You may also use or a to ensure the data stays valid as long as the returned numpy array does. When returning such a reference of map, pybind11 additionally respects the readonly-status of the returned value, marking the numpy array as non-writeable if the reference or map was itself read-only. Sparse types are always copied when returned.\n\nPassing arguments via has some limitations that you must be aware of in order to effectively pass matrices by reference. First and foremost is that the default class requires contiguous storage along columns (for column-major types, the default in Eigen) or rows if is specifically an storage type. The former, Eigen’s default, is incompatible with ’s default row-major storage, and so you will not be able to pass numpy arrays to Eigen by reference without making one of two changes. (Note that this does not apply to vectors (or column or row matrices): for such types the “row-major” and “column-major” distinction is meaningless). The first approach is to change the use of to the more general (or similar type with a fully dynamic stride type in the third template argument). Since this is a rather cumbersome type, pybind11 provides a type alias for your convenience (along with EigenDMap for the equivalent Map, and EigenDStride for just the stride type). This type allows Eigen to map into any arbitrary storage order. This is not the default in Eigen for performance reasons: contiguous storage allows vectorization that cannot be done when storage is not known to be contiguous at compile time. The default stride type allows non-contiguous storage along the outer dimension (that is, the rows of a column-major matrix or columns of a row-major matrix), but not along the inner dimension. This type, however, has the added benefit of also being able to map numpy array slices. For example, the following (contrived) example uses Eigen with a numpy slice to multiply by 2 all coefficients that are both on even rows (0, 2, 4, …) and in columns 2, 5, or 8: The second approach to avoid copying is more intrusive: rearranging the underlying data types to not run into the non-contiguous storage problem in the first place. In particular, that means using matrices with storage, where appropriate, such as: // Use RowMatrixXd instead of MatrixXd Now bound functions accepting arguments will be callable with numpy’s (default) arrays without involving a copying. You can, alternatively, change the storage order that numpy arrays use by adding the option when creating an array: Such an object will be passable to a bound function accepting an (or similar column-major Eigen type). One major caveat with this approach, however, is that it is not entirely as easy as simply flipping all Eigen or numpy usage from one to the other: some operations may alter the storage order of a numpy array. For example, results in being a view of that references the same data, but in the opposite storage order! While this approach allows fully optimized vectorized calculations in Eigen, it cannot be used with array slices, unlike the first approach. When returning a matrix to Python (either a regular matrix, a reference via , or a map/block into a matrix), no special storage consideration is required: the created numpy array will have the required stride that allows numpy to properly interpret the array, whatever its storage order.\n\nThe default behaviour when binding Eigen references is to copy matrix values when passed a numpy array that does not conform to the element type of or does not have a compatible stride layout. If you want to explicitly avoid copying in such a case, you should bind arguments using the annotation (as described in the Non-converting arguments documentation). The following example shows an example of arguments that don’t allow data copying to take place: // The method and function to be bound: // <- Don't allow copying for this arg // <- This one can be copied if needed With the above binding code, attempting to call the method on a object, or attempting to call will raise a rather than making a temporary copy of the array. It will, however, allow the argument to be copied into a temporary if necessary. Note that explicitly specifying is not required for mutable Eigen references (e.g. without on the ): mutable references will never be called with a temporary copy.\n\nEigen and numpy have fundamentally different notions of a vector. In Eigen, a vector is simply a matrix with the number of columns or rows set to 1 at compile time (for a column vector or row vector, respectively). NumPy, in contrast, has comparable 2-dimensional 1xN and Nx1 arrays, but also has 1-dimensional arrays of size N. When passing a 2-dimensional 1xN or Nx1 array to Eigen, the Eigen type must have matching dimensions: That is, you cannot pass a 2-dimensional Nx1 numpy array to an Eigen value expecting a row vector, or a 1xN numpy array as a column vector argument. On the other hand, pybind11 allows you to pass 1-dimensional arrays of length N as Eigen parameters. If the Eigen type can hold a column vector of length N it will be passed as such a column vector. If not, but the Eigen type constraints will accept a row vector, it will be passed as a row vector. (The column vector takes precedence when both are supported, for example, when passing a 1D numpy array to a MatrixXd argument). Note that the type need not be explicitly a vector: it is permitted to pass a 1D numpy array of size 5 to an Eigen : you would end up with a 1x5 Eigen matrix. Passing the same to an would result in a 5x1 Eigen matrix. When returning an Eigen vector to numpy, the conversion is ambiguous: a row vector of length 4 could be returned as either a 1D array of length 4, or as a 2D array of size 1x4. When encountering such a situation, pybind11 compromises by considering the returned Eigen type: if it is a compile-time vector–that is, the type has either the number of rows or columns set to 1 at compile time–pybind11 converts to a 1D numpy array when returning the value. For instances that are a vector only at run-time (e.g. , ), pybind11 returns the vector as a 2D array to numpy. If this isn’t want you want, you can use to get a view of the same data in the desired dimensions. The file contains a complete example that shows how to pass Eigen sparse and dense data types in more detail."
    },
    {
        "link": "https://stackoverflow.com/questions/8805521/optimizing-a-loop-vs-code-duplication",
        "document": "My dilemma concerns how to best handle long heavy loops which can accept parameters. Consider the following method:\n\nThis method will do what I want, but I am using 10000000 unnecessary s inside the loop.\n\nHad I written the same method like this:\n\nI would get the same result, though my entire loop code would have to be duplicated. This is not a big deal if we're talking about one parameter, but when you have 4 independent parameters I would have to write 16 different versions of the loop.\n\nWhat is the \"correct\" solution in cases like this? If this were a language like Python I could just dynamically build a function to handle the loop. Is there something similar in C++?\n\nNeedless to say, the code is only an example and not the actual case. Please don't give solutions pertaining to per se. I am a C++ novice so forgive me if there is a simple solution I am not aware of. Also forgive me if there are syntax errors, I don't have a compiler handy at the moment.\n\nEdit: The issue is dealing with statements that can not be pre-calculated outside of the loop."
    },
    {
        "link": "https://hackernoon.com/c-performance-optimization-best-practices",
        "document": "Performance optimization is a critical aspect of C++ programming, as it can significantly impact the speed and efficiency of your applications. In this article, we'll explore various techniques and best practices for optimizing C++ code. Whether you're a beginner or an experienced developer, these tips will help you write faster and more efficient C++ programs.\n\n1. Use the Right Data Structures\n\nChoosing the appropriate data structures can have a massive impact on performance. Use for dynamic arrays, or for key-value pairs and or for unique values. Avoid linked lists when you need random access, as they can lead to poor cache performance.\n\nExample: Using for Dynamic Arrays\n\nCopying objects can be expensive. Use references or move semantics ( ) when passing and returning objects to minimize unnecessary copying. If you use then try to change it in some cases, it will have a better performance.\n\nAllocate objects on the stack whenever possible, as stack allocation is faster than heap allocation. Use dynamic allocation (e.g., and ) only when the object's lifetime extends beyond the current scope.\n\nHowever, it's important to note that stack allocation has limitations:\n• Fixed Size: Stack memory is of fixed size and is limited. This means you can't allocate very large objects or a dynamic number of objects on the stack.\n• Risk of Stack Overflow: Excessive stack memory usage can lead to a stack overflow if the available stack space is exhausted. Heap memory doesn't have this limitation.\n\nProfiling tools can help identify performance bottlenecks. Use tools like (GNU Profiler) or platform-specific profilers to analyze your code's execution time and memory usage.\n• Identify what areas of code are taking how much time\n• See if you can use better data structures/ algorithms to make things faster\n\nExcessive memory allocation and deallocation can lead to performance issues. Reuse objects when possible and consider using object pools for frequently created and destroyed objects.\n\nLoops are often the core of algorithms. Optimize loops by minimizing loop overhead, reducing unnecessary calculations, and using the right loop constructs (e.g., range-based loops).\n\nModern C++ compilers provide optimization flags (e.g., , ) that can significantly improve code performance. Use these flags during compilation to enable various optimization techniques.\n• : Enables basic optimization. This includes optimizations such as common subexpression elimination and instruction scheduling. It's a good balance between optimization and compilation time.\n• : Enables more aggressive optimization, including inlining functions, loop optimizations, and better code scheduling. It provides a significant performance boost.\n• : Enables even more aggressive optimizations. It can lead to faster code but may increase compilation time and the size of the executable.\n\nMinimize function calls within tight loops. Inlining functions (e.g., using or compiler optimizations) can eliminate function call overhead.\n\nOptimize for cache efficiency by minimizing cache misses. Access data sequentially, avoid non-contiguous memory accesses, and use data structures that promote cache locality.\n\nBenchmark your code after each optimization step to measure the impact of changes accurately. Iteratively apply optimizations, focusing on the most significant bottlenecks.\n\nOptimizing C++ code is a crucial skill for achieving high-performance applications. You can significantly enhance your code's speed and efficiency by using the right data structures, avoiding unnecessary copying, and following best practices. Profiling, benchmarking, and iterative optimization are essential tools for achieving optimal performance. Remember that premature optimization is not always beneficial; focus on optimizing critical sections of your code when necessary."
    },
    {
        "link": "https://stackoverflow.com/questions/13027550/c-repeating-codes-with-a-loops",
        "document": "Im not sure if this is a stupid question, so shoot me if it is!\n\nI am having this \"dilemna\" which I encounter very often. I have say two overloaded functions in C++\n\nsay we have this two overloads of F (just a pseudocode below)\n\nwhere A and B are different types and G1 and G2 are different functions doing different things. The code part of the overloads except for G1 and G2 lines are the same and they are sometimes very long and extensive. Now the question is.. how can I write my code more efficiently. Naturally I want NOT to repeat the code (even if it's easy to do that, because its just a copy paste routine). A friend suggested macro... but that would look dirty. Is this simple, because if it is Im quite stupid to know right now. Would appreciate any suggestions/help.\n\nEdit: Im sorry for those wanting a code example. The question was really meant to be abstract as I encounter different \"similar\" situation in which I ask myself how I am able to make the code shorter/cleaner. In most cases codes are long otherwise I wouldn't bother asking this in the first place. As KilianDS pointed out, it's also good to make sure that the function itself isn't very long. But sometimes this is just unavoidable. Many cases where I encounter this, the loop is even nested (i.e. several loops within each other) and the beginning of F we have the start of a loop and the end of F we end that loop."
    },
    {
        "link": "https://reddit.com/r/cpp/comments/gefh4r/what_are_your_speed_optimization_secrets",
        "document": "I know you. You've been coding C++ for a while now and have your tricks for how to make C++ run faster. I want to know your secrets to speed like tricks like C-style/C++ input reading desynchronization to make reading input faster. That's really the only one I know.\n\nPlease wise C++ masters, share your secret knowledge."
    },
    {
        "link": "https://quora.com/What-are-the-best-practices-for-optimizing-C-code-and-improving-its-performance",
        "document": "Something went wrong. Wait a moment and try again."
    }
]