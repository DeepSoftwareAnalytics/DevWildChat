[
    {
        "link": "https://boost.org/doc/libs/1_83_0/doc/html/boost_asio/examples.html",
        "document": ""
    },
    {
        "link": "https://boost.org/doc/libs/1_81_0/doc/html/boost_asio/examples.html",
        "document": ""
    },
    {
        "link": "https://medium.com/@extio/boost-asio-a-powerful-networking-library-for-c-e86367c2568d",
        "document": "In the world of modern software development, networking has become an indispensable part of most applications. Whether you’re building a web server, a peer-to-peer communication tool, or a real-time game, efficient and robust networking capabilities are crucial. C++ developers have long been in search of a comprehensive networking library, and that’s where Boost.Asio comes to the rescue.\n\nBoost.Asio is a C++ library that provides a set of tools for asynchronous I/O, concurrency, and networking. It is a part of the Boost C++ Libraries collection, which is a well-established and widely-used open-source project. Boost.Asio aims to simplify the process of writing asynchronous networked applications while also offering excellent performance.\n\nBoost.Asio boasts several key features that make it a popular choice among C++ developers:\n• Asynchronous I/O: Boost.Asio’s asynchronous model allows you to perform I/O operations without blocking the execution of the program. This feature is especially valuable for networking tasks, as it allows the application to handle multiple connections concurrently.\n• Networking Protocols: The library supports a wide range of networking protocols, including TCP, UDP, SSL/TLS, and serial ports. This enables developers to build various networking applications, from simple TCP servers to secure communication channels using SSL/TLS.\n• Platform Independence: Boost.Asio abstracts the platform-specific details, making it easier to write portable networking code that works across different operating systems without modification.\n• Error Handling: The library provides a robust error handling mechanism, which is essential for writing reliable networking applications. Errors are reported through exceptions or error codes, giving developers the flexibility to choose their preferred error-handling approach.\n• Timers: Boost.Asio includes timer functionality, allowing you to schedule tasks to be executed at specific intervals or after a delay. This feature is valuable for implementing time-sensitive operations.\n\nTo use Boost.Asio, you’ll need to download and install the Boost C++ Libraries. You can find the latest version of Boost at www.boost.org.\n\nOnce you have Boost installed, you can start using Boost.Asio in your C++ project. Here’s a simple example of a TCP echo server using Boost.Asio:\n\nIn this example, we create a simple TCP echo server that listens on port 8888. When a client connects, the server reads data from the client, echoes it back, and closes the connection. The handles asynchronous I/O operations, and listens for incoming connections.\n\nLet’s take another example with a real use-case using the Boost.Asio library to perform the HTTPS request. To use it for HTTPS requests, we’ll need to handle SSL/TLS as well. Make sure you have the Boost library and OpenSSL installed on your system before compiling the code. Here’s the C++ program using Boost.Asio to send an XML request to a HTTPS endpoint:\n\nAgain, please make sure you have Boost and OpenSSL installed on your system and link them appropriately during compilation. This code establishes an SSL/TLS connection to the specified HTTPS endpoint, sends the XML payload, and asynchronously reads the response. The HTTP status code is not explicitly extracted in this example, but you can do so by parsing the HTTP response headers if required.\n\nBoost.Asio is a powerful C++ library that simplifies networking and asynchronous I/O programming. Its flexibility, portability, and performance make it an excellent choice for building networked applications. The library’s rich feature set, error handling mechanism, and support for various protocols provide developers with the tools they need to create robust and efficient networking solutions.\n\nAs with any library, Boost.Asio requires some time and effort to master. However, once you become familiar with its concepts and patterns, you’ll find yourself equipped to tackle a wide range of networking challenges with ease.\n\nSo, why not give Boost.Asio a try and supercharge your C++ networking applications today! Happy coding!"
    },
    {
        "link": "https://think-async.com",
        "document": "Asio is a cross-platform C++ library for network and low-level I/O programming that provides developers with a consistent asynchronous model using a modern C++ approach.\n\nThe following video illustrates the use of Asio in C++20.\n\nWhen targeting C++11 (or later), most of Asio can be used without any extra dependencies. More…​\n\nAsio provides the basic building blocks for C++ networking, concurrency and other kinds of I/O.\n\nWhat kind of applications can use Asio? Asio is used in all kinds of applications, from phone apps to the world’s fastest share markets. Here is a list of some applications that use Asio."
    },
    {
        "link": "https://stackoverflow.com/questions/244453/best-documentation-for-boostasio",
        "document": "First, I've been using Boost.Asio for quite a while already -- and I share your concern. To address your question:\n• There really is very scarce documentation about Boost.Asio aside from the introduction and tutorial. I am not the author, but this is mostly because there are just too many things to document for something as low-level as an Asynchronous IO Library.\n• The examples give more away than the tutorials do. If you don't mind spending a little time looking at the different examples, I would think they should suffice to get you started. If you want to run away with it, then the reference documentation should help you a lot.\n• Ask around in the Boost Users and Boost Developers mailing list if you're really stuck or looking for specific guidance. I'm pretty sure a lot of people will be willing to address your concerns on the mailing lists.\n\nThere are efforts (not part of Boost.Asio) to expose a lot of the functionality and possible alternative use cases. This at best is scattered around the web in blogs and other forms of non-packaged documentation.\n\nOne thing that is unclear and which will really need close coordination with the author and developers of the Boost.Asio library would be as far as extending and customizing it for a specific platform or adding specific new functionality. This should be improved though but the good thing is it's looking like Asio will be a reference implementation for a standard library technical report (for an asynchronous IO library in the STL) in the future."
    },
    {
        "link": "https://stackoverflow.com/questions/56645320/stdmutex-best-practice",
        "document": "Since when an exception is thrown the only code is ensured to being run is the destructor, code like this might generate a memory leak\n\nIs it a best practice do something similar to this or might there is other better option? The idea is taking advantage of RAII in order to ensure than mutex will be freed if exception is thrown."
    },
    {
        "link": "https://johnfarrier.com/powerful-tips-and-techniques-for-stdmutex-in-c",
        "document": "Concurrency and parallelism have become vital aspects of modern C++ programming, demanding robust mechanisms for handling multi-threaded environments. Enter , a fundamental synchronization primitive in the C++ Standard Library. When utilized correctly, ensures that only one thread accesses a particular resource at any given time, preventing data races and ensuring thread safety.\n\nThis article is tailored for experienced C++ developers looking to enhance their understanding of . (And developers who want to build Technical Capital in their projects!) We will explore its core concepts, various types, and effective techniques for seamless integration into your applications. As multi-threading becomes increasingly prevalent in software development, mastering is essential for writing efficient and safe code.\n\nUnderstanding how to work with effectively can significantly improve the performance and reliability of your software. Let’s embark on this journey to unlock the full potential of in C++.\n\nA mutex, short for mutual exclusion, is a synchronization primitive used to control access to a shared resource by multiple threads in a concurrent programming environment. In C++, is provided by the standard library as a means to implement this mutual exclusion. By locking a mutex, a thread ensures that other threads attempting to lock the same mutex will be blocked until the mutex is unlocked.\n\nin C++ is designed to have exclusive, non-recursive ownership semantics. This means that once a thread has locked a , no other thread can lock it until it is explicitly unlocked by the owning thread. Unlike recursive mutexes, does not allow the same thread to lock it multiple times without first unlocking it. This exclusivity ensures that the critical section guarded by the mutex is accessed by only one thread at a time, preventing race conditions.\n\nImproper handling of can lead to undefined behavior, making it crucial to understand and correctly implement mutex usage. Common pitfalls include:\n• Double locking: Attempting to lock a that the current thread has already locked can cause a program to deadlock or exhibit undefined behavior.\n• Unlocking by a different thread: Only the thread that has locked a should unlock it. Unlocking a mutex from a different thread leads to undefined behavior.\n• Not unlocking a mutex: Failing to unlock a will result in other threads being blocked indefinitely when they try to acquire the same mutex.\n\nProper usage of is essential for writing safe and efficient concurrent programs in C++. By understanding its core characteristics and avoiding common mistakes, developers can ensure their applications remain robust and maintainable.\n\nTypes of Mutex in C++\n\nWhen it comes to synchronizing threads and ensuring safe access to shared resources in C++, the makes an appearance as a crucial tool. However, is not the only type available in the C++ Standard Library. Let’s explore the different types of mutexes provided by the library and their use cases.\n\nis the simplest and most commonly used mutex type. It provides exclusive, non-recursive ownership semantics. This means a thread must acquire a before accessing the shared resource and release it once done. Failing to release the mutex can result in deadlocks, preventing other threads from accessing the resource. Here’s an example of using :\n\nHere, ensures that and do not interleave their outputs.\n\nextends by offering the ability to attempt to lock the mutex for a specified period or until a specific point in time. This can be helpful in scenarios where a thread should not wait indefinitely to acquire a lock. Instead, it can perform other tasks if it fails to acquire the mutex within a given time frame. Here’s how you can use :\n\nIn this code, attempts to lock the mutex for one second.\n\nA allows the same thread to acquire the same mutex multiple times without causing a deadlock. This is useful in recursive code where a function that holds a mutex lock may need to call itself or another function that also tries to acquire the same mutex. Here’s an example:\n\nSimilar to , allows the same thread to lock the same mutex multiple times but also includes timed locking capabilities. This type combines the functionalities of both and . Let’s look at an example of its usage:\n\nThis example demonstrates attempting to lock a recursive mutex with a timeout period. By understanding these variants, you can choose the most suitable mutex type for your specific synchronization needs, enhancing the efficiency and reliability of your C++ applications.\n\nTypes of Locks in C++\n\nLocks are essential when working with to ensure thread safety and proper synchronization in concurrent C++ programming. Here, we explore the primary types of locks you can use with :\n\nThe is a simple, lightweight locking mechanism that provides a convenient way to manage the ownership of a mutex. When an instance of is created, it locks the mutex, ensuring that the current thread has exclusive access to the protected resource. When the instance goes out of scope, the destructor automatically releases the lock, preventing any possible resource leaks or deadlock situations.\n\noffers more flexibility compared to . While locks the mutex upon creation and releases it upon destruction, allows deferred locking, timed locking, and manual unlocking. This flexibility can be beneficial in complex scenarios where you might need to lock and unlock the mutex multiple times within the same scope.\n\nis used with shared mutexes (such as ), and allows multiple threads to hold the same mutex in a read-only mode simultaneously. This is particularly useful in scenarios where resources are read more frequently than they are modified. However, cannot be used with directly, requiring instead.\n\nUnderstanding these lock types enables developers to harness the full potential of and related synchronization primitives in C++. By choosing the proper lock based on the requirements, you can write efficient, thread-safe code that minimizes contention and maximizes performance.\n\nsimplifies managing multiple mutexes simultaneously by acquiring all the locks in a consistent order, preventing deadlocks. It locks the mutexes at the start of the scope and releases them automatically when the scope ends.\n\nA spin lock is a low-level synchronization primitive that keeps a thread in a busy-wait loop until it successfully acquires the lock. Unlike a traditional mutex, which may put a thread to sleep if the lock is unavailable, a spin lock continues “spinning” (repeatedly checking the lock status) until it can proceed. This can be more efficient for short critical sections where the wait time is minimal, as it avoids the overhead of context switching between threads. However, spin locks can become costly if contention is high, as they consume CPU cycles without performing useful work.\n\nWhen to Use Spin Locks\n\nSpin locks are ideal for scenarios where:\n• Critical sections are very short, and the lock is expected to be held for a brief time.\n• The overhead of suspending and resuming threads (as done with mutexes) would outweigh the cost of spinning.\n• High performance is required, and the risk of contention is low.\n\nHowever, they should be avoided in cases where:\n• There is high contention among threads, which could lead to performance degradation from excessive spinning.\n• Locks may be held for longer durations.\n\nHere, the function sets the lock and returns its previous value. If the lock was already set, the thread keeps spinning (busy-waiting) until it acquires the lock. After completing the critical section, the lock is released using , allowing other threads to proceed.\n• Suitable for real-time systems where thread suspension is undesirable.\n• Can waste CPU cycles if contention is high.\n• Not ideal for long critical sections or when multiple threads contend for the same resource.\n\n, introduced in C++20, provides a synchronization mechanism for coordinating threads in phases. It ensures that a group of threads must reach a specific point (the barrier) before any can continue. After all threads arrive, they proceed together, enabling phased parallel processing.\n\nUsing in conjunction with is a powerful technique for managing thread synchronization and communication. The allows threads to efficiently wait for and be notified of specific conditions. This setup is particularly useful for scenarios where threads must wait for certain conditions before proceeding.\n\nA is an object used to block one or more threads until another thread modifies a shared variable and notifies the condition variable. It is a synchronization primitive that enables blocking of threads but efficiently releases the CPU while waiting.\n\nTo use a , you need to associate it with an . Here is a simple example to illustrate the usage:\n\nIn this example, we see how is used to protect the shared variable. allows threads to wait until they are notified. is called by the threads to wait until is set to true. Finally, wakes all waiting threads once the condition is met.\n\nBenefits of Using std::mutex with std::condition_variable\n• Thread Efficiency: A condition variable allows threads to sleep while waiting, reducing CPU consumption significantly compared to busy-waiting.\n• Inter-Thread Communication: Facilitates communication between threads, enabling them to coordinate their actions based on shared states.\n• Avoid Spurious Wakes: Use a while-loop for checking the condition before proceeding after . This ensures that the thread only proceeds when the condition is actually met.\n• Minimal Scope: Hold the mutex lock for the minimal possible duration to minimize contention.\n• Clear Signaling: Use if only one thread needs to be awakened, and if multiple threads must proceed.\n\nIntegrating with enriches your multithreading toolkit. By mastering this technique, your C++ applications will be able to handle more complex synchronization scenarios efficiently and reliably.\n\nBest Practices and Tips Using std::mutex\n\nEffectively utilizing in C++ requires adhering to several best practices to ensure safe and efficient concurrency management. Below are some essential tips for using :\n\nA fundamental rule when using is to avoid deadlocks, which occur when two or more threads are stuck waiting for each other indefinitely. To prevent this:\n• Consistent Locking Order: Always lock multiple mutexes in a predefined order across all threads. This practice reduces the risk of circular dependencies.\n• Use std::lock(): The function can lock multiple objects simultaneously, minimizing the risk of deadlocks by avoiding intermediate states.\n\nAdhering to the RAII principle is crucial for managing resources like . RAII ensures that resources are properly released when they go out of scope:\n\nstd::lock_guard: Use for simple mutex locking. It locks the mutex upon construction and automatically releases it when the goes out of scope.\n\nstd::unique_lock: For more complex scenarios where you need to lock and unlock the mutex multiple times within a block, use . It provides more flexibility than .\n\nWhile is essential for thread synchronization, it can also introduce performance bottlenecks if not used correctly:\n\nMinimize Locked Code: Only protect the smallest possible critical section. Extensive locking can lead to contention, decreasing performance\n\nAvoid Unnecessary Locks: Ensure that only shared resources that need protection are surrounded by . Overuse of mutexes can serialize your code and degrade performance.\n\nImplementing these best practices when working with will help ensure safe, efficient, and performant multi-threaded applications in C++.\n\nMastering the use of is fundamental for developing robust multithreaded applications in C++. Proper usage of ensures that critical sections of code are executed atomically, thereby preventing race conditions and data corruption. By understanding the different types of mutexes and locks, such as and , developers can choose the most appropriate synchronization mechanism for their specific use case.\n\nIt’s essential to adhere to best practices when working with to avoid common pitfalls such as deadlocks and performance bottlenecks. Utilizing techniques like RAII (Resource Acquisition Is Initialization) can help manage the lifecycle of mutex locks efficiently and prevent resource leaks.\n\nAdditionally, integrating with can offer more sophisticated synchronization by allowing threads to wait for specific conditions to be met before proceeding. This combination is especially useful in producer-consumer scenarios and other complex multithreaded applications.\n\nBy consistently applying the strategies and techniques discussed, you can significantly improve the reliability and performance of your multithreaded C++ programs. Embrace these practices to make the most of and elevate your concurrency programming skills.\n\nLearn More about the C++ Standard Library! Boost your C++ knowledge with my new book: Data Structures and Algorithms with the C++ STL!"
    },
    {
        "link": "https://stackoverflow.com/questions/51372861/is-stdmutex-as-a-member-variable-thread-safe-for-multiple-threads",
        "document": "Is std::mutex as a member variable thread-safe for multiple threads?\n\nThere is an instance of a class has a variable and mutex as a member.\n\nEach function is called in different threads.\n\nI am curious that is it okay to use a mutex like this?\n\nThere are lots of example codes using a kind of wrapper class for mutex like guard or something.\n\nI prefer to use it simply and want to unlock explicitly. Not in destructing time."
    },
    {
        "link": "https://medium.com/@pauljlucas/advanced-thread-safety-in-c-4cbab821356e",
        "document": "C++ supports writing programs where parts of the code run concurrently using threads. When writing such programs, you have to take additional steps to ensure that data shared among threads can not cause race conditions. Typically, race conditions are avoided by proper use of mutexes and locks. However, in high-performance code, mutexes can sometimes be too costly, especially with high-contention data. C++ supports alternatives.\n\nNote: what follows will not describe how to create or join threads since those things are largely straightforward. Instead, what follows will describe how to write programs safely in a multithreaded program.\n\nIn the late 1960s, early 1970s, programs (running on computers such as the PDP-7 and later the PDP-11) were:\n• Single-threaded. (While nascent “threads” appeared in 1966, POSIX threads didn’t arrive until 1995.)\n• Executed sequentially. (That is, machine code instructions generated by compilers were executed one at a time and in the same order as the original statements in your program were written.)\n• Running in flat (non-hierarchical) memory. (CPU caches didn’t exist until the late 1970s and didn’t go mainstream until 1993 with the Intel Pentium.)\n\nIn order not to have overall performance constrained by memory, CPU designers had to employ mitigation tactics of:\n\nAll of these tactics are done entirely by the hardware. You as the programmer have no way either to know or really influence what’s going on under the hood.\n\nAdditionally, compiler implementors also employ their own mitigation tactics of:\n• … and many more.\n\nThe compiler is free to do any optimization so long as it makes no change to the observable behavior of your program (the “as-if rule”).\n\nAs long as your program has only a single thread, you can remain blissfully unaware of either the hardware or compiler tactics being employed. However, once you have multiple threads, you must become acutely aware of what’s going on under the hood.\n\nWhen doing multithreaded programming, you’ll hear the term “atomic” used. What, exactly, is meant by “atomic” anyway? There are three related, but distinct, meanings:\n• A value operation (read or write) completes with no possible intervening operation by another thread, e.g., writing single-byte values on any CPU, or 16- or 32-bit values on a 32-bit CPU, or 64-bit values on a 64-bit CPU.\n• An updated value is visible to other CPUs.\n• Multiple value operations complete with no possible intervening operation by another thread, e.g., updating A and B together (“transactional”).\n\n“Atomic” always means #1; it usually also means #2; or all three.\n\nConsider the following code where the main thread spawns another. Both threads run so long as remains . To shut down, the main thread sets to . In response, both loops will exit and the main thread will join the other thread. Unfortunately, the code is wrong. (More unfortunately, I’ve seen such code in production.)\n\nIt’s wrong because is shared by multiple threads improperly. Some programmers mistakenly think along the lines of:\n\nThe problem is that while that’s actually true, it’s insufficient. A is atomic only by meaning #1; it’s not atomic by meaning #2. That is, just because a is updated by one thread does not mean that updated value is visible to other threads (running on other CPUs).\n\nI’ve also seen code where programmers knew they had to do something more to make such code thread-safe. Unfortunately, that something turned out to be:\n\nThat is, they inserted because they kind-of understand what it does, but not what it’s for. The attempted use of to make something thread-safe is always wrong in C++.\n\nThe classic way to fix this code (correctly) would be to use a mutex:\n\nThe mutexes and locks make be atomic by meaning #1 and meaning #2. While not necessary for this program, mutexes and locks also make things atomic by meaning #3 as well, so they’re the most general, one-size-fits-all tool for making programs thread-safe. However, C++ offers alternative tools that provide a more custom fit especially useful in high-performance code.\n\nWriting safe multithreaded code even with ordinary mutexes and locks is hard to get right, even for experts. Writing safe multithreaded code using advanced thread-safety techniques is even harder! Before considering them, you should:\n• Know that using efficient algorithms matters far more than locking technique. For example, an N⋅log(N) algorithm with slower mutexes will still very likely be more performant than an N^2 algorithm with faster locking (or lock-free) techniques.\n• Measure to see if your code is spending too much time locking. (Remember: results are CPU-dependent!)\n• Only if locking takes a significant percentage of time, then consider the following techniques.\n\nAn Even Better Fix:\n\nWhile use of mutexes and locks is correct, they’re fairly heavy-weight. In the case of using a , a much lighter-weight fix is:\n\nUse of does not make atomic by meaning #1 (it already is); it makes it atomic by meaning #2.\n\nThe class is specialized for all built-in types ( , , , etc.) and all pointer types ( for any ). It can even be used for your own type , but only when:\n• is trivially copyable (that is, if would correctly copy ).\n\nWhen is an integral or pointer type, has conventional operators overloaded for it:\n\nThe operators are shorthands for the member functions shown in the comments. While the operators are convenient, I always use the member functions to make it obvious in code that the variable being manipulated is a .\n\nis not thread-safe because the read of and the write to are not atomic by meaning #3.\n\nEven though offers better performance over mutexes and locks, it’s not guaranteed to. For a type on a specific CPU under specific conditions, and check. Those conditions include:\n• Accessing unaligned values, e.g., reading an at a memory address that’s not evenly divisible by (assuming the CPU can even do it at all without generating a bus error).\n• Runtime CPU dispatching, e.g., a particular program during initialization may detect whether the CPU supports the instruction and, if not, may be forced to use locks.\n\nThe only type that’s guaranteed to be always lock-free on all CPUs under all conditions is that’s a special-case of . (An example using is shown later.)\n\nAtomic (by any meaning) is not enough to ensure thread-safety because the order of memory operations does not necessarily match the order of statements in a program. Why not? Because the hardware, the compiler, or both, may reorder things to improve performance. The “as-if” rule holds only from the perspective of a single thread.\n\nMemory barriers (aka, “fences”) help ensure thread-safety by selectively prohibiting reordering of memory operations across the barrier. They also provide some synchronization among threads. C++ provides .\n\nThe sometimes confusing thing about memory barriers is that they’re about controlling the order of operations, not the operations themselves.\n\nThis is the safest memory order which is why it’s the default. For example, the signature of is:\n\nIt’s also the least efficient because it establishes a global memory ordering (bottleneck) across all threads. In specific cases, more efficient memory orders can be used.\n\nThis is the least safe memory order in that it does not guarantee operation order or synchronization, but still guarantees modification order. What good is that? A typical use-case is incrementing reference counts:\n\nAt this point, you might ask something along the lines of:\n\nThe answer is no for two reasons:\n• Each increment is still atomic (by meanings #1 and #2) so each thread always sees the latest value. What does is allow the hardware or the compiler to reorder other operations having nothing to do with either before or after its increment to improve overall performance, not the performance of specifically.\n• Because is part of a private data structure, it’s guaranteed that no actions are ever conditionally taken by another thread based on its current value. Said another way, when only performing an increment and not altering the control flow in any thread, then use of is safe.\n\nAn example of when is not safe is when decrementing reference counts. (More later.)\n\nNow that you hopefully understand , you should never use it unless you can prove your use of it is correct and it actually significantly improves performance. Correct use of is very hard to do.\n\nThese memory orders are safer and always used in pairs:\n• (used with ) is used to “publish” information: no accesses can be reordered after.\n• (used with ) is used to “subscribe” to information: no accesses can be reordered before.\n\nIn this example, is shared among two threads and that acts as a semaphore:\n• The sets then signals that the data is ready (or “publishes” it). The write to “happens before” the write to because the use of guarantees that the write to can not be reordered after .\n• Meanwhile, the busy waits for the signal on , then safely reads . The read from “happens before” the read from because the use of guarantees that the read from can not be reordered before .\n• itself need not be atomic. (This is particularly useful for data that can not be made atomic either because it’s too big or is not trivially copyable.)\n• You can set any amount of data, then “publish” all of it simultaneously.\n\nAt this point, you might ask whether busy waiting is efficient. In general, it’s efficient when the waits are short — shorter than the time it would take to lock and unlock a mutex.\n\nA general spinlock (a kind of busy waiting) can be implemented using :\n\nThis memory order is a special case of that allows operations to be dependency-ordered that can be more efficient on weakly ordered CPUs (ARM, PowerPC; but not x86). From the earlier example:\n\nThere’s no actual dependency between and other than what’s in our minds — which the compiler has no way to know. To create an actual dependency that the compiler can use to keep things in dependency order, we can instead do:\n\nNow, we’ve created an actual dependency using a pointer and a pointed-to value in that the value of the pointer has to be loaded before the pointer is dereferenced. The compiler can preserve this order without using an additional and more expensive memory barrier.\n\nThis memory order is and combined into one used for read-modify-write operations: no accesses can be reordered before or after. A typical use-case is decrementing reference counts:\n\nThe reason can’t be used here is because a use of the reference counted object in another thread could be reordered to be after the happens resulting in undefined behavior.\n\nUsing with is atomic without a barrier; using is a barrier without a specific atomic. It’s useful if you want to update several atomic values together. For example, instead of doing:\n\nyou can do:\n\nBut there is another difference. Given:\n\nNo operations in (1) will be reordered after the store-release of into (2); however, store operations in (2) can be reordered before the release into (1).\n\nimposes stronger ordering guarantees than an operation on an atomic with the same memory order. Given:\n\nNo operations in (1) will be reordered after all subsequent store operations into (3) and store operations in (3) can not be reordered before the fence into (1). However, since controls store operations only, load operations in (2) can be reordered before the release into (1).\n\nThe rules for are similar except it controls load operations only and enforces ordering in the opposite direction. The differences are subtle. In general, an acquire or release associated with a particular atomic is preferred.\n\nCompare-and-swap (CAS), as its name suggests, is both compare and swap operations done together atomically (by all atomic meanings). Conceptually, it’s implemented as:\n\nexcept done atomically. The idea is that you check the value of an atomic variable and:\n• If it’s the value you expect (or hope for), then (and only then) set it to a new desired value; or:\n• If it’s not the value you expect (or hope for), it means some other thread changed the value, so do nothing. You are free to reattempt setting the value.\n\nThere are actually two flavors: and . (More on the difference later.)\n\nFor example, we can reimplement the previous class using CAS:\n\nUsing CAS, we check the value of :\n• If it’s , it means it’s currently unlocked, so set it to to indicate it’s now locked and return .\n• If it’s , it means it’s currently locked (by another thread), so do not set the value and return . Note that in this case, the first parameter of is an in/out parameter and it’s set to the atomic’s current value (here, ) even though we don’t care. This forces us to reset to prior to another attempt.\n\nOne of the primary use-cases for CAS is that it allows you to implement lock-free operations on data structures. For example, part of a lock-free implementation might be:\n\nAfter a new node is created, we try to update :\n• If it’s still equal to (the original value of ), update to point to using the memory order specified by the third argument.\n• If it’s not equal, it means another thread snuck in and updated to point to different new node, so do nothing and reattempt. (Note that has been updated to be the new pointing to the different node using the memory order specified by the fourth argument.)\n\nThe existence of implies there’s a — and there is. The difference between them is:\n• fails (returns ) only if the current value is not the expected value.\n• may also fail if there’s a “spurious failure.”\n\nA “spurious failure” is quirk on weakly ordered CPUs (e.g., ARM and PowerPC; but not x86) where the operation fails for reasons other than the value not being the expected value.\n\nSo if never fails spuriously, why does exist? Before answering that question, let’s look at a conceptual implementation of them both:\n\nAssume there’s only that implements a weak version of CAS:\n• , however, wraps it with a loop that effectively filters out spurious failures. The important thing to remember here is that there’s a loop.\n\nGiven that, the benefits of are:\n• Spurious failures tend not to happen all that often.\n• When your code is using a loop anyway, the weak version will yield better performance on weakly ordered CPUs (ARM, PowerPC; but not x86 — but it’s no worse).\n• Detects the ABA Problem (on weak CPUs) — more later.\n\nSo then why does exist?\n• If you have a loop only to filter out spurious failures, don’t: use .\n• But if you have a loop anyway, use .\n• However, if handling a spurious failure is expensive (for example, if you have to discard and reconstruct a new object), use .\n• But doesn’t detect the ABA Problem (more later).\n\nGiven all that, you might wonder when would you ever not have a loop for ? One example is implementing :\n\nThe ABA Problem can be illustrated as follows. Suppose thread 1 performs the following steps:\n• Read the same memory location again (value is still “A”).\n\nThe problem is suppose thread 2 performs the following steps while thread 1 is doing its step 2:\n• Write “B” to the same memory location.\n• Write “A” to the same memory location.\n\nBy the time thread 1 does its step 3, it reads “A” and believes nothing has changed — even though it has. You might now ask:\n\nThe answer is: sometimes it doesn’t — but sometimes it does.\n\nConsider the code from earlier (repeated here for convenience):\n\nand the following illustration:\n\nState (1) shows the initial conditions where A and B are nodes on the stack, (H) points to A, and (N) has its also point to A. The dotted box contains and that are being compared. State (3) shows the desired final state where points to and points to A.\n\nBut what if another thread sneaks in before the loop is entered, pushes a new node X shown by state (2), then immediately pops it? Both and still point to A, so will be set to — which is correct. In this case, the ABA Problem isn’t actually a problem.\n\nBut now consider what the code for might be:\n\nand the following illustration:\n\nState (1) shows the initial conditions where A, B, and C are nodes on the stack and (H) and (F) both point to A. The dotted box contains and that are being compared. State (5) shows the desired final state where points to B.\n\nBut what if another thread sneaks in before the loop is entered, pops A and B shown by state (2), then pushes A shown by state (3)? Both and still point to A, so will be set to — which is wrong! Why? Originally, pointed to A whose pointed to B, hence (the desired argument in the compare) is B. But B was deleted in (2) so we’ll end up in state (4) with being a dangling pointer to B that was deleted. In this case, the ABA Problem is actually a problem! (It wasn’t a problem for because the desired value of could never become stale.)\n\nEven worse, there’s no easy fix for this. In this case, the problem is that part of the desired value expression can change. While stays the same, what points to can change. Detecting ABA Problems is hard, even for experts.\n\nHow can this be fixed?\n• Give up and just use a mutex and locks.\n\nA versioned pointer is an ordinary pointer plus an additional “version number” such that every time the value of the pointer changes, the version number is incremented. Conceptually, something like:\n\nThen use instead of :\n\nThis would work because, even though the part of would still point to A, its part would be different, so and would not compare equal, so would not be set to B, the stale value of .\n\nThe caveats of , however, are:\n• This will work only if the CPU supports double-pointer-wide (16 bytes on a 64-bit CPU) CAS (which, for example, x86_64 does via the instruction).\n• As mentioned, a specialization of requires to be trivially copyable — which is why can’t be implemented.\n\nFor many CPUs, the L1 cache is “chunked” into cache lines typically ranging from 16–64K in size each. To read a given memory location from main memory into the cache, the location and the surrounding chunk-sized bytes are all read in together. Similarly, to write a given memory location from the cache into main memory, the entire cache line is written. For code that exhibits locality of reference, the chunking yields a performance gain. However, in some cases, it can yield a performance loss.\n\nAssume that your code has one thread pushing items onto the tail of the queue (repeatedly updating ) and a second thread popping items from the head of the queue (repeatedly updating ).\n\nIn the code as given, and will very likely reside on the same cache line. This means updating one will invalidate the entire cache line adding otherwise unnecessary contention for the other.\n\nIn a case such as this, you want and to reside on different cache lines so updating one doesn’t affect the other. You can actually achieve that by using and :\n\nThis will waste a little bit of memory, but ensure that and reside on different cache lines.\n• Your algorithm matters far more than locking technique.\n• Thread-safety in general and using atomics and barriers directly specifically is very hard to get right, even for experts.\n• Before using advanced thread-safety techniques, measure to see if your code is spending too much time locking. (Remember: results are CPU-dependent!)\n• Only if locking takes a significant percentage of time, then consider atomics and barriers.\n• Atomics and barriers are two sides of the same coin.\n• Be aware of the ABA Problem."
    },
    {
        "link": "https://reddit.com/r/cpp_questions/comments/150jvqf/how_to_make_c_thread_safe",
        "document": "I'm a novice that learned about smart pointers yesterday. But, I also heard that they're not \"thread safe\".\n\nIs there are way to make C++ thread safe with stuff from the standard library up to C++20?"
    }
]