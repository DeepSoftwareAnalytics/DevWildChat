[
    {
        "link": "https://medium.com/@shahbaz.gondal588/understanding-ecg-signal-processing-with-python-b9dd4ea68682",
        "document": "Electrocardiogram (ECG) signals are vital for diagnosing various heart conditions. Processing and analyzing ECG signals can provide valuable insights into cardiac health. In this blog post, we’ll explore a Python script that preprocesses ECG signals, detects QRS complexes, and calculates RR intervals.\n\nFor this ECG signal processing task, we’ll be using Python along with the following libraries:\n• SciPy: for signal processing functions such as filtering.\n• BioSPPy: a biosignal processing library that provides tools for ECG analysis.\n• Preprocessing ECG Signal: The function filters and downsamples the raw ECG signal to remove noise and reduce computational complexity.\n• Plotting ECG Signal: The function visualizes the processed ECG signal using Matplotlib, showing amplitude against time.\n• Detecting QRS Complexes: The function uses BioSPPy to detect QRS complexes in the ECG signal, identifying heartbeats.\n• Calculating RR Intervals: The function computes RR intervals (the time between successive heartbeats) from the detected QRS complexes.\n• Processing ECG Files: The function iterates through all ECG files in a specified directory, processes each file using the aforementioned functions, and prints insights about each ECG signal.\n• Filtering: Raw ECG signals often contain noise from various sources. Filtering techniques, such as Butterworth filters, are applied to remove noise and artifacts.\n• QRS Detection: QRS complexes represent the electrical activity associated with ventricular depolarization. Detecting QRS complexes is essential for identifying heartbeats in the ECG signal.\n• RR Interval Calculation: RR intervals are the time durations between successive QRS complexes. These intervals provide valuable information about heart rate variability and cardiac health.\n\nThe Python script presented in this blog post demonstrates the process of ECG signal processing, including preprocessing, QRS detection, and RR interval calculation. By leveraging Python libraries and signal processing techniques, researchers and healthcare professionals can analyze ECG signals to gain insights into cardiac function and diagnose heart conditions."
    },
    {
        "link": "https://docs.scipy.org/doc/scipy/reference/signal.html",
        "document": "Evaluate a cubic spline at the new set of points. Evaluate a quadratic spline at the new set of points.\n\nPerform an order filter on an N-D array. Implement a smoothing IIR filter with mirror-symmetric boundary conditions using a cascade of first-order sections. Implement a smoothing IIR filter with mirror-symmetric boundary conditions using a cascade of second-order sections. Filter data along one-dimension with an IIR or FIR filter. Construct initial conditions for lfilter given input and output vectors. Deconvolves out of using inverse filtering. Filter data along one dimension using cascaded second-order sections. Compute the envelope of a real- or complex-valued signal. Downsample the signal after applying an anti-aliasing filter. Remove linear or constant trend along axis from data. Resample x to num samples using Fourier method along the given axis. Resample x along the given axis using polyphase filtering.\n\nReturn zero, pole, gain (z, p, k) representation from a numerator, denominator representation of a linear filter. Return second-order sections from zeros, poles, and gain of a system Return zeros, poles, and gain of a series of second-order sections\n\nFor window functions, see the namespace. In the namespace, there is a convenience function to obtain these windows by name: Return a window of a given length and type.\n\nCalculate the prominence of each peak in a signal. Calculate the width of each peak in a signal.\n\nEstimate the magnitude squared coherence estimate, Cxy, of discrete-time signals X and Y using Welch's method. Determine the vector strength of the events corresponding to the given period. Check whether the Constant OverLap Add (COLA) constraint is met. Check whether the Nonzero Overlap Add (NOLA) constraint is met.\n\nCompute the frequency response around a spiral in the Z plane. Compute the DFT of x only for frequencies in range fn. Return the points at which the chirp z-transform is computed. The functions are simpler to use than the classes, but are less efficient when using the same transform on many arrays of the same length, since they repeatedly generate the same chirp signal with every call. In these cases, use the classes to create a reusable function instead."
    },
    {
        "link": "https://docs.scipy.org/doc/scipy/tutorial/signal.html",
        "document": "The signal processing toolbox currently contains some filtering functions, a limited set of filter design tools, and a few B-spline interpolation algorithms for 1- and 2-D data. While the B-spline algorithms could technically be placed under the interpolation category, they are included here because they only work with equally-spaced data and make heavy use of filter-theory and transfer-function formalism to provide a fast B-spline transform. To understand this section, you will need to understand that a signal in SciPy is an array of real or complex numbers.\n\nA B-spline is an approximation of a continuous function over a finite- domain in terms of B-spline coefficients and knot points. If the knot- points are equally spaced with spacing \\(\\Delta x\\), then the B-spline approximation to a 1-D function is the finite-basis expansion. In two dimensions with knot-spacing \\(\\Delta x\\) and \\(\\Delta y\\), the function representation is In these expressions, \\(\\beta^{o}\\left(\\cdot\\right)\\) is the space-limited B-spline basis function of order \\(o\\). The requirement of equally-spaced knot-points and equally-spaced data points, allows the development of fast (inverse-filtering) algorithms for determining the coefficients, \\(c_{j}\\), from sample-values, \\(y_{n}\\). Unlike the general spline interpolation algorithms, these algorithms can quickly find the spline coefficients for large images. The advantage of representing a set of samples via B-spline basis functions is that continuous-domain operators (derivatives, re- sampling, integral, etc.), which assume that the data samples are drawn from an underlying continuous function, can be computed with relative ease from the spline coefficients. For example, the second derivative of a spline is Using the property of B-splines that If \\(o=3\\), then at the sample points: Thus, the second-derivative signal can be easily calculated from the spline fit. If desired, smoothing splines can be found to make the second derivative less sensitive to random errors. The savvy reader will have already noticed that the data samples are related to the knot coefficients via a convolution operator, so that simple convolution with the sampled B-spline function recovers the original data from the spline coefficients. The output of convolutions can change depending on how the boundaries are handled (this becomes increasingly more important as the number of dimensions in the dataset increases). The algorithms relating to B-splines in the signal-processing subpackage assume mirror-symmetric boundary conditions. Thus, spline coefficients are computed based on that assumption, and data-samples can be recovered exactly from the spline coefficients by assuming them to be mirror-symmetric also. Currently the package provides functions for determining second- and third- order cubic spline coefficients from equally-spaced samples in one and two dimensions ( , , , ). For large \\(o\\), the B-spline basis function can be approximated well by a zero-mean Gaussian function with standard-deviation equal to \\(\\sigma_{o}=\\left(o+1\\right)/12\\) : A function to compute this Gaussian for arbitrary \\(x\\) and \\(o\\) is also available ( ). The following code and figure use spline-filtering to compute an edge-image (the second derivative of a smoothed spline) of a raccoon’s face, which is an array returned by the command . The command was used to apply a separable 2-D FIR filter with mirror-symmetric boundary conditions to the spline coefficients. This function is ideally-suited for reconstructing samples from spline coefficients and is faster than , which convolves arbitrary 2-D filters and allows for choosing mirror-symmetric boundary conditions. Alternatively, we could have done:\n\nFiltering is a generic name for any system that modifies an input signal in some way. In SciPy, a signal can be thought of as a NumPy array. There are different kinds of filters for different kinds of operations. There are two broad kinds of filtering operations: linear and non-linear. Linear filters can always be reduced to multiplication of the flattened NumPy array by an appropriate matrix resulting in another flattened NumPy array. Of course, this is not usually the best way to compute the filter, as the matrices and vectors involved may be huge. For example, filtering a \\(512 \\times 512\\) image with this method would require multiplication of a \\(512^2 \\times 512^2\\) matrix with a \\(512^2\\) vector. Just trying to store the \\(512^2 \\times 512^2\\) matrix using a standard NumPy array would require \\(68,719,476,736\\) elements. At 4 bytes per element this would require \\(256\\textrm{GB}\\) of memory. In most applications, most of the elements of this matrix are zero and a different method for computing the output of the filter is employed. Many linear filters also have the property of shift-invariance. This means that the filtering operation is the same at different locations in the signal and it implies that the filtering matrix can be constructed from knowledge of one row (or column) of the matrix alone. In this case, the matrix multiplication can be accomplished using Fourier transforms. Let \\(x\\left[n\\right]\\) define a 1-D signal indexed by the integer \\(n.\\) Full convolution of two 1-D signals can be expressed as This equation can only be implemented directly if we limit the sequences to finite-support sequences that can be stored in a computer, choose \\(n=0\\) to be the starting point of both sequences, let \\(K+1\\) be that value for which \\(x\\left[n\\right]=0\\) for all \\(n\\geq K+1\\) and \\(M+1\\) be that value for which \\(h\\left[n\\right]=0\\) for all \\(n\\geq M+1\\), then the discrete convolution expression is For convenience, assume \\(K\\geq M.\\) Then, more explicitly, the output of this operation is Thus, the full discrete convolution of two finite sequences of lengths \\(K+1\\) and \\(M+1\\), respectively, results in a finite sequence of length \\(K+M+1=\\left(K+1\\right)+\\left(M+1\\right)-1.\\) 1-D convolution is implemented in SciPy with the function . This function takes as inputs the signals \\(x,\\) \\(h\\), and two optional flags ‘mode’ and ‘method’, and returns the signal \\(y.\\) The first optional flag, ‘mode’, allows for the specification of which part of the output signal to return. The default value of ‘full’ returns the entire signal. If the flag has a value of ‘same’, then only the middle \\(K\\) values are returned, starting at \\(y\\left[\\left\\lfloor \\frac{M-1}{2}\\right\\rfloor \\right]\\), so that the output has the same length as the first input. If the flag has a value of ‘valid’, then only the middle \\(K-M+1=\\left(K+1\\right)-\\left(M+1\\right)+1\\) output values are returned, where \\(z\\) depends on all of the values of the smallest input from \\(h\\left[0\\right]\\) to \\(h\\left[M\\right].\\) In other words, only the values \\(y\\left[M\\right]\\) to \\(y\\left[K\\right]\\) inclusive are returned. The second optional flag, ‘method’, determines how the convolution is computed, either through the Fourier transform approach with or through the direct method. By default, it selects the expected faster method. The Fourier transform method has order \\(O(N\\log N)\\), while the direct method has order \\(O(N^2)\\). Depending on the big O constant and the value of \\(N\\), one of these two methods may be faster. The default value, ‘auto’, performs a rough calculation and chooses the expected faster method, while the values ‘direct’ and ‘fft’ force computation with the other two methods. The code below shows a simple example for convolution of 2 sequences: This same function can actually take N-D arrays as inputs and will return the N-D convolution of the two arrays, as is shown in the code example below. The same input flags are available for that case as well. Correlation is very similar to convolution except that the minus sign becomes a plus sign. Thus, is the (cross) correlation of the signals \\(y\\) and \\(x.\\) For finite-length signals with \\(y\\left[n\\right]=0\\) outside of the range \\(\\left[0,K\\right]\\) and \\(x\\left[n\\right]=0\\) outside of the range \\(\\left[0,M\\right],\\) the summation can simplify to Assuming again that \\(K\\geq M\\), this is The SciPy function implements this operation. Equivalent flags are available for this operation to return the full \\(K+M+1\\) length sequence (‘full’) or a sequence with the same size as the largest sequence starting at \\(w\\left[-K+\\left\\lfloor \\frac{M-1}{2}\\right\\rfloor \\right]\\) (‘same’) or a sequence where the values depend on all the values of the smallest sequence (‘valid’). This final option returns the \\(K-M+1\\) values \\(w\\left[M-K\\right]\\) to \\(w\\left[0\\right]\\) inclusive. The function can also take arbitrary N-D arrays as input and return the N-D convolution of the two arrays on output. When \\(N=2,\\) and/or can be used to construct arbitrary image filters to perform actions such as blurring, enhancing, and edge-detection for an image. Calculating the convolution in the time domain as above is mainly used for filtering when one of the signals is much smaller than the other ( \\(K\\gg M\\) ), otherwise linear filtering is more efficiently calculated in the frequency domain provided by the function . By default, estimates the fastest method using . If the filter function \\(w[n,m]\\) can be factored according to convolution can be calculated by means of the function . As an example, we consider a Gaussian filter which is often used for blurring. A general class of linear 1-D filters (that includes convolution filters) are filters described by the difference equation where \\(x\\left[n\\right]\\) is the input sequence and \\(y\\left[n\\right]\\) is the output sequence. If we assume initial rest so that \\(y\\left[n\\right]=0\\) for \\(n<0\\), then this kind of filter can be implemented using convolution. However, the convolution filter sequence \\(h\\left[n\\right]\\) could be infinite if \\(a_{k}\n\neq0\\) for \\(k\\geq1.\\) In addition, this general class of linear filter allows initial conditions to be placed on \\(y\\left[n\\right]\\) for \\(n<0\\) resulting in a filter that cannot be expressed using convolution. The difference equation filter can be thought of as finding \\(y\\left[n\\right]\\) recursively in terms of its previous values Often, \\(a_{0}=1\\) is chosen for normalization. The implementation in SciPy of this general difference equation filter is a little more complicated than would be implied by the previous equation. It is implemented so that only one signal needs to be delayed. The actual implementation equations are (assuming \\(a_{0}=1\\) ): where \\(K=\\max\\left(N,M\\right).\\) Note that \\(b_{K}=0\\) if \\(K>M\\) and \\(a_{K}=0\\) if \\(K>N.\\) In this way, the output at time \\(n\\) depends only on the input at time \\(n\\) and the value of \\(z_{0}\\) at the previous time. This can always be calculated as long as the \\(K\\) values \\(z_{0}\\left[n-1\\right]\\ldots z_{K-1}\\left[n-1\\right]\\) are computed and stored at each time step. The difference-equation filter is called using the command in SciPy. This command takes as inputs the vector \\(b,\\) the vector, \\(a,\\) a signal \\(x\\) and returns the vector \\(y\\) (the same length as \\(x\\) ) computed using the equation given above. If \\(x\\) is N-D, then the filter is computed along the axis provided. If desired, initial conditions providing the values of \\(z_{0}\\left[-1\\right]\\) to \\(z_{K-1}\\left[-1\\right]\\) can be provided or else it will be assumed that they are all zero. If initial conditions are provided, then the final conditions on the intermediate variables are also returned. These could be used, for example, to restart the calculation in the same state. Sometimes, it is more convenient to express the initial conditions in terms of the signals \\(x\\left[n\\right]\\) and \\(y\\left[n\\right].\\) In other words, perhaps you have the values of \\(x\\left[-M\\right]\\) to \\(x\\left[-1\\right]\\) and the values of \\(y\\left[-N\\right]\\) to \\(y\\left[-1\\right]\\) and would like to determine what values of \\(z_{m}\\left[-1\\right]\\) should be delivered as initial conditions to the difference-equation filter. It is not difficult to show that, for \\(0\\leq m<K,\\) Using this formula, we can find the initial-condition vector \\(z_{0}\\left[-1\\right]\\) to \\(z_{K-1}\\left[-1\\right]\\) given initial conditions on \\(y\\) (and \\(x\\) ). The command performs this function. As an example, consider the following system: The code calculates the signal \\(y[n]\\) for a given signal \\(x[n]\\); first for initial conditions \\(y[-1] = 0\\) (default case), then for \\(y[-1] = 2\\) by means of . Note that the output signal \\(y[n]\\) has the same length as the length as the input signal \\(x[n]\\). Linear system described a linear-difference equation can be fully described by the coefficient vectors \\(a\\) and \\(b\\) as was done above; an alternative representation is to provide a factor \\(k\\), \\(N_z\\) zeros \\(z_k\\) and \\(N_p\\) poles \\(p_k\\), respectively, to describe the system by means of its transfer function \\(H(z)\\), according to This alternative representation can be obtained with the scipy function ; the inverse is provided by . For the above example we have i.e., the system has a zero at \\(z=-1/2\\) and a pole at \\(z=1/3\\). The scipy function allows calculation of the frequency response of a system described by the coefficients \\(a_k\\) and \\(b_k\\). See the help of the function for a comprehensive example. Time-discrete filters can be classified into finite response (FIR) filters and infinite response (IIR) filters. FIR filters can provide a linear phase response, whereas IIR filters cannot. SciPy provides functions for designing both types of filters. The function designs filters according to the window method. Depending on the provided arguments, the function returns different filter types (e.g., low-pass, band-pass…). The example below designs a low-pass and a band-stop filter, respectively. Note that uses, per default, a normalized frequency defined such that the value \\(1\\) corresponds to the Nyquist frequency, whereas the function is defined such that the value \\(\\pi\\) corresponds to the Nyquist frequency. The function allows design of almost arbitrary frequency responses by specifying an array of corner frequencies and corresponding gains, respectively. The example below designs a filter with such an arbitrary amplitude response. Note the linear scaling of the y-axis and the different definition of the Nyquist frequency in and (as explained above). SciPy provides two functions to directly design IIR and , where the filter type (e.g., elliptic) is passed as an argument and several more filter design functions for specific filter types, e.g., . The example below designs an elliptic low-pass filter with defined pass-band and stop-band ripple, respectively. Note the much lower filter order (order 4) compared with the FIR filters from the examples above in order to reach the same stop-band attenuation of \\(\\approx 60\\) dB. It is important to note that the cutoffs for and are defined differently. For , the cutoff-frequency is at half-amplitude (i.e. -6dB). For , the cutoff is at half-power (i.e. -3dB). Filter coefficients can be stored in several different formats: Functions, such as and , can convert between them. The or format is a 2-tuple representing a transfer function, where b is a length array of coefficients of the M-order numerator polynomial, and a is a length array of coefficients of the N-order denominator, as positive, descending powers of the transfer function variable. So the tuple of \\(b = [b_0, b_1, ..., b_M]\\) and \\(a =[a_0, a_1, ..., a_N]\\) can represent an analog filter of the form: or a discrete-time filter of the form: This “positive powers” form is found more commonly in controls engineering. If M and N are equal (which is true for all filters generated by the bilinear transform), then this happens to be equivalent to the “negative powers” discrete-time form preferred in DSP: Although this is true for common filters, remember that this is not true in the general case. If M and N are not equal, the discrete-time transfer function coefficients must first be converted to the “positive powers” form before finding the poles and zeros. This representation suffers from numerical error at higher orders, so other formats are preferred when possible. The format is a 3-tuple , where z is an M-length array of the complex zeros of the transfer function \\(z = [z_0, z_1, ..., z_{M-1}]\\), p is an N-length array of the complex poles of the transfer function \\(p = [p_0, p_1, ..., p_{N-1}]\\), and k is a scalar gain. These represent the digital transfer function: Although the sets of roots are stored as ordered NumPy arrays, their ordering does not matter: is the same filter as . The format is a 4-tuple of arrays representing the state-space of an N-order digital/discrete-time system of the form: or a continuous/analog system of the form: with P inputs, Q outputs and N state variables, where:\n• None y is the output vector of length Q\n• None u is the input vector of length P\n• None A is the state matrix, with shape\n• None B is the input matrix with shape\n• None C is the output matrix with shape\n• None D is the feedthrough or feedforward matrix with shape . (In cases where the system does not have a direct feedthrough, all values in D are zero.) State-space is the most general representation and the only one that allows for multiple-input, multiple-output (MIMO) systems. There are multiple state-space representations for a given transfer function. Specifically, the “controllable canonical form” and “observable canonical form” have the same coefficients as the representation, and, therefore, suffer from the same numerical errors. The format is a single 2-D array of shape , representing a sequence of second-order transfer functions which, when cascaded in series, realize a higher-order filter with minimal numerical error. Each row corresponds to a second-order representation, with the first three columns providing the numerator coefficients and the last three providing the denominator coefficients: The coefficients are typically normalized, such that \\(a_0\\) is always 1. The section order is usually not important with floating-point computation; the filter output will be the same, regardless of the order. The IIR filter design functions first generate a prototype analog low-pass filter with a normalized cutoff frequency of 1 rad/sec. This is then transformed into other frequencies and band types using the following substitutions: Here, \\(\\omega_0\\) is the new cutoff or center frequency, and \\(\\mathrm{BW}\\) is the bandwidth. These preserve symmetry on a logarithmic frequency axis. To convert the transformed analog filter into a digital filter, the transform is used, which makes the following substitution: where T is the sampling time (the inverse of the sampling frequency). The signal processing package provides many more filters as well. A median filter is commonly applied when noise is markedly non-Gaussian or when it is desired to preserve edges. The median filter works by sorting all of the array pixel values in a rectangular region surrounding the point of interest. The sample median of this list of neighborhood pixel values is used as the value for the output array. The sample median is the middle-array value in a sorted list of neighborhood values. If there are an even number of elements in the neighborhood, then the average of the middle two values is used as the median. A general purpose median filter that works on N-D arrays is . A specialized version that works only for 2-D arrays is available as . A median filter is a specific example of a more general class of filters called order filters. To compute the output at a particular pixel, all order filters use the array values in a region surrounding that pixel. These array values are sorted and then one of them is selected as the output value. For the median filter, the sample median of the list of array values is used as the output. A general-order filter allows the user to select which of the sorted values will be used as the output. So, for example, one could choose to pick the maximum in the list or the minimum. The order filter takes an additional argument besides the input array and the region mask that specifies which of the elements in the sorted list of neighbor array values should be used as the output. The command to perform an order filter is . The Wiener filter is a simple deblurring filter for denoising images. This is not the Wiener filter commonly described in image-reconstruction problems but, instead, it is a simple, local-mean filter. Let \\(x\\) be the input signal, then the output is where \\(m_{x}\\) is the local estimate of the mean and \\(\\sigma_{x}^{2}\\) is the local estimate of the variance. The window for these estimates is an optional input parameter (default is \\(3\\times3\\) ). The parameter \\(\\sigma^{2}\\) is a threshold noise parameter. If \\(\\sigma\\) is not given, then it is estimated as the average of the local variances. The Hilbert transform constructs the complex-valued analytic signal from a real signal. For example, if \\(x=\\cos\\omega n\\), then \\(y=\\textrm{hilbert}\\left(x\\right)\\) would return (except near the edges) \\(y=\\exp\\left(j\\omega n\\right).\\) In the frequency domain, the hilbert transform performs where \\(H\\) is \\(2\\) for positive frequencies, \\(0\\) for negative frequencies, and \\(1\\) for zero-frequencies. The functions , , and the filter design functions for specific filter types (e.g., ) all have a flag analog, which allows the design of analog filters as well. The example below designs an analog (IIR) filter, obtains via the poles and zeros and plots them in the complex s-plane. The zeros at \\(\\omega \\approx 150\\) and \\(\\omega \\approx 300\\) can be clearly seen in the amplitude response. \\[% LaTeX Macros to make the LaTeX formulas more readable: \n\newcommand{\\IC}{{\\mathbb{C}}} % set of complex numbers \n\newcommand{\\IN}{{\\mathbb{N}}} % set of natural numbers \n\newcommand{\\IR}{{\\mathbb{R}}} % set of real numbers \n\newcommand{\\IZ}{{\\mathbb{Z}}} % set of integers \n\newcommand{\\jj}{{\\mathbb{j}}} % imaginary unit \n\newcommand{\\e}{\\operatorname{e}} % Euler's number \n\newcommand{\\dd}{\\operatorname{d}} % infinitesimal operator \n\newcommand{\\abs}[1]{\\left|#1\\right|} % absolute value \n\newcommand{\\conj}[1]{\\overline{#1}} % complex conjugate \n\newcommand{\\conjT}[1]{\\overline{#1^T}} % transposed complex conjugate \n\newcommand{\\inv}[1]{\\left(#1\\right)^{\\!-1}} % inverse % Since the physics package is not loaded, we define the macros ourselves: \n\newcommand{\\vb}[1]{\\mathbf{#1}} % vectors and matrices are bold % new macros: \n\newcommand{\\rect}{\\operatorname{rect}} % rect or boxcar function \n\newcommand{\\sinc}{\\operatorname{sinc}} % sinc(t) := sin(pi*t) / (pi*t)\\]\n\nSpectral analysis refers to investigating the Fourier transform of a signal. Depending on the context, various names, like spectrum, spectral density or periodogram exist for the various spectral representations of the Fourier transform. This section illustrates the most common representations by the example of a continuous-time sine wave signal of fixed duration. Then the use of the discrete Fourier transform on a sampled version of that sine wave is discussed. Separate subsections are devoted to the spectrum’s phase, estimating the power spectral density without ( ) and with averaging ( ) as well for non-equally spaced signals ( ). Note that the concept of Fourier series is closely related but differs in a crucial point: Fourier series have a spectrum made up of discrete-frequency harmonics, while in this section the spectra are continuous in frequency. Consider a sine signal with amplitude \\(a\\), frequency \\(f_x\\) and duration \\(\\tau\\), i.e., Since the \\(\\rect(t)\\) function is one for \\(|t|<1/2\\) and zero for \\(|t|>1/2\\), it limits \\(x(t)\\) to the interval \\([0, \\tau]\\). Expressing the sine by complex exponentials shows its two periodic components with frequencies \\(\\pm f_x\\). We assume \\(x(t)\\) to be a voltage signal, so it has the unit \\(\\text{V}\\). In signal processing the integral of the absolute square \\(|x(t)|^2\\) is utilized to define energy and power of a signal, i.e., The power \\(P_x\\) can be interpreted as the energy \\(E_x\\) per unit time interval. Unit-wise, integrating over \\(t\\) results in multiplication with seconds. Hence, \\(E_x\\) has unit \\(\\text{V}^2\\text{s}\\) and \\(P_x\\) has the unit \\(\\text{V}^2\\). results in two \\(\\sinc(f) := \\sin(\\pi f) /(\\pi f)\\) functions centered at \\(\\pm f_x\\). The magnitude (absolute value) \\(|X(f)|\\) has two maxima located at \\(\\pm f_x\\) with value \\(|a|\\tau/2\\). It can be seen in the plot below that \\(X(f)\\) is not concentrated around the main lobes at \\(\\pm f_x\\), but contains side lobes with heights decreasing proportional to \\(1/(\\tau f)\\). This so-called “spectral leakage” is caused by confining the sine to a finite interval. Note that the shorter the signal duration \\(\\tau\\) is, the higher the leakage. To be independent of the signal duration, the so-called “amplitude spectrum” \\(X(f)/\\tau\\) can be used instead of the spectrum \\(X(f)\\). Its value at \\(f\\) corresponds to the amplitude of the complex exponential \\(\\exp(\\jj2\\pi f t)\\). Due to Parseval’s theorem, the energy can be calculated from its Fourier transform \\(X(f)\\) by as well. E.g., it can be shown by direct calculation that the energy of \\(X(f)\\) of Eq. (4) is \\(|a|^2\\tau/2\\). Hence, the signal’s power in a frequency band \\([f_a, f_b]\\) can be determined with Thus the function \\(|X(f)|^2\\) can be defined as the so-called “energy spectral density and \\(S_{xx}(f) := |X(f)|^2 / \\tau\\) as “power spectral density” (PSD) of \\(x(t)\\). Instead of the PSD, the so-called “amplitude spectral density” \\(X(f) / \\sqrt{\\tau}\\) is also used, which still contains the phase information. Its absolute square is the PSD and thus it is closely related to the concept of the root-mean-square (RMS) value \\(\\sqrt{P_x}\\) of a signal. In summary, this subsection presented five ways to represent a spectrum: of Eq. : Comparison of Spectral Representations of sine signalof Eq. (1) with unit # Note that the units presented in the table above are not unambiguous, e.g., \\(\\text{V}^2\\text{s} / \\text{Hz} = \\text{V}^2\\text{s}^2 = \\text{V}^2/ \\text{Hz}^2\\). When using the absolute value of \\(|X(f) / \\tau|\\) of the amplitude spectrum, it is called a magnitude spectrum. Furthermore, note that the naming scheme of the representations is not consistent and varies in literature. For real-valued signals the so-called “onesided” spectral representation is often utilized. It only uses the non-negative frequencies (due to \\(X(-f)= \\conj{X}(f)\\) if \\(x(t)\\in\\IR\\)). Sometimes the values of the negative frequencies are added to their positive counterparts. Then the amplitude spectrum allows to read off the full (not half) amplitude sine of \\(x(t)\\) at \\(f_x\\) and the area of an interval in the PSD represents its full (not half) power. Note that for amplitude spectral densities the positive values are not doubled but multiplied by \\(\\sqrt{2}\\), since it is the square root of the PSD. Furthermore, there is no canonical way for naming a doubled spectrum. The following plot shows three different spectral representations of four sine signals \\(x(t)\\) of Eq. (1) with different amplitudes \\(a\\) and durations \\(\\tau\\). For less clutter, the spectra are centered at \\(f_x\\) and being are plotted next to each other: Note that depending on the representation, the height of the peaks vary. Only the interpretation of the magnitude spectrum is straightforward: The peak at \\(f_x\\) in the second plot represents half the magnitude \\(|a|\\) of the sine signal. For all other representations the duration \\(\\tau\\) needs to be taken into account to extract information about the signal’s amplitude. In practice sampled signals are widely used. I.e., the signal is represented by \\(n\\) samples \\(x_k := x(kT)\\), \\(k=0, \\ldots, n-1\\), where \\(T\\) is the sampling interval, \\(\\tau:=nT\\) the signal’s duration and \\(f_S := 1/T\\) the sampling frequency. Note that the continuous signal needs to be band-limited to \\([-f_S/2, f_S/2]\\) to avoid aliasing, with \\(f_S/2\\) being called Nyquist frequency. Replacing the integral by a sum to calculate the signal’s energy and power, i.e., delivers the identical result as in the continuous time case of Eq. (2). The discrete Fourier transform (DFT) and its inverse (as implemented using efficient FFT calculations in the module) is given by The DFT and can be interpreted as an unscaled sampled version of the continuous Fourier transform of Eq. (3), i.e., The following plot shows the magnitude spectrum of two sine signals with unit amplitude and frequencies of 20 Hz and 20.5 Hz. The signal is made up of \\(n=100\\) samples with a sampling interval of \\(T=10\\) ms resulting in a duration of \\(\\tau=1\\) s and a sampling frequency of \\(f_S=100\\) Hz. The interpretation of the 20 Hz signal seems straightforward: All values are zero except at 20 Hz. There it is 0.5, which corresponds to half the amplitude of the input signal in accordance with Eq. (1). The peak of the 20.5 Hz signal on the other hand is dispersed along the frequency axis. Eq. (3) shows that this difference is caused by the reason that 20 Hz is a multiple of the bin width of 1 Hz whereas 20.5 Hz is not. The following plot illustrates this by overlaying continuous spectrum over the sampled one: That a slight variation in frequency produces significantly different looking magnitude spectra is obviously undesirable behavior for many practical applications. The following two common techniques can be utilized to improve a spectrum: The so-called “zero-padding” decreases \\(\\Delta f\\) by appending zeros to the end of the signal. To oversample the frequency q times, pass the parameter to the / function with being the length of the input signal. The second technique is called windowing, i.e., multiplying the input signal with a suited function such that typically the secondary lobes are suppressed at the cost of widening the main lobe. The windowed DFT can be expressed as where \\(w_k\\), \\(k=0,\\ldots,n-1\\) is the sampled window function. To calculate the sampled versions of the spectral representations given in the previous subsection, the following normalization constants need to be utilized. The first one ensures that a peak in the spectrum is consistent with the signal’s amplitude at that frequency. E.g., the magnitude spectrum can be expressed by \\(|X^w_l / c^\\text{amp}|\\). The second constant guarantees that the power of a frequency interval as defined in Eq. (5) is consistent. The absolute values are needed since complex-valued windows are not forbidden. The following plot shows the result of applying a window and three times over-sampling to \\(x(t)\\): Now both lobes look almost identical and the side lobes are well suppressed. The maximum of the 20.5 Hz spectrum is also very close to the expected height of one half. Spectral energy and spectral power can be calculated analogously to Eq. (4), yielding in identical results, i.e., This formulation is not to be confused with the special case of a rectangular window (or no window), i.e., \\(w_k = 1\\), \\(X^w_l=X_l\\), \\(c^\\text{den}=\\sqrt{n}\\), which results in is defined over the frequency range \\([0, f_S)\\) and can be interpreted as power per frequency interval \\(\\Delta f\\). Integrating over a frequency band \\([l_a\\Delta f, l_b\\Delta f)\\), like in Eq. (5), becomes the sum The windowed frequency-discrete energy spectral density \\(\\tau S^w_{xx}\\) can be defined analogously. The discussion above shows that sampled versions of the spectral representations as in the continuous-time case can be defined. The following tables summarizes these: of Eq. : Comparison of Spectral Representations of a windowed DFTof Eq. (6) for a sampled signal with unit # Note that for the densities, the magnitude values at \\(\\pm f_x\\) differ to the continuous time case due the change from integration to summation for determining spectral energy/power. Though the hann window is the most common window function used in spectral analysis, other windows exist. The following plot shows the magnitude spectrum of various window functions of the submodule. It may be interpreted as the lobe shape of a single frequency input signal. Note that only the right half is shown and the \\(y\\)-axis is in decibel, i.e., it is logarithmically scaled. # number of samples without and with zero-padding This plot shows that the choice of window function is typically a trade-off between width of the main lobes and the height of the side lobes. Note that the window corresponds to a \\(\\rect\\) function, i.e., to no windowing. Furthermore, many of the depicted windows are more frequently used in filter design than in spectral analysis. The phase (i.e., ) of the Fourier transform is typically utilized for investigating the time delay of the spectral components of a signal passing through a system like a filter. In the following example the standard test signal, an impulse with unit power, is passed through a simple filter, which delays the input by three samples. The input consists of \\(n=50\\) samples with sampling interval \\(T = 1\\) s. The plot shows magnitude and phase over frequency of the input and the output signal: The input has a unit magnitude and zero-phase Fourier transform, which is the reason for the use as a test signal. The output has also unit magnitude but a linearly falling phase with a slope of \\(-6\\pi\\). This is expected, since delaying a signal \\(x(t)\\) by \\(\\Delta t\\) produces an additional linear phase term in its Fourier transform, i.e., Note that in the plot the phase is not limited to the interval \\((+\\pi, \\pi]\\) (output of ) and hence does not have any discontinuities. This is achieved by utilizing the function. If the transfer function of the filter is known, can be used to determine the spectral response of a filter directly. The function calculates a power spectral density ( ) or a squared magnitude spectrum ( ). To obtain a smoothed periodogram, the function can be used. It does the smoothing by dividing the input signal into overlapping segments, to then calculate the windowed DFT of each segment. The result is to the average of those DFTs. The example below shows the squared magnitude spectrum and the power spectral density of a signal made up of a \\(1.27\\,\\text{kHz}\\) sine signal with amplitude \\(\\sqrt{2}\\,\\text{V}\\) and additive gaussian noise having a spectral power density with mean of \\(10^{-3}\\,\\text{V}^2/\\text{Hz}\\). The plots shows that the function produces a much smoother noise floor at the expense of the frequency resolution. Due to the smoothing the height of sine’s lobe is wider and not as high as in the periodogram. The left plot can be used to read the height of the lobe, i.e., half sine’s squared magnitude of \\(1\\,\\text{V}^2\\). The right plot can be used to determine the noise floor of \\(10^{-3}\\,\\text{V}^2/\\text{Hz}\\). Note that the lobe height of the averaged squared magnitude spectrum is not exactly one due to limited frequency resolution. Either zero-padding (e.g., passing to ) or reducing the number of segments by increasing the segment length (setting parameter ) could be utilized to increase the number of frequency bins. Least-squares spectral analysis (LSSA) is a method of estimating a frequency spectrum, based on a least-squares fit of sinusoids to data samples, similar to Fourier analysis. Fourier analysis, the most used spectral method in science, generally boosts long-periodic noise in long-gapped records; LSSA mitigates such problems. The Lomb-Scargle method performs spectral analysis on unevenly-sampled data and is known to be a powerful way to find, and test the significance of, weak periodic signals. For a time series comprising \\(N_{t}\\) measurements \\(X_{j}\\equiv X(t_{j})\\) sampled at times \\(t_{j}\\), where \\((j = 1, \\ldots, N_{t})\\), assumed to have been scaled and shifted, such that its mean is zero and its variance is unity, the normalized Lomb-Scargle periodogram at frequency \\(f\\) is Here, \\(\\omega \\equiv 2\\pi f\\) is the angular frequency. The frequency-dependent time offset \\(\\tau\\) is given by The function calculates the periodogram using a slightly modified algorithm created by Zechmeister and Kürster , which allows for the weighting of individual samples and calculating an unknown offset (also called a “floating-mean”) for each frequency independently.\n\nThis section gives some background information on using the class: The short-time Fourier transform (STFT) can be utilized to analyze the spectral properties of signals over time. It divides a signal into overlapping chunks by utilizing a sliding window and calculates the Fourier transform of each chunk. For a continuous-time complex-valued signal \\(x(t)\\) the STFT is defined as where \\(w(t)\\) is a complex-valued window function with its complex conjugate being \\(\\conj{w(t)}\\). It can be interpreted as determining the scalar product of \\(x\\) with the window \\(w\\) which is translated by the time \\(t\\) and then modulated (i.e., frequency-shifted) by the frequency \\(f\\). For working with sampled signals \\(x[k] := x(kT)\\), \\(k\\in\\IZ\\), with sampling interval \\(T\\) (being the inverse of the sampling frequency ), the discrete version, i.e., only evaluating the STFT at discrete grid points \\(S[q, p] := S(q \\Delta f, p\\Delta t)\\), \\(q,p\\in\\IZ\\), needs to be used. It can be formulated as with p representing the time index of \\(S\\) with time interval \\(\\Delta t := h T\\), \\(h\\in\\IN\\) (see ), which can be expressed as the size of \\(h\\) samples. \\(q\\) represents the frequency index of \\(S\\) with step size \\(\\Delta f := 1 / (N T)\\) (see ), which makes it FFT compatible. \\(w[m] := w(mT)\\), \\(m\\in\\IZ\\) is the sampled window function. To be more aligned to the implementation of , it makes sense to reformulate Eq. (7) as a two-step process:\n• None Extract the \\(p\\)-th slice by windowing with the window \\(w[m]\\) made up of \\(M\\) samples (see ) centered at \\(t[p] := p \\Delta t = h T\\) (see ), i.e., where the integer \\(\\lfloor M/2\\rfloor\\) represents , i.e., it is the mid point of the window ( ). For notational convenience, \\(x[k]:=0\\) for \\(k\n\not\\in\\{0, 1, \\ldots, N-1\\}\\) is assumed. In the subsection Sliding Windows the indexing of the slices is discussed in more detail.\n• None Then perform a discrete Fourier transform (i.e., an FFT) of \\(x_p[m]\\). Note that a linear phase \\(\\phi_m\\) (see ) can be specified, which corresponds to shifting the input by \\(\\phi_m\\) samples. The default is \\(\\phi_m = \\lfloor M/2\\rfloor\\) (corresponds per definition to ), which suppresses linear phase components for unshifted signals. Furthermore, the FFT may be oversampled by padding \\(w[m]\\) with zeros. This can be achieved by specifying to be larger than the window length —this sets \\(M\\) to (implying that also \\(w[m]:=0\\) for \\(m\n\not\\in\\{0, 1, \\ldots, M-1\\}\\) holds). The inverse short-time Fourier transform ( ) is implemented by reversing these two steps:\n• None Sum the shifted slices weighted by \\(w_d[m]\\) to reconstruct the original signal, i.e., for \\(k \\in [0, \\ldots, n-1]\\). \\(w_d[m]\\) is the so-called canonical dual window of \\(w[m]\\) and is also made up of \\(M\\) samples. Note that an inverse STFT does not necessarily exist for all windows and hop sizes. For a given window \\(w[m]\\) the hop size \\(h\\) must be small enough to ensure that every sample of \\(x[k]\\) is touched by a non-zero value of at least one window slice. This is sometimes referred as the “non-zero overlap condition” (see ). Some more details are given in the subsection Inverse STFT and Dual Windows. This subsection discusses how the sliding window is indexed in the by means of an example: Consider a window of length 6 with a interval of two and a sampling interval of one, e.g., . The following image schematically depicts the first four window positions also named time slices: The x-axis denotes the time \\(t\\), which corresponds to the sample index k indicated by the bottom row of blue boxes. The y-axis denotes the time slice index \\(p\\). The signal \\(x[k]\\) starts at index \\(k=0\\) and is marked by a light blue background. Per definition the zeroth slice (\\(p=0\\)) is centered at \\(t=0\\). The center of each slice ( ), here being the sample , is marked by the text “mid”. By default the calculates all slices which have some overlap with the signal. Hence the first slice is at = -1 with the lowest sample index being = -5. The first sample index unaffected by a slice not sticking out to the left of the signal is \\(p_{lb} = 2\\) and the first sample index unaffected by border effects is \\(k_{lb} = 5\\). The property returns the tuple \\((k_{lb}, p_{lb})\\). The behavior at the end of the signal is depicted for a signal with \\(n=50\\) samples below, as indicated by the blue background: Here the last slice has index \\(p=26\\). Hence, following Python convention of the end index being outside the range, = 27 indicates the first slice not touching the signal. The corresponding sample index is = 55. The first slice, which sticks out to the right is \\(p_{ub} = 24\\) with its first sample at \\(k_{ub}=45\\). The function returns the tuple \\((k_{ub}, p_{ub})\\). The term dual window stems from frame theory where a frame is a series expansion which can represent any function in a given Hilbert space. There the expansions \\(\\{g_k\\}\\) and \\(\\{h_k\\}\\) are dual frames if for all functions \\(f\\) in the given Hilbert space \\(\\mathcal{H}\\) holds, where \\(\\langle ., .\\rangle\\) denotes the scalar product of \\(\\mathcal{H}\\). All frames have dual frames . An STFT evaluated only at discrete grid points \\(S(q \\Delta f, p\\Delta t)\\) is called a “Gabor frame” in literature . Since the support of the window \\(w[m]\\) is limited to a finite interval, the falls into the class of the so-called “painless non-orthogonal expansions” . In this case the dual windows always have the same support and can be calculated by means of inverting a diagonal matrix. A rough derivation only requiring some understanding of manipulating matrices will be sketched out in the following: Since the STFT given in Eq. (7) is a linear mapping in \\(x[k]\\), it can be expressed in vector-matrix notation. This allows us to express the inverse via the formal solution of the linear least squares method (as in ), which leads to a beautiful and simple result. We begin by reformulating the windowing of Eq. (8) where the \\(M\\times N\\) matrix \\(\\vb{W}_{\\!p}\\) has only non-zeros entries on the \\((ph)\\)-th minor diagonal, i.e., with \\(\\delta_{k,l}\\) being the Kronecker Delta. Eq. (9) can be expressed as which allows the STFT of the \\(p\\)-th slice to be written as Note that \\(\\vb{F}\\) is unitary, i.e., the inverse equals its conjugate transpose meaning \\(\\conjT{\\vb{F}}\\vb{F} = \\vb{I}\\). To obtain a single vector-matrix equation for the STFT, the slices are stacked into one vector, i.e., where \\(P\\) is the number of columns of the resulting STFT. To invert this equation the Moore-Penrose inverse \\(\\vb{G}^\\dagger\\) can be utilized is invertible. Then \\(\\vb{x} = \\vb{G}^\\dagger\\vb{G}\\,\\vb{x} = \\inv{\\conjT{\\vb{G}}\\vb{G}}\\,\\conjT{\\vb{G}}\\vb{G}\\,\\vb{x}\\) obviously holds. \\(\\vb{D}\\) is always a diagonal matrix with non-negative diagonal entries. This becomes clear, when simplifying \\(\\vb{D}\\) further to due to \\(\\vb{F}\\) being unitary. Furthermore shows that \\(\\vb{D}_p\\) is a diagonal matrix with non-negative entries. Hence, summing \\(\\vb{D}_p\\) preserves that property. This allows to simplify Eq. (13) further, i.e., Utilizing Eq. (11), (14), (15), \\(\\vb{U}_p=\\vb{W}_{\\!p}\\vb{D}^{-1}\\) can be expressed as This shows \\(\\vb{U}_p\\) has the identical structure as \\(\\vb{W}_p\\) in Eq. (11), i.e., having only non-zero entries on the \\((ph)\\)-th minor diagonal. The sum term in the inverse can be interpreted as sliding \\(|w[\\mu]|^2\\) over \\(w[m]\\) (with an incorporated inversion), so only components overlapping with \\(w[m]\\) have an effect. Hence, all \\(U_p[m, k]\\) far enough from the border are identical windows. To circumvent border effects, \\(x[k]\\) is padded with zeros, enlarging \\(\\vb{U}\\) so all slices which touch \\(x[k]\\) contain the identical dual window Since \\(w[m] = 0\\) holds for \\(m \n\not\\in\\{0, \\ldots, M-1\\}\\), it is only required to sum over the indexes \\(\\eta\\) fulfilling \\(|\\eta| < M/h\\). The name dual window can be justified by inserting Eq. (12) into Eq. (16), i.e., showing that \\(\\vb{U}_p\\) and \\(\\vb{W}_{\\!p}\\) are interchangeable. Hence, \\(w_d[m]\\) is also a valid window with dual window \\(w[m]\\). Note that \\(w_d[m]\\) is not a unique dual window, due to \\(\\vb{s}\\) typically having more entries than \\(\\vb{x}\\). It can be shown, that \\(w_d[m]\\) has the minimal energy (or \\(L_2\\) norm) , which is the reason for being named the “canonical dual window”. The functions , , and the predate the implementation. This section discusses the key differences between the older “legacy” and the newer implementations. The main motivation for a rewrite was the insight that integrating dual windows could not be done in a sane way without breaking compatibility. This opened the opportunity for rethinking the code structure and the parametrization, thus making some implicit behavior more explicit. The following example compares the two STFTs of a complex valued chirp signal with a negative slope: # Calculate extent of plot with centered bins since # imshow does not interpolate by default: That the produces 3 more time slices than the legacy version is the main difference. As laid out in the Sliding Windows section, all slices which touch the signal are incorporated in the new version. This has the advantage that the STFT can be sliced and reassembled as shown in the code example. Furthermore, using all touching slices makes the ISTFT more robust in the case of windows that are zero somewhere. Note that the slices with identical time stamps produce equal results (up to numerical accuracy), i.e.: Generally, those additional slices contain non-zero values. Due to the large overlap in our example, they are quite small. E.g.: The ISTFT can be utilized to reconstruct the original signal: Note that the legacy implementation returns a signal which is longer than the original. On the other hand, the new allows to explicitly specify the start index k0 and the end index k1 of the reconstructed signal. The length discrepancy in the old implementation is caused by the fact that the signal length is not a multiple of the slices. Further differences between the new and legacy versions in this example are:\n• None The parameter ensures that the zero frequency is vertically centered for two-sided FFTs in the plot. With the legacy implementation, needs to be utilized. produces the same behavior as the old version.\n• None The parameter ensures identical phases of the two versions. ’s default value of produces STFT slices with an additional linear phase term. A spectrogram is defined as the absolute square of the STFT . The provided by the sticks to that definition, i.e.: On the other hand, the legacy provides another STFT implementation with the key difference being the different handling of the signal borders. The following example shows how to use the to obtain an identical SFT as produced with the legacy : # Legacy spectrogram (detrending for complex signals not useful): The difference from the other STFTs is that the time slices do not start at 0 but at , i.e.: Furthermore, only slices which do not stick out to the right are returned, centering the last slice at 4.875 s, which makes it shorter than with the default parametrization. Using the parameter, the legacy can also return the ‘angle’, ‘phase’, ‘psd’ or the ‘magnitude’. The behavior of the legacy is not straightforward, since it depends on the parameters , and . There is no direct correspondence for all combinations in the , since it provides only ‘magnitude’, ‘psd’ or no of the window at all. The following table shows those correspondences: When using output on complex-valued input signals, the old switches to mode. The raises a , since the utilized function only accepts real-valued inputs. Consult the Spectral Analysis section above for a discussion on the various spectral representations which are induced by the various parameterizations. Some further reading and related software:"
    },
    {
        "link": "https://indigits.com/post/2022/10/ecg_python",
        "document": "Python has excellent support for digital signal processing of ECG signals. In this post, we shall explore some basic capabilities for plotting ECG data and doing some basic signal processing for identifying the R peaks inside the signals.\n\nWe shall use for basic numerical computations and for plotting.\n\nThe provided signal is an excerpt (19:35 to 24:35) from the record 208 (lead MLII) provided by the MIT-BIH Arrhythmia Database [1] on PhysioNet [2]. This sample records the heart’s electrical activity at a sampling frequency of 360 Hz.\n\nLet us load the signal\n\nTypical ECG strips contain a 10 second snapshot of the signal which provides sufficient information for a quick examination of the heart’s activity. Let us extract a 10 second sample from the beginning.\n\nAn ECG strip is organized in major and minor ticks which make it easy for the doctors to quickly estimate the heart rate. Let us write a function to plot an ECG signal accordingly.\n\nWe can now look at our signal.\n\nNotice how the baseline of the ECG signal is wandering over time. We can see strong R peaks throughout the signal at regular interval. In the following, our goal will be to locate these picks via signal processing and then use the location of R peaks to estimate the heart rate.\n\nMost of the frequency components in ECG signals are low frequency. It is worthwhile to examine its frequency spectrum.\n\nprovides a utility function named . The R-peaks in the QRS complexes of an ECG signal are the most prominent feature. Let us see if we can identify the peaks directly from the raw ECG signal. The function requires us to provide some parameters which can be used to distinguish the prominent peaks from other local maxima. One is the minimum distance between two peaks. The other is the height of the peak viz. nearby signal content. Since there is so much baseline wander happening, the height is not a reliable factor.\n\nThere is a absolute refractory period of 200 ms after an R peak during which no new electrical synapse can be fired in the heart. We can use this to decide the minimum gap (in samples) between two R-peaks.\n\nWe shall use the minimum distance as well as a threshold of 60% in the range of values of the ECG signal for peak detection in the following function:\n\nUsing the function to find the peaks:\n\nWe can use the positions of these peaks to estimate a rough average heart rate value:\n\nLet us estimate the heart rate.\n\nWe should check if we identified the peaks correctly by marking the peaks on top of the ECG plot.\n\nAh, a careful inspection shows that we have missed 2 of the peaks between the time 5 and 6 seconds and we have detected a false peak near 8 second. It is time to look for a more robust peak detection algorithm.\n\nPan and Tompkins proposed a famous QRS complex detection algorithm in [3]. The algorithm involves some preprocessing steps on the ECG signal:\n\nThe ECG signal obtained after these steps is far more cleaner and R peaks are easily identifiable. Let us see how we can carry out these steps efficiently in Python. We shall use signal processing features in for these steps:\n\nPan and Tompkins suggest a bandpass filter of 5-15Hz on the input raw ECG signal. Let us design the filter coefficients:\n\nLet us now apply the filter and normalize the signal after filtering.\n\nFrom the perspective of detection of the location of R-peaks, the exact value of the signal doesn’t matter. Only, the form matters. Hence, normalization makes our job easier by limiting the range of ECG signal values.\n\nLet us now identify the peaks.\n\nVoila! After the band pass filtering, we have been able to correctly identify all the peaks. However, we should not be placated by this and carry out the remaining preprocessing tasks also.\n\nPan and Tompkins had originally suggested a derivative filter with coefficients . However, this was suggested for ECG signals sampled at 200Hz. Since our signal is at 360Hz, we need to revise the filter coefficients to match this sampling frequency. The code below does the resampling of the filter coefficients.\n\nIt is all good. No mistakes.\n\nSquaring the signal is quite easy:\n\nPan and Tompkins suggested a moving average integration over a period of 150 ms.\n\nHere is the filter for the same:\n\nLet us perform the moving average integration\n\nLet us identify the peaks from this integrated signal\n\nAh, the signal is clean but we are missing most of the peaks. Turns out that many of the R peaks are much smaller than the largest peak value in this signal. Let us decrease the threshold and check again.\n\nThe detection looks good now.\n\nAfter all this work, we definitely wish if there was a library which would do all this for us so that we don’t have to do all these steps manually. In fact, there are some additional details of the Pan Tompkins algorithm which we have skipped here. It turns out, there is indeed a Python library providing this capability.\n\nWFDB-Python is a pure Python library which provides interfaces for accessing the physiological signals in PhysioNet database. It provides functions for reading, writing, processing, and plotting physiologic signal and annotation data. The core I/O functionality is based on the Waveform Database (WFDB) specifications.\n\nIt also provides some QRS detection and instantaneous heart rate computation algorithms. You can install the library using either or :\n\nLet us import the relevant modules:\n\nWe shall use the GQRS algorithm provided in the library to detect the peaks.\n\nOnce the QRS complexes have been located, we can compute the instantaneous heart rates:\n\nWe can now overlay the peak locations and instantaneous heart rates on top of the signal:\n\nThere is a slight problem. The peak locations are somewhat earlier than the actual peaks. This happens due to the different filtering steps in the QRS detection algorithm which lead to some sample delays.\n\nAlso note that the instantaneous heart rate calculation skips some cycles as there is not enough data yet to do the computation reliably. The ECG signal amplitude is marked on the left Y axis while the heart rate value is marked on the right Y axis (varying between 100 to 130 beats per minute).\n\nWe can rectify the peak location problem by searching for the correct peaks in the neighborhood of the detected locations. This process is known as the peak correction.\n\nWe are now ready to plot our detected peaks at correct locations:\n• Moody GB, Mark RG. The impact of the MIT-BIH Arrhythmia Database. IEEE Eng in Med and Biol 20(3):45-50 (May-June 2001). (PMID: 11446209); DOI:10.13026/C2F305\n• Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh, Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals. Circulation 101(23):e215-e220; DOI:10.1161/01.CIR.101.23.e215"
    },
    {
        "link": "https://neuropsychology.github.io/NeuroKit/functions/ecg.html",
        "document": "This function runs different preprocessing steps: Cleaning (using ), peak detection (using ), heart rate calculation (using ), signal quality assessment (using ), QRS complex delineation (using ), and cardiac phase determination (using ). Help us improve the documentation of this function by making it more tidy and useful!\n• None sampling_rate (int) – The sampling frequency of (in Hz, i.e., samples/second). Defaults to 1000.\n• None method (str) – The processing method used for signal cleaning (using ) and peak detection (using ). Defaults to . Available methods are , , , , . We aim at improving this aspect to make the available methods more transparent, and be able to generate specific reports. Please get in touch if you are interested in helping out with this.\n• None signals (DataFrame) – A DataFrame of the same length as the containing the following columns:\n• None : The quality of the cleaned signal.\n• None : The R-peaks marked as “1” in a list of zeros.\n• None : The R-onsets marked as “1” in a list of zeros.\n• None : The R-offsets marked as “1” in a list of zeros.\n• None : The P-peaks marked as “1” in a list of zeros.\n• None : The P-onsets marked as “1” in a list of zeros.\n• None : The P-offsets marked as “1” in a list of zeros.\n• None : The Q-peaks marked as “1” in a list of zeros.\n• None : The S-peaks marked as “1” in a list of zeros.\n• None : The T-peaks marked as “1” in a list of zeros.\n• None : The T-onsets marked as “1” in a list of zeros.\n• None : The T-offsets marked as “1” in a list of zeros.\n• None : Cardiac phase, marked by “1” for systole and “0” for diastole.\n• None : Cardiac phase (atrial) completion, expressed in percentage (from 0 to 1), representing the stage of the current cardiac phase.\n• None : Cardiac phase (ventricular) completion, expressed in percentage (from 0 to 1), representing the stage of the current cardiac phase.\n• None rpeaks (dict) – A dictionary containing the samples at which the R-peaks occur, accessible with the key , as well as the signals’ sampling rate. Performs ECG analysis by computing relevant features and indices on either epochs (event-related analysis) or on longer periods of data (interval-related analysis), such as resting-state data.\n• None data (Union[dict, pd.DataFrame]) – A dictionary of epochs, containing one DataFrame per epoch, usually obtained via , or a DataFrame containing all epochs, usually obtained via . Can also take a DataFrame of processed signals from a longer period of data, typically generated by or . Can also take a dict containing sets of separate periods of data.\n• None sampling_rate (int) – The sampling frequency of the signal (in Hz, i.e., samples/second). Defaults to 1000Hz.\n• None method (str) – Can be one of for event-related analysis on epochs, or for analysis on longer periods of data. Defaults to , where the method will be chosen based on the mean duration of the data ( for duration under 10s). DataFrame – A dataframe containing the analyzed ECG features. If event-related analysis is conducted, each epoch is indicated by the column. See and docstrings for details. # Get a dataframe with all the results Generate an artificial (synthetic) ECG signal of a given duration and sampling rate using either the ECGSYN dynamical model (McSharry et al., 2003) or a simpler model based on Daubechies wavelets to roughly approximate cardiac cycles.\n• None length (int) – The desired length of the signal (in samples).\n• None heart_rate (int) – Desired simulated heart rate (in beats per minute). The default is 70. Note that for the method, random fluctuations are to be expected to mimick a real heart rate. These fluctuations can cause some slight discrepancies between the requested heart rate and the empirical heart rate, especially for shorter signals.\n• None method (str) – The model used to generate the signal. Can be for a simulation based on Daubechies wavelets that roughly approximates a single cardiac cycle. If (default), will use the model desbribed McSharry et al. (2003). If , will return a DataFrame containing 12-leads (see 12-leads ECG simulation).\n• None random_state (None, int, numpy.random.RandomState or numpy.random.Generator) – Seed for the random number generator. See for for further information.\n• None random_state_distort ({‘legacy’, ‘spawn’}, None, int, numpy.random.RandomState or numpy.random.Generator) – Random state to be used to distort the signal. If , use the same random state used to generate the signal (discouraged as it creates dependent random streams). If , spawn independent children random number generators from the random_state argument. If any of the other types, generate independent children random number generators from the random_state_distort provided (this allows generating multiple version of the same signal distorted by different random noise realizations).\n• None **kwargs – Other keywords parameters for ECGSYN algorithm, such as , , , .\n• None info (dict) – The information Dict returned by . Defaults to .\n• None Though the function returns nothing, the figure can be retrieved and saved as follows\n• None .. code-block:: python – # To be run after ecg_plot() fig = plt.gcf() fig.set_size_inches(10, 12, forward=True) fig.savefig(“myfig.png”)\n\nClean an ECG signal to remove noise and improve peak-detection accuracy. Different cleaning methods are implemented.\n• None (default): 0.5 Hz high-pass butterworth filter (order = 5), followed by powerline filtering (see ). By default, .\n• None : Method used in the BioSPPy package. A FIR filter ([0.67, 45] Hz; order = 1.5 * SR). The 0.67 Hz cutoff value was selected based on the fact that there are no morphological features below the heartrate (assuming a minimum heart rate of 40 bpm).\n• None : Method used in Pan & Tompkins (1985). Please help providing a better description!\n• None : Method used in Hamilton (2002). Please help providing a better description!\n• None : Method used in Elgendi et al. (2010). Please help providing a better description!\n• None : Method used in Engelse & Zeelenberg (1979). Please help providing a better description!\n• None : Method used in Visibility Graph Based Detection Emrich et al. (2023) and Koka et al. (2022). A 4.0 Hz high-pass butterworth filter (order = 2).\n• None sampling_rate (int) – The sampling frequency of (in Hz, i.e., samples/second). Defaults to 1000.\n• None method (str) – The processing pipeline to apply. Can be one of (default), , , , , , .\n• None **kwargs – Other arguments to be passed to specific methods.\n• None Engelse, W. A., & Zeelenberg, C. (1979). A single scan algorithm for QRS-detection and feature extraction. Computers in cardiology, 6(1979), 37-42.\n• None Elgendi, M., Jonkman, M., & De Boer, F. (2010). Frequency Bands Effects on QRS Detection. Biosignals, Proceedings of the Third International Conference on Bio-inspired Systems and Signal Processing, 428-431. Assess the quality of the ECG Signal using various methods:\n• None The method computes a continuous index of quality of the ECG signal, by interpolating the distance of each QRS segment from the average QRS segment present in the * data. This index is therefore relative: 1 corresponds to heartbeats that are the closest to the average sample and 0 corresponds to the most distant heartbeat from that average sample. Note that 1 does not necessarily means “good”: if the majority of samples are bad, then being close to the average will likely mean bad as well. Use this index with care and plot it alongside your ECG signal to see if it makes sense.\n• None The method (Zhao et al., 2018) extracts several signal quality indexes (SQIs): QRS wave power spectrum distribution pSQI, kurtosis kSQI, and baseline relative power basSQI. An additional R peak detection match qSQI was originally computed in the paper but left out in this algorithm. The indices were originally weighted with a ratio of [0.4, 0.4, 0.1, 0.1] to generate the final classification outcome, but because qSQI was dropped, the weights have been rearranged to [0.6, 0.2, 0.2] for pSQI, kSQI and basSQI respectively.\n• None ecg_cleaned (Union[list, np.array, pd.Series]) – The cleaned ECG signal in the form of a vector of values.\n• None rpeaks (tuple or list) – The list of R-peak samples returned by . If None, peaks is computed from the signal input.\n• None sampling_rate (int) – The sampling frequency of the signal (in Hz, i.e., samples/second).\n• None method (str) – The method for computing ECG signal quality, can be (default) or .\n• None approach (str) – The data fusion approach as documented in Zhao et al. (2018). Can be or . The former performs simple heuristic fusion of SQIs and the latter performs fuzzy comprehensive evaluation. If (default), simple heuristic fusion is used.\n• None **kwargs – Keyword arguments to be passed to in the computation of basSQI and pSQI. array or str – Vector containing the quality index ranging from 0 to 1 for method, returns string classification ( , or ) of the signal for method.\n• None Zhao, Z., & Zhang, Y. (2018). “SQI quality evaluation mechanism of single-lead ECG signal based on simple heuristic fusion and fuzzy comprehensive evaluation”. Frontiers in Physiology, 9, 727. Help is required to double-check whether the implementation match the papers.\n• None ecg_rate (array) – The heart rate signal as obtained via .\n• None sampling_rate (int) – The sampling frequency of the signal that contains the R-peaks (in Hz, i.e., samples/second). Defaults to 1000Hz.\n• None method (str) – Can be one of (default), , or . # Get ECG Derived Respiration (EDR) and add to the data\n• None van Gent, P., Farah, H., van Nes, N., & van Arem, B. (2019). HeartPy: A novel heart rate algorithm for the analysis of noisy signals. Transportation research part F: traffic psychology and behaviour, 66, 368-378.\n• None Sarkar, S., Bhattacherjee, S., & Pal, S. (2015). Extraction of respiration signal from ECG for respiratory rate estimation.\n• None Charlton, P. H., Bonnici, T., Tarassenko, L., Clifton, D. A., Beale, R., & Watkinson, P. J. (2016). An assessment of algorithms to estimate respiratory rate from the electrocardiogram and photoplethysmogram. Physiological measurement, 37(4), 610.\n• None Soni, R., & Muniyandi, M. (2019). Breath rate variability: a novel measure to study the meditation effects. International Journal of Yoga, 12(1), 45. Find R-peaks in an ECG signal using the specified method. You can pass an unfiltered ECG signals as input, but typically a filtered ECG (cleaned using ) will result in better results.\n• None neurokit (default): QRS complexes are detected based on the steepness of the absolute gradient of the ECG signal. Subsequently, R-peaks are detected as local maxima in the QRS complexes. The method is unpublished, but see: (i) neuropsychology/NeuroKit#476 for discussion of this algorithm; and (ii) https://doi.org/10.21105/joss.02621 for the original validation of this algorithm.\n• None engzeemod2012: Original algorithm by Engelse & Zeelenberg (1979) modified by Lourenço et al. (2012).\n• None manikandan2012: Algorithm by Manikandan & Soman (2012) based on the Shannon energy envelope (SEE).\n• None nabian2018: Algorithm by Nabian et al. (2018) based on the Pan-Tompkins algorithm.\n• None rodrigues2021: Adaptation of the work by Sadhukhan & Mitra (2012) and Gutiérrez-Rivas et al. (2015) by Rodrigues et al. (2021).\n• None emrich2023: FastNVG Algorithm by Emrich et al. (2023) based on the visibility graph detector of Koka et al. (2022). Provides fast and sample-accurate R-peak detection. The algorithm transforms the ecg into a graph representation and extracts exact R-peak positions using graph metrics.\n• None promac: ProMAC combines the result of several R-peak detectors in a probabilistic way. For a given peak detector, the binary signal representing the peak locations is convolved with a Gaussian distribution, resulting in a probabilistic representation of each peak location. This procedure is repeated for all selected methods and the resulting signals are accumulated. Finally, a threshold is used to accept or reject the peak locations. See this discussion for more information on the origins of the method: neuropsychology/NeuroKit#222 Please help us improve the methods’ documentation by adding a small description.\n• None ecg_cleaned (Union[list, np.array, pd.Series]) – The cleaned ECG channel as returned by .\n• None sampling_rate (int) – The sampling frequency of (in Hz, i.e., samples/second). Defaults to 1000.\n• None method (string) – The algorithm to be used for R-peak detection.\n• None correct_artifacts (bool) – Whether or not to identify and fix artifacts, using the method by Lipponen & Tarvainen (2019).\n• None show (bool) – If , will show a plot of the signal with peaks. Defaults to .\n• None **kwargs – Additional keyword arguments, usually specific for each method.\n• None signals (DataFrame) – A DataFrame of same length as the input signal in which occurrences of R-peaks marked as in a list of zeros with the same length as . Accessible with the keys .\n• None info (dict) – A dictionary containing additional information, in this case the samples at which R-peaks occur, accessible with the key , as well as the signals’ sampling rate, accessible with the key .\n• None Example 1: Find R-peaks using the default method ( ). # Collect all R-peak lists by iterating through the result dicts\n• None Zong, W., Heldt, T., Moody, G. B., & Mark, R. G. (2003). An open-source algorithm to detect onset of arterial blood pressure pulses. In Computers in Cardiology, 2003 (pp. 259-262). IEEE.\n• None Zong, W., Moody, G. B., & Jiang, D. (2003, September). A robust open-source algorithm to detect onset and duration of QRS complexes. In Computers in Cardiology, 2003 (pp. 737-740). IEEE.\n• None Elgendi, M., Jonkman, M., & De Boer, F. (2010). Frequency Bands Effects on QRS Detection. Biosignals, Proceedings of the Third International Conference on Bio-inspired Systems and Signal Processing, 428-431.\n• None Engelse, W. A., & Zeelenberg, C. (1979). A single scan algorithm for QRS-detection and feature extraction. Computers in cardiology, 6(1979), 37-42.\n• None Manikandan, M. S., & Soman, K. P. (2012). A novel method for detecting R-peaks in electrocardiogram (ECG) signal. Biomedical Signal Processing and Control, 7(2), 118-128.\n• None Kalidas, V., & Tamil, L. (2017, October). Real-time QRS detector using stationary wavelet transform for automated ECG analysis. In 2017 IEEE 17th International Conference on Bioinformatics and Bioengineering (BIBE) (pp. 457-461). IEEE.\n• None Nabian, M., Yin, Y., Wormwood, J., Quigley, K. S., Barrett, L. F., Ostadabbas, S. (2018). An Open-Source Feature Extraction Tool for the Analysis of Peripheral Physiological Data. IEEE Journal of Translational Engineering in Health and Medicine, 6, 1-11.\n• None Sadhukhan, D., & Mitra, M. (2012). R-peak detection algorithm for ECG using double difference and RR interval processing. Procedia Technology, 4, 873-877.\n• None Rodrigues, T., Samoutphonh, S., Silva, H., & Fred, A. (2021, January). A Low-Complexity R-peak Detection Algorithm with Adaptive Thresholding for Wearable Devices. In 2020 25th International Conference on Pattern Recognition (ICPR) (pp. 1-8). IEEE.\n• None T. Koka and M. Muma, “Fast and Sample Accurate R-Peak Detection for Noisy ECG Using Visibility Graphs,” 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), 2022, pp. 121-126.\n• None\n• None Unpublished. It runs different methods and derives a probability index using convolution. See this discussion for more information on the method: neuropsychology/NeuroKit#222\n• None Lipponen, J. A., & Tarvainen, M. P. (2019). A robust algorithm for heart rate variability time series artefact correction using novel beat classification. Journal of medical engineering & technology, 43(3), 173-181. Function to delineate the QRS complex, i.e., the different waves of the cardiac cycles. A typical ECG heartbeat consists of a P wave, a QRS complex and a T wave. The P wave represents the wave of depolarization that spreads from the SA-node throughout the atria. The QRS complex reflects the rapid depolarization of the right and left ventricles. Since the ventricles are the largest part of the heart, in terms of mass, the QRS complex usually has a much larger amplitude than the P-wave. The T wave represents the ventricular repolarization of the ventricles.On rare occasions, a U wave can be seen following the T wave. The U wave is believed to be related to the last remnants of ventricular repolarization.\n• None ecg_cleaned (Union[list, np.array, pd.Series]) – The cleaned ECG channel as returned by .\n• None rpeaks (Union[list, np.array, pd.Series]) – The samples at which R-peaks occur. Accessible with the key “ECG_R_Peaks” in the info dictionary returned by .\n• None sampling_rate (int) – The sampling frequency of (in Hz, i.e., samples/second). Defaults to 1000.\n• None method (str) – Can be one of for a peak-based method, for a peak-prominence-based method (Emrich et al., 2024), for continuous wavelet transform or (default) for discrete wavelet transform. The method might be useful to detect the waves, allowing to set individual physiological limits (see kwargs), while the method might be more precise for detecting the onsets and offsets of the waves (but might exhibit lower accuracy when there is significant variation in wave morphology). The method, which uses the zero-crossings of the signal derivatives, works best with very clean signals.\n• None show (bool) – If , will return a plot to visualizing the delineated waves information.\n• None show_type (str) – The type of delineated waves information showed in the plot. Can be , , , or .\n• None check (bool) – Defaults to . If , replaces the delineated features with if its standardized distance from R-peaks is more than 3.\n• None **kwargs – Other optional arguments: If using the method, additional parameters (in milliseconds) can be passed to set individual physiological limits for the search boundaries: - : The maximum allowable QRS complex interval. Defaults to 180 ms. - : The maximum PR interval duration. Defaults to 300 ms. - : Maximum duration for the R-wave rise. Defaults to 120 ms. - : Typical duration of the ST segment. Defaults to 150 ms. - : The maximum interval between P-wave on- and offset. Defaults to 100 ms. - : The maximum interval between R-wave on- and offset. Defaults to 100 ms. - : The maximum interval between T-wave on- and offset. Defaults to 200 ms.\n• None waves (dict) – A dictionary containing additional information. For derivative method, the dictionary contains the samples at which P-peaks, Q-peaks, S-peaks, T-peaks, P-onsets and T-offsets occur, accessible with the keys , , , , , , respectively. For the wavelet and prominence methods, in addition to the above information, the dictionary contains the samples at which QRS-onsets and QRS-offsets occur, accessible with the key , , , , , , , , , , respectively.\n• None signals (DataFrame) – A DataFrame of same length as the input signal in which occurrences of peaks, onsets and offsets marked as “1” in a list of zeros. Compute cardiac phase (for both atrial and ventricular), labelled as 1 for systole and 0 for diastole.\n• None ecg_cleaned (Union[list, np.array, pd.Series]) – The cleaned ECG channel as returned by .\n• None rpeaks (list or array or DataFrame or Series or dict) – The samples at which the different ECG peaks occur. If a dict or a DataFrame is passed, it is assumed that these containers were obtained with or .\n• None delineate_info (dict) – A dictionary containing additional information of ecg delineation and can be obtained with .\n• None sampling_rate (int) – The sampling frequency of (in Hz, i.e., samples/second). Defaults to . signals (DataFrame) – A DataFrame of same length as containing the following columns:\n• None : cardiac phase, marked by “1” for systole and “0” for diastole.\n• None : cardiac phase (atrial) completion, expressed in percentage (from 0 to 1), representing the stage of the current cardiac phase.\n• None : cardiac phase, marked by “1” for systole and “0” for diastole.\n• None : cardiac phase (ventricular) completion, expressed in percentage (from 0 to 1), representing the stage of the current cardiac phase. Calculate signal rate (per minute) from a series of peaks. It is a general function that works for any series of peaks (i.e., not specific to a particular type of signal). It is computed as , where the period is the time between the peaks (see func: ). This function is implemented under , but it also re-exported under different names, such as , or . The aliases are provided for consistency.\n• None peaks (Union[list, np.array, pd.DataFrame, pd.Series, dict]) – The samples at which the peaks occur. If an array is passed in, it is assumed that it was obtained with . If a DataFrame is passed in, it is assumed it is of the same length as the input signal in which occurrences of R-peaks are marked as “1”, with such containers obtained with e.g., :func:.`ecg_findpeaks` or .\n• None sampling_rate (int) – The sampling frequency of the signal that contains peaks (in Hz, i.e., samples/second). Defaults to 1000.\n• None desired_length (int) – If left at the default None, the returned rated will have the same number of elements as . If set to a value larger than the sample at which the last peak occurs in the signal (i.e., ), the returned rate will be interpolated between peaks over samples. To interpolate the rate over the entire duration of the signal, set to the number of samples in the signal. Cannot be smaller than or equal to the sample at which the last peak occurs in the signal. Defaults to .\n• None interpolation_method (str) – Method used to interpolate the rate between peaks. See . is chosen as the default interpolation method since it ensures monotone interpolation between data points (i.e., it prevents physiologically implausible “overshoots” or “undershoots” in the y-direction). In contrast, the widely used cubic spline interpolation does not ensure monotonicity. array – A vector containing the rate (peaks per minute). # Visualize signal and rate on the same scale Segment an ECG signal into single heartbeats. Convenient for visualizing all the heart beats.\n• None ecg_cleaned (Union[list, np.array, pd.Series]) – The cleaned ECG channel as returned by .\n• None rpeaks (dict) – The samples at which the R-peaks occur. Dict returned by . Defaults to .\n• None sampling_rate (int) – The sampling frequency of (in Hz, i.e., samples/second). Defaults to 1000.\n• None show (bool) – If , will return a plot of heartbeats. Defaults to . If “return”, returns the axis of the plot.\n• None **kwargs – Other arguments to be passed. dict – A dict containing DataFrames for all segmented heartbeats.\n• None epochs (Union[dict, pd.DataFrame]) – A dict containing one DataFrame per event/trial, usually obtained via , or a DataFrame containing all epochs, usually obtained via . DataFrame – A dataframe containing the analyzed ECG features for each epoch, with each epoch indicated by the column (if not present, by the column). The analyzed features consist of the following:\n• None : The mean heart rate after stimulus onset.\n• None : The standard deviation of the heart rate after stimulus onset.\n• None : The time at which maximum heart rate occurs.\n• None : The time at which minimum heart rate occurs.\n• None : Indication of whether the onset of the event concurs with respiratory systole (1) or diastole (0).\n• None : Indication of whether the onset of the event concurs with respiratory systole (1) or diastole (0).\n• None : Indication of the stage of the current cardiac (atrial) phase (0 to 1) at the onset of the event.\n• None : Indication of the stage of the current cardiac (ventricular) phase (0 to 1) at the onset of the event. We also include the following experimental features related to the parameters of a quadratic model:\n• None : The parameter corresponding to the linear trend.\n• None : The parameter corresponding to the curvature.\n• None : The quality of the quadratic model. If too low, the parameters might not be reliable or meaningful. Performs ECG analysis on longer periods of data (typically > 10 seconds), such as resting-state data.\n• None data (Union[dict, pd.DataFrame]) – A DataFrame containing the different processed signal(s) as different columns, typically generated by or . Can also take a dict containing sets of separately processed DataFrames.\n• None sampling_rate (int) – The sampling frequency of the signal (in Hz, i.e., samples/second). Defaults to 1000Hz. DataFrame – A dataframe containing the analyzed ECG features. The analyzed features consist of the following:\n\nLow-level function used by to identify R-peaks in an ECG signal using a different set of algorithms. Use the main function and see its documentation for details.\n• None show (bool) – If , will return a plot to visualizing the thresholds used in the algorithm. Useful for debugging.\n• None **kwargs – Additional keyword arguments, usually specific for each . info (dict) – A dictionary containing additional information, in this case the samples at which R-peaks occur, accessible with the key . Checks whether an ECG signal is inverted, and if so, corrects for this inversion. To automatically detect the inversion, the ECG signal is cleaned, the mean is subtracted, and with a rolling window of 2 seconds, the original value corresponding to the maximum of the squared signal is taken. If the median of these values is negative, it is assumed that the signal is inverted.\n• None sampling_rate (int) – The sampling frequency of (in Hz, i.e., samples/second). Defaults to 1000.\n• None force (bool) – Whether to force inversion of the signal regardless of whether it is detected as inverted. The default is False.\n• None show (bool) – Shows a plot of the original and inverted signal.\n• None bool – Whether the inversion was performed. Any function appearing below this point is not explicitly part of the documentation and should be added. Please open an issue if there is one."
    },
    {
        "link": "https://github.com/antimattercorrade/Pan_Tompkins_QRS_Detection",
        "document": "A dependable QRS recognition algorithm has numerous applications. A popular technique is the computer interpretation of the 12-lead ECG. Arrhythmia monitors are now widely used in coronary care units. Holter tape recording, which is widely used, necessitates a Holter scanning device that includes a QRS detector to analyse the tapes much faster than in real-time. Arrhythmia monitors for ambulatory patients that analyse the ECG in real time are currently in development. When an arrhythmia occurs, such a monitor can be programmed to immediately store an interval of the abnormal ECG for subsequent transmission to a central station where a physician can interpret it. Such a device necessitates highly accurate QRS recognition. False detection results in unnecessary data transmission to the central station or requires an extensive memory to store any ECG segments that are captured unnecessarily. As a result, an accurate QRS detector is an essential component of many ECG instruments.\n\nThe Pan-Tompkins Algorithm is used to detect R waves from the QRS complex present in the ECG signals to determine the Heart Rate of an individual. The algorithm works by analysing the slope, amplitude and width of the QRS complexes present in the filtered ECG signal. The ECG signal is filtered so as to reduce noise and decrease detection thresholds, thereby increasing the sensitivity towards detection of the QRS complex.\n\nThe algorithm can be divided into various phases, the first phase consists of applying the filtered on the input ECG signal, followed by peak detection in the filtered signal. The peak detection again works in three phases: Learning Phase 1, Learning Phase 2 and Detection. Learning Phase 1 is required to initialize the signal and noise thresholds followed by Learning Phase 2 in which the RR intervals and the RR limit values are initialized. The detection phase works by adjusting the thresholds appropriately and recognizing the QRS complexes. A dual threshold is used to increase the detection sensitivity along with the improvement in the signal to noise ratio by the bandpass filter.\n• Bandpass Filter: Bandpass filter is used to attenuate the noise in the input signal. To acheive a passband of 5-15 Hz, the input signal is first passed through a low pass filter having a cutoff frequency of 11 Hz and then through a high pass filter with a cutoff frequency of 5 Hz, thus achieving the required thresholds. \n\n The low pass filter has the recursive equation: \n\n The high pass filter has the recursive equation:\n• Derivative Filter: The derivative of the input signal is taken to obtain the information of the slope of the signal. Thus, the rate of change of input is obtain in this step of the algorithm. \n\n The derivative filter has the recursive equation:\n• Squaring: The squaring process is used to intensify the slope of the frequency response curve obtained in the derivative step. This step helps in restricting false positives which may be caused by T waves in the input signal. \n\n The squaring filter has the recursive equation:\n• Moving Window Integration: The moving window integration process is done to obtain information about both the slope and width of the QRS complex. A window size of 0.15*(sample frequency) is used for more accurate results. \n\n The moving window integration has the recursive equation: \n\n where N is the number of samples in the width of integration window.\n• Fiducial Mark: An approximate location of the QRS complex can be obtained in the initial phase of detection by sensing the rising edge of the integration waveform. Since, a peak is determined by the change in slope of the curve, the differentiated signal is used to determine the fiducial marks.\n• Adjusting Thresholds: Since the signal to noise ratio is improved by the bandpass filter, two sets thresholds are maintained to account for low threshold values. The higher thresholds of each set are used to detect peaks in the first analysis and the lower thresholds in the searchback process. The thresholds are adjusted accordingly to account for the detected signal peaks and noise values.\n• Adjusting RR Interval and Limits: To keep track of the time between two successive R peaks, two RR intervals are maintained. The first RR interval keeps track of the eight most recent beats while the second RR interval keeps track of the eight most recent beats having RR intervals that fall within the rate limits. Two averages pertaining to these RR intervals are calculated. These averages are then used to update the rate limits for the RR intervals. If a QRS compelx is not found within the calculated limits a searchback process is initiated to find the maximal peak value within the two calculated thresholds and this peak is taken to be a QRS candidate.\n• T Wave Identification: If the calculated RR interval is less than 360 ms, which in this case is the sample frequency of the input ECG signal, then the maximal slope of this waveform is calculated. This is done to ensure that the interval in consideration is actually a QRS complex or a T wave. If the calculated maximal slope of the considered interval is less than half of the slope of the last QRS complex detected, then the current interval is considered to be a T wave.\n\nAfter the successfull detection of the R peaks, the heart rate of an individual can be calculated by considering the time difference between successive R peaks. The heart rate can be calculated as:\n• The dataset is being downloaded from the given hashed link.\n• To run, open the colab file and select the image using the slider given. Then run the corresponding cells to get the results."
    },
    {
        "link": "https://medium.com/@cosmicwanderer/pan-tompkins-algorithm-for-detecting-qrs-waves-29c5f2927906",
        "document": "The Pan–Tompkins algorithm is the most common algorithm used for detecting QRS complexes in ECG signals. The QRS complex represents ventricular depolarization and is one of the prominent features of the ECG signal, detecting this wave is important for various applications such as determining heart rate and heart rhythm analysis.\n\nThe Pan-Tompkins algorithm for QRS complex detection in ECG signals involves several preprocessing steps like bandpass filtering, differentiation, squaring, and moving average smoothing to enhance QRS complexes power. The algorithm then applies a peak detection function to identify QRS peaks.\n\nThe ECG signal is filtered through a passband filter to reduce noise and unwanted interferences and maximizing the QRS energy. The passband range of the filter is 5–15Hz. This range was specifically chosen to reduce muscle noise and baseline wander, hence smoothing the signal, and remove low frequency noise below 5 Hz such as the T-waves which in this signal are highly elevated.\n\nLet’s get to the coding part. The code I provided below is originally written by Hooman Sedghamiz (link below), I only did a few modifications to make it a bit simpler.\n\nApplying the band pass filter in python is very straightforward. We first define the filters parameters, such as the cutoff frequencies (which are normalized) and the order of the filter.\n\nYou can see how the filter did a great job in attenuating baseline wander and the T-wave.\n\nAfter the filtering process, the ECG signal was differentiated. The derivative stage plays a crucial role in enhancing the steepness or slope of the QRS complex, because differentiating a curve enhances rapid changes and sharp edges.\n\nThe signal was squared after the derivative stage to ensure that all signal components exhibited positive values. The squaring particularly amplified the higher amplitudes associated with the QRS complex while attenuating other smaller interferences.\n\nImplementing that in code is very relatively simple\n\nWe are not yet ready to find QRS peaks, we need to merge the peaks resulting from the differentiation stage into a single peak to make it easier for us to detect the QRS peaks\n\nA moving average window was applied to the signal to merge the peaks from the previous stage together and cancel sharp edges to facilitate QRS detection. The filter is implemented by convolving the signal with a rectangular window. The window size must be selected appropriately for the filter to work, the size of the window is affected by the sampling frequency and QRS width.\n\nA sample y[k] is considered a peak if its value is bigger than its two immediate neighbors (local maximum)\n\nThis basic criteria is not sufficient for noisy signals, because it will result in a lot of false positives. This is why we processed the ECG signal before trying to locate the QRS peaks.\n\nFor simplicity, we will use the built-in python function signal.find_peaks to locate the QRS peaks. The function uses the criteria we defined above in addition to two thresholds. The first limits the minimum height of the peak. The threshold will be defined to be the mean amplitude of the processed ECG signal, this is a very basic threshold, there are other ways of setting this threshold value, but are more complicated. The second threshold is the minimum distance between two peaks, which is set to be 200 msec.\n\nThe function actually did a great job in finding the QRS peaks, but of course the accuracy of peak detection depends on how well-defined the thresholds are.\n\nThere are other newer and more advanced methods for QRS detection, such as wavelet transforms and machine learning approaches, which could give more accurate results. However, the Pan-Tompkins algorithm’s computational simplicity and good performance make it a valuable tool for a wide range of applications, from basic heart rate monitoring to more complex arrhythmia analysis.\n\n— Fariha, M. A. Z., et al. (2020). Analysis of Pan-Tompkins Algorithm Performance with Noisy ECG Signals. Journal of Physics: Conference Series, 1532, 012022."
    },
    {
        "link": "https://github.com/c-labpl/qrs_detector",
        "document": "The modules published in this repository are Python implementations of online and offline QRS complex detectors in ECG signal, based on the Pan-Tomkins algorithm (Pan J., Tompkins W. J., A real-time QRS detection algorithm, IEEE Transactions on Biomedical Engineering, Vol. BME-32, No. 3, March 1985, pp. 230-236).\n\nThe QRS complex corresponds to the depolarization of the right and left ventricles of the human heart. It is the most visually obvious part of the ECG signal. QRS complex detection is essential for time-domain ECG signal analyses, namely heart rate variability. It makes it possible to compute inter-beat interval (RR interval) values that correspond to the time between two consecutive R peaks. Thus, a QRS complex detector is an ECG-based heart contraction detector.\n\nYou can find out more about cardiac cycle and QRS complex here and here.\n\nThis repository contains two versions of the Pan-Tomkins QRS detection algorithm implementation:\n• QRSDetectorOnline - Online version detects QRS complexes in a real-time acquired ECG signal. Therefore, it requires an ECG device to be plugged in and receiving a signal in real-time.\n\nThis implementation of a QRS Complex Detector is by no means a certified medical tool and should not be used in health monitoring. It was created and used for experimental purposes in psychophysiology and psychology.\n\nThe published QRS Detector module is an implementation of the QRS detection algorithm known as the Pan-Tomkins QRS detection algorithm, first described in a paper by Jiapu Pan and Willis J. Tomkins titled \"A Real-Time QRS Detection Algorithm\" (1985). The full version of the paper is accessible here.\n\nThe direct input to the algorithm is a raw ECG signal. The detector processes the signal in two stages: filtering and thresholding.\n\nFirst, in the filtering stage each raw ECG measurement is filtered using a cascade of low-pass and high-pass filters that together form a band-pass filter. This filtering mechanism ensures that only parts of the signal related to heart activity can pass through. The filters eliminate most of the measurement noise that could cause false positive detection. The band-pass filtered signal is then differentiated to identify signal segments with high signal change values. These changes are then squared and integrated to make them more distinct. In the last step of the processing stage, the integrated signal is screened by a peak detection algorithm to identify potential QRS complexes within the integrated signal.\n\nIn the next stage, the identified QRS complex candidates are classified by means of dynamically set thresholds, either as QRS complexes or as noise peaks. The thresholds are real-time adjusted: a threshold in a given moment is based on the signal value of the previously detected QRS and noise peaks. The dynamic thresholding accounts for changes in the noise level. The dynamic thresholding and complex filtering ensure sufficient detection sensitivity with relatively few false positive QRS complex detections.\n\nImportantly, not all of the features presented in the original Pan and Tomkins paper were implemented in this module. Specifically, we decided not to implement supplementary mechanisms proposed in the paper that are not core elements of QRS detection. Therefore, we did not implement the following features: fiducial mark on filtered data, use of another set of thresholds based on the filtered ECG, irregular heart rate detection, and missing QRS complex detection search-back mechanism. Despite the lack of these supplementary features, implementation of the core features proposed by Pan and Tompkins allowed us to achieve a sufficient level of QRS detection.\n\nModules published here consist of the following dependencies:\n\nAll the dependencies are in the requirements.py file.\n\nThe modules are implemented for use with Python 3.x. However, they are relatively easy to convert to work with Python 2.x:\n• remove the decode() function call when reading data from an ECG device in the online version of the QRS Detector, i.e. use\n\nThe QRS Detector module was implemented in two separate versions: Online and Offline. Each has a different application and method of use.\n\nThe Online version is designed to work with a directly connected ECG device. As an input it uses an ECG signal received in real-time, detects QRS complexes, and outputs them to be used by other scripts in order to trigger external events. For example, the Online QRS Detector can be used to trigger visual, auditory, or tactile stimuli. It has already been successfully implemented in PsychoPy (Peirce, J. W. (2009). Generating stimuli for neuroscience using PsychoPy (Peirce, J. (2009). Generating stimuli for neuroscience using PsychoPy. Frontiers in Neuroinformatics, 2 (January), 1–8. http://doi.org/10.3389/neuro.11.010.2008) and tested to study cardioceptive (interoceptive) abilities, namely, in Schandry’s heartbeat tracking task (Schandry, R. (1981). Heart beat perception and emotional experience. Psychophysiology, 18(4), 483–488. http://doi.org/10.1111/j.1469-8986.1981.tb02486.x) and heartbeat detection training based on that proposed by Schandry and Weitkunat (Schandry, R., & Weitkunat, R. (1990). Enhancement of heartbeat-related brain potentials through cardiac awareness training. The International Journal of Neuroscience, 53(2-4), 243–53. http://dx.doi.org/10.3109/00207459008986611).\n\nThe Online version of the QRS Detector module has been implemented to work with the Arduino-based e-Health Sensor Platform V2.0 ECG device. You will find more about this device here.\n\nAn Arduino e-Health ECG device sketch is also provided in this repository. The sampling rate of the ECG signal acquisition is set to 250 (Hz) samples per second. Measurements are sent in a real-time in a string format, \"timestamp,measurement\", and have to be parsed by the QRS Detector module.\n\nTo use the Online QRS Detector module, the ECG device with the loaded Arduino sketch has to be connected to a USB port. Then QRS Complex Detector object is initialized with the port name where the ECG device is connected and the measurement baud rate is set. No further calibration or configuration is needed. The Online QRS Detector starts detection immediately after initialization.\n\nBelow is example code of how to run the Online QRS Detector:\n\nIf you want to use the Online QRS Detector in the background with other processes running on the layer above it (e.g. display a visual stimulus or play a tone triggered by the detected QRS complexes), we suggest using a Python multiprocessing mechanism. Multiprocessing offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using sub-processes instead of threads. Here is example code of the many ways to achieve this:\n\nEven though the Online QRS Detector was implemented to be used with an Arduino-based e-Health Sensor Platform ECG device with 250 Hz sampling rate and a specified data format, it can be easily modified to be used with any other ECG device, sampling rate or data format. For more information, check the \"Customization\" section.\n\nThe Offline version of the detector works with ECG measurement datasets stored in .csv format. These can be loaded into the detector to perform QRS detection.\n\nThe Offline QRS Detector loads the data, analyses it, and detects QRS complexes in the same way as the online version, but it uses an entire existing dataset instead of real-time measurements directly from an ECG device.\n\nThis module is intended for offline QRS detection in ECG measurements when real-time QRS-based event triggering is not crucial, but when more complex data analysis, visualisation and detection information are needed. It can also be used simply as a debugging tool to verify whether the online version works as intended, or to check the behaviour of QRS Detector intermediate data processing stages.\n\nThe offline version of the QRS Detector module was implemented to work with any kind of ECG measurements data that can be loaded from a file. Unlike the online version, it was not designed to work with some specific device. The Offline QRS Detector expects \"timestamp,measurement\" data format stored in .csv format and is tuned for measurement data acquired with 250 Hz sampling rate. Both the format of the data and the sampling rate can be easily modified. For more information, check the \"Customization\" section.\n\nThe Offline QRS Detector requires initialization with a path to the ECG measurements file. The QRS Detector will load the dataset, analyse measurements, and detect QRS complexes. It outputs a detection log file with marked detected QRS complexes. In the file, the detected QRS complexes are marked with a '1' flag in the 'qrs_detected' log data column. Additionally, the Offline QRS Detector stores detection results internally as an ecg_data_detected attribute of an Offline QRS Detector object. Optionally, it produces plots with all intermediate signal-processing steps and saves it to a .csv file.\n\nBelow is example code showing how to run the offline version of the QRS Detector module:\n\nCheck qrs_detector_offline_example.ipynb Jupyter notebook for an example usage of the Offline QRS Detector with generated plots and logs.\n\nThe Online QRS Detector module can be easily modified to work with other ECG devices, different sampling rates, and different data formats:\n• QRSDetectorOnline works on a real-time received signal from an ECG device. Data is sent to the module in a \"timestamp,measurement\" format string. If a different ECG data format is expected, the process_measurement() function requires some changes in order to enable correct data parsing. The Online QRS Detector works even if only measurement values without corresponding timestamps are available.\n• QRSDetectorOnline is tuned for 250 Hz sampling rate by default; however, this can be customized by changing seven configuration attributes (marked in the code) according to the desired signal_frequency. For example, to change the signal sampling rate from 250 to 125 samples per second, simply divide all the parameters by 2: set signal_frequency to 125, number_of_samples_stored to 100 samples, integration_window to 8 samples, findpeaks_spacing to 25 samples, detection_window to 20 samples and refractory_period to 60 samples.\n\nThe Offline QRS Detector is hardware independent because it uses an ECG measurements dataset instead of real-time acquired ECG signal. Therefore, the only parameters that need to be adjusted are sampling rate and data format:\n• QRSDetectorOffline is by default tuned for a sampling rate of 250 samples per second. It can be customized by changing 4 configuration attributes (marked in the code) according to the desired signal_frequency. For example, to change signal sampling rate from 250 to 125 samples per second, divide all parameters by 2: set signal_frequency value to 125, integration_window to 8 samples, findpeaks_spacing to 25 samples and refractory_period to 60 samples.\n• QRSDetectorOffline uses a loaded ECG measurements dataset. Data is expected to be in csv format with each line in \"timestamp,measurement\" format. If a different ECG data format is expected, changes needs to be made in the load_ecg_data() function, which loads the dataset, or in the detect_peaks() function, which processes measurement values in:\n\nThe algorithm will work fine even if only measurement values without timestamps are available.\n\nIf you use these modules in a research project, please consider citing it:\n\nIf you use these modules in any other project, please refer to MIT open-source license.\n\nThe following modules and repository were created as a part of the project “Relationship between interoceptive awareness and metacognitive abilities in near-threshold visual perception” supported by the National Science Centre Poland, PRELUDIUM 7, grant no. 2014/13/N/HS6/02963. Special thanks for Michael Timberlake for proofreading."
    },
    {
        "link": "https://diva-portal.org/smash/get/diva2:1775400/FULLTEXT01.pdf",
        "document": ""
    },
    {
        "link": "https://mathworks.com/matlabcentral/fileexchange/45840-complete-pan-tompkins-implementation-ecg-qrs-detector",
        "document": ""
    }
]