[
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html",
        "document": ""
    },
    {
        "link": "https://pandas.pydata.org/docs/whatsnew/index.html",
        "document": "This is the list of changes to pandas between each release. For full details, see the commit logs. For install and upgrade instructions, see Installation."
    },
    {
        "link": "https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas",
        "document": "This post is meant for readers who,\n• Would like to understand what this warning means\n• Would like to understand different ways of suppressing this warning\n• Would like to understand how to improve their code and follow good practices to avoid this warning in the future.\n\nWhat is the ?\n\nTo know how to deal with this warning, it is important to understand what it means and why it is raised in the first place.\n\nWhen filtering DataFrames, it is possible slice/index a frame to return either a view, or a copy, depending on the internal layout and various implementation details. A \"view\" is, as the term suggests, a view into the original data, so modifying the view may modify the original object. On the other hand, a \"copy\" is a replication of data from the original, and modifying the copy has no effect on the original.\n\nAs mentioned by other answers, the was created to flag \"chained assignment\" operations. Consider in the setup above. Suppose you would like to select all values in column \"B\" where values in column \"A\" is > 5. Pandas allows you to do this in different ways, some more correct than others. For example,\n\nThese return the same result, so if you are only reading these values, it makes no difference. So, what is the issue? The problem with chained assignment, is that it is generally difficult to predict whether a view or a copy is returned, so this largely becomes an issue when you are attempting to assign values back. To build on the earlier example, consider how this code is executed by the interpreter:\n\nWith a single call to . OTOH, consider this code:\n\nNow, depending on whether returned a view or a copy, the operation may not work.\n\nIn general, you should use for label-based assignment, and for integer/positional based assignment, as the spec guarantees that they always operate on the original. Additionally, for setting a single cell, you should use and .\n\nMore can be found in the documentation.\n\nfrom pandas >= 2.0, you can enable Copy-on-write optimizations to save on memory and avoid making copies of data until written to (if possible).\n\nThis can be enabled by\n\nAfter this, attempts to make chained assignments will result in\n\nThe error is raised in a similar setting to the .\n\nJust tell me how to suppress the warning!\n\nConsider a simple operation on the \"A\" column of . Selecting \"A\" and dividing by 2 will raise the warning, but the operation will work.\n\nThere are a couple ways of directly silencing this warning:\n• None Change Can be set to , , or . is the default. will suppress the warning entirely, and will throw a , preventing the operation from going through.\n\n@Peter Cotton in the comments, came up with a nice way of non-intrusively changing the mode (modified from this gist) using a context manager, to set the mode only as long as it is required, and the reset it back to the original state when finished.\n\nThe usage is as follows:\n\nOr, to raise the exception\n\nThe \"XY Problem\": What am I doing wrong?\n\nA lot of the time, users attempt to look for ways of suppressing this exception without fully understanding why it was raised in the first place. This is a good example of an XY problem, where users attempt to solve a problem \"Y\" that is actually a symptom of a deeper rooted problem \"X\". Questions will be raised based on common problems that encounter this warning, and solutions will then be presented.\n\nWrong way to do this:\n\nRight way using :\n\nQuestion 21 I am trying to set the value in cell (1, 'D') to 12345. My expected output is I have tried different ways of accessing this cell, such as . What is the best way to do this? 1. This question isn't specifically related to the warning, but it is good to understand how to do this particular operation correctly so as to avoid situations where the warning could potentially arise in future.\n\nYou can use any of the following methods to do this.\n\nThis is actually probably because of code higher up in your pipeline. Did you create from something larger, like\n\n? In this case, boolean indexing will return a view, so will reference the original. What you'd need to do is assign to a copy:\n\nThis is because must have been created as a view from some other slicing operation, such as\n\nThe solution here is to either make a of , or use , as before."
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/dsintro.html",
        "document": "We’ll start with a quick, non-comprehensive overview of the fundamental data structures in pandas to get you started. The fundamental behavior about data types, indexing, axis labeling, and alignment apply across all of the objects. To get started, import NumPy and load pandas into your namespace:\n\nFundamentally, data alignment is intrinsic. The link between labels and data will not be broken unless done so explicitly by you.\n\nWe’ll give a brief intro to the data structures, then consider all of the broad categories of functionality and methods in separate sections.\n\nis a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. The basic method to create a is to call: Here, can be many different things: The passed index is a list of axis labels. Thus, this separates into a few cases depending on what data is: If is an ndarray, index must be the same length as data. If no index is passed, one will be created having values . pandas supports non-unique index values. If an operation that does not support duplicate index values is attempted, an exception will be raised at that time. can be instantiated from dicts: If an index is passed, the values in data corresponding to the labels in the index will be pulled out. NaN (not a number) is the standard missing data marker used in pandas. If is a scalar value, an index must be provided. The value will be repeated to match the length of index. acts very similarly to a and is a valid argument to most NumPy functions. However, operations such as slicing will also slice the index. We will address array-based indexing like in section on indexing. This is often a NumPy dtype. However, pandas and 3rd-party libraries extend NumPy’s type system in a few places, in which case the dtype would be an . Some examples within pandas are Categorical data and Nullable integer data type. See dtypes for more. If you need the actual array backing a , use . Accessing the array can be useful when you need to do some operation without the index (to disable automatic alignment, for example). will always be an . Briefly, an ExtensionArray is a thin wrapper around one or more concrete arrays like a . pandas knows how to take an and store it in a or a column of a . See dtypes for more. While is ndarray-like, if you need an actual ndarray, then use . Even if the is backed by a , will return a NumPy ndarray. A is also like a fixed-size dict in that you can get and set values by index label: If a label is not contained in the index, an exception is raised: Traceback (most recent call last) in in in in in : 'f' Traceback (most recent call last) in # Convert generator to list before going through hashable part # (We will iterate through the generator there to check for slices) in # Similar to Index.get_value, but we do not fall back to positional in # If we have a listlike key, _check_indexing_error will raise # InvalidIndexError. Otherwise we fall through and re-raise : 'f' Using the method, a missing label will return None or specified default: These labels can also be accessed by attribute. When working with raw NumPy arrays, looping through value-by-value is usually not necessary. The same is true when working with in pandas. can also be passed into most NumPy methods expecting an ndarray. A key difference between and ndarray is that operations between automatically align the data based on label. Thus, you can write computations without giving consideration to whether the involved have the same labels. The result of an operation between unaligned will have the union of the indexes involved. If a label is not found in one or the other, the result will be marked as missing . Being able to write code without doing any explicit data alignment grants immense freedom and flexibility in interactive data analysis and research. The integrated data alignment features of the pandas data structures set pandas apart from the majority of related tools for working with labeled data. In general, we chose to make the default result of operations between differently indexed objects yield the union of the indexes in order to avoid loss of information. Having an index label, though the data is missing, is typically important information as part of a computation. You of course have the option of dropping labels with missing data via the dropna function. The can be assigned automatically in many cases, in particular, when selecting a single column from a , the will be assigned the column label. You can rename a with the method. Note that and refer to different objects.\n\nis a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object. Like Series, DataFrame accepts many different kinds of input: Along with the data, you can optionally pass index (row labels) and columns (column labels) arguments. If you pass an index and / or columns, you are guaranteeing the index and / or columns of the resulting DataFrame. Thus, a dict of Series plus a specific index will discard all data not matching up to the passed index. If axis labels are not passed, they will be constructed from the input data based on common sense rules. From dict of Series or dicts# The resulting index will be the union of the indexes of the various Series. If there are any nested dicts, these will first be converted to Series. If no columns are passed, the columns will be the ordered list of dict keys. The row and column labels can be accessed respectively by accessing the index and columns attributes: When a particular set of columns is passed along with a dict of data, the passed columns override the keys in the dict. All ndarrays must share the same length. If an index is passed, it must also be the same length as the arrays. If no index is passed, the result will be , where is the array length. This case is handled identically to a dict of arrays. DataFrame is not intended to work exactly like a 2-dimensional NumPy ndarray. You can automatically create a MultiIndexed frame by passing a tuples dictionary. The result will be a DataFrame with the same index as the input Series, and with one column whose name is the original name of the Series (only if no other column name provided). The field names of the first in the list determine the columns of the . The remaining namedtuples (or tuples) are simply unpacked and their values are fed into the rows of the . If any of those tuples is shorter than the first then the later columns in the corresponding row are marked as missing values. If any are longer than the first , a is raised. Data Classes as introduced in PEP557, can be passed into the DataFrame constructor. Passing a list of dataclasses is equivalent to passing a list of dictionaries. Please be aware, that all values in the list should be dataclasses, mixing types in the list would result in a . To construct a DataFrame with missing data, we use to represent missing values. Alternatively, you may pass a as the data argument to the DataFrame constructor, and its masked entries will be considered missing. See Missing data for more. takes a dict of dicts or a dict of array-like sequences and returns a DataFrame. It operates like the constructor except for the parameter which is by default, but which can be set to in order to use the dict keys as row labels. If you pass , the keys will be the row labels. In this case, you can also pass the desired column names: takes a list of tuples or an ndarray with structured dtype. It works analogously to the normal constructor, except that the resulting DataFrame index may be a specific field of the structured dtype. You can treat a semantically like a dict of like-indexed objects. Getting, setting, and deleting columns works with the same syntax as the analogous dict operations: Columns can be deleted or popped like with a dict: When inserting a scalar value, it will naturally be propagated to fill the column: When inserting a that does not have the same index as the , it will be conformed to the DataFrame’s index: You can insert raw ndarrays but their length must match the length of the DataFrame’s index. By default, columns get inserted at the end. inserts at a particular location in the columns: Inspired by dplyr’s verb, DataFrame has an method that allows you to easily create new columns that are potentially derived from existing columns. In the example above, we inserted a precomputed value. We can also pass in a function of one argument to be evaluated on the DataFrame being assigned to. always returns a copy of the data, leaving the original DataFrame untouched. Passing a callable, as opposed to an actual value to be inserted, is useful when you don’t have a reference to the DataFrame at hand. This is common when using in a chain of operations. For example, we can limit the DataFrame to just those observations with a Sepal Length greater than 5, calculate the ratio, and plot: Since a function is passed in, the function is computed on the DataFrame being assigned to. Importantly, this is the DataFrame that’s been filtered to those rows with sepal length greater than 5. The filtering happens first, and then the ratio calculations. This is an example where we didn’t have a reference to the filtered DataFrame available. The function signature for is simply . The keys are the column names for the new fields, and the values are either a value to be inserted (for example, a or NumPy array), or a function of one argument to be called on the . A copy of the original is returned, with the new values inserted. The order of is preserved. This allows for dependent assignment, where an expression later in can refer to a column created earlier in the same . In the second expression, will refer to the newly created column, that’s equal to . The basics of indexing are as follows: Row selection, for example, returns a whose index is the columns of the : For a more exhaustive treatment of sophisticated label-based indexing and slicing, see the section on indexing. We will address the fundamentals of reindexing / conforming to new sets of labels in the section on reindexing. Data alignment between objects automatically align on both the columns and the index (row labels). Again, the resulting object will have the union of the column and row labels. When doing an operation between and , the default behavior is to align the index on the columns, thus broadcasting row-wise. For example: For explicit control over the matching and broadcasting behavior, see the section on flexible binary operations. To transpose, access the attribute or , similar to an ndarray: # only show the first 5 rows Most NumPy functions can be called directly on and . is not intended to be a drop-in replacement for ndarray as its indexing semantics and data model are quite different in places from an n-dimensional array. implements , which allows it to work with NumPy’s universal functions. The ufunc is applied to the underlying array in a . When multiple are passed to a ufunc, they are aligned before performing the operation. Like other parts of the library, pandas will automatically align labeled inputs as part of a ufunc with multiple inputs. For example, using on two with differently ordered labels will align before the operation. As usual, the union of the two indices is taken, and non-overlapping values are filled with missing values. When a binary ufunc is applied to a and , the implementation takes precedence and a is returned. NumPy ufuncs are safe to apply to backed by non-ndarray arrays, for example (see Sparse calculation). If possible, the ufunc is applied without converting the underlying data to an ndarray. A very large will be truncated to display them in the console. You can also get a summary using . (The baseball dataset is from the plyr R package): However, using will return a string representation of the in tabular form, though it won’t always fit the console width: Wide DataFrames will be printed across multiple rows by default: You can change how much to print on a single row by setting the option: You can adjust the max width of the individual columns by setting You can also disable this feature via the option. This will print the table in one block. If a column label is a valid Python variable name, the column can be accessed like an attribute: The columns are also connected to the IPython completion mechanism so they can be tab-completed:"
    },
    {
        "link": "https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.errors.IndexingError.html",
        "document": "Exception is raised when trying to index and there is a mismatch in dimensions.\n\n# IndexingError: indexer may only contain one '...' entry"
    },
    {
        "link": "https://datacamp.com/tutorial/stemming-lemmatization-python",
        "document": "Dive into data science using Python and learn how to effectively analyze and visualize your data. No coding experience or skills needed."
    },
    {
        "link": "https://geeksforgeeks.org/python-lemmatization-with-nltk",
        "document": "Lemmatization is a fundamental text pre-processing technique widely applied in natural language processing (NLP) and machine learning. Serving a purpose akin to stemming, lemmatization seeks to distill words to their foundational forms. In this linguistic refinement, the resultant base word is referred to as a “lemma.” The article aims to explore the use of lemmatization and demonstrates how to perform lemmatization with NLTK.\n\nLemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So, it links words with similar meanings to one word. \n\nText preprocessing includes both Stemming as well as lemmatization. Many times, people find these two terms confusing. Some treat these two as the same. Lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.\n\nOne major difference with stemming is that lemmatize takes a part of speech parameter, “pos” If not supplied, the default is “noun.”\n\nLemmatization techniques in natural language processing (NLP) involve methods to identify and transform words into their base or root forms, known as lemmas. These approaches contribute to text normalization, facilitating more accurate language analysis and processing in various NLP applications. Three types of lemmatization techniques are:\n\nRule-based lemmatization involves the application of predefined rules to derive the base or root form of a word. Unlike machine learning-based approaches, which learn from data, rule-based lemmatization relies on linguistic rules and patterns.\n\nHere’s a simplified example of rule-based lemmatization for English verbs:\n\nRule: For regular verbs ending in “-ed,” remove the “-ed” suffix.\n\nThis approach extends to other verb conjugations, providing a systematic way to obtain lemmas for regular verbs. While rule-based lemmatization may not cover all linguistic nuances, it serves as a transparent and interpretable method for deriving base forms in many cases.\n\nDictionary-based lemmatization relies on predefined dictionaries or lookup tables to map words to their corresponding base forms or lemmas. Each word is matched against the dictionary entries to find its lemma. This method is effective for languages with well-defined rules.\n\nSuppose we have a dictionary with lemmatized forms for some words:\n\nWhen we apply dictionary-based lemmatization to a text like “I was running to become a better athlete, and then I went home,” the resulting lemmatized form would be: “I was run to become a good athlete, and then I go home.”\n\nMachine learning-based lemmatization leverages computational models to automatically learn the relationships between words and their base forms. Unlike rule-based or dictionary-based approaches, machine learning models, such as neural networks or statistical models, are trained on large text datasets to generalize patterns in language.\n\nConsider a machine learning-based lemmatizer trained on diverse texts. When encountering the word ‘went,’ the model, having learned patterns, predicts the base form as ‘go.’ Similarly, for ‘happier,’ the model deduces ‘happy’ as the lemma. The advantage lies in the model’s ability to adapt to varied linguistic nuances and handle irregularities, making it robust for lemmatizing diverse vocabularies.\n\nBelow is the implementation of lemmatization words using\n\nNLTK (Natural Language Toolkit) is a Python library used for natural language processing. One of its modules is the WordNet Lemmatizer, which can be used to perform lemmatization on words.\n\nLemmatization is the process of reducing a word to its base or dictionary form, known as the lemma. For example, the lemma of the word “cats” is “cat”, and the lemma of “running” is “run”.\n• Improves text analysis accuracy: Lemmatization helps in improving the accuracy of text analysis by reducing words to their base or dictionary form. This makes it easier to identify and analyze words that have similar meanings.\n• Reduces data size: Since lemmatization reduces words to their base form, it helps in reducing the data size of the text, which makes it easier to handle large datasets.\n• Better search results: Lemmatization helps in retrieving better search results since it reduces different forms of a word to a common base form, making it easier to match different forms of a word in the text.\n• Time-consuming: Lemmatization can be time-consuming since it involves parsing the text and performing a lookup in a dictionary or a database of word forms.\n• Not suitable for real-time applications: Since lemmatization is time-consuming, it may not be suitable for real-time applications that require quick response times.\n• May lead to ambiguity: Lemmatization may lead to ambiguity, as a single word may have multiple meanings depending on the context in which it is used. In such cases, the lemmatizer may not be able to determine the correct meaning of the word.\n\nHow is lemmatization different from stemming?\n\nWhy is lemmatization important in natural language processing (NLP)?\n\nHow does lemmatization handle different parts of speech?\n\nWhat are some common lemmatization tools or libraries in Python?"
    },
    {
        "link": "https://geeksforgeeks.org/python-lemmatization-approaches-with-examples",
        "document": "The following is a step by step guide to exploring various kinds of Lemmatization approaches in python along with a few examples and code implementation. It is highly recommended that you stick to the given flow unless you have an understanding of the topic, in which case you can look up any of the approaches given below.\n\nWhat is Lemmatization? \n\nIn contrast to stemming, lemmatization is a lot more powerful. It looks beyond word reduction and considers a language’s full vocabulary to apply a morphological analysis to words, aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n\nFor clarity, look at the following examples given below:\n\nVarious Approaches to Lemmatization: \n\nWe will be going over 9 different approaches to perform Lemmatization along with multiple examples and code implementations.\n\n1. Wordnet Lemmatizer \n\nWordnet is a publicly available lexical database of over 200 languages that provides semantic relationships between its words. It is one of the earliest and most commonly used lemmatizer technique.\n• It is present in the nltk library in python.\n• It groups synonyms in the form of synsets.\n• synsets : a group of data elements that are semantically equivalent.\n\nHow to use:\n• Download nltk package : In your anaconda prompt or terminal, type: \n\npip install nltk\n• Download Wordnet from nltk : In your python console, do the following : \n\nimport nltk \n\nnltk.download(‘wordnet’) \n\nnltk.download(‘averaged_perceptron_tagger’)\n\n2. Wordnet Lemmatizer (with POS tag) \n\nIn the above approach, we observed that Wordnet results were not up to the mark. Words like ‘sitting’, ‘flying’ etc remained the same after lemmatization. This is because these words are treated as a noun in the given sentence rather than a verb. To overcome come this, we use POS (Part of Speech) tags. \n\nWe add a tag with a particular word defining its type (verb, noun, adjective etc). \n\nFor Example, \n\n + —> \n\ndriving + verb ‘v’ —> drive\n\ndogs + noun ‘n’ —> dog\n\n3. TextBlob \n\nTextBlob is a python library used for processing textual data. It provides a simple API to access its methods and perform basic NLP tasks.\n\nDownload TextBlob package : In your anaconda prompt or terminal, type: \n\npip install textblob\n\n4. TextBlob (with POS tag) \n\nSame as in Wordnet approach without using appropriate POS tags, we observe the same limitations in this approach as well. So, we use one of the more powerful aspects of the TextBlob module the ‘Part of Speech’ tagging to overcome this problem.\n\n5. spaCy \n\nspaCy is an open-source python library that parses and “understands” large volumes of text. Separate models are available that cater to specific languages (English, French, German, etc.).\n\nIn the above code, we observed that this approach was more powerful than our previous approaches as :\n• Even Pro-nouns were detected. ( identified by -PRON-)\n• Even best was changed to good.\n\n6. TreeTagger \n\nThe TreeTagger is a tool for annotating text with part-of-speech and lemma information. The TreeTagger has been successfully used to tag over 25 languages and is adaptable to other languages if a manually tagged training corpus is available.\n\n7. Pattern \n\nPattern is a Python package commonly used for web mining, natural language processing, machine learning, and network analysis. It has many useful NLP capabilities. It also contains a special feature which we will be discussing below.\n\n8. Gensim \n\nGensim is designed to handle large text collections using data streaming. Its lemmatization facilities are based on the pattern package we installed above.\n• gensim.utils.lemmatize() function can be used for performing Lemmatization. This method comes under the utils module in python.\n• We can use this lemmatizer from pattern to extract UTF8-encoded tokens in their base form=lemma.\n• Only considers nouns, verbs, adjectives, and adverbs by default (all other lemmas are discarded).\n\nIn the above code as you may have already noticed, the gensim lemmatizer ignore the words like ‘the’, ‘with’, ‘by’ as they did not fall into the 4 lemma categories mentioned above. (noun/verb/adjective/adverb)\n\n9. Stanford CoreNLP \n\nCoreNLP enables users to derive linguistic annotations for text, including token and sentence boundaries, parts of speech, named entities, numeric and time values, dependency and constituency parses, sentiment, quote attributions, and relations.\n• CoreNLP is your one stop shop for natural language processing in Java!\n\nConclusion: \n\nSo these are the various Lemmatization approaches that you can refer while working on an NLP project. The selection of the Lemmatization approach is solely dependent upon project requirements. Each approach has its set of pros and cons. Lemmatization is mandatory for critical projects where sentence structure matter like language applications etc."
    },
    {
        "link": "https://nltk.org/book/ch03.html",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python",
        "document": "I wanted to use wordnet lemmatizer in python and I have learnt that the default pos tag is NOUN and that it does not output the correct lemma for a verb, unless the pos tag is explicitly specified as VERB.\n\nMy question is what is the best shot inorder to perform the above lemmatization accurately?\n\nI did the pos tagging using and I am lost in integrating the tree bank pos tags to wordnet compatible pos tags. Please help\n\nI get the output tags in NN,JJ,VB,RB. How do I change these to wordnet compatible tags?\n\nAlso do I have to train with a tagged corpus or can I use it directly on my data to evaluate?"
    }
]