[
    {
        "link": "https://discuss.pytorch.org/t/plotting-loss-curve/42632",
        "document": "Plot your learning curves in real time in your web browser. - nalepae/pierogi"
    },
    {
        "link": "https://digitalocean.com/community/tutorials/vgg-from-scratch-pytorch",
        "document": "Continuing my series on building classical convolutional neural networks that revolutionized the field of computer vision in the last 1-2 decades, we next will build VGG, a very deep convolutional neural network, from scratch using PyTorch. You can see the previous articles in the series on my profile, mainly LeNet5 and AlexNet.\n\nAs before, we will be looking into the architecture and intuition behind VGG and how the results were at that time. We will then explore our dataset, CIFAR100, and load into our program using memory-efficient code. Then, we will implement VGG16 (number refers to the number of layers, there are two versions basically VGG16 and VGG19) from scratch using PyTorch and then train it our dataset along with evaluating it on our test set to see how it performs on unseen data\n\nBuilding on the work of AlexNet, VGG focuses on another crucial aspect of Convolutional Neural Networks (CNNs), depth. It was developed by Simonyan and Zisserman. It normally consists of 16 convolutional layers but can be extended to 19 layers as well (hence the two versions, VGG-16 and VGG-19). All the convolutional layers consists of 3x3 filters. You can read more about the network in the official paper here\n\nBefore building the model, one of the most important things in any Machine Learning project is to load, analyze, and pre-process the dataset. In this article, we’ll be using the CIFAR-100 dataset. This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a “fine” label (the class to which it belongs) and a “coarse” label (the superclass to which it belongs). We’ll be using the “fine” label here. Here’s the list of classes in the CIFAR-100:\n\nWe’ll be working mainly with (used for building the model and training), (for data loading/processing, contains datasets and methods for processing those datasets in computer vision), and (for mathematical manipulation). We will also be defining a variable so that the program can use GPU if available\n\nis a library that provides easy access to tons of computer vision datasets and methods to pre-process these datasets in an easy and intuitive manner\n• We define a function that returns either train/validation data or test data depending on the arguments\n• We start by defining the variable with the mean and standard deviations of each of the channel (red, green, and blue) in the dataset. These can be calculated manually, but are also available online. This is used in the variable where we resize the data, convert it to tensors and then normalize it\n• If the argument is true, we simply load the test split of the dataset and return it using data loaders (explained below)\n• In case is false (default behaviour as well), we load the train split of the dataset and randomly split it into train and validation set (0.9:0.1)\n• Finally, we make use of data loaders. This might not affect the performance in the case of a small dataset like CIFAR100, but it can really impede the performance in case of large datasets and is generally considered a good practice. Data loaders allow us to iterate through the data in batches, and the data is loaded while iterating and not all at once in start into your RAM\n\nTo build the model from scratch, we need to first understand how model definitions work in and the different types of layers that we’ll be using here:\n• Every custom models need to inherit from the class as it provides some basic functionality that helps the model to train.\n• Secondly, there are two main things that we need to do. First, define the different layers of our model inside the function and the sequence in which these layers will be executed on the input inside the function\n\nLet’s now define the various types of layers that we are using here:\n• : These are the convolutional layers that accepts the number of input and output channels as arguments, along with kernel size for the filter. It also accepts any strides or padding if you want to apply those\n• : This applies batch normalization to the output from the convolutional layer\n• : This is the activation applied to various outputs in the network\n• : This applies max pooling to the output with the kernel size given\n• : This is used to apply dropout to the output with a given probability\n• : This is technically not a type of layer but it helps in combining different operations that are part of the same step\n\nUsing this knowledge, we can now build our VGG16 model using the architecture in the paper:\n\nOne of the important parts of any machine or deep learning projects is to optimize the hyper-parameters. Here, we won’t experiment with different values for those but we will have to define them before hand. These include defining the number of epochs, batch size, learning rate, loss function along with the optimizer\n\nWe are now ready to train our model. We’ll first look into how we train our model in and then look at the code:\n• For every epoch, we go through the images and labels inside our and move those images and labels to the GPU if available. This happens automatically\n• We use our model to predict on the labels ( )and then calculate the loss between the predictions and the true labels using our loss function ( )\n• Then we use that loss to backpropagate ( ) and update the weights ( ). But do remember to set the gradients to zero before every update. This is done using\n• Also, at the end of every epoch we use our validation set to calculate the accuracy of the model as well. In this case, we don’t need gradients so we use for faster evaluation\n\nNow, we combine all of this into the following code:\n\nWe can see the output of the above code as follows which does show that the model is actually learning as the loss is decreasing with every epoch:\n\nFor testing, we use exactly the same code as validation but with the :\n\nUsing the above code and training the model for 20 epochs, we were able to achieve an accuracy of 75% on the test set.\n\nLet’s now conclude what we did in this article:\n• We started by understanding the architecture and different kinds of layers in the VGG-16 model\n• Next, we loaded and pre-processed the CIFAR100 dataset using\n• Then, we used to build our VGG-16 model from scratch along with understanding different types of layers available in\n• Finally, we trained and tested our model on the CIFAR100 dataset, and the model seemed to perform well on the test dataset with 75% accuracy\n\nUsing this article, you get a good introduction and hand-on learning but you’ll learn much more if you extend this and see what you can do else:\n• You can try using different datasets. One such dataset is CIFAR10 or a subset of ImageNet dataset.\n• You can experiment with different hyperparameters and see the best combination of them for the model\n• Finally, you can try adding or removing layers from the dataset to see their impact on the capability of the model. Better yet, try to build the VGG-19 version of this model"
    },
    {
        "link": "https://github.com/Leo-xxx/pytorch-notebooks/blob/master/Torn-shirt-classifier/VGG16-transfer-learning.ipynb",
        "document": "To see all available qualifiers, see our documentation .\n\nSaved searches Use saved searches to filter your results more quickly\n\nWe read every piece of feedback, and take your input very seriously.\n\nYou signed in with another tab or window. Reload to refresh your session.\n\nYou signed out in another tab or window. Reload to refresh your session.\n\nYou switched accounts on another tab or window. Reload to refresh your session."
    },
    {
        "link": "https://analyticsvidhya.com/blog/2021/06/transfer-learning-using-vgg16-in-pytorch",
        "document": "This article was published as a part of the Data Science Blogathon\n\nWe’re always told that “Practice makes a man perfect” and we’re made to practice tons of problems in different domains to prepare us for the doom day i.e our final exam. The more variety of problems we solve, the better we get at transferring that knowledge to solve a new problem. What if there’s a way to apply the same technique to solve classification, regression, or clustering problems.\n\nTransfer learning is a technique by which we can use the model weights trained on standard datasets such as ImageNet to improve the efficiency of our given task.\n\nBefore we go further into how transfer learning works, let’s look at the benefits we gain after doing transfer learning. The learning process during transfer learning is:\n• Fast – Normal Convolutional neural networks will take days or even weeks to train, but you can cut short the process with transfer learning.\n• Needs less training data- Being trained on a large dataset, the model can already detect specific features and need less training data to further improve the model.\n\nTo demonstrate transfer learning here, I’ve chosen a simple dataset of the binary classifier which can be found here:\n\nThis data consists of two classes of cats and dogs, i.e 2.5k images for cats and 2.5k images for the dog.\n\nThere are two models available in VGG, VGG-16, and VGG-19. In this blog, we’ll be using VGG-16 to classify our dataset. VGG-16 mainly has three parts: convolution, Pooling, and fully connected layers.\n• Convolution layer- In this layer, filters are applied to extract features from images. The most important parameters are the size of the kernel and stride.\n• Pooling layer- Its function is to reduce the spatial size to reduce the number of parameters and computation in a network.\n• Fully Connected- These are fully connected connections to the previous layers as in a simple neural network.\n\nGiven figure shows the architecture of the model:\n\nTo perform transfer learning import a pre-trained model using PyTorch, remove the last fully connected layer or add an extra fully connected layer in the end as per your requirement(as this model gives 1000 outputs and we can customize it to give a required number of outputs) and run the model.\n\nPreprocessing images before training is a very essential step to avoid errors. Preprocessing can resize the images to the same dimension and transform every image uniformly. different transformation tools available in torchvison.transforms is used for this process.\n\nThe images are loaded using ImageFolder and saved into a data loader. ImageFolder saves the images and their respective labels according to the folders they’re present in, and the dataloader divides the data into different batches for training. Here, a batch size of 8 is chosen.\n\nVisualising the dataset before training the data is a good practice. This can be used to make sure data is loaded properly along with their labels and transformations are applied successfully.\n\nFor this process, the images are saved in a tensor format in a grid, and labels are extracted from the dictionary.\n\nThe pre-trained model can be imported using Pytorch. The device can further be transferred to use GPU, which can reduce the training time.\n\nThe dataset is further divided into training and validation set to avoid overfitting. Some parameters used in this model while training is as follows:\n• Exponential Learning rate scheduler- This reduces the value of learning rate every 7 steps by a factor of gamma=0.1.\n\nA linear fully connected layer is added in the end to converge the output to give two predicted labels.\n\nThese parameters can be chosen according to your own convenience and depending on the dataset.\n\nInitially, we pass the inputs and labels to the model, and we get a predicted value of the label as an output. This predicted value and the actual value of the label are used to compute the cross-entropy loss, which is further used in backpropagation to update the value of weights and biases.\n\nAfter this step, you’ve successfully trained the model.\n\nThe media shown in this article are not owned by Analytics Vidhya and are used at the Author’s discretion."
    },
    {
        "link": "https://discuss.pytorch.org/t/vgg16-using-cifar10-not-converging/114693",
        "document": "I’m training VGG16 model from scratch on CIFAR10 dataset. The validation loss diverges from the start of the training. I have tried with Adam optimizer as well as SGD optimizer. I cannot figure out what it is that I am doing incorrectly. Please point me in the right direction. # Importing Dependencies import os import torch import torch.nn as nn import torch.nn.functional as F from torchvision.datasets import CIFAR10 from torchvision import transforms from torch.utils.data import DataLoader from tqdm import tqdm from datetime import datetime # Defining model arch = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'] class VGGNet(nn.Module): def __init__(self, in_channels, num_classes): super().__init__() self.in_channels = in_channels self.conv_layers = self.create_conv_layers(arch) self.fcs = nn.Sequential( nn.Linear(in_features=512*1*1, out_features=4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(in_features=4096, out_features=4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, num_classes) ) def forward(self, x): x = self.conv_layers(x) # print(x.shape) x = x.reshape(x.shape[0], -1) x = self.fcs(x) return x def create_conv_layers(self, arch): layers = [] in_channels = self.in_channels for x in arch: if type(x) == int: out_channels = x layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), nn.BatchNorm2d(x), nn.ReLU(), ] in_channels = x elif x =='M': layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))] return nn.Sequential(*layers) # Hyperparameters and settings device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(device) TRAIN_BATCH_SIZE = 64 VAL_BATCH_SIZE = 16 EPOCHS = 50 train_data = CIFAR10(root=\".\", train=True, transform=transforms.Compose([transforms.ToTensor()]), download=True) # print(len(train_data)) val_data = CIFAR10(root=\".\", train=False, transform=transforms.Compose([transforms.ToTensor()]), download=True) # print(len(val_data)) train_loader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=8) val_loader = DataLoader(val_data, batch_size=VAL_BATCH_SIZE, shuffle=True, num_workers=8) # print(len(train_loader)) # print(len(val_loader)) num_train_batches = int(len(train_data)/TRAIN_BATCH_SIZE) num_val_batches = int(len(val_data)/VAL_BATCH_SIZE) # Training and Val Loop model = VGGNet(3, 10).to(device) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # optim = torch.optim.Adam(model.parameters(), lr=0.01) scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, verbose=True) # save_path = os.path.join(r\"trained_models\", f'{datetime.now().strftime(\"%m%d_%H%M%S\")}.pth') def train_val(): for epoch in range(1, EPOCHS+1): print(f\"Epoch: {epoch}/20\") model.train() total_loss = 0 for data in train_loader: image, target = data[0], data[1] image, target = image.to(device), target.to(device) optimizer.zero_grad() output = model(image) loss = criterion(output, target) total_loss += loss.item() loss.backward() optimizer.step() print(f\"Loss : {total_loss / num_train_batches}\") save_path = os.path.join(r\"trained_models\", f'{datetime.now().strftime(\"%m%d_%H%M%S\")}_{epoch}.pth') if epoch % 5 == 0: torch.save(model.state_dict(), save_path) with torch.no_grad(): model.eval() total_val_loss = 0 for data in val_loader: image, target = data[0], data[1] image, target = image.to(device), target.to(device) output = model(image) val_loss = criterion(output, target) total_val_loss += val_loss total_val_loss = total_val_loss/num_val_batches print(f\"Val Loss: {total_val_loss}\") scheduler.step(total_val_loss) I trained it till 50 epochs but the val_loss was still seeing almost same numbers on losses.\n• Is there anything wrong with my model?\n• Is there anything wrong in my training or validation loop?\n• What can I try to make it converge?\n\nThe problem is in AI there is something called overfitting. Basically this happens when your model is able to memorize the train dataset but does poorly on the validation or test datasets. Because your model has learned the exact training set it cannot generalize. Here is a good article on it. To fix it you could look at some more transformations found here. I would recommend random rotation, color jitter, and random resized crop but you can mess around with them all to see which do best.\n\nThank you for pointing out the overfitting issue. I wasn’t sure how to tackle it. The change suggested by @patrickwilliams3 definitely helped in converging the val_loss better. However, my model is still overfitting so I’ll try out the transformations suggested by you.\n\nIs it possible your validation accuracy is for a single batch instead of the entire validation set? If so the fluctuation would be perfectly normal since your accuracy is based on only 16 predictions which would fluctuate heavily. Otherwise, the heavy fluctuations in your validation set would not make sense across a larger sample, especially as the training and validation losses steadily decline.\n\nYou are right. I was not reporting the accuracy correctly. I referred to this tutorial to fix my training loop. It looks like this now: def train_val(): for epoch in range(1, EPOCHS+1): print(f\"Epoch: {epoch}/{EPOCHS}\", end='\\t') model.train() running_loss = 0 total = 0 correct = 0 for data in train_loader: image, target = data[0], data[1] image, target = image.to(device), target.to(device) optimizer.zero_grad() output = model(image) loss = criterion(output, target) running_loss += loss.item() _, pred = torch.max(output, dim=1) total += target.size(0) correct += torch.sum(pred == target).item() loss.backward() optimizer.step() print(f\"Training Loss: {running_loss/len(train_loader):.3f}\\tTraining Acc: {correct/total}\", end='\\t') save_path = os.path.join(r\"trained_models\", f'{datetime.now().strftime(\"%m%d_%H%M%S\")}_{epoch}.pth') if epoch % 5 == 0: torch.save(model.state_dict(), save_path) with torch.no_grad(): model.eval() running_val_loss = 0 total = 0 correct = 0 for data in val_loader: image, target = data[0], data[1] image, target = image.to(device), target.to(device) output = model(image) val_loss = criterion(output, target) running_val_loss += val_loss _, pred = torch.max(output, dim=1) correct += torch.sum(pred == target).item() total += target.size(0) running_val_loss = running_val_loss/len(val_loader) print(f\"Val Loss: {running_val_loss:.3f}\\tVal Acc: {correct/total}\") scheduler.step(running_val_loss) Does it look okay to you? Especially how i am calculating losses and accuracies. The training log also makes more sense to me now. Here it is. Also, I had following doubts in my mind.\n\n The validation loss decreases very minimally after epoch 12. However, the validation accuracy keeps on improving till the end. But then again, it overfits by the time training ends. Around which epoch should the training should have stopped? Should I have used EarlyStopping for it or is there some other approach? Thank you being patient with these doubts.\n\nYour code seems fine. Yes the model is overfitting but at least the test accuracy is decent as well. To improve it you probably just want to keep messing around with your data transforms. Try different ones out and see if it improves."
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.argmax.html",
        "document": "Returns the indices of the maximum value of all elements in the tensor.\n\nThis is the second value returned by . See its documentation for the exact semantics of this method.\n\nReturns the indices of the maximum values of a tensor across a dimension.\n\nThis is the second value returned by . See its documentation for the exact semantics of this method."
    },
    {
        "link": "https://pytorch.org/docs/master/generated/torch.argmax.html",
        "document": ""
    },
    {
        "link": "https://discuss.pytorch.org/t/argmax-with-pytorch/1528",
        "document": "(hard) is not differentiable in general (this has nothing to do with PyTorch), i.e. one can not use gradient based methods with argmax. See e.g. https://www.reddit.com/r/MachineLearning/comments/4e2get/argmax_differentiable/ on how to train models involving argmax functions. One potential alternative suggested there is to use softmax instead.\n\nAny ideas for how to use this max function in a differentiable way? A custom loss function i’m writing has to do with the indices of max values. Not sure how to redo the loss function such that it uses differentiable components.\n\nI am not sure if I understand your problem. Do you want to have gradients with respect to indices? Well, indices are integers by definition and you cannot take derivatives of a function with respect to a variable that is defined over the integers only…\n\nYeah I found the zero to be confusing too. It’s the dimension along which you want to find the max. I was getting confused because in my case, the thing I wanted to find the max of had shape (1, 49), which meant when I did , I would just get back the whole array, and it didn’t make any sense. I needed to do , and indeed that returned (max value, index)\n\nyou can now do torch.argmax(preds, dim=1) in version 0.4.0 @BlakeWest dimension 0 is the batch and dimension 1 is the class probabilities (assuming you use softmax on your final output). Therefore you would want to to do an argmax along dimension 1 ie. the class with the highest probabilities\n\nhello. How can I manipulate the by the , for example what should I do if I want to change the value of elements that corresponding to the ."
    },
    {
        "link": "https://restack.io/p/pytorch-answer-activation-functions",
        "document": "Explore various activation functions in Pytorch, their applications, and how they impact neural network performance.\n\nActivation functions play a crucial role in neural networks, particularly in introducing non-linearity into the model. This section delves into the most commonly used activation functions in PyTorch, providing insights into their characteristics and applications. The sigmoid function is one of the earliest activation functions used in neural networks. It maps any input to a value between 0 and 1, making it particularly useful for binary classification tasks. The formula for the sigmoid function is:\n• Derivative: The derivative of the sigmoid function can be computed as , which is useful for backpropagation.\n• Limitations: It suffers from the vanishing gradient problem, especially for deep networks. The tanh function is another popular activation function that outputs values between -1 and 1. It is often preferred over the sigmoid function because it centers the data, leading to faster convergence during training.\n• Derivative: The derivative can be expressed as , which helps in gradient calculations.\n• Advantages: It mitigates the vanishing gradient problem better than the sigmoid function. ReLU has become the default activation function for many types of neural networks due to its simplicity and effectiveness. It outputs the input directly if it is positive; otherwise, it outputs zero.\n• Derivative: The derivative is 1 for positive inputs and 0 for negative inputs, which can lead to dead neurons if not managed properly.\n• Usage: Commonly used in hidden layers of deep networks. Leaky ReLU is a variant of ReLU that allows a small, non-zero gradient when the input is negative. This helps to keep the neurons active during training.\n• Advantages: Addresses the dead neuron problem associated with standard ReLU. The softmax function is typically used in the output layer of a multi-class classification model. It converts raw scores (logits) into probabilities that sum to one.\n• Range: (0, 1) for each class, with the sum equal to 1. In summary, understanding these activation functions is essential for building effective neural networks in PyTorch. Each function has its unique properties and use cases, making them suitable for different types of tasks in deep learning.\n• None Explore Pytorch AMP for efficient mixed precision training, enhancing performance and reducing memory usage in deep learning.\n• None Explore how Pytorch leverages GPU capabilities for enhanced performance in deep learning tasks and model training.\n\nIn PyTorch, the and functions are essential for various tasks, particularly in classification and ranking problems. These functions allow you to extract the indices of the maximum values in a tensor, which is crucial for understanding model predictions and performance. The function returns the index of the maximum value in a tensor along a specified dimension. This is particularly useful when you want to determine the predicted class in a classification task. Here’s a simple example: import torch # Create a sample tensor scores = torch.tensor([[0.1, 0.3, 0.6], [0.4, 0.5, 0.1]]) # Get the indices of the maximum values along the last dimension predicted_classes = torch.argmax(scores, dim=1) print(predicted_classes) # Output: tensor([2, 1]) The function is used to retrieve the top k elements from a tensor along a specified dimension. This is particularly useful in scenarios where you want to evaluate the top predictions of a model. Here’s how you can use it: import torch # Create a sample tensor scores = torch.tensor([[0.1, 0.3, 0.6], [0.4, 0.5, 0.1]]) # Get the top 2 scores and their indices values, indices = torch.topk(scores, k=2, dim=1) print(values) # Output: tensor([[0.6, 0.3], [0.5, 0.4]]) print(indices) # Output: tensor([[2, 1], [1, 0]])\n• Classification Tasks: Use to determine the predicted class from the output of a neural network.\n• Recommendation Systems: Use to find the most relevant items based on user preferences.\n• Performance Evaluation: Both functions can be used to analyze model performance by comparing predicted classes against true labels. By effectively utilizing and , you can enhance your model's interpretability and performance evaluation, making these functions indispensable tools in your PyTorch toolkit.\n• None Learn about broadcasting in Pytorch, a powerful feature that simplifies tensor operations and enhances computational efficiency.\n• None Explore the fundamentals of Pytorch, covering tensors, operations, and neural networks for efficient deep learning.\n\nIn PyTorch, the function is essential for identifying the indices of the maximum values along a specified dimension of a tensor. This function is particularly useful in various applications, including classification tasks where you need to determine the predicted class from the output probabilities. The function can be utilized as follows: import torch # Create a sample tensor tensor = torch.tensor([[1, 2, 3], [4, 5, 6]]) # Get the indices of the maximum values along dimension 0 max_indices = torch.argmax(tensor, dim=0) print(max_indices) # Output: tensor([1, 1, 1]) This code snippet demonstrates how to find the indices of the maximum values in each column of a 2D tensor. The parameter specifies the dimension along which to perform the operation. The function in PyTorch is crucial for performing backpropagation. It computes the gradient of a tensor with respect to some scalar value, typically the loss. This is essential for optimizing models during training. To illustrate, consider the following example: In this example, we compute the gradients of the tensor with respect to the result. The method calculates the derivatives, which can then be used for updating the model parameters during training.\n• The function is used to find the index of the maximum value in a tensor, which is vital for classification tasks.\n• The function is used to compute gradients, enabling the optimization of model parameters.\n• Both functions are integral to the training process in PyTorch, allowing for efficient computation and model updates. For more detailed information, refer to the official PyTorch documentation: PyTorch Documentation.\n• None Learn about broadcasting in Pytorch, a powerful feature that simplifies tensor operations and enhances computational efficiency.\n• None Explore the fundamentals of Pytorch, covering tensors, operations, and neural networks for efficient deep learning. Build reliable and accurate AI agents in code, capable of running and persisting month-lasting processes in the background."
    },
    {
        "link": "https://annie-wangliu.medium.com/understand-the-basic-6-functions-in-pytorch-675d78a1d743",
        "document": "When I started to learn PyTorch, I found that there are various functions which seem vague to understand for me. Today I would like to summarize them with examples, which I think helpful greatly.\n\nYou can also check the explanations from the official website one by one, but summarizing them together helps me.\n\nsqueeze(i): it is kind of dimension reduction: if the original dimension is 1, then it can be reduced. Let’s check an example:\n\nIn the example, the original tensor shape is [2,1,4]. Because the dimension of index 0 is 2, it can’t be reduced and the size keeps the same, but the dimension of index 1 is 1, it can be reduced and the tensor from 3 dimensions into 2 dimension matrix."
    }
]