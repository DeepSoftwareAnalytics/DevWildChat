[
    {
        "link": "https://medium.com/@alxkm/java-event-driven-architecture-dc456d324ba5",
        "document": "Some aspects of event driven architecture with examples\n\nNowadays, one of the most important and interesting architecture patterns is event-driven architecture. Let’s imagine and analyze such a practical butt. It is clear that we are developing a microservice project for purchasing tickets. We have divided the domain model of the project so much that we have identified 4 microservices:\n\nWe will have for the four main pages in the application:\n• Allows users to search for routes for their trips.\n• Users can enter their starting point and destination to find available routes.\n• The page displays various options for routes, including duration, cost, and stops along the way.\n• Users can select a route and proceed to book tickets or place an order.\n• Users can select a route from the options provided on the Find Route for Trip page.\n• The page allows users to enter details such as the number of tickets, passenger information, and any additional preferences.\n• Once the order details are entered, users can proceed to the payment stage.\n• This page handles the payment process for the order placed on the Place Order page.\n• Users can choose from various payment methods such as credit/debit card, online banking, or digital wallets.\n• After completing the payment, users receive a confirmation of their order, including ticket details and trip information.\n• Users can proceed to view their order details or cancel the order if necessary.\n• Users can cancel their order at two stages: while placing the order and after purchasing the order.\n• During the placing order stage, users can cancel the order before proceeding to payment.\n• After purchasing the order, users can cancel it within a specified time frame, subject to cancellation policies.\n• The Cancel Order page allows users to view their order history and select the order they wish to cancel.\n• Users confirm the cancellation, and the system processes the request, providing a confirmation of the cancellation.\n\nThese pages together form the main user journey for booking trips, placing orders, and managing bookings in the application. Each page serves a specific purpose and provides users with the necessary functionality to complete their desired actions.\n\nEach stage is represented as a distinct screen within the client application. During the payment process, users choose their preferred payment method and its associated details. They then proceed to either confirm the payment or opt to save the order, reserving a spot with the flexibility to pay at a later time.\n\nPayment for an order, like any financial transaction, can take a considerable amount of time. It may involve other financial (banking) websites and systems, including authorization processes, and so forth. How does our system know that the payment has been successfully processed? When we send a request to the payment system, we trigger a callback from our API, which will be the payment service API. But how do other services become aware of this? For example, the ticketing service will need to update the order status, the notification service will need to inform the client and send the ticket along with the notification. The trip service will also be involved in this chain because it needs to mark a seat as sold.\n\nPayment operations must adhere to the principles of atomicity, consistency, isolation, and durability (ACID) to ensure reliability and data integrity. In a distributed environment involving multiple microservices, payment transactions become distributed transactions, requiring careful handling to ensure correctness and rollback in case of errors.\n\nImplementing such a system typically involves using standard approaches like REST APIs. However, handling complex scenarios such as service unavailability can be challenging. Storing failed cases for future processing may introduce delivery issues, as it requires a mechanism to guarantee eventual processing.\n\nMoreover, ensuring atomicity and rollback operations often necessitates supporting operations akin to those in the SAGA pattern. This approach involves orchestrating a series of compensating actions to revert changes made during a transaction in case of failure. Thus, designing a robust payment system involves considering these factors and implementing appropriate error handling and rollback mechanisms.\n\nTo solve this we need to avoid coupling of our microservices, and resolve problem of storing and correctly handling system falls. And looks like here will be good way to use some messaging architectures and patterns.\n\nEvent-driven architecture (EDA) is a design pattern where components within a system communicate with each other by producing and consuming events. In EDA, components are loosely coupled and interact asynchronously through events. Here’s how we can implement an event-driven architecture for ticket system:\n• Events: Define the types of events that will be exchanged between services. In this example, events may include “TicketReservedEvent,” “TripScheduledEvent,” “NotificationSentEvent,” and “PaymentProcessedEvent.”\n• Event Producer: Each service acts as an event producer, generating events when certain actions occur. For example, the Ticketing Service produces a “TicketReservedEvent” when a ticket is reserved.\n• Event Consumer: Services subscribe to events they are interested in and react accordingly. For instance, the Notification Service subscribes to the “TicketReservedEvent” and sends notifications to the user.\n• Event Bus/Queue: Use a message broker or a queue system to facilitate communication between services. Common choices include Apache Kafka, RabbitMQ, or Amazon SQS. For this example, we can use Apache Kafka due to its scalability and fault-tolerance.\n• Integration: Each service integrates with the chosen message broker to publish events and consume events relevant to its functionality.\n• Loose Coupling: EDA promotes loose coupling between system components. Components communicate indirectly through events, allowing them to evolve independently without affecting each other.\n• Scalability: EDA enables horizontal scalability by distributing event processing across multiple nodes. As the system grows, you can add more event consumers to handle the load.\n• Flexibility: EDA provides flexibility in system design. New features can be added by introducing new event types and handlers without modifying existing components.\n• Asynchronous Communication: Events are typically processed asynchronously, which improves system responsiveness and resilience. Components can continue functioning even if some event consumers are temporarily unavailable.\n• Decoupled Systems Integration: EDA facilitates integration between disparate systems. Systems can exchange data and trigger actions by publishing and subscribing to events, without direct dependencies.\n• Complexity: Implementing EDA introduces additional complexity, especially in managing event routing, handling failures, and ensuring event consistency across the system.\n• Event Ordering: Ensuring the correct order of events can be challenging in EDA, especially in distributed systems. Events may arrive out of order, requiring mechanisms to handle this scenario.\n• Debugging and Monitoring: Tracking and debugging event flows in EDA systems can be difficult, especially in complex scenarios with multiple event producers and consumers.\n• Eventual Consistency: EDA often relies on eventual consistency, where updates to the system propagate through events asynchronously. Achieving strong consistency may require additional coordination and complexity.\n• Potential Overhead: Handling events and managing event-driven infrastructure may introduce overhead, especially in terms of resource utilization and latency compared to synchronous communication.\n\nNow, let’s define the main classes for each service:\n• TripScheduledEventHandler: Reacts to the “TripScheduledEvent” by updating trip-related data.\n• NotificationSentEvent: Event indicating that a notification has been sent.\n• NotificationSentEventHandler: Processes the “NotificationSentEvent” to record sent notifications.\n• EventQueue: Represents the message broker or queue system used for event communication. For Apache Kafka, use the KafkaProducer and KafkaConsumer classes to publish and consume events, respectively.\n\nBy implementing this event-driven architecture, the system becomes more resilient, scalable, and decoupled, allowing for easier maintenance and evolution of the individual services.\n\nThis article provides a concise overview of the advantages and disadvantages of event-driven architecture (EDA). While it offers a glimpse into the benefits and challenges associated with this architectural approach, a deeper exploration of its implementation can be achieved through the utilization of Java technologies such as Kafka and Spring Integration. It’s worth noting that the examples and discussions in this article are tailored specifically for Java-based environments."
    },
    {
        "link": "https://connect2grp.medium.com/understanding-event-driven-design-pattern-in-microservices-with-java-4b7c46db0b3b",
        "document": "Today, We will go through an Overview of Understanding Event-Driven Design Pattern in Microservices with Java and the benefits it brings to Application Developers.\n\nIntroduction:\n\nEvent Sourcing: Event Sourcing is a crucial concept in the context of CQRS and Event-Driven Design (EDD). In traditional systems, the state of an application is typically stored in a database, and changes to the state are recorded as updates. However, in event sourcing, the state of an application is derived from a series of events.\n\nEvents represent meaningful occurrences or transactions that happen within the system. Each event captures the details of a specific action or state change, such as user registrations, orders placed, or inventory updates. These events are immutable and are stored in an event log.\n\nBy replaying the events sequentially, the application can reconstruct the state of the system at any given point in time. This event-centric approach provides a comprehensive audit trail, enables temporal querying, and facilitates the implementation of features like event replay, time-travel debugging, and undo/redo functionality."
    },
    {
        "link": "https://stackoverflow.com/questions/46615008/microservices-service-discovery-circuit-breaker-for-event-driven-architecture",
        "document": "I believe there is a misunderstanding in the question in that you assume that event-driven architectures cannot be implemented on top of HTTP.\n\nAn event-driven architecture may be implemented in many different ways and (when the architecture is that of a distributed system), on top of many different protocols.\n\nIt can be implemented using a message broker (i.e. Kafka, RabbitMQ, ActiveMQ, etc) as you suggested it too. However, this is just a choice and certainly not the only way to do it.\n\nFor example, the seminal book Building Microservices by Sam Newman, in Chapter 4: Integration, under Implementing Asynchronous Event-Based Collaboration says:\n\n“Another approach is to try to use HTTP as a way of propagating events. ATOM is a REST-compliant specification that defines semantics (among other things) for publishing feeds of resources. Many client libraries exist that allow us to create and consume these feeds. So our customer service could just publish an event to such a feed when our customer service changes. Our consumers just poll the feed, looking for changes. On one hand, the fact that we can reuse the existing ATOM specification and any associated libraries is useful, and we know that HTTP handles scale very well. However, HTTP is not good at low latency (where some message brokers excel), and we still need to deal with the fact that the consumers need to keep track of what messages they have seen and manage their own polling schedule. I have seen people spend an age implementing more and more of the behaviors that you get out of the box with an appropriate message broker to make ATOM work for some use cases. For example, the Competing Consumer pattern describes a method whereby you bring up multiple worker instances to compete for messages, which works well for scaling up the number of workers to handle a list of independent jobs. However, we want to avoid the case where two or more workers see the same message, as we’ll end up doing the same task more than we need to. With a message broker, a standard queue will handle this. With ATOM, we now need to manage our own shared state among all the workers to try to reduce the chances of reproducing effort. If you already have a good, resilient message broker available to you, consider using it to handle publishing and subscribing to events. But if you don’t already have one, give ATOM a look, but be aware of the sunk-cost fallacy. If you find yourself wanting more and more of the support that a message broker gives you, at a certain point you might want to change your approach.”\n\nLikewise, if your design uses a message broker for the event-driven architecture, then I'm not sure if a circuit breaker is needed, because in that case the consumer applications control the rate at which event messages are being consumed from the queues. The producer application can publish event messages at its own pace, and the consumer applications can add as many competing consumers as they want to keep up with that pace. If the server application is down the client applications can still continue consuming any remaining messages in the queues, and once the queues are empty, they will just remain waiting for more messages to arrive. But that does not put any burden on the producer application. The producer and the consumer applications are decoupled in this scenario, and all the work the circuit breaker does in other scenarios would be solved by the message broker application.\n\nSomewhat similar can be said of the service discovery feature. Since the producer and the consumer do not directly talk to each other, but only through the message broker, then the only service you need to discover would be the message broker."
    },
    {
        "link": "https://dzfweb.gitbooks.io/microsoft-microservices-book/content/multi-container-microservice-net-applications/integration-event-based-microservice-communications.html",
        "document": "As described earlier, when you use event-based communication, a microservice publishes an event when something notable happens, such as when it updates a business entity. Other microservices subscribe to those events. When a microservice receives an event, it can update its own business entities, which might lead to more events being published. This publish/subscribe system is usually performed by using an implementation of an event bus. The event bus can be designed as an interface with the API needed to subscribe and unsubscribe to events and to publish events. It can also have one or more implementations based on any inter-process or messaging communication, such as a messaging queue or a service bus that supports asynchronous communication and a publish/subscribe model.\n\nYou can use events to implement business transactions that span multiple services, which gives you eventual consistency between those services. An eventually consistent transaction consists of a series of distributed actions. At each action, the microservice updates a business entity and publishes an event that triggers the next action.\n\nThis section describes how you can implement this type of communication with .NET by using a generic event bus interface, as shown in Figure 8-18. There are multiple potential implementations, each using a different technology or infrastructure such as RabbitMQ, Azure Service Bus, or any other third-party open source or commercial service bus.\n\nUsing message brokers and services buses for production systems\n\nAs noted in the architecture section, you can choose from multiple messaging technologies for implementing your abstract event bus. But these technologies are at different levels. For instance, RabbitMQ, a messaging broker transport, is at a lower level than commercial products like Azure Service Bus, NServiceBus, MassTransit, or Brighter. Most of these products can work on top of either RabbitMQ or Azure Service Bus. Your choice of product depends on how many features and how much out-of-the-box scalability you need for your application.\n\nFor implementing just an event bus proof-of-concept for your development environment, as in the eShopOnContainers sample, a simple implementation on top of RabbitMQ running as a container might be enough. But for mission-critical and production systems that need high scalability, you might want to evaluate and use Azure Service Fabric. If you require high-level abstractions and richer features like sagas for long-running processes that make distributed development easier, other commercial and open-source service buses like NServiceBus, MassTransit, and Brighter are worth evaluating. Of course, you could always build your own service bus features on top of lower-level technologies like RabbitMQ and Docker, but the work needed to reinvent the wheel might be too costly for a custom enterprise application.\n\nTo reiterate: the sample event bus abstractions and implementation showcased in the eShopOnContainers sample are intended to be used only as a proof of concept. Once you have decided that you want to have asynchronous and event-driven communication, as explained in the current section, you should choose the service bus product that best fits your needs.\n\nIntegration events are used for bringing domain state in sync across multiple microservices or external systems. This is done by publishing integration events outside the microservice. When an event is published to multiple receiver microservices (to as many microservices as are subscribed to the integration event), the appropriate event handler in each receiver microservice handles the event.\n\nAn integration event is basically a data-holding class, as in the following example:\n\nThe integration event class can be simple; for example, it might contain a GUID for its ID.\n\nThe integration events can be defined at the application level of each microservice, so they are decoupled from other microservices, in a way comparable to how ViewModels are defined in the server and client. What is not recommended is sharing a common integration events library across multiple microservices; doing that would be coupling those microservices with a single event definition data library. You do not want to do that for the same reasons that you do not want to share a common domain model across multiple microservices: microservices must be completely autonomous.\n\nThere are only a few kinds of libraries you should share across microservices. One is libraries that are final application blocks, like the Event Bus client API, as in eShopOnContainers. Another is libraries that constitute tools that could also be shared as NuGet components, like JSON serializers.\n\nAn event bus allows publish/subscribe-style communication between microservices without requiring the components to explicitly be aware of each other, as shown in Figure 8-19.\n\nThe event bus is related to the Observer pattern and the publish-subscribe pattern.\n\nIn the Observer pattern, your primary object (known as the Observable) notifies other interested objects (known as Observers) with relevant information (events).\n\nThe purpose of the Pub/Sub pattern is the same as the Observer pattern: you want to notify other services when certain events take place. But there is an important semantic difference between the Observer and Pub/Sub patterns. In the Pub/Sub pattern, the focus is on broadcasting messages. In contrast, in the Observer pattern, the Observable does not know who the events are going to, just that they have gone out. In other words, the Observable (the publisher) does not know who the Observers (the subscribers) are.\n\nHow do you achieve anonymity between publisher and subscriber? An easy way is let a middleman take care of all the communication. An event bus is one such middleman.\n\nAn event bus is typically composed of two parts:\n• None One or more implementations.\n\nIn Figure 8-19 you can see how, from an application point of view, the event bus is nothing more than a Pub/Sub channel. The way you implement this asynchronous communication can vary. It can have multiple implementations so that you can swap between them, depending on the environment requirements (for example, production versus development environments).\n\nIn Figure 8-20 you can see an abstraction of an event bus with multiple implementations based on infrastructure messaging technologies like RabbitMQ, Azure Service Bus, or other service buses like NServiceBus, MassTransit, etc.\n\nHowever, as highlighted previously, using abstractions (the event bus interface) is possible only if you need basic event bus features supported by your abstractions. If you need richer service bus features, you should probably use the API provided by your preferred service bus instead of your own abstractions.\n\nLet’s start with some implementation code for the event bus interface and possible implementations for exploration purposes. The interface should be generic and straightforward, as in the following interface.\n\nThe Publish method is straightforward. The event bus will broadcast the integration event passed to it to any microservice subscribed to that event. This method is used by the microservice that is publishing the event.\n\nThe Subscribe method is used by the microservices that want to receive events. This method has two parts. The first is the integration event to subscribe to (IntegrationEvent). The second part is the integration event handler (or callback method) to be called (IIntegrationEventHandler<T>) when the microservice receives that integration event message."
    },
    {
        "link": "https://openlegacy.com/blog/microservices-architecture-patterns",
        "document": ""
    },
    {
        "link": "https://medium.com/@mesutpiskin/building-a-distributed-job-scheduler-for-microservices-8b7ab2ce5f91",
        "document": "The scheduler service is a system that manages the execution of jobs, typically based on a schedule or some other trigger. It consists of three components: a job trigger, a scheduler jobs data collector/consumer, and a job executor. The job trigger determines when a job should be run, the data collector/consumer gathers information about the jobs and passes it to the executor, and the executor runs the jobs according to their definitions. The scheduler service ensures that jobs are run in a timely and efficient manner.\n\nThe job trigger module is a part of a larger software system. Its main function is to retrieve the necessary information about a scheduled job from a database and initiate the execution of the job according to a cron schedule.\n\nThe data collector/consumer module is another component of the system that is responsible for receiving data about scheduled jobs from the scheduler and sending it to the appropriate business services using the Kafka messaging system. This module also prepares the data for further processing.\n\nThe job executor module is the final component of the system and is responsible for executing the business logic of the scheduled jobs and sending notifications about their status after completion.\n\nIn this implementation, the system is built using the Java Spring Framework, which is a popular framework for developing applications in Java. The data is stored in a PostgreSQL database, and Kafka is used as the messaging system to facilitate communication between different components of the system. While Kafka is a good choice for an event bus, in this case, other event bus products such as RabbitMQ can also be used.\n\nThis module is part of the job schedule microservice and is responsible for getting the job definition from the database and triggering jobs with the cron parameter. Job definition has different properties: id, job name, description, status, cron definition, handler method, and owner service. The handler method is your business code in the scheduler service, and the owner service is a data retrieval business service.\n\nJob triggers get job description data from the database and perform jobs when the time comes, as defined by cron.\n\nFor a basic implementation of the Job Trigger module:\n\nThe data collector/consumer module is responsible for communicating scheduler job data with business services over Kafka and preparing data for the job executor. This module listens to the Kafka topic where the business services publish their data, process the data, and sends it to the job executor. The data collector/consumer also maintains the status of the jobs, such as “pending,” “in progress,” and “completed,” and updates the database accordingly. This module is crucial for ensuring that the job executor has the right data at the right time to perform the job.\n\nThis is an example event data that the data collector/consumer module can process and send to the job executor:\n\nThis event data contains user information for the “first-job” job. The data collector/consumer module will receive this data, process it, and send it to the job executor once the job is completed. This ensures that the job can be executed using up-to-date data.\n\nFor a basic implementation of the Data Collector module, using the Java Spring Framework and Kafka:\n\nThis code listens to a Kafka topic named “data-collection-service” and tries to process the incoming data. It updates the status of the job and sends the data to the worker module. This is just an example and you can modify the code to suit your needs.\n\nThe job executor module is responsible for executing the business code associated with the job and sending a notification after the job is completed. This module receives the data from the data collector/consumer and uses the “handler” method specified in the job definition to execute the business code. After the job is completed, the job executor sends a notification to the owner service and updates the job status in the database. This module is essential for ensuring that the business code is executed correctly and that the owner service is notified of the job’s completion.\n\nIn conclusion, our scheduler service is a powerful tool that can help us automate your jobs in a user-friendly and platform-independent way. By using a scheduler service with event-driven architecture, you can make your jobs highly available, compatible with distributed environments, extendable, retryable, and monitorable. With the right technology stack and design, you can develop a custom scheduler service that meets your specific needs.\n\nOne of the main reasons why our custom scheduling system is better than products like Quartz, Hangfire, etc. is its compatibility with distributed environments. Our system is designed to be highly available and can handle failures gracefully, ensuring that jobs are executed consistently and reliably. In addition, our system is extendable and can be easily customized to meet the specific needs of different services. This makes it more flexible and versatile than products like Quartz, Hangfire, etc., which may not offer the same level of customization.\n\nAnother advantage of our custom solution is its ability to retry failed jobs. This is especially important in a microservice architecture, where services may depend on each other and a failure in one service can affect the others. By implementing a retry mechanism, we can ensure that jobs are executed successfully even if there are temporary failures or errors. Overall, our custom scheduling system offers a number of benefits over products like Hangfire. It is highly available, compatible with distributed environments, extendable, retryable, and monitorable, making it a superior solution for automating jobs in a microservice architecture."
    },
    {
        "link": "https://articles.microservices.com/designing-a-cron-scheduler-microservice-18a52471d13f",
        "document": "When building a new application starting with microservices is really hard and maybe time-wasting if you don't know the domain very well.\n\nIn scenarios where you even don’t know what the product will be, designing decoupled services itself may require several hours of the team and might be not possible at all. It really sounds premature but it's not a excuse to always start building a next monolith.\n\nOne easy service you could start extracting from your domain is the very hand cron job.\n\nWhy would you create a cron service out from your main application?\n• You aren't going to keep one main application for a long time right? Right so we need to find a place to have our cron service.\n• You will definitely have several problems when you start clustering.\n\nI remember the pain we had running cron jobs inside our majestic monolith on cluster environment. An solution was using locking and concurrency control on database.\n\nLet’s take a look at this example:\n\nThe application was running in a cluster with eight machines, so on the application startup the cron started to schedule a event in the same time in all machines. As we were controlling concurrency, only one machine was able to schedule a event. This way we avoid all machines scheduling and sending events in the same time.\n\nThis complexity was spread over several projects in the company. So I decided to extract a service which only handles event scheduling. This service would require only a little hardware capability, a perfect fit to run in a docker container so we would not need a machine to have only this service. With docker you can have several containers running on the same machine.\n\nThere are two options we could design the cron service:\n• Scheduling cron tasks by hand and send a amqp message or http request.\n\nLet’s take a look at our cron service:\n\nI would expose the simple one, scheduling cron tasks by hand.\n\nThe cron service should not handle the task itself, instead it should notify other applications to execute it.\n\nYou may want to notify producing a amqp message. I find amqp more elegant. You can design your system in a truly async fashion, and also it is safe as it has delivery warranty capabilities.\n\nWhat if the client does not speak amqp?\n\nAn solution would be sending a request over http with a retry mechanism which is really elegant was well. The client should only expose a url so we could callback.\n\nTake a look at the the check unprocessed orders function which triggers the ESB for fetching unprocessed orders on SAP and SalesForce respectively.\n\nThe other option is design the service through a rest api. You could allow any other application in your company to create cron events dynamically:\n\nI love the rest approach, this is really useful for large applications, but looks too much for the most cases.\n\nI have an example of both approaches written in go and elixir. The service in go is the simple option and the elixir demo exposes a rest to create, update and remove cron jobs. Both are already containerized\n\nThis was the second post in a series dealing with small services. In the next one we will create a event trace service, see you soon."
    },
    {
        "link": "https://quora.com/In-a-microservice-architecture-how-can-I-schedule-services-to-do-work-at-a-given-time",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://reddit.com/r/microservices/comments/11284zd/scheduler_for_microservices",
        "document": "Currently having few microservices built in Spring Boot that have cron settings. Problem is that I'd like to manage the scheduling from a central point, I find it wrong to do it per microservice.\n\nWhat is the best way to manage cron for all of them? What is the correct approach here?"
    },
    {
        "link": "https://bmc.com/blogs/microservices-best-practices",
        "document": ""
    }
]