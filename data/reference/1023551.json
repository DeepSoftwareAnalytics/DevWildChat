[
    {
        "link": "https://askubuntu.com/questions/1236553/mpich-installation",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://askubuntu.com/questions/1010438/how-can-i-install-mpich-library",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://mpich.org/static/downloads/4.0.1/mpich-4.0.1-installguide.pdf",
        "document": ""
    },
    {
        "link": "https://rantahar.github.io/introduction-to-mpi/setup.html",
        "document": "Make sure you can compile C or Fortran programs using a compiler or a development environment. You will need an implementation of the MPI (Message Passing Interface) library. Several implementations of MPI exist, but for example Open MPI will work on Linux and macOS, and the Microsoft Distribution of MPICH will work on Windows.\n\nMost Linux distributions will have an or package. On Ubuntu, open a terminal and run:\n\nCheck that you Open MPI is properly installed by running:\n\nThe output should be similar to this:\n\nOn macOS you can install Open MPI for the command line using homebrew. After installing Homebrew, open the Terminal in Applications/Utilities and run:\n\nThe output should be similar to this:\n\nOn windows, the preferred option is to use the Cygwin terminal emulator or the Windows Subsystem for Linux for Windows 10. Both provide an interface that is similar to most HPC clusters.\n\nIf you decide to use Cygwin, you need to install some packages using the Cygwin installer. If you already have Cygwin installed, you nevertheless should run the installer and make sure the following packages are selected:\n• , or your text editor of choice\n\nYou can verify that it works by compiling any C or Fortran program using the or compiler.\n\nOn Windows, if you use Visual Studio, you can install Microsoft MPI. The download includes two files, and . Download and run both installers. Follow these instructions to create a project with the MPI compiler and library.\n\nScalasca is an open source application for profiling MPI programs. We use Scalasca in one of the afternoon lessons. Downloading and installing at least the CubeGUI is useful. If you have access to a cluster and Scalasca is installed, you don’t need to install Scalasca itself on your laptop.\n• The Cubelib library, used by Scalasca and CubeGUI.\n\nCubeGUI can be used to view profiling information produced on different platforms. Binary packages are provided for Windows and macOS on the Scalasca Downloads website.\n\nTo install it on Linux, download the Cube Bundle on the same website. Compile and install with:\n\nYou may also wish to install Scalasca itself. This allows you to run the profiler on your laptop. You can use Scalasca to profile MPI programs on your local system.\n\nStart with Score-P. Download it from the scorep website and run:\n\nFinally install Scalasca itself. Download it from the Scalasca Downloads page. Install in the same way:"
    },
    {
        "link": "https://stackoverflow.com/questions/35235541/where-did-mpich-install",
        "document": "I used in the Terminal. Everything installed just fine. But now I must edit an rc file for mgrid2 according to these instructions. One thing I must specify is the path where mpich is installed and I cannot find it! I did find its .h files in /usr/include but am certain that isn't the correct path. I've pasted the install output below.\n\nAny thoughts on how to find where mpich installed? I'm running Ubuntu 15.10.\n\nThe instructions suggest the path but I don't have a just a and it's not there.\n\nPoint is, nothing I've tried so far had the subdirectory ch_p4 in it when I did an ."
    },
    {
        "link": "https://curc.readthedocs.io/en/latest/programming/MPI-C.html",
        "document": "Using MPI with C#\n\nParallel programs enable users to fully utilize the multi-node structure of supercomputing clusters. Message Passing Interface (MPI) is a standard used to allow several different processors on a cluster to communicate with each other. In this tutorial we will be using the Intel C++ Compiler, GCC, IntelMPI, and OpenMPI to create a multiprocessor ‘hello world’ program in C++. This tutorial assumes the user has experience in both the Linux terminal and C++.\n\nBegin by logging into the cluster and logging in to a compile node. This can be done by loading the Alpine scheduler and using the command: Next we must load MPI into our environment. Begin by loading in your choice of C++ compiler and its corresponding MPI library. Use the following commands if using the GNU C++ compiler: This should prepare your environment with all the necessary tools to compile and run your MPI code. Let’s now begin to construct our C++ file. In this tutorial, we will name our code file: Open and begin by including the C standard library and the MPI library , and by constructing the main function of the C++ code: Now let’s set up several MPI directives to parallelize our code. In this ‘Hello World’ tutorial we’ll be utilizing the following four directives: The function initializes the MPI environment. It takes in the addresses of the C++ command line arguments and . The function returns the total size of the environment via quantity of processes. The function takes in the MPI environment, and the memory address of an integer variable. The function returns the process ID of the processor that called the function. The function takes in the MPI environment, and the memory address of an integer variable. The function cleans up the MPI environment and ends MPI communications. These four directives should be enough to get our parallel ‘Hello World’ program running. We will begin by creating two variables, , and , to store an identifier for each of the parallel processes and the number of processes running in the cluster, respectively. We will also implement the function which will initialize the MPI communicator: Let’s now obtain some information about our cluster of processors and print the information out for the user. We will use the functions and to obtain the count of processes and the rank of a process, respectively: \"Hello World from process %d of %d \"Hello World from process %d of %d Now the code is complete and ready to be compiled. Because this is an MPI program, we have to use a specialized compiler. Be sure to use the correct command based on which compiler you have loaded. This will produce an executable we can pass to the cluster as a job. In order to execute MPI compiled code, a special command must be used: The flag specifies the number of processors that are to be utilized in execution of the program. In your job script, load the same compiler and OpenMPI choices you used above to compile the program, and run the job with Slurm to execute the application. Your job script should look something like this: On Alpine, there are at most 64 cores per node. For applications that require more than 64 processes, you will need to request multiple nodes in your job. Our output file should look something like this: Hello World from process of Hello World from process of Hello World from process of Hello World from process of\n\nLike many other parallel programming utilities, synchronization is an essential tool in thread safety and ensuring certain sections of code are handled at certain points. is a process lock that holds each process at a certain line of code until all processes have reached that line in code. can be called as such: To get a handle on barriers, let’s modify our “Hello World” program so that it prints out each process in order of thread ID. Starting with our “Hello World” code from the previous section, begin by nesting our print statement in a loop: \"Hello World from process %d of %d Next, let’s implement a conditional statement in the loop to print only when the loop iteration matches the process rank. \"Hello World from process %d of %d Lastly, implement the barrier function in the loop. This will ensure that all processes are synchronized when passing through the loop. \"Hello World from process %d of %d Compiling and running this code will result in this output:\n\nMessage passing is the primary utility in the MPI application interface that allows for processes to communicate with each other. In this tutorial, we will learn the basics of message passing between 2 processes. Message passing in MPI is handled by the corresponding functions and their arguments: The arguments are as follows: //Address for the message you are sending. //Number of elements being sent through the address. //The MPI specific data type being passed through the address. //Address to the message you are receiving. //Number of elements being sent through the address. //The MPI specific data type being passed through the address. Let’s implement message passing in an example: We will create a two-process program that will pass the number 42 from one process to another. We will use our “Hello World” program as a starting point for this program. Let’s begin by creating a variable to store some information. Now create and conditionals that specify appropriate process to call and functions. In this example we want process 1 to send out a message containing the integer 42 to process 2. Lastly we must call and . We will pass the following parameters into the functions: //Address of the message we are sending. //Number of elements handled by that address. //MPI_TYPE of the message we are sending. //Address of the message we are receiving. //Number of elements handled by that address. //MPI_TYPE of the message we are sending. Lets implement these functions in our code: Compiling and running our code with 2 processes will result in the following output:\n\nGroup operators are very useful for MPI. They allow for swaths of data to be distributed from a root process to all other available processes, or data from all processes can be collected at one process. These operators can eliminate the need for a surprising amount of boilerplate code via the use of two functions: //Address of the variable that will be scattered. //Number of elements that will be scattered. //MPI Datatype of the data that is scattered. //Address of the variable that will store the scattered data. //Number of data elements that will be received per process. //MPI Datatype of the data that will be received. //The rank of the process that will scatter the information. //Address of the variable that will be sent. //Number of data elements that will sent . //MPI Datatype of the data that is sent. //Address of the variable that will store the received data. //Number of data elements per process that will be received. //MPI Datatype of the data that will be received. //The rank of the process rank that will gather the information. In order to get a better grasp on these functions, let’s go ahead and create a program that will utilize the scatter function. Note that the gather function (not shown in the example) works similarly, and is essentially the converse of the scatter function. Further examples which utilize the gather function can be found in the MPI tutorial linked at the beginning of this document. We will create a program that scatters one element of a data array to each process. Specifically, this code will scatter the four elements of an array to four different processes. We will start with a basic C++ main function along with variables to store process rank and number of processes. Now let’s set up the MPI environment using , , , and Next let’s generate an array named to store four numbers. We will also create a variable called that we shall scatter the data to. Now we will begin the use of group operators. We will use the operator scatter to distribute into . Let’s take a look at the parameters we will use in this function: //Address of array we are scattering from. //Number of items we are sending each processor //Address of array we are receiving scattered data. //Amount of data each process will receive. //Process ID that will distribute the data. Let’s see this implemented in code. We will also write a print statement following the scatter call: Running this code will print out the four numbers in the distro array as four separate numbers each from different processors (note the order of ranks isn’t necessarily sequential):"
    },
    {
        "link": "https://paulnorvig.com/guides/introduction-to-mpi-with-c.html",
        "document": ""
    },
    {
        "link": "https://mpitutorial.com/tutorials",
        "document": "Welcome to the MPI tutorials! In these tutorials, you will learn a wide array of concepts about MPI. Below are the available lessons, each of which contain example code.\n\nThe tutorials assume that the reader has a basic knowledge of C, some C++, and Linux.\n• Sending and receiving with MPI_Send and MPI_Recv (中文版) (日本語)\n• Using MPI_Reduce and MPI_Allreduce for parallel number reduction (中文版) (日本語)"
    },
    {
        "link": "https://htor.inf.ethz.ch/teaching/mpi_tutorials/ppopp13/2013-02-24-ppopp-mpi-basic.pdf",
        "document": ""
    },
    {
        "link": "https://curc.readthedocs.io/en/latest/programming/MPIBestpractices.html",
        "document": "MPI, or Message Passing Interface, is a powerful library standard that allows for the parallel execution of applications across multiple processors on a system. It differs from other parallel execution libraries like OpenMP by also allowing a user to run their applications across multiple nodes. Unfortunately it can sometimes be a bit tricky to run a compiled MPI application within an HPC resource. The following page outlines best practices in running your MPI applications across CURC resources.\n\nSeveral families of compilers are available to users: Intel, GCC, and AOCC (Alpine only). Intel compilers have Intel MPI available for messsage passing, and GCC and AOCC compilers have OpenMPI available for message passing. To load a compiler/MPI combo run one the following commands from a job script or compile node (note that you should subsitute the version you need for in the examples below; available compiler versions can be seen by typing ): module load gcc/<version> openmpi # Uncomment this additional line when adding this command to a JobScript! module load aocc/<version> openmpi # Uncomment this additional line when adding this command to a JobScript! It is important to note that use of OpenMPI should be paired with the environment variable to ensure the job can function when scheduled from a login node! On Blanca, in most situations you will want to try to compile and run your applications utilizing the Intel set of compilers and MPI libraries. Most CPUs on Blanca are of Intel architecture, so utilizing Intel will ensure the highest level of optimization comes from your compiler. GCC should only be utilized when your application cannot be compiled on intel software or if compiler specific optimizations exist within your code. We do not yet have compiler/MPI recommendations for Alpine, which has AMD CPUs.\n\nRegardless of compiler or MPI distribution, there are 3 “wrapper” commands that will run MPI applications: , , and . These “wrapper” commands should be used after loading in your desired compiler and MPI distribution and simply prepend whatever application you wish to run. Each command offers their own pros and cons alongside nuance as to how they function. is probably the most direct method to run MPI applications with the command being tied to the distribution. This means distribution dependent flags can be passed directly through the command. is a standardized MPI command execution command that allows for more general MPI flags to be passed. This means that commands are universal across all distributions. The final command is probably the most abstracted away from a specific implementation. This command lets Slurm figure out specific MPI features that are available in your environment and handles running the process as a job. This command is usually a little less efficient and may have some issues with reliability. RC usually recommends and for simplicity and reliability when running MPI applications. should be used sparingly to avoid issues with execution.\n\nRunning MPI jobs on Alpine is relatively straightforward. However, one caveat on Alpine is that MPI jobs cannot be run across chassis, which limits them to a maximum count of 4096 cores (64 nodes per chassis * 64 cores each). Simply select the Compiler and MPI wrapper you wish to use and place it in a job script. In the following example, we run a 128 core, 4 hour job with a gcc compiler and OpenMPI: #!/bin/bash #SBATCH --nodes=2 #SBATCH --time=04:00:00 #SBATCH --partition=amilan #SBATCH --constraint=ib #SBATCH --ntasks=128 #SBATCH --job-name=mpi-job #SBATCH --output=mpi-job.%j.out module purge module load gcc/10.3 openmpi export SLURM_EXPORT_ENV=ALL #Run a 128 core job across 2 nodes: mpirun -np $SLURM_NTASKS /path/to/mycode.exe #Note: $SLURM_NTASKS has a value of the amount of cores you requested When running MPI jobs on Alpine, you can use the flag to force the job onto an Alpine node that has Infiniband, the networking fabric used by MPI.\n\nBlanca is often a bit more complicated due to the variety of nodes available. In general, there are 3 types of nodes on Blanca that can all run single node multi-core MPI processes that may require additional flags and parameters to achieve cross node parallelism. General Blanca nodes are not intended to run multi-node processes but this can still be achieved through the manipulation of some network fabric settings. In order to achieve cross node parallelism we must force MPI to utilize ethernet instead of our normal high speed network fabric. We can enforce this with various flags for each respective compiler. This does not ensure high speed communications in message passing, but it will allow for basic parallelization across nodes. Blanca HPC comes equipped with InfiniBand high speed interconnects that allow for high speed communication between nodes. These nodes supoort the Intel and Intel MPI compiler/MPI combo, as well as the / modules (note: bve sure to use the ucx version of the OpenMPI module). Blanca HPC nodes can easily be distinguished from other Blanca nodes with the node’s name in the cluster. Nodes will clearly be distinguished with the prefix. They also will have the feature in their feature list if you query them with . If you are using Open MPI, jobs on Blanca HPC nodes can be run using without any special arguments, although be sure to prior to invoking . If you are using IMPI, select the (Open Fabrics Alliance) option to enable Infiniband-based message passing, the fastest interconnect availble on the nodes. You can do this with the following flag: The nodes in Blanca chassis 5 (nodes named ) are equipped with high speed network fabrics that are more suited for cross node MPI processes. These nodes are labeled as RoCE enabled and require applications to be compiled with UCX-enabled openmpi modules, which are available with both and . If you are unsure if your node supports RoCE feature then you can check by using the scontrol command on your node. You will be presented a block information that details all the nodes features. The key feature you should look for is . If your Blanca node lacks this feature then it is not ROCE Enabled. Jobs on RoCE nodes can be run using without any special arguments, although be sure to prior to invoking ."
    }
]