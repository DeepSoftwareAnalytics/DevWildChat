[
    {
        "link": "https://talks.peopleanalytics-regression-book.org/r_ladies_tunis.html",
        "document": ""
    },
    {
        "link": "https://csub.edu/~emontoya2/rcomp/reg.html",
        "document": "The goal of this chapter is to have you learn to use R to carry out the analyses and techniques generally covered in a regression analysis course. Currently, we carry out some of the examples using R found in\n\nThe abbreviation ALRM is used for this textbook. Datasets for this textbook may be found at\n\nIn this section, we use R to fit the simple linear regression (SLM) model \\[ y_i=\\beta_0+\\beta_1 x_i + \\varepsilon_i \\] where \\(y_i\\) is the dependent/response variable, \\(x_i\\) is the independent/predictor variable, and the random error terms have mean 0, constant variance \\(\\sigma^2\\) and are uncorrelated. We follow the Toluca Company example given in ALRM to illustrate how to use R to obtain scatter plots, the least square estimates, fitted values, residuals, and a point estimator of \\(\\sigma^2\\). Scatterplots can be used to explore the relationship between two variables. Recall that the formula to create a graphical numerical summary follows the general form where is the variable you want to graph. However, now we have an additional variable. When the data is bivariate the general formula changes a bit: , where is the name of the response/dependent variable and is the name of the explanatory/predictor variable. To create a scatterplot, is replaced with . We begin by importing the data: # always load the mosaic package first. ### The datasets from the book are available to download or ### read from a url, but we have to make some changes ### to read.csv() because these files are not csv files. ### Use this as a template for importing datasets referenced ### This is not a .csv file and the file does not ### include variable names, so some changes ### have to be made to read.csv(). , header , col.names , , sep We can take a quick look at the data by using : Note that we named the response variable and explanatory variable \\(yvar\\) and \\(xvar\\), respectively. So these names must be used in or any other function that uses : The function has other arguments that allows you to change label axis, add a title, etc.: ### xyplot( y ~ x | gfactor, data, main, xlab, ylab, col, pch) # y: respnose variable # x: explanatory variable # gfactor: a factor variable (optional) so that a plot is returned for each level of f1 (optional) # data: the name of the dataframe where the observed data is found # main: title for plot (optional) # ylab: label for y-axis (optional) # xlab: label for x-axis (optional) # pch: point symbol. Must be a number between 1-25 (optional) To obtain the LS estimates, we can either compute them using the algebraic formulas or fit the SLR model using the function . The function follows the general form of , where goal is replaced with . Note that stands for linear model. To illustrate the first method, recall that the LS estimators of \\(\\beta_0\\) and \\(\\beta_1\\) are \\[ b_1=\\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y}) }{\\sum (x_i-\\bar{x})^2}, \\quad ~~~~b_0=\\bar{y}- b_1\\bar{x} \\] To apply these formulas, we use the function . This function follows the general form but becomes : ### extract 'xvar' and assign the values to `myx': ### Note that we have no data argument ### since 'myx' was defined above and is The function follows the formula . This function requires the name of the dataframe and the variables used in the dataframe. Recall that the variables in are called and : Although the function only prints the LS estimates, it actually computes a lot of information that is only retrievable if we store the result in an R object. Such objects are generally called lm objects. So, we store the result of to an R object (I called it below) so that we can extract certain information as needed: ### fit the model and store the model fit , data To view the LS estimates, we summarize the lm object, , using the function: From the output, we see that \\(b_0=62.366\\) and \\(b_1=3.570\\). More on the output provided by later. Alternateivly, we can extract the LS estimates by applying the function to the lm object: ### The only info required is the name Once we have access to our data or we fit and save the simple linear regression model using , we can plot the data along with the estimated LS line. There are two approaches for plotting the LS line: or . The function is used the same way as before but with an additional argument of : ### Note: \"p\" tells R we want to plot the points. ### \"r\" tells R we want to add the est. regression , data , type , The argument tells R to plot the points and the estiamted regression line. Also recall that has additional arguments to change axis label, add a title, etc. will plot the data and the estimated LS line. It only has one argument, which will be an lm object: Which function should one use? They both provide the same info, but if you prefer to change some of the aesthetics of the plot, you will have to use since, as of now, it is not possible to change the aesthetics of resulting plot provided by . For example: To obtain the fitted vales, \\(\\hat{y}_i=b_0+b_1 x_i\\) for \\(i=1,2,...,n\\), we may use R by directly using the least square estimates obtained by using the summation formulas or more conveniently use the or on an lm object: ### Only one argument: An lm object Both or can provided the fitted/predicted values for the observed values of \\(x\\). If instead you wanted to predict or fit values of \\(y\\) for certain levels or values of \\(x\\), we use with an additional argument that corresponds to a dataframe that holds the x value(s) of interest. For example: ### This is a template on how to predict values of y ### for certain values of x. ### Suppose we wanted to predict the response when x=31 or x=119. ### First set up the dataframe that holds these values. Note that ### we have to use the name 'xvar' since this is the name of xvar , ### You have to specifiy an 'lm object'. ### newdata is a data frame holds the x values of interest. , newdata The residuals are the differences between the observed values and the fitted values, denoted by \\(e_i=y_i-\\hat{y}_i\\). The residuals can be obtained using the function whose only argument is an lm object: ### You have to specifiy an 'lm object'. You may also compute them via: ### You have to specifiy an 'lm object'. So what is printed by lm object ? The following output is given: The output provides the LS estimates, the standard errors of the LS estimates, the test statistic for the regression line parameters, two-sided p-value to assess the significance of these parameters, the MSE, \\(R^2\\), and other information that we will revisit later. To estimate the error variance component, one can either use the formula or use the output. Using the formula we get: ### Recall we defined the residuals in a previous section The summary output provides \\(\\sqrt{MSE}\\), which is an estimate of \\(\\sigma\\). Based on the output, \\(\\hat{\\sigma}^2=48.82^2 = 2383.392\\)\n\nIn this section, inference of the regression parameters using confidence intervals and hypothesis testing, inference about the mean response, prediction intervals for new observations, the ANOVA approach, and measures of association are addressed using R. The Toluca Company example from ALRM is used to illustrate inference on the slope and intercept of the model. Generally, inference is made about the slope of the model. A test and confidence interval concerning the intercept can be set up in the same manner as that of \\(\\beta_1\\). To obtain the test statistic, along with p-values for test regarding those parameters, we summarize the lm object (the model fit) by using : Recall that the output provides the following: Based on the output of , we obtain the estimates of \\(\\beta_1\\) and \\(\\beta_0\\) along with the test statistics \\(b_1/s\\{b_1\\}\\) and \\(b_0/s\\{b_0\\}\\) with the corresponding two-sided p-value for each. Alternatively, we can also obtain the test statistic using the following commands: # The test for the intercept is computed similarly Under \\(H_0\\), the distribution of the test statistic is \\(t\\) distribution with \\(n-2\\) (Notation: \\(t_{n-2}\\)). Recall that the p-value is the probability that the test statistic would take a value as extreme (or more extreme) as the observed test statistic in the the direction of the alternative if \\(H_0\\) were true. If \\(H_a: \\beta >0\\), the p-value = \\(P( t_{n-2} > 10.290 )\\). To obtain this probability we use the function . The value will the value of interest (10.290 in this example), corresponds to the degrees of freedom (n-2), and will either be set equal to (computes the area to the left of ) or (computes the area to the right of ). To compute \\(P( t_{n-2} > 10.290 )\\), set : The desired probability will be printed in the console: \\(P( t_{n-2} > 10.290 ) \\approx .0000000002\\). This function will also produce a graph of the probability distribution with the area to the left of shaded one color (area A) and the area to the right of shaded another color (area B). If instead \\(H_a: \\beta <0\\), then set : The output shows that \\(P( t_{n-2} < 10.290 ) \\approx 1\\). Lastly, if \\(H_a: \\beta \n\neq 0\\), then we care about both possible extremes: \\[P( t_{n-2} < -10.290 ) + P( t_{n-2} > 10.290 ) = 2 \\times P( t_{n-2} > |10.290| )=.0000000004\\] For a confidence interval (CI) for \\(\\beta_1\\), recall that the \\(1-\\alpha/2\\) confidence limits for \\(\\beta_1\\) are \\[b_1 \\pm t_{1-\\alpha/2, n-2} s\\{b_1\\}\\] To derive the CI, we can either use the output along with the critical values provided by , or you can use . Let’s first derive the CI using . This function provides the quantile for a specified probability. For the Toluca Company example suppose a 95% confidence interval is to be computed. Then, \\(t_{1-.05/2,25-2}=\\) = 2.069. To obtain the CI, run the code below: Since the model has been fitted, will provide the CIs for both parameters. This function has two arguments: an lm object and the desired confidence level: CI for the mean response ( ) and a prediction interval for Following the Toluca Company example from ALRM, we obtain a CI for the point estimate \\(y_h\\) for \\(x_h=100\\) units by using the function . However, the function has two additional arguments, and . We set and level to the desired confidence level: xvar # xvar refers to the name of the variable , newdata , interval , level The prediction of a new observation and its corresponding prediction interval (PI) can be obtained in the same manner as the confidence interval for the mean response but we set : To obtain the ANOVA table in R, we use the function lm object : The following output is given by lm object : The output from show that the p-value \\(\\approx\\) 0. We could also use the function to compute the p-value. The value is the value of interest (105.88 in this example), corresponds to the numerator degree of freedom (1), corresponds to the denominator degree of freedom (25-2), and would be set to since we want the p-value for this F-test: Note that the shaded “A” area in the graph always corresponds to the area to the left of the value of interest. Area “B” does not appear since it is too small (\\(\\approx 0\\)) The coefficient of determination (\\(R^2\\)) and coefficient of correlation (r) can be obtained from the SSR and SSTO (from the ANOVA table), by applying the summation formulas, or obtained from the lm object output. The coefficient of or correlation is sign(\\(b_1\\))\\((\\sqrt{R^2})\\). Using the summation formulas: # recall the value of the slope est. Note that this matches the information provided in the output of summary discussed in a previous section.\n\nThis section deals with assessing the appropriateness of the simple regression model. Residual analysis are performed on Toluca Company example from ALRM. If we assume that \\(\\varepsilon_i \\overset{iid}{\\sim} N(0,\\sigma^2)\\) in the normal simple linear regression model, then the residuals \\(e_i\\)’s should reflect this property. We examine the assumption using the following residual plots: residuals vs x or ; QQ-plot of the residuals If we do not assume normality, the QQ-plot should still be examined for signs that the residuals show may be heavy-tailed distributed. To create the plots to assess each of these assumptions, we use and . The following code creates the required plots: ### extract the fitted values from the lm object ### extract the residuals from the lm object ### Note: There is no time variable , data , main ### There is no data argument since the residuals are ### already in R's memory since we created , distribution Although the above graphical summaries above are generally sufficient for diagnostics, one may examine other residual plots: R code is provided to carry out the Shapiro-Wilk and Lilliefors (Kolmogorov-Smirnov) test for normality. Both of these test may be used to detect if the residuals do not follow a normal distribution. The null hypothesis is that the residuals are normally distributed. The alternative is that they are not normally distributed. Thus, we are not testing if the residuals are normally distributed but rather if they depart from the normal distribution. For both tests, the p-value is much larger than a reasonable \\(\\alpha\\) level. A possible remedial measure when the simple linear regression model is not appropriate is to transform the response and/or predictor variable. To transform a variable, we use the which comes from the package. The package is automatically installed when you install the package, and the package is automatically loaded when you load the package. The function requires the name of the dataframe and the name of a new variable/object that will hold your transformed variable: To illustrate transformation, we use the data from Toluca Company example. Specifically, we want to apply a square root transformation to the response variable and a natural log transformation to the explanatory variable: ### Apply the square root trans. to the ### response. This function will add it to , sqrty # the squared root variable will be called sqrty We want to be able to use the transformed variable, so we store the resulting new dataframe to a new object: Now apply the natural log transformation to the explanatory variable: ### Note that we are using toluca.dataVer1 ### dataframe since it already includes 'sqrty'. , lnx ### One could also apply as many trans. as one would ### like by using mutate only once: , sqrty , lnx , lny , sqrd , cubertx Once the transformed variables are stored in a dataframe, you can plot them or fit the simple linear regression model: Th lack of fit test requires repeated observations at one or more \\(x\\) levels. To proceed, one may compute the MSLF and MSPE using the summation formulas using R. An easier approach is determine the full and reduced model when testing for a lack of fit. Recall that in this setting we have the following models: If the full and reduced model can be determined (and are nested), then one may use the general F-test to test whether we can reject the null hypothesis (which postulates the reduced model). In R, the general F-test can be carried out using reduced model , full model , . Note that in this case, the reduced model is a single-factor (potentially) unbalanced AOV model. We follow the Bank Example from ALRM to illustrate this in R: , header , col.names , , sep , xfac # 'levels' counts how many levels are present in a ### (Note: for those that have taken 4220: this is a one-way unbalanced aov problem) , data , data , , test What information is given by ? We obtain\n\nRecall that the function creates a vector. For example, We can select a given element from this vector by placing after the vector name with a specified index: A matrix is a rectangular way of storing data. You can think of it as simply a way to store data. Matrices will have two dimensions: rows and columns. That is, each matrix will consist of rows and columns of elements (or data). The function allows a user to create a matrix from a vector of data. The function has four arguments: (a vector of data), (desired number of rows), (desired number of columns), and (set equal to if the matrix is to be filled by column, set to if to be filled by row). The code below create a vector of data to be transformed into a matrix: It may be necessary to perform certain matrix operations. In this section you will learn how to perform matrix multiplication, transpose a matrix, obtain the inverse of a matrix, and element by element multiplication. The commands that we will use are: What does it do? The commands are illustrated with the following code: , , , , , , , , , nrow , ncol , , , , , , , , , nrow , ncol # this is NOT matrix multiplication The simple linear regression model in matrix form is The matrix approach is illustrated in R using the Toluca Company example: ### Extract both variables to set up the matrices , nrow , ncol # First we need to define a vectors of 1's: , times # If you have the columns of the matrix # already defined, then we can create the # matrix by binding the columns using 'cbind()'. , The LS estimates may be obtained via \\(\\boldsymbol{b}= ( \\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^{T} \\boldsymbol{y}\\). In R we have: These are the same estimates provided by . The fitted values, \\(\\boldsymbol{\\hat{y}} = \\boldsymbol{X} \\boldsymbol{b}\\), are obtained via: These values may also be obtained by using the hat matrix:\n\nThis section deals with the multiple regression model \\[ Y_i=\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{i,p-1} + \\varepsilon_i\\] where the \\(\\beta\\)’s are the parameters, \\(x\\)’s are known constants, and we assume that \\(\\varepsilon_i \\overset{iid}{\\sim} N(0, \\sigma^2)\\). To illustrate the model fitting with R we follow the Dwaine Studios Example from ALRM. Note that the model can be expressed in matrix form and any subsequent analyses can be done in terms of matrices. Matrices will be used to obtain the least square estimates as an illustration of using matrices R, but the rest of the analyses will be done using and . A matrix approach can be used as follows: We can also to fit a MLR model. It is used in the same manner as before but we add additional explanatory variables by adding a after each explanatory variable: All previously discussed functions that extract information from an lm obejct such as the LS estimates, fitted values, ANOVA table, etc. can be use in the same manner when fitting a MLR model. Recall that returns the LS estimates: The fitted values and residuals can be obtained via: To obtain the ANOVA table, run the following: What information is given by ? We obtain The ANOVA table returned by R gives the decomposition of \\(SSR(X_1, X_2)\\) (Chapter 7 material). Note that \\(SSR(X_1,X_2)=SSR(X_1)+SSR(X_2|X_1)\\). To determine the significance of the regression parameter(s) we can summarized the fit: Note that the following is printed by : Note that we are provided with the test statistic \\(F^*\\) to test whether sales are related to target population and per capita disposable income (\\(\\beta_1=0\\) and \\(\\beta_2=0\\)), and we are provided with the test statistic \\(t^*\\) for each regression parameter. Inference about the Mean Response and Prediction of New Observation As before, we can use to obtain a CI for the mean response and a PI for a predicted response. For example, a 95% confidence interval for \\(E(y_h)\\) (expected sales in cities) in a community with \\(x_{h1}=65.4\\) (number of persons aged 16 or younger) and \\(x_{h2}=17.6\\) (per capita disposable personal income) is obtained in R by: A 95% PI at these same levels of and is: To predict at two difference communities in which in one community the number of persons aged 16 or younger is 65.4 and the per capita disposable personal income is 17.6, and the other has the number of persons aged 16 or younger is 53.1 and the per capita disposable personal income is 17.7 we enter the following: Diagnostics are conducted in the same manner (QQ-plot of the residuals, scatterplots of the residuals versus \\(\\hat{y}\\), scatterplots of the residuals versus \\(\\hat{y}\\) versus a given \\(x\\)) as when the simple linear regression model was discussed, but now we also consider the scatter plot matrix and correlation matrix. The argument for both of these functions will be a dataframe. Here R is used to obtain an anova table with a decomposition of the SSR. We follow the Body Fat example from ALRM using R to obtain the anova table with the decomposition of SSR. The function will automatically decompose the SSR with the decomposition depending on the order that you enter the predictor variables into . For example, to get the decomposition \\[SSR(x_1, x_2, x_3)=SSR(x_3)+SSR(x_2|x_3)+SSR(x_1|x_3, x_2)\\] we enter the predictors into as \\(\\thicksim\\) . ### This is not a .csv file and the file does not ### include variable names, so some changes ### have to be made to read.csv(). , header , col.names , , , , sep , data One could also use the extra sums of squares formulas to get the decomposition desired. Testing whether for some There are several ways to go about testing whether a single or multiple regression coefficients are significant using R. For example, we could get the appropriate extra sums of squares for the reduced model and the full model to calculate the appropriate test statistic. If the reduced model is nested under the full model then we can use to compute the test statistic and corresponding p-value. All that is required is to specify the full model and the model under the null hypothesis (reduced model). Suppose it is of interest to test the following hypothesis for the Body Fat example from ALRM: The reduced model is \\(Y_i=\\beta_0+\\beta_1 X_{i1} + \\varepsilon_i\\) and the full models is \\(y_i=\\beta_0+\\beta_1 x_{i1} + \\beta_2 x_{i2} +\\beta_3 x_{i3}+ \\varepsilon_i\\). Clearly the models are nested therefore we use the function : The following is printed by :\n\nThis section deals with different criteria for model selection for MLR. To illustrate different model selection criteria with R, we follow the Surgical Unit Example from ALRM. To start, import the data: Tell R that certain varialbes are categorical: \\(R^2\\) does not decrease as the number of covariates in the model increases. The adjusted coefficient of determination, \\(R_{a,p}^2\\) may decrease as the number of covariates in the model increases, and so takes into account the number of explanatory variables that are in the model. To compute \\(R_{a,p}^2\\) for several candidate models, we use the function function from the R package. First install the R package : First, we have to define a matrix whose columns consist of the variables under consideration (this is not the design/model matrix). Note that can only handle factor variables with two levels and they must be numeric (don’t use the factored versions of \\(x6\\), \\(x7\\), and \\(x8\\): The function requires this matrix, the response variable and the model selection criterion ( or ): To better organize the information provided by , run the following: , , # add names to make it easier to read info , , , , , , , , , Note that the models with among the higher \\(R_{a,p}^2\\) are the models that include \\(x1\\), \\(x2\\), \\(x3\\), and \\(x8\\) and include \\(x1\\), \\(x2\\), \\(x3\\), \\(x6\\), and \\(x8\\). Mallows’ \\(C_p\\) criterion may also be obtained in the sammer manner using but we replace with : x , y , method , , # add names to make it easier to read info , , , , , , , , , Note that a model with \\(x1\\), \\(x2\\), \\(x3\\), \\(x5\\), and \\(x8\\) has a \\(C_p\\) that is near and above \\(p\\). The AIC criterion may be computed using the funciton . The only argument for this funciton is an object. For example: The BIC criterion may be computed using the function as well but we have to add an additional argument: Akaike’s corrected Information Criterion may also be computed with by adding the “correction”: # get AIC of a model and store result , , , , , k , , The function in R performs forward and backward stepwise regression. The function uses AIC (by default) for its model selection criterion. The arguments of the step function are the initial model (an lm object) to start the procedure, the scope (the covariates under consideration), and the direction of the search ( or ) Forward selection starts with a model with no explanatory variables and decides which covariates to add at each step by getting the explanatory variables that gives the lowest AIC, if in the next step a lower AIC is not obtained, then it stops searching. Therefore, the model to specify in must be a model with just the intercept and the scope must include all the predictor variables to include in the search. We demonstrate with the Surgical Unit data but we limit ourselves for using the first 4 predictor variables as before for the purpose of illustration: , data #model with just the intercept , scope , direction Now obtain results using the forward stepwise regression results using the BIC criterion: Backward selection starts with a full model with all predictors and it decides which predictor to delete at each step by seeing which predictor deletion leads to the lowest AIC, once a lower AIC is not obtained in the next step it stops. Therefore the model to specify in must be a model with all the predictor variables and the scope must include all the predictor variables to include in the search: , data #model with just the intercept , scope , direction Now obtain results using the backward stepwise regression results using the BIC criterion: The cross validation (CV) or PRESS criterion provides a sense of how well the model can predict new values of y. We use the computational friendly from the CV criterion to compute a CV scores. To illustrate, the CV scores of , , and are computed. # hatvalues # <--- provides the diagonal elements of H # hatvalues # <--- provides the diagonal elements of H"
    },
    {
        "link": "https://econometrics-with-r.org/7.3-joint-hypothesis-testing-using-the-f-statistic.html",
        "document": ""
    },
    {
        "link": "https://stats.libretexts.org/Bookshelves/Advanced_Statistics/Intermediate_Statistics_with_R_(Greenwood)/08%3A_Multiple_linear_regression/8.07%3A_Overall_F-test_in_multiple_linear_regression",
        "document": "In the MLR summary, there is an \\(F\\)-test and p-value reported at the bottom of the output. For the model with Elevation and Maximum Temperature, the last row of the model summary is:\n\nThis test is called the overall F-test in MLR and is very similar to the \\(F\\)-test in a reference-coded One-Way ANOVA model. It tests the null hypothesis that involves setting every coefficient except the \\(y\\)-intercept to 0 (so all the slope coefficients equal 0). We saw this reduced model in the One-Way material when we considered setting all the deviations from the baseline group to 0 under the null hypothesis. We can frame this as a comparison between a full and reduced model as follows:\n\nThe reduced model estimates the same values for all \\(y\\text{'s}\\), \\(\\widehat{y}_i = \\bar{y} = b_0\\) and corresponds to the null hypothesis of:\n\n\\(\\boldsymbol{H_0:}\\) No explanatory variables should be included in the model: \\(\\beta_1 = \\beta_2 = \\cdots = \\beta_K = 0\\).\n\nThe full model corresponds to the alternative:\n\n\\(\\boldsymbol{H_A:}\\) At least one explanatory variable should be included in the model: Not all \\(\\beta_k\\text{'s} = 0\\) for \\((k = 1,\\ldots,K)\\).\n\nNote that \\(\\beta_0\\) is not set to 0 in the reduced model (under the null hypothesis) – it becomes the true mean of \\(y\\) for all values of the \\(x\\text{'s}\\) since all the predictors are multiplied by coefficients of 0.\n\nThe test statistic to assess these hypotheses is \\(F = \\text{MS}_{\\text{model}}/\\text{MS}_E\\), which is assumed to follow an \\(F\\)-distribution with \\(K\\) numerator df and \\(n-K-1\\) denominator df under the null hypothesis. The output provides us with \\(F(2, 20) = 56.43\\) and a p-value of \\(5.979*10^{-9}\\) (p-value \\(<0.00001\\)) and strong evidence against the null hypothesis. Thus, there is strong evidence against the null hypothesis that the true slopes for the two predictors are 0 and so we would conclude that at least one of the two slope coefficients (Max.Temp’s or Elevation’s) is different from 0 in the population of SNOTEL sites in Montana on this date. While this test is a little bit interesting and a good indicator of something interesting existing in the model, the moment you see this result, you want to know more about each predictor variable. If neither predictor variable is important, we will discover that in the \\(t\\)-tests for each coefficient and so our general recommendation is to start there.\n\nThe overall F-test, then, is really about testing whether there is something good in the model somewhere. And that certainly is important but it is also not too informative. There is one situation where this test is really interesting, when there is only one predictor variable in the model (SLR). In that situation, this test provides exactly the same p-value as the \\(t\\)-test. \\(F\\)-tests will be important when we are mixing categorical and quantitative predictor variables in our MLR models (Section 8.12), but the overall \\(F\\)-test is of very limited utility."
    },
    {
        "link": "https://online.stat.psu.edu/stat462/node/135",
        "document": "The \"general linear F-test\" involves three basic steps, namely:\n• Define a larger full model. (By \"larger,\" we mean one with more parameters.)\n• Define a smaller reduced model. (By \"smaller,\" we mean one with fewer parameters.)\n• Use an F-statistic to decide whether or not to reject the smaller reduced model in favor of the larger full model.\n\nAs you can see by the wording of the third step, the null hypothesis always pertains to the reduced model, while the alternative hypothesis always pertains to the full model.\n\nThe easiest way to learn about the general linear F-test is to first go back to what we know, namely the simple linear regression model. Once we understand the general linear F-test for the simple case, we then see that it can be easily extended to the multiple case. We take that approach here.\n\nThe \"full model\", which is also sometimes referred to as the \"unrestricted model,\" is the model thought to be most appropriate for the data. For simple linear regression, the full model is:\n\nHere's a plot of a hypothesized full model for a set of data that we worked with previously in this course (student heights and grade point averages):\n\nAnd, here's another plot of a hypothesized full model that we previously encountered (state latitudes and skin cancer mortalities):\n\nIn each plot, the solid line represents what the hypothesized population regression line might look like for the full model. The question we have to answer in each case is \"does the full model describe the data well?\" Here, we might think that the full model does well in summarizing the trend in the second plot but not the first.\n\nThe \"reduced model,\" which is sometimes also referred to as the \"restricted model,\" is the model described by the null hypothesis H . For simple linear regression, a common null hypothesis is H : β = 0. In this case, the reduced model is obtained by \"zeroing-out\" the slope β that appears in the full model. That is, the reduced model is:\n\nThis reduced model suggests that each response y is a function only of some overall mean, β , and some error ε .\n\nLet's take another look at the plot of student grade point average against height, but this time with a line representing what the hypothesized population regression line might look like for the reduced model:\n\nNot bad — there (fortunately?!) doesn't appear to be a relationship between height and grade point average. And, it appears as if the reduced model might be appropriate in describing the lack of a relationship between heights and grade point averages. How does the reduced model do for the skin cancer mortality example?\n\nIt doesn't appear as if the reduced model would do a very good job of summarizing the trend in the population.\n\nHow do we decide if the reduced model or the full model does a better job of describing the trend in the data when it can't be determined by simply looking at a plot? What we need to do is to quantify how much error remains after fitting each of the two models to our data. That is, we take the general linear F-test approach:\n• \"Fit the full model\" to the data.\n• Obtain the least squares estimates of β and β .\n• Determine the error sum of squares, which we denote \"SSE(F).\"\n• \"Fit the reduced model\" to the data.\n• Obtain the least squares estimate of β .\n• Determine the error sum of squares, which we denote \"SSE(R).\"\n\nRecall that, in general, the error sum of squares is obtained by summing the squared distances between the observed and fitted (estimated) responses:\n\nTherefore, since \\(y_i\\) is the observed response and \\(\\hat{y}_i\\) is the fitted response for the full model:\n\nAnd, since \\(y_i\\) is the observed response and \\(\\bar{y}\\) is the fitted response for the reduced model:\n\nLet's get a better feel for the general linear F-test approach by applying it to two different two datasets. First, let's look at the heightgpa data. The following plot of grade point averages against heights contains two estimated regression lines — the solid line is the estimated line for the full model, and the dashed line is the estimated line for the reduced model:\n\nAs you can see, the estimated lines are almost identical. Calculating the error sum of squares for each model, we obtain:\n\nThe two quantities are almost identical. Adding height to the reduced model to obtain the full model reduces the amount of error by only 0.0276 (from 9.7331 to 9.7055). That is, adding height to the model does very little in reducing the variability in grade point averages. In this case, there appears to be no advantage in using the larger full model over the simpler reduced model.\n\nLook what happens when we fit the full and reduced models to the skin cancer mortality and latitude dataset:\n\nHere, there is quite a big difference in the estimated equation for the reduced model (solid line) and the estimated equation for the full model (dashed line). The error sums of squares quantify the substantial difference in the two estimated equations:\n\nAdding latitude to the reduced model to obtain the full model reduces the amount of error by 36464 (from 53637 to 17173). That is, adding latitude to the model substantially reduces the variability in skin cancer mortality. In this case, there appears to be a big advantage in using the larger full model over the simpler reduced model.\n\nWhere are we going with this general linear F-test approach? In short:\n• The general linear F-test involves a comparison between SSE(R) and SSE(F).\n• SSE(R) can never be smaller than SSE(F). It is always larger than (or possibly the same as) SSE(F).\n• If SSE(F) is close to SSE(R), then the variation around the estimated full model regression function is almost as large as the variation around the estimated reduced model regression function. If that's the case, it makes sense to use the simpler reduced model.\n• On the other hand, if SSE(F) and SSE(R) differ greatly, then the additional parameter(s) in the full model substantially reduce the variation around the estimated regression function. In this case, it makes sense to go with the larger full model.\n\nHow different does SSE(R) have to be from SSE(F) in order to justify using the larger full model? The general linear F-statistic:\n\nhelps answer this question. The F-statistic intuitively makes sense — it is a function of SSE(R)-SSE(F), the difference in the error between the two models. The degrees of freedom — denoted df and df — are those associated with the reduced and full model error sum of squares, respectively.\n\nWe use the general linear F-statistic to decide whether or not:\n• to reject the null hypothesis H : the reduced model,\n• in favor of the alternative hypothesis H : the full model.\n\nIn general, we reject H if F* is large — or equivalently if its associated P-value is small.\n\nThe test applied to the simple linear regression model\n\nFor simple linear regression, it turns out that the general linear F-test is just the same ANOVA F-test that we learned before. As noted earlier for the simple linear regression case, the full model is:\n\nand the reduced model is:\n\nTherefore, the appropriate null and alternative hypotheses are specified either as:\n\nThe degrees of freedom associated with the error sum of squares for the reduced model is n-1, and:\n\nThe degrees of freedom associated with the error sum of squares for the full model is n-2, and:\n\nNow, we can see how the general linear F-statistic just reduces algebraically to the ANOVA F-test that we know:\n\nThat is, the general linear F-statistic reduces to the ANOVA F-statistic:\n\nFor the student height and grade point average example:\n\nFor the skin cancer mortality example:\n\nThe P-value is calculated as usual. The P-value answers the question: \"what is the probability that we’d get an F* statistic as large as we did, if the null hypothesis were true?\" The P-value is determined by comparing F* to an F distribution with 1 numerator degree of freedom and n-2 denominator degrees of freedom. For the student height and grade point average example, the P-value is 0.761 (so we fail to reject H and we favor the reduced model), while for the skin cancer mortality example, the P-value is 0.000 (so we reject H and we favor the full model).\n\nDoes alcoholism have an effect on muscle strength? Some researchers (Urbano-Marquez, et al, 1989) who were interested in answering this question collected the following data (alcoholarm.txt) on a sample of 50 alcoholic men:\n• x = the total lifetime dose of alcohol (kg per kg of body weight) consumed\n• y = the strength of the deltoid muscle in the man's non-dominant arm\n\nThe full model is the model that would summarize a linear relationship between alcohol consumption and arm strength. The reduced model, on the other hand, is the model that claims there is no relationship between alcohol consumption and arm strength.\n\nTherefore, the appropriate null and alternative hypotheses are specified either as:\n\nUpon fitting the reduced model to the data, we obtain:\n\nNote that the reduced model does not appear to summarize the trend in the data very well.\n\nUpon fitting the full model to the data, we obtain:\n\nThe full model appears to decribe the trend in the data better than the reduced model.\n\nThe good news is that in the simple linear regression case, we don't have to bother with calculating the general linear F-statistic. Statistical software does it for us in the ANOVA table:\n\nAs you can see, the output reports both SSE(F) — the amount of error associated with the full model — and SSE(R) — the amount of error associated with the reduced model. The F-statistic is:\n\nand its associated P-value is < 0.001 (so we reject H and we favor the full model). We can conclude that there is a statistically significant linear association between lifetime alcohol consumption and arm strength."
    },
    {
        "link": "http://zhiyzuo.github.io/Linear-Regression-Diagnostic-in-Python",
        "document": "While linear regression is a pretty simple task, there are several assumptions for the model that we may want to validate. I follow the regression diagnostic here, trying to justify four principal assumptions, namely LINE in Python:\n• Independence (This is probably more serious for time series. I’ll pass it for now)\n\nI learnt this abbreviation of linear regression assumptions when I was taking a course on correlation and regression taught by Walter Vispoel at UIowa. Really helped me to remember these four little things!\n\nIn fact, itself contains useful modules for regression diagnostics. In addition to those, I want to go with somewhat manual yet very simple ways for more flexible visualizations.\n\nLet’s go with the depression data. More toy datasets can be found here. For simplicity, I randomly picked 3 columns.\n\nLinear regression is simple, with . We are able to use R style regression formula.\n\nNow let’s try to validate the four assumptions one by one\n\nBoth can be tested by plotting residuals vs. predictions, where residuals are prediction errors.\n\nIt seems like the corresponding residual plot is reasonably random. To confirm that, let’s go with a hypothesis test, Harvey-Collier multiplier test, for linearity\n\nSeveral tests exist for equal variance, with different alternative hypotheses. Let’s go with Breusch-Pagan test as an example. More can be found here. Small p-value ( below) shows that there is violation of homoscedasticity.\n\nUsually assumption violations are not independent of each other. Having one violations may lead to another. In this case, we see that both linearity and homoscedasticity are not met. Possible data transformation such as log, Box-Cox power transformation, and other fixes may be needed to get a better regression outcome.\n\nWe can apply normal probability plot to assess how the data (error) depart from normality visually:\n\nThe good fit indicates that normality is a reasonable approximation."
    },
    {
        "link": "https://statsmodels.org/dev/examples/notebooks/generated/regression_diagnostics.html",
        "document": "This example file shows how to use a few of the regression diagnostic tests in a real-life context. You can learn about more tests and find out more information about the tests here on the Regression Diagnostics page.\n\nNote that most of the tests described here only return a tuple of numbers, without any annotation. A full description of outputs is always included in the docstring and in the online documentation. For presentation purposes, we use the construct to pretty-print short descriptions in the examples below.\n\nKurtosis below is the sample kurtosis, not the excess kurtosis. A sample from the normal distribution has kurtosis equal to 3.\n\nDW statistic always ranges from 0 to 4. The closer to 2, the less autocorrelation is in the sample.\n\nOnce created, an object of class holds attributes and methods that allow users to assess the influence of each observation. For example, we can compute and extract the first few rows of DFbetas by:\n\nUseful information on leverage can also be plotted:\n\nOther plotting options can be found on the Graphics page.\n\nHarvey-Collier multiplier test for Null hypothesis that the linear specification is correct:"
    },
    {
        "link": "https://statsmodels.org/dev/examples/notebooks/generated/linear_regression_diagnostics_plots.html",
        "document": "Does not come with any sort of warranty. Please test the code one your end before using. (1) Fixed incorrect annotation of the top most extreme residuals in the Residuals vs Fitted and, especially, the Normal Q-Q plots. (2) Changed Residuals vs Leverage plot to match closer the y-axis range shown in the equivalent plot in the R package ggfortify. (3) Added horizontal line at y=0 in Residuals vs Leverage plot to match the plots in R package ggfortify and base R. (4) Added option for placing a vertical guideline on the Residuals vs Leverage plot using the rule of thumb of h = 2p/n to denote (5) Added two more ways to compute the Cook's Distance (D) threshold: (6) Fixed class name to conform to Pascal casing convention (7) Fixed Residuals vs Leverage legend to work with loc='best' must be instance of statsmodels.regression.linear_model object TypeError: if instance does not belong to above object In case you do not need all plots you can also independently make an individual plot/table \"result must be instance of statsmodels.regression.linear_model.RegressionResultsWrapper object\" (Roughly) Horizontal red line is an indicator that the residual has a linear pattern Used to visually check if residuals are normally distributed. Points spread along the diagonal line will suggest so. Used to check homoscedasticity of the residuals. Points falling outside Cook's distance curves are considered observation that can sway the fit Good to have none outside the curves. \"threshold_method must be one of the following: 'convention', 'dof', or 'baseR' (default)\" VIF, the variance inflation factor, is a measure of multicollinearity. VIF > 5 for a variable indicates that it is highly collinear with the"
    },
    {
        "link": "https://kirenz.com/blog/posts/2021-11-14-linear-regression-diagnostics-in-python",
        "document": ""
    },
    {
        "link": "https://jeffmacaluso.github.io/post/LinearRegressionAssumptions",
        "document": "Checking model assumptions is like commenting code. Everybody should be doing it often, but it sometimes ends up being overlooked in reality. A failure to do either can result in a lot of time being confused, going down rabbit holes, and can have pretty serious consequences from the model not being interpreted correctly.\n\nLinear regression is a fundamental tool that has distinct advantages over other regression algorithms. Due to its simplicity, it’s an exceptionally quick algorithm to train, thus typically makes it a good baseline algorithm for common regression scenarios. More importantly, models trained with linear regression are the most interpretable kind of regression models available - meaning it’s easier to take action from the results of a linear regression model. However, if the assumptions are not satisfied, the interpretation of the results will not always be valid. This can be very dangerous depending on the application.\n\nThis post contains code for tests on the assumptions of linear regression and examples with both a real-world dataset and a toy dataset.\n\nFor our real-world dataset, we’ll use the Boston house prices dataset from the late 1970’s. The toy dataset will be created using scikit-learn’s make_regression function which creates a dataset that should perfectly satisfy all of our assumptions.\n\nOne thing to note is that I’m assuming outliers have been removed in this blog post. This is an important part of any exploratory data analysis (which isn’t being performed in this post in order to keep it short) that should happen in real world scenarios, and outliers in particular will cause significant issues with linear regression. See Anscombe’s Quartet for examples of outliers causing issues with fitting linear regression models.\n\nHere are the variable descriptions for the Boston housing dataset straight from the documentation:\n• ZN: Proportion of residential land zoned for lots over 25,000 sq.ft.\n• B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n• Note: I really don’t like this variable because I think it’s both highly unethical to determine house prices by the color of people’s skin in a given area in a predictive modeling scenario and it irks me that it singles out one ethnicity rather than including all others. I am leaving it in for this post to keep the code simple, but I would remove it in a real-world situation.\n• MEDV: Median value of owner-occupied homes in $1,000’s\n\nNow that the data is loaded in, let’s preview it:\n\nBefore we test the assumptions, we’ll need to fit our linear regression models. I have a master function for performing all of the assumption testing at the bottom of this post that does this automatically, but to abstract the assumption tests out to view them independently we’ll have to re-write the individual tests to take the trained model as a parameter.\n\nAdditionally, a few of the tests use residuals, so we’ll write a quick function to calculate residuals. These are also calculated once in the master function at the bottom of the page, but this extra function is to adhere to DRY typing for the individual tests that use residuals.\n\nWe’re all set, so onto the assumption testing!\n\nThis assumes that there is a linear relationship between the predictors (e.g. independent variables or features) and the response variable (e.g. dependent variable or label). This also assumes that the predictors are additive.\n\nWhy it can happen: There may not just be a linear relationship among the data. Modeling is about trying to estimate a function that explains a process, and linear regression would not be a fitting estimator (pun intended) if there is no linear relationship.\n\nWhat it will affect: The predictions will be extremely inaccurate because our model is underfitting. This is a serious violation that should not be ignored.\n\nHow to detect it: If there is only one predictor, this is pretty easy to test with a scatter plot. Most cases aren’t so simple, so we’ll have to modify this by using a scatter plot to see our predicted values versus the actual values (in other words, view the residuals). Ideally, the points should lie on or around a diagonal line on the scatter plot.\n\nHow to fix it: Either adding polynomial terms to some of the predictors or applying nonlinear transformations . If those do not work, try adding additional variables to help capture the relationship between the predictors and the label.\n\nWe can see a relatively even spread around the diagonal line.\n\nNow, let’s compare it to the Boston dataset:\n\nWe can see in this case that there is not a perfect linear relationship. Our predictions are biased towards lower values in both the lower end (around 5-10) and especially at the higher values (above 40).\n\nMore specifically, this assumes that the error terms of the model are normally distributed. Linear regressions other than Ordinary Least Squares (OLS) may also assume normality of the predictors or the label, but that is not the case here.\n\nWhy it can happen: This can actually happen if either the predictors or the label are significantly non-normal. Other potential reasons could include the linearity assumption being violated or outliers affecting our model.\n\nWhat it will affect: A violation of this assumption could cause issues with either shrinking or inflating our confidence intervals.\n\nHow to detect it: There are a variety of ways to do so, but we’ll look at both a histogram and the p-value from the Anderson-Darling test for normality.\n\nHow to fix it: It depends on the root cause, but there are a few options. Nonlinear transformations of the variables, excluding specific variables (such as long-tailed variables), or removing outliers may solve this problem.\n\nAs with our previous assumption, we’ll start with the linear dataset:\n\nNow let’s run the same test on the Boston dataset:\n\nThis isn’t ideal, and we can see that our model is biasing towards under-estimating.\n\nThis assumes that the predictors used in the regression are not correlated with each other. This won’t render our model unusable if violated, but it will cause issues with the interpretability of the model.\n\nWhy it can happen: A lot of data is just naturally correlated. For example, if trying to predict a house price with square footage, the number of bedrooms, and the number of bathrooms, we can expect to see correlation between those three variables because bedrooms and bathrooms make up a portion of square footage.\n\nWhat it will affect: Multicollinearity causes issues with the interpretation of the coefficients. Specifically, you can interpret a coefficient as “an increase of 1 in this predictor results in a change of (coefficient) in the response variable, holding all other predictors constant.” This becomes problematic when multicollinearity is present because we can’t hold correlated predictors constant. Additionally, it increases the standard error of the coefficients, which results in them potentially showing as statistically insignificant when they might actually be significant.\n\nHow to detect it: There are a few ways, but we will use a heatmap of the correlation as a visual aid and examine the variance inflation factor (VIF).\n\nHow to fix it: This can be fixed by other removing predictors with a high variance inflation factor (VIF) or performing dimensionality reduction.\n\nEverything looks peachy keen. Onto the Boston dataset:\n\nThis isn’t quite as egregious as our normality assumption violation, but there is possible multicollinearity for most of the variables in this dataset.\n\nIV) No Autocorrelation of the Error Terms\n\nThis assumes no autocorrelation of the error terms. Autocorrelation being present typically indicates that we are missing some information that should be captured by the model.\n\nWhy it can happen: In a time series scenario, there could be information about the past that we aren’t capturing. In a non-time series scenario, our model could be systematically biased by either under or over predicting in certain conditions. Lastly, this could be a result of a violation of the linearity assumption.\n\nWhat it will affect: This will impact our model estimates.\n\nHow to detect it: We will perform a Durbin-Watson test to determine if either positive or negative correlation is present. Alternatively, you could create plots of residual autocorrelations.\n\nHow to fix it: A simple fix of adding lag variables can fix this problem. Alternatively, interaction terms, additional variables, or additional transformations may fix this.\n\nAnd with our Boston dataset:\n\nWe’re having signs of positive autocorrelation here, but we should expect this since we know our model is consistently under-predicting and our linearity assumption is being violated. Since this isn’t a time series dataset, lag variables aren’t possible. Instead, we should look into either interaction terms or additional transformations.\n\nThis assumes homoscedasticity, which is the same variance within our error terms. Heteroscedasticity, the violation of homoscedasticity, occurs when we don’t have an even variance across the error terms.\n\nWhy it can happen: Our model may be giving too much weight to a subset of the data, particularly where the error variance was the largest.\n\nWhat it will affect: Significance tests for coefficients due to the standard errors being biased. Additionally, the confidence intervals will be either too wide or too narrow.\n\nHow to detect it: Plot the residuals and see if the variance appears to be uniform.\n\nHow to fix it: Heteroscedasticity (can you tell I like the scedasticity words?) can be solved either by using weighted least squares regression instead of the standard OLS or transforming either the dependent or highly skewed variables. Performing a log transformation on the dependent variable is not a bad place to start.\n\nPlotting the residuals of our ideal dataset:\n\nThere don’t appear to be any obvious problems with that.\n\nNext, looking at the residuals of the Boston dataset:\n\nWe can’t see a fully uniform variance across our residuals, so this is potentially problematic. However, we know from our other tests that our model has several issues and is under predicting in many cases.\n\nWe can clearly see that a linear regression model on the Boston dataset violates a number of assumptions which cause significant problems with the interpretation of the model itself. It’s not uncommon for assumptions to be violated on real-world data, but it’s important to check them so we can either fix them and/or be aware of the flaws in the model for the presentation of the results or the decision making process.\n\nIt is dangerous to make decisions on a model that has violated assumptions because those decisions are effectively being formulated on made-up numbers. Not only that, but it also provides a false sense of security due to trying to be empirical in the decision making process. Empiricism requires due diligence, which is why these assumptions exist and are stated up front. Hopefully this code can help ease the due diligence process and make it less painful.\n\nThis function performs all of the assumption tests listed in this blog post:\n\n\"\"\" Tests a linear regression on the model to see if assumptions are being met \"\"\" # Setting feature names to x1, x2, x3, etc. if they are not defined # Multi-threading if the dataset is a size where doing so is beneficial \"\"\" Linearity: Assumes there is a linear relationship between the predictors and the response variable. If not, either a polynomial term or another algorithm should be used. \"\"\" 'Assumption 1: Linear Relationship between the Target and the Features' 'Checking with a scatter plot of actual vs. predicted. Predictions should follow the diagonal line.' 'If non-linearity is apparent, consider adding a polynomial term' \"\"\" Normality: Assumes that the error terms are normally distributed. If they are not, nonlinear transformations of variables may solve this. This assumption being violated primarily causes issues with the confidence intervals \"\"\" 'Assumption 2: The error terms are normally distributed' 'Using the Anderson-Darling test for normal distribution' # Performing the test on the residuals 'p-value from the test - below 0.05 generally means non-normal:' # Reporting the normality of the residuals 'Residuals are not normally distributed' 'Confidence intervals will likely be affected' \"\"\" Multicollinearity: Assumes that predictors are not correlated with each other. If there is correlation among the predictors, then either remove prepdictors with high Variance Inflation Factor (VIF) values or perform dimensionality reduction This assumption being violated causes issues with interpretability of the coefficients and the standard errors of the coefficients. \"\"\" 'Assumption 3: Little to no multicollinearity among predictors' '> 10: An indication that multicollinearity may be present' '> 100: Certain multicollinearity among the variables' # Gathering the VIF for each variable # Gathering and printing total cases of possible or definite multicollinearity \"\"\" Autocorrelation: Assumes that there is no autocorrelation in the residuals. If there is autocorrelation, then there is a pattern that is not explained due to the current value being dependent on the previous value. This may be resolved by adding a lag variable of either the dependent variable or some of the predictors. \"\"\" 'Values of 1.5 < d < 2.5 generally show that there is no autocorrelation in the data' 'Little to no autocorrelation' 'If heteroscedasticity is apparent, confidence intervals and predictions will be affected'"
    }
]