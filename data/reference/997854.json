[
    {
        "link": "https://tensorflow.org/guide/keras",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nKeras is the high-level API of the TensorFlow platform. It provides an approachable, highly-productive interface for solving machine learning (ML) problems, with a focus on modern deep learning. Keras covers every step of the machine learning workflow, from data processing to hyperparameter tuning to deployment. It was developed with a focus on enabling fast experimentation.\n\nWith Keras, you have full access to the scalability and cross-platform capabilities of TensorFlow. You can run Keras on a TPU Pod or large clusters of GPUs, and you can export Keras models to run in the browser or on mobile devices. You can also serve Keras models via a web API.\n\nKeras is designed to reduce cognitive load by achieving the following goals:\n• Minimize the number of actions required for common use cases.\n• Follow the principle of progressive disclosure of complexity: It's easy to get started, and you can complete advanced workflows by learning as you go.\n\nWho should use Keras\n\nThe short answer is that every TensorFlow user should use the Keras APIs by default. Whether you're an engineer, a researcher, or an ML practitioner, you should start with Keras.\n\nThere are a few use cases (for example, building tools on top of TensorFlow or developing your own high-performance platform) that require the low-level TensorFlow Core APIs. But if your use case doesn't fall into one of the Core API applications, you should prefer Keras.\n\nThe core data structures of Keras are layers and models. A layer is a simple input/output transformation, and a model is a directed acyclic graph (DAG) of layers.\n\nThe class is the fundamental abstraction in Keras. A encapsulates a state (weights) and some computation (defined in the method).\n\nWeights created by layers can be trainable or non-trainable. Layers are recursively composable: If you assign a layer instance as an attribute of another layer, the outer layer will start tracking the weights created by the inner layer.\n\nYou can also use layers to handle data preprocessing tasks like normalization and text vectorization. Preprocessing layers can be included directly into a model, either during or after training, which makes the model portable.\n\nA model is an object that groups layers together and that can be trained on data.\n\nThe simplest type of model is the model, which is a linear stack of layers. For more complex architectures, you can either use the Keras functional API, which lets you build arbitrary graphs of layers, or use subclassing to write models from scratch.\n• : Trains the model for a fixed number of epochs.\n• : Returns the loss and metrics values for the model; configured via the method.\n\nThese methods give you access to the following built-in training features:\n• Callbacks. You can leverage built-in callbacks for early stopping, model checkpointing, and TensorBoard monitoring. You can also implement custom callbacks.\n• Distributed training. You can easily scale up your training to multiple GPUs, TPUs, or devices.\n• Step fusing. With the argument in , you can process multiple batches in a single call, which greatly improves device utilization on TPUs.\n\nFor a detailed overview of how to use , see the training and evaluation guide. To learn how to customize the built-in training and evaluation loops, see Customizing what happens in .\n\nKeras provides many other APIs and tools for deep learning, including:\n\nFor a full list of available APIs, see the Keras API reference. To learn more about other Keras projects and initiatives, see The Keras ecosystem.\n\nTo get started using Keras with TensorFlow, check out the following topics:\n• Making new layers and models via subclassing\n• Customizing what happens in fit()\n\nTo learn more about Keras, see the following topics at keras.io:"
    },
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/Model",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nA model grouping layers into an object with training/inference features.\n\nUsed in the notebooks\n\nThere are three ways to instantiate a :\n\nYou start from , you chain layer calls to specify the model's forward pass, and finally you create your model from inputs and outputs:\n\nA new Functional API model can also be created by using the intermediate tensors. This enables you to quickly extract sub-components of the model.\n\nNote that the and models are not created with objects, but with the tensors that originate from objects. Under the hood, the layers and weights will be shared across these models, so that user can train the , and use or to do feature extraction. The inputs and outputs of the model can be nested structures of tensors as well, and the created models are standard Functional API models that support all the existing APIs.\n\nIn that case, you should define your layers in and you should implement the model's forward pass in .\n\nIf you subclass , you can optionally have a argument (boolean) in , which you can use to specify a different behavior in training and inference:\n\nOnce the model is created, you can config the model with losses and metrics with , train the model with , or use the model to do prediction with .\n\nIn addition, is a special case of model where the model is purely a stack of single-input, single-output layers.\n\nCompiles the model with the information given in config.\n\nThis method uses the information in the config (optimizer, loss, metrics, etc.) to compile the model.\n\nCompute the total loss, validate it, and return it.\n\nSubclasses can optionally override this method to provide custom loss computation logic.\n\nUpdate metric states and collect all metrics to be returned.\n\nSubclasses can optionally override this method to provide custom metric updating and collection logic.\n\nReturns the loss value & metrics values for the model in test mode.\n\nComputation is done in batches (see the arg.)\n\nThis method lets you export a model to a lightweight SavedModel artifact that contains the model's forward pass only (its method) and can be served via e.g. TF-Serving. The forward pass is registered under the name (see example below).\n\nThe original code of the model (including any custom layers you may have used) is no longer necessary to reload the artifact -- it is entirely standalone.\n\nIf you would like to customize your serving endpoints, you can use the lower-level class. The method relies on internally.\n\nTrains the model for a fixed number of epochs (dataset iterations).\n\nUnpacking behavior for iterator-like inputs: A common pattern is to pass an iterator like object such as a or a to , which will in fact yield not only features ( ) but optionally targets ( ) and sample weights ( ). Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for and respectively. Any other type provided will be wrapped in a length-one tuple, effectively treating everything as . When yielding dicts, they should still adhere to the top-level tuple structure, e.g. . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the . The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: where it is unclear if the tuple was intended to be unpacked into , , and or passed through as a single element to .\n\nThis method is the reverse of , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by ).\n\nReturns a serialized config with information for compiling the model.\n\nThis method returns a config dictionary containing all the information (optimizer, loss, metrics, etc.) with which the model was compiled.\n\nRetrieves a layer based on either its name (unique) or index.\n\nIf and are both provided, will take precedence. Indices are based on order of horizontal graph traversal (bottom-up).\n\nIf any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method.\n\nWeights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights.\n\nIf you have modified your model, for instance by adding a new layer (with weights) or by changing the shape of the weights of a layer, you can choose to ignore errors and continue loading by setting . In this case any layer with mismatching weights will be skipped. A warning will be displayed for each skipped layer.\n\nComputation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time.\n\nFor small numbers of inputs that fit in one batch, directly use for faster execution, e.g., , or if you have layers such as that behave differently during inference.\n\nNote that is an alias for .\n• The model's optimizer's state (if any)\n\nThus models can be reinstantiated in the exact same state.\n\nTest the model on a single batch of samples.\n\nTo load a network from a JSON save file, use ."
    },
    {
        "link": "https://keras.io",
        "document": "\"Keras is one of the key building blocks in YouTube Discovery's new modeling infrastructure. It brings a clear, consistent API and a common way of expressing modeling ideas to 8 teams across the major surfaces of YouTube recommendations.\" \"Keras has tremendously simplified the development workflow of Waymo's ML practitioners, with the benefits of a significantly simplified API, standardized interface and behaviors, easily shareable model building components, and highly improved debuggability.\" \"The best thing you can say about any software library is that the abstractions it chooses feel completely natural, such that there is zero friction between thinking about what you want to do and thinking about how you want to code it. That's exactly what you get with Keras.\" \"Keras allows us to prototype, research and deploy deep learning models in an intuitive and streamlined manner. The functional API makes code comprehensible and stylistic, allowing for effective knowledge transfer between scientists on my team.\" \"Keras has something for every user: easy customisability for the academic; out-of-the-box, performant models and pipelines for use by the industry, and readable, modular code for the student. Keras has made it very simple to quickly iterate over experiments without worrying about low-level details.\" \"Keras is the perfect abstraction layer to build and operationalize Deep Learning models. I've been using it since 2018 to develop and deploy models for some of the largest companies in the world [...] a combination of Keras, TensorFlow, and TFX has no rival.\" \"What I personally like the most about Keras (aside from its intuitive APIs), is the ease of transitioning from research to production. I can train a Keras model, convert it to TF Lite and deploy it to mobile & edge devices.\" \"Keras is that sweet spot where you get flexibility for research and consistency for deployment. Keras is to Deep Learning what Ubuntu is to Operating Systems.\" \"Keras's user-friendly design means it's easy to learn and easy to use [...] it allows for the rapid prototyping and deployment of models across a variety of platforms.\"\n\nThe purpose of Keras is to give an unfair advantage to any developer looking to ship Machine Learning-powered apps. Keras focuses on debugging speed, code elegance & conciseness, maintainability, and deployability. When you choose Keras, your codebase is smaller, more readable, easier to iterate on. Your models run faster thanks to XLA compilation with JAX and TensorFlow, and are easier to deploy across every surface (server, mobile, browser, embedded) thanks to the serving components from the TensorFlow and PyTorch ecosystems, such as TF Serving, TorchServe, TF Lite, TF.js, and more. Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages. Keras also gives the highest priority to crafting great documentation and developer guides. Keras works with JAX, TensorFlow, and PyTorch. It enables you to create models that can move across framework boundaries and that can benefit from the ecosystem of all three of these frameworks. Keras is an industry-strength framework that can scale to large clusters of GPUs or an entire TPU pod. It's not only possible; it's easy. Keras is used by CERN, NASA, NIH, and many more scientific organizations around the world (and yes, Keras is used at the LHC). Keras has the low-level flexibility to implement arbitrary research ideas while offering optional high-level convenience features to speed up experimentation cycles."
    },
    {
        "link": "https://tensorflow.org/tutorials",
        "document": "The TensorFlow tutorials are written as Jupyter notebooks and run directly in Google Colab—a hosted notebook environment that requires no setup. At the top of each tutorial, you'll see a Run in Google Colab button. Click the button to open the notebook and run the code yourself."
    },
    {
        "link": "https://keras.io/getting_started",
        "document": "Are you a machine learning engineer looking for a Keras introduction one-pager? Read our guide Introduction to Keras for engineers.\n\nWant to learn more about Keras 3 and its capabilities? See the Keras 3 launch announcement.\n\nAre you looking for detailed guides covering in-depth usage of different parts of the Keras API? Read our Keras developer guides.\n\nAre you looking for tutorials showing Keras in action across a wide range of use cases? See the Keras code examples: over 150 well-explained notebooks demonstrating Keras best practices in computer vision, natural language processing, and generative AI.\n\nYou can install Keras from PyPI via:\n\nYou can check your local Keras version number via:\n\nTo use Keras 3, you will also need to install a backend framework – either JAX, TensorFlow, or PyTorch:\n\nIf you install TensorFlow 2.15, you should reinstall Keras 3 afterwards. The cause is that will overwrite your Keras installation with . This step is not necessary for TensorFlow versions 2.16 onwards as starting in TensorFlow 2.16, it will install Keras 3 by default.\n\nKerasCV and KerasHub can be installed via pip:\n\nYou can export the environment variable or you can edit your local config file at to configure your backend. Available backend options are: , , . Example:\n\nIn Colab, you can do:\n\nNote: The backend must be configured before importing Keras, and the backend cannot be changed after the package has been imported.\n\nIf you are running on Colab or Kaggle, the GPU should already be configured, with the correct CUDA version. Installing a newer version of CUDA on Colab or Kaggle is typically not possible. Even though pip installers exist, they rely on a pre-installed NVIDIA driver and there is no way to update the driver on Colab or Kaggle.\n\nIf you want to attempt to create a \"universal environment\" where any backend can use the GPU, we recommend following the dependency versions used by Colab (which seeks to solve this exact problem). You can install the CUDA driver from here, then pip install backends by following their respective CUDA installation instructions: Installing JAX, Installing TensorFlow, Installing PyTorch\n\nThis setup is recommended if you are a Keras contributor and are running Keras tests. It installs all backends but only gives GPU access to one backend at a time, avoiding potentially conflicting dependency requirements between backends. You can use the following backend-specific requirements files:\n\nThese install all CUDA-enabled dependencies via pip. They expect a NVIDIA driver to be preinstalled. We recommend a clean python environment for each backend to avoid CUDA version mismatches. As an example, here is how to create a JAX GPU environment with Conda:\n\nFrom TensorFlow 2.0 to TensorFlow 2.15 (included), doing will also install the corresponding version of Keras 2 – for instance, will install . That version of Keras is then available via both and (the namespace).\n\nStarting with TensorFlow 2.16, doing will install Keras 3. When you have TensorFlow >= 2.16 and Keras 3, then by default ( ) will be Keras 3.\n\nMeanwhile, the legacy Keras 2 package is still being released regularly and is available on PyPI as (or equivalently – note that and are equivalent in PyPI package names). To use it, you can install it via then import it via .\n\nShould you want to stay on Keras 2 after upgrading to TensorFlow 2.16+, you can configure your TensorFlow installation so that points to . To achieve this:\n• Make sure to install . Note that TensorFlow does not install it by default.\n\nThere are several ways to export the environment variable:\n• You can simply run the shell command before launching the Python interpreter.\n• You can add to your file. That way the variable will still be exported when you restart your shell.\n• You can start your Python script with:\n\nThese lines would need to be before any statement.\n\nThe following Keras + JAX versions are compatible with each other:\n\nThe following Keras + TensorFlow versions are compatible with each other:\n\nThe following Keras + PyTorch versions are compatible with each other:"
    },
    {
        "link": "https://stackoverflow.com/questions/56293964/categorical-focal-loss-on-keras",
        "document": "Sure. I found this by googling . It was the first result, and took even less time to implement.\n\nThis was the second result on google. Tried it too, and it also works fine; took one of my classification problems up to roc score of 0.9726."
    },
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/losses/CategoricalFocalCrossentropy",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nUse this crossentropy loss function when there are two or more label classes and if you want to handle class imbalance without using . We expect labels to be provided in a representation.\n\nAccording to Lin et al., 2018, it helps to apply a focal factor to down-weight easy examples and focus more on hard examples. The general formula for the focal loss (FL) is as follows:\n\nwhere is defined as follows:\n\nis the , where is a focusing parameter. When = 0, there is no focal effect on the cross entropy. reduces the importance given to simple examples in a smooth manner.\n\nThe authors use alpha-balanced variant of focal loss (FL) in the paper:\n\nwhere is the weight factor for the classes. If = 1, the loss won't be able to handle class imbalance properly as all classes will have the same weight. This can be a constant or a list of constants. If alpha is a list, it must have the same length as the number of classes.\n\nThe formula above can be generalized to:\n\nwhere minus comes from (CE).\n\nExtending this to multi-class case is straightforward:\n\nIn the snippet below, there is floating pointing values per example. The shape of both and are ."
    },
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy",
        "document": "Computes the crossentropy loss between the labels and predictions.\n\nUsed in the notebooks\n\nUse this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a representation. If you want to provide labels as integers, please use loss. There should be floating point values per feature, i.e., the shape of both and are ."
    },
    {
        "link": "https://keras.io/api/losses",
        "document": "The purpose of loss functions is to compute the quantity that a model should seek to minimize during training.\n\nNote that all losses are available both via a class handle and via a function handle. The class handles enable you to pass configuration arguments to the constructor (e.g. ), and they perform reduction by default when used in a standalone way (see details below).\n\nThis is the class to subclass in order to create new custom losses.\n• reduction: Type of reduction to apply to the loss. In almost all cases this should be . Supported options are , , , or . sums the loss, and sum the loss and divide by the sample size, and sums the loss and divides by the sum of the sample weights. and perform no aggregation. Defaults to .\n• name: Optional name for the loss instance.\n• dtype: The dtype of the loss's computations. Defaults to , which means using . is a unless set to different value (via ). If a is provided, then the will be utilized.\n\nTo be implemented by subclasses:\n• : Contains the logic for loss calculation using , .\n\nA loss function is one of the two arguments required for compiling a Keras model:\n\nAll built-in loss functions may also be passed via their string identifier:\n\nLoss functions are typically created by instantiating a loss class (e.g. ). All losses are also provided as function handles (e.g. ).\n\nUsing classes enables you to pass configuration arguments at instantiation time, e.g.:\n• y_true: Ground truth values, of shape . For sparse loss functions, such as sparse categorical crossentropy, the shape should be\n• sample_weight: Optional acts as reduction weighting coefficient for the per-sample losses. If a scalar is provided, then the loss is simply scaled by the given value. If is a tensor of size , then the total loss for each sample of the batch is rescaled by the corresponding element in the vector. If the shape of is (or can be broadcasted to this shape), then each loss element of is scaled by the corresponding value of . (Note on : all loss functions reduce by 1 dimension, usually .)\n\nBy default, loss functions return one scalar loss value for each input sample in the batch dimension, e.g.\n\nHowever, loss class instances feature a constructor argument, which defaults to (i.e. average). Allowable values are \"sum_over_batch_size\", \"sum\", and \"none\":\n• \"sum_over_batch_size\" means the loss instance will return the average of the per-sample losses in the batch.\n• \"sum\" means the loss instance will return the sum of the per-sample losses in the batch.\n• \"none\" means the loss instance will return the full array of per-sample losses.\n\nNote that this is an important difference between loss functions like and default loss class instances like : the function version does not perform reduction, but by default the class instance does.\n\nWhen using , this difference is irrelevant since reduction is handled by the framework.\n\nHere's how you would use a loss class instance as part of a simple training loop:\n\nAny callable with the signature that returns an array of losses (one of sample in the input batch) can be passed to as a loss. Note that sample weighting is automatically supported for any such loss.\n\nLoss functions applied to the output of a model aren't the only way to create losses.\n\nWhen writing the method of a custom layer or a subclassed model, you may want to compute scalar quantities that you want to minimize during training (e.g. regularization losses). You can use the layer method to keep track of such loss terms.\n\nHere's an example of a layer that adds a sparsity regularization loss based on the L2 norm of the inputs:\n\nLoss values added via can be retrieved in the list property of any or (they are recursively retrieved from every underlying layer):\n\nThese losses are cleared by the top-level layer at the start of each forward pass – they don't accumulate. So always contain only the losses created during the last forward pass. You would typically use these losses by summing them before computing your gradients when writing a training loop.\n\nWhen using , such loss terms are handled automatically.\n\nWhen writing a custom training loop, you should retrieve these terms by hand from , like this:\n\nSee the documentation for more details."
    },
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/losses",
        "document": "Save and categorize content based on your preferences.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nThis file was autogenerated. Do not edit it by hand, since your modifications would be overwritten.\n\n: Computes the cross-entropy loss between true labels and predicted labels.\n\n: Computes the crossentropy loss between the labels and predictions.\n\n: Computes the Dice loss value between and .\n\n: Computes the logarithm of the hyperbolic cosine of the prediction error.\n\n: Computes the mean of absolute difference between labels and predictions.\n\n: Computes the mean absolute percentage error between & .\n\n: Computes the mean of squares of errors between labels and predictions.\n\n: Computes the mean squared logarithmic error between & .\n\n: Computes the crossentropy loss between the labels and predictions.\n\n: Computes the Tversky loss value between and .\n\n: Computes the mean absolute error between labels and predictions.\n\n: Computes the mean absolute percentage error between & .\n\n: Computes the mean squared error between labels and predictions.\n\n: Computes the mean squared logarithmic error between & .\n\n: Computes the cosine similarity between labels and predictions.\n\n: Computes the Dice loss value between and .\n\n: Logarithm of the hyperbolic cosine of the prediction error.\n\n: Computes the mean absolute error between labels and predictions.\n\n: Computes the mean absolute percentage error between & .\n\n: Computes the mean squared error between labels and predictions.\n\n: Computes the mean squared logarithmic error between & .\n\n: Computes the Poisson loss between y_true and y_pred.\n\n: Computes the Tversky loss value between and ."
    }
]