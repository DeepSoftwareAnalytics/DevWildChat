[
    {
        "link": "https://khronos.org/opengl/wiki/Framebuffer_Object",
        "document": "Framebuffer Objects are OpenGL Objects, which allow for the creation of user-defined Framebuffers. With them, one can render to non-Default Framebuffer locations, and thus render without disturbing the main screen.\n\nFramebuffer objects are a collection of attachments. To help explain lets explicitly define certain terminology.\n\nAs standard OpenGL Objects, FBOs have the usual glGenFramebuffers and glDeleteFramebuffers functions. As expected, it also has the usual glBindFramebuffer function, to bind an FBO to the context.\n\nThe target​ parameter for this object can take one of 3 values: GL_FRAMEBUFFER, GL_READ_FRAMEBUFFER, or GL_DRAW_FRAMEBUFFER. The last two allow you to bind an FBO so that reading commands (glReadPixels, etc) and writing commands (all rendering commands) can happen to two different framebuffers. The GL_FRAMEBUFFER binding target simply sets both the read and the write to the same FBO.\n\nThe default framebuffer has buffer names like GL_FRONT, GL_BACK, GL_AUXi, GL_ACCUM, and so forth. FBOs do not use these.\n\nInstead, FBOs have a different set of image names. Each FBO image represents an attachment point, a location in the FBO where an image can be attached. FBOs have the following attachment points:\n• : These are an implementation-dependent number of attachment points. You can query to determine the number of color attachments that an implementation will allow. The minimum value for this is 8, so you are guaranteed to be able to have at least color attachments 0-7. These attachment points can only have images bound to them with color-renderable formats. All compressed image formats are not color-renderable, and thus cannot be attached to an FBO.\n• : This attachment point can only have images with depth formats bound to it. The image attached becomes the Depth Buffer for the FBO. Note that if no depth image is attached, Depth Testing will be disabled when rendering to this FBO.\n• : This attachment point can only have images with stencil formats bound to it. The image attached becomes the stencil buffer for the FBO.\n• : This is shorthand for \"both depth and stencil\". The image attached becomes both the depth and stencil buffers.\n\nNow that we know where images can be attached to FBOs, we can start talking about how to actually attach images to these. In order to attach images to an FBO, we must first bind the FBO to the context with glBindFramebuffer.\n\nYou can attach images from most texture types to the framebuffer object. However, framebuffers are designed for 2D rendering. So there is a need to consider how different texture types map to framebuffer images. Remember that textures are a set of images. Textures can have mipmaps, and individual mipmap levels could contain one or more images.\n\nThe way different texture types map to framebuffer images is as follows:\n• Images in a 1D texture are considered 2D images with a vertical height of 1. Each individual image can be uniquely identified by a mipmap .\n• Images in a 2D texture are taken as normal. Each individual image can be uniquely identified by a mipmap .\n• A mipmap level of a 3D texture is considered to be a set of 2D images, with the number of these images being the extent of the Z coordinate for that mipmap level. Each integer value for the Z of a 3D texture mipmap level is a separate 2D layer. So each image in a 3D texture is uniquely identified by a and a mipmap . Recall that different mipmap levels in a 3D texture will have different counts of Z coordinates.\n• Rectangle Textures contain a single 2D image, which is identified by a of 0.\n• Cubemap Textures contain 6 2D images per mipmap. Thus, each image in a cubemap texture can be uniquely identified by a face and a mipmap . However, in some API functions, individual faces in a mipmap level are identified by a index instead of a .\n• Each mipmap level of 1D or 2D Array Textures contains a number of images, equal to the count images in the array. Thus, each individual image is uniquely identified by a (the array index) and a mipmap . For 1D array textures, each image has a height of 1. Unlike 3D textures, the doesn't change when going down the mipmap hierarchy.\n• Cubemap Array Textures work like 2D array textures, only with an image count 6 times the number of its layers. A 2D image in the cubemap array is identified by the layer-face index and a mipmap .\n• Buffer Textures cannot be attached to framebuffers.\n\nThe words level​, layer​, and target​ above are significant, as they match the parameters of the following functions used for attaching textures:\n\n\n\n The target​ parameter here is the same as the one for bind. However, GL_FRAMEBUFFER doesn't mean both read and draw (as that would make no sense); instead, it is the same as GL_DRAW_FRAMEBUFFER. The attachment​ parameter is one of the above attachment points.\n\nThe texture​ argument is the texture object name you want to attach from. If you pass zero as texture​, this has the effect of clearing the attachment for this attachment​, regardless of what kind of image was attached there.\n\nBecause texture objects can hold multiple images, you must specify exactly which image to attach to this attachment point. The parameters match their above definitions, with the exception of textarget​.\n\nWhen attaching a non-cubemap, textarget​ should be the proper texture type: GL_TEXTURE_1D, GL_TEXTURE_2D_MULTISAMPLE, etc. When attaching a (non-array) cubemap, you must use the Texture2D function, and the textarget​ must be one of the 6 targets for cubemap binding. When attaching an image from a cubemap array, you must use TextureLayer, with the layer​ being a layer-face.\n\nRenderbuffers can also be attached to FBOs. Indeed, this is the only way to use them besides just creating the storage for them.\n\nOnce you have created a renderbuffer object and made storage for it (given a size and format), you can attach it to an FBO with this function:\n\nThe parameters work mostly the same as with texture attachment. The renderbuffertarget​ param must be GL_RENDERBUFFER. The renderbuffer​ parameter is the renderbuffer object's name.\n\nA layered image, as previously defined, is an ordered sequence of images of a particular size. A number of different kinds of textures can be considered layered. Layered images are used with Layered Rendering, which sends different primitives to different layers within the framebuffer.\n\nA single mipmap level of a 1D or 2D Array Texture can be attached as a layered image, where the number of layers is the array size. A single mipmap level of a 3D texture likewise can be attached as a layered image, where the number of layers is the depth of that particular mipmap level. Also, a mipmap level of a Cubemap Texture can be attached as a layered image. For cubemaps, you get exactly 6 layers, one for each face. And the order of the faces is the same as the order of the enumerators:\n\nFor cubemap arrays, the value that gl_Layer represents is the layer-face index. Thus it is the face within a layer, ordered as above. So if you want to render to the 3rd layer, +z face, you would set gl_Layer to (2 * 6) + 4, or 16.\n\nEach mipmap level, when can be attached as a layered image, has a specific number of layers. For 1D and 2D array textures, it is the number of layers in the texture as a whole. For 3D textures, this is the depth of that particular mipmap level. For cubemaps, this is always exactly 6 layers: one per face. Cubemap arrays have 6 * the number of layers, which is the number of layer-faces.\n\nTo attach a mipmap level of a texture as a layered image, use the following command:\n\nThe parameters have the same meaning as above. Indeed, this function can replace many of the uses for glFramebufferTexture1D, 2D, or Layer, as long as you do not intend to attach specific layers of array textures, cubemaps, or 3D textures as regular, non-layered images. If the texture​ is one of these kinds of textures, then the given mipmap level​ will be attached as a layered image with the number of layers that the given texture has.\n\nIt is possible to render to a framebuffer object that has no attachments. Obviously none of the fragment shader outputs will be written to anywhere in this case, but rendering can otherwise proceed as normal. This is useful for using arbitrary reading and writing of image data from shaders, rather than writing to a bound framebuffer.\n\nHowever, the rasterization of primitives is always based on the area and characteristics of the bound framebuffer. These characteristics (size, number of samples for multisample rendering, etc) would normally be defined by the attached images. If no images are attached, these characteristics must be defined in some other fashion.\n\nThe characteristics for an FBO with no attachments can be set with this function:\n\ntarget​ is the location where the framebuffer object is bound. To set the width, set pname​ to GL_FRAMEBUFFER_DEFAULT_WIDTH; to set the height, use GL_FRAMEBUFFER_DEFAULT_HEIGHT.\n\nLayered framebuffers can be simulated by setting GL_FRAMEBUFFER_DEFAULT_LAYERS to a layer count other than 0. Multisample framebuffers can be simulated by setting GL_FRAMEBUFFER_DEFAULT_SAMPLES to a number of samples other than 0. Fixed multisample location can similarly be simulated by setting GL_FRAMEBUFFER_DEFAULT_FIXED_SAMPLE_LOCATIONS to a non-zero value.\n\nNote that rendering is only limited to these parameters if no images are attached to the FBO. If images are attached, then these parameters are ignored. You should only set these values if you intend to use the FBO without images.\n\nEach attachment point in a FBO has specific restrictions on the format of images that can be attached to it. However, it is not an immediate GL error to attach an image to an attachment point that doesn't support that format. It is an error to try to use an FBO that has been improperly set up. There are also a number of other issues with regard to sizes of images and so forth that must be detected in order to be able to safely use the FBO.\n\nAn FBO that is valid for use is said to be \"framebuffer complete\". To test framebuffer completeness, call this function:\n\nYou are not required to call this manually. However, using an incomplete FBO is an error, so it's always a good idea to check.\n\nThe return value is GL_FRAMEBUFFER_COMPLETE if the FBO can be used. If it is something else, then there is a problem. Below are the rules for completeness and the associated return values you will receive if they are not followed.\n\nEach attachment point itself must be complete according to these rules. Empty attachments (attachments with no image attached) are complete by default. If an image is attached, it must adhere to the following rules:\n• The source object for the image still exists and has the same type it was attached with.\n• The image has a non-zero width and height (the height of a 1D image is assumed to be 1). The width/height must also be less than and respectively (if GL 4.3/ARB_framebuffer_no_attachments).\n• The layer for 3D or array textures attachments is less than the depth of the texture. It must also be less than (if GL 4.3/ARB_framebuffer_no_attachments).\n• The number of samples must be less than (if GL 4.3/ARB_framebuffer_no_attachments).\n• The image's format must match the attachment point's requirements, as defined above. Color-renderable formats for color attachments, etc.\n\nThese are the rules for framebuffer completeness. The order of these rules matters.\n• If the of references the Default Framebuffer (ie: FBO object number 0 is bound), and the default framebuffer does not exist, then you will get . If the default framebuffer exists, then you always get . The rest of the rules apply when an FBO is bound.\n• All attachments must be attachment complete. ( when false).\n• There must be at least one image attached to the FBO, or if OpenGL 4.3 ARB_framebuffer_no_attachments is available, the and parameters of the framebuffer must both be non-zero. ( when false).\n• Each draw buffer must either specify color attachment points that have images attached or must be . ( when false). Note that this test is not performed if OpenGL 4.1 ARB_ES2_compatibility is available.\n• If the read buffer is set, then it must specify an attachment point that has an image attached. ( when false). Note that this test is not performed if OpenGL 4.1 ARB_ES2_compatibility is available.\n• All images must have the same number of multisample samples. All images must also use the same fixed sample layout setting. ( when false).\n• If a layered image is attached to one attachment, then all attachments must be layered attachments. The attached layers do not have to have the same number of layers, nor do the layers have to come from the same kind of texture (a cubemap color texture can be paired with an array depth texture) ( when false).\n\nNotice that there is no restriction based on size. The effective size of the FBO is the intersection of all of the sizes of the bound images (ie: the smallest in each dimension).\n\nThese rules are all code-based. If you ever get any of these values from glCheckFramebufferStatus, it is because your program has done something wrong in setting up the FBO. Each one has a specific remedy for it.\n\nThere's one more rule that can trip you up:\n• The implementation likes your combination of attached image formats. ( when false).\n\nOpenGL allows implementations to state that they do not support some combination of image formats for the attached images; they do this by returning GL_FRAMEBUFFER_UNSUPPORTED when you attempt to use an unsupported format combination.\n\nHowever, the OpenGL specification also requires that implementations support certain format combinations; if you use these, implementations are forbidden to return GL_FRAMEBUFFER_UNSUPPORTED. Implementations must allow any combination of color formats, so long as all of those color formats come from the required set of color formats.\n\nThese color formats can be combined with a depth attachment with any of the required depth formats. Stencil attachments can also be used, again with the required stencil formats, as well as the combined depth/stencil formats. However, implementations are only required to support both a depth and stencil attachment simultaneously if both attachments refer to the same image.\n\nThis means that if you want stencil with no depth, you can use one of the required stencil formats. If you want depth with no stencil, you can use one of the required depth formats. But if you want depth and stencil, you must use a depth/stencil format and the same image in that texture must be attached to both the depth and stencil attachments.\n\nStaying within these limits means you won't see GL_FRAMEBUFFER_UNSUPPORTED. Going outside of these limits makes it entirely possible to get this incompleteness.\n\nIt is possible to bind a texture to an FBO, bind that same texture to a shader, and then try to render with it at the same time.\n\nIt is perfectly valid to bind one image from a texture to an FBO and then render with that texture, as long as you prevent yourself from sampling from that image. If you do try to read and write to the same image, you get undefined results. Meaning it may do what you want, the sampler may get old data, the sampler may get half old and half new data, or it may get garbage data. Any of these are possible outcomes.\n\nDo not do this. What you will get is undefined behavior.\n\nOpenGL OpenGL 4.5 or ARB_texture_barrier reduces the cases of feedback loops to just reading/writing from the same pixels, and even allows a limited ability to read/write the same pixels.\n\nA pseudo implementation will make framebuffers much easier to understand.\n\nWhere Attachment can be one of\n\nAttaching a texture or renderbuffer works like this\n\nSetting the draw buffers and read buffer settings works like this\n\nThe original form of FBOs was this extension. It lacked quite a bit of the above functionality, which later extensions granted. The biggest difference is that it has more hard-coded restrictions on framebuffer completeness. All of the images have to be the same size in the EXT spec, for example. Some of these limitations were hardware-based. So there may be hardware that supports EXT_FBO and not ARB_FBO, even thought they support things like EXT_FBO_blit and other parts of ARB_FBO.\n• Framebuffer Object Extension Examples Note that these examples use the (old) syntax for the extension, not the core functionality.\n• Category:Core API Ref Framebuffer Objects: Reference documentation for functions that deal with framebuffer objects and renderbuffer objects."
    },
    {
        "link": "https://khronos.org/opengl/wiki/Renderbuffer_Object",
        "document": "Renderbuffer Objects are OpenGL Objects that contain images. They are created and used specifically with Framebuffer Objects. They are optimized for use as render targets, while Textures may not be, and are the logical choice when you do not need to sample (i.e. in a post-pass shader) from the produced image. If you need to resample (such as when reading depth back in a second shader pass), use Textures instead. Renderbuffer objects also natively accommodate Multisampling (MSAA).\n\nRenderbuffer objects are standard OpenGL objects. So they have the usual glGenRenderbuffers/glDeleteRenderbuffers creation and destruction functions. They also have the usual glBindRenderbuffer function to bind them. It takes a target parameter, but the only viable target is GL_RENDERBUFFER.\n\nSimilar to Texture objects, renderbuffers are empty at initialization. Before you can bind them to a Framebuffer Object, you must allocate storage for the renderbuffer. To do this, simply call:\n\nThe target​ must be GL_RENDERBUFFER, the same target you bound the renderbuffer object to. The internalformat​ should be an internal format used for images. The article on Image Formats elaborates in detail on the meaning of these formats. The width​ and height​ are the width and height of the renderbuffer.\n\nIf you wish to create a multisample renderbuffer, you use a slightly different function:\n\nThis works exactly like the original except for the samples​ parameter. Indeed, it works exactly like the original if you pass 0 for samples​. The samples​ parameter is the number of samples in the buffer. It must be less than GL_MAX_SAMPLES.\n\nSimilar to glTexImage2D, calling this function on a renderbuffer that has already had this function called on it will cause it to deallocate any resources associated with the previous call and allocate new storage.\n\nYou may have noticed that, unlike glTexImage2D, there are no parameters in these creation functions to actually initialize the data. The contents of the renderbuffer are undefined (it could be holding old data, or zeroed out, or anything) There is also no function to upload data to the renderbuffer like glTexImage2D.\n\nNo, the only way to use a renderbuffer object is to attach it to a Framebuffer Object. After you bind that FBO to the context, and set up the draw or read buffers appropriately, you can use pixel transfer operations to read and write to it. Of course, you can also render to it. The standard glClear function and its friends like glClearBuffer will also clear the appropriate buffer.\n• Category:Core API Ref Framebuffer Objects: Reference documentation for functions that manage renderbuffers and framebuffer objects."
    },
    {
        "link": "https://stackoverflow.com/questions/2213030/whats-the-concept-of-and-differences-between-framebuffer-and-renderbuffer-in-op",
        "document": "The Framebuffer object is not actually a buffer, but an aggregator object that contains one or more attachments, which by their turn, are the actual buffers. You can understand the Framebuffer as C structure where every member is a pointer to a buffer. Without any attachment, a Framebuffer object has very low footprint.\n\nNow each buffer attached to a Framebuffer can be a Renderbuffer or a texture.\n\nThe Renderbuffer is an actual buffer (an array of bytes, or integers, or pixels). The Renderbuffer stores pixel values in native format, so it's optimized for offscreen rendering. In other words, drawing to a Renderbuffer can be much faster than drawing to a texture. The drawback is that pixels uses a native, implementation-dependent format, so that reading from a Renderbuffer is much harder than reading from a texture. Nevertheless, once a Renderbuffer has been painted, one can copy its content directly to screen (or to other Renderbuffer, I guess), very quickly using pixel transfer operations. This means that a Renderbuffer can be used to efficiently implement the double buffer pattern that you mentioned.\n\nRenderbuffers are a relatively new concept. Before them, a Framebuffer was used to render to a texture, which can be slower because a texture uses a standard format. It is still possible to render to a texture, and that's quite useful when one needs to perform multiple passes over each pixel to build a scene, or to draw a scene on a surface of another scene!\n\nThe OpenGL wiki has this page that shows more details and links."
    },
    {
        "link": "https://learnopengl.com/Advanced-OpenGL/Framebuffers",
        "document": "So far we've used several types of screen buffers: a color buffer for writing color values, a depth buffer to write and test depth information, and finally a stencil buffer that allows us to discard certain fragments based on some condition. The combination of these buffers is stored somewhere in GPU memory and is called a . OpenGL gives us the flexibility to define our own framebuffers and thus define our own color (and optionally a depth and stencil) buffer.\n\nThe rendering operations we've done so far were all done on top of the render buffers attached to the . The default framebuffer is created and configured when you create your window (GLFW does this for us). By creating our own framebuffer we can get an additional target to render to.\n\nThe application of framebuffers may not immediately make sense, but rendering your scene to a different framebuffer allows us to use that result to create mirrors in a scene, or do cool post-processing effects for example. First we'll discuss how they actually work and then we'll use them by implementing those cool post-processing effects.\n\nJust like any other object in OpenGL we can create a framebuffer object (abbreviated to FBO) by using a function called :\n\nThis pattern of object creation and usage is something we've seen dozens of times now so their usage functions are similar to all the other object's we've seen: first we create a framebuffer object, bind it as the active framebuffer, do some operations, and unbind the framebuffer. To bind the framebuffer we use :\n\nBy binding to the target all the next read and write framebuffer operations will affect the currently bound framebuffer. It is also possible to bind a framebuffer to a read or write target specifically by binding to or respectively. The framebuffer bound to is then used for all read operations like and the framebuffer bound to is used as the destination for rendering, clearing and other write operations. Most of the times you won't need to make this distinction though and you generally bind to both with .\n\nUnfortunately, we can't use our framebuffer yet because it is not . For a framebuffer to be complete the following requirements have to be satisfied:\n• We have to attach at least one buffer (color, depth or stencil buffer).\n• There should be at least one color attachment.\n• All attachments should be complete as well (reserved memory).\n• Each buffer should have the same number of samples.\n\nDon't worry if you don't know what samples are, we'll get to those in a later chapter.\n\nFrom the requirements it should be clear that we need to create some kind of attachment for the framebuffer and attach this attachment to the framebuffer. After we've completed all requirements we can check if we actually successfully completed the framebuffer by calling with . It then checks the currently bound framebuffer and returns any of these values found in the specification. If it returns we're good to go:\n\nAll subsequent rendering operations will now render to the attachments of the currently bound framebuffer. Since our framebuffer is not the default framebuffer, the rendering commands will have no impact on the visual output of your window. For this reason it is called when rendering to a different framebuffer. If you want all rendering operations to have a visual impact again on the main window we need to make the default framebuffer active by binding to :\n\nWhen we're done with all framebuffer operations, do not forget to delete the framebuffer object:\n\nNow before the completeness check is executed we need to attach one or more attachments to the framebuffer. An is a memory location that can act as a buffer for the framebuffer, think of it as an image. When creating an attachment we have two options to take: textures or objects.\n\nWhen attaching a texture to a framebuffer, all rendering commands will write to the texture as if it was a normal color/depth or stencil buffer. The advantage of using textures is that the render output is stored inside the texture image that we can then easily use in our shaders.\n\nCreating a texture for a framebuffer is roughly the same as creating a normal texture:\n\nThe main differences here is that we set the dimensions equal to the screen size (although this is not required) and we pass as the texture's parameter. For this texture, we're only allocating memory and not actually filling it. Filling the texture will happen as soon as we render to the framebuffer. Also note that we do not care about any of the wrapping methods or mipmapping since we won't be needing those in most cases.\n\nNow that we've created a texture, the last thing we need to do is actually attach it to the framebuffer:\n\nThe function has the following parameters:\n• : the framebuffer type we're targeting (draw, read or both).\n• : the type of attachment we're going to attach. Right now we're attaching a color attachment. Note that the at the end suggests we can attach more than 1 color attachment. We'll get to that in a later chapter.\n• : the type of the texture you want to attach.\n• : the mipmap level. We keep this at .\n\nNext to the color attachments we can also attach a depth and a stencil texture to the framebuffer object. To attach a depth attachment we specify the attachment type as . Note that the texture's and type should then become to reflect the depth buffer's storage format. To attach a stencil buffer you use as the second argument and specify the texture's formats as .\n\nIt is also possible to attach both a depth buffer and a stencil buffer as a single texture. Each 32 bit value of the texture then contains 24 bits of depth information and 8 bits of stencil information. To attach a depth and stencil buffer as one texture we use the type and configure the texture's formats to contain combined depth and stencil values. An example of attaching a depth and stencil buffer as one texture to the framebuffer is given below:\n\nwere introduced to OpenGL after textures as a possible type of framebuffer attachment, Just like a texture image, a renderbuffer object is an actual buffer e.g. an array of bytes, integers, pixels or whatever. However, a renderbuffer object can not be directly read from. This gives it the added advantage that OpenGL can do a few memory optimizations that can give it a performance edge over textures for off-screen rendering to a framebuffer.\n\nRenderbuffer objects store all the render data directly into their buffer without any conversions to texture-specific formats, making them faster as a writeable storage medium. You cannot read from them directly, but it is possible to read from them via the slow . This returns a specified area of pixels from the currently bound framebuffer, but not directly from the attachment itself.\n\nBecause their data is in a native format they are quite fast when writing data or copying data to other buffers. Operations like switching buffers are therefore quite fast when using renderbuffer objects. The function we've been using at the end of each frame may as well be implemented with renderbuffer objects: we simply write to a renderbuffer image, and swap to the other one at the end. Renderbuffer objects are perfect for these kind of operations.\n\nCreating a renderbuffer object looks similar to the framebuffer's code:\n\nAnd similarly we want to bind the renderbuffer object so all subsequent renderbuffer operations affect the current :\n\nSince renderbuffer objects are write-only they are often used as depth and stencil attachments, since most of the time we don't really need to read values from them, but we do care about depth and stencil testing. We need the depth and stencil values for testing, but don't need to sample these values so a renderbuffer object suits this perfectly. When we're not sampling from these buffers, a renderbuffer object is generally preferred.\n\nCreating a depth and stencil renderbuffer object is done by calling the function:\n\nCreating a renderbuffer object is similar to texture objects, the difference being that this object is specifically designed to be used as a framebuffer attachment, instead of a general purpose data buffer like a texture. Here we've chosen as the internal format, which holds both the depth and stencil buffer with 24 and 8 bits respectively.\n\nThe last thing left to do is to actually attach the renderbuffer object:\n\nRenderbuffer objects can be more efficient for use in your off-screen render projects, but it is important to realize when to use renderbuffer objects and when to use textures. The general rule is that if you never need to sample data from a specific buffer, it is wise to use a renderbuffer object for that specific buffer. If you need to sample data from a specific buffer like colors or depth values, you should use a texture attachment instead.\n\nNow that we know how framebuffers (sort of) work it's time to put them to good use. We're going to render the scene into a color texture attached to a framebuffer object we created and then draw this texture over a simple quad that spans the whole screen. The visual output is then exactly the same as without a framebuffer, but this time it's all printed on top of a single quad. Now why is this useful? In the next section we'll see why.\n\nFirst thing to do is to create an actual framebuffer object and bind it, this is all relatively straightforward:\n\nNext we create a texture image that we attach as a color attachment to the framebuffer. We set the texture's dimensions equal to the width and height of the window and keep its data uninitialized:\n\nWe also want to make sure OpenGL is able to do depth testing (and optionally stencil testing) so we have to make sure to add a depth (and stencil) attachment to the framebuffer. Since we'll only be sampling the color buffer and not the other buffers we can create a renderbuffer object for this purpose.\n\nCreating a renderbuffer object isn't too hard. The only thing we have to remember is that we're creating it as a depth and stencil attachment renderbuffer object. We set its internal format to which is enough precision for our purposes:\n\nOnce we've allocated enough memory for the renderbuffer object we can unbind the renderbuffer.\n\nThen, as a final step before we complete the framebuffer, we attach the renderbuffer object to the depth and stencil attachment of the framebuffer:\n\nThen we want to check if the framebuffer is complete and if it's not, we print an error message.\n\nBe sure to unbind the framebuffer to make sure we're not accidentally rendering to the wrong framebuffer.\n\nNow that the framebuffer is complete, all we need to do to render to the framebuffer's buffers instead of the default framebuffers is to simply bind the framebuffer object. All subsequent render commands will then influence the currently bound framebuffer. All the depth and stencil operations will also read from the currently bound framebuffer's depth and stencil attachments if they're available. If you were to omit a depth buffer for example, all depth testing operations will no longer work.\n\nSo, to draw the scene to a single texture we'll have to take the following steps:\n• Render the scene as usual with the new framebuffer bound as the active framebuffer.\n• Draw a quad that spans the entire screen with the new framebuffer's color buffer as its texture.\n\nWe'll render the same scene we've used in the depth testing chapter, but this time with the old-school container texture.\n\nTo render the quad we're going to create a fresh set of simple shaders. We're not going to include fancy matrix transformations since we'll be supplying the vertex coordinates as normalized device coordinates so we can directly forward them as output of the vertex shader. The vertex shader looks like this:\n\nNothing too fancy. The fragment shader is even more basic since the only thing we have to do is sample from a texture:\n\nIt is then up to you to create and configure a VAO for the screen quad. A single render iteration of the framebuffer procedure has the following structure:\n\nThere are a few things to note. First, since each framebuffer we're using has its own set of buffers, we want to clear each of those buffers with the appropriate bits set by calling . Second, when drawing the quad, we're disabling depth testing since we want to make sure the quad always renders in front of everything else; we'll have to enable depth testing again when we draw the normal scene though.\n\nThere are quite some steps that could go wrong here, so if you have no output, try to debug where possible and re-read the relevant sections of the chapter. If everything did work out successfully you'll get a visual result that looks like this:\n\nThe left shows the visual output, exactly the same as we've seen in the depth testing chapter, but this time rendered on a simple quad. If we render the scene in wireframe it's obvious we've only drawn a single quad in the default framebuffer.\n\nYou can find the source code of the application here.\n\nSo what was the use of this again? Well, because we can now freely access each of the pixels of the completely rendered scene as a single texture image, we can create some interesting effects in the fragment shader.\n\nNow that the entire scene is rendered to a single texture we can create cool effects by manipulating the scene texture. In this section we'll show you some of the more popular post-processing effects and how you may create your own with some added creativity.\n\nLet's start with one of the simplest post-processing effects.\n\nWe have access to each of the colors of the render output so it's not so hard to return the inverse of these colors in the fragment shader. We can take the color of the screen texture and inverse it by subtracting it from :\n\nWhile inversion is a relatively simple post-processing effect it already creates funky results:\n\nThe entire scene now has all its colors inversed with a single line of code in the fragment shader. Pretty cool huh?\n\nAnother interesting effect is to remove all colors from the scene except the white, gray and black colors; effectively grayscaling the entire image. An easy way to do this is by taking all the color components and averaging their results:\n\nThis already creates pretty good results, but the human eye tends to be more sensitive to green colors and the least to blue. So to get the most physically accurate results we'll need to use weighted channels:\n\nYou probably won't notice the difference right away, but with more complicated scenes, such a weighted grayscaling effect tends to be more realistic.\n\nAnother advantage about doing post-processing on a single texture image is that we can sample color values from other parts of the texture not specific to that fragment. We could for example take a small area around the current texture coordinate and sample multiple texture values around the current texture value. We can then create interesting effects by combining them in creative ways.\n\nA (or convolution matrix) is a small matrix-like array of values centered on the current pixel that multiplies surrounding pixel values by its kernel values and adds them all together to form a single value. We're adding a small offset to the texture coordinates in surrounding directions of the current pixel and combine the results based on the kernel. An example of a kernel is given below:\n\nThis kernel takes 8 surrounding pixel values and multiplies them by and the current pixel by . This example kernel multiplies the surrounding pixels by several weights determined in the kernel and balances the result by multiplying the current pixel by a large negative weight.\n\nKernels are an extremely useful tool for post-processing since they're quite easy to use and experiment with, and a lot of examples can be found online. We do have to slightly adapt the fragment shader a bit to actually support kernels. We make the assumption that each kernel we'll be using is a 3x3 kernel (which most kernels are):\n\nIn the fragment shader we first create an array of 9 offsets for each surrounding texture coordinate. The offset is a constant value that you could customize to your liking. Then we define the kernel, which in this case is a kernel that sharpens each color value by sampling all surrounding pixels in an interesting way. Lastly, we add each offset to the current texture coordinate when sampling and multiply these texture values with the weighted kernel values that we add together.\n\nThis particular sharpen kernel looks like this:\n\nThis could be the base of some interesting effects where your player may be on a narcotic adventure.\n\nA kernel that creates a effect is defined as follows:\n\nBecause all values add up to 16, directly returning the combined sampled colors would result in an extremely bright color so we have to divide each value of the kernel by . The resulting kernel array then becomes:\n\nBy only changing the kernel array in the fragment shader we can completely change the post-processing effect. It now looks something like this:\n\nSuch a blur effect creates interesting possibilities. We could vary the blur amount over time to create the effect of someone being drunk, or increase the blur whenever the main character is not wearing glasses. Blurring can also be a useful tool for smoothing color values which we'll see use of in later chapters.\n\nYou can see that once we have such a little kernel implementation in place it is quite easy to create cool post-processing effects. Let's show you a last popular effect to finish this discussion.\n\nBelow you can find an kernel that is similar to the sharpen kernel:\n\nThis kernel highlights all edges and darkens the rest, which is pretty useful when we only care about edges in an image.\n\nIt probably does not come as a surprise that kernels like this are used as image-manipulating tools/filters in tools like Photoshop. Because of a graphic card's ability to process fragments with extreme parallel capabilities, we can manipulate images on a per-pixel basis in real-time with relative ease. Image-editing tools therefore tend to use graphics cards for image-processing.\n• Can you use framebuffers to create a rear-view mirror? For this you'll have to draw your scene twice: one with the camera rotated 180 degrees and the other as normal. Try to create a small quad at the top of your screen to apply the mirror texture on, something like this; solution.\n• Play around with the kernel values and create your own interesting post-processing effects. Try searching the internet as well for other interesting kernels."
    },
    {
        "link": "https://stackoverflow.com/questions/21299342/difference-between-render-buffer-attachment-and-texture-attachment",
        "document": "I know is so late to answer, but I was searching the same doubt and I've seen your post. The difference between them is that a texture allow upload and read pixels from c++ program (CPU). The render buffers are always in the GPU."
    },
    {
        "link": "https://community.khronos.org/t/please-help-me-understand-the-concept-of-how-stencil-buffering-works-in-vulkan/7592",
        "document": "Unfortunately, the example(s) I have seen go too deep into the weeds, obscuring what I’m trying to understand. Not sure what is really needed or not. Here I’ll detail what I have come to understand which may or may not be correct.\n• I want to render a graphic that will punch a hole into the depth/stencil image so scissors is not what I want to do\n• I have the depth part working so I’m good there.\n• I created a separate pipeline for just depth buffer use and one for depth/stencil use.\n• I’m assuming that if I render with the depth/stencil pipeline, it will punch a hole into depth/stencil image.\n• Now that there’s a hole in the depth/stencil image, further renders within the same frame buffer will only be visible through the hole. This is what I have come to understand but I’m unable to get it working. Thanks.\n\nWhat I mean by “punch a hole into the depth/stencil image” is with a pipeline that has stencilTestEnable set to true, if you render with an image that is black with white spots, the white areas will allow other rendering to pass. I just had a thought, does the image I want to render to the stencil buffer need it’s aspect mask set to VK_IMAGE_ASPECT_STENCIL_BIT? Right now the image I’m using as a stencil mask is visible in the render even though stencilTestEnable = true in that pipeline. Not sure what I’m doing wrong. Been looking this demo over and over and I don’t see what I’m missing.\n\n https://github.com/SaschaWillems/Vulkan/blob/master/examples/stencilbuffer/stencilbuffer.cpp\n\nif you render with an image that is black with white spots, the white areas will allow other rendering to pass. Two things. First, I don’t understand what you mean by “allow other rendering to pass”. Do you want previous rendered stuff to be visible through these white areas? Do you want it to affect subsequently rendered objects? Second, the stencil buffer doesn’t care about colors. And since the fragment shader cannot manipulate the stencil buffer, you can’t do a thing where the FS detects “white” and causes something stencil-related to happen. The stencil test is about [i]math[/i], operations done based on the stencil value for the fragment (constant for all fragments in the rendering command) and the stencil value in the corresponding sample in the stencil buffer. And some depth stuff too. the image I want to render to the stencil buffer You cannot render “colors” to the stencil. And, as previously stated, the fragment shader cannot affect the fragment’s stencil value. The stencil buffer’s value can only be affected by the stencil reference value you set when you rendered those objects. It’s a single, unchanging value over the course of the rendering command.\n\nDo you want previous rendered stuff to be visible through these white areas? Do you want it to affect subsequently rendered objects? Yes. In OpenGL, I would disable the depth and color buffers, enable the stencil buffer, this would now render to the stencil buffer, re-enable the color and depth buffers and what I rendered would only show through what I did to the stencil buffers. I know it’s all math but I’m a visual thinker and communicator. For my needs, I only used the stencil buffer for simple masking. The stencil buffer’s value can only be affected by the stencil reference value you set when you rendered those objects. Can you tell me where in the example I sited it’s doing that? Do you have any example code?\n\nYes. In OpenGL, I would disable the depth and color buffers, enable the stencil buffer, this would now render to the stencil buffer, re-enable the color and depth buffers and what I rendered would only show through what I did to the stencil buffers. Then do that in Vulkan. It has all of the same switches as OpenGL does. Vulkan has write masks, just like OpenGL. Vulkan has the same stencil reference state, as well as stencil function state. By “disable/enable the color and depth buffers”, I assume you mean turning off write masks. Because changing the FBO itself is highly unnecessary for such a task. Can you tell me where in the example I sited it’s doing that? You said “if you render with an image that is black with white spots, the white areas will allow other rendering to pass.”. “White” and “black” are colors; ergo, you want to use colors to determine the value of the stencil. That’s not allowed. So either you haven’t explained very well what you’re trying to do, or what you’re trying to do isn’t possible, in either OpenGL or Vulkan.\n\nYou said “if you render with an image that is black with white spots, the white areas will allow other rendering to pass.” Here’s some of my OpenGL code using a stencil buffer to mask the items in a scroll box. Since a stencil is an 8 bit buffer, you can think of it as a grey scale image or perhaps something similar to an alpha channel. // Disable color and depth buffers glColorMask( GL_FALSE, GL_FALSE, GL_FALSE, GL_FALSE ); glDepthMask( GL_FALSE ); // Start using the stencil glEnable( GL_STENCIL_TEST ); glStencilFunc( GL_ALWAYS, 0x1, 0x1 ); glStencilOp( GL_REPLACE, GL_REPLACE, GL_REPLACE ); // Render to the stencil m_upStencilMaskSprite->render( matrix ); // Re-enable color and depth glColorMask( GL_TRUE, GL_TRUE, GL_TRUE, GL_TRUE ); glDepthMask( GL_TRUE ); // Where a 1 was not rendered glStencilFunc( GL_EQUAL, 0x1, 0x1 ); // Keep the pixel glStencilOp( GL_KEEP, GL_KEEP, GL_KEEP ); for( int i = m_visStartPos; i < m_visEndPos; ++i ) m_pScrollControlVec[i]->render( matrix ); // Finished using stencil glDisable( GL_STENCIL_TEST ); Vulkan has the same stencil reference state, as well as stencil function state. I assume you mean the VkStencilOpState. I understand that but the problem is I assume I need to write something to the stencil buffer. I just don’t know how that is done and I don’t see that happening in the example I sited. Working with the Vulkan stencil buffer is very different then what I did with OpenGL or DirectX. I need a simple, stencil buffer example because I’m just not getting it.\n\nWorking with the Vulkan stencil buffer is very different then what I did with OpenGL or DirectX. No, it isn’t. Vulkan stencil operations work in exactly the same way as it does in OpenGL or D3D. If you can understand what that OpenGL code you posted is doing, then you understand exactly how it works in Vulkan too. The only difference is where you put the stuff. All you need to do is map those things to stuff in Vulkan. You say “I need to write something to the stencil buffer.” Is that not exactly what “m_upStencilMaskSprite->render( matrix );” does? The state setting before that rendering call in OpenGL becomes state that is part of the pipeline object in Vulkan. The state set by the glStencil* and glEnable(GL_STENCIL_TEST) are in . The depth write mask is found in . And the color write masks are found in the array of in . There is pretty much a 1:1 mapping between OpenGL function calls and Vulkan pipeline structure state when it comes to stencil stuff. Yes, you have to have a separate pipeline object for the mask render compared to the render using the stencil mask. But that’s it."
    },
    {
        "link": "https://reddit.com/r/vulkan/comments/pajkfk/how_to_write_to_stencil_buffer_in_my_shader",
        "document": "I need a custom stencil buffer to not execute some pixel shader (via early stencil test).\n\nAnd the stencil buffer is based on the history frame(rgb image).\n\nSo I need a way to generate my stencil buffer.\n\nThere are two ways i found:\n• using VK_EXT_SHADER_STENCIL_EXPORT_EXTENSION_NAME extension and gl_FragStencilRef to write to the stencil buffer via pixel shader by drawing two triangles coverd the full screen. But it seems this extension is not widely supported (pixel shader failed to compile)\n• using my compute shader to write to the depth stencil buffer. But i don't know how to reach the buffer in my compute shader. I tried to use imageStore() function, but no value was written in.\n\nAnyway, I need a method to generate a custom stencil buffer. And i also expect it to run on the mobile devices;"
    },
    {
        "link": "https://learnopengl.com/Advanced-OpenGL/Stencil-testing",
        "document": "Once the fragment shader has processed the fragment a so called is executed that, just like the depth test, has the option to discard fragments. After that the remaining fragments are passed to the depth test where OpenGL could possibly discard even more fragments. The stencil test is based on the content of yet another buffer called the that we're allowed to update during rendering to achieve interesting effects.\n\nA stencil buffer (usually) contains bits per that amounts to a total of different stencil values per pixel. We can set these stencil values to values of our liking and we can discard or keep fragments whenever a particular fragment has a certain stencil value.\n\nA simple example of a stencil buffer is shown below (pixels not-to-scale):\n\nThe stencil buffer is first cleared with zeros and then an open rectangle of s is stored in the stencil buffer. The fragments of the scene are then only rendered (the others are discarded) wherever the stencil value of that fragment contains a .\n\nStencil buffer operations allow us to set the stencil buffer at specific values wherever we're rendering fragments. By changing the content of the stencil buffer while we're rendering, we're writing to the stencil buffer. In the same (or following) frame(s) we can read these values to discard or pass certain fragments. When using stencil buffers you can get as crazy as you like, but the general outline is usually as follows:\n• Render objects, updating the content of the stencil buffer.\n• Render (other) objects, this time discarding certain fragments based on the content of the stencil buffer.\n\nBy using the stencil buffer we can thus discard certain fragments based on the fragments of other drawn objects in the scene.\n\nYou can enable stencil testing by enabling . From that point on, all rendering calls will influence the stencil buffer in one way or another.\n\nNote that you also need to clear the stencil buffer each iteration just like the color and depth buffer:\n\nAlso, just like the depth testing's function, there is an equivalent function for the stencil buffer. The function allows us to set a bitmask that is ed with the stencil value about to be written to the buffer. By default this is set to a bitmask of all s not affecting the output, but if we were to set this to all the stencil values written to the buffer end up as s. This is equivalent to depth testing's :\n\nMost of the cases you'll only be using or as the stencil mask, but it's good to know there are options to set custom bit-masks.\n\nSimilar to depth testing, we have a certain amount of control over when a stencil test should pass or fail and how it should affect the stencil buffer. There are a total of two functions we can use to configure stencil testing: and .\n\nThe has three parameters:\n• : sets the stencil test function that determines whether a fragment passes or is discarded. This test function is applied to the stored stencil value and the 's value. Possible options are: , , , , , , and . The semantic meaning of these is similar to the depth buffer's functions.\n• : specifies the reference value for the stencil test. The stencil buffer's content is compared to this value.\n• : specifies a mask that is ed with both the reference value and the stored stencil value before the test compares them. Initially set to all s.\n\nSo in the case of the simple stencil example we've shown at the start, the function would be set to:\n\nThis tells OpenGL that whenever the stencil value of a fragment is equal ( ) to the reference value , the fragment passes the test and is drawn, otherwise discarded.\n\nBut only describes whether OpenGL should pass or discard fragments based on the stencil buffer's content, not how we can actually update the buffer. That is where comes in.\n\nThe contains three options of which we can specify for each option what action to take:\n• : action to take if the stencil test fails.\n• : action to take if the stencil test passes, but the depth test fails.\n• : action to take if both the stencil and the depth test pass.\n\nThen for each of the options you can take any of the following actions:\n\nBy default the function is set to so whatever the outcome of any of the tests, the stencil buffer keeps its values. The default behavior does not update the stencil buffer, so if you want to write to the stencil buffer you need to specify at least one different action for any of the options.\n\nSo using and we can precisely specify when and how we want to update the stencil buffer and when to pass or discard fragments based on its content.\n\nIt would be unlikely if you completely understood how stencil testing works from the previous sections alone so we're going to demonstrate a particular useful feature that can be implemented with stencil testing alone called .\n\nObject outlining does exactly what it says it does. For each object (or only one) we're creating a small colored border around the (combined) objects. This is a particular useful effect when you want to select units in a strategy game for example and need to show the user which of the units were selected. The routine for outlining your objects is as follows:\n• Set the stencil op to before drawing the (to be outlined) objects, updating the stencil buffer with s wherever the objects' fragments are rendered.\n• Scale each of the objects by a small amount.\n• Use a different fragment shader that outputs a single (border) color.\n• Draw the objects again, but only if their fragments' stencil values are not equal to .\n• Enable depth testing again and restore stencil func to .\n\nThis process sets the content of the stencil buffer to s for each of the object's fragments and when it's time to draw the borders, we draw scaled-up versions of the objects only where the stencil test passes. We're effectively discarding all the fragments of the scaled-up versions that are part of the original objects' fragments using the stencil buffer.\n\nSo we're first going to create a very basic fragment shader that outputs a border color. We simply set a hardcoded color value and call the shader :\n\nUsing the scene from the previous chapter we're going to add object outlining to the two containers, so we'll leave the floor out of it. We want to first draw the floor, then the two containers (while writing to the stencil buffer), and then draw the scaled-up containers (while discarding the fragments that write over the previously drawn container fragments).\n\nWe first need to enable stencil testing:\n\nAnd then in each frame we want to specify the action to take whenever any of the stencil tests succeed or fail:\n\nIf any of the tests fail we do nothing; we simply keep the currently stored value that is in the stencil buffer. If both the stencil test and the depth test succeed however, we want to replace the stored stencil value with the reference value set via which we later set to .\n\nWe clear the stencil buffer to s at the start of the frame and for the containers we update the stencil buffer to for each fragment drawn:\n\nBy using as the stencil op function we make sure that each of the containers' fragments update the stencil buffer with a stencil value of . Because the fragments always pass the stencil test, the stencil buffer is updated with the reference value wherever we've drawn them.\n\nNow that the stencil buffer is updated with s where the containers were drawn we're going to draw the upscaled containers, but this time with the appropriate test function and disabling writes to the stencil buffer:\n\nWe set the stencil function to to make sure that we're only drawing parts of the containers that are not equal to . This way we only draw the part of the containers that are outside the previously drawn containers. Note that we also disable depth testing so the scaled up containers (e.g. the borders) do not get overwritten by the floor. Make sure to enable the depth buffer again once you're done.\n\nThe total object outlining routine for our scene looks something like this:\n\nAs long as you understand the general idea behind stencil testing this shouldn't be too hard to understand. Otherwise try to carefully read the previous sections again and try to completely understand what each of the functions does now that you've seen an example of it can be used.\n\nThe result of the outlining algorithm then looks like this:\n\nCheck the source code here to see the complete code of the object outlining algorithm.\n\nThe object outlining algorithm you've seen is commonly used in games to visualize selected objects (think of strategy games) and an algorithm like this can easily be implemented within a model class. You could set a boolean flag within the model class to draw either with borders or without. If you want to be creative you could even give the borders a more natural look with the help of post-processing filters like Gaussian Blur.\n\nStencil testing has many more purposes (beside outlining objects) like drawing textures inside a rear-view mirror so it neatly fits into the mirror shape, or rendering real-time shadows with a stencil buffer technique called . Stencil buffers give us with yet another nice tool in our already extensive OpenGL toolkit."
    },
    {
        "link": "https://zeux.io/2020/02/27/writing-an-efficient-vulkan-renderer",
        "document": "In 2018, I wrote an article “Writing an efficient Vulkan renderer” for GPU Zen 2 book, which was published in 2019. In this article I tried to aggregate as much information about Vulkan performance as I could - instead of trying to focus on one particular aspect or application, this is trying to cover a wide range of topics, give readers an understanding of the behavior of different APIs on real hardware and provide a range of options for each problem that needs to be solved.\n\nAt the time of publishing this article, the Kindle edition of the book is available for $2.99 on Amazon - that’s cheaper than a cup of coffee and it’s definitely worth your time and money. It contains many great articles about rendering effects and design.\n\nThis, however, is the full, free of charge copy of the article - hopefully it will help graphics programmers to understand and use Vulkan to the full of its ability. The article has been lightly edited to mention Vulkan 1.1/1.2 promotions where applicable - fortunately, not much has changed in the last two years for Vulkan performance, so the content should still be mostly accurate.\n\nVulkan is a new explicit cross-platform graphics API. It introduces many new concepts that may be unfamiliar to even seasoned graphics programmers. The key goal of Vulkan is performance – however, attaining good performance requires in-depth knowledge about these concepts and how to apply them efficiently, as well as how particular driver implementations implement these. This article will explore topics such as memory allocation, descriptor set management, command buffer recording, pipeline barriers, render passes and discuss ways to optimize CPU and GPU performance of production desktop/mobile Vulkan renderers today as well as look at what a future looking Vulkan renderer could do differently.\n\nModern renderers are becoming increasingly complex and must support many different graphics APIs with varying levels of hardware abstraction and disjoint sets of concepts. This sometimes makes it challenging to support all platforms at the same level of efficiency. Fortunately, for most tasks Vulkan provides multiple options that can be as simple as reimplementing concepts from other APIs with higher efficiency due to targeting the code specifically towards the renderer needs, and as hard as redesigning large systems to make them optimal for Vulkan. We will try to cover both extremes when applicable – ultimately, this is a tradeoff between maximum efficiency on Vulkan-capable systems and implementation and maintenance costs that every engine needs to carefully pick. Additionally, efficiency is often application-dependent – the guidance in this article is generic and ultimately best performance is achieved by profiling the target application on a target platform and making an informed implementation decision based on the results.\n\nThis article assumes that the reader is familiar with the basics of Vulkan API, and would like to understand them better and/or learn how to use the API efficiently.\n\nMemory management remains an exceedingly complex topic, and in Vulkan it gets even more so due to the diversity of heap configurations on different hardware. Earlier APIs adopted a resource-centric concept – the programmer doesn’t have a concept of graphics memory, only that of a graphics resource, and different drivers are free to manage the resource memory based on API usage flags and a set of heuristics. Vulkan, however, forces to think about memory management up front, as you must manually allocate memory to create resources.\n\nA perfectly reasonable first step is to integrate (henceforth abbreviated as VMA), which is an open-source library developed by AMD that solves some memory management details for you by providing a general purpose resource allocator on top of Vulkan functions. Even if you do use that library, there are still multiple performance considerations that apply; the rest of this section will go over memory caveats without assuming you use VMA; all of the guidance applies equally to VMA.\n\nWhen creating a resource in Vulkan, you have to choose a heap to allocate memory from. Vulkan device exposes a set of memory types where each memory type has flags that define the behavior of that memory, and a heap index that defines the available size.\n\nMost Vulkan implementations expose two or three of the following flag combinations:\n• – this is generally referring to GPU memory that is not directly visible from CPU; it’s fastest to access from the GPU and this is the memory you should be using to store all render targets, GPU-only resources such as buffers for compute, and also all static resources such as textures and geometry buffers.\n• – on AMD hardware, this memory type refers to up to 256 MB of video memory that the CPU can write to directly, and is perfect for allocating reasonable amounts of data that is written by CPU every frame, such as uniform buffers or dynamic vertex/index buffers\n• – this is referring to CPU memory that is directly visible from GPU; reads from this memory go over PCI-express bus. In absence of the previous memory type, this generally speaking should be the choice for uniform buffers or dynamic vertex/index buffers, and also should be used to store staging buffers that are used to populate static resources allocated with with data.\n• – this is referring to GPU memory that might never need to be allocated for render targets on tiled architectures. It is recommended to use lazily allocated memory to save physical memory for large render targets that are never stored to, such as MSAA images or depth images. On integrated GPUs, there is no distinction between GPU and CPU memory – these devices generally expose that you can allocate all static resources through as well.\n\nWhen dealing with dynamic resources, in general allocating in non-device-local host-visible memory works well – it simplifies the application management and is efficient due to GPU-side caching of read-only data. For resources that have a high degree of random access though, like dynamic textures, it’s better to allocate them in and upload data using staging buffers allocated in memory – similarly to how you would handle static textures. In some cases you might need to do this for buffers as well – while uniform buffers typically don’t suffer from this, in some applications using large storage buffers with highly random access patterns will generate too many PCIe transactions unless you copy the buffers to GPU first; additionally, host memory does have higher access latency from the GPU side that can impact performance for many small draw calls.\n\nWhen allocating resources from , in case of VRAM oversubscription you can run out of memory; in this case you should fall back to allocating the resources in non-device-local memory. Naturally you should make sure that large frequently used resources such as render targets are allocated first. There are other things you can do in an event of an oversubscription, such as migrating resources from GPU memory to CPU memory for less frequently used resources – this is outside of the scope of this article; additionally, on some operating systems like Windows 10 correct handling of oversubscription requires APIs that are not currently available in Vulkan.\n\nUnlike some other APIs that allow an option to perform one memory allocation per resource, in Vulkan this is impractical for large applications – drivers are only required to support up to 4096 individual allocations. In addition to the total number being limited, allocations can be slow to perform, may waste memory due to assuming worst case possible alignment requirements, and also require extra overhead during command buffer submission to ensure memory residency. Because of this, suballocation is necessary. A typical pattern of working with Vulkan involves performing large (e.g. 16 MB – 256 MB depending on how dynamic the memory requirements are) allocations using , and performing suballocation of objects within this memory, effectively managing it yourself. Critically, the application needs to handle alignment of memory requests correctly, as well as limit that restricts valid configurations of buffers and images.\n\nBriefly, restricts the relative placement of buffer and image resources in the same allocation, requiring additional padding between individual allocations. There are several ways to handle this:\n• Always over-align image resources (as they typically have larger alignment to begin with) by bufferImageGranularity, essentially using a maximum of required alignment and for address and size alignment.\n• Track resource type for each allocation, and have the allocator add the requisite padding only if the previous or following resource is of a different type. This requires a somewhat more complex allocation algorithm.\n• Allocate images and buffers in separate Vulkan allocations, thus sidestepping the entire problem. This reduces internal fragmentation due to smaller alignment padding but can waste more memory if the backing allocations are too big (e.g. 256 MB).\n\nOn many GPUs the required alignment for image resources is substantially bigger than it is for buffers which makes the last option attractive – in addition to reducing waste due to lack of extra padding between buffers and images, it reduces internal fragmentation due to image alignment when an image follows a buffer resource. VMA provides implementations for option 2 (by default) and option 3 (see ).\n\nWhile the memory management model that Vulkan provides implies that the application performs large allocations and places many resources within one allocation using suballocation, on some GPUs it’s more efficient to allocate certain resources as one dedicated allocation. That way the driver can allocate the resources in faster memory under special circumstances.\n\nTo that end, Vulkan provides an extension (core in 1.1) to perform dedicated allocations – when allocating memory, you can specify that you are allocating this memory for this individual resource instead of as an opaque blob. To know if this is worthwhile, you can query the extended memory requires via or ; the resulting struct, , will contain (which might be set if the allocated resource needs to be shared with other processes) and flags.\n\nIn general, applications may see performance improvements from dedicated allocations on large render targets that require a lot of read/write bandwidth depending on the hardware and drivers.\n\nVulkan provides two options when mapping memory to get a CPU-visible pointer:\n• Do this before CPU needs to write data to the allocation, and unmap once the write is complete\n• Do this right after the host-visible memory is allocated, and never unmap memory\n\nThe second option is otherwise known as persistent mapping and is generally a better tradeoff – it minimizes the time it takes to obtain a writeable pointer ( is not particularly cheap on some drivers), removes the need to handle the case where multiple resources from the same memory object need to be written to simultaneously (calling on an allocation that’s already been mapped and not unmapped is not valid) and simplifies the code in general.\n\nThe only downside is that this technique makes the 256 MB chunk of VRAM that is host visible and device local on AMD GPU that was described in “Memory heap selection” less useful – on systems with Windows 7 and AMD GPU, using persistent mapping on this memory may force WDDM to migrate the allocations to system memory. If this combination is a critical performance target for your users, then mapping and unmapping memory when needed might be more appropriate.\n\nUnlike earlier APIs with a slot-based binding model, in Vulkan the application has more freedom in how to pass resources to shaders. Resources are grouped into descriptor sets that have an application-specified layout, and each shader can use several descriptor sets that can be bound individually. It’s the responsibility of the application to manage the descriptor sets to make sure that CPU doesn’t update a descriptor set that’s in use by the GPU, and to provide the descriptor layout that has an optimal balance between CPU-side update cost and GPU-side access cost. In addition, since different rendering APIs use different models for resource binding and none of them match Vulkan model exactly, using the API in an efficient and cross-platform way becomes a challenge. We will outline several possible approaches to working with Vulkan descriptor sets that strike different points on the scale of usability and performance.\n\nWhen working with Vulkan descriptor sets, it’s useful to have a mental model of how they might map to hardware. One such possibility – and the expected design – is that descriptor sets map to a chunk of GPU memory that contains descriptors – opaque blobs of data, 16-64 bytes in size depending on the resource, that completely specify all resource parameters necessary for shaders to access resource data. When dispatching shader work, CPU can specify a limited number of pointers to descriptor sets; these pointers become available to shaders as the shader threads launch.\n\nWith that in mind, Vulkan APIs can map more or less directly to this model – creating a descriptor set pool would allocate a chunk of GPU memory that’s large enough to contain the maximum specified number of descriptors. Allocating a set out of descriptor pool can be as simple as incrementing the pointer in the pool by the cumulative size of allocated descriptors as determined by (note that such an implementation would not support memory reclamation when freeing individual descriptors from the pool; would set the pointer back to the start of pool memory and make the entire pool available for allocation again). Finally, would emit command buffer commands that set GPU registers corresponding to descriptor set pointers.\n\nNote that this model ignores several complexities, such as dynamic buffer offsets, limited number of hardware resources for descriptor sets, etc. Additionally, this is just one possible implementation – some GPUs have a less generic descriptor model and require the driver to perform additional processing when descriptor sets are bound to the pipeline. However, it’s a useful model to plan for descriptor set allocation/usage.\n\nGiven the mental model above, you can treat descriptor sets as GPU-visible memory – it’s the responsibility of the application to group descriptor sets into pools and keep them around until GPU is done reading them.\n\nA scheme that works well is to use free lists of descriptor set pools; whenever you need a descriptor set pool, you allocate one from the free list and use it for subsequent descriptor set allocations in the current frame on the current thread. Once you run out of descriptor sets in the current pool, you allocate a new pool. Any pools that were used in a given frame need to be kept around; once the frame has finished rendering, as determined by the associated fence objects, the descriptor set pools can reset via and returned to free lists. While it’s possible to free individual descriptors from a pool via , this complicates the memory management on the driver side and is not recommended.\n\nWhen a descriptor set pool is created, application specifies the maximum number of descriptor sets allocated from it, as well as the maximum number of descriptors of each type that can be allocated from it. In Vulkan 1.1, the application doesn’t have to handle accounting for these limits – it can just call and handle the error from that call by switching to a new descriptor set pool. Unfortunately, in Vulkan 1.0 without any extensions, it’s an error to call if the pool does not have available space, so application must track the number of sets and descriptors of each type to know beforehand when to switch to a different pool.\n\nDifferent pipeline objects may use different numbers of descriptors, which raises the question of pool configuration. A straightforward approach is to create all pools with the same configuration that uses the worst-case number of descriptors for each type – for example, if each set can use at most 16 texture and 8 buffer descriptors, one can allocate all pools with maxSets=1024, and pool sizes 16*1024 for texture descriptors and 8*1024 for buffer descriptors. This approach can work but in practice it can result in very significant memory waste for shaders with different descriptor count – you can’t allocate more than 1024 descriptor sets out of a pool with the aforementioned configuration, so if most of your pipeline objects use 4 textures, you’ll be wasting 75% of texture descriptor memory.\n\nTwo alternatives that provide a better balance wrt memory use are:\n• Measure an average number of descriptors used in a shader pipeline per type for a characteristic scene and allocate pool sizes accordingly. For example, if in a given scene we need 3000 descriptor sets, 13400 texture descriptors, and 1700 buffer descriptors, then the average number of descriptors per set is 4.47 textures (rounded up to 5) and 0.57 buffers (rounded up to 1), so a reasonable configuration of a pool is maxSets=1024, 5*1024 texture descriptors, 1024 buffer descriptors. When a pool is out of descriptors of a given type, we allocate a new one – so this scheme is guaranteed to work and should be reasonably efficient on average.\n• Group shader pipeline objects into size classes, approximating common patterns of descriptor use, and pick descriptor set pools using the appropriate size class. This is an extension of the scheme described above to more than one size class. For example, it’s typical to have large numbers of shadow/depth prepass draw calls, and large numbers of regular draw calls in a scene – but these two groups have different numbers of required descriptors, with shadow draw calls typically requiring 0 to 1 textures per set and 0 to 1 buffers when dynamic buffer offsets are used. To optimize memory use, it’s more appropriate to allocate descriptor set pools separately for shadow/depth and other draw calls. Similarly to general-purpose allocators that can have size classes that are optimal for a given application, this can still be managed in a lower-level descriptor set management layer as long as it’s configured with application specific descriptor set usages beforehand.\n\nFor each resource type, Vulkan provides several options to access these in a shader; application is responsible for choosing an optimal descriptor type.\n\nFor buffers, application must choose between uniform and storage buffers, and whether to use dynamic offsets or not. Uniform buffers have a limit on the maximum addressable size – on desktop hardware, you get up to 64 KB of data, however on mobile hardware some GPUs only provide 16 KB of data (which is also the guaranteed minimum by the specification). The buffer resource can be larger than that, but shader can only access this much data through one descriptor.\n\nOn some hardware, there is no difference in access speed between uniform and storage buffers, however for other hardware depending on the access pattern uniform buffers can be significantly faster. Prefer uniform buffers for small to medium sized data especially if the access pattern is fixed (e.g. for a buffer with material or scene constants). Storage buffers are more appropriate when you need large arrays of data that need to be larger than the uniform buffer limit and are indexed dynamically in the shader.\n\nFor textures, if filtering is required, there is a choice of combined image/sampler descriptor (where, like in OpenGL, descriptor specifies both the source of the texture data, and the filtering/addressing properties), separate image and sampler descriptors (which maps better to Direct3D 11 model), and image descriptor with an immutable sampler descriptor, where the sampler properties must be specified when pipeline object is created.\n\nThe relative performance of these methods is highly dependent on the usage pattern; however, in general immutable descriptors map better to the recommended usage model in other newer APIs like Direct3D 12, and give driver more freedom to optimize the shader. This does alter renderer design to a certain extent, making it necessary to implement certain dynamic portions of the sampler state, like per-texture LOD bias for texture fade-in during streaming, using shader ALU instructions.\n\nA simplistic alternative to Vulkan binding model is Metal/Direct3D11 model where an application can bind resources to slots, and the runtime/driver manage descriptor memory and descriptor set parameters. This model can be implemented on top of Vulkan descriptor sets; while not providing the most optimal results, it generally is a good model to start with when porting an existing renderer, and with careful implementation it can be surprisingly efficient.\n\nTo make this model work, application needs to decide how many resource namespaces are there and how they map to Vulkan set/slot indices. For example, in Metal each stage (VS, FS, CS) has three resource namespaces – textures, buffers, samplers – with no differentiation between e.g. uniform buffers and storage buffers. In Direct3D 11 the namespaces are more complicated since read-only structured buffers belong to the same namespace as textures, but textures and buffers used with unordered access reside in a separate one.\n\nVulkan specification only guarantees a minimum of 4 descriptor sets accessible to the entire pipeline (across all stages); because of this, the most convenient mapping option is to have resource bindings match across all stages – for example, a texture slot 3 would contain the same texture resource no matter what stage it’s accessed from – and use different descriptor sets for different types, e.g. set 0 for buffers, set 1 for textures, set 2 for samplers. Alternatively, an application can use one descriptor set per stage and perform static index remapping (e.g. slots 0-16 would be used for textures, slots 17-24 for uniform buffers, etc.) – this, however, can use much more descriptor set memory and isn’t recommended. Finally, one could implement optimally compact dynamic slot remapping for each shader stage (e.g. if a vertex shader uses texture slots 0, 4, 5, then they map to Vulkan descriptor indices 0, 1, 2 in set 0, and at runtime application extracts the relevant texture information using this remapping table.\n\nIn all these cases, the implementation of setting a texture to a given slot wouldn’t generally run any Vulkan commands and would just update shadow state; just before the draw call or dispatch you’d need to allocate a descriptor set from the appropriate pool, update it with new descriptors, and bind all descriptor sets using . Note that if a descriptor set has 5 resources, and only one of them changed since the last draw call, you still need to allocate a new descriptor set with 5 resources and update all of them.\n\nTo reach good performance with this approach, you need to follow several guidelines:\n• Don’t allocate or update descriptor sets if nothing in the set changed. In the model with slots that are shared between different stages, this can mean that if no textures are set between two draw calls, you don’t need to allocate/update the descriptor set with texture descriptors.\n• Batch calls to if possible – on some drivers, each call has measurable overhead, so if you need to update multiple sets, allocating both in one call can be faster\n• To update descriptor sets, either use with descriptor write array, or use from Vulkan 1.1. Using the descriptor copy functionality of is tempting with dynamic descriptor management for copying most descriptors out of a previously allocated array, but this can be slow on drivers that allocate descriptors out of write-combined memory. Descriptor templates can reduce the amount of work application needs to do to perform updates – since in this scheme you need to read descriptor information out of shadow state maintained by application, descriptor templates allow you to tell the driver the layout of your shadow state, making updates substantially faster on some drivers.\n• Finally, prefer dynamic uniform buffers to updating uniform buffer descriptors. Dynamic uniform buffers allow to specify offsets into buffer objects using argument of without allocating and updating new descriptors. This works well with dynamic constant management where constants for draw calls are allocated out of large uniform buffers, substantially reduce CPU overhead, and can be more efficient on GPU. While on some GPUs the number of dynamic buffers must be kept small to avoid extra overhead in the driver, one or two dynamic uniform buffers should work well in this scheme on all architectures.\n\nIn general, the approach outlined above can be very efficient in terms of performance – it’s not as efficient as approaches with more static descriptor sets that are described below, but it can still run circles around older APIs if implemented carefully. On some drivers, unfortunately the allocate & update path is not very optimal – on some mobile hardware, it may make sense to cache descriptor sets based on the descriptors they contain if they can be reused later in the frame.\n\nWhile slot-based resource binding model is simple and familiar, it doesn’t result in optimal performance. Some mobile hardware may not support multiple descriptor sets; however, in general Vulkan API and driver expect an application to manage descriptor sets based on frequency of change.\n\nA more Vulkan centric renderer would organize data that the shaders need to access into groups by frequency of change, and use individual sets for individual frequencies, with set=0 representing least frequent change, and set=3 representing most frequent. For example, a typical setup would involve:\n• Set=0 descriptor set containing uniform buffer with global, per-frame or per-view data, as well as globally available textures such as shadow map texture array/atlas\n• Set=1 descriptor set containing uniform buffer and texture descriptors for per-material data, such as albedo map, Fresnel coefficients, etc.\n• Set=2 descriptor set containing dynamic uniform buffer with per-draw data, such as world transform array\n\nFor set=0, the expectation is that it only changes a handful of times per frame; it’s sufficient to use a dynamic allocation scheme similar to the previous section.\n\nFor set=1, the expectation is that for most objects, the material data persists between frames, and as such could be allocated and updated only when the gameplay code changes material data.\n\nFor set=2, the data would be completely dynamic; due to the use of a dynamic uniform buffer, we’d rarely need to allocate and update this descriptor set – assuming dynamic constants are uploaded to a series of large per-frame buffers, for most draws we’d need to update the buffer with the constant data, and call with new offsets.\n\nNote that due to compatibility rules between pipeline objects, in most cases it’s enough to bind sets 1 and 2 whenever a material changes, and only set 2 when material is the same as that for the previous draw call. This results in just one call to per draw call.\n\nFor a complex renderer, different shaders might need to use different layouts – for example, not all shaders need to agree on the same layout for material data. In rare cases it might also make sense to use more than 3 sets depending on the frame structure. Additionally, given the flexibility of Vulkan it’s not strictly required to use the same resource binding system for all draw calls in the scene. For example, post-processing draw call chains tend to be highly dynamic, with texture/constant data changing completely between individual draw calls. Some renderers initially implement the dynamic slot-based binding model from the previous section and proceed to additionally implement the frequency-based sets for world rendering to minimize the performance penalty for set management, while still keeping the simplicity of slot-based model for more dynamic parts of the rendering pipeline.\n\nThe scheme described above assumes that in most cases, per-draw data is larger than the size that can be efficiently set via push constants. Push constants can be set without updating or rebinding descriptor sets; with a guaranteed limit of 128 bytes per draw call, it’s tempting to use them for per-draw data such as a 4x3 transform matrix for an object. However, on some architectures the actual number of constants available to push quickly depends on the descriptor setup the shaders use, and is closer to 12 bytes or so. Exceeding this limit can force the driver to spill the push constants into driver-managed ring buffer, which can end up being more expensive than moving this data to a dynamic uniform buffer on the application side. While limited use of push constants may still be a good idea for some designs, it’s more appropriate to use them in a fully bindless scheme described in the next section.\n\nFrequency-based descriptor sets reduce the descriptor set binding overhead; however, you still need to bind one or two descriptor sets per draw call. Maintaining material descriptor sets requires a management layer that needs to update GPU-visible descriptor sets whenever material parameters change; additionally, since texture descriptors are cached in material data, this makes global texture streaming systems hard to deal with – whenever some mipmap levels in a texture get streamed in or out, all materials that refer to this texture need to be updated. This requires complex interaction between material system and texture streaming system and introduces extra overhead whenever a texture is adjusted – which partially offsets the benefits of the frequency-based scheme. Finally, due to the need to set up descriptor sets per draw call it’s hard to adapt any of the aforementioned schemes to GPU-based culling or command submission.\n\nIt is possible to design a bindless scheme where the number of required set binding calls is constant for the world rendering, which decouples texture descriptors from materials, making texture streaming systems easier to implement, and facilitates GPU-based submission. As with the previous scheme, this can be combined with dynamic ad-hoc descriptor updates for parts of the scene where the number of draw calls is small, and flexibility is important, such as post-processing.\n\nTo fully leverage bindless, core Vulkan may or may not be sufficient; some bindless implementations require updating descriptor sets without rebinding them after the update, which is not available in core Vulkan 1.0 or 1.1 but is possible to achieve with extension (core in Vulkan 1.2). However, basic design described below can work without extensions, given high enough descriptor set limits. This requires double buffering for the texture descriptor array described below to update individual descriptors since the array would be constantly accessed by GPU.\n\nSimilarly to the frequency-based design, we’ll split the shader data into global uniforms and textures (set 0), material data and per-draw data. Global uniforms and textures can be specified via a descriptor set the same way as described the previous section.\n\nFor per-material data, we will move the texture descriptors into a large texture descriptor array (note: this is a different concept than a texture array – texture array uses one descriptor and forces all textures to have the same size and format; descriptor array doesn’t have this limitation and can contain arbitrary texture descriptors as array elements, including texture array descriptors). Each material in the material data will have an index into this array instead of texture descriptor; the index will be part of the material data, which will also have other material constants.\n\nAll material constants for all materials in the scene will reside in one large storage buffer; while it’s possible to support multiple material types with this scheme, for simplicity we’ll assume that all materials can be specified using the same data. An example of material data structure is below:\n\nSimilarly, all per-draw constants for all objects in the scene can reside in another large storage buffer; for simplicity, we’ll assume that all per-draw constants have identical structure. To support skinned objects in a scheme like this, we’ll extract transform data into a separate, third storage buffer:\n\nSomething that we’ve ignored so far is the vertex data specification. While Vulkan provides a first-class way to specify vertex data by calling , having to bind vertex buffers per-draw would not work for a fully bindless design. Additionally, some hardware doesn’t support vertex buffers as a first-class entity, and the driver has to emulate vertex buffer binding, which causes some CPU-side slowdowns when using . In a fully bindless design, we need to assume that all vertex buffers are suballocated in one large buffer and either use per-draw vertex offsets ( argument to ) to have hardware fetch data from it, or pass an offset in this buffer to the shader with each draw call and fetch data from the buffer in the shader. Both approaches can work well, and might be more or less efficient depending on the GPU; here we will assume that the vertex shader will perform manual vertex fetching.\n\nThus, for each draw call we need to specify three integers to the shader:\n• Material index; used to look up material data from material storage buffer. The textures can then be accessed using the indices from the material data and the descriptor array.\n• Transform data index; used to look up transform data from transform storage buffer\n• Vertex data offset; used to look up vertex attributes from vertex storage buffer\n\nWe can specify these indices and additional data, if necessary, via draw data:\n\nThe shader will need to access storage buffers containing , , as well as a storage buffer containing vertex data. These can be bound the shader via the global descriptor set; the only remaining piece of information is the draw data index, that can be passed via a push constant.\n\nWith this scheme, we’d need to update the storage buffers used by materials and draw calls each frame and bind them once using our global descriptor set; additionally, we need to bind index data – assuming that, like vertex data, index data is allocated in one large index buffer, we only need to bind it once using . With the global setup complete, for each draw call we need to call if the shader changes, followed by to specify an index into the draw data buffer, followed by .\n\nIn a GPU-centric design, we can use or (provided by extension, promoted to core Vulkan 1.2) and fetch per-draw constants using (provided by extension) as an index instead of push constants. The only caveat is that for GPU-based submission, we’d need to bucket draw calls based on pipeline object on CPU since there’s no support for switching pipeline objects otherwise.\n\nWith this, vertex shader code to transform the vertex could look like this:\n\nFragment shader code to sample material textures could look like this:\n\nThis scheme minimizes the CPU-side overhead. Of course, fundamentally it’s a balance between multiple factors:\n• While the scheme can be extended to multiple formats of material, draw and vertex data, it gets harder to manage\n• Using storage buffers exclusively instead of uniform buffers can increase GPU time on some architectures\n• Fetching texture descriptors from an array indexed by material data indexed by material index can add an extra indirection on GPU compared to some alternative designs\n• On some hardware, various descriptor set limits may make this technique impractical to implement; to be able to index an arbitrary texture dynamically from the shader, should be large enough to accomodate all material textures - while many desktop drivers expose a large limit here, the specification only guarantees a limit of 16, so bindless remains out of reach on some hardware that otherwise supports Vulkan\n\nAs the renderers get more and more complex, bindless designs will become more involved and eventually allow moving even larger parts of rendering pipeline to GPU; due to hardware constraints this design is not practical on every single Vulkan-compatible device, but it’s definitely worth considering when designing new rendering paths for future hardware.\n\nIn older APIs, there is a single timeline for GPU commands; commands executed on CPU execute on the GPU in the same order, as there is generally only one thread recording them; there is no precise control over when CPU submits commands to GPU, and the driver is expected to manage memory used by the command stream as well as submission points optimally.\n\nIn contrast, in Vulkan the application is responsible for managing command buffer memory, recording commands in multiple threads into multiple command buffers, and submitting them for execution with appropriate granularity. While with carefully written code a single-core Vulkan renderer can be significantly faster than older APIs, the peak efficiency and minimal latency is obtained by utilizing many cores in the system for command recording, which requires careful memory management.\n\nSimilarly to descriptor sets, command buffers are allocated out of command pools; it’s valuable to understand how a driver might implement this to be able to reason about the costs and usage implications.\n\nCommand pool has to manage memory that will be filled with commands by CPU and subsequently read by GPU command processor. The amount of memory used by the commands can’t be statically determined; a typical implementation of a pool would involve thus a free list of fixed-size pages. Command buffer would contain a list of pages with actual commands, with special jump commands that transfer control from each page to the next one so that GPU can execute all of them in sequence. Whenever a command needs to be allocated from a command buffer, it will be encoded into the current page; if the current page doesn’t have space, the driver would allocate the next page using a free list from the associated pool, encode a jump to that page into the current page and switch to the next page for subsequent command recording.\n\nEach command pool can only be used from one thread concurrently, so the operations above don’t need to be thread-safe. Freeing the command buffer using may return the pages used by the command buffer into the pool by adding them to the free list. Resetting the command pool may put all pages used by all command buffers into the pool free list; when is used, the pages can be returned to the system so that other pools can reuse them.\n\nNote that there is no guarantee that actually returns memory to the pool; alternative designs may involve multiple command buffers allocating chunks within larger pages, which would make it hard for to recycle memory. Indeed, on one mobile vendor, is necessary to reuse memory for future command recording in a default setup when pools are allocated without .\n\nTwo crucial restrictions in Vulkan for command pool usage are:\n• Command buffers allocated from one pool may not be recorded concurrently by multiple threads\n• Command buffers and pools can not be freed or reset while GPU is still executing the associated commands\n\nBecause of these, a typical threading setup requires a set of command buffer pools. The set has to contain F*T pools, where F is the frame queue length – F is usually 2 (one frame is recorded by the CPU while another frame is being executed by the GPU) or 3; T is the number of threads that can concurrently record commands, which can be as high as the core count on the system. When recording commands from a thread, the thread needs to allocate a command buffer using the pool associated with the current frame & thread and record commands into it. Assuming that command buffers aren’t recorded across a frame boundary, and that at a frame boundary the frame queue length is enforced by waiting for the last frame in the queue to finish executing, we can then free all command buffers allocated for that frame and reset all associated command pools.\n\nAdditionally, instead of freeing command buffers, it’s possible to reuse them after calling - which would mean that command buffers don’t have to be allocated again. While in theory allocating command buffers could be cheap, some driver implementations have a measurable overhead associated with command buffer allocation. This also makes sure that the driver doesn’t ever need to return command memory to the system which can make submitting commands into these buffers cheaper.\n\nNote that depending on the frame structure, the setup above may result in unbalanced memory consumption across threads; for example, shadow draw calls typically require less setup and less command memory. When combined with effectively random workload distribution across threads that many job schedulers produce, this can result in all command pools getting sized for the worst-case consumption. If an application is memory constrained and this becomes a problem, it’s possible to limit the parallelism for each individual pass and select the command buffer/pool based on the recorded pass to limit the waste.\n\nThis requires introducing the concept of size classes to the command buffer manager. With a command pool per thread and a manual reuse of allocated command buffers as suggested above, it’s possible to keep a free list per size class, with size classes defined based on the number of draw calls (e.g. “<100”, “100-400”, etc.) and/or the complexity of individual draw calls (depth-only, gbuffer). Picking the buffer based on the expected usage leads to a more stable memory consumption. Additionally, for passes that are too small it is worthwhile to reduce the parallelism when recording these - for example, if a pass has <100 draw calls, instead of splitting it into 4 recording jobs on a 4-core system, it can be more efficient to record it in one job since that can reduce the overhead of command memory management and command buffer submission.\n\nWhile it’s important to record multiple command buffers on multiple threads for efficiency, since state isn’t reused across command buffers and there are other scheduling limitations, command buffers need to be reasonably large to make sure GPU is not idle during command processing. Additionally, each submission has some overhead both on the CPU side and on the GPU side. In general a Vulkan application should target <10 submits per frame (with each submit accounting for 0.5ms or more of GPU workload), and <100 command buffers per frame (with each command buffer accounting for 0.1ms or more of GPU workload). This might require adjusting the concurrency limits for command recording for individual passes, e.g. if a shadow pass for a specific light has <100 draw calls, it might be necessary to limit the concurrency on the recording for this pass to just one thread; additionally, for even shorter passes combining them with neighboring passes into one command buffer becomes beneficial. Finally, the fewer submissions a frame has the better – this needs to be balanced with submitting enough GPU work earlier in the frame to increase CPU and GPU parallelism though, for example it might make sense to submit all command buffers for shadow rendering before recording commands for other parts of the frame.\n\nCrucially, the number of submissions refers to the total number of structured submitted in all calls in a frame, not to the number of calls per se. For example, when submitting 10 command buffers, it’s much more efficient to use one that submits 10 command buffers compared to 10 structures with one command buffer per each, even if in both cases only one call is performed. Essentially, is a unit of synchronization/scheduling on GPU since it has its own set of fences/semaphores.\n\nWhen one of the render passes in the application contains a lot of draw calls, such as the gbuffer pass, for CPU submission efficiency it’s important to split the draw calls into multiple groups and record them on multiple threads. There are two ways to do this:\n• Record primary command buffers that render chunks of draw calls into the same framebuffer, using and ; execute the resulting command buffers using (batching submits for efficiency)\n• Record secondary command buffers that render chunks of draw calls, passing the render pass to along with ; use with in the primary command buffer, followed by to execute all recorded secondary command buffers\n\nWhile on immediate mode GPUs the first approach can be viable, and it can be a bit easier to manage wrt synchronization points on the CPU, it’s vital to use the second approach on GPUs that use tiled rendering instead. Using the first approach on tilers would require that the contents of the tiles is flushed to memory and loaded back from memory between each command buffer, which is catastrophic for performance.\n\nWith the guidance on the command buffer submission above, in most cases submitting a single command buffer multiple times after recording becomes impractical. In general approaches that pre-record command buffers for parts of the scene are counter-productive since they can result in excessive GPU load due to inefficient culling required to keep command buffer workload large and can trigger inefficient code paths on some tiled renderers, and instead applications should focus on improving the threading and draw call submission cost on the CPU. As such, applications should use to make sure the driver has freedom to generate commands that don’t need to be replayed more than once.\n\nThere are occasional exceptions for this rule. For example, for VR rendering, an application might want to record the command buffer for the combined frustum between left and right eye once. If the per-eye data is read out of a single uniform buffer, this buffer can then be updated between the command buffers using , followed by if secondary command buffers are used, or . Having said that, for VR it might be worthwhile to explore extension if available (core in Vulkan 1.1), since it should allow the driver to perform a similar optimization.\n\nPipeline barriers remain one of the most challenging parts of Vulkan code. In older APIs, the runtime and driver were responsible for making sure appropriate hardware-specific synchronization was performed in case of hazards such as fragment shader reading from the texture that was previously rendered to. This required meticulous tracking of every single resource binding and resulted in an unfortunate mix of excessive CPU overhead to perform a sometimes excessive amount of GPU synchronization (for example, Direct3D 11 driver typically inserts a barrier between any two consecutive compute dispatches that use the same UAV, even though depending on the application logic the hazards may be absent). Because inserting barriers quickly and optimally can require knowledge about the application’s use of resources, Vulkan requires the application to do this.\n\nFor optimal rendering, the pipeline barrier setup must be perfect. A missing barrier risks the application encountering a timing-dependent bug on an untested – or, worse, not-yet-existing – architecture, that in the worst case could cause a GPU crash. An unnecessary barrier can reduce the GPU utilization by reducing potential opportunity for parallel execution – or, worse, trigger very expensive decompression operations or the like. To make matters worse, while the cost of excessive barriers can be now visualized by tools like Radeon Graphics Profiler, missing barriers are generally not detected by validation tools.\n\nBecause of this, it’s vital to understand the behavior or barriers, the consequences of overspecifying them as well as how to work with them.\n\nThe specification describes barriers in terms of execution dependencies and memory visibility between pipeline stages (e.g. a resource was previously written to by a compute shader stage, and will be read by the transfer stage), as well as layout changes for images (e.g. a resource was previously in the format that is optimal to write via the color attachment output and should be transitioned to a format that is optimal to read from the shader). However, it might be easier to think about barriers in terms of their consequences – as in, what can happen on a GPU when a barrier is used. Note that the GPU behavior is of course dependent on the specific vendor and architecture, but it helps to map barriers that are specified in an abstract fashion to more concrete constructs to understand their performance implications.\n\nA barrier can cause three different things to happen:\n• Stalling execution of a specific stage until another stage is drained of all current work. For example, if a render pass renders data to a texture, and a subsequent render pass uses a vertex shader to read from this shader, GPU must wait for all pending fragment shader and ROP work to complete before launching shader threads for the vertex work in a subsequent pass. Most barrier operations will lead to execution stalling for some stages.\n• Flushing or invalidating an internal GPU-side cache and waiting for the memory transactions to finish to make sure another stage can read the resulting work. For example, on some architectures ROP writes might go through the L2 texture cache, but transfer stage might operate directly on memory. If a texture has been rendered to in a render pass, then the following transfer operation might read stale data unless the cache is flushed before the copy. Similarly, if a texture stage needs to read an image that was copied using transfer stage, L2 texture cache may need to get invalidated to make sure it doesn’t contain stale data. Not all barrier operations will need to do this.\n• Converting the format the resource is stored in, most commonly to decompress the resource storage. For example, MSAA textures on some architectures are stored in a compressed form where each pixel has a sample mask indicating how many unique colors this pixel contains, and a separate storage for sample data. Transfer stage or shader stage might be unable to read directly from a compressed texture, so a barrier that transitions from to or might need to decompress the texture, writing all samples for all pixels to memory. Most barrier operations won’t need to do this, but the ones that do can be incredibly expensive.\n\nWith this in mind, let’s try to understand the guidance for using barriers.\n\nWhen generating commands for each individual barrier, the driver only has a local view of the barrier and is unaware of past or future barriers. Because of this, the first important rule is that barriers need to be batched as aggressively as possible. Given a barrier that implies a wait-for-idle for fragment stage and an L2 texture cache flush, the driver will dutifully generate that every time you call . If you specify multiple resources in a single call, the driver will only generate one L2 texture cache flush command if it’s necessary for any transitions, reducing the cost.\n\nTo make sure the cost of the barriers isn’t higher than it needs to be, only relevant stages need to be included. For example, one of the most common barrier types is one that transitions a resource from to . When specifying this barrier, you should specify the shader stages that will actually read this resource via . It’s tempting to specify the stage mask as to support compute shader or vertex shader reads. Doing so, however, would mean that vertex shader workload from the subsequent draw commands can not start, which is problematic:\n• On immediate mode renderers, this slightly reduces the parallelism between draw calls, requiring all fragment threads to finish before vertex threads can start, which leads to GPU utilization dropping to 0 at the end of the pass and gradually rising from 0 to, hopefully, 100% as the next render pass begins;\n• On tiled mode renderers, for some designs the expectation is that all vertex work from the subsequent pass executes to completion before fragment work can start; waiting for fragment work to end for any vertex work to begin thus completely eliminates the parallelism between vertex and fragment stages and is one of the largest potential performance problems that a naively ported Vulkan title can encounter.\n\nNote that even if the barriers are specified correctly – in this case, assuming the texture is read from the fragment stage, should be – the execution dependency is still present, and it can still lead to reduced GPU utilization. This can come up in multiple situations including compute, where to read data from a compute shader generated by another compute shader you need to express an execution dependency between CS and CS but specifying a pipeline barrier is guaranteed to drain the GPU of compute work entirely, followed by slowly filling it with compute work again. Instead, it can be worthwhile to specify the dependency via what’s called a split barrier: instead of using , use after the write operation completes, and before the read operations starts. Of course, using immediately after is counter-productive and can be slower than ; instead you should try to restructure your algorithm to make sure there’s enough work submitted between Set and Wait, so that by the time GPU needs to process Wait, the event is most likely already signaled and there is no efficiency loss.\n\nAlternatively, in some cases the algorithm can be restructured to reduce the number of synchronization points while still using pipeline barriers, making the overhead less significant. For example, a GPU-based particle simulation might need to run two compute dispatches for each particle effect: one to emit new particles, and another one to simulate particles. These dispatches require a pipeline barrier between them to synchronize execution, which requires a pipeline barrier per particle system if particle systems are simulated sequentially. A more optimal implementation would first submit all dispatches to emit particles (that would not depend on each other), then submit a barrier to synchronize emission and simulation dispatches, then submit all dispatches to simulate particles - which would keep GPU well utilized for longer. From there on using split barriers could help completely hide the synchronization cost.\n\nAs far as resource decompression goes, it’s hard to give a general advice – on some architectures this never happens, and on some it does but depending on the algorithm it might not be avoidable. Using vendor specific tools such as Radeon Graphics Profiler is critical to understanding the performance impact decompression has on your frame; in some cases, it may be possible to adjust the algorithm to not require the decompression in the first place, for example by moving the work to a different stage. Of course it should be noted that resource decompression may happen in cases where it’s completely unnecessary and is a result of overspecifying barriers – for example, if you render to a framebuffer that contains a depth buffer and never read depth contents in the future, you should leave the depth buffer in layout instead of needlessly transitioning it into which might trigger a decompression (remember, the driver doesn’t know if you are going to read the resource in the future!).\n\nWith all the complexity involved in specifying barriers, it helps to have examples of commonly required barriers. Fortunately, Khronos Group provides many examples of valid and optimal barriers for various types of synchronization as part of Vulkan-Docs repository on GitHub. These can serve to improve the understanding of general barrier behavior, and can also be used directly in a shipping application.\n\nAdditionally, for cases not covered by these examples and, in general, to simplify the specification code and make it more correct, it is possible to switch to a simpler model where, instead of fully specifying access masks, stages and image layouts, the only concept that needs to be known about a resource is the resource state that encapsulates the stages that can use the resource and the usage mode for most common types of access. Then all transitions involve transitioning a resource from state A from state B, which is much easier to understand. To that end, Tobias Hector, a member of Khronos Group and a co-author of the Vulkan specification, wrote an open-source library, simple_vulkan_synchronization, that translates resource state (otherwise known as access type in the library) transitions into Vulkan barrier specification. The library is small and simple and provides support for split barriers as well as full pipeline barriers.\n\nThe performance guidelines outlined in the previous section are hard to follow in practice, especially given conventional immediate mode rendering architectures.\n\nTo make sure that the stages and image layout transitions are not overspecified, it’s important to know how the resource is going to be used in the future – if you want to emit a pipeline barrier after render pass ends, without this information you’re generally forced to emit a barrier with all stages in the destination stage mask, and an inefficient target layout.\n\nTo solve this problem, it’s tempting to instead emit the barriers before the resource is read, since at that point it’s possible to know how the resource was written to; however, this makes it hard to batch barriers. For example, in a frame with 3 render passes, A, B, and C, where C reads A’s output and B’s output in two separate draw calls, to minimize the number of texture cache flushes and other barrier work it’s generally beneficial specify a barrier before C that correctly transitions outputs of both A and B; instead what would happen is that there’s a barrier before each of C’s draw calls. Split barriers in some cases can reduce the associated costs, but in general just-in-time barriers will be overly expensive.\n\nAdditionally, using just-in-time barriers requires tracking the resource state to know the previous layout; this is very hard to do correctly in a multithreaded system since the final execution order on GPU can only be known once all commands are recorded and linearized.\n\nDue to the aforementioned problems, many modern renderers are starting to experiment with render graphs as a way to declaratively specify all dependencies between frame resources. Based on the resulting DAG structure, it’s possible to establish correct barriers, including barriers required for synchronizing work across multiple queues, and allocate transient resources with minimal use of physical memory.\n\nA full description of a render graph system is out of scope of this article, but interested readers are encouraged to refer to the following talks and articles:\n\nDifferent engines pick different parameters of the solution, for example Frostbite render graph is specified by the application using the final execution order (which the author of this article finds more predictable and preferable), whereas two other presentations linearize the graph based on certain heuristics to try to find a more optimal execution order. Regardless, the important part is that dependencies between passes must be declared ahead of time for the entire frame to make sure that barriers can be emitted appropriately. Importantly, the frame graph systems work well for transient resources that are limited in number and represent the bulk of required barriers; while it’s possible to specify barriers required for resource uploads and similar streaming work as part of the same system, this can make the graphs too complex and the processing time too large, so these are generally best handled outside of a frame graph system.\n\nOne concept that is relatively unique to Vulkan compared to both older APIs and new explicit APIs is render passes. Render passes allow an application to specify a large part of their render frame as a first-class object, splitting the workload into individual sub-passes and explicitly enumerating dependencies between sub-passes to allow the driver to schedule the work and place appropriate synchronization commands. In that sense, render passes are similar to render graphs described above and can be used to implement these with some limitations (for example, render passes currently can only express rasterization workloads which means that multiple render passes should be used if compute workloads are necessary to support). This section, however, will focus on simpler uses of render passes that are more practical to integrate into existing renderers, and still provide performance benefits.\n\nOne of the most important features of render passes is the ability to specify load and store operations. Using these, the application can choose whether the initial contents of each framebuffer attachments needs to be cleared, loaded from memory, or remain unspecified and unused by the application, and whether after the render pass is done the attachment needs to be stored to memory.\n\nThese operations are important to get right – on tiled architectures, using redundant load or store operations leads to wasted bandwidth which reduces performance and increases power consumption. On non-tiled architectures, driver can still use these to perform certain optimizations for subsequent rendering – for example, if the previous contents of an attachment is irrelevant but the attachment has associated compression metadata, driver may clear this metadata to make subsequent rendering more efficient.\n\nTo allow maximum freedom for the driver, it’s important to specify the weakest load/store operations necessary – for example, when rendering a full-screen quad to the attachment that writes all pixels, on tiled GPUs is likely to be faster than , and on immediate mode GPUs LOAD is likely to be faster – specifying is important so that the driver can perform an optimal choice. In some cases can be better than either LOAD or CLEAR since it allows the driver to avoid an expensive clear operation for the image contents, but still clear image metadata to accelerate subsequent rendering.\n\nSimilarly, should be used in case the application is not expecting to read the data rendered to the attachment - this is commonly the case for depth buffers and MSAA targets.\n\nAfter rendering data to an MSAA texture, it’s common to resolve it into a non-MSAA texture for further processing. If fixed-function resolve functionality is sufficient, there are two ways to implement this in Vulkan:\n• Using for the MSAA texture and after the render pass ends\n• Using for the MSAA texture and specifying the resolve target via member of\n\nIn the latter case, the driver will perform the necessary work to resolve MSAA contents as part of work done when subpass/renderpass ends.\n\nThe second approach can be significantly more efficient. On tiled architectures, using the first approach requires storing the entire MSAA texture to main memory, followed by reading it from memory and resolving to the destination; the second approach can perform in-tile resolve in the most efficient manner. On immediate mode architectures, some implementation may not support reading compressed MSAA textures using the transfer stage – the API requires a transition into layout before calling , which may lead to decompression of the MSAA texture, wasting bandwidth and performance. With , the driver can perform the resolve operation at maximum performance regardless of the architecture.\n\nIn some cases, fixed function MSAA resolve is insufficient. In this case, it’s necessary to transition the texture to and do the resolve in a separate render pass. On tiled architectures, this has the same efficiency issues as fixed-function method; on immediate mode architectures the efficiency depends on GPU and driver. One possible alternative is to use an extra subpass that reads the MSAA texture via an input attachment.\n\nFor this to work, the first subpass that renders to MSAA texture has to specify the MSAA texture via , with as the store op. The second subpass that performs the resolve needs to specify MSAA texture via pInputAttachments and the resolve target via pColorAttachments; the subpass then needs to render a full-screen quad or triangle with a shader that uses subpassInputMS resource to read MSAA data. Additionally, the application needs to specify a dependency between two subpasses that indicates the stage/access masks, similarly to pipeline barriers, and dependency flags . With this, the driver should have enough information to arrange the execution such that on tiled GPUs, the MSAA contents never leaves the tile memory and instead is resolved in-tile, with the resolve result being written to main memory. Note that whether this happens depends on the driver and is unlikely to result in significant savings on immediate mode GPUs.\n\nOlder APIs typically used to split the GPU state into blocks based on functional units – for example, in Direct3D 11 the full state of GPUs modulo resource bindings can be described using the set of shader objects for various stages (VS, PS, GS, HS, DS) as well as a set of state objects (rasterizer, blend, depth stencil), input assembly configuration (input layout, primitive topology) and a few other implicit bits like output render target formats. The API user then could set individual bits of the state separately, without regards to the design or complexity of the underlying hardware.\n\nUnfortunately, this model doesn’t match the model hardware typically uses, with several performance pitfalls that can occur:\n• While an individual state object is supposed to model parts of GPU state and could be directly transferred to commands that setup GPU state, on some GPUs the configuration of the GPU state required data from multiple different state blocks. Because of this, drivers typically must keep a shadow copy of all state and convert the state to the actual GPU commands at the time of Draw/DrawIndexed\n• With the rasterization pipeline getting more complex and gaining more programmable stages, some GPUs didn’t map them directly to hardware stages, which means that the shader microcode can depend on whether other shader stages are active and, in some cases, on the specific microcode for other stages; this meant that the driver might have to compile new shader microcode from state that can only be discovered at the time of Draw/DrawIndexed\n• Similarly, on some GPUs, fixed functional units from the API description were implemented as part of one of the shader stages – changing the vertex input format, blending setup, or render target format could affect the shader microcode. Since the state is only known at the time of Draw/DrawIndexed, this, again, is where the final microcode had to be compiled\n\nWhile the first problem is more benign, the second and third problem can lead to significant stalls during rendering as, due to the complexity of modern shaders and shader compilation pipelines, shader compilation can take tens to hundreds of milliseconds depending on hardware. To solve this, Vulkan and other new APIs introduce the concept of pipeline object – it encapsulates most GPU state, including vertex input format, render target format, state for all stages and shader modules for all stages. The expectation is that on every supported GPU, this state is sufficient to build final shader microcode and GPU commands required to set the state up, so the driver never has to compile microcode at draw time and can optimize pipeline object setup to the extent possible.\n\nThis model, however, presents challenges when implementing renderers on top of Vulkan. There are multiple ways to solve this problem, with different tradeoffs wrt complexity, efficiency, and renderer design.\n\nThe most straightforward way to support Vulkan is to use just-in-time compilation for pipeline objects. In many engines due to the lack of first-class concepts that match Vulkan, the rendering backend must gather information about various parts of the pipeline state as a result of various state setup calls, similarly to what a Direct3D 11 driver might do. Then, just before the draw/dispatch where the full state is known, all individual bits of state would be grouped together and looked up in a hash table; if there’s already a pipeline state object in the cache, it can be used directly, otherwise a new object can be created.\n\nThis scheme works to get the application running but suffers from two performance pitfalls.\n\nA minor concern is that the state that needs to be hashed together is potentially large; doing this for every draw call can be time consuming when the cache already contains all relevant objects. This can be mitigated by grouping state into objects and hashing pointers to these objects, and in general simplifying the state specification from the high-level API point of view.\n\nA major concern, however, is that for any pipeline state object that must be created, the driver might need to compile multiple shaders to the final GPU microcode. This process is time consuming; additionally, it can not be optimally threaded with a just-in-time compilation model – if the application only uses one thread for command submission, this thread would typically also compile pipeline state objects; even with multiple threads, often multiple threads would request the same pipeline object, serializing compilation, or one thread would need several new pipeline objects, which increases the overall latency of submission since other threads would finish first and have no work to do.\n\nFor multi-threaded submission, accessing the cache can result in contention between cores even when the cache is full. Fortunately, this can be solved by a two-level cache scheme as follows:\n\nThe cache would have two parts, the immutable part that never changes during the frame, and the mutable part. To perform a pipeline cache lookup, we first check if the immutable cache has the object – this is done without any synchronization. In the event of the cache miss, we lock a critical section and check if the mutable cache has the object; if it doesn’t, we unlock the critical section, create the pipeline object, and then lock it again and insert the object into the cache, potentially displacing another object (additional or synchronization might be required if, when two threads request the same object, only one compilation request is issued to the driver). At the end of the frame, all objects from the mutable cache are added to the immutable cache and the mutable cache is cleared, so that on the next frame access to these objects can be free-threaded.\n\nWhile just-in-time compilation can work, it results in significant amount of stuttering during gameplay. Whenever an object with a new set of shaders/state enters the frame, we end up having to compile a pipeline object for it which could be slow. This is a similar problem to what Direct3D 11 titles would have, however in Direct3D 11 the drivers did a lot of work behind the scenes to try to hide the compilation latency, precompiling some shaders earlier and implementing custom schemes for patching bytecode on the fly that didn’t require a full recompilation. In Vulkan, the expectation is that the application handles pipeline object creation manually and intelligently, so a naive approach doesn’t work very well.\n\nTo make just-in-time compilation more practical, it’s important to use the Vulkan pipeline cache, serialize it between runs, and pre-warm the in-memory cache described in the previous section at application startup from multiple threads.\n\nVulkan provides a pipeline cache object, , that can store driver-specific bits of state and shader microcode to improve compilation time for pipeline objects. For example, if an application creates two pipeline objects with identical setup except for culling mode, the shader microcode would typically be the same. To make sure the driver only compiles the object once, the application should pass the same instance of to in both calls, in which case the first call would compile the shader microcode and the second call would be able to reuse it. If these calls happen concurrently in different threads the driver might still compile the shaders twice since the data would only be added to the cache when one of the calls finishes.\n\nIt’s vital to use the same object when creating all pipeline objects and serialize it to disk between runs using and member of . This makes sure that the compiled objects are reused between runs and minimizes the frame spikes during subsequent application runs.\n\nUnfortunately, during the first play through the shader compilation spikes will still occur since the pipeline cache will not contain all used combinations. Additionally, even when the pipeline cache contains the necessary microcode, isn’t free and as such compilation of new pipeline objects can still increase the frame time variance. To solve that, it’s possible to pre-warm the in-memory cache (and/or ) during load time.\n\nOne possible solution here is that at the end of the gameplay session, the renderer could save the in-memory pipeline cache data – which shaders were used with which state – to a database. Then, during QA playthroughs, this database could be populated with data from multiple playthroughs at different graphics settings etc. – effectively gathering the set of states that are likely to be used during the actual gameplay.\n\nThis database can then be shipped with the game; at game startup, the in-memory cache could be prepopulated with all states created using the data from that database (or, depending on the amount of pipeline states, this pre-warming phase could be limited to just the states for the current graphics settings). This should happen on multiple threads to reduce the load time impact; the first run would still have a longer load time (which can be further reduced with features like Steam pre-caching), but frame spikes due to just-in-time pipeline object creation can be mostly avoided.\n\nIf a particular set of state combinations wasn’t discovered during QA playthroughs, the system can still function correctly – at the expense of some amount of stuttering. The resulting scheme is more or less universal and practical – but requires a potentially large effort to play through enough levels with enough different graphics settings to capture most realistic workloads, making it somewhat hard to manage.\n\nThe “perfect” solution – one that Vulkan was designed for – is to remove just-in-time compilation caches and pre-warming, and instead just have every single possible pipeline object available ahead of time.\n\nThis typically requires changing the renderer design and integrating the concept of the pipeline state into the material system, allowing a material to specify the state completely. There are different possible designs; this section will outline just one, but the important thing is the general principle.\n\nAn object is typically associated with the material that specifies the graphics state and resource bindings required to render the object. In this case, it’s important to separate resource bindings from the graphics state as the goal is to be able to enumerate all combinations of graphics state in advance. Let’s call the collection of the graphics state a “technique” (this terminology is intentionally similar to terminology from Direct3D Effect Framework, although there the state was stored in the pass). Techniques can then be grouped into effects, and a material would be referring to the effect, and to some sort of key to specify the technique from the effect.\n\nThe set of effects and set of techniques in an effect would be static; the set of effects would also be static. Effects are not as vital to being able to precompile pipeline objects as techniques but can serve as useful semantical grouping of techniques – for example, often material is assigned an effect at material creation time, but technique can vary based on where the object is rendered (e.g. shadow pass, gbuffer pass, reflection pass) or on the gameplay effects active (e.g. highlight).\n\nCrucially, the technique must specify all state required to create a pipeline object, statically, ahead of time – typically as part of the definition in some text file, whether in a D3DFX-like DSL, or in a JSON/XML file. It must include all shaders, blend states, culling states, vertex format, render target formats, depth state. Here’s an example of how this might look:\n\nAssuming all draw calls, including ones used for post-effects etc, use the effect system to specify render state, and assuming the set of effects and techniques is static, it’s trivial to precreate all pipeline objects – each technique needs just one – at load time using multiple threads, and at runtime use very efficient code with no need for in-memory caches or possibility of frame spikes.\n\nIn practice, implementing this system in a modern renderer is an exercise in complexity management. It’s common to use complex shader or state permutations – for example, for two-sided rendering you typically need to change culling state and perhaps change the shaders to implement two-sided lighting. For skinned rendering, you need to change vertex format and add some code to the vertex shader to transform the attributes using skinned matrices. On some graphics settings, you might decide that the render target format needs to be floating-point R10G11B10 instead of RGBA16F, to conserve bandwidth. All these combinations multiply and require you to be able to represent them concisely and efficiently when specifying technique data (for example, by allowing #ifdef sections inside technique declarations as shown above), and – importantly – being aware of the steadily growing amount of combinations and refactoring/simplifying them as appropriate. Some effects are rare enough that they could be rendered in a separate pass without increasing the number of permutations. Some computations are simple enough that always running them in all shaders can be a better tradeoff than increasing the number of permutations. And some rendering techniques offer better decoupling and separation of concerns, which can also reduce the number of permutations.\n\nImportantly though, adding state permutations to the mix makes the problem harder but doesn’t make it different – many renderers have to solve the problem of a large number of shader permutations anyway, and once you incorporate all render state into shader/technique specification and focus on reducing the number of technique permutations, the same complexity management solutions apply equally to both problems. The benefit of implementing a system like this is perfect knowledge of all required combinations (as opposed to having to rely on fragile permutation discovery systems), great performance with minimal frame-to-frame variance including the first load, and a forcing function to keep the complexity of rendering code at bay.\n\nVulkan API shifts a large amount of responsibility from driver developers onto application developers. Navigating the landscape of various rendering features becomes more challenging when many implementation options are available; it’s challenging enough to write a correct Vulkan renderer, but performance and memory consumption is paramount. This article tried to discuss various important considerations when dealing with specific problems in Vulkan, present multiple implementation approaches that provide different tradeoffs between complexity, ease of use and performance, and span the range between porting existing renderers to redesigning renderers around Vulkan.\n\nUltimately, it’s hard to give a general advice that works across all vendors and is applicable to all renderers. For this reason, it’s vital to profile the resulting code on the target platform/vendor – for Vulkan, it’s important to monitor the performance across all vendors that the game is planning to ship on as the choices the application makes are even more important, and in some cases a specific feature, like fixed-function vertex buffer bindings, is the fast path on one vendor but a slow path on another.\n\nBeyond using validation layers to ensure code correctness and vendor-specific profiling tools, such as AMD Radeon Graphics Profiler or NVidia Nsight Graphics, many open-source libraries that can help optimize your renderer for Vulkan are available:\n• VulkanMemoryAllocator - provides convenient and performant memory allocators for Vulkan as well as other memory-related algorithms such as defragmentation.\n• volk - provides an easy way to use driver-provided Vulkan entrypoints from the driver directly which can reduce function call overhead\n• simple_vulkan_synchronization - provides a way to specify Vulkan barriers using a simplified access type model, which helps balance correctness and performance\n• Fossilize - provides serialization support for various Vulkan objects, most notably for pipeline state creation info which can be used to implement pre-warming for a pipeline cache.\n• perfdoc - provides layers similar to validation layers, that analyze the stream of rendering command and identify potential performance problems on ARM GPUs\n• niagara - provides an example bindless renderer that follows some of the advice from this article (but not all of it!)\n• Vulkan-Samples - provides many samples that explore various tradeoffs in implementation of Vulkan rendering techniques along with details on the performance on mobile.\n\nFinally, some vendors develop open-source Vulkan drivers for Linux; studying their sources can help gain more insight into performance of certain Vulkan constructs:\n\nGPUOpen-Drivers for AMD - contains xgl which has the Vulkan driver source, and PAL which is a library used by xgl; many Vulkan function calls end up going through both xgl and PAL"
    },
    {
        "link": "https://vulkan-tutorial.com/Depth_buffering",
        "document": "The geometry we've worked with so far is projected into 3D, but it's still completely flat. In this chapter we're going to add a Z coordinate to the position to prepare for 3D meshes. We'll use this third coordinate to place a square over the current square to see a problem that arises when geometry is not sorted by depth.\n\nChange the struct to use a 3D vector for the position, and update the in the corresponding :\n\nNext, update the vertex shader to accept and transform 3D coordinates as input. Don't forget to recompile it afterwards!\n\nIf you run your application now, then you should see exactly the same result as before. It's time to add some extra geometry to make the scene more interesting, and to demonstrate the problem that we're going to tackle in this chapter. Duplicate the vertices to define positions for a square right under the current one like this:\n\nUse Z coordinates of and add the appropriate indices for the extra square:\n\nRun your program now and you'll see something resembling an Escher illustration:\n\nThe problem is that the fragments of the lower square are drawn over the fragments of the upper square, simply because it comes later in the index array. There are two ways to solve this:\n• Sort all of the draw calls by depth from back to front\n\nThe first approach is commonly used for drawing transparent objects, because order-independent transparency is a difficult challenge to solve. However, the problem of ordering fragments by depth is much more commonly solved using a depth buffer. A depth buffer is an additional attachment that stores the depth for every position, just like the color attachment stores the color of every position. Every time the rasterizer produces a fragment, the depth test will check if the new fragment is closer than the previous one. If it isn't, then the new fragment is discarded. A fragment that passes the depth test writes its own depth to the depth buffer. It is possible to manipulate this value from the fragment shader, just like you can manipulate the color output.\n\nThe perspective projection matrix generated by GLM will use the OpenGL depth range of to by default. We need to configure it to use the Vulkan range of to using the definition.\n\nA depth attachment is based on an image, just like the color attachment. The difference is that the swap chain will not automatically create depth images for us. We only need a single depth image, because only one draw operation is running at once. The depth image will again require the trifecta of resources: image, memory and image view.\n\nCreate a new function to set up these resources:\n\nCreating a depth image is fairly straightforward. It should have the same resolution as the color attachment, defined by the swap chain extent, an image usage appropriate for a depth attachment, optimal tiling and device local memory. The only question is: what is the right format for a depth image? The format must contain a depth component, indicated by in the .\n\nUnlike the texture image, we don't necessarily need a specific format, because we won't be directly accessing the texels from the program. It just needs to have a reasonable accuracy, at least 24 bits is common in real-world applications. There are several formats that fit this requirement:\n\nThe stencil component is used for stencil tests, which is an additional test that can be combined with depth testing. We'll look at this in a future chapter.\n\nWe could simply go for the format, because support for it is extremely common (see the hardware database), but it's nice to add some extra flexibility to our application where possible. We're going to write a function that takes a list of candidate formats in order from most desirable to least desirable, and checks which is the first one that is supported:\n\nThe support of a format depends on the tiling mode and usage, so we must also include these as parameters. The support of a format can be queried using the function:\n\nThe struct contains three fields:\n• : Use cases that are supported with linear tiling\n• : Use cases that are supported with optimal tiling\n• : Use cases that are supported for buffers\n\nOnly the first two are relevant here, and the one we check depends on the parameter of the function:\n\nIf none of the candidate formats support the desired usage, then we can either return a special value or simply throw an exception:\n\nWe'll use this function now to create a helper function to select a format with a depth component that supports usage as depth attachment:\n\nMake sure to use the flag instead of in this case. All of these candidate formats contain a depth component, but the latter two also contain a stencil component. We won't be using that yet, but we do need to take that into account when performing layout transitions on images with these formats. Add a simple helper function that tells us if the chosen depth format contains a stencil component:\n\nCall the function to find a depth format from :\n\nWe now have all the required information to invoke our and helper functions:\n\nHowever, the function currently assumes that the subresource is always the , so we will need to turn that field into a parameter:\n\nUpdate all calls to this function to use the right aspect:\n\nThat's it for creating the depth image. We don't need to map it or copy another image to it, because we're going to clear it at the start of the render pass like the color attachment.\n\nWe don't need to explicitly transition the layout of the image to a depth attachment because we'll take care of this in the render pass. However, for completeness I'll still describe the process in this section. You may skip it if you like.\n\nMake a call to at the end of the function like so:\n\nThe undefined layout can be used as initial layout, because there are no existing depth image contents that matter. We need to update some of the logic in to use the right subresource aspect:\n\nAlthough we're not using the stencil component, we do need to include it in the layout transitions of the depth image.\n\nThe depth buffer will be read from to perform depth tests to see if a fragment is visible, and will be written to when a new fragment is drawn. The reading happens in the stage and the writing in the . You should pick the earliest pipeline stage that matches the specified operations, so that it is ready for usage as depth attachment when it needs to be.\n\nWe're now going to modify to include a depth attachment. First specify the :\n\nThe should be the same as the depth image itself. This time we don't care about storing the depth data ( ), because it will not be used after drawing has finished. This may allow the hardware to perform additional optimizations. Just like the color buffer, we don't care about the previous depth contents, so we can use as .\n\nAdd a reference to the attachment for the first (and only) subpass:\n\nUnlike color attachments, a subpass can only use a single depth (+stencil) attachment. It wouldn't really make any sense to do depth tests on multiple buffers.\n\nNext, update the struct to refer to both attachments.\n\nFinally, we need to extend our subpass dependencies to make sure that there is no conflict between the transitioning of the depth image and it being cleared as part of its load operation. The depth image is first accessed in the early fragment test pipeline stage and because we have a load operation that clears, we should specify the access mask for writes.\n\nThe next step is to modify the framebuffer creation to bind the depth image to the depth attachment. Go to and specify the depth image view as second attachment:\n\nThe color attachment differs for every swap chain image, but the same depth image can be used by all of them because only a single subpass is running at the same time due to our semaphores.\n\nYou'll also need to move the call to to make sure that it is called after the depth image view has actually been created:\n\nBecause we now have multiple attachments with , we also need to specify multiple clear values. Go to and create an array of structs:\n\nThe range of depths in the depth buffer is to in Vulkan, where lies at the far view plane and at the near view plane. The initial value at each point in the depth buffer should be the furthest possible depth, which is .\n\nNote that the order of should be identical to the order of your attachments.\n\nThe depth attachment is ready to be used now, but depth testing still needs to be enabled in the graphics pipeline. It is configured through the struct:\n\nThe field specifies if the depth of new fragments should be compared to the depth buffer to see if they should be discarded. The field specifies if the new depth of fragments that pass the depth test should actually be written to the depth buffer.\n\nThe field specifies the comparison that is performed to keep or discard fragments. We're sticking to the convention of lower depth = closer, so the depth of new fragments should be less.\n\nThe , and fields are used for the optional depth bound test. Basically, this allows you to only keep fragments that fall within the specified depth range. We won't be using this functionality.\n\nThe last three fields configure stencil buffer operations, which we also won't be using in this tutorial. If you want to use these operations, then you will have to make sure that the format of the depth/stencil image contains a stencil component.\n\nUpdate the struct to reference the depth stencil state we just filled in. A depth stencil state must always be specified if the render pass contains a depth stencil attachment.\n\nIf you run your program now, then you should see that the fragments of the geometry are now correctly ordered:\n\nThe resolution of the depth buffer should change when the window is resized to match the new color attachment resolution. Extend the function to recreate the depth resources in that case:\n\nThe cleanup operations should happen in the swap chain cleanup function:\n\nCongratulations, your application is now finally ready to render arbitrary 3D geometry and have it look right. We're going to try this out in the next chapter by drawing a textured model!"
    }
]