[
    {
        "link": "https://dev.epicgames.com/documentation/en-us/unreal-engine/unreal-engine-5-5-documentation",
        "document": "Ask questions and help your peers\n\nWrite your own tutorials or read those from others"
    },
    {
        "link": "https://dev.epicgames.com/documentation/en-us/unreal-engine/xr-best-practices-in-unreal-engine",
        "document": "Virtual reality (VR) is an immersive new medium, with unique aspects to consider when authoring and presenting content to users on these platforms. Considerations include user comfort, content optimization, and limitations of the platforms. Use this page as a reference for these topics as you develop your projects for VR.\n\nWhen you create a new project targeting VR platforms, start with the VR Template in the Games category. The VR Template provides everything you need to start develping VR projects in Unreal Engine 5.\n\nIf you don't want to use the VR Template as a base for your project, you can create a new blank C++ or Blueprint project with the following settings:\n\nThese settings create an Unreal project with minimal rendering features enabled. This ensures that the project starts with a good framerate, and you can add any other rendering features you need.\n\nAfter you create the project, set the following project settings to improve the performance of your apps:\n• Go to Edit > Project Settings > Description, and enable Start In VR.\n• In Edit > Project Settings > Rendering > Default Settings, set the Anti-Aliasing Method to Multisample Anti-Aliasing (MSAA).\n\nSee the Scalability Reference for explanations and examples on scaling the graphics of your app to improve its performance or quality. The following table describes some console variables and their recommended values for VR projects.\n\nThe following .ini settings are from the UE4-powered VR demo Showdown. You can use these settings as a starting point by copying them to your project's Config/DefaultEngine.ini file under SystemSettings.\n\nMost VR applications implement their own procedures to control the VR frame rate. Because of this, you should disable several general UE4 project settings that can interfere with VR applications.\n\nFollow these steps to disable general framerate settings in UE:\n• From the editor's main menu, choose Edit > Project Settings to open the Project Settings window.\n• In the Project Settings window, choose General Settings in the Engine section.\n\nEnsuring the correct scale of your world is one of the most important ways to help deliver the best user experience possible on VR platforms. Having the wrong scale can lead to all kinds of sensory issues for users, and could even cause simulation sickness. Objects are most easily viewed in VR when they are in a range of 0.75 to 3.5 meters from the player's camera. Inside of UE4, 1 Unreal Unit (UU) is equal to 1 Centimeter (CM). This means that objects inside of Unreal are best viewed when they are 75 UU to 350 UU away from the player's camera (when using VR).\n\nYou can adjust the scale of your world using the World to Meters variable located under World Settings. Increasing or decreasing this number will make the user feel larger or smaller in relation to the world around them. Assuming your content was built with 1 Unreal Unit = 1 cm, setting World To Meters to 10 will make the world seem very large, while setting World To Meters to 1000 will make the world seem very small.\n\nSimulation sickness is a form of motion sickness that can affect users during immersive experiences. The following list describes some best practices to limit the discomfort that users can experience in VR.\n• Maintain framerate: Low framerates can cause simulation sickness. Optimizing your project as much as possible can improve the experience for the user. See the following table for the recommended frame rates that you need to target for your XR platform.\n\n* User testing: Test with many different users, and monitor any discomfort they might experience from the VR app to prevent simulation sickness.\n• Keep users in control of the camera: Cinematic cameras, or anything that takes control of camera movements away from the player, contribute to user discomfort in immersive experiences. Camera effects, such as head bobbing and camera shaking, should be avoided because they can lead to user discomfort if the user is not controlling them.\n• Field of View (FOV) must match the device: The FOV value is set through the device's SDK and internal configuration, and matches the physical geometry of the headset and lenses. Because of this, the FOV should not be modified in UE, and should not be modifiable by the user. If the FOV value is changed, the world can appear to warp when you turn your head, leading to discomfort.\n• Use lights and colors that are more dim, and avoid smearing: When designing elements for VR, you might need to use dimmer lights and colors than you normally would. Strong and vibrant lighting in VR can cause simulation sickness to occur more quickly. Using cooler shades and dimmer lights can help prevent user discomfort. This also helps with preventing smearing between bright and dark areas on the display.\n• Movement speed should not change: Users should start at full speed instead of gradually accelerating to full speed.\n• Avoid post process effects that greatly affect what the user sees: Avoid post process effects such as Depth of Field and Motion Blur to prevent user discomfort.\n• Visual Effects such as Motion Blur and Depth of Field should be avoided.\n• Consider things like character height, width, speed, and camera location as they need to be slightly modified for VR characters.\n\nVR camera setup in UE4 depends entirely on whether your VR experience is seated or standing:\n• Seated experience: You will need to artificially raise the camera origin to the desired player height for the project. And call the Set Tracking Origin function, with Origin set to \"Eye Level\".\n• Standing experience: Make sure the camera origin is set to 0, relative to the pawn's root, which is typically on the ground. Attach a camera component to a scene component at the base of the pawn, at the ground level. And call Set Tracking Origin with the Origin parameter set to Floor Level.\n\nWhen creating VR content, remember that users can look at that content from multiple angles. Use the following list as a reference as you create content for VR:\n• Scale: The scale of objects in a VR world should mimic reality as closely as possible. Making objects bigger or smaller than their real world counterparts could lead to confusion or simulation sickness.\n• Missing Polygon Faces: In non-immersive experiences, it is a common practice to remove polygon faces from objects that cannot be seen by the player. However, in VR experiences, players may have more freedom to look around. Missing polygon faces can sometimes lead to users being able to see things that they're not supposed to see.\n• Lighting Types: You should use Static Lights and Lightmaps for VR projects. This is the cheapest lighting option to render. If you need to use dynamic lighting, make sure to limit the amount of dynamic lights to as few as possible, and make sure that they never touch one another.\n• VR and VFX: Some VFX tricks, like using SubUV Textures to simulate fire or smoke, do not hold up very well when viewed in VR. In many cases, you will need to use static meshes instead of 2D particles to simulate effects like explosions or smoke trails. Near-field effects, or effects that happen very close to the camera, work well in VR, but only when the effects are made up of Static Mesh particles.\n• VR and Transparency: In 3D graphics, rendering transparency is extremely costly, because transparency will generally have to be reevaluated per frame to ensure that nothing has changed. Because of this reevaluation, rendering transparency in VR can be so costly that its cost outweighs its benefits. However, to get around this issue, you can use the DitherTemporalAA Material Function. This Material Function will allow a Material to look like it is using transparency and can help avoid common transparency issues, such as self-sorting. Click for full image.\n• Fake everything you can: Finding clever ways to recreate expensive rendering options, like dynamic shadows or lighting, can be a huge win for hitting your performance goals in VR. In Showdown, having the characters cast dynamic shadows proved to be too expensive per-frame, which meant that dynamic shadows had to be cut from the project. However, this made the characters look like they were floating while moving. To fix this, fake blob shadows were introduced that could dynamically adjust their position and intensity based on how close the character was to an object in the world. This helped give the illusion that a character was casting shadows when they came close to the ground (or other objects).\n\nBelow you will find a list of features that might not work as expected in VR because of how HMDs are designed, along with possible workarounds to address them.\n• Screen Space Reflections(SSR): While SSR will still work in VR, the reflections they produce could have issues matching up to what it is reflecting in the world. Instead of using SSR, use Reflection Probes as they are much cheaper and suffer less from reflection-alignment issues.\n• Screen Space Global Illumination: Screen space techniques can generate differences between the displays for the two eyes in the HMD. These differences can cause user discomfort. See Lighting Types for recommended lighting types in VR for replacements.\n• Ray Tracing: VR apps using ray tracing currently aren't able to maintain the resolution and frame rate needed for a comfortable VR experience.\n\nWhen viewing Normal maps on objects in VR, you will notice that they do not have the impact that they might have once had. This is because Normal mapping does not account for having a binocular display or motion parallax. As a result, Normal maps often look flat when viewed with a VR device. However, that does not mean that you should not or will not need to use Normal maps; it just means that you need to more closely evaluate if the data in the Normal map would be better off made out of geometry. Below, you will find a technique that can be used in place of Normal maps.\n• Parallax Mapping: Parallax mapping takes Normal mapping to the next level by accounting for depth cues that Normal mapping does not. A Parallax mapping shader can better display depth information, making objects appear to have more detail than they do. This is because no matter which angle you look at, a Parallax map will always correct itself to show you the correct depth information from your viewpoint. The best use of a Parallax map would be for things like cobblestone pathways and surfaces with fine detail."
    },
    {
        "link": "https://unrealengine.com/en-US/unreal-engine-5",
        "document": "Lumen is a fully dynamic global illumination and reflections solution that enables indirect lighting to adapt on the fly to changes to direct lighting or geometry—for example, changing the sun’s angle with the time of day or opening an exterior door.\n\nWith Lumen, you no longer have to author lightmap UVs, wait for lightmaps to bake, or place reflection captures; what you see inside the Unreal Editor is what you get on console."
    },
    {
        "link": "https://unrealengine.com/en-US/blog/game-animation-sample",
        "document": "Interested in creating high-quality character locomotion and other gameplay animation systems? Look no further. We’re pleased to announce the Game Animation Sample Project is now available to download for free! \n\n \n\n Supercharge your development with this animation database of over 500 AAA-quality animations, complete with a fully functional character and animation Blueprint.\n\nWhether you want to learn or just need a head start, this project contains everything you need to get a character up and running using our brand new Motion Matching toolset. And stay tuned—this is only the beginning. We plan to update this project with new tools, techniques, and animations in future releases.\n\nThe Game Animation Sample Project showcases dynamic, fully functional animation systems that are commonly needed in games. Built using best practices, it can be used for learning or to help you kickstart your own creations.\n\nFor indie devs, the sample provides plug-and-play locomotion, as well as hundreds of game-ready animations compatible with all UE Mannequins that can be used for humanoid locomotion in your own projects. You’re free to use all these animations with or without their accompanying animation Blueprints and retarget them to your own characters.\n\n\n\n \n\n Animation selection in the sample is mostly powered by The Game Animation Sample Project comes with multiple Motion Matching databases. Check out the project documentation to understand the benefits of separating animations into multiple databases; learn why the Pose Search schema is set up like it is; and explore scenarios where the system might select a dense dataset with hundreds of animations versus a sparser set with less than a hundred.Animation selection in the sample is mostly powered by Choosers and Proxy Tables . To address gaps in animation coverage, we use Pose Warping . The sample also includes world-class authoring and debugging tools like the Rewind Debugger, which enables you to record, inspect, and edit your motion databases to understand and improve your transitions.\n\nMotion Matching is a simple yet powerful way to animate characters in games. It continuously selects the best frame of animation to play from its motion database, aiming to closely match the current pose of the character as well as its past and future movement.\n\nThe system scales to the size of your dataset to reach your desired fidelity and uses Pose Warping algorithms to fill in the gaps. \n\n \n\n Motion Matching is about more than locomotion—it’s a powerful tool for jumping, falling, and complex traversal (think parkour). Using 3D trajectory prediction, Motion Matching can produce seamless transitions with anticipation and long follow-throughs for realistic results."
    },
    {
        "link": "https://developer.varjo.com/docs/unreal/ue5/unreal5-tips",
        "document": "The Console Variables Editor is a panel that displays information about all the console variables set in the project, and provides a central location to view and modify all the variables.\n\n2.Useful VR Console Commands. You can use these settings as a starting point by copying them to your project’s Config/DefaultEngine.ini file under SystemSettings or in a new Console Variables Editor.\n\n4.While previewing the project you can switch to camera mode to “fly” around the scene by typing the console command ToggleDebugCamera\n\n5.It is better to test/preview the unbuild project using Standalone Game method and not just the VRpreview\n•To make it run in VR go to Editor - Preferences - Level Editor - Play in Standalone Game - Additional Launch Parameters and add -vr\n\n6.VR Mode not supported by OpenXR but can be used with SteamVR plugin. For its full features enable Legacy editor UI mode\n\nUse VR Mode to design and build worlds in a virtual reality environment using the full capabilities of the Unreal Editor toolset combined with interaction models designed specifically for Virtual Reality (VR) world building.\n\n-Dont use Event Tick when you dont need it.\n\n1.Use Columns view for more information about the assets in content browser.\n\nThe Columns view lays out all Assets with a spreadsheet-like arrangement of properties.\n\nFilters and Collections are both ways to sort and group Assets inside the Content Browser.\n\n3.Use Advanced Search Syntax and save it into Custom Filter.\n\nYou can search for your content in the Content Browser using advanced search operators. These operators can refine your search, making it easier for you to find what you need. You can also use them to search for key-value pairs from Asset metadata, and access special key values.\n\n4.Use Bulk Edit via Property Matrix to change setting for a group of assets (Nanite,LODs, etc).\n\nUse the Property Matrix to bulk edit and compare values for large numbers of Objects or Actors. It displays a configurable set of properties for a collection of objects as columns in a table view that can be sorted on any column. The Property Matrix also provides a standard property editor that displays all properties for the current selection set in the table view.\n\nMaterial instancing is a way to create a parent Material that you can use as a base to make a wide variety of different looking children (Material instances).\n\n6.Textures should always be a power of 2 (128,512,1024,2048,4096)\n\nUse power of two sizes when possible, such as 32, 64, 128, 2048, and so on. Power of two values can be mipmapped and streamed. Non-power of two sizes are never streamed and do not generate mipmaps. Some GPUs have hardware limits in the maximum size texture they can support. For example, some GPUs may not support texture sizes larger than 8192 pixels (8k).}\n\nThe Level Of Detail section displays mip and LOD settings. Mip-map generation happens during importing of the Texture and creates a mip-map chain for the Texture\n\nCombine two or more Static Mesh Actors into a single new Actor using Actor merging. This reduces draw calls and helps with project optimization\n\n9.Use Hierarchical Level of Detail (HLOD) system as a way to increase performance while keeping the quality high.\n\nHLODs can replace multiple Static Mesh Actors with single, combined SM Actor at long view distances. This reduces the number of Actors needed to be rendered for the scene, increasing the performance by lowering the number of draw calls per frame.\n\n10.Use Packed Level Actor by selecting a group of actors - right clicking on them and sending them to separate level instance.\n\nIt groups a Levels static meshes into as few static mesh instances as possible.\n\nModeling Mode contains a variety of tools for creating both simple and complex 3D meshes for all your visualization projects. These tools are designed to streamline the modeling creation process and are compatible with meshes created using traditional workflows, as well as more complex meshes that are generated by photogrammetry"
    },
    {
        "link": "https://dev.epicgames.com/documentation/en-us/unreal-engine/importing-animations-using-fbx-in-unreal-engine",
        "document": "We can import animations into Unreal Engine from external 3D modeling applications, such as 3DS Max, Maya or Blender. Although we're using 3DS Max and Maya to illustrate this lesson's goal, you can import animations into Unreal Engine from any 3D modeling application with a save feature.\n\nThe focus of this guide is to show you how to import animations from external 3D modeling programs.\n\nAfter going through this guide, you'll know:\n• How to export animations from external 3D modeling applications.\n• How to import animations into Unreal Editor from external 3D modeling applications.\n\nAnimations must be exported individually with one animation per skeletal mesh in a single file.\n\nUnreal's FBX animation import pipeline allows content developers to import animations with or without skeletal meshes.\n• In the Content Browser, click the Import button.\n• Locate and select the FBX file you want to import.\n• Click Open to begin importing the FBX file you want to import to your project.\n• Update the appropriate settings in the FBX Import Options dialog. Default settings should be sufficient when importing a mesh that doesn't share an existing skeleton. When importing LODs, the name of the imported mesh will follow default Naming Conventions. Refer to our FBX Import Dialog documentation for additional information about all of the settings. There are two import buttons available to us in the FBX Importer. The first option we have is the Import button, allowing us to import the currently selected FBX file with our specified settings. The second option we have is the Import All button, allowing us to import all of the currently selected FBX files with our specified settings. For more information on the settings available to us in the FBX Importer, go to the FBX Import Options Reference page.\n• Clicking Import or Import All adds your meshes to the project.\n• The imported skeletal mesh and animations appear inside the Content Browser if the process succeeded. By default, the AnimationSequence created to hold the imported animation is named after the skeleton's root bone.\n\nUnreal allows content creators to import multiple animations in a single FBX file; however, many DCC tools (such as 3ds Max and Maya) do no support the saving of multiple animations to a single file. If you export from supporting applications (such as Motion Builder), Unreal will import all of the animations contained in that file.\n\nBefore beginning this section, you'll need an AnimationSequence to import the animation into. Animation sequences can be created through the Content Browser or directly in the AnimationSequence Editor.\n• Locate and select the FBX file you want to import.\n• Click Open to begin importing the FBX file you want to import to your project.\n• Update the appropriate settings in the FBX Import Options dialog. Default settings should be sufficient when importing a mesh that doesn't share an existing skeleton. When importing LODs, the name of the imported mesh will follow default naming conventions. Refer to our FBX Import Dialog documentation for additional information about all of the settings. You must specify an existing skeleton when importing animations individually.\n• The imported skeletal mesh and animations appear inside the Content Browser if the process succeeded. By default, the AnimationSequence created to hold the imported animation is named after the skeleton's root bone.\n\nNow that we've reached the end of this guide, you've learned:\n\n✓ How to export animations from external 3D modeling applications.\n\n ✓ How to import animations into Unreal Editor from external 3D modeling applications."
    },
    {
        "link": "https://dev.epicgames.com/documentation/en-us/unreal-engine/import-and-export-cinematic-fbx-animations-in-unreal-engine",
        "document": "This page describes how you can export a scene from Sequencer as an FBX file and make alterations to that file inside a third-party application like 3ds Max or Maya. Once satisfied with your changes, you can then import the FBX file back into your Sequencer scene in Unreal Engine 4 (UE4) along with your updated changes.\n\nThere are some caveats to this process that you should keep in mind:\n• When exporting, you can export all the objects with animations in your scene to your FBX file.\n• Importing FBX files will only import animation. It will not create new objects.\n• Export/Import does not work with Master Sequences, Shots within a Master, or Subscenes.\n• Make sure that the time settings in your third-party application matches the time frame used for your scene in Unreal Engine.\n\nAs long as you export from a source Level Sequence and not a shot that sits inside a Master, and are only altering the animation of existing assets, you will be able to import your changes directly back into your scene inside UE4. Also, using the same time frames in both UE4 and your third-party application will ensure that they line up, and that the imported scene is not resampled at a different time rate.\n\nBelow, we export a scene from the Sequencer Subway Project (pictured below) available for free from the Learn tab of the Launcher. We then take it into Maya and alter the camera movement in the scene, then import that FBX along with our changes back into UE4 where our existing scene is automatically updated with our changes.\n\nFirst, determine which Level Sequence to export.\n• In the clip below, we pick the shot we want to alter and open it up. We then go into the General Options and choose Export. We have a Master Track that includes shot0040_001 so we open that particular Level Sequence to export it. Upon performing the FBX Export process, you will be shown the FBX Export Options window where you can define how to export your content. Use these options to choose the FBX Export Compatibility version as well as the option to export the Skeletal Mesh's Vertex Color, Level of Detail or Collision settings. There is also the option to Map Skeletal Motion to Root which will (if enabled) map the Skeletal Actor motion to the root bone of the Skeleton. For this example, we use the default settings and choose Export to export our content.\n\nWhen exporting an object in a Shot or Subsequence, you can determine if you want to export the Shot by Master Sequence time or Local Sequence time. Master Sequence time is useful when you are looking at the animation holistically and in comparison to other shots. Local Sequence time is useful when you are working shot by shot. Exporting to either time maps to the shot keys in Maya, and in Sequencer you can move from Local to Master time easily.\n\nThe default setting is to Export Local Time (checked). To export by Master Time, uncheck the box.\n\nOnce we have an exported FBX from Sequencer, we open our third-party application to import it so we can start editing.\n• Before we import our FBX into Maya, we set our working units for Time to match our Time Rate used in Unreal Engine.\n• We then import our FBX into Maya and set up our scene so that we can begin making edits. After importing:\n• Set the Perspective under Panel to use the camera shotCamA in our scene.\n• Hide any elements obstructing the view (such as the god ray mesh) by pressing Ctrl + H with it selected.\n• Then, under View we Select Camera to display the properties and keyframes for shotCamA, which we can then begin to edit.\n• Next, we make edits to the movement of our camera. Below, we select the Translation and Rotation properties of our shotCamA. Then, right-click and select Break Connections. We then pick a new starting point for the camera to be in, to start our scene, pressing the S key to keyframe the new position. We scrub ahead slightly and move the camera to the ending position. Press the S key again to keyframe the end position so that our camera moves along a new path.\n• With our changes in place, we then export our scene using Export Selection from the File menu. During the export process from Maya, we first change the export type to FBX export. Then, under Advanced Options and Axis Conversion, we change the Up Axis to follow the Z-axis to allow for a compatible import into Unreal Engine. Future versions of the engine may address this so that you do not have to convert the axis, but you will need to do this for now to avoid any importing problems.\n\nNow that we have an edited version in FBX format we can import it and the changes into Unreal Engine, updating our scene.\n• Inside our shot0040_001 that we wanted to edit, we right-click on our shotCamA and select Import. This will take the transform data inside the FBX and apply it to the selected object in Unreal Engine. This can be useful if you want to apply the same transform data to multiple objects inside your scene.1. Upon importing our FBX we are presented with the Import FBX window. This (if enabled) allows you to Force Front XAxis which will convert the scene from the FBX coordinate system to UE4 coordinate system with front X axis instead of -Y. If cameras do not already exist in the Level, go ahead and enable the Create Cameras option. You can also import the FBX from the General Options window instead of using the right-click method. When choosing this method, Unreal Engine will try to match the names of your objects. For example, we have shotCamA in both our FBX file and our Sequence so it will apply the data from the FBX file to that object inside Unreal Engine.\n\nIn our example, we only applied Transform changes to our camera. However, we could have keyframed new values for our Focal Length to produce a different looking shot as well.\n\nYou can experiment with this workflow and use what works best in your given situation. There may be times when you want to export to a 3D package like Maya or 3ds Max to fine-tune aspects of your scene and then import your changes back into your scene in Unreal Engine."
    },
    {
        "link": "https://base.movella.com/s/article/FBX-import-into-Unreal-Engine?language=en_US",
        "document": ""
    },
    {
        "link": "https://base.movella.com/s/article/FBX-import-into-Unreal-Engine",
        "document": ""
    },
    {
        "link": "https://forums.unrealengine.com/t/vr-controller-button-play-animation-in-scene/672930",
        "document": "Describes how Animation Montages can be played back at runtime."
    },
    {
        "link": "https://forums.unrealengine.com/t/can-vr-motion-controller-be-configured-to-control-animations-on-a-character-independent-to-headset/646815",
        "document": "Hi, apologies if the question seems unclear. Can anyone shed some light on whether I can set up a motion controller to simply play different animations on a rigged character, and not be tied to the position of the headset? The headset wearer is observing and speaking only, and since they’re in the headset, I might as well use the motion controllers to perform actions they can see, and respond to.\n\nThink of it like a VR gameshow, where the contestant is wearing the headset and is in a static position. The controllers are in the possession/use of a second actual person, playing the host and in the same room. The gameshow host is represented by a Mixamo character, and is voiced by the second person, not far from the contestant in the headset. The host asks questions and the contestant in the headset answers. Based on the answer given, the host chooses an animation and presses grab, trigger etc, and the Mixamo character responds. General animations are sufficient, I’m not going down the Livelink path of face tracking, etc. I just want nodding, idle, falling over, shaking head, that sort of stuff.\n\nUp until this point I figured the way to go was to approach it like swapping out motion controller hands for actual hands with blueprinted animations, then attach them to the Mixamo character skeletal mesh. Is this how other people would approach the concept??\n\n Is there a way that I can simply use them as an animation controller for a static character, instead of a motion controller? Would I set them as replacement “hands”, then remove reference to location in the blueprints so they no longer respond to physical motion?\n\nThanks for any tips, just trying to see if I’m on the right track or if I’m way off and there’s a better way to go. Cheers"
    },
    {
        "link": "https://forums.unrealengine.com/t/enhanced-input-and-motion-controllers/251909",
        "document": "First thing first, i’m using a custom branch of engine source code master branch (so is more 4.28 than 4.27) where i have enabled Motion Controllers mapping in the Input Mapping Context, you can see the pull request here https://github.com/EpicGames/UnrealEngine/pull/8389 .\n\nThis also happen with UE5 Early Access 2.\n\nNow that i can map Motion Controllers to an Input Action i noticed a weird behavior, the Input Action is triggered only if the Motion Controller key i want to use is also bind in the project settings.\n\nFor example, if i want to use the A button on my Valve Index Controllers i have to:\n• Add the Input Action to the Input Mapping Context and bind it to the button A\n• Use my Input Action where i need it\n\nLooks like the action mapping we create in the project settings “act like a bridge” between the Motion Controller and the Input Mapping Context, you don’t even need to use the action mapping, it just has to be here as a dummy.\n\nThis happen only with Motion Controllers, or at least with Valve Index Controller (i only have this VR device so i can’t test on others like Quest 1/2), mouse, keyboard and gamepad works fine.\n\nI wonder if is a limitation of the Enhanced input system or of OpenXR Input."
    },
    {
        "link": "https://developers.meta.com/horizon/documentation/unreal/unreal-controller-input-mapping-reference",
        "document": ""
    },
    {
        "link": "https://reddit.com/r/UnrealEngine5/comments/171zrqo/use_vr_controllers_to_move_my_actor_not_making_a",
        "document": "I want to use my Oculus Touch VR controllers to control my Actor. I don't want to use the headset at all.\n\nI don't want to make a VR game, I have a static camera and I want to control my Actor with the motion controls, that's it.\n\nDo I have to start with an Oculus game template and strip away all the things I don't need to get my motion controls setup? or can I add just that functionality somehow? I have the functionality all worked out I just can't figure how to get the Touch controllers in there.\n\n* Not sure if there is a difference, but I don't want people to be able to control this with VR controllers, it's not a game where I am worried about what controllers people will use. I want to be able to control my character with my VR controllers"
    }
]