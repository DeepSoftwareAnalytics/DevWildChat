[
    {
        "link": "https://postgresql.org/docs/current/datatype.html",
        "document": "PostgreSQL has a rich set of native data types available to users. Users can add new types to PostgreSQL using the CREATE TYPE command.\n\nTable 8.1 shows all the built-in general-purpose data types. Most of the alternative names listed in the “Aliases” column are the names used internally by PostgreSQL for historical reasons. In addition, some internally used or deprecated types are available, but are not listed here.\n\nEach data type has an external representation determined by its input and output functions. Many of the built-in types have obvious external formats. However, several types are either unique to PostgreSQL, such as geometric paths, or have several possible formats, such as the date and time types. Some of the input and output functions are not invertible, i.e., the result of an output function might lose accuracy when compared to the original input."
    },
    {
        "link": "https://postgresql.org/docs/current/datatype-datetime.html",
        "document": ""
    },
    {
        "link": "https://neon.tech/postgresql/postgresql-tutorial/postgresql-uuid",
        "document": "Summary: in this tutorial, you will learn about the PostgreSQL UUID data type and how to generate UUID values using a supplied module.\n\nUUID stands for Universal Unique Identifier defined by RFC 4122 and other related standards.\n\nA UUID value is a 128-bit quantity generated by an algorithm that makes it unique in the known universe using the same algorithm.\n\nThe following shows some examples of UUID values:\n\nA UUID is a sequence of 32 digits of hexadecimal digits represented in groups separated by hyphens.\n\nBecause of its uniqueness feature, you often find UUID in distributed systems because it guarantees a better uniqueness than the data type which generates unique values within a single database.\n\nTo store UUID values in the PostgreSQL database, you use the UUID data type.\n\nPostgreSQL provides you with a function to generate a UUID:\n\nThe function returns a version 4 (random) UUID. For example:\n\nWe will create a table whose primary key is a UUID data type. Additionally, the values of the primary key column will be generated automatically using the function.\n\nIn this statement, the data type of the column is .\n\nThe column has a default value provided by the function, therefore, whenever you insert a new row without specifying the value for the contact_id , PostgreSQL will call the function to generate the value for it.\n\nSecond, insert some data into the table:\n\nThe output indicates that the column has been populated by the UUID values generated by the function.\n\nUsing uuid-ossp module in the old version of PostgreSQL\n\nIf you use an old version of PostgreSQL, you need to use a third-party module uuid-ossp that provides specific algorithms to generate UUIDs\n\nTo install the module, you use the statement as follows:\n\nThe clause allows you to avoid re-installing the module.\n\nIf you want to generate a UUID value, you can use the function. For example:\n\nFor more information on the functions for UUID generation, check out the uuid-ossp module documentation.\n• Use the function to generate a version 4 (random) UUID."
    },
    {
        "link": "https://postgresql.org/files/documentation/pdf/10/postgresql-10-A4.pdf",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/12505158/generating-a-uuid-in-postgres-for-insert-statement",
        "document": "The answer by Craig Ringer is correct. Here's a little more info for Postgres 9.1 and later…\n\nYou can only install an extension if it has already been built for your Postgres installation (your cluster in Postgres lingo). For example, I found the uuid-ossp extension included as part of the installer for Mac OS X kindly provided by EnterpriseDB.com. Any of a few dozen extensions may be available.\n\nTo see if the uuid-ossp extension is available in your Postgres cluster, run this SQL to query the system catalog:\n\nTo install that UUID-related extension, use the CREATE EXTENSION command as seen in this this SQL:\n\nBeware: I found the QUOTATION MARK characters around extension name to be required, despite documentation to the contrary.\n\nThe SQL standards committee or Postgres team chose an odd name for that command. To my mind, they should have chosen something like \"INSTALL EXTENSION\" or \"USE EXTENSION\".\n\nYou can verify the extension was successfully installed in the desired database by running this SQL to query the system catalog:\n\nFor more info, see the Question: Default value for UUID column in Postgres\n\nThe Old Way\n\nThe information above uses the new Extensions feature added to Postgres 9.1. In previous versions, we had to find and run a script in a .sql file. The Extensions feature was added to make installation easier, trading a bit more work for the creator of an extension for less work on the part of the user/consumer of the extension. See my blog post for more discussion.\n\nBy the way, the code in the Question calls the function . This generates a type known as Version 4 where nearly all of the 128 bits are randomly generated. While this is fine for limited use on smaller set of rows, if you want to virtually eliminate any possibility of collision, use another \"version\" of UUID.\n\nFor example, the original Version 1 combines the MAC address of the host computer with the current date-time and an arbitrary number, the chance of collisions is practically nil.\n\nFor more discussion, see my Answer on related Question."
    },
    {
        "link": "https://cybertec-postgresql.com/en/subqueries-and-performance-in-postgresql",
        "document": "SQL allows you to use subqueries almost anywhere where you could have a table or column name. All you have to do is surround the query with parentheses, like , and you can use it in arbitrary expressions. This makes SQL a powerful language – and one that can be hard to read. But I don't want to discuss the beauty or ugliness of SQL. In this article, I want to tell you how to write subqueries that perform well. I'll start simple, but get to more surprising and complicated topics later.\n\nIn a subquery you can use table columns from the outside, like\n\nThe subquery will be different for each row in “ ”. Such a subquery is usually called a correlated subquery. An uncorrelated subquery is one that does not reference anything from the outside.\n\nUncorrelated subqueries are simple. If the PostgreSQL optimizer does not “pull it up” (integrate it in the main query tree), the executor will calculate it in a separate step. You can see that as an (initial plan) in the output of . Uncorrelated subqueries are almost never a performance problem. In the rest of this article, I will mostly deal with correlated subqueries.\n\nIf you write a subquery in a place in an SQL statement where you would otherwise have to write a single value, it is a scalar subquery. An example for a scalar subquery is the one in the previous section. A different example would be\n\nIf a scalar subquery returns no result, the resulting value is NULL. If the query returns more than a single row, you will receive a run-time error:\n\nA tabular subquery appears in a context where it can return more than one value:\n\nMy rule of thumb is: avoid correlated scalar subqueries whenever you can. The reason is that PostgreSQL can only execute a scalar subquery as a nested loop. For example, PostgreSQL will execute the subquery from the first section once for each row in table “ ”. This can be fine if “ ” is a small table (remember, my recommendation is just a rule of thumb). However, if table “ ” is large, even a fast subquery will make the query execution unpleasantly slow.\n\nRewriting a scalar subquery in the list or clause\n\nIf correlated scalar subqueries are bad for performance, how can we avoid them? There is no single, straightforward answer, and you probably won't be able to rewrite the query to avoid such subqueries in all cases. But usually the solution is to convert the subquery into a join. For our first query, that will look like this:\n\nThat query is semantically equivalent, with the exception that we don't get a run-time error if a row in “ ” matches more than one row in “ ”. We need an outer join to account for the case where the subquery returns no result.\n\nFor our second example, the rewritten query would look like this:\n\nHere, is the primary key of “ ”. Grouping by would not be sufficient, because two different rows from table “ ” could have the same value for .\n\nThe advantage of rewriting the queries as shown above is that PostgreSQL can choose the optimal join strategy and is not restricted to nested loops. If the table “ ” has few rows, that may not make a difference, since a nested loop join may be the most efficient join strategy anyway. But you also won't lose by rewriting the query in that case. And if “ ” is large, you will be much faster with a hash or a merge join.\n\nWhile correlated scalar subqueries are usually bad, the case is not so simple with tabular subqueries. Let's consider the different cases separately.\n\nThese cases are almost identical, because you can always rewrite a CTE to a subquery in unless it is a recursive, a or a data modifying CTE. CTEs are never correlated, so they are never problematic. However, a clause entry can be correlated in a lateral join:\n\nAgain, PostgreSQL will execute such a subquery in a nested loop, which can perform badly for large table “ ”. Therefore, it's usually a good idea to rewrite the query to avoid a correlated subquery:\n\nThe rewritten query will perform better if “ ” has many rows, but it could perform worse if “ ” is small and “ “ is large, but has an index on .\n\nThis is a special case. So far I have always recommended avoiding correlated subqueries. But with and , the PostgreSQL optimizer is able to transform the clause to a semi-join and anti-join, respectively. That allows PostgreSQL to use all join strategies, not only nested loops.\n\nConsequently, PostgreSQL can process correlated subqueries in and efficiently.\n\nThe tricky case of and\n\nYou will maybe expect that these two cases behave alike, but that is not the case. A query using with a subquery can always be rewritten to use . For example, the following statement:\n\nThe PostgreSQL optimizer can do that and will process the subquery in as efficiently as the one in .\n\nHowever, the case with is quite different. You can rewrite to similar to the above, but that is not a transformation that PostgreSQL can do automatically, because the rewritten statement is semantically different: If the subquery returns at least one NULL value, will never be TRUE. The clause does not exhibit this surprising behavior.\n\nNow people normally don't care about this property of (and in fact, too few people know about it). Most people would prefer the behavior of anyway. But you have to rewrite the SQL statement yourself and cannot expect PostgreSQL to do it automatically. So my recommendation is that you never use with a subquery and always use instead.\n\nSo far, I have told you how to rewrite an SQL statement to avoid forcing the optimizer to use a nested loop. Yet sometimes you need the exact opposite: you want the optimizer to use a nested loop join, because you happen to know that that is the best join strategy. Then you can deliberately rewrite a regular join to a lateral cross join to force a nested loop. For example, this query\n\nIf you want good performance with subqueries, it is often a good idea to follow these guidelines:\n• use uncorrelated subqueries as much as you like, as long as they don't make the statement hard to understand\n• avoid correlated subqueries everywhere except in , and clauses\n\nDon't take these rules as iron laws. Sometimes a correlated subquery can actually perform better, and sometimes you can use a correlated subquery to force the optimizer to use a nested loop if you are certain that is the right strategy to use.\n\nIf you are interested in improving the performance of a query by rewriting it, you may want to read my article about forcing the join order in PostgreSQL.\n\nIn order to receive regular updates on important changes in PostgreSQL, subscribe to our newsletter, or follow us on Twitter, Facebook, or LinkedIn."
    },
    {
        "link": "https://stackoverflow.com/questions/34969720/optimizing-sql-query-with-multiple-joins-and-grouping-postgres-9-3",
        "document": "\n• None More normalization in order to get smaller tables and indexes and faster access. Create lookup-tables for , and and only write a small integer ID in .\n• None Seems like the columns .. and could be , but are defined as ?\n• None The column seems to be , but is defined as .\n• None I also changed the order of columns to be more efficient. Details:\n\nBuilding on above modifications and for Postgres 9.3:\n• None Since is not defined , it's probably wrong to it. It's also inefficient. Use instead (and join to later if you need the name in the result).\n• None All instances of are pointless, since you have predicates on the left tables anyway and / or use the column in .\n• None Parentheses around -ed conditions are not needed.\n• None In Postgres 9.4 or later you could use the new aggregate syntax instead. Details and alternatives:\n• How can I simplify this game statistics query?\n\nThe partial index on you already have should look like this to allow index-only scans:\n\nBut from what I see, you might just add a column to and drop the table completely (unless there is more to it).\n\nAlso, that index is not going to help much, because (telling from your query plan) the table has 800k rows, half of which qualify:\n\nThis index on will help a little more (later) when you have more different matchtypes and matchversions:\n\nStill, while 100k rows qualify out of 400k, the index is only useful for an index only scan. Otherwise, a sequential scan will be faster. An index typically pays for about selecting 5 % of the table or less.\n\nYour main problem is that you are obviously running a test case with hardly realistic data distribution. With more selective predicates, indexes will be used more readily.\n\nMake sure you have configured basic Postgres settings like or etc.\n\ngoes without saying. This is only turned off for debugging or locally as a desperate measure of last resort."
    },
    {
        "link": "https://dba.stackexchange.com/questions/340053/postgresql-query-optimization-over-multiple-tables-with-moderately-large-data",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://thoughtbot.com/blog/advanced-postgres-performance-tips",
        "document": "You’ve added the es, both partial and covering. You’ve d. You ed and ed everything to a single query. And yet, your report is still taking too long. What do you do when the low-hanging fruit has been harvested?\n\nIt’s time to get down and dirty with some of the lesser known SQL constructs in Rails land.\n\nIf you’ve gotten this far, you’re probably familiar with using and to get insight into what approach Postgres is taking to execute queries and the actual performance of those approaches. You know that an Index Scan is preferable to a Seq Scan, but you’ll settle for a Heap Scan as a Join Cond.\n\nQuery plans aren’t the easiest thing to read, however. They’re packed with information and it’s closer to being machine parsable than human readable.\n\nPostgres Explain Viewer (PEV) is a tool to simplify reading query plans. It provides a horizontal tree with each node representing a node in the query plan. It includes timing information, the error amount in the planned versus actual times, and badges for interesting nodes like “costliest” or “bad estimate”.\n\nIt’s pretty useful to have at-a-glance feedback about your query. But knowing how long queries take is just the beginning: what can we do to speed these up?\n\nViews are a tool for storing “partial queries”. That is, they allow the database to store a parsed query that you can treat as a table in most respects later. You can SELECT (and sometimes UPDATE or DELETE) from a view with identical syntax as if you were executing the statement against a table. They’re useful when you’d like to perform a complex statement repeatedly and don’t want to deal with cluttering your Ruby files with arcane ARel or strung-together scopes.\n\nA view can be materialized, which means the results are stored by Postgres at and time. The cost of the partial query is paid at these times, so we can benefit from that over and over, especially in read-heavy situations (most situations are read-heavy in my experience).\n\nMaterialized views are especially helpful when your select list includes a subset of columns, you perform identical operations such as COUNT or SUM or extracting part of a jsonb object on every query, and when JOINing additional tables. When it comes time to actually retrieve rows, these rows can be queried against to return only relevant data. This is often cheaper to execute than a full statement. These can be further beneficial by creating indices on the materialized views themselves (such as on the column you’re JOINed by, or on a data column for a reporting query).\n\nAs an aside, Ruby on Rails does not have first-class support for views, despite that from a usage perspective they’re very similar to tables. We maintain a Rails extension, Scenic, which helps with versioning, migrating, and maintaining SQL views.\n\nThe 37 minute query plan above can’t be improved on by a materialized view, unfortunately, because there aren’t any good candidates for caching partial results. This was an exceptional case and I was surprised to find that this feature wouldn’t be helpful to me.\n\nCommon Table Expressions such as and Subqueries such as are tools for breaking up complex SQL queries, and sometimes the only way to achieve a goal. While CTEs are arguably easier to read than subqueries, in Postgres they are an “optimization fence”, preventing the query optimizer from rewriting queries by moving constraints into or out of the CTE. For example this query:\n\nThe query could be optimized by “pushing” the WHERE clause into the subquery to avoid performing the read and count operations for every row in big_table:\n\nHowever, a CTE such as this would prevent the optimization, causing the entire table to be read, aggregated, and materialized to disk before being re-scanned from the materialized table for the constraint:\n\nThere are also times when the optimization fence is useful, such as when using data-modifying statements (INSERT, UPDATE, or DELETE) in WITH. As the CTE is only executed once, the result is the same where a subquery is allowed to be called multiple times by the planner and would not return information from deleted or updated rows.\n\nBoth Common Table Expressions and subqueries are useful, and one or the other may be more performant in a specific case. This is one example where subqueries are the better option, but I usually find that a CTE is as faster or better than a subquery and lean on them most of the time. Experiment with both forms in EXPLAIN ANALYZE to find the right tool for your query.\n\nControl yourself; Take only what you need from it\n\nThe common answer to why we should rather than is that we reduce network traffic. In most cases, for reasonably normalized databases, and with today’s high-speed network connections, this is unlikely to be a huge deal. In one example involving 326/89K rows, the difference in network traffic caused by selecting a single column versus all columns was about 10%.\n\nA better reason to limit columns to only what’s needed is index lookups. We want the planner to hit indexes instead of doing a sequential scan of the table itself. We generally optimize this by creating indexes on important columns such as foreign keys and of course primary keys come with an index by default. These are great, helping to identify the disk location of rows that match whatever constraints we have in a query for easier lookup or ordering.\n\nWe can also use covering indexes, which include the specific columns and expressions useful to a query, to store all of the relevant information. This prevents the second step of reading from the table itself and can provide a big performance gain. Using as the select list in a query would likely not make use of a covering index unless you index the entire table and keep it up-to-date with added columns.\n\nPrepared statements split the work of parsing, analyzing, rewriting, planning, and executing a statement in the same way that materialized views split the work of preparing parts of queries that don’t change by caching their results. When a statement is prepared, Postgres parses, analyzes, and rewrites it. It generally uses placeholders for values being provided at EXECUTE time.\n\nPREPARE is an optimization for the very specific use-case of similar statements being executed many times in a single session. Prepared statements are not persisted or shared between sessions, so they’re not something you can use to optimize a general use without setting up session connect events externally.\n\nSince we previously mentioned that Rails does not support views, it’s only fair to point out that since version 3.1 it does make use of prepared statements.\n\nDo more in the database\n\nThe ultimate Postgres performance tip is to do more in the database. Postgres is optimized to be very efficient at data storage, retrieval, and complex operations such as aggregates, JOINs, etc. Let your web application deal with displaying data and your database with manipulating and converting data. Becoming comfortable with using Postgres for even the most complex reports starts with familiarity with using it, from its syntax to its functions and types. Read the Postgres documentation as you do the Ruby, Go, or Elixir docs."
    },
    {
        "link": "https://ashimabha-bose328.medium.com/supercharge-your-sql-complex-joins-with-postgresql-8989da8855c2",
        "document": "In the world of SQL, the ability to perform complex joins is a superpower that empowers you to extract valuable insights from your data. PostgreSQL, with its robust set of join capabilities, offers you the tools to supercharge your SQL queries. In this comprehensive guide, we’ll dive deep into complex joins and explore various scenarios where they can be a game-changer.\n\nSection 1: Self-Joins — Looking in the Mirror\n\nSelf-joins occur when a table is joined to itself. It’s a technique often used to represent hierarchical data or relationships within the same table. Consider an employee table where each row includes a manager ID, enabling us to create a self-join to find employees and their respective managers.\n\nSelf-joins are valuable for tasks like creating organizational charts or analyzing hierarchical structures."
    }
]