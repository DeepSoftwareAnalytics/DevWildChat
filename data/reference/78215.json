[
    {
        "link": "https://netsolcloudservices.com/blog/stable-diffusion-based-solution-for-medical-imaging",
        "document": "Thanks to the power of artificial intelligence (AI) and generative algorithms, the world is experiencing remarkable transformation and advancements in the medical imaging field. One particularly cutting-edge approach to making waves is “Stable Diffusion-Based Solutions.” This fantastic technique could revolutionize capturing, processing, and generating medical images. In this blog post, we’ll deeply dive into the fascinating world of stable diffusion-based solutions and explore how they work alongside generative AI to take medical imaging to the next level.\n\nBefore diving into the applications of stable diffusion in medical imaging, it is crucial to understand the complex mathematical concept behind it. Stable Diffusion is an innovative Generative AI-based model that enables users to generate captivating, high-quality images, videos, and animations from simple text and image prompts. Powered by state-of-the-art diffusion technology, this model produces photorealistic outputs that are both unique and visually stunning. So, what’s the technology? Stable Diffusion uses latent space, significantly reducing processing requirements and making running the model on any desktop or laptop equipped with GPUs possible. Moreover, the model is highly customizable and can be fine-tuned to meet specific needs. In the context of healthcare, Stable diffusion is transforming data in a controlled manner using carefully crafted noise. This process is gradual and carefully executed to generate high-quality, realistic images. The noise added to the image is designed to be non-disruptive, and its addition is carefully controlled to achieve the desired outcome. Stable diffusion is a valuable tool for generative AI because it generates hyper realistic medical imaging without compromising quality. The industry can better diagnose patients treated with various critical conditions with high-quality medical imaging.\n\nStable diffusion and its powerful medical imaging are game changers for the healthcare sector. Here’s how both are stirring a revolution, assisting healthcare professionals in diagnosing patients accurately. Medical imaging datasets are limited and thus require generating diverse datasets through stable diffusion and generative AI models. This approach adds contrast to images, improving the handling of real-world variations while adding model reliability and accuracy. Achieving precision in the medical field is paramount, as even a slight error can have drastic consequences. Stable diffusion offers the attainment of accurate results through noise regularization. The images produced become more stable and reliable, allowing for better interpretation and diagnosis, ultimately reducing uncertainty and enhancing the process of accurate diagnoses. Enhancing and generating images is of immense value. With advancements in stable diffusion and generative AI, researchers and clinicians can now improve the quality of medical images to an unprecedented level. It makes it easier to identify subtle complexities and anomalies that might have gone unnoticed previously. Stable diffusion-based generative AI models have proven to be a valuable tool for medical professionals by analyzing incomplete or low-quality scans. These models can generate synthetic images that accurately depict the medical condition being examined, allowing medical practitioners to make more informed diagnoses and monitor the progression of medical conditions with greater accuracy and confidence.\n\nMedical imaging is an indispensable component of modern healthcare, enabling physicians to detect and diagnose various diseases more accurately. Over the past few years, generative models such as GANs and VAEs have been used for producing realistic medical images. Despite their potential, these models’ limitations hinder them from producing the desired result. Medical professionals require a reliable and efficient generative model that can produce realistic medical images to enhance their ability to analyze, diagnose, and treat diseases. The healthcare sector utilizes stable diffusion to generate medical images with remarkable precision, realism, and high-quality pictures with intricate spatial details. In this case, the stable diffusion model used the Fokker-Planck equation to simulate the diffusion process of the image’s probability density function (PDF). A diffusion kernel was applied to the PDF at each time step to generate a new image. The kernel captured spatial information and learned from data during training. Stable diffusion generated high-quality and diverse images by accurately capturing complex spatial information and overcoming the mode collapse problem through diffusion. The combination of stable diffusion-based solutions and generative AI is leading to significant advancements in medical imaging. It is expected to witness breakthroughs in the accuracy and speed of diagnostic imaging and improvements in the detection and treatment of diseases. By utilizing stable diffusion, it is possible to create 3D medical images that are realistic and highly accurate in their depiction of various anatomical structures. This advanced technique enhances the diagnostic capabilities of medical professionals by providing a more detailed and comprehensive view of the affected areas, ultimately leading to better patient outcomes. Generating accurate training data for AI models is crucial in healthcare. However, obtaining precise patient data can be daunting due to several reasons, such as privacy concerns and the limited availability of data. To overcome this challenge, stable diffusion techniques can be used to generate synthetic medical images that can be used for training AI models. These techniques can produce high-quality photos with realistic features, providing a reliable alternative to real patient data. By utilizing custom, synthetic images based on individual patient data, medical imaging can be tailored to the specific healthcare needs of each patient. This approach enhances the accuracy of diagnoses and treatment plans and ensures that patients receive the most effective and efficient care possible. With personalized medicine, patients can be confident that their healthcare is optimized for their unique needs and circumstances. Medical professionals can provide faster and more accurate diagnoses due to technology that has led to enhanced image quality and noise reduction. These improvements allow for a more precise and detailed view of the body’s internal systems, enabling doctors to identify potential health issues quickly and precisely. Fusion with Brighter Future for Better Medical Advancement The integration of stable diffusion-based solutions with generative AI is revolutionizing the field of medical imaging. This cutting-edge technology has the potential to substantially improve the accuracy of diagnoses, enhance the quality of images, and provide new and innovative ways to address healthcare challenges. As researchers and AI developers continue to push the boundaries of what is possible, we can expect a future where medical imaging becomes even more powerful, versatile, and indispensable, ultimately benefiting patients and medical professionals. Interested in knowing more about how Generative AI and Stable Diffusion can change the game for the healthcare sector? Get in touch with our NETSOL representative and learn more!"
    },
    {
        "link": "https://nature.com/articles/s41746-024-01061-4",
        "document": "How to read the algorithm journey map\n\nThe algorithm journey map is organized around four stages based on related work defining algorithm adoption stages17. The four lifecycle stages are:\n• Problem identification: How the organization identified sepsis as a problem that needed to be addressed and why a solution that uses AI is the best approach to address the problem. This stage ends with investing resources to build a sepsis AI tool.\n• Development: The building of the sepsis AI tool, preparing the clinical environment in which it operates, and designing the user interface and user experience. This stage ends with the decision to integrate the AI tool into clinical care. This stage zooms into two sub-stages, which are:\n• Model build and validation: building and validating a machine learning model on retrospective data\n• User interface build and user experience design: defining and developing the user interface and user experience.\n• Integration: Integrating the sepsis AI tool into the clinical environment and ends with a decision to continue using the sepsis AI tool after initial integration. This stage zooms into two sub-stages, which are:\n• Technical integration: integrating the technology into legacy systems and creating a way for the sepsis AI tool to run on real-time data\n• Clinical integration: integrating the sepsis AI tool into the clinical workflow\n• Lifecycle management: This stage describes post-rollout activities to manage, maintain, evaluate, and update the sepsis AI tool. This stage continues for as long as the AI tool is used in clinical care. It also includes monitoring the appropriate use of the tool and ensuring its decommissioning is initiated if it becomes obsolete or irrelevant.\n\nA full list of stakeholders mentioned throughout the algorithm journey map is identified in Table 1. We use the traditional event shapes from the swimlane literature—start/stop (oval), action (rectangle), and decision (diamond) using their canonical shapes from process maps18—and supplement them with some additional markers, all shown in Fig. 1. We introduce light bulb icons to denote ‘lessons learned’ that were identified by participants during interviews and dotted gray circles to denote “the path not taken” from each decision point. Due to the complexity of the multi-year effort, some processes are broken down into sub-processes; if a sub-process is complex and distinct enough we represent it with a green box and its own standalone map, whereas if it is small enough then we embed it in the original map but with a dotted blue line border.\n\nAn explanation of how we created the algorithm journey map below is provided in the “Methods” section later in the paper.\n\nFigure 2 shows the process of identifying and prioritizing the problem that led to the development and adoption of a sepsis AI tool. This process began in the fall of 2015 when health system leaders launched an innovation competition (i.e. the Request for Applications (RFA) process) that featured a strategic priority to reduce inpatient mortality. A small group of clinicians applied to the innovation competition and proposed to use machine learning to predict sepsis.\n\nThe proposal to develop a sepsis AI tool was selected by health system leaders for funding. Resources were allocated to pursue the opportunity, and staff from an internal innovation team were embedded in the project. In 2016, machine learning expertise within the school of medicine was limited and there was no mechanism for faculty in quantitative sciences departments to directly collaborate on operational health system projects. Health system leaders worked with the vice provost for research to establish a process whereby a statistics faculty and graduate student could dedicate effort to the sepsis AI tool project. The project team featured clinicians across specialties, project management, and statistics and machine learning expertise.\n\nFigure 3 shows the development stage. The innovation team project manager, in consultation with the clinical champion, guided the project through the many steps. During this stage, the clinical champions defined project goals and requirements, including:\n• How is sepsis defined (e.g. CDC criteria, CMS criteria, presence of an ICD code)?\n• What data elements are important for the predictive model?\n• Who is the user (e.g. attending, resident, bedside nurse, rapid response team)?\n• Which patients is the model run on (e.g. emergency department, all floors of main hospital, main hospital and regional partner hospital, ICU, non-ICU)?\n\nThe IRB reviewed the project to approve the development of the algorithm on retrospective data and granted a waiver of consent to use patient data for model development.\n\nAs will be described in the following sections, the above decisions guided the design and development of the sepsis AI tool and had significant downstream implications. Once it was decided that the rapid response team (RRT) nurse would be the primary user of the tool, the project team needed to ensure that organizational priorities would incentivize the tool to actually be used. RRT nurses were historically cardiac critical care nurses who supported care in the cardiac ICU when not responding to urgent events. These nurses reported to the cardiology service line, which was not primarily responsible for sepsis care quality. The project team worked with hospital leaders to create a new structure–the patient response program–that would house the RRT nurses and become responsible for sepsis care quality. During this restructuring, the clinical champion for the project became the patient response program director. These changes aligned RRT nurse management incentives with the objectives of the sepsis AI tool to improve sepsis care.\n\nFigures 4 and 5 detail the development of the sepsis predictive model and UI design, respectively. These processes are described in more detail below. Once these prototypes were built, department physician leadership reviewed the progress and approved moving forward with the integration process.\n\nFigure 4 details the steps involved in building a machine-learning model on retrospective data. These steps are likely very familiar to machine learning model developers. After the team received cuts of historical data, the project manager worked with the clinical champions to clean the raw data. This included both grouping related raw elements (e.g., arterial blood pressure, blood pressure measured from left arm cuff) and performing quality checks to ensure the data aligns with clinical expectations. The data quality activities conducted for this project in 2016 - 2017 laid the foundation for a data quality assurance framework that was formally validated at a later date19.\n\nOnce the data engineer grouped and cleaned the data, the statisticians on the team built a machine-learning model, evaluating and refining it until it achieved sufficient performance on unseen data. The statistician then reviewed the output of the model with the clinical champion, both with summary statistics and chart reviews to assess whether the model was ready to move forward.\n\nIn parallel to the model building and validation sub-phase, Fig. 5 outlines the development of the UI design. This was an iterative process that began with scoping what the tool can help with based on the status quo workflow for delivery of care (i.e., reacting to sepsis once the patient already starts to deteriorate) and the general capabilities for what the tool can do (i.e., forecast who is at risk for deteriorating in the next N hours).\n\nNext, there was an iterative design process where the UI designer would prototype ideas and discuss them with the end user (RRT nurses) for refinement. For instance, the original goal of the tool was just to flag high-risk patients in a dashboard, but the RRT nurses communicated that it would be even better if the tool helped them not just detect but also manage interventions to treat sepsis. That feedback resulted in reconceptualizing the AI tool as a “workflow tool” and not a dashboard. The UI designer and RRT nurse agreed on a workflow with four patient states (Triage, Screened, Monitoring, and Treatment) and the user would move the patients through the process as sepsis is detected and managed. This functionality could not be implemented in the hospital’s electronic health record at the time, so the team made the decision to develop an initial UI as a custom web application.\n\nAnother such example of iterative feedback involved model output visualization. Initially, a given patient’s predicted probability of sepsis was going to be plotted over time (to help remind the user of the patients they were keeping an eye on). However, after some feedback sessions with users, the UI designers began to worry that the users would use the trajectory/trend as an indicator, itself, and begin to over-rely on it. They concluded that such a scenario would require additional training for users to understand how to interpret time-based plots, so instead they focused on point-in-time visualizations to more closely match the setting the model was trained on without as large a risk of user misconceptions.\n\nOnce the iterative feedback was incorporated into the design, the prototype was presented to the clinical champion to ensure that the tool would be aligned with the project’s goals. In this case, the goal was both early identification of sepsis as well as timely treatment once identified. Treatment was to be measured based on sepsis bundle compliance as defined by SEP-1 sepsis bundle regulations issued by CMS20.\n\nFigure 6 visualizes the next stage, integration. Integration contains two sub-stages, technical integration, and clinical integration, which are described in more detail below.\n\nFigure 7 displays the process for technical Integration. It begins with extensive collaboration between the innovation team and the health system IT department. The teams navigated the tension between developing a fully customized solution, which would have higher maintenance and ownership costs and relying fully on existing tools, which would have lower maintenance and ownership costs. A major question that had to be addressed was whether the model could be integrated into Epic via its Cognitive Computing Platform (https://www.healthcareitnews.com/news/epic-cognitive-computing-platform-primer). Over a period of 6 months, the two teams conducted due diligence on the Epic solution and determined it was not able to run the sepsis AI tool. The teams agreed to develop a custom solution that extracts data from the EHR, pipes it to a server that runs the model, and sends those predictions to a database that displays results on a custom web application.\n\nThe IT and innovation teams built a data pipeline to extract data out of Epic’s Chronicles database in real-time. This required IT to build web endpoints to supply Epic data and the innovation team to build a schema for organizing the data that was received. Additionally, there needed to be resources for the server and database. A few months of testing were done to ensure the system could handle the volume of data being extracted, particularly because vitals are collected very frequently. Once IT signed off on the data pipeline, the sepsis AI tool was configured to pull real-time data once every 5 min. In addition, the innovation team built monitoring tools to regularly test the input/output connections and measure the volume of inputs.\n\nOnce the data pipeline was functioning, the innovation team submitted another study proposal to run a ‘silent trial’ to enable end-to-end system monitoring and testing. During this IRB review, the innovation team met repeatedly with regulatory affairs leadership to ensure that the sepsis AI tool aligned with the FDA’s definition of clinical decision support (CDS). Specifically, the relevant standards were based on the FDA’s “Clinical and Patient Decision Support Software” draft guidance which was posted in December 2017 (https://www.regulations.gov/document/FDA-2017-D-6569-0002). Because the tool did not make clinical decisions or treatment recommendations and supported independent review by clinicians, regulatory leaders determined that the technology qualified as non-device CDS. Once the ‘silent trial’ was approved, the innovation team conducted user testing to get feedback about the UI and the performance of the model. Once the user was satisfied with the changes, the innovation team presented the functioning tool to department physician leadership for approval. The clinical integration process began after approval.\n\nFigure 8 visualizes the clinical integration process. This involved fine-tuning the workflow and user interface, developing training material, and assembling a governance committee. This began with two parallel processes, one for physicians and one for nurses.\n\nThe ED physician leadership finalized some decision points that hadn’t been fully specified, such as who the RRT nurse should call when the model predicts a high risk of sepsis (call the attending, not the resident), who will administer treatment (the bedside nurse, not the RRT nurse), etc. Additionally, there were some final suggestions for marketing and communications, such as removing any reference to a “code sepsis.”\n\nAt this stage, the innovation team met with the chief nursing officer for the health system to discuss the rollout of the sepsis AI tool. During this initial meeting, it became clear that there were additional stakeholders who needed to be engaged before the AI tool could be launched. Up until this point, the project team had been working primarily with physician leaders at both the hospital and department levels. Unfortunately, this left out nurse leaders at the health system level and within relevant service lines (e.g., emergency department) who needed to deploy resources to support the rollout. To address this lack of communication, the chief nursing officer convened a meeting with the innovation team, hospital nurse leaders, ED nurse leaders, and certified nurse educators (CNEs) to map out steps leading up to roll-out. Working closely with nurse stakeholders, several adaptations were made to the workflow, including direct communication between the RRT nurse and bedside nurses in the ED.\n\nThe innovation team worked with CNEs to develop training material, particularly for RRT nurses, to equip new users to appropriately use the sepsis AI tool. Finally, a governance committee was established including stakeholders from both physician and nursing leadership to meet monthly during the initial rollout in order to resolve any emergent issues.\n\nUnlike prior stages, post-rollout lifecycle management is not a linear-flow process. Some tasks are predictable whereas others are responsive to events that occur (e.g., user requests, technical failures, etc). Instead of employing a swimlane-oriented diagram to convey lifecycle management, Fig. 9 shows a variety of different activities, categorized by both type of task (monitoring, updates, and operational management) and frequency (one-off, semi-recurring, recurring, and event-based). These activities also involve many stakeholders, principally driven by the project manager and clinical champion.\n\nMonitoring involves both regular dev-ops duties (e.g., is the system still up? Were there large changes in data volume?) as well as periodic, thorough data science analysis (e.g., does the model still perform well? Have clinical outcomes improved?). Monitoring and evaluation are a technical component of system audits, assessing model performance as clinical outcomes, and process measures over time. These audits have also involved user research, such as shadowing the RRT nurses who use the tool. Additionally, CMS requires hospitals to conduct external auditing of bundle compliance in order to maintain eligibility for Medicare payments.\n\nUpdates are performed both as needed (event-based, semi-regularly) and on a regular schedule (recurring) in order to ensure the robust performance of the sepsis AI tool. An example of a recurring update that occurs every 6 months is a coordinated effort between the innovation team and clinical champions to refresh data element “groupers.” This process ensures that any new medications, vital sign monitors, or laboratory equipment are accounted for in the data pipeline. An example of a one-off update responding to a user request was adding new functionality to automatically identify treatment bundle compliance in real-time. Although there is no formal process, when users request new features, the project team must categorize the feature as either an ‘update to the existing product’ or a ‘new project that needs separate, dedicated effort.’ An example of a task that spun off into a standalone product is the alert notification system that now supports many AI tools.\n\nOperational management involves ongoing ownership and accountability. The sepsis AI tool has 2 “owners”—the clinical champion and the innovation team project manager—who communicate with each other and liaise with additional members to support the sepsis AI tool as needed. They periodically need to secure funding and resources for the project and assess how well the solution is addressing the original objective. The project owners also ensure that existing and new staff are appropriately trained and that training material is maintained to reflect evolving standards of care for sepsis. Additional entities also play an active role, such as how institutional leaders are now seeking input from the FDA to ensure that the use of the tool continues to comply with the intention of non-device CDS in light of the 2022 final CDS guidance (https: //www.fda.gov/regulatory-information/search-fda-guidance-documents).\n\nThroughout the construction of the algorithm journey map, we asked interviewees to identify not just what happened but also what they might have done differently in retrospect. This includes both narrow/technical and broader opportunities, and such reflections were indicated in the algorithm journey maps with lightbulb icons. The icons were separated into three categories: modeling assumptions (red), stakeholder inclusion (yellow), and organizational structure (blue). Below, we highlight specific learnings from the journey map and also abstract generalizable insights that can inform other efforts to develop and integrate AI into clinical care.\n\nThere were multiple areas where technical decisions about the model hampered the project. Most of these decisions occurred early in the algorithm journey:\n• [Development] Scoping the solution: Early on, clinical collaborators at that time felt the sepsis AI tool would not be used in the ICU, so data was thus truncated at the time of ICU transfer. This single decision limited the future ability to expand the use of the sepsis AI tool beyond the ED to general inpatient wards. When a user requested to expand the use of the AI tool to inpatient wards, a new “2.0” project had to be initiated. Generalizable insight: Carefully consider the downstream impact of inclusion and exclusion criteria applied at the level of patients and individual data values. If the use of an AI tool may extend to adjacent use cases, make sure that relevant data is included in model training and evaluation.\n• [Model Building] Outcome definition: The project team did not initially appreciate the difficulty in finding the “right” definition of sepsis. Physicians had differing opinions about which outcome to use, and the published literature didn’t show consensus. Modeling became easier once the outcome definition was modularized, allowing for easily changing the criteria and retraining the model. Generalizable insight: Do not limit outcome labels to single sets of criteria and develop (and validate) models for multiple types of definitions. Even if there is a professional consensus today on how a disease is defined, anticipate future changes.\n• [Model Building] Real-time access to model inputs: When determining which data elements to include as inputs for the model, the team had not initially considered that any data for the model needed to not just be captured in the EHR but also available in real-time. Epic’s backend databases involve both a real-time feed of the current day and a historical archive, and access to real-time data requires the IT department to build specific data endpoints for each kind of element. Generalizable insight: Only include data elements in an AI tool if the data is available and robustly captured at the time predictions need to be made.\n• [Model Building] Environment constraints on model: Initial versions of the model involved a technically complex Multi-task Gaussian Process for data imputation, followed by an LSTM classifier. The plan had been to integrate this model into production, but during technical integration, the team realized this approach required matrix inversion and significant computation. Eventually, the team used fill-forward data imputation for the LSTM, but it should have been knowable at the time that the runtime environment would limit expensive model architectures and decisions. Generalizable insight: Plan for ablation studies that evaluate the impact of removing model components or input features. When building an AI tool for integration, reduce unnecessary complexity.\n\nBeyond technical lessons, there was another ‘obvious’ insight from the mapping exercise. Many decision points throughout the process (e.g., problem formulation, workflow design, signing off with integration) were shaped and approved by hospital and department leaders who were physicians, but not nurses. This culminated in the clinical integration stage being nontrivially complex and stressful. This oversight also created tension between different clinical stakeholders that needed to be addressed leading up to a large project launch.\n\nAlthough the RRT nurse users were included in the early designs, it was not well understood that physicians and nursing leaders within service lines and hospitals manage separate activities. The innovation team needed to be directly engaging leaders across both chains of command, rather than expect communication between the two groups. As a result of this oversight, the clinical integration stage involved meeting many levels of nursing leadership (health system-level, hospital-level, and department-level) as well as directly engaging certified nurse educators to finalize programmatic decisions and develop training material.\n\nThe yellow light bulb icons indicate all of the opportunities where nurse leadership could be (or eventually was) involved in the project approval.\n\nThe generalizable insight from this lesson is to identify up-front the reporting structures, training requirements, and communication channels for all clinical professions affected by an AI tool put into practice. Even if clinicians across professions appear to work closely together in the same unit, reporting, training, and communication channels may be distinct. Project leaders also cannot assume that information shared with front-line workers or business-unit leaders is shared upwards within reporting structures. Executive leaders need to be informed and have their concerns addressed prior to the integration of new AI tools.\n\nOne final set of learning opportunities comes from identifying commonalities around structures and workflows. These events were not about what should have been done differently in the moment but instead flagged organizational changes that took a great deal of effort and could be streamlined. The following learnings are highlighted with blue lightbulb icons:\n• [Problem Identification] Recruit statisticians: The innovation team partnered with a faculty and Ph.D. student from the statistics department of our organization’s university. However, there was no mechanism for research faculty to dedicate time to health system projects. The Vice Provost for Research at the university helped facilitate the collaboration. Since that time, the innovation team has grown significant internal machine learning expertise in order to move more quickly on projects. Generalizable insight: Ensure senior-level support to engage perceived outsiders in the development of AI tools used in clinical care. Even if technical expertise exists within the organization, trust must be established between senior technical and clinical leaders.\n• [Development] Create a patient response program: As discussed in greater depth earlier, the project team needed to ensure that organizational incentives enabled RRT nurses to prioritize the use of the tool. This involved working with hospital leaders to move RRT nurses out of cardiology and into a new structure that became responsible for sepsis care quality. This effort was critical to ensure alignment in organizational priorities because otherwise, busy nurses would likely not consistently be able to make time to use (let alone offer feedback about) the sepsis AI tool. Generalizable insight: Invest time and energy in modernizing the organization to most effectively utilize emerging technologies like AI. In cases where an AI tool does not fit seamlessly within workflows or professional roles, the project team may need to be empowered to drive organizational change.\n• [Clinical Integration] Workflow burden: During the rollout, iPads were prepared for both the RRT nurses and also patient workflow coordinator (PWC) nurses in the ED. However, after a few months post-clinical integration, the innovation team learned that the patient workflow coordinators were too busy with other duties to be using the sepsis AI tool; the model predictions were not as critical to PWC nurses’ immediate priorities. This process could have been streamlined by having regular check-ins with front-line workers, in addition to managers, to surface friction on the ground. Generalizable insight: Adapt the workflow to the needs of front-line workers and build flexibility into early pilots. Being able to respond to feedback also builds trust among front-line workers."
    },
    {
        "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10993141",
        "document": "Generative artificial intelligence tools and applications (GenAI) are being increasingly used in health care. Physicians, specialists, and other providers have started primarily using GenAI as an aid or tool to gather knowledge, provide information, train, or generate suggestive dialogue between physicians and patients or between physicians and patients’ families or friends. However, unless the use of GenAI is oriented to be helpful in clinical service encounters that can improve the accuracy of diagnosis, treatment, and patient outcomes, the expected potential will not be achieved. As adoption continues, it is essential to validate the effectiveness of the infusion of GenAI as an intelligent technology in service encounters to understand the gap in actual clinical service use of GenAI. This study synthesizes preliminary evidence on how GenAI assists, guides, and automates clinical service rendering and encounters in health care The review scope was limited to articles published in peer-reviewed medical journals. We screened and selected 0.38% (161/42,459) of articles published between January 1, 2020, and May 31, 2023, identified from PubMed. We followed the protocols outlined in the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines to select highly relevant studies with at least 1 element on clinical use, evaluation, and validation to provide evidence of GenAI use in clinical services. The articles were classified based on their relevance to clinical service functions or activities using the descriptive and analytical information presented in the articles. Of 161 articles, 141 (87.6%) reported using GenAI to assist services through knowledge access, collation, and filtering. GenAI was used for disease detection (19/161, 11.8%), diagnosis (14/161, 8.7%), and screening processes (12/161, 7.5%) in the areas of radiology (17/161, 10.6%), cardiology (12/161, 7.5%), gastrointestinal medicine (4/161, 2.5%), and diabetes (6/161, 3.7%). The literature synthesis in this study suggests that GenAI is mainly used for diagnostic processes, improvement of diagnosis accuracy, and screening and diagnostic purposes using knowledge access. Although this solves the problem of knowledge access and may improve diagnostic accuracy, it is oriented toward higher value creation in health care. GenAI informs rather than assisting or automating clinical service functions in health care. There is potential in clinical service, but it has yet to be actualized for GenAI. More clinical service–level evidence that GenAI is used to streamline some functions or provides more automated help than only information retrieval is needed. To transform health care as purported, more studies related to GenAI applications must automate and guide human-performed services and keep up with the optimism that forward-thinking health care organizations will take advantage of GenAI.\n\nGenerative artificial intelligence tools and applications (GenAI) systems automatically learn patterns and structures from text, images, sounds, animation, models, or other media inputs to generate new data with similar characteristics [1]. GenAI is used to search, write, and create models, computer codes, and art forms without human assistance. GenAI has emerged significantly in the current decade to help every industry through different products such as ChatGPT, Bing Chat, Bard, LLaMA, Stable Diffusion, Midjourney, and DALL-E [2-5]. Almost all industries share an optimistic vision, with significant investment in using GenAI to transform aspects of value chains [6-10]. However, similar to many other technology hypes, whether this optimism will translate to value outcomes or be a “fad or fashion” remains to be tested over time. The adoption of GenAI in health care is emerging. Studies point to the use of GenAI in service interactions involving breast cancer diagnoses [11], bariatric surgery [12], cardiopulmonary resuscitation [13], and breast cancer radiologic decision-making [14]. GenAI has the potential to transform by performing tasks at higher quality than humans, which may reduce errors associated with humans in expert domains such as cancer detection [15] and neurological clinical decisions [16]. The rise of GenAI is also referred to as the “second machine age” [17], whereby “instead of machines performing mechanical work they are taking on cognitive work exclusively in the human domain” [17]. Although these instances are encouraging, how exactly GenAI helps in health care processes needs to be articulated and evaluated to provide an understanding of use and value linkages [18,19]. Thus, we asked the following research questions (RQs) in this study: (1) How is GenAI used across different aspects of health care services? (RQ 1) and (2) What is the preliminary evidence of GenAI use across health care services? (RQ 2). It is essential to explore these 2 RQs for several reasons. Exploring GenAI’s use in health care services is essential for realizing its potential benefits, addressing ethical concerns, and continually improving its applications to enhance patient care and the health care ecosystem. This impact spans different areas. For instance, GenAI can help analyze data to provide personalized treatment and tailor interventions. It has shown promise in improving diagnostic accuracy, with higher levels of accuracy in the interpretation of images and scans. AI applications can enhance patient engagement by providing personalized health recommendations, reminders for medications, and real-time monitoring of vital signs. On the provider side, GenAI can save costs by streamlining administrative tasks and improving efficiency, early disease detection, and preventive care. Similarly, knowing the preliminary evidence of GenAI use across health care services is crucial for making informed decisions, ensuring regulatory compliance, building trust, guiding research initiatives, and addressing ethical considerations. This sets the stage for the responsible and effective integration of GenAI into the health care landscape. The impact of GenAI in health care depends on various factors, including the specific application, quality of data used for training, ethical considerations, and regulatory framework in place. Continuous monitoring, evaluation, and responsible deployment are essential to maximize the positive impact and mitigate potential negative consequences. For instance, artificial intelligence (AI) assists pathologists in diagnosing diseases from pathology slides, leading to faster and more accurate diagnoses and improving patient outcomes [20]. Analysis of oncology literature, clinical trial data, and patient records can help oncologists identify personalized, evidence-based treatment options for patients with cancer, potentially improving treatment decisions [21]. AI has been applied to analyze medical images for conditions such as diabetic retinopathy, aiding in early detection and intervention [22]. AI analyzes clinical and molecular data to help physicians make more informed decisions about cancer treatment and steer them toward personalized and effective therapies [23]. Concerns about using GenAI remain because of algorithmic bias in predictive models that causes discrimination, unequal distribution of health care resources, and exacerbated health disparities [24]. Data privacy and the need for clear guidelines on AI in health care remain a gap, with reported misuse [25]. Misinterpretations or errors in algorithms can lead to incorrect diagnoses, specifically for image readings, which underscores the importance of human oversight in critical health care decisions [26]. Furthermore, implementing and maintaining AI systems can be costly, and overreliance on technology without sufficient human oversight may result in overlooking critical clinical nuances and potentially compromising patient care [27]. Therefore, it is essential to note that the impact of AI on health care is a dynamic and evolving field. Regular updates and scrutiny of the latest research and applications are necessary to understand the positive and negative aspects of GenAI in health care. Using a literature scoping, review, and synthesis approach in this study, we evaluated the proportionate evidence of using GenAI to assist, guide, and automate clinical service functions. Technologies in general help standardize [28], provide flexibility [29], increase experience and satisfaction through relational benefits [30], induce higher switching costs [31], and enhance the overall quality [32] and value [33] of services. However, high technology may reduce personal touch, trust, and loyalty in service settings [34-38]. Complex technologies may introduce anxiety, confusion, and isolation [39] or disconnection, disruption, and passivity stressors [13] that can erode satisfaction, loyalty, and retention in service settings [28,40-42]. Given the mixed evidence in previous research on the role of technology in services [28,43,44], it is timely to assess to what extent GenAI may even have a role in shaping or disrupting health care services. Overall, the ground realities of the potential for emerging GenAI to benefit health care services rather than just being another knowledge and collation tool need to be assessed and reported to influence further research and practice activities. This study took a deep dive to review and synthesize preliminary evidence on how GenAI is used to assist, guide, and automate activities or functions during clinical service encounters in health care, with plausible indications for differential use. More evidence on the actual use is needed to assert that GenAI plays a considerable role in the digital transformation of health care. Therefore, this study aims to identify how GenAI is used in clinical settings by systematically reviewing preliminary evidence on its applications to assist, guide, and automate clinical activities or functions.\n\nThis study aims to identify how physicians use GenAI in clinical settings, as evidenced in published studies. The design of this study adheres to the protocols outlined in the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) statement [45,46]. Figure 1 provides a flowchart of this study’s article search and inclusion process. Literature screening process for relevant articles on generative artificial intelligence (AI) tools and applications. We focused our search exclusively on PubMed to ensure the credibility of this study’s medical or clinical service settings. PubMed is part of the National Library of Medicine and a trusted national source of peer-reviewed publications on medical devices, software applications, and techniques used in the clinical setting. We performed keyword searches to retrieve relevant GenAI publications in PubMed that used “artificial intelligence” anywhere in the text of the article written in English. The sampling period of the publications was from January 1, 2020, to May 31, 2023. The search yielded 42,459 results in the first round of identification of articles for evaluation. Within PubMed’s classification system for articles, we used the “article type” that described the material presented in the article (eg, review, clinical trial, retracted publication, or letter). We used this article type feature in the PubMed classification system to identify peer-reviewed articles and other relevant types of publications that are pertinent to our study. A total of 52.02% (22,086/42,459) of the returned articles did not have an article type assigned from the 75 article types in PubMed’s classification system and were excluded from the study sample. We included clinical, multicenter, case report, news, evaluation, and validation studies. We excluded article types that were out of scope, such as uncategorized articles, government-funded studies, reviews, editorials, errata, opinion articles, nonscientific articles, retracted publications, and supplementary files. We also excluded preprint article types that were unlikely to have attracted attention. Errata or retracted publications (404/42,459, 0.95%), supplementary files (117/42,459, 0.28%), and 50 article types that had too few search returns (243/42,459, 0.57%) were also excluded. The screening stage excluded review articles (6732/42,459, 15.86%) with an objective that was neither aligned with nor redundant to this study’s goal. Opinion articles such as editorials, letters, and commentaries were excluded (2455/42,459, 5.78%). Articles whose funding came from the government or a government agency were not considered because of a conflict of interest for the researchers of the evaluated study (8936/42,459, 21.05%), and preprint articles (77/42,459, 0.2%) were excluded because of lack of availability to the public. We also considered the full text availability of the article, and 32.39% (490/1513) of the articles were excluded in the eligibility stage. The resulting set of records included 1023 publications. To ensure the credibility of the publication source, we used CiteScore (Elsevier) [47] as a citation index to remove publication sources whose influence is limited. Any publication source whose citation index was unavailable or <10 was removed, resulting in 268 records. In total, 2 raters, 1 author (DY) and 1 graduate assistant (BB), evaluated 161 articles. The 2 raters’ agreement was 91.93%, and the expected agreement was 82.99%. The κ score was 0.5252 (SE 0.0544; Z score=9.66; probability>Z score=0.0000). The author and the graduate student performed manual coding by reading the paper’s title, abstract, and introduction paragraph to gain a preliminary understanding of the study. After reading the abstract and introduction paragraph, each rater classified each article according to the definition of the 3 classes. For articles that were difficult to understand, the rater read the article further to gain a better understanding of the article. We defined clinical service settings to include the life cycle of physician encounters with patients for the diagnosis, prognosis, and management of health conditions. The research and development of drug discovery, for instance, was not considered. This process eliminated 107 records. The final data set of articles considered for this study was 161. The data collected for this study were obtained from publicly available sources. The study did not involve any interaction with users. Therefore, ethics approval was not required for this study. We adopted a modified thematic synthesis approach for data analysis that involved coding the text, developing descriptive themes, and generating analytical themes [48]. Initially, each author coded each line of text extracted from the articles, assigning it to different dimensions. This line-by-line coding process facilitated identifying and capturing critical article information and concepts. Next, each author developed descriptive themes by grouping related codes and identifying common patterns or topics emerging from the coded data. These descriptive themes provided a broad overview of the various aspects of AI in the clinical service context. Building on the descriptive themes, each author generated analytical pieces to deepen the understanding and interpretation of the data. The analytical themes involved exploring relationships, connections, and implications within and across the articles, allowing for the extraction of meaningful insights. Throughout the analysis process, all the authors engaged in extensive discussions to refine and finalize the results of the thematic synthesis. By collectively examining and interpreting the data, the research team ensured the robustness and reliability of the synthesized findings. Similar dimensions were then merged to generate the following 3 meaningful dimensions (assist, guide, and automate) and for relevance to the study objectives, as shown in Textbox 1. The researchers manually coded each article into several groups. They then tried to synthesize them into 1 of the 3 categories of assist, guide, and automate by looking at the title, abstract, and introduction (where applicable). Use of generative artificial intelligence tools and applications in clinical services in the reviewed articles (N=161).\n• None Improve diagnostic accuracy or reduce error by accessing knowledge during clinical services (141/161, 87.6%) [49-96]\n• None Minimize or eliminate human provider involvement in clinical services or follow-ups (7/161, 4.3%) [94,95,161-165] In addition to manual coding by human researchers, we used ChatGPT (version 3.5; OpenAI) for automatic coding. ChatGPT-3.5 was used for speed and cost. ChatGPT-4 is less accessible to users who do not have the funds to pay for its monthly subscription. ChatGPT-3.5 training used one-shot learning using the standard user interface with the “foundational” mode, and no fine-tuning was performed. Future studies may use focused data sets for fine-tuning to improve classification accuracy. However, our study demonstrates that classification accuracy is high and robust even without fine-tuning. This procedure was implemented to check for any subjective bias and demonstrate AI’s potential use to complement the human coding process. The abstracts and introductions of these 161 articles were fed into ChatGPT using in-context or a few short learning processes that fine-tune a pair of domain-specific inputs and outputs to train, thereby enhancing the relevance and accuracy of ChatGPT’s automated coding output [166,167]. For instance, a sample of input we used in the study was the abstract, which summarizes the article. The output is the categories identified by the experts. ChatGPT learns how to code a set of articles by repeating the pair of inputs and outputs. One-shot learning, which consists of a single pair of inputs and outputs in general, performs as well as >2 samples and zero-shot learning. The benefits of in-context learning (ICL) in ChatGPT include enhanced relevance, where the foundational model becomes better at generating content for domain-specific tasks without additional training of the full model; controlled output such as developing a single word matching the desired coding category or variable; and reduced biases inherent in manual coding. We used the definitions provided in Textbox 1 to train and restrict ChatGPT to choose only 1 of the 3 use-case categories. We further compared ChatGPT’s classification with expert coding and found a high level of agreement between the 2, with a κ score of 0.94. As mentioned previously, the manual coding process involved the raters coding and evaluating each article. After each rater coded the article, the results were compared and discussed to further refine the classification definition and derive consensus on the final assignment of the article classification. This “gold standard” classification was compared with automatic coding performed by ChatGPT (version 3.5). Automatic coding was performed by ChatGPT-3.5. Classification training was performed using one-shot ICL. ChatGPT learns how to classify articles by being fed a pair of articles and classification labels. For example, a user can feed a prompt or use control tokens to indicate an article abstract and the label associated with the article. In our context, 3 articles and labels were fed to the interface. After this initial prompt session of training on 3 classification labels, subsequent interactions of providing only the article abstract with a prompt asking for a class label would return ChatGPT’s prompt completion. Alternatively, training could involve >1 example of the article and its label, which would then be called few-shot learning. To summarize, 161 articles were coded by ChatGPT-3.5 based on a single instance of ICL.\n\nFindings From the Synthesis on the Use of GenAI to Assist in Different Aspects of Health Care Services GenAI can improve clinical services in 3 ways. First, of the 161 articles, 141 (87.6%) reported using GenAI to assist services through knowledge access, collation, and filtering. The assistance of GenAI was used for disease detection (19/161, 11.8%) [58,63,67,69,71,73,77,90,97-107], diagnosis (14/161, 8.7%) [100,108-120], and screening processes (12/161, 7.5%) [65,86,87,93,121-127,168,169] in the areas of radiology (17/161, 10.6%) [49-63,65,66], cardiology (12/161, 7.5%) [67-72,74,76-79,129], gastrointestinal medicine (4/161, 2.5%) [81-84], and diabetes (6/161, 3.7%) [86-91]. Thus, although the use of GenAI has percolated across almost all disease-relevant and main service–relevant areas in health care, it is mainly for assisting through knowledge access, collation, and filtering. The use of GenAI in disease diagnosis has long-term implications. For instance, identifying “referrable” diabetic retinopathy using routinely collected data would help in population health planning and prevention [86-90]; however, rigorous testing and validation of the applications are critical before clinical implementation [94]. Similarly, using GenAI in remote care helps improve glycemia and weight loss [95], yet challenges related to variable patient uptake and increased clinician participation necessitated by shared decision-making must be considered [96]. In radiology services, prediction models using deep learning and machine learning methods for predictive accuracy and as diagnostic aids have shown potential, and natural language processing has been used to improve readability by generating captions; however, studies report using high-quality images, highlighting the need for a future standardized pipeline for data collection and imaging detection. In cardiology, AI analysis allows for early detection, population-level screening, and automated evaluation. It expands the reach of electrocardiography to clinical settings in which immediate interrogation of anatomy and cardiac function is needed and to locations with limited resources [67-69,71,73-75,95]. Nevertheless, there is evidence suggesting that integrating AI with patient data, including social determinants of health, enables disease prediction and early disease identification, which could lead to more precise and timely diagnoses, improving patient outcomes. GenAI aids in diagnostic accuracy, although its focus on higher value creation in health care is limited. The articles in this review reported that they used deep learning (34/161, 21.1%) [49,59,60,62,63,65,68,71,79,89,100,107,108,111,115,123,125,130-145], machine learning (9/161, 5.6%) [53,55,83,91,110,146-149], and image analysis approaches of GenAI during the assistance process (13/161, 8.1%) [68,88,104,110,111,114,116,119,133,135,138,150,151]. Knowledge access using GenAI has the potential to enable more options and flexibility in serving patients. Evidence of GenAI Use for Guiding or Automation Services Only 8.1% (13/161) of the studies provided insights into how GenAI is used to guide some services by seeking recommended treatment options, step-by-step instructions, or checklists to improve clinical services [64,80,85,96,152-160]. Of the 161 studies, 1 (0.6%) study sought personalized treatment plans and discussed monitored and managed service processes using GenAI [96]. Although this use category is nascent, GenAI can help provide speed efficiency and customized solutions in health services as in other contexts [37,127,170]. Finally, only 4.3% (7/161) of the articles indicated the use of GenAI to automate any service functions that could minimize or eliminate human provider involvement. When used appropriately, automation provides a predictable, reliable, and faster experience everywhere, every time for all customers, which will be a standardized way to provide several health care services [94,95,161-165]. The use of GenAI in some instances of service automation and guidance may be in its infancy but is encouraging. Providers are trying to explore unique ways to use AI, which requires a set of steps such as understanding the current workflow and the changes needed or aspirational workflows and aligning or designing GenAI to help in the workflow. This is similar to modifying restaurant food delivery options to suit drive-in rather than sit-in options. The providers need some work to fully automate, streamline, or re-engineer the service functions using GenAI in the future. To summarize our findings, in this study, we conducted a systematic scoping review of the literature on how GenAI is used in clinical settings by synthesizing evidence on its application to assist, guide, and automate clinical activities and functions. Of the 161 articles, 141 (87.6%) reported using GenAI to assist services through knowledge access, collation, and filtering. The assistance of GenAI was used for disease detection (19/161, 11.8%), diagnosis (14/161, 8.7%), and screening processes (12/161, 7.5%) in the areas of radiology (17/161, 10.6%), cardiology (12/161, 7.5%), gastrointestinal medicine (4/161, 2.5%), and diabetes (6/161, 3.7%). Thus, we conclude that GenAI mainly informs rather than assisting and automating service functions. Presumably, the potential in clinical service is there, but it has yet to be actualized for GenAI. To ensure the comprehensiveness and robustness of our findings, we expanded the search to Web of Science using similar keywords and strategies (suggested by the review team). We used the same keyword, “artificial intelligence,” in all text fields over the sampling period between January 1, 2020, and November 27, 2023. Our search was restricted to peer-reviewed academic journal articles written in English. We used the Web of Science–provided “Highly Cited Papers” criterion as a filtering mechanism to follow influential papers. Given the nonclinical context of the journals in the database, we believe that filtering based on the article’s importance is reasonable. Initial search results included 1958 articles from the Web of Science Core Collection. The preliminary analysis of the annual breakdown comprised 414 articles in 2023, a total of 651 articles in 2022, a total of 519 articles in 2021, and a total of 374 articles in 2020. The search results were further reduced by removing PubMed articles for redundancy, resulting in 1221 articles. Next, Web of Science journals include medical, nonmedical, and other clinical journals. Thus, we used simple keywords for filtering nonmedical and clinical contexts. We used the keywords “medical” and “health” mentioned in the abstract, which led to 133 articles. Finally, we read the abstracts and titles to exclude survey or meta-review and nonclinical studies. This process further narrowed down the selection to 51 relevant articles. Using ChatGPT-3.5 on November 27, 2023, we applied one-shot learning by providing 3 class definitions. We asked ChatGPT-3.5 to classify the article’s abstract, with 63% (32/51) in the assist category, 29% (15/51) in the guide category, and 8% (4/51) in the automated category. Diagnostic assistance articles dominated, similar to the results from PubMed. However, the other categories—prescriptive guidance and clinical service recommendations—were slightly higher. This difference is explained by the nonmedical and clinical nature of the journals included in the database. The “applied” nature of the journals is more likely to explore prescriptive guidance and clinical service recommendation use cases.\n\nThis study asked RQs about how GenAI is used, with evidence, to shape health care services. It showed that 11.8% (19/161) of the studies were on automation and guidance, whereas 87.6% (141/161) reflected the assistance role of GenAI. These findings are essential to discuss and distinguish between the optimism and actual use of GenAI in health care. The aspiration that GenAI has the potential to change health care significantly needs a careful revisit. Health care organizations need to assess the actual ground use for GenAI and prepare for and understand the exciting possibilities with a cautious approach rather than overly high expectations. Concerns related to the cost, privacy, misuse, and regulatory aspects of implementing and using GenAI [24-26] will become more pronounced, particularly when there is a perceived overreliance without clear promising results or actual practical use [26]. The literature synthesis in this study suggests that GenAI is mainly used for screening and diagnostic purposes using knowledge access; diagnostic processes such as predicted disease outcomes, survival, or disease classification; and improvement of the accuracy of diagnosis. This solves the problem of knowledge being available and accessible in time in a well-articulated manner to provide or render the services. This could help health care providers make more accurate and timely diagnoses, leading to earlier treatment and better patient outcomes. Such knowledge distillation helps improve diagnostic accuracy through GenAI, which can provide enough knowledge to physicians during service encounters; however, this is not hugely oriented toward higher value creation in health care. The research synthesis also suggests that there has been some use of GenAI during different steps and aspects of guiding the service delivery processes. Still, such use could be more encouraging and significant across the board. Plausibly, GenAI can analyze large amounts of disparate data from patients to suggest personalized medicine—which may help inform treatment plans for individuals. Service delivery needs some guidance or step-by-step help to be efficient and meet the duration or time requirements to render the clinical service on time, which GenAI may solve. However, we have not yet found strong evidence for such use by any health system. Currently, the automation of service functions using GenAI has only seen minimal instances and is yet to see widespread implementation. Automation helps offset some manual activities. However, automation may help in service functions’ cost, efficiency, and flexibility while maintaining some standards across similar services. Similarly, although we did not consider this area in the synthesis as it was out of the scope of services, GenAI can also be used in drug development and clinical trial pathways—a value proposition yet to be seen in practice. However, we do not undermine that many laboratories and pharmaceutical companies have used machine learning and AI tools and techniques in drug development and clinical trials. However, reported commercial GenAI use has not come to the limelight. Some other plausible uses of GenAI in health care include managing supply chain data, managing medical equipment assets, maintaining gadgets and equipment, and building a robust intelligent information infrastructure to support several other activities. For example, active efforts are being undertaken to incorporate GenAI, especially in administrative use cases such as the In Basket patient messaging applications. However, assessing the clinical accuracy of such tools remains a concern. In addition, we must incorporate user-centered design and sociotechnical frameworks into designing and building GenAI for health care use cases, for instance, to explore how GenAI can prevent a common pitfall of developing models opportunistically—based on data availability or end-point labels, adopting a user-centered design framework is vital for GenAI tools [171]. Similarly, scientific or research-oriented use of GenAI for knowledge search, articulation, or synthesis is helpful [172]. However, how far that will translate to the transformative clinical health care delivery processes while creating higher-order organizational capabilities to create value remains a concern [173]. Limitations of the Study and Scope for Future Research Several limitations and constraints affect the interpretation and generalizability of the findings of this study. Some of these limitations indicate the need for future research in relevant areas that we discuss further. First, the study’s findings were constrained by the availability of relevant and high-quality publications and the exclusion of preprints and unpublished data to limit the specifically designed scope of the study on using GenAI in health care clinical services, which influences the comprehensiveness and accuracy of the review. There also might be a tendency for studies with positive or significant results to be published, leading to a potential publication bias. In addition, harmful or neutral findings may not be adequately represented in the review, influencing the overall assessment of GenAI's effectiveness in health care. Research should focus on patient-centered outcomes, including patient satisfaction and engagement and the impact of GenAI on the patient-provider relationship. Understanding the patient perspective is crucial for successfully integrating AI technologies into health care. Second, the field of GenAI in health care is rapidly advancing, and new technologies and applications are continuously emerging. The findings of this study might not capture the most recent developments, and the ’conclusions of this study may become outdated quickly, specifically when some technologies have the potential to be adopted beyond institutional mechanisms, such as using GenAI mobile apps to scan images for retinopathy. Furthermore, an in-depth analysis of specific GenAI applications may open newer directions, and future research should focus on specific GenAI applications to provide detailed insights into their effectiveness and limitations. This could include applications such as diagnostic tools, treatment planning algorithms, and predictive analytics. Such heterogeneity of GenAI in health care encompasses a wide range of applications, and investigating these could make it challenging to draw overarching conclusions about GenAI’s impact on clinical services. Third, this review may not comprehensively address ethical considerations and potential biases in the use of GenAI in health care. Ethical issues related to data privacy, algorithmic bias, and the responsible deployment of AI technologies may require more in-depth exploration. Future research should systematically explore the ethical considerations associated with GenAI use in health care. This includes issues related to data privacy, consent, transparency, and the ethical deployment of AI algorithms in clinical settings. Finally, more data, papers, articles, and longitudinal developments on some applications may enrich this study and enhance its current limited generalizability. Longitudinal studies are needed to track the impact of GenAI in health care over an extended period. This will help researchers understand the sustained effects, identify potential challenges that may arise over time, and assess the scalability and adaptability of these technologies. Future studies could undertake comparative effectiveness research to assess how GenAI compares with traditional approaches in health care. Understanding the relative advantages and disadvantages will contribute to evidence-based decision-making. In addition, it is not clear what and how to measure the GenAI applications’ effectiveness in clinical services, leading to a call for standardized study metrics that can incorporate outcome measures and evaluation frameworks. Future research should investigate how the integration of GenAI into clinical health care services affects the workflow of health care providers. This includes understanding the time savings, challenges, and potential improvements in decision-making processes. By addressing these areas, future research can contribute to a more comprehensive understanding of the role, challenges, and potential benefits of GenAI in clinical health care services. The proliferation of technology often outpaces the development of appropriate regulatory and policy frameworks that are necessary for guiding proper dissemination. Our call is that, given that GenAI is emerging, policy agencies and health care organizations play a role in proactively guiding the use of GenAI in health care organizations. What are some actionable steps for stakeholders, including health care organizations and policy makers, to navigate the integration of GenAI in health care? For health care organizations, the steps may include conducting a technology assessment vis-à-vis goals to achieve outcomes from GenAI. Evaluating the existing infrastructure and technological capabilities within the health care organization to determine readiness for GenAI integration is a first step. This will provide an understanding of the current state of technology and ensure that the necessary upgrades or modifications can be implemented to support GenAI applications, thus garnering the benefits of GenAI. The second step is to invest in staff training and education through the development of training programs to enhance the skills of health care professionals in understanding and using GenAI technologies. Well-trained staff is essential for the effective and ethical implementation of GenAI, fostering a culture of continuous learning and adaptability. Third, health care organizations need to develop and communicate clear protocols and guidelines for the use of GenAI in different health care services, outlining ethical considerations, data privacy measures, and accountability standards. Transparent protocols help ensure the responsible and standardized use of GenAI, fostering trust among health care professionals and patients. Fourth, health care organizations need to engage in research on GenAI through collaboration with research institutions and industry partners to participate actively in studies evaluating the effectiveness and impact of GenAI applications in specific health care domains. Involvement in research contributes to the evidence base, informs best practices, and positions the organization as a leader in health care innovation. Finally, as mentioned previously, implementing the gradual integration of GenAI rather than jumping into irrational decisions is a caution. All health systems need to gradually plan and introduce GenAI technologies, starting with pilot programs in specific departments or use cases. Gradual integration allows for careful monitoring of performance, identification of potential challenges, and iterative improvement before broader implementation. For policy makers, much work must be done at the regulatory framework level to realize GenAI better. Policy makers must establish clear and adaptive regulatory frameworks that address the unique challenges GenAI poses in health care, ensuring patient safety, data privacy, and ethical use. There is a concern that bias in GenAI algorithms could lead to discrimination in care delivery across patients, and the role of policy guidelines in this aspect to train and use GenAI appropriately is critical. Policy frameworks must be developed to ensure less risk, safe and ethical use, and responsible effectiveness of GenAI. Policy and industry partnerships among experts to determine relevant frameworks are vital to guide the future of GenAI to help transform health care. Robust regulations will provide a foundation for the responsible and standardized integration of GenAI technologies. An underlying challenge of GenAI is integrating it across different legacy IT systems, which involves developing and adopting interoperability standards to ensure seamless communication and data exchange between different GenAI applications and existing health care systems. Interoperability enhances efficiency, reduces redundancy, and facilitates the integration of diverse GenAI solutions. In this process, creating incentives for responsible innovation for ethical considerations and the continuous improvement of GenAI applications will drive a culture of responsibility and quality improvement, aligning technological advancements with societal needs. Policy-level efforts also need to be oriented to allocate resources to enhance health care infrastructure, including robust connectivity and data storage capabilities, to support the data-intensive nature of GenAI applications. Adequate infrastructure is crucial for the reliable and secure functioning of GenAI in health care. Many of these enhancements may require collaboration between public health care systems, private organizations, and academia to leverage collective expertise and resources for GenAI research, development, and implementation. Finally, policies that address potential biases in GenAI applications and ensure equitable access to these technologies across diverse populations are necessary to help with proactive measures to prevent the exacerbation of existing health care disparities through the adoption of GenAI. GenAI is both a tool and a complex technology. Complexity is the basis for GenAI, and thus, the use of GenAI in health care creates a set of unparalleled challenges. GenAI is costly to implement and integrate across all aspects of a health system [174]. In envisioning the future of GenAI in health care, we glimpse a transformative landscape in which technology and compassion converge for the betterment of humanity. As we stand at the intersection of innovation and responsibility, the prospect of GenAI holds immense promise in revolutionizing health care, shaping a future in which personalized, efficient, and equitable clinical services are not just aspirations but tangible realities. Our vision embraces a symbiotic relationship between technology and human touch, recognizing that the power of GenAI lies not only in its computational prowess but also in its potential to amplify the capabilities of health care professionals. Picture a world in which diagnostic accuracy is elevated, treatment plans are truly personalized, and each patient’s journey is marked by precision and empathy. Crucially, this vision hinges on responsible adoption. We envisage a future in which regulatory frameworks ensure the ethical use of GenAI, safeguard patient privacy, and uphold the principles of equity. It is a future in which interdisciplinary collaboration flourishes, bridging the expertise of health care providers, policy makers, technologists, and ethicists to navigate the complexities of this evolving landscape. In the future, the impact of AI on human lives will be profound. Patients experience a health care system that not only heals but also understands, a system in which the integration of GenAI contributes to quicker diagnoses, more effective treatments, and improved outcomes. The human experience is at the forefront—GenAI becomes a tool for health care professionals to better connect with patients and spend more time understanding their unique needs, fears, and hopes. As we embark on this journey, it is crucial to remember that the heart of health care lies in the compassion, empathy, and wisdom of its human stewards. GenAI catalyzes empowerment, freeing health care professionals from mundane tasks to engage in meaningful interactions. It fosters a health care culture in which technology serves humanity, and the collective mission is to enhance the quality of care and life. In embracing this vision, we are not just architects of technological progress but also custodians of a future in which GenAI and human touch coalesce to redefine health care possibilities. Let our strides be guided by a commitment to responsible innovation, a dedication to inclusivity, and an unwavering focus on the well-being of those we serve. The future of GenAI in health care is not just a scientific evolution, but it is a narrative of healing; compassion; and a shared commitment to a healthier, more humane world. However, without enough evidence, we are skeptical about the current euphoria regarding GenAI in health care. This systematic narrative review of the preliminary evidence of using GenAI in health care clinical services provides valuable insights into the evolving landscape of AI applications in health care. The existing literature synthesis reveals promising advancements and critical considerations for integrating GenAI into clinical settings. The positive evidence underscores the potential of GenAI to revolutionize health care by offering personalized treatment plans, enhancing diagnostic accuracy, and contributing to the development of innovative therapeutic solutions. The applications of GenAI in areas such as pathology assistance, oncology decision support, and medical imaging interpretation showcase its capacity to augment health care professionals’ capabilities and improve patient outcomes. However, this review also highlights several limitations and challenges that warrant careful consideration. Issues such as the quality of available data, the rapid pace of technological evolution, and the potential for algorithmic bias highlight the complexities associated with adopting GenAI in health care. Ethical concerns, data privacy considerations, and the need for transparent guidelines underscore the importance of a thoughtful and measured approach to integration. As we navigate the preliminary evidence, it becomes evident that a collaborative effort is required among health care organizations, policy makers, researchers, and technology developers. Establishing clear regulatory frameworks, fostering interdisciplinary collaboration, and prioritizing ethical considerations are crucial steps in ensuring the responsible deployment of GenAI. Addressing the identified limitations through targeted research initiatives, ongoing evaluation, and continuous improvement will be essential for maximizing the benefits of GenAI while mitigating potential risks. Moving forward, it is imperative to recognize that integrating GenAI into health care is dynamic and evolving. Future research should focus on refining our understanding of the long-term impact, patient-centered outcomes, and scalability of GenAI applications. By collectively addressing the challenges outlined in this review, stakeholders can contribute to a health care landscape in which GenAI is a powerful ally in delivering personalized, efficient, and equitable clinical services."
    },
    {
        "link": "https://deeperinsights.com/ai-blog/navigating-ai-powered-image-creation-a-stable-diffusion-primer",
        "document": "Stable Diffusion marks a notable progress in generative AI (GenAI), offering a multifaceted tool suitable for a wide array of applications. Grounded in advanced AI technology, this model goes beyond its technical construct to serve as a foundation for a variety of innovative endeavours. It skillfully merges the complexities of AI with user-friendly functionality, becoming an essential resource for both technological research and real-world applications. As a significant milestone in AI development, Stable Diffusion not only showcases a leap in technical prowess but also opens up a spectrum of opportunities for commercial and creative uses.\n\nAt the foundation of Stable Diffusion lies the utilisation of diffusion mechanisms combined with Variational Autoencoders (VAEs), a class of deep learning models that are pivotal in learning compressed representations of data. VAEs are renowned for their ability to efficiently encode and decode images, forming the bedrock of Stable Diffusion's image synthesis process. Additionally, the model leverages the power of Transformer-based neural networks, particularly in refining the generated images. Transformers, with their self-attention mechanisms, are adept at handling the complexities of image data, ensuring that the synthesised images maintain high fidelity to the desired outputs. This synergy of VAEs and Transformers underpins the model's capacity to produce highly detailed and varied images, setting a new benchmark in generative AI.\n\nA key focus during the development of Stable Diffusion was on optimising the model for greater computational efficiency. This was achieved through a strategic implementation of Latent Diffusion Models (LDMs), which operate in a lower-dimensional latent space. By transforming images into a latent space before the diffusion process, the LDM significantly reduces the computational load without compromising the quality of the output. This approach addresses one of the major challenges in generative AI - balancing computational resources with output quality. Furthermore, the developers employed advanced training techniques, including the use of denoising diffusion probabilistic models (DDPMs), to fine-tune the model's performance. DDPMs contribute to the model's stability and coherence in image generation, enabling Stable Diffusion to efficiently produce high-quality images with a notable reduction in training and generation times.\n\nThis open-source approach encourages a collaborative environment where developers, researchers, and creatives can contribute to and benefit from the model's continuous evolution. It not only accelerates the pace of innovation in AI-driven image generation but also ensures that these advancements are not confined to organisations with significant resources. Small businesses, independent artists, and academic researchers now have the same opportunity, as resource rich companies, to harness this powerful technology for their projects. This accessibility is crucial in driving forward industry-wide advancements, fostering an inclusive ecosystem where ideas and improvements can be shared, enhancing the model's capabilities and applications.\n\nBy making all the tools freely available online it has broadened its reach to a diverse spectrum of users, including hobbyists, independent creators, small businesses, and academic researchers. The primary requirements are simply a basic computer, a reliable graphics card, and internet connectivity. Such minimal hardware needs allow individuals and smaller organisations to employ advanced AI without significant technological investments, substantially reducing the barrier to entry for those keen to explore and utilise cutting-edge image synthesis technology. Additionally, the knowledge to access and utilise these digital tools is readily and freely available online. Numerous instructional videos and resources offer step-by-step guidance on how to install and use Stable Diffusion, making it accessible even for those with limited technical expertise.\n\nWhat practical benefits does a tool like Stable Diffusion offer? Its ability to generate high-quality, tailored images quickly and cost-effectively makes it an invaluable asset in the modern digital landscape. Its usefulness extends from theoretical concepts to a wide range of practical applications in various industries:\n• Advertising and Marketing: In the world of advertising and marketing, Stable Diffusion can be employed to create unique and compelling visual content. For example, agencies can use the model to generate custom images for campaigns, adapting to specific themes or brand identities quickly and cost-effectively. This capability can revolutionise the way visual content is produced for digital marketing, allowing for more personalised and creative advertisements.\n• Film and Entertainment: In the film and entertainment industry, Stable Diffusion can be a game-changer for visual effects (VFX) and conceptual art. Production studios can utilise it to generate detailed concept art and pre-visualizations for sets and characters, significantly speeding up the creative process and offering a plethora of design options.\n• Fashion and Retail: For the fashion and retail sectors, Stable Diffusion offers an innovative approach to product visualisation and marketing. Fashion designers can use it to visualise new designs or patterns, while retailers can create high-quality, lifelike images of products for online stores, enhancing customer engagement and potentially reducing the need for extensive photoshoots.\n• Architecture and Interior Design: In architecture and interior design, this model can be used to create detailed renderings and visualisations of architectural projects and interior spaces. This aids architects and designers in presenting their ideas more vividly to clients, providing a realistic representation of the final product.\n• Educational and Training Materials: Stable Diffusion can also play a significant role in the creation of educational and training materials. It can generate detailed images and diagrams for textbooks, especially in fields like biology, medicine, and engineering, where visual representation is crucial for understanding complex concepts.\n• Healthcare and Medical Imaging: In healthcare, Stable Diffusion has the potential to assist in medical imaging by theoretically providing enhanced image reconstruction, aiding in more accurate diagnoses. It can also be used to generate synthetic data for training medical AI systems, ensuring patient privacy while offering ample data for AI training.\n\nThe advancements brought forth by Stable Diffusion are not devoid of ethical complexities. The model's capacity to create highly realistic deepfakes poses significant challenges in copyright and data integrity. It necessitates a balanced approach to harness its potential, ensuring ethical compliance and responsible usage in AI-driven content creation.\n\nStable Diffusion stands as a testament to the remarkable progress in the field of generative AI. This versatile tool transcends traditional boundaries, offering innovative solutions across a diverse range of applications. Its foundation in advanced AI technology, coupled with its practical usability, positions it as a valuable asset for both in-depth technological research and effective real-world implementations. As we witness this significant leap in AI capabilities, generative AI not only illustrates the potential for enhanced technical performance but also paves the way for expansive and diverse commercial and creative opportunities to stay ahead in a competitive digital world. In essence, Stable Diffusion is not just a tool but a catalyst for future advancements and innovations in the ever-evolving landscape of artificial intelligence."
    },
    {
        "link": "https://viso.ai/deep-learning/stable-diffusion",
        "document": "Stable Diffusion (SD) is a Generative AI model that uses latent diffusion to generate stunning images. This deep learning model can generate high-quality images from text descriptions, other images, and even more capabilities, revolutionizing the way artists and creators approach image creation. Despite its powerful capabilities, learning to use Stable Diffusion effectively can have a steep learning curve.\n\nIn this comprehensive guide, we’ll break down the complexities. We’ll cover everything from the fundamentals of how it works to advanced techniques for fine-tuning the model to create unique and personalized images.\n\nAbout us: Viso Suite is a flexible and scalable infrastructure developed for enterprises to integrate computer vision into their tech ecosystems seamlessly. Viso Suite allows enterprise ML teams to train, deploy, manage, and secure computer vision applications in a single interface. Book a demo with our team of experts to learn more.\n\nBefore diving into the practical aspects of Stable Diffusion, it is important to understand the inner workings of this model. While it shares some core concepts with other generative AI models, there are also core differences. The latent spaces concept and diffusion processes are shared, but Stable Diffusion (SD) has a unique architecture and training methodologies.\n\nBy understanding how SD works, you will gain the knowledge needed to use this model, craft effective prompts, and even fine-tune. So, let’s start by answering some fundamental questions.\n\nStable Diffusion is a latent diffusion generative model made by researchers at CompVis. Those latent diffusion models came from the development of probabilistic diffusion models which depended on early methods that use probability to sample images. After GANs and VAEs, latent diffusion came as a powerful development in image generation with many capabilities. Those capabilities are a result of the integration of attention mechanisms from Transformers.\n• Inpainting: Masking a part of an image and generating in its place.\n• Image conditioning: Condition the generation based on an image, creating image variations or upscaling the image.\n\nThese capabilities made latent diffusion technology a state-of-the-art method for image generation. Later when the model checkpoints were released, researchers and developers made custom models, making Stable Diffusion models faster, more memory efficient, and more performant. Since its release, newer versions followed such as the ones below.\n• SD v1.1-1.4: These were released by CompVis with 256×256 and 512×512 resolutions and almost a million training steps for the 1.4.\n• SD 1.5: Released by RunwayML with different weights resuming from previous checkpoints.\n• SD 2.0-2.1: Trained from scratch by Stabilityai, has up to 768×768 resolution with great results.\n• SD XL 1.0/Turbo: Also from Stability AI, this pipeline utilizes an SD base model to deliver stunning results and improved image-to-image features.\n• SD 3.0: An early preview of a family of models by Stabilityai as well. With parameters ranging from 800M to 8B, taking us to a new level of realism in image generation.\n\nLet’s now look at the basic architecture of Stable diffusion models and their inner workings.\n\nGenerally speaking, diffusion models are trained to denoise random noise called Gaussian noise step by step, until we get to the sample of interest which is the image. Diffusion models are probability-based, predicting the likelihood of an image’s appearance.\n\nThese models showed great results, but the downside was the speed and resource-intensive nature of the denoising process. Denoising is a sequential process, happening in the pixel space, which can become huge with high-resolution images.\n\nThe latent diffusion architecture reduces memory usage and computing complexity by applying the diffusion process to a lower-dimensional latent space. This distinguishes latent diffusion models like Stable Diffusion from traditional ones: they generate compressed image representations instead of using the Pixel space. To do this, latent diffusion has the components below.\n• U-Net Backbone: Using the same U-Net as previous diffusion models but with the addition of cross-attention layers for the denoising process.\n• VAE: An encoder encodes input images to latent representations for the U-Net, while a decoder transforms the output back into an image.\n• Conditioning: Allows latent diffusion models to be conditioned in multiple ways, for example, text conditioning allows for text-to-image generation.\n\nDuring inference, the stable diffusion AI model takes a latent seed and a condition. The seed is used to generate a random image representation and the condition is encoded respectively.\n\nFor text-to-image models, the CLIP-ViT text encoder is used to generate text embeddings. The U-Net then denoises the generated noise while being conditioned. The output of the U-Net is then used to compute a denoised latent image representation via a scheduler algorithm.\n\nNow that we have enough knowledge of Stable Diffusion AI and its inner workings, we can move to the practical steps.\n\nImage generation models, especially Stable Diffusion, require a large amount of training data, thus training from scratch is usually not the best path with these models. However, inference and fine-tuning are great ways to use Stable Diffusion models.\n\nIn this section, we will delve into the practical side of using Stable Diffusion. The setup of our environment will be on Kaggle notebooks, which provides free access to GPUs to run the model. We’ll leverage the Diffusers library to streamline the process, and for this guide, we’ll focus on Stable Diffusion XL 1.0, for different types of inference and parameter tuning. We’ll then look at fine-tuning and the process it involves.\n\nKaggle notebooks provide good GPU options and an easy setup to work with. Stable Diffusion XL (SDXL) can be heavy to run locally, so using a hosted notebook is beneficial. While other options like Google Colab are available, they no longer allow Stable Diffusion models to be run on it.\n\nSo, to get started, log in or sign up to Kaggle and create a new notebook. Once that’s open you can now see the default notebook view.\n\nYou can rename the notebook in the top left corner. Next, let’s delete that default cell as we won’t be needing it by right-clicking and deleting the cell. Before starting with the code, let’s also set up the GPU for a smooth run.\n\nGo to the 3 vertical dots, choose accelerator, and then the P100 GPU. P100 is a good GPU option that will allow us to run SDXL. Now that we have that setup, press the power button, and let’s get the notebook running. To start with our code, let’s install the needed libraries.\n\nAfter installing the libraries, next we use the Stable Diffusion XL.\n\nAdd a code block and then use the following code to import the libraries and load the Stable Diffusion XL pipeline.\n\nThis code may take some time to run, so let’s break it down. We import the DiffusionPipeline from the diffusers library, torch is Pytorch, allowing us to work with tensors.\n\nNext, we create the variable pipe which contains our model. To load the model we use the DiffusionPipeline and give it the first parameter which is the model repository identifier from Hugging Face Hub “stabilityai/stable-diffusion-xl-base-1.0”. The torch_dtype=torch.float16 parameter sets the data type to be 16-bit floating point (FP16) to give faster computation and reduced memory usage.\n\nThe variant parameter specifies that we used FP16 and then the use_safetensors parameter specifies to save the model as a safe tensor. The last part is “.to(“cuda”)” which moves the pipeline to the GPU.\n\nThe last step before we infer the model is to make the generation process faster and more efficient.\n\nThe prompt is adjustable, adjust it to whatever you want. When you run it, inference should start and your image should be saved in the images array. Let’s look at the generated image.\n\nThis code will save your output image in the output folder on the right side of the Kaggle interface named “knight-cat.png”. Also, we display the image using the Matplot library. Here is what the output looked like.\n\nThat output looked cool, but what if we want more control over the image generation process? We can do that using some advanced features. Let’s explore that. We need to load an additional pipeline that will allow us more options over the generation process, which is the refiner pipeline. Assuming you still have your notebook running and the Stable Diffusion XL pipeline loaded as pipe, we can use the below code to load the refiner.\n\nThe refiner has similar parameters to the SDXL pipeline but with a few additions like the “VAE” parameter which takes the VAE from the pipe we loaded, and the same for the text encoder. Now that we loaded the refiner, we can define the options to adjust the generation.\n\nThese options will affect the generation process greatly, the n_steps determines the number of denoising steps the model will take. The high_noise_frac is a percentage value determining how much work to split between the base model (pipe) and the refiner. In our case, we tried 0.75 which means the base model does 75% (45 steps) of the work, and 25% by the refiner (15 steps).\n\nBefore generating an image with our settings, we could take an additional step that will help us reduce GPU memory usage.\n\nNow, to run inference on both pipelines we can do the following.\n\nRunning this will run both the refiner and the Stable Diffusion XL pipeline with the settings we defined. Then we can display and save the generated image just like before.\n\nHere is what the output looks like.\n\nTrying different values for the “n_steps” and “high_noise_frac” will allow you to explore how they make a difference in the generated image. A quick tip: Try using different prompts for the refiner and base.\n\nWe previously mentioned the capabilities of Stable Diffusion in other tasks like image-to-image generation and inpainting. We can use almost the same code to use those features, reading the documentation can be helpful as well. Here is a quick code to use the image-to-image feature, assuming you have run the previous code.\n\nThis code will use an example image from the HuggingFace datasets as the condition and load it through the URL. You can use your image there. We are loading the image-to-image pipeline, but to save memory we load it from our already loaded pipe.\n\nThere are parameters like strength that control the influence of the initial image on the final result. The guidance scale determines how closely the model follows the text prompt. Below is what the output looks like.\n\nWe can see how the generated image (on the right) followed the style of the condition image on the left. Image-to-image generation is a cool feature with Stable Diffusion showing the power of latent diffusion model architecture and the different conditions we can have. Our advice is to explore the documentation and try different tasks, parameters, or even other Stable Diffusion versions. The code is similar, so go out there and explore.\n\nOlder versions like SD 1.5 could even allow more complex tunings for the parameters, and maybe even a wider range of tasks. Those models can perform well and use fewer computational resources, potentially allowing a better experimenting experience. To take the next step towards mastering Stable Diffusion AI, let us explore fine-tuning.\n\nFine-tunning or transfer learning is a technique used in deep learning to further train a pre-trained model on a smaller, targeted dataset. This allows the model to maintain its capabilities, but also gain new specified knowledge. So, we can take a model like Stable Diffusion, which has been trained on a massive dataset of images, and refine it further on a smaller, more focused dataset.\n\nLet’s explore how this works, its uses, and popular techniques for Stable Diffusion fine-tuning.\n\nWhat is Fine-tunning and Why Do It?\n\nGeneralization is a big drawback when it comes to computer vision or image generation models. This is usually because you might have a specific niche use that was not represented well in the model’s training data. As well as the inevitable bias in computer vision datasets.\n\nThis approach usually involves a few steps, such as collecting the dataset, preprocessing, and cleaning it according to the expected input of Stable Diffusion. The dataset will usually be hundreds or thousands of images, which is still much smaller than the original training data.\n\nThe main concept in fine-tuning is freezing some layers, which is done by retaining the initial layers of the model, that usually capture basic features and textures, unchanged or frozen. While later layers are adjusted and continue training on the new data.\n\nAnother important metric is the learning rate which determines how much a model’s weights are adjusted during training. However, fine-tuning has several advantages and drawbacks.\n• Performance: Allowing Stable Diffusion to perform better on a specific niche.\n• Efficiency: Fine-tuning a pre-trained model is much faster and more cost-effective than training from scratch.\n• Democratization: Making models more accessible through different niches.\n• Overfitting: Fine-tuning with the wrong parameters can lead the model to overfit, forgetting its general training data.\n• Reliance: When fine-tuning a pre-trained model we rely on the previous training it had to be sufficient to continue. Also, if the original model had biases or security issues, we can expect those to persist.\n\nFine-tuning Stable Diffusion has been a popular destination for most developers. A few methods have been developed to fine-tune those models easily, even without code.\n• Dreambooth: a fine-tuning technique that can teach Stable Diffusion new concepts using only (3~5) images. Allowing anyone to personalize their model using a few images of the subject. (Applied to Stable Diffusion 1.4)\n• Textual Inversion: This approach allows for learning new ideas from just a few example images. It accomplishes this by creating new “concepts” within the embedding space of the text encoder utilized in the image generation pipeline. These specialized concepts can then be integrated into text prompts to provide very granular control over the generated images. (Applied to Stable Diffusion 1.5)\n• Text-To-Image Fine-Tuning: This is the classical way of fine-tuning, where you would prepare a dataset according to the expected format and train some layers of the model on it. This method allows for greater control over the process, but at the same time, it is easy to overfit or run into issues like catastrophic forgetting.\n\nStable Diffusion AI has improved the world of image generation forever. Whether it’s generating photorealistic landscapes, creating characters, or even social media posts, the only limit is our imagination. Researchers are using Stable Diffusion for tasks other than image generation, like Natural Language Processing (NLP) and audio tasks.\n\nWhen it comes to real-world influence, we are already seeing this in many industries. Artists and designers are creating stunning graphics, artwork, and logos. Marketing teams are making engaging campaigns, and educators are exploring personalized learning experiences using this technology. We can even go beyond that with video creation and image editing.\n\nUsing Stable Diffusion is fairly easy through platforms like HuggingFace, or libraries like Diffusers, but new tools like ComfyUI are making it even more accessible with no-code interfaces. This means more people can experiment with it. However, as with any powerful tool, we must consider ethical implications. Things like deepfakes, copyright infringement, and biases in the training data can be a real concern, and raise important questions about responsible AI use.\n\nWhere will Stable Diffusion and generative AI take us next? The future of AI-generated content is exciting and it’s up to us to take a responsible path, ensuring this technology enhances creativity, drives innovation, and respects ethical boundaries.\n\nIf you enjoyed reading this blog, we recommend our other blogs:\n• None\n• The 10 Top Applications of Computer Vision in Retail"
    },
    {
        "link": "https://milvus.io/ai-quick-reference/what-ethical-considerations-are-involved-in-deploying-diffusion-models",
        "document": "Deploying diffusion models, which generate content like images or videos by iteratively refining noise, involves several ethical considerations. The primary concerns include misuse for harmful content, biases in training data, and environmental impact. Developers must balance innovation with responsibility to avoid unintended consequences.\n\nFirst, diffusion models can be misused to create deceptive or harmful content. For example, generating realistic deepfakes could spread misinformation or enable impersonation for fraud. Even benign uses, like creating stock images, might inadvertently infringe on copyrighted material if the model was trained on unlicensed data. A case in point is the controversy around Stable Diffusion’s training dataset, which included copyrighted artwork scraped without explicit consent. Developers must implement safeguards, such as content filters or watermarking AI-generated outputs, and ensure training data complies with legal and ethical standards. Proactively restricting the model’s ability to replicate specific copyrighted styles or identities can mitigate risks.\n\nSecond, biases in training data can lead to harmful outputs. If a diffusion model is trained on datasets lacking diversity, it may generate stereotypical or exclusionary content. For instance, a model trained primarily on images of light-skinned faces might struggle to generate accurate representations of darker skin tones, reinforcing societal biases. Addressing this requires curating diverse datasets and auditing outputs for fairness. Tools like OpenAI’s DALL-E 2 use post-processing filters to block biased or unsafe content, but these solutions are imperfect and require ongoing refinement. Developers should document data sources and biases transparently, allowing users to understand limitations.\n\nFinally, the environmental cost of training and running large diffusion models raises sustainability concerns. Training a model like Stable Diffusion requires significant computational resources, contributing to carbon emissions. Even inference (generating content) demands high GPU usage, which scales with user demand. Developers can optimize model efficiency through techniques like distillation or quantization and prioritize renewable energy for data centers. Ethically, teams should weigh the benefits of model scale against its ecological impact and consider alternatives like smaller, task-specific models when possible. Transparent reporting of energy usage, as done by some research groups, helps users make informed decisions."
    },
    {
        "link": "https://arxiv.org/html/2406.18071v1",
        "document": "† † Make sure to enter the correct conference title from your rights confirmation emai; June 03–05, 2018; Woodstock, NY\n\n† † Software and its engineering Software organization and properties\n\nAI-enabled software refers to software systems containing AI algorithm components which facilitate learning and problem solving . It is used in a range of systems, including critical domains such as health care , law enforcement , and education , due to rapid advances in AI techniques in recent years . However, biases and instances of dangerous model use are still frequently encountered, posing potential harm to end users. For example, investigations have revealed cases where opaque AI-enabled software predicts a higher risk of future crime for African-Americans compared to white individuals . Therefore, ensuring adherence to ethical best-practice, vis-à-vis the behaviour of AI models, becomes important. Recent pushes for fairness, transparency, and accountability in AI systems give rise to proposals to incorporate these concerns in documentation practices for software developers. The most prominent proposals, including Mitchell et al.’s model cards and Gebru et al.’s datasheets have been eyed by regulators as potential loci of AI governance paradigms. In particular, model card has become the default introductory document in Hugging Face registries . From a software engineering perspective, software documentation encompasses essential information regarding system design, architecture, and the development process, serving as a valuable means of communication among stakeholders . The completeness and usability of software documentation are two of the critical factors relevant to developers . However, documentation is found to be one of the collaboration challenges in machine learning (ML) enabled systems, as team communication frequently relies on verbal exchanges lacking clear documentation . Documenting ethical considerations is particularly important, as it informs software developers about potential ethical risks of the underlying models and the corresponding mitigation methods. Bhat et al. found that ethics-related discussions are shallow in current documentation practice. However, there is limited empirical evidence on what ethical considerations are documented for the current AI models. By understanding the current practice in AI ethics documentation, we can identify specific documentation issues, and provide guidelines for both documentation maintainers and policy makers. This paper intends to bridge this gap, and provide concrete suggestions for aiding the documentation process. We conducted an empirical study of the ethics-related AI model documentation practice on two major open source platforms, GitHub and Hugging Face, specifically focusing on both model cards and README files. We refer to these two types of artefacts as AI model documents consistently throughout the paper. We collected an initial set of 2,347 documents, and developed a comprehensive keyword set to identify ethics-related documentation. After filtering the non-related documents and the ones that have identical ethical documentation, we performed thematic analysis on 265 documents. Our analysis shows that documentation about ethics in the model cards revolves around six themes including: (1) Data quality concerns, (2) Model behavioural risks, (3) Model risk mitigation, (4) Model use cases, (5) Reference to other materials, (6) Others. We found that most of the documentation is about model behavioural risk, while data quality concerns and suggestions for ethical risk mitigation are less discussed. In addition, we identify that most ethics-related risks discussed are only briefly outlined, and the already scarce risk mitigation suggestions are not concrete and actionable. We discuss the implications for various stakeholders and provide concrete suggestions. The replication package is at https://zenodo.org/records/12531181. The key contributions of our research are as follows: (1) a keyword set that effectively captures documentation on ethics-related issues in open source documents, (2) a comprehensive set of themes on ethical considerations in the current AI model documents as found in our dataset, and (3) a replication package with the dataset and our qualitative analysis to facilitate future research.\n\nA pragmatic framework for discussing ethics is principlism, which encompasses principles such as respect for autonomy, nonmaleficence, beneficence, and justice . Schwartz recognised ten value categories including security and conformity, encompassing 58 human values . The values, such as helpfulness, are used in mapping with ethical considerations in software engineering. Some of the values are incorporated in the software process management frameworks and decision making or to measure values in software engineering . Using this framework, Alidoosti et al. conducted a systematic literature review on software engineering ethics, discovering relationships among different ethical values, namely value conflicts and value tensions. They identified stakeholders that are affected by ethical considerations in software engineering, namely that system users, system development organisations, and indirect stakeholders are mostly concerned with safety. In addition, Perera et al. conducted a study investigating publications in top software engineering venues pertaining to various human values. They found that values such as helpfulness, privacy, and environmental protection are discussed in software engineering research. Several studies, both empirical and algorithmic in nature, were proposed to enhance helpfulness , privacy , and environmental protection . Bias and fairness are crucial aspects of ethics in modern software development . Brun and Meliou visioned fairness challenges across software engineering phases, including requirement specification, architecture design, testing, and verification. Tools are needed to address root causes of bias, such as issues in requirements, algorithms, implementation, or data. Galhotra et al. proposed a testing-based method to measure software discrimination. In the domain of machine learning, various techniques are proposed. The bias in machine learning models can be mitigated with pre-processing, in-processing, and post-processing techniques . For pre-processing, techniques are used to transform the data so that the underlying discrimination is removed . The in-processing techniques aim to incorporate changes into the objective function or impose constraints during model training . For post-processing, Gohar et al. identified ensemble as a way to mitigate bias in machine learning models. To complement existing work on ethical aspects in software engineering and to support the developers of AI-enabled software, in this study, we investigate the documentation practice for ethical considerations in AI models. Machine learning documentation reports information about the model and dataset, aiming to provide transparency among stakeholders about the systems . Regarding data, datasheets proposed by Gebru et al. suggest documenting content such as motivation, composition, collection process, and recommended uses. Pushkarna et al. proposed an at-scale framework of Data Cards for better transparency and explainability, with information across a dataset’s lifecycle. Hutchinson et al. proposed a framework to make visible the often overlooked work and decisions involved in dataset creation, covering dataset requirements, design, implementation testing, and maintenance. For model documents, Model Cards and FactSheets are proposed standard frameworks for AI models. In particular, Model Cards have received substantial interest. They serve as the default document in Hugging Face, one of the largest open source machine learning model platforms, and are adopted by companies such as Google and Salesforce for their public models . The proposed framework in Model Cards includes model details, intended use, factors, metrics, evaluation data, training data, quantitative analyses, ethical considerations, and caveats and recommendations. Despite the proposal of machine learning documentation frameworks, document practice effort and quality still fall short. A recent study interviewed 45 practitioners from 28 organisations and identified documentation as one of the biggest challenges when building and deploying machine learning systems into production . One of the participants in their study stated “team members often collect information and keep track of unwritten details in their heads”. Therefore, similar to traditional software document issues , completeness and usability issues, among others, also appear in machine learning documentation. Recent studies have started examining machine learning documentation practices to provide insights for developing more comprehensive guidelines or documentation tools. Bhat et al. quantitatively studied the model card documentation practice alignment with the proposed framework, and identified that most documentation has an unbalanced coverage of aspects in the model card proposal. Yang et al. quantitatively investigated Hugging Face dataset documentation, shedding light on current dataset documentation practices. Pepe et al. studied the transparency perspective of pre-trained transformer models dataset, bias, and license declaration documented in Hugging Face model cards. However, none of the previous work focused on ethics. Our work contributes to this field by examining machine learning documentation practices in ethics-related content. Our results inform developers of AI-enabled software by providing a comprehensive view of how ethical considerations are documented.\n\nIn this section, we introduce our research methodology and the design of our study. We structured our investigation into two research questions (RQs). RQ1: How accurately can we detect ethics-related discussions within different types of documentation? This research question focuses on the development of a set of keywords to detect ethical content within AI model documentation. We employed expansion techniques and experimented with the removal of keywords that resulted in false positives to derive the final set of keywords. RQ2:What is documented about ethics in AI-enabled open source software projects? This research question focuses on identifying the ethical considerations documented within open source projects targeted at the developers of AI-enabled software. We conducted thematic analysis to address this question. We selected two Open Source Software (OSS) platforms as data source, namely GitHub and Hugging Face. GitHub is the largest OSS platform, hosting various types of software projects, while Hugging Face is the largest model registry, enabling machine learning model reuse. In terms of the artefact, we focused on “model cards”. However, there is no standardised location for model card documents in OSS. For example, aparrish/pincelate provides its model card within the README file rather than having a dedicated location. Only searching for separate artefacts would miss such cases. Therefore, README files are also included as our target. For GitHub, we used the search API to find repositories containing an explicit model_card.md or model-card.md document, as well as the ones mentioning the term “model card” in their README files. We distinguish these two data sources, and refer them as to GH_CARD and GH_README. For GH_CARD, we obtained 589 repositories in total. However, the GitHub search API has a limitation: for data sources exceeding 1,000 results, it cannot return the entire population . Therefore, we only obtained a sample of 1,020 repositories for the GH_README data source after one search. We further applied filters for the repositories, requiring them to have at least ten stars and no forks to ensure the quality of repositories selected , narrowing down the selection to 173 and 305, respectively for GH_CARD and GH_README. For Hugging Face, we systematically collected models that were among the top 2,000 downloaded in December 2023, and kept the ones having model card documents, ending up with 1,869 projects. We refer to the Hugging Face data source as HF_CARD. However, it is possible that certain documents within these repositories do not mention ethical considerations, as we did not initially filter for such content. Training machine learning models to identify whether documents are related to ethics requires a large amount of annotated data. Annotating such data would be time-consuming and require domain expertise. Therefore, we adopted a keyword-based filter to heuristically obtain the ethics-related documents. Given the absence of a gold standard for ethical keywords in mining documents, we adopted an iterative keyword expansion method akin to prior work . Figure 1 describes our filtering steps. Specifically, we initially selected a set of keywords, including “ethics”, terms in “FAccT” (fairness, accountability, and transparency), plus ”morality” and ”responsibility,” as identified in a systematic literature review . Then, we searched the noun, verb, adjective, and adverb forms of those words using a tool from GitHub . Forms that deviate from their “ethical” meaning would lead to false positives (e.g., the adverb variant ”fairly” of the adjective ”fair”), and we did not include them within our base keyword set. After filtering with the base keywords, we obtained relevant GH_CARD, GH_READE, and HF_CARD documents, totaling of 86, 86, and 249, respectively. We manually filtered out false positive documents within this set, resulting in 86, 27, and 222 documents. False positives refer to documents that do not contain ethics-related content or repositories that are not machine learning registries or machine learning applications. In GH_CARD and HF_CARD, the main reason for false positives was keyword usage, such as ”bias” being used as a statistical term. Meanwhile, most false positives occurring in GH_README are because the repositories do not contain machine learning registries or applications. Based on the selected documents, we expanded the keyword set using KeyBERT . The basic idea for KeyBERT is to extract the most relevant words from a given text based on the cosine similarity between each of the words and the whole text. As our documents cover diverse topics such as model description and performance, utilising the entire document for keyword extraction may yield suboptimal results. Meanwhile, topics are relatively stable within the granularity of paragraph. Consequently, we used paragraphs that contain at least one of our base keywords and applied KeyBERT for the keyword expansion. 205 new keywords were generated, and two authors separately annotated them as relevant or not to ethics. An initial Cohen’s Kappa score of 0.61 was reached, indicating a substantial level of agreement . Disagreements were resolved after discussion. This step finally provides 44 new keywords and is expanded to 106 considering all derivatives. After integrating expanded keywords, we can detect more ethics-related documents, with 149, 172, and 656 for sources of GH_CARD, GH_README and HF_CARD. To validate the efficacy of our expanded keywords, we randomly sampled a 95% confidence rate subset of 107, 119, and 243 documents, respectively. We manually annotated the documents as relevant or irrelevant to ethical considerations. 6, 73, and 88 documents were identified as irrelevant, corresponding to a false positive rate of 5.6%, 61.3%, and 36.2%, respectively. Given that false positives in GH_README are mostly due to repositories (e.g., non-software and non-ML repositories) rather than documents, we looked at keywords that cause false positives in the other two sources. As indicated in Figure 2, the keywords “disclaimer” and “private” contribute to most of the false positives, while other keywords cause fewer errors. The keyword “disclaimer” is identified to appear in many Hugging Face model cards, stating that the documents were written by the Hugging Face team instead of the repository authors. Meanwhile, the keyword “private” is noisy, with false positive examples in “private discussion channel”, and “private dataset” (i.e., not released). We set the frequency threshold at five and excluded two keywords exceeding it. Within our sampled dataset, these two keywords often occur by themselves when the data record is a false positive, and removing them would drop 46 false positives while only one true positive is ignored. Therefore, we removed them from our keyword set. We further conducted another pilot study, randomly sampling 50, 21, and 50 negative documents from GH_CARD, GH_README, and HF_CARD respectively. Only two repositories within GH_CARD were supposed to be positive. This suggests that our chosen keywords effectively encompass the majority of ethics-related content, with minimal instances of oversight. Therefore, to answer RQ1, we developed a comprehensive ethics-related keyword set to heuristically filter contents of AI model documentation. This filter can capture the vast majority of the ethics-related documents for model card documents. By applying these filters, we can immediately exclude documents that are clearly not-related to ethics. However, further manual efforts are still required to filter out false positives in the identified documents. The set of keywords is available in our replication package. In our previous manual investigation of ethics-related documents, we observed the phenomenon of document reuse across numerous repositories. Notably, many projects share highly similar, or even identical, contents with respect to ethical considerations. To mitigate the potential dominance of such repetitive documents in our analysis, we clustered the documents into distinct content groups. In particular, for each document, we extracted paragraphs related to ethics using our curated keywords and calculated pairwise document distances using TF-IDF-based cosine distance. Figure 3 shows the distance distribution. A prominent peak is observed at 0, indicating identical contents, while a small number of documents exhibit similarity within the threshold of 0.1. Furthermore, the document reuse phenomenon primarily comprises instances of identical content, resulting in close distances between them. Meanwhile, most of the documents still have a larger distance. We used this distance matrix for agglomerative hierarchical clustering, truncating clusters at a distance threshold of 0.1. This yielded 43, 150, and 281 distinct clusters in GH_CARD, GH_README, and HF_CARD respectively, indicating the ethical document reuse is more prevalent in model cards than in REDAME files. We randomly sampled one document from each cluster to conduct the following thematic analysis. To cover the diverse range of ethics-related topics comprehensively without neglecting certain perspectives, four authors first looked at a sample of ten documents. We annotated the parts of the documents that contain ethical considerations independently, before discussing to reach agreement. After that, the first author analysed the remaining documents. Throughout the process, the first author flagged uncertain or ambiguous content for further discussion. Finally, 41, 38, and 186 documents from the source of GH_CARD, GH_README and HF_CARD respectively, were included for the analysis. We used the following steps from thematic analysis for the qualitative analysis stage of the included documents. Getting familiar with data: The first author organised the documents and created spreadsheets for analysis, which are shared among all the authors. The first author extracted key points of the ethical discussions from the documents, as suggested to initiate the open coding . The second and third author closely examined this process. Disagreements were resolved during regular meetings. Generating base level codes: After agreeing on the extracted key points, the first author proceeded with open coding . During the process, careful investigation of key points along with raw data in the chosen documents were conducted to understand the context and ensure the accuracy of assigned codes. Throughout the process, regular discussions were held among the first three authors to verify the codes within and across different documents. This iterative process of code development led to continuous adjustment of the codes, with some being deleted, merged, or split. Synthesising and finalising themes: In this phase we grouped the emerging codes into higher level themes, as suggested in thematic analysis . Here, we got inspiration from a template suggested by for documenting model cards, when grouping the emerged codes from our data into themes. Finally, we generated a mind-map visualising all the themes and organisation of codes. The mind-map was used to facilitate discussion among all the authors and reaching to agreement. Figure 4 provides an illustrative example for generating one of the themes called “Data quality concern”, and Figure 5 provides an example for coding on the raw data.\n\nWe synthesised 81 codes belonging to six major themes, which are: (1) Data quality issues, (2) Model behavioural risks, (3) Model risks mitigation, (4) Model use cases, (5) Reference to other materials, and (6) Others. Table 1 presents the sub-themes under each theme, their frequencies, as well as the proportion for each theme covered in three different data sources. Since one document could cover multiple sub-themes, the “Total” is not the sum of the percentages in each sub-theme. We also added the frequency for each sub-theme in parentheses after its description. Generation of, or reaction to objectionable content Potential malicious use and misuse of the model The behaviour of machine learning models is significantly influenced by the quality of datasets used for their training . 27.55% of the documents contain data quality concerns. We synthesised three sub-themes and discuss them as follows: Data content concern (69). When discussing data quality, ”data content concern” is a frequently-occurring sub-theme. This relates to the presence of inappropriate content within certain entries in the dataset. Specifically, concerns revolve around the possibility of biased , offensive, sexual, and violent content. For example, the model card of DeepFloyd/IF-II-M-v1.0 notes, “The model was trained on a subset of the large-scale dataset LAION-5B, which contains adult, violent and sexual content.”. Additionally, private information issues were also raised. For example, facebook/nllb-200-3.3B mentions, “Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated.”. However, for seven of the documents, their discussion on data quality issues is vague (e.g., only stating “low-quality data”), and cannot be categorised specifically. Therefore, we refer to them as “generic inappropriate content”. We argue that these statements are not as useful, as they do not provide any specific, concrete directions for downstream developers to investigate. Data distributional concern (23). As highlighted by Buolamwini et al. , under-representative groups in the data can introduce bias into trained models. The data distributional concern refers to the discussion that certain data is statistically imbalanced in its distribution. The most mentioned distributional bias pertains to language and culture, as exemplified in the statement “… mostly trained on English text, other languages and cultures are insufficiently accounted for” in DeepFloyd/IF-II-M-v1.0. Other distributional bias including style, gender, and data collection time, are also mentioned. Some discussions suggest internet data may be biased toward those with easier access. For example, facebook/flava-full notes that “A larger portion of this dataset comes from internet and thus can have bias towards people most connected to internet such as those from developed countries and younger, male users (sic).” Similarly, we observed that five of the documents include more high-level comments about potential data distribution concerns (e.g., “There may be distributional bias in the RedPajama dataset that can manifest in various forms in the downstream model deployment in cerebras/btlm-3b-8k-base”), without providing more details. Data curation concern (5). The sub-theme of ”Data curation concerns” is often overlooked. Unlike the previous two sub-themes, which primarily focus on the data, the way that data is curated reflects the interpretation and interests from human annotators. An example is how a “…dataset likely reflects the interests and semantic choices of Databricks employees, a demographic which is not representative of the global population at large.”, mentioned in the model card of project databricks/dolly-v2-7b. This caveats the thoroughness of ML developers when they consider potential ethical issues, due to factors of demography, subjectivity, and perspectives. : The main discussion points related to “data quality concerns” focus on objectionable and private data. Data distribution concerns address under-represented groups, while data curation concerns involve biases from human annotators, often overlooked in ethical considerations in our dataset. This theme encompasses various ethics-related risks associated with the behaviours of ML models. In our analysed data, this is the most frequently mentioned theme, with approximately half of the documents containing such discussions. We synthesised three sub-themes and discuss them as follows: Possibility of biased output (143). This sub-theme relates to the phenomenon that certain groups perceive bias in the output of AI models. One of the key considerations in ethics is related to the biased behaviour of models. Among different factors, gender bias ( ) is the factor that is most frequently brought up, followed by culture and language ( ), racial ( ), occupation ( ), age ( ), religion ( ), and disability ( ). However, the largest proportion ( ) is related to generic mention of model output bias. These abstract mentions provide little helpful information to downstream software developers, and act more like a disclaimer from document writers’ perspective. Meanwhile, different documents vary in the level of detail for biased outputs. For instance, “Based on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion)” in koboldAI/OPT-13B-Nerys-v2, briefly mentions different bias factors without delving into the details. In contrast, albert-base-v2 provides code examples that demonstrate gender bias by showcasing the model’s predictions for male and female inputs. Moreover, martin-ha/toxic-comment-model includes a table detailing the model performance when considering various ethnic groups, genders, and disabilities. Offering comprehensive details on potential model biases can help downstream developers better understand the nature and quantify the severity of the issues. Furthermore, there is also discussion on biased behaviours of models, stemming from an original pre-trained model, or being inherited after further fine-tuning. Document users can refer to the upstream models for additional model bias information. Generation of, or reaction to objectionable content (61). In the era of Large Language Models (LLMs), machine learning models learn to generate responses to complex human requests. However, uncensored models may comply to unethical requests and generate objectionable content. Two frequently mentioned types of objectionable content are those that are rude and harmful to individuals ( ), and sexual content ( ). Notably, of the mentioned entries only provide a generic warning that the generated content might be inappropriate, without further substantiation. For instance, google/flan-t5-base notes, “As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.”. Such generic statements lack detailed insights that could be investigated by downstream developers. Similarly, the levels of detail in documenting objectionable content vary. Notably, some documents provide model safety measurements on established benchmarks. For instance, one of the Llama model documents evaluates different versions of the model’s toxic generation on a benchmark called ”Toxigen”. In addition to input, there are also mentions on the model lacking control over unethical requests. “It will be highly compliant to any requests, even unethical ones.”. These discussions provide directions for downstream developers to fix. Correctness and reliability of model behaviour (51). The model correctness discussed here refers to the factual accuracy of the output content, not performance accuracy. The term “hallucination” is often used, describing the phenomenon where the model produces self-contradictory or misaligned responses with established world knowledge . In bigscience/T0_3B, an example is provided where the model responds “yes” to the question “Is the earth flat?”, indicating the model’s agnosticity to real-world facts. However, most documents do not provide specific details, as we have observed and discussed earlier. Other reliability factors are also discussed, albeit less frequently. These include the possibility of model output revealing information from training data and the lack of transparency in the model. : The most discussed sub-theme is potential biased model output. Additionally, model output can be objectionable or comply with unethical requests. Correctness and reliability issues focus on hallucinations, revealing training data, and lack of transparency. Understanding what can be done to mitigate potential model risks is crucial, as mere awareness is not enough. This theme encompasses the efforts made by model developers as well as recommendations offered to downstream developers to mitigate various model risks. We synthesised three sub-themes and discuss them as follows: Risk mitigation suggestions for downstream developers (108). Model cards, among other document artefacts, have provided model architectures along with their development process. However, conflicts around development practices between data scientist and software engineers are found, with some software engineers reporting insufficient and hard-to-understand documentation . Therefore, risk mitigation recommendations could help downstream developers bridge the gap between the lack of domain expertise and the necessity for safe model deployment. The most common recommendation suggests downstream developers to conduct risk assessment tailored to their specific use cases ( ). A similar mitigation recommendation involves requiring them to further fine-tune the model for their use cases ( ). However, other than a few instances which provide initial resources for reference (e.g., ), both types of suggestions lack substantive information that developers can follow. For instance, stabilityai/StableBeluga-13B notes, “Therefore, before deploying any applications of Beluga, developers should perform safety testing and tuning tailored to their specific applications of the model.”. Furthermore, the current model card documents also contain prevalent mitigation suggestions of a more generic nature. These include encouraging downstream developers to familiarise themselves with potential model risks, to inform end users about these risks, as well as abstract references to taking ”appropriate” measures to mitigate model risks. These suggestions collectively account for of the total recommendations. In contrast, suggestions on adopting additional techniques to censor the input or output of the model are more concrete and could lead to actionable solutions, accounting for of the total recommendations. For instance, Salesforce/ctrl wrote “A second recommendation to further screen the input into and output from the model will be implemented through the addition of a check in the CTRL interface to prohibit the insertion into the model of certain negative inputs, which will help control the output that can be generated.”. Risk mitigation in the model development phase (48). In addition to recommendations to developers, practitioners also document their endeavours in mitigating various ethics-related risks for the model. Regarding their efforts during model development, the focus revolves around data. Mitigation measures encompass data pre-processing, meticulous selection of data sources, and data annotators. Furthermore, there are also references to the algorithms utilised during the training phase. Risk mitigation in the post-model development phase (13). Some documents note that the model has undergone safety evaluations, while others acknowledge their lack of efforts in follow-up risk assessment and mitigation. Notably, suno/bark notes “To further reduce the chances of unintended use of Bark, we also release a simple classifier to detect Bark-generated audio with high accuracy”. This supplementary tool aids in detecting model misuse, providing downstream developers with a mitigation solution. Various ethics-related risk mitigation practices could also provide insights for downstream developers to leverage. However, these factors are often not the focus of model card documents, and discussions tend to be brief. Instead, practitioners’ efforts are typically accompanied by discussions of corresponding ethics-related risks associated with the model, effectively serving as explanations for any deficiencies in the models. This “forward risk mitigation” can be seen as a very preliminary step towards “thinking through the instances in which unfairness or harm might arise but that are not yet formally addressed or even recognized”, within the framework of what Elish terms the moral crumple zone. : The most frequent sub-theme is recommendations for downstream developers, but many are too abstract or generic to be actionable. Risk mitigation endeavours by model developers can be divided into processes during and after the model development, with discussions typically being brief. Instead of providing usage instructions on the model, this theme specifies restrictions on the permissible uses of the model. This is reminiscent of cautions and legal repercussions of medicinal ‘off-label’ use in the context of medical ethics , or, closer to home, OSS licenses restricting use/deployment if it causes harm . We synthesised three sub-themes and discuss them as follows: Out-of-scope use cases (87). This sub-theme specifies scenarios in which the model should not be used. These out-of-scope use cases arise either because the model is not designed to address the situation or because the stakes for using AI are deemed too high. Among these discussions, the use of the model for critical tasks or systems is most frequent, accounting for of the discourse. Critical tasks include generating factually-correct content. For example, some model cards emphasise that ”The model was not trained to provide factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for its abilities.” Similar to the model risk of hallucination, this statement emphasises that the model should not engage in such tasks. Tasks involving critical decisions or actions against human benefits are also considered critical. For instance, Minej/bert-base-personality is a model that predicts the personality based on input texts. In the model card document, they note, “It should not be used for making critical decisions or judgments about individuals in areas such as employment, education, or legal matters”. Some instances also mention that the model is not intended for use in safety-critical systems. Model deployment situations are addressed in of the discussions. Some projects state that the direct use of the model without appropriate risk assessment and mitigation is considered out-of-scope. For example, in the project tiiuae/falcon-180B, the statement ”Production use without adequate assessment of risks and mitigation” is located in the out-of-scope section. Additionally, some projects explicitly mention in their model cards that the model is not intended for production use as they are solely research models. For instance, facebook/nllb-200-3.3B notes, ”NLLB-200 is a research model and is not released for production deployment.” Some discussions under this sub-theme also address human interaction with the model, emphasising that the system is not designed for human-facing situations without appropriate guardrails. They also cover unintended use for certain groups and caution against using the model without human supervision. Potential malicious use and misuse of the model (82). Model malicious use and misuse refer to instances where users are aware of the potential risks and limitations of the model but deliberately use it in ways that cause harm or damage. The most discussed form of misuse is the generation of harmful and offensive content, accounting for . This is followed by generating disinformation and propaganda ( ), violating human privacy ( ), and violating copyright or deceiving model behaviour ( ). Notably, the content addressing misuse is typically concise, often presented in bullet points listing considerations, indicating the practitioners have given thoughts to this aspect. Model term of use (41). This sub-theme encompasses the licenses, laws, regulations, and policies governing how the model should be used. When stating the licenses, model cards often include corresponding links for readers to follow. However, specific laws and regulations are not typically mentioned in the discussion. For instance, when reading ”Users should also respect the privacy and consent of the data subjects, and adhere to the relevant laws and regulations in their jurisdictions,” in KoalaAI/Text-Moderation, users may not be able to identify which regulations to refer to. : The out-of-scope sub-theme includes uses in critical tasks, deployment scenarios, and human interaction. Potential malicious use refers to deliberate misuse to cause harm. Model terms of use include licenses, laws, regulations, and policies. Containing all information within the document would cause information overload. “Other materials” refer to other documentation and non-documentation artefacts that are not embedded within the model card document . We synthesised two sub-themes and discuss them as follows: Reference to third-party external materials (60). This sub-theme includes references to external literature, license, responsible usage guidelines, machine learning documents, and external datasets. Many documents refer to additional literature to setup the background for ethics-related issues. “Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)” is a commonly adopted template for laying the background shared among 21 documents. Additionally, materials such as license agreements and responsible usage guidelines provided by large companies are frequently referenced. Reference to in-house materials (34). Meanwhile, reference are also made to materials developed by the document authors themselves. References are made to blogs, publications, or discussion forums for the corresponding model for additional details. For example, in klue/bert-base, “The model developers discuss several ethical considerations related to the model in the paper” is presented in the document, with the URL provided to the corresponding paper. For data source of HF_CARD and GH_README, documents also make reference to model cards on their GitHub projects. This is because for Hugging Face models, some projects keep their codebase on GitHub, and README files only introduce the repositories. Summary of reference to other materials : References deliver additional ethical content beyond the project’s scope, providing background context without “information overload”. These references can point to third-party or in-house materials. There are also codes that do not fit into the previously mentioned themes. They are considered as “Others”. We synthesised four sub-themes and discuss them as follows: Future socio-ethical research direction (34). This sub-theme pertains to future directions introduced by this model that have impact on socio-ethical perspectives. The majority code in this sub-theme is related to the potential for the model to be used for future ethical research. Aspirations including developing accessibility tools or calling for better model regulation are also discussed. Limited exploration of ethical considerations (33). This sub-theme encompasses insufficient efforts on ethical factors. In 21 such Hugging Face model cards, authors used template section structures, and under ethics-related sections, explicit documentation debt such as ”More information needed” is present. The lack of ethical considerations is also evident in discussions where solving the ethical considerations is deferred to future work, or where users are encouraged to discover and report ethical issues themselves. Model developers’ disclaimer (28). Some documents disclaim responsibility for generated content by the model or consequences from potential model misuse. Environmental impact (15). Some documents keep track of the energy consumption or carbon emission for training their models; akin to the trend of publishing carbon footprints on flight tickets.\n\nWe began with an initial set of 3,609 projects from GitHub and Hugging Face. However, after filtering and removing duplicates, only 265 documents were analysed. The small proportion of valid ethics-related documents suggests limited emphasis on documenting ethical considerations in open-source AI models. Additionally, we noted instances of duplicate ethical discussions across documents. It appears that AI model developers did not customise documents for specific cases but rather employed them as templates. From Table 1, we observed that the most discussed themes in OSS documents are model behavioural risks, model use cases, and model risk mitigation. Conversely, data quality concerns were covered by the least amount of documents. Meanwhile, only discussion in “risk mitigation suggestions for downstream developers” provides solutions to mitigate ethics-related risks, with documents covering it. Therefore, the open source AI model documents we collected focus more on articulating ethical problems intrinsic to the model and use case restrictions rather than presenting solutions. When comparing results across various data sources, GH_README encompass the least coverage across all themes except for ”reference to other materials.” This aligns with expectations since README files provide diverse introductory perspectives to a repository, rather than focusing solely on ethical considerations . Regarding the disparity between the two model card data sources, HF_CARD comprises more content in four out of six themes, while GH_CARD exceeds solely in model use cases and others themes. As discussed in the previous section, the theme of “model behavioural risk” and the sub-theme of “risk mitigation suggestions for downstream developers” contain different levels of detail. During the coding process, we kept side notes of the level of detail being documented. Given the subjective nature of determining the usefulness of a statement, for the model behavioural risk, we categorise the details into “brief mention”, “examples”, and “evaluated result”. Similarly, suggestions were categorised as “actionable direction” or “abstract advice”. As an illustration, within the examples provided in the results section, the suggestion from Salesforcce/ctrl is categorised as ”actionable direction,” whereas the suggestion from stabilityai/StableBeluga-13B is deemed “abstract advice.” Within the subset of documents addressing model behavioural risk, offer evaluated results for certain ethical factors, while present examples illustrating potential ethics-related risks. The majority of documents ( ) merely provide brief mentions of various perspectives on model risks. In terms of risk mitigation recommendations, within the subset of documents addressing this sub-theme, offer actionable directions, while the remainder provide abstract advice. Consequently, we identify that most of the ethics-related risks discussed in OSS documents are only briefly outlined. Moreover, the mitigation recommendations provided are limited, and concrete suggestions are even more scarce. We provide implications for various stakeholders below. (1) AI Model Developers. Based on our results, we provide several recommendations to AI model developers when composing ethics-related model card documents. First, they need to allocate more efforts to documenting dataset-related ethical issues, particularly focusing on data curation and its distribution as the dataset is a primary element for model transparency . Checklists or tools could help raise awareness of the ethical issues that need to be documented. We recognise that dedicated documentation artefacts, such as datasheets , are also used for recording such concerns, but references to such documents are rarely found in our dataset. Second, AI model developers need to provide more details about the identified risks. Due to the expertise gap, some briefly mentioned terminologies might be challenging for documentation users. Providing examples or even concrete evaluations (as illustrated in ) can help downstream developers better understand and quantify the issues. Lastly, AI model developers should incorporate more risk mitigation suggestions in their model card documents. While we acknowledge that devising comprehensive suggestions covering all perspectives is challenging, practitioners can take proactive steps to address this issue. They can actively engage with downstream developers to gather their thoughts on risk mitigation and invite successful downstream application collaborators to contribute their solutions, leading to the development of more robust and effective risk mitigation strategies. (2) Software Developers. First, based on our findings, ethics-related model behavioural risks are inadequately covered. Therefore, downstream software developers should independently conduct comprehensive model risk assessments without solely relying on the provided model cards. Second, since mitigation methods provided are limited, developers can cross-reference documents of models with similar risks, especially those trained on the same dataset or built on the same architecture. Third, developers can contribute to the original model card by adding additionally identified ethical risks and their corresponding mitigation strategies. This collaborative effort can enhance the pre-trained model ecosystem and provide additional mitigation strategies for future users. Last, products exposed to the end-users should prioritise ethical sensitivity. Therefore, we advise software developers to create more concrete ethical documents on top of the model documents that include identified risks, evaluated methods, and mitigation solutions. (3) Researchers. Researchers can pursue several future directions to enhance ethical documentation practices. First, studying how changes in the models are reflected in ethical documentation could offer insights into documentation maintenance practices and related issues. Second, exploring the alignment between claimed ethical risks in documentation and the actual behaviors of the model would be beneficial. This analysis could identify discrepancies, leading to more accurate documentation. Third, conducting a study from the perspective of downstream developers to understand their views on different ethics-related discussions could offer valuable guidance for improving documentation practices. Understanding the specific needs and preferences of developers could inform the creation of more effective and user-friendly documentation practice. (4) Educators. Given the insufficient efforts in documenting ethical concerns in current model cards, educators should put more emphasis on ethics , and provide practical examples, to ensure that students understand the importance of considering ethics when developing or using AI/ML models. Providing templates and detailed aspects when documenting ethics, such as the findings from our study, could be beneficial for students. This approach would not only raise awareness about ethical considerations in AI models but also equip future practitioners with the knowledge and skills necessary to document ethical concerns effectively in their work. (5) Policy makers. This work shows clear evidence that the ethical documentation efforts are insufficient even with the use of various proposed documentation frameworks. Therefore, policy makers need to enforce or highlight the specific ethical concerns and required level of detail to be addressed within the documents, prior to model release. In addition, they could also require downstream software developers to conduct more thorough risk assessment and mitigation beyond the points listed in the AI model documents.\n\nThis section covers the threats to validity of this study. Internal validity: We used the GitHub search API to gather relevant GitHub projects containing model cards. We employed ”model_card” and ”model-card” as search strings for file names. This approach carries the risk of overlooking model card documents that adhere to different naming conventions. To mitigate this risk, we opted not to perform exact matching. Instead, we allowed for variations in upper and lower case letters, and as long as one of the search strings appeared in the file name, we included the project. We used a keyword-based approach to filter ethics-related documentation. Despite our systematic method of generation and assessment, some documents not explicitly using these terms might have been excluded. Additionally, we did not test software for ethical issues, and documented ethical considerations may not reflect the software’s ethical properties. Thus, we advise developers not to rely solely on the provided documentation for ethical assessments. Construct validity: The subjectivity involved in determining the ethics-related discussions in documents and allocating them into corresponding codes poses a threat to the validity of our results and conclusions. To mitigate this, we rigorously examined the ethics-related content first and resolved any disagreements to reach a consensus via weekly meetings. Regarding the coding themes and results, we cross-referenced with the model card template proposed by Mitchell et al. . The first author conducted the coding process, with the other two authors closely monitoring how the codes were assigned and synthesised. The coding process and mind map were shared among the group, with consensus achieved. Additionally, to ensure that our analysis is not biased towards statements in duplicate documents, we used cosine distance filter to remove documents that share the same ethical discussions. External validity: We acknowledge the limitation of the GitHub search API leading to fewer repositories collected from GH_README. However, since our observation indicates fewer ethical considerations are covered in this source, the impact of the smaller number of documents is minimal. Additionally, our work is limited by the relatively small number of documents. Our results were derived from the analysed repositories, and software documents of other repositories might cover additional ethical considerations. Future work could examine more documents to increase the robustness and generalisability of the findings. Moreover, considering that different processes are used for proprietary AI models and products, our findings are only limited to open source models and software. To increase the generalisability of our results, we analysed documentation artefacts on both GitHub and Hugging Face. We analysed both README files and model cards, as these data sources are mostly used in OSS for documentation, and therefore, more possible for us to find ethical documentation. Regarding data representativeness, our results are more representative of recent popular AI models due to our sampling strategy. Therefore, our findings might not be indicative of the documentation practices for older and less popular models. However, as a previous study identified , less popular repositories tend to document less, so our results should cover comprehensive aspects of ethical documentation practices."
    },
    {
        "link": "https://yulleyi.medium.com/the-ethical-challenges-of-generative-ai-applications-8478ecdfe2a4",
        "document": "Generative AI (GenAI) techniques utilizing deep learning, neural networks, and other sophisticated machine learning have enabled tremendous advances in applications like computer vision, natural language processing, creative content generation, and more. However, the powerful capabilities of GenAI also raise critical ethical concerns regarding bias, transparency, accountability, and unintended societal consequences that must be proactively addressed as adoption continues growing rapidly.\n\nOne report breaks the types of ethical and moral concerns from AI into eight categories:\n• Lack and impact of regulation, now and in the future\n• Effect on labor, both in the data mines and the coming battlefield of machine versus human productivity\n\nThis post provides an in-depth look at a handful of these key ethical issues posed by GenAI applications, highlighting recent problems, and analyzing potential solutions that allow the technology to be harnessed responsibly.\n\nOne of the most pressing issues with GenAI systems is the perpetuation of unfair bias based on flaws in the training data or algorithms. Without careful consideration, models can learn and amplify existing societal biases around race, gender, age, ethnicity and more from biased data. This leads to discriminatory and prejudicial outcomes that can severely impact individuals and groups.\n\nThe below graphic from CSA Research illustrates how training data can lead to different types of bias in models. What makes this even more difficult to pin down is that most model developers never release even the metadata of the training data — if anything is open sourced, it is usually the fine-tuning data, which is highly curated and not representative of the full training set.\n\nHow does this bias in the training data manifest in AI? Facial analysis models have demonstrated higher error rates for women and darker-skinned individuals. Hiring algorithms have disadvantaged qualified candidates from minority backgrounds. Predictive policing systems exhibit racial biases, leading to over-policing in marginalized communities.\n\nIf we look at generative AI in particular, Bloomberg conducted a study analyzing the outputs of Stable Diffusion text-to-image models and concluded, “The world according to Stable Diffusion is run by white male CEOs. Women are rarely doctors, lawyers or judges. Men with dark skin commit crimes, while women with dark skin flip burgers.\n\nStable Diffusion generates images using artificial intelligence, in response to written prompts. Like many AI models, what it creates may seem plausible on its face but is actually a distortion of reality. An analysis of more than 5,000 images created with Stable Diffusion found that it takes racial and gender disparities to extremes — worse than those found in the real world.” (Link)\n• Fairness metrics: Rigorously defining, evaluating, and optimizing for fairness measures can quantify and mitigate algorithmic biases.\n• Inclusive design from the beginning can ensure that other use cases are considered. If data is primarily sourced from\n• Testing models in different environments. One framework that the World Economic Forum offers is STEEPV. “STEEPV analysis is a method of performing strategic analysis of external environments. It is an acronym for social (i.e. societal attitudes, culture and demographics), technological, economic (ie interest, growth and inflation rate), environmental, political and values. Performing a STEEPV analysis will help you detect fairness and non-discrimination risks in practice.” (Link)\n• Increasing diversity of data and teams building GenAI systems also helps offset biases. Many leading companies also have specific responsible AI teams or an AI ethics office that are engaged in the end-to-end process.\n\nThere is no straightforward answer to solving these challenges, but a mix of these methods is starting to result in a set of best practices that work toward mitigating bias.\n\nWith complex GenAI models like deep neural networks, it is often not possible to fully explain the rationale behind their decisions and predictions. However, understanding model behaviors, especially for sensitive applications in areas like finance, healthcare and criminal justice, is essential to ensure transparency, trace accountability, and avoid harm.\n\nSeveral approaches to enhance interpretability are being explored:\n• Local explainability focuses on explaining the reasons for specific individual predictions. This fosters user confidence in the system.\n• Rule-based explanations approximate the model logic through more interpretable rules and logical statements.\n\nGenAI systems must adhere to existing regulations around data privacy, consumer protections and anti-discrimination laws. However, the rapid evolution of AI capabilities presents policy challenges. Key steps needed include:\n• Developing techniques like federated learning and differential privacy to preserve user privacy.\n• Establishing rigorous internal governance procedures for responsible data collection and use practices.\n• Advocating for regulations specific to GenAI systems, focused on transparency and accountability.\n• Adhering to ethical principles and frameworks on fairness and bias mitigation proposed by organizations like the EU and OECD.\n\nA known issue with large language models is the propensity to generate false information or content that promotes harmful stereotypes due to lack of real world knowledge and common sense. For instance, ChatGPT has produced toxic text, harmful misinformation and biased narratives when prompted.\n\nPotential solutions to mitigate the risk of societal harm include:\n• Adversarial testing to catch flaws and make models more robust.\n• Incorporating human feedback loops to correct bad behavior and guide the model.\n• Monitoring deployments and having human oversight over high-risk applications.\n\nAddressing the multifaceted ethical challenges posed by advancing GenAI requires sustained engagement between stakeholders from research, policy, industry and civil society. Key focal areas for enabling responsible GenAI include:\n• Promoting participation of diverse voices in GenAI development and governance.\n• Supporting ongoing research to improve techniques and best practices for bias detection and mitigation, explainability, transparency, and error handling.\n• Advocating for regulatory policy development to ensure accountability and prevent harm from unchecked GenAI deployment.\n• Incentivizing ethical engineering practices focused on safety, transparency and fairness throughout the GenAI model lifecycle.\n\nWith proactive efforts to put ethics at the core of the GenAI application development process, this transformative technology can uplift society in countless ways while minimizing risks from misuse and unintended consequences. All stakeholders have a shared responsibility to steer the GenAI revolution towards just and beneficial outcomes for all.\n\nGenAI innovation carries tremendous potential for positive impact but also poses novel challenges we must thoughtfully address. This will require sustained collaboration and open dialogue between all parties to establish norms, incentives, guidelines and technical standards that enable GenAI technology to be deployed safely, ethically and for the benefit of humanity as a whole. If we make the necessary investments and tradeoffs, the GenAI applications of the future will represent the best of human values like compassion, creativity and justice."
    },
    {
        "link": "https://community.trustcloud.ai/docs/grc-launchpad/grc-101/governance/data-privacy-and-ai-ethical-considerations-and-best-practices",
        "document": "This article comprises an article discussing ethical considerations and best practices for data privacy in the age of artificial intelligence (AI) with TrustCloud. It is a platform offering resources and tools for governance, risk, and compliance (GRC) and support for various compliance standards like GDPR, HIPAA, and ISO 27001, utilising AI to streamline audit processes.\n\nThe article explores the ethical implications of AI’s data usage, emphasising the importance of transparency, informed consent, and robust security measures.\n\nWith rapid technological advancements, Artificial Intelligence (AI) has emerged as a transformative force, revolutionizing various industries and aspects of our lives. However, as AI systems become increasingly sophisticated and data-driven, concerns over data privacy have risen to the forefront. With the ability to process vast amounts of personal information, AI poses significant risks to individual privacy if it is not handled responsibly.\n\nAI systems rely heavily on data to learn, make decisions, and provide valuable insights. However, this data often includes personal information, such as browsing habits, location data, and even biometric identifiers. Without proper safeguards, this information could be misused, compromised, or exploited, leading to severe consequences for individuals and organizations alike.\n\nStriking the right balance between the power of AI and preserving data privacy is a complex challenge that requires careful consideration and a proactive approach. By understanding the ethical implications and implementing robust data privacy measures, you can leverage the benefits of AI while maintaining the trust and confidence of your customers, employees, and other stakeholders.\n\nUnderstanding the ethical considerations of AI and data privacy\n\nThe intersection of AI and data privacy raises several ethical concerns that must be addressed to ensure the responsible and ethical development and deployment of AI systems. Here are some key considerations:\n• Privacy vs. utility: There is often a trade-off between the utility of AI systems, which rely on data to function effectively, and the need to protect individual privacy. Striking the right balance is crucial to avoid compromising either aspect.\n• Fairness and non-discrimination: AI algorithms can perpetuate or amplify existing biases present in the training data, leading to unfair or discriminatory outcomes. Ensuring fairness and non-discrimination in AI systems is an ethical imperative.\n• Transparency and accountability: Many AI systems operate as “black boxes,” making it difficult to understand their decision-making processes. Transparency and accountability are essential to building trust and ensuring responsible AI development.\n• Consent and control: Individuals should have the right to control their personal data and provide informed consent for its use in AI systems. Respecting individual autonomy and choice is a fundamental ethical principle.\n• Security and privacy by design: Privacy and security should be integral components of AI system design, rather than afterthoughts. Incorporating privacy-enhancing technologies and secure data handling practices from the outset is crucial.\n\nBy addressing these ethical considerations, you can ensure that AI systems are developed and deployed in a responsible and ethical manner, fostering trust and protecting the fundamental rights of individuals.\n\nProtecting data privacy is crucial in today’s digital landscape, where personal and sensitive information is constantly at risk. Ensuring data privacy helps build trust between organizations and their customers, fostering long-term relationships. It also mitigates the risk of data breaches, which can lead to financial loss, legal penalties, and reputational damage.\n\nAdditionally, safeguarding data privacy is essential for compliance with regulations like GDPR and HIPAA, which mandate strict data handling and processing practices. Ultimately, prioritizing data privacy not only protects individuals’ rights but also enhances organizational integrity and sustainability in a competitive market.\n\nProtecting data privacy is not just an ethical imperative but also a legal and business necessity. Here are some key reasons why safeguarding data privacy is crucial:\n• Compliance with regulations: Various data privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union and the California Consumer Privacy Act (CCPA) in the United States, impose strict requirements on organizations to protect personal data. Non-compliance can result in significant fines and legal consequences.\n• Building trust and reputation: Customers and stakeholders are increasingly aware of data privacy concerns and expect organizations to handle their personal information responsibly. Failing to protect data privacy can erode trust, damage reputations, and lead to business losses.\n• Preventing data breaches: Data breaches can have severe consequences, including financial losses, legal liabilities, and reputational damage. Implementing robust data privacy measures can help prevent such incidents and mitigate their impact.\n• Ethical and social responsibility: As AI systems become more prevalent, organizations have a moral and social responsibility to respect individual privacy and protect personal data. Upholding ethical principles is crucial for responsible AI development and deployment.\n\nBy prioritizing data privacy, you not only comply with legal requirements but also demonstrate your commitment to ethical practices, build trust with stakeholders, and contribute to the responsible development of AI technologies.\n\nTo ensure compliance and avoid legal consequences, it is essential to understand and adhere to relevant data privacy regulations and laws. Here are some key regulations and their implications:\n• General Data Protection Regulation (GDPR): Implemented in 2018, the GDPR is a comprehensive data privacy regulation in the European Union that sets strict rules for the collection, processing, and storage of personal data. It applies to any organization that handles the personal data of EU citizens, regardless of its location.\n• California Consumer Privacy Act (CCPA): Effective since 2020, the CCPA is a data privacy law in California that grants consumers certain rights over their personal information, including the right to access, delete, and opt-out of the sale of their data.\n• Health Insurance Portability and Accountability Act (HIPAA): HIPAA is a federal law in the United States that establishes standards for protecting sensitive patient health information. It applies to covered entities, such as healthcare providers, health plans, and healthcare clearinghouses.\n• Children’s Online Privacy Protection Act (COPPA): COPPA is a U.S. federal law that regulates the collection and use of personal information from children under the age of 13. It imposes specific requirements on websites and online services that collect data from children.\n• Other regional and industry-specific regulations: Various countries and industries have their own data privacy regulations and guidelines, such as the Personal Information Protection and Electronic Documents Act (PIPEDA) in Canada and the Payment Card Industry Data Security Standard (PCI DSS) for payment card data.\n\nStaying up-to-date with these regulations and ensuring compliance is crucial for avoiding legal penalties, maintaining customer trust, and operating ethically in the age of AI.\n\nBest practices for data privacy in AI systems\n\nImplementing best practices for data privacy is essential to mitigate risks and ensure the responsible development and deployment of AI systems. Here are some key best practices to consider:\n• Data minimization: collect and process only the personal data that is strictly necessary for the intended purpose. Minimize the collection and retention of unnecessary data to reduce privacy risks.\n• Consent and transparency: Obtain explicit and informed consent from individuals for the collection and use of their personal data. Provide clear and transparent information about data processing practices, purposes, and potential risks.\n• Access and control: Empower individuals with the ability to access, correct, and delete their personal data, as well as the right to opt-out or withdraw consent for its use in AI systems.\n• Data security: Implement robust security measures, such as encryption, access controls, and secure data storage, to protect personal data from unauthorized access, breaches, or misuse.\n• Privacy by design: Incorporate privacy principles and safeguards from the early stages of AI system design and development, rather than treating them as an afterthought.\n• Anonymization and de-identification: Employ techniques like data anonymization and de-identification to remove or obfuscate personally identifiable information, while still preserving the utility of the data for AI systems.\n• Ethical AI development: Adopt ethical AI principles and frameworks to ensure fairness, accountability, transparency, and respect for human rights in the development and deployment of AI systems.\n• Continuous monitoring and auditing: Regularly monitor and audit AI systems for compliance with data privacy regulations and best practices, and promptly address any identified issues or vulnerabilities.\n\nBy implementing these best practices, you can demonstrate your commitment to responsible AI development, build trust with stakeholders, and mitigate the risks associated with data privacy violations.\n\nSee how the TrustCloud offers a streamlined approach to managing audits through the TrustOps audit dashboard with the help of AI.\n\nIn the age of AI, building and maintaining customer trust is paramount. Customers are increasingly aware of data privacy concerns and expect organizations to handle their personal information responsibly and transparently. By adopting transparent data practices, you can foster trust, build stronger relationships with customers, and differentiate yourself from competitors.\n\nHere are some strategies to build trust through transparent data practices:\n• Clear and accessible privacy policies: Develop clear and easy-to-understand privacy policies that explain how personal data is collected, used, shared, and protected. Make these policies readily available and easily accessible to customers.\n• Proactive communication: Proactively communicate with customers about any changes or updates to your data practices, and provide them with the opportunity to consent or opt-out as necessary.\n• Data breach transparency: In the unfortunate event of a data breach, be transparent and promptly notify affected customers, providing them with clear information about the incident and the steps being taken to mitigate the impact and prevent future occurrences.\n• Third-party audits and certifications: Consider undergoing third-party audits or obtaining certifications, such as ISO 27001 for information security management, to demonstrate your commitment to data privacy and security.\n• Customer control and choice: Empower customers with control over their personal data by providing them with options to access, modify, or delete their information, as well as the ability to opt-out of certain data processing activities.\n• Ethical AI principles: Adopt and communicate your organization’s ethical AI principles, highlighting your commitment to responsible data practices, fairness, accountability, and respect for individual privacy.\n\nBy fostering trust through transparent data practices, you can build stronger customer relationships, enhance brand loyalty, and position your organization as a responsible and ethical leader in the age of AI.\n\nRead our GRC Launchpad article: GRC Automation in governance: unleashing the potential of leveraging AI\n\nSecure data storage and encryption are critical components of a comprehensive data privacy strategy. With the increasing volume and sensitivity of personal data being collected and processed by AI systems, implementing robust security measures is essential to protect this information from unauthorized access, breaches, or misuse.\n\nHere are some key practices for implementing secure data storage and encryption:\n• Encryption at rest and in transit: Encrypt all personal data, both when it is stored (at rest) and when it is being transmitted (in transit), using industry-standard encryption algorithms and protocols, such as AES-256 and TLS/SSL.\n• Access controls and least privilege: Implement strict access controls and follow the principle of least privilege, ensuring that only authorized personnel have access to personal data, and only to the extent necessary for their legitimate business purposes.\n• Secure data centers and cloud storage: If storing data on-premises, ensure that your data centers have robust physical security measures in place. If using cloud storage services, choose reputable providers with strong security credentials and data privacy commitments.\n• Key management and rotation: Implement secure key management practices, including regular key rotation, to protect encryption keys and prevent unauthorized access to encrypted data.\n• Secure data disposal: When personal data is no longer needed, ensure secure and permanent disposal methods, such as data wiping or physical destruction of storage media, to prevent data leaks or unauthorized access.\n• Security monitoring and incident response: Implement proactive security monitoring and incident response processes to detect and respond to potential security incidents or data breaches in a timely and effective manner.\n• Regular security audits and penetration testing: Conduct regular security audits and penetration testing to identify and address potential vulnerabilities in your data storage and encryption practices.\n\nBy implementing these secure data storage and encryption practices, you can significantly reduce the risk of data breaches, protect sensitive personal information, and demonstrate your commitment to data privacy and security.\n\nData anonymization and de-identification are critical techniques for protecting individual privacy while still enabling the use of data for AI systems and other analytical purposes. These techniques involve removing or obfuscating personally identifiable information (PII) from datasets, making it difficult or impossible to link the data to specific individuals.\n\nHere are some common data anonymization and de-identification techniques:\n• Data masking: This technique involves replacing sensitive data elements, such as names, addresses, or identification numbers, with fictitious or masked values, while preserving the overall structure and format of the data.\n• Data aggregation: Aggregating individual data records into larger groups or categories can help obscure individual identities while still providing valuable insights for analysis.\n• Data pseudonymization: This technique replaces direct identifiers, such as names or social security numbers, with pseudonyms or coded values, allowing data to be processed without directly identifying individuals.\n• Differential privacy: Differential privacy is a mathematical technique that introduces controlled noise or randomization to datasets, ensuring that the presence or absence of any individual’s data has a negligible impact on the overall results.\n• Synthetic data generation: Synthetic data generation involves creating artificial datasets that mimic the statistical properties and patterns of real-world data, without containing any actual personal information.\n• K-anonymity: This technique ensures that each record in a dataset is indistinguishable from at least k-1 other records, making it difficult to identify individuals based on combinations of quasi-identifiers, such as age, gender, and zip code.\n\nBy employing these techniques, organizations can strike a balance between protecting individual privacy and enabling the use of data for AI systems and other analytical purposes, while complying with data privacy regulations and ethical principles.\n\nAs AI systems become increasingly integrated into various aspects of our lives, it is crucial to ensure that their development and usage adhere to ethical principles and respect individual privacy. Ethical AI development and usage involve a range of considerations, including:\n• Fairness and non-discrimination: AI systems should be designed and trained to avoid perpetuating or amplifying existing biases and discriminatory practices. Ensuring fairness and equal treatment for all individuals, regardless of protected characteristics such as race, gender, or age, is essential.\n• Transparency and accountability: AI systems should be transparent in their decision-making processes, and there should be clear lines of accountability for their actions and outcomes. Explainable AI techniques can help achieve this goal.\n• Human oversight and control: While AI systems can automate certain tasks, it is important to maintain meaningful human oversight and control, particularly in high-stakes decision-making processes that can significantly impact individuals’ lives.\n• Privacy and data protection: As discussed throughout this article, the development and deployment of AI systems must prioritize the protection of individual privacy and adhere to data privacy regulations and best practices.\n• Societal benefit and well-being: AI systems should be designed and used in a manner that promotes societal benefit and well-being, rather than causing harm or exacerbating existing inequalities.\n• Ethical governance and oversight: Organizations should establish robust ethical governance frameworks, including oversight committees, advisory boards, and clear policies and procedures, to ensure the responsible development and deployment of AI systems.\n\nBy embedding ethical principles into the core of AI development and usage, organizations can build trust with stakeholders, mitigate risks, and contribute to the responsible advancement of AI technologies that respect individual privacy and promote societal well-being.\n\nConducting regular data privacy audits and ensuring compliance with relevant regulations and best practices is crucial for organizations operating in the age of AI. These audits help identify potential vulnerabilities, gaps, or areas of non-compliance, enabling organizations to take proactive measures to address them.\n\nHere are some key aspects of data privacy audits and compliance:\n• Regulatory compliance assessment: Assess your organization’s compliance with relevant data privacy regulations, such as the GDPR, CCPA, HIPAA, or industry-specific regulations. Identify any areas of non-compliance and develop remediation plans.\n• Data inventory and mapping: Conduct a comprehensive inventory and mapping of all personal data collected, processed, and stored by your organization, including data sources, data flows, and data storage locations.\n• Data privacy impact assessments (DPIAs): Perform DPIAs for high-risk data processing activities, such as the deployment of new AI systems or the introduction of new data collection methods, to identify and mitigate potential privacy risks.\n• Access controls and data handling practices: Review and evaluate the effectiveness of your access controls, data handling practices, and security measures to ensure the protection of personal data.\n• Third-party vendor assessments: Assess the data privacy practices and compliance of third-party vendors, partners, or service providers that have access to or process personal data on your behalf.\n• Incident response and breach notification processes: Review and test your incident response and data breach notification processes to ensure they are effective and compliant with regulatory requirements.\n• Employee training and awareness: Evaluate the effectiveness of your employee training and awareness programs on data privacy best practices, and identify areas for improvement.\n• Continuous monitoring and improvement: Establish processes for continuous monitoring and improvement of your data privacy practices, ensuring that they remain up-to-date and aligned with evolving regulations and industry best practices.\n\nBy conducting regular data privacy audits and ensuring compliance, you can proactively identify and address potential risks, demonstrate your commitment to data privacy, and maintain the trust of customers, partners, and regulatory authorities.\n\nListen to our podcasts on YouTube or Spotify—your go-to podcast series exploring the evolving landscape of security and governance, risk, and compliance (GRC).\n\nThe role of individuals in protecting their own data privacy\n\nWhile organizations have a significant responsibility for protecting data privacy, individuals also play a crucial role in safeguarding their personal information. In the age of AI, where vast amounts of data are collected and processed, it is essential for individuals to be proactive and take steps to protect their privacy.\n\nHere are some strategies individuals can adopt to protect their data privacy:\n• Be mindful of data sharing: Be cautious about the personal information you share online, on social media platforms, or with third-party applications. Only provide the necessary information required for legitimate purposes. Carefully review the privacy policies and terms of service of the applications, websites, and services you use to understand how your data is collected, used, and shared. Exercise your rights to access, correct, or delete your personal information as provided by these platforms.\n• Use privacy-enhancing tools and services: Leverage privacy-enhancing tools and services, such as virtual private networks (VPNs), ad blockers, and privacy-focused search engines, to minimize the collection and tracking of your online activities and personal data.\n• Strengthen account security: Implement strong and unique passwords for your online accounts, enable two-factor authentication where available, and be cautious of phishing attempts or other social engineering tactics that could compromise your account security.\n• Be selective with location services: Many applications and devices request access to your location data. Be selective about granting location permissions and consider disabling location services when not necessary.\n• Review and adjust privacy settings: Regularly review and adjust the privacy settings on your devices, applications, and online accounts to control the amount of personal information shared and limit data collection to only what is necessary.\n• Stay informed and educated: Stay up-to-date with the latest developments in data privacy, emerging threats, and best practices by following reputable sources and attending educational programs or workshops.\n• Support privacy-focused organizations and initiatives: Consider supporting organizations, initiatives, and advocacy groups that champion data privacy and work to protect individual rights in the digital age.\n\nBy taking an active role in protecting their personal information, individuals can exercise greater control over their data privacy and contribute to creating a more responsible and ethical data ecosystem in the age of AI.\n\nThe future of data privacy in the age of AI\n\nEven in fiture data privacy will remain a critical concern and a key determinant of trust between individuals, organizations, and technology. While AI holds immense potential for innovation and progress, its responsible development and deployment hinge on our ability to strike the right balance between harnessing its power and protecting individual privacy.\n\nThe future of data privacy in the age of AI will likely be shaped by several factors, including:\n• Evolving regulations and governance: We can expect to see continued evolution and refinement of data privacy regulations and governance frameworks, both at the national and international levels, to keep pace with technological advancements and address emerging privacy challenges.\n• Privacy-enhancing technologies: The development and adoption of privacy-enhancing technologies, such as advanced encryption, differential privacy, and secure multi-party computation, will play a crucial role in enabling the use of data for AI while preserving individual privacy.\n• Ethical AI frameworks and principles: The establishment and widespread adoption of ethical AI frameworks and principles will be essential to ensuring that AI systems are developed and deployed in a responsible and privacy-respecting manner.\n• Public awareness and advocacy: Increased public awareness and advocacy efforts will continue to shape the discourse around data privacy and hold organizations accountable for their data practices.\n• Collaboration and multistakeholder approaches: Addressing the complex challenges of data privacy in the age of AI will require collaboration and multistakeholder approaches, involving governments, industry, academia, civil society, and individuals working together to find balanced and effective solutions.\n\nIt is clear that protecting data privacy will be an ongoing journey, requiring continuous vigilance, adaptation, and a commitment to ethical principles. With responsible data practices, fostering trust, and prioritizing individual privacy, we can use the full potential of AI while preserving the fundamental rights and freedoms that underpin our digital society.\n\nJoin our TrustCommunity to learn about security, privacy, governance, risk and compliance, collaborate with your peers, and share and review the trust posture of companies that value trust and transparency!\n• Why is data privacy a concern with AI? AI systems rely heavily on data, often including personal information like browsing habits, location data, and even biometric identifiers. Without proper safeguards, this data could be misused, leading to serious consequences for individuals and organisations.\n• How can AI systems be discriminatory? AI algorithms can inherit biases from the data they are trained on. This can lead to unfair or discriminatory outcomes, for example, in loan approvals or job applications. It is crucial to address these biases to ensure fairness.\n• What are some key data privacy regulations I should be aware of?\n• GDPR (General Data Protection Regulation): Applicable to EU citizens’ data, regardless of the organisation’s location.\n• HIPAA (Health Insurance Portability and Accountability Act): Protects sensitive patient health information in the US."
    },
    {
        "link": "https://datacamp.com/tutorial/ethics-in-generative-ai",
        "document": "Learn how to use ChatGPT. Discover best practices for writing prompts and explore common business use cases for the powerful AI tool."
    }
]