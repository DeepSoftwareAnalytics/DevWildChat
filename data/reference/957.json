[
    {
        "link": "https://docs.python.org/3/library/math.html",
        "document": "This module provides access to the mathematical functions defined by the C standard.\n\nThese functions cannot be used with complex numbers; use the functions of the same name from the module if you require support for complex numbers. The distinction between functions which support complex numbers and those which don’t is made since most users do not want to learn quite as much mathematics as required to understand complex numbers. Receiving an exception instead of a complex result allows earlier detection of the unexpected complex number used as a parameter, so that the programmer can determine how and why it was generated in the first place.\n\nThe following functions are provided by this module. Except when explicitly noted otherwise, all return values are floats.\n\nReturn the number of ways to choose k items from n items without repetition and without order. Evaluates to when and evaluates to zero when . Also called the binomial coefficient because it is equivalent to the coefficient of k-th term in polynomial expansion of . Raises if either of the arguments are not integers. Raises if either of the arguments are negative. Return n factorial as an integer. Raises if n is not integral or is negative. Changed in version 3.10: Floats with integral values (like ) are no longer accepted. Return the greatest common divisor of the specified integer arguments. If any of the arguments is nonzero, then the returned value is the largest positive integer that is a divisor of all arguments. If all arguments are zero, then the returned value is . without arguments returns . Changed in version 3.9: Added support for an arbitrary number of arguments. Formerly, only two arguments were supported. Return the integer square root of the nonnegative integer n. This is the floor of the exact square root of n, or equivalently the greatest integer a such that a² ≤ n. For some applications, it may be more convenient to have the least integer a such that n ≤ a², or in other words the ceiling of the exact square root of n. For positive n, this can be computed using . Return the least common multiple of the specified integer arguments. If all arguments are nonzero, then the returned value is the smallest positive integer that is a multiple of all arguments. If any of the arguments is zero, then the returned value is . without arguments returns . Return the number of ways to choose k items from n items without repetition and with order. Evaluates to when and evaluates to zero when . If k is not specified or is , then k defaults to n and the function returns . Raises if either of the arguments are not integers. Raises if either of the arguments are negative.\n\nReturn the ceiling of x, the smallest integer greater than or equal to x. If x is not a float, delegates to , which should return an value. Return the absolute value of x. Return the floor of x, the largest integer less than or equal to x. If x is not a float, delegates to , which should return an value. Fused multiply-add operation. Return , computed as though with infinite precision and range followed by a single round to the format. This operation often provides better accuracy than the direct expression . This function follows the specification of the fusedMultiplyAdd operation described in the IEEE 754 standard. The standard leaves one case implementation-defined, namely the result of and . In these cases, returns a NaN, and does not raise any exception. Return the floating-point remainder of , as defined by the platform C library function . Note that the Python expression may not return the same result. The intent of the C standard is that be exactly (mathematically; to infinite precision) equal to for some integer n such that the result has the same sign as x and magnitude less than . Python’s returns a result with the sign of y instead, and may not be exactly computable for float arguments. For example, is , but the result of Python’s is , which cannot be represented exactly as a float, and rounds to the surprising . For this reason, function is generally preferred when working with floats, while Python’s is preferred when working with integers. Return the fractional and integer parts of x. Both results carry the sign of x and are floats. Note that has a different call/return pattern than its C equivalents: it takes a single argument and return a pair of values, rather than returning its second return value through an ‘output parameter’ (there is no such thing in Python). Return the IEEE 754-style remainder of x with respect to y. For finite x and finite nonzero y, this is the difference , where is the closest integer to the exact value of the quotient . If is exactly halfway between two consecutive integers, the nearest even integer is used for . The remainder thus always satisfies . Special cases follow IEEE 754: in particular, is x for any finite x, and and raise for any non-NaN x. If the result of the remainder operation is zero, that zero will have the same sign as x. On platforms using IEEE 754 binary floating point, the result of this operation is always exactly representable: no rounding error is introduced. Return x with the fractional part removed, leaving the integer part. This rounds toward 0: is equivalent to for positive x, and equivalent to for negative x. If x is not a float, delegates to , which should return an value. For the , , and functions, note that all floating-point numbers of sufficiently large magnitude are exact integers. Python floats typically carry no more than 53 bits of precision (the same as the platform C double type), in which case any float x with necessarily has no fractional bits.\n\nReturn a float with the magnitude (absolute value) of x but the sign of y. On platforms that support signed zeros, returns -1.0. Return the mantissa and exponent of x as the pair . m is a float and e is an integer such that exactly. If x is zero, returns , otherwise . This is used to “pick apart” the internal representation of a float in a portable way. Note that has a different call/return pattern than its C equivalents: it takes a single argument and return a pair of values, rather than returning its second return value through an ‘output parameter’ (there is no such thing in Python). Return if the values a and b are close to each other and otherwise. Whether or not two values are considered close is determined according to given absolute and relative tolerances. If no errors occur, the result will be: . rel_tol is the relative tolerance – it is the maximum allowed difference between a and b, relative to the larger absolute value of a or b. For example, to set a tolerance of 5%, pass . The default tolerance is , which assures that the two values are the same within about 9 decimal digits. rel_tol must be nonnegative and less than . abs_tol is the absolute tolerance; it defaults to and it must be nonnegative. When comparing to , is computed as , which is for any nonzero and rel_tol less than . So add an appropriate positive abs_tol argument to the call. The IEEE 754 special values of , , and will be handled according to IEEE rules. Specifically, is not considered close to any other value, including . and are only considered close to themselves. Return if x is neither an infinity nor a NaN, and otherwise. (Note that is considered finite.) Return if x is a positive or negative infinity, and otherwise. Return if x is a NaN (not a number), and otherwise. Return . This is essentially the inverse of function . Return the floating-point value steps steps after x towards y. If x is equal to y, return y, unless steps is zero.\n• None goes up: towards positive infinity.\n• None goes down: towards minus infinity.\n• None goes towards zero.\n• None goes away from zero. Return the value of the least significant bit of the float x:\n• None If x is a NaN (not a number), return x.\n• None If x is equal to zero, return the smallest positive denormalized representable float (smaller than the minimum positive normalized float, ).\n• None If x is equal to the largest positive representable float, return the value of the least significant bit of x, such that the first float smaller than x is .\n• None Otherwise (x is a positive finite number), return the value of the least significant bit of x, such that the first float bigger than x is . ULP stands for “Unit in the Last Place”. See also and .\n\nReturn e raised to the power x, where e = 2.718281… is the base of natural logarithms. This is usually more accurate than or . Return e raised to the power x, minus 1. Here e is the base of natural logarithms. For small floats x, the subtraction in can result in a significant loss of precision; the function provides a way to compute this quantity to full precision: With one argument, return the natural logarithm of x (to base e). With two arguments, return the logarithm of x to the given base, calculated as . Return the natural logarithm of 1+x (base e). The result is calculated in a way which is accurate for x near zero. Return the base-2 logarithm of x. This is usually more accurate than . returns the number of bits necessary to represent an integer in binary, excluding the sign and leading zeros. Return the base-10 logarithm of x. This is usually more accurate than . Return x raised to the power y. Exceptional cases follow the IEEE 754 standard as far as possible. In particular, and always return , even when x is a zero or a NaN. If both x and y are finite, x is negative, and y is not an integer then is undefined, and raises . Unlike the built-in operator, converts both its arguments to type . Use or the built-in function for computing exact integer powers. Changed in version 3.11: The special cases and were changed to return instead of raising , for consistency with IEEE 754.\n\nReturn the Euclidean distance between two points p and q, each given as a sequence (or iterable) of coordinates. The two points must have the same dimension. Return an accurate floating-point sum of values in the iterable. Avoids loss of precision by tracking multiple intermediate partial sums. The algorithm’s accuracy depends on IEEE-754 arithmetic guarantees and the typical case where the rounding mode is half-even. On some non-Windows builds, the underlying C library uses extended precision addition and may occasionally double-round an intermediate sum causing it to be off in its least significant bit. For further discussion and two alternative approaches, see the ASPN cookbook recipes for accurate floating-point summation. Return the Euclidean norm, . This is the length of the vector from the origin to the point given by the coordinates. For a two dimensional point , this is equivalent to computing the hypotenuse of a right triangle using the Pythagorean theorem, . Changed in version 3.8: Added support for n-dimensional points. Formerly, only the two dimensional case was supported. Changed in version 3.10: Improved the algorithm’s accuracy so that the maximum error is under 1 ulp (unit in the last place). More typically, the result is almost always correctly rounded to within 1/2 ulp. Calculate the product of all the elements in the input iterable. The default start value for the product is . When the iterable is empty, return the start value. This function is intended specifically for use with numeric values and may reject non-numeric types. Return the sum of products of values from two iterables p and q. Raises if the inputs do not have the same length. For float and mixed int/float inputs, the intermediate products and sums are computed with extended precision.\n\nThe mathematical constant π = 3.141592…, to available precision. The mathematical constant e = 2.718281…, to available precision. The mathematical constant τ = 6.283185…, to available precision. Tau is a circle constant equal to 2π, the ratio of a circle’s circumference to its radius. To learn more about Tau, check out Vi Hart’s video Pi is (still) Wrong, and start celebrating Tau day by eating twice as much pie! A floating-point positive infinity. (For negative infinity, use .) Equivalent to the output of . A floating-point “not a number” (NaN) value. Equivalent to the output of . Due to the requirements of the IEEE-754 standard, and are not considered to equal to any other numeric value, including themselves. To check whether a number is a NaN, use the function to test for NaNs instead of or . Example: Changed in version 3.11: It is now always available. CPython implementation detail: The module consists mostly of thin wrappers around the platform C math library functions. Behavior in exceptional cases follows Annex F of the C99 standard where appropriate. The current implementation will raise for invalid operations like or (where C99 Annex F recommends signaling invalid operation or divide-by-zero), and for results that overflow (for example, ). A NaN will not be returned from any of the functions above unless one or more of the input arguments was a NaN; in that case, most functions will return a NaN, but (again following C99 Annex F) there are some exceptions to this rule, for example or . Note that Python makes no effort to distinguish signaling NaNs from quiet NaNs, and behavior for signaling NaNs remains unspecified. Typical behavior is to treat all NaNs as though they were quiet. Complex number versions of many of these functions."
    },
    {
        "link": "https://geeksforgeeks.org/how-to-solve-a-pair-of-nonlinear-equations-using-python",
        "document": "Solving the nonlinear equation includes finding the values of variables that satisfy the equation. In Python, nonlinear equations can be solved using the SciPy, NumPy, and SymPy libraries. The methods and approaches we will discuss in this article will require the installation of these Python libraries.\n\nA nonlinear equation is an equation in which the minimum degree of at least one variable term is 2 or more than two and the relationship between a nonlinear equation's variables cannot be represented by a straight line when plotted on a graph.\n\nInstall these Python libraries using the following commands in your terminal:.\n\nBelow are some ways by which we can solve a pair of nonlinear equations using Python:\n\nWe will perform all methods on these equations:\n\nThis Python code uses the fsolve function from the scipy.optimize library to find the numerical solution to a system of nonlinear equations. The equations are defined in the equations function, where eq1 and eq2 represent the equations. The initial guess for the solution is set to [1, 1] for [x,y], and fsolve is used to iteratively use this guess until it reaches to a solution. The final solution is then printed."
    },
    {
        "link": "https://stackoverflow.com/questions/19542801/solving-non-linear-equations-in-python",
        "document": "There are two ways to do this.\n• Linearize the problem and solve it in the least-squares sense\n\nSo, as I understand your question, you know F, a, b, and c at 4 different points, and you want to invert for the model parameters X, Y, and Z. We have 3 unknowns and 4 observed data points, so the problem is overdetermined. Therefore, we'll be solving in the least-squares sense.\n\nIt's more common to use the opposite terminology in this case, so let's flip your equation around. Instead of:\n\nWhere we know , , , and at 4 different points (e.g. ).\n\nWe're just changing the names of the variables, not the equation itself. (This is more for my ease of thinking than anything else.)\n\nIt's actually possible to linearize this equation. You can easily solve for , , , and . To make this a bit easier, let's relabel things yet again:\n\nNow the equation is a lot simpler: . It's easy to do a least-squares linear inversion for , , , and . We can then get , , and from:\n\nOkay, let's write this up in matrix form. We're going to translate 4 observations of (the code we'll write will take any number of observations, but let's keep it concrete at the moment):\n\nOr: (I'm a geophysist, so we use for \"Green's Functions\" and for \"Model Parameters\". Usually we'd use for \"data\" instead of , as well.)\n\nIn python, this would translate to:\n\nYou could also solve this using , as @Joe suggested. The most accessible function in is which uses a Levenberg-Marquardt method by default.\n\nLevenberg-Marquardt is a \"hill climbing\" algorithm (well, it goes downhill, in this case, but the term is used anyway). In a sense, you make an initial guess of the model parameters (all ones, by default in ) and follow the slope of in your parameter space downhill to the bottom.\n\nCaveat: Picking the right non-linear inversion method, initial guess, and tuning the parameters of the method is very much a \"dark art\". You only learn it by doing it, and there are a lot of situations where things won't work properly. Levenberg-Marquardt is a good general method if your parameter space is fairly smooth (this one should be). There are a lot of others (including genetic algorithms, neural nets, etc in addition to more common methods like simulated annealing) that are better in other situations. I'm not going to delve into that part here.\n\nThere is one common gotcha that some optimization toolkits try to correct for that doesn't try to handle. If your model parameters have different magnitudes (e.g. ), you'll need to rescale things so that they're similar in magnitude. Otherwise 's \"hill climbing\" algorithms (like LM) won't accurately calculate the estimate the local gradient, and will give wildly inaccurate results. For now, I'm assuming that , , and have relatively similar magnitudes. Also, be aware that essentially all non-linear methods require you to make an initial guess, and are sensitive to that guess. I'm leaving it out below (just pass it in as the kwarg to ) because the default is a fairly accurate guess for .\n\nWith the caveats out of the way, expects to be passed a function, a set of points where the observations were made (as a single array), and the observed values.\n\nSo, if we write the function like this:\n\nWe'll need to wrap it to accept slightly different arguments before passing it to .\n\nStand-alone Example of the two methods:\n\nTo give you a full implementation, here's an example that\n• generates randomly distributed points to evaluate the function on,\n• evaluates the function on those points (using set model parameters),\n• and then inverts for the model parameters using both the linear and non-linear methods described above."
    },
    {
        "link": "https://docs.sympy.org/latest/guides/solving/index.html",
        "document": "The Python package SymPy can symbolically solve equations, differential equations, linear equations, nonlinear equations, matrix problems, inequalities, Diophantine equations, and evaluate integrals. SymPy can also solve numerically.\n\nThe Solving Guidance page provides recommendations applicable to many types of solving tasks.\n\nLearn how to use SymPy computer algebra system to:\n• None SymPy has a function called which is designed to find the solutions of an equation or system of equations, or the roots of a function. SymPy may or may not be what you need for a particular problem, so we recommend you use the links on this page to learn how to “solve” your problem.\n• None While a common, colloquial expression is, for example, “solve an integral,” in SymPy’s terminology it would be “evaluate an integral.” This page does not provide guidance for such tasks. Please search the documentation for the type of expression you want to evaluate."
    },
    {
        "link": "http://hplgit.github.io/prog4comp/doc/pub/p4c-sphinx-Python/._pylight007.html",
        "document": "As a reader of this book you are probably well into mathematics and often “accused” of being particularly good at “solving equations” (a typical comment at family dinners!). However, is it really true that you, with pen and paper, can solve many types of equations? Restricting our attention to algebraic equations in one unknown \\(x\\), you can certainly do linear equations: \\(ax+b=0\\), and quadratic ones: \\(ax^2 + bx + c = 0\\). You may also know that there are formulas for the roots of cubic and quartic equations too. Maybe you can do the special trigonometric equation \\(\\sin x + \\cos x = 1\\) as well, but there it (probably) stops. Equations that are not reducible to one of the mentioned cannot be solved by general analytical techniques, which means that most algebraic equations arising in applications cannot be treated with pen and paper!\n\nIf we exchange the traditional idea of finding exact solutions to equations with the idea of rather finding approximate solutions, a whole new world of possibilities opens up. With such an approach, we can in principle solve any algebraic equation.\n\nLet us start by introducing a common generic form for any algebraic equation:\n\nHere, \\(f(x)\\) is some prescribed formula involving \\(x\\). For example, the equation\n\nJust move all terms to the left-hand side and then the formula to the left of the equality sign is \\(f(x)\\).\n\nSo, when do we really need to solve algebraic equations beyond the simplest types we can treat with pen and paper? There are two major application areas. One is when using implicit numerical methods for ordinary differential equations. These give rise to one or a system of algebraic equations. The other major application type is optimization, i.e., finding the maxima or minima of a function. These maxima and minima are normally found by solving the algebraic equation \\(F'(x)=0\\) if \\(F(x)\\) is the function to be optimized. Differential equations are very much used throughout science and engineering, and actually most engineering problems are optimization problems in the end, because one wants a design that maximizes performance and minimizes cost.\n\nWe restrict the attention here to one algebraic equation in one variable, with our usual emphasis on how to program the algorithms. Systems of nonlinear algebraic equations with many variables arise from implicit methods for ordinary and partial differential equations as well as in multivariate optimization. However, we consider this topic beyond the scope of the current text.\n\nThe representation of a mathematical function \\(f(x)\\) on a computer takes two forms. One is a Python function returning the function value given the argument, while the other is a collection of points \\((x, f(x))\\) along the function curve. The latter is the representation we use for plotting, together with an assumption of linear variation between the points. This representation is also very suited for equation solving and optimization: we simply go through all points and see if the function crosses the \\(x\\) axis, or for optimization, test for a local maximum or minimum point. Because there is a lot of work to examine a huge number of points, and also because the idea is extremely simple, such approaches are often referred to as brute force methods. However, we are not embarrassed of explaining the methods in detail and implementing them. Assume that we have a set of points along the curve of a function \\(f(x)\\): We want to solve \\(f(x)=0\\), i.e., find the points \\(x\\) where \\(f\\) crosses the \\(x\\) axis. A brute force algorithm is to run through all points on the curve and check if one point is below the \\(x\\) axis and if the next point is above the \\(x\\) axis, or the other way around. If this is found to be the case, we know that \\(f\\) must be zero in between these two \\(x\\) points. More precisely, we have a set of \\(n+1\\) points \\((x_i, y_i)\\), \\(y_i=f(x_i)\\), \\(i=0,\\ldots,n\\), where \\(x_0 < \\ldots < x_n\\). We check if \\(y_i < 0\\) and \\(y_{i+1} > 0\\) (or the other way around). A compact expression for this check is to perform the test \\(y_i y_{i+1} < 0\\). If so, the root of \\(f(x)=0\\) is in \\([x_i, x_{i+1}]\\). Assuming a linear variation of \\(f\\) between \\(x_i\\) and \\(x_{i+1}\\), we have the approximation which, when set equal to zero, gives the root Given some Python implementation of our mathematical function, a straightforward implementation of the above numerical algorithm looks like 'Could not find any root in [ 'Find (the first) root as x= Note the nice use of setting to : we can simply test if root is None to see if we found a root and overwrote the value, or if we did not find any root among the tested points. Running this program with some function, say \\(f(x)=e^{-x^2}\\cos(4x)\\) (which has a solution at \\(x = \\frac{\\pi}{8}\\)), gives the root 0.392701, which has an error of \\(1.9\\cdot 10^{-6}\\). Increasing the number of points with a factor of ten gives a root with an error of \\(2.4\\cdot 10^{-8}\\). After such a quick “flat” implementation of an algorithm, we should always try to offer the algorithm as a Python function, applicable to as wide a problem domain as possible. The function should take \\(f\\) and an associated interval \\([a,b]\\) as input, as well as a number of points (\\(n\\)), and return a list of all the roots in \\([a,b]\\). Here is our candidate for a good implementation of the brute force rooting finding algorithm: This time we use another elegant technique to indicate if roots were found or not: is an empty list if the root finding was unsuccessful, otherwise it contains all the roots. Application of the function to the previous example can be coded as 'Could not find any roots' Note that evaluates to if is non-empty. This is a general test in Python: evaluates to if is non-empty or has a nonzero value. We realize that \\(x_i\\) corresponds to a maximum point if \\(y_{i-1} < y_i > y_{i+1}\\). Similarly, \\(x_i\\) corresponds to a minimum if \\(y_{i-1} > y_i < y_{i+1}\\). We can do this test for all “inner” points \\(i=1,\\ldots,n-1\\) to find all local minima and maxima. In addition, we need to add an end point, \\(i=0\\) or \\(i=n\\), if the corresponding \\(y_i\\) is a global maximum or minimum. The algorithm above can be translated to the following Python function (file brute_force_optimizer.py): # Let maxima and minima hold the indices corresponding # What about the end points? \\ The and functions are standard Python functions for finding the maximum and minimum element of a list or an object that one can iterate over with a for loop. An application to \\(f(x)=e^{-x^2}\\cos(4x)\\) looks like We shall consider the very simple problem of finding the square root of 9, which is the positive solution of \\(x^2=9\\). The nice feature of solving an equation whose solution is known beforehand is that we can easily investigate how the numerical method and the implementation perform in the search for the solution. The \\(f(x)\\) function corresponding to the equation \\(x^2=9\\) is Our interval of interest for solutions will be \\([0,1000]\\) (the upper limit here is chosen somewhat arbitrarily). In the following, we will present several efficient and accurate methods for solving nonlinear algebraic equations, both single equation and systems of equations. The methods all have in common that they search for approximate solutions. The methods differ, however, in the way they perform the search for solutions. The idea for the search influences the efficiency of the search and the reliability of actually finding a solution. For example, Newton’s method is very fast, but not reliable, while the bisection method is the slowest, but absolutely reliable. No method is best at all problems, so we need different methods for different problems. What is the difference between linear and nonlinear equations You know how to solve linear equations \\(ax+b=0\\): \\(x=-b/a\\). All other types of equations \\(f(x)=0\\), i.e., when \\(f(x)\\) is not a linear function of \\(x\\), are called nonlinear. A typical way of recognizing a nonlinear equation is to observe that \\(x\\) is “not alone” as in \\(ax\\), but involved in a product with itself, such as in \\(x^3 + 2x^2 -9=0\\). We say that \\(x^3\\) and \\(2x^2\\) are nonlinear terms. An equation like \\(\\sin x + e^x\\cos x=0\\) is also nonlinear although \\(x\\) is not explicitly multiplied by itself, but the Taylor series of \\(\\sin x\\), \\(e^x\\), and \\(\\cos x\\) all involve polynomials of \\(x\\) where \\(x\\) is multiplied by itself.\n\nNewton’s method, also known as Newton-Raphson’s method, is a very famous and widely used method for solving nonlinear algebraic equations. Compared to the other methods we will consider, it is generally the fastest one (usually by far). It does not guarantee that an existing solution will be found, however. A fundamental idea of numerical methods for nonlinear equations is to construct a series of linear equations (since we know how to solve linear equations) and hope that the solutions of these linear equations bring us closer and closer to the solution of the nonlinear equation. The idea will be clearer when we present Newton’s method and the secant method. Figure Illustrates the idea of Newton’s method with \\( f(x) = x^2 - 9 \\) , repeatedly solving for crossing of tangent lines with the \\( x \\) axis shows the \\(f(x)\\) function in our model equation \\(x^2-9=0\\). Numerical methods for algebraic equations require us to guess at a solution first. Here, this guess is called \\(x_0\\). The fundamental idea of Newton’s method is to approximate the original function \\(f(x)\\) by a straight line, i.e., a linear function, since it is straightforward to solve linear equations. There are infinitely many choices of how to approximate \\(f(x)\\) by a straight line. Newton’s method applies the tangent of \\(f(x)\\) at \\(x_0\\), see the rightmost tangent in Figure Illustrates the idea of Newton’s method with \\( f(x) = x^2 - 9 \\) , repeatedly solving for crossing of tangent lines with the \\( x \\) axis. This linear tangent function crosses the \\(x\\) axis at a point we call \\(x_1\\). This is (hopefully) a better approximation to the solution of \\(f(x)=0\\) than \\(x_0\\). The next fundamental idea is to repeat this process. We find the tangent of \\(f\\) at \\(x_1\\), compute where it crosses the \\(x\\) axis, at a point called \\(x_2\\), and repeat the process again. Figure Illustrates the idea of Newton’s method with \\( f(x) = x^2 - 9 \\) , repeatedly solving for crossing of tangent lines with the \\( x \\) axis shows that the process brings us closer and closer to the left. It remains, however, to see if we hit \\(x=3\\) or come sufficiently close to this solution. How do we compute the tangent of a function \\(f(x)\\) at a point \\(x_0\\)? The tangent function, here called \\(\\tilde f(x)\\), is linear and has two properties:\n• the tangent touches the curve at So, if we write the tangent function as \\(\\tilde f(x)=ax+b\\), we must require \\(\\tilde f'(x_0)=f'(x_0)\\) and \\(\\tilde f(x_0)=f(x_0)\\), resulting in The key step in Newton’s method is to find where the tangent crosses the \\(x\\) axis, which means solving \\(\\tilde f(x)=0\\): This is our new candidate point, which we call \\(x_1\\): With \\(x_0 = 1000\\), we get \\(x_1 \\approx 500\\), which is in accordance with the graph in Figure Illustrates the idea of Newton’s method with \\( f(x) = x^2 - 9 \\) , repeatedly solving for crossing of tangent lines with the \\( x \\) axis. Repeating the process, we get The general scheme of Newton’s method may be written as The computation in (162) is repeated until \\(f\\left(x_n\\right)\\) is close enough to zero. More precisely, we test if \\(|f(x_n)|<\\epsilon\\), with \\(\\epsilon\\) being a small number. We moved from 1000 to 250 in two iterations, so it is exciting to see how fast we can approach the solution \\(x=3\\). A computer program can automate the calculations. Our first try at implementing Newton’s method is in a function : The argument is the starting value, called \\(x_0\\) in our previous mathematical description. We use to ensure that an integer division does not happen by accident if and both are integers for some . To solve the problem \\(x^2=9\\) we also need to implement Why not use an array for the \\(x\\) approximations Newton’s method is normally formulated with an iteration index \\(n\\), Seeing such an index, many would implement this as Such an array is fine, but requires storage of all the approximations. In large industrial applications, where Newton’s method solves millions of equations at once, one cannot afford to store all the intermediate approximations in memory, so then it is important to understand that the algorithm in Newton’s method has no more need for \\(x_n\\) when \\(x_{n+1}\\) is computed. Therefore, we can work with one variable and overwrite the previous value: Running results in the approximate solution 3.000027639. A smaller value of will produce a more accurate solution. Unfortunately, the plain function does not return how many iterations it used, nor does it print out all the approximations \\(x_0,x_1,x_2,\\ldots\\), which would indeed be a nice feature. If we insert such a printout, a rerun results in We clearly see that the iterations approach the solution quickly. This speed of the search for the solution is the primary strength of Newton’s method compared to other methods. The function works fine for the example we are considering here. However, for more general use, there are some pitfalls that should be fixed in an improved version of the code. An example may illustrate what the problem is: let us solve \\(\\tanh(x)=0\\), which has solution \\(x=0\\). With \\(|x_0|\\leq 1.08\\) everything works fine. For example, \\(x_0\\) leads to six iterations if \\(\\epsilon=0.001\\): Adjusting \\(x_0\\) slightly to 1.09 gives division by zero! The approximations computed by Newton’s method become The division by zero is caused by \\(x_7=-1.26055913647\\cdot 10^{11}\\), because \\(\\tanh(x_7)\\) is 1.0 to machine precision, and then \\(f'(x)=1 - \\tanh(x)^2\\) becomes zero in the denominator in Newton’s method. The underlying problem, leading to the division by zero in the above example, is that Newton’s method diverges: the approximations move further and further away from \\(x=0\\). If it had not been for the division by zero, the condition in the loop would always be true and the loop would run forever. Divergence of Newton’s method occasionally happens, and the remedy is to abort the method when a maximum number of iterations is reached. Another disadvantage of the function is that it calls the \\(f(x)\\) function twice as many times as necessary. This extra work is of no concern when \\(f(x)\\) is fast to evaluate, but in large-scale industrial software, one call to \\(f(x)\\) might take hours or days, and then removing unnecessary calls is important. The solution in our function is to store the call in a variable ( ) and reuse the value instead of making a new call . To summarize, we want to write an improved function for implementing Newton’s method where we A more robust and efficient version of the function, inserted in a complete program Newtons_method.py for solving \\(x^2 - 9 = 0\\), is listed below. # Here, either a solution is found, or too many iterations Handling of the potential division by zero is done by a construction. Python tries to run the code in the block. If anything goes wrong here, or more precisely, if Python raises an exception caused by a problem (such as division by zero, array index out of bounds, use of undefined variable, etc.), the execution jumps immediately to the block. Here, the programmer can take appropriate actions. In the present case, we simply stop the program. (Professional programmers would avoid calling inside a function. Instead, they would raise a new exception with an informative error message, and let the calling code have another construction to stop the program.) The division by zero will always be detected and the program will be stopped. The main purpose of our way of treating the division by zero is to give the user a more informative error message and stop the program in a gentler way. Calling with an argument different from zero (here ) signifies that the program stopped because of an error. It is a good habit to supply the value , because tools in the operating system can then be used by other programs to detect that our program failed. To prevent an infinite loop because of divergent iterations, we have introduced the integer variable to count the number of iterations in Newton’s method. With we can easily extend the condition in the such that no more iterations take place when the number of iterations reaches 100. We could easily let this limit be an argument to the function rather than a fixed constant. The function returns the approximate solution and the number of iterations. The latter equals \\(-1\\) if the convergence criterion \\(|f(x)|<\\epsilon\\) was not reached within the maximum number of iterations. In the calling code, we print out the solution and the number of function calls. The main cost of a method for solving \\(f(x)=0\\) equations is usually the evaluation of \\(f(x)\\) and \\(f'(x)\\), so the total number of calls to these functions is an interesting measure of the computational work. Note that in function there is an initial call to \\(f(x)\\) and then one call to \\(f\\) and one to \\(f'\\) in each iteration. Running , we get the following printout on the screen: As we did with the integration methods in the chapter Computing integrals, we will collect our solvers for nonlinear algebraic equations in a separate file named for easy import and use. The first function placed in this file is then . The Newton scheme will work better if the starting value is close to the solution. A good starting value may often make the difference as to whether the code actually finds a solution or not. Because of its speed, Newton’s method is often the method of first choice for solving nonlinear algebraic equations, even if the scheme is not guaranteed to work. In cases where the initial guess may be far from the solution, a good strategy is to run a few iterations with the bisection method (see the chapter The bisection method) to narrow down the region where \\(f\\) is close to zero and then switch to Newton’s method for fast convergence to the solution. Newton’s method requires the analytical expression for the derivative \\(f'(x)\\). Derivation of \\(f'(x)\\) is not always a reliable process by hand if \\(f(x)\\) is a complicated function. However, Python has the symbolic package SymPy, which we may use to create the required function. In our sample problem, the recipe goes as follows: The nice feature of this code snippet is that is the exact analytical expression for the derivative, , if you print it out. This is a symbolic expression so we cannot do numerical computing with it, but the constructions turn symbolic expressions into callable Python functions. The next method is the secant method, which is usually slower than Newton’s method, but it does not require an expression for \\(f'(x)\\), and it has only one function call per iteration.\n\nWhen finding the derivative \\(f'(x)\\) in Newton’s method is problematic, or when function evaluations take too long; we may adjust the method slightly. Instead of using tangent lines to the graph we may use secants. The approach is referred to as the secant method, and the idea is illustrated graphically in Figure Illustrates the use of secants in the secant method when solving . From two chosen starting values, and the crossing of the corresponding secant with the axis is computed, followed by a similar computation of from and for our example problem \\(x^2 - 9 = 0\\). The idea of the secant method is to think as in Newton’s method, but instead of using \\(f'(x_n)\\), we approximate this derivative by a finite difference or the secant, i.e., the slope of the straight line that goes through the two most recent approximations \\(x_n\\) and \\(x_{n-1}\\). This slope reads Inserting this expression for \\(f'(x_n)\\) in Newton’s method simply gives us the secant method: Comparing (164) to the graph in Figure Illustrates the use of secants in the secant method when solving . From two chosen starting values, and the crossing of the corresponding secant with the axis is computed, followed by a similar computation of from and , we see how two chosen starting points (\\(x_0 = 1000\\), \\(x_1= 700\\), and corresponding function values) are used to compute \\(x_2\\). Once we have \\(x_2\\), we similarly use \\(x_1\\) and \\(x_2\\) to compute \\(x_3\\). As with Newton’s method, the procedure is repeated until \\(f(x_n)\\) is below some chosen limit value, or some limit on the number of iterations has been reached. We use an iteration counter here too, based on the same thinking as in the implementation of Newton’s method. We can store the approximations \\(x_n\\) in an array, but as in Newton’s method, we notice that the computation of \\(x_{n+1}\\) only needs knowledge of \\(x_n\\) and \\(x_{n-1}\\), not “older” approximations. Therefore, we can make use of only three variables: for \\(x_{n+1}\\), for \\(x_n\\), and for \\(x_{n-1}\\). Note that and must be given (guessed) for the algorithm to start. A program secant_method.py that solves our example problem may be written as: # Here, either a solution is found, or too many iterations The number of function calls is now related to , i.e., the number of iterations, as , since we need two function calls before entering the loop, and then one function call per loop iteration. Note that, even though we need two points on the graph to compute each updated estimate, only a single function call ( ) is required in each iteration since becomes the “old” and may simply be copied as (the exception is the very first iteration where two function evaluations are needed). Running , gives the following printout on the screen: As with the function , we place in the file for easy import and use later.\n\nNeither Newton’s method nor the secant method can guarantee that an existing solution will be found (see Exercise 73: Understand why Newton’s method can fail and Exercise 74: See if the secant method fails). The bisection method, however, does that. However, if there are several solutions present, it finds only one of them, just as Newton’s method and the secant method. The bisection method is slower than the other two methods, so reliability comes with a cost of speed. To solve \\(x^2 - 9 = 0\\), \\(x \\in \\left[0, 1000\\right]\\), with the bisection method, we reason as follows. The first key idea is that if \\(f(x) = x^2 - 9\\) is continuous on the interval and the function values for the interval endpoints (\\(x_L = 0\\), \\(x_R =1000\\)) have opposite signs, \\(f(x)\\) must cross the \\(x\\) axis at least once on the interval. That is, we know there is at least one solution. The second key idea comes from dividing the interval in two equal parts, one to the left and one to the right of the midpoint \\(x_M = 500\\). By evaluating the sign of \\(f(x_M)\\), we will immediately know whether a solution must exist to the left or right of \\(x_M\\). This is so, since if \\(f(x_M) \\ge 0\\), we know that \\(f(x)\\) has to cross the \\(x\\) axis between \\(x_L\\) and \\(x_M\\) at least once (using the same argument as for the original interval). Likewise, if instead \\(f(x_M) \\le 0\\), we know that \\(f(x)\\) has to cross the \\(x\\) axis between \\(x_M\\) and \\(x_R\\) at least once. In any case, we may proceed with half the interval only. The exception is if \\(f(x_M) \\approx 0\\), in which case a solution is found. Such interval halving can be continued until a solution is found. A “solution” in this case, is when \\(|f(x_M)|\\) is sufficiently close to zero, more precisely (as before): \\(|f(x_M)|<\\epsilon\\), where \\(\\epsilon\\) is a small number specified by the user. The sketched strategy seems reasonable, so let us write a reusable function that can solve a general algebraic equation \\(f(x)=0\\) (bisection_method.py): \"Error! Function does not have opposite Note that we first check if \\(f\\) changes sign in \\([a,b]\\), because that is a requirement for the algorithm to work. The algorithm also relies on a continuous \\(f(x)\\) function, but this is very challenging for a computer code to check. We get the following printout to the screen when is run: We notice that the number of function calls is much higher than with the previous methods. If the starting interval of the bisection method is bounded by \\(a\\) and \\(b\\), and the solution at step \\(n\\) is taken to be the middle value, the error is bounded as \\[ \\tag{165} \\frac{|b-a|}{2^n},\\]\\[because the initial interval has been halved \\( n \\) times. Therefore, to meet a tolerance \\( \\epsilon \\) , we need \\( n \\) iterations such that the length of the current interval equals \\( \\epsilon \\) :\\]\\[.. math:: \\frac{|b-a|}{2^n}=\\epsilon\\quad\\Rightarrow\\quad n = \\frac{\\ln ((b-a)/\\epsilon)}{\\ln 2}\\thinspace .\\]\\[ This is a great advantage of the bisection method: we know beforehand how many iterations \\( n \\) it takes to meet a certain accuracy \\( \\epsilon \\) in the solution.\\] As with the two previous methods, the function is placed in the file for easy import and use.\n\nWith the methods above, we noticed that the number of iterations or function calls could differ quite substantially. The number of iterations needed to find a solution is closely related to the rate of convergence, which is the speed of the error as we approach the root. More precisely, we introduce the error in iteration \\(n\\) as \\(e_n=|x-x_n|\\), and define the convergence rate \\(q\\) as where \\(C\\) is a constant. The exponent \\(q\\) measures how fast the error is reduced from one iteration to the next. The larger \\(q\\) is, the faster the error goes to zero, and the fewer iterations we need to meet the stopping criterion \\(|f(x)|<\\epsilon\\). A single \\(q\\) in (166) is defined in the limit \\(n\\rightarrow\\infty\\). For finite \\(n\\), and especially smaller \\(n\\), \\(q\\) will vary with \\(n\\). To estimate \\(q\\), we can compute all the errors \\(e_n\\) and set up (166) for three consecutive experiments \\(n-1\\), \\(n\\), and \\(n+1\\): Dividing these two equations by each other and solving with respect to \\(q\\) gives Since this \\(q\\) will vary somewhat with \\(n\\), we call it \\(q_n\\). As \\(n\\) grows, we expect \\(q_n\\) to approach a limit (\\(q_n\\rightarrow q\\)). To compute all the \\(q_n\\) values, we need all the \\(x_n\\) approximations. However, our previous implementations of Newton’s method, the secant method, and the bisection method returned just the final approximation. Therefore, we have extended the implementations in the module file such that the user can choose whether the final value or the whole history of solutions is to be returned. Each of the extended implementations now takes an extra parameter . This parameter is a boolean, set to if the function is supposed to return all the root approximations, or , if the function should only return the final approximation. As an example, let us take a closer look at : # Here, either a solution is found, or too many iterations The function is found in the file nonlinear_solvers.py. and get a list returned. With knowledge of the exact solution \\(x\\) of \\(f(x)=0\\) we can compute all the errors \\(e_n\\) and all the associated \\(q_n\\) values with the compact function The error model (166) works well for Newton’s method and the secant method. For the bisection method, however, it works well in the beginning, but not when the solution is approached. We can compute the rates \\(q_n\\) and print them nicely, The result for is indicating that \\(q=2\\) is the rate for Newton’s method. A similar computation using the secant method, gives the rates Here it seems that \\(q\\approx 1.6\\) is the limit. Remark. If we in the bisection method think of the length of the current interval containing the solution as the error \\(e_n\\), then (166) works perfectly since \\(e_{n+1}=\\frac{1}{2}e_n\\), i.e., \\(q=1\\) and \\(C=\\frac{1}{2}\\), but if \\(e_n\\) is the true error \\(|x-x_n|\\), it is easily seen from a sketch that this error can oscillate between the current interval length and a potentially very small value as we approach the exact solution. The corresponding rates \\(q_n\\) fluctuate widely and are of no interest.\n\nSo far in this chapter, we have considered a single nonlinear algebraic equation. However, systems of such equations arise in a number of applications, foremost nonlinear ordinary and partial differential equations. Of the previous algorithms, only Newton’s method is suitable for extension to systems of nonlinear equations. Suppose we have \\(n\\) nonlinear equations, written in the following abstract form: \\[\\tag{171} It will be convenient to introduce a *vector notation*\\] The system can now be written as \\(\\boldsymbol{F} (\\boldsymbol{x}) = \\boldsymbol{0}\\). As a specific example on the notation above, the system can be written in our abstract form by introducing \\(x_0=x\\) and \\(x_1=y\\). Then We follow the ideas of Newton’s method for one equation in one variable: approximate the nonlinear \\(f\\) by a linear function and find the root of that function. When \\(n\\) variables are involved, we need to approximate a vector function \\(\\boldsymbol{F}(\\boldsymbol{x})\\) by some linear function \\(\\tilde\\boldsymbol{F} = \\boldsymbol{J}\\boldsymbol{x} + \\boldsymbol{c}\\), where \\(\\boldsymbol{J}\\) is an \\(n\\times n\\) matrix and \\(\\boldsymbol{c}\\) is some vector of length \\(n\\). The technique for approximating \\(\\boldsymbol{F}\\) by a linear function is to use the first two terms in a Taylor series expansion. Given the value of \\(\\boldsymbol{F}\\) and its partial derivatives with respect to \\(\\boldsymbol{x}\\) at some point \\(\\boldsymbol{x}_i\\), we can approximate the value at some point \\(\\boldsymbol{x}_{i+1}\\) by the two first term in a Taylor series expansion around \\(\\boldsymbol{x}_i\\): The next terms in the expansions are omitted here and of size \\(||\\boldsymbol{x}_{i+1}-\\boldsymbol{x}_i||^2\\), which are assumed to be small compared with the two terms above. The expression \\(\n\nabla\\boldsymbol{F}\\) is the matrix of all the partial derivatives of \\(\\boldsymbol{F}\\). Component \\((i,j)\\) in \\(\n\nabla\\boldsymbol{F}\\) is For example, in our \\(2\\times 2\\) system (172)-(173) we can use SymPy to compute the Jacobian: The matrix \\(\n\nabla\\boldsymbol{F}\\) is called the Jacobian of \\(\\boldsymbol{F}\\) and often denoted by \\(\\boldsymbol{J}\\). The idea of Newton’s method is that we have some approximation \\(\\boldsymbol{x}_i\\) to the root and seek a new (and hopefully better) approximation \\(\\boldsymbol{x}_{i+1}\\) by approximating \\(\\boldsymbol{F}(\\boldsymbol{x}_{i+1})\\) by a linear function and solve the corresponding linear system of algebraic equations. We approximate the nonlinear problem \\(\\boldsymbol{F}(\\boldsymbol{x}_{i+1})=0\\) by the linear problem where \\(\\boldsymbol{J}(\\boldsymbol{x}_i)\\) is just another notation for \\(\n\nabla\\boldsymbol{F}(\\boldsymbol{x}_i)\\). The equation (174) is a linear system with coefficient matrix \\(\\boldsymbol{J}\\) and right-hand side vector \\(\\boldsymbol{F}(\\boldsymbol{x}_i)\\). We therefore write this system in the more familiar form where we have introduce a symbol \\(\\boldsymbol{\\delta}\\) for the unknown vector \\(\\boldsymbol{x}_{i+1}-\\boldsymbol{x}_i\\) that multiplies the Jacobian \\(\\boldsymbol{J}\\). The \\(i\\)-th iteration of Newton’s method for systems of algebraic equations consists of two steps:\n• Solve the linear system with respect to . Solving systems of linear equations must make use of appropriate software. Gaussian elimination is the most common, and in general the most robust, method for this purpose. Python’s package has a module that interfaces the well-known LAPACK package with high-quality and very well tested subroutines for linear algebra. The statement solves a system \\(Ax=b\\) with a LAPACK method based on Gaussian elimination. When nonlinear systems of algebraic equations arise from discretization of partial differential equations, the Jacobian is very often sparse, i.e., most of its elements are zero. In such cases it is important to use algorithms that can take advantage of the many zeros. Gaussian elimination is then a slow method, and (much) faster methods are based on iterative techniques. Here is a very simple implementation of Newton’s method for systems of nonlinear algebraic equations: J is the Jacobian of F. Both F and J must be functions of x. At input, x holds the start value. The iteration continues # Here, either a solution is found, or too many iterations We can test the function with the \\(2\\times 2\\) system (172)-(173): Here, the testing is based on the L2 norm of the error vector. Alternatively, we could test against the values of that the algorithm finds, with appropriate tolerances. For example, as chosen for the error norm, if , a tolerance of \\(10^{-4}\\) can be used for and .\n\nThe purpose of this exercise is to understand when Newton’s method works and fails. To this end, solve \\(\\tanh x=0\\) by Newton’s method and study the intermediate details of the algorithm. Start with \\(x_0=1.08\\). Plot the tangent in each iteration of Newton’s method. Then repeat the calculations and the plotting when \\(x_0=1.09\\). Explain what you observe. Solution. The program may be written as: # Here, either a solution is found, or too many iterations # Plot both f(x) and the tangent \\ Running the program with set to \\(1.08\\) produces a series of plots (and prints) showing the graph and the tangent for the present value of . There are quite many plots, so we do not show them here. However, the tangent line “jumps” around a few times before it settles. In the final plot the tangent line goes through the solution at \\(x = 0\\). The final printout brings the information: When we run the program anew, this time with set to \\(1.09\\), we get another series of plots (and prints), but this time the tangent moves away from the (known) solution. The final printout we get states that: Here, stands for “not a number”, meaning that we got no solution value for . That is, Newton’s method diverged. Exercise 74: See if the secant method fails¶ Does the secant method behave better than Newton’s method in the problem described in Exercise 73: Understand why Newton’s method can fail? Try the initial guesses Solution. The program may be written as: # Here, either a solution is found, or too many iterations The script converges with the three first-mentioned alternatives for \\(x_0\\) and \\(x_1\\). With the final set of parameter values, the method diverges with a printout: and a few more lines stating that an exception error has occurred. Exercise 75: Understand why the bisection method cannot fail¶ Solve the same problem as in Exercise 73: Understand why Newton’s method can fail, using the bisection method, but let the initial interval be \\([-5,3]\\). Report how the interval containing the solution evolves during the iterations. Solution. The code may be written as: Running the program produces the following printout: An attractive idea is to combine the reliability of the bisection method with the speed of Newton’s method. Such a combination is implemented by running the bisection method until we have a narrow interval, and then switch to Newton’s method for speed. Write a function that implements this idea. Start with an interval \\([a,b]\\) and switch to Newton’s method when the current interval in the bisection method is a fraction \\(s\\) of the initial interval (i.e., when the interval has length \\(s(b-a)\\)). Potential divergence of Newton’s method is still an issue, so if the approximate root jumps out of the narrowed interval (where the solution is known to lie), one can switch back to the bisection method. The value of \\(s\\) must be given as an argument to the function, but it may have a default value of 0.1. Try the new method on \\(\\tanh(x)=0\\) with an initial interval \\([-10,15]\\). Solution. The code may be written as: \"Error! Function does not have opposite \\ \\ Running the program produces the following printout: The purpose of this function is to verify the implementation of Newton’s method in the function in the file Construct an algebraic equation and perform two iterations of Newton’s method by hand or with the aid of SymPy. Find the corresponding size of \\(|f(x)|\\) and use this as value for when calling . The function should then also perform two iterations and return the same approximation to the root as you calculated manually. Implement this idea for a unit test as a test function . Solution. Here is the complete module with the test function. We use SymPy to do the manual calculations. # this eps gives two iterations An important engineering problem that arises in a lot of applications is the vibrations of a clamped beam where the other end is free. This problem can be analyzed analytically, but the calculations boil down to solving the following nonlinear algebraic equation: where \\(\\beta\\) is related to important beam parameters through where \\(\\varrho\\) is the density of the beam, \\(A\\) is the area of the cross section, \\(E\\) is Young’s modulus, and \\(I\\) is the moment of the inertia of the cross section. The most important parameter of interest is \\(\\omega\\), which is the frequency of the beam. We want to compute the frequencies of a vibrating steel beam with a rectangular cross section having width \\(b=25\\) mm and height \\(h=8\\) mm. The density of steel is \\(7850 \\mbox{ kg/m}^3\\), and \\(E= 2\\cdot 10^{11}\\) Pa. The moment of inertia of a rectangular cross section is \\(I=bh^3/12\\). a) Plot the equation to be solved so that one can inspect where the zero crossings occur. Hint. When writing the equation as \\(f(\\beta)=0\\), the \\(f\\) function increases its amplitude dramatically with \\(\\beta\\). It is therefore wise to look at an equation with damped amplitude, \\(g(\\beta) = e^{-\\beta}f(\\beta) = 0\\). Plot \\(g\\) instead. b) Compute the first three frequencies. Solution. Here is a complete program, using the Bisection method for root finding, based on intervals found from the plot above. \"\"\"Damp the amplitude of f. It grows like cosh, i.e. exp.\"\"\" The output of \\(\\beta\\) reads \\(1.875\\), \\(4.494\\), \\(7.855\\), and corresponding \\(\\omega\\) values are \\(29\\), \\(182\\), and \\(509\\) Hz."
    },
    {
        "link": "https://geeksforgeeks.org/program-for-bisection-method",
        "document": "The bisection method is a technique for finding solutions to equations with a single unknown variable. Among various numerical methods, it stands out for its simplicity and effectiveness, particularly when dealing with transcendental equations (those that cannot be solved using algebraic methods alone). The method is also called the interval halving method, the binary search method or the dichotomy method.\n\nThis method is used to find root of an equation in a given interval that is value of ‘x’ for which f(x) = 0 . \n\n\n\nThe bisection method is based on the Intermediate Value Theorem, which states that if f(x) is a continuous function on the interval [a, b] and f(a) and f(b) have opposite signs (i.e., f(a)⋅f(b)<0), then there is at least one root of the equation f(x)=0 in the interval (a,b).\n• None If f(c) = 0, then c is the root of the equation.\n• None\n• None If f(a)⋅f(c)<0, the root lies between a and c, so we recur with the interval [ a, c].\n• None Else, if f(b)⋅f(c)<0, the root lies between b and ccc, so we recur with the interval [ c, b].\n• None If neither of these conditions hold, then the function does not satisfy the assumptions of the bisection method.\n• None Since the root may be a floating-point number, we repeat the above steps until the difference between a and b is less than or equal to a very small value\n\nWe use the bisection method to find the root of the polynomial: f(x)=x3−x−2 Since f(1) and f(2) have opposite signs, there is a root between 1 and 2. Step 2: First Iteration, Calculate the midpoint: [Tex]c_1 = \\frac{1 + 2}{2} = 1.5[/Tex]\n\nEvaluate the function at c ​: f(1.5) = −0.125 Since f(1.5) is negative, update the interval to [1.5, 2]. Step 3: Repeat: Repeat the process until the interval becomes sufficiently small, converging on the root.\n\nA few steps of the bisection method applied over the starting range [a ;b ]. The bigger red dot is the root of the function.\n\n\n\nBelow is implementation of above steps. \n\n\n\n// C++ program for implementation of Bisection Method for // An example function whose solution is determined using // Prints root of func(x) with error of EPSILON \"You have not assumed right a and b // Decide the side to repeat the steps \"The value of root is : \" // An example function whose solution is determined using // Prints root of func(x) with error of EPSILON // Decide the side to repeat the steps //prints value of c upto 4 decimal places \"The value of root is : %.4f\" // This code is contributed by Nirmal Patel # Python program for implementation of Bisection Method for solving equations # An example function whose solution is determined using Bisection Method. \"You have not assumed right a and b # Decide the side to repeat the steps \"The value of root is : \" // An example function whose // This code is contributed by ajit // An example function whose solution is determined using // Prints root of func(x) with error of EPSILON // Decide the side to repeat the steps //prints value of c upto 4 decimal places // This code is contributed by susmitakundugoaldanga. // An example function whose \"The value of root is : \" // This code is contributed by ajit\n\nThe value of root is : -1.0025\n\nDetermine the root of the given equation x2−3 = 0 for x∈[1,2]x .\n\nNow, find the value of f(x)at a=1 and b=2.\n\nf(1)=12−3=1−3=−2 (which is <0), \n\nf(2)=22−3=4−3=1 (which is >0)\n\nThe given function is continuous, and since f(1)⋅f(2) < 0 , the root lies in the interval [1, 2].\n\nLet t be the midpoint of the interval: [Tex]t = \\frac{1 + 2}{2} = \\frac{3}{2} = 1.5[/Tex]\n\nNow, find the value of the function at t=1.5 :\n\nf(1.5)=(1.5)2−3=2.25−3=−0.75 (which is <0)\n\nIterations for the Given Function:\n\nAt iteration 7, the final interval is [1.7266, 1.7344].\n\nHence, the approximated solution is 1.7344.\n• Guaranteed Convergence: It always converges to a root if the function is continuous and f(a)⋅f(b) < 0.\n• No Derivatives Needed: Unlike other methods, it doesn’t require the derivative of the function.\n• Robust: Can be used for a wide range of equations, even in real-world applications.\n• Reliable: Provides a stable solution, though with slower convergence compared to some other methods.\n\nDisadvantage of Bisection Method is that it cannot detect multiple roots.\n\nProblem 1: Use the bisection method to find the root of f(x) = x2−5 in the interval [2,3] up to 4 decimal places.\n\nProblem 2: Apply the bisection method to solve f(x) = cos⁡(x)−x in the interval [0, 1] up to 3 decimal places.\n\nProblem 3: Use the bisection method to find the root of f(x) = x3−2x−5min the interval [2 , 3 ]up to 5 decimal places.\n\nProblem 4: Solve the equation f(x) = x2−2x−3 for a root in the interval [1, 4] using the bisection method.\n\nProblem 5: Use the bisection method to approximate the root of f(x)=ex−3 in the interval [0, 2] to 3 decimal places.\n\nHow accurate is the bisection method?\n\nWhat are Algebraic and Transcendental functions?\n\nCan the bisection method find all types of roots?\n\nWhat happens if f(a)⋅f(b) ≥ 0 ?\n\nHow does the bisection method compare to other root-finding methods?"
    },
    {
        "link": "https://flexiple.com/python/bisection-method-python",
        "document": "The Bisection Method in Python efficiently finds a function's root by repeatedly dividing an interval. First, define the function and identify an interval where the sign of the function changes. Implement the method by halving this interval iteratively. In each step, evaluate the function at the midpoint and adjust the interval bounds based on the sign. This process continues until the interval is sufficiently small, pinpointing the root with desired accuracy. Python's simplicity and precision make it ideal for implementing this numerical method. This guide delves into the concepts behind the Bisection Method and demonstrates its implementation in Python.\n\nThe Bisection Method, at its core, is an iterative algorithm used to find a root (a point where the function equals zero) of a continuous function in a specified interval. The method operates under the assumption that the function changes sign over the interval, indicating the presence of a root.\n\nHow Does It Work?\n• Initial Interval: Select an interval [a,b] where the function f(x) changes sign, i.e., f(a)×f(b)<0.\n• Midpoint Calculation: Find the midpoint c = (a + b) / 2 of the interval.\n• Interval Halving: Check the sign of f(c). If f(c) is of the same sign as f(a), replace a with c; otherwise, replace b with c.\n• Convergence Check: Repeat the process until the interval is sufficiently small or until f(c) is close enough to zero.\n\nImplementing the Bisection Method in Python starts with defining the target function and setting the initial interval. You need to ensure that the function's values at the interval endpoints have opposite signs.\n\nThe step-by-step guide to implementing the methods is given below.\n• While the interval width is larger than the tolerance:\n• Check the function's sign at c.\n• Update a or b based on where the sign change occurs.\n\nThis code snippet efficiently finds the root of x^2 - 4, demonstrating the Bisection Method's precision and Python's effectiveness in numerical computing.\n• Guaranteed Convergence: The method always converges to a root if it starts with an interval where the function changes signs.\n• Robustness: It works reliably for a wide range of functions, making it a go-to choice in numerical analysis.\n• Simplicity and Clarity: Python’s straightforward syntax makes implementing the Bisection Method easy and clear.\n• Efficiency: The method efficiently narrows down the root's location, while Python’s computational power ensures quick execution.\n• Accessibility: Ideal for beginners in numerical methods due to its straightforward algorithm.\n• Versatility in Function Types: It can be applied to any continuous function, enhancing its usability.\n• Minimal Pre-requisites: Requires minimal mathematical or computational background to implement and understand.\n• Strong Python Community Support: Extensive libraries and community resources in Python aid in refining and optimizing the method.\n• Slower Convergence: Compared to other methods like Newton-Raphson, it converges to the root more slowly.\n• Dependence on Initial Interval: The method requires a correctly chosen initial interval where the function changes signs.\n• Single Root Finding: It can only find one root at a time, even if multiple roots exist.\n• Continuous Functions Required: The Bisection Method is only applicable to continuous functions within the chosen interval.\n• Precision Limitation: The method's precision depends on the width of the final interval and the tolerance set.\n• Inefficiency with Flat Functions: Functions with flat regions near the root can slow down the convergence significantly.\n• No Derivative Information Used: Unlike methods that use derivative information, Bisection only relies on function values, which can be a drawback in some cases.\n• Programming Overheads: Implementing the method in Python, although straightforward, requires careful handling of intervals and convergence criteria.\n\nThe Bisection Method in Python is a straightforward yet powerful tool for finding roots of continuous functions. While it has its limitations, its ease of implementation and robustness make it an excellent choice for many practical applications. As you venture into numerical computing, mastering such fundamental algorithms paves the way for tackling more complex computational challenges."
    },
    {
        "link": "https://stackoverflow.com/questions/14392208/how-to-do-the-bisection-method-in-python",
        "document": "I want to make a Python program that will run a bisection method to determine the root of:\n\nThe Bisection method is a numerical method for estimating the roots of a polynomial f(x).\n\nAre there any available pseudocode, algorithms or libraries I could use to tell me the answer?"
    },
    {
        "link": "https://lemesurierb.people.charleston.edu/introduction-to-numerical-methods-and-analysis-python/main/root-finding-by-interval-halving.html",
        "document": "\n• None Section 1.1 The Bisection Method in Numerical Analysis by Sauer [Sauer, 2019]\n• None Section 2.1 The Bisection Method in Numerical Analysis by Burden, Faires and Burden [Burden et al., 2016]\n\nOne of the most basic tasks in numerical computing is finding the roots (or “zeros”) of a function — solving the equation \\(f(x) = 0\\) where \\(f:\\mathbb{R} \\to \\mathbb{R}\\) is a continuous function from and to the real numbers. As with many topics in this course, there are multiple methods that work, and we will often start with the simplest and then seek improvement in several directions:\n• None reliability or robustness — how good it is at avoiding problems in hard cases, such as division by zero.\n• None accuracy and guarantees about accuracy like estimates of how large the error can be — since in most cases, the result cannot be computed exactly.\n• None speed or cost — often measure by minimizing the amount of arithemtic involved, or the number of times that a function must be evaluated. This is a simple equation for which there is no exact formula for a solution, but we can easily ensure that there is a solution, and moreover, a unique one. It is convenient to put the equation into “zero-finding” form \\(f(x) = 0\\), by defining Also, note that \\(|\\cos x| \\leq 1\\), so a solution to the original equation must have \\(|x| \\leq 1\\). So we will start graphing the function on the interval \\([a, b] = [-1, 1]\\). This is our first use of two Python packages that some of you might not have seen before: Numpy and Matplotlib. If you want to learn more about them, see for example the Python Review sections on Python Variables, Lists, Tuples, and Numpy arrays and Graphing with Matplotlib Or for now, just learn from the examples here. # We will often need resources from the modules numpy and pyplot: # We can also import items from a module individually, so they can be used by \"first name only\". # Here this is done for mathematical functions; in some later sections it will be done for all imports. # If you want to see what `linspace` gives, run this cell This shows that the zero lies between 0.5 and 0.75, so zoom in: And we could repeat, geting an approximation of any desired accuracy. However this has two weaknesses: it is very inefficient (the function is evaluated about fifty times at each step in order to draw the graph), and it requires lots of human intervention. To get a procedure that can be efficiently implemented in Python (or another programming language of your choice), we extract one key idea here: finding an interval in which the function changes sign, and then repeatedly find a smaller such interval within it. The simplest way to do this is to repeatedly divide an interval known to contain the root in half and check which half has the sign change in it. Graphically, let us start again with interval \\([a, b] = [-1, 1]\\), but this time focus on three points of interest: the two ends and the midpoint, where the interval will be bisected: # And just as a visual aid: Note on line 3 above that the function from Numpy (full name ) can be evaluated simultaneously on a list of numbers; the version from module can only handle one number at a time. This is one reason why we will avoid in favor of . \\(f(a)\\) and \\(f(c)\\) have the same sign, while \\(f(c)\\) and \\(f(b)\\) have opposite signs, so the root is in \\([c, b]\\); update the a, b, c values and plot again: # new left end is old center # redundant, as the right end is unchanged Again \\(f(c)\\) and \\(f(b)\\) have opposite signs, so the root is in \\([c, b]\\), and … # new left end is old center again This time \\(f(a)\\) and \\(f(c)\\) have opposite sign, so the root is at left, in \\([a, c]\\): # this time, the value of a does not need to be updated ... # ... and the new right end is the former center\n\nA first algorithm for the bisection method# Now it is time to dispense with the graphs, and describe the procedure in mathematical terms:\n• None if \\(f(a)\\) and \\(f(c)\\) have opposite signs, the root is in interval \\([a, c]\\), which becomes the new version of interval \\([a, b]\\).\n• None otherwise, \\(f(c)\\) and \\(f(b)\\) have opposite signs, so the root is in interval \\([c, b]\\) As a useful bridge from the mathematical desciption of an algorithm with words and formulas to actual executable code, these notes will often describe algorithms in pseudo-code — a mix of words and mathematical formulas with notation that somewhat resembles code in a language like Python. This is also preferable to going straight to code in a particular language (such as Python) because it makes it easier if, later, you wish to implement algorithms in a different programming language. Note well one feature of the pseudo-code used here: assignment is denoted with a left arrow: is the instruction to cause the value of variable to become the current value of a. This is to distinguish from which is a comparison: the true-or-false assertion that the two quantities already have the same value. Unfortunately however, Python (like most programming languages) does not use this notation: instead assignment is done with so that asserting equality needs a differnt notation: this is done with ; note well that double equal sign! Also, the pseudo-code marks the end of blocks like , and with a lines . Many programming languages do something like this, but Python does not: instead it uses only the end of indentation as the indication that a block is finished. With those notational issues out of the way, the key step in the bisection strategy is the update of the interval: \\(\\displaystyle c \\leftarrow \\frac{a + b}{2}\\) \n\n if \\(f(a) f(c) < 0\\) then: \n\n \\(\\quad\\) \\(b \\leftarrow c\\) \n\n else: \n\n \\(\\quad\\) \\(a \\leftarrow c\\) \n\n end This needs to be repeated a finite number of times, and the simplest way is to specify the number of iterations. (We will consider more refined methods soon.)\n• None Get an initial interval \\([a, b]\\) with a sign-change: \\(f(a) f(b) < 0\\).\n• None for i from 1 to N: \n\n \\(\\quad\\) \\(\\displaystyle c \\leftarrow \\frac{a + b}{2}\\) \n\n \\(\\quad\\) if \\(f(a) f(c) < 0\\) then: \n\n \\(\\quad\\)\\(\\quad\\) \\(b \\leftarrow c\\) \n\n \\(\\quad\\) else: \n\n \\(\\quad\\)\\(\\quad\\) \\(a \\leftarrow c\\) \n\n \\(\\quad\\) end \n\n end\n• None The approximate root is the final value of \\(c\\). A Python version of the iteration is not a lot different: for i in range(N): c = (a+b)/2 if f(a) * f(c) < 0: b = c else: a = c Create a Python function which implements the first algorithm for bisection abive, which performd a fixed number \\(N\\) of iterations; the usage should be: Test it with the above example: \\(f(x) = x - \\cos x = 0\\), \\([a, b] = [-1, 1]\\)\n\nThe above method of iteration for a fixed number of times is simple, but usually not what is wanted in practice. Instead, a better goal is to get an approximation with a guaranteed maximum possible error: a result consisting of an approximation \\(\\tilde{r}\\) to the exact root \\(r\\) and also a bound \\(E_{max}\\) on the maximum possible error; a guarantee that \\(|r - \\tilde{r}| \\leq E_{max}\\). To put it another way, a guarantee that the root \\(r\\) lies in the interval \\([\\tilde{r} - E_{max}, \\tilde{r} + E_{max}]\\). In the above example, each iteration gives a new interval \\([a, b]\\) guaranteed to contain the root, and its midpoint \\(c = (a+b)/2\\) is with a distance \\((b-a)/2\\) of any point in that interval, so at each iteration, we can have:\n• None \\(\\tilde{r}\\) is the current value of \\(c = (a+b)/2\\)"
    },
    {
        "link": "https://medium.com/@cmluna2913/bisection-method-32c8fb2e76a0",
        "document": "When talking about root finding methods, Bisection Method comes up pretty often. This is because it’s a very simple and easy algorithm to implement into code to solve for roots.\n\nAs always, I’m going to walk through an example with the magic of python.\n\nTo begin, I will go through the main idea of how the Bisection Method works. Given a continuous function f and starting numbers a and b where f(a)f(b)<0, then I can continuously iterate through this method to find a close approximation to a root on the interval [a,b].\n\nI decided to let python come up with the function I will work with for me. I ended up getting f(x) = x⁶-7.\n\nAs I can see through this visualization, I have 2 roots. I will go through steps of finding the positive root. I can see that there is a root between 1 and 1.5, which will be my starting numbers respectively. Before I begin, it is important to check the condition that f(a)f(b) < 0, or else this method will not work.\n\nI get f(a) = f(1) = -6, and f(b) = f(1.5) = 4.390625. Thus, f(a)f(b) = -26.34375, which is less than 0. So the first step checks out. Implementing this into code, I can start as follows:\n\nAs you may notice, there is a threshold. This is because the threshold will serve as the error I am willing to stop at. I have set it as a default of 0.001, where the values a and b will have an absolute difference of at most 0.001.\n\nNow, I have to calculate the midpoint between a and b and compare the values. I have assigned this midpoint to the variable mid to start. Now, while the absolute difference between a and b are above my threshold, I will continue to iterate through the process. It will stop once this threshold is reached.\n\nI have to check several conditions and do the following:\n• If f(mid) is 0, then I can stop as I have found my root.\n• If f(a)f(mid) < 0, then I update my value b to have the same value as mid.\n• If f(a)f(mid) ≥ 0, then instead I update my value a to have the same value as mid.\n\nBy checking if f(a)f(mid) is less than 0, I am checking if I have taken a step in the correct direction. If f(a) and f(mid) are both negative or both positive, then I have missed my root. If only one of f(a) or f(mid) are negative, then that implies that my root is still between both a and b. Once I have met my threshold, I will show what the root approximation is! Implementing these steps into code will look a little something like:\n\nRunning this function I get an approximation of 1.3837890625.\n\nI have a very close approximation for my root! The Bisection Method can be very useful with functions where the roots aren’t so easy to find. There are other root finding methods that I will show at a later point."
    }
]