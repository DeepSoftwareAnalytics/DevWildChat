[
    {
        "link": "https://blog.cloudflare.com/randomness-101-lavarand-in-production",
        "document": "Lava lamps in the Cloudflare lobby Courtesy of @mahtin\n\nAs some of you may know, there's a wall of lava lamps in the lobby of our San Francisco office that we use for cryptography. In this post, we’re going to explore how that works. This post assumes no technical background. For a more in-depth look at the technical details, see LavaRand in Production: The Nitty-Gritty Technical Details.\n\nAs we’ve discussed in the past, cryptography relies on the ability to generate random numbers that are both unpredictable and kept secret from any adversary.\n\nBut “random” is a pretty tricky term; it’s used in many different fields to mean slightly different things. And like all of those fields, its use in cryptography is very precise. In some fields, a process is random simply if it has the right statistical properties. For example, the digits of pi are said to be random because all sequences of numbers appear with equal frequency (“15” appears as frequently as “38”, “426” appears as frequently as “297”, etc). But for cryptography, this isn’t enough - random numbers must be unpredictable.\n\nTo understand what unpredictable means, it helps to consider that all cryptography is based on an asymmetry of information. If you’re trying to securely perform some cryptographic operation, then what you’re concerned about is that somebody - an adversary - will try to break your security. The only thing that distinguishes you from the adversary is that you know some things that the adversary does not, and the job of cryptography is to ensure that this asymmetry of information is enough to keep you secure.\n\nLet’s consider a simple example. Imagine that you and a friend are trying to go to a movie, but you don’t want your nemesis to figure out which movie you’re going to go to (lest she show up and thwart your movie watching!). This week, it’s your turn to choose the movie. Once you’ve made your choice, you’re going to need to send a message to your friend telling him which movie you’ve chosen, but you’re going to need to ensure that even if your nemesis intercepts the message, she won’t be able to figure out what it says.\n\nYou devise the following scheme: since there are only two movies available to watch at the moment, you label one A and the other B. Then, while in the presence of your friend, you flip a coin. You agree on the following table outlining which message that you will send depending on your choice of what movie to watch and whether the coin comes up heads (H) or tails (T). Later, once you’ve made up your mind about which movie to see, you’ll use this table to send an encrypted message to your friend telling him which movie you’ve chosen.\n\nIf you were to decide on movie B, and the coin came up heads, you would send the message, “In Hertford, Hereford, and Hampshire, hurricanes hardly ever happen.” Since your friend knows that the coin came up heads - he was there when it happened - he knows that you must have decided on movie B. But consider it from your nemesis’ perspective. She doesn’t know the result of the coin toss - all she knows is that there’s a 50% chance that the coin came up heads, and a 50% chance that it came up tails. Thus, seeing the message “In Hertford, Hereford, and Hampshire, hurricanes hardly ever happen” doesn’t help her at all! There’s a 50% chance that the coin came up heads (implying movie B), and a 50% chance that it came up tails (implying movie A). She doesn’t know anything more than she knew before!\n\nLet’s return now to the concept of unpredictability. Imagine that the result of the coin toss was completely predictable - say your nemesis planted a trick coin that always comes up heads on the first toss, tails on the second, heads on the third, and so on. Since she would know that there was a 100% chance of the first coin toss coming up heads, then regardless of which message you sent, she would know which movie you were going to see. While the trick coin still exhibits some basic properties of “randomness” as the term is used in the field of statistics - it comes up heads as often as it comes up tails - it’s predictable, which makes it useless for cryptography. The takeaway: when we say random in the context of cryptography, we mean unpredictable.\n\nUnfortunately for cryptographers, if there’s one thing computers are good at, it’s being predictable. They can execute the same code a million times, and so long as they are given the same inputs each time, they’ll always come up with the same outputs. This is very good for reliability, but it’s tricky when it comes to cryptography - after all, we need unpredictability!\n\nThe solution to this problem is cryptographically-secure pseudorandom number generators (CSPRNGs). CSPRNGs are algorithms which, provided an input which is itself unpredictable, produce a much larger stream of output which is also unpredictable. This stream can be extended indefinitely, producing as much output as required at any time in the future. In other words, if you were to flip a coin a number of times (a process which is known to be unpredictable) and then use the output of those coin flips as the input to a CSPRNG, an adversary who wasn’t able to predict the output of those coin flips would also be unable to predict the output of the CSPRNG - no matter how much output was consumed from the CSPRNG.\n\nBut even though CSPRNGs are a very powerful tool, they’re only one half of the equation - they still need an unpredictable input to operate. But as we said, computers aren’t unpredictable, so aren’t we back at square one? Well, not quite. It turns out that computers do usually have sources of unpredictability that they can use, but they’re pretty slow. What we can do is combine that slow process of gathering unpredictable input with a CSPRNG, which can take that input and produce a much larger amount of input more quickly, and we can satisfy all of our randomness needs!\n\nBut where can a computer get such unpredictable input, even slowly? The answer is the real world. While computers provide a nice simplification of the world for programmers to live in, real, physical computers still exist in the real, physical world. And that world is unpredictable. Computers have all sorts of ways of taking input from the real world - temperature sensors, keyboards, network interfaces, etc. All of these provide the ability to take measurements of the real world, and all of those measurements have some degree of inherent inaccuracy. As we’ll explain in a moment, that inaccuracy is the same thing as unpredictability, and this can be used! Common sources of unpredictable randomness include measuring the temperature of the CPU with high - and thus inaccurate - precision, measuring the timing of keystrokes on the keyboard with high precision, etc. To see how this can be used to produce unpredictable randomness, consider the question, “what’s the temperature of the room I’m sitting in right now?” You can probably estimate to within a couple of degrees - say, somewhere between 70 and 75 degrees Fahrenheit. But you probably have no idea what the temperature is to 2 decimal places of accuracy - is it 73.42 degrees or 73.47 degrees? By measuring the temperature with high precision and then using only the low-order digits of the measured value, you can get highly unpredictable randomness just by observing the world around you. And so can computers.\n• None Randomness used in cryptography needs to be unpredictable.\n• None Computers can slowly get a small amount of unpredictable randomness by measuring their environment.\n• None Computers can greatly extend this randomness by using a CSPRNG which can quickly turn it into a large amount of unpredictable randomness.\n\nIf there’s one thing cryptographers are wary about, it’s certainty. Cryptographic systems are routinely shown to be less secure than they were originally thought to be, and we’re constantly updating our understanding of what algorithms are safe to use in what scenarios.\n\nSo it shouldn’t be any surprise that cryptographers like to hedge their bets by using more security than they believe necessary in case it turns out that one of their assumptions was wrong. It’s sort of like the cryptographer’s version of the engineering practice of designing buildings to withstand far more weight or wind or heat than they think will arise in practice.\n\nWhen it comes to randomness, this hedging often takes the form of mixing. Unpredictable random values have the neat property that if they’re mixed in the right way with more unpredictable random values, then the result is at least as unpredictable as either of the inputs. That means that if you mix a random value which is highly unpredictability with a random value which is somewhat predictable, the result will be a highly unpredictable value.\n\nThis mixing property is useful because it allows you to mix unpredictable random values from many sources, and if you later discover that one of those sources was less unpredictable than you’d originally thought, it’s still OK - the other sources come to the rescue.\n\nAt Cloudflare, we have thousands of computers in data centers all around the world, and each one of these computers needs cryptographic randomness. Historically, they got that randomness using the default mechanism made available by the operating system that we run on them, Linux.\n\nBut being good cryptographers, we’re always trying to hedge our bets. We wanted a system to ensure that even if the default mechanism for acquiring randomness was flawed, we’d still be secure. That’s how we came up with LavaRand.\n\nThe view from the camera\n\nLavaRand is a system that uses lava lamps as a secondary source of randomness for our production servers. A wall of lava lamps in the lobby of our San Francisco office provides an unpredictable input to a camera aimed at the wall. A video feed from the camera is fed into a CSPRNG, and that CSPRNG provides a stream of random values that can be used as an extra source of randomness by our production servers. Since the flow of the “lava” in a lava lamp is very unpredictable,1 “measuring” the lamps by taking footage of them is a good way to obtain unpredictable randomness. Computers store images as very large numbers, so we can use them as the input to a CSPRNG just like any other number.\n\nWe're not the first ones to do this. Our LavaRand system was inspired by a similar system first proposed and built by Silicon Graphics and patented in 1996 (the patent has since expired).\n\nHopefully we’ll never need it. Hopefully, the primary sources of randomness used by our production servers will remain secure, and LavaRand will serve little purpose beyond adding some flair to our office. But if it turns out that we’re wrong, and that our randomness sources in production are actually flawed, then LavaRand will be our hedge, making it just a little bit harder to hack Cloudflare.\n• None Noll, L.C. and Mende, R.G. and Sisodiya, S., Method for seeding a pseudo-random number generator with a cryptographic hash of a digitization of a chaotic system"
    },
    {
        "link": "https://cloudflare.com/learning/ssl/lava-lamp-encryption",
        "document": "Why does Cloudflare use lava lamps to help with encryption?\n\nRandomness is extremely important for secure encryption. Each new key that a computer uses to encrypt data must be truly random, so that an attacker won't be able to figure out the key and decrypt the data. However, computers are designed to provide predictable, logical outputs based on a given input. They aren't designed to produce the random data needed for creating unpredictable encryption keys.\n\nTo produce the unpredictable, chaotic data necessary for strong encryption, a computer must have a source of random data. The \"real world\" turns out to be a great source for randomness, because events in the physical world are unpredictable.\n\nAs one might expect, lava lamps are consistently random. The \"lava\" in a lava lamp never takes the same shape twice, and as a result, observing a group of lava lamps is a great source for random data.\n\nTo collect this data, Cloudflare has arranged about 100 lava lamps on one of the walls in the lobby of the Cloudflare headquarters and mounted a camera pointing at the lamps. The camera takes photos of the lamps at regular intervals and sends the images to Cloudflare servers. All digital images are really stored by computers as a series of numbers, with each pixel having its own numerical value, and so each image becomes a string of totally random numbers that the Cloudflare servers can then use as a starting point for creating secure encryption keys.\n\nThus, with the help of lava lamps, Cloudflare is able to offer extremely strong (and sufficiently random) SSL/TLS encryption to its customers. This is especially important considering that millions of Internet properties use Cloudflare.\n\nWhat does 'random' mean in the context of cryptography?\n\nIn cryptography, random does not just mean statistically random; it also means unpredictable.\n\nEncrypted data should look like totally random data, since predictable data can be guessed. If there is any pattern – if certain values are used for encryption more than others, or if values appear in a certain order consistently – then mathematical analysis will pick up on the pattern and allow an attacker to have a much easier time guessing the key used for encryption. Essentially, if encrypted data is predictable, it might as well already be compromised.\n\nThe process of encryption itself is a predictable one: Encrypted data plus the right key equals decrypted data, and the decrypted data is the same as it was before it was encrypted. But the encryption keys used have to be unpredictable.\n\nTo understand why unpredictability is so important, imagine two poker players: Bob always bets when he has good cards and folds (declines to match other players' bets) when he has bad cards. Alice, meanwhile, mixes up her betting strategy so that there's no discernable pattern to it: sometimes she bets when she has good cards, sometimes she contents herself with matching other players' bets, and sometimes she even bluffs by betting big when she has bad cards. When Alice and Bob enter the same poker tournament, Alice lasts much longer than Bob does, because Bob is too predictable. Opponents quickly figure out when Bob has good cards and react accordingly. Even though they can't see his cards, they can discern roughly what cards he's holding.\n\nSimilarly, even though attackers can't see the \"cards\" – or, the encrypted content – that's sent over a network, they can guess it if the method for concealing the content is too predictable.\n\nComputers run on logic. A computer program is based on if-then statements: If certain conditions are met, then perform this specified action. The same input into a program results in the same output every time.\n\nThis is by design. An input should lead to an expected output, not an unexpected one. Imagine the chaos if a printer printed random text that was different from the text in the document that was sent to the printer, or if smartphones were to call a different phone number than the one the user entered. Computers are only useful because of their (relative) reliability and predictability. However, that predictability is a liability when it comes to generating secure encryption keys.\n\nSome computer programs are good at simulating randomness, but not good enough at it for creating encryption keys.\n\nHow can a computer use random, real-world inputs to generate random data?\n\nA software program called a pseudorandom number generator (PRNG) is able to take an unpredictable input and use it to generate unpredictable outputs. Theoretically, a PRNG can produce unlimited random outputs from a random input.\n\nSuch an algorithm is called \"pseudorandom\" and not \"random\" because its outputs are not actually completely random. Why is this the case? There are 2 main reasons:\n• When given the same seed to start with twice in a row, a PRNG will produce the exact same results.\n• It's difficult to prove if the results it generates will be completely random the entire time (if the PRNG runs indefinitely).\n\nBecause of reason No. 2, the algorithm continually needs new inputs of randomness. A random input is known as a \"cryptographic seed.\"\n\nA cryptographically secure pseudorandom number generator, or CSPRNG, is a PRNG that meets more stringent standards, making it safer to use for cryptography. A CSPRNG meets two requirements that PRNGs may not necessarily meet:\n• It has to pass certain statistical randomness tests to prove unpredictability.\n• An attacker must not be able to predict the outputs of the CSPRNG even if they have partial access to the program.\n\nLike a PRNG, a CSPRNG needs random data (the cryptographic seed) as a starting point from which to produce more random data.\n\nTo generate encryption keys for SSL/TLS encryption, Cloudflare uses a CSPRNG, with data collected from the lava lamps as part of the cryptographic seed.\n\nA cryptographic seed is the data that a CSPRNG starts with for generating random data. Although a CSPRNG could theoretically produce unlimited random outputs from a single cryptographic seed, it is far more secure to regularly refresh the cryptographic seed. An attacker may eventually compromise the initial cryptographic seed – and remember, a CSPRNG will produce the exact same outputs again if it is fed the same seed, so the attacker could then duplicate the random outputs. Additionally, even the most rigorously tested CSPRNG is not guaranteed to produce unpredictable results indefinitely.\n\nWith the lava lamps, Cloudflare has a continual source for new cryptographic seed data. Each image the camera takes of the lamps is different, resulting in a different random sequence of numerical values that can be used as a seed.\n\nAre the lava lamps the only source for the cryptographic seed?\n\nMany operating systems have their own sources of random data for use in cryptographic seeds, for instance from user actions (mouse movements, typing on a keyboard, etc.), although they obtain this data relatively slowly. Cloudflare mixes the random data obtained from the lava lamps with data generated by the Linux operating system on two different machines in order to maximize entropy when creating cryptographic seeds for SSL/TLS encryption.\n\nIn general, \"entropy\" means disorder or chaos. But entropy has a specific meaning in cryptography: it refers to unpredictability. Cryptographers will actually measure how much entropy a given set of data has in terms of the number of bits of entropy. Because of this, Cloudflare refers to the lava lamp wall as the \"Wall of Entropy.\"\n\nWhat if someone stands in front of the lava lamps?\n\nBecause the lava lamp wall is in the busy lobby of the Cloudflare headquarters, this happens all the time. People come and go in the lobby, walking by or stopping to talk in front of the lamps. Such obstructions become part of the randomness that the camera captures, so people partially blocking the camera's view of the lava lamps actually helps generate entropy.\n\nWhat if someone shuts off or damages the camera?\n\nIf this happens, Cloudflare still has two other sources for randomization from the Linux operating system running on Cloudflare servers. In addition, Cloudflare has easy physical access to the camera because it's in a Cloudflare-owned space, and Cloudflare can quickly turn it back on or replace it as needed.\n\nDo all Cloudflare offices have the lava lamp wall?\n\nThe other two main Cloudflare offices are in London and Singapore, and each office has its own method for generating random data from real-world inputs. London takes photos of a double-pendulum system mounted in the office (a pendulum connected to a pendulum, the movements of which are mathematically unpredictable). The Singapore office measures the radioactive decay of a pellet of uranium (a small enough amount to be harmless).\n\nWas Cloudflare the first company to use lava lamps for encryption?\n\nSurprisingly, no – a company called Silicon Graphics designed a similar system called \"Lavarand\" in 1996, although the patent has since expired.\n\nTo learn more about the Cloudflare lava lamp wall check out these two blog posts:\n\nSee this page to learn how to use free SSL encryption from Cloudflare."
    },
    {
        "link": "https://blog.cloudflare.com/lavarand-in-production-the-nitty-gritty-technical-details",
        "document": "Lava lamps in the Cloudflare lobby. Courtesy of @mahtin\n\nAs some of you may know, there's a wall of lava lamps in the lobby of our San Francisco office that we use for cryptography. In this post, we’re going to explore how that works in technical detail. This post assumes a technical background. For a higher-level discussion that requires no technical background, see Randomness 101: LavaRand in Production.\n\nAs we’ve discussed in the past, cryptography relies on the ability to generate random numbers that are both unpredictable and kept secret from any adversary. In this post, we’re going to go into fairly deep technical detail, so there is some background that we’ll need to ensure that everybody is on the same page.\n\nIn cryptography, the term random means unpredictable. That is, a process for generating random bits is secure if an attacker is unable to predict the next bit with greater than 50% accuracy (in other words, no better than random chance).\n\nWe can obtain randomness that is unpredictable using one of two approaches. The first produces true randomness, while the second produces pseudorandomness.\n\nTrue randomness is any information learned through the measurement of a physical process. Its unpredictability relies either on the inherent unpredictability of the physical process being measured (e.g., the unpredictability of radioactive decay), or on the inaccuracy inherent in taking precise physical measurements (e.g., the inaccuracy of the least significant digits of some physical measurement such as the measurement of a CPU’s temperature or the timing of keystrokes on a keyboard). Random values obtained in this manner are unpredictable even to the person measuring them (the person performing the measurement can’t predict what the value will be before they have performed the measurement), and thus are just as unpredictable to an external attacker. All randomness used in cryptographic algorithms begins life as true randomness obtained through physical measurements.\n\nHowever, obtaining true random values is usually expensive and slow, so using them directly in cryptographic algorithms is impractical. Instead, we use pseudorandomness. Pseudorandomness is generated through the use of a deterministic algorithm that takes as input some other random value called a seed and produces a larger amount of random output (these algorithms are called cryptographically secure pseudorandom number generators, or CSPRNGs). A CSPRNG has two key properties: First, if an attacker is unable to predict the value of the seed, then that attacker will be similarly unable to predict the output of the CSPRNG (and even if the attacker is shown the output up to a certain point - say the first 10 bits - the rest of the output - bits 11, 12, etc - will still be completely unpredictable). Second, since the algorithm is deterministic, running the algorithm twice with the same seed as input will produce identical output.\n\nThe CSPRNGs used in modern cryptography are both very fast and also capable of securely producing an effectively infinite amount of output1 given a relatively small seed (on the order of a few hundred bits). Thus, in order to efficiently generate a lot of secure randomness, true randomness is obtained from some physical process (this is slow), and fed into a CSPRNG which in turn produces as much randomness as is required by the application (this is fast). In this way, randomness can be obtained which is both secure (since it comes from a truly random source that cannot be predicted by an attacker) and cheap (since a CSPRNG is used to turn the truly random seed into a much larger stream of pseudorandom output).\n\nA common misconception is that a CSPRNG, if used for long enough, can “run out” of randomness. This is an understandable belief since, as we’ll discuss in the next section, operating systems often re-seed their CSPRNGs with new randomness to hedge against attackers discovering internal state, broken CSPRNGs, and other maladies.\n\nBut if an algorithm is a true CSPRNG in the technical sense, then the only way for it to run out of randomness is for somebody to consume far more values from it than could ever be consumed in practice (think consuming values from a CSPRNG as fast as possible for thousands of years or more).2\n\nHowever, none of the fast CSPRNGs that we use in practice are proven to be true CSPRNGs. They’re just strongly believed to be true CSPRNGs, or something close to it. They’ve withstood the test of academic analysis, years of being used in production, attacks by resourced adversaries, and so on. But that doesn’t mean that they are without flaws. For example, SHA-1, long considered to be a cryptographically-secure collision-resistant hash function (a building block that can be used to construct a CSPRNG) was eventually discovered to be insecure. Today, it can be broken for $110,000’s worth of cloud computing resources.3\n\nThus, even though we aren’t concerned with running out of randomness in a true CSPRNG, we also aren’t sure that what we’re using in practice are true CSPRNGs. As a result, to hedge against the possibility that an attacker has figured out how to break our CSPRNGs, designers of cryptographic systems often choose to re-seed CSPRNGs with fresh, newly-acquired true randomness just in case.\n\nIn most computer systems, one of the responsibilities of the operating system is to provide cryptographically-secure pseudorandomness for use in various security applications. Since the operating system cannot know ahead of time which applications will require pseudorandomness (or how much they will require), most systems simply keep an _entropy pool_4 - a collection of randomness that is believed to be secure - that is used to seed a CSPRNG (e.g., on Linux) which serves requests for randomness. The system then takes on the responsibility of not only seeding this entropy pool when the system first boots, but also of periodically updating the pool (and re-seeding the CSPRNG) with new randomness from whatever sources of true randomness are available to the system in order to hedge against broken CSPRNGs or attackers having compromised the entropy pool through other non-cryptographic attacks.\n\nFor brevity, and since Cloudflare’s production system’s run Linux, we will refer to the system’s pseudorandomness provider simply as , although note that everything in this discussion is true of other operating systems as well.\n\nGiven this setup of an entropy pool and CSPRNG, there are a few situations that could compromise the security of :\n• None The sources of true randomness used to seed theentropy pool could be too predictable, allowing an attacker to guess the values obtained from these sources, and thus to predict the output of .\n• None An attacker could have access to the sources of true randomness, thus being able to observe their values and thus predict the output of .\n• None An attacker could have the ability to modify the sources of true randomness, thus being able to influence the values obtained from these sources and thus predict the output of .\n\nA common approach to addressing these security issues is to mix multiple sources of randomness together in the system’s entropy pool, the idea being that so long as some of the sources remain uncompromised, the system remains secure. For example, if sources X, Y, and Z, when queried for random outputs, provide values x, y, and z, we might seed our entropy pool with H(x, y, z), where H is a cryptographically-secure collision-resistant hash function. Even if we assume that two of these sources - say, X and Y - are malicious, so long as the attackers in control of them are not able to observe Z’s output,5 then no matter what values of x and y they produce, H(x, y, z) will still be unpredictable to them.\n\nThe view from the camera\n\nWhile the probability is obviously very low that somebody will manage to predict or modify the output of the entropy sources on our production machines, it would be irresponsible of us to pretend that it is impossible. Similarly, while cryptographic attacks against state-of-the-art CSPRNGs are rare, they do occasionally happen. It’s important that we hedge against these possibilities by adding extra layers of defense.\n\nThat’s where LavaRand comes in.\n\nIn short, LavaRand is a system that provides an additional entropy source to our production machines. In the lobby of our San Francisco office, we have a wall of lava lamps (pictured above). A video feed of this wall is used to generate entropy that is made available to our production fleet.\n\nWe're not the first ones to do this. Our LavaRand system was inspired by a similar system first proposed and built by Silicon Graphics and patented in 1996 (the patent has since expired).\n\nThe flow of the “lava” in a lava lamp is very unpredictable,6 and so the entropy in those lamps is incredibly high. Even if we conservatively assume that the camera has a resolution of 100x100 pixels (of course it’s actually much higher) and that an attacker can guess the value of any pixel of that image to within one bit of precision (e.g., they know that a particular pixel has a red value of either 123 or 124, but they aren’t sure which it is), then the total amount of entropy produced by the image is 100x100x3 = 30,000 bits (the x3 is because each pixel comprises three values - a red, a green, and a blue channel). This is orders of magnitude more entropy than we need.\n\nThe flow of entropy in LavaRand\n\nThe overall design of the LavaRand system is pictured above. The flow of entropy can be broken down into the following steps:\n\nThe wall of lava lamps in the office lobby provides a source of true entropy.\n\nIn the lobby, a camera is pointed at the wall. It obtains entropy from both the visual input from the lava lamps and also from random noise in the individual photoreceptors.\n\nIn the office, there’s a server which connects to the camera. The server has its own entropy system, and the output of that entropy system is mixed with the entropy from the camera to produce a new entropy feed.\n\nIn one of our production data centers, there’s a service which connects to the server in the office and consumes its entropy feed. That service combines this entropy feed with output from its own local entropy system to produce yet another entropy feed. This feed is made available for any production service to consume.\n\nWe might conceive of a number of attacks that could be leveraged against this system:\n• None An attacker could train a camera on the wall of lava lamps, attempting to reproduce the image captured by our camera.\n• None An attacker could reduce the entropy from the wall of lava lamps by turning off the power to the lamps, shining a bright light at the camera, placing a lens cap on the camera, or any number of other physical attacks.\n• None An attacker able to compromise the camera could exfiltrate or modify the feed of frames from the camera, replicating or controlling the entropy source used by the server in the office.\n• None An attacker with code running on the office server could observe or modify the output of the entropy feed generated by that server.\n• None An attacker with code running in the production service could observe or modify the output of the entropy feed generated by that service.\n\nOnly one of these attacks would be fatal if successfully carried out: running code on the production service which produces the final entropy feed. In every other case, the malicious entropy feed controlled by the attacker is mixed with a non-malicious feed that the attacker can neither observe nor modify.7 As we discussed in a previous section, as long as the attacker is unable to predict the output of these non-malicious feeds, they will be unable to predict the output of the entropy feed generated by mixing their malicious feed with the non-malicious feed.\n\nHaving a secure entropy source is only half of the story - the other half is actually using it!\n\nThe goal of LavaRand is to ensure that our production machines have access to secure randomness even if their local entropy sources are compromised. Just after boot, each of our production machines contacts LavaRand over TLS to obtain a fixed-size chunk of fresh entropy called a “beacon.” It mixes this beacon into the entropy system (on Linux, by writing the beacon to ). After this point, in order to predict or control the output of , an attacker would need to compromise both the machine’s local entropy sources and the LavaRand beacon.\n\nUnfortunately, the reality isn’t quite that simple. We’ve gotten ourselves into something of a chicken-and-egg problem here: we’re trying to hedge against bad entropy from our local entropy sources, so we have to assume those might be compromised. But TLS, like many cryptographic protocols, requires secure entropy in order to operate. And we require TLS to request a LavaRand beacon. So in order to ensure secure entropy, we have to have secure entropy…\n\nWe solve this problem by introducing a second special-purpose CSPRNG, and seeding it in a very particular way. Every machine in Cloudflare’s production fleet has its own permanent store of secrets that it uses just after boot to prove its identity to the rest of the fleet in order to bootstrap the rest of the boot process. We piggyback on that system by storing an extra random seed - unique for each machine - that we use for that first TLS connection to LavaRand.\n\nThere’s a simple but very useful result from cryptography theory that says that an HMAC - a hash-based message authentication code - when combined with a random, unpredictable seed, behaves (from the perspective of an attacker) like a random oracle. That’s a lot of crypto jargon, but it basically means that if you have a secret, randomly-generated seed, s, then an attacker will be completely unable to guess the output of HMAC(s, x) regardless of the value of x - even if x is completely predictable! Thus, you can use HMAC(s, x) as the seed to a CSPRNG, and the output of the CSPRNG will be unpredictable. Note, though, that if you need to do this multiple times, you will have to pick different values for x! Remember that while CSPRNGs are secure if used with unpredictable seeds, they’re also deterministic. Thus, if the same value is used for x more than once, then the CSPRNG will end up producing the same stream of “random” values more than once, which in cryptography is often very insecure!\n\nThis means that we can combine those unique, secret seeds that we store on each machine with an HMAC and produce a secure random value. We use the current time with nanosecond precision as the input to ensure that the same value is never used twice on the same machine. We use the resulting value to seed a CSPRNG, and we use that CSPRNG for the TLS connection to LavaRand. That way, even if the system’s entropy sources are compromised, we’ll still be able to make a secure connection to LavaRand, obtain a new, secure beacon, and bootstrap the system’s entropy back to a secure state!\n\nHopefully we’ll never need LavaRand. Hopefully, the primary entropy sources used by our production machines will remain secure, and LavaRand will serve little purpose beyond adding some flair to our office. But if it turns out that we’re wrong, and that our randomness sources in production are actually flawed, then hopefully LavaRand will be our hedge, making it just a little bit harder to hack Cloudflare.\n\n1 Some CSPRNGs exist with constraints on how much output can be consumed securely, but those are not the sort that we are concerned with in this post.\n\n2 Section 3.1, Recommendations for Randomness in the Operating System\n\n3 The first collision for full SHA-1\n\n4 “Entropy” and “randomness” are synonyms in cryptography - the former is the more technical term.\n\n5 If the attacker controls X and Y and can also observe the output of Z, then the attacker can still partially influence the output of H(x, y, z). See here for a discussion of possible attacks.\n\n6 Noll, L.C. and Mende, R.G. and Sisodiya, S., Method for seeding a pseudo-random number generator with a cryptographic hash of a digitization of a chaotic system\n\n7 A surprising example of the effectiveness of entropy is the mixing of the image captured by the camera with the random noise in the camera’s photoreceptors. If we assume that every pixel captured is either recorded as the “true” value or is instead recorded as one value higher than the true value (50% probability for each), then even if the input image can be reproduced by an attacker with perfect accuracy, the camera still provides one bit of entropy for each pixel channel. As discussed before, even for a 100x100 pixel camera, that’s 30,000 bits!"
    },
    {
        "link": "https://blog.cloudflare.com/fr-fr/randomness-101-lavarand-in-production",
        "document": "Lava lamps in the Cloudflare lobby Courtesy of @mahtin\n\nAs some of you may know, there's a wall of lava lamps in the lobby of our San Francisco office that we use for cryptography. In this post, we’re going to explore how that works. This post assumes no technical background. For a more in-depth look at the technical details, see LavaRand in Production: The Nitty-Gritty Technical Details.\n\nAs we’ve discussed in the past, cryptography relies on the ability to generate random numbers that are both unpredictable and kept secret from any adversary.\n\nBut “random” is a pretty tricky term; it’s used in many different fields to mean slightly different things. And like all of those fields, its use in cryptography is very precise. In some fields, a process is random simply if it has the right statistical properties. For example, the digits of pi are said to be random because all sequences of numbers appear with equal frequency (“15” appears as frequently as “38”, “426” appears as frequently as “297”, etc). But for cryptography, this isn’t enough - random numbers must be unpredictable.\n\nTo understand what unpredictable means, it helps to consider that all cryptography is based on an asymmetry of information. If you’re trying to securely perform some cryptographic operation, then what you’re concerned about is that somebody - an adversary - will try to break your security. The only thing that distinguishes you from the adversary is that you know some things that the adversary does not, and the job of cryptography is to ensure that this asymmetry of information is enough to keep you secure.\n\nLet’s consider a simple example. Imagine that you and a friend are trying to go to a movie, but you don’t want your nemesis to figure out which movie you’re going to go to (lest she show up and thwart your movie watching!). This week, it’s your turn to choose the movie. Once you’ve made your choice, you’re going to need to send a message to your friend telling him which movie you’ve chosen, but you’re going to need to ensure that even if your nemesis intercepts the message, she won’t be able to figure out what it says.\n\nYou devise the following scheme: since there are only two movies available to watch at the moment, you label one A and the other B. Then, while in the presence of your friend, you flip a coin. You agree on the following table outlining which message that you will send depending on your choice of what movie to watch and whether the coin comes up heads (H) or tails (T). Later, once you’ve made up your mind about which movie to see, you’ll use this table to send an encrypted message to your friend telling him which movie you’ve chosen.\n\nIf you were to decide on movie B, and the coin came up heads, you would send the message, “In Hertford, Hereford, and Hampshire, hurricanes hardly ever happen.” Since your friend knows that the coin came up heads - he was there when it happened - he knows that you must have decided on movie B. But consider it from your nemesis’ perspective. She doesn’t know the result of the coin toss - all she knows is that there’s a 50% chance that the coin came up heads, and a 50% chance that it came up tails. Thus, seeing the message “In Hertford, Hereford, and Hampshire, hurricanes hardly ever happen” doesn’t help her at all! There’s a 50% chance that the coin came up heads (implying movie B), and a 50% chance that it came up tails (implying movie A). She doesn’t know anything more than she knew before!\n\nLet’s return now to the concept of unpredictability. Imagine that the result of the coin toss was completely predictable - say your nemesis planted a trick coin that always comes up heads on the first toss, tails on the second, heads on the third, and so on. Since she would know that there was a 100% chance of the first coin toss coming up heads, then regardless of which message you sent, she would know which movie you were going to see. While the trick coin still exhibits some basic properties of “randomness” as the term is used in the field of statistics - it comes up heads as often as it comes up tails - it’s predictable, which makes it useless for cryptography. The takeaway: when we say random in the context of cryptography, we mean unpredictable.\n\nUnfortunately for cryptographers, if there’s one thing computers are good at, it’s being predictable. They can execute the same code a million times, and so long as they are given the same inputs each time, they’ll always come up with the same outputs. This is very good for reliability, but it’s tricky when it comes to cryptography - after all, we need unpredictability!\n\nThe solution to this problem is cryptographically-secure pseudorandom number generators (CSPRNGs). CSPRNGs are algorithms which, provided an input which is itself unpredictable, produce a much larger stream of output which is also unpredictable. This stream can be extended indefinitely, producing as much output as required at any time in the future. In other words, if you were to flip a coin a number of times (a process which is known to be unpredictable) and then use the output of those coin flips as the input to a CSPRNG, an adversary who wasn’t able to predict the output of those coin flips would also be unable to predict the output of the CSPRNG - no matter how much output was consumed from the CSPRNG.\n\nBut even though CSPRNGs are a very powerful tool, they’re only one half of the equation - they still need an unpredictable input to operate. But as we said, computers aren’t unpredictable, so aren’t we back at square one? Well, not quite. It turns out that computers do usually have sources of unpredictability that they can use, but they’re pretty slow. What we can do is combine that slow process of gathering unpredictable input with a CSPRNG, which can take that input and produce a much larger amount of input more quickly, and we can satisfy all of our randomness needs!\n\nBut where can a computer get such unpredictable input, even slowly? The answer is the real world. While computers provide a nice simplification of the world for programmers to live in, real, physical computers still exist in the real, physical world. And that world is unpredictable. Computers have all sorts of ways of taking input from the real world - temperature sensors, keyboards, network interfaces, etc. All of these provide the ability to take measurements of the real world, and all of those measurements have some degree of inherent inaccuracy. As we’ll explain in a moment, that inaccuracy is the same thing as unpredictability, and this can be used! Common sources of unpredictable randomness include measuring the temperature of the CPU with high - and thus inaccurate - precision, measuring the timing of keystrokes on the keyboard with high precision, etc. To see how this can be used to produce unpredictable randomness, consider the question, “what’s the temperature of the room I’m sitting in right now?” You can probably estimate to within a couple of degrees - say, somewhere between 70 and 75 degrees Fahrenheit. But you probably have no idea what the temperature is to 2 decimal places of accuracy - is it 73.42 degrees or 73.47 degrees? By measuring the temperature with high precision and then using only the low-order digits of the measured value, you can get highly unpredictable randomness just by observing the world around you. And so can computers.\n• None Randomness used in cryptography needs to be unpredictable.\n• None Computers can slowly get a small amount of unpredictable randomness by measuring their environment.\n• None Computers can greatly extend this randomness by using a CSPRNG which can quickly turn it into a large amount of unpredictable randomness.\n\nIf there’s one thing cryptographers are wary about, it’s certainty. Cryptographic systems are routinely shown to be less secure than they were originally thought to be, and we’re constantly updating our understanding of what algorithms are safe to use in what scenarios.\n\nSo it shouldn’t be any surprise that cryptographers like to hedge their bets by using more security than they believe necessary in case it turns out that one of their assumptions was wrong. It’s sort of like the cryptographer’s version of the engineering practice of designing buildings to withstand far more weight or wind or heat than they think will arise in practice.\n\nWhen it comes to randomness, this hedging often takes the form of mixing. Unpredictable random values have the neat property that if they’re mixed in the right way with more unpredictable random values, then the result is at least as unpredictable as either of the inputs. That means that if you mix a random value which is highly unpredictability with a random value which is somewhat predictable, the result will be a highly unpredictable value.\n\nThis mixing property is useful because it allows you to mix unpredictable random values from many sources, and if you later discover that one of those sources was less unpredictable than you’d originally thought, it’s still OK - the other sources come to the rescue.\n\nAt Cloudflare, we have thousands of computers in data centers all around the world, and each one of these computers needs cryptographic randomness. Historically, they got that randomness using the default mechanism made available by the operating system that we run on them, Linux.\n\nBut being good cryptographers, we’re always trying to hedge our bets. We wanted a system to ensure that even if the default mechanism for acquiring randomness was flawed, we’d still be secure. That’s how we came up with LavaRand.\n\nThe view from the camera\n\nLavaRand is a system that uses lava lamps as a secondary source of randomness for our production servers. A wall of lava lamps in the lobby of our San Francisco office provides an unpredictable input to a camera aimed at the wall. A video feed from the camera is fed into a CSPRNG, and that CSPRNG provides a stream of random values that can be used as an extra source of randomness by our production servers. Since the flow of the “lava” in a lava lamp is very unpredictable,1 “measuring” the lamps by taking footage of them is a good way to obtain unpredictable randomness. Computers store images as very large numbers, so we can use them as the input to a CSPRNG just like any other number.\n\nWe're not the first ones to do this. Our LavaRand system was inspired by a similar system first proposed and built by Silicon Graphics and patented in 1996 (the patent has since expired).\n\nHopefully we’ll never need it. Hopefully, the primary sources of randomness used by our production servers will remain secure, and LavaRand will serve little purpose beyond adding some flair to our office. But if it turns out that we’re wrong, and that our randomness sources in production are actually flawed, then LavaRand will be our hedge, making it just a little bit harder to hack Cloudflare.\n• None Noll, L.C. and Mende, R.G. and Sisodiya, S., Method for seeding a pseudo-random number generator with a cryptographic hash of a digitization of a chaotic system"
    },
    {
        "link": "https://blog.cloudflare.com/harnessing-office-chaos",
        "document": "In the children’s book The Snail and Whale, after an unexpectedly far-flung adventure, the principal character returns to declarations of “How time’s flown” and “Haven’t you grown?” It has been about four years since we last wrote about LavaRand and during that time the story of how Cloudflare uses physical sources of entropy to add to the security of the Internet has continued to travel and be a source of interest to many. What was initially just a single species of physical entropy source – lava lamps – has grown and diversified. We want to catch you up a little on the story of LavaRand. This blog post will cover the new sources of “chaos” that have been added to LavaRand and how you can make use of that harnessed chaos in your next application. We’ll cover how public randomness can open up uses of publicly trusted randomness — imagine not needing to take the holders of a “random draw” at their word when they claim the outcome is not manipulated in some way. And finally we’ll discuss timelock encryption which is a way to ensure that a message cannot be decrypted until some chosen time in the future.\n\nThe entropy sourced from our wall of lava lamps in San Francisco has long played its part in the randomness that secures connections made through Cloudflare.\n\nCloudflare’s servers collectively handle upwards of 55 million HTTP requests per second, the vast majority of which are secured via the TLS protocol to ensure authenticity and confidentiality. Under the hood, cryptographic protocols like TLS require an underlying source of secure randomness – otherwise, the security guarantees fall apart.\n\nSecure randomness used in cryptography needs to be computationally indistinguishable from “true” randomness. For this, it must both pass statistical randomness tests, and the output needs to be unpredictable to any computationally-bounded adversary, no matter how much previous output they’ve already seen. The typical way to achieve this is to take some random ‘seed’ and feed it into a Cryptographically Secure Pseudorandom Number Generator (CSPRNG) that can produce an essentially-endless stream of unpredictable bytes upon request. The properties of a CSPRNG ensure that all outputs are practically indistinguishable from truly random outputs to anyone that does not know its internal state. However, this all depends on having a secure random seed to begin with. Take a look at this blog for more details on true randomness versus pseudorandomness, and this blog for some great examples of what can go wrong with insecure randomness.\n\nFor many years, Cloudflare’s servers relied on local sources of entropy (such as the precise timing of packet arrivals or keyboard events) to seed their entropy pools. While there’s no reason to believe that the local entropy sources on those servers are insecure or could be easily compromised, we wanted to hedge our bets against that possibility. Our solution was to set up a system where our servers could periodically refresh their entropy pools with true randomness from an external source.\n\nThat brings us to LavaRand. “Lavarand” has long been the name given to systems used for the generation of randomness (first by Silicon Graphics in 1997). Cloudflare launched its instantiation of a LavaRand system in 2017 as a system that collects entropy from the wall of lava lamps in our San Francisco office and makes it available via an internal API. Our servers then periodically query the API to retrieve fresh randomness from LavaRand and incorporate it into their entropy pools. The contributions made by LavaRand can be considered spice added to the entropy pool mix! (For more technical details on contributions made by LavaRand, read our previous blog post.)\n\nOur lava lamps in San Francisco have been working tirelessly for years to supply fresh entropy to our systems, but they now have siblings across the world to help with their task! As Cloudflare has grown, so has the variety of entropy sources found in and sourced from our offices. Cloudflare’s Places team works hard to ensure that our offices reflect aspects of our values and culture. Several of our larger office locations include installations of physical systems of entropy, and it is these installations that we have worked to incorporate into LavaRand over time. The tangible and exciting draw of these systems is their basis in physical mechanics that we intuitively consider random. The gloops of warmed ascending “lava” floating past cooler sinking blobs within lava lamps attract our attention just as other unpredictable (and often beautiful) dynamic systems capture our interest.\n\nVisible to visitors of our London office is a wall of double pendulums whose beautiful swings translate to another source of entropy to LavaRand and to the pool of randomness that Cloudflare’s servers pull from.\n\nTo the untrained eye the shadows of the pendulum stands and those cast by the rotating arms on the rear wall might seem chaotic. If so, then this installation should be labeled a success! Different light conditions and those shadows add to the chaos that is captured from this entropy source.\n\nIndeed, even with these arms restricted to motion in two dimensions, the path traced by the arms is mesmerizingly varied, and can be shown to be mathematically chaotic. Even if we forget air resistance, temperature, and the environment, and then assume that the mutation is completely deterministic, still the resulting long-term motion is hard to predict. In particular the system is very sensitive to initial conditions, this initial state – how they are set in motion – paired with deterministic behavior produces a unique path that is traced until the pendulum comes to rest, and the system is set in motion by a Cloudflare employee in London once again.\n\nThe beautiful new Cloudflare office in Austin, Texas recently celebrated its first year since opening. This office contributes its own spin on physical entropy: suspended above the entrance of the Cloudflare office in downtown Austin is an installation of translucent rainbow mobiles. These twirl, reflecting the changing light, and cast coloured patterns on the enclosing walls. The display of hanging mobiles and their shadows are very sensitive to a physical environment which includes the opening and closing of doors, HVAC changes, and ambient light. This chaotic system’s mesmerizing and changing scene is captured periodically and fed into the stream of LavaRand randomness.\n\nWe incorporated the new sources of office chaos into the LavaRand system (still called LavaRand despite including much more than lava lamps) in the same way as the existing lava lamps, which we’ve previously described in detail.\n\nTo recap, at repeated intervals, a camera captures an image of the current state of the randomness display. Since the underlying system is truly random, the produced image contains true randomness. Even shadows and changing light conditions play a part in producing something unique and unpredictable! There is another secret that we should share: at a base level, image sensors in the real world are often a source of sufficient noise that even images taken without the lens cap removed could work well as a source of entropy! We consider this added noise to be a serendipitous addition to the beautiful chaotic motion of these installations.\n\nOnce we have a still image that captures the state of the randomness display at a particular point in time, we compute a compact representation – a hash – of the image to derive a fixed-sized output of truly random bytes.\n\nThe random bytes are then used as an input (along with the previous seed and some randomness from the system’s local entropy sources) to a Key Derivation Function (KDF) to compute a new randomness seed that is fed into a Cryptographically Secure Pseudorandom Number Generator (CSPRNG) that can produce an essentially-endless stream of unpredictable bytes upon request. The properties of a CSPRNG ensure that all outputs are practically indistinguishable from truly random outputs to anyone that does not know its internal state. LavaRand then exposes this stream of randomness via a simple internal API where clients can request fresh randomness.\n\nApplications typically use secure randomness in one of two flavors: private and public.\n\nPrivate randomness is used for generating passwords, cryptographic keys, user IDs, and other values that are meant to stay secret forever. As we’ve previously described, our servers periodically request fresh private randomness from LavaRand to help to update their entropy pools. Because of this, randomness from LavaRand is essentially available to the outside world! One easy way for developers to tap into private randomness from LavaRand is to use the Web Crypto API’s getRandomValues function from a Cloudflare Worker, or use one that someone has already built, like csprng.xyz (source).\n\nPublic randomness consists of unpredictable and unbiased random values that are made available to everyone once they are published, and for this reason should not be used for generating cryptographic keys. The winning lottery numbers and the coin flip at the start of a sporting event are some examples of public random values. A double-headed coin would not be an unbiased and unpredictable source of entropy and would have drastic impacts on the sports betting world.\n\nIn addition to being unpredictable and unbiased, it’s also desirable for public randomness to be trustworthy so that consumers of the randomness are assured that the values were faithfully produced. Not many people would buy lottery tickets if they believed that the winning ticket was going to be chosen unfairly! Indeed, there are known cases of corrupt insiders subverting public randomness for personal gain, like the state lottery employee who co-opted the lottery random number generator, allowing his friends and family to win millions of dollars.\n\nA fundamental challenge of public randomness is that one must trust the authority producing the random outputs. Trusting a well-known authority like NIST may suffice for many applications, but could be problematic for others (especially for applications where decentralization is important).\n\nTo help solve this problem of trust, Cloudflare joined forces with seven other independent and geographically distributed organizations back in 2019 to form the League of Entropy to launch a public randomness beacon using the drand (pronounced dee-rand) protocol. Each organization contributes its own unique source of randomness into the joint pool of entropy used to seed the drand network – with Cloudflare using randomness from LavaRand, of course!\n\nWhile the League of Entropy started out as an experimental network, with the guidance and support from the drand team at Protocol Labs, it’s become a reliable and production-ready core Internet service, relied upon by applications ranging from distributed file storage to online gaming to timestamped proofs to timelock encryption (discussed further below). The League of Entropy has also grown, and there are now 18 organizations across four continents participating in the drand network.\n\nThe League of Entropy’s drand beacons (each of which runs with different parameters, such as how frequently random values are produced and whether the randomness is chained – more on this below) have two important properties that contribute to their trustworthiness: they are decentralized and verifiable. Decentralization ensures that one or two bad actors cannot subvert or bias the randomness beacon, and verifiability allows anyone to check that the random values are produced according to the drand protocol and with participation from a threshold (at least half, but usually more) of the participants in the drand network. Thus, with each new member, the trustworthiness and reliability of the drand network continues to increase.\n\nWe give a brief overview of how drand achieves these properties using distributed key generation and threshold signatures below, but for an in-depth dive see our previous blog post and some of the excellent posts from the drand team.\n\nDuring the initial setup of a drand beacon, nodes in the network run a distributed key generation (DKG) protocol based on the Pedersen commitment scheme, the result of which is that each node holds a “share” (a keypair) for a distributed group key, which remains fixed for the lifetime of the beacon. In order to do something useful with the group secret key like signing a message, at least a threshold (for example 7 out of 9) of nodes in the network must participate in constructing a BLS threshold signature. The group information for the quicknet beacon on the League of Entropy’s mainnet drand network is shown below:\n\nThe nodes in the network are configured to periodically (every 3s for quicknet) work together to produce a signature over some agreed-upon message, like the current round number and previous round signature (more on this below). Each node uses its share of the group key to produce a partial signature over the current round message, and broadcasts it to other nodes in the network. Once a node has enough partial signatures, it can aggregate them to produce a group signature for the given round.\n\nThe group signature for a round is the randomness (in the output above, the randomness value is simply the sha256 hash of the signature, for applications that prefer a shorter, fixed-sized output). The signature is unpredictable in advance as long as enough (at least a majority, but can be configured to be higher) of the nodes in the drand network are honest and do not collude. Further, anyone can validate the signature for a given round using the beacon’s group public key. It’s recommended that developers use the drand client libraries or CLI to perform verification on every value obtained from the beacon.\n\nWhen the League of Entropy launched its first generation of drand beacons in 2019, the per-round message over which the group signature was computed included the previous round’s signature. This creates a chain of randomness rounds all the way to the first “genesis” round. Chained randomness provides some nice properties for single-source randomness beacons, and is included as a requirement in NIST’s spec for interoperable public randomness beacons.\n\nHowever, back in 2022 the drand team introduced the notion of unchained randomness, where the message to be signed is predictable and doesn’t depend on any randomness from previous rounds, and showed that it provides the same security guarantees as chained randomness for the drand network (both require an honest threshold of nodes). In the implementation of unchained randomness in the quicknet, the message to be signed simply consists of the round number.\n\nUnchained randomness provides some powerful properties and usability improvements. In terms of usability, a consumer of the randomness beacon does not need to reconstruct the full chain of randomness to the genesis round to fully validate a particular round – the only information needed is the current round number and the group public key. This provides much more flexibility for clients, as they can choose how frequently they consume randomness rounds without needing to continuously follow the randomness chain.\n\nSince the messages to be signed are known in advance (since they’re just the round number), unchained randomness also unlocks a powerful new property: timelock encryption.\n\nTimelock (or “timed-release”) encryption is a method for encrypting a message such that it cannot be decrypted until a certain amount of time has passed. Two basic approaches to timelock encryption were described by Rivest, Shamir, and Wagner:\n\nThere are two natural approaches to implementing timed release cryptography:\n\n- Use “time-lock puzzles” – computational problems that cannot be solved without running a computer continuously for at least a certain amount of time.\n\n- Use trusted agents who promise not to reveal certain information until a specified date.\n\nUsing trusted agents has the obvious problem of ensuring that the agents are trustworthy. Secret sharing approaches can be used to alleviate this concern.\n\nThe drand network is a group of independent agents using secret sharing for trustworthiness, and the ‘certain information’ not to be revealed until a specified date sounds a lot like the per-round randomness! We describe next how timelock encryption can be implemented on top of a drand network with unchained randomness, and finish with a practical demonstration. While we don’t delve into the bilinear groups and pairings-based cryptography that make this possible, if you’re interested we encourage you to read tlock: Practical Timelock Encryption from Threshold BLS by Nicolas Gailly, Kelsey Melissaris, and Yolan Romailler.\n\nFirst, identify the randomness round that, once revealed, will allow your timelock-encrypted message to be decrypted. An important observation is that since drand networks produce randomness at fixed intervals, each round in a drand beacon is closely tied to a specific timestamp (modulo small delays for the network to actually produce the beacon) which can be easily computed taking the beacon’s genesis timestamp and then adding the round number multiplied by the beacon’s period.\n\nOnce the round is decided upon, the properties of bilinear groups allow you to encrypt your message to some round with the drand beacon’s group public key.\n\nAfter the nodes in the drand network cooperate to derive the randomness for the round (really, just the signature on the round number using the beacon’s group secret key), anyone can decrypt the ciphertext (this is where the magic of bilinear groups comes in).\n\nTo make this practical, the timelocked message is actually the secret key for a symmetric scheme. This means that we encrypt the message with a symmetric key and encrypt the key with timelock, allowing for a decryption in the future.\n\nNow, for a practical demonstration of timelock encryption, we use a tool that one of our own engineers built on top of Cloudflare Workers. The source code is publicly available if you’d like to take a look under the hood at how it works.\n\nWe hope you’ve enjoyed revisiting the tale of LavaRand as much as we have, and are inspired to visit one of Cloudflare’s offices in the future to see the randomness displays first-hand, and to use verifiable public randomness and timelock encryption from drand in your next project.\n\nChaos is required by the encryption that secures the Internet. LavaRand at Cloudflare will continue to turn the chaotic beauty of our physical world into a randomness stream – even as new sources are added – for novel uses all of us explorers – just like that snail – have yet to dream up.\n\nAnd she gazed at the sky, the sea, the landThe waves and the caves and the golden sand.She gazed and gazed, amazed by it all,And she said to the whale, “I feel so small.”\n\nTune in for more news, announcements and thought-provoking discussions! Don't miss the full Security Week hub page."
    },
    {
        "link": "https://stackoverflow.blog/2020/03/02/best-practices-for-rest-api-design",
        "document": "REST APIs are one of the most common kinds of web interfaces available today. They allow various clients including browser apps to communicate with services via the REST API. Therefore, it's very important to design REST APIs properly so that we won't run into problems down the road. We have to take into account security, performance, and ease of use for API consumers.\n\nOtherwise, we create problems for clients that use our APIs, which isn’t pleasant and detracts people from using our API. If we don’t follow commonly accepted conventions, then we confuse the maintainers of the API and the clients that use them since it’s different from what everyone expects.\n\nIn this article, we'll look at how to design REST APIs to be easy to understand for anyone consuming them, future-proof, and secure and fast since they serve data to clients that may be confidential.\n• Use nouns instead of verbs in endpoint paths\n\nA REST API is an application programming interface architecture style that conforms to specific architectural constraints, like stateless communication and cacheable data. It is not a protocol or standard. While REST APIs can be accessed through a number of communication protocols, most commonly, they are called over HTTPS, so the guidelines below apply to REST API endpoints that will be called over the internet.\n\nNote: For REST APIs called over the internet, you'll like want to follow the best practices for REST API authentication.\n\nEven though some people think REST should only return hypertext (including Roy Fielding who created the term) REST APIs should accept JSON for request payload and also send responses to JSON. JSON is the standard for transferring data. Almost every networked technology can use it: JavaScript has built-in methods to encode and decode JSON either through the Fetch API or another HTTP client. Server-side technologies have libraries that can decode JSON without doing much work.\n\nThere are other ways to transfer data. XML isn’t widely supported by frameworks without transforming the data ourselves to something that can be used, and that’s usually JSON. We can’t manipulate this data as easily on the client-side, especially in browsers. It ends up being a lot of extra work just to do normal data transfer.\n\nForm data is good for sending data, especially if we want to send files. But for text and numbers, we don’t need form data to transfer those since—with most frameworks—we can transfer JSON by just getting the data from it directly on the client side. It’s by far the most straightforward to do so.\n\nTo make sure that when our REST API app responds with JSON that clients interpret it as such, we should set Content-Type in the response header to application/json after the request is made. Many server-side app frameworks set the response header automatically. Some HTTP clients look at the Content-Type response header and parse the data according to that format.\n\nThe only exception is if we’re trying to send and receive files between client and server. Then we need to handle file responses and send form data from client to server. But that is a topic for another time.\n\nWe should also make sure that our endpoints return JSON as a response. Many server-side frameworks have this as a built-in feature.\n\nLet’s take a look at an example API that accepts JSON payloads. This example will use the Express back end framework for Node.js. We can use the body-parser middleware to parse the JSON request body, and then we can call the res.json method with the object that we want to return as the JSON response as follows:\n\nbodyParser.json() parses the JSON request body string into a JavaScript object and then assigns it to the req.body object.\n\nSet the Content-Type header in the response to application/json; charset=utf-8 without any changes. The method above applies to most other back end frameworks.\n\nUse nouns instead of verbs in endpoint paths\n\nWe shouldn't use verbs in our endpoint paths. Instead, we should use the nouns which represent the entity that the endpoint that we're retrieving or manipulating as the pathname.\n\nThis is because our HTTP request method already has the verb. Having verbs in our API endpoint paths isn’t useful and it makes it unnecessarily long since it doesn’t convey any new information. The chosen verbs could vary by the developer’s whim. For instance, some like ‘get’ and some like ‘retrieve’, so it’s just better to let the HTTP GET verb tell us what and endpoint does.\n\nThe action should be indicated by the HTTP request method that we're making. The most common methods include GET, POST, PUT, and DELETE.\n• POST submits new data to the server.\n\nWith the two principles we discussed above in mind, we should create routes like GET /articles/ for getting news articles. Likewise, POST /articles/ is for adding a new article , PUT /articles/:id is for updating the article with the given id. DELETE /articles/:id is for deleting an existing article with the given ID.\n\n/articles represents a REST API resource. For instance, we can use Express to add the following endpoints for manipulate articles as follows:\n\nIn the code above, we defined the endpoints to manipulate articles. As we can see, the path names do not have any verbs in them. All we have are nouns. The verbs are in the HTTP verbs.\n\nThe POST, PUT, and DELETE endpoints all take JSON as the request body, and they all return JSON as the response, including the GET endpoint.\n\nWhen designing endpoints, it makes sense to group those that contain associated information. That is, if one object can contain another object, you should design the endpoint to reflect that. This is good practice regardless of whether your data is structured like this in your database. In fact, it may be advisable to avoid mirroring your database structure in your endpoints to avoid giving attackers unnecessary information.\n\nFor example, if we want an endpoint to get the comments for a news article, we should append the /comments path to the end of the /articles path. We can do that with the following code in Express:\n\nIn the code above, we can use the GET method on the path '/articles/:articleId/comments'. We get comments on the article identified by articleId and then return it in the response. We add 'comments' after the '/articles/:articleId' path segment to indicate that it's a child resource of /articles.\n\nThis makes sense since comments are the children objects of the articles, assuming each article has its own comments. Otherwise, it’s confusing to the user since this structure is generally accepted to be for accessing child objects. The same principle also applies to the POST, PUT, and DELETE endpoints. They can all use the same kind of nesting structure for the path names.\n\nHowever, nesting can go too far. After about the second or third level, nested endpoints can get unwieldy. Consider, instead, returning the URL to those resources instead, especially if that data is not necessarily contained within the top level object.\n\nFor example, suppose you wanted to return the author of particular comments. You could use /articles/:articleId/comments/:commentId/author. But that's getting out of hand. Instead, return the URI for that particular user within the JSON response instead:\n\nTo eliminate confusion for API users when an error occurs, we should handle errors gracefully and return HTTP response codes that indicate what kind of error occurred. This gives maintainers of the API enough information to understand the problem that’s occurred. We don’t want errors to bring down our system, so we can leave them unhandled, which means that the API consumer has to handle them.\n• 401 Unauthorized - This means the user isn't not authorized to access a resource. It usually returns when the user isn't authenticated.\n• 403 Forbidden - This means the user is authenticated, but it's not allowed to access a resource.\n• 404 Not Found - This indicates that a resource is not found.\n• 500 Internal server error - This is a generic server error. It probably shouldn't be thrown explicitly.\n• 502 Bad Gateway - This indicates an invalid response from an upstream server.\n• 503 Service Unavailable - This indicates that something unexpected happened on server side (It can be anything like server overload, some parts of the system failed, etc.).\n\nWe should be throwing errors that correspond to the problem that our app has encountered. For example, if we want to reject the data from the request payload, then we should return a 400 response as follows in an Express API:\n\nIn the code above, we have a list of existing users in the users array with the given email.\n\nThen if we try to submit the payload with the email value that already exists in users, we'll get a 400 response status code with a 'User already exists' message to let users know that the user already exists. With that information, the user can correct the action by changing the email to something that doesn't exist.\n\nError codes need to have messages accompanied with them so that the maintainers have enough information to troubleshoot the issue, but attackers can’t use the error content to carry our attacks like stealing information or bringing down the system.\n\nWhenever our API does not successfully complete, we should fail gracefully by sending an error with information to help users make corrective action.\n\nThe databases behind a REST API can get very large. Sometimes, there's so much data that it shouldn’t be returned all at once because it’s way too slow or will bring down our systems. Therefore, we need ways to filter items.\n\nWe also need ways to paginate data so that we only return a few results at a time. We don't want to tie up resources for too long by trying to get all the requested data at once.\n\nFiltering and pagination both increase performance by reducing the usage of server resources. As more data accumulates in the database, the more important these features become.\n\nHere’s a small example where an API can accept a query string with various query parameters to let us filter out items by their fields:\n\nIn the code above, we have the req.query variable to get the query parameters. We then extract the property values by destructuring the individual query parameters into variables using the JavaScript destructuring syntax. Finally, we run filter on with each query parameter value to locate the items that we want to return.\n\nOnce we have done that, we return the results as the response. Therefore, when we make a GET request to the following path with the query string:\n\nas the returned response since we filtered by lastName and age.\n\nLikewise, we can accept the page query parameter and return a group of entries in the position from (page - 1) * 20 to page * 20.\n\nWe can also specify the fields to sort by in the query string. For instance, we can get the parameter from a query string with the fields we want to sort the data for. Then we can sort them by those individual fields.\n\nFor instance, we may want to extract the query string from a URL like:\n\nWhere + means ascending and - means descending. So we sort by author’s name in alphabetical order and datepublished from most recent to least recent.\n\nMost communication between client and server should be private since we often send and receive private information. Therefore, using SSL/TLS for security is a must.\n\nA SSL certificate isn't too difficult to load onto a server and the cost is free or very low. There's no reason not to make our REST APIs communicate over secure channels instead of in the open.\n\nPeople shouldn't be able to access more information that they requested. For example, a normal user shouldn't be able to access information of another user. They also shouldn't be able to access data of admins.\n\nTo enforce the principle of least privilege, we need to add role checks either for a single role, or have more granular roles for each user.\n\nIf we choose to group users into a few roles, then the roles should have the permissions that cover all they need and no more. If we have more granular permissions for each feature that users have access to, then we have to make sure that admins can add and remove those features from each user accordingly. Also, we need to add some preset roles that can be applied to a group users so that we don’t have to do that for every user manually.\n\nWe can add caching to return data from the local memory cache instead of querying the database to get the data every time we want to retrieve some data that users request. The good thing about caching is that users can get data faster. However, the data that users get may be outdated. This may also lead to issues when debugging in production environments when something goes wrong as we keep seeing old data.\n\nThere are many kinds of caching solutions like Redis, in-memory caching, and more. We can change the way data is cached as our needs change.\n\nFor instance, Express has the apicache middleware to add caching to our app without much configuration. We can add a simple in-memory cache into our server like so:\n\nThe code above just references the apicache middleware with apicache.middleware and then we have:\n\nto apply the caching to the whole app. We cache the results for five minutes, for example. We can adjust this for our needs.\n\nIf you are using caching, you should also include Cache-Control information in your headers. This will help users effectively use your caching system.\n\nWe should have different versions of API if we're making any changes to them that may break clients. The versioning can be done according to semantic version (for example, 2.0.6 to indicate major version 2 and the sixth patch) like most apps do nowadays.\n\nThis way, we can gradually phase out old endpoints instead of forcing everyone to move to the new API at the same time. The v1 endpoint can stay active for people who don’t want to change, while the v2, with its shiny new features, can serve those who are ready to upgrade. This is especially important if our API is public. We should version them so that we won't break third party apps that use our APIs.\n\nVersioning is usually done with /v1/, /v2/, etc. added at the start of the API path.\n\nFor example, we can do that with Express as follows:\n\nWe just add the version number to the start of the endpoint URL path to version them.\n\nThe most important takeaways for designing high-quality REST APIs is to have consistency by following web standards and conventions. JSON, SSL/TLS, and HTTP status codes are all standard building blocks of the modern web.\n\nPerformance is also an important consideration. We can increase it by not returning too much data at once. Also, we can use caching so that we don't have to query for data all the time.\n\nPaths of endpoints should be consistent, we use nouns only since the HTTP methods indicate the action we want to take. Paths of nested resources should come after the path of the parent resource. They should tell us what we’re getting or manipulating without the need to read extra documentation to understand what it’s doing."
    },
    {
        "link": "https://vinaysahni.com/best-practices-for-a-pragmatic-restful-api",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/614728/best-practice-for-rate-limiting-users-of-a-rest-api",
        "document": "This is all done with outer webserver, which listens to the world (i recommend nginx or lighttpd).\n\nRegarding rate limits, nginx is able to limit, i.e. 50 req/minute per each IP, all over get 503 page, which you can customize.\n\nRegarding expected temporary down, in rails world this is done via special maintainance.html page. There is some kind of automation that creates or symlinks that file when rails app servers go down. I'd recommend relying not on file presence, but on actual availability of app server.\n\nBut really you are able to start/stop services without losing any connections at all. I.e. you can run separate instance of app server on different UNIX socket/IP port and have balancer (nginx/lighty/haproxy) use that new instance too. Then you shut down old instance and all clients are served with only new one. No connection lost. Of course this scenario is not always possible, depends on type of change you introduced in new version.\n\nhaproxy is a balancer-only solution. It can extremely efficiently balance requests to app servers in your farm.\n\nFor quite big service you end-up with something like:\n• each balancer proxies requests to M webservers for static and P app servers for dynamic content. Oh well your REST API don't have static files, does it?\n\nFor quite small service (under 2K rps) all balancing is done inside one-two webservers."
    },
    {
        "link": "https://schweizerischebundesbahnen.github.io/api-principles/restful/best-practices",
        "document": "This chapter is not subject to IT Governance. It is more a collection of extractions from literature, experiences and patterns that we document as shared knowledge. We also included some ideas from other API Principles (e.g. Zalando’s and Paypal’s). Nevertheless engineers should read this section as it contains very valuable knowledge of how to design and implement RESTful APIs.\n• General API Design Principles\n• Use specific API instead of a generic one\n• Common Headers\n• Consider to Support Together With / Header\n• Compatibility\n• Always Return JSON Objects As Top-Level Data Structures To Support Extensibility\n• Treat Open API Definitions As Open For Extension By Default\n• Use Open-Ended List of Values ( ) Instead of Enumerations\n• Data Formats\n• Use Standards for Country, Language and Currency Codes\n• Hypermedia (REST level 3)\n• Use Simple Hypertext Controls for Pagination and Self-References\n• Do not Use Link Headers with JSON Entities\n• JSON Best Practices\n• Boolean property values must not be null\n• Use same semantics for and absent properties\n• Empty array values should not be null\n• Enumerations should be represented as Strings\n• Time durations and intervals could conform to ISO 8601\n• Resources\n• Use lowercase separate words with hyphens for path segments\n• Consider Using (Non-) Nested URLs\n• HTTP Requests\n• Consider To Design and Idempotent\n• Error handling & status codes\n• Use Code 207 for Batch or Bulk Requests\n• Use Code 429 with Headers for Rate Limits\n\nComparing SOA web service interfacing style of SOAP vs. REST, the former tend to be centered around operations that are usually use-case specific and specialized. In contrast, REST is centered around business (data) entities exposed as resources that are identified via URIs and can be manipulated via standardized CRUD-like methods using different representations, and hypermedia. RESTful APIs tend to be less use-case specific and comes with less rigid client / server coupling and are more suitable for an ecosystem of (core) services providing a platform of APIs to build diverse new business services. We apply the RESTful web service principles to all kind of application (micro-) service components, independently from whether they provide functionality via the internet or intranet.\n• We prefer systems to be truly RESTful (level 2)\n\nAn important principle for API design and usage is Postel’s Law, aka The Robustness Principle (see also RFC 1122):\n• Be liberal in what you accept, be conservative in what you send\n\nReadings: Some interesting reads on the RESTful API design style and service architecture:\n• Fielding Dissertation: Architectural Styles and the Design of Network-Based Software Architectures\n\nYou should follow the API First Principle, more specifically:\n• You should design APIs first, before coding its implementation.\n• You should design your APIs consistently with these guidelines.\n• You should call for early review feedback from peers and client developers. Also consider to apply for a lightweight review by the API Team.\n\nSee also the principle Design API first.\n\nIn addition to the API Specification, it is good practice to provide an API user manual to improve client developer experience, especially of engineers that are less experienced in using this API. A helpful API user manual typically describes the following API aspects:\n• Domain Knowledge and context, including API scope, purpose, and use cases\n\nUse specific API instead of a generic one\n\nUse an easy-to-use API with well defined return types whenever possible. If not possible, you may consider using ODATA.\n\nThis section describes a handful of headers which are useful in particular circumstances but not widely known.\n\nContent or entity headers are headers with a prefix. They describe the content of the body of the message and they can be used in both, HTTP requests and responses. Commonly used content headers include but are not limited to:\n• {Content-Disposition} can indicate that the representation is supposed to be saved as a file, and the proposed file name.\n• {Content-Encoding} indicates compression or encryption algorithms applied to the content.\n• {Content-Length} indicates the length of the content (in bytes).\n• {Content-Language} indicates that the body is meant for people literate in some human language(s).\n• {Content-Location} indicates where the body can be found otherwise.\n• {Content-Range} is used in responses to range requests to indicate which part of the requested resource representation is delivered with the body.\n• {Content-Type} indicates the media type of the body content.\n\nUse this list and mention its support in your OpenAPI definition.\n\nConsider to Support Together With / Header\n\nWhen creating or updating resources it may be necessary to expose conflicts and to prevent the ‘lost update’ or ‘initially created’ problem. Following RFC-7232 HTTP: Conditional Requests this can be best accomplished by supporting the {ETag} header together with the {If-Match} or {If-None-Match} conditional header. The contents of an header is either (a) a hash of the response body, (b) a hash of the last modified field of the entity, or (c) a version number or identifier of the entity version.\n\nTo expose conflicts between concurrent update operations via {PUT}, {POST}, or {PATCH}, the header can be used to force the server to check whether the version of the updated entity is conforming to the requested {entity-tag}. If no matching entity is found, the operation is supposed a to respond with status code {412} - precondition failed.\n\nBeside other use cases, can be used in a similar way to expose conflicts in resource creation. If any matching entity is found, the operation is supposed a to respond with status code {412} - precondition failed.\n\nThe {ETag}, {If-Match}, and {If-None-Match} headers can be defined as follows in the API definition:\n\nWhen creating or updating resources it can be helpful or necessary to ensure a strong idempotent behavior comprising same responses, to prevent duplicate execution in case of retries after timeout and network outages. Generally, this can be achieved by sending a client specific unique request key – that is not part of the resource – via {Idempotency-Key} header.\n\nThe unique request key is stored temporarily, e.g. for 24 hours, together with the response and the request hash (optionally) of the first request in a key cache, regardless of whether it succeeded or failed. The service can now look up the unique request key in the key cache and serve the response from the key cache, instead of re-executing the request, to ensure idempotent behavior. Optionally, it can check the request hash for consistency before serving the response. If the key is not in the key store, the request is executed as usual and the response is stored in the key cache.\n\nThis allows clients to safely retry requests after timeouts, network outages, etc. while receive the same response multiple times. Note: The request retry in this context requires to send the exact same request, i.e. updates of the request that would change the result are off-limits. The request hash in the key cache can protection against this misbehavior. The service is recommended to reject such a request using status code {400}.\n\nImportant: To grant a reliable idempotent execution semantic, the resource and the key cache have to be updated with hard transaction semantics – considering all potential pitfalls of failures, timeouts, and concurrent requests in a distributed systems. This makes a correct implementation exceeding the local context very hard.\n\nThe {Idempotency-Key} header should be defined as follows, but you are free to choose your expiration time:\n\nHint: The key cache is not intended as request log, and therefore should have a limited lifetime, else it could easily exceed the data resource in size.\n\nNote: The {Idempotency-Key} header unlike other headers in this section is not standardized in an RFC. Our only reference are the usage in the Stripe API. However, as it fit not into our section about proprietary-headers, and we did not want to change the header name and semantic, we decided to treat it as any other common header.\n\nWhen API versioning is unavoidable, you should design your multi-version RESTful APIs carefully using URI type versioning. It is well known that versions in URIs also do have their negative consequences (like content negotiation issues). We still prefer URI Versioning because it is better supported by tools and infrastructure like API Management and Proxies use URL Mappings as an important property for routing and applying policies for the sake of performance (like e.g. different API subscription plans for different versions).\n\nWe explicitly avoid Resource based versioning, because this is a complexity which an API should not reflect to its consumers.\n\nVersions in the Specification should follow the principles described by SemVer. Versions in URIs are always major versions. Or in a more generic way: we avoid introducing minors or patches anywhere where it could break consumers compatibility.\n\nEvery element on the API that is being deprecated should also be marked in its documentation, using the OpenAPI property “deprecated”.\n\nDuring the deprecation timespan the responses of the deprected API SHOULD have a header accoring to RFC 7234 with warn code 299 and text: “This API call is deprecated and will be removed. Refer release notes for details.\n\nListed below, the table of warn codes described in RFC 7234:\n\nAlways Return JSON Objects As Top-Level Data Structures To Support Extensibility\n\nIn a response body, you must always return a JSON object (and not e.g. an array) as a top level data structure to support future extensibility. JSON objects support compatible extension by additional attributes. This allows you to easily extend your response and e.g. add pagination later, without breaking backwards compatibility.\n\nMaps, even though technically objects, are also forbidden as top level data structures, since they don’t support compatible, future extensions.\n\nTreat Open API Definitions As Open For Extension By Default\n\nThe Open API 2.0 specification is not very specific on default extensibility of objects, and redefines JSON-Schema keywords related to extensibility, like . Following our overall compatibility guidelines, Open API object definitions are considered open for extension by default as per Section 5.18 “additionalProperties” of JSON-Schema.\n\nWhen it comes to Open API 2.0, this means an declaration is not required to make an object definition extensible:\n• API clients consuming data must not assume that objects are closed for extension in the absence of an declaration and must ignore fields sent by the server they cannot process. This allows API servers to evolve their data formats.\n• For API servers receiving unexpected data, the situation is slightly different. Instead of ignoring fields, servers may reject requests whose entities contain undefined fields in order to signal to clients that those fields would not be stored on behalf of the client. API designers must document clearly how unexpected fields are handled for {PUT}, {POST}, and {PATCH} requests.\n\nAPI formats must not declare to be false, as this prevents objects being extended in the future.\n\nNote that this guideline concentrates on default extensibility and does not exclude the use of with a schema as a value, which might be appropriate in some circumstances.\n\nUse Open-Ended List of Values ( ) Instead of Enumerations\n\nEnumerations are per definition closed sets of values, that are assumed to be complete and not intended for extension. This closed principle of enumerations imposes compatibility issues when an enumeration must be extended. To avoid these issues, we strongly recommend to use an open-ended list of values instead of an enumeration unless:\n• the API has full control of the enumeration values, i.e. the list of values does not depend on any external tool or interface, and\n• the list of value is complete with respect to any thinkable and unthinkable future feature.\n\nTo specify an open-ended list of values use the marker {x-extensible-enum} as follows:\n\nNote: {x-extensible-enum} is not JSON Schema conform but will be ignored by most tools.\n\nUse JSON-encoded body payload for transferring structured data. The JSON payload must follow RFC-7159 by having (if possible) a serialized object as the top-level structure, since it would allow for future extension. This also applies for collection resources where one naturally would assume an array.\n\nRead more about date and time format in //TODO date format definieren.\n\nHttp headers including the proprietary headers use the HTTP date format defined in RFC-7231#section-7.1.1.1.\n\nUse Standards for Country, Language and Currency Codes\n\nUse the following standard formats for country, language and currency codes:\n\nWhenever an API defines a property of type or , the precision should be defined by the format as follows to prevent clients from guessing the precision incorrectly, and thereby changing the value unintentionally:\n\nThe precision should be translated by clients and servers into the most specific language types. E.g. for the following definitions the most specific language types in Java will translate to for and or for the :\n\nOwners of APIs used in production must monitor usage of deprecated APIs until the API can be shut down in order to align deprecation and avoid uncontrolled breaking effects.\n\nHint: Use API Management (internal link) to keep track of the usage of your APIs\n\nDuring deprecation phase, the producer should add a header (see RFC 7234 - Warning header) field. When adding the header, the must be and the should be in form of\n\nwith a link to a documentation describing why the API is no longer supported in the current form and what clients should do about it. Adding the header is not sufficient to gain client consent to shut down an API.\n\nClients should monitor the header in HTTP responses to see if an API will be deprecated in future.\n\nWe strive for a good implementation of REST Maturity Level 2 as it enables us to build resource-oriented APIs that make full use of HTTP verbs and status codes. Although this is not HATEOAS, it should not prevent you from designing proper link relationships in your APIs as stated in some best practices, listed below.\n\nWe do not generally recommend to implement REST Maturity Level 3. HATEOAS comes with additional API complexity without real value in our SOA context where client and server interact via REST APIs and provide complex business functions as part of our e-commerce SaaS platform.\n\nOur major concerns regarding the promised advantages of HATEOAS (see also RESTistential Crisis over Hypermedia APIs, Why I Hate HATEOAS and others for a detailed discussion):\n• We follow the API First principle with APIs explicitly defined outside the code with standard specification language. HATEOAS does not really add value for SOA client engineers in terms of API self-descriptiveness: a client engineer finds necessary links and usage description (depending on resource state) in the API reference definition anyway.\n• Generic HATEOAS clients which need no prior knowledge about APIs and explore API capabilities based on hypermedia information provided, is a theoretical concept that we haven’t seen working in practice and does not fit to our SOA set-up. The OpenAPI description format (and tooling based on OpenAPI) doesn’t provide sufficient support for HATEOAS either.\n• In practice relevant HATEOAS approximations (e.g. following specifications like HAL or JSON API) support API navigation by abstracting from URL endpoint and HTTP method aspects via link types. So, Hypermedia does not prevent clients from required manual changes when domain model changes over time.\n• Hypermedia make sense for humans, less for SOA machine clients. We would expect use cases where it may provide value more likely in the frontend and human facing service domain.\n• Hypermedia does not prevent API clients to implement shortcuts and directly target resources without ‘discovering’ them.\n\nHowever, we do not forbid HATEOAS; you could use it, if you checked its limitations and still see clear value for your usage scenario that justifies its additional complexity.\n\nLinks to other resource must always use full, absolute URI.\n\nMotivation: Exposing any form of relative URI (no matter if the relative URI uses an absolute or relative path) introduces avoidable client side complexity. It also requires clarity on the base URI, which might not be given when using features like embedding subresources. The primary advantage of non-absolute URI is reduction of the payload size, which is better achievable by following the recommendation to use gzip compression.\n\nWhen embedding links to other resources into representations you must use the common hypertext control object. It contains at least one attribute:\n• {href}: The URI of the resource the hypertext control is linking to. All our API are using HTTP(s) as URI scheme.\n\nIn API that contain any hypertext controls, the attribute name {href} is reserved for usage within hypertext controls.\n\nThe schema for hypertext controls can be derived from this model:\n\nThe name of an attribute holding such a object specifies the relation between the object that contains the link and the linked resource. Implementations should use names from the {link-relations}[IANA Link Relation Registry] whenever appropriate. As IANA link relation names use hyphen-case notation, while this guide enforces camelCase notation for attribute names, hyphens in IANA names have to be replaced (e.g. the IANA link relation type would become the attribute )\n\nSpecific link objects may extend the basic link type with additional attributes, to give additional information related to the linked resource or the relationship between the source resource and the linked one.\n\nE.g. a service providing “Person” resources could model a person who is married with some other person with a hypertext control that contains attributes which describe the other person ( , ) but also the relationship “spouse” between the two persons ( ):\n\nHypertext controls are allowed anywhere within a JSON model. While this specification would allow HAL, we actually don’t recommend/enforce the usage of HAL anymore as the structural separation of meta-data and data creates more harm than value to the understandability and usability of an API.\n\nUse Simple Hypertext Controls for Pagination and Self-References\n\nFor pagination and self-references a simplified form of the extensible common hypertext controls should be used to reduce the specification and cognitive overhead. It consists of a simple URI value in combination with the corresponding {link-relations}[link relations], e.g. {next}, {prev}, {first}, {last}, or {self}.\n\nDo not Use Link Headers with JSON Entities\n\nFor flexibility and precision, we prefer links to be directly embedded in the JSON payload instead of being attached using the uncommon link header syntax. As a result, the use of the Header defined by RFC 8288 in conjunction with JSON media types is forbidden.\n\nThis is a set of best practices for using JSON as a HTTP body format. JSON here refers to RFC 7159 (which updates RFC 4627), the “application/json” media type and custom JSON media types defined for APIs. This section only cover some specific cases of JSON design decisions. The first some of the following guidelines are about property names, the later ones about values.\n\nProperty names are restricted to ASCII strings in lower case camelCase, matching the following format: . The only exception are keywords like .\n\nRationale: It’s essential to establish a consistent look and feel such that JSON looks as if it came from the same hand, independent of the system that provides the API. It is also very important to stick to names that do generate valid source code when generating code from specs like OpenAPI.\n\nA “map” here is a mapping from string keys to some other type. In JSON this is represented as an object, the key-value pairs being represented by property names and property values. In OpenAPI schema (as well as in JSON schema) they should be represented using additionalProperties with a schema defining the value type. Such an object should normally have no other defined properties.\n\nThe map keys don’t count as property names, and can follow whatever format is natural for their domain. Please document this in the description of the map object’s schema.\n\nHere is an example for such a map definition (the property):\n\nAn actual JSON object described by this might then look like this:\n\nTo indicate they contain multiple values prefer to pluralize array names. This implies that object names should in turn be singular.\n\nBoolean property values must not be null\n\nSchema based JSON properties that are by design booleans must not be presented as nulls. A boolean is essentially a closed enumeration of two values, true and false. If the content has a meaningful null value, strongly prefer to replace the boolean with enumeration of named values or statuses - for example acceptedTermsAndConditions with true or false can be replaced with termsAndConditions with values , and .\n\nUse same semantics for and absent properties\n\nOpen API 3.x allows to mark properties as and as to specify whether properties may be absent ( ) or ( ). If a property is defined to be not and (see 2nd row in Table below), this rule demands that both cases must be handled in the exact same manner by specification.\n\nThe following table shows all combinations and whether the examples are valid:\n\nWhile API designers and implementers may be tempted to assign different semantics to both cases, we explicitly decide against that option, because we think that any gain in expressiveness is far outweighed by the risk of clients not understanding and implementing the subtle differences incorrectly.\n\nAs an example, an API that provides the ability for different users to coordinate on a time schedule, e.g. a meeting, may have a resource for options in which every user has to make a . The difference between undecided and decided against any of the options could be modeled as absent and respectively. It would be safer to express the case with a dedicated Null object, e.g. compared to .\n\nMoreover, many major libraries have somewhere between little to no support for a /absent pattern (see Gson, Moshi, Jackson, JSON-B). Especially strongly-typed languages suffer from this since a new composite type is required to express the third state. Nullable / / types could be used but having nullable references of these types completely contradicts their purpose.\n\nThe only exception to this rule is JSON Merge Patch RFC 7396) which uses to explicitly indicate property deletion while absent properties are ignored, i.e. not modified.\n\nEmpty array values should not be null\n\nEmpty array values can unambiguously be represented as the empty list, .\n\nEnumerations should be represented as Strings\n\nStrings are a reasonable target for values that are by design enumerations.\n\nDates and date-time properties should end with to distinguish them from boolean properties which otherwise would have very similar or even identical names:\n\nUse the date and time formats defined by RFC 3339:\n• for “date” use strings matching , for example:\n• for “date-time” use strings matching , for example\n\nNote that the OpenAPI format “date-time” corresponds to “date-time” in the RFC) and for a date (note that the OpenAPI format “date” corresponds to “full-date” in the RFC). Both are specific profiles, a subset of the international standard ISO 8601.\n\nA zone offset may be used (both, in request and responses) — this is simply defined by the standards. However, we encourage restricting dates to UTC and without offsets. For example rather than . From experience we have learned that zone offsets are not easy to understand and often not correctly handled. Note also that zone offsets are different from local times that might be including daylight saving time. Localization of dates should be done by the services that provide user interfaces, if required.\n\nWhen it comes to storage, all dates should be consistently stored in UTC without a zone offset. Localization should be done locally by the services that provide user interfaces, if required.\n\nSometimes it can seem data is naturally represented using numerical timestamps, but this can introduce interpretation issues with precision - for example whether to represent a timestamp as 1460062925, 1460062925000 or 1460062925.000. Date strings, though more verbose and requiring more effort to parse, avoid this ambiguity.\n\nTime durations and intervals could conform to ISO 8601\n\nSchema based JSON properties that are by design durations and intervals could be strings formatted as recommended by ISO 8601 (Appendix A of RFC 3339 contains a grammar for durations).\n\nConsider to introduce pagination as soon as you provide search functionality on your API with more than some few dozens of possible records.\n\nAccess to lists of data items must support pagination to protect the service against overload as well as for best client side iteration and batch processing experience. This holds true for all lists that are (potentially) larger than just a few hundred entries.\n\nThere are two well known page iteration techniques:\n• Cursor/Limit-based — aka key-based — pagination: a unique key element identifies the first page entry (see also Facebook’s guide)\n\nThe technical conception of pagination should also consider user experience related issues. As mentioned in this article, jumping to a specific page is far less used than navigation via {next}/{prev} page links. This favours cursor-based over offset-based pagination.\n\nCursor-based pagination is usually better and more efficient when compared to offset-based pagination. Especially when it comes to high-data volumes and/or storage in NoSQL databases.\n\nBefore choosing cursor-based pagination, consider the following trade-offs:\n• Usability/framework support:\n• Offset-based pagination is more widely known than cursor-based pagination, so it has more framework support and is easier to use for API clients\n• Use case - jump to a certain page:\n• If jumping to a particular page in a range (e.g., 51 of 100) is really a required use case, cursor-based navigation is not feasible.\n• Data changes may lead to anomalies in result pages:\n• Offset-based pagination may create duplicates or lead to missing entries if rows are inserted or deleted between two subsequent paging requests.\n• If implemented incorrectly, cursor-based pagination may fail when the cursor entry has been deleted before fetching the pages.\n• Performance considerations - efficient server-side processing using offset-based pagination is hardly feasible for:\n• Very big data sets, especially if they cannot reside in the main memory of the database.\n• Cursor-based navigation may not work if you need the total count of results.\n\nThe {cursor} used for pagination is an opaque pointer to a page, that must never be inspected or constructed by clients. It usually encodes (encrypts) the page position, i.e. the identifier of the first or last page element, the pagination direction, and the applied query filters - or a hash over these - to safely recreate the collection. The {cursor} may be defined as follows:\n\nThe page information for cursor-based pagination should consist of a {cursor} set, that besides {next} may provide support for {prev}, {first}, {last}, and {self} as follows:\n\nTo simplify client design, APIs should support simplified hypertext controls for pagination over collections whenever applicable. Beside {next} this may comprise the support for {prev}, {first}, {last}, and {self} as {link-relations}[link relations].\n\nThe page content is transported via {items}, while the {query} object may contain the query filters applied to the collection resource as follows:\n\nNote: In case of complex search requests, e.g. when {GET-with-body} is required, the {cursor} may not be able to encode all query filters. In this case, it is best practice to encode only page position and direction in the {cursor} and transport the query filter in the body - in the request as well as in the response. To protect the pagination sequence, in this case it is recommended, that the {cursor} contains a hash over all applied query filters for pagination request validation.\n\nRemark: You should avoid providing a total count unless there is a clear need to do so. Very often, there are significant system and performance implications when supporting full counts. Especially, if the data set grows and requests become complex queries and filters drive full scans. While this is an implementation detail relative to the API, it is important to consider the ability to support serving counts over the life of a service.\n\nAPIs should support techniques for reducing bandwidth based on client needs. This holds for APIs that (might) have high payloads and/or are used in high-traffic scenarios like the public Internet and telecommunication networks. Typical examples are APIs used by mobile web app clients with (often) less bandwidth connectivity.\n• {ETag} and {If-Match}/{If-None-Match} headers to avoid re-fetching of unchanged resources\n• {Prefer} header with or to anticipate reduced processing requirements of clients\n• Pagination for incremental access of larger collections of data items\n• Caching of master data items, i.e. resources that change rarely or not at all after creation\n\nKeep in mind that performance features must always well documented in the API documentation (e.g. if you use caching).\n\nThe WAF checks all known OWASP 10 security risks. To avoid false positives, all API Provider should use the OWASP Secure Coding Practice Checklist \n\n Example: \n\n Do not use <> in the payloads:\n\nwill be blocked as an html injection. In this case you should use an empty or null value:\n\nExceptions can be configured in the WAF and must be reported to the Network and Security Team.\n\nAPIs must define permissions to protect their resources. Thus, at least one permission must be assigned to each endpoint. Permissions are defined as shown in the previous section.\n\nThe naming schema for permissions corresponds to the naming schema for hostnames and event type names.\n\nAPIs should stick to component specific permissions without resource extension to avoid governance complexity of too many fine grained permissions. For the majority of use cases, restricting access to specific API endpoints using read and write is sufficient for controlling access for client types like merchant or retailer business partners, customers or operational staff. However, in some situations, where the API serves different types of resources for different owners, resource specific scopes may make sense.\n\nAfter permission names are defined and the permission is declared in the security definition at the top of an API specification, it should be assigned to each API operation by specifying a security requirement like this:\n\nHint: you need not explicitly define the “Authorization” header; it is a standard header so to say implicitly defined via the security section.\n\nAs long as the functional naming is not supported for permissions, permission names in APIs must conform to the following naming pattern:\n\nREST is all about your resources, so consider the domain entities that take part in web service interaction, and aim to model your API around these using the standard HTTP methods as operation indicators. For instance, if an application has to lock articles explicitly so that only one user may edit them, create an article lock with {PUT} or {POST} instead of using a lock action.\n\nThe added benefit is that you already have a service for browsing and filtering article locks.\n\nAn API should contain the complete business processes containing all resources representing the process. This enables clients to understand the business process, foster a consistent design of the business process, allow for synergies from description and implementation perspective, and eliminates implicit invisible dependencies between APIs.\n\nIn addition, it prevents services from being designed as thin wrappers around databases, which normally tends to shift business logic to the clients.\n\nAs a rule of thumb resources should be defined to cover 90% of all its client’s use cases. A useful resource should contain as much information as necessary, but as little as possible. A great way to support the last 10% is to allow clients to specify their needs for more/less information by supporting filtering and embedding.\n\nThe API describes resources, so the only place where actions should appear is in the HTTP methods. In URLs, use only nouns. Instead of thinking of actions (verbs), it’s often helpful to think about putting a message in a letter box: e.g., instead of having the verb cancel in the url, think of sending a message to cancel an order to the cancellations letter box on the server side.\n\nAPI resources represent elements of the application’s domain model. Using domain-specific nomenclature for resource names helps developers to understand the functionality and basic semantics of your resources. It also reduces the need for further documentation outside the API definition. For example, “sales-order-items” is superior to “order-items” in that it clearly indicates which business object it represents. Along these lines, “items” is too general.\n\nSome API resources may contain or reference sub-resources. Embedded sub-resources, which are not top-level resources, are parts of a higher-level resource and cannot be used outside of its scope. Sub-resources should be referenced by their name and identifier in the path segments.\n\nComposite identifiers must not contain as a separator. In order to improve the consumer experience, you should aim for intuitively understandable URLs, where each sub-path is a valid reference to a resource or a set of resources. For example, if is a valid path of your API, then , and must be valid as well in principle.\n\nUse lowercase separate words with hyphens for path segments\n\nThis applies to concrete path segments and not the names of path parameters.\n\nConsider Using (Non-) Nested URLs\n\nIf a sub-resource is only accessible via its parent resource and may not exists without parent resource, consider using a nested URL structure, for instance:\n\nHowever, if the resource can be accessed directly via its unique id, then the API should expose it as a top-level resource. For example, customer has a collection for sales orders; however, sales orders have globally unique id and some services may choose to access the orders directly, for instance:\n\nTo keep maintenance and service evolution manageable, we should follow “functional segmentation” and “separation of concern” design principles and do not mix different business functionalities in same API definition. In practice this means that the number of resource types exposed via an API should be limited. In this context a resource type is defined as a set of highly related resources such as a collection, its members and any direct sub-resources.\n\nFor example, the resources below would be counted as three resource types, one for customers, one for the addresses, and one for the customers’ related addresses:\n• We consider part of the resource type because it has a one-to-one relation to the customer without an additional identifier.\n• We consider and as separate resource types because also exists with an additional identifier for the address.\n• We consider and as separate resource types because there’s no reliable way to be sure they are the same.\n\nGiven this definition, our experience is that well defined APIs involve no more than 4 to 8 resource types. There may be exceptions with more complex business domains that require more resources, but you should first check if you can split them into separate subdomains with distinct APIs.\n\nNevertheless one API should hold all necessary resources to model complete business processes helping clients to understand these flows.\n\nThere are main resources (with root url paths) and sub-resources (or nested resources with non-root urls paths). Use sub-resources if their life cycle is (loosely) coupled to the main resource, i.e. the main resource works as collection resource of the subresource entities. You should use <= 3 sub-resource (nesting) levels — more levels increase API complexity and url path length. (Remember, some popular web browsers do not support URLs of more than 2000 characters.)\n\nFor the definition of how to use http methods, see the principle Must use HTTP methods correctly.\n• {RFC-safe} - the operation semantic is defined to be read-only, meaning it must not have intended side effects, i.e. changes, to the server state.\n• {RFC-idempotent} - the operation has the same intended effect on the server state, independently whether it is executed once or multiple times. Note: this does not require that the operation is returning the same response or status code.\n• {RFC-cacheable} - to indicate that responses are allowed to be stored for future reuse. In general, requests to safe methods are cachable, if it does not require a current or authoritative response from the server.\n\nNote: The above definitions, of intended (side) effect allows the server to provide additional state changing behavior as logging, accounting, pre- fetching, etc. However, these actual effects and state changes, must not be intended by the operation so that it can be held accountable.\n\nMethod implementations must fulfill the following basic properties according to RFC 7231:\n\nConsider To Design and Idempotent\n\nIn many cases it is helpful or even necessary to design {POST} and {PATCH} idempotent for clients to expose conflicts and prevent resource duplicate (a.k.a. zombie resources) or lost updates, e.g. if same resources may be created or changed in parallel or multiple times. To design an idempotent API endpoint owners should consider to apply one of the following three patterns.\n• A resource specific conditional key provided via header in the request. The key is in general a meta information of the resource, e.g. a hash or version number, often stored with it. It allows to detect concurrent creations and updates to ensure idempotent behavior.\n• A resource specific secondary key provided as resource property in the request body. The secondary key is stored permanently in the resource. It allows to ensure idempotent behavior by looking up the unique secondary key in case of multiple independent resource creations from different clients (use Secondary Key for Idempotent Design).\n• A client specific idempotency key provided via {Idempotency-Key} header in the request. The key is not part of the resource but stored temporarily pointing to the original response to ensure idempotent behavior when retrying a request.\n\nNote: While conditional key and secondary key are focused on handling concurrent requests, the idempotency key is focused on providing the exact same responses, which is even a stronger requirement than the idempotency defined above. It can be combined with the two other patterns.\n\nTo decide, which pattern is suitable for your use case, please consult the following table showing the major properties of each pattern:\n\nNote: The patterns applicable to {PATCH} can be applied in the same way to {PUT} and {DELETE} providing the same properties.\n\nIf you mainly aim to support safe retries, we suggest to apply conditional key secondary key pattern before the Idempotency Key pattern.\n\nThe most important pattern to design {POST} idempotent for creation is to introduce a resource specific secondary key provided in the request body, to eliminate the problem of duplicate (a.k.a zombie) resources.\n\nThe secondary key is stored permanently in the resource as alternate key or combined key (if consisting of multiple properties) guarded by a uniqueness constraint enforced server-side, that is visible when reading the resource. The best and often naturally existing candidate is a unique foreign key, that points to another resource having one-on-one relationship with the newly created resource, e.g. a parent process identifier.\n\nA good example here for a secondary key is the shopping cart ID in an order resource.\n\nNote: When using the secondary key pattern without {Idempotency-Key} all subsequent retries should fail with status code {409} (conflict). We suggest to avoid {200} here unless you make sure, that the delivered resource is the original one implementing a well defined behavior. Using {204} without content would be a similar well defined option.\n\nHeader and query parameters allow to provide a collection of values, either by providing a comma-separated list of values or by repeating the parameter multiple times with different values as follows:\n\nAs Open API does not support both schemas at once, an API specification must explicitly define the collection format to guide consumers as follows:\n\nWhen choosing the collection format, take into account the tool support, the escaping of special characters and the maximal URL length.\n\nSometimes certain collection resources or queries will not list all the possible elements they have, but only those for which the current client is authorized to access.\n\nImplicit filtering could be done on:\n• the collection of resources being return on a parent {GET} request\n• the fields returned for the resource’s detail\n\nIn such cases, the implicit filtering must be in the API specification (in its description).\n\nIf an employee of the company Foo accesses one of our business-to-business service and performs a , it must, for legal reasons, not display any other business partner that is not owned or contractually managed by her/his company. It should never see that we are doing business also with company Bar.\n\nResponse as seen from a consumer working at :\n\nResponse as seen from a consumer working at :\n\nThe API Specification should then specify something like this:\n\nAPIs should define the functional, business view and abstract from implementation aspects. Success and error responses are a vital part to define how an API is used correctly.\n\nTherefore, you must define all success and service specific error responses in your API specification. Both are part of the interface definition and provide important information for service clients to handle standard as well as exceptional situations.\n\nHint: In most cases it is not useful to document all technical errors, especially if they are not under control of the service provider. Thus unless a response code conveys application-specific functional semantics or is used in a none standard way that requires additional explanation, multiple error response specifications can be combined using the following pattern:\n\nAPI designers should also think about a troubleshooting board as part of the associated online API documentation. It provides information and handling guidance on application-specific errors and is referenced via links from the API specification. This can reduce service support tasks and contribute to service client and provider performance.\n\nYou must use the most specific HTTP status code when returning information about your request processing status or error situations.\n\nUse Code 207 for Batch or Bulk Requests\n\nSome APIs are required to provide either batch or bulk requests using {POST} for performance reasons, i.e. for communication and processing efficiency. In this case services may be in need to signal multiple response codes for each part of an batch or bulk request. As HTTP does not provide proper guidance for handling batch/bulk requests and responses, we herewith define the following approach:\n• A batch or bulk request always has to respond with HTTP status code {207}, unless it encounters a generic or unexpected failure before looking at individual parts.\n• A batch or bulk response with status code {207} always returns a multi-status object containing sufficient status and/or monitoring information for each part of the batch or bulk request.\n• A batch or bulk request may result in a status code {4xx}/{5xx}, only if the service encounters a failure before looking at individual parts or, if an unanticipated failure occurs.\n\nThe before rules apply even in the case that processing of all individual part fail or each part is executed asynchronously! They are intended to allow clients to act on batch and bulk responses by inspecting the individual results in a consistent way.\n\nNote: while a batch defines a collection of requests triggering independent processes, a bulk defines a collection of independent resources created or updated together in one request. With respect to response processing this distinction normally does not matter.\n\nUse Code 429 with Headers for Rate Limits\n\nAPIs that wish to manage the request rate of clients must use the {429} (Too Many Requests) response code, if the client exceeded the request rate (see RFC 6585). Such responses must also contain header information providing further details to the client. There are two approaches a service can take for header information:\n• Return a {Retry-After} header indicating how long the client ought to wait before making a follow-up request. The Retry-After header can contain a HTTP date value to retry after or the number of seconds to delay. Either is acceptable but APIs should prefer to use a delay in seconds.\n• Return a trio of headers. These headers (described below) allow a server to express a service level in the form of a number of allowing requests within a given window of time and when the window is reset.\n• : The maximum number of requests that the client is allowed to make in this window.\n• : The number of requests allowed in the current window.\n• : The relative time in seconds when the rate limit window will be reset. Beware that this is different to Github and Twitter’s usage of a header with the same name which is using UTC epoch seconds instead.\n\nThe reason to allow both approaches is that APIs can have different needs. Retry-After is often sufficient for general load handling and request throttling scenarios and notably, does not strictly require the concept of a calling entity such as a tenant or named account. In turn this allows resource owners to minimise the amount of state they have to carry with respect to client requests. The ‘X-RateLimit’ headers are suitable for scenarios where clients are associated with pre-existing account or tenancy structures. ‘X-RateLimit’ headers are generally returned on every request and not just on a 429, which implies the service implementing the API is carrying sufficient state to track the number of requests made within a given window for each named entity.\n\n{RFC-7807}[RFC 7807] defines a Problem JSON object and the media type . Operations should return it (together with a suitable status code) when any problem occurred during processing and you can give more details than the status code itself can supply, whether it be caused by the client or the server (i.e. both for {4xx} or {5xx} error codes).\n\nThe Open API schema definition of the Problem JSON object can be found on github. You can reference it by using:\n\nYou may define custom problem types as extension of the Problem JSON object if your API need to return specific additional error detail information.\n\nHint for backward compatibility: A previous version of this guideline (before the publication of RFC-7807 and the registration of the media type) told to return custom variant of the media type . Servers for APIs defined before this change should pay attention to the header sent by the client and set the header of the problem response correspondingly. Clients of such APIs should accept both media types.\n\nStack traces contain implementation details that are not part of an API, and on which clients should never rely. Moreover, stack traces can leak sensitive information that partners and third parties are not allowed to receive and may disclose insights about vulnerabilities to attackers."
    },
    {
        "link": "https://testfully.io/blog/api-rate-limit",
        "document": "API rate limiting is a technique for controlling how many requests a user or an application can send to an API in a timeframe. It is like setting a speed limit on a road. Just as speed limits ensure traffic moves smoothly without accidents, rate limits ensure that APIs handle traffic efficiently without getting overwhelmed.\n\nRate limiting is important for maintaining the performance and reliability of APIs, especially those that are publicly accessible or serve a large number of users. By limiting the number of requests, developers can prevent any one user or service from consuming too many resources, which could degrade the experience for others or even cause the API to become unavailable. This helps keep the system running smoothly, ensuring everyone gets a fair share of the API’s capabilities.\n\nLet me tell you why API rate limiting is such a big deal:\n• Protecting Resource Usage: Let’s face it, resources are finite, and we have to be smart about how we use them. API rate limiting helps protect the server from being overwhelmed by too many requests at once. By capping the number of requests, it ensures that all users get access to the API without any hiccups.\n• Controlling Data Flow: In a high-traffic environment, data can become a bottleneck. Rate limiting helps manage this flow, ensuring that data is delivered consistently and reliably. This is key to maintaining a smooth user experience, especially when the API is under heavy load.\n• Maximizing Cost-Efficiency: Running an API, especially at scale, can be expensive. Rate limiting helps you manage those costs by reducing unnecessary requests. This way, you’re not paying for excess server capacity that you don’t need. It’s all about making sure that the API runs efficiently without draining your budget.\n• Managing Different User Access Levels: Not all users are the same, and rate limiting can help you manage that. For example, you might offer higher limits for premium users compared to those on a free plan. This ensures that your top-tier customers get the service they’re paying for while still allowing access to other users. It’s a balancing act that rate limiting handles quite well.\n• Preventing DoS and DDoS Attacks: One of the main reasons for rate limiting is to protect against Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks. These attacks aim to overwhelm your API with traffic, but with rate limits in place, you can significantly reduce their impact. It’s like having a safety valve that releases pressure, keeping the system from crashing under the load.\n• Improving User Experience: When users interact with your API, they expect it to be fast and reliable. Rate limiting ensures that the API remains responsive, even when demand is high. This consistency is key to a good user experience, making sure that everyone gets what they need without delay.\n• Reducing Costs: Let’s be honest; no one likes unexpected costs. Rate limiting helps keep your operational costs in check by preventing overuse of the API. By capping the number of requests, you avoid the need to scale up your infrastructure unnecessarily, which can save a lot of money in the long run.\n\nAPI rate limiting can be implemented using several techniques and algorithms, each tailored to handle different traffic patterns and system requirements. Here’s a breakdown of the most common methods:\n• Fixed Window: This straightforward technique limits the number of requests allowed within a fixed time period (e.g., 100 requests per minute). It’s easy to implement but can lead to issues like traffic spikes at the start of each new window because users may learn to time their requests to maximize usage.\n• Sliding Window: Unlike the fixed window, the sliding window technique continuously tracks requests over a rolling time frame, such as the last 60 seconds. This method helps smooth out traffic, making it a better option for handling bursty traffic where requests might cluster at certain times.\n• Leaky Bucket: This algorithm processes requests at a consistent, controlled rate. Think of it like water dripping steadily out of a bucket. If requests come in faster than they can be processed, they’re either queued or dropped, helping to prevent sudden traffic surges from overwhelming the system.\n• Token Bucket: Similar to the leaky bucket but with more flexibility, the token bucket algorithm allows bursts of requests as long as there are tokens available. Tokens are added to a bucket at a steady rate, and each request consumes a token. If the bucket is empty, requests are delayed or denied until tokens are replenished. This method is ideal for scenarios where short bursts of high traffic are acceptable.\n\nThese techniques are supported by specific algorithms designed to enforce the rate limits effectively:\n• Fixed Window Counter: Tracks the number of requests within a fixed time window. It’s simple but can result in uneven traffic distribution.\n• Sliding Log: Logs each request with a timestamp and checks it against the current time to enforce limits, offering more precise control but at the cost of higher resource usage.\n• Sliding Window Counter: Combines the fixed window and sliding log approaches, applying limits more evenly by continuously adjusting the window as new requests are made.\n\nAPI rate limiting can be adjusted to fit different use cases depending on the needs of the service and the type of traffic it handles. Here’s an overview of the various methods and types of rate limiting commonly used:\n\nKey-level rate limiting focuses on controlling the number of API requests that can be made with a specific API key. This method is particularly useful when you want to manage usage based on individual users or applications. By applying limits at the key level, you can ensure that no single user or application consumes too many resources, which helps maintain the API’s overall performance and availability for everyone. This approach is often used in tiered service models where different users have different levels of access.\n\nAPI-level rate limiting applies limits across all users accessing a particular API. This method is useful for protecting the API as a whole, especially when there is a risk of overwhelming the service with too many requests at once. By setting a global limit, you can prevent traffic increases that could degrade the performance of the API for all users. API-level rate limiting is often implemented in scenarios where the API serves a large number of users and the risk of excessive simultaneous requests is high.\n\nResource-based rate limiting targets specific resources within an API, such as particular endpoints or data services. This method is especially useful when certain parts of the API are more resource-intensive or have higher demand than others. By limiting the number of requests to these specific resources, you can ensure that they remain available and responsive even under heavy load. This approach helps prevent any single resource from becoming a bottleneck that could slow down the entire API.\n\nIP throttling limits the number of requests based on the user’s IP address. This method is very effective in scenarios where you need to reduce the risk of abuse from specific sources, such as automated bots or potential malicious actors. By tracking requests by IP address, you can prevent a single source from overwhelming the API, which is especially useful for defending against DoS attacks. IP throttling is often used in conjunction with other rate-limiting methods to provide a layered defence strategy.\n\nWhile rate limiting and throttling are often mentioned together, they are not the same thing, though they do serve similar purposes in managing API traffic.\n• Rate Limiting: This involves setting a cap on the number of requests that can be made to an API within a specified time frame. If the limit is exceeded, further requests are blocked until the limit resets. This approach is useful for ensuring that the API remains available and responsive by preventing overuse.\n• API Throttling: Throttling, on the other hand, doesn’t necessarily block requests completely but rather slows down the rate at which requests are processed. Instead of being denied, a user’s following requests are queued or delayed when they exceed the permitted request rate. This can help manage traffic more gracefully, allowing legitimate users to continue accessing the API, albeit at a reduced speed.\n\nUse Cases for Each\n• Rate Limiting: Best used when you need to strictly enforce limits to prevent any single user from overloading the system. This is particularly important in scenarios where resource availability is critical and where allowing excessive requests could degrade service for everyone.\n• Throttling: Ideal for situations where you want to manage traffic without outright blocking users. Throttling is useful for maintaining service availability during traffic spikes, as it ensures that requests are processed more slowly rather than denied, which can help maintain a better user experience during peak times.\n\nHow to Implement Rate Limiting in API Environments\n\nImplementing rate limiting in an API environment involves choosing the right strategy and configuring it to meet the needs of your application. Here’s a step-by-step guide on how to do it:\n• Choose the Right Algorithm: Depending on your API’s traffic patterns and usage needs, select the appropriate rate-limiting algorithm (Fixed Window, Sliding Window, Leaky Bucket, or Token Bucket). Each has its own advantages, so it’s important to understand your specific requirements before deciding.\n• Set Limits Based on API Usage: Determine the appropriate limits based on historical data and expected traffic. Consider factors like peak usage times, the average number of requests per user, and how often you expect bursts of traffic. This data will help you set realistic and effective limits.\n• Configure the API Gateway or Middleware: Most modern API gateways, such as Kong, NGINX, or AWS API Gateway, support rate limiting out of the box. Configure your chosen gateway to enforce the limits you’ve set. If your API doesn’t use a gateway, you may need to implement rate-limiting logic directly within your application code.\n• Implement Dynamic Rate Limits: Consider implementing dynamic rate limits that can adjust based on real-time conditions, such as server load or the number of active users. This approach can help your API adapt to changing conditions and maintain performance even under heavy load.\n• Monitor and Adjust: After implementing rate limiting, continuously monitor the API’s performance to ensure that the limits are effective and that they don’t negatively impact legitimate users. Use metrics and logs to track how often rate limits are being hit and adjust as necessary to balance performance and user experience.\n\nTesting is a crucial part of implementing rate limiting. Here’s how to do it effectively:\n• Simulate High Traffic: Use tools like Apache JMeter or Postman to simulate high-traffic scenarios and observe how your API responds to reaching the rate limit. This will help you identify any issues with your configuration and ensure that the API behaves as expected under load.\n• Check for Edge Cases: Test how your API handles edge cases, such as just before and just after the rate limit is hit. This will help ensure that users experience a smooth transition and that legitimate requests aren’t mistakenly blocked.\n• Monitor API Responses: Pay attention to the HTTP status codes your API returns. When the rate limit is exceeded, the API should return a 429 status code (“Too Many Requests”) along with a message indicating the limit has been reached. Ensure that this behavior is consistent across different endpoints and user types.\n• User Feedback: Implement user-friendly error messages and headers that inform users when they’ve hit the rate limit and when they can retry. This improves the user experience and clarifies why requests are denied.\n\nTo make the most out of API rate limiting, follow these best practices:\n• Assess API Call Frequency: Regularly review your API call frequency to ensure your rate limits are aligned with actual usage patterns. This helps you set limits that are neither too restrictive nor too lenient.\n• Use Dynamic Rate Limits: Implement dynamic rate limits that can adjust based on server load or user behavior, helping to maintain performance during unexpected traffic spikes.\n• Monitor User Activity: Keep an eye on how users are interacting with your API. Monitoring tools can help you detect patterns that might indicate abuse or excessive use, allowing you to adjust limits accordingly.\n• Set Appropriate Timeouts: Set timeouts that are reasonable for your API’s typical use cases. This helps prevent users from getting locked out for too long, improving their overall experience.\n• Use Caching: Implement caching strategies to reduce the number of requests that need to hit the API directly. This can help alleviate some of the load and reduce the likelihood of users hitting rate limits.\n• Provide Feedback on Rate Limit Errors: Make sure your API users are aware of when they’ve hit rate limits and what they can do next. This transparency can reduce frustration and help users manage their usage more effectively.\n• Have a Backup Plan: Consider having fallback strategies in place for when rate limits are hit, such as redirecting users to cached data or providing alternative API endpoints that are less heavily used.\n\nAs APIs grow in complexity and scale, basic rate limiting might not be sufficient to handle all the challenges that arise. Advanced strategies help manage traffic more effectively, ensuring fairness, performance, and a positive user experience. Let’s explore some of these advanced strategies:\n\nSudden spikes in traffic can occur for various reasons, such as a popular event, marketing campaign, or unexpected user activity. These spikes can overwhelm your API if not properly managed.\n• Dynamic Rate Limiting: This approach adjusts the rate limits in real time based on current traffic and server load. By dynamically scaling the rate limits, your API can handle traffic spikes more smoothly, preventing service disruptions. Dynamic rate limiting is particularly useful when traffic patterns are unpredictable, allowing the API to adapt on the fly.\n\nAs your API scales, it’s crucial to ensure that your rate-limiting strategies can handle the increased traffic without becoming a bottleneck.\n• Distributed Rate Limiting: This approach spreads the load across multiple servers or instances, preventing any single point from becoming a bottleneck. Distributed rate limiting allows your system to scale horizontally, handling more traffic without sacrificing performance. It’s especially useful for large-scale APIs that need to maintain high availability under heavy load.\n• Integrating with CDNs and Load Balancers: Leveraging Content Delivery Networks (CDNs) and load balancers can further enhance your rate-limiting strategy. By distributing traffic and caching content closer to users, these tools can reduce the strain on your API and improve response times, even under heavy load.\n\nTo better understand how these concepts work in real-world scenarios, let’s look at some examples of how major platforms implement API rate limiting:\n\nFacebook uses rate limiting to control the number of requests third-party developers can make to its API. This helps ensure that the platform remains stable and that resources are fairly distributed among all users and applications. Facebook’s API limits can vary depending on the type of data being accessed and the user’s privacy settings. This helps the platform manage its resources efficiently while maintaining user privacy and data security.\n\nGoogle Maps uses API rate limiting to manage access to its extensive location data. Developers can monitor their usage and quotas through the Google Developer Console, which provides detailed reports and alerts. This transparency allows developers to manage their API usage effectively and avoid unexpected charges. Google also offers the option to purchase additional quotas or credits if the default limits are not sufficient for a project’s needs.\n\nSocial media platforms like X (formerly Twitter) and Instagram implement rate limits to protect their services from abuse and to ensure that third-party applications do not negatively impact the user experience. These limits are typically enforced across all endpoints and are adjusted based on the specific API being accessed and the type of data requested. By implementing strict rate limits, these platforms can maintain the integrity of their services while allowing third-party developers to build on their ecosystems.\n\nImplementing rate limiting in APIs can be complex, especially as the API scales and the number of users increases. Here are some common challenges and their potential solutions:\n\nChallenge: Setting the correct rate limit is critical but can be difficult. If the limit is too low, legitimate users may be blocked from accessing the service. If it’s too high, it may not effectively prevent abuse or protect the system from being overwhelmed.\n\nSolution: To identify the appropriate rate limit, start by analyzing historical data to understand typical usage patterns. Monitor the average number of requests per user and peak usage times. This data can inform your decision on where to set the initial limits. It’s also helpful to start with a conservative limit and gradually adjust it based on real-time monitoring and feedback.\n\nChallenge: Designing a rate limiting system that is both efficient and scalable can be complex. The system needs to be robust enough to handle varying traffic patterns without becoming a bottleneck itself.\n\nSolution: An efficient rate-limiting system should include the following key components:\n• Scalability: Use distributed rate limiting to spread the load across multiple servers, ensuring that no single point of failure can disrupt the system.\n• Flexibility: Implement dynamic rate limits that can adjust based on current traffic conditions, allowing the system to respond to spikes in demand without manual intervention.\n• Accuracy: Ensure that your rate-limiting algorithms are precise, avoiding both false positives (where legitimate traffic is blocked) and false negatives (where abusive traffic is allowed through).\n\nWhile API rate limiting is essential for maintaining service quality and preventing abuse, there are methods that some users or developers may use to bypass these restrictions. Understanding these techniques and implementing safeguards is crucial to ensure that your API remains secure and fair for all users.\n\nAPI rate limits are in place to ensure fair usage, protect resources, and maintain the overall stability of the system. Bypassing these limits can lead to resource strain, degraded performance for other users, and even legal issues. It’s important to communicate clearly with your users about the purpose of rate limits and the potential consequences of bypassing them. Offering flexible pricing plans or usage tiers can help meet the needs of different users, reducing the temptation to circumvent limits.\n• Using Multiple API Keys: Some users may try to bypass rate limits by acquiring multiple API keys and distributing their requests across these keys. This can effectively increase their request capacity beyond the intended limit.\n• Countermeasure: Implementing key-level rate limiting and tracking usage across all API keys associated with a single user or account can help mitigate this tactic. By monitoring for patterns that suggest multiple keys are being used to circumvent limits, you can take action, such as suspending keys or accounts that violate terms.\n• IP Address Rotation via Proxies or VPNs: Users might rotate their IP addresses by using proxy servers or VPNs to avoid hitting IP-based rate limits. This method allows them to appear as multiple users, each with its own rate limit.\n• Countermeasure: To counter this, you can implement more sophisticated IP tracking methods that analyze patterns in IP usage, such as frequent changes in IP addresses associated with the same user. Additionally, combining IP-based rate limiting with key-level or user-level limits can create multiple layers of defence, making it more difficult for users to bypass limits by changing their IP address.\n• Exploiting Inconsistent Rate Limiting Rules: If different endpoints within an API have inconsistent or poorly enforced rate limits, users might exploit these differences to bypass overall restrictions. For example, they might target endpoints with higher or non-existent limits.\n• Countermeasure: Ensure that rate limits are consistently applied across all endpoints and that more sensitive or resource-intensive endpoints have appropriate limits. Regular audits of rate-limiting configurations can help identify and correct any inconsistencies.\n\nAPI rate limiting is not just a technical safeguard; it’s a fundamental part of maintaining a reliable, secure, and fair API service. By setting appropriate limits, implementing advanced strategies, and continuously monitoring usage, you can protect your API from abuse and ensure that it scales effectively as your user base grows.\n\nAs APIs become more central to digital services, the importance of robust rate limiting cannot be overstated. Whether you’re preventing traffic spikes, managing different user access levels, or defending against attempts to bypass limits, a well-designed rate-limiting strategy is key to delivering a consistent and reliable user experience.\n\nIn summary, by carefully planning, implementing, and maintaining your rate-limiting strategies, you can ensure that your API remains a valuable and sustainable resource for all users."
    },
    {
        "link": "https://alooba.com/skills/concepts/data-science/random-number-generation",
        "document": "Random number generation is the process of generating numbers that lack any discernible pattern or predictability. It is an essential concept in the field of data science and finds applications in various domains, such as computer simulations, cryptography, statistical analysis, and gaming. In essence, random number generation involves producing a sequence of numbers that appear to be random and unbiased. These numbers should not follow any specific pattern, making them unpredictable and suitable for use in situations where true randomness is required. Computer algorithms play a crucial role in generating random numbers. These algorithms utilize mathematical formulas and procedures to produce sequences of seemingly random numbers. However, despite their deterministic nature, they are designed to mimic randomness by incorporating various factors like system time, environmental variables, or user inputs as inputs for their calculations. The generated random numbers serve as a basis for numerous applications in data science. For instance, they can be used to model complex systems, simulate real-world scenarios, or test the effectiveness of algorithms. In addition, random number generation plays a vital role in cryptography, where unpredictable numbers are necessary for secure communication and data protection. It's important to note that the quality of random number generation is crucial. A high-quality random number generator ensures that the generated sequence of numbers exhibits statistical properties similar to true random numbers. These properties include uniform distribution, independence, and consistency in long-term behavior.\n\nAssessing a candidate's understanding of random number generation is crucial for a variety of reasons. Firstly, proficiency in random number generation allows individuals to accurately model and simulate complex systems and real-world scenarios. By generating random numbers, data scientists can simulate the behavior of a wide range of phenomena, such as population growth, financial market fluctuations, or the spread of diseases. This ability helps organizations make informed decisions, identify potential risks, and optimize their strategies. Secondly, random number generation plays a vital role in cryptography. Encrypting sensitive information requires the use of random numbers as cryptographic keys. Without a robust understanding of random number generation, the security of communications and data transmission can be compromised. Skills in random number generation ensure that organizations can implement strong encryption algorithms and safeguard their confidential information. Lastly, assessing a candidate's knowledge and abilities in random number generation is crucial for statistical analysis. Random numbers are essential in conducting unbiased experiments, sampling techniques, and hypothesis testing. An individual who possesses a solid grasp of random number generation can contribute to accurate statistical analyses, resulting in more reliable insights and conclusions. By assessing a candidate's understanding of random number generation, organizations can ensure they are selecting individuals who possess the necessary skills to make informed decisions, maintain data security, and conduct reliable statistical analyses. At Alooba, we provide a comprehensive assessment platform that enables organizations to evaluate and identify candidates who excel in this essential concept within the field of data science.\n\nRandom number generation encompasses several subtopics that are essential to understand. Here are some key areas covered within random number generation:\n• Pseudorandom Number Generators: Pseudorandom number generators (PRNGs) are algorithms that produce sequences of numbers that may appear random but are actually deterministic. These generators rely on mathematical formulas and initial values, known as seeds, to generate a sequence of numbers that exhibit properties similar to randomness. Understanding the mechanisms behind PRNGs is crucial for accurate and efficient random number generation.\n• Uniform Distribution: Random numbers should be uniformly distributed, meaning they are equally likely to occur within a specified range. Achieving uniformity ensures that each possible outcome has an equal chance of being selected. In random number generation, ensuring a uniform distribution of numbers is vital to maintain fairness and unbiased results.\n• Seeding and Reproducibility: Seeding is the process of initializing a random number generator with a starting value. By choosing a specific seed, it is possible to reproduce the same sequence of random numbers later. Seeding is important in scenarios where reproducibility and consistency of random number generation are required, such as in scientific experiments or debugging processes.\n• Randomness Testing: Randomness testing involves evaluating the statistical properties of generated random numbers to ensure their quality. Various tests and techniques can be used to assess the randomness of a sequence, such as the Chi-Square test, runs test, and spectral tests. These assessments help verify the effectiveness of the random number generator in producing numbers with desired qualities.\n• Applications in Cryptography: Random number generation is crucial for cryptographic applications, such as generating secure encryption keys, initialization vectors, or nonces. Randomness ensures the strength and unpredictability of cryptographic algorithms, protecting sensitive information and ensuring data security. Understanding these subtopics within random number generation provides a solid foundation for comprehending the intricacies and applications of this important concept in the field of data science and beyond.\n\nRandom number generation finds applications in various fields and industries due to its critical role in generating unpredictability and randomness. Here are some notable applications:\n• Simulation and Modeling: Random number generation is fundamental in computer simulations and modeling. By using random numbers, scientists and engineers can replicate real-world scenarios and explore different outcomes. Simulations help in studying complex systems, such as climate patterns, population dynamics, or the behavior of financial markets. Accurate random number generation enables reliable and realistic simulations, aiding in decision-making and predicting outcomes.\n• Statistical Analysis: Random numbers play a pivotal role in statistical analysis. Random sampling, hypothesis testing, and bootstrapping are statistical techniques that rely on random number generation. Sampling random data from a larger population allows for making inferences and drawing conclusions. Random numbers are also used to create randomization tests and assess the effects of different variables on statistical models. Reliable random number generation is thus vital in ensuring the validity and accuracy of statistical analyses.\n• Cryptography: Random number generation is significant in cryptography for encryption and secure communication. Cryptographic algorithms rely on unpredictable and random numbers to generate encryption keys, initialization vectors, or nonces. The use of strong random numbers ensures the resistance of cryptographic systems against attacks, protecting sensitive data during transmission or storage.\n• Gaming and Gambling: Random number generation is the backbone of fairness in gaming and gambling industries. Random numbers determine game outcomes, card shuffling, slot machine results, or other chance-based events. Reliable random number generation guarantees that players have an equal and unbiased chance of winning, ensuring integrity and fairness in the gaming and gambling experience.\n• Monte Carlo Simulations: Monte Carlo simulations use random numbers to simulate a range of possibilities for complex systems or problems. This technique is widely used in finance, engineering, and physics to estimate outcomes and assess risks. Random number generation enables the exploration of different scenarios and solutions, providing valuable insights into decision-making processes. Random number generation's applications extend across various disciplines, including simulations, statistical analysis, cryptography, gaming, and more. Understanding the significance and usage of random number generation can help organizations harness its power to make informed decisions, ensure fairness, and secure sensitive data.\n\nAnalytics Engineers are responsible for preparing data for analytical or operational uses. These professionals bridge the gap between data engineering and data analysis, ensuring data is not only available but also accessible, reliable, and well-organized. They typically work with data warehousing tools, ETL (Extract, Transform, Load) processes, and data modeling, often using SQL, Python, and various data visualization tools. Their role is crucial in enabling data-driven decision making across all functions of an organization. Artificial Intelligence Engineers are responsible for designing, developing, and deploying intelligent systems and solutions that leverage AI and machine learning technologies. They work across various domains such as healthcare, finance, and technology, employing algorithms, data modeling, and software engineering skills. Their role involves not only technical prowess but also collaboration with cross-functional teams to align AI solutions with business objectives. Familiarity with programming languages like Python, frameworks like TensorFlow or PyTorch, and cloud platforms is essential. Data Scientists are experts in statistical analysis and use their skills to interpret and extract meaning from data. They operate across various domains, including finance, healthcare, and technology, developing models to predict future trends, identify patterns, and provide actionable insights. Data Scientists typically have proficiency in programming languages like Python or R and are skilled in using machine learning techniques, statistical modeling, and data visualization tools such as Tableau or PowerBI. Demand Analysts specialize in predicting and analyzing market demand, using statistical and data analysis tools. They play a crucial role in supply chain management, aligning product availability with customer needs. This involves collaborating with sales, marketing, and production teams, and utilizing CRM and BI tools to inform strategic decisions. The Fraud Analyst role involves deep analysis of financial transactions and behaviors to identify and mitigate risks of fraud and financial crime. This position requires a blend of data analysis skills, expertise in fraud detection methodologies, and the ability to work with complex datasets. The role is critical in safeguarding against fraudulent activities and ensuring secure financial operations, making it suitable for those with a keen eye for detail and a strong analytical mindset. Machine Learning Engineers specialize in designing and implementing machine learning models to solve complex problems across various industries. They work on the full lifecycle of machine learning systems, from data gathering and preprocessing to model development, evaluation, and deployment. These engineers possess a strong foundation in AI/ML technology, software development, and data engineering. Their role often involves collaboration with data scientists, engineers, and product managers to integrate AI solutions into products and services. Master Data Analysts play a critical role in managing and maintaining the integrity of master data within an organization. They are responsible for ensuring that key data across business units, such as product, customer, and vendor information, is accurate, consistent, and up-to-date. Their expertise in data governance, data quality management, and data analysis is essential in supporting operational efficiency, compliance, and strategic initiatives. Master Data Analysts typically work with ERP systems like SAP or Oracle and are adept at collaborating with various departments to align data standards and policies. People Analysts utilize data analytics to drive insights into workforce management, employee engagement, and HR processes. They are adept in handling HR-specific datasets and tools, like Workday or SuccessFactors, to inform decision-making and improve employee experience. Their role encompasses designing and maintaining HR dashboards, conducting compensation analysis, and supporting strategic HR initiatives through data-driven solutions. Pricing Analysts play a crucial role in optimizing pricing strategies to balance profitability and market competitiveness. They analyze market trends, customer behaviors, and internal data to make informed pricing decisions. With skills in data analysis, statistical modeling, and business acumen, they collaborate across functions such as sales, marketing, and finance to develop pricing models that align with business objectives and customer needs. Reporting Analysts specialize in transforming data into actionable insights through detailed and customized reporting. They focus on the extraction, analysis, and presentation of data, using tools like Excel, SQL, and Power BI. These professionals work closely with cross-functional teams to understand business needs and optimize reporting. Their role is crucial in enhancing operational efficiency and decision-making across various domains. Visualization Analysts specialize in turning complex datasets into understandable, engaging, and informative visual representations. These professionals work across various functions such as marketing, sales, finance, and operations, utilizing tools like Tableau, Power BI, and D3.js. They are skilled in data manipulation, creating interactive dashboards, and presenting data in a way that supports decision-making and strategic planning. Their role is pivotal in making data accessible and actionable for both technical and non-technical audiences. Visualization Developers specialize in creating interactive, user-friendly visual representations of data using tools like Power BI and Tableau. They work closely with data analysts and business stakeholders to transform complex data sets into understandable and actionable insights. These professionals are adept in various coding and analytical languages like SQL, Python, and R, and they continuously adapt to emerging technologies and methodologies in data visualization.\n\nOver 200,000 Candidates Can't Be Wrong Overall, it was a truly excellent interview. The quality of the questions, and the overall flow of the conversation were impressive. Despite being aware of my shortcomings in certain areas, I am satisfied of this interview. The test is designed very well where it tests you different aspect of data comprehension. From data reading, data analysis, Excel formula, inference, and pattern recognition. The free response test is very interesting where it is simple enough to test your communication skill. One of the most professional assessments I have ever seen. it is strongly related to the job role and efficient for the talent acquisition team to know more about me. This was a great platform to give the exam and was pretty easy to use for me, even as a newbie to this platform. I like the way the Test is presented to me. Enough time is given to prepare for the Test. Also the questions are very clearly presented with enough time limit to answer it. Everything went very well - I liked the structure of this test and everything was relevant to the job This is a great test experience that I've not come across before. It has inspired me to brush up on my analytical skills whether or not I'd be offered this role. I'd like to thank the team for this setup and for the time and consideration. This is really different kind of experience through a interview process, where I like the journey and the motive of this screening process. That was definitely my first time ever being interviewed for skill assessment with the Alooba platform. Great experience and the value bestowed through such means is utterly respected on my behalf! I believe such online assessments should become more and more ubiquitous. Overall I am very happy with the way this test is structured, specially adding the video at the end is an unique experience where it showcases my personality to the recruitment team. I enjoyed taking this assessment, it was refreshing to undergo these kind of test to be able to navigate to the skills and knowledge to do the job. Frankly, I loved the entire experience, I learned my shortcoming, giving a test like this after a while. An we know, practise and practise will make the you perfect!! The assessment exam was interesting enough to test my sales and marketing knowledge. The website itself was amazing, and I liked it more than any LinkedIn or other assessment I took before. It shows how seriously you are taking this and made me enter the test mode without being stressed. It is very interesting way to take a test. I have not experienced such a pleasant test like this. Overall, I found the test platform to be very user-friendly and well-designed. It provided a smooth and efficient experience throughout the assessment. Thank you for the opportunity to take your assessment test. It was a great experience, it is the best test assesment I have taken so far. I like the way of getting into this new job i think its a very complete assessment i like it a lot! Thanks for the opportunity Overall I found the questions to be fair and appropriate and I believe you have an excellent system for testing and its the best one I have had to use in the last 18 months of my job search. thank you for your time. The test was conducted in all fairness and without any prejudice. It was very well set and the difficulty levels were well measured. I would like to take this opportunity to thank/congratulate the team for the methodology in conducting the test. I attended many online assessments which are kinda complicated where the questions makes no sense considering the job code but these questions makes sense and I can sense what kinda role that I should be doing if I'm selected. The questions are crisp and easy to understand. This was a very interesting round and definitely tests our business acumen. Would be excited to see what's ahead. Very great initiative taken my alooba, It's complete fair for all candidate to test their skill and it's help us to improve our performance. I'm excited to see the results. This was a very eye-opening assessment. I loved using alooba and the way the assessment was formatted. It was also great to see how some of the knowledge I gained from university can be applied to real-life scenarios and business problems. Overall, I found the test to be well-designed and comprehensive, effectively assessing the relevant skills and knowledge required for the position. The questions were thought-provoking and challenged me to think critically. A great experience overall, smooth platform, easy to use, challenging questions and very relevant to the role. Overall, it was a truly excellent interview. The quality of the questions, and the overall flow of the conversation were impressive. Despite being aware of my shortcomings in certain areas, I am satisfied of this interview. The test is designed very well where it tests you different aspect of data comprehension. From data reading, data analysis, Excel formula, inference, and pattern recognition. The free response test is very interesting where it is simple enough to test your communication skill. One of the most professional assessments I have ever seen. it is strongly related to the job role and efficient for the talent acquisition team to know more about me. This was a great platform to give the exam and was pretty easy to use for me, even as a newbie to this platform. I like the way the Test is presented to me. Enough time is given to prepare for the Test. Also the questions are very clearly presented with enough time limit to answer it. Everything went very well - I liked the structure of this test and everything was relevant to the job This is a great test experience that I've not come across before. It has inspired me to brush up on my analytical skills whether or not I'd be offered this role. I'd like to thank the team for this setup and for the time and consideration. This is really different kind of experience through a interview process, where I like the journey and the motive of this screening process. That was definitely my first time ever being interviewed for skill assessment with the Alooba platform. Great experience and the value bestowed through such means is utterly respected on my behalf! I believe such online assessments should become more and more ubiquitous. Overall I am very happy with the way this test is structured, specially adding the video at the end is an unique experience where it showcases my personality to the recruitment team. I enjoyed taking this assessment, it was refreshing to undergo these kind of test to be able to navigate to the skills and knowledge to do the job. Frankly, I loved the entire experience, I learned my shortcoming, giving a test like this after a while. An we know, practise and practise will make the you perfect!! The assessment exam was interesting enough to test my sales and marketing knowledge. The website itself was amazing, and I liked it more than any LinkedIn or other assessment I took before. It shows how seriously you are taking this and made me enter the test mode without being stressed. It is very interesting way to take a test. I have not experienced such a pleasant test like this. Overall, I found the test platform to be very user-friendly and well-designed. It provided a smooth and efficient experience throughout the assessment. Thank you for the opportunity to take your assessment test. It was a great experience, it is the best test assesment I have taken so far. I like the way of getting into this new job i think its a very complete assessment i like it a lot! Thanks for the opportunity Overall I found the questions to be fair and appropriate and I believe you have an excellent system for testing and its the best one I have had to use in the last 18 months of my job search. thank you for your time. The test was conducted in all fairness and without any prejudice. It was very well set and the difficulty levels were well measured. I would like to take this opportunity to thank/congratulate the team for the methodology in conducting the test. I attended many online assessments which are kinda complicated where the questions makes no sense considering the job code but these questions makes sense and I can sense what kinda role that I should be doing if I'm selected. The questions are crisp and easy to understand. This was a very interesting round and definitely tests our business acumen. Would be excited to see what's ahead. Very great initiative taken my alooba, It's complete fair for all candidate to test their skill and it's help us to improve our performance. I'm excited to see the results. This was a very eye-opening assessment. I loved using alooba and the way the assessment was formatted. It was also great to see how some of the knowledge I gained from university can be applied to real-life scenarios and business problems. Overall, I found the test to be well-designed and comprehensive, effectively assessing the relevant skills and knowledge required for the position. The questions were thought-provoking and challenged me to think critically. A great experience overall, smooth platform, easy to use, challenging questions and very relevant to the role. Overall, it was a truly excellent interview. The quality of the questions, and the overall flow of the conversation were impressive. Despite being aware of my shortcomings in certain areas, I am satisfied of this interview. The test is designed very well where it tests you different aspect of data comprehension. From data reading, data analysis, Excel formula, inference, and pattern recognition. The free response test is very interesting where it is simple enough to test your communication skill. One of the most professional assessments I have ever seen. it is strongly related to the job role and efficient for the talent acquisition team to know more about me. This was a great platform to give the exam and was pretty easy to use for me, even as a newbie to this platform."
    },
    {
        "link": "https://reddit.com/r/crypto/comments/9yadc3/whats_the_best_way_to_generate_random_numbers_can",
        "document": "Cryptography is the art of creating mathematical assurances for who can do what with data, including but not limited to encryption of messages such that only the key-holder can read it. Cryptography lives at an intersection of math and computer science. This is a technical subreddit covering the theory and practice of modern and *strong* cryptography."
    },
    {
        "link": "https://stackoverflow.com/questions/1986859/unbiased-random-number-generator-using-a-biased-one",
        "document": "The procedure to produce an unbiased coin from a biased one was first attributed to Von Neumann (a guy who has done enormous work in math and many related fields). The procedure is super simple:\n• If the results match, start over, forgetting both results.\n• If the results differ, use the first result, forgetting the second.\n\nThe reason this algorithm works is because the probability of getting HT is , which is the same as getting TH . Thus two events are equally likely.\n\nI am also reading this book and it asks the expected running time. The probability that two tosses are not equal is , therefore the expected running time is .\n\nThe previous example looks encouraging (after all, if you have a biased coin with a bias of , you will need to throw your coin approximately 50 times, which is not that many). So you might think that this is an optimal algorithm. Sadly it is not.\n\nHere is how it compares with the Shannon's theoretical bound (image is taken from this answer). It shows that the algorithm is good, but far from optimal.\n\nYou can come up with an improvement if you will consider that HHTT will be discarded by this algorithm, but in fact it has the same probability as TTHH. So you can also stop here and return H. The same is with HHHHTTTT and so on. Using these cases improves the expected running time, but are not making it theoretically optimal.\n\nAnd in the end - python code:\n\nIt is pretty self-explanatory, and here is an example result:\n\nAs you see, nonetheless we had a bias of , we got approximately the same number of and"
    },
    {
        "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC6379992",
        "document": "Randomness is far from a disturbing disorder in nature. Rather, it underlies many processes and functions. Randomness can be used to improve the efficacy of development and of systems under certain conditions. Moreover, valid unpredictable random-number generators are needed for secure communication, rendering predictable pseudorandom strings unsuitable. This paper reviews methods of generating randomness in various fields. The potential use of these methods is also discussed. It is suggested that by disordering a “false order,” an effective disorder can be generated to improve the function of systems.\n\nRandomness underlies many processes in nature. In terms of scientific investigations, randomness pertains to quantum mechanics, chemistry, and biological systems. Moreover, randomness can be used under some conditions to improve the function and efficacy of systems. For instance, noise can induce phenomena that cannot be understood from underlying deterministic models alone. Indeed, not all noise is similar, and each aspect of stochasticity can lead to new behavior [1]. Many systems, including communication systems, rely on random-number generators (RNGs) for encryption. True random number generators (TRNGs) are systems whose outputs cannot be determined, even if their internal structure and response history are known. By contrast, pseudorandom number generators (PRNGs) produce sequences of numbers that are completely predictable though not easily distinguished from sequences obtained by truly random methods. Security can be established only if an RNG satisfies two conditions. First, the user must know how numbers have been generated to verify the validity of a procedure. This is unrealistic because RNGs can deviate from their intended plan owing to imperfections, component ageing, failures, or explicit tampering. The second requirement is that the system must be a black box from an adversary’s perspective. However, RNGs violate Kerckhoffs’s principle insofar as “the enemy knows the system being used” [2]. Thus cryptographic systems should be designed under the assumption that adversaries are familiar with them [2]. Quantum random generators (QRNGs) can overcome some of these obstacles. In what follows, we review methods of generating randomness in various systems and their potential use.\n\nGenerating and using randomness in physics and algorithm development Random-number generators require high-quality sources of random numbers, yet effective methods are needed for assessing whether a source produces truly random sequences. The sole demonstrations of TRNGs have proceeded through thermal noise and/or quantum effects, and this approach is expensive and requires complex equipment. Current methods either do not rely on a formal description of randomness (e.g., the NIST test suite) or are inapplicable in principle, requiring testing of all possible computer programs that could produce the sequence. A method that behaves like a genuine QRNG and overcomes these difficulties based on Bayesian model selection was proposed [3]. Moreover, hardware TRNGs are used to create encryption keys, and offer advantages over software PRNGs. However, the majority of devices and sensors require small, low-cost, and mechanically flexible TRNGs with low computational complexity. These rigorous constraints position solution-processed semiconducting single-walled carbon nanotubes as candidates. A TRNG that uses static random access memory cells based on solution-processed carbon nanotubes to digitize thermal noise was used to generate random bits [4]. The thermodynamic costs of the three main approaches to generating random numbers via the recently introduced Information Processing Second Law were presented [5]. Given access to a specified source of randomness, the RNG produces samples from a desired target probability distribution. This differs from PRNGs, which use wholly deterministic algorithms, and from TRNGs in which the source of randomness is a physical system. The thermodynamics of generators enables direct bounds on the required physical resources, specifically on heat dissipation and work consumption during the operation of several classes of RNG methods. TRNGs can generate random numbers and convert thermal energy to stored work [5]. A self-powered TRNG was proposed that uses triboelectric technology to collect random signals from nature [6]. It is based on coupled triboelectric and electrostatic induction effects at a liquid-dielectric interface that includes an elaborately designed triboelectric generator with an irregular grating structure, an electronic–optical device, and an optical–electronic device. The generator has nonlinear input–output behavior, contributing to increased randomness. Random number sequences are deduced from electrical signals received by an optical-electronic device. Physical layer security uses the randomness of the wireless transmission channel for security. However, it is limited insofar as the main channel must be better than the eavesdropper channel. Cooperative communication was shown to ease this difficulty [7]. An actual-size concave grating with structural randomness was numerically analyzed. Numerical electromagnetic analyses that solve Maxwell’s equations provide diffracted fields and polarization characteristics. A difference-field boundary element method for analyzing actual-size concave gratings with 10,000 random blazed grooves was also proposed [8]. It offers vectorial diffracted or scattered waves with low computational resources. Relations between the degree of randomness and the diffraction efficiency were shown, including the polarization dependency. The path effect on a polished surface always matches tool motions, manifested as mid-spatial frequency errors in magnetorheological jet polishing processing. The Zernike extension and Neighbor–Gerchberg extension were used to design an extended surface with a weak edge effect in simulation [9]. Under the constraint of a pitch principle, a unicursal part-constrained path enhances the randomness of the tool path, including path turns and dwell-point positions. Linear canonical transforms (LCTs) are a family of integral transforms with wide applications in optical, acoustical, electromagnetic, and other wave propagation problems. The random discrete linear canonical transform (RDLCT) was designed by randomizing a kernel transform matrix of the discrete linear canonical transform (DLCT) [10]. It offers a greater degree of randomness because of the randomization of both eigenvectors and eigenvalues. The magnitude and phase of the RDLCT output are both random and can be used for image encryption. An entropy extraction mechanism based on sampling phase jitter in ring oscillators was designed to make high throughput TRNGs in a field programmable gate array [11]. A multi-phase sampling method was used to harvest the clock jitter with maximum entropy and fast sampling speeds. An evaluation of its randomness and its robustness to ambient temperature confirmed that this purely digital method provides high-speed high-quality random bit sequences. Ring oscillator-based physical unclonable functions protect the security of sensor nodes by generating random responses for a key extraction mechanism. They prevent non-volatile memory from storing secret keys. The hardware efficiency, randomness, uniqueness, and reliability of wireless sensor networks (WSNs) are also of relevance. A configurable device based on exclusive-or gates was used to increase hardware efficiency and mitigate vulnerability to EM analysis attacks, passing NIST’s statistical test [12]. A new type of elementary logic circuit, named random flip-flop, was also described [13]. Its action is maximally unpredictable and derives from a fundamentally random process of emission and detection of light quanta. It differs from conventional Boolean logic circuits whose action is deterministic and highly reproducible. Applications include randomness-preserving frequency division, random frequency synthesis, and RNGs. Most optical-chaos-based RNGs produce random bit sequences by offline post-processing with large optical components. A real-time hardware implementation of a fast physical RNG with a photonic integrated circuit and a field programmable gate array electronic board was presented [14]. An encryption key generated by two distant complex nonlinear units, forced into synchronization by a chaotic driver was also described [15]. The latter can be implemented on photonic, optoelectronic or electronic platforms, with reconfigurable key bitstream generation from chaotic signals. Although derived from a deterministic process, this bit series fulfills randomness conditions. A realistic protocol was proposed that amplifies the randomness of Santha-Vazirani sources and produces cryptographically secure random bits [16]. The protocol amplifies any such source that is not fully deterministic into a fully random source; it tolerates a constant noise rate and is robust to general no-signaling adversaries. A TRNG using the intrinsic variation of memristors as a natural source of entropy was presented [17]. Random bits were produced by cyclically switching a pair of tantalum oxide-based memristors and comparing their resistance values in the off state. Using an alternating read scheme in the designed TRNG circuit improved the bias of random numbers (i.e., making them less biased). A spatial network was modeled as a soft random geometric graph with two sources of randomness: nodes located randomly in space, and links formed independently between pairs of nodes with probability given by a specified function of their mutual distance. When randomness arises in the node positions and pairwise connections, for a given pair distance, the corresponding edge state is a random variable. The conditional entropy of an ensemble given the node location distribution for hard and probabilistic pair connection functions was studied. A connection function that generates maximum entropy was described [18]. Finally, a secured broadcast single-pixel imaging system using block-permutated Hadamard basis patterns for illumination was proposed [19]. The randomness in permutation operations facilitates system security, with satisfactory imaging quality and efficiency. This type of single-pixel imaging provides a solution to developing secured imaging systems for non-visible wavebands. Analytical expressions have been provided for the near- and far-field diffraction of random Ronchi diffraction gratings where the slits of the grating are randomly displaced around their periodical positions [20]. The effect of randomness in the position of the slits of the grating decreased the contrast and even disappearance of the self-images for high near-field randomness. Diffracted orders, inherent to lensless endoscopy using coherent beam-combining and aperiodic multicore fibers (MCFs) with periodically arranged cores, reduce the field-of-view (FoV). Randomness in MCF core positions increase the FoV to the diffraction limit set by a single fiber core, while maintaining the experimental feasibility of the MCF. This system is appropriate for beam scanning imaging by applying a tilt to the proximal wavefront [21]. Digitally simulating the intrinsic randomness of broadband light passing through a spiral phase plate was used to generate partially coherent vortex beams with an arbitrary azimuthal index using only a spatial light modulator [22]. Anderson localization has been observed in matter waves, optical waves, and acoustic waves, but its effect can also be induced in metallic nonlinear nanoparticle arrays excited by a random electrically driving field. The dipole-induced nonlinearity results in the ballistic expansion of dipole intensity during evolution. The randomness of the external driving field suppresses such an expansion. By increasing the strength of randomness above a threshold, a localized pattern of dipole intensity can be generated in metallic nanoparticle arrays. The generated Anderson localization is highly confined, with its size limited to the scale of the incident wavelength. The data facilitate the manipulations of electromagnetic fields in the scale of the wavelength [23]. A 2D mixed state for the polarization of light is represented by a combination of a pure state and a fully random state. A Mueller matrix is represented by a convex combination of a pure component and three additional components whose randomness is scaled objectively. Such decomposition characterizes the polarimetric randomness of a system represented by a given Mueller matrix and provides criteria for optimally filtering noise in experimental polarimetry [24]. An anti-velocity jamming strategy was proposed to enhance the ability of pulse-Doppler radar to detect moving targets in the presence of translational and/or micro-motion velocity jamming generated by digital radio frequency memory repeat jammers. The strategy uses random-pulse initial phase pulses as its transmitted signal and derives memory jammers that are not adaptable to the randomness of the initial phase of the transmitted pulses in the pulse repetition interval domain. An entropy-based multi-channel processing scheme was used to extract the information of the received signal without assuming that true and false targets should be included within one coherent processing interval [25]. Datasets in astronomics, genomics, internet search logs, sensor networks, and social network feeds are often employed. Generating such data is viewed as a sampling process from a so-called big source of at least a few gigabytes. Previous approaches to big sources rely on statistical assumptions about the samples. A method that extracts almost-uniform random bits from big sources was shown [26]. A method of generating chaotic maps with expected dynamics was proposed using the inherent relation between the Lyapunov exponents of a Cat map and its associated Cat matrix, constructing a -dimensional (-D) hyperchaotic Cat map with any desired number of positive Lyapunov exponents [27]. The model constructs a -D hyperchaotic Cat map with less computation complexity and outputs with demonstrably strong randomness. Generating random bits from uncertain events whose outcomes are routinely recorded in the form of massive datasets has been studied. For instance, a PRNG that computes the chaotic true orbits of a Bernoulli map on quadratic algebraic integers was proposed [28]. It offers a mode for selecting the initial points (or seeds) to generate multiple pseudorandom binary sequences. It distributes the initial points almost uniformly in the unit interval, guaranteeing that the latter parts of the generated sequences do not coincide. A new PRNG was generated using a chaotic map of a dynamic parameter-control chaotic system [29]. This model of 1-D chaotic maps has a simple structure that uses outputs of a chaotic map (control map) to dynamically control the parameters of another chaotic map (seed map). The model produces many new chaotic maps that are sensitive to their initial states, and have wider chaotic ranges, better unpredictability, and more complex chaotic behavior than their seed maps. A method of composing new orbits from a given chaotic map was presented [30]. It tests discrete-time chaotic maps in a “deep-zoom” manner using k-digits to the right of the decimal separator of a given point from the underlying chaotic map. Rapid randomization was observed, whereby chaotic patterns became indistinguishable from the original orbits of the underlying chaotic map. Using this randomization improvement, a PRNG based on the k-logistic map was proposed. Randomness is also used to improve modeling. Observer model performance was evaluated using data from GEANT4 Monte Carlo simulations for photons using custom models of plutonium inspection objects and a radiation imaging system [31]. The ideal observer was studied under signal-known-exactly conditions and in the presence of unknowns such as object orientation and absolute count-rate variability. When these additional sources of randomness were present, their incorporation into the observer yielded superior performance. Automatic Web-service selection is a research tool with which predictions of quality of service are possible based on historical service invocations [32]. As such, highly accurate predictions of missing quality-of-service data can be made by building an ensemble of non-negative latent factor models. These are diversified through feature sampling and randomness injection. Studies of randomized local binary features are used with methods such as Random Forests, Random Ferns, BRIEF, ORB, and AKAZE. With these methods, the randomness of operators reflects the sampling position. The quality of the binary feature space can be improved by increasing the randomness using a Randomized Intensity Difference (RID) operator to observe image patches. Compared to traditional incompletely randomized binary features (RIT features), randomized sampling generates a higher-quality binary feature space [33]. The phase diversity (PD) technique requires optimization algorithms to minimize the error metric and find the global minimum. Particle swarm optimization is suitable for PD due to its simple structure, fast convergence, and global searching ability. However, it suffers from a stagnation problem that can lead to incorrect solutions. To solve this problem, an inherent optimization mechanism was proposed. To improve the efficiency of this redistribution mechanism, randomized Halton sequences were introduced to ensure a uniform distribution and randomness of the redistributed particles in the search space [34]. Using a lightweight random partitioning scheme together with a carefully designed merging algorithm with results from random partitions overcomes the problem of scalable causal discovery used for biomedical studies and social network evolution [35].\n\nIn many cases, using standard methods to generate randomness involves concepts that are impractical. Quantum-based randomness provides a means of generating genuine randomness that is impossible with classical deterministic processes. The unpredictability of randomness can be certified in a manner that is independent of implementation devices [36]. Intrinsic randomness is central to device-independent quantum technologies [37]. Two principles underlie the strain between quantum physics and local realism. The first is locality. Observing a particle at one physical location cannot have immediate effects on the properties of a particle at a different location. Indeed, no effect can travel faster than the speed of light. The second is realism, which expresses how the observable features of particles and photon polarizations exist, even if we do not actively measure them. Local realism means that two distant objects have only limited correlations: events undergone by one object cannot be correlated to another beyond a certain degree. Bell formulated this limit between physical objects in mathematical inequalities [38]. However, in quantum mechanics, correlations between distant particles exist, violating local realism. Events between quantum particles are indeed correlated, wherever they are in the universe. The hypothesis is that unknown physical parameters exist, such that the constraint imposed by inequalities would be correct all the same. It is thus possible to have two correlated particles that are distant from each other. By measuring the first we can learn something about the second without observing it directly [38]. Bell offered a way to tackle the threat to local realism posed by quantum mechanics: by studying quantum correlations in the form of entanglement [38]. Bell described local realism with a statistical limit such that if the results of an experiment violate Bell’s inequality, the local hidden-variable (LHV) model is not explanatory [39]. The Bell test examines whether or not the real world satisfies local realism, which requires the presence of some which are not a feature of quantum theory, to explain the behavior of particles like photons and electrons. If nature functions in accordance with any theory of LHVs, then the results of the test will be constrained in a particular, quantifiable way [39]. The Bell test assumes that no signal travels faster than light, and that it requires spatially distributed entanglement, fast and efficient detection, and unpredictable measurement settings [40–43]. Bell defined the LHV model as a class of non-quantum theories that are simultaneously local and realist [40]. Studies on device-independent quantum information demonstrate that Bell inequality violations (BIVs) challenge causal determinism [44]. This is a necessary and sufficient condition for the quantum protocol to overcome classical protocols [45]. A BIV can only be explained within local realism when events across history conspire to produce measured outcomes [46, 47]. Free variables are thus used to select measurements [48]. If some processes are “free” in the required sense, then other processes are similarly free [49]. This conditional relation leaves open the freedom-of-choice loophole, which defines the option that hidden variables influence setting choices. Such freedom is uncertain within local realism, and tests must assume physical indeterminacy [42]. Bell tests confirm the validity of quantum theory, but they leave open the option of non-quantum explanations as to why local realism is violated. Thus, physicists have been looking for ways to close these loopholes [38]. Many types of Bell tests have been proposed in an effort to do so [42]. BIVs have been observed in experiments that showed a qualitative connection to randomness. However, most experiments that violate Bell inequalities are nevertheless affected by loopholes, and cannot be considered black-box demonstrations [2]. Quantum randomness is based on BIVs. Such randomness is device-independent. That is, it does rely on any particular device model [44, 50]. The entanglement properties of random quantum states and dynamics are important [51]. Quantum randomness is the result of context and quantization. This approach challenges reductionist methods that seek to preserve classical physical theories [52]. QRNGs harness the intrinsic randomness in measurement processes. Their measurement outputs are truly random, given that the input state is a superposition of the eigenstates of the measurement operators [53]. QRNGs are ideal due to their intrinsic uncertainty [54]. Adversaries have no knowledge of their internal mechanisms, even though they have a full description of it [2]. However, the generation of pseudorandomness is much harder in the quantum case. Random quantum unitary time evolutions, or circuits, are a potent source of quantum pseudorandomness. They can sometimes replace fully random operations. Generic quantum dynamics cannot be distinguished from truly random processes [55]. There are many methods of generating quantum randomness in which the final random bit sequences pass all the NIST-STS and DIEHARD tests. It follows from this that BIVs provide an experimental signature of randomness. BIVs can be verified by a user only from the statistics of the observed outputs of such processes. The verification procedure represents a black-box test of randomness [2, 56]. The limitation that any two photons must exchange signals at subluminal speeds was not enforced in the demonstrations of randomness generation based on Bell inequalities [4, 15]. An experiment was thus conducted with two photons in an entangled state such that their properties were strongly correlated. Each photon was sent to a different remote measurement station, where their polarizations were recorded. The photons were unable to interact given their distance, unless their signals travelled faster than the speed of light. Nonetheless, their measurement outcomes were correlated because of the photons’ entangled nature. The measurement outcomes were thus unpredictable, due to the strongly correlated behavior and distance of the photons. However, their randomness was small, even after millions of runs. A post-processing technique was used to generate truly random bits from these measurements, with minimal physical assumptions about the photons’ behavior [56]. Improved models were developed to explain the realization of such randomness [2, 56]. Over many runs, the sequence of measurement outcomes gathered enough uncertainty that truly random bits could be extracted. A method that weakened BIVs for generated random bits was developed as a secure QRNG [2]. Random Gaussian-free fermions satisfy the eigenstate thermalization hypothesis in the multiparticle sector, by analytically computing the correlations and entanglement entropies of the theory. The differences between fully random Hamiltonians and random Gaussian systems were described, providing a physically motivated notion of the randomness of a microscopic quantum state [57]. Electron transport through a nanoscale system is an inherently stochastic quantum mechanical process. Given that an electron has tunneled into an electronically unoccupied system from the source electrode at some particular time, the time it takes for it to tunnel out to the drain electrode is calculated [58]. Resonant tunneling diodes are thus used as practical true random number generators based on quantum mechanical effects [54]. A viable source of unbiased quantum random numbers was presented, the statistical properties of which can be arbitrarily programmed without the need for post-processing [59]. The method is based on measuring the arrival time of single photons in shaped temporal modes tailored with an electro-optical modulator. For a system of visual phototransduction, the process responsible for converting photons of light into usable electrical signals (or quantum bumps) requires randomness in both the photon inputs, regarded as extrinsic noise, and the conversion process, or intrinsic noise. Quantifying the relative effects of extrinsic and intrinsic noise has been studied. One such recent study in invertebrate phototransduction used minimum mean squared error reconstruction techniques based on Bayesian point process filters [60]. The algorithm estimates photon times from quantum bumps and uses Snyder filters to estimate random light intensities. The dominant noise source transitions from extrinsic to intrinsic as the light intensity increases with a delay that is critical insofar it can limit the speed at which invertebrates respond to stimuli. Furthermore, true randomness can be generated from a mixed state if a system entangled with that mixed state is well protected. RNG based on measuring the quadrature fluctuations of a single-mode thermal state using an optical homodyne detector was demonstrated [61]. By mixing the output of a broadband amplified spontaneous emission source with a single-mode local oscillator at a beam splitter and performing differential photo-detection, the quadrature fluctuation of a single-mode output of the amplified spontaneous emission source was perceived. The model tolerated much higher detector noise than QRNGs based on measuring vacuum noise [53]. An all-optical QRNG using a dual-pumped degenerate optical parametric oscillator in a silicon nitride microresonator was developed. Quantum entanglement in magnetic materials produces a quantum spin liquid, in which strong quantum fluctuations prevent magnetic ordering even at zero temperature. A quantum spin liquid state was described in a spin-1/2 honeycomb lattice with randomness in the exchange interaction. Randomness was introduced into the organic radial-based complex leading to a random-singlet state. The magnetic and thermodynamic data supported liquid-like behavior consistent with that expected in the random-singlet state [62]. Polarization is a fundamental property of light. A dynamically unpolarized single-photon emission from a single [111]-oriented nitrogen-vacancy center in diamond was shown [63]. In this system the single-photon stream is unpolarized, exhibiting intrinsic randomness with vanishing polarization correlation between time-adjacent photons. It thus allows for true RNG. A practical random bit generation method was proposed based on the detections of a coherent state in the few-photon regime by a gated single-photon threshold detector, operating at the telecom wavelength of 1550 nm [64]. The method was applied in a free-running single-photon detector for increased throughput by chopping the light signal instead of gating the detector. A self-testing QRNG from a prepare-and-measure scenario with independent devices was proposed [65]. The Han16 protocol doubles the generation rate of the quantum random number compared with previous protocols. The protocol tolerates loss and noise. Device-independent QRNG based on a detection-loophole-free Bell test with entangled photons was also described [36]. A model for QRNG based on a random population of the output spatial modes of a beam splitter was shown when both inputs are simultaneously fed with indistinguishable weak coherent states [66]. Generating random bits as a function of the average photon number per input was demonstrated. Interference reduced the probability of coincident counts between the detectors associated with bits 0 and 1, increasing the probability of a valid output. A QRNG was described by measuring the amplified spontaneous emission noise of superluminescent light-emitting diodes [67]. By detecting and amplifying spontaneous emission noise, randomness extraction was integrated in a field programmable gate array. A method to extract randomness and achieve an entropy source for an RNG was described [68]. Its photon statistics and the bunching of a semiconductor laser with external optical feedback were studied. In a chaotic regime, the photon number underwent a transition from a Bose–Einstein distribution to a Poisson distribution. The second-order degree of coherence decreased gradually from 2 to 1. Based on a Hanbury Brown–Twiss scheme, pronounced photon bunching was noted for various injection currents and feedback strengths, suggesting randomness of the associated emission light. A high-speed physical random bit generator at gigabits per second without a time-delay signature was shown based on chaotic power fluctuations of a random fiber laser [69]. It was configured by means of a ring structure with semiconductor optical amplifiers as the optical gain and a fiber random grating as the random feedback medium. Its rate and randomness were limited by laser relaxation oscillation and external-cavity resonance and can be improved by post-processing. A chaotic external-cavity semiconductor laser is an entropy source for generating high-speed physical random bits. Physical broadband white chaos generated by optical heterodyning of two lasers was described as an entropy source to construct high-speed random bit generation with minimal post-processing [70]. Following quantization with a multi-bit analog–digital convertor, random bits were obtained by extracting several least-significant bits. White chaos was produced with a high entropy rate by single-bit quantization. A real-time QRNG was designed by measuring laser phase fluctuations to generate ultra-high-speed random numbers [71]. The speed limit of a practical QRNG depends on the restricted speed of randomness extraction. A method for closing the gap between fast randomness generation and slow post-processing was thus proposed. A secure key distribution scheme based on the dynamic chaos synchronization of two external cavity vertical-cavity surface-emitting lasers subject to symmetric random-polarization injections was demonstrated [72]. By exchanging random parameters that control the polarization angles of the driving injection, Alice and Bob identified the time slots in which high-quality private chaos synchronization was achieved, and independently generated a shared key from the synchronized polarization difference signals of their local lasers. Randomness generated by an optically injected semiconductor laser in chaos was studied by state-space reconstruction [73]. Randomness was evaluated by the divergence of neighboring states, quantified by time-dependent exponents (TDEs). The mean TDE is observed to be positive as it increases over time through chaotic mixing. At constant laser noise strength, the mean TDE for chaos was greater than that for periodic dynamics, attributed to the effect of noise amplification by chaos. After discretization, the Shannon entropies generated by the laser for the output bits were estimated to provide a fundamental basis for random bit generation. An ultra-fast physical RNG utilizing a photonic integrated device-based broadband chaotic source with a simple post-data processing method was also described [74]. The compact chaotic source is implemented using a monolithic integrated dual-mode amplified feedback laser with self-injection, where a robust chaotic signal with RF frequency coverage is generated. A real-time scheme for ultrafast random number (RN) extraction from a broadband photonic entropy source was proposed [75]. Ultralow jitter mode-locked pulses were used to sample the stochastic intensity fluctuations of the entropy source in the optical domain. Discrete self-delay comparison technology was used to quantize the sampled pulses into continuous RN streams. The model is bias free, eliminating the electronic jitter bottleneck confronted by currently available physical RN generators, and it has no need for threshold tuning and post-processing. Two strings of quantum random numbers simultaneously generated from the intensity fluctuations of twin beams generated by a nondegenerate optical parametric oscillator were proposed [76]. These were extracted with a post-processing algorithm by post-selecting identical data from two raw sequences and using a hash function. Tests using physical randomness generators to choose measurement settings demonstrated a relationship between physical processes. If spontaneous emission is “free,” the outcomes of measurements on entangled electrons will also be free [49, 77]. One obstacle to manually done Bell tests is generating enough choices for statistically significance. A person can only generate three random bits per second, whereas a strong test requires millions of setting choices within minutes to hours. To achieve such rates, 100,000 human participants played an online video game that incentivized fast, sustained input of unpredictable selections and illustrated Bell-test methodology [78]. The game rewarded sustained, high-rate input of unpredictable bits. The participants’ choices were tested in various laboratories that verified local realism using photons [79, 80], single atoms [81], atomic ensembles [82] and superconducting devices [83]. The data confirmed the violation of Bell inequalities. Measurement-setting independence, provided by human agency, disagrees with causal determinism [44, 48]. The results thus closed the freedom-of-choice loophole—that setting choices are influenced by hidden variables to correlate with the properties of particles [84]. The human capacity for free choice eliminates the need for assumptions about physical indeterminism. Human choices show imperfect sequence randomness. Assuming no faster-than-light communication, such experiments prove that if human will is free, there are physical events that are intrinsically random, that is, impossible to predict [50].\n\nGenerating and using randomness in chemistry Randomness affects textural evolution. Plastically deformed metals are controlled by deterministic factors arising out of applied loads and by stochastic effects due to fluctuations of internal stress. Stochastic dislocation processes and inhomogeneous modes lead to randomness in the final deformation structure. Noise is involved in the analysis of a class of linear and nonlinear Wiener and Ornstein–Uhlenbeck processes. Linear Wiener processes are unaffected by the second time scale in the problem [85]. Silicon chips are vulnerable to counterfeiting, tampering and information leakage through side-channel attacks. However, an unclonable electronic random structure was constructed at low cost from carbon nanotubes [86]. This method uses two-dimensional random bit arrays to generate a ternary-bit architecture and a secure cryptographic key. A method for generating robust security primitives from layered transition metal dichalcogenides was described [87]. Physically unclonable primitives from layered molybdenum disulfide were designed by leveraging the natural randomness of their island growth during chemical vapor deposition. The distribution of islands on the film exhibits complete spatial randomness. The feasibility of embedding periodically arranged squares with a planar and vertical texture was demonstrated [88]. Using the natural randomness and uncontrollable variations of fingerprint textures, a polymer-stabilized graphic cholesteric liquid crystal symbol with a 2D barcode pattern was implemented with enhanced anti-counterfeiting features and improved security. Randomness was also important to the development of an electronic nose that distinguishes indoor pollutants [89]. The gas recognition rate was improved using an enhanced krill herd algorithm that relies on randomness to converge rapidly. Many attempts have been proposed to control crack formation, which compromises the strength and integrity of materials. A method to create modified films using electroplating on a pre-patterned substrate was used [90]. In thicker films, some randomness in the characteristic sizes of the fragments was introduced due to competition between crack propagation and crack creation. This method generated high-performance electrochromic structures. Surfactants provide an approach to building in randomness in generated magnetic behavior that can be manipulated via the formation of micelles and the design of a surfactant molecular architecture [91]. Activated carbon was synthesized with a chemical activation process [92]. Thermodynamic experiments suggested that the adsorption was spontaneous, endothermic, and increasingly random. Graphene oxide aerogels were used for adsorption of lead(II) ions from aqueous solutions [93]. The aerogels were fabricated from graphene oxide colloidal suspensions. Thermodynamic analysis demonstrated that its adsorption process was also spontaneous and endothermic with increased randomness at the solid–liquid interface. An activated carbon fiber/graphene oxide/polyethyleneimine composite was fabricated [94]. Its adsorption kinetics showed that the kinetic data fit with a pseudo-second-order kinetic model. Thermodynamic parameters showed that the adsorption process was spontaneous, endothermic and increasingly random. Similarly, a magnetic Schiff’s base chitosan composite was prepared whose sorption was endothermic, spontaneous, increasingly random [95]. Indeed, increased randomness was shown in several chemical reactions as a part of improving the function of the reactions. A chitosan-g-itaconic acid/bentonite and chitosan/bentonite nanocomposites were made for the adsorption of methylene blue from an aqueous solution [96]. The kinetic results indicated that the adsorption fitted with a pseudo-second-order kinetic model that suggested random adsorption at the interface. Polypyrrole wrapped oxidized multiwalled carbon nanotube nanocomposites were prepared via in situ chemical polymerization of pyrrole monomer in the presence of an oxidant [97]. The calculated values of the thermodynamic parameters showed that the adsorption process was spontaneous, endothermic, and marked with increased randomness at the solid–liquid interface. Solid waste from Jordanian olive oil processing was used to prepare biochar samples with increased randomness of the interface during the adsorption process [98]. Humic acid derived from rice straw was demonstrated to have Cu sorption that is endothermic, spontaneous, increasingly random [99]. The adsorption of chemical oxygen demand and biological oxygen demand from treated sewage with low-cost activated carbon was studied [100]. The results indicated that the adsorption was spontaneous, endothermic, and increasingly random. An activated carbon fiber modified by nitric acid was studied with absorption kinetics described by a pseudo-second-order model [101]. Again, the adsorption was shown to be spontaneous, endothermic, and increasingly random, as it was when studying the adsorption of copper ions onto chitosan films [102], a Gum xanthan-psyllium hybrid backbone graft co-polymerized with polyacrylic acid-co-polyitaconic acid chains [103], the removal of Chromium from an aqueous solution using sulfuric- and phosphoric-acid-activated Strychnine tree fruit shells as biosorbents [104], and the production of a polyvinyl alcohol–sodium alginate matrix embedded with red algae Jania rubens to remove lead from aqueous solutions [105].\n\nRandom sequences have also been explored in psychology [106, 107]. For instance, humans do poorly when asked to produce a random sequence [106]. Furthermore, our choices are biased because when generating random sequences humans tend to systematically under- and over-represent certain subsequences relative to the number expected from an unbiased random process. Indeed, our choices contain statistical regularities, yet they also deviate from a uniform distribution [41]. For this reason, Bell argued that human choices could be considered free variables insofar as human intention and will is free [39]. Thus, experimental settings derived from human intentions fulfill the assumptions of Bell’s theorem [41]. In addition, common misperceptions of randomness reflect genuine aspects of the statistical environment. When cognitive constraints are taken into account they impact how that environment is experienced [108]. When people consider a series of random binary events, such as flipping a coin, they tend to erroneously underrate the probability of sequences with less internal structure [109]. This is explained by a so-called representativeness heuristic in which we assume that the properties of long sequences should also apply to those of short sequences. Imposing structure on randomness in the environment is evident in the gambler’s fallacy—the mistaken belief that, for example, after flipping a coin and getting Heads many times the occurrence of Tails is more likely [110]. A recent study showed that humans are susceptible to the bias of attributing more randomness to sequences with more alternation (e.g., when Heads follows Tails, rather than repeating). This so-called over-alternation bias was tested to determine its presence in stimuli that vary across feature dimensions, sensory modalities, presentation modes, and probing methods [111]. It was shown that participants judged over-alternating stimuli as the most random. This bias was consistent across temporal and spatial presentation modes, color and shape, sensory modalities, speed, stimulus size, and probing methods. The results suggested that the subjective concept of randomness is highly stable across stimulus variations. The asymmetric measure of entropy has been suggested as an explanation for human biases and as a way to quantify subjective randomness [112]. A fitted asymmetric entropy measure was predictive when applied to different datasets of randomness-related tasks. Comparing human-generated sequences to unbiased process-generated binary sequences demonstrated that the constraints imposed on human experience provide a more meaningful picture of our ability to perceive randomness. A model of human random-sequence generation was thus proposed [108]. Binary sequences consist of a mixture of alternation and repetition. A study aimed to determine how people perceive such sequences and how they process alternation and repetition in binary sequences [113]. The data implied that, compared to repetition, alternation in a binary sequence is less noticeable. Human judgments were better explained by representativeness in the alternation rate than by objective probabilities. The data showed that participants were not sensitive to variation in the objective probabilities of a sub-sequence and used heuristics based on distinct forms of representativeness [109]. Research has also suggested that when generating random sequences, different participants adopt different cognitive strategies to suppress sequential dependencies [114]."
    },
    {
        "link": "https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-019-1798-2",
        "document": "Randomness underlies many processes in nature. In terms of scientific investigations, randomness pertains to quantum mechanics, chemistry, and biological systems. Moreover, randomness can be used under some conditions to improve the function and efficacy of systems. For instance, noise can induce phenomena that cannot be understood from underlying deterministic models alone. Indeed, not all noise is similar, and each aspect of stochasticity can lead to new behavior [1]. Many systems, including communication systems, rely on random-number generators (RNGs) for encryption. True random number generators (TRNGs) are systems whose outputs cannot be determined, even if their internal structure and response history are known. By contrast, pseudorandom number generators (PRNGs) produce sequences of numbers that are completely predictable though not easily distinguished from sequences obtained by truly random methods. Security can be established only if an RNG satisfies two conditions. First, the user must know how numbers have been generated to verify the validity of a procedure. This is unrealistic because RNGs can deviate from their intended plan owing to imperfections, component ageing, failures, or explicit tampering. The second requirement is that the system must be a black box from an adversary’s perspective. However, RNGs violate Kerckhoffs’s principle insofar as “the enemy knows the system being used” [2]. Thus cryptographic systems should be designed under the assumption that adversaries are familiar with them [2]. Quantum random generators (QRNGs) can overcome some of these obstacles. In what follows, we review methods of generating randomness in various systems and their potential use.\n\nGenerating and using randomness in physics and algorithm development\n\nRandom-number generators require high-quality sources of random numbers, yet effective methods are needed for assessing whether a source produces truly random sequences. The sole demonstrations of TRNGs have proceeded through thermal noise and/or quantum effects, and this approach is expensive and requires complex equipment. Current methods either do not rely on a formal description of randomness (e.g., the NIST test suite) or are inapplicable in principle, requiring testing of all possible computer programs that could produce the sequence. A method that behaves like a genuine QRNG and overcomes these difficulties based on Bayesian model selection was proposed [3]. Moreover, hardware TRNGs are used to create encryption keys, and offer advantages over software PRNGs. However, the majority of devices and sensors require small, low-cost, and mechanically flexible TRNGs with low computational complexity. These rigorous constraints position solution-processed semiconducting single-walled carbon nanotubes as candidates. A TRNG that uses static random access memory cells based on solution-processed carbon nanotubes to digitize thermal noise was used to generate random bits [4]. The thermodynamic costs of the three main approaches to generating random numbers via the recently introduced Information Processing Second Law were presented [5]. Given access to a specified source of randomness, the RNG produces samples from a desired target probability distribution. This differs from PRNGs, which use wholly deterministic algorithms, and from TRNGs in which the source of randomness is a physical system. The thermodynamics of generators enables direct bounds on the required physical resources, specifically on heat dissipation and work consumption during the operation of several classes of RNG methods. TRNGs can generate random numbers and convert thermal energy to stored work [5]. A self-powered TRNG was proposed that uses triboelectric technology to collect random signals from nature [6]. It is based on coupled triboelectric and electrostatic induction effects at a liquid-dielectric interface that includes an elaborately designed triboelectric generator with an irregular grating structure, an electronic–optical device, and an optical–electronic device. The generator has nonlinear input–output behavior, contributing to increased randomness. Random number sequences are deduced from electrical signals received by an optical-electronic device.\n\nPhysical layer security uses the randomness of the wireless transmission channel for security. However, it is limited insofar as the main channel must be better than the eavesdropper channel. Cooperative communication was shown to ease this difficulty [7]. An actual-size concave grating with structural randomness was numerically analyzed. Numerical electromagnetic analyses that solve Maxwell’s equations provide diffracted fields and polarization characteristics. A difference-field boundary element method for analyzing actual-size concave gratings with 10,000 random blazed grooves was also proposed [8]. It offers vectorial diffracted or scattered waves with low computational resources. Relations between the degree of randomness and the diffraction efficiency were shown, including the polarization dependency. The path effect on a polished surface always matches tool motions, manifested as mid-spatial frequency errors in magnetorheological jet polishing processing. The Zernike extension and Neighbor–Gerchberg extension were used to design an extended surface with a weak edge effect in simulation [9]. Under the constraint of a pitch principle, a unicursal part-constrained path enhances the randomness of the tool path, including path turns and dwell-point positions. Linear canonical transforms (LCTs) are a family of integral transforms with wide applications in optical, acoustical, electromagnetic, and other wave propagation problems. The random discrete linear canonical transform (RDLCT) was designed by randomizing a kernel transform matrix of the discrete linear canonical transform (DLCT) [10]. It offers a greater degree of randomness because of the randomization of both eigenvectors and eigenvalues. The magnitude and phase of the RDLCT output are both random and can be used for image encryption. An entropy extraction mechanism based on sampling phase jitter in ring oscillators was designed to make high throughput TRNGs in a field programmable gate array [11]. A multi-phase sampling method was used to harvest the clock jitter with maximum entropy and fast sampling speeds. An evaluation of its randomness and its robustness to ambient temperature confirmed that this purely digital method provides high-speed high-quality random bit sequences. Ring oscillator-based physical unclonable functions protect the security of sensor nodes by generating random responses for a key extraction mechanism. They prevent non-volatile memory from storing secret keys. The hardware efficiency, randomness, uniqueness, and reliability of wireless sensor networks (WSNs) are also of relevance. A configurable device based on exclusive-or gates was used to increase hardware efficiency and mitigate vulnerability to EM analysis attacks, passing NIST’s statistical test [12].\n\nA new type of elementary logic circuit, named random flip-flop, was also described [13]. Its action is maximally unpredictable and derives from a fundamentally random process of emission and detection of light quanta. It differs from conventional Boolean logic circuits whose action is deterministic and highly reproducible. Applications include randomness-preserving frequency division, random frequency synthesis, and RNGs. Most optical-chaos-based RNGs produce random bit sequences by offline post-processing with large optical components. A real-time hardware implementation of a fast physical RNG with a photonic integrated circuit and a field programmable gate array electronic board was presented [14]. An encryption key generated by two distant complex nonlinear units, forced into synchronization by a chaotic driver was also described [15]. The latter can be implemented on photonic, optoelectronic or electronic platforms, with reconfigurable key bitstream generation from chaotic signals. Although derived from a deterministic process, this bit series fulfills randomness conditions. A realistic protocol was proposed that amplifies the randomness of Santha-Vazirani sources and produces cryptographically secure random bits [16]. The protocol amplifies any such source that is not fully deterministic into a fully random source; it tolerates a constant noise rate and is robust to general no-signaling adversaries. A TRNG using the intrinsic variation of memristors as a natural source of entropy was presented [17]. Random bits were produced by cyclically switching a pair of tantalum oxide-based memristors and comparing their resistance values in the off state. Using an alternating read scheme in the designed TRNG circuit improved the bias of random numbers (i.e., making them less biased). A spatial network was modeled as a soft random geometric graph with two sources of randomness: nodes located randomly in space, and links formed independently between pairs of nodes with probability given by a specified function of their mutual distance. When randomness arises in the node positions and pairwise connections, for a given pair distance, the corresponding edge state is a random variable. The conditional entropy of an ensemble given the node location distribution for hard and probabilistic pair connection functions was studied. A connection function that generates maximum entropy was described [18]. Finally, a secured broadcast single-pixel imaging system using block-permutated Hadamard basis patterns for illumination was proposed [19]. The randomness in permutation operations facilitates system security, with satisfactory imaging quality and efficiency. This type of single-pixel imaging provides a solution to developing secured imaging systems for non-visible wavebands.\n\nAnalytical expressions have been provided for the near- and far-field diffraction of random Ronchi diffraction gratings where the slits of the grating are randomly displaced around their periodical positions [20]. The effect of randomness in the position of the slits of the grating decreased the contrast and even disappearance of the self-images for high near-field randomness. Diffracted orders, inherent to lensless endoscopy using coherent beam-combining and aperiodic multicore fibers (MCFs) with periodically arranged cores, reduce the field-of-view (FoV). Randomness in MCF core positions increase the FoV to the diffraction limit set by a single fiber core, while maintaining the experimental feasibility of the MCF. This system is appropriate for beam scanning imaging by applying a tilt to the proximal wavefront [21]. Digitally simulating the intrinsic randomness of broadband light passing through a spiral phase plate was used to generate partially coherent vortex beams with an arbitrary azimuthal index using only a spatial light modulator [22]. Anderson localization has been observed in matter waves, optical waves, and acoustic waves, but its effect can also be induced in metallic nonlinear nanoparticle arrays excited by a random electrically driving field. The dipole-induced nonlinearity results in the ballistic expansion of dipole intensity during evolution. The randomness of the external driving field suppresses such an expansion. By increasing the strength of randomness above a threshold, a localized pattern of dipole intensity can be generated in metallic nanoparticle arrays. The generated Anderson localization is highly confined, with its size limited to the scale of the incident wavelength. The data facilitate the manipulations of electromagnetic fields in the scale of the wavelength [23]. A 2D mixed state for the polarization of light is represented by a combination of a pure state and a fully random state. A Mueller matrix is represented by a convex combination of a pure component and three additional components whose randomness is scaled objectively. Such decomposition characterizes the polarimetric randomness of a system represented by a given Mueller matrix and provides criteria for optimally filtering noise in experimental polarimetry [24]. An anti-velocity jamming strategy was proposed to enhance the ability of pulse-Doppler radar to detect moving targets in the presence of translational and/or micro-motion velocity jamming generated by digital radio frequency memory repeat jammers. The strategy uses random-pulse initial phase pulses as its transmitted signal and derives memory jammers that are not adaptable to the randomness of the initial phase of the transmitted pulses in the pulse repetition interval domain. An entropy-based multi-channel processing scheme was used to extract the information of the received signal without assuming that true and false targets should be included within one coherent processing interval [25].\n\nDatasets in astronomics, genomics, internet search logs, sensor networks, and social network feeds are often employed. Generating such data is viewed as a sampling process from a so-called big source of at least a few gigabytes. Previous approaches to big sources rely on statistical assumptions about the samples. A method that extracts almost-uniform random bits from big sources was shown [26]. A method of generating chaotic maps with expected dynamics was proposed using the inherent relation between the Lyapunov exponents of a Cat map and its associated Cat matrix, constructing a -dimensional (-D) hyperchaotic Cat map with any desired number of positive Lyapunov exponents [27]. The model constructs a -D hyperchaotic Cat map with less computation complexity and outputs with demonstrably strong randomness. Generating random bits from uncertain events whose outcomes are routinely recorded in the form of massive datasets has been studied. For instance, a PRNG that computes the chaotic true orbits of a Bernoulli map on quadratic algebraic integers was proposed [28]. It offers a mode for selecting the initial points (or seeds) to generate multiple pseudorandom binary sequences. It distributes the initial points almost uniformly in the unit interval, guaranteeing that the latter parts of the generated sequences do not coincide. A new PRNG was generated using a chaotic map of a dynamic parameter-control chaotic system [29]. This model of 1-D chaotic maps has a simple structure that uses outputs of a chaotic map (control map) to dynamically control the parameters of another chaotic map (seed map). The model produces many new chaotic maps that are sensitive to their initial states, and have wider chaotic ranges, better unpredictability, and more complex chaotic behavior than their seed maps. A method of composing new orbits from a given chaotic map was presented [30]. It tests discrete-time chaotic maps in a “deep-zoom” manner using k-digits to the right of the decimal separator of a given point from the underlying chaotic map. Rapid randomization was observed, whereby chaotic patterns became indistinguishable from the original orbits of the underlying chaotic map. Using this randomization improvement, a PRNG based on the k-logistic map was proposed.\n\nRandomness is also used to improve modeling. Observer model performance was evaluated using data from GEANT4 Monte Carlo simulations for photons using custom models of plutonium inspection objects and a radiation imaging system [31]. The ideal observer was studied under signal-known-exactly conditions and in the presence of unknowns such as object orientation and absolute count-rate variability. When these additional sources of randomness were present, their incorporation into the observer yielded superior performance. Automatic Web-service selection is a research tool with which predictions of quality of service are possible based on historical service invocations [32]. As such, highly accurate predictions of missing quality-of-service data can be made by building an ensemble of non-negative latent factor models. These are diversified through feature sampling and randomness injection. Studies of randomized local binary features are used with methods such as Random Forests, Random Ferns, BRIEF, ORB, and AKAZE. With these methods, the randomness of operators reflects the sampling position. The quality of the binary feature space can be improved by increasing the randomness using a Randomized Intensity Difference (RID) operator to observe image patches. Compared to traditional incompletely randomized binary features (RIT features), randomized sampling generates a higher-quality binary feature space [33]. The phase diversity (PD) technique requires optimization algorithms to minimize the error metric and find the global minimum. Particle swarm optimization is suitable for PD due to its simple structure, fast convergence, and global searching ability. However, it suffers from a stagnation problem that can lead to incorrect solutions. To solve this problem, an inherent optimization mechanism was proposed. To improve the efficiency of this redistribution mechanism, randomized Halton sequences were introduced to ensure a uniform distribution and randomness of the redistributed particles in the search space [34]. Using a lightweight random partitioning scheme together with a carefully designed merging algorithm with results from random partitions overcomes the problem of scalable causal discovery used for biomedical studies and social network evolution [35].\n\nIn many cases, using standard methods to generate randomness involves concepts that are impractical. Quantum-based randomness provides a means of generating genuine randomness that is impossible with classical deterministic processes. The unpredictability of randomness can be certified in a manner that is independent of implementation devices [36]. Intrinsic randomness is central to device-independent quantum technologies [37]. Two principles underlie the strain between quantum physics and local realism. The first is locality. Observing a particle at one physical location cannot have immediate effects on the properties of a particle at a different location. Indeed, no effect can travel faster than the speed of light. The second is realism, which expresses how the observable features of particles and photon polarizations exist, even if we do not actively measure them. Local realism means that two distant objects have only limited correlations: events undergone by one object cannot be correlated to another beyond a certain degree. Bell formulated this limit between physical objects in mathematical inequalities [38]. However, in quantum mechanics, correlations between distant particles exist, violating local realism. Events between quantum particles are indeed correlated, wherever they are in the universe. The hypothesis is that unknown physical parameters exist, such that the constraint imposed by inequalities would be correct all the same. It is thus possible to have two correlated particles that are distant from each other. By measuring the first we can learn something about the second without observing it directly [38].\n\nBell offered a way to tackle the threat to local realism posed by quantum mechanics: by studying quantum correlations in the form of entanglement [38]. Bell described local realism with a statistical limit such that if the results of an experiment violate Bell’s inequality, the local hidden-variable (LHV) model is not explanatory [39]. The Bell test examines whether or not the real world satisfies local realism, which requires the presence of some which are not a feature of quantum theory, to explain the behavior of particles like photons and electrons. If nature functions in accordance with any theory of LHVs, then the results of the test will be constrained in a particular, quantifiable way [39]. The Bell test assumes that no signal travels faster than light, and that it requires spatially distributed entanglement, fast and efficient detection, and unpredictable measurement settings [40,41,42,43]. Bell defined the LHV model as a class of non-quantum theories that are simultaneously local and realist [40]. Studies on device-independent quantum information demonstrate that Bell inequality violations (BIVs) challenge causal determinism [44]. This is a necessary and sufficient condition for the quantum protocol to overcome classical protocols [45]. A BIV can only be explained within local realism when events across history conspire to produce measured outcomes [46, 47]. Free variables are thus used to select measurements [48]. If some processes are “free” in the required sense, then other processes are similarly free [49]. This conditional relation leaves open the freedom-of-choice loophole, which defines the option that hidden variables influence setting choices. Such freedom is uncertain within local realism, and tests must assume physical indeterminacy [42]. Bell tests confirm the validity of quantum theory, but they leave open the option of non-quantum explanations as to why local realism is violated. Thus, physicists have been looking for ways to close these loopholes [38]. Many types of Bell tests have been proposed in an effort to do so [42]. BIVs have been observed in experiments that showed a qualitative connection to randomness. However, most experiments that violate Bell inequalities are nevertheless affected by loopholes, and cannot be considered black-box demonstrations [2].\n\nQuantum randomness is based on BIVs. Such randomness is device-independent. That is, it does rely on any particular device model [44, 50]. The entanglement properties of random quantum states and dynamics are important [51]. Quantum randomness is the result of context and quantization. This approach challenges reductionist methods that seek to preserve classical physical theories [52]. QRNGs harness the intrinsic randomness in measurement processes. Their measurement outputs are truly random, given that the input state is a superposition of the eigenstates of the measurement operators [53]. QRNGs are ideal due to their intrinsic uncertainty [54]. Adversaries have no knowledge of their internal mechanisms, even though they have a full description of it [2]. However, the generation of pseudorandomness is much harder in the quantum case. Random quantum unitary time evolutions, or circuits, are a potent source of quantum pseudorandomness. They can sometimes replace fully random operations. Generic quantum dynamics cannot be distinguished from truly random processes [55]. There are many methods of generating quantum randomness in which the final random bit sequences pass all the NIST-STS and DIEHARD tests.\n\nIt follows from this that BIVs provide an experimental signature of randomness. BIVs can be verified by a user only from the statistics of the observed outputs of such processes. The verification procedure represents a black-box test of randomness [2, 56]. The limitation that any two photons must exchange signals at subluminal speeds was not enforced in the demonstrations of randomness generation based on Bell inequalities [4, 15]. An experiment was thus conducted with two photons in an entangled state such that their properties were strongly correlated. Each photon was sent to a different remote measurement station, where their polarizations were recorded. The photons were unable to interact given their distance, unless their signals travelled faster than the speed of light. Nonetheless, their measurement outcomes were correlated because of the photons’ entangled nature. The measurement outcomes were thus unpredictable, due to the strongly correlated behavior and distance of the photons. However, their randomness was small, even after millions of runs. A post-processing technique was used to generate truly random bits from these measurements, with minimal physical assumptions about the photons’ behavior [56]. Improved models were developed to explain the realization of such randomness [2, 56]. Over many runs, the sequence of measurement outcomes gathered enough uncertainty that truly random bits could be extracted. A method that weakened BIVs for generated random bits was developed as a secure QRNG [2].\n\nRandom Gaussian-free fermions satisfy the eigenstate thermalization hypothesis in the multiparticle sector, by analytically computing the correlations and entanglement entropies of the theory. The differences between fully random Hamiltonians and random Gaussian systems were described, providing a physically motivated notion of the randomness of a microscopic quantum state [57]. Electron transport through a nanoscale system is an inherently stochastic quantum mechanical process. Given that an electron has tunneled into an electronically unoccupied system from the source electrode at some particular time, the time it takes for it to tunnel out to the drain electrode is calculated [58]. Resonant tunneling diodes are thus used as practical true random number generators based on quantum mechanical effects [54]. A viable source of unbiased quantum random numbers was presented, the statistical properties of which can be arbitrarily programmed without the need for post-processing [59]. The method is based on measuring the arrival time of single photons in shaped temporal modes tailored with an electro-optical modulator. For a system of visual phototransduction, the process responsible for converting photons of light into usable electrical signals (or quantum bumps) requires randomness in both the photon inputs, regarded as extrinsic noise, and the conversion process, or intrinsic noise. Quantifying the relative effects of extrinsic and intrinsic noise has been studied. One such recent study in invertebrate phototransduction used minimum mean squared error reconstruction techniques based on Bayesian point process filters [60]. The algorithm estimates photon times from quantum bumps and uses Snyder filters to estimate random light intensities. The dominant noise source transitions from extrinsic to intrinsic as the light intensity increases with a delay that is critical insofar it can limit the speed at which invertebrates respond to stimuli.\n\nFurthermore, true randomness can be generated from a mixed state if a system entangled with that mixed state is well protected. RNG based on measuring the quadrature fluctuations of a single-mode thermal state using an optical homodyne detector was demonstrated [61]. By mixing the output of a broadband amplified spontaneous emission source with a single-mode local oscillator at a beam splitter and performing differential photo-detection, the quadrature fluctuation of a single-mode output of the amplified spontaneous emission source was perceived. The model tolerated much higher detector noise than QRNGs based on measuring vacuum noise [53]. An all-optical QRNG using a dual-pumped degenerate optical parametric oscillator in a silicon nitride microresonator was developed. Quantum entanglement in magnetic materials produces a quantum spin liquid, in which strong quantum fluctuations prevent magnetic ordering even at zero temperature. A quantum spin liquid state was described in a spin-1/2 honeycomb lattice with randomness in the exchange interaction. Randomness was introduced into the organic radial-based complex leading to a random-singlet state. The magnetic and thermodynamic data supported liquid-like behavior consistent with that expected in the random-singlet state [62]. Polarization is a fundamental property of light. A dynamically unpolarized single-photon emission from a single [111]-oriented nitrogen-vacancy center in diamond was shown [63]. In this system the single-photon stream is unpolarized, exhibiting intrinsic randomness with vanishing polarization correlation between time-adjacent photons. It thus allows for true RNG. A practical random bit generation method was proposed based on the detections of a coherent state in the few-photon regime by a gated single-photon threshold detector, operating at the telecom wavelength of 1550 nm [64]. The method was applied in a free-running single-photon detector for increased throughput by chopping the light signal instead of gating the detector. A self-testing QRNG from a prepare-and-measure scenario with independent devices was proposed [65]. The Han16 protocol doubles the generation rate of the quantum random number compared with previous protocols. The protocol tolerates loss and noise. Device-independent QRNG based on a detection-loophole-free Bell test with entangled photons was also described [36].\n\nA model for QRNG based on a random population of the output spatial modes of a beam splitter was shown when both inputs are simultaneously fed with indistinguishable weak coherent states [66]. Generating random bits as a function of the average photon number per input was demonstrated. Interference reduced the probability of coincident counts between the detectors associated with bits 0 and 1, increasing the probability of a valid output. A QRNG was described by measuring the amplified spontaneous emission noise of superluminescent light-emitting diodes [67]. By detecting and amplifying spontaneous emission noise, randomness extraction was integrated in a field programmable gate array. A method to extract randomness and achieve an entropy source for an RNG was described [68]. Its photon statistics and the bunching of a semiconductor laser with external optical feedback were studied. In a chaotic regime, the photon number underwent a transition from a Bose–Einstein distribution to a Poisson distribution. The second-order degree of coherence decreased gradually from 2 to 1. Based on a Hanbury Brown–Twiss scheme, pronounced photon bunching was noted for various injection currents and feedback strengths, suggesting randomness of the associated emission light. A high-speed physical random bit generator at gigabits per second without a time-delay signature was shown based on chaotic power fluctuations of a random fiber laser [69]. It was configured by means of a ring structure with semiconductor optical amplifiers as the optical gain and a fiber random grating as the random feedback medium. Its rate and randomness were limited by laser relaxation oscillation and external-cavity resonance and can be improved by post-processing. A chaotic external-cavity semiconductor laser is an entropy source for generating high-speed physical random bits. Physical broadband white chaos generated by optical heterodyning of two lasers was described as an entropy source to construct high-speed random bit generation with minimal post-processing [70]. Following quantization with a multi-bit analog–digital convertor, random bits were obtained by extracting several least-significant bits. White chaos was produced with a high entropy rate by single-bit quantization. A real-time QRNG was designed by measuring laser phase fluctuations to generate ultra-high-speed random numbers [71]. The speed limit of a practical QRNG depends on the restricted speed of randomness extraction. A method for closing the gap between fast randomness generation and slow post-processing was thus proposed. A secure key distribution scheme based on the dynamic chaos synchronization of two external cavity vertical-cavity surface-emitting lasers subject to symmetric random-polarization injections was demonstrated [72]. By exchanging random parameters that control the polarization angles of the driving injection, Alice and Bob identified the time slots in which high-quality private chaos synchronization was achieved, and independently generated a shared key from the synchronized polarization difference signals of their local lasers. Randomness generated by an optically injected semiconductor laser in chaos was studied by state-space reconstruction [73]. Randomness was evaluated by the divergence of neighboring states, quantified by time-dependent exponents (TDEs). The mean TDE is observed to be positive as it increases over time through chaotic mixing. At constant laser noise strength, the mean TDE for chaos was greater than that for periodic dynamics, attributed to the effect of noise amplification by chaos. After discretization, the Shannon entropies generated by the laser for the output bits were estimated to provide a fundamental basis for random bit generation. An ultra-fast physical RNG utilizing a photonic integrated device-based broadband chaotic source with a simple post-data processing method was also described [74]. The compact chaotic source is implemented using a monolithic integrated dual-mode amplified feedback laser with self-injection, where a robust chaotic signal with RF frequency coverage is generated. A real-time scheme for ultrafast random number (RN) extraction from a broadband photonic entropy source was proposed [75]. Ultralow jitter mode-locked pulses were used to sample the stochastic intensity fluctuations of the entropy source in the optical domain. Discrete self-delay comparison technology was used to quantize the sampled pulses into continuous RN streams. The model is bias free, eliminating the electronic jitter bottleneck confronted by currently available physical RN generators, and it has no need for threshold tuning and post-processing. Two strings of quantum random numbers simultaneously generated from the intensity fluctuations of twin beams generated by a nondegenerate optical parametric oscillator were proposed [76]. These were extracted with a post-processing algorithm by post-selecting identical data from two raw sequences and using a hash function.\n\nTests using physical randomness generators to choose measurement settings demonstrated a relationship between physical processes. If spontaneous emission is “free,” the outcomes of measurements on entangled electrons will also be free [49, 77]. One obstacle to manually done Bell tests is generating enough choices for statistically significance. A person can only generate three random bits per second, whereas a strong test requires millions of setting choices within minutes to hours. To achieve such rates, 100,000 human participants played an online video game that incentivized fast, sustained input of unpredictable selections and illustrated Bell-test methodology [78]. The game rewarded sustained, high-rate input of unpredictable bits. The participants’ choices were tested in various laboratories that verified local realism using photons [79, 80], single atoms [81], atomic ensembles [82] and superconducting devices [83]. The data confirmed the violation of Bell inequalities. Measurement-setting independence, provided by human agency, disagrees with causal determinism [44, 48]. The results thus closed the freedom-of-choice loophole—that setting choices are influenced by hidden variables to correlate with the properties of particles [84]. The human capacity for free choice eliminates the need for assumptions about physical indeterminism. Human choices show imperfect sequence randomness. Assuming no faster-than-light communication, such experiments prove that if human will is free, there are physical events that are intrinsically random, that is, impossible to predict [50].\n\nGenerating and using randomness in chemistry\n\nRandomness affects textural evolution. Plastically deformed metals are controlled by deterministic factors arising out of applied loads and by stochastic effects due to fluctuations of internal stress. Stochastic dislocation processes and inhomogeneous modes lead to randomness in the final deformation structure. Noise is involved in the analysis of a class of linear and nonlinear Wiener and Ornstein–Uhlenbeck processes. Linear Wiener processes are unaffected by the second time scale in the problem [85]. Silicon chips are vulnerable to counterfeiting, tampering and information leakage through side-channel attacks. However, an unclonable electronic random structure was constructed at low cost from carbon nanotubes [86]. This method uses two-dimensional random bit arrays to generate a ternary-bit architecture and a secure cryptographic key. A method for generating robust security primitives from layered transition metal dichalcogenides was described [87]. Physically unclonable primitives from layered molybdenum disulfide were designed by leveraging the natural randomness of their island growth during chemical vapor deposition. The distribution of islands on the film exhibits complete spatial randomness. The feasibility of embedding periodically arranged squares with a planar and vertical texture was demonstrated [88]. Using the natural randomness and uncontrollable variations of fingerprint textures, a polymer-stabilized graphic cholesteric liquid crystal symbol with a 2D barcode pattern was implemented with enhanced anti-counterfeiting features and improved security. Randomness was also important to the development of an electronic nose that distinguishes indoor pollutants [89]. The gas recognition rate was improved using an enhanced krill herd algorithm that relies on randomness to converge rapidly. Many attempts have been proposed to control crack formation, which compromises the strength and integrity of materials. A method to create modified films using electroplating on a pre-patterned substrate was used [90]. In thicker films, some randomness in the characteristic sizes of the fragments was introduced due to competition between crack propagation and crack creation. This method generated high-performance electrochromic structures. Surfactants provide an approach to building in randomness in generated magnetic behavior that can be manipulated via the formation of micelles and the design of a surfactant molecular architecture [91].\n\nActivated carbon was synthesized with a chemical activation process [92]. Thermodynamic experiments suggested that the adsorption was spontaneous, endothermic, and increasingly random. Graphene oxide aerogels were used for adsorption of lead(II) ions from aqueous solutions [93]. The aerogels were fabricated from graphene oxide colloidal suspensions. Thermodynamic analysis demonstrated that its adsorption process was also spontaneous and endothermic with increased randomness at the solid–liquid interface. An activated carbon fiber/graphene oxide/polyethyleneimine composite was fabricated [94]. Its adsorption kinetics showed that the kinetic data fit with a pseudo-second-order kinetic model. Thermodynamic parameters showed that the adsorption process was spontaneous, endothermic and increasingly random. Similarly, a magnetic Schiff’s base chitosan composite was prepared whose sorption was endothermic, spontaneous, increasingly random [95]. Indeed, increased randomness was shown in several chemical reactions as a part of improving the function of the reactions. A chitosan-g-itaconic acid/bentonite and chitosan/bentonite nanocomposites were made for the adsorption of methylene blue from an aqueous solution [96]. The kinetic results indicated that the adsorption fitted with a pseudo-second-order kinetic model that suggested random adsorption at the interface. Polypyrrole wrapped oxidized multiwalled carbon nanotube nanocomposites were prepared via in situ chemical polymerization of pyrrole monomer in the presence of an oxidant [97]. The calculated values of the thermodynamic parameters showed that the adsorption process was spontaneous, endothermic, and marked with increased randomness at the solid–liquid interface. Solid waste from Jordanian olive oil processing was used to prepare biochar samples with increased randomness of the interface during the adsorption process [98]. Humic acid derived from rice straw was demonstrated to have Cu sorption that is endothermic, spontaneous, increasingly random [99]. The adsorption of chemical oxygen demand and biological oxygen demand from treated sewage with low-cost activated carbon was studied [100]. The results indicated that the adsorption was spontaneous, endothermic, and increasingly random. An activated carbon fiber modified by nitric acid was studied with absorption kinetics described by a pseudo-second-order model [101]. Again, the adsorption was shown to be spontaneous, endothermic, and increasingly random, as it was when studying the adsorption of copper ions onto chitosan films [102], a Gum xanthan-psyllium hybrid backbone graft co-polymerized with polyacrylic acid-co-polyitaconic acid chains [103], the removal of Chromium from an aqueous solution using sulfuric- and phosphoric-acid-activated Strychnine tree fruit shells as biosorbents [104], and the production of a polyvinyl alcohol–sodium alginate matrix embedded with red algae Jania rubens to remove lead from aqueous solutions [105].\n\nRandom sequences have also been explored in psychology [106, 107]. For instance, humans do poorly when asked to produce a random sequence [106]. Furthermore, our choices are biased because when generating random sequences humans tend to systematically under- and over-represent certain subsequences relative to the number expected from an unbiased random process. Indeed, our choices contain statistical regularities, yet they also deviate from a uniform distribution [41]. For this reason, Bell argued that human choices could be considered free variables insofar as human intention and will is free [39]. Thus, experimental settings derived from human intentions fulfill the assumptions of Bell’s theorem [41]. In addition, common misperceptions of randomness reflect genuine aspects of the statistical environment. When cognitive constraints are taken into account they impact how that environment is experienced [108]. When people consider a series of random binary events, such as flipping a coin, they tend to erroneously underrate the probability of sequences with less internal structure [109]. This is explained by a so-called representativeness heuristic in which we assume that the properties of long sequences should also apply to those of short sequences. Imposing structure on randomness in the environment is evident in the gambler’s fallacy—the mistaken belief that, for example, after flipping a coin and getting Heads many times the occurrence of Tails is more likely [110]. A recent study showed that humans are susceptible to the bias of attributing more randomness to sequences with more alternation (e.g., when Heads follows Tails, rather than repeating). This so-called over-alternation bias was tested to determine its presence in stimuli that vary across feature dimensions, sensory modalities, presentation modes, and probing methods [111]. It was shown that participants judged over-alternating stimuli as the most random. This bias was consistent across temporal and spatial presentation modes, color and shape, sensory modalities, speed, stimulus size, and probing methods. The results suggested that the subjective concept of randomness is highly stable across stimulus variations.\n\nThe asymmetric measure of entropy has been suggested as an explanation for human biases and as a way to quantify subjective randomness [112]. A fitted asymmetric entropy measure was predictive when applied to different datasets of randomness-related tasks. Comparing human-generated sequences to unbiased process-generated binary sequences demonstrated that the constraints imposed on human experience provide a more meaningful picture of our ability to perceive randomness. A model of human random-sequence generation was thus proposed [108]. Binary sequences consist of a mixture of alternation and repetition. A study aimed to determine how people perceive such sequences and how they process alternation and repetition in binary sequences [113]. The data implied that, compared to repetition, alternation in a binary sequence is less noticeable. Human judgments were better explained by representativeness in the alternation rate than by objective probabilities. The data showed that participants were not sensitive to variation in the objective probabilities of a sub-sequence and used heuristics based on distinct forms of representativeness [109]. Research has also suggested that when generating random sequences, different participants adopt different cognitive strategies to suppress sequential dependencies [114].\n\nPatients with drug-resistant epilepsy are particularly challenging to treat. Further, they often have poor prognoses in terms of seizure control, along with higher morbidity and mortality [115]. Drug resistance is a risk factor in status epilepticus and sudden death in epilepsy [116]. As the duration of the disease increases, there is a risk of drug resistance and polypharmacy. Worse, second-generation antiepileptic drugs provide no additional effect for poor responders to first-generation drugs [117]. Studies have proposed using randomness in such cases to partially overcome tolerance to therapies and to improve the efficacy of therapies [118,119,120].\n\nIn summary, randomness underlies many processes in various fields, and cannot be viewed as a disturbing disorder in nature. Randomness can indeed improve the efficacy of systems in physics, chemistry, biology, and psychology. Developing methods to better generate and understand randomness facilitates its use in these various fields."
    }
]