[
    {
        "link": "https://docs.scrapy.org",
        "document": "Scrapy is a fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing.\n\nUnderstand what Scrapy is and how it can help you. Get Scrapy installed on your computer. Learn more by playing with a pre-made Scrapy project.\n\nSee what has changed in recent Scrapy versions. Learn how to contribute to the Scrapy project."
    },
    {
        "link": "https://scrapfly.io/blog/web-scraping-with-scrapy",
        "document": "How to Find All URLs on a Domain\n\nLearn how to efficiently find all URLs on a domain using Python and web crawling. Guide on how to crawl entire domain to collect all website data"
    },
    {
        "link": "https://docs.scrapy.org/en/latest/intro/tutorial.html",
        "document": "In this tutorial, we’ll assume that Scrapy is already installed on your system. If that’s not the case, see Installation guide.\n\nWe are going to scrape quotes.toscrape.com, a website that lists quotes from famous authors.\n\nThis tutorial will walk you through these tasks:\n• None Exporting the scraped data using the command line\n\nScrapy is written in Python. The more you learn about Python, the more you can get out of Scrapy.\n\nIf you’re already familiar with other languages and want to learn Python quickly, the Python Tutorial is a good resource.\n\nIf you’re new to programming and want to start with Python, the following books may be useful to you:\n• None How To Think Like a Computer Scientist\n\nYou can also take a look at this list of Python resources for non-programmers, as well as the suggested resources in the learnpython-subreddit.\n\nSpiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass and define the initial requests to be made, and optionally, how to follow links in pages and parse the downloaded page content to extract data. This is the code for our first Spider. Save it in a file named under the directory in your project: As you can see, our Spider subclasses and defines some attributes and methods:\n• None : identifies the Spider. It must be unique within a project, that is, you can’t set the same name for different Spiders.\n• None : must return an iterable of Requests (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.\n• None : a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of that holds the page content and has further helpful methods to handle it. The method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests ( ) from them. How to run our spider¶ To put our spider to work, go to the project’s top level directory and run: This command runs the spider named that we’ve just added, that will send some requests for the domain. You will get an output similar to this: Now, check the files in the current directory. You should notice that two new files have been created: quotes-1.html and quotes-2.html, with the content for the respective URLs, as our method instructs. If you are wondering why we haven’t parsed the HTML yet, hold on, we will cover that soon. What just happened under the hood?¶ Scrapy schedules the objects returned by the method of the Spider. Upon receiving a response for each one, it instantiates objects and calls the callback method associated with the request (in this case, the method) passing the response as an argument. Instead of implementing a method that generates objects from URLs, you can just define a class attribute with a list of URLs. This list will then be used by the default implementation of to create the initial requests for your spider. The method will be called to handle each of the requests for those URLs, even though we haven’t explicitly told Scrapy to do so. This happens because is Scrapy’s default callback method, which is called for requests without an explicitly assigned callback. The best way to learn how to extract data with Scrapy is trying selectors using the Scrapy shell. Run: Remember to always enclose URLs in quotes when running Scrapy shell from the command line, otherwise URLs containing arguments (i.e. character) will not work. On Windows, use double quotes instead: You will see something like: Using the shell, you can try selecting elements using CSS with the response object: The result of running is a list-like object called , which represents a list of objects that wrap around XML/HTML elements and allow you to run further queries to refine the selection or extract the data. To extract the text from the title above, you can do: There are two things to note here: one is that we’ve added to the CSS query, to mean we want to select only the text elements directly inside element. If we don’t specify , we’d get the full title element, including its tags: The other thing is that the result of calling is a list: it is possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do: As an alternative, you could’ve written: Accessing an index on a instance will raise an exception if there are no results: You might want to use directly on the instance instead, which returns if there are no results: There’s a lesson here: for most scraping code, you want it to be resilient to errors due to things not being found on a page, so that even if some parts fail to be scraped, you can at least get some data. Besides the and methods, you can also use the method to extract using regular expressions: In order to find the proper CSS selectors to use, you might find it useful to open the response page from the shell in your web browser using . You can use your browser’s developer tools to inspect the HTML and come up with a selector (see Using your browser’s Developer Tools for scraping). Selector Gadget is also a nice tool to quickly find CSS selector for visually selected elements, which works in many browsers. Besides CSS, Scrapy selectors also support using XPath expressions: XPath expressions are very powerful, and are the foundation of Scrapy Selectors. In fact, CSS selectors are converted to XPath under-the-hood. You can see that if you read the text representation of the selector objects in the shell closely. While perhaps not as popular as CSS selectors, XPath expressions offer more power because besides navigating the structure, it can also look at the content. Using XPath, you’re able to select things like: the link that contains the text “Next Page”. This makes XPath very fitting to the task of scraping, and we encourage you to learn XPath even if you already know how to construct CSS selectors, it will make scraping much easier. We won’t cover much of XPath here, but you can read more about using XPath with Scrapy Selectors here. To learn more about XPath, we recommend this tutorial to learn XPath through examples, and this tutorial to learn “how to think in XPath”. Now that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page. Each quote in https://quotes.toscrape.com is represented by HTML elements that look like this: “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.” by Albert Einstein (about) Tags: change deep-thoughts thinking world Let’s open up scrapy shell and play a bit to find out how to extract the data we want: We get a list of selectors for the quote HTML elements with: Each of the selectors returned by the query above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote: Now, let’s extract the , and from that quote using the object we just created: '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”' Given that the tags are a list of strings, we can use the method to get all of them: Having figured out how to extract each bit, we can now iterate over all the quote elements and put them together into a Python dictionary: {'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']} {'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']} Let’s get back to our spider. Until now, it hasn’t extracted any data in particular, just saving the whole HTML page to a local file. Let’s integrate the extraction logic above into our spider. A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the Python keyword in the callback, as you can see below: To run this spider, exit the scrapy shell by entering: Now, it should output the extracted data with the log: '“It is better to be hated for what you are than to be loved for what you are not.”' \"“I have not failed. I've just found 10,000 ways that won't work.”\"\n\nThe simplest way to store the scraped data is by using Feed exports, with the following command: That will generate a file containing all scraped items, serialized in JSON. The command-line switch overwrites any existing file; use instead to append new content to any existing file. However, appending to a JSON file makes the file contents invalid JSON. When appending to a file, consider using a different serialization format, such as JSON Lines: The JSON Lines format is useful because it’s stream-like, so you can easily append new records to it. It doesn’t have the same problem as JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like JQ to help do that at the command-line. In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an Item Pipeline. A placeholder file for Item Pipelines has been set up for you when the project is created, in . Though you don’t need to implement any item pipelines if you just want to store the scraped items.\n\nLet’s say, instead of just scraping the stuff from the first two pages from https://quotes.toscrape.com, you want quotes from all the pages in the website. Now that you know how to extract data from pages, let’s see how to follow links from them. The first thing to do is extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup: We can try extracting it in the shell: This gets the anchor element, but we want the attribute . For that, Scrapy supports a CSS extension that lets you select the attribute contents, like this: There is also an property available (see Selecting element attributes for more): Now let’s see our spider, modified to recursively follow the link to the next page, extracting data from it: Now, after extracting the data, the method looks for the link to the next page, builds a full absolute URL using the method (since the links can be relative) and yields a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages. What you see here is Scrapy’s mechanism of following links: when you yield a Request in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes. Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it’s visiting. In our example, it creates a sort of loop, following all the links to the next page until it doesn’t find one – handy for crawling blogs, forums and other sites with pagination. As a shortcut for creating Request objects you can use : Unlike scrapy.Request, supports relative URLs directly - no need to call urljoin. Note that just returns a Request instance; you still have to yield this Request. You can also pass a selector to instead of a string; this selector should extract necessary attributes: For elements there is a shortcut: uses their href attribute automatically. So the code can be shortened further: To create multiple requests from an iterable, you can use instead: or, shortening it further: Here is another spider that illustrates callbacks and following links, this time for scraping author information: This spider will start from the main page, it will follow all the links to the authors pages calling the callback for each of them, and also the pagination links with the callback as we saw before. Here we’re passing callbacks to as positional arguments to make the code shorter; it also works for . The callback defines a helper function to extract and cleanup the data from a CSS query and yields the Python dict with the author data. Another interesting thing this spider demonstrates is that, even if there are many quotes from the same author, we don’t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured in the setting. Hopefully by now you have a good understanding of how to use the mechanism of following links and callbacks with Scrapy. As yet another example spider that leverages the mechanism of following links, check out the class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it. Also, a common pattern is to build an item with data from more than one page, using a trick to pass additional data to the callbacks."
    },
    {
        "link": "https://zenrows.com/blog/scrapy-python",
        "document": ""
    },
    {
        "link": "https://medium.com/@datajournal/web-scraping-with-scrapy-5560a26b6e26",
        "document": "In this tutorial, we’ll walk you through how to get started with Scrapy for web scraping and build your first scraping project.\n\nScrapy is fast, efficient, and highly customizable. It’s especially useful for large-scale scraping projects where you must crawl hundreds or thousands of pages. The framework is built for performance, handling HTTP requests and parsing responses concurrently.\n\nIf you are looking for Scrapy alternatives, I can recommend 3 of the top web scraping providers in the industry (I am not affiliated with any of them, don’t worry):\n• Concurrency and asynchronous I/O: Efficient handling of multiple requests at once.\n• Support for XPath and CSS selectors: Powerful ways to navigate through HTML and extract data.\n• Robust API: Allows you to define how data is handled and stored.\n• Install Scrapy: Use pip install scrapy to get started. Make sure you have Python 3.6+ installed.\n\nTo begin using Scrapy, you’ll need to install it. The simplest way to do This is to use pip, Python’s package manager.\n\nOnce installed, verify the installation by typing the following command:\n\nIf Scrapy is installed correctly, this command will return the Scrapy version number.\n\nScrapy operates around the concept of projects. To create your first project, navigate to the directory where you want your project to reside and run:\n\nThis will create a folder with the name myproject, containing all the essential files to get started.\n\nAfter creating the project, you’ll notice a folder structure like this:\n• items.py: Defines the structure of the data you want to scrape.\n• middlewares.py: Allows you to modify requests and responses.\n• spiders/: Contains the spider code where all scraping logic will reside.\n\nA spider is a class in Scrapy that defines how a particular website or a group of websites should be scraped.\n\nTo create a spider, navigate to the spiders directory and create a new Python file. Let’s call it quotes_spider.py to scrape data from the famous quotes.toscrape.com website, which is great for beginners.\n• name: This is the spider’s name. Scrapy uses this name to identify which spider to run.\n• start_urls: This is a list of URLs from which the spider will start scraping.\n• parse(): This method is where the extraction logic resides. It defines how the content of the page is processed. Here, we’re using CSS selectors (response.css) to extract the text, author, and tags of quotes.\n• Pagination: After processing the current page, the spider looks for the next page’s URL and follows it to scrape the next set of quotes.\n\nTo run your spider, simply use the following command:\n\nScrapy will visit the starting URL, extract the data, follow links to the next pages, and scrape additional quotes.\n\nScrapy makes it easy to export the scraped data. You can export data in JSON, CSV, or XML formats. To export data to a JSON file, use this command:\n\nThis command will save the scraped data into quotes.json. Similarly, you can export it in CSV by changing the extension.\n\nScrapy’s behavior can be configured through the settings.py file. Here are some important settings you may want to adjust:\n• USER_AGENT: Some websites block requests that don’t have a user agent. You can set your spider’s user agent in the settings.\n• CONCURRENT_REQUESTS: This setting defines how many requests Scrapy should make concurrently.\n• DOWNLOAD_DELAY: You can use a delay between requests to avoid overwhelming the server.\n\nMany modern websites use JavaScript to load content dynamically, and Scrapy by itself cannot execute JavaScript. In such cases, you can use Scrapy-Splash or integrate Scrapy with a headless browser like Selenium.\n\nSplash is a headless browser designed for web scraping. To use it, you need to install it and then integrate it with Scrapy.\n\nYou also need to update the settings.py file to include:\n\nBy integrating Splash, Scrapy can handle dynamic content better and fetch data from JavaScript-based websites.\n\nOnce you’ve scraped your data, you’ll need a way to store or process it. This is where pipelines come into play.\n\nLet’s say you want to store the scraped data in a MongoDB database. First, install the pymongo library:\n\nNow, all your scraped data will be stored in a MongoDB collection.\n\nWeb scraping with Scrapy is a powerful way to extract data from websites efficiently. From setting up Scrapy, creating spiders, handling dynamic content, and storing the data in a database, Scrapy offers flexibility for beginners and experienced developers.\n\nMastering Scrapy can automate your data collection processes, giving you the tools to gather valuable insights and drive industry decision-making.\n\nNow that you understand how Scrapy works build your next web scraping project!"
    },
    {
        "link": "https://docs.scrapy.org/en/latest/intro/tutorial.html",
        "document": "In this tutorial, we’ll assume that Scrapy is already installed on your system. If that’s not the case, see Installation guide.\n\nWe are going to scrape quotes.toscrape.com, a website that lists quotes from famous authors.\n\nThis tutorial will walk you through these tasks:\n• None Exporting the scraped data using the command line\n\nScrapy is written in Python. The more you learn about Python, the more you can get out of Scrapy.\n\nIf you’re already familiar with other languages and want to learn Python quickly, the Python Tutorial is a good resource.\n\nIf you’re new to programming and want to start with Python, the following books may be useful to you:\n• None How To Think Like a Computer Scientist\n\nYou can also take a look at this list of Python resources for non-programmers, as well as the suggested resources in the learnpython-subreddit.\n\nSpiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass and define the initial requests to be made, and optionally, how to follow links in pages and parse the downloaded page content to extract data. This is the code for our first Spider. Save it in a file named under the directory in your project: As you can see, our Spider subclasses and defines some attributes and methods:\n• None : identifies the Spider. It must be unique within a project, that is, you can’t set the same name for different Spiders.\n• None : must return an iterable of Requests (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.\n• None : a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of that holds the page content and has further helpful methods to handle it. The method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests ( ) from them. How to run our spider¶ To put our spider to work, go to the project’s top level directory and run: This command runs the spider named that we’ve just added, that will send some requests for the domain. You will get an output similar to this: Now, check the files in the current directory. You should notice that two new files have been created: quotes-1.html and quotes-2.html, with the content for the respective URLs, as our method instructs. If you are wondering why we haven’t parsed the HTML yet, hold on, we will cover that soon. What just happened under the hood?¶ Scrapy schedules the objects returned by the method of the Spider. Upon receiving a response for each one, it instantiates objects and calls the callback method associated with the request (in this case, the method) passing the response as an argument. Instead of implementing a method that generates objects from URLs, you can just define a class attribute with a list of URLs. This list will then be used by the default implementation of to create the initial requests for your spider. The method will be called to handle each of the requests for those URLs, even though we haven’t explicitly told Scrapy to do so. This happens because is Scrapy’s default callback method, which is called for requests without an explicitly assigned callback. The best way to learn how to extract data with Scrapy is trying selectors using the Scrapy shell. Run: Remember to always enclose URLs in quotes when running Scrapy shell from the command line, otherwise URLs containing arguments (i.e. character) will not work. On Windows, use double quotes instead: You will see something like: Using the shell, you can try selecting elements using CSS with the response object: The result of running is a list-like object called , which represents a list of objects that wrap around XML/HTML elements and allow you to run further queries to refine the selection or extract the data. To extract the text from the title above, you can do: There are two things to note here: one is that we’ve added to the CSS query, to mean we want to select only the text elements directly inside element. If we don’t specify , we’d get the full title element, including its tags: The other thing is that the result of calling is a list: it is possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do: As an alternative, you could’ve written: Accessing an index on a instance will raise an exception if there are no results: You might want to use directly on the instance instead, which returns if there are no results: There’s a lesson here: for most scraping code, you want it to be resilient to errors due to things not being found on a page, so that even if some parts fail to be scraped, you can at least get some data. Besides the and methods, you can also use the method to extract using regular expressions: In order to find the proper CSS selectors to use, you might find it useful to open the response page from the shell in your web browser using . You can use your browser’s developer tools to inspect the HTML and come up with a selector (see Using your browser’s Developer Tools for scraping). Selector Gadget is also a nice tool to quickly find CSS selector for visually selected elements, which works in many browsers. Besides CSS, Scrapy selectors also support using XPath expressions: XPath expressions are very powerful, and are the foundation of Scrapy Selectors. In fact, CSS selectors are converted to XPath under-the-hood. You can see that if you read the text representation of the selector objects in the shell closely. While perhaps not as popular as CSS selectors, XPath expressions offer more power because besides navigating the structure, it can also look at the content. Using XPath, you’re able to select things like: the link that contains the text “Next Page”. This makes XPath very fitting to the task of scraping, and we encourage you to learn XPath even if you already know how to construct CSS selectors, it will make scraping much easier. We won’t cover much of XPath here, but you can read more about using XPath with Scrapy Selectors here. To learn more about XPath, we recommend this tutorial to learn XPath through examples, and this tutorial to learn “how to think in XPath”. Now that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page. Each quote in https://quotes.toscrape.com is represented by HTML elements that look like this: “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.” by Albert Einstein (about) Tags: change deep-thoughts thinking world Let’s open up scrapy shell and play a bit to find out how to extract the data we want: We get a list of selectors for the quote HTML elements with: Each of the selectors returned by the query above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote: Now, let’s extract the , and from that quote using the object we just created: '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”' Given that the tags are a list of strings, we can use the method to get all of them: Having figured out how to extract each bit, we can now iterate over all the quote elements and put them together into a Python dictionary: {'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']} {'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']} Let’s get back to our spider. Until now, it hasn’t extracted any data in particular, just saving the whole HTML page to a local file. Let’s integrate the extraction logic above into our spider. A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the Python keyword in the callback, as you can see below: To run this spider, exit the scrapy shell by entering: Now, it should output the extracted data with the log: '“It is better to be hated for what you are than to be loved for what you are not.”' \"“I have not failed. I've just found 10,000 ways that won't work.”\"\n\nThe simplest way to store the scraped data is by using Feed exports, with the following command: That will generate a file containing all scraped items, serialized in JSON. The command-line switch overwrites any existing file; use instead to append new content to any existing file. However, appending to a JSON file makes the file contents invalid JSON. When appending to a file, consider using a different serialization format, such as JSON Lines: The JSON Lines format is useful because it’s stream-like, so you can easily append new records to it. It doesn’t have the same problem as JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like JQ to help do that at the command-line. In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an Item Pipeline. A placeholder file for Item Pipelines has been set up for you when the project is created, in . Though you don’t need to implement any item pipelines if you just want to store the scraped items.\n\nLet’s say, instead of just scraping the stuff from the first two pages from https://quotes.toscrape.com, you want quotes from all the pages in the website. Now that you know how to extract data from pages, let’s see how to follow links from them. The first thing to do is extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup: We can try extracting it in the shell: This gets the anchor element, but we want the attribute . For that, Scrapy supports a CSS extension that lets you select the attribute contents, like this: There is also an property available (see Selecting element attributes for more): Now let’s see our spider, modified to recursively follow the link to the next page, extracting data from it: Now, after extracting the data, the method looks for the link to the next page, builds a full absolute URL using the method (since the links can be relative) and yields a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages. What you see here is Scrapy’s mechanism of following links: when you yield a Request in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes. Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it’s visiting. In our example, it creates a sort of loop, following all the links to the next page until it doesn’t find one – handy for crawling blogs, forums and other sites with pagination. As a shortcut for creating Request objects you can use : Unlike scrapy.Request, supports relative URLs directly - no need to call urljoin. Note that just returns a Request instance; you still have to yield this Request. You can also pass a selector to instead of a string; this selector should extract necessary attributes: For elements there is a shortcut: uses their href attribute automatically. So the code can be shortened further: To create multiple requests from an iterable, you can use instead: or, shortening it further: Here is another spider that illustrates callbacks and following links, this time for scraping author information: This spider will start from the main page, it will follow all the links to the authors pages calling the callback for each of them, and also the pagination links with the callback as we saw before. Here we’re passing callbacks to as positional arguments to make the code shorter; it also works for . The callback defines a helper function to extract and cleanup the data from a CSS query and yields the Python dict with the author data. Another interesting thing this spider demonstrates is that, even if there are many quotes from the same author, we don’t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured in the setting. Hopefully by now you have a good understanding of how to use the mechanism of following links and callbacks with Scrapy. As yet another example spider that leverages the mechanism of following links, check out the class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it. Also, a common pattern is to build an item with data from more than one page, using a trick to pass additional data to the callbacks."
    },
    {
        "link": "https://docs.scrapy.org/en/2.11/intro/tutorial.html",
        "document": "In this tutorial, we’ll assume that Scrapy is already installed on your system. If that’s not the case, see Installation guide.\n\nWe are going to scrape quotes.toscrape.com, a website that lists quotes from famous authors.\n\nThis tutorial will walk you through these tasks:\n• None Exporting the scraped data using the command line\n\nScrapy is written in Python. If you’re new to the language you might want to start by getting an idea of what the language is like, to get the most out of Scrapy.\n\nIf you’re already familiar with other languages, and want to learn Python quickly, the Python Tutorial is a good resource.\n\nIf you’re new to programming and want to start with Python, the following books may be useful to you:\n• None How To Think Like a Computer Scientist\n\nYou can also take a look at this list of Python resources for non-programmers, as well as the suggested resources in the learnpython-subreddit.\n\nSpiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass and define the initial requests to make, optionally how to follow links in the pages, and how to parse the downloaded page content to extract data. This is the code for our first Spider. Save it in a file named under the directory in your project: As you can see, our Spider subclasses and defines some attributes and methods:\n• None : identifies the Spider. It must be unique within a project, that is, you can’t set the same name for different Spiders.\n• None : must return an iterable of Requests (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.\n• None : a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of that holds the page content and has further helpful methods to handle it. The method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests ( ) from them. How to run our spider¶ To put our spider to work, go to the project’s top level directory and run: This command runs the spider with name that we’ve just added, that will send some requests for the domain. You will get an output similar to this: Now, check the files in the current directory. You should notice that two new files have been created: quotes-1.html and quotes-2.html, with the content for the respective URLs, as our method instructs. If you are wondering why we haven’t parsed the HTML yet, hold on, we will cover that soon. What just happened under the hood?¶ Scrapy schedules the objects returned by the method of the Spider. Upon receiving a response for each one, it instantiates objects and calls the callback method associated with the request (in this case, the method) passing the response as argument. Instead of implementing a method that generates objects from URLs, you can just define a class attribute with a list of URLs. This list will then be used by the default implementation of to create the initial requests for your spider. The method will be called to handle each of the requests for those URLs, even though we haven’t explicitly told Scrapy to do so. This happens because is Scrapy’s default callback method, which is called for requests without an explicitly assigned callback. The best way to learn how to extract data with Scrapy is trying selectors using the Scrapy shell. Run: Remember to always enclose urls in quotes when running Scrapy shell from command-line, otherwise urls containing arguments (i.e. character) will not work. On Windows, use double quotes instead: You will see something like: Using the shell, you can try selecting elements using CSS with the response object: The result of running is a list-like object called , which represents a list of objects that wrap around XML/HTML elements and allow you to run further queries to fine-grain the selection or extract the data. To extract the text from the title above, you can do: There are two things to note here: one is that we’ve added to the CSS query, to mean we want to select only the text elements directly inside element. If we don’t specify , we’d get the full title element, including its tags: The other thing is that the result of calling is a list: it is possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do: As an alternative, you could’ve written: Accessing an index on a instance will raise an exception if there are no results: You might want to use directly on the instance instead, which returns if there are no results: There’s a lesson here: for most scraping code, you want it to be resilient to errors due to things not being found on a page, so that even if some parts fail to be scraped, you can at least get some data. Besides the and methods, you can also use the method to extract using regular expressions: In order to find the proper CSS selectors to use, you might find it useful to open the response page from the shell in your web browser using . You can use your browser’s developer tools to inspect the HTML and come up with a selector (see Using your browser’s Developer Tools for scraping). Selector Gadget is also a nice tool to quickly find CSS selector for visually selected elements, which works in many browsers. Besides CSS, Scrapy selectors also support using XPath expressions: XPath expressions are very powerful, and are the foundation of Scrapy Selectors. In fact, CSS selectors are converted to XPath under-the-hood. You can see that if you read closely the text representation of the selector objects in the shell. While perhaps not as popular as CSS selectors, XPath expressions offer more power because besides navigating the structure, it can also look at the content. Using XPath, you’re able to select things like: select the link that contains the text “Next Page”. This makes XPath very fitting to the task of scraping, and we encourage you to learn XPath even if you already know how to construct CSS selectors, it will make scraping much easier. We won’t cover much of XPath here, but you can read more about using XPath with Scrapy Selectors here. To learn more about XPath, we recommend this tutorial to learn XPath through examples, and this tutorial to learn “how to think in XPath”. Now that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page. Each quote in https://quotes.toscrape.com is represented by HTML elements that look like this: “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.” by Albert Einstein (about) Tags: change deep-thoughts thinking world Let’s open up scrapy shell and play a bit to find out how to extract the data we want: We get a list of selectors for the quote HTML elements with: Each of the selectors returned by the query above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote: Now, let’s extract , and the from that quote using the object we just created: '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”' Given that the tags are a list of strings, we can use the method to get all of them: Having figured out how to extract each bit, we can now iterate over all the quotes elements and put them together into a Python dictionary: {'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']} {'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']} Let’s get back to our spider. Until now, it doesn’t extract any data in particular, just saves the whole HTML page to a local file. Let’s integrate the extraction logic above into our spider. A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the Python keyword in the callback, as you can see below: To run this spider, exit the scrapy shell by entering: Now, it should output the extracted data with the log: '“It is better to be hated for what you are than to be loved for what you are not.”' \"“I have not failed. I've just found 10,000 ways that won't work.”\"\n\nThe simplest way to store the scraped data is by using Feed exports, with the following command: That will generate a file containing all scraped items, serialized in JSON. The command-line switch overwrites any existing file; use instead to append new content to any existing file. However, appending to a JSON file makes the file contents invalid JSON. When appending to a file, consider using a different serialization format, such as JSON Lines: The JSON Lines format is useful because it’s stream-like, you can easily append new records to it. It doesn’t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like JQ to help do that at the command-line. In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an Item Pipeline. A placeholder file for Item Pipelines has been set up for you when the project is created, in . Though you don’t need to implement any item pipelines if you just want to store the scraped items.\n\nLet’s say, instead of just scraping the stuff from the first two pages from https://quotes.toscrape.com, you want quotes from all the pages in the website. Now that you know how to extract data from pages, let’s see how to follow links from them. First thing is to extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup: We can try extracting it in the shell: This gets the anchor element, but we want the attribute . For that, Scrapy supports a CSS extension that lets you select the attribute contents, like this: There is also an property available (see Selecting element attributes for more): Let’s see now our spider modified to recursively follow the link to the next page, extracting data from it: Now, after extracting the data, the method looks for the link to the next page, builds a full absolute URL using the method (since the links can be relative) and yields a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages. What you see here is Scrapy’s mechanism of following links: when you yield a Request in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes. Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it’s visiting. In our example, it creates a sort of loop, following all the links to the next page until it doesn’t find one – handy for crawling blogs, forums and other sites with pagination. As a shortcut for creating Request objects you can use : Unlike scrapy.Request, supports relative URLs directly - no need to call urljoin. Note that just returns a Request instance; you still have to yield this Request. You can also pass a selector to instead of a string; this selector should extract necessary attributes: For elements there is a shortcut: uses their href attribute automatically. So the code can be shortened further: To create multiple requests from an iterable, you can use instead: or, shortening it further: Here is another spider that illustrates callbacks and following links, this time for scraping author information: This spider will start from the main page, it will follow all the links to the authors pages calling the callback for each of them, and also the pagination links with the callback as we saw before. Here we’re passing callbacks to as positional arguments to make the code shorter; it also works for . The callback defines a helper function to extract and cleanup the data from a CSS query and yields the Python dict with the author data. Another interesting thing this spider demonstrates is that, even if there are many quotes from the same author, we don’t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured by the setting . Hopefully by now you have a good understanding of how to use the mechanism of following links and callbacks with Scrapy. As yet another example spider that leverages the mechanism of following links, check out the class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it. Also, a common pattern is to build an item with data from more than one page, using a trick to pass additional data to the callbacks."
    },
    {
        "link": "https://zenrows.com/blog/scrapy-python",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/13437402/how-to-run-scrapy-from-within-a-python-script",
        "document": "I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:\n\nI can't figure out where I should put my spider code and how to call it from the main function. Please help. This is the example code:"
    },
    {
        "link": "https://analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy",
        "document": "The internet has become an expansive resource of data, providing numerous opportunities for data science enthusiasts. Web scraping using Scrapy, a powerful Python-based open-source web crawling framework, has become essential for extracting valuable insights from this vast amount of unstructured data. This article explores the fundamentals of web scraping using Scrapy Python, providing examples and case studies to demonstrate its capabilities. You will learn how to scrape data from various sources, including Reddit and e-commerce sites, and gain practical experience in handling common challenges in web scraping.\n• Understand the fundamentals of web scraping using Scrapy Python, a powerful open-source web crawling framework.\n• Learn how to set up and configure Scrapy for extracting data from websites.\n• Explore various Scrapy examples to scrape data from Reddit, e-commerce websites, and other sources.\n• Master techniques to handle challenges in web scraping using Scrapy.\n• Develop skills to extract structured data and store it in different formats such as CSV and JSON.\n• Discover practical tips and best practices for efficient web scraping using Scrapy Python.\n\nNote: We have created a free course for web scraping using the BeautifulSoup library. You can check it out here – Introduction to Web Scraping using Python.\n\nThis article was published as a part of the Data Science Blogathon.\n\nScrapy is a powerful, open-source web crawling framework for Python, designed to handle large-scale web scraping projects. It combines an efficient web crawler with a flexible processing framework, allowing you to extract data from websites and store it in your preferred format.\n\nThe internet’s diversity means there’s no one-size-fits-all approach to extracting data. Ad hoc solutions can lead to writing code for every task, effectively creating your own scraping framework. Scrapy solves this problem by providing a robust framework that eliminates the need to reinvent the wheel.\n\nNote: There are no specific prerequisites for this article. Basic knowledge of HTML and CSS is preferred. If you still think you need a refresher, do a quick read of this article.\n\nCheckout this article for Web Scraping in Python using BeautifulSoup\n\nWrite Your First Web Scraping Code With Scrapy\n\nWe will first quickly take a look at how to set up your system for web scraping and then see how we can build a simple web scraping system step-by-step for extracting data from the Reddit website.\n\nScrapy supports both versions of Python 2 and Python 3. If you’re using Anaconda, you can install the package from the conda-forge channel, which has up-to-date packages for Linux, Windows, and OS X.\n\nAlternatively, if you’re on Linux or Mac OSX, you can directly install scrapy by:\n\nNote: This article will follow Python 2 to use Scrapy.\n\nRecently there was a season launch of a prominent TV series (GoTS7), and social media was on fire. People all around were posting memes, theories, their reactions, etc. I had just learned scrapy and was wondering if it could be used to catch a glimpse of people’s reactions.\n\nI love the python shell, it helps me “try out” things before I can implement them in detail. Similarly, scrapy provides a shell of its own that you can use to experiment. To start the scrapy shell in your command line, type:\n\nWoah! Scrapy wrote a bunch of stuff. For now, you don’t need to worry about it. In order to get information from Reddit (about GoT) you will have to first run a crawler on it. A crawler is a program that browses websites and downloads content. Sometimes crawlers are also referred to as spiders.\n\nReddit is a discussion forum website. It allows users to create “subreddits” for a single topic of discussion. It supports all the features that conventional discussion portals have, like creating a post, voting, replying to posts, including images and links, etc. Reddit also ranks posts based on their votes using a ranking algorithm of its own.\n\nGetting back to Scrapy. A crawler needs a starting point to start crawling(downloading) content. Let’s see, on googling “game of thrones Reddit,” I found that Reddit has a subreddit exclusively for the game of thrones here; this will be the crawler’s start URL.\n\nTo run the crawler in the shell type:\n\nWhen you crawl something with scrapy, it returns a “response” object that contains the downloaded information. Let’s see what the crawler has downloaded:\n\nThis command will open the downloaded page in your default browser.\n\nWow, that looks exactly like the website. The crawler has successfully downloaded the entire web page.\n\nLet’s see how does the raw content look like:\n\nThat’s a lot of content, but not all of it is relevant. Let’s create a list of things that need to be extracted:\n• Number of votes it has\n\nScrapy provides ways to extract information from HTML based on css selectors like class, id, etc. Let’s find the css selector for the title, right-click on any post’s title, and select “Inspect” or “Inspect Element”:\n\nThis will open the developer tools in your browser:\n\nAs can be seen, the css class “title” is applied to all <p> tags that have titles. This will help in filtering out titles from the rest of the content in the response object:\n\nHere response.css(..) is a function that helps extract content based on css selector passed to it. The ‘.’ is used with the title because it’s a css Also, you need to use “::text” to tell your scraper to extract only the text content of the matching elements. This is done because scrapy directly returns the matching element along with the HTML code. Look at the following two examples:\n\nNotice how “::text” helped us filter and extract only the text content.\n\nNow this one is tricky. On inspecting, you get three scores:\n\nThe “score” class is applied to all three, so it can’t be used as a unique selector is required. On further inspection, it can be seen that the selector that uniquely matches the vote count that we need is the one that contains both “score” and “unvoted.”\n\nWhen more than two selectors are required to identify an element, we use them both. Also, since both are CSS classes, we have to use “.” with their names. Let’s try it out first by extracting the first element that matches:\n\nSee that the number of votes for the first post is correctly displayed. Note that on Reddit, the votes score is dynamic based on the number of upvotes and downvotes, so it’ll be changing in real-time. We will add “::text” to our selector so that we only get the vote value and not the complete vote element. To fetch all the votes:\n\nNote: Scrapy has two functions to extract the content extract() and extract_first().\n\nOn inspecting the post, it is clear that the “time” element contains the time of the post.\n\nThere is a catch here, though this is only the relative time(16 hours ago, etc.) of the post. This doesn’t give any information about the date or time zone the time is in. If we want to do some analytics, we won’t know by which date we have to calculate “16 hours ago”. Let’s inspect the time element a little more:\n\nThe “title” attribute of time has both the date and the time in UTC. Let’s extract this instead:\n\nThe .attr(attributename) is used to get the value of the specified attribute of the matching element.\n• response – An object that the scrapy crawler returns. This object contains all the information about the downloaded content.\n• response.css(..) – Matches the element with the given CSS selectors.\n• extract_first(..) – Extracts the “first” element that matches the given criteria.\n• extract(..) – Extracts “all” the elements that match the given criteria.\n\nNote: CSS selectors are a very important concept as far as web scraping is concerned. You can read more about it here and how to use CSS selectors with scrapy.\n\nAs mentioned above, a spider is a program that downloads content from websites or a given URL. When extracting data on a larger scale, you would need to write custom spiders for different websites since there is no “one size fits all” approach in web scraping owing to the diversity in website designs. You also would need to write code to convert the extracted data to a structured format and store it in a reusable format like CSV, JSON (JavaScript Object Notation), excel, etc. That’s a lot of code to write. Luckily, scrapy comes with most of these functionalities built in.\n\nLet’s exit the scrapy shell first and create a new scrapy project:\n\nThis will create a folder, “ourfirstscraper” with the following structure:\n\nFor now, the two most important files are:\n• settings.py – This file contains the settings you set for your project. You’ll be dealing a lot with it.\n• spiders/ – This folder will store all your custom spiders. Every time you ask scrapy to run a spider, it will look for it in this folder.\n\nLet’s change the directory into our first scraper and create a basic spider “redditbot”:\n\nThis will create a new spider, “redditbot.py” in your spiders/ folder with a basic template:\n\nFew things to note here:\n• name: Name of the spider, in this case, it is “redditbot”. Naming spiders properly becomes a huge relief when you have to maintain hundreds of spiders.\n• allowed_domains: An optional list of strings containing domains that this spider is allowed to crawl. Requests for URLs not belonging to the domain names specified in this list won’t be followed.\n• parse(self, response): This parse function is called whenever the crawler successfully crawls a URL. Remember the response object from earlier? This is the same response object that is passed to the parse(..).\n\nAfter every successful crawl, the parse(..) method is called, and so that’s where you write your extraction logic. Let’s add the logic written earlier to extract titles, time, votes, etc., in the parse method:\n\nNote: Here, yield scraped_info does all the magic. This line returns the scraped info(the dictionary of votes, titles, etc.) to scrapy, which in turn processes it and stores it.\n\nSave the file redditbot.py and head back to the shell. Run the spider with the following command:\n\nScrapy would print a lot of stuff on the command line. Let’s focus on the data.\n\nNotice that all the data is downloaded and extracted in a dictionary-like object that meticulously has the votes, title, created_at, and comments.\n\nGetting all the data on the command line is nice, but as a data scientist, it is preferable to have data in certain formats like CSV, Excel, JSON, etc., that can be imported into programs. Scrapy provides this nifty little functionality where you can export the downloaded content in various formats. Many of the popular formats are already supported.\n\nOpen the settings.py file and add the following code to it:\n\nThis will now export all scraped data into a file called reddit.csv. Let’s see how the CSV looks:\n• FEED_FORMAT: The format in which you want the data to be exported. Supported formats are: JSON, JSON lines, XML and CSV.\n• FEED_URI: The location of the exported file.\n\nThere are a plethora of forms that scrapy supports for exporting feed. If you want to dig deeper, you can check here and use css selectors in scrapy.\n\nNow that you have successfully created a system that crawls web content from a link, scrapes(extracts) selective data from it, and saves it in an appropriately structured format, let’s take the game a notch higher and learn more about web scraping.\n\nLet’s now look at a few case studies to get more experience with scrapy as a tool and its various functionalities.\n\nThe advent of the internet and smartphones has been an impetus to the e-commerce industry. With millions of customers and billions of dollars at stake, the market has started seeing a multitude of players. This, in turn, has led to rising of e-commerce aggregator platforms that collect and show you information regarding your products from across multiple portals. For example, when planning to buy a smartphone, you would want to see the prices on different platforms in a single place. What does it take to build such an aggregator platform? Here’s my small take on building an e-commerce site scraper.\n\nAs a test site, you will scrape ShopClues for 4G-Smartphones\n\nThis is what the ShopClues web page looks like:\n\nThe following information needs to be extracted from the page:\n\nOn careful inspection, it can be seen that the attribute “data-img” of the <img> tag can be used to extract image URLs:\n\nNotice that the “title” attribute of the <img> tag contains the product’s full name:\n\nScrapy provides reusable image pipelines for downloading files attached to a particular item (for example, when you scrape products and also want to download their images locally).\n\nThe Images Pipeline has a few extra functions for processing images. It can:\n• Convert all downloaded images to a common format (JPG) and mode (RGB)\n• Check the image’s width/height to make sure they meet a minimum constraint\n\nIn order to use the images pipeline to download images, it needs to be enabled in the settings.py file. Add the following lines to the file:\n\nyou are basically telling scrapy to use the ‘Images Pipeline,’ and the location for the images should be in the folder ‘tmp/images/.’ The final spider would now be:\n\nHere are a few things to note:\n• custom_settings: This is used to set the settings of an individual spider. Remember that settings.py is for the whole project, so here you tell scrapy that the output of this spider should be stored in a CSV file “shopclues.csv” that is to be stored in the “tmp” folder.\n• scraped_info[“image_urls”]: This is the field that scrapy checks for the image’s link. If you set this field with a list of URLs, scrapy will automatically download and store those images for you.\n\nOn running the spider, the output can be read from “tmp/shopclues.csv”:\n\nYou also get the images downloaded. Check the folder “tmp/images/full,” and you will see the images:\n\nAlso, notice that scrapy automatically adds the download path of the image on your system in the csv:\n\nThere you have your own little e-commerce aggregator.\n\nIf you want to dig in, you can read more about Scrapy’s Images Pipeline here.\n\nTechcrunch is one of my favorite blogs that I follow to stay abreast with news about startups and the latest technology products. Just like many blogs nowadays, TechCrunch gives its own RSS feed here: https://techcrunch.com/feed/. One of Scrapy’s features is its ability to handle XML data with ease, and in this part, you are going to extract data from Techcrunch’s RSS feed.\n\nLet’s have a look at the XML; the marked portion is data of interest:\n\nHere are some observations from the page:\n• Each article is present between <item></item> tags, and there are 20 such items(articles).\n• The title of the post is in <title></title> tags.\n• The link to the article can be found in <link> tags.\n• <pubDate> contains the date of publishing.\n• The author’s name is enclosed between funny-looking <dc:creator> tags.\n\nXPath is a syntax that is used to define XML documents. It can be used to traverse through an XML document. Note that XPath follows a hierarchy.\n\nExtracting the title of the post\n\nLet’s extract the title of the first post. Similar to response.css(..), the function response.xpath(..) in scrapy deals with XPath. The following code should do it:\n\nWow! That’s a lot of content, but only the text content of the title is of interest. Let’s filter it out:\n\nThis is much better. Notice that text() here is equivalent of ::text from CSS selectors. Also, look at the XPath //item/title/text(); here, you are basically saying to find the element “item” and extract the “text” content of its sub-element “title”.\n\nSimilarly, the Xpaths for the link, pubDate as:\n\nExtracting author name: dealing with namespaces in XML\n\nThe tag itself has some text “dc:” because of which it can’t be extracted using XPath, and the author name itself is crowded with “![CDATA..” irrelevant text. These are just XML namespaces, and you don’t want to have anything to do with them, so we’ll ask scrapy to remove the namespace:\n\nNow when you try extracting the author name, it will work:\n\nThe complete spider for TechCrunch would be:\n\nAnd there you have your own RSS reader!\n\nAlso, check out some of the interesting projects built with Scrapy:\n\nAlso, there are multiple libraries for web scraping. BeautifulSoup, Selenium is one of those libraries. To learn more, you go through our free course- Introduction to Web Scraping using Python.\n\nWeb scraping using Scrapy Python offers a comprehensive solution for extracting data from websites efficiently and effectively. With its robust framework, Scrapy Python simplifies the process, allowing you to focus on data processing and storage without worrying about the intricacies of web crawling. Whether you’re working on a small project or a large-scale data extraction task, Scrapy provides the tools and flexibility you need. By exploring various Scrapy examples, you can quickly learn how to harness its capabilities, making web scraping using Scrapy a valuable skill for any data-driven project.\n\nAll the code used in this scrapy tutorial is available on GitHub.\n• Web scraping using Scrapy Python allows for efficient data extraction and provides the flexibility to handle diverse web scraping requirements.\n• Scrapy Python offers a robust and comprehensive framework that simplifies the process of web scraping, eliminating the need for ad hoc solutions.\n• With numerous Scrapy examples available, both beginners and experienced developers can quickly learn and implement web scraping techniques.\n• Scrapy facilitates not just data extraction but also processing and storing data in various formats, making it a versatile tool for data-driven projects.\n• By using Scrapy for web scraping, you save time and effort, allowing you to focus on analyzing and utilizing the extracted data effectively."
    }
]