[
    {
        "link": "https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/features",
        "document": "This is a feature showcase page for Stable Diffusion web UI.\n\nAll examples are non-cherrypicked unless specified otherwise.\n\nSupport for SD-XL was added in version , with additional memory optimizations and built-in sequenced refiner inference added in version .\n\nRead here for a list of tips for optimizing inference: Optimum-SDXL-Usage\n\nTwo models are available. The first is the primary model.\n\nThis is a model designed for generating quality -sized images.\n\nIt's tested to produce same (or very close) images as Stability-AI's repo (need to set Random number generator source = CPU in settings)\n\nThis secondary model is designed to process the SD-XL image near completion, to further enhance and refine details in your final output picture. As of version 1.6.0, this is now implemented in the webui natively.\n\nsupport for stable-diffusion-2-1-unclip checkpoints that are used for generating image variations.\n\nIt works in the same way as the current support for the SD2.0 depth model, in that you run it from the img2img tab, it extracts information from the input image (in this case, CLIP or OpenCLIP embeddings), and feeds those into the model in addition to the text prompt. Normally you would do this with denoising strength set to 1.0, since you don't actually want the normal img2img behaviour to have any influence on the generated image.\n\nWebsite. Checkpoint. The checkpoint is fully supported in img2img tab. No additional actions are required. Previously an extension by a contributor was required to generate pictures: it's no longer required, but should still work. Most of img2img implementation is by the same person.\n\nTo reproduce results of the original repo, use denoising of 1.0, Euler a sampler, and edit the config in to say:\n\nA single button with a picture of a card on it. It unifies multiple extra ways to extend your generation into one UI.\n\nFind it next to the big Generate button:\n\nExtra networks provides a set of cards, each corresponding to a file with a part of model you either train or obtain from somewhere. Clicking the card adds the model to prompt, where it will affect generation.\n\nA method to fine tune weights for a token in CLIP, the language model used by Stable Diffusion, from summer 2021. Author's site. Long explanation: Textual Inversion\n\nA method to fine tune weights for CLIP and Unet, the language model and the actual image de-noiser used by Stable Diffusion, published in 2021. Paper. A good way to train LoRA is to use kohya-ss.\n\nSupport for LoRA is built-in into the Web UI, but there is an extension with original implementation by kohya-ss.\n\nCurrently, LoRA networks for Stable Diffusion 2.0+ models are not supported by Web UI.\n\nLoRA is added to the prompt by putting the following text into any location: , where is the name of file with LoRA on disk, excluding extension, and is a number, generally from 0 to 1, that lets you choose how strongly LoRA will affect the output. LoRA cannot be added to the negative prompt.\n\nThe text for adding LoRA to the prompt, , is only used to enable LoRA, and is erased from prompt afterwards, so you can't do tricks with prompt editing like . A batch with multiple different prompts will only use the LoRA from the first prompt.\n\nSince version , webui supports other network types through the built-in extension.\n\nSee the details in the [PR]\n\nA method to fine tune weights for CLIP and Unet, the language model and the actual image de-noiser used by Stable Diffusion, generously donated to the world by our friends at Novel AI in autumn 2022. Works in the same way as LoRA except for sharing weights for some layers. Multiplier can be used to choose how strongly the hypernetwork will affect the output.\n\nSame rules for adding hypernetworks to the prompt apply as for LoRA: .\n\nA model trained to accept inputs in different languages. More info. PR.\n• Download the checkpoint from huggingface. Click the down arrow to download.\n• Download your checkpoint file from huggingface. Click the down arrow to download.\n\nThe depth-guided model will only work in img2img tab. More info. PR.\n\nor inpainting conditioning mask strength works on this too.\n\nOutpainting extends the original image and inpaints the created empty space.\n\nOriginal image by Anonymous user from 4chan. Thank you, Anonymous user.\n\nYou can find the feature in the img2img tab at the bottom, under Script -> Poor man's outpainting.\n\nOutpainting, unlike normal image generation, seems to profit very much from large step count. A recipe for a good outpainting is a good prompt that matches the picture, sliders for denoising and CFG scale set to max, and step count of 50 to 100 with Euler ancestral or DPM2 ancestral samplers.\n\nIn img2img tab, draw a mask over a part of the image, and that part will be in-painted.\n• draw a mask yourself in the web editor\n• erase a part of the picture in an external editor and upload a transparent picture. Any even slightly transparent areas will become part of the mask. Be aware that some editors save completely transparent areas as black by default.\n• change mode (to the bottom right of the picture) to \"Upload mask\" and choose a separate black and white image for the mask (white=inpaint).\n\nRunwayML has trained an additional model specifically designed for inpainting. This model accepts additional inputs - the initial image without noise plus the mask - and seems to be much better at the job.\n\nDownload and extra info for the model is here: https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion\n\nTo use the model, you must rename the checkpoint so that its filename ends in , for example, .\n\nAfter that just select the checkpoint as you'd usually select any checkpoint and you're good to go.\n\nThe masked content field determines content is placed to put into the masked regions before they are inpainted. This does not represent final output, it's only a look at what's going on mid-process.\n\nNormally, inpainting resizes the image to the target resolution specified in the UI. With enabled, only the masked region is resized, and after processing it is pasted back to the original picture. This allows you to work with large pictures and render the inpainted object at a much larger resolution.\n\nThere are two options for masked mode:\n• Inpaint masked - the region under the mask is inpainted\n• Inpaint not masked - under the mask is unchanged, everything else is inpainted\n\nBasic coloring tool for the img2img tab. Chromium-based browsers support a dropper tool. (this is on firefox)\n\nSeparate multiple prompts using the character, and the system will produce an image for every combination of them. For example, if you use prompt, there are four combinations possible (first part of the prompt is always kept):\n\nFour images will be produced, in this order, all with the same seed and each with a corresponding prompt:\n\nAnother example, this time with 5 prompts and 16 variations:\n\nYou can find the feature at the bottom, under Script -> Prompt matrix.\n\nUpscale image using RealESRGAN/ESRGAN and then go through tiles of the result, improving them with img2img. It also has an option to let you do the upscaling part yourself in an external program, and just go through tiles with img2img.\n\nOriginal idea by: https://github.com/jquesnelle/txt2imghd. This is an independent implementation.\n\nTo use this feature, select (img2img tab).\n\nThe input image will be upscaled to twice the original width and height, and UI's width and height sliders specify the size of individual tiles. Because of overlap, the size of the tile can be very important: 512x512 image needs nine 512x512 tiles (because of overlap), but only four 640x640 tiles.\n• Denoising strength: 0.2, can go up to 0.4 if you feel adventurous\n• A larger denoising strength is problematic due to the fact SD upscale works in tiles, as the diffusion process is then unable to give attention to the image as a whole.\n\nTyping past standard 75 tokens that Stable Diffusion usually accepts increases prompt size limit from 75 to 150. Typing past that increases prompt size further. This is done by breaking the prompt into chunks of 75 tokens, processing each independently using CLIP's Transformers neural network, and then concatenating the result before feeding into the next component of stable diffusion, the Unet.\n\nFor example, a prompt with 120 tokens would be separated into two chunks: first with 75 tokens, second with 45. Both would be padded to 75 tokens and extended with start/end tokens to 77. After passing those two chunks though CLIP, we'll have two tensors with shape of . Concatenating those results in tensor that is then passed to Unet without issue.\n\nAdding a keyword (must be uppercase) fills the current chunks with padding characters. Adding more text after text will start a new chunk.\n\nUsing in the prompt increases the model's attention to enclosed words, and decreases it. You can combine multiple modifiers:\n• - increase attention to by a factor of 1.1\n• - increase attention to by a factor of 1.21 (= 1.1 * 1.1)\n• - decrease attention to by a factor of 1.1\n• - increase attention to by a factor of 1.5\n• - decrease attention to by a factor of 4 (= 1 / 0.25)\n\nWith , a weight can be specified like this: . If the weight is not specified, it is assumed to be 1.1. Specifying weight only works with not with .\n\nIf you want to use any of the literal characters in the prompt, use the backslash to escape them: .\n\nOn 2022-09-29, a new implementation was added that supports escape characters and numerical weights. A downside of the new implementation is that the old one was not perfect and sometimes ate characters: \"a (((farm))), daytime\", for example, would become \"a farm daytime\" without the comma. This behavior is not shared by the new implementation which preserves all text correctly, and this means that your saved seeds may produce different pictures. For now, there is an option in settings to use the old implementation.\n\nNAI uses my implementation from before 2022-09-29, except they have 1.05 as the multiplier and use instead of . So the conversion applies:\n\nSelecting the loopback script in img2img allows you to automatically feed output image as input for the next batch. Equivalent to saving output image and replacing the input image with it. Batch count setting controls how many iterations of this you get.\n\nUsually, when doing this, you would choose one of many images for the next iteration yourself, so the usefulness of this feature may be questionable, but I've managed to get some very nice outputs with it that I wasn't able to get otherwise.\n\nOriginal image by Anonymous user from 4chan. Thank you, Anonymous user.\n\nCreates multiple grids of images with varying parameters. X and Y are used as the rows and columns, while the Z grid is used as a batch dimension.\n\nSelect which parameters should be shared by rows, columns and batch by using X type, Y type and Z Type fields, and input those parameters separated by comma into X/Y/Z values fields. For integer, and floating point numbers, and ranges are supported. Examples:\n• Ranges with the count in square brackets:\n\nPrompt S/R is one of more difficult to understand modes of operation for X/Y Plot. S/R stands for search/replace, and that's what it does - you input a list of words or phrases, it takes the first from the list and treats it as keyword, and replaces all instances of that keyword with other entries from the list.\n\nFor example, with prompt , and Prompt S/R you will get three prompts:\n\nThe list uses the same syntax as a line in a CSV file, so if you want to include commas into your entries you have to put text in quotes and make sure there is no space between quotes and separating commas:\n\nWith this script it is possible to create a list of jobs which will be executed sequentially.\n\nThere are three options for resizing input images in img2img mode:\n• Just resize - simply resizes the source image to the target resolution, resulting in an incorrect aspect ratio\n• Crop and resize - resize source image preserving aspect ratio so that entirety of target resolution is occupied by it, and crop parts that stick out\n• Resize and fill - resize source image preserving aspect ratio so that it entirely fits target resolution, and fill empty space by rows/columns from the source image\n\nPick out of multiple sampling methods for txt2img:\n\nThis function allows you to generate images from known seeds at different resolutions. Normally, when you change resolution, the image changes entirely, even if you keep all other parameters including seed. With seed resizing you specify the resolution of the original image, and the model will very likely produce something looking very similar to it, even at a different resolution. In the example below, the leftmost picture is 512x512, and others are produced with exact same parameters but with larger vertical resolution.\n\nAncestral samplers are a little worse at this than the rest.\n\nYou can find this feature by clicking the \"Extra\" checkbox near the seed.\n\nA Variation strength slider and Variation seed field allow you to specify how much the existing picture should be altered to look like a different one. At maximum strength, you will get pictures with the Variation seed, at minimum - pictures with the original Seed (except for when using ancestral samplers).\n\nYou can find this feature by clicking the \"Extra\" checkbox near the seed.\n\nPress the \"Save prompt as style\" button to write your current prompt to styles.csv, the file with a collection of styles. A dropbox to the right of the prompt will allow you to choose any style out of previously saved, and automatically append it to your input. To delete a style, manually delete it from styles.csv and restart the program.\n\nif you use the special string in your style, it will substitute anything currently in the prompt into that position, rather than appending the style to your prompt.\n\nAllows you to use another prompt of things the model should avoid when generating the picture. This works by using the negative prompt for unconditional conditioning in the sampling process instead of an empty string.\n\nCLIP interrogator allows you to retrieve the prompt from an image. The prompt won't allow you to reproduce this exact image (and sometimes it won't even be close), but it can be a good start.\n\nThe first time you run CLIP interrogator it will download a few gigabytes of models.\n\nCLIP interrogator has two parts: one is a BLIP model that creates a text description from the picture. Other is a CLIP model that will pick few lines relevant to the picture out of a list. By default, there is only one list - a list of artists (from ). You can add more lists by doing the following:\n• create directory in the same place as webui\n• put text files in it with a relevant description on each line\n\nFor example of what text files to use, see https://github.com/pharmapsychotic/clip-interrogator/tree/main/clip_interrogator/data. In fact, you can just take files from there and use them - just skip artists.txt because you already have a list of artists in (or use that too, who's going to stop you). Each file adds one line of text to the final description. If you add \".top3.\" to filename, for example, , the three most relevant lines from this file will be added to the prompt (other numbers also work).\n\nThere are settings relevant to this feature:\n• - do not unload Interrogate models from memory after using them. For users with a lot of VRAM.\n• - adds artist from when interrogating. Can be useful to disable when you have your list of artists in directory\n• - parameter that affects how detailed descriptions from BLIP model are (the first part of generated prompt)\n• - interrogator will only consider this many first lines in a file. Set to 0, the default is 1500, which is about as much as a 4GB videocard can handle.\n\nPrompt editing allows you to start sampling one picture, but in the middle swap to something else. The base syntax for this is:\n\nWhere and are arbitrary texts, and is a number that defines how late in the sampling cycle should the switch be made. The later it is, the less power the model has to draw the text in place of text. If is a number from to , it's a fraction of the number of steps after which to make the switch. If it's an integer (no decimal point) greater than zero, it's just the step after which to make the switch.\n\nNesting one prompt editing inside another also works.\n• - adds to the prompt after a fixed number of steps ( )\n• - removes from the prompt after a fixed number of steps ( )\n\n1.6.0 Update: For numbers with a fractional point in a range of to , this now targets the second pass AKA hires fix pass. More information can be found on the seed breaking changes page and the relevant PR.\n• At start, the model will be drawing .\n• After step 16, it will switch to drawing , continuing from where it stopped with fantasy.\n\nHere's a more complex example with multiple edits: fantasy landscape with a [mountain:lake:0.25] and [an oak:a christmas tree:0.75][ in foreground::0.6][ in background:0.25] [shoddy:masterful:0.5] (sampler has steps)\n• at start (step ), fantasy landscape with a mountain and an oak in foreground shoddy\n• after step , fantasy landscape with a lake and an oak in foreground in background shoddy\n• after step , fantasy landscape with a lake and an oak in foreground in background masterful\n• after step , fantasy landscape with a lake and an oak in background masterful\n• after step , fantasy landscape with a lake and a christmas tree in background masterful\n\nThe picture at the top was made with the prompt:\n\nOfficial portrait of a smiling world war ii general, [male:female:0.99], cheerful, happy, detailed face, 20th century, highly detailed, cinematic lighting, digital art painting by Greg Rutkowski\n\nAnd the number is replaced with whatever you see in column labels on the image.\n\nThe last column in the picture is , which essentially means that you are asking the model to draw a female from the start, without starting with a male general, and that is why it looks so different from others.\n\nNote: This syntax does not work with extra networks, such as LoRA. See this discussion post for details. For similar functionality, see the sd-webui-loractl extension.\n\nConvenient Syntax for swapping every other step.\n\nOn step 1, prompt is \"cow in a field.\" Step 2 is \"horse in a field.\" Step 3 is \"cow in a field\" and so on.\n\nSee more advanced example below. On step 8, the chain loops back from \"man\" to \"cow.\"\n\nPrompt editing was first implemented by Doggettx in this reddit post.\n\nNote: This syntax does not work with extra networks, such as LoRA. See this discussion post for details. For similar functionality, see the sd-webui-loractl extension.\n\nA convenience option to partially render your image at a lower resolution, upscale it, and then add details at a high resolution. In other words, this is equivalent to generating an image in txt2img, upscaling it via a method of your choice, and running a second pass on the now upscaled image in img2img to further refine the upscale and create the final result.\n\nBy default, SD1/2 based models create horrible images at very high resolutions, as these models were only trained at 512px or 768px. This method makes it possible to avoid this issue by utilizing the small picture's composition in the denoising process of the larger version. Enabled by checking the \"Hires. fix\" checkbox on the txt2img page.\n\n1.8.0 Update: Images can now be upscaled with hires. fix as a separate process after the initial generation by clicking on the [✨] button while the relevant image is selected in the image viewer.\n\nSmall picture is rendered at whatever resolution you set using width/height sliders. Large picture's dimensions are controlled by three sliders: \"Scale by\" multiplier (Hires upscale), \"Resize width to\" and/or \"Resize height to\" (Hires resize).\n• If \"Resize width to\" and \"Resize height to\" are 0, \"Scale by\" is used.\n• If \"Resize width to\" is 0, \"Resize height to\" is calculated from width and height.\n• If \"Resize height to\" is 0, \"Resize width to\" is calculated from width and height.\n• If both \"Resize width to\" and \"Resize height to\" are non-zero, image is upscaled to be at least those dimensions, and some parts are cropped.\n\nIn older versions of the webui, the final width and height were input manually (the last option listed above). In new versions, the default is to use the \"Scale by\" factor, which is the default and preferred.\n\nTo potentially further enhance details in hires. fix, see the notes on extra noise.\n\nA dropdown allows you to to select the kind of upscaler to use for resizing the image. In addition to all upscalers you have available on extras tab, there is an option to upscale a latent space image, which is what stable diffusion works with internally - for a 3x512x512 RGB image, its latent space representation would be 4x64x64. To see what each latent space upscaler does, you can set Denoising strength to 0 and Hires steps to 1 - you'll get a very good approximation of what stable diffusion would be working with on upscaled image.\n\nBelow are examples of how different latent upscale modes look.\n\nAntialiased variations were PRd in by a contributor and seem to be the same as non-antialiased.\n\nA method to allow the combination of multiple prompts. combine prompts using an uppercase AND\n\nSupports weights for prompts: The default weight value is 1. It can be quite useful for combining multiple embeddings to your result: creature_embedding in the woods:0.7 AND arcane_embedding:0.5 AND glitch_embedding:0.2\n\nUsing a value lower than 0.1 will barely have an effect. will produce basically the same output as\n\nThis could be handy for generating fine-tuned recursive variations, by continuing to append more prompts to your total. creature_embedding on log AND frog:0.13 AND yellow eyes:0.08\n\nOptimizations for GPUs with low VRAM. This should make it possible to generate 512x512 images on videocards with 4GB memory.\n\nis a reimplementation of an optimization idea by basujindal. Model is separated into modules, and only one module is kept in GPU memory; when another module needs to run, the previous is removed from GPU memory. The nature of this optimization makes the processing run slower -- about 10 times slower compared to normal operation on my RTX 3090.\n\nis another optimization that should reduce VRAM usage significantly by not processing conditional and unconditional denoising in the same batch.\n\nThis implementation of optimization does not require any modification to the original Stable Diffusion code.\n\nWith this lightweight VAE enabled via settings, it typically allows for very large, fast generations with a small quality loss. This gain can be very large, maximum generations with --lowvram can increase from to\n\nLets you improve faces in pictures using either GFPGAN or CodeFormer. There is a checkbox in every tab to use face restoration, and also a separate tab that just allows you to use face restoration on any picture, with a slider that controls how visible the effect is. You can choose between the two methods in settings.\n\nFull guide with other info is here: https://imgur.com/a/VjFi5uM\n\nClick the Save button under the output section, and generated images will be saved to a directory specified in settings; generation parameters will be appended to a csv file in the same directory.\n\nGradio's loading graphic has a very negative effect on the processing speed of the neural network. My RTX 3090 makes images about 10% faster when the tab with gradio is not active. By default, the UI now hides loading progress animation and replaces it with static \"Loading...\" text, which achieves the same effect. Use the commandline option to revert this and show loading animations.\n\nIf you want faster swapping between models, increase the counter in settings. Webui will keep models you've swapped from in ram.\n\nMake sure you set the appropriate number according to your remaining available ram.\n\nStable Diffusion has a limit for input text length. If your prompt is too long, you will get a warning in the text output field, showing which parts of your text were truncated and ignored by the model.\n\nAdds information about generation parameters to PNG as a text chunk. You can view this information later using any software that supports viewing PNG chunk info, for example: https://www.nayuki.io/page/png-file-chunk-inspector\n\nStarting in version 1.8.0, comments are now supported and enabled by default. Comments function similarly to single-line comments in Python, using the hash symbol ( ). When generating an image, the webui will ignore any text following a symbol on the same line. Comments are not included in the PNG info of generated images.\n\nHere's an example of how to use comments in a prompt:\n\nWhen viewing the PNG info of an image generated with the above prompt, the end-user will see:\n\nThe comments will not be visible in the PNG info, as they are only meant for the user's reference within the webui.\n\nA tab with settings, allows you to use UI to edit more than half of parameters that previously were commandline. Settings are saved to . Settings that remain as commandline options are ones that are required at startup.\n\nThe field in the Settings tab allows customization of generated txt2img and img2img images filenames. This pattern defines the generation parameters you want to include in filenames and their order. The supported tags are:\n\nThis list will evolve though, with new additions. You can get an up-to-date list of supported tags by hovering your mouse over the \"Images filename pattern\" label in the UI.\n\nNote about \"prompt\" tags: will add underscores between the prompt words, while will keep the prompt intact (easier to copy/paste into the UI again). is a simplified and cleaned-up version of your prompt, already used to generate subdirectories names, with only the words of your prompt (no punctuation).\n\nIf you leave this field empty, the default pattern will be applied ( ).\n\nPlease note that the tags are actually replaced inside the pattern. It means that you can also add non-tags words to this pattern, to make filenames even more explicit. For example:\n\nIf the program is launched with option, an extra text input field for script code is available at the bottom of the page, under Scripts -> Custom code. It allows you to input python code that will do the work with the image.\n\nIn code, access parameters from web UI using the variable, and provide outputs for web UI using the function. All globals from the script are also accessible.\n\nA simple script that would just process the image and output it normally:\n\nYou can change parameters for UI elements in , it is created automatically when the program first starts. Some options:\n\nCheckboxes that would usually expand a hidden section will not initially do so when set as UI config entries.\n\nIt's possible to use ESRGAN models on the Extras tab, as well as in SD upscale. Paper here.\n\nTo use ESRGAN models, put them into ESRGAN directory in the same location as webui.py. A file will be loaded as a model if it has .pth extension. Grab models from the Model Database.\n\nNot all models from the database are supported. All 2x models are most likely not supported.\n\nDeconstructs an input image using a reverse of the Euler diffuser to create the noise pattern used to construct the input prompt.\n\nAs an example, you can use this image. Select the img2img alternative test from the scripts section.\n\nAdjust your settings for the reconstruction process:\n• Use a brief description of the scene: \"A smiling woman with brown hair.\" Describing features you want to change helps. Set this as your starting prompt, and 'Original Input Prompt' in the script settings.\n• You MUST use the Euler sampling method, as this script is built on it.\n• Sampling steps: 50-60. This MUCH match the decode steps value in the script, or you'll have a bad time. Use 50 for this demo.\n• CFG scale: 2 or lower. For this demo, use 1.8. (Hint, you can edit ui-config.json to change \"img2img/CFG Scale/step\" to .1 instead of .5.\n• Denoising strength - this does matter, contrary to what the old docs said. Set it to 1.\n• Width/Height - Use the width/height of the input image.\n• Seed...you can ignore this. The reverse Euler is generating the noise for the image now.\n• Decode cfg scale - Somewhere lower than 1 is the sweet spot. For the demo, use 1.\n• Decode steps - as mentioned above, this should match your sampling steps. 50 for the demo, consider increasing to 60 for more detailed images.\n\nOnce all of the above are dialed in, you should be able to hit \"Generate\" and get back a result that is a very close approximation to the original.\n\nAfter validating that the script is re-generating the source photo with a good degree of accuracy, you can try to change the details of the prompt. Larger variations of the original will likely result in an image with an entirely different composition than the source.\n\nExample outputs using the above settings and prompts below (Red hair/pony not pictured)\n\n\"A smiling woman with blue hair.\" Works. \"A frowning woman with brown hair.\" Works. \"A frowning woman with red hair.\" Works. \"A frowning woman with red hair riding a horse.\" Seems to replace the woman entirely, and now we have a ginger pony.\n\nThis is a fully independent implementation developed specifically for the webui and was graciously provided by this user. More information can be found via the built-in help in the webui, or the relevant PR.\n\nCreate a file named near and put custom CSS code into it. For example, this makes the gallery taller:\n\nA useful tip is you can append to your webui url to enable a built in dark theme \n\ne.g. ( )\n\nAlternatively, you can add the to the in \n\n e.g.\n\nIf an audio file named is present in webui's root folder, it will be played when the generation process completes.\n\nThis is a slider in settings, and it controls how early the processing of prompt by CLIP network should be stopped.\n\nCLIP is a very advanced neural network that transforms your prompt text into a numerical representation. Neural networks work very well with this numerical representation and that's why devs of SD chose CLIP as one of 3 models involved in stable diffusion's method of producing images. As CLIP is a neural network, it means that it has a lot of layers. Your prompt is digitized in a simple way, and then fed through layers. You get numerical representation of the prompt after the 1st layer, you feed that into the second layer, you feed the result of that into third, etc, until you get to the last layer, and that's the output of CLIP that is used in stable diffusion. This is the slider value of 1. But you can stop early, and use the output of the next to last layer - that's slider value of 2. The earlier you stop, the less layers of neural network have worked on the prompt.\n\nSome models were trained with this kind of tweak, so setting this value helps produce better results on those models.\n\nNote: All SDXL models are trained with the next to last (penultimate) layer. This is why Clip Skip intentionally does not change the result of the model, as it would simply make the result worse. The option is only provided due to the fact early SDv1 models do not provide any way to determine the correct layer to use.\n\nAdds additional noise from the random seed, determined by the setting, defaulting to . Implemented in version 1.6.0 via #12564, available in settings under -> . As noted in the UI, this parameter should always be lower than the denoising strength used to yield the best results.\n\nOne purpose for this tweak is to add back additional detail into hires fix. For a very simplified understanding, you may think of it as a cross between GAN upscaling and latent upscaling.\n\nThe below example is of a 512x512 image with hires fix applied, using a GAN upscaler (4x-UltraSharp), at a denoising strength of 0.45. The image on the right utilizes this extra noise tweak.\n\nNote that the previous setting implemented at the time many months ago ( ) technically achieves the same effect, but as noted in the name only applies to img2img (not hires. fix), and due to it was implemented it is very sensitive, realisticly only useful in a range of to . For almost all operations it would be suggested to use the new parameter instead.\n\nFor developers, a callback also exists ( ). Here is an example of use that makes the region to add noise to maskable. https://gist.github.com/catboxanon/69ce64e0389fa803d26dc59bb444af53"
    },
    {
        "link": "https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features/eaad63b1ff96fd4c92bbdc001029819ec90d168b",
        "document": "This is a feature showcase page for Stable Diffusion web UI.\n\nAll examples are non-cherrypicked unless specified otherwise.\n\nTwo models are available. The first is the primary model.\n\nThese models specifically are recommended for generating, merging and training.\n\nThis is a model designed for generating quality -sized images. It is not meant to generate good pictures at .\n\nIt's tested to produce same (or very close) images as Stability-AI's repo (need to set Random number generator source = CPU in settings)\n\nThis secondary model is designed to process the SD-XL image near completion*, to further enhance and refine details in your final output picture. You could use it to refine finished pictures in the img2img tab as well.\n• wcde/sd-webui-refiner *To try this kind of generation, you can use this extension.\n\nsupport for stable-diffusion-2-1-unclip checkpoints that are used for generating image variations.\n\nIt works in the same way as the current support for the SD2.0 depth model, in that you run it from the img2img tab, it extracts information from the input image (in this case, CLIP or OpenCLIP embeddings), and feeds those into the model in addition to the text prompt. Normally you would do this with denoising strength set to 1.0, since you don't actually want the normal img2img behaviour to have any influence on the generated image.\n\nWebsite. Checkpoint. The checkpoint is fully supported in img2img tab. No additional actions are required. Previously an extension by a contributor was required to generate pictures: it's no longer required, but should still work. Most of img2img implementation is by the same person.\n\nTo reproduce results of the original repo, use denoising of 1.0, Euler a sampler, and edit the config in to say:\n\nA single button with a picture of a card on it. It unifies multiple extra ways to extend your generation into one UI.\n\nFind it next to the big Generate button:\n\nExtra networks provides a set of cards, each corresponding to a file with a part of model you either train or obtain from somewhere. Clicking the card adds the model to prompt, where it will affect generation.\n\nA method to fine tune weights for a token in CLIP, the language model used by Stable Diffusion, from summer 2021. Author's site. Long explanation: Textual Inversion\n\nA method to fine tune weights for CLIP and Unet, the language model and the actual image de-noiser used by Stable Diffusion, published in 2021. Paper. A good way to train LoRA is to use kohya-ss.\n\nSupport for LoRA is built-in into the Web UI, but there is an extension with original implementation by kohya-ss.\n\nCurrently, LoRA networks for Stable Diffusion 2.0+ models are not supported by Web UI.\n\nLoRA is added to the prompt by putting the following text into any location: , where is the name of file with LoRA on disk, excluding extension, and is a number, generally from 0 to 1, that lets you choose how strongly LoRA will affect the output. LoRA cannot be added to the negative prompt.\n\nThe text for adding LoRA to the prompt, , is only used to enable LoRA, and is erased from prompt afterwards, so you can't do tricks with prompt editing like . A batch with multiple different prompts will only use the LoRA from the first prompt.\n\nSince version , webui supports other network types through the built-in extension.\n\nSee the details in the [PR]\n\nA method to fine tune weights for CLIP and Unet, the language model and the actual image de-noiser used by Stable Diffusion, generously donated to the world by our friends at Novel AI in autumn 2022. Works in the same way as LoRA except for sharing weights for some layers. Multiplier can be used to choose how strongly the hypernetwork will affect the output.\n\nSame rules for adding hypernetworks to the prompt apply as for LoRA: .\n\nA model trained to accept inputs in different languages. More info. PR.\n• Download the checkpoint from huggingface. Click the down arrow to download.\n• Download your checkpoint file from huggingface. Click the down arrow to download.\n\nThe depth-guided model will only work in img2img tab. More info. PR.\n\nor inpainting conditioning mask strength works on this too.\n\nOutpainting extends the original image and inpaints the created empty space.\n\nOriginal image by Anonymous user from 4chan. Thank you, Anonymous user.\n\nYou can find the feature in the img2img tab at the bottom, under Script -> Poor man's outpainting.\n\nOutpainting, unlike normal image generation, seems to profit very much from large step count. A recipe for a good outpainting is a good prompt that matches the picture, sliders for denoising and CFG scale set to max, and step count of 50 to 100 with Euler ancestral or DPM2 ancestral samplers.\n\nIn img2img tab, draw a mask over a part of the image, and that part will be in-painted.\n• draw a mask yourself in the web editor\n• erase a part of the picture in an external editor and upload a transparent picture. Any even slightly transparent areas will become part of the mask. Be aware that some editors save completely transparent areas as black by default.\n• change mode (to the bottom right of the picture) to \"Upload mask\" and choose a separate black and white image for the mask (white=inpaint).\n\nRunwayML has trained an additional model specifically designed for inpainting. This model accepts additional inputs - the initial image without noise plus the mask - and seems to be much better at the job.\n\nDownload and extra info for the model is here: https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion\n\nTo use the model, you must rename the checkpoint so that its filename ends in , for example, .\n\nAfter that just select the checkpoint as you'd usually select any checkpoint and you're good to go.\n\nThe masked content field determines content is placed to put into the masked regions before they are inpainted. This does not represent final output, it's only a look at what's going on mid-process.\n\nNormally, inpainting resizes the image to the target resolution specified in the UI. With enabled, only the masked region is resized, and after processing it is pasted back to the original picture. This allows you to work with large pictures and render the inpainted object at a much larger resolution.\n\nThere are two options for masked mode:\n• Inpaint masked - the region under the mask is inpainted\n• Inpaint not masked - under the mask is unchanged, everything else is inpainted\n\nBasic coloring tool for the img2img tab. Chromium-based browsers support a dropper tool. (this is on firefox)\n\nSeparate multiple prompts using the character, and the system will produce an image for every combination of them. For example, if you use prompt, there are four combinations possible (first part of the prompt is always kept):\n\nFour images will be produced, in this order, all with the same seed and each with a corresponding prompt:\n\nAnother example, this time with 5 prompts and 16 variations:\n\nYou can find the feature at the bottom, under Script -> Prompt matrix.\n\nUpscale image using RealESRGAN/ESRGAN and then go through tiles of the result, improving them with img2img. It also has an option to let you do the upscaling part yourself in an external program, and just go through tiles with img2img.\n\nOriginal idea by: https://github.com/jquesnelle/txt2imghd. This is an independent implementation.\n\nTo use this feature, select (img2img tab).\n\nThe input image will be upscaled to twice the original width and height, and UI's width and height sliders specify the size of individual tiles. Because of overlap, the size of the tile can be very important: 512x512 image needs nine 512x512 tiles (because of overlap), but only four 640x640 tiles.\n• Denoising strength: 0.2, can go up to 0.4 if you feel adventurous\n• A larger denoising strength is problematic due to the fact SD upscale works in tiles, as the diffusion process is then unable to give attention to the image as a whole.\n\nTyping past standard 75 tokens that Stable Diffusion usually accepts increases prompt size limit from 75 to 150. Typing past that increases prompt size further. This is done by breaking the prompt into chunks of 75 tokens, processing each independently using CLIP's Transformers neural network, and then concatenating the result before feeding into the next component of stable diffusion, the Unet.\n\nFor example, a prompt with 120 tokens would be separated into two chunks: first with 75 tokens, second with 45. Both would be padded to 75 tokens and extended with start/end tokens to 77. After passing those two chunks though CLIP, we'll have two tensors with shape of . Concatenating those results in tensor that is then passed to Unet without issue.\n\nAdding a keyword (must be uppercase) fills the current chunks with padding characters. Adding more text after text will start a new chunk.\n\nUsing in the prompt increases the model's attention to enclosed words, and decreases it. You can combine multiple modifiers:\n• - increase attention to by a factor of 1.1\n• - increase attention to by a factor of 1.21 (= 1.1 * 1.1)\n• - decrease attention to by a factor of 1.1\n• - increase attention to by a factor of 1.5\n• - decrease attention to by a factor of 4 (= 1 / 0.25)\n\nWith , a weight can be specified like this: . If the weight is not specified, it is assumed to be 1.1. Specifying weight only works with not with .\n\nIf you want to use any of the literal characters in the prompt, use the backslash to escape them: .\n\nOn 2022-09-29, a new implementation was added that supports escape characters and numerical weights. A downside of the new implementation is that the old one was not perfect and sometimes ate characters: \"a (((farm))), daytime\", for example, would become \"a farm daytime\" without the comma. This behavior is not shared by the new implementation which preserves all text correctly, and this means that your saved seeds may produce different pictures. For now, there is an option in settings to use the old implementation.\n\nNAI uses my implementation from before 2022-09-29, except they have 1.05 as the multiplier and use instead of . So the conversion applies:\n\nSelecting the loopback script in img2img allows you to automatically feed output image as input for the next batch. Equivalent to saving output image and replacing the input image with it. Batch count setting controls how many iterations of this you get.\n\nUsually, when doing this, you would choose one of many images for the next iteration yourself, so the usefulness of this feature may be questionable, but I've managed to get some very nice outputs with it that I wasn't able to get otherwise.\n\nOriginal image by Anonymous user from 4chan. Thank you, Anonymous user.\n\nCreates multiple grids of images with varying parameters. X and Y are used as the rows and columns, while the Z grid is used as a batch dimension.\n\nSelect which parameters should be shared by rows, columns and batch by using X type, Y type and Z Type fields, and input those parameters separated by comma into X/Y/Z values fields. For integer, and floating point numbers, and ranges are supported. Examples:\n• Ranges with the count in square brackets:\n\nPrompt S/R is one of more difficult to understand modes of operation for X/Y Plot. S/R stands for search/replace, and that's what it does - you input a list of words or phrases, it takes the first from the list and treats it as keyword, and replaces all instances of that keyword with other entries from the list.\n\nFor example, with prompt , and Prompt S/R you will get three prompts:\n\nThe list uses the same syntax as a line in a CSV file, so if you want to include commas into your entries you have to put text in quotes and make sure there is no space between quotes and separating commas:\n\nWith this script it is possible to create a list of jobs which will be executed sequentially.\n\nThere are three options for resizing input images in img2img mode:\n• Just resize - simply resizes the source image to the target resolution, resulting in an incorrect aspect ratio\n• Crop and resize - resize source image preserving aspect ratio so that entirety of target resolution is occupied by it, and crop parts that stick out\n• Resize and fill - resize source image preserving aspect ratio so that it entirely fits target resolution, and fill empty space by rows/columns from the source image\n\nPick out of multiple sampling methods for txt2img:\n\nThis function allows you to generate images from known seeds at different resolutions. Normally, when you change resolution, the image changes entirely, even if you keep all other parameters including seed. With seed resizing you specify the resolution of the original image, and the model will very likely produce something looking very similar to it, even at a different resolution. In the example below, the leftmost picture is 512x512, and others are produced with exact same parameters but with larger vertical resolution.\n\nAncestral samplers are a little worse at this than the rest.\n\nYou can find this feature by clicking the \"Extra\" checkbox near the seed.\n\nA Variation strength slider and Variation seed field allow you to specify how much the existing picture should be altered to look like a different one. At maximum strength, you will get pictures with the Variation seed, at minimum - pictures with the original Seed (except for when using ancestral samplers).\n\nYou can find this feature by clicking the \"Extra\" checkbox near the seed.\n\nPress the \"Save prompt as style\" button to write your current prompt to styles.csv, the file with a collection of styles. A dropbox to the right of the prompt will allow you to choose any style out of previously saved, and automatically append it to your input. To delete a style, manually delete it from styles.csv and restart the program.\n\nif you use the special string in your style, it will substitute anything currently in the prompt into that position, rather than appending the style to your prompt.\n\nAllows you to use another prompt of things the model should avoid when generating the picture. This works by using the negative prompt for unconditional conditioning in the sampling process instead of an empty string.\n\nCLIP interrogator allows you to retrieve the prompt from an image. The prompt won't allow you to reproduce this exact image (and sometimes it won't even be close), but it can be a good start.\n\nThe first time you run CLIP interrogator it will download a few gigabytes of models.\n\nCLIP interrogator has two parts: one is a BLIP model that creates a text description from the picture. Other is a CLIP model that will pick few lines relevant to the picture out of a list. By default, there is only one list - a list of artists (from ). You can add more lists by doing the following:\n• create directory in the same place as webui\n• put text files in it with a relevant description on each line\n\nFor example of what text files to use, see https://github.com/pharmapsychotic/clip-interrogator/tree/main/clip_interrogator/data. In fact, you can just take files from there and use them - just skip artists.txt because you already have a list of artists in (or use that too, who's going to stop you). Each file adds one line of text to the final description. If you add \".top3.\" to filename, for example, , the three most relevant lines from this file will be added to the prompt (other numbers also work).\n\nThere are settings relevant to this feature:\n• - do not unload Interrogate models from memory after using them. For users with a lot of VRAM.\n• - adds artist from when interrogating. Can be useful to disable when you have your list of artists in directory\n• - parameter that affects how detailed descriptions from BLIP model are (the first part of generated prompt)\n• - interrogator will only consider this many first lines in a file. Set to 0, the default is 1500, which is about as much as a 4GB videocard can handle.\n\nPrompt editing allows you to start sampling one picture, but in the middle swap to something else. The base syntax for this is:\n\nWhere and are arbitrary texts, and is a number that defines how late in the sampling cycle should the switch be made. The later it is, the less power the model has to draw the text in place of text. If is a number between 0 and 1, it's a fraction of the number of steps after which to make the switch. If it's an integer greater than zero, it's just the step after which to make the switch.\n\nNesting one prompt editing inside another does work.\n• - adds to the prompt after a fixed number of steps ( )\n• - removes from the prompt after a fixed number of steps ( )\n• At start, the model will be drawing .\n• After step 16, it will switch to drawing , continuing from where it stopped with fantasy.\n\nHere's a more complex example with multiple edits: fantasy landscape with a [mountain:lake:0.25] and [an oak:a christmas tree:0.75][ in foreground::0.6][ in background:0.25] [shoddy:masterful:0.5] (sampler has 100 steps)\n• at start, fantasy landscape with a mountain and an oak in foreground shoddy\n• after step 25, fantasy landscape with a lake and an oak in foreground in background shoddy\n• after step 50, fantasy landscape with a lake and an oak in foreground in background masterful\n• after step 60, fantasy landscape with a lake and an oak in background masterful\n• after step 75, fantasy landscape with a lake and a christmas tree in background masterful\n\nThe picture at the top was made with the prompt:\n\nOfficial portrait of a smiling world war ii general, [male:female:0.99], cheerful, happy, detailed face, 20th century, highly detailed, cinematic lighting, digital art painting by Greg Rutkowski\n\nAnd the number 0.99 is replaced with whatever you see in column labels on the image.\n\nThe last column in the picture is [male:female:0.0], which essentially means that you are asking the model to draw a female from the start, without starting with a male general, and that is why it looks so different from others.\n\nNote: This syntax does not work with extra networks, such as LoRA. See this discussion post for details. For similar functionality, see the sd-webui-loractl extension.\n\nConvenient Syntax for swapping every other step.\n\nOn step 1, prompt is \"cow in a field.\" Step 2 is \"horse in a field.\" Step 3 is \"cow in a field\" and so on.\n\nSee more advanced example below. On step 8, the chain loops back from \"man\" to \"cow.\"\n\nPrompt editing was first implemented by Doggettx in this reddit post.\n\nNote: This syntax does not work with extra networks, such as LoRA. See this discussion post for details. For similar functionality, see the sd-webui-loractl extension.\n\nA convenience option to partially render your image at a lower resolution, upscale it, and then add details at a high resolution. In other words, this is equivalent to generating an image in txt2img, upscaling it via a method of your choice, and running a second pass on the now upscaled image in img2img to further refine the upscale and create the final result.\n\nBy default, SD1/2 based models create horrible images at very high resolutions, as these models were only trained at 512px or 768px. This method makes it possible to avoid this issue by utilizing the small picture's composition in the denoising process of the larger version. Enabled by checking the \"Hires. fix\" checkbox on the txt2img page.\n\nSmall picture is rendered at whatever resolution you set using width/height sliders. Large picture's dimensions are controlled by three sliders: \"Scale by\" multiplier (Hires upscale), \"Resize width to\" and/or \"Resize height to\" (Hires resize).\n• If \"Resize width to\" and \"Resize height to\" are 0, \"Scale by\" is used.\n• If \"Resize width to\" is 0, \"Resize height to\" is calculated from width and height.\n• If \"Resize height to\" is 0, \"Resize width to\" is calculated from width and height.\n• If both \"Resize width to\" and \"Resize height to\" are non-zero, image is upscaled to be at least those dimensions, and some parts are cropped.\n\nIn older versions of the webui, the final width and height were input manually (the last option listed above). In new versions, the default is to use the \"Scale by\" factor, which is the default and preferred.\n\nA dropdown allows you to to select the kind of upscaler to use for resizing the image. In addition to all upscalers you have available on extras tab, there is an option to upscale a latent space image, which is what stable diffusion works with internally - for a 3x512x512 RGB image, its latent space representation would be 4x64x64. To see what each latent space upscaler does, you can set Denoising strength to 0 and Hires steps to 1 - you'll get a very good approximation of what stable diffusion would be working with on upscaled image.\n\nBelow are examples of how different latent upscale modes look.\n\nAntialiased variations were PRd in by a contributor and seem to be the same as non-antialiased.\n\nA method to allow the combination of multiple prompts. combine prompts using an uppercase AND\n\nSupports weights for prompts: The default weight value is 1. It can be quite useful for combining multiple embeddings to your result: creature_embedding in the woods:0.7 AND arcane_embedding:0.5 AND glitch_embedding:0.2\n\nUsing a value lower than 0.1 will barely have an effect. will produce basically the same output as\n\nThis could be handy for generating fine-tuned recursive variations, by continuing to append more prompts to your total. creature_embedding on log AND frog:0.13 AND yellow eyes:0.08\n\nOptimizations for GPUs with low VRAM. This should make it possible to generate 512x512 images on videocards with 4GB memory.\n\nis a reimplementation of an optimization idea by basujindal. Model is separated into modules, and only one module is kept in GPU memory; when another module needs to run, the previous is removed from GPU memory. The nature of this optimization makes the processing run slower -- about 10 times slower compared to normal operation on my RTX 3090.\n\nis another optimization that should reduce VRAM usage significantly by not processing conditional and unconditional denoising in the same batch.\n\nThis implementation of optimization does not require any modification to the original Stable Diffusion code.\n\nLets you improve faces in pictures using either GFPGAN or CodeFormer. There is a checkbox in every tab to use face restoration, and also a separate tab that just allows you to use face restoration on any picture, with a slider that controls how visible the effect is. You can choose between the two methods in settings.\n\nFull guide with other info is here: https://imgur.com/a/VjFi5uM\n\nClick the Save button under the output section, and generated images will be saved to a directory specified in settings; generation parameters will be appended to a csv file in the same directory.\n\nGradio's loading graphic has a very negative effect on the processing speed of the neural network. My RTX 3090 makes images about 10% faster when the tab with gradio is not active. By default, the UI now hides loading progress animation and replaces it with static \"Loading...\" text, which achieves the same effect. Use the commandline option to revert this and show loading animations.\n\nIf you want faster swapping between models, increase the counter in settings. Webui will keep models you've swapped from in ram.\n\nMake sure you set the appropriate number according to your remaining available ram.\n\nStable Diffusion has a limit for input text length. If your prompt is too long, you will get a warning in the text output field, showing which parts of your text were truncated and ignored by the model.\n\nAdds information about generation parameters to PNG as a text chunk. You can view this information later using any software that supports viewing PNG chunk info, for example: https://www.nayuki.io/page/png-file-chunk-inspector\n\nA tab with settings, allows you to use UI to edit more than half of parameters that previously were commandline. Settings are saved to . Settings that remain as commandline options are ones that are required at startup.\n\nThe field in the Settings tab allows customization of generated txt2img and img2img images filenames. This pattern defines the generation parameters you want to include in filenames and their order. The supported tags are:\n\nThis list will evolve though, with new additions. You can get an up-to-date list of supported tags by hovering your mouse over the \"Images filename pattern\" label in the UI.\n\nNote about \"prompt\" tags: will add underscores between the prompt words, while will keep the prompt intact (easier to copy/paste into the UI again). is a simplified and cleaned-up version of your prompt, already used to generate subdirectories names, with only the words of your prompt (no punctuation).\n\nIf you leave this field empty, the default pattern will be applied ( ).\n\nPlease note that the tags are actually replaced inside the pattern. It means that you can also add non-tags words to this pattern, to make filenames even more explicit. For example:\n\nIf the program is launched with option, an extra text input field for script code is available at the bottom of the page, under Scripts -> Custom code. It allows you to input python code that will do the work with the image.\n\nIn code, access parameters from web UI using the variable, and provide outputs for web UI using the function. All globals from the script are also accessible.\n\nA simple script that would just process the image and output it normally:\n\nYou can change parameters for UI elements in , it is created automatically when the program first starts. Some options:\n\nCheckboxes that would usually expand a hidden section will not initially do so when set as UI config entries.\n\nIt's possible to use ESRGAN models on the Extras tab, as well as in SD upscale. Paper here.\n\nTo use ESRGAN models, put them into ESRGAN directory in the same location as webui.py. A file will be loaded as a model if it has .pth extension. Grab models from the Model Database.\n\nNot all models from the database are supported. All 2x models are most likely not supported.\n\nDeconstructs an input image using a reverse of the Euler diffuser to create the noise pattern used to construct the input prompt.\n\nAs an example, you can use this image. Select the img2img alternative test from the scripts section.\n\nAdjust your settings for the reconstruction process:\n• Use a brief description of the scene: \"A smiling woman with brown hair.\" Describing features you want to change helps. Set this as your starting prompt, and 'Original Input Prompt' in the script settings.\n• You MUST use the Euler sampling method, as this script is built on it.\n• Sampling steps: 50-60. This MUCH match the decode steps value in the script, or you'll have a bad time. Use 50 for this demo.\n• CFG scale: 2 or lower. For this demo, use 1.8. (Hint, you can edit ui-config.json to change \"img2img/CFG Scale/step\" to .1 instead of .5.\n• Denoising strength - this does matter, contrary to what the old docs said. Set it to 1.\n• Width/Height - Use the width/height of the input image.\n• Seed...you can ignore this. The reverse Euler is generating the noise for the image now.\n• Decode cfg scale - Somewhere lower than 1 is the sweet spot. For the demo, use 1.\n• Decode steps - as mentioned above, this should match your sampling steps. 50 for the demo, consider increasing to 60 for more detailed images.\n\nOnce all of the above are dialed in, you should be able to hit \"Generate\" and get back a result that is a very close approximation to the original.\n\nAfter validating that the script is re-generating the source photo with a good degree of accuracy, you can try to change the details of the prompt. Larger variations of the original will likely result in an image with an entirely different composition than the source.\n\nExample outputs using the above settings and prompts below (Red hair/pony not pictured)\n\n\"A smiling woman with blue hair.\" Works. \"A frowning woman with brown hair.\" Works. \"A frowning woman with red hair.\" Works. \"A frowning woman with red hair riding a horse.\" Seems to replace the woman entirely, and now we have a ginger pony.\n\nCreate a file named near and put custom CSS code into it. For example, this makes the gallery taller:\n\nA useful tip is you can append to your webui url to enable a built in dark theme \n\ne.g. ( )\n\nAlternatively, you can add the to the in \n\n e.g.\n\nIf an audio file named is present in webui's root folder, it will be played when the generation process completes.\n\nThis is a slider in settings, and it controls how early the processing of prompt by CLIP network should be stopped.\n\nCLIP is a very advanced neural network that transforms your prompt text into a numerical representation. Neural networks work very well with this numerical representation and that's why devs of SD chose CLIP as one of 3 models involved in stable diffusion's method of producing images. As CLIP is a neural network, it means that it has a lot of layers. Your prompt is digitized in a simple way, and then fed through layers. You get numerical representation of the prompt after the 1st layer, you feed that into the second layer, you feed the result of that into third, etc, until you get to the last layer, and that's the output of CLIP that is used in stable diffusion. This is the slider value of 1. But you can stop early, and use the output of the next to last layer - that's slider value of 2. The earlier you stop, the less layers of neural network have worked on the prompt.\n\nSome models were trained with this kind of tweak, so setting this value helps produce better results on those models.\n\nAdds additional noise from the random seed, determined by the setting, defaulting to . Implemented in #12564, available in settings under -> . As noted in the UI, it should always be lower than the denoising strength used to yield the best results.\n\nOne purpose for this tweak is to add back additional detail into hires fix. For a very simplified understanding, you may think of it as a cross between GAN upscaling and latent upscaling.\n\nThe below example is of a 512x512 image with hires fix applied, using a GAN upscaler (4x-UltraSharp), at a denoising strength of 0.45. The image on the right utilizes this extra noise tweak.\n\nNote that the previous setting implemented at the time ( ) technically achieves the same effect, but as noted in the name only applies to img2img (not hires fix), and due to how that parameter functions it is very sensitive, realisticly only useful in a range of to . For almost all operations it would be suggested to use the new parameter instead."
    },
    {
        "link": "https://stable-diffusion-art.com/automatic1111",
        "document": "Stable Diffusion WebUI (AUTOMATIC1111 or A1111 for short) is the de facto GUI for advanced users. Thanks to the passionate community, most new features come to this free Stable Diffusion GUI first. But it is not the easiest software to use. Documentation is lacking. The extensive list of features it offers can be intimidating.\n\nThis guide will teach you how to use AUTOTMATIC1111 GUI. You can use it as a tutorial. There are plenty of examples you can follow step-by-step.\n\nYou can also use this guide as a reference manual. Skip through it and see what is there. Come back to it when you actually need to use a feature.\n\nYou will see many examples to demonstrate the effect of a setting because I believe this is the only way to make it clear.\n\nYou can use Stable Diffusion WebUI on Windows, Mac, or Google Colab.\n\nRead the Quick Start Guide to decide which Stable Diffusion to use.\n\nCheck out some useful extensions for beginners.\n\nYou will see the txt2img tab when you first start the GUI. This tab does the most basic function of Stable Diffusion: turning a text prompt into images.\n\nThese are the settings you may want to change if this is your first time using AUTOMATIC1111.\n\nStable Diffusion Checkpoint: Select the model you want to use. First-time users can use the v1.5 base model.\n\nPrompt: Describe what you want to see in the images. Below is an example. See the complete guide for prompt building for a tutorial.\n\nWidth and height: The size of the output image. You should set at least one side to 512 pixels when using a v1 model. For example, set the width to 512 and the height to 768 for a portrait image with a 2:3 aspect ratio.\n\nBatch size: Number of images to be generated each time. You want to generate at least a few when testing a prompt because each one will differ.\n\nFinally, hit the Generate button. After a short wait, you will get your images!\n\nBy default, you will get an additional image of composite thumbnails.\n\nYou can save an image to your local storage. First, select the image using the thumbnails below the main image canvas. Right-click the image to bring up the context menu. You should have options to save the image or copy the image to the clipboard.\n\nThat’s all you need to know for the basics! The rest of this section explains each function in more detail.\n\nStable Diffusion checkpoint is a dropdown menu for selecting models. You need to put model files in the folder > > . See more about installing models.\n\nThe refresh button next to the dropdown menu is for refreshing the list of models. It is used when you have just put a new model in the model folder and wish to update the list.\n\nPrompt text box: Put what you want to see in the images. Be detailed and specific. Use some try-and-true keywords. You can find a short list here or a more extensive list in the prompt generator.\n\nNegative Prompt text box: Put what you don’t want to see. You should use a negative prompt when using v2 models. You can use a universal negative prompt. See this article for details.\n\nSampling method: The algorithm for the denoising process. I use DPM++ 2M Karras because it balances speed and quality well. See this section for more details. You may want to avoid any ancestral samplers (The ones with an a) because their images are unstable even at large sampling steps. This made tweaking the image difficult.\n\nSampling steps: Number of sampling steps for the denoising process. The more the better, but it also takes longer. 25 steps work for most cases.\n\nWidth and height: The size of the output image. You should set at least one side to 512 pixels for v1 models. For example, set the width to 512 and the height to 768 for a portrait image with a 2:3 aspect ratio. Set at least one side to 768 when using the v2-768px model.\n\nBatch count: Number of times you run the image generation pipeline.\n\nBatch size: Number of images to generate each time you run the pipeline.\n\nThe total number of images generated equals the batch count times the batch size. You would usually change the batch size because it is faster. You will only change the batch count if you run into memory issues.\n\nCFG scale: Classifier Free Guidance scale is a parameter to control how much the model should respect your prompt.\n\n1 – Mostly ignore your prompt.\n\n3 – Be more creative.\n\n7 – A good balance between following the prompt and freedom.\n\n15 – Adhere more to the prompt.\n\n30 – Strictly follow the prompt.\n\nThe images below show the effect of changing CFG with fixed seed values. You don’t want to set CFG values too high or too low. Stable Diffusion will ignore your prompt if the CFG value is too low. The color of the images will be saturated when it is too high.\n\nSeed: The seed value used to generate the initial random tensor in the latent space. Practically, it controls the content of the image. Each image generated has its own seed value. AUTOMATIC1111 will use a random seed value if it is set to -1.\n\nA common reason to fix the seed is to fix the content of an image and tweak the prompt. Let’s say I generated an image using the following prompt.\n\nI like this image and want to tweak the prompt to add bracelets to her wrists. You will set the seed to the value of this image. The seed value is in the log message below the image canvas.\n\nCopy this value to the seed value input box. Or use the recycle button to copy the seed value.\n\nNow add the term “bracelet” to the prompt\n\nYou get a similar picture with bracelets on her wrists.\n\nThe scene could completely change because some keywords are strong enough to alter the composition. You may experiment with swapping in a keyword at a later sampling step.\n\nUse the dice icon to set the seed back to -1 (random).\n\nChecking the Extra option will reveal the Extra Seed menu.\n\nVariation seed: An additional seed value you want to use.\n\nVariation strength: Degree of interpolation between the seed and the variation seed. Setting it to 0 uses the seed value. Setting it to 1 uses the variation seed value.\n\nHere’s an example. Let’s say you have generated 2 images from the same prompt and settings. They have their own seed values, 1 and 3.\n\nYou want to generate a blend of these two images. You would set the seed to 1, the variation seed to 3, and adjust the variation strength between 0 and 1. In the experiment below, variation strength allows you to produce a transition of image content between the two seeds. The girl’s pose and background change gradually when the variation strength increases from 0 to 1.\n\nResize seed from width/height: Images will change dramatically if you change the image size, even if you use the same seed. This setting tries to fix the content of the image when resizing the image. You will put the new size in width and height sliders and the width and height of the original image here. Put the original seed value in the seed input box. Set variation strength to 0 to ignore the variation seed.\n\nLet’s say you like this image, which is 512×800 with a seed value of 3.\n\nThe composition will change drastically when you change the image size, even when keeping the same seed value.\n\nYou will get something much closer to the original one with the new size when you turn on the resize seed from height and width settings. They are not perfectly identical, but they are close.\n\nRestore faces applies an additional model trained for restoring defects on faces. Below are before and after examples.\n\nYou must specify which face restoration model to use before using Restore Faces. First, visit the Settings tab. Navigate to the Face restoration section. Select a face restoration model. CodeFormer is a good choice. Set CodeFormer weight to 0 for maximal effect. Remember to click the Apply settings button to save the settings!\n\nGo back to the txt2img tab. Check Restore Faces. The face restoration model will be applied to every image you generate.\n\nYou may want to turn off face restoration if you find that the application affects the style on the faces. Alternatively, you can increase the CodeFormer weight parameter to reduce the effect.\n\nYou can use Stable Diffusion WebUI to create a repeating pattern like a wallpaper.\n\nNote: The Tiling checkbox is now on the Settings page.\n\nUse the Tiling option to produce a periodic image that can be tiled. Below is an example.\n\nThis image can be tiled like wallpaper.\n\nThe true treasure of using Stable Diffusion is allowing you to create tiles of any images, not just traditional patterns. All you need is to come up with a text prompt.\n\nThe high-resolution fix option applies an upsacler to enlarge your image. You need this because the native resolution of Stable Diffusion is 512 pixels (or 768 pixels for certain v2 models). The image is too small for many usages.\n\nWhy can’t you just set the width and height to higher, like 1024 pixels? Deviating from the native resolution would affect compositions and create problems like generating images with two heads.\n\nSo, you must first generate a small image of 512 pixels on either side. Then scale it up to a bigger one.\n\nUpscaler: Choose an upscaler to use. See this article for a primer.\n\nThe various Latent upscaler options scale the image in the latent space. It is done after the sampling steps of the text-to-image generation. The process is similar to image-to-image.\n\nOther options are a mix of traditional and AI upscalers. See the AI upscaler article for details.\n\nHires steps: Only applicable to latent upscalers. It is the number of sampling steps after upscaling the latent image.\n\nDenoising strength: Only applicable to latent upscalers. This parameter has the same meaning as in image-to-image. It controls the noise added to the latent image before performing the Hires sampling steps.\n\nNow, let’s look at the effect of upscaling the image below to 2x, using latent as the upscaler.\n\nFor some reason, it must be larger than 0.5 to get a sharp image. Setting it too high will change the image a lot.\n\nThe benefit of using a latent upscaler is the lack of upscaling artifacts other upscalers like ESRGAN may introduce. The decoder of Stable Diffusion produces the image, ensuring the style is consistent. The drawback is it would change the images to some extent, depending on the value of denoising strength.\n\nThe upscale factor controls how many times larger the image will be. For example, setting it to 2 scales a 512-by-768 pixel image to 1024-by-1536 pixels.\n\nAlternatively, you can specify the values of “resize width to” and “resize height to” to set the new image size.\n\nYou can avoid the troubles of setting the correct denoising strength by using an AI upscalers like ESRGAN. In general, separating the txt2img and the upscaling into two steps gives you more flexibility. I don’t use the high-resolution fix option but use the Extra page to do upscaling instead.\n\nFrom left to right:\n• Read the last parameters: It will populate all fields so that you will generate the same images when pressing the Generate button. Note that the seed and the model override will be set. If this is not what you want, set the seed to -1 and remove the override.\n\n2. Trash icon: Delete the current prompt and the negative prompt.\n\n3. Model icon: Show extra networks. This button is for inserting hypernetworks, embeddings, and LoRA phrases into the prompt.\n\nYou can use the following two buttons to load and save a prompt and a negative prompt. The set is called a style. It can be a short phrase like an artist’s name, or it can be a full prompt.\n\n4. Load style: You can select multiple styles from the style dropdown menu below. Use this button to insert them into the prompt and the negative prompt.\n\n5. Save style: Save the prompt and the negative prompt. You will need to name the style.\n\nYou will find a row of buttons for performing various functions on the images generated. From left to right…\n\nOpen folder: Open the image output folder. It may not work for all systems.\n\nSave: Save an image. After clicking, it will show a download link below the buttons. It will save all images if you select the image grid.\n\nZip: Zip up the image(s) for download.\n\nSend to img2img: Send the selected image to the img2img tab.\n\nSend to inpainting: Send the selected image to the inpainting tab in the img2img tab.\n\nSend to extras: Send the selected image to the Extras tab.\n\nThe img2img tab is where you use the image-to-image functions. Most users would visit this tab for inpainting and turning an image into another.\n\nAn everyday use case in the img2img tab is to do… image-to-image. You can create new images that follow the composition of the base image.\n\nStep 1: Drag and drop the base image to the img2img tab on the img2img page.\n\nStep 2: Adjust width or height, so the new image has the same aspect ratio. You should see a rectangular frame in the image canvas indicating the aspect ratio. In the above landscape image, I set the width to 760 while keeping the height at 512.\n\nStep 3: Set the sampling method and sampling steps. I typically use DPM++ 2M Karass with 25 steps.\n\nStep 5: Write a prompt for the new image. I will use the following prompt.\n\nStep 6: Press the Generate button to generate images. Adjust denoising strength and repeat. Below are images with varying denoising strengths.\n\nMany settings are shared with txt2img. I am only going to explain the new ones.\n\nResize mode: If the aspect ratio of the new image is not the same as that of the input image, there are a few ways to reconcile the difference.\n• “Just resize” scales the input image to fit the new image dimension. It will stretch or squeeze the image.\n• “Crop and resize” fits the new image canvas into the input image. The parts that don’t fit are removed. The aspect ratio of the original image will be preserved.\n• “Resize and fill” fits the input image into the new image canvas. The extra part is filled with the average color of the input image. The aspect ratio will be preserved.\n• “Just resize (latent upscale)” is similar to “Just resize”, but the scaling is done in the latent space. Use denoising strength larger than 0.5 to avoid blurry images.\n\nDenoising strength: Control how much the image will change. Nothing changes if it is set to 0. New images don’t follow the input image if it is set to 1. 0.75 is a good starting point that have a good amount of changes.\n\nYou can use the built-in script poor man’s outpainting: For extending an image. See the outpainting guide.\n\nInstead of uploading an image, you can sketch the initial picture. You should enable the color sketch tool using the following argument when starting the webui. (It is already enabled in the Google Colab notebook in the Quick Start Guide)\n\nStep 1: Navigate to sketch tab on the img2img page.\n\nStep 2: Upload a background image to the canvas. You can use the black or white backgrounds below.\n\nStep 3: Sketch your creation. With color sketch tool enabled, you should be able to sketch in color.\n\nYou don’t have to draw something from scratch. You can use the sketch function to modify an image. Below is an example of removing the braids by painting them over and doing a round of image-to-image. Use the eye dropper tool to pick a color from the surrounding areas.\n\nPerhaps the most used function in the img2img tab is inpainting. You generated an image you like in the txt2img tab. But there’s a minor defect, and you want to regenerate it.\n\nLet’s say you have generated the following image in the txt2img tab. You want to regenerate the face because it is garbled. You can use the Send to inpaint button to send an image from the txt2img tab to the img2img tab.\n\nYou should see your image when switching to the Inpaint tab of the img2img page. Use the paintbrush tool to create a mask over the area to be regenerated.\n\nParameters like image sizes have been set correctly because you used the “Send to inpaint” function. You usually would adjust\n• denoising strength: Start at 0.75. Increase to change more. Decrease to change less.\n\nPress the Generate button. Pick the one you like.\n\nDo you have difficulty in inpainting a small area? Hover over the information icon in the top left corner to see keyboard shortcuts for zoom and pan.\n• Alt + Wheel / Opt + Wheel: Zoom in and out.\n• Hold F and move the cursor to pan.\n\nThese shortcuts also work in Sketch and Inpaint Sketch.\n\nInpaint sketch combines inpainting and sketch. It lets you paint like in the sketch tab but only regenerates the painted area. The unpainted area is unchanged. Below is an example.\n\nInpaint upload lets you upload a separate mask file instead of drawing it.\n\nBatch lets you inpaint or perform image-to-image for multiple images.\n\nGet prompt from an image\n\nAUTOMATIC1111’s Interogate CLIP button takes the image you upload to the img2img tab and guesses the prompt. It is useful when you want to work on images you don’t know the prompt. To get a guessed prompt from an image:\n\nStep 2: Upload an image to the img2img tab.\n\nA prompt will show up in the prompt text box.\n\nThe Interrogate DeepBooru button offers a similar function, except it is designed for anime images.\n\nYou will go to the Extra page for scaling up an image. Why do you need AUTOMATIC1111 to enlarge an image? You can use an AI upscaler that is usually unavailable on your PC. Instead of paying for an AI upscaling service, you can do it for free here.\n\nFollow these steps to upscale an image.\n\nStep 2: Upload an image to the image canvas.\n\nStep 3: Set the Scale by factor under the resize label. The new image will be this many times larger on each side. For example, a 200×400 image will become 800×1600 with a scale factor of 4.\n\nStep 5: Press Generate. You should get a new image on the right.\n\nMake sure to inspect the new image at full resolution. For example, you can open the new image in a new tab and disable auto-fit. Upscalers could produce artifacts that you might overlook if it is shrunk.\n\nEven if you don’t need 4x larger, for example, it can still enlarge it to 4x and resize it later. This could help improve sharpness.\n\nScale to: Instead of setting a scale factor, you can specify the dimensions to resize in the “scale to” tab.\n\nUpscalers: The Upscaler dropdown menu lists several built-in options. You can also install your own. See the AI upscaler article for instructions.\n\nLanczos and Nearest are old-school upscalers. They are not as powerful but the behavior is predictable.\n\nESRGAN, R-ESRGAN, ScuNet, and SwinIR are AI upscalers. They can literally make up content to increase resolution. Some are trained for a particle style. The best way to find out if they work for your image is to test them out. I may sound like a broken record now, but make sure to look at the image closely at full resolution.\n\nUpscaler 2: Sometimes, you want to combine the effect of two upscalers. This option lets you combine the results of two upscalers. The amount of blending is controlled by the Upscaler 2 Visibility slider. A higher value shows upscaler 2 more.\n\nCan’t find the upscaler you like? You can install additional upscalers from the model library. See installation instructions.\n\nYou can optionally restore faces in the upscaling process. Two options are available: (1) GFPGAN, and (2) CodeFormer. Set the visibility of either one of them to apply the correction. As a rule of thumbnail, you should set the lowest value you can get away with so that the style of the image is not affected.\n\nMany Stable Diffusion GUIs, including AUTOMATIC1111, write generation parameters to the image png file. This is a convenient function to get back the generation parameters quickly.\n\nIf AUTOMATIC1111 generates the image, you can use the Send to buttons to quickly copy the parameters to various pages.\n\nIt is useful when you find an image on the web and want to see if the prompt is left in the file.\n\nThis function could be helpful even for an image that is not generated. You can quickly send the image and its dimension to a page.\n\nTo install an extension in\n\n4. Enter the extension’s URL in the URL for extension’s git repository field.\n\n5. Wait for the confirmation message that the installation is complete.\n\n6. Restart AUTOMATIC1111. (Tips: Don’t use the Apply and Restart button. It doesn’t work sometimes. Close and Restart Stable Diffusion WebUI completely)\n\nA common question is applying a style to the AI-generated images in Stable Diffusion WebUI. There are a few ways.\n\nUsing prompts alone can achieve amazing styles, even using a base model like Stable Diffusion v1.5 or SDXL. For example, see over a hundred styles achieved using prompts with the SDXL model.\n\nIf you prefer a more automated approach to applying styles with prompts, you can use the SDXL Style Selector extension to add style keywords to your prompt.\n\nThousands of custom checkpoint models fine-tuned to generate various styles are freely available. Go find them on Civitai or Huggingface.\n\nLora, LyCORIS, embedding, and hypernetwork models are small files that modify a checkpoint model. They can be used to achieve different styles. Again, find them on Civitai or Huggingface.\n\nAUTOMATIC1111’s checkpoint merger is for combining two or more models. You can combine up to 3 models to create a new model. It is usually for mixing the styles of two or more models. However, the merge result is not guaranteed. It could sometimes produce undesirable artifacts.\n\nPrimary model (A, B, C): The input models. The merging will be done according to the formula displayed. The formula will change according to the interpolation method selected.\n• No interpolation: Use model A only. This is for file conversion or replacing the VAE.\n• Weighted sum: Merge two models A and B, with multiplier weight M applying to B. The formula is A * (1 – M) + B * M.\n• Add difference: Merge three models using the formula A + (B – C) * M.\n• safetensors: SafeTensors is a new model format developed by Hugging Face. It is safe because, unlike ckpt models, loading a Safe Tensor model won’t execute any malicious codes even if they are in the model.\n\nBake in VAE: Replace the VAE decoder with the one selected. It is for replacing the original one with a better one released by Stability.\n\nThe Train page is for training models. It currently supports textual inversion (embedding) and hypernetwork. I don’t have good luck using AUTOMATIC1111 for training, so I will not cover this section.\n\nThere is an extensive list of settings on AUTOMATIC1111’s setting page. I won’t be able to go through them individually in this article. Here are some you want to check.\n\nMake sure to click Apply settings after changing any settings.\n\nMake sure to select the default face restoration method. CodeFormer is a good one.\n\nDownload and select a VAE released by Stability to improve eyes and faces in v1 models.\n\nYou can enable custom shortcuts on the top.\n\nOn the Settings page, click Show All Pages on the left panel.\n\nSearch the word Quicksettings gets you to the Quick Setting field.\n\nThere are a lot of settings available for selection. For example, the following enables shortcuts for Clip Skip and custom image output directories.\n\nAfter saving the settings and reloading the Web-UI, you will see the new shortcuts at the top of the page.\n\nThe custom output directories come in handy for organizing the images.\n\nHere is the list of Quick settings that are useful to enable"
    },
    {
        "link": "https://huggingface.co/docs/diffusers/v0.30.1/using-diffusers/inpaint",
        "document": "Inpainting replaces or edits specific areas of an image. This makes it a useful tool for image restoration like removing defects and artifacts, or even replacing an image area with something entirely new. Inpainting relies on a mask to determine which regions of an image to fill in; the area to inpaint is represented by white pixels and the area to keep is represented by black pixels. The white pixels are filled in by the prompt.\n\nWith 🤗 Diffusers, here is how you can do inpainting:\n• Load an inpainting checkpoint with the AutoPipelineForInpainting class. This’ll automatically detect the appropriate pipeline class to load based on the checkpoint:\n• Create a prompt to inpaint the image with and pass it to the pipeline with the base and mask images:\n\nThroughout this guide, the mask image is provided in all of the code examples for convenience. You can inpaint on your own images, but you’ll need to create a mask image for it. Use the Space below to easily create a mask image.\n\nUpload a base image to inpaint on and use the sketch tool to draw a mask. Once you’re done, click Run to generate and download the mask image.\n\nThe method provides an option for how to blend the original image and inpaint area. The amount of blur is determined by the parameter. Increasing the increases the amount of blur applied to the mask edges, softening the transition between the original image and inpaint area. A low or zero preserves the sharper edges of the mask.\n\nTo use this, create a blurred mask with the image processor.\n\nStable Diffusion Inpainting, Stable Diffusion XL (SDXL) Inpainting, and Kandinsky 2.2 Inpainting are among the most popular models for inpainting. SDXL typically produces higher resolution images than Stable Diffusion v1.5, and Kandinsky 2.2 is also capable of generating high-quality images.\n\nStable Diffusion Inpainting is a latent diffusion model finetuned on 512x512 images on inpainting. It is a good starting point because it is relatively fast and generates good quality images. To use this model for inpainting, you’ll need to pass a prompt, base and mask image to the pipeline:\n\nSDXL is a larger and more powerful version of Stable Diffusion v1.5. This model can follow a two-stage model process (though each model can also be used alone); the base model generates an image, and a refiner model takes that image and further enhances its details and quality. Take a look at the SDXL guide for a more comprehensive guide on how to use SDXL and configure it’s parameters.\n\nThe Kandinsky model family is similar to SDXL because it uses two models as well; the image prior model creates image embeddings, and the diffusion model generates images from them. You can load the image prior and diffusion model separately, but the easiest way to use Kandinsky 2.2 is to load it into the AutoPipelineForInpainting class which uses the KandinskyV22InpaintCombinedPipeline under the hood.\n\nSo far, this guide has used inpaint specific checkpoints such as runwayml/stable-diffusion-inpainting. But you can also use regular checkpoints like runwayml/stable-diffusion-v1-5. Let’s compare the results of the two checkpoints.\n\nThe image on the left is generated from a regular checkpoint, and the image on the right is from an inpaint checkpoint. You’ll immediately notice the image on the left is not as clean, and you can still see the outline of the area the model is supposed to inpaint. The image on the right is much cleaner and the inpainted area appears more natural.\n\nHowever, for more basic tasks like erasing an object from an image (like the rocks in the road for example), a regular checkpoint yields pretty good results. There isn’t as noticeable of difference between the regular and inpaint checkpoint.\n\nThe trade-off of using a non-inpaint specific checkpoint is the overall image quality may be lower, but it generally tends to preserve the mask area (that is why you can see the mask outline). The inpaint specific checkpoints are intentionally trained to generate higher quality inpainted images, and that includes creating a more natural transition between the masked and unmasked areas. As a result, these checkpoints are more likely to change your unmasked area.\n\nIf preserving the unmasked area is important for your task, you can use the method to force the unmasked area of an image to remain the same at the expense of some more unnatural transitions between the masked and unmasked areas.\n\nImage features - like quality and “creativity” - are dependent on pipeline parameters. Knowing what these parameters do is important for getting the results you want. Let’s take a look at the most important parameters and see how changing them affects the output.\n\nis a measure of how much noise is added to the base image, which influences how similar the output is to the base image.\n• 📈 a high value means more noise is added to an image and the denoising process takes longer, but you’ll get higher quality images that are more different from the base image\n• 📉 a low value means less noise is added to an image and the denoising process is faster, but the image quality may not be as great and the generated image resembles the base image more\n\naffects how aligned the text prompt and generated image are.\n• 📈 a high value means the prompt and generated image are closely aligned, so the output is a stricter interpretation of the prompt\n• 📉 a low value means the prompt and generated image are more loosely aligned, so the output may be more varied from the prompt\n\nYou can use and together for more control over how expressive the model is. For example, a combination high and values gives the model the most creative freedom.\n\nA negative prompt assumes the opposite role of a prompt; it guides the model away from generating certain things in an image. This is useful for quickly improving image quality and preventing the model from generating things you don’t want.\n\nA method for increasing the inpainting image quality is to use the parameter. When enabled, this option crops the masked area with some user-specified padding and it’ll also crop the same area from the original image. Both the image and mask are upscaled to a higher resolution for inpainting, and then overlaid on the original image. This is a quick and easy way to improve image quality without using a separate pipeline like StableDiffusionUpscalePipeline.\n\nAdd the parameter to the pipeline call and set it to the desired padding value.\n\nAutoPipelineForInpainting can be chained with other 🤗 Diffusers pipelines to edit their outputs. This is often useful for improving the output quality from your other diffusion pipelines, and if you’re using multiple pipelines, it can be more memory-efficient to chain them together to keep the outputs in latent space and reuse the same pipeline components.\n\nChaining a text-to-image and inpainting pipeline allows you to inpaint the generated image, and you don’t have to provide a base image to begin with. This makes it convenient to edit your favorite text-to-image outputs without having to generate an entirely new image.\n\nStart with the text-to-image pipeline to create a castle:\n\nLoad the mask image of the output from above:\n\nAnd let’s inpaint the masked area with a waterfall:\n\nYou can also chain an inpainting pipeline before another pipeline like image-to-image or an upscaler to improve the quality.\n\nNow let’s pass the image to another inpainting pipeline with SDXL’s refiner model to enhance the image details and quality:\n\nFinally, you can pass this image to an image-to-image pipeline to put the finishing touches on it. It is more efficient to use the from_pipe() method to reuse the existing pipeline components, and avoid unnecessarily loading all the pipeline components into memory again.\n\nImage-to-image and inpainting are actually very similar tasks. Image-to-image generates a new image that resembles the existing provided image. Inpainting does the same thing, but it only transforms the image area defined by the mask and the rest of the image is unchanged. You can think of inpainting as a more precise tool for making specific changes and image-to-image has a broader scope for making more sweeping changes.\n\nGetting an image to look exactly the way you want is challenging because the denoising process is random. While you can control certain aspects of generation by configuring parameters like , there are better and more efficient methods for controlling image generation.\n\nPrompt weighting provides a quantifiable way to scale the representation of concepts in a prompt. You can use it to increase or decrease the magnitude of the text embedding vector for each concept in the prompt, which subsequently determines how much of each concept is generated. The Compel library offers an intuitive syntax for scaling the prompt weights and generating the embeddings. Learn how to create the embeddings in the Prompt weighting guide.\n\nOnce you’ve generated the embeddings, pass them to the (and if you’re using a negative prompt) parameter in the AutoPipelineForInpainting. The embeddings replace the parameter:\n\nControlNet models are used with other diffusion models like Stable Diffusion, and they provide an even more flexible and accurate way to control how an image is generated. A ControlNet accepts an additional conditioning image input that guides the diffusion model to preserve the features in it.\n\nFor example, let’s condition an image with a ControlNet pretrained on inpaint images:\n\nNow generate an image from the base, mask and control images. You’ll notice features of the base image are strongly preserved in the generated image.\n\nYou can take this a step further and chain it with an image-to-image pipeline to apply a new style:\n\nIt can be difficult and slow to run diffusion models if you’re resource constrained, but it doesn’t have to be with a few optimization tricks. One of the biggest (and easiest) optimizations you can enable is switching to memory-efficient attention. If you’re using PyTorch 2.0, scaled-dot product attention is automatically enabled and you don’t need to do anything else. For non-PyTorch 2.0 users, you can install and use xFormers’s implementation of memory-efficient attention. Both options reduce memory usage and accelerate inference.\n\nYou can also offload the model to the CPU to save even more memory:\n\nTo speed-up your inference code even more, use . You should wrap around the most intensive component in the pipeline which is typically the UNet:\n\nLearn more in the Reduce memory usage and Torch 2.0 guides."
    },
    {
        "link": "https://reddit.com/r/StableDiffusion/comments/17sc8jf/inpainting_a_complete_guide_stable_diffusion_art",
        "document": "/r/StableDiffusion is an unofficial community embracing the open-source material of all related. Post art, ask questions, create discussions, contribute new tech, or browse the subreddit. It’s up to you."
    },
    {
        "link": "https://stable-diffusion-art.com/inpainting_basics",
        "document": "No matter how good your prompt and model are, it is rare to get a perfect image in one shot.\n\nInpainting is an indispensable way to fix small defects. In this post, I will go through a few basic examples to use inpainting for fixing defects.\n\nIf you are new to AI images, you may want to read the beginner’s guide first.\n\nThis is part 3 of the beginner’s guide series.\n\nRead part 1: Absolute beginner’s guide.\n\nRead part 2: Prompt building.\n\nRead part 4: Models.\n\nWe will use Stable Diffusion AI and AUTOMATIC1111 GUI. See my quick start guide for setting up in Google’s cloud server.\n\nIn this section, I will show you step-by-step how to use inpainting to fix small defects.\n\nI will use an original image from the Lonely Palace prompt:\n\nIt’s a fine image but I would like to fix the following issues\n• The right arm is missing.\n\nDo you know there is a Stable Diffusion model trained for inpainting? You can use it if you want to get the best result. But usually, it’s OK to use the same model you generated the image with for inpainting.\n\nTo install the v1.5 inpainting model, download the model checkpoint file and put it in the folder\n\nIn AUTOMATIC1111, press the refresh icon next to the checkpoint selection dropbox at the top left. Select to enable the model.\n\nIn AUTOMATIC1111 GUI, Select the img2img tab and select the Inpaint sub-tab. Upload the image to the inpainting canvas.\n\nWe will inpaint both the right arm and the face at the same time. Use the paintbrush tool to create a mask. This is the area you want Stable Diffusion to regenerate the image.\n\nYou can reuse the original prompt for fixing defects. This is like generating multiple images but only in a particular area.\n\nThe image size needs to be adjusted to be the same as the original image. (704 x 512 in this case).\n\nIf you are inpainting faces, you can turn on restore faces. You will also need to select and apply the face restoration model to be used in the Settings tab. CodeFormer is a good one.\n\nCaution that this option may generate unnatural looks. It may also generate something inconsistent with the style of the model.\n\nThe next important setting is Mask Content.\n\nSelect original if you want the result guided by the color and shape of the original content. Original is often used when inpainting faces because the general shape and anatomy were ok. We just want it to look a bit different.\n\nIn most cases, you will use Original and change denoising strength to achieve different effects.\n\nYou can use latent noise or latent nothing if you want to regenerate something completely different from the original, for example removing a limb or hiding a hand. These options initialize the masked area with something other than the original image. It will produce something completely different.\n\nDenoising strength controls how much change it will make compared with the original image. Nothing will change when you set it to 0. You will get an unrelated inpainting when you set it to 1.\n\n0.75 is usually a good starting point. Decrease if you want to change less.\n\nMake sure to generate a few images at a time so that you can choose the best ones. Set the seed to -1 so that every image is different.\n\nBelow are some of the inpainted images.\n\nOne more round of inpainting\n\nI like the last one but there’s an extra hand under the newly inpainted arm. Follow similar steps of uploading this image and creating a mask. Masked content must be set to latent noise to generate something completely different.\n\nThe hand under the arm is removed with the second round of inpainting:\n\nAnd this is my final image.\n\nInpainting is an iterative process. You can apply it as many times as you want to refine an image.\n\nSee this post for another more extreme example of inpainting.\n\nSee the tutorial for removing extra limbs with inpainting.\n\nSometimes you want to add something new to the image.\n\nLet’s try adding a hand fan to the picture.\n\nFirst, upload the image to the inpainting canvas and create a mask around the chest and right arm.\n\nAdd the prompt “holding a hand fan” to the beginning of the original prompt. The prompt for inpainting is\n\nAdding new objects to the original prompt ensures consistency in style. You can adjust the keyword weight (1.2 above) to make the fan show.\n\nAdjust denoising strength and CFG scale to fine-tune the inpainted images.\n\nAfter some experimentation, our mission is accomplished:\n\nDenoising strength controls how much respect the final image should pay to the original content. Setting it to 0 changes nothing. Setting to 1 you got an unrelated image.\n\nSet to a low value if you want small change and a high value if you want big change.\n\nSimilar to usage in text-to-image, the Classifier Free Guidance scale is a parameter to control how much the model should respect your prompt.\n\n1 – Mostly ignore your prompt.\n\n3 – Be more creative.\n\n7 – A good balance between following the prompt and freedom.\n\n15 – Adhere more to the prompt.\n\n30 – Strictly follow the prompt.\n\nMasked content controls how the masked area is initialized.\n• Fill: Initialize with a highly blurred of the original image.\n• Latent noise: Masked area initialized with fill and random noise is added to the latent space.\n• Latent nothing: Like latent noise except no noise is added to the latent space.\n\nBelow are the initial mask content before any sampling steps. This gives you some idea of what they are.\n\nSuccessful inpainting requires patience and skill. Here are some take homes for using inpainting\n• Keep masked content at Original and adjust denoising strength works 90% of the time.\n• Play with masked content to see which one works the best.\n• If nothing works well within AUTOMATIC1111’s settings, use photo editing software like Photoshop or GIMP to paint the area of interest with the rough shape and color you wanted. Upload that image and inpaint with original content."
    },
    {
        "link": "https://medium.com/data-science-at-microsoft/introduction-to-image-inpainting-with-a-practical-example-from-the-e-commerce-industry-f81ae6635d5e",
        "document": "LaMa is popular, state-of-the-art, image inpainting system invented and described in a research paper published in 2021 by Roman Suvorov et al.\n\nEven though several years have passed and some newer methods (outperforming LaMa in some benchmarks) have been introduced in this field, LaMa remains the most popular and most broadly used approach for image inpainting tasks. If you want to see why, you can try it for yourself by visiting this web app.\n\nLaMa owes its popularity to several factors:\n• It was a breakthrough in image inpainting tasks by leveraging Fast Fourier Convolutions (FFCs), which allow for a larger effective receptive field that covers an entire image even in the early layers of the network.\n• It can be trained on low-resolution images but generalizes well to high-resolution images (never seen during training).\n• The overall model, while powerful, is rather lightweight and unlike most of the other SOTA models, it’s a single-stage, end-to-end convolutional, Generative Adversarial Network (GAN) with superb inference speed (even when running on CPU).\n• The research paper was published with (mostly) well documented source code, examples, and instructions on how to use it, which made it popular in open-source community.\n\nSince we already mentioned that LaMa is a Generative Adversarial Network, it’s also worth mentioning that more recent image inpainting solutions are based on Stable Diffusion. Although this line of models is much more powerful than GANs, it requires more data and more computing power to train it or even use it for inferencing. The key advantage of Stable Diffusion is the ability to control generated output through textual prompts or commands provided by the user.\n\nIn the following section we cover the details of LaMa model and the novelties it introduced. Although useful and interesting, these details are not required for building solutions with LaMa, so if you’re only interested in practical applications, feel free to skip to the “Using LaMa” section below.\n\nLooking at LaMa from a high-level perspective, it’s easy to fall into a false assumption that simply replacing convolutional layers with Fast Fourier Convolutions (FFCs) is the sole reason why this model performs so well. However, according to comments provided by the authors in their paper, there’s much more to it. More precisely, there are three major components that contribute to its success:\n\nInterestingly, according to a YouTube interview with the authors, Suvorov et al. started the research by coming up with the last component in the process of doing some initial experiments with pix2pix models and hypothesizing that in order to improve the performance, they needed to use larger masks during training. After further experiments, they realized that classic convolutional layers were the bottleneck and so they required something with a wider receptive field, something that would understand global features and patterns in images. That’s where Fast Fourier Convolutions came into play.\n\nThe next sections explain details of each of these three key components of the LaMa model. I cover these in an order that in my perspective best aligns with how the research and thought process may have looked to the authors and how intermediate findings led to some of the key architectural decisions and final outcomes.\n\nAs with most Deep Learning models, we need large amounts of data to train models for image inpainting tasks. Some of the most popular datasets in this space include Places365 (about 10 million images) or ImageNet (about 14 million images). Luckily, because the goal of this task is to fill in pixels for missing (masked) parts of the image, the core training strategy for most solutions is focused on self-supervised learning. This means that instead of manually annotating masks for training, we provide only the original images and masks are generated online based on the chosen policy. This approach makes fine-tuning LaMa models to our data much easier and cheaper (because labeling the data can be costly too!).\n\nIn the case of LaMa, masks for inpainting tasks are automatically generated based on the mask generation policy. Early in their research, the authors came up with a hypothesis that by introducing larger masks during training, they could pave the path to high-performing models.\n\nIn their experiments the authors tested out a few different mask generation policies (see Figure 4) and ended up going with a 50/50 split between “Large masks wide” and “Large masks box”. This strategy not only showed better results in general, but it also generalized well to tests run against “Narrow masks”. As hinted at by the authors, it “[…] suggests that increasing diversity of the masks might be beneficial for various inpainting systems.”\n\nLastly, it’s worth mentioning that training masks are generated online during training time, but the test dataset masks are generated offline before training and used as a fixed benchmark.\n\nThe need for wide receptive field\n\nAccording to Suvorov et al., using large masks during training is required to unlock high performance capabilities of models for image inpainting tasks, but this also introduces a significant challenge. More specifically, the task of inpainting large areas of missing pixels is much harder than with small or narrow areas. With large masks, it’s much more likely for the generator model to get stuck in a position inside the mask while lacking the local context to correctly predict missing pixels. To overcome this, it needs to be able to look outside of the mask, at global features and patterns — more precisely, it needs to have a wide effective receptive field.\n\nThe authors argue that conventional, fully convolutional models such as ResNet “suffer from slow growth of effective receptive field.” This is because classic convolutional layers use small kernels (e.g., 3x3 pixels), so in early layers these kernels pick up very low-level features that lack global context. To access global context, we need a deep enough network — but even then, these features are only available at later layers, as visualized in the figure below.\n\nIn their paper, Suvorov et al. briefly mention that to replace convolutional layers they successfully experimented with dilated convolutions (a comprehensive, high-level overview of different convolutions can be found here).\n\nDilated convolutions worked better for this task because when the convolutional kernel is looking at inputs, the dilation makes it skip d–1 (where d is the dilation parameter) pixels and thus it enlarges the receptive field without increasing the number of parameters (see Figure 7). A big assumption here is that by skipping some pixels, we don’t lose too much of the information from the input, especially when dealing with high-resolution images.\n\nEventually, the authors proposed an even better alternative: A transformation in the frequency domain by using Fourier transform.\n\nTypically, in computer vision, images are processed in the spatial domain, meaning by working directly on pixel values and their positions relative to each other to search for some emerging patterns and shapes. Fourier transform is a mathematical transform that decomposes a function into its frequencies — you can think of it as just another way of representing the image. The big idea behind using it in Deep Learning is that by switching from a spatial to frequency domain, we look at the rate of change of pixel values (Figure 8) and thus we can pick up global features and patterns.\n\nTo cite the authors: “The use of fast Fourier Convolutions has two important benefits: they give the inpainting model access to the image-wide global context right away and capture periodic structures often found in images (e.g. brick walls, ladders, etc).”\n\nFast Fourier Convolution is a novel convolutional operator, proposed as a replacement for classical convolutional layers in a research paper introduced at NeurIPS 2020. As the name implies, it leverages a Fourier transform to extract global features from a frequency domain. However, looking at the diagram of a standard FFC block (see Figure 10) reveals that it’s a mix of local features (extracted through standard convolutional layers) added to global features (from a frequency domain).\n\nThe key component in capturing global features is the Spectral Transform block where we apply channel-wise FFT (Fast Fourier Transform), perform a 1x1 convolution in the frequency domain, and finally recover spatial structure with inversed FFT.\n\nOne very interesting, positive side effect of using FFCs is that even if the model was trained on small images (e.g., 256x256) it generalizes very well to much larger images during inferencing.\n\nAs the authors mention, “[…] interestingly, sharing the same convolutions across all frequencies shifts the model towards scale equivariance.” The intuition behind this is that by applying the same convolutional filters to all frequencies in Fourier domain, the network does a better job at capturing scale-independent features.\n\nHow does FFC fit into the LaMa architecture?\n\nThe inpainting network of the LaMa model is similar to U-NET–like architecture with downsampling and upsampling arms and the core part between them with a sequence of FFC Residual Blocks (up to 18 in the Big LaMa model).\n\nBased on the source code and insights from the paper I created the architecture diagram below (see Figure 12) to help myself grasp how the generator network works and how data flow looks end to end. In my visualization, I skipped some of the layers that don’t directly affect the shape of outputs such as batch normalization or ReLU activation functions. The input of the network is an RGB image of 256x256 size stacked with a binary mask representing areas marked for inpainting task that give the final input shape of (4, 256, 256). The output layer of this network is a sigmoid activation function that creates final predictions for missing pixel values across RGB channels.\n\nAs you can see, the image inpainting portion of the LaMa model is beautiful in its simplicity. It’s a single stage network without any additional feature generation steps or dependencies required. It’s not using any dense layers and relies only on convolutions (both for downsampling and upsampling), which makes it a fully convolutional neural network able to take in variable input sizes.\n\nThe image inpainting task is an ambiguous and difficult problem to evaluate because there are many potentially correct outputs for the same missing area, but in the ground truth there’s always only one correct output — the original image before applying the mask. Especially when we’re dealing with large masks, and thus large areas to fill, there is a nearly infinite number of plausible fillings.\n\nTo train an image inpainting model, you need a good loss function to steer the training in the right direction and your loss function needs to take the considerations below into account while evaluating outputs from your generator:\n• Naive approaches, based on comparing pixel-level information, do not work well with the variability of plausible fillings.\n• Ensuring that generated areas fit into the global structure of the overall image is key.\n• Local details are important as well — you need to make sure your fillings are correct at a local level\n\nTo deal with the complex nature of this problem, Suvorov et al. proposed an equally complex loss function, which in fact is a weighted sum of several losses, addressing separately the problems I listed above, and which can be expressed with following formula:\n\nAs seen above, the final loss function consists of:\n• L HRFPL — a novel variation of perceptual loss based on a high receptive field pretrained network. The sole purpose of this loss is to focus on high-level patterns and global features.\n• L DiscPL — discriminator-based perceptual loss is a variant of perceptual loss where the discriminator is used as the base network for features extraction. According to the authors, “it is known to stabilize training, and in some cases slightly improves the performance.”\n• R1 gradient penalty is a regularization technique that helps maintain stability in the GAN training process by preventing the discriminator from making drastic and incorrect decisions that could disrupt the equilibrium.\n\nOne fun fact is that in his YouTube interview, when asked about future research, Suvorov said that even though this loss function worked, it would be his top priority to come up with a single, simplified loss function that would work equally well or even better.\n\nThe first component we look at is a perceptual loss, which in principle compares features-level distance between predicted output versus ground truth. Because it analyzes features instead of final pixel values, the authors argue that this loss function allows for variations in outputs because it does not require exact reconstruction.\n\nThe general idea behind perceptual loss is fairly simple:\n• Prepare your ground truth and predicted images and run them through a base pretrained network (e.g., some ImageNet pretrained ResNet model).\n• Collect feature maps from various convolutional layers in the network for each input image.\n• Calculate differences between predicted and target feature maps, which may include some inter/intra-layer mean operations to get the final distance value.\n\nThere are two important factors that the authors added to the perceptual loss function:\n• They argue that the task on which the pretrained network was trained influences the final features and what they focus on. If your base pretrained network was trained with a classification task, it will focus more on the textures, while segmentation task networks focus more on high-level information about objects and their parts. That’s why the authors selected the latter as their pretrained base.\n• As we previously discussed, with large masks, the focus is shifted toward global features and structures. To address this, you need a base network with a fast-growing receptive field. The authors introduce High Receptive Field Perceptual Loss (HRFPL), which replaces the standard convolutional layers with dilated convolutions (using FFC is also an option) to enlarge the receptive field.\n\nThe final distance or loss value between predicted and target image is achieved in two steps:\n• Calculating the mean value of element-wise subtraction of the predicted and target image feature maps at different layers of the network.\n• Calculating the inter-layer mean for these values.\n\nTo ensure their inpainting model generates natural-looking local details, the authors used adversarial loss by defining an additional network (discriminator), which works at a local level and discriminates between “real” and “fake” patches. Thanks to HRFPL, the inpainting network quickly learns to copy unmasked parts of the image, so we label them as “real” for adversarial loss. We only label masked areas as “fake”, so that the discriminator loss focuses only on these patches. Lastly, the authors used a non-saturating adversarial loss, which maximizes the probability of images being predicted as real.\n\nIn previous sections, we discussed key components of the LaMa model. By now you should have a good intuition of what contributes to its high performance and popularity within the community, why certain approaches were taken, how they work “under the hood,” and understand their impact on the final solution. In the next section, we cover the practical applications of LaMa, using it to solve a hypothetical (but real) problem in the e-commerce industry."
    },
    {
        "link": "https://medium.com/the-research-nest/how-to-create-fancy-artistic-text-effects-using-stable-diffusion-1857169f8c5d",
        "document": "How to create fancy artistic text effects using Stable Diffusion\n\nBefore we get started, I assume that you are already familiar with the following;\n• You know how to run and use various features of Stable Diffusion, locally or on Google Colab.\n• You know how to debug stuff when things don’t work correctly.\n• You have decent experience in programming and understand how to read/write code.\n• You already know all the terminology related to Stable Diffusion.\n\nThat said, I will focus only on the critical aspects of a possible algorithm that can get things done for us. I won’t be focusing on the environment setup, installations, etc. This is not a beginner-level tutorial. However, you may casually read to understand the kind of stuff possible with Stable Diffusion.\n\nRecently, Adobe revealed their new suite of Generative AI called Firefly. One of the applications was to create stylized text like the above. Can we create similar effects using Stable Diffusion?\n\nWhen I tried to input the same prompt directly to SD, here’s the result I got.\n\nStable Diffusion has never been good with texts. One possible solution is to prompt engineer to get the desired effect. For example, you could prompt objects in the shape of the alphabet as a proxy to them and get creative with it. Figuring out the right prompt that works can be tricky. Is there any other method that’s a lot more precise and controllable?\n\nThere is a straightforward approach to try.\n• Use the stable diffusion inpainting model to paint the parts of the image filled with text with whatever we imagine.\n\nFirst, I created a few helper functions, create_image, create_mask, and create_text.\n\nimport torch\n\nfrom diffusers import StableDiffusionInpaintPipeline, DPMSolverMultistepScheduler\n\nfrom PIL import Image, ImageDraw, ImageFont\n\nimport numpy as np\n\n\n\npipe = StableDiffusionInpaintPipeline.from_pretrained(\n\n \"stabilityai/stable-diffusion-2-inpainting\", torch_dtype=torch.float16\n\n)\n\n\n\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n\npipe = pipe.to(\"cuda\")\n\n\n\n# Returns a PIL image of text in black at the center of a 512*512 white background\n\n# Note: Fontsize need to be changed for words, letters of different length\n\n# # Single letters, font_size = 500, 5 letter words, font_size = 150\n\ndef create_text(text, font_size):\n\n font = ImageFont.truetype(\"arialbd.ttf\", font_size)\n\n\n\n # Create a new image with a white background\n\n image = Image.new(\"RGB\", (512, 512), (255, 255, 255))\n\n\n\n # Get the size of the text and calculate its position in the center of the image\n\n text_size = font.getbbox(text)\n\n text_x = (image.width - text_size[2]) / 2\n\n text_y = (image.height - text_size[3]) / 2\n\n\n\n # A method to create thicker text on the image\n\n # Define the number of shadow layers to create and the offset distance\n\n num_layers = 5\n\n offset = 2\n\n\n\n # Draw the text onto the image multiple times with a slight offset to create a shadow effect\n\n draw = ImageDraw.Draw(image)\n\n for i in range(num_layers):\n\n x = text_x + (i * offset)\n\n y = text_y + (i * offset)\n\n draw.text((x, y), text, font=font, fill=(0, 0, 0))\n\n\n\n # Draw the final text layer on top of the shadows\n\n draw.text((text_x, text_y), text, font=font, fill=(0, 0, 0))\n\n\n\n return image\n\n\n\n# Returns a PIL image of the mask of the given image\n\n# param = \"letter\" masks the letters to inpaint\n\n# paraam = \"background\" masks the background\n\ndef create_mask(image, param):\n\n # Convert the image to grayscale\n\n gray_image = image.convert('L')\n\n\n\n # Convert the grayscale image to a numpy array\n\n gray_array = np.array(gray_image)\n\n\n\n # Threshold the array to create a binary mask\n\n threshold = 128\n\n\n\n # letter -> letter is painted white\n\n # background -> background is painted white\n\n # All the white area is used for inpainting\n\n if param == 'letter':\n\n mask_array = np.where(gray_array > threshold, 0, 255).astype(np.uint8)\n\n if param == 'background':\n\n mask_array = np.where(gray_array > threshold, 255, 0).astype(np.uint8)\n\n \n\n # Convert the mask array back to a PIL image\n\n mask_image = Image.fromarray(mask_array)\n\n\n\n return mask_image\n\n\n\n# Sometimes, outputs are detected as NSFW (false positive). This function disabled the NSFW filter\n\ndef safety_checker(images, clip_input):\n\n return images, False\n\n\n\n# Saves and returns a PIL image generated by Stable Diffusion\n\ndef create_image(prompt, image, mask, file_name):\n\n #image and mask_image should be PIL images.\n\n #The mask structure is white for inpainting and black for keeping as is\n\n pipe.safety_checker = safety_checker\n\n output = pipe(\n\n prompt=prompt, \n\n image=image, \n\n mask_image=mask, \n\n num_inference_steps=50).images[0]\n\n output.save(file_name + \".png\")\n\n return output\n\nLet us test with the Stable Diffusion 2 inpainting model. Let’s create a few alphabets and see what we can get.\n\nHere are the outputs with both prompts.\n\nSo, it works! However, it doesn’t give good results all the time. Sometimes, it outputs just a blank image for some reason. I am unsure what kind of prompt structure works best in this context.\n\nNot bad. I want to try one final experiment for this blog post.\n\nWhat if we do inpainting twice? Once for the letter design and once for the empty background. Can we effectively create a coherent picture with text on it?\n\nTo do this, I simply create two masks, one for the letters and one for the background. Then, perform inpainting twice respectively.\n\nI tried a few more prompts and combinations. Sometimes it gives a good result, but often it makes incomplete or blank images. I think the thickness of the font and mask needs to be improved further.\n\nAfter many different iterations, this is probably one of the best pics I got. However, I think with the right set of prompts, you can get much better results.\n• How are the results when newer or more fine-tuned inpainting models are used?\n• Can we create more complex images with text with the help of object localization and detection? Think, “An image of a signpost in a beautiful forest that says Hello.” To do this, we can create a background image of a beautiful forest with an empty signpost. We can then create a text image with PIL superimposing the coordinates of the signpost. Those coordinates can be obtained by object detection. We can then create a mask of the text in that image and perform an inpainting as required. What kind of results can we get?\n• To create such images in more resolutions and dimensions.\n• To use colors to direct the model better. Will it work? Instead of creating text in black, what if we use colors relevant to the prompt? Will that have an impact while inpainting?\n• Is it possible to create better results by integrating with any other model?\n\nI may try making the second and fourth points here in a future tutorial. #StayTuned"
    },
    {
        "link": "https://replicate.com/docs/guides/stable-diffusion/inpainting",
        "document": "Inpainting is like an AI-powered erasing and painting tool.\n\nIt can be used to:\n• replace or change existing objects in an image\n• expand the canvas of an image (outpainting)\n\nIt is similar to image-to-image.\n\nYou can use the 'prompt strength' parameter to change how much the starting image guides the area being inpainted.\n\nA value of 0.65 will keep the same colors and some of the structure that was there before, but will also generate new content based on the prompt.\n\nSetting prompt strength to 1 will completely ignore the original image and only generate new content.\n\nIn the example below, you can see the difference – the cat is no longer dependent on the white-ish color of the dog – it is now much more orange as specified in the prompt.\n\nIn this guide, we've covered how to use inpainting to remove unwanted objects from an image, replace or change existing objects in an image, and fix ugly or broken parts of a previously generated image. But you can also use it to expand the canvas of an image. Check out the outpainting guide to learn more about that process.\n\nOutpainting expands the canvas of an image. Consider a portrait photo where the top of the head is cropped out. Outpainting can be used to generate the missing part of the head.\n\nIt’s very similar to inpainting. But instead of generating a region within an existing image, the model generates a region outside of it. Learn more about outpainting."
    },
    {
        "link": "https://nightcafe.studio/blogs/info/inpainting-with-stable-diffusion-what-it-is-and-how-to-get-great-results",
        "document": "Inpainting With Stable Diffusion: What it Is and How to Get Great Results\n\nThe use of artificial intelligence (AI) tools to generate images and other pieces of art has proved to be quite effective and progressive. AI-generated images are becoming quite the norm as more professional artists and amateurs continue to use the emerging AI art generators to advance their work and creativity.\n\nOne of the popular AI art generation tools today is Stable Diffusion. This diffusion model is helping artists to generate photorealistic images that are identical to original paintings done by hand. But although Stable Diffusion can generate high-quality images, it doesn’t always give you the exact results you desire.\n\nFor instance, it may not generate images with the precise style of hair or fingers you desire. This calls for the need to modify your images and fine-tune them as you do with photos taken with a camera. Thankfully, there are numerous image modification approaches you can take to enhance the quality of your images.\n\nFor instance, you can use DreamBooth with Stable Diffusion to enhance its training so that it can generate better images. Furthermore, Stable Diffusion offers an inpainting function that allows you to modify your images before you render and generate them. So, if you want to use Stable Diffusion effectively, you’ll want to understand what inpainting stands for and how you can use it to get the best results.\n\nWhat Is Inpainting in Stable Diffusion?\n\nIn Stable Diffusion, inpainting is an indispensable way of fixing minor defects in your outputs (AI images). No matter how progressive your AI image generator is, it won’t always give you perfect images. Therefore, you’ll need the inpainting function to fix the imperfections and fill in the missing details of your images.\n\nThe main aim of implementing the inpainting function, when generating images with Stable Diffusion, is to hide any traces of image restoration. Many artists use this method to remove undesired elements from their images and to restore damaged parts of historical photos. Although Stable Diffusion’s inpainting function is a fairly new concept, it is yielding some promising effects.\n\nThis technique can also be used on images generated using the Deepfloyd AI art generator and other deep-learning models. With Stable Diffusion, the process of inpainting is quite straightforward. Just click on the Stable Diffusion ‘Inpainting’ button and choose the ‘Upload Image’ option. Once the image is successfully uploaded, erase the parts of your image you want to replace or modify.\n\nThen, key in the text prompts you want to use to fix the image in the prompt bar provided. Then click on the ‘Run’ option and wait for the modifications to be implemented. Repeat this process with modified text prompts until you get the kind of image you need.\n\nWhat Does Stable Diffusion Inpainting Do?\n\nPreviously, artists used inpainting to restore and reconstruct old and damaged photos by removing visible cracks, dust spots, scratches, and other blemishes from the photos. But with the advancing AI image generation technology, Stable Diffusion inpainting has become useful in many other ways with artists using it to achieve more.\n\nFor example, this inpainting function allows you to not only restore the missing element of your images but also render new elements in different parts of an image. With this image modification capability, you’re only limited by your imagination. This feature offers a wide variety of erasing brushes to help you remove unwanted parts of your images.\n\nThese brushes come in different shapes and sizes to meet the needs of every artist. You just need to select your preferred brush, adjust its size, and drag it over the areas you want to erase content from your image. It works the same way as an ordinary graphics editor. However, you have to ensure that the entire element you want to erase from your AI image is completely erased.\n\nLeaving a few pixels of the unwanted components of the image will misguide your Stable Diffusion platform to generate more unwanted pieces. Once you’ve erased everything you wish to replace, and keyed in the right prompt to introduce the new elements of the image, the next important step is to render the new elements you’ve added to your image.\n\nThis involves moving the ‘Generation Frame’ to cover the erased areas. While doing this, make sure the ‘Inpaint/Outpaint’ option is selected. In the prompt bar, describe the changes you want to make to your image and click on the ‘Generate’ button. The AI Editor in Stable Diffusion will provide you with four images to choose from.\n\nAt this point, you can choose the image that contains all the elements you need and continue editing it for better quality, or cancel the whole modification process and start afresh. Lastly, make sure the rendering process captures everything you want to have on your image."
    }
]