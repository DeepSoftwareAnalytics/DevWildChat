[
    {
        "link": "https://en.wikipedia.org/wiki/C_date_and_time_functions",
        "document": "The C date and time functions are a group of functions in the standard library of the C programming language implementing date and time manipulation operations.[1] They provide support for time acquisition, conversion between date formats, and formatted output to strings.\n\nThe format string used in traces back to at least PWB/UNIX 1.0, released in 1977. Its system command includes various formatting options.[2][3] In 1989, the ANSI C standard is released including and other date and time functions.[4]\n\nThe C date and time operations are defined in the header file ( header in C++).\n\nThe and related types were originally proposed by Markus Kuhn to provide a variety of time bases, but only was accepted.[6] The functionalities were, however, added to C++ in 2020 in std::chrono.\n\nThe following C source code prints the current time to the standard output stream."
    },
    {
        "link": "https://codingunit.com/c-tutorial-how-to-use-time-and-date-in-c",
        "document": "In today’s C programming language tutorial we take a look at how to use time and date from C programs.\n\n To make use of time and date function you need to include the time.h header file from the standard C library.\n\nThis header file contains functions and macros that provide standardized access to time and date. It also contains functions to manipulate and format the time and date output. The time.h reference page contains more information about the different time and date functions.\n\nThe function time() returns the type time_t. The time that is returned represents the number of seconds elapsed since 00:00 hours, Jan 1, 1970 UTC. It’s also called UNIX EPOCH time.\n\nIt is widely used not only on Unix-like operating systems but also in many other computing systems.\n\nFun note: On February 13, 2009 at exactly 23:31:30 (UTC), UNIX time was equal to ‘1234567890’.\n\nLet’s take a look at a source code example:\n\nMicrosoft Windows is also different time for different things. Normally we only place programs that will compile on UNIX, Linux and Windows, but this time we make an exception and show you also a Windows example.\n\nWindows has it’s own SYSTEMTIME structure which stores information on the date and time. For instance: Year, Month, Day, Hour, Minutes, etc. The format of this structure looks as follows:\n\nTake a look at the following example that uses GetSystemTime function to get the UTC time:\n\nNote: the windows.h include, this program will not work on other platforms. Another thing we need to warn you about is that the program doesn’t respond to daylight saving time issues.\n\nThe Unix EPOCH time is not readable for humans. To get a human-readable version of the current local time you can use the ctime() function. The function returns a C string containing the date and time information.\n\nThis string is followed by a new-line character (‘\n\n’) and it will convert the time_t object pointed by the timer to a C string containing a human-readable version of the corresponding local time and date.\n\nPlease note: that the functions ctime() and asctime() share the array which holds the time string.\n\n If either one of these functions is called, the content of the array is overwritten.\n\nAn example of ctime() use:\n\nThe string that is returned will have the following format:\n\nIt is also possible to manipulate the time structure and to create your own time using mktime().\n\nLet’s look at an example (note that the example will not change your system, it only prints a new time.):\n\nNote: that you can set anything you want for the hour, min and sec as long as they don’t cause a new day to occur.\n\nAs you can see we can also manipulate the time into the future. If you look at str_time.tm_hour and the output you will see that they don’t correspond (10 and 11). This is because of the daylight saving time set on our computer. If you also have this then you should try the following: set the str_time.tm_mon to 0 to see the effect of daylight saving time. The result will be that the output now will be 10.\n\nThe function difftime() is a very useful function, because it can be used to measure the performance time of a certain part of code. For instance in our example we measure the time of a loop (that is doing nothing at all.)\n\n Take a look at the example:\n\nIt is also possible to work with different timeszones by using gmtime() to convert calendar time to UTC.\n\n If you know the UTC time you can add CET, for example Amsterdam +1 hour. Take a look at the following example:\n\nIt is also possible to use clock ticks elapsed since the start of the program in your own programs by using the function clock(). For instance you can build a wait function or use it in your frame per second (FPS) function.\n\nTake a look at the following wait example:\n\nAs you can see there are many ways to use time and dates in your programs. You never know when it time to use time functions in your programs, so learn them or at least play with them by making some example programs of your own. Also take a look at our C language calendar tutorial for a more advance use of the things explained in this tutorial."
    },
    {
        "link": "https://stackoverflow.com/questions/1442116/how-can-i-get-the-date-and-time-values-in-a-c-program",
        "document": "You can use the WinAPI to get the date and time, this method is specific to Windows, but if you are targeting Windows only, or are already using the WinAPI then this is definitely a possibility1:\n\nYou can get both the time and date by using the . You also need to call one of two functions (either or ) to fill out the struct.\n\nwill give you the time and date specific to your time zone.\n\nwill give you the time and date in UTC.\n\nThe has the following members:\n\nYou then need to just access the struct in the regular way.\n\nThe output will be something like this:\n\nAll the WinAPI documentation (most already listed above):\n\nAn extremely good beginners tutorial on this subject by ZetCode:\n\nSimple operations with datetime on The Code Project:\n\n1: As mentioned in the comments in Ori Osherov's answer (\" Given that OP started with date +%F, they're almost certainly not using Windows. – melpomene Sep 9 at 22:17 \") the OP is not using Windows. However, since this question doesn't have any platform-specific tag (nor does it mention anywhere that the answer should be for that particular system), and is one of the top results when googling \"get time in c\" both answers belong here, some users searching for an answer to this question may be on Windows and therefore will be useful to them."
    },
    {
        "link": "http://catb.org/esr/time-programming",
        "document": "JavaScript must be enabled in your browser to display the table of contents.\n\nInternally, Unix time is represented as SI (Système International) seconds since midnight of January 1st 1970 at the Greenwich meridian, without leap-second correction. This is time counted in seconds as though it had been incremented every second in constant-length days of 86400 seconds each since then. This is sometimes described as \"UTC time\" or \"UTC seconds\" because it is based on the same zero meridian and uses the same SI second as Coordinated Universal Time (abbreviated UTC to be neutral between this and the French 'Temps Universel Coordonné' ), the modern standard [UTC]. The detailed history of UTC and its antecedents is out of scope for this document (a summary is as [TIMESCALES]), but a few points about it are relevant. One is that leap seconds were not introduced into UTC until 1972. Another is that UTC was then and still is occasionally confused with a previous international time standard, Greenwich Mean Time (GMT) [GMT]. This is why one of the principal Unix time functions is named . There is one subtle but important difference between Unix UTC and the official UTC standard time. Official UTC time is by definition solar calendar time (year/month/day/hour/minute/second) rather than a seconds counter from an epoch like Unix UTC time. The difference becomes, as we shall shortly see, significant near leap seconds. But the relationship with solar UTC time remains close, and in the rest of this document we shall continue to speak of Unix UTC. In U.S. military usage dating back to WWII GMT/UTC is referred to as \"Zulu\" time; the word \"Zulu\" means nothing in this context but was chosen to abbreviate to \"Z\" in order to avoid colliding with any existing single-letter military timezone designation. Use of the \"Zulu\" designation spread to international aviation and has left a mark on the ISO-8601 international standard for representing calendar time and date, which uses Z as a suffix to indicate UTC rather than local time. Thus, the Unix epoch (zero second) is formally represented as 1970-01-01T00:00:00Z. The standard Unix type for holding this UTC/GMT/Zulu seconds counter is . It is integral and signed, and negative values designate seconds before 1970-01-01T00:00:00Z. The current value is returned by the function. (Note that ANSI C allows for to be a float value, but this choice seems never to have been made in a real operating system and is excluded in later POSIX revisions. There have been a few approximately Unix-like systems on which was unsigned and could not represent dates before 1970, notably QNX.) The absence of leap-second correction means that occasionally, in order to stay synchronized with solar time, the Unix time counter has to skip or duplicate a second when it crosses UTC midnight during a leap-second insertion or deletion. A very detailed explanation of these discontinuities can be found at the Wikipedia page on Unix time [UNIX-TIME]. The rationale for excluding leap-seconds can be found at [POSIX-TIME]. In practice, almost the only way these discontinuities could be an issue is if you are computing with differences in timestamps taken on opposite sides of one or more leap-second insertion or deletions. In that case actual elapsed time could be a second or more different than the result of subtracting the older Unix timestamp from the newer.\n\nDuring the evolution of the Unix time API from 1968 onwards, typical machine word lengths have changed twice - from 16 to 32 in the early 1980s and then to 64 bits, which has been typical since about 2008 and seems unlikely to change in the foreseeable future. The changes in word lengths have left some scars in the Unix API. At the very beginning was defined as a signed 32-bit quantity, so it couldn’t be held in the 18-bit registers of the PDP-7 or the 16-bit registers of the later PDP-11. This is why many of the Unix time calls take a pointer to rather than a ; by passing the address of a 32-bit span in memory the design could get around the narrowness of the register width. This interface glitch was not fixed when word lengths went to 32 bits. A more serious problem is that 32-bit Unix counters will turn over just after 2038-01-19T03:14:07Z. This is expected to cause problems for embedded Unix systems; it is difficult to anticipate their magnitude, but we can only hope the event will be the same sort of damp squib that the Year 2000 rollover turned out to be. Modern Unix systems use a signed 64-bit . These counters will turn over approximately 292 billion years from now, at 15:30:08 on Sunday, 4 December 292,277,026,596. No problems with this are presently anticipated. Register length limits have also affected the representation of time at subsecond precision. As a workaround against them, and to avoid floating-point roundoff and comparison issues, the C API traditionally avoided representing fractional-second times as a scalar float or double quantity. The reason had to do with the precision offered by different float formats: When the Unix time API first had to represent subsecond precision, microsecond resolution was required to represent times comparable to a machine cycle. The problem was that a microsecond count requires 20 bits. A microsecond-precision time with 32 bits of integer part is on the far edge of what a double-precision float can hold: That would barely have fit and seemed likely to be a bit flaky in actual use due to floating point rounding. Doing any math with it would lose precision quickly. Trying to go finer-grained to nanosecond resolution would have required 11 more bits that weren’t there in double precision. Thus, quad-precision floating point would have been required for even 32-bit times. Given the high cost of FPU computation at the time and the near-waste of 64 bits of expensive storage, this took float representation out of the running. This is why fractional times are normally represented by two-element structures in which the first member is seconds since the epoch and the second is an integral offset in sub-second units - originally microseconds. The original subsecond-precision time structure was associated with the system call in 4.2BSD Unix, dating from the 1980s. It looks like this: Note the microsecond resolution. The newer POSIX time functions use this: This has nanosecond resolution. The change is related to the tremendous increase in machine speeds since the 1980s, and the correspondingly increased resolution of hardware clocks. While it is conceivable that in the future we may see further generations of these structures in which the subsecond offset is in picoseconds or smaller units, some breakthrough in fundamental physics would be required first - at time of writing in 2014 processor cycle times seem to be topping out in the roughly 0.1ns range due to quantum-mechanical limits on the construction of electron logic. Although the timeval and timespec structures are very useful for manipulating high-precision timestamps, there are unfortunately no standard functions for performing even the most basic arithmetic on them, so you’re often left to roll your own. Another structure, used for interval timers and describing a time interval with nanosecond precision, looks like this: While the C time API tends to shape the time APIs presented by higher-level languages implemented in C, these subsecond-precision structures are one area where signs of revolt have long been visible. Python has chosen to instead accept the minor problems of using a floating-point scalar representation; Ruby uses integral nanoseconds since the Unix epoch. Perl uses a mixture, a BSD-like seconds/microseconds pair in some functions and floating-point time since the Unix epoch in others. On today’s true 64-bit machines with relatively inexpensive floating point the natural float representation of time would look like this: offering sub-picosecond resolution with plenty of headroom to avoid serious roundoff issues. So the scripting languages are heading in a direction that the C API could in theory eventually follow. The Go language, designed by old Unix hands abd first released in 2009, provides a potential model for future language libraries under Unix. Describing it in detail is beyond the scope of this document (a reference can be found at [GO-TIME]) but its time representation makes an instructive contrast with C’s. Go uses 64-bit integers to count nanoseconds since January 1, year 1, 00:00:00.000000000 UTC with no leap-second correction. Every time includes a timezone field, intended to be used to compute a time zone offset through the IANA database.\n\nThe simplest portions of the Unix time API are clock and delay functions that only deal with the basic second- and fractional-second-oriented time structures. First we’ll survey the delay and interval-timing functions, then the clock functions. If you are mainly interested in clock/calendar/timezone issues, you can safely skip this section. These functions are the poster children for the mess created by having multiple time representations. All three simply delay execution of the calling program for some span of time; what differs is how that time is specified. The oldest, , takes an integer argument in seconds; takes microseconds, and takes a with nanosecond resolution. All three may be interrupted by an unignored signal. The function takes a second timespec which is filled with time remaining if it was interrupted. The call originated in BSD Unix and, though it was carried forward in POSIX, was deprecated in favor of in POSIX.1-2001/SUSv3 and then removed entirely in POSIX.1-2008/SUSv4. It’s probably best not to count on it being portable. Of these three functions, only is guaranteed not to have any interaction with signals (the other two may be implemented using some signal and a hidden handler). It is is thread- and signal-safe. The function is required by POSIX to be async-signal-safe and thread-safe. The thread-safe requirement means can’t be signal-based in multithreaded programs, but can be in single-threaded programs. These functions should not be mixed with the ones in the next group. #include <unistd.h> unsigned alarm(unsigned seconds); useconds_t ualarm(useconds_t usecs, useconds_t interval); #include <sys/time.h> int getitimer(int which, struct itimerval *value); int setitimer(int which, const struct itimerval *restrict value, struct itimerval *restrict ovalue); The alarm functions are another case of call proliferation due to multiple time units. The oldest, , sends SIGALRM after a specified number of seconds; sends SIGALRM after a specified number of microseconds (and a second argument, if nonzero, causes SIGALRM to be sent at regular intervals afterwards). The and functions come from the Single UNIX Specification before 1996, when SUSv2 was merged with POSIX.1-1996 to develop POSIX.1-2001/SUSv3. They are now deprecated in favor of the POSIX timer_* calls in the next section. These have three advantages over the traditional alarm calls: (1) they use timeval structures, so have microsecond resolution; (2) they can trigger the sending not just of SIGALRM but of more specialized profiling signals; and (3) they give you options about what derivative of the system hardware clock you want to use. The gory details about the interval timer functions are best learned from their manual pages. For our purposes, the important thing to cover is how they fit with the rest of the time and calendar API. Bluntly, that is not very well. The manual pages are full of ominous \"unspecified behavior\" warnings if you mix uses of the sleep group, the alarm group, and the itimer group in the same program. The reason for this is historical. The sleep and alarm function groups developed by accretion in early Unixes. The group was often implemented with and a specialized SIGALRM handler; likewise for and . The itimer functions were the result of a later effort to specify a more general facility that could subsume these, starting from a clean sheet of paper. They were declared obsolete in POSIX.1-2001/SUSv3 in favor of the timer_* group. Thus, it is possible that your system’s implementation of sleep and alarm functions is a thin layer over POSIX itimer calls (or, possibly, the timer_* group documented next) using the ITIMER_REAL clock. When that isn’t the case, legacy implementations of the sleep and alarm calls won’t necessarily play well with interval timers - in particular SIGALRMs might be flying around when you don’t expect it, or setting an implied signal handler through the sleep functions might be interfered with by . It’s best not to go there. Heed the warnings and don’t mix up these function groups. There is a group of POSIX functions , , , , and that can be used to set per-process interval timers even more flexibly than the itimer group. In particular, they allow setting a function-call hook to be called directly at the end of an interval without going through either an explicit or implicit signal. These functions were introduced in POSIX.1b-1993. After POSIX and SUSv2 merged in 2001 and were declared obsolete. Neither the native Linux nor POSIX man pages document interactions with the older calls. A prudent programmer should assume that the alarm and sleep group is implemented in terms of one of the two groups of newer POSIX interval-timer functions, and not mix any of them. If you want to avoid interval variability due to NTP frequency adjustments, you probably want to use these with CLOCK_MONOTONIC. These functions are Linux-specific (not standardized), offering a way to monitor timer expiration via selecting or polling on a file descriptor rather than via signal handler. They are mentioned here for completeness, but should be avoided in code intended for portability. Consult the Linux manual pages for details. A full discussion of the and system calls would be beyond the scope of this document, as they are not used for time/calendar programming. We mention them only to note two points:\n• takes a argument, reflecting its origins in BSD Unix. The resolution to which you can specify a timeout is thus limited to a microsecond.\n\nNow we come to the part that is the most interesting to application programmers and (probably) the most confusing. Unfortunately, most of the difficulties here cannot readily be solved in software. The calendaring parts are not problematic. The Unix time API supports the Gregorian calendar, which is well standardized and now in sufficiently universal use worldwide that any conversion problems to local standards can be handled locally. The real difficulties arise from timezones, especially when a program must deal with or report times from a zone other than the one configured as local on the machine where it is running. Before the mid-19th century, clocks were set (more or less haphazardly) to indicate local solar time. The goal was essentially for noon to coincide with the average time of the Sun’s maximum declination. Humans moving by foot, horse, or ship did not move quickly enough to make time synchronization between regions with different mean solar times a problem (with a very limited technical exception for marine navigation). The impetus for standard time came from rail travel. The railroads needed standardized time in order to set and publish precise schedules . While there were significant commercial advantages to adopting railroad time as a civil standard wherever it reached, human beings did not want to give up rough synchronization of their clocks and watches to local mean solar time. Time zones developed as a compromise. By partitioning the globe into hour-wide longitudinal bands with a fixed time offset from one world reference time, two desirable properties could be achieved. First, rough synchronization with mean solar time would be maintained everywhere. Second, the relationship between standard time and any local time would remain easy to compute. The actual time zone system never approximated this ideal very closely. Adoption was slow and patchy after the development of the first railroad standard time system (in 1847 in Great Britain) and was not roughly complete until the early 20th century. From the beginning, various time authorities showed a tendency to move timezone borders to avoid putting regions they were closely tied to off into a different timezone, even if that meant accepting large clock deviations from mean solar time in outlying regions. Some regions adopted half-hour or even quarter-hour offsets. These meant, unhappily for computer programs and programmers, that the time offset of a location is not a simple function of its longitude. Worse, the time zone system has been unstable over time as the political and commercial pressures on it resulted in frequent changes to the definitions of zones. And still worse than that was the partial and unstable adoption of Daylight Saving Time, aka DST, aka Summer Time, in which the time definition within a zone endures further changes, by another hour twice a year, so people get up an hour earlier and thus get an extra hour of daylight in the evening. As a final wrinkle, some jurisdictions use different timezone names when DST is in effect; this is common in the U.S. where, for example, the Eastern timezone uses Eastern Standard Time (EST) as its basic designator but Eastern Daylight Time (EDT) during summer months. But this is not necessarily true everywhere. More historical details can be found at [TIME-ZONE]. The result of this mess is the following:\n• Every computer has to be configured with (at least) a local time offset - that is, the difference between its local time and UTC.\n• In order to display and/or interpret local times from other (named) timezones, any computer that wants to do it needs a database of named-timezone-to-offset mappings and DST start/end times for each zone. Each zone may in fact require a series of such mappings spanning different historical and future ranges of UTC time; the database needs to be updated whenever a zone changes its rules.\n• There was a lot of pressure from early on in the development of software standards to abandon named timezones in computer date formats in favor of transmitting dates as local time plus the originating location’s numeric offset from meridian time, usually in the form +hhmm or -hhmm. While this still required every computer to configure the local offset (and perhaps change it twice a year for DST) it at least removed the requirement for a timezone database.\n• Past timestamps using named time zones have to be interpreted with caution, in particular because it may not be evident whether DST was in effect or not.\n• Embedded systems often evade the whole issue by wiring themselves to Zulu time no matter where they’re deployed. For an entertaining video lecture about the messy consequences of this history, see [COMPUTERPHILE]. As he says near the end, your only sane option (assuming you don’t go the everything-is-UTC route) is to trust that tzdata or whatever other timezone history your operating system subscribes to is properly maintained and then simply refuse to worry about that level of the problem, because you’d go mad if you tried. In the earliest Unixes, a machine could handle exactly one timezone other than UTC. A timezone offset and name string (actually, a pair of name strings, one for summer and one for winter time) were configured into the kernel and available to C programs. Later (by System III in 1982) it became possible to set a login session’s notion of the local time zone by modifying an environment variable named TZ and calling a function . The rules for interpreting the zone specification in the value of TZ became part of the POSIX standard. They were complex [OLDTZ] and now mainly of historical interest. We mention this history mainly because it has left remnants in the manual pages - notably of - that may confuse the uninformed reader about what is going on. V7 and older BSD Unixes had various other methods for configuring the local timezone that didn’t involve using or interpreting TZ. These have left no traces on modern Unixes. They shared a fatal flaw with POSIX TZ interpretation, which was that they weren’t designed to cope with the historical instability of the timezone system. They could not express an entire set of historical offset/DST rules in order to get past local times as well as present ones correct. On modern Unix systems the TZ variable might not be set at all, but the system default timezone can be overridden by explicitly setting TZ in any process. The timezone designator configured at boot time, or via an overriding value of TZ, is a geographical location (usually but not always an area/major-city pair) such as \"America/New_York\", or \"Europe/Vienna\" or \"Asia/Taipei\". If the designator is set via TZ it may need to be preceded by a colon; this is for backwards-compatibility with the POSIX standard, to distinguish it from an old-style timezone specification. (Not all implementations enforce this.) The location-based zone naming scheme [IANA-ZONES] is managed by IANA, the Internet Assigned Numbers Authority. It is intended to supersede (in computing, anyway) the system of customary names we’ll refer to later in this document as \"civil timezone names\". Civil timezone names such as EST/EDT are ambiguous: they have name collisions in different countries, and some civil zones have multiple names. They tend to be short (at least under Unix/Linux, though not necessarily under Windows). They may change without notice. Some of them are official (in that they appear in officially-approved legislative documents in the jurisdictions where they apply), but many of them reflect mere ad-hoc conventions. The IANA names, on the other hand, are intended to be unique, future-proof, and for use by system administrators and knowledgeable users to set the time zone they wish to apply. IANA timezone designators are looked up through a locally-installed copy of a time zone database (\"tzdata\") maintained by IANA, the Internet Assigned Numbers Authority [TZSOURCES], [TZDATA]. You may hear the name \"Arthur David Olson\" used in connection with this database; he was the founding contributor. The contents of the database entry for the designator describe the history of the zone name, zone offset and the DST start/end times (if any) for the location, as those have changed over time since 1970 . The timezone system is sufficiently chaotic in ways previously noted that some of the comments on this history make entertaining reading. The Unix time API evolved when many system designers and software standardizers were still trying to hang on to named timezones, which were thought to be more human-friendly. Internet protocols evolving around the same time, however, abandoned named timezones early. The results of this history are visible as a plethora of different date presentation formats. Thus, for example, the output of the Unix command looks like this: \"Wed Sep 24 15:32:27 EDT 2014\" (with named timezone), while a date in an SMTP mail header is more likely to look like this: \"Wed, 24 Sep 2014 15:32:27 -0400\" with an hhmm numeric offset. Here is a table duplicating this example in a number of presentation formats commonly found on Unix systems: Of these, the last is the most recent and (so far) least widely adopted presentation format; ISO-8601 was not promulgated until 1988 and RFC-3339 not published until 2002 [ISO-8601], [RFC-3339]. However, it has the best properties of any (human readability, non-ambiguity, compactness, fixed length, single token, string sort order matches time order) and we strongly recommend it. All of these presentation formats are straightforwardly constructible using the strftime(3) function which we’ll be meeting in a few paragraphs (although parsing them can be trickier). Some Unix time API functions deal in numeric zone offsets, others in named timezone and DST. Still others (generally older ones) evade the whole issue by not reporting timezone at all. We will be more specific when we discuss individual calls. Just to add to the hilarity, the IANA database, RFC-822/RFC-2822 dates, and Unix consider offsets east of Greenwich to be positive and west of it to be negative, but POSIX time-zone formats inexplicably reversed this. The representation of timestamps in electronic mail has evolved through five IETF RFCs. We also include the USENET bulletin-board system (which used email-like headers) in this sequence. The following table displays a constant date in the default format for each. The first two lines of this table is a bit spurious, as no year \"14\" has never been seen in the wild - 1914 long pre-dated email and by 2014 4-digit years were well established, having been required by RFC-1123 since 1989. RFC-822 described single-letter military timezones, but got the offsets wrong. The error was called out in RFC2822 4.3. The author has never seen these military timezone specifiers in the wild. All versions of email date stamps allow the day-of-week part (and its comma) to be omitted. Look twice and check your data sources and your work when you are parsing things that look like email dates. It is regrettably common for programs generating timestamps intended to look like these to make small errors like omitting the comma, swapping the day and month parts, or even swapping the year and time-of-day parts. Some programs doing this sort of thing will even falsely claim to be shipping RFC822-conformant timestamps; Git is a notorious example. Another source of such variation is appending a fractional-seconds part, as in \"Fri, 24 Oct 2014 19:32:27.130 +0000\". None of the email standards describe this. When there is a fractional-seconds part it is usually milliseconds, but that’s not a rule to be counted on. Unix represents calendar time using the following structure, which is consumed or emitted by several key functions: struct tm { int tm_sec; /* seconds [0,60] (60 for + leap second) */ int tm_min; /* minutes [0,59] */ int tm_hour; /* hour [0,23] */ int tm_mday; /* day of month [1,31] */ int tm_mon ; /* month of year [0,11] */ int tm_year; /* years since 1900 */ int tm_wday; /* day of week [0,6] (Sunday = 0) */ int tm_yday; /* day of year [0,365] */ int tm_isdst; /* daylight saving flag */ }; Many of the most common problems with using the Unix time API can be traced to the rather confused and inadequate design of this structure. In what is probably unintentional humor, various manual pages and standards documents refer to it as \"broken-down time\".\n• is easily misinterpreted as a flag when it’s actually three-valued: 0 = DST off, 1 = DST on, -1 = DST status unknown. (Actually, values < -1 are usually treated as -1 and values > 1 treated as 1 - but do not count on this, as buggy implementations may treat values outside this range as offsets in hours.)\n• The base value of 1900 rather than zero - easily forgotten and confusing, especially when interpreting negative values.\n• Inconsistency about 0- vs. 1-origin in month and day numbers. The number of programmers who have gotten tm_mon wrong as a result is staggering. There is a potential overflow problem with on systems where sizeof(int) < sizeof(time_t); historically this was often true on 32-bit systems and may remain a hidden problem in embedded deployments. The year derived from a can be so large that it won’t fit in an , which means and could silently fail on valid time stamps in the far past or far future. A subtler problem is that when > INT_MAX - 1900 the Gregorian year doesn’t fit in an ; a lot of code gets this wrong. Because INT_MAX on a 32-bit system is 2^31 = 2,147,483,647 this is unlikely to be a problem for normal historical dates. Some versions of BSD remedied the last and most serious problem by adding two additional fields specifying the timezone used to generate the instance: long int tm_gmtoff; /* time offset in seconds east of UTC/GMT */ const char *tm_zone; /* name of timezone */ The GNU C library, used on most open-source Unixes, supports these members. The POSIX/SUS/OpenGroup standards allow them, but do not mandate them; in a strict ISO-C environment they will not be visible. You may have to before including to make them visible. The GNU C library documentation says this: \"[The tm_gmtoff] field describes the time zone that was used to compute this broken-down time value, including any adjustment for daylight saving; it is the number of seconds that you must add to UTC to get local time. You can also think of this as the number of seconds east of UTC. For example, for U.S. Eastern Standard Time, the value is -5*60*60.\" With the offset member (but not without it) a broken-down time specification is unambiguous. The tm_zone member is less useful. The zone name strings it points to are not standardized and can be somewhat haphazard. They’re not the IANA geographic designators. Under Linux and BSD they’re typically the familiar (but ambiguous) three- and four-letter abbreviations (EDT, CEST, etc.), but under Windows they’re longer strings like \"W. Europe Standard Time\". (And they’re sometimes brain-bending monstrosities like \"GMT Daylight Time\", which is Microsoft’s own special name for what the Brits call British Summer Time.[MTZ]) #include <time.h> void tzset (void); extern char *tzname[2]; /* time zone name */ extern long timezone; /* seconds west of UTC, *not* DST-corrected */ extern int daylight; /* nonzero if DST is ever in effect here */ The call computes the timezone offset for purposes of local-time computation. This information is expressed as three global variables (you may have to set feature macros to expose them). These values are derived from system administrative settings and (if it is present) the value of the environment variable , in ways too complex to delve into here; consult the manual pages for details. The key point is that after a call, the local timezone information should be available. Note that the timezone offset does not include DST correction and is of opposite sign to the member. The variable is a pair of strings; is the zone name without and and is with DST correction. These will be civil zone names (like GMT/GST or EST/DST) not the IANA naming scheme. is required by POSIX to be thread-safe, but access to the globals it sets is intrinsically thread-unsafe. It should usually not be necessary to call directly, as it is called at the beginning of processing by most of the functions in this section. Exceptions will be noted below. This part of the Unix time interface is very old; goes back to System III, and at least one of the globals it sets traces back to V6. Later portions of the Unix API design almost completely avoided predefined global variables in favor of returns from function calls. Some early Unixes (including V7) had a a different way of fetching similar information; . This call filled in a structure with members containing information similar to the exposed globals above: struct timeb { time_t time; /* seconds since epoch */ unsigned short millitm; /* milliseconds part of current time */ short timezone; /* local time offset, minutes west of GMT */ short dstflag; /* if nonzero, local zone uses DST */ }; The Single UNIX Specification carried this forward, but implementations were weak and defective. POSIX.1-2001/SUSv3 said that the contents of the and fields are unspecified and should not be relied on. POSIX.1-2008/SUSv4 removed entirely. The archaic interface remains. Final warning: some BSD Unixes do not implement at all! These are the basic functions for making a broken-down time structure from a . The difference between the gm* pair and the local* pair is this: The local* pair computes the local time corresponding to the input Unix UTC time, taking the local time zone and all DST corrections into account. It does this by, in effect, adding the local timezone offset to the UTC seconds before performing the same computation performed by gmtime, although in a high-quality implementation it’s more complicated than that, because of the possibility that the addition could overflow.\n• Implementations that have a time zone history database, such as glibc with tzdata, apply the historical offset that was in effect at the time being converted according to the current revision of the IANA timezone history. This is normal on Unix systems. There is more detail on this behavior under the discussion of\n• Implementations that lack such a database, such as Visual Studio runtime on Windows, apply the offset that would have been in effect at the time being converted according to the rules at the time the software shipped. The gm* pair applies no such offset, so the output expresses Unix UTC seconds as UTC time. The names of the gm* pair are a historical error, since they actually convert to UTC. These systems are not actually identical; for real GMT we would incorporate the DST bit, but for UTC it must be ignored. The functions and return a pointer to internal static storage that is overwritten on each call (in the GNU C library this static storage is shared among both functions). This is not thread-safe. The *_r functions are, on the other hand, re-entrant, with the user being required to pass in the address of the storage to be modified, and can thus be thread-safe. POSIX implies that is required to behave as though has been called, but that is not. This is almost certainly because sets globals; if localtime_r(3) also set them its callers could well become thread-unsafe. For portable code should be called before - but note that this can lead to subtle errors when processing historical date/times because of the historical-date problem described under mktime(3). Now that we’ve gone through the technical details, here is a less formal but more evocative way of summarizing them: localtime(3) is a pit of horrors that wants to be your personal hell - avoid. This is especially true if you are concerned about reproducibility (e.g. in unit and regression tests) where timezone- and DST-related problems can introduce random glitches like off-by-one-hour errors with no cause in your visible code. The C11 standard [C11], in section 7.27.2.5, specifies a timespec_get(3) function that fills a struct timespec compatible with the POSIX definition, but is expected to be available on non-UNIX operating systems. According to the standard, it \"sets the interval pointed to by ts to hold the current calendar time based on the specified time base\". Hoever, it also says that with a base of , \"the tv_sec member is set to the number of seconds since an implementation defined epoch\". No base values other than are specified in the standard, but impolementations ave the option to define others. This language is more than a little confusing. The statement about TIME_UTC appears to imply interval time, a monotonic clock without skips or stutters due to leap second corrections as in the a call to with mode ; on the other hand, \"calendar time\" implies an approximation to solar time with leap-second corrections. It seems possible, even likely, that the behavior of this function is for base values other than is expected to be different depending on whether the OS uses NTP to implicitly correct to true UTC time with leap-second correction. Use with caution and read your C library’s documentation to discover what might be specified about this. The function inverts , turning an input into a . It is standardized. There is no analogous standardized function that inverts , but see the description of below. The function has a side effect that can trip you up seriously if you’re not aware of it. In addition to returning a Unix seconds value, it also modifies the contents of the . This means, in particular, that calling a second time on the same won’t necessarily return the same value as it did on the first call! The most likely error is for the second return to be 3600 seconds off because the first call set the member. \"The mktime() function modifies the fields of the tm structure as follows: tm_wday and tm_yday are set to values determined from the contents of the other fields; if structure members are outside their valid interval, they will be normalized (so that, for example, 40 October is changed into 9 November); tm_isdst is set (regardless of its initial value) to a positive value or to 0, respectively, to indicate whether DST is or is not in effect at the specified time. Calling mktime() also sets the external variable tzname with information about the current timezone.\" The last sentence is fairly subtle: it means that the implicit tzset(3) call at the beginning of mktime(3) can behave differently from an ordinary tzset(3) call, in that the implicit call can take advantage of knowing the time stamp that the caller is interested in, and can set global variables to values that are tailored for that time stamp. For example, if the time zone is Europe/London and the time stamp argument to mktime is circa 1970, mktime can set the global variables to values appropriate for a location that is at UTC+1 year round, which is what London was observing back in 1970; whereas if the time stamp is circa 2014, mktime can set the global variables to values appropriate for a location that is at UTC in winter and UTC+1 in summer, which is what London was doing in 2014. The normalizing side-effect of mktime(3) makes it useful for various kinds of date/time arithmetic on broken-down time structures. For example, you can increment a tm_day member, call mktime(3) on it, and expect overflow beyond the end of month to be handled (but beware that this will be done without adjusting for leap-second discontinuities). These are GNU C Library extensions, not standardized. They are modeled on calls introduced in BSD and still present in FreeBSD/NetBSD/OpenBSD. The function is identical to standard (except for assuming is initially negative, which does not) inverting . The function inverts . The GNU C Library documentation recommends against using these functions, as they introduce a dependency. It recommends this as a portable alternative (but note that it is thread-unsafe): #include <time.h> #include <stdlib.h> time_t my_timegm(struct tm *tm) { time_t ret; char *tz; tz = getenv(\"TZ\"); if (tz) tz = strdup(tz); setenv(\"TZ\", \"\", 1); tzset(); ret = mktime(tm); if (tz) { setenv(\"TZ\", tz, 1); free(tz); } else unsetenv(\"TZ\"); tzset(); return ret; } In older Unixes the following functions were available to report time and date as a string. They wired in bad design choices (the date string is of unpredictable length and includes a trailing \"\n\n\"), are not locale-aware, and have undefined behavior for years before 0 or after 9999. These functions are obsolete and should not be used in production code; use instead. If you are curious about the details, read their manual pages. This is the modern function for formatting timestamps into string representations; read its manual page. Here are some potentially interesting recipes that illustrate usage patterns: RFC-822/RFC-2822 specifies English month and weekday names, but %a and %b are locale-aware; thus, the third recipe will deliver interestingly nonstandard results in a non-Anglophone locale. Some of the format specifiers need to be used with caution; older implementations may not support all of them. Unit-test your formatting, checking carefully for %z and the other Single UNIX Specification and glibc additions (%C, %D, %E, %h, %n, %O, %P, %r, %t, %T, %V) if you must rely on them. If called with a manually populated struct tm with all Standard C members populated, but without the and members populated, strftime will, on some systems, not give sensible results for %z or %Z. Worse, it directly reads from the possibly-uninitialized pointer. More generally, %z and %Z for anything other than an immediate / pair can get wacky. In particular:\n• If you’re on a system without and , they’ll print the local time zone even for a gmtime(3)-initialized struct tm.\n• If you change the value of TZ between and , if you have , %Z will use a random zone for the new zone, not the abbreviation for the old one. If the new zone has fewer abbreviations than the old one, (and thus %Z) can even end up pointing into freed memory!\n• If you’re constructing struct tm instances by hand before passing them to strftime (or, for that matter, mktime/gmtime), it’s best to zero them out first, with memset or the equivalent.\n• If you can help it, only call strftime on struct tm instances that localtime/gmtime have constructed for you. If you must construct them by hand, try to limit your strftime format specifiers to the \"normal\" ones that obviously correspond to fields you have set. One unfortunate deficiency of strftime(3) is that it doesn’t have a format specifier for fractional seconds. The function is a near-inverse of function, with one painful exception; it does not parse timezones. (If only because the misdesign of the standardized leaves it no place to put the information.) The full list of specifiers not supported are %F, %g, %G, %u, %v, %z, and %Z. Do not use the %y specifier. Its interpretation differs incompatibly among implementations. Test code that uses carefully on real data. Implementations are bug-prone. The function and its re-entrant twin are complex and dubiously-designed attempts to do adaptive date parsing. We recommend against attempting to use them. See their manual pages for details."
    },
    {
        "link": "https://geeksforgeeks.org/getdate-and-setdate-function-in-c-with-examples",
        "document": "getdate() function is defined in dos.h header file. This function fills the date structure *dt with the system’s current date. Syntax\n\nParameter: This function accepts a single parameter dt which is the object of structure date.\n\nReturn Value: This method does not return anything. It just gets the system date and sets it in the specified structure.\n\nsetdate() function is defined in dos.h header file. This function sets the system date to the date in *dt. Syntax\n\nParameter: This function accepts a single parameter dt which is the object of structure date that has to be set as the system date. Return Value: This method do not returns anything. It just sets the system date as specified. Program 1: Implementation of setdate() function"
    },
    {
        "link": "https://programiz.com/c-programming/examples/leap-year",
        "document": "Learn to code solving problems and writing code with our hands-on C Programming course.\n\nLearn to code solving problems with our hands-on C Programming course!"
    },
    {
        "link": "https://stackoverflow.com/questions/71255029/leap-year-program-in-c",
        "document": "I am just starting to code in C and as a part of a tutorial assignment, I was given to write a program to take input and check if it's a leap year or not. Very basic. I pretty much wrote the code and ran it but during debugging I realized my 'year' variable was storing some garbage value. I searched on the web quite extensively and found out that my code is not wrong. But I'm surely wrong as I'm getting garbage values but not able to find out where I made a mistake. Here is the code.\n\nI am using VSCode as my IDE.\n\nPlease help me figure out what I am missing here. Thank you in advance."
    },
    {
        "link": "https://geeksforgeeks.org/c-leap-year",
        "document": "A leap year is a year that contains an additional day in February month i.e. February 29. It means that a leap year has 366 days instead of the usual 365 days. In this article, we will see the program to check for leap year in C.\n\nA leap year occurs once every four years and to check whether a year is a leap year, the following conditions should be satisfied:\n\nFor example, 2000 is a leap year but 1900 is not.\n\nAlgorithm to Check Leap Year in C"
    },
    {
        "link": "https://stackoverflow.com/questions/32550201/leap-year-c-program",
        "document": "What you need is dynamic memory allocation as you haven't got any clue how many years the user will enter.\n\nFirstly, you need a pointer to allocate memory. Since what you're going to store are s, you need a pointer to an :\n\nNow, we need a variable to keep track of how much memory we've allocated:\n\nWhy have I initialized to 10? Because we'll firstly allocate memory for 10 s.\n\nNow, allocating memory dynamically is done by a function called from . Here is how you should :\n\nWe've allocated memory for (10) s. Was the memory allocation successful? Memory allocation will be unsuccessful when returns (and is pointed to by ). So, Lets check it:\n\nNow that you've allocated memory, we can write to the allocated memory segment. But how do we know if all the memory has been used up and more needs to be reallocated? We need a new variable to keep track of that:\n\nI've initialized to 0 as nothing has been written into .\n\nNow, we can add the user input into the array just after\n\nneeds to be incremented so that we know that the first slot has been used and the next free slot is 1. But...What happens when the user enters more than 10 years? Data will be written to invalid memory locations invoking a phenomenon called Undefined Behavior(UB). The consequences of UB can be terrifying. So, it is best that you avoid it at all costs!\n\nWe need to allocate more memory when the user wishes to enter more than 10 years. (from ) is a function which you can use to allocate more memory. Just add\n\nbefore the code that writes the user input to , i.e, before\n\nOk. Now, Our program can handle more than 10 years when the user wishes to. But something is missing. Ah. We need to print the inputs after the loop. Simply add the following code\n\njust after the loop to print the results.\n\nFinally, we need to free the allocated memory so that there will not be a memory leak. Add\n\njust after the loop where you print the results, i.e, just after"
    },
    {
        "link": "https://reddit.com/r/cpp_questions/comments/lohw0d/how_to_handle_leap_year",
        "document": "I have a function that accepts a custom 'Date' struct (just an int month, int day, int year) and adds one year to the date. The 'addOneYear' function (below) converts my struct into a std::tm struct in order to create a time point (std::chrono::system_clock::time_point). Then I take that time point, add 365 days to it, and convert it back to std::tm struct and then to my custom struct to be return from the function. My question is how would I go about handling leap years? Because I can't just check to see if the current year or the next year is a leap year, I have to know if when I'm adding to my time point if I will \"pass\" the leap day.\n\nFor example, if my original date is 1/1/2019 and I add a year to it, it becomes 1/1/2020. Now 2020 is a leap year but since I haven't gotten to the leap day (2/29/2020), I just want to add 365 days like I am doing. However, if my original date is say 3/1/2019 and I add a year to it, now I am \"passing\" that leap day of 2/29/2020 and thus I want to add 366 days instead of 365 so my returned value is 3/1/2020.\n\nMaybe this is simple and I am just overthinking it but any help would be awesome, thanks!"
    }
]