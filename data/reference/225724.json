[
    {
        "link": "https://medium.com/@ali-dot-com/cracking-the-code-mastering-static-dynamic-web-scraping-with-puppeteer-in-node-js-914975d23264",
        "document": "In today’s digital world, being able to gather data from the web is essential for various tasks like market research, data analysis, and content aggregation. However, scraping data from websites, especially ones with dynamic content, can be quite challenging. In this blog, we’ll learn how to build a web scraper using Puppeteer in Node.js that can extract data from both static and dynamic websites.\n\nBefore we begin, make sure you have Node.js installed on your machine. You can download it from the official Node.js website and follow the installation instructions. Additionally, ensure that you have a basic understanding of JavaScript and web development concepts.\n\nFirst, let’s set up our project structure:\n\nWe have separate controllers for each website we want to scrape, views to display the results, and a routing mechanism to handle http requests. The extracted data is organized into directories based on content type for better management.\n\nOur project relies on the following npm packages:\n\nEnsure these dependencies are listed in your package.json file and install them using:\n\nNow, let’s dive into the implementation of scraping static and dynamic websites using puppeteer through examples.\n\nMy Project Structure looks like this:\n\nThis file serves as the entry point for our application. It sets up an Express server and defines routes for scraping data from different websites.\n\nThe routes module handles incoming requests and delegates them to the appropriate controller functions.\n\nEach controller file contains the logic for scraping data from a specific website. Let’s explore and dive deep into how the logic in controller files will help us extract data from a website.\n\nOnce the data has been scraped, we’ll organize it into separate directories based on the type of content, such as images, text, and videos. For example, extracted images from Altnews will be stored in the “altnews_images” directory, while the scraped text data will be saved in CSV format under the “text” directory. This structured approach ensures that the extracted data is easy to manage and access for further analysis or processing.\n\nFor static websites like Politifact, the scraping process is relatively straightforward. We can use Puppeteer to load the page, parse the HTML content, and extract the desired data directly. Since there are no dynamic elements to wait for, the scraping process is more deterministic and requires fewer steps compared to dynamic websites.\n\nScraping of a static website can consist of the following steps:\n\nWe use Puppeteer to launch a headless Chromium browser instance. This browser instance will be used to navigate to the static website to extract data from it.\n\nOnce the browser is launched, we navigate to the URL of the static website using Puppeteer’s page navigation methods, such as page.goto().\n\nAfter the page has loaded, we use Puppeteer’s DOM manipulation capabilities and scrolling abilities to select and extract the desired data from the webpage. This may involve identifying specific HTML elements using CSS selectors and extracting text, links, or images.\n\nFinally, we save the extracted data into a structured format, such as CSV or JSON, for further analysis or storage. We may use Node.js file system APIs (fs) to write the data to a file on the local filesystem.\n\nDynamic websites we’ll be scraping, Altnews and Mastodon, is dynamic in nature, meaning its content is loaded dynamically. Puppeteer allows us to simulate user interaction within the page, enabling us to wait for dynamic content to load fully before extracting the desired data. By leveraging Puppeteer’s APIs, we can navigate through the website, interact with elements, and extract the required information seamlessly even though it’s dynamic.\n\nScraper of above-mentioned dynamic websites can have following steps:\n\nDynamic websites, such as Mastodon, require special handling as their content is loaded asynchronously via JavaScript. We use Puppeteer’s capabilities such as page.waitForSelector() to wait for dynamic content or the dynamic element to load fully before selecting and extracting data.\n\nTo handle dynamic content, I simulated user interactions such as scrolling and waiting for specific elements to appear on the page before selecting it. We can use vanilla javascript and Puppeteer methods like page.evaluate() to accomplish this.\n\nOnce the dynamic content is loaded, we follow similar steps as scraping static websites to extract the desired data from the webpage. An example of extracting usernames from Mastadon is as follows:\n\nYou might see duplicate elements in your extartced data. The main reason behind this is the one time scrolling might not be enough to scroll to the next element yet and the element already pushed might still be on your view height after one scroll. So the scraper will get that again. To handle this I used a data structure known as “Sets”, in which you can only have unique elements.\n\nJust Like with static websites, we save the extracted data into structured formats using Node.js file system APIs. After removing duplications you can save the data just like static websites. You can also save video links in the same csv using similar techniques to that of saving names, usernames etc.\n\nExtracting All Images from Dynamic and Static Websites\n\nTo extract images from a website, various methods can be employed. One approach involves identifying image elements within the webpage’s HTML structure and then either capturing screenshots or retrieving the source URLs of these images. Another method entails making HTTP requests to the website and extracting images from the response received.\n\nBoth methods offer distinct advantages and can be chosen based on factors such as the website’s design, desired level of automation, and specific project requirements.\n\nMy project requirements included extracting all images from a website. So to achieve this I opted to use the second method and got all the images from the http response against my http request made through puppeteer.\n\nExplore the GitHub repository for my latest project and delve into the code behind it. From web scraping tools to innovative projects, you’ll find it all here: https://github.com/ali-dot-com/scrappier.\n\nAdditionally, feel free to connect with me on LinkedIn to stay updated on my latest endeavors and join in on the conversation. Let’s connect and collaborate. Linkedin\n\nBy combining Puppeteer’s powerful web scraping capabilities with Node.js, we can efficiently extract data from both static and dynamic websites. Whether we’re gathering news articles, monitoring social media trends, or conducting competitive analysis, Puppeteer provides the tools needed to build robust web scraping solutions tailored to our specific requirements."
    },
    {
        "link": "https://stackoverflow.com/questions/78747026/puppeteer-scraping-dynamic-content",
        "document": "I'm trying to scrape data from a Looker Studio web page report using Puppeteer in Node.js, but I'm encountering issues because the report is dynamic. When I fetch the data, the body is empty. Here's\n\nThe issue I'm facing is that the text is always empty, even though I can see the data when I open the URL in a browser. How can I modify my Puppeteer script to successfully scrape the dynamically loaded content from this Looker Studio report?\n\nAny help or guidance would be greatly appreciated. Thank you!\n• Using 'networkidle0' as the wait condition\n\nWhat I want to do: If you open the link, the page has a table, I am trying to fetch the rows of the table. The first few rows of the table.\n\nHowever, none of these approaches have worked. The page seems to load its content dynamically, and I'm not sure how to capture this data."
    },
    {
        "link": "https://zenrows.com/blog/puppeteer-web-scraping",
        "document": ""
    },
    {
        "link": "https://github.com/website-scraper/website-scraper-puppeteer",
        "document": "Plugin for website-scraper which returns html for dynamic websites using puppeteer.\n\nThis module is an Open Source Software maintained by one developer in free time. If you want to thank the author of this module you can use GitHub Sponsors or Patreon.\n• - (optional) - puppeteer launch options, can be found in puppeteer docs\n• - (optional) - puppeteer page.goto options, can be found in puppeteer docs\n• - (optional) - in some cases, the page needs to be scrolled down to render its assets (lazyloading). Because some pages can be really endless, the scrolldown process can be interrupted before reaching the bottom when one or both of the bellow limitations are reached:\n• - (optional) - defines whether navigation away from the page is permitted or not. If it is set to true, then the page is locked to the current url and redirects with will not pass. Defaults to\n\nIt starts Chromium in headless mode which just opens page and waits until page is loaded. It is far from ideal because probably you need to wait until some resource is loaded or click some button or log in. Currently this module doesn't support such functionality."
    },
    {
        "link": "https://serpapi.com/blog/web-scraping-dynamic-website-with-puppeteer",
        "document": "We will explore how to extract data from websites that rely on JavaScript to load their content, making them challenging to scrape with traditional methods. We will be using two powerful tools for this task: Puppeteer and Node.js.\n\nWhat is Puppeteer?\n\n\"Puppeteer is a Node.js library that provides a high-level API to control Chrome/Chromium. Puppeteer runs in headless mode by default but can be configured to run in full (\"headful\") Chrome/Chromium.\" - pptr.dev\n\n\n\nWhy use Puppeteer for web scraping?\n\nPuppeteer allows us to interact with the website just like a human would, enabling us to retrieve the data we need even if it is generated by JavaScript. We will cover different cases where we can utilize Puppeteer.\n\nSo, grab a cup of coffee, and let’s get started on learning how to scrape dynamic web content with Javascript (Node.js)!\n\nThis is a step-by-step tutorial on how to use Puppeteer in Node.js. So, make sure you already have Node.js installed.\n\nCreate a new file to start writing our code\n\nThis is the basic setup for running a browser.\n\nNow we have a browser we can play with!\n\nStep 3: Plan what actions need to be done\n\nLet's say we want to fill out and submit a form on google.com. I'll open the actual URL in my real browser and find out what HTML tag or CSS selector this element is on.\n\n\n\nThen, start to build up from there. If you want to click a button, then find out what's the button id or class.\n\nNow we already know where these HTML elements are located, we can use the puppeteer selector and action for this.\n\nCode explanation\n\n- We type 'cat' into the textarea with name=q as an attribute. (You can type anything instead of cat)\n\n- We need to wait for the input element to be ready, so we're using the method for this case. \n\n- Then run a click action on that selected button.\n\n\n\nThat's a basic example of how to use Puppeteer. Don't worry. We'll learn more actions and tips in this post.\n\nHow to do things in Puppeteer\n\nWhat can you do with Puppeteer? Well, everything you normally manually do in the browser can also be done with Puppeteer. From generating screenshots to automating form submission, it is all possible! Let's see how to do each of these actions.\n\nQuery selectors in Puppeteer are used to select DOM elements on a page, much like you would in the browser using JavaScript. They are the foundation for interacting with the web page's content, enabling you to retrieve, manipulate, and evaluate nodes in the DOM.\n\nTo select an element, we can use\n\nYou can select based on an id, class, text, x-path or even the aria label.\n\nRead more about Puppeteer selector here.\n\nHow to scroll a page with Puppeteer?\n\n\n\nScrolling to a specific section in Puppeteer can be done by selecting an element representing that section and scrolling into its view. One common method is to use the to find the DOM element and then use method to scroll the element into view.\n\nHere is an example of how you could scroll to a certain section with an :\n\nHow to scroll to the bottom\n\nYou can use the method for this.\n\nHow to fill and submit a form using Puppeteer?\n\nWe've seen this on the initial sample. We can use method to typing on the selector in the first parameter, with value in the second. The targeted element can be an input or textarea.\n\nFor ticking a checkbox or choosing a radio input, we can use\n\nHow to click a button with Puppeteer?\n\nWe can use method for clicking on a button. For example:\n\nHow to take screenshots in Puppeteer?\n\nTaking screenshots is one of the essential features that Puppeteer provides, allowing you to capture a visual snapshot of the page at any point in time. Here's how you can take a screenshot with Puppeteer:\n• Basic Screenshot:\n\nTo capture the whole page, you can use the method on the object.\n• Element Screenshot:\n\nTo capture a screenshot of a specific element, you first have to select that element and then call the method on the element handle.\n• Full Page Screenshot:\n\nIf the content of the page is longer than the viewport, set the property to to capture the entire page.\n• Screenshot settings\n\nPuppeteer allows you to define other options such as to specify the image format ( or ), (for only), and to make the background transparent (for only)\n• How to save screenshot on specific path?\n\nYou can change the value including a name. Just make sure the directory is already exists, otherwise it will throw an error.\n\nCollecting data from scraped data and export to CSV\n\nWe probably want to store the data we're collecting from web scraping. Let's learn how to collect a data and save them in CSV format so we can view it in application like Microsoft Excel or Numbers. With CSV format, we can also easily import or convert it to database engine like MySQL.\n\nFor this sample, we'll collect an organic search results from Google.\n\nGet the raw HTML data\n\nFirst, let's launch a browser that visit the Google search result, fee free to change the keyword to anything you like by adjusting the value at q=\n\nFinding Pattern\n\nWe'll get a very hard to read raw HTML data in our console. I will copy this data and paste it to a text-editor, so I can analyze where my data is located. To make it easier, you can compare the real result from an actual browser, then finding some a certain text in your raw HTML data in text-editor, so you don't have to go through each line from top to bottom.\n\n\n\nFor example, here is the real Google result from \"learn web scraping\" keyword\n\nNow, I know that one of the organic results include \"Learn Web scraping with Beautiful Soup\" text. I will then search this text on my text-editor.\n\nCollect the data programmatically\n\nSince, we already get the targeted result, we can start build up from here. See what HTML element or CSS classes this organic results in. Then, implement it in our Puppeteer code.\n\n\n\nThe selected class here could be very different from your result, that's why you need to find a pattern like the previous step mentioned.\n\nTry to run this program in your terminal, then you'll see the organic results that we need.\n\nPerform a pagination to get all the data, not just the first page\n\nGoogle has changed, it doesn't use a pagination in number anymore, but it rather detect scrolling to get more data. So we need to scroll to the bottom first for this.\n\nI'll use the code snippet from this GitHub issue. The idea is auto scroll the browser until the end of the page every X ms.\n\nTime to export the data\n\nFirst, import and modules, to create the CSV file\n\nNow, put this code after you get al the results. I change the delimiter from to semicolon since our text could include a . Other way around this, is to escape your data first.\n\nNow take a look at your directory, you'll find your generated file.\n\nPuppeteer has a lot of tools that let you do tricky things with a web browser right away. Because of this, it's easy for bad habits to sneak into your code. Read \"Avoiding Puppeteer Antipattern\" to avoid some mistakes when using Puppeteer.\n• Can we use Puppeteer in Python?\n\nNo, Puppeteer is a Node library and is designed to be used with JavaScript (or TypeScript). There are other alternatives for Python, such as and .\n• What are some Puppeteer alternatives?\n\nSelenium and Playwright are also popular choices when you need a headless browser for web scraping. Take a look at other JavaScript libraries for web scraping."
    },
    {
        "link": "https://zenrows.com/blog/dynamic-web-pages-scraping-python",
        "document": ""
    },
    {
        "link": "https://oxylabs.io/blog/dynamic-web-scraping-python",
        "document": "For static websites built with HTML and CSS, simple tools like Python’s request library and Beautiful Soup can often do the job when web scraping. However, things get tricky when dealing with advanced websites built on dynamic JavaScript frameworks like React, Angular, and Vue. These frameworks streamline web development by providing pre-built components and architecture, but they also make dynamic websites difficult to scrape. In this blog post, we’ll overview the differences between dynamic and static websites and give a step-by-step guide on scraping data from dynamic targets, particularly those featuring infinite scroll. Static pages offer limited modes of interaction, mainly allowing content viewing and basic actions like following links or logging in. Dynamic pages provide a much broader range of interaction, often adapting to user behavior. This is often made possible by employing different approaches to website rendering. With static pages, you prepare an HTML file, put it on a server, and the user will then access that fixed file. Because the file is always the same, each user will see the exact same version of the web page. Dynamic web pages, on the other hand, are not directly renderable by the web browser. Instead, when a server receives a request, it engages in intermediate actions to generate the final result on the fly. Consequently, it loads content dynamically and different users might see different versions of the same web page. We've previously discussed dynamic scraping techniques like employing Selenium, utilizing a headless browser, and making AJAX request. So, we invite you to check these resources out as well. Additionally, check out the advanced web scraping with Python guide to learn more tactics, such as emulating AJAX calls and handling infinite scroll. However, it's important to note that each target presents unique challenges. Indeed, there can be different elements that require special attention, such as pop-up windows, drop-down menus, live search suggestions, real-time updates, sortable tables, and more. One of the most encountered challenges is continuous or infinite scroll — a technique that allows content to be dynamically loaded as the user scrolls down a webpage. To scrape websites with infinite scroll, you need to customize your scraper, which is exactly what we’ll discuss below using Google Search as an example of a dynamic target. How to scrape a dynamic target using Selenium This section will go through the numbered steps to scrape dynamic sites using Selenium in Python. For our target, we will use the Google Search page with some keywords. First, make sure you have installed the latest version of Python on your system. You’ll also need to install Selenium library using the following command: Then, download the Chrome driver and ensure that the version matches your Google Chrome version. Start by creating a new Python file and import the required libraries: time from selenium webdriver from selenium webdriver chrome service Service ChromeService from bs4 BeautifulSoup Then set up Chrome Webdriver with Selenium by copying the path to your driver executable file and pasting in the following code: Following that, navigate to the Google Search Page and provide your search keyword: Now you can simulate a continuous scroll on Selenium. Using the script below, you’ll scroll multiple times to get more results and then scrape the results: Define the number times to scroll scroll_count # Simulate continuous scrolling using JavaScript _ scroll_count driver time # Wait the to load Finally, use BeautifulSoap to parse and extract the results: Run the final code as specified below to see the results: time from selenium webdriver from selenium webdriver chrome service Service ChromeService from bs4 BeautifulSoup # Set up the Chrome WebDriver driver webdriver # Navigate to Google Search search_keyword driver search_keyword # Define the number times to scroll scroll_count # Simulate continuous scrolling using JavaScript _ scroll_count driver time # Wait the to adjust needed # Get the page source after scrolling page_source driver page_source # Parse the page source BeautifulSoup soup page_source # Extract and print search results search_results soup class_ result title result h3 text link result a f f # Close the WebDriver driver The output should look something like this:\n\nOne problem with this web scraping approach is that Google might mistake you for a malicious bot, which would frequently trigger CAPTCHA. This means you'll have to integrate reliable proxies into your script and rotate them continuously. So, if you’re looking for a solution to extract data on a large scale, you might also want to consider a commercial solution that will deal with anti-bot systems for you, and we’ll cover this in the subsequent section. It’s important to note that you’re not restricted to Selenium when web scraping dynamic targets. In fact, there are alternative ways to do that: One of the best ways to scrape dynamic content is using a specialized scraper service. For example, Oxylabs Scraper API is designed for web scraping tasks and is adapted to the most popular web scraping targets. This solution leverages Oxylabs data gathering infrastructure, meaning that you don’t need to worry about IP blocks or JavaScript-rendered content, making it a valuable tool for web scraping dynamic targets. There are numerous additional libraries and tools for web scraping that can handle dynamic material in addition to Selenium. Advanced Python libraries like Scrapy paired with Splash, Requests-HTML with Pyppeteer or Playwright, Node.js with Cheerio may be among these. These libraries give users the ability to interact with dynamic elements, render JavaScript, and extract data from dynamically loaded pages. There are also some no-code scrapers designed to handle dynamic content. These tools typically offer user-friendly interfaces that allow users to select and extract specific elements from dynamic pages without the need for writing code. However, these solutions can sometimes lack flexibility and are more suitable for more basic scraping projects. As mentioned above, you can use a commercial Scraper API to scrape dynamic content. The benefit of this approach is that you won’t need to worry about having your own scraping infrastructure. Specifically, you don’t need to pass any additional parameters to deal with CAPTCHAs and IP blocks, as the tool does these things for you. What’s more, Oxylabs Scraper APIs are designed to deal with dynamically loaded content. For example, our Web Scraper API automatically detects infinite scroll and efficiently loads the requested organic results without extra parameters required. Let’s see how it works in action! Before we start scraping, make sure you have the following libraries installed: Also, to use the Web Scraper API, you need to get access to the Oxylabs API by creating an account. After doing that, you will be directed to the Oxylabs API dashboard. Head to the Users tab and create a new API user. These API user credentials will be used later in the code.\n\nAfter getting the credentials, you can start writing the code in Python. Begin by importing the required library files in your Python file: Then, create the Payload for SERP Scraper API following this structure: You can then initialize the request and get the response from the API: After receiving the response, you can get the content from the JSON results using pandas: df pd columns results response items results items_list items it df pd pd it it it it columns df columns df ignore_index True Here, we have created a dataFrame and added all the data to it. You can then convert the dataFrame and CSV and JSON files like this: That’s it! Let’s combine all the code and execute it to see the output: requests pandas pd # Structure payload payload # Get response response requests auth json payload df pd columns results response items results items_list items it df pd pd it it it it columns df columns df ignore_index True df df index False df orient index False As a result, you’ll get a CSV file which you can open in Excel:"
    },
    {
        "link": "https://medium.com/@pankaj_pandey/web-scraping-using-python-for-dynamic-web-pages-and-unveiling-hidden-insights-8dbc7da6dd26",
        "document": "In the world of data-driven decision-making, web scraping has become an indispensable tool for extracting valuable information from websites. However, traditional web scraping techniques often struggle with dynamic web pages, particularly those built using modern technologies like React. In this blog post, we will explore how Python can be harnessed to scrape data from such dynamic web pages, uncovering a wealth of insights that can drive business strategies and research efforts.\n\nIf you’re unable to view the full post and wish to read it, please use: “Read Full Post”\n\nDynamic web pages, often powered by JavaScript frameworks like React, update content dynamically without requiring a full page reload. While this provides a seamless user experience, it poses a challenge for traditional web scraping tools that rely on parsing static HTML.\n\nImagine a scenario where you want to gather real-time product prices from an e-commerce site built with React. The prices might change as users interact with the page, making manual data collection impractical. Here’s where Python comes to the rescue!"
    },
    {
        "link": "https://stackoverflow.com/questions/69326238/parsing-a-dynamically-loaded-webpage-with-selenium",
        "document": "I'm trying to parse https://www.flashscore.com/football/albania/ using Selenium in Python, but my webdriver often doesn't wait for the scores to finish loading.\n\nOccasionally, this will print out source code for a flashscore page with a blank table (i.e. the driver does not wait for the scores to finish loading). I suspect that this is because some of the live scores on the page are dynamically loaded. Is there any way to improve my wait condition?"
    },
    {
        "link": "https://serpapi.com/blog/selenium-web-scraping-python",
        "document": "Web scraping is a way to collect information from websites. However, not all websites are easy to get data from, especially dynamic websites. These websites change what they show you depending on what you do, like when you click a button or enter information. To get data from these types of websites, we can use a tool called Selenium.\n\nSelenium helps by acting like a real person browsing the website. It can click buttons, enter information, and move through pages like we do. This makes it possible to collect data from websites that change based on user's behavior.\n• Python is installed on your machine\n\nStep 2: Download WebDriver\n\nYou'll need a WebDriver for the browser you want to automate (e.g., Chrome, Firefox). For Chrome, download ChromeDriver. Make sure the WebDriver version matches your browser version. Place the WebDriver in a known directory or update the system path.\n\nStep 3: Import Selenium and Initialize WebDriver\n\nImport Selenium and initialize the WebDriver in your script.\n\nStep 4: Sample running browser\n\nOpen a website and fetch its content. Let's use as an example.\n\nPrint title\n\nHere is an example of how to get a specific element on the page.\n\nTry to run this script. You'll see a new browser pop up and open the page.\n\nFor example, I want to search for certain keyword by adding text on the search box and submit it.\n\nYou should be able to see the current URL , which means the form submission through Selenium is working.\n\nStep 6: Print content\n\nNow, you can print the content after performing a certain action on the page. For example, I want to print the table content:\n\nStep 7: Close the Browser\n\nOnce done, don't forget to close the browser:\n• Selenium can perform almost all actions that you can do manually in a browser.\n• For complex web pages, consider using explicit waits to wait for elements to load.\n\nHere is a video tutorial on YouTube using Selenium for automation in Python by NeuralNine.\n\nHow to take screenshots with Selenium?\n\nYou can take screenshots of the whole window or specific area from your code.\n\nLet's say we want to take a screenshot for the table element:\n\nYou can name the file whatever you want.\n\nScreenshot of the whole page\n\nWhile there is no method for this, we can try this by zooming out the page first\n\nHow do you add a proxy in Selenium?\n\nYou can adjust many settings to the browser that runs in Selenium, including the proxy, using the method when launching the browser or using the method.\n\nFor example, using the Chrome drive\n\nAlternatively, you can also add the method. Here is an example taken from Selenium documentation:\n\nHeadless mode in Selenium allows you to run your browser-based program without the need to display the browser's UI, making it especially useful for running tests in server environments or for continuous integration (CI) processes where no display is available.\n\nWhy use headless mode in Selenium?\n• Headless mode enables faster test execution by eliminating the need for UI rendering.\n• It facilitates automated testing in environments without a graphical user interface (GUI).\n\nHow to set headless mode in Selenium?\n\nAdd in the method before launching the browser. Here is an example code for Chrome:\n\nWhy use Selenium for Web scraping?\n\nSelenium is particularly useful for web scraping in scenarios where dynamic content and interactions are involved. Here are 10 sample use cases where Selenium might be the preferred choice:\n• Automating Form Submissions: Scraping data from a website after submitting a form, such as a search query, login credentials, or any input that leads to dynamic results.\n• Dealing with Pagination: Automatically navigating through multiple pages of search results or listings to collect comprehensive data sets.\n• Extracting Data Behind Login Walls: Logging into a website to access and scrape data that is only available to authenticated users.\n• Interacting with JavaScript Elements: Managing websites that rely heavily on JavaScript to render their content, including clicking buttons or links that load more data without refreshing the page.\n• Capturing Real-time Data: Collecting data that changes frequently, such as stock prices, weather forecasts, or live sports scores, requiring automation to refresh or interact with the page.\n• Scraping Data from Web Applications: Extracting information from complex web applications that rely on user interactions to display data, such as dashboards with customizable charts or maps.\n• Automating Browser Actions: Simulating a user's navigation through a website, including back and forward button presses, to scrape data from a user's perspective.\n• Handling Pop-ups and Modal Dialogs: Interacting with pop-ups, alerts, or confirmation dialogs that need to be accepted or closed before accessing certain page content.\n• Extracting Information from Dynamic Tables: Scraping data from tables that load dynamically or change based on user inputs, filters, or sorting options.\n• Automated Testing of Web Applications: Although not strictly a scraping use case, Selenium's ability to automate web application interactions makes it a valuable tool for testing web applications, ensuring they work as expected under various scenarios.\n\nWhat are some alternatives to Selenium?\n\nIn Python, you can try Pyppeteer, an open source program based on Javascript web scraping tool: Puppeteer.\n\nIf the website you want to scrape doesn't require interaction, you can use Beautiful Soup in Python to parse the HTML data.\n\n\n\nThat's it! I hope you enjoy reading this post!"
    },
    {
        "link": "https://crummy.com/software/BeautifulSoup/bs4/doc",
        "document": "Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. But you'll only ever have to deal with about four kinds of objects: , , , and . These objects represent the HTML elements that comprise the page. A object corresponds to an XML or HTML tag in the original document. Tags have a lot of attributes and methods, and I'll cover most of them in Navigating the tree and Searching the tree. For now, the most important methods of a tag are for accessing its name and attributes. If you change a tag's name, the change will be reflected in any markup generated by Beautiful Soup down the line: An HTML or XML tag may have any number of attributes. The tag has an attribute \"id\" whose value is \"boldest\". You can access a tag's attributes by treating the tag like a dictionary: You can access the dictionary of attributes directly as : You can add, remove, and modify a tag's attributes. Again, this is done by treating the tag as a dictionary: HTML 4 defines a few attributes that can have multiple values. HTML 5 removes a couple of them, but defines a few more. The most common multi-valued attribute is (that is, a tag can have more than one CSS class). Others include , , , , and . By default, Beautiful Soup stores the value(s) of a multi-valued attribute as a list: When you turn a tag back into a string, the values of any multi-valued attributes are consolidated: If an attribute looks like it has more than one value, but it's not a multi-valued attribute as defined by any version of the HTML standard, Beautiful Soup stores it as a simple string: You can force all attributes to be stored as strings by passing as a keyword argument into the constructor: You can use to always return the value in a list container, whether it's a string or multi-valued attribute value: If you parse a document as XML, there are no multi-valued attributes: Again, you can configure this using the argument: You probably won't need to do this, but if you do, use the defaults as a guide. They implement the rules described in the HTML specification: A tag can contain strings as pieces of text. Beautiful Soup uses the class to contain these pieces of text: A is just like a Python Unicode string, except that it also supports some of the features described in Navigating the tree and Searching the tree. You can convert a to a Unicode string with : You can't edit a string in place, but you can replace one string with another, using replace_with(): supports most of the features described in Navigating the tree and Searching the tree, but not all of them. In particular, since a string can't contain anything (the way a tag may contain a string or another tag), strings don't support the or attributes, or the method. If you want to use a outside of Beautiful Soup, you should call on it to turn it into a normal Python Unicode string. If you don't, your string will carry around a reference to the entire Beautiful Soup parse tree, even when you're done using Beautiful Soup. This is a big waste of memory. The object represents the parsed document as a whole. For most purposes, you can treat it as a object. This means it supports most of the methods described in Navigating the tree and Searching the tree. You can also pass a object into one of the methods defined in Modifying the tree, just as you would a . This lets you do things like combine two parsed documents: Since the object doesn't correspond to an actual HTML or XML tag, it has no name and no attributes. But sometimes it's useful to reference its (such as when writing code that works with both and objects), so it's been given the special \"[document]\": , , and cover almost everything you'll see in an HTML or XML file, but there are a few leftover bits. The main one you'll probably encounter is the . \"<b><!--Hey, buddy. Want to buy a used parser?--></b>\" The object is just a special type of : # 'Hey, buddy. Want to buy a used parser' But when it appears as part of an HTML document, a is displayed with special formatting: # <!--Hey, buddy. Want to buy a used parser?--> Beautiful Soup defines a few subclasses to contain strings found inside specific HTML tags. This makes it easier to pick out the main body of the page, by ignoring strings that probably represent programming directives found within the page. (These classes are new in Beautiful Soup 4.9.0, and the html5lib parser doesn't use them.) A subclass that represents embedded CSS stylesheets; that is, any strings found inside a tag during document parsing. A subclass that represents embedded Javascript; that is, any strings found inside a tag during document parsing. A subclass that represents embedded HTML templates; that is, any strings found inside a tag during document parsing. Beautiful Soup defines some classes for holding special types of strings that can be found in XML documents. Like , these classes are subclasses of that add something extra to the string on output. A subclass representing the declaration at the beginning of an XML document. A subclass representing the document type declaration which may be found near the beginning of an XML document. A subclass that represents the contents of an XML processing instruction.\n\nHere's the \"Three sisters\" HTML document again: <p class=\"story\">Once upon a time there were three little sisters; and their names were and they lived at the bottom of a well.</p> I'll use this as an example to show you how to move from one part of a document to another. Tags may contain strings and more tags. These elements are the tag's children. Beautiful Soup provides a lot of different attributes for navigating and iterating over a tag's children. Note that Beautiful Soup strings don't support any of these attributes, because a string can't have children. The simplest way to navigate the parse tree is to find a tag by name. To do this, you can use the method: For convenience, just saying the name of the tag you want is equivalent to (if no built-in attribute has that name). If you want the <head> tag, just say : You can use this trick again and again to zoom in on a certain part of the parse tree. This code gets the first <b> tag beneath the <body> tag: (and its convenience equivalent) gives you only the first tag by that name: If you need to get all the <a> tags, you can use : For more complicated tasks, such as pattern-matching and filtering, you can use the methods described in Searching the tree. A tag's children are available in a list called : The object itself has children. In this case, the <html> tag is the child of the object.: A string does not have , because it can't contain anything: Instead of getting them as a list, you can iterate over a tag's children using the generator: If you want to modify a tag's children, use the methods described in Modifying the tree. Don't modify the the list directly: that can lead to problems that are subtle and difficult to spot. The and attributes consider only a tag's direct children. For instance, the <head> tag has a single direct child—the <title> tag: But the <title> tag itself has a child: the string \"The Dormouse's story\". There's a sense in which that string is also a child of the <head> tag. The attribute lets you iterate over all of a tag's children, recursively: its direct children, the children of its direct children, and so on: The <head> tag has only one child, but it has two descendants: the <title> tag and the <title> tag's child. The object only has one direct child (the <html> tag), but it has a whole lot of descendants: If a tag has only one child, and that child is a , the child is made available as : If a tag's only child is another tag, and that tag has a , then the parent tag is considered to have the same as its child: If a tag contains more than one thing, then it's not clear what should refer to, so is defined to be : If there's more than one thing inside a tag, you can still look at just the strings. Use the generator to see all descendant strings: # 'Once upon a time there were three little sisters; and their names were\n\n' # ';\n\nand they lived at the bottom of a well.' Newlines and spaces that separate tags are also strings. You can remove extra whitespace by using the generator instead: # 'Once upon a time there were three little sisters; and their names were' # ';\n\n and they lived at the bottom of a well.' Here, strings consisting entirely of whitespace are ignored, and whitespace at the beginning and end of strings is removed. Continuing the \"family tree\" analogy, every tag and every string has a parent: the tag that contains it. You can access an element's parent with the attribute. In the example \"three sisters\" document, the <head> tag is the parent of the <title> tag: The title string itself has a parent: the <title> tag that contains it: The parent of a top-level tag like <html> is the object itself: And the of a object is defined as None: You can iterate over all of an element's parents with . This example uses to travel from an <a> tag buried deep within the document, to the very top of the document: The generator is a variant of which gives you the entire ancestry of an element, including the element itself: Consider a simple document like this: The <b> tag and the <c> tag are at the same level: they're both direct children of the same tag. We call them siblings. When a document is pretty-printed, siblings show up at the same indentation level. You can also use this relationship in the code you write. You can use and to navigate between page elements that are on the same level of the parse tree: The <b> tag has a , but no , because there's nothing before the <b> tag on the same level of the tree. For the same reason, the <c> tag has a but no : The strings \"text1\" and \"text2\" are not siblings, because they don't have the same parent: In real documents, the or of a tag will usually be a string containing whitespace. Going back to the \"three sisters\" document: You might think that the of the first <a> tag would be the second <a> tag. But actually, it's a string: the comma and newline that separate the first <a> tag from the second: The second <a> tag is then the of the comma string: You can iterate over a tag's siblings with or : # '; and they lived at the bottom of a well.' # 'Once upon a time there were three little sisters; and their names were\n\n' Take a look at the beginning of the \"three sisters\" document: An HTML parser takes this string of characters and turns it into a series of events: \"open an <html> tag\", \"open a <head> tag\", \"open a <title> tag\", \"add a string\", \"close the <title> tag\", \"open a <p> tag\", and so on. The order in which the opening tags and strings are encountered is called document order. Beautiful Soup offers tools for searching a document's elements in document order. The attribute of a string or tag points to whatever was parsed immediately after the opening of the current tag or after the current string. It might be the same as , but it's usually drastically different. Here's the final <a> tag in the \"three sisters\" document. Its is a string: the conclusion of the sentence that was interrupted by the start of the <a> tag: # ';\n\nand they lived at the bottom of a well.' But the of that <a> tag, the thing that was parsed immediately after the <a> tag, is not the rest of that sentence: it's the string \"Tillie\" inside it: That's because in the original markup, the word \"Tillie\" appeared before that semicolon. The parser encountered an <a> tag, then the word \"Tillie\", then the closing </a> tag, then the semicolon and rest of the sentence. The semicolon is on the same level as the <a> tag, but the word \"Tillie\" was encountered first. The attribute is the exact opposite of . It points to the opening tag or string that was parsed immediately before this one: You should get the idea by now. You can use these iterators to move forward or backward in the document as it was parsed: # ';\n\nand they lived at the bottom of a well.'\n\nBeautiful Soup's main strength is in searching the parse tree, but you can also modify the tree and write your changes as a new HTML or XML document. I covered this earlier, in , but it bears repeating. You can rename a tag, change the values of its attributes, add new attributes, and delete attributes: If you set a tag's attribute to a new string, the tag's contents are replaced with that string: Be careful: if the tag contained other tags, they and all their contents will be destroyed. You can add to a tag's contents with . It works just like calling on a Python list: Starting in Beautiful Soup 4.7.0, also supports a method called , which adds every element of a list to a , in order: If you need to add a string to a document, no problem—you can pass a Python string in to , or you can call the constructor: If you want to create a comment or some other subclass of , just call the constructor: # ['Hello', ' there', 'Nice to see you.'] What if you need to create a whole new tag? The best solution is to call the factory method : Only the first argument, the tag name, is required. Because insertion methods return the newly inserted element, you can create, insert, and obtain an element in one step: is just like , except the new element doesn't necessarily go at the end of its parent's . It will be inserted at whatever numeric position you say, similar to on a Python list: # <a href=\"http://example.com/\">I linked to but did not endorse <i>example.com</i></a> # ['I linked to ', 'but did not endorse ', <i>example.com</i>] You can pass more than one element into . All the elements will be inserted, starting at the numeric position you provide. The method inserts tags or strings immediately before something else in the parse tree: The method inserts tags or strings immediately after something else in the parse tree: Both methods return the list of newly inserted elements. removes a tag or string from the tree. It returns the tag or string that was extracted: At this point you effectively have two parse trees: one rooted at the object you used to parse the document, and one rooted at the tag that was extracted. You can go on to call on a child of the element you extracted: removes a tag from the tree, then completely destroys it and its contents: The behavior of a decomposed or is not defined and you should not use it for anything. If you're not sure whether something has been decomposed, you can check its property (new in Beautiful Soup 4.9.0): extracts a tag or string from the tree, then replaces it with one or more tags or strings of your choice: returns the tag or string that got replaced, so that you can examine it or add it back to another part of the tree. The ability to pass multiple arguments into replace_with() is new in Beautiful Soup 4.10.0. wraps an element in the object you specify. It returns the new wrapper: This method is new in Beautiful Soup 4.0.5. is the opposite of . It replaces a tag with whatever's inside that tag. It's good for stripping out markup: Like , returns the tag that was replaced. After calling a bunch of methods that modify the parse tree, you may end up with two or more objects next to each other. Beautiful Soup doesn't have any problems with this, but since it can't happen in a freshly parsed document, you might not expect behavior like the following: You can call to clean up the parse tree by consolidating adjacent strings: This method is new in Beautiful Soup 4.8.0.\n\nAny HTML or XML document is written in a specific encoding like ASCII or UTF-8. But when you load that document into Beautiful Soup, you'll discover it's been converted to Unicode: It's not magic. (That sure would be nice.) Beautiful Soup uses a sub-library called Unicode, Dammit to detect a document's encoding and convert it to Unicode. The autodetected encoding is available as the attribute of the object: If is , that means the document was already Unicode when it was passed into Beautiful Soup: Unicode, Dammit guesses correctly most of the time, but sometimes it makes mistakes. Sometimes it guesses correctly, but only after a byte-by-byte search of the document that takes a very long time. If you happen to know a document's encoding ahead of time, you can avoid mistakes and delays by passing it to the constructor as . Here's a document written in ISO-8859-8. The document is so short that Unicode, Dammit can't get a lock on it, and misidentifies it as ISO-8859-7: We can fix this by passing in the correct : If you don't know what the correct encoding is, but you know that Unicode, Dammit is guessing wrong, you can pass the wrong guesses in as : Windows-1255 isn't 100% correct, but that encoding is a compatible superset of ISO-8859-8, so it's close enough. ( is a new feature in Beautiful Soup 4.4.0.) In rare cases (usually when a UTF-8 document contains text written in a completely different encoding), the only way to get Unicode may be to replace some characters with the special Unicode character \"REPLACEMENT CHARACTER\" (U+FFFD, �). If Unicode, Dammit needs to do this, it will set the attribute to on the or object. This lets you know that the Unicode representation is not an exact representation of the original—some data was lost. If a document contains �, but is , you'll know that the � was there originally (as it is in this paragraph) and doesn't stand in for missing data. When you write out an output document from Beautiful Soup, you get a UTF-8 document, even if the input document wasn't in UTF-8 to begin with. Here's a document written in the Latin-1 encoding: Note that the <meta> tag has been rewritten to reflect the fact that the document is now in UTF-8. If you don't want UTF-8, you can pass an encoding into : You can also call encode() on the object, or any element in the soup, just as if it were a Python string: Any characters that can't be represented in your chosen encoding will be converted into numeric XML entity references. Here's a document that includes the Unicode character SNOWMAN: The SNOWMAN character can be part of a UTF-8 document (it looks like ☃), but there's no representation for that character in ISO-Latin-1 or ASCII, so it's converted into \"☃\" for those encodings: You can use Unicode, Dammit without using Beautiful Soup. It's useful whenever you have data in an unknown encoding and you just want it to become Unicode: Unicode, Dammit's guesses will get a lot more accurate if you install one of these Python libraries: , , or . The more data you give Unicode, Dammit, the more accurately it will guess. If you have your own suspicions as to what the encoding might be, you can pass them in as a list: Unicode, Dammit has two special features that Beautiful Soup doesn't use. You can use Unicode, Dammit to convert Microsoft smart quotes to HTML or XML entities: You can also convert Microsoft smart quotes to ASCII quotes: Hopefully you'll find this feature useful, but Beautiful Soup doesn't use it. Beautiful Soup prefers the default behavior, which is to convert Microsoft smart quotes to Unicode characters along with everything else: Sometimes a document is mostly in UTF-8, but contains Windows-1252 characters such as (again) Microsoft smart quotes. This can happen when a website includes data from multiple sources. You can use to turn such a document into pure UTF-8. Here's a simple example: This document is a mess. The snowmen are in UTF-8 and the quotes are in Windows-1252. You can display the snowmen or the quotes, but not both: Decoding the document as UTF-8 raises a , and decoding it as Windows-1252 gives you gibberish. Fortunately, will convert the string to pure UTF-8, allowing you to decode it to Unicode and display the snowmen and quote marks simultaneously: only knows how to handle Windows-1252 embedded in UTF-8 (or vice versa, I suppose), but this is the most common case. Note that you must know to call on your data before passing it into or the constructor. Beautiful Soup assumes that a document has a single encoding, whatever it might be. If you pass it a document that contains both UTF-8 and Windows-1252, it's likely to think the whole document is Windows-1252, and the document will come out looking like . is new in Beautiful Soup 4.1.0.\n\nIf you're having trouble understanding what Beautiful Soup does to a document, pass the document into the function. (This function is new in Beautiful Soup 4.2.0.) Beautiful Soup will print out a report showing you how different parsers handle the document, and tell you if you're missing a parser that Beautiful Soup could be using: # I noticed that html5lib is not installed. Installing it may help. # Trying to parse your data with html.parser # Here's what html.parser did with the document: Just looking at the output of diagnose() might show you how to solve the problem. Even if not, you can paste the output of when asking for help. There are two different kinds of parse errors. There are crashes, where you feed a document to Beautiful Soup and it raises an exception (usually an ). And there is unexpected behavior, where a Beautiful Soup parse tree looks a lot different than the document used to create it. These problems are almost never problems with Beautiful Soup itself. This is not because Beautiful Soup is an amazingly well-written piece of software. It's because Beautiful Soup doesn't include any parsing code. Instead, it relies on external parsers. If one parser isn't working on a certain document, the best solution is to try a different parser. See Installing a parser for details and a parser comparison. If this doesn't help, you might need to inspect the document tree found inside the object, to see where the markup you're looking for actually ended up.\n• None (on the line ): Caused by running an old Python 2 version of Beautiful Soup under Python 3, without converting the code.\n• None - Caused by running an old Python 2 version of Beautiful Soup under Python 3.\n• None - Caused by running the Python 3 version of Beautiful Soup under Python 2.\n• None - Caused by running Beautiful Soup 3 code in an environment that doesn't have BS3 installed. Or, by writing Beautiful Soup 4 code without knowing that the package name has changed to .\n• None - Caused by running Beautiful Soup 4 code in an environment that doesn't have BS4 installed. By default, Beautiful Soup parses documents as HTML. To parse a document as XML, pass in \"xml\" as the second argument to the constructor: You'll need to have lxml installed.\n• None If your script works on one computer but not another, or in one virtual environment but not another, or outside the virtual environment but not inside, it's probably because the two environments have different parser libraries available. For example, you may have developed the script on a computer that has lxml installed, and then tried to run it on a computer that only has html5lib installed. See Differences between parsers for why this matters, and fix the problem by mentioning a specific parser library in the constructor.\n• None Because HTML tags and attributes are case-insensitive, all three HTML parsers convert tag and attribute names to lowercase. That is, the markup <TAG></TAG> is converted to <tag></tag>. If you want to preserve mixed-case or uppercase tags and attributes, you'll need to parse the document as XML.\n• None (or just about any other ) - This problem shows up in two main situations. First, when you try to print a Unicode character that your console doesn't know how to display. (See this page on the Python wiki for help.) Second, when you're writing to a file and you pass in a Unicode character that's not supported by your default encoding. In this case, the simplest solution is to explicitly encode the Unicode string into UTF-8 with .\n• None - Caused by accessing when the tag in question doesn't define the attribute. The most common errors are and . Use if you're not sure is defined, just as you would with a Python dictionary.\n• None - This usually happens because you expected to return a single tag or string. But returns a list of tags and strings—a object. You need to iterate over the list and look at the of each one. Or, if you really only want one result, you need to use instead of .\n• None - This usually happens because you called and then tried to access the attribute of the result. But in your case, didn't find anything, so it returned , instead of returning a tag or a string. You need to figure out why your call isn't returning anything.\n• None - This usually happens because you're treating a string as though it were a tag. You may be iterating over a list, expecting that it contains nothing but tags, when it actually contains both tags and strings. Beautiful Soup will never be as fast as the parsers it sits on top of. If response time is critical, if you're paying for computer time by the hour, or if there's any other reason why computer time is more valuable than programmer time, you should forget about Beautiful Soup and work directly atop lxml. That said, there are things you can do to speed up Beautiful Soup. If you're not using lxml as the underlying parser, my advice is to start. Beautiful Soup parses documents significantly faster using lxml than using html.parser or html5lib. You can speed up encoding detection significantly by installing the cchardet library. Parsing only part of a document won't save you much time parsing the document, but it can save a lot of memory, and it'll make searching the document much faster.\n\nBeautiful Soup 3 is the previous release series, and is no longer supported. Development of Beautiful Soup 3 stopped in 2012, and the package was completely discontinued in 2021. There's no reason to install it unless you're trying to get very old software to work, but it's published through PyPi as : You can also download a tarball of the final release, 3.2.2. If you ran or , but your code doesn't work, you installed Beautiful Soup 3 by mistake. You need to run . The documentation for Beautiful Soup 3 is archived online. Most code written against Beautiful Soup 3 will work against Beautiful Soup 4 with one simple change. All you should have to do is change the package name from to . So this:\n• None If you get the \"No module named BeautifulSoup\", your problem is that you're trying to run Beautiful Soup 3 code, but you only have Beautiful Soup 4 installed.\n• None If you get the \"No module named bs4\", your problem is that you're trying to run Beautiful Soup 4 code, but you only have Beautiful Soup 3 installed. Although BS4 is mostly backward-compatible with BS3, most of its methods have been deprecated and given new names for PEP 8 compliance. There are numerous other renames and changes, and a few of them break backward compatibility. Here's what you'll need to know to convert your BS3 code and habits to BS4: Beautiful Soup 3 used Python's , a module that was deprecated and removed in Python 3.0. Beautiful Soup 4 uses by default, but you can plug in lxml or html5lib and use that instead. See Installing a parser for a comparison. Since is not the same parser as , you may find that Beautiful Soup 4 gives you a different parse tree than Beautiful Soup 3 for the same markup. If you swap out for lxml or html5lib, you may find that the parse tree changes yet again. If this happens, you'll need to update your scraping code to process the new tree. I renamed three attributes to avoid using words that have special meaning to Python. Unlike my changes to method names (which you'll see in the form of deprecation warnings), these changes did not preserve backwards compatibility. If you used these attributes in BS3, your code will break in BS4 until you change them. Some of the generators used to yield after they were done, and then stop. That was a bug. Now the generators just stop. There is no longer a class for parsing XML. To parse XML you pass in \"xml\" as the second argument to the constructor. For the same reason, the constructor no longer recognizes the argument. Beautiful Soup's handling of empty-element XML tags has been improved. Previously when you parsed XML you had to explicitly say which tags were considered empty-element tags. The argument to the constructor is no longer recognized. Instead, Beautiful Soup considers any empty tag to be an empty-element tag. If you add a child to an empty-element tag, it stops being an empty-element tag. An incoming HTML or XML entity is always converted into the corresponding Unicode character. Beautiful Soup 3 had a number of overlapping ways of dealing with entities, which have been removed. The constructor no longer recognizes the or arguments. (Unicode, Dammit still has , but its default is now to turn smart quotes into Unicode.) The constants , , and have been removed, since they configure a feature (transforming some but not all entities into Unicode characters) that no longer exists. If you want to turn Unicode characters back into HTML entities on output, rather than turning them into UTF-8 characters, you need to use an output formatter. Tag.string now operates recursively. If tag A contains a single tag B and nothing else, then A.string is the same as B.string. (Previously, it was None.) Multi-valued attributes like have lists of strings as their values, not simple strings. This may affect the way you search by CSS class. objects now implement the method, such that two objects are considered equal if they generate the same markup. This may change your script's behavior if you put objects into a dictionary or set. If you pass one of the methods both string and a tag-specific argument like name, Beautiful Soup will search for tags that match your tag-specific criteria and whose Tag.string matches your string value. It will not find the strings themselves. Previously, Beautiful Soup ignored the tag-specific arguments and looked for strings. The constructor no longer recognizes the argument. It's now the parser's responsibility to handle markup correctly. The rarely-used alternate parser classes like and have been removed. It's now the parser's decision how to handle ambiguous markup. The method now returns a Unicode string, not a bytestring."
    },
    {
        "link": "https://beautiful-soup-4.readthedocs.io/en/latest",
        "document": "If you’re using a recent version of Debian or Ubuntu Linux, you can install Beautiful Soup with the system package manager: Beautiful Soup 4 is published through PyPi, so if you can’t install it with the system packager, you can install it with or . The package name is , and the same package works on Python 2 and Python 3. Make sure you use the right version of or for your Python version (these may be named and respectively if you’re using Python 3). If you don’t have or installed, you can download the Beautiful Soup 4 source tarball and install it with . If all else fails, the license for Beautiful Soup allows you to package the entire library with your application. You can download the tarball, copy its directory into your application’s codebase, and use Beautiful Soup without installing it at all. I use Python 2.7 and Python 3.2 to develop Beautiful Soup, but it should work with other recent versions. Beautiful Soup is packaged as Python 2 code. When you install it for use with Python 3, it’s automatically converted to Python 3 code. If you don’t install the package, the code won’t be converted. There have also been reports on Windows machines of the wrong version being installed. If you get the “No module named HTMLParser”, your problem is that you’re running the Python 2 version of the code under Python 3. If you get the “No module named html.parser”, your problem is that you’re running the Python 3 version of the code under Python 2. In both cases, your best bet is to completely remove the Beautiful Soup installation from your system (including any directory created when you unzipped the tarball) and try the installation again. If you get the “Invalid syntax” on the line , you need to convert the Python 2 code to Python 3. You can do this either by installing the package: or by manually running Python’s conversion script on the directory: Beautiful Soup supports the HTML parser included in Python’s standard library, but it also supports a number of third-party Python parsers. One is the lxml parser. Depending on your setup, you might install lxml with one of these commands: Another alternative is the pure-Python html5lib parser, which parses HTML the way a web browser does. Depending on your setup, you might install html5lib with one of these commands: This table summarizes the advantages and disadvantages of each parser library:\n• Lenient (As of Python 2.7.3 and 3.2.)\n• Not as fast as lxml, less lenient than html5lib.\n• The only currently supported XML parser\n• Parses pages the same way a web browser does If you can, I recommend you install and use lxml for speed. If you’re using a version of Python 2 earlier than 2.7.3, or a version of Python 3 earlier than 3.2.2, it’s that you install lxml or html5lib–Python’s built-in HTML parser is just not very good in older versions. Note that if a document is invalid, different parsers will generate different Beautiful Soup trees for it. See Differences between parsers for details.\n\n<p class=\"story\">Once upon a time there were three little sisters; and their names were and they lived at the bottom of a well.</p> I’ll use this as an example to show you how to move from one part of a document to another. Tags may contain strings and other tags. These elements are the tag’s . Beautiful Soup provides a lot of different attributes for navigating and iterating over a tag’s children. Note that Beautiful Soup strings don’t support any of these attributes, because a string can’t have children. The simplest way to navigate the parse tree is to say the name of the tag you want. If you want the <head> tag, just say : You can do use this trick again and again to zoom in on a certain part of the parse tree. This code gets the first <b> tag beneath the <body> tag: Using a tag name as an attribute will give you only the tag by that name: If you need to get the <a> tags, or anything more complicated than the first tag with a certain name, you’ll need to use one of the methods described in Searching the tree, such as : A tag’s children are available in a list called : The object itself has children. In this case, the <html> tag is the child of the object.: A string does not have , because it can’t contain anything: Instead of getting them as a list, you can iterate over a tag’s children using the generator: The and attributes only consider a tag’s children. For instance, the <head> tag has a single direct child–the <title> tag: But the <title> tag itself has a child: the string “The Dormouse’s story”. There’s a sense in which that string is also a child of the <head> tag. The attribute lets you iterate over of a tag’s children, recursively: its direct children, the children of its direct children, and so on: The <head> tag has only one child, but it has two descendants: the <title> tag and the <title> tag’s child. The object only has one direct child (the <html> tag), but it has a whole lot of descendants: If a tag has only one child, and that child is a , the child is made available as : If a tag’s only child is another tag, and tag has a , then the parent tag is considered to have the same as its child: If a tag contains more than one thing, then it’s not clear what should refer to, so is defined to be : If there’s more than one thing inside a tag, you can still look at just the strings. Use the generator: # u'Once upon a time there were three little sisters; and their names were\n\n' # u';\n\nand they lived at the bottom of a well.' These strings tend to have a lot of extra whitespace, which you can remove by using the generator instead: # u'Once upon a time there were three little sisters; and their names were' # u';\n\nand they lived at the bottom of a well.' Here, strings consisting entirely of whitespace are ignored, and whitespace at the beginning and end of strings is removed. Continuing the “family tree” analogy, every tag and every string has a : the tag that contains it. You can access an element’s parent with the attribute. In the example “three sisters” document, the <head> tag is the parent of the <title> tag: The title string itself has a parent: the <title> tag that contains it: The parent of a top-level tag like <html> is the object itself: And the of a object is defined as None: You can iterate over all of an element’s parents with . This example uses to travel from an <a> tag buried deep within the document, to the very top of the document: Consider a simple document like this: The <b> tag and the <c> tag are at the same level: they’re both direct children of the same tag. We call them . When a document is pretty-printed, siblings show up at the same indentation level. You can also use this relationship in the code you write. You can use and to navigate between page elements that are on the same level of the parse tree: The <b> tag has a , but no , because there’s nothing before the <b> tag on the same level of the tree . For the same reason, the <c> tag has a but no : The strings “text1” and “text2” are siblings, because they don’t have the same parent: In real documents, the or of a tag will usually be a string containing whitespace. Going back to the “three sisters” document: You might think that the of the first <a> tag would be the second <a> tag. But actually, it’s a string: the comma and newline that separate the first <a> tag from the second: The second <a> tag is actually the of the comma: You can iterate over a tag’s siblings with or : # u'; and they lived at the bottom of a well.' # u'Once upon a time there were three little sisters; and their names were\n\n' Take a look at the beginning of the “three sisters” document: An HTML parser takes this string of characters and turns it into a series of events: “open an <html> tag”, “open a <head> tag”, “open a <title> tag”, “add a string”, “close the <title> tag”, “open a <p> tag”, and so on. Beautiful Soup offers tools for reconstructing the initial parse of the document. The attribute of a string or tag points to whatever was parsed immediately afterwards. It might be the same as , but it’s usually drastically different. Here’s the final <a> tag in the “three sisters” document. Its is a string: the conclusion of the sentence that was interrupted by the start of the <a> tag.: # '; and they lived at the bottom of a well.' But the of that <a> tag, the thing that was parsed immediately after the <a> tag, is the rest of that sentence: it’s the word “Tillie”: That’s because in the original markup, the word “Tillie” appeared before that semicolon. The parser encountered an <a> tag, then the word “Tillie”, then the closing </a> tag, then the semicolon and rest of the sentence. The semicolon is on the same level as the <a> tag, but the word “Tillie” was encountered first. The attribute is the exact opposite of . It points to whatever element was parsed immediately before this one: You should get the idea by now. You can use these iterators to move forward or backward in the document as it was parsed: # u';\n\nand they lived at the bottom of a well.'\n\nBeautiful Soup defines a lot of methods for searching the parse tree, but they’re all very similar. I’m going to spend a lot of time explaining the two most popular methods: and . The other methods take almost exactly the same arguments, so I’ll just cover them briefly. Once again, I’ll be using the “three sisters” document as an example: <p class=\"story\">Once upon a time there were three little sisters; and their names were and they lived at the bottom of a well.</p> By passing in a filter to an argument like , you can zoom in on the parts of the document you’re interested in. Before talking in detail about and similar methods, I want to show examples of different filters you can pass into these methods. These filters show up again and again, throughout the search API. You can use them to filter based on a tag’s name, on its attributes, on the text of a string, or on some combination of these. The simplest filter is a string. Pass a string to a search method and Beautiful Soup will perform a match against that exact string. This code finds all the <b> tags in the document: If you pass in a byte string, Beautiful Soup will assume the string is encoded as UTF-8. You can avoid this by passing in a Unicode string instead. If you pass in a regular expression object, Beautiful Soup will filter against that regular expression using its method. This code finds all the tags whose names start with the letter “b”; in this case, the <body> tag and the <b> tag: This code finds all the tags whose names contain the letter ‘t’: If you pass in a list, Beautiful Soup will allow a string match against item in that list. This code finds all the <a> tags all the <b> tags: The value matches everything it can. This code finds the tags in the document, but none of the text strings: If none of the other matches work for you, define a function that takes an element as its only argument. The function should return if the argument matches, and otherwise. Here’s a function that returns if a tag defines the “class” attribute but doesn’t define the “id” attribute: Pass this function into and you’ll pick up all the <p> tags: This function only picks up the <p> tags. It doesn’t pick up the <a> tags, because those tags define both “class” and “id”. It doesn’t pick up tags like <html> and <title>, because those tags don’t define “class”. If you pass in a function to filter on a specific attribute like , the argument passed into the function will be the attribute value, not the whole tag. Here’s a function that finds all tags whose attribute does not match a regular expression: The function can be as complicated as you need it to be. Here’s a function that returns if a tag is surrounded by string objects: Now we’re ready to look at the search methods in detail. The method looks through a tag’s descendants and retrieves descendants that match your filters. I gave several examples in Kinds of filters, but here are a few more: # u'Once upon a time there were three little sisters; and their names were\n\n' Some of these should look familiar, but others are new. What does it mean to pass in a value for , or ? Why does find a <p> tag with the CSS class “title”? Let’s look at the arguments to . Pass in a value for and you’ll tell Beautiful Soup to only consider tags with certain names. Text strings will be ignored, as will tags whose names that don’t match. This is the simplest usage: Recall from Kinds of filters that the value to can be a string, a regular expression, a list, a function, or the value True. Any argument that’s not recognized will be turned into a filter on one of a tag’s attributes. If you pass in a value for an argument called , Beautiful Soup will filter against each tag’s ‘id’ attribute: If you pass in a value for , Beautiful Soup will filter against each tag’s ‘href’ attribute: You can filter an attribute based on a string, a regular expression, a list, a function, or the value True. This code finds all tags whose attribute has a value, regardless of what the value is: You can filter multiple attributes at once by passing in more than one keyword argument: Some attributes, like the data-* attributes in HTML 5, have names that can’t be used as the names of keyword arguments: # SyntaxError: keyword can't be an expression You can use these attributes in searches by putting them into a dictionary and passing the dictionary into as the argument: You can’t use a keyword argument to search for HTML’s ‘name’ element, because Beautiful Soup uses the argument to contain the name of the tag itself. Instead, you can give a value to ‘name’ in the argument: It’s very useful to search for a tag that has a certain CSS class, but the name of the CSS attribute, “class”, is a reserved word in Python. Using as a keyword argument will give you a syntax error. As of Beautiful Soup 4.1.2, you can search by CSS class using the keyword argument : As with any keyword argument, you can pass a string, a regular expression, a function, or : Remember that a single tag can have multiple values for its “class” attribute. When you search for a tag that matches a certain CSS class, you’re matching against of its CSS classes: You can also search for the exact string value of the attribute: But searching for variants of the string value won’t work: If you want to search for tags that match two or more CSS classes, you should use a CSS selector: In older versions of Beautiful Soup, which don’t have the shortcut, you can use the trick mentioned above. Create a dictionary whose value for “class” is the string (or regular expression, or whatever) you want to search for: With you can search for strings instead of tags. As with and the keyword arguments, you can pass in a string, a regular expression, a list, a function, or the value True. Here are some examples: \"\"\"Return True if this string is the only child of its parent tag.\"\"\" Although is for finding strings, you can combine it with arguments that find tags: Beautiful Soup will find all tags whose matches your value for . This code finds the <a> tags whose is “Elsie”: The argument is new in Beautiful Soup 4.4.0. In earlier versions it was called : returns all the tags and strings that match your filters. This can take a while if the document is large. If you don’t need the results, you can pass in a number for . This works just like the LIMIT keyword in SQL. It tells Beautiful Soup to stop gathering results after it’s found a certain number. There are three links in the “three sisters” document, but this code only finds the first two: If you call , Beautiful Soup will examine all the descendants of : its children, its children’s children, and so on. If you only want Beautiful Soup to consider direct children, you can pass in . See the difference here: Here’s that part of the document: The <title> tag is beneath the <html> tag, but it’s not beneath the <html> tag: the <head> tag is in the way. Beautiful Soup finds the <title> tag when it’s allowed to look at all descendants of the <html> tag, but when restricts it to the <html> tag’s immediate children, it finds nothing. Beautiful Soup offers a lot of tree-searching methods (covered below), and they mostly take the same arguments as : , , , , and the keyword arguments. But the argument is different: and are the only methods that support it. Passing into a method like wouldn’t be very useful. Because is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the object or a object as though it were a function, then it’s the same as calling on that object. These two lines of code are equivalent: These two lines are also equivalent: The method scans the entire document looking for results, but sometimes you only want to find one result. If you know a document only has one <body> tag, it’s a waste of time to scan the entire document looking for more. Rather than passing in every time you call , you can use the method. These two lines of code are equivalent: The only difference is that returns a list containing the single result, and just returns the result. If can’t find anything, it returns an empty list. If can’t find anything, it returns : Remember the trick from Navigating using tag names? That trick works by repeatedly calling : I spent a lot of time above covering and . The Beautiful Soup API defines ten other methods for searching the tree, but don’t be afraid. Five of these methods are basically the same as , and the other five are basically the same as . The only differences are in what parts of the tree they search. First let’s consider and . Remember that and work their way down the tree, looking at tag’s descendants. These methods do the opposite: they work their way the tree, looking at a tag’s (or a string’s) parents. Let’s try them out, starting from a string buried deep in the “three daughters” document: # <p class=\"story\">Once upon a time there were three little sisters; and their names were # and they lived at the bottom of a well.</p> One of the three <a> tags is the direct parent of the string in question, so our search finds it. One of the three <p> tags is an indirect parent of the string, and our search finds that as well. There’s a <p> tag with the CSS class “title” in the document, but it’s not one of this string’s parents, so we can’t find it with . You may have made the connection between and , and the .parent and .parents attributes mentioned earlier. The connection is very strong. These search methods actually use to iterate over all the parents, and check each one against the provided filter to see if it matches. These methods use .next_siblings to iterate over the rest of an element’s siblings in the tree. The method returns all the siblings that match, and only returns the first one: These methods use .previous_siblings to iterate over an element’s siblings that precede it in the tree. The method returns all the siblings that match, and only returns the first one: These methods use .next_elements to iterate over whatever tags and strings that come after it in the document. The method returns all matches, and only returns the first match: # u';\n\nand they lived at the bottom of a well.', u'\n\n\n\n', u'...', u'\n\n'] In the first example, the string “Elsie” showed up, even though it was contained within the <a> tag we started from. In the second example, the last <p> tag in the document showed up, even though it’s not in the same part of the tree as the <a> tag we started from. For these methods, all that matters is that an element match the filter, and show up later in the document than the starting element. These methods use .previous_elements to iterate over the tags and strings that came before it in the document. The method returns all matches, and only returns the first match: # [<p class=\"story\">Once upon a time there were three little sisters; ...</p>, The call to found the first paragraph in the document (the one with class=”title”), but it also finds the second paragraph, the <p> tag that contains the <a> tag we started with. This shouldn’t be too surprising: we’re looking at all the tags that show up earlier in the document than the one we started with. A <p> tag that contains an <a> tag must have shown up before the <a> tag it contains. As of version 4.7.0, Beautiful Soup supports most CSS4 selectors via the SoupSieve project. If you installed Beautiful Soup through , SoupSieve was installed at the same time, so you don’t have to do anything extra. has a method which uses SoupSieve to run a CSS selector against a parsed document and return all the matching elements. has a similar method which runs a CSS selector against the contents of a single tag. The SoupSieve documentation lists all the currently supported CSS selectors, but here are some of the basics: Find tags that match any selector from a list of selectors: Test for the existence of an attribute: There’s also a method called , which finds only the first tag that matches a selector: If you’ve parsed XML that defines namespaces, you can use them in CSS selectors.: When handling a CSS selector that uses namespaces, Beautiful Soup uses the namespace abbreviations it found when parsing the document. You can override this by passing in your own dictionary of abbreviations: All this CSS selector stuff is a convenience for people who already know the CSS selector syntax. You can do all of this with the Beautiful Soup API. And if CSS selectors are all you need, you should parse the document with lxml: it’s a lot faster. But this lets you CSS selectors with the Beautiful Soup API.\n\nBeautiful Soup’s main strength is in searching the parse tree, but you can also modify the tree and write your changes as a new HTML or XML document. I covered this earlier, in Attributes, but it bears repeating. You can rename a tag, change the values of its attributes, add new attributes, and delete attributes: If you set a tag’s attribute to a new string, the tag’s contents are replaced with that string: Be careful: if the tag contained other tags, they and all their contents will be destroyed. You can add to a tag’s contents with . It works just like calling on a Python list: Starting in Beautiful Soup 4.7.0, also supports a method called , which works just like calling on a Python list: If you need to add a string to a document, no problem–you can pass a Python string in to , or you can call the constructor: If you want to create a comment or some other subclass of , just call the constructor: # [u'Hello', u' there', u'Nice to see you.'] What if you need to create a whole new tag? The best solution is to call the factory method : Only the first argument, the tag name, is required. is just like , except the new element doesn’t necessarily go at the end of its parent’s . It’ll be inserted at whatever numeric position you say. It works just like on a Python list: # <a href=\"http://example.com/\">I linked to but did not endorse <i>example.com</i></a> # [u'I linked to ', u'but did not endorse', <i>example.com</i>] The method inserts tags or strings immediately before something else in the parse tree: The method inserts tags or strings immediately following something else in the parse tree: removes a tag or string from the tree. It returns the tag or string that was extracted: At this point you effectively have two parse trees: one rooted at the object you used to parse the document, and one rooted at the tag that was extracted. You can go on to call on a child of the element you extracted: removes a tag from the tree, then completely destroys it and its contents : removes a tag or string from the tree, and replaces it with the tag or string of your choice: returns the tag or string that was replaced, so that you can examine it or add it back to another part of the tree. wraps an element in the tag you specify. It returns the new wrapper: This method is new in Beautiful Soup 4.0.5. is the opposite of . It replaces a tag with whatever’s inside that tag. It’s good for stripping out markup: Like , returns the tag that was replaced. After calling a bunch of methods that modify the parse tree, you may end up with two or more objects next to each other. Beautiful Soup doesn’t have any problems with this, but since it can’t happen in a freshly parsed document, you might not expect behavior like the following: You can call to clean up the parse tree by consolidating adjacent strings: The method is new in Beautiful Soup 4.8.0.\n\nSpecifying the parser to use¶ If you just need to parse some HTML, you can dump the markup into the constructor, and it’ll probably be fine. Beautiful Soup will pick a parser for you and parse the data. But there are a few additional arguments you can pass in to the constructor to change which parser is used. The first argument to the constructor is a string or an open filehandle–the markup you want parsed. The second argument is you’d like the markup parsed. If you don’t specify anything, you’ll get the best HTML parser that’s installed. Beautiful Soup ranks lxml’s parser as being the best, then html5lib’s, then Python’s built-in parser. You can override this by specifying one of the following:\n• What type of markup you want to parse. Currently supported are “html”, “xml”, and “html5”.\n• The name of the parser library you want to use. Currently supported options are “lxml”, “html5lib”, and “html.parser” (Python’s built-in HTML parser). If you don’t have an appropriate parser installed, Beautiful Soup will ignore your request and pick a different parser. Right now, the only supported XML parser is lxml. If you don’t have lxml installed, asking for an XML parser won’t give you one, and asking for “lxml” won’t work either. Beautiful Soup presents the same interface to a number of different parsers, but each parser is different. Different parsers will create different parse trees from the same document. The biggest differences are between the HTML parsers and the XML parsers. Here’s a short document, parsed as HTML: Since an empty <b /> tag is not valid HTML, the parser turns it into a <b></b> tag pair. Here’s the same document parsed as XML (running this requires that you have lxml installed). Note that the empty <b /> tag is left alone, and that the document is given an XML declaration instead of being put into an <html> tag.: There are also differences between HTML parsers. If you give Beautiful Soup a perfectly-formed HTML document, these differences won’t matter. One parser will be faster than another, but they’ll all give you a data structure that looks exactly like the original HTML document. But if the document is not perfectly-formed, different parsers will give different results. Here’s a short, invalid document parsed using lxml’s HTML parser. Note that the dangling </p> tag is simply ignored: Here’s the same document parsed using html5lib: Instead of ignoring the dangling </p> tag, html5lib pairs it with an opening <p> tag. This parser also adds an empty <head> tag to the document. Here’s the same document parsed with Python’s built-in HTML parser: Like html5lib, this parser ignores the closing </p> tag. Unlike html5lib, this parser makes no attempt to create a well-formed HTML document by adding a <body> tag. Unlike lxml, it doesn’t even bother to add an <html> tag. Since the document “<a></p>” is invalid, none of these techniques is the “correct” way to handle it. The html5lib parser uses techniques that are part of the HTML5 standard, so it has the best claim on being the “correct” way, but all three techniques are legitimate. Differences between parsers can affect your script. If you’re planning on distributing your script to other people, or running it on multiple machines, you should specify a parser in the constructor. That will reduce the chances that your users parse a document differently from the way you parse it.\n\nAny HTML or XML document is written in a specific encoding like ASCII or UTF-8. But when you load that document into Beautiful Soup, you’ll discover it’s been converted to Unicode: It’s not magic. (That sure would be nice.) Beautiful Soup uses a sub-library called Unicode, Dammit to detect a document’s encoding and convert it to Unicode. The autodetected encoding is available as the attribute of the object: Unicode, Dammit guesses correctly most of the time, but sometimes it makes mistakes. Sometimes it guesses correctly, but only after a byte-by-byte search of the document that takes a very long time. If you happen to know a document’s encoding ahead of time, you can avoid mistakes and delays by passing it to the constructor as . Here’s a document written in ISO-8859-8. The document is so short that Unicode, Dammit can’t get a lock on it, and misidentifies it as ISO-8859-7: We can fix this by passing in the correct : If you don’t know what the correct encoding is, but you know that Unicode, Dammit is guessing wrong, you can pass the wrong guesses in as : Windows-1255 isn’t 100% correct, but that encoding is a compatible superset of ISO-8859-8, so it’s close enough. ( is a new feature in Beautiful Soup 4.4.0.) In rare cases (usually when a UTF-8 document contains text written in a completely different encoding), the only way to get Unicode may be to replace some characters with the special Unicode character “REPLACEMENT CHARACTER” (U+FFFD, �). If Unicode, Dammit needs to do this, it will set the attribute to on the or object. This lets you know that the Unicode representation is not an exact representation of the original–some data was lost. If a document contains �, but is , you’ll know that the � was there originally (as it is in this paragraph) and doesn’t stand in for missing data. When you write out a document from Beautiful Soup, you get a UTF-8 document, even if the document wasn’t in UTF-8 to begin with. Here’s a document written in the Latin-1 encoding: Note that the <meta> tag has been rewritten to reflect the fact that the document is now in UTF-8. If you don’t want UTF-8, you can pass an encoding into : You can also call encode() on the object, or any element in the soup, just as if it were a Python string: Any characters that can’t be represented in your chosen encoding will be converted into numeric XML entity references. Here’s a document that includes the Unicode character SNOWMAN: The SNOWMAN character can be part of a UTF-8 document (it looks like ☃), but there’s no representation for that character in ISO-Latin-1 or ASCII, so it’s converted into “☃” for those encodings: You can use Unicode, Dammit without using Beautiful Soup. It’s useful whenever you have data in an unknown encoding and you just want it to become Unicode: Unicode, Dammit’s guesses will get a lot more accurate if you install the or Python libraries. The more data you give Unicode, Dammit, the more accurately it will guess. If you have your own suspicions as to what the encoding might be, you can pass them in as a list: Unicode, Dammit has two special features that Beautiful Soup doesn’t use. You can use Unicode, Dammit to convert Microsoft smart quotes to HTML or XML entities: You can also convert Microsoft smart quotes to ASCII quotes: Hopefully you’ll find this feature useful, but Beautiful Soup doesn’t use it. Beautiful Soup prefers the default behavior, which is to convert Microsoft smart quotes to Unicode characters along with everything else: Sometimes a document is mostly in UTF-8, but contains Windows-1252 characters such as (again) Microsoft smart quotes. This can happen when a website includes data from multiple sources. You can use to turn such a document into pure UTF-8. Here’s a simple example: This document is a mess. The snowmen are in UTF-8 and the quotes are in Windows-1252. You can display the snowmen or the quotes, but not both: Decoding the document as UTF-8 raises a , and decoding it as Windows-1252 gives you gibberish. Fortunately, will convert the string to pure UTF-8, allowing you to decode it to Unicode and display the snowmen and quote marks simultaneously: only knows how to handle Windows-1252 embedded in UTF-8 (or vice versa, I suppose), but this is the most common case. Note that you must know to call on your data before passing it into or the constructor. Beautiful Soup assumes that a document has a single encoding, whatever it might be. If you pass it a document that contains both UTF-8 and Windows-1252, it’s likely to think the whole document is Windows-1252, and the document will come out looking like . is new in Beautiful Soup 4.1.0.\n\nBeautiful Soup 3 is the previous release series, and is no longer being actively developed. It’s currently packaged with all major Linux distributions: It’s also published through PyPi as .: You can also download a tarball of Beautiful Soup 3.2.0. If you ran or , but your code doesn’t work, you installed Beautiful Soup 3 by mistake. You need to run . The documentation for Beautiful Soup 3 is archived online. Most code written against Beautiful Soup 3 will work against Beautiful Soup 4 with one simple change. All you should have to do is change the package name from to . So this:\n• If you get the “No module named BeautifulSoup”, your problem is that you’re trying to run Beautiful Soup 3 code, but you only have Beautiful Soup 4 installed.\n• If you get the “No module named bs4”, your problem is that you’re trying to run Beautiful Soup 4 code, but you only have Beautiful Soup 3 installed. Although BS4 is mostly backwards-compatible with BS3, most of its methods have been deprecated and given new names for PEP 8 compliance. There are numerous other renames and changes, and a few of them break backwards compatibility. Here’s what you’ll need to know to convert your BS3 code and habits to BS4: Beautiful Soup 3 used Python’s , a module that was deprecated and removed in Python 3.0. Beautiful Soup 4 uses by default, but you can plug in lxml or html5lib and use that instead. See Installing a parser for a comparison. Since is not the same parser as , you may find that Beautiful Soup 4 gives you a different parse tree than Beautiful Soup 3 for the same markup. If you swap out for lxml or html5lib, you may find that the parse tree changes yet again. If this happens, you’ll need to update your scraping code to deal with the new tree. Some arguments to the Beautiful Soup constructor were renamed for the same reasons: I renamed one method for compatibility with Python 3: I renamed one attribute to use more accurate terminology: I renamed three attributes to avoid using words that have special meaning to Python. Unlike the others, these changes are not backwards compatible. If you used these attributes in BS3, your code will break on BS4 until you change them. I gave the generators PEP 8-compliant names, and transformed them into properties: Some of the generators used to yield after they were done, and then stop. That was a bug. Now the generators just stop. There are two new generators, .strings and .stripped_strings. yields NavigableString objects, and yields Python strings that have had whitespace stripped. There is no longer a class for parsing XML. To parse XML you pass in “xml” as the second argument to the constructor. For the same reason, the constructor no longer recognizes the argument. Beautiful Soup’s handling of empty-element XML tags has been improved. Previously when you parsed XML you had to explicitly say which tags were considered empty-element tags. The argument to the constructor is no longer recognized. Instead, Beautiful Soup considers any empty tag to be an empty-element tag. If you add a child to an empty-element tag, it stops being an empty-element tag. An incoming HTML or XML entity is always converted into the corresponding Unicode character. Beautiful Soup 3 had a number of overlapping ways of dealing with entities, which have been removed. The constructor no longer recognizes the or arguments. (Unicode, Dammit still has , but its default is now to turn smart quotes into Unicode.) The constants , , and have been removed, since they configure a feature (transforming some but not all entities into Unicode characters) that no longer exists. If you want to turn Unicode characters back into HTML entities on output, rather than turning them into UTF-8 characters, you need to use an output formatter. Tag.string now operates recursively. If tag A contains a single tag B and nothing else, then A.string is the same as B.string. (Previously, it was None.) Multi-valued attributes like have lists of strings as their values, not strings. This may affect the way you search by CSS class. If you pass one of the methods both string a tag-specific argument like name, Beautiful Soup will search for tags that match your tag-specific criteria and whose Tag.string matches your value for string. It will find the strings themselves. Previously, Beautiful Soup ignored the tag-specific arguments and looked for strings. The constructor no longer recognizes the argument. It’s now the parser’s responsibility to handle markup correctly. The rarely-used alternate parser classes like and have been removed. It’s now the parser’s decision how to handle ambiguous markup. The method now returns a Unicode string, not a bytestring."
    },
    {
        "link": "https://oxylabs.io/blog/beautiful-soup-parsing-tutorial",
        "document": "If you want to build your first web scraper, we recommend checking our video tutorial below or our article that details everything you need to know to get started with Python web scraping . Yet, in this tutorial, we’ll focus specifically on parsing a sample HTML file in Python and using Selenium to render dynamic pages.\n\nAlthough web scraping in its totality is a complex and nuanced field of knowledge, building your own basic web scraper isn’t all that difficult. And that’s mostly due to coding languages such as Python . This language makes the process much more straightforward thanks to its relative ease of use and the many useful libraries that it offers. In this tutorial, we’ll be focusing on one of these wildly popular libraries named Beautiful Soup , a Python package used for parsing HTML and XML documents.\n\nOur previous article on what is parsing sums up this topic nicely. You can also check our video tutorial on how to parse data with Beautiful Soup or keep reading the text below:\n\nA well-built parser will identify the needed HTML string and the relevant information within it. Based on predefined criteria and the rules of the parser, it’ll filter and combine the needed information into CSV, JSON, or any other format.\n\nData parsing is a process during which a piece of data gets converted into a different type of data according to specified criteria. It’s an important part of web scraping since it helps transform raw HTML data into a more easily readable format that can be understood and analyzed.\n\nBefore getting to the matter at hand, let’s first take a look at some of the fundamentals of this topic.\n\nThis tutorial is useful for those seeking to quickly grasp the value that Python and Beautiful Soup 4 offer. After following the provided examples, you should be able to understand the basic principles of how to parse HTML data. The examples will demonstrate traversing a document for HTML tags, printing the full content of the tags, finding elements by ID, extracting text from specified tags, and exporting it to a CSV file.\n\nFinally, since this article explores working with a sample file written in HTML, you should be at least somewhat familiar with the HTML structure.\n\nIf you’re using Windows, it’s recommended to run the terminal as administrator to ensure that everything works out smoothly.\n\nThe next step is to install the Beautiful Soup 4 library on your system. No matter the OS, you can easily do it by using this command on the terminal to install the latest version of Beautiful Soup:\n\nOn Windows, when installing Python, make sure to tick the PATH installation checkbox. PATH installation adds executables to the default OS Command Prompt executable search. The OS will then recognize commands like pip or python without having to point to the directory of the executable, which makes things more convenient.\n\nBefore following this tutorial, you should have a Python programming environment set up on your machine. For this tutorial, we’ll assume that PyCharm is used since it’s a convenient choice even for the less experienced with Python and is a great starting point. Otherwise, simply use your go-to IDE.\n\nBeautiful Soup is a Python package for parsing HTML and XML documents. It creates a parse tree for parsed web pages based on specific criteria that can be used to extract, navigate, search, and modify data from HTML, which is mostly used for web scraping. Beautiful Soup 4 is supported on Python versions 3.6 and greater. Being a useful library, it can save programmers loads of time when collecting data and parsing it.\n\nA sample HTML document will help demonstrate the main methods of how Beautiful Soup parses data. This file is much more simple than your average modern website; however, it’ll be sufficient for the scope of this tutorial. If you want to use data from a table found on your target website, check this tutorial on how to scrape HTML tables with Python.\n\nFor PyCharm to use this file, simply copy it to any text editor and save it with the .html extension to the directory of your PyCharm project. Alternatively, you can create an HTML file in PyCharm by right-clicking on the project area, then navigating to New > HTML File and pasting the HTML code from above.\n\nGoing further, you can create a new Python file by navigating to New > Python File. Congratulations, and welcome to your new playground!\n\nFirst, you can use Beautiful Soup to extract a list of all the tags used in our sample HTML file. For this step, you can use the soup.descendants generator:\n\nClick the Run button, and you should get the below output:\n\nBeautiful Soup traversed our HTML file and printed all the HTML tags that it found sequentially. Let’s take a quick look at what each line did:\n\nThis tells Python to import the Beautiful Soup library.\n\nThis code snippet above, as you could probably guess, gives an instruction to open our sample HTML file, read its contents, and store them in the contents variable.\n\nThis line creates a Python Beautiful Soup object and passes it to Python’s built-in HTML parser. Other parsers, such as lxml, might also be used, but it’s a separate external library, and for the purpose of this tutorial, the built-in parser will do just fine.\n\nThe final piece of code, namely the soup.descendants generator, instructs Beautiful Soup to look for HTML tag names and print them in the PyCharm console. The results can also easily be exported to a CSV file, but we’ll get to this later.\n\nTo extract the content of HTML tags, this is what you can do:\n\nIt’s a simple parsing instruction that outputs the HTML tag with its full content in the specified order. Here’s what the output should look like:\n\nAdditionally, you can remove the HTML tags and print the text only by adding .text:\n\nWhich gives the following output:\n\nNote that this only prints the first instance of the specified tag. Let’s continue to see how to find an HTML element by ID and use the find_all method to filter all elements by specific criteria.\n\nYou can use two similar ways to find elements by ID:\n\nBoth of these will output the same result in the Python Console:\n\n6. Find all instances of a tag and extract text\n\nThe find_all method is a great way to extract all the data stored in specific elements from an HTML file. It accepts many criteria that make it a flexible tool allowing users to filter data in convenient ways. Let’s find all the items within the <li> tags and print them as text only:\n\nThis is how the full code should look like:\n\nBeautiful Soup has excellent support for CSS selectors as it provides several methods to interact with HTML content using selectors. Under the hood, Beautiful Soup uses the soupsieve package. When you install Beautiful Soup with Python’s package-management system pip, it’ll automatically install the soupsieve dependency for you. Be sure to check out their documentation to learn more about the supported CSS selectors.\n\nBeautiful Soup primarily provides two methods to interact with HTML web page content using CSS selectors: select and select_one. Let’s try out both of them.\n\nYou can grab the title from our HTML sample file using the select method. Your code should look like the below:\n\nSimple, isn’t it? Notice how the CSS selector navigates the HTML by going through the hierarchy of the HTML elements sequentially.\n\nThis method is useful when you need to grab only one element using a CSS selector that matches multiple elements. For instance, our HTML sample has several <li> elements. If you want to grab only the first one, you can use the following CSS selector:\n\nThis will pick the first <li> element of the <ul> tag, which has several other <li> elements.\n\nTo extract a specific <li> element, you can add :nth-of-type(n) to your CSS selector. For instance, you can extract the third <li> element, which in our HTML file is <li>Shared proxies</li>, using the following line:\n\nMost websites these days tend to load content dynamically, meaning data can be left out if JavaScript isn’t triggered to load the content. The requests library and Beautiful Soup libraries aren’t equipped to handle JavaScript-rendered web pages. Consequently, using these libraries to download the HTML document of a website would exclude any dynamically-loaded content.\n\nYou’ll have to use other libraries that can render the website by executing JavaScript to parse dynamic elements. Python’s Selenium package offers powerful capabilities to interact with and manipulate DOM elements. In a nutshell, its WebDriver utilizes popular web browsers and renders JavaScript-based dynamic websites quickly. By combining Beautiful Soup with Selenium WebDriver, you can easily parse dynamic content from any website.\n\nAdditionally, there are other ways you can scrape dynamic websites that we have explored in our Playwright and Scrapy Splash tutorials.\n\nFirst, install Selenium with the below command:\n\nAs of Selenium 4.6, the browser driver is downloaded automatically. Yet, if you’re using an older version of Selenium or the driver wasn’t found, you’ll have to manually download the WebDriver. Visit this page to find the driver download links for the supported web browsers.\n\nNow that you’ve installed all the required dependencies, you can jump right into writing the code. Let’s begin by importing the newly installed library and Beautiful Soup:\n\nNext, you’ll have to initiate a browser instance using the below code:\n\nThe above code uses the Chrome() driver to launch an instance of a Chrome browser.\n\nNow, you can use this driver object to fetch dynamic content. So let’s extract the HTML of this JavaScript-rendered dummy website http://quotes.toscrape.com/js/:\n\nAs soon as you execute the above code, you’ll notice the Chrome browser instance automatically navigating to the desired website and rendering the JavaScript-based content. The new object named js_content contains the HTML content of the website.\n\nNow that you’ve got the HTML content in a string format, you can simply use the BeautifulSoup() constructor to create the Beautiful Soup object with parsed data:\n\nYou can now navigate the soup object with Beautiful Soup and parse any HTML element using the methods outlined previously. For example, let’s extract the first quote found on our target website. Every quote is within the <span> tag with an attribute set to class=\"text\", so the code line to extract the content from the quote can look like this:\n\nNote the additional underscore _ within class_=\"text\" – you must use it. Otherwise, Python will interpret it as a reserved class keyword.\n\nTo learn more about common issues that can arise when performing such tasks and how to fix them, take a look at our how to find HTML element by class with BeautifulSoup guide.\n\nWhen parsing dynamic websites, keep in mind that some websites have strong anti-bot measures that can easily detect Selenium-based web scrapers. Mostly, this is achieved by identifying the Selenium web driver's common request patterns and using various other fingerprinting techniques. Thus, it’s extremely difficult to avoid such anti-bot measures. In case your IP address gets blocked, you might want to consider using a proxy, and implementing other anti-detection methods. Make sure to look into which proxy type would be most suitable for your scraping project, whether that be datacenter or residential proxies.\n\nBy now you should now have a basic understanding of how Beautiful Soup can be used to parse and extract data. It should be noted that the information presented in this article is useful as introductory material, yet real-world web scraping and parsing with BeautifulSoup is usually much more complicated than this. For a more in-depth look at Beautiful Soup, you’ll hardly find a better source than its official documentation, so be sure to check it out too.\n\nA very common real-world application would be exporting data to a CSV file for later analysis. Although this is outside the scope of this tutorial, let’s take a quick look at how this might be achieved.\n\nFirst, you would need to install an additional Python library called pandas that helps Python create structured data. This can be easily done by entering the following line in your terminal:\n\nYou should also add this line to the beginning of your code to import the library:\n\nGoing further, let’s add some lines that’ll export the list we extracted earlier to a CSV file. This is how your full code should look like:\n\nWhat happens here exactly? Let’s take a look:\n\nThis line finds all instances of the <li> tag and stores it in the results object.\n\nAnd here, we see the pandas library at work, storing our results into a table (DataFrame) and exporting it to a CSV file.\n\nIf all goes well, a new file titled names.csv should appear in the running directory of your Python project, and inside, you should see a table with the proxy types list. That’s it! Now you not only know how data extraction from an HTML document works, but you can also programmatically export the data to a new file.\n\nAs you can see, Beautiful Soup is a greatly useful HTML parser. With a relatively low learning curve, you can quickly grasp how to navigate, search, and modify the parse tree. With the addition of libraries, such as pandas, you can further manipulate and analyze the data, which offers a powerful package for a near-infinite amount of data collection and analysis use cases.\n\nAnd if you’d like to expand your knowledge on Python web scraping in general and get familiar with other Python libraries, we recommend heading over to What is Python used for? and Python Requests blog posts."
    },
    {
        "link": "https://stackabuse.com/guide-to-parsing-html-with-beautifulsoup-in-python",
        "document": "Web scraping is programmatically collecting information from various websites. While there are many libraries and frameworks in various languages that can extract web data, Python has long been a popular choice because of its plethora of options for web scraping.\n\nThis article will give you a crash course on web scraping in Python with Beautiful Soup - a popular Python library for parsing HTML and XML.\n\nWeb scraping is ubiquitous and gives us data as we would get with an API. However, as good citizens of the internet, it's our responsibility to respect the site owners we scrape from. Here are some principles that a web scraper should adhere to:\n• Don't claim scraped content as our own. Website owners sometimes spend a lengthy amount of time creating articles, collecting details about products or harvesting other content. We must respect their labor and originality.\n• Don't scrape a website that doesn't want to be scraped. Websites sometimes come with a file - which defines the parts of a website that can be scraped. Many websites also have Terms of Use which may not allow scraping. We must respect websites that do not want to be scraped.\n• Is there an API available already? Splendid, there's no need for us to write a scraper. APIs are created to provide access to data in a controlled way as defined by the owners of the data. We prefer to use APIs if they're available.\n• Making requests to a website can cause a toll on a website's performance. A web scraper that makes too many requests can be as debilitating as a DDOS attack. We must scrape responsibly so we won't cause any disruption to the regular functioning of the website.\n\nThe HTML content of the web pages can be parsed and scraped with Beautiful Soup. In the following section, we will be covering those functions that are useful for scraping web pages.\n\nWhat makes Beautiful Soup so useful is the myriad functions it provides to extract data from HTML. This image below illustrates some of the functions we can use:\n\nLet's get hands-on and see how we can parse HTML with Beautiful Soup. Consider the following HTML page saved to file as :\n\nThe following code snippets are tested on . You can install the module by typing the following command in the terminal:\n\nThe HTML file needs to be prepared. This is done by passing the file to the constructor, let's use the interactive Python shell for this, so we can instantly print the contents of a specific part of a page:\n\nNow we can use Beautiful Soup to navigate our website and extract data.\n\nFrom the soup object created in the previous section, let's get the title tag of :\n\nHere's a breakdown of each component we used to get the title:\n\nBeautiful Soup is powerful because our Python objects match the nested structure of the HTML document we are scraping.\n\nTo get the text of the first tag, enter this:\n\nTo get the title within the HTML's body tag (denoted by the \"title\" class), type the following in your terminal:\n\nFor deeply nested HTML documents, navigation could quickly become tedious. Luckily, Beautiful Soup comes with a search function so we don't have to navigate to retrieve HTML elements.\n\nThe method takes an HTML tag as a string argument and returns the list of elements that match with the provided tag. For example, if we want all tags in :\n\nWe'll see this list of tags as output:\n\nHere's a breakdown of each component we used to search for a tag:\n\nWe can search for tags of a specific class as well by providing the argument. Beautiful Soup uses because is a reserved keyword in Python. Let's search for all tags that have the \"element\" class:\n\nAs we only have two links with the \"element\" class, you'll see this output:\n\nWhat if we wanted to fetch the links embedded inside the tags? Let's retrieve a link's attribute using the option. It works just like but it returns the first matching element instead of a list. Type this in your shell:\n\nThe and functions also accept a regular expression instead of a string. Behind the scenes, the text will be filtered using the compiled regular expression's method. For example:\n\nThe list upon iteration, fetches the tags starting with the character which includes and :\n\nWe've covered the most popular ways to get tags and their attributes. Sometimes, especially for less dynamic web pages, we just want the text from it. Let's see how we can get it!\n\nGetting the Whole Text\n\nThe function retrieves all the text from the HTML document. Let's get all the text of the HTML document:\n\nYour output should be like this:\n\nSometimes the newline characters are printed, so your output may look like this as well:\n\nNow that we have a feel for how to use Beautiful Soup, let's scrape a website!\n\nNow that we have mastered the components of Beautiful Soup, it's time to put our learning to use. Let's build a scraper to extract data from https://books.toscrape.com/ and save it to a CSV file. The site contains random data about books and is a great space to test out your web scraping techniques.\n\nFirst, create a new file called . Let's import all the libraries we need for this script:\n\nIn the modules mentioned above:\n• - performs the URL request and fetches the website's HTML\n• - limits how many times we scrape the page at once\n• - helps us export our scraped data to a CSV file\n• - allows us to write regular expressions that will come in handy for picking text based on its pattern\n• - yours truly, the scraping module to parse the HTML\n\nYou would have already installed, and , , and are built-in packages in Python. You'll need to install the module directly like this:\n\nBefore you begin, you need to understand how the webpage's HTML is structured. In your browser, let's go to http://books.toscrape.com/catalogue/page-1.html. Then right-click on the components of the web page to be scraped, and click on the inspect button to understand the hierarchy of the tags as shown below.\n\nThis will show you the underlying HTML for what you're inspecting. The following picture illustrates these steps:\n\nFrom inspecting the HTML, we learn how to access the URL of the book, the cover image, the title, the rating, the price, and more fields from the HTML. Let's write a function that scrapes a book item and extract its data:\n\nThe last line of the above snippet points to a function to write the list of scraped strings to a CSV file. Let's add that function now:\n\nAs we have a function that can scrape a page and export to CSV, we want another function that crawls through the paginated website, collecting book data on each page.\n\nTo do this, let's look at the URL we are writing this scraper for:\n\nThe only varying element in the URL is the page number. We can format the URL dynamically so it becomes a seed URL:\n\nThis string formatted URL with the page number can be fetched using the method . We can then create a new object. Every time we get the soup object, the presence of the \"next\" button is checked so we can stop at the last page. We keep track of a counter for the page number that's incremented by 1 after successfully scraping a page.\n\nThe function above, , is recursively called until the function returns . At this point, the code will scrape the remaining part of the webpage and exit.\n\nFor the final piece to the puzzle, we initiate the scraping flow. We define the and call the to get the data. This is done under the block:\n\nIf you'd like to learn more about the block, check out our guide on how it works.\n\nYou can execute the script as shown below in your terminal and get the output as:\n\nThe scraped data can be found in the current working directory under the filename . Here's a sample the file's content:\n\nGood job! If you wanted to have a look at the scraper code as a whole, you can find it on GitHub.\n\nIn this tutorial, we learned the ethics of writing good web scrapers. We then used Beautiful Soup to extract data from an HTML file using the Beautiful Soup's object properties, and its various methods like , and . We then built a scraper that retrieves a book list online and exports to CSV.\n\nWeb scraping is a useful skill that helps in various activities such as extracting data like an API, performing QA on a website, checking for broken URLs on a website, and more. What's the next scraper you're going to build?"
    },
    {
        "link": "https://geeksforgeeks.org/beautifulsoup-scraping-paragraphs-from-html",
        "document": "In this article, we will discuss how to scrap paragraphs from HTML using Beautiful Soup\n• bs4: Beautiful Soup(bs4) is a Python library for pulling data out of HTML and XML files. For installing the module-\n• urllib: urllib is a package that collects several modules for working with URLs. It can also be installed the same way, it is most of the in-built in the environment itself.\n\nThe html file contains several tags and like the anchor tag <a>, span tag <span>, paragraph tag <p> etc. So, the beautiful soup helps us to parse the html file and get our desired output such as getting the paragraphs from a particular url/html file.\n\nAfter importing the modules urllib and bs4 we will provide a variable with a url which is to be read, the urllib.request.urlopen() function forwards the requests to the server for opening the url. BeautifulSoup() function helps us to parse the html file or you say the encoding in html. The loop used here with find_all() finds all the tags containing paragraph tag <p></p> and the text between them are collected by the get_text() method.\n\nBelow is the implementation:\n• bs4: Beautiful Soup(bs4) is a Python library for pulling data out of HTML and XML files. This module does not come built-in with Python. To install this type the below command in the terminal.\n• requests: Requests allows you to send HTTP/1.1 requests extremely easily. This module also does not comes built-in with Python. To install this type the below command in the terminal.\n• Create an HTML document and specify the ‘<p>’ tag into the code\n• Pass the HTML document into the Beautifulsoup() function\n• Use the ‘P’ tag to extract paragraphs from the Beautifulsoup object\n• Get text from the HTML document with get_text()."
    }
]