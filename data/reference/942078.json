[
    {
        "link": "https://discuss.pytorch.org/t/best-practices-to-solve-nan-ctc-loss/151913",
        "document": "I’ve encountered the CTC loss going NaN several times, and I believe there are many people facing this problem from time to time. So perhaps a collective list of best practices would be helpful for this.\n\n1. Mismatch between model’s number of classes and class_ids in labels\n\nA common problem is that, seeing the largest class in our label_list is C, we mistakenly set the model’s number of classes also to C. Instead, it should be C + 1, because class 0 is usually reserved for BLANK or PAD token.\n\nSuppose you have classes 1 to 100 in your labels, and the model also has number_of_classes = 100. Whenever the CTC loss encounters a label tensor that has 100 in it, and model predictions of shape (T, B, 100), it would silently go bonkers, as it actually needs (T, B, 101).\n\nYou have to consider what to do if you have labels out of your class_list. Either drop those samples, or have a specific class UNKNOWN (I usually have it as class 1) where you assign unknown classes.\n\nCheck 2 can easily be done while doing check 1; I’m only writing them separately for emphasis.\n\nTarget length should not exceed model’s prediction’s sequence length. Also blank labels and too short labels cause problems.\n\nDrop all samples with too short/blank labels. Either drop anomalous samples which have really long or labels beyond some threshold, or try to increase model’s sequence length so that it is guaranteed to be longer than your labels.\n\nI’ve found model sequence length = double of max_label_length to work well empirically; would like to hear other opinions on this.\n\n4. Catch CTC problem right at the start\n\nIn your training script, insert the following line somewhere at the start:\n\nWill immediately halt training and point to the first place where something went wrong.\n\nIn and , there is a parameter which is False by default. Use .\n\nSometimes loss first becomes inf before NaN, in which case the inf loss (and gradients) can be reset to zero.\n\nIn your training step, clip the gradient norm to some value. I read somewhere that a good value lies in the closed range [0.5, 5.0], but haven’t verified this; I usually clip to 1.0 and it works fine.\n\nIt may be possible that a seemingly normal learning rate (like 0.01) may be too large for certain problems.\n\nGenerally, a combination of AdamW, with learning rate = 0.0003, weight_decay = 0.01, warmup scheduling for first 1000 steps, followed by cosine annealing scheduling for the remaining training period is a good initial combination when tackling a new problem / project. After the initial 3e-4 learning rate, I explore by a factor of 3 in either direction, like 0.001 and 0.0001. The trio of (0.001, 0.0003, 0.0001) usually gives me a sense of what type of learning rates work. For some problems where more regularization seems necessary, I also search for weight decay among (0.01, 0.03, 0.1).\n\nProblem: Each call in your dataset produces labels (and perhaps data) of variable length. To load them with your dataloader, and also to process with your model, you most likely need them to be of the same sequence length.\n\nSolution: The obvious way to do this is to pad your data and targets to either a constant maximum length, or to the length of the longest sequence in the batch. You can do this inside the collate function (collate_fn parameter in dataloader; a good discussion here).\n\nI only have to handle labels here. In other tasks like video recognition, it may be necessary to pad image_sequences too (probably better idea to that inside , I think).\n\nThe CTC loss also requires input lengths and label lengths. It also expects inputs (model predictions) to be logsoftmax-ed first, which sometimes can be overlooked, since typically the last layer of a model is , and also as we don’t need to do this when using common losses like . A full example for the loss I use, again for text recognition:\n\nModels usually output predictions of fixed sequence lengths, so prediction lengths can be created with . And target lengths can be extracted from the padded target tensor by , given that we used 0 to pad them in the first place.\n\nStrange phenomenon which happen when using CTC, and solutions (if any):\n• NaN loss occurs within a few steps of training, but if the batch_size is set to 1, NaN loss no longer occurs.\n• Because of the nature of this occurrence, it seems that there may be some issue with padding the targets, at first glance. However, I have personally encountered this recently, and the underlying problem eventually turned out to be case 1: model had 1 less class than necessary. Would be great if we could figure out why NaN doesn’t show up with batch_size 1 in such cases.\n• NaN loss occurs during GPU training, but if CPU is used it doesn’t happen, strangely enough.\n• This most likely happened only in old versions of torch, due to some bug. But would like to know if this phenomenon is still around.\n• Model only predicts blanks at the start, but later starts working normally\n• Model works normally for a while, but eventually keeps predicting blanks\n• Don’t know why this happens, but usually goes away when other issues are resolved.\n• Please correct me if I wrote anything incorrect! Some of the things I’ve written here are from empirical evidence and hearsay, and not backed by any theory. Would love to know more about why X happens.\n• If there are other best practices to do/try when facing NaN issues / other solutions which work, would be great if you could share it here. Or you could simply link to an existing great post.\n• Other strange things which are being encountered with CTC, and possible underlying reasons (theoretically proved or empirically discovered).\n\nSome other useful threads: [a, b, c, d]"
    },
    {
        "link": "https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/46292835/how-should-i-handle-input-data-with-nan-values-in-tensorflow",
        "document": "How can I somehow tell my network to ignore some input data. For example when the input data is nan\n\nThis is very similar to adding a mask to your input data. You want your input data to pass through, nans turned to zeros, but you want somehow to also signal to the neural network to ignore where the nans were and pay attention to everything else.\n\nIn this question about adding a mask I review how a mask can successfully be added to an image but also give a code demonstration for a non-image problem.\n• First create a mask, 1's where data exists in the input and 0's where nan exist.\n• Second, clean up the input converting nans to 0's, or 0.5's, or anything really.\n• Third, stack the mask onto the input. If the input is an image, then the mask becomes another colour channel.\n\nThe code in the masking question shows that when the mask is added the neural net is able to learn well and when the mask is not added it is not able to learn well."
    },
    {
        "link": "https://omi.me/blogs/tensorflow-errors/nan-loss-values-in-tensorflow-causes-and-how-to-fix",
        "document": "TensorFlow is a robust library for numerical computation and machine learning. However, during training, you may encounter an error where the loss value becomes 'NaN' (not a number). Understanding the root causes of this error is critical for diagnosing and improving your model's performance.\n• Numerical Instability: One of the primary reasons for encountering NaN loss values is numerical instability. Operations like division by zero, logarithms of zero or negative numbers, or operations resulting in infinite values can cause NaN values. For example, if you have a log operation where the input value approaches zero (e.g., `log(x) where x ≈ 0`), this will result in negative infinity, and further operations might propagate NaN.\n• Exploding Gradients: In some cases, especially in recurrent networks, the gradients can become excessively large. This is known as the exploding gradient problem. When the gradient becomes too large to fit into the numerical precision of the floating point numbers, it can result in NaN values. For example, with a large learning rate, you might have weights updates like: \\`\\`\\`python W += large_learning_rate \\* large\\_gradient \\`\\`\\` which can go beyond the floating-point range.\n• Inappropriate Model Initialization: Poor initialization of model weights can also precipitate NaN errors. If weights are initialized in such a way that the outputs of some layers result in extremely large values, it may cause the activation functions (like sigmoid or tanh) to saturate, making backpropagation unstable.\n• Data Issues: If the input data contains extreme or incorrect values, it can cause the model to produce NaN outputs. This includes cases where input data is not normalized or contains missing values represented by NaNs or infinities, thereby influencing the computations leading to the loss.\n• Inadequate Numerical Precision: When dealing with deep networks or very sensitive problems, using 32-bit floating-point precision might not suffice causing computational errors that propagate to NaN values. This happens because the precision of floating-point numbers can only represent numbers with a certain level of accuracy. Understanding these causes can significantly aid in diagnosing and resolving NaN loss values during model training. Paying close attention to initialization, normalization, as well as ensuring stability through careful gradient control, can mitigate these issues."
    },
    {
        "link": "https://discuss.pytorch.org/t/nan-loss-coming-after-some-time/11568",
        "document": "The loss function is a combination of Mean Sqaured error loss and cross-entropy loss.\n\n When i am training my model, there is a finite loss but after some time, the loss is NaN and continues to be so.\n\n When I am training my model just on a single batch of 10 images, the loss is finite most of the times, but sometimes that is also NaN.\n\n Please suggest a possible solution. Cant get model to train\n\nYou could use a normalization layer. Alternatively, you can try dividing by some constant first (perhaps equal to the max value of your data?) The idea is to get the values low enough that they don’t cause really large gradients. Output becomes zero after optimizer.step()\n\nHere is a way of debuging the nan problem.\n\n First, print your model gradients because there are likely to be nan in the first place.\n\n And then check the loss, and then check the input of your loss…Just follow the clue and you will find the bug resulting in nan problem. There are some useful infomation about why nan problem could happen:\n\n 1.the learning rate\n\n 2.sqrt(0)\n\n 3.ReLU->LeakyReLU\n\nDo you get a NaN output from your model if you are using samples from certain folders or what do you mean by: I am getting nan loss value in some folders If your model is returning NaNs, you could set at the beginning of your script to get a stack trace, which would hopefully point to the operation, which is creating the NaNs.\n\nIf larger gradient magnitudes are expected and would thus create invalid values, you might clip the gradients. You could start with a max norm value of 1 or refer to any paper, which uses a similar approach. Note however, that s have a maximal value of: so you should make sure that the NaNs are not created by an invalid operation.\n\nI would recommend to try to figure out what is causing the NaNs instead of ignoring them.\n\n Based on the raised error, the loss function might either have created the NaNs or might have gotten them through their input. To isolate it, you could try to make the script deterministic following the reproducibility docs. Once it’s deterministic and you could trigger the NaNs in a single step, you could check the parameters, inputs, gradients etc. for the iteration which causes the NaNs."
    },
    {
        "link": "https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem",
        "document": "Gradient clipping solves one of the biggest problems that we have while calculating gradients in backpropagation for a neural network.\n\nYou see, in a backward pass, we calculate the gradients of all weights and biases in order to converge our cost function. These gradients, and the way they are calculated, are the secret behind the success of artificial neural networks in every domain.\n\nBut every good thing comes with some sort of caveat.\n\nGradients tend to encapsulate information they collect from the data, which also includes long-range dependencies in large text or multidimensional data. So, while calculating complex data, things can go south really quickly, and you’ll blow your next million-dollar model in the process.\n\nLuckily, you can solve it before it occurs (with gradient clipping). Let’s first look at the problem in depth.\n\nBy the end of this article, you’ll know:\n• What is gradient clipping and how does it occur?\n• How to implement it in Tensorflow and PyTorch\n• Additional research for you to read\n\nThe backpropagation algorithm is the heart of all modern-day machine learning applications, and it’s ingrained more deeply than you think.\n\nBackpropagation calculates the gradients of the cost function w.r.t. the weights and biases in the network.\n\nIt tells you about all the changes you need to make to your weights to minimize the cost function (it’s actually -1*∇ to see the steepest decrease, and +∇ would give you the steepest increase in the cost function).\n\nWhat about deeper networks, like Deep Recurrent Networks?\n\nThe translation of the effect of a change in cost function (C) to the weight in an initial layer, or the norm of the gradient, becomes so small due to increased model complexity with more hidden units that it becomes zero after a certain point. This is what we call vanishing gradients.\n\nThis hampers the learning of the model. The weights can no longer contribute to the reduction in cost function (C) and go unchanged, affecting the network in the forward pass and eventually stalling the model.\n\nOn the other hand, the exploding gradient problem refers to a large increase in the norm of the gradient during training.\n\nSuch events are caused by an explosion of long-term components, which can grow exponentially more than short-term ones. This results in an unstable network that, at best, cannot learn from the training data, making the gradient descent step impossible to execute.\n\nFor calculating gradients in Deep Recurrent Networks we use an approach called backpropagation through time (BPTT), where the recurrent model is represented as a deep multi-layer one (with an unbounded number of layers), and backpropagation is applied to the unrolled model.\n\nIn other words, it’s backpropagation on an unrolled RNN.\n\nUnrolling recurrent neural networks in time by creating a copy of the model for each time step:\n\nWe denote by a<t> the hidden state of the network at time t, by x<t> the input and ŷ<t> the output of the network at time t, and by C<t> the error obtained from the output at time t.\n\nLet’s work through some equations and see the root where it all starts.\n\nWe will diverge from classical BPTT equations at this point and rewrite the gradients in order to better highlight the exploding gradients problem.\n\nThese equations were obtained by writing the gradients in sum-of-products form.\n\nWe will also break down our flow into two parts:\n\nFirst, let’s check what intermediate activations of neurons at timestep <t> will look like:\n\nW_rec represents the recurrent matrix that will carry the translative effect of previous timesteps.\n\nW_in is the matrix that gets multiplied by the current timestep data and b represents bias.\n\nAn intermediate output at timestep <t> will look something like:\n\nNote that here “σ” represents any activation function of your choice. Popular picks include sigmoid, tanh, and ReLU.\n\nUsing the predicted and ground-truth values, we can calculate the cost function of the entire network by summing over individual Cost functions for every timestep <t>.\n\nNow that we have the general equations for the calculation of the cost function, let’s do a backward propagation and see how gradients get calculated.\n\nIn order to acquire the gradients from all timesteps, we again do a summation of all intermediate gradients at timestep <t> and then the gradients are averaged over all training examples.\n\nSo, we have to calculate the intermediate cost function (C) at any timestep <t> using the chain rule of derivatives.\n\nLet’s look at the dependency graph to identify the chain of derivatives:\n\nFor a timestep <3>, our cost function will look something like:\n\nNote: We’ve only mentioned the derivative w.r.t to W, which represents all the weights and bias matrices we’re trying to optimize.\n\nAs you can see above, we get the activation a<3>, which will depend on a<2>, and so on until the first layer’s activation is not calculated.\n\nHence, the dependency will go something like:\n\nSo if we take the derivative to W, we can’t simply treat a<3> as a constant.\n\nWe need to apply the chain rule again. Finally, our equation will look something like this:\n\nWe sum up the contributions of each time step to the gradient.\n\nIn other words, because W is used in every step up to the output we care about, we need to backpropagate gradients from t = 3 through the network all the way to t = 0.\n\nSo, in order to calculate activation for a timestep <t>, we need all the activations from previous timesteps.\n\nNote that ∂a<T>/∂a<k> is a chain rule in itself! For example:\n\nAlso note that because we are taking the derivative of a vector function with respect to a vector, the result is a matrix (called the Jacobian matrix) whose elements are all the pointwise derivatives.\n\nWe can rewrite the above gradient in Eq. 1.6:\n\nThis represents a Jacobian Matrix whose value is calculated using Frobenius or 2-norm.\n\nTo understand this phenomenon, we need to look at the form of each temporal component, and in particular at the matrix factors ∂a<t>/ ∂a<k> (Eq:1.6,1.9) that take the form of a product of (t − k) Jacobian matrices.\n\nIn the same way that a product of (t − k) real numbers can shrink to zero or explode to infinity, so can this product of matrices (along with some direction v).\n\nHence, we get the condition of Exploding Gradients due to this temporal component.\n\nHow to identify and catch exploding gradients?\n\nThe identification of these gradient problems is hard to comprehend before the training process is even started.\n\nWhen your network is a deep recurrent one, you have to:\n\nThis will tell you whether these jumps are frequent and if the norm of the gradient is increasing exponentially.\n\nThe best way to do this is by monitoring logs on a visualization dashboard.\n\nWe’ll utilize neptune.ai’s visualization and dashboarding of the logs to monitor the loss and other metrics which will help in the identification of Exploding gradients. You can head over to the detailed documentation to set up your dashboard.\n\nThere are a couple of techniques that focus on exploring gradients problem.\n\nOne common approach is L2 regularization, which applies “weight decay” to the cost function of the network.\n\nThe regularization parameter gets bigger, the weights get smaller, effectively making them less useful and, as a result, making the model more linear.\n\nHowever, we’ll focus on a technique that’s far superior in terms of gaining results and ease of implementation – Gradient Clipping.\n\nGradient clipping is a method where the error derivative is changed or clipped to a threshold during backward propagation through the network, and the clipped gradients are used to update the weights.\n\nBy rescaling the error derivative, the updates to the weights will also be rescaled, dramatically decreasing the likelihood of an overflow or underflow.\n\nGradient Clipping is implemented in two variants:\n\nGradient clipping is implemented in two variants:\n\nThe idea behind clipping-by-value is simple. We define a minimum and a maximum clip value.\n\nIf a gradient exceeds some threshold value, we clip that gradient to the threshold. If the gradient is less than the lower limit, then we clip that too, to the lower limit of the threshold.\n\nThe algorithm is as follows:\n\nif ‖g‖ ≥ max_threshold or ‖g‖ ≤ min_threshold then\n\nwhere max_threshold and min_threshold are the boundary values, and between them lies a range of values that gradients can take. g, here is the gradient, and ‖g‖ is the norm of g.\n\nThe idea behind clipping-by-norm is similar to clipping-by-value. The difference is that we clip the gradients by multiplying the unit vector of the gradients with the threshold.\n\nThe algorithm is as follows:\n\nwhere the threshold is a hyperparameter, g is the gradient, and ‖g‖ is the norm of g. Since g/‖g‖ is a unit vector, after rescaling, the new g will have a norm equal to the threshold. Note that if ‖g‖ < c, then we don’t need to do anything.\n\nGradient clipping ensures the gradient vector g has a norm at most equal to the threshold.\n\nThis helps gradient descent to have reasonable behavior even if the loss landscape of the model is irregular, most likely a cliff.\n\nNow we know why exploding gradients occur and how gradient clipping can resolve it.\n\nWe also saw two different methods by virtue of which you can apply clipping to your deep neural network. \n\nLet’s see an implementation of both gradient clipping algorithms in major machine learning frameworks: Keras, and PyTorch.\n\nWe’ll employ the MNIST dataset, which is open-source digit classification data meant for image classification.\n\nBefore we jump into code, please install the following libraries in a virtual environment:\n\nFirst, let’s import the necessary modules and libraries:\n\nNext, create a file named .env in your working directory and paste the following contents:\n\nThis will enable you to load these variables into your notebook or script when initializing a Neptune run object, like below:\n\nThe run object helps us log training details and model metadata to your Neptune project dashboard.\n\nNow, let’s load and pre-process the MNIST dataset:\n\nNow, we come to the critical part: specifying how we perform the clipping. Fortunately, we can do this in a single line of code while defining the optimizer to use:\n\nThe above snippet shows both clipping by norm and clipping by value (commented) versions of the code. The rest of the code involves training the model:\n\nAnd capturing the training details from the history object with Neptune:\n\nThe above lines will log training and validation loss and accuracy to the higher-level training directory.\n\nOnce the run finishes, you can visit the Runs tab of your Neptune dashboard to see the training process visualized, like below:\n\nYou can capture more information and metadata if you use the Neptune-Keras integration. Instead of using a run object and logging metrics manually with the append() method, you can use a single class and pass it as a callback to the fit method of the Keras model object:\n\nThe training details will be captured automatically.\n\nIn this section of implementation with PyTorch, we’ll load data again, but now with the PyTorch DataLoader class, and use the pythonic syntax to calculate gradients and clip them using the two methods we studied.\n\nNow, let’s declare some hyperparameters and the DataLoader class in PyTorch.\n\nNext, we’ll declare a model with LSTM as the input layer and Dense as the logit layer.\n\nNow, we’ll define the training loop in which gradient calculation and optimizer steps will be defined. Here we’ll also define our clipping instructions.\n\nSince PyTorch saves the gradients in the parameter name itself (a.grad), we can pass the model parameters directly to the clipping instruction. Line 17 describes how you can apply clip-by-value using PyTorch’s clip_grad_value_ function.\n\nTo apply Clip-by-Norm, you can change this line to:\n\nYou can see the above metrics visualized in my Neptune project.\n\nSo, up to this point, you understand what clipping does and how to implement it. Now, in this section, we’ll see it in action, sort of a before-and-after scenario to get you to understand the importance of it.\n\nThe framework of choice for this will be PyTorch since it has features to calculate norms on the fly and store it in variables.\n\nLet’s start with the usual imports and dependencies.\n\nFor the sake of keeping the norms small, we’ll only define two linear or dense layers for our neural network.\n\nWe created the ordered_layers variable in order to loop over them to extract norms.\n\nThis is a basic training function housing the main event loop, which contains gradient calculations and optimizer steps. Here, we extract norms from the said ordered_layers variable. Now, we only have to initialize the model and call this function.\n\nThis will create a chart for the metrics you specified, which will look something like this:\n\nCongratulations! You’ve successfully understood the Gradient Clipping Methods, the problem they solve, and the Exploding Gradient Problem.\n\nBelow are a few endnotes and future research things for you to follow through on.\n\nWhenever you work on a big project with a large network, make sure that you are thorough with your logging procedures.\n\nExploding gradients can only be efficiently caught if you are logging and monitoring properly.\n\nYou can use your current logging mechanisms in Tensorboard to visualize and monitor other metrics in Neptune’s dashboard. What you have to do is simply install neptune-client to accomplish that. Head over here to explore the documentation.\n\nThat’s it for now, stay tuned for more! Adios!"
    },
    {
        "link": "https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch",
        "document": "(which is actually deprecated in favor of following the more consistent syntax of a trailing when in-place modification is performed) clips the norm of the overall gradient by concatenating all parameters passed to the function, as can be seen from the documentation:\n\nFrom your example it looks like that you want instead which has a similar syntax and also modifies the gradients in-place:\n\nAnother option is to register a backward hook. This takes the current gradient as an input and may return a tensor which will be used in-place of the previous gradient, i.e. modifying it. This hook is called each time after a gradient has been computed, i.e. there's no need for manually clipping once the hook has been registered:"
    },
    {
        "link": "https://stackoverflow.com/questions/61756557/gradient-clipping-in-pytorch-has-no-effect-gradient-exploding-still-happens",
        "document": "Your code looks right, but try using a smaller value for the clip-value argument. Here's the documentation on the clip_grad_value_() function you're using, which shows that each individual term in the gradient is set such that its magnitude does not exceed the clip value.\n\nYou have clip value set to 100, so if you have 100 parameters then can be as large as 10,000 (100*100)."
    },
    {
        "link": "https://cnvrg.io/gradient-clipping",
        "document": "In this case, a neural network would need to keep track the context of who is jumping over the fence to make the right translation. This is also applicable when classifying the sentiment of a sentence because certain words are positive while others are negative. Therefore, the network has to understand how the word is used in the whole sentence to determine its polarity. RNNs majorly deal with sequence data. Therefore, backpropagation happens through time. Since the networks are usually deep, the weights are updated at various intervals. The weights could tend to disappear through time leading to vanishing gradients or they could become too large leading to the exploding gradients problem. In this article, let’s explore these problems and their possible solutions.\n\nThe number of time steps depends on the design of your network. Remember that weights are initialized to very small numbers closer to zero at the beginning. These weights are updated during the backpropagation process. When these small weights are multiplied by small gradients, the result is even smaller weights. This means that the network is updated with very tiny weights and hence learns very slowly. The small weights then result in small gradients during the forward propagation. This results in the gradients decreasing gradually. At a certain point, the gradient can become zero. This is referred to as the vanishing gradients problem. The result of this is a useless network that cannot be used in practice. So far we have talked about situations where the weights become too small leading to very small gradients. In other scenarios, the weights could become so large resulting in enormous gradients. This is referred to as the exploding gradients problem. The vanishing gradients and exploding gradients problems can be addressed by scaling the gradients and clipping gradients that are above or below a certain threshold. This is usually referred to as gradient clipping."
    },
    {
        "link": "https://geeksforgeeks.org/gradient-clipping-in-pytorch-methods-implementation-and-best-practices",
        "document": "Gradient clipping is a crucial technique in deep learning, especially for addressing the exploding gradients problem. This issue can lead to numerical instability and impede the training process of neural networks. In this article, we will explore the concept of gradient clipping, its significance, and how to implement it in PyTorch. PyTorch offers basic functions such as torch.nn.utils.clip_grad_norm_ and torch.nn.utils.clip_grad_value_ to enhance optimization.\n\nBy applying these methods in conjunction with gradient computation, training can become more efficient and stable. We will discuss these methods, and provide practical examples to demonstrate these techniques.\n\nGradient clipping is a technique used to prevent the gradients from becoming excessively large during the training of neural networks. When gradients grow too large, they can cause the model's weights to update by huge amounts, leading to numerical instability and potentially causing the model to produce NaN (Not a Number) values or overflow errors. This phenomenon is known as the exploding gradients problem.\n\nGradient clipping is crucial for maintaining numerical stability during training. By limiting the magnitude of the gradients, it ensures that the model learns effectively and prevents it from getting stuck in local minima. This technique is particularly important for training deep neural networks, such as Recurrent Neural Networks (RNNs), which are prone to exploding gradients due to their sequential nature.\n\nPyTorch provides three classic gradient-clipping techniques to avoid exploding gradient problems. They are as follows:\n\nClipping by value is the most straightforward approach, where the gradients are individually clipped so that they lie in the predefined range. Here, each component of the gradient vector is clipped individually.\n\nIn Pytorch, one can clip the gradient by using the function. The syntax is as follows:\n\nHere the gradients will be clipped to the range [- , ]. That means we can only specify a single clip value, which will be used for both the upper and lower bounds.\n\nLet's discuss the steps to do gradient clipping in Pytorch using clipping by value. The steps are as follows:\n• None Define a simple neural network using nn.Module() from Pytorch and instantiate the model.\n• None Create a criterion that measures the mean squared error using a torch. nn.MSELoss.\n• None Perform gradient clipping by value using the clip_grad_value_ method.\n• None Print the number of training loops and their loss.\n\nLet's construct the code based on the above steps. The code is as follows:\n\nIn this example, the gradients of all the parameters are clipped with the function clip_grad_value_ to ensure that they fall within [-0.1 and +0.1]. This prevents any gradient value from going beyond the +0.1 and -0.1 absolute values, which can be used to prevent the fluctuation of training in one way or another.\n\nUsing the backward hook approach, one can clip the gradients to an unsymmetric interval. In Pytorch, we can make use of the method . The syntax is as follows:\n\nHere, the hook will be invoked every time a gradient w.r.t the Tensor is computed.\n\nLet's discuss the steps to do gradient clipping in Pytorch using the register_hook() method. The steps are as follows:\n• None Define a simple neural network using nn.Module() from Pytorch and instantiate the model.\n• register_hook() ) for each model parameter. Using the torch.clamp() method, one can clamp all the elements in an input into the range [min, max].\n• None Create a criterion that measures the mean squared error using a torch. nn.MSELoss.\n• None Print each training loop and its loss.\n\nLet's construct the code based on the above steps. The code is as follows:\n\nIn this example, the gradients of all the parameters are clipped using the register_hook() method. By using the torch.clamp() method, we clamped all the elements into the range [-0.1, 1.0], thereby providing an unsymmetric gradient as a parameter to the register_hook() method for clipping.\n\nIn the gradient clipping by norm method, the gradients are clipped if their norm is greater than the specified threshold value. The given approach involves clipping the gradient values in such a way that the gradients are limited to a specific value.\n\nOne can make use of the ' clips the gradients using a vector norm . The syntax is as follows:\n\nLet's discuss the steps to do gradient clipping in Pytorch using clipping by norm. The steps are as follows:\n• None Define a simple neural network using nn.Module() from Pytorch and instantiate the model.\n• None Create a criterion that measures the mean squared error using the torch. nn.MSELoss.\n• None Perform gradient clipping by norm using the clip_grad_norm_ method.\n• None Print the number of training loops and their loss.\n\nLet's construct the code based on the above steps. The code is as follows:\n\nIn this example, nn.utils.clip_grad_norm_ applies a scaling factor to the gradients so that division by zero does not occur due to norms greater than 1.0. This restricts the control one gradient can impose on the update, making training much steadier compared to otherwise.\n• Choosing the Clipping Threshold: Selecting the appropriate clipping threshold is crucial for the effectiveness of gradient clipping. The threshold should be chosen based on the specific characteristics of the model and the training data. A common approach is to monitor the gradient norms during training and set the threshold to a value that prevents excessive gradient magnitudes without overly restricting the learning process.\n• Monitoring Clipped Gradients: Logging the frequency and magnitude of clipped gradients can provide valuable insights into the training process. This information can help you adjust the clipping threshold and other hyperparameters to improve the model's performance and stability.\n• Combining with Other Techniques: Gradient clipping can be combined with other techniques, such as learning rate scheduling and weight regularization, to further enhance the stability and performance of the training process. Experimenting with different combinations of techniques can help you find the optimal configuration for your specific use case.\n\nGradient clipping is a vital technique in deep learning to prevent the exploding gradients problem. PyTorch provides two methods for gradient clipping: clip-by-norm and clip-by-value. By understanding how to implement these methods correctly, you can ensure that your neural networks train efficiently and effectively."
    },
    {
        "link": "https://discuss.pytorch.org/t/numerical-stability-differences-between-pytorch-0-4-1-and-1-0-0/35808",
        "document": "I was developing a new model on machines with pytorch 0.4.1 and then shifted the code to machines with 1.0.0. On machines with 1.0.0 the models performed significantly worse. I controlled for changes to default inits in nn.Linear and nn.Conv2d by forcing all layers to initialize using the 0.4.1 defaults. I also pulled the 0.4.1 version of Adam into my codebase so that all models used the same optimizer. The performance difference seems to be related to numerical stability, particularly in F.log_softmax and nn.CrossEntropy (which I think relies on F.log_softmax). I believe this for the following reasons. I setup a pair of conda environments on the same machine, which differed only in their pytorch version. In each env I ran two models using the same code in each env. Within a given env, the only difference between models was that one model cast the “logit tensors” to float64 before computing losses based on F.log_softmax and nn.CrossEntropy, and the other model cast them to float32. After computing the losses, I always cast back to float32 before summing the losses and doing the backwards pass. I.e., I ran four models: (0.4.1, float32), (0.4.1, float64), (1.0.0, float32), and (1.0.0, float64). Both 0.4.1 models performed the same across all values I am monitoring during training. The float64 1.0.0 model significantly outperformed the float32 1.0.0 model. Both 0.4.1 models significantly outperformed the float64 1.0.0 model. The gap between the 1.0.0 and 0.4.1 models was BIG. I am running more tests to see how controlling the magnitude/variance of inputs to F.log_softmax and nn.CrossEntropy differentially affect performance across pytorch versions, and these seem to support my belief that numerical instabilities are the main cause of differences across pytorch versions. What changes were made to the backend numerical stuff for these functions? Do you have any advice for more conclusively determining what’s causing performance differences across pytorch versions?\n\nThe performance difference seems to be related to numerical stability, particularly in F.log_softmax and nn.CrossEntropy (which I think relies on F.log_softmax). I believe this for the following reasons. Actually, I had the opposite experience. I have had numerical stability issues before (more due to a custom implementation of a loss function) and thought that 1.0 might fix that because there was some bugfix related to However, I noticed zero difference between running the code in 0.4.1 and 1.0 though. A potential explanation (assuming when you say “models significantly outperformed the float64 1.0.0 model” you mean predictive performance of the model?) is maybe that you previously used hyperparameters optimal for the previously “buggy” log_softmax, and these parameters are now not the best choice in 1.0 given that bug fix? The bugfix mentioned above was not actually a bug fix and made things worse.\n\nI’m developing an extended/augmented version of the “Deep InfoMax” model described in this paper: https://openreview.net/forum?id=Bklr3j0cKX While looking for the root cause of differences between 0.4.1 and 1.0.0, I stripped the model down until it was approximately the same as the “local DIM” model in the ICLR paper, modulo some data augmentation tricks. I’m measuring model “performance” during training in terms of two tasks – (i) a log-likelihood loss from a noise-contrastive estimation objective that drives self-supervised representation learning, and (ii) a classifier which is trained at the same time using the features learned by the self-supervised objective. The classifier is a simple MLP with one ReLU hidden layer and does not backprop its loss into the features provided by the encoder. The classifier is for monitoring the quality of features learned via self-supervised learning. The NLL for the classifier and the encoder’s NCE cost are both significantly worse when trained in 1.0.0 than in 0.4.1, whether I compute the losses in fp32 or fp64. If I cast the relevant tensors to fp64 prior to computing the NCE and classifier losses, and then cast back to fp32 after computing the losses, the performance in 0.4.1 is unaffected but the performance in 1.0.0 improves relative to the case where I use fp32 inside the loss computations. I believe everything is computed on the GPU, but it’s possible that the pytorch backend silently shifts some things off GPU. The main step in computing the NCE cost is to do F.log_softmax on a large tensor of pair-wise “scores” between “global” and “local” features taken from all the inputs in a minibatch. The softmax normalization is over a set of 1k-10k (potentially) high-variance logits. The classifier cost is pretty standard, and just passes the MLP’s output logits to an nn.CrossEntropy instance. Training in 0.4.1 has always been very stable, including while developing the models described in the ICLR paper. I did not encounter any instability until I started working with 1.0.0. One weird guess for what could cause the problem is that “improvements” in numerical stability may be causing some grads/losses which underflow to 0 in 0.4.1 to no longer underflow, and these grads are the ones that explode in backprop and cause the instability. But, idk.\n\nI provided more info in my reply to rasbt. I believe all computations are on GPU. I’m using P100s in an Azure VM. The tensor I apply log_softmax to has shape (64, 49, 64*49), and the normalization is along dim=2. When tracking the [50, 90, 95] percentiles of grad norms observed in each epoch, the difference between training with 0.4.1 and 1.0.0 is stark. It is clear from the first epoch, and not at all subtle. It is possible that something weird is going on with my conda envs or the VM environment. For now I am just going to work in 0.4.1. If I have time I will try and record a simpler and more precise set of conditions in which the observed differences arise."
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html",
        "document": "This tutorial introduces the fundamental concepts of PyTorch through self-contained examples.\n\nAt its core, PyTorch provides two main features:\n• None An n-dimensional Tensor, similar to numpy but can run on GPUs\n\nWe will use a problem of fitting \\(y=\\sin(x)\\) with a third order polynomial as our running example. The network will have four parameters, and will be trained with gradient descent to fit random data by minimizing the Euclidean distance between the network output and the true output.\n\nBefore introducing PyTorch, we will first implement the network using numpy. Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a third order polynomial to sine function by manually implementing the forward and backward passes through the network using numpy operations: # Backprop to compute gradients of a, b, c, d with respect to loss Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning. Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Behind the scenes, Tensors can keep track of a computational graph and gradients, but they’re also useful as a generic tool for scientific computing. Also unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you simply need to specify the correct device. Here we use PyTorch Tensors to fit a third order polynomial to sine function. Like the numpy example above we need to manually implement the forward and backward passes through the network: # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU # Backprop to compute gradients of a, b, c, d with respect to loss\n\nIn the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks. Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients. This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If is a Tensor that has then is another Tensor holding the gradient of with respect to some scalar value. Here we use PyTorch Tensors and autograd to implement our fitting sine wave with third order polynomial example; now we no longer need to manually implement the backward pass through the network: # We want to be able to train our model on an `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__ # such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU. # By default, requires_grad=False, which indicates that we do not need to # compute gradients with respect to these Tensors during the backward pass. # Create random Tensors for weights. For a third order polynomial, we need # Setting requires_grad=True indicates that we want to compute gradients with # respect to these Tensors during the backward pass. # Compute and print loss using operations on Tensors. # Now loss is a Tensor of shape (1,) # loss.item() gets the scalar value held in the loss. # Use autograd to compute the backward pass. This call will compute the # gradient of loss with respect to all Tensors with requires_grad=True. # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding # the gradient of the loss with respect to a, b, c, d respectively. # because weights have requires_grad=True, but we don't need to track this # Manually zero the gradients after updating weights Under the hood, each primitive autograd operator is really two functions that operate on Tensors. The forward function computes output Tensors from input Tensors. The backward function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value. In PyTorch we can easily define our own autograd operator by defining a subclass of and implementing the and functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data. In this example we define our model as \\(y=a+b P_3(c+dx)\\) instead of \\(y=a+bx+cx^2+dx^3\\), where \\(P_3(x)=\\frac{1}{2}\\left(5x^3-3x\\right)\\) is the Legendre polynomial of degree three. We write our own custom autograd function for computing forward and backward of \\(P_3\\), and use it to implement our model: We can implement our own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward passes In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU # By default, requires_grad=False, which indicates that we do not need to # compute gradients with respect to these Tensors during the backward pass. # Create random Tensors for weights. For this example, we need # 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized # not too far from the correct result to ensure convergence. # Setting requires_grad=True indicates that we want to compute gradients with # respect to these Tensors during the backward pass. # To apply our Function, we use Function.apply method. We alias this as 'P3'. # Use autograd to compute the backward pass. # Manually zero the gradients after updating weights\n\nComputational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives; however for large neural networks raw autograd can be a bit too low-level. When building neural networks we frequently think of arranging the computation into layers, some of which have learnable parameters which will be optimized during learning. In TensorFlow, packages like Keras, TensorFlow-Slim, and TFLearn provide higher-level abstractions over raw computational graphs that are useful for building neural networks. In PyTorch, the package serves this same purpose. The package defines a set of Modules, which are roughly equivalent to neural network layers. A Module receives input Tensors and computes output Tensors, but may also hold internal state such as Tensors containing learnable parameters. The package also defines a set of useful loss functions that are commonly used when training neural networks. In this example we use the package to implement our polynomial model network: # For this example, the output y is a linear function of (x, x^2, x^3), so # we can consider it as a linear layer neural network. Let's prepare the # In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape # (3,), for this case, broadcasting semantics will apply to obtain a tensor # Use the nn package to define our model as a sequence of layers. nn.Sequential # is a Module which contains other Modules, and applies them in sequence to # produce its output. The Linear Module computes output from input using a # linear function, and holds internal Tensors for its weight and bias. # The Flatten layer flatens the output of the linear layer to a 1D tensor, # to match the shape of `y`. # The nn package also contains definitions of popular loss functions; in this # case we will use Mean Squared Error (MSE) as our loss function. # Forward pass: compute predicted y by passing x to the model. Module objects # override the __call__ operator so you can call them like functions. When # doing so you pass a Tensor of input data to the Module and it produces # Compute and print loss. We pass Tensors containing the predicted and true # values of y, and the loss function returns a Tensor containing the # Zero the gradients before running the backward pass. # Backward pass: compute gradient of the loss with respect to all the learnable # parameters of the model. Internally, the parameters of each Module are stored # in Tensors with requires_grad=True, so this call will compute gradients for # all learnable parameters in the model. # Update the weights using gradient descent. Each parameter is a Tensor, so # we can access its gradients like we did before. # You can access the first layer of `model` like accessing the first item of a list # For linear layer, its parameters are stored as `weight` and `bias`. Up to this point we have updated the weights of our models by manually mutating the Tensors holding learnable parameters with . This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like , , , and other. The package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms. In this example we will use the package to define our model as before, but we will optimize the model using the algorithm provided by the package: # Use the nn package to define our model and loss function. # Use the optim package to define an Optimizer that will update the weights of # the model for us. Here we will use RMSprop; the optim package contains many other # optimization algorithms. The first argument to the RMSprop constructor tells the # optimizer which Tensors it should update. # Forward pass: compute predicted y by passing x to the model. # Before the backward pass, use the optimizer object to zero all of the # gradients for the variables it will update (which are the learnable # weights of the model). This is because by default, gradients are # accumulated in buffers( i.e, not overwritten) whenever .backward() # is called. Checkout docs of torch.autograd.backward for more details. # Backward pass: compute gradient of the loss with respect to model # Calling the step function on an Optimizer makes an update to its Sometimes you will want to specify models that are more complex than a sequence of existing Modules; for these cases you can define your own Modules by subclassing and defining a which receives input Tensors and produces output Tensors using other modules or other autograd operations on Tensors. In this example we implement our third order polynomial as a custom Module subclass: In the constructor we instantiate four parameters and assign them as In the forward function we accept a Tensor of input data and we must return a Tensor of output data. We can use Modules defined in the constructor as well as arbitrary operators on Tensors. Just like any class in Python, you can also define custom method on PyTorch modules # Construct our model by instantiating the class defined above # Construct our loss function and an Optimizer. The call to model.parameters() # in the SGD constructor will contain the learnable parameters (defined # with torch.nn.Parameter) which are members of the model. # Forward pass: Compute predicted y by passing x to the model # Zero gradients, perform a backward pass, and update the weights. As an example of dynamic graphs and weight sharing, we implement a very strange model: a third-fifth order polynomial that on each forward pass chooses a random number between 3 and 5 and uses that many orders, reusing the same weights multiple times to compute the fourth and fifth order. For this model we can use normal Python flow control to implement the loop, and we can implement weight sharing by simply reusing the same parameter multiple times when defining the forward pass. We can easily implement this model as a Module subclass: In the constructor we instantiate five parameters and assign them as members. For the forward pass of the model, we randomly choose either 4, 5 and reuse the e parameter to compute the contribution of these orders. Since each forward pass builds a dynamic computation graph, we can use normal Python control-flow operators like loops or conditional statements when defining the forward pass of the model. Here we also see that it is perfectly safe to reuse the same parameter many Just like any class in Python, you can also define custom method on PyTorch modules # Construct our model by instantiating the class defined above # Construct our loss function and an Optimizer. Training this strange model with # vanilla stochastic gradient descent is tough, so we use momentum # Forward pass: Compute predicted y by passing x to the model # Zero gradients, perform a backward pass, and update the weights."
    },
    {
        "link": "https://github.com/xbeat/Machine-Learning/blob/main/Deep%20Learning%20Titans%20TensorFlow%20vs.%20PyTorch%20in%20Python.md",
        "document": "TensorFlow and PyTorch are two of the most popular deep learning frameworks in the world of artificial intelligence and machine learning. Both offer powerful tools for building and training neural networks, but they have different philosophies and strengths. In this presentation, we'll explore the key features, similarities, and differences between these two titans of deep learning.\n\nTensorFlow, developed by Google, is an open-source library for numerical computation and large-scale machine learning. It uses a static computational graph approach, which means the graph is defined before the model runs. This can lead to optimized performance, especially in production environments.\n\nPyTorch, developed by Facebook's AI Research lab, is known for its dynamic computational graph. This allows for more flexible and intuitive model development, making it popular among researchers and those who prefer a more \"Pythonic\" approach to deep learning.\n\nBoth frameworks use tensors as their primary data structure. Tensors are multi-dimensional arrays that can represent various types of data. Let's compare basic tensor operations in TensorFlow and PyTorch.\n\nTensorFlow's Keras API and PyTorch both offer ways to define neural network architectures. Let's compare how to create a simple feedforward neural network in both frameworks.\n\nTensorFlow 2.x introduced eager execution, making it more similar to PyTorch's dynamic computation graph. Let's compare training loops in both frameworks.\n\nEfficient data loading and preprocessing are crucial for deep learning projects. Both TensorFlow and PyTorch offer tools for handling datasets. Let's compare their approaches.\n\nBoth frameworks provide mechanisms to save and load models, which is essential for deploying models and resuming training. Let's compare the approaches.\n\nBoth TensorFlow and PyTorch integrate with visualization tools to help understand model architecture and training progress. TensorFlow uses TensorBoard, while PyTorch can use TensorBoard or other libraries like Visdom.\n\nWhen it comes to deploying models in production, both frameworks offer solutions. TensorFlow has TensorFlow Serving, while PyTorch uses TorchServe.\n\nLet's compare how to implement a simple image classification model using both frameworks. We'll use a pre-trained ResNet model for this example.\n\nNow, let's compare how to implement a simple sentiment analysis model using both frameworks. We'll use pre-trained word embeddings for this example.\n\nBoth TensorFlow and PyTorch have extensive ecosystems and strong community support. TensorFlow benefits from Google's backing and integration with other Google tools, while PyTorch is known for its research-friendly design and growing adoption in academia.\n\nPerformance can vary depending on the specific use case, hardware, and implementation. Both frameworks have made significant improvements in speed and efficiency over time.\n\nSlide 15: Conclusion and Choosing the Right Framework\n\nBoth TensorFlow and PyTorch are excellent choices for deep learning projects. TensorFlow excels in production environments and has strong support for mobile and embedded deployments. PyTorch is often preferred for research and rapid prototyping due to its dynamic nature and Pythonic design.\n\nThe choice between TensorFlow and PyTorch often comes down to personal preference, project requirements, and team expertise. Many data scientists and researchers are proficient in both frameworks, allowing them to choose the best tool for each specific task.\n\nFor those looking to deepen their understanding of TensorFlow and PyTorch, here are some valuable resources:\n• \"Deep Learning with Python\" by François Chollet (focuses on TensorFlow/Keras)\n• \"Deep Learning with PyTorch\" by Eli Stevens, Luca Antiga, and Thomas Viehmann\n\nRemember to always refer to the official documentation for the most up-to-date information on these rapidly evolving frameworks."
    },
    {
        "link": "https://stackoverflow.com/questions/42599498/numerically-stable-softmax",
        "document": "The softmax exp(x)/sum(exp(x)) is actually numerically well-behaved. It has only positive terms, so we needn't worry about loss of significance, and the denominator is at least as large as the numerator, so the result is guaranteed to fall between 0 and 1.\n\nThe only accident that might happen is over- or under-flow in the exponentials. Overflow of a single or underflow of all elements of x will render the output more or less useless.\n\nBut it is easy to guard against that by using the identity softmax(x) = softmax(x + c) which holds for any scalar c: Subtracting max(x) from x leaves a vector that has only non-positive entries, ruling out overflow and at least one element that is zero ruling out a vanishing denominator (underflow in some but not all entries is harmless).\n\nFootnote: theoretically, catastrophic accidents in the sum are possible, but you'd need a ridiculous number of terms. For example, even using 16 bit floats which can only resolve 3 decimals---compared to 15 decimals of a \"normal\" 64 bit float---we'd need between 2^1431 (~6 x 10^431) and 2^1432 to get a sum that is off by a factor of two."
    },
    {
        "link": "https://tensorflow.org/guide/mixed_precision",
        "document": "Mixed precision is the use of both 16-bit and 32-bit floating-point types in a model during training to make it run faster and use less memory. By keeping certain parts of the model in the 32-bit types for numeric stability, the model will have a lower step time and train equally as well in terms of the evaluation metrics such as accuracy. This guide describes how to use the Keras mixed precision API to speed up your models. Using this API can improve performance by more than 3 times on modern GPUs, 60% on TPUs and more than 2 times on latest Intel CPUs.\n\nToday, most models use the float32 dtype, which takes 32 bits of memory. However, there are two lower-precision dtypes, float16 and bfloat16, each which take 16 bits of memory instead. Modern accelerators can run operations faster in the 16-bit dtypes, as they have specialized hardware to run 16-bit computations and 16-bit dtypes can be read from memory faster.\n\nNVIDIA GPUs can run operations in float16 faster than in float32, and TPUs and supporting Intel CPUs can run operations in bfloat16 faster than float32. Therefore, these lower-precision dtypes should be used whenever possible on those devices. However, variables and a few computations should still be in float32 for numeric reasons so that the model trains to the same quality. The Keras mixed precision API allows you to use a mix of either float16 or bfloat16 with float32, to get the performance benefits from float16/bfloat16 and the numeric stability benefits from float32.\n\nWhile mixed precision will run on most hardware, it will only speed up models on recent NVIDIA GPUs, Cloud TPUs and recent Intel CPUs. NVIDIA GPUs support using a mix of float16 and float32, while TPUs and Intel CPUs support a mix of bfloat16 and float32.\n\nAmong NVIDIA GPUs, those with compute capability 7.0 or higher will see the greatest performance benefit from mixed precision because they have special hardware units, called Tensor Cores, to accelerate float16 matrix multiplications and convolutions. Older GPUs offer no math performance benefit for using mixed precision, however memory and bandwidth savings can enable some speedups. You can look up the compute capability for your GPU at NVIDIA's CUDA GPU web page. Examples of GPUs that will benefit most from mixed precision include RTX GPUs, the V100, and the A100.\n\nAmong Intel CPUs, starting with the 4th Gen Intel Xeon Processors (code name Sapphire Rapids), will see the greatest performance benefit from mixed precision as they can accelerate bfloat16 computations using AMX instructions (requires Tensorflow 2.12 or later).\n\nYou can check your GPU type with the following. The command only exists if the NVIDIA drivers are installed, so the following will raise an error otherwise.\n\nEven on older Intel CPUs, other x86 CPUs without AMX, and older GPUs, where no speedup is expected, mixed precision APIs can still be used for unit testing, debugging, or just to try out the API. However, mixed_bfloat16 on CPUs without AMX instructions and mixed_float16 on all x86 CPUs will run significantly slower.\n\nTo use mixed precision in Keras, you need to create a , typically referred to as a dtype policy. Dtype policies specify the dtypes layers will run in. In this guide, you will construct a policy from the string and set it as the global policy. This will cause subsequently created layers to use mixed precision with a mix of float16 and float32.\n\nFor short, you can directly pass a string to , which is typically done in practice.\n\nThe policy specifies two important aspects of a layer: the dtype the layer's computations are done in, and the dtype of a layer's variables. Above, you created a policy (i.e., a created by passing the string to its constructor). With this policy, layers use float16 computations and float32 variables. Computations are done in float16 for performance, but variables must be kept in float32 for numeric stability. You can directly query these properties of the policy.\n\nAs mentioned before, the policy will most significantly improve performance on NVIDIA GPUs with compute capability of at least 7.0. The policy will run on other GPUs and CPUs but may not improve performance. For TPUs and CPUs, the policy should be used instead.\n\nNext, let's start building a simple model. Very small toy models typically do not benefit from mixed precision, because overhead from the TensorFlow runtime typically dominates the execution time, making any performance improvement on the GPU negligible. Therefore, let's build two large layers with 4096 units each if a GPU is used.\n\nEach layer has a policy and uses the global policy by default. Each of the layers therefore have the policy because you set the global policy to previously. This will cause the dense layers to do float16 computations and have float32 variables. They cast their inputs to float16 in order to do float16 computations, which causes their outputs to be float16 as a result. Their variables are float32 and will be cast to float16 when the layers are called to avoid errors from dtype mismatches.\n\nNext, create the output predictions. Normally, you can create the output predictions as follows, but this is not always numerically stable with float16.\n\nA softmax activation at the end of the model should be float32. Because the dtype policy is , the softmax activation would normally have a float16 compute dtype and output float16 tensors.\n\nThis can be fixed by separating the Dense and softmax layers, and by passing to the softmax layer:\n\nPassing to the softmax layer constructor overrides the layer's dtype policy to be the policy, which does computations and keeps variables in float32. Equivalently, you could have instead passed ; layers always convert the dtype argument to a policy. Because the layer has no variables, the policy's variable dtype is ignored, but the policy's compute dtype of float32 causes softmax and the model output to be float32.\n\nAdding a float16 softmax in the middle of a model is fine, but a softmax at the end of the model should be in float32. The reason is that if the intermediate tensor flowing from the softmax to the loss is float16 or bfloat16, numeric issues may occur.\n\nYou can override the dtype of any layer to be float32 by passing if you think it will not be numerically stable with float16 computations. But typically, this is only necessary on the last layer of the model, as most layers have sufficient precision with and .\n\nEven if the model does not end in a softmax, the outputs should still be float32. While unnecessary for this specific model, the model outputs can be cast to float32 with the following:\n\nNext, finish and compile the model, and generate input data:\n\nThis example casts the input data from int8 to float32. You don't cast to float16 since the division by 255 is on the CPU, which runs float16 operations slower than float32 operations. In this case, the performance difference is negligible, but in general you should run input processing math in float32 if it runs on the CPU. The first layer of the model will cast the inputs to float16, as each layer casts floating-point inputs to its compute dtype.\n\nThe initial weights of the model are retrieved. This will allow training from scratch again by loading the weights.\n\nNotice the model prints the time per step in the logs: for example, \"25ms/step\". The first epoch may be slower as TensorFlow spends some time optimizing the model, but afterwards the time per step should stabilize.\n\nIf you are running this guide in Colab, you can compare the performance of mixed precision with float32. To do so, change the policy from to in the \"Setting the dtype policy\" section, then rerun all the cells up to this point. On GPUs with compute capability 7.X, you should see the time per step significantly increase, indicating mixed precision sped up the model. Make sure to change the policy back to and rerun the cells before continuing with the guide.\n\nOn GPUs with compute capability of at least 8.0 (Ampere GPUs and above), you likely will see no performance improvement in the toy model in this guide when using mixed precision compared to float32. This is due to the use of TensorFloat-32, which automatically uses lower precision math in certain float32 ops such as . TensorFloat-32 gives some of the performance advantages of mixed precision when using float32. However, in real-world models, you will still typically experience significant performance improvements from mixed precision due to memory bandwidth savings and ops which TensorFloat-32 does not support.\n\nIf running mixed precision on a TPU, you will not see as much of a performance gain compared to running mixed precision on GPUs, especially pre-Ampere GPUs. This is because TPUs do certain ops in bfloat16 under the hood even with the default dtype policy of float32. This is similar to how Ampere GPUs use TensorFloat-32 by default. Compared to Ampere GPUs, TPUs typically see less performance gains with mixed precision on real-world models.\n\nFor many real-world models, mixed precision also allows you to double the batch size without running out of memory, as float16 tensors take half the memory. This does not apply however to this toy model, as you can likely run the model in any dtype where each batch consists of the entire MNIST dataset of 60,000 images.\n\nLoss scaling is a technique which automatically performs with the policy to avoid numeric underflow. This section describes what loss scaling is and the next section describes how to use it with a custom training loop.\n\nThe float16 data type has a narrow dynamic range compared to float32. This means values above \\(65504\\) will overflow to infinity and values below \\(6.0 \\times 10^{-8}\\) will underflow to zero. float32 and bfloat16 have a much higher dynamic range so that overflow and underflow are not a problem.\n\nIn practice, overflow with float16 rarely occurs. Additionally, underflow also rarely occurs during the forward pass. However, during the backward pass, gradients can underflow to zero. Loss scaling is a technique to prevent this underflow.\n\nThe basic concept of loss scaling is simple: simply multiply the loss by some large number, say \\(1024\\), and you get the loss scale value. This will cause the gradients to scale by \\(1024\\) as well, greatly reducing the chance of underflow. Once the final gradients are computed, divide them by \\(1024\\) to bring them back to their correct values.\n\nThe pseudocode for this process is:\n\nChoosing a loss scale can be tricky. If the loss scale is too low, gradients may still underflow to zero. If too high, the opposite the problem occurs: the gradients may overflow to infinity.\n\nTo solve this, TensorFlow dynamically determines the loss scale so you do not have to choose one manually. If you use , loss scaling is done for you so you do not have to do any extra work. If you use a custom training loop, you must explicitly use the special optimizer wrapper in order to use loss scaling. This is described in the next section.\n\nSo far, you have trained a Keras model with mixed precision using . Next, you will use mixed precision with a custom training loop. If you do not already know what a custom training loop is, please read the Custom training guide first.\n\nRunning a custom training loop with mixed precision requires two changes over running it in float32:\n• Build the model with mixed precision (you already did this)\n• Explicitly use loss scaling if is used.\n\nFor step (2), you will use the class, which wraps an optimizer and applies loss scaling. By default, it dynamically determines the loss scale so you do not have to choose one. Construct a as follows.\n\nIf you want, it is possible choose an explicit loss scale or otherwise customize the loss scaling behavior, but it is highly recommended to keep the default loss scaling behavior, as it has been found to work well on all known models. See the documentation if you want to customize the loss scaling behavior.\n\nNext, define the loss object and the s:\n\nNext, define the training step function. You will use two new methods from the loss scale optimizer to scale the loss and unscale the gradients:\n• : Multiplies the loss by the loss scale\n• : Takes in a list of scaled gradients as inputs, and divides each one by the loss scale to unscale them\n\nThese functions must be used in order to prevent underflow in the gradients. will then apply gradients if none of them have s or s. It will also update the loss scale, halving it if the gradients had s or s and potentially increasing it otherwise.\n\nThe will likely skip the first few steps at the start of training. The loss scale starts out high so that the optimal loss scale can quickly be determined. After a few steps, the loss scale will stabilize and very few steps will be skipped. This process happens automatically and does not affect training quality.\n\nLoad the initial weights of the model, so you can retrain from scratch:\n\nHere are some performance tips when using mixed precision on GPUs.\n\nIf it doesn't affect model quality, try running with double the batch size when using mixed precision. As float16 tensors use half the memory, this often allows you to double your batch size without running out of memory. Increasing batch size typically increases training throughput, i.e. the training elements per second your model can run on.\n\nAs mentioned previously, modern NVIDIA GPUs use a special hardware unit called Tensor Cores that can multiply float16 matrices very quickly. However, Tensor Cores requires certain dimensions of tensors to be a multiple of 8. In the examples below, an argument is bold if and only if it needs to be a multiple of 8 for Tensor Cores to be used.\n• tf.keras.layers.Conv2d(filters=48, kernel_size=7, stride=3)\n• And similarly for other convolutional layers, such as tf.keras.layers.Conv3d\n• tf.keras.layers.LSTM(units=64)\n• And similar for other RNNs, such as tf.keras.layers.GRU\n\nYou should try to use Tensor Cores when possible. If you want to learn more, NVIDIA deep learning performance guide describes the exact requirements for using Tensor Cores as well as other Tensor Core-related performance information.\n\nXLA is a compiler that can further increase mixed precision performance, as well as float32 performance to a lesser extent. Refer to the XLA guide for details.\n\nAs with GPUs, you should try doubling your batch size when using Cloud TPUs because bfloat16 tensors use half the memory. Doubling batch size may increase training throughput.\n\nTPUs do not require any other mixed precision-specific tuning to get optimal performance. They already require the use of XLA. TPUs benefit from having certain dimensions being multiples of \\(128\\), but this applies equally to the float32 type as it does for mixed precision. Check the Cloud TPU performance guide for general TPU performance tips, which apply to mixed precision as well as float32 tensors.\n• You should use mixed precision if you use TPUs, NVIDIA GPUs with at least compute capability 7.0, or Intel CPUs with support for AMX instructions, as it will improve performance by up to 3x.\n• None You can use mixed precision with the following lines: # On TPUs and CPUs, use 'mixed_bfloat16' instead\n• None If your model ends in softmax, make sure it is float32. And regardless of what your model ends in, make sure the output is float32.\n• None If you use a custom training loop with , in addition to the above lines, you need to wrap your optimizer with a . Then call to scale the loss, and to unscale the gradients.\n• None If you use a custom training loop with , setting the global_policy mentioned above is sufficient.\n• None Double the training batch size if it does not reduce evaluation accuracy\n• None On GPUs, ensure most tensor dimensions are a multiple of \\(8\\) to maximize performance\n\nFor an example of mixed precision using the API, check functions and classes related to training performance. Check out the official models, such as Transformer, for details."
    }
]