[
    {
        "link": "https://petsc.org/release/manualpages/Sys/PetscInt",
        "document": "PETSc type that represents an integer, used primarily to represent size of arrays and indexing into arrays. Its size can be configured with the option to be either 32-bit (default) or 64-bit.\n\nFor MPI calls that require datatypes, use as the datatype for . It will automatically work correctly regardless of the size of ."
    },
    {
        "link": "https://petsc.org/release/manualpages/Sys/PetscInt.html",
        "document": ""
    },
    {
        "link": "https://petsc.org/main/manualpages/Sys/PetscInt",
        "document": "PETSc type that represents an integer, used primarily to represent size of arrays and indexing into arrays. Its size can be configured with the option to be either 32-bit (default) or 64-bit.\n\nFor MPI calls that require datatypes, use as the datatype for . It will automatically work correctly regardless of the size of ."
    },
    {
        "link": "https://mcs.anl.gov/petsc/petsc-3.15/docs/docs/manualpages/Sys/PetscInt.html",
        "document": ""
    },
    {
        "link": "https://web.cels.anl.gov/projects/petsc/vault/petsc-3.19/docs/manualpages/Sys/PetscInt.html",
        "document": "PETSc type that represents an integer, used primarily to represent size of arrays and indexing into arrays. Its size can be configured with the option to be either 32-bit (default) or 64-bit.\n\nFor MPI calls that require datatypes, use as the datatype for . It will automatically work correctly regardless of the size of ."
    },
    {
        "link": "https://stackoverflow.com/questions/1733143/converting-between-c-stdvector-and-c-array-without-copying",
        "document": "I would like to be able to convert between std::vector and its underlying C array int* without explicitly copying the data.\n\nDoes std::vector provide access to the underlying C array? I am looking for something like this\n\nAlso, is it possible to do the converse, i.e. how would I initialize an from a C array without copying?"
    },
    {
        "link": "https://petsc.org/release/_sources/manual/getting_started.rst.txt",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/2923272/how-to-convert-vector-to-array",
        "document": "What for? You need to clarify: Do you need a pointer to the first element of an array, or an array?\n\nIf you're calling an API function that expects the former, you can do , where is a vector of s. The elements of a vector are contiguous.\n\nOtherwise, you just have to copy each element:\n\nEnsure not only thar is big enough, but that gets filled up, or you have uninitialized values."
    },
    {
        "link": "https://petsc.org/main/manual/vec",
        "document": "Vectors (denoted by ) are used to store discrete PDE solutions, right-hand sides for linear systems, etc. Users can create and manipulate entries in vectors directly with a basic, low-level interface or they can use the PETSc objects to connect actions on vectors to the type of discretization and grid that they are working with. These higher-level interfaces handle much of the details of the interactions with vectors and hence, are preferred in most situations. This chapter is organized as follows:\n\nOne can examine (print out) a vector with the command To print the vector to the screen, one can use the viewer , which ensures that parallel vectors are printed correctly to . To display the vector in an X-window, one can use the default X-windows viewer , or one can create a viewer with the routine . A variety of viewers are discussed further in Viewers: Looking at PETSc Objects. To create a new vector of the same format and parallel layout as an existing vector, use To create several new vectors of the same format as an existing vector, use This routine creates an array of pointers to vectors. The two routines are useful because they allow one to write library code that does not depend on the particular format of the vectors being used. Instead, the subroutines can automatically create work vectors based on the specified existing vector. When a vector is no longer needed, it should be destroyed with the command To destroy an array of vectors, use the command It is also possible to create vectors that use an array the user provides rather than having PETSc internally allocate the array space. Such vectors can be created with the routines such as The pointer should be a GPU memory location for GPU vectors. Note that here, one must provide the value ; it cannot be and the user is responsible for providing enough space in the array; .\n\nMany PDE problems require ghost (or halo) values in each MPI process or even more general parallel communication of vector values. These values are needed to perform function evaluation on that MPI process. The exact structure of the ghost values needed depends on the type of grid being used. provides a uniform API for communicating the needed values. We introduce the concept in detail for . Each object defines the layout of two vectors: a distributed global vector and a local vector that includes room for the appropriate ghost points. The object provides information about the size and layout of these vectors. The user can create vector objects that use the layout information with the routines These vectors will generally serve as the building blocks for local and global PDE solutions, etc. If additional vectors with such layout information are needed in a code, they can be obtained by duplicating or via or . We emphasize that a provides the information needed to communicate the ghost value information between processes. In most cases, several different vectors can share the same communication information (or, in other words, can share a given ). The design of the object makes this easy, as each operation may operate on vectors of the appropriate size, as obtained via and or as produced by . At certain stages of many applications, there is a need to work on a local portion of the vector that includes the ghost points. This may be done by scattering a global vector into its local parts by using the two-stage commands which allows the overlap of communication and computation. Since the global and local vectors, given by and , respectively, must be compatible with the , , they should be generated by and (or be duplicates of such a vector obtained via ). The can be or among other possible values. One can scatter the local vectors into the distributed global vector with the command In general this is used with an of , because if one wishes to insert values into the global vector, they should access the global vector directly and put in the values. A third type of scatter is from a local vector (including ghost points that contain irrelevant values) to a local vector with correct ghost point values. This scatter may be done with the commands Since both local vectors, and , must be compatible with , they should be generated by (or be duplicates of such vectors obtained via ). The can be either or . In most applications, the local ghosted vectors are only needed temporarily during user “function evaluations”. PETSc provides an easy, light-weight (requiring essentially no CPU time) way to temporarily obtain these work vectors and return them when no longer needed. This is done with the routines\n\nMost users of PETSc who can utilize a will not need to utilize the lower-level routines discussed in the rest of this section and should skip ahead to Matrices. To facilitate creating general vector scatters and gathers used, for example, in updating ghost points for problems for which no currently exists PETSc employs the concept of an index set, via the class. An index set, a generalization of a set of integer indices, is used to define scatters, gathers, and similar operations on vectors and matrices. Much of the underlying code that implements communication is built on the infrastructure discussed below. The following command creates an index set based on a list of integers: When is , this routine copies the indices passed to it by the integer array . Thus, the user should be sure to free the integer array when it is no longer needed, perhaps directly after the call to . The communicator, , should include all processes using the . Another standard index set is defined by a starting point ( ) and a stride ( ), and can be created with the command The meaning of , , and correspond to the MATLAB notation . Index sets can be destroyed with the command On rare occasions, the user may need to access information directly from an index set. Several commands assist in this process: The function returns a pointer to a list of the indices in the index set. For certain index sets, this may be a temporary array of indices created specifically for the call. Thus, once the user finishes using the array of indices, the routine should be called to ensure that the system can free the space it may have used to generate the list of indices. A blocked version of index sets can be created with the command This version is used for defining operations in which each element of the index set refers to a block of vector entries. Related routines analogous to those described above exist as well, including , , , . Most PETSc applications use a particular object to manage the communication details needed for their grids. In some rare cases, however, codes need to directly set up their required communication patterns. This is done using PETSc’s and (for more general data than vectors). One can select any subset of the components of a vector to insert or add to any subset of the components of another vector. We refer to these operations as generalized scatters, though they are a combination of scatters and gathers. To copy selected components from one vector to another, one uses the following set of commands: Here denotes the index set of the first vector, while indicates the index set of the destination vector. The vectors can be parallel or sequential. The only requirements are that the number of entries in the index set of the first vector, , equals the number in the destination index set, , and that the vectors be long enough to contain all the indices referred to in the index sets. If both and are parallel, their communicator must have the same set of processes, but their process order can differ. The argument specifies that the vector elements will be inserted into the specified locations of the destination vector, overwriting any existing values. To add the components, rather than insert them, the user should select the option instead of . One can also use or to replace the destination with the maximal or minimal of its current value and the scattered values. To perform a conventional gather operation, the user makes the destination index set, , be a stride index set with a stride of one. Similarly, a conventional scatter can be done with an initial (sending) index set consisting of a stride. The scatter routines are collective operations (i.e. all processes that own a parallel vector must call the scatter routines). When scattering from a parallel vector to sequential vectors, each process has its own sequential vector that receives values from locations as indicated in its own index set. Similarly, in scattering from sequential vectors to a parallel vector, each process has its own sequential vector that contributes to the parallel vector. Caution: When is used, if two different processes contribute different values to the same component in a parallel vector, either value may be inserted. When is used, the correct sum is added to the correct location. In some cases, one may wish to “undo” a scatter, that is, perform the scatter backward, switching the roles of the sender and receiver. This is done by using Note that the roles of the first two arguments to these routines must be swapped whenever the option is used. Once a object has been created, it may be used with any vectors that have the same parallel data layout. That is, one can call and with different vectors than used in the call to as long as they have the same parallel layout (the number of elements on each process are the same). Usually, these “different” vectors would have been obtained via calls to from the original vectors used in the call to . can only access local values from the vector. To get off-process values, the user should create a new vector where the components will be stored and then perform the appropriate vector scatter. For example, if one desires to obtain the values of the 100th and 200th entries of a parallel vector, , one could use a code such as that below. In this example, the values of the 100th and 200th components are placed in the array values. In this example, each process now has the 100th and 200th component, but obviously, each process could gather any elements it needed, or none by creating an index set with no entries. The scatter comprises two stages to allow for the overlap of communication and computation. The introduction of the context allows the communication patterns for the scatter to be computed once and reused repeatedly. Generally, even setting up the communication for a scatter requires communication; hence, it is best to reuse such information when possible. Scatters provide a very general method for managing the communication of required ghost values for unstructured grid computations. One scatters the global vector into a local “ghosted” work vector, performs the computation on the local work vectors, and then scatters back into the global solution vector. In the simplest case, this may be written as /* For example, do local calculations from localin to localout */ In this case, the scatter is used in a way similar to the usage of and discussed above. When working with a global representation of a vector (usually on a vector obtained with ) and a local representation of the same vector that includes ghost points required for local computation (obtained with ). PETSc provides routines to help map indices from a local numbering scheme to the PETSc global numbering scheme, recall their use above for the routine introduced above. This is done via the following routines Here denotes the number of local indices, contains the global number of each local number, and is the resulting PETSc object that contains the information needed to apply the mapping with either or . Note that the routines serve a different purpose than the routines. In the former case, they provide a mapping from a local numbering scheme (including ghost points) to a global numbering scheme, while in the latter, they provide a mapping between two global numbering schemes. Many applications may use both and routines. The routines are first used to map from an application global ordering (that has no relationship to parallel processing, etc.) to the PETSc ordering scheme (where each process has a contiguous set of indices in the numbering). Then, to perform function or Jacobian evaluations locally on each process, one works with a local numbering scheme that includes ghost points. The mapping from this local numbering scheme back to the global PETSc numbering can be handled with the routines. If one is given a list of block indices in a global numbering, the routine will provide a new list of indices in the local numbering. Again, negative values in are left unmapped. But in addition, if is set to , then is set to and all global values in that are not represented in the local to global mapping are replaced by -1. When is set to , the values in that are not represented locally in the mapping are not included in , so that potentially is smaller than . One must pass in an array long enough to hold all the indices. One can call with equal to to determine the required length (returned in ) and then allocate the required space and call a second time to set the values. Often it is convenient to set elements into a vector using the local node numbering rather than the global node numbering (e.g., each process may maintain its own sublist of vertices and elements and number them locally). To set values into a vector with the local numbering, one must first call Now the use the local numbering rather than the global, meaning the entries lie in \\([0,n)\\) where \\(n\\) is the local size of the vector. Global vectors obtained from a already have the global to local mapping provided by the . One can use global indices with or to assemble global stiffness matrices. Alternately, the global node number of each local node, including the ghost nodes, can be obtained by calling Now, entries may be added to the vector and matrix using the local numbering and and . The example SNES Tutorial ex5 illustrates the use of a in the solution of a nonlinear problem. The analogous Fortran program is SNES Tutorial ex5f90; see SNES: Nonlinear Solvers for a discussion of the nonlinear solvers. There are two minor drawbacks to the basic approach described above for unstructured grids:\n• None the extra memory requirement for the local work vector, , which duplicates the local values in the memory in , and\n• None the extra time required to copy the local values from to . An alternative approach is to allocate global vectors with space preallocated for the ghost values. Here is the number of local vector entries, is the number of global entries (or ), and is the number of ghost entries. The array is of size and contains the global vector location for each local ghost location. Using or on a ghosted vector will generate additional ghosted vectors. In many ways, a ghosted vector behaves like any other MPI vector created by . The difference is that the ghosted vector has an additional “local” representation that allows one to access the ghost locations. This is done through the call to The vector is a sequential representation of the parallel vector that shares the same array space (and hence numerical values); but allows one to access the “ghost” values past “the end of the” array. Note that one accesses the entries in using the local numbering of elements and ghosts, while they are accessed in using the global numbering. A common usage of a ghosted vector is given by The routines and are equivalent to the routines and above, except that since they are scattering into the ghost locations, they do not need to copy the local vector values, which are already in place. In addition, the user does not have to allocate the local work vector since the ghosted vector already has allocated slots to contain the ghost values. The input arguments and cause the ghost values to be correctly updated from the appropriate process. The arguments and update the “local” portions of the vector from all the other processes’ ghost values. This would be appropriate, for example, when performing a finite element assembly of a load vector. One can also use or with . does not yet support ghosted vectors sharing memory with the global representation. This is a work in progress; if you are interested in this feature, please contact the PETSc community members. Partitioning discusses the important topic of partitioning an unstructured grid."
    },
    {
        "link": "https://petsc.org/release/manual/vec",
        "document": "Vectors (denoted by ) are used to store discrete PDE solutions, right-hand sides for linear systems, etc. Users can create and manipulate entries in vectors directly with a basic, low-level interface or they can use the PETSc objects to connect actions on vectors to the type of discretization and grid that they are working with. These higher-level interfaces handle much of the details of the interactions with vectors and hence, are preferred in most situations. This chapter is organized as follows:\n\nMany PDE problems require ghost (or halo) values in each MPI process or even more general parallel communication of vector values. These values are needed to perform function evaluation on that MPI process. The exact structure of the ghost values needed depends on the type of grid being used. provides a uniform API for communicating the needed values. We introduce the concept in detail for . Each object defines the layout of two vectors: a distributed global vector and a local vector that includes room for the appropriate ghost points. The object provides information about the size and layout of these vectors. The user can create vector objects that use the layout information with the routines These vectors will generally serve as the building blocks for local and global PDE solutions, etc. If additional vectors with such layout information are needed in a code, they can be obtained by duplicating or via or . We emphasize that a provides the information needed to communicate the ghost value information between processes. In most cases, several different vectors can share the same communication information (or, in other words, can share a given ). The design of the object makes this easy, as each operation may operate on vectors of the appropriate size, as obtained via and or as produced by . At certain stages of many applications, there is a need to work on a local portion of the vector that includes the ghost points. This may be done by scattering a global vector into its local parts by using the two-stage commands which allows the overlap of communication and computation. Since the global and local vectors, given by and , respectively, must be compatible with the , , they should be generated by and (or be duplicates of such a vector obtained via ). The can be or among other possible values. One can scatter the local vectors into the distributed global vector with the command In general this is used with an of , because if one wishes to insert values into the global vector, they should access the global vector directly and put in the values. A third type of scatter is from a local vector (including ghost points that contain irrelevant values) to a local vector with correct ghost point values. This scatter may be done with the commands Since both local vectors, and , must be compatible with , they should be generated by (or be duplicates of such vectors obtained via ). The can be either or . In most applications, the local ghosted vectors are only needed temporarily during user “function evaluations”. PETSc provides an easy, light-weight (requiring essentially no CPU time) way to temporarily obtain these work vectors and return them when no longer needed. This is done with the routines\n\nMost users of PETSc who can utilize a will not need to utilize the lower-level routines discussed in the rest of this section and should skip ahead to Matrices. To facilitate creating general vector scatters and gathers used, for example, in updating ghost points for problems for which no currently exists PETSc employs the concept of an index set, via the class. An index set, a generalization of a set of integer indices, is used to define scatters, gathers, and similar operations on vectors and matrices. Much of the underlying code that implements communication is built on the infrastructure discussed below. The following command creates an index set based on a list of integers: When is , this routine copies the indices passed to it by the integer array . Thus, the user should be sure to free the integer array when it is no longer needed, perhaps directly after the call to . The communicator, , should include all processes using the . Another standard index set is defined by a starting point ( ) and a stride ( ), and can be created with the command The meaning of , , and correspond to the MATLAB notation . Index sets can be destroyed with the command On rare occasions, the user may need to access information directly from an index set. Several commands assist in this process: The function returns a pointer to a list of the indices in the index set. For certain index sets, this may be a temporary array of indices created specifically for the call. Thus, once the user finishes using the array of indices, the routine should be called to ensure that the system can free the space it may have used to generate the list of indices. A blocked version of index sets can be created with the command This version is used for defining operations in which each element of the index set refers to a block of vector entries. Related routines analogous to those described above exist as well, including , , , . Most PETSc applications use a particular object to manage the communication details needed for their grids. In some rare cases, however, codes need to directly set up their required communication patterns. This is done using PETSc’s and (for more general data than vectors). One can select any subset of the components of a vector to insert or add to any subset of the components of another vector. We refer to these operations as generalized scatters, though they are a combination of scatters and gathers. To copy selected components from one vector to another, one uses the following set of commands: Here denotes the index set of the first vector, while indicates the index set of the destination vector. The vectors can be parallel or sequential. The only requirements are that the number of entries in the index set of the first vector, , equals the number in the destination index set, , and that the vectors be long enough to contain all the indices referred to in the index sets. If both and are parallel, their communicator must have the same set of processes, but their process order can differ. The argument specifies that the vector elements will be inserted into the specified locations of the destination vector, overwriting any existing values. To add the components, rather than insert them, the user should select the option instead of . One can also use or to replace the destination with the maximal or minimal of its current value and the scattered values. To perform a conventional gather operation, the user makes the destination index set, , be a stride index set with a stride of one. Similarly, a conventional scatter can be done with an initial (sending) index set consisting of a stride. The scatter routines are collective operations (i.e. all processes that own a parallel vector must call the scatter routines). When scattering from a parallel vector to sequential vectors, each process has its own sequential vector that receives values from locations as indicated in its own index set. Similarly, in scattering from sequential vectors to a parallel vector, each process has its own sequential vector that contributes to the parallel vector. Caution: When is used, if two different processes contribute different values to the same component in a parallel vector, either value may be inserted. When is used, the correct sum is added to the correct location. In some cases, one may wish to “undo” a scatter, that is, perform the scatter backward, switching the roles of the sender and receiver. This is done by using Note that the roles of the first two arguments to these routines must be swapped whenever the option is used. Once a object has been created, it may be used with any vectors that have the same parallel data layout. That is, one can call and with different vectors than used in the call to as long as they have the same parallel layout (the number of elements on each process are the same). Usually, these “different” vectors would have been obtained via calls to from the original vectors used in the call to . can only access local values from the vector. To get off-process values, the user should create a new vector where the components will be stored and then perform the appropriate vector scatter. For example, if one desires to obtain the values of the 100th and 200th entries of a parallel vector, , one could use a code such as that below. In this example, the values of the 100th and 200th components are placed in the array values. In this example, each process now has the 100th and 200th component, but obviously, each process could gather any elements it needed, or none by creating an index set with no entries. The scatter comprises two stages to allow for the overlap of communication and computation. The introduction of the context allows the communication patterns for the scatter to be computed once and reused repeatedly. Generally, even setting up the communication for a scatter requires communication; hence, it is best to reuse such information when possible. Scatters provide a very general method for managing the communication of required ghost values for unstructured grid computations. One scatters the global vector into a local “ghosted” work vector, performs the computation on the local work vectors, and then scatters back into the global solution vector. In the simplest case, this may be written as /* For example, do local calculations from localin to localout */ In this case, the scatter is used in a way similar to the usage of and discussed above. When working with a global representation of a vector (usually on a vector obtained with ) and a local representation of the same vector that includes ghost points required for local computation (obtained with ). PETSc provides routines to help map indices from a local numbering scheme to the PETSc global numbering scheme, recall their use above for the routine introduced above. This is done via the following routines Here denotes the number of local indices, contains the global number of each local number, and is the resulting PETSc object that contains the information needed to apply the mapping with either or . Note that the routines serve a different purpose than the routines. In the former case, they provide a mapping from a local numbering scheme (including ghost points) to a global numbering scheme, while in the latter, they provide a mapping between two global numbering schemes. Many applications may use both and routines. The routines are first used to map from an application global ordering (that has no relationship to parallel processing, etc.) to the PETSc ordering scheme (where each process has a contiguous set of indices in the numbering). Then, to perform function or Jacobian evaluations locally on each process, one works with a local numbering scheme that includes ghost points. The mapping from this local numbering scheme back to the global PETSc numbering can be handled with the routines. If one is given a list of block indices in a global numbering, the routine will provide a new list of indices in the local numbering. Again, negative values in are left unmapped. But in addition, if is set to , then is set to and all global values in that are not represented in the local to global mapping are replaced by -1. When is set to , the values in that are not represented locally in the mapping are not included in , so that potentially is smaller than . One must pass in an array long enough to hold all the indices. One can call with equal to to determine the required length (returned in ) and then allocate the required space and call a second time to set the values. Often it is convenient to set elements into a vector using the local node numbering rather than the global node numbering (e.g., each process may maintain its own sublist of vertices and elements and number them locally). To set values into a vector with the local numbering, one must first call Now the use the local numbering rather than the global, meaning the entries lie in \\([0,n)\\) where \\(n\\) is the local size of the vector. Global vectors obtained from a already have the global to local mapping provided by the . One can use global indices with or to assemble global stiffness matrices. Alternately, the global node number of each local node, including the ghost nodes, can be obtained by calling Now, entries may be added to the vector and matrix using the local numbering and and . The example SNES Tutorial ex5 illustrates the use of a in the solution of a nonlinear problem. The analogous Fortran program is SNES Tutorial ex5f90; see SNES: Nonlinear Solvers for a discussion of the nonlinear solvers. There are two minor drawbacks to the basic approach described above for unstructured grids:\n• None the extra memory requirement for the local work vector, , which duplicates the local values in the memory in , and\n• None the extra time required to copy the local values from to . An alternative approach is to allocate global vectors with space preallocated for the ghost values. Here is the number of local vector entries, is the number of global entries (or ), and is the number of ghost entries. The array is of size and contains the global vector location for each local ghost location. Using or on a ghosted vector will generate additional ghosted vectors. In many ways, a ghosted vector behaves like any other MPI vector created by . The difference is that the ghosted vector has an additional “local” representation that allows one to access the ghost locations. This is done through the call to The vector is a sequential representation of the parallel vector that shares the same array space (and hence numerical values); but allows one to access the “ghost” values past “the end of the” array. Note that one accesses the entries in using the local numbering of elements and ghosts, while they are accessed in using the global numbering. A common usage of a ghosted vector is given by The routines and are equivalent to the routines and above, except that since they are scattering into the ghost locations, they do not need to copy the local vector values, which are already in place. In addition, the user does not have to allocate the local work vector since the ghosted vector already has allocated slots to contain the ghost values. The input arguments and cause the ghost values to be correctly updated from the appropriate process. The arguments and update the “local” portions of the vector from all the other processes’ ghost values. This would be appropriate, for example, when performing a finite element assembly of a load vector. One can also use or with . does not yet support ghosted vectors sharing memory with the global representation. This is a work in progress; if you are interested in this feature, please contact the PETSc community members. Partitioning discusses the important topic of partitioning an unstructured grid."
    }
]