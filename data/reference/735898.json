[
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html",
        "document": "Return the state of the scheduler as a .\n\nIt contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas.\n\nWhen saving or loading the scheduler, please make sure to also save or load the state of the optimizer."
    },
    {
        "link": "https://pytorch.org/docs/stable/optim.html",
        "document": "Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.\n\nHow to use an optimizer¶ To use you have to construct an optimizer object that will hold the current state and will update the parameters based on the computed gradients. To construct an you have to give it an iterable containing the parameters (all should be s) or named parameters (tuples of (str, )) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc. s also support specifying per-parameter options. To do this, instead of passing an iterable of s, pass in an iterable of s. Each of them will define a separate parameter group, and should contain a key, containing a list of parameters belonging to it. Other keys should match the keyword arguments accepted by the optimizers, and will be used as optimization options for this group. For example, this is very useful when one wants to specify per-layer learning rates: This means that ’s parameters will use a learning rate of , whereas ’s parameters will stick to the default learning rate of . Finally a momentum of will be used for all parameters. You can still pass options as keyword arguments. They will be used as defaults, in the groups that didn’t override them. This is useful when you only want to vary a single option, while keeping all others consistent between parameter groups. Also consider the following example related to the distinct penalization of parameters. Remember that returns an iterable that contains all learnable parameters, including biases and other parameters that may prefer distinct penalization. To address this, one can specify individual penalization weights for each parameter group: In this manner, bias terms are isolated from non-bias terms, and a of is set specifically for the bias terms, as to avoid any penalization for this group. All optimizers implement a method, that updates the parameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be called once the gradients are computed using e.g. . Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it.\n\nSparseAdam implements a masked version of the Adam algorithm suitable for sparse gradients. Many of our algorithms have various implementations optimized for performance, readability and/or generality, so we attempt to default to the generally fastest implementation for the current device if no particular implementation has been specified by the user. We have 3 major categories of implementations: for-loop, foreach (multi-tensor), and fused. The most straightforward implementations are for-loops over the parameters with big chunks of computation. For-looping is usually slower than our foreach implementations, which combine parameters into a multi-tensor and run the big chunks of computation all at once, thereby saving many sequential kernel calls. A few of our optimizers have even faster fused implementations, which fuse the big chunks of computation into one kernel. We can think of foreach implementations as fusing horizontally and fused implementations as fusing vertically on top of that. In general, the performance ordering of the 3 implementations is fused > foreach > for-loop. So when applicable, we default to foreach over for-loop. Applicable means the foreach implementation is available, the user has not specified any implementation-specific kwargs (e.g., fused, foreach, differentiable), and all tensors are native. Note that while fused should be even faster than foreach, the implementations are newer and we would like to give them more bake-in time before flipping the switch everywhere. We summarize the stability status for each implementation on the second table below, you are welcome to try them out though! Below is a table showing the available and default implementations of each algorithm: Below table is showing the stability status for fused implementations:\n\nprovides several methods to adjust the learning rate based on the number of epochs. allows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer’s update; e.g., you should write your code this way: Most learning rate schedulers can be called back-to-back (also referred to as chaining schedulers). The result is that each scheduler is applied one after the other on the learning rate obtained by the one preceding it. In many places in the documentation, we will use the following template to refer to schedulers algorithms. Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer’s update; 1.1.0 changed this behavior in a BC-breaking way. If you use the learning rate scheduler (calling ) before the optimizer’s update (calling ), this will skip the first value of the learning rate schedule. If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check if you are calling at the wrong time. Multiply the learning rate of each parameter group by the factor given in the specified function. Decays the learning rate of each parameter group by gamma every step_size epochs. Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Multiply the learning rate of each parameter group by a small constant factor. Decays the learning rate of each parameter group by linearly changing small multiplicative factor. Decays the learning rate of each parameter group by gamma every epoch. Decays the learning rate of each parameter group using a polynomial function in the given total_iters. Set the learning rate of each parameter group using a cosine annealing schedule. Contains a list of schedulers expected to be called sequentially during the optimization process. Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). Sets the learning rate of each parameter group according to the 1cycle learning rate policy. Set the learning rate of each parameter group using a cosine annealing schedule.\n\nHow to utilize named parameters to load optimizer state dict¶ The function stores the optional content from the loaded state dict if present. However, the process of loading the optimizer state is not affected, as the order of the parameters matters to maintain compatibility (in case of different ordering). To utilize the loaded parameters names from the loaded state dict, a custom needs to be implemented according to the desired behavior. This can be useful, for instance, when the model architecture changes, but the weights and optimizer states need to remain unchanged. The following example demonstrates how to implement this customization. Let’s say that implements an expert (MoE), and we want to duplicate it and resume training for two experts, both initialized the same way as the layer. For the following we create two layers identical to and resume training by loading the model weights and optimizer states from into both and of (and adjust them accordingly): To load the state dict for with the state dict of the previous optimizer such that both and will be initialized with a copy of optimizer states (to resume training for each layer from ), we can use the following hook: # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict. # Copy the state of the corresponding parameter This ensures that the adapted state_dict with the correct states for the layers of will be used during model loading. Note that this code is designed specifically for this example (e.g., assuming a single parameter group), and other cases might require different adaptations. The following example shows how to handle missing parameters in a loaded when the model structure changes. The adds a new layer, which is not present in the original . To resume training, a custom hook is used to adapt the optimizer’s , ensuring existing parameters are mapped correctly, while missing ones (like the bypass layer) remain unchanged (as initialized in this example). This approach enables smooth loading and resuming of the optimizer state despite model changes. The new bypass layer will be trained from scratch: # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict. # Copy the state of the corresponding parameter As a third example, instead of loading a state according to the order of parameters (the default approach), this hook can be used to load according to the parameters’ names: # Copy the state of the corresponding parameter\n\nimplements Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA), implements the SWA learning rate scheduler and is a utility function used to update SWA/EMA batch normalization statistics at the end of training. SWA has been proposed in Averaging Weights Leads to Wider Optima and Better Generalization. EMA is a widely known technique to reduce the training time by reducing the number of weight updates needed. It is a variation of Polyak averaging, but using exponential weights instead of equal weights across iterations. The class serves to compute the weights of the SWA or EMA model. You can create an SWA averaged model by running: EMA models are constructed by specifying the argument as follows: Decay is a parameter between 0 and 1 that controls how fast the averaged parameters are decayed. If not provided to , the default is 0.999. Decay value should be close to 1.0, as smaller values can cause optimization convergence issues. returns a function that applies the following EMA equation to the weights: where alpha is the EMA decay. Here the model can be an arbitrary object. will keep track of the running averages of the parameters of the . To update these averages, you should use the function after the : For SWA and EMA, this call is usually done right after the optimizer . In the case of SWA, this is usually skipped for some numbers of steps at the beginning of the training. By default, computes a running equal average of the parameters that you provide, but you can also use custom averaging functions with the or parameters:\n• None allows defining a function operating on each parameter tuple (averaged parameter, model parameter) and should return the new averaged parameter.\n• None allows defining more efficient operations acting on a tuple of parameter lists, (averaged parameter list, model parameter list), at the same time, for example using the functions. This function must update the averaged parameters in-place. In the following example computes an exponential moving average using the parameter: In the following example computes an exponential moving average using the more efficient parameter: Typically, in SWA the learning rate is set to a high constant value. is a learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it constant. For example, the following code creates a scheduler that linearly anneals the learning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by setting . is a utility function that allows to compute the batchnorm statistics for the SWA model on a given dataloader at the end of training: applies the to every element in the dataloader and computes the activation statistics for each batch normalization layer in the model. assumes that each batch in the dataloader is either a tensors or a list of tensors where the first element is the tensor that the network should be applied to. If your dataloader has a different structure, you can update the batch normalization statistics of the by doing a forward pass with the on each element of the dataset. Putting it all together: SWA¶ In the example below, is the SWA model that accumulates the averages of the weights. We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule and start to collect SWA averages of the parameters at epoch 160: # Update bn statistics for the swa_model at the end # Use swa_model to make predictions on test data Putting it all together: EMA¶ In the example below, is the EMA model that accumulates the exponentially-decayed averages of the weights with a decay rate of 0.999. We train the model for a total of 300 epochs and start to collect EMA averages immediately. \\ # Update bn statistics for the ema_model at the end # Use ema_model to make predictions on test data Anneals the learning rate in each parameter group to a fixed value. Get the function applying exponential moving average (EMA) across multiple params. It performs one pass over data in to estimate the activation statistics for BatchNorm layers in the model.\n• None loader (torch.utils.data.DataLoader) – dataset loader to compute the activation statistics on. Each data batch should be either a tensor, or a list/tuple whose first element is a tensor containing data.\n• None model (torch.nn.Module) – model for which we seek to update BatchNorm statistics.\n• None device (torch.device, optional) – If set, data will be transferred to before being passed into . The utility assumes that each data batch in is either a tensor or a list or tuple of tensors; in the latter case it is assumed that should be called on the first element of the list or tuple corresponding to the data batch."
    },
    {
        "link": "https://varunbommagunta.medium.com/adjusting-learning-rate-in-pytorch-a64d36c4e226",
        "document": "Why should we adjust the learning rate?\n• Start with a value like 0.1 and the gradually decrease to 0.01,0.001,….\n• If the model is doing well at value like 0.01 then also check the values like 0.02,0.03,….\n• Doing like this might leads to have a good learning rate for your model.\n\nWe have several functions in PyTorch to adjust the learning rate:\n\nNow we will see each method,\n\nHere, we use lambda functions to set the learning rate.\n\nThe code is described clearly, observe the learning rates increasing for each epoch.\n\nSo,this is similar to the LambdaLR but for the further steps, it will multiply with the last learning rate.\n\nthen for the first epoch, lr = 0.1*0.9 = 0.09 (cache = cache*factor(0.9))\n\nthen for the second epoch, lr = 0.09*0.9 = 0.081 …….\n\nSo,learning_rate adjusts to learning_rate*gamma after every step.\n\nThis is similar to StepLR when step_size = 1, for every epochs, the learning rate decreases.\n• This is the most popular learning rate adjuster .\n• This is different from rest of the naive learning rate adjusters.\n• In this method, the learning rate adjusts when there is no improvement in the specified metrics.\n• This adjuster reads a metrics quantity and if no improvement is seen for a ‘patience’ number of epochs, the learning rate is reduced.\n\nAfter epoch 5,the loss starts climbing so after 5 epochs (patience = 5) of no decrease in loss ,the learning rate automatically reduced to 0.001."
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/ptcheat.html",
        "document": "# data and the desired file name\n\n# tensor with all 1's [or 0's] # reshapes x into size (b,a) for some b # CPU to GPU and return new object\n\n# and the kernel size is s"
    },
    {
        "link": "https://github.com/pytorch/pytorch/blob/main/torch/optim/lr_scheduler.py",
        "document": ""
    },
    {
        "link": "https://neptune.ai/blog/how-to-choose-a-learning-rate-scheduler",
        "document": "Researchers generally agree that neural network models are difficult to train. One of the biggest issues is the large number of hyperparameters to specify and optimize. The list goes on, including the number of hidden layers, activation functions, optimizers, learning rate, and regularization.\n\nTuning these hyperparameters can significantly improve neural network models. For us, as data scientists, building neural network models is about solving an optimization problem. We want to find the minima (global or sometimes local) of the objective function by gradient-based methods, such as gradient descent.\n\nOf all the gradient descent hyperparameters, the learning rate is one of the most critical ones for good model performance. In this article, we will explore this parameter and explain why scheduling our learning rate during model training is crucial.\n\nMoving from there, we’ll see how to schedule learning rates by implementing and using various schedulers in Keras. We will then create experiments in Neptune to compare how these schedulers perform.\n\nWhat is the learning rate in neural networks?\n\nWhat is the learning rate, and what does it do to a neural network? The learning rate (or step size) is explained as the magnitude of change/update to model weights during the backpropagation training process. As a configurable hyperparameter, the learning rate is usually specified as a positive value less than 1.0.\n\nIn back-propagation, model weights are updated to reduce the error estimates of our loss function. Rather than changing the weights using the full amount, we multiply it by some learning rate value. For example, setting the learning rate to 0.5 would mean updating (usually subtracting) the weights with 0.5*estimated weight errors (i.e., gradients or total error change w.r.t. the weights).\n\nThe learning rate controls how big of a step it takes for an optimizer to reach a minimum of the loss function. What does this do to our optimization algorithm? Look at these graphs:\n• With a large learning rate (on the right), the algorithm learns fast, but it may also cause the algorithm to oscillate around or even jump over the minima. Even worse, a high learning rate equals large weight updates, which might cause the weights to overflow.\n• On the contrary, with a small learning rate (on the left), updates to the weights are small, which will guide the optimizer gradually towards the minima. However, the optimizer may take too long to converge or get stuck in a plateau or undesirable local minima;\n• A good learning rate is a tradeoff between the coverage rate and overshooting (in the middle). It’s not too small so that our algorithm can converge swiftly, and it’s not too large so that our algorithm won’t jump back and forth without reaching a minimum.\n\nAlthough the theoretical principle of finding an appropriate learning rate is straightforward (not too large, not too small), it’s easier said than done! Learning rate scheduling can help solve this problem.\n\nA learning rate schedule is a predefined framework that adjusts the learning rate between epochs or iterations as the training progresses. Two of the most common techniques for learning rate scheduling are:\n• Constant learning rate: as the name suggests, we initialize a learning rate and don’t change it during training;\n• Learning rate decay: we select an initial learning rate, then gradually reduce it in accordance with a scheduler.\n\nKnowing what learning rate schedules are, you must be wondering why we need to decrease the learning rate in the first place. Well, in a neural network, our model weights are updated as:\n\nwhere eta is the learning rate, and partial derivative is the gradient.\n\nThis is good for the training process. Early in the training, the learning rate is set to be large in order to reach a set of weights that are good enough. Over time, these weights are fine-tuned to reach higher accuracy by leveraging a small learning rate.\n\nFor the demonstration purpose, we will be working with the popular Fashion-MINIST data that comes with Keras. This dataset consists of 70,000 images (the training set and testing set are 60,000 and 10,000, respectively). These images are 28×28 pixels and are grouped into ten classes.\n\nTo compare our model performance with different learning rate schedulers, we’ll track our experiments in Neptune. Neptune monitors everything model-related. Refer to the Quickstart documentation page for detailed step-by-step instructions on how to get your Neptune projects set up and configured with Python.\n\nSpecifically, create a project named “learning-rate-scheduling” under your own workspace and save your credentials based on those instructions. Now, open a new Python script or a Jupyter Notebook (I prefer a notebook) and define a function to create Neptune Run objects using your credentials:\n\nThen, run the below snippet to import other necessary packages:\n\nNext, we’ll load the dataset with some utility functions available in Keras.\n\nTo reduce the runtime, our model will be trained against 20,000 images rather than the entire 60,000. Thus, we will randomly select 20,000 data records using the code below.\n\nOn top of that, we will also define several helper functions to save and plot the learning rate as training goes:\n• The current dataset is normalized by dividing it by 255; Thus, it’s rescaled to a range of 0-1;\n• We defined a function get_lr_metric() to save and print out the learning rate as a part of our Keras metrics. Don’t worry too much about its implementation; it simply captures the learning rate from our given optimizer at every iteration.\n\nIn addition, let’s also create a helper function to log learning rates and model performance charts to Neptune throughout our experiments:\n\nHaving the dataset and helper functions ready to go, we can now build a neural network model as an image classifier. For simplicity, our current model contains two hidden layers and an output layer with the ‘softmax’ activation function for multi-class classification:\n\nHere’s the model structure, which is a reasonably simple network.\n\nAs aforementioned, the constant schedule is the simplest scheme among all learning rate schedulers. To set a performance baseline, we will train the model using a learning rate of 0.01 consistently through all epochs:\n• created a Neptune experiment under our credentials to track the base model performance;\n• specified the learning rate using the arg. in the standard SGD optimizer in Keras;\n• added the lr_metric as a user-defined metric to monitor, which enables learning rate information to be shown in the training verbatim;\n• logged the learning rate and performance charts (loss and accuracy curves) in Neptune.\n\nLooking at the training progress, we can confirm that the current learning rate is fixed at 0.01 without changing:\n\nIn our Neptune experiment, we’ll find the following performance charts:\n\nAs learning unfolds, training loss decreases and accuracy increases (the downward trend in accuracy suggests that we need to do more epochs); nonetheless, when it comes to the validation set, model performance doesn’t change too much (just over 80%). This will be our baseline model for benchmarking with the decay schedulers later.\n\nKeras offers a built-in standard decay policy, and it can be enabled using the ExponentialDecay scheduler. First, the scheduler must be defined with logic that specifies how often the decay must happen. A good rule is to decay at every epoch, as written in the decay_rate parameter. Also, how much to decay is specified in the decay_rate parameter. The lower this parameter is, the faster the learning rate declines.\n\nOnce you define the scheduler, you can pass it to the SGD optimizer’s learning_rate parameter:\n\nIn the rest of this article, we’ll create our own schedulers using the Callback() functionality in Keras to gain a deeper understanding of how different schedulers work.\n\nThe underlying mechanism of learning rate decay is to reduce the learning rate as epochs increase. So, we basically want to specify our learning rate to be some decreasing function of epochs.\n\nAmong all potential candidates, a linear function is the most straightforward one, so the learning rate linearly decreases with epochs. Due to its simplicity, linear decay is usually considered the first attempt to experiment with.\n\nWith this scheme, the learning rate will decay to zero by the end of the training epochs. To implement linear decay:\n\nHere, we defined a class LRPolynomialDecay, where the power argument controls how fast the decay would be; that is, a smaller power makes the learning rate decay more slowly, yet a larger power makes the decay more quickly.\n\nSetting the power equal to 1 gives us linear decay, the plot of which is shown below.\n\nTo train our model with this custom linear decay, we need to:\n• Pass the result to the argument of the method of our model.\n\nRunning this model, we can see the following performance chart in our Neptune project:\n\nFrom the loss and accuracy curves on the validation set, we observe:\n• Both metrics fluctuate during the entire training process;\n• After about 30 epochs, model overfitting occurs, where training loss continues to decrease while validation loss starts to increase (and accuracy is almost flat).\n\nThis pattern indicates that our model is diverging as training goes on, and it’s most likely because the learning rate is too high.\n\nShould we reduce the learning rate as a linear function of epochs? Maybe not. It works better to have a policy where the learning rate decays faster when training begins and then gradually flattens out to a small value towards the end of the training.\n\nThis is the basic concept of non-linear decay, among which the most commonly used ones are time-based and exponential decay.\n\nThe formula for time-based decay is defined as:\n\nwhere decay is a parameter that is normally calculated as:\n\nLet’s specify the following parameters:\n\nthen this chart shows the generated learning rate curve:\n\nAs compared to the linear function, time-based decay causes the learning rate to decrease faster upon training start and much slower later. As before, let’s pass this scheduler to the LearningRateScheduler callback and log the performance charts to Neptune.\n\nHere’s the performance of this model:\n\nAs we can see, this model fits better than the linear decay one against the validation set. A couple of observations,\n• Learning almost stops at around 30 epochs as our learning rate is reduced to values close to zero;\n• Similar to the linear scenario, there are some large fluctuations when the training starts.\n\nNow, is there a way to smooth out these fluctuations? Let’s turn to exponential decay, which is defined as an exponential function of the number of epochs:\n\nAgain, specifying initial_learning_rate = 0.5 and epochs = 100 will produce the following decay curve (vs. linear and time-based decays):\n\nThe exponential scheme offers an even smoother decay path at the beginning, which should lead to a smoother training curve. Let’s run this model to find out if this is the case:\n\nHere is our result:\n\nIt’s easier to see that the training curve from exponential decay is much smoother than that from time-based decay. Overall, the exponential decay outperforms slightly.\n\nSo far, we have only looked at the continuous decay policies; how about a discrete one? Next, we’ll move on to a popular discrete staircase decay, a.k.a., step-based decay.\n\nUnder this policy, our learning rate is scheduled to reduce a certain amount every N epochs:\n\nHere, the drop_rate specifies the amount that the learning rate is modified, and the epochs_drop specifies how frequent the modification is.\n\nSame as above, setting our initial_learning_rate = 0.5 and epochs = 100 generates this step-looking learning curve:\n\nHowever, we will keep the initial learning rate at 0.1 for performance reasons and set the epoch drop rate to 20. Passing this to our model:\n\nWe would have performance charts quite similar to the linear decay, where our model almost overfits.\n\nWith various decay schemes implemented, we can now bring things together to compare how the model performs.\n\nBased on our experiments, it appears that overall, the learning stops at approximately 60 epochs; thus, for easy visualization, I have rerun all our experiments with 60 epochs:\n\nTo compare all the metrics of these experiments, you can click on the eye icons of each and switch to the charts tab:\n\nPerformance charts above from the current exercise imply that the linear decay (labeled as polynomial decay) performs the best, followed by the time-based decay.\n\nBesides SGD with a learning rate scheduler, the second most influential optimization technique is adaptive optimizers, such as AdaGrad, RMSprop, or Adam. These optimizers approximate the gradient using model internal feedback; this means that they’re almost parameter-free and are incompatible with our learning rate schedulers as opposed to SGD.\n\nAmong all the adaptive optimizers, Adam has been a favorite of machine learning practitioners. Although details about this optimizer are beyond the scope of this article, it’s worth mentioning that Adam updates the learning rate separately for each model parameter/weight. This implies that with Adam, the learning rate may first increase at early layers and thus help improve the efficiency of deep neural networks.\n\nNow for good measure, let’s train our model with the Keras default Adam optimizer as the last experiment:\n\nNow, undoubtedly, this Adam learner makes our model diverge fairly quickly:\n\nAs you can see, the test loss reaches its low after only about 10 epochs, and the model starts overfitting. This suggests that we need to introduce some regularization into our model architecture.\n\nDespite being a highly effective learner, Adam isn’t always the optimal choice right off the bat without hyperparameter tuning. SGD, on the other hand, can perform significantly better with tuned learning rates or decay schedulers.\n\nWith all our experiments, we should get a better understanding as to how important learning rate schedules are; an excessively aggressive decay results in optimizers never reaching the minima, whereas a slow decay leads to chaotic updates without significant improvement.\n• To select a learning rate schedule, a common practice is to start with a value that’s not too small, e.g., 0.5, and then exponentially lower it to get the smaller values, such as 0.01, 0.001, 0.0001;\n• Although oftentimes being the default optimizer in deep learning applications, under the hood does not necessarily outperforms all the time; it can cause model divergence.\n• To build an effective model, we should also factor in other hyperparameters, such as momentum and regularization parameters (dropout, early stostopping, etc.).\n\nFinally, it’s worth mentioning that the current result is based on one neural network and dataset. When it comes to other models using other datasets, the optimal learning rate schedule may differ. Nevertheless, this article should provide you with a guide as to how to systematically choose a learning rate scheduler that best suits your specific model and dataset."
    },
    {
        "link": "https://d2l.ai/chapter_optimization/lr-scheduler.html",
        "document": "So far we primarily focused on optimization algorithms for how to update the weight vectors rather than on the rate at which they are being updated. Nonetheless, adjusting the learning rate is often just as important as the actual algorithm. There are a number of aspects to consider:\n• None Most obviously the magnitude of the learning rate matters. If it is too large, optimization diverges, if it is too small, it takes too long to train or we end up with a suboptimal result. We saw previously that the condition number of the problem matters (see e.g., Section 12.6 for details). Intuitively it is the ratio of the amount of change in the least sensitive direction vs. the most sensitive one.\n• None Secondly, the rate of decay is just as important. If the learning rate remains large we may simply end up bouncing around the minimum and thus not reach optimality. Section 12.5 discussed this in some detail and we analyzed performance guarantees in Section 12.4. In short, we want the rate to decay, but probably more slowly than \\(\\mathcal{O}(t^{-\\frac{1}{2}})\\) which would be a good choice for convex problems.\n• None Another aspect that is equally important is initialization. This pertains both to how the parameters are set initially (review Section 5.4 for details) and also how they evolve initially. This goes under the moniker of warmup, i.e., how rapidly we start moving towards the solution initially. Large steps in the beginning might not be beneficial, in particular since the initial set of parameters is random. The initial update directions might be quite meaningless, too.\n• None Lastly, there are a number of optimization variants that perform cyclical learning rate adjustment. This is beyond the scope of the current chapter. We recommend the reader to review details in Izmailov et al. (2018), e.g., how to obtain better solutions by averaging over an entire path of parameters.\n\nGiven the fact that there is a lot of detail needed to manage learning rates, most deep learning frameworks have tools to deal with this automatically. In the current chapter we will review the effects that different schedules have on accuracy and also show how this can be managed efficiently via a learning rate scheduler.\n\nWe begin with a toy problem that is cheap enough to compute easily, yet sufficiently nontrivial to illustrate some of the key aspects. For that we pick a slightly modernized version of LeNet ( instead of activation, MaxPooling rather than AveragePooling), as applied to Fashion-MNIST. Moreover, we hybridize the network for performance. Since most of the code is standard we just introduce the basics without further detailed discussion. See Section 7 for a refresher as needed. # The code is almost identical to `d2l.train_ch6` defined in the # The code is almost identical to `d2l.train_ch6` defined in the # The code is almost identical to `d2l.train_ch6` defined in the Let’s have a look at what happens if we invoke this algorithm with default settings, such as a learning rate of \\(0.3\\) and train for \\(30\\) iterations. Note how the training accuracy keeps on increasing while progress in terms of test accuracy stalls beyond a point. The gap between both curves indicates overfitting.\n\nOne way of adjusting the learning rate is to set it explicitly at each step. This is conveniently achieved by the method. We could adjust it downward after every epoch (or even after every minibatch), e.g., in a dynamic manner in response to how optimization is progressing. More generally we want to define a scheduler. When invoked with the number of updates it returns the appropriate value of the learning rate. Let’s define a simple one that sets the learning rate to \\(\\eta = \\eta_0 (t + 1)^{-\\frac{1}{2}}\\). Let’s plot its behavior over a range of values. Now let’s see how this plays out for training on Fashion-MNIST. We simply provide the scheduler as an additional argument to the training algorithm. This worked quite a bit better than previously. Two things stand out: the curve was rather more smooth than previously. Secondly, there was less overfitting. Unfortunately it is not a well-resolved question as to why certain strategies lead to less overfitting in theory. There is some argument that a smaller stepsize will lead to parameters that are closer to zero and thus simpler. However, this does not explain the phenomenon entirely since we do not really stop early but simply reduce the learning rate gently.\n\nWhile we cannot possibly cover the entire variety of learning rate schedulers, we attempt to give a brief overview of popular policies below. Common choices are polynomial decay and piecewise constant schedules. Beyond that, cosine learning rate schedules have been found to work well empirically on some problems. Lastly, on some problems it is beneficial to warm up the optimizer prior to using large learning rates. One alternative to a polynomial decay would be a multiplicative one, that is \\(\\eta_{t+1} \\leftarrow \\eta_t \\cdot \\alpha\\) for \\(\\alpha \\in (0, 1)\\). To prevent the learning rate from decaying beyond a reasonable lower bound the update equation is often modified to \\(\\eta_{t+1} \\leftarrow \\mathop{\\mathrm{max}}(\\eta_{\\mathrm{min}}, \\eta_t \\cdot \\alpha)\\). This can also be accomplished by a built-in scheduler in MXNet via the object. It takes a few more parameters, such as warmup period, warmup mode (linear or constant), the maximum number of desired updates, etc.; Going forward we will use the built-in schedulers as appropriate and only explain their functionality here. As illustrated, it is fairly straightforward to build your own scheduler if needed. A common strategy for training deep networks is to keep the learning rate piecewise constant and to decrease it by a given amount every so often. That is, given a set of times when to decrease the rate, such as \\(s = \\{5, 10, 20\\}\\) decrease \\(\\eta_{t+1} \\leftarrow \\eta_t \\cdot \\alpha\\) whenever \\(t \\in s\\). Assuming that the values are halved at each step we can implement this as follows. The intuition behind this piecewise constant learning rate schedule is that one lets optimization proceed until a stationary point has been reached in terms of the distribution of weight vectors. Then (and only then) do we decrease the rate such as to obtain a higher quality proxy to a good local minimum. The example below shows how this can produce ever slightly better solutions. A rather perplexing heuristic was proposed by Loshchilov and Hutter (2016). It relies on the observation that we might not want to decrease the learning rate too drastically in the beginning and moreover, that we might want to “refine” the solution in the end using a very small learning rate. This results in a cosine-like schedule with the following functional form for learning rates in the range \\(t \\in [0, T]\\). Here \\(\\eta_0\\) is the initial learning rate, \\(\\eta_T\\) is the target rate at time \\(T\\). Furthermore, for \\(t > T\\) we simply pin the value to \\(\\eta_T\\) without increasing it again. In the following example, we set the max update step \\(T = 20\\). In the context of computer vision this schedule can lead to improved results. Note, though, that such improvements are not guaranteed (as can be seen below). In some cases initializing the parameters is not sufficient to guarantee a good solution. This is particularly a problem for some advanced network designs that may lead to unstable optimization problems. We could address this by choosing a sufficiently small learning rate to prevent divergence in the beginning. Unfortunately this means that progress is slow. Conversely, a large learning rate initially leads to divergence. A rather simple fix for this dilemma is to use a warmup period during which the learning rate increases to its initial maximum and to cool down the rate until the end of the optimization process. For simplicity one typically uses a linear increase for this purpose. This leads to a schedule of the form indicated below. Note that the network converges better initially (in particular observe the performance during the first 5 epochs). Warmup can be applied to any scheduler (not just cosine). For a more detailed discussion of learning rate schedules and many more experiments see also (Gotmare et al., 2018). In particular they find that a warmup phase limits the amount of divergence of parameters in very deep networks. This makes intuitively sense since we would expect significant divergence due to random initialization in those parts of the network that take the most time to make progress in the beginning."
    },
    {
        "link": "https://numberanalytics.com/blog/optimizing-learning-rate-guide-convergence-deep-models",
        "document": "Unlock the secrets of optimal learning rate tuning to accelerate convergence in deep learning models. In this article, we explore the fundamental importance of learning rate in training, outline practical tuning methods, and share valuable implementation tips for deep learning frameworks. Whether you’re a beginner or an experienced practitioner, this comprehensive guide will provide insights and techniques to help you refine your training process effectively.\n\nTraining deep learning models can be likened to navigating a rugged landscape: the goal is to find the lowest valley in a complex loss surface, and the learning rate acts as the pace at which your optimizer travels. Set it too high, and your optimizer might overshoot minima; set it too low, and the journey may take an impractical amount of time. In mathematical terms, the process of training a model often involves iterative updates governed by equations like:\n• represents the parameters of the model at iteration ,\n• and is the gradient of the loss function at .\n\nIn this article, we delve into the rationale behind selecting an optimal learning rate and discuss multiple strategies that can be adapted to practically tune and monitor the learning process across a range of deep learning architectures.\n\nA solid foundation in the concept and role of the learning rate is essential for anyone working with deep learning. In this section, we will break down the basics of learning rate, its impact on convergence, and its critical role in deep models.\n\nAt its core, the learning rate is a hyperparameter that determines the step size at each iteration while moving toward a minimum of the loss function. To elaborate:\n• Small Learning Rate:\n\n A small learning rate means the optimizer takes tiny steps. Although this can result in more accurate convergence, it also demands more iterations and computational resources, potentially causing the model to get stuck in local minima.\n• Large Learning Rate:\n\n On the other hand, a large learning rate can expedite learning but may cause divergence if step sizes are too big. Worse still, oscillations around the minimum might occur, leading to unstable training dynamics.\n\nMathematically, for a given estimated gradient descent update, the choice of η (learning rate) is critical in ensuring that:\n\nwhere L is the Lipschitz constant of the gradient of the loss function. This inequality, derived from convergence theory, helps in understanding the stability of gradient descent updates.\n\nThe convergence speed and quality of training directly depend on the right setting of the learning rate. A learning rate that is too aggressive or too timid may have several repercussions:\n• Overshooting or Oscillating:\n\n If the learning rate is high, subsequent updates may overshoot the minimum, bouncing around without settling into a region of low loss. This phenomenon can be visualized as repeatedly overshooting the bottom of a bowl instead of settling at its lowest point.\n• Slow Convergence:\n\n Conversely, a learning rate that is too low might not make a significant change in the weights, especially in the early phases of training. This not only extends the training time but may also lead the optimizer to converge to a suboptimal solution.\n\nThe effects of the learning rate can be modeled in the context of an iterative process:\n\nwhere θ∗ is the optimal parameter vector and λ is an eigenvalue related to the Hessian matrix of the loss function. This equation highlights that the choice of η influences how rapidly the difference θt​−θ∗ shrinks.\n\nDeep learning models, which often include hundreds of layers and millions of parameters, have intricate loss surfaces with multiple local minima and saddle points. In these scenarios, the learning rate plays several critical roles:\n• Avoiding Saddle Points:\n\n In high-dimensional spaces, saddle points are common. A properly tuned learning rate helps the optimizer to escape these regions and make progress toward a true minimum.\n• Smooth Transitions:\n\n Advanced optimization techniques, such as those using momentum or adaptive learning rates (e.g., Adam, RMSprop), depend heavily on the initial learning rate to balance exploration and exploitation in the loss landscape.\n• Balancing Speed and Precision:\n\n The learning rate must be chosen to not only accelerate convergence but also maintain precision in parameter updates. Fine-tuning this balance is often the difference between a model that learns efficiently and one that struggles to reach an optimal solution.\n\nEach deep model architecture poses its unique challenges, and understanding these aspects is a prerequisite to applying the best tuning methods, which we will explore in the next section.\n\nAfter establishing the fundamental importance of learning rate optimization, the next step is to employ practical strategies for fine-tuning the learning rate during model training. In this section, we review several methods and techniques that have been successfully adopted by practitioners.\n\nGrid search is among the simplest methods for learning rate tuning. Essentially, grid search involves specifying a range of learning rate values and then training the model with each of these values to identify which one yields the best performance.\n• Systematic Exploration:\n\n Imagine trying out learning rate candidates η∈{10−3,10−2,10−1} or a more refined set such as {10−4,5×10−4,10−3,5×10−3}. By comparing the resulting loss curves or validation accuracies, one can select a learning rate that offers a good balance between convergence speed and accuracy.\n• Computational Cost:\n\n One of the downsides of grid search is that it can become computationally expensive when the search space is large. However, it remains a robust baseline method, especially in scenarios where more sophisticated automated methods have not been implemented.\n\nExponential decay is a widely adopted strategy to adjust the learning rate as training progresses. It starts with a relatively high learning rate and gradually reduces it, following a decay schedule.\n\nThe exponential decay formula is commonly represented as:\n• λ is the decay rate, and\n• Advantages:\n\n By reducing the learning rate gradually, the optimizer takes larger steps during the early phases of training and fine-tunes as it approaches convergence. This is especially beneficial in high-dimensional loss surfaces, where initial exploration is key.\n• Practical Considerations:\n\n Selecting the right decay rate λ is crucial. A decay rate that is too high can effectively “freeze” the model early, while a too-slow decay may not adequately fine-tune the model during later stages of training.\n\nCyclical learning rates (CLR) represent a more dynamic approach where the learning rate cyclically varies between predefined bounds rather than monotonically decreasing. The idea behind CLR is to help the optimizer escape local minima and saddle points by periodically increasing the learning rate.\n\nThere are multiple variants of cyclical patterns, including triangular, cosine annealing, and others. A basic cyclical learning rate schedule might look like:\n• ηmin​ and ηmax​ are the lower and upper bounds of learning rate,\n• T is the duration of one cycle.\n• Benefits:\n\n This method has been shown to enhance model performance by providing periodic bursts of exploration. Such bursts can enable the optimizer to jump out of local traps and find better regions of the loss surface.\n• Implementation:\n\n Many modern deep learning libraries, such as TensorFlow and PyTorch, offer native implementations or callbacks for cyclical learning rate scheduling, making this strategy accessible to researchers and developers alike.\n\nHaving covered the theoretical and practical aspects of learning rate tuning, this section provides actionable tips and tricks to implement these strategies efficiently using current tools and best practices.\n\nPython remains the lingua franca for deep learning, and several libraries provide robust tools for learning rate scheduling:\n• TensorFlow and Keras:\n\n Both offer an assortment of built-in callbacks (e.g., , ) that allow for dynamic adjustment of the learning rate. An example using Keras might involve: from tensorflow.keras.callbacks import LearningRateScheduler def scheduler(epoch, lr): if epoch % 10 == 0 and epoch != 0: return lr * 0.9 # Reduce learning rate by 10% return lr lr_scheduler = LearningRateScheduler(scheduler) model.fit(x_train, y_train, epochs=100, callbacks=[lr_scheduler])\n• PyTorch:\n\n PyTorch users can utilize the module. For example, applying an exponential decay might look like: import torch.optim as optim optimizer = optim.Adam(model.parameters(), lr=0.001) scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) for epoch in range(100): train(...) validate(...) scheduler.step()\n\nThese libraries simplify applying complex learning rate schedules, freeing up more time for model architecture development and experimentation.\n\nKeeping track of experiments is crucial when performing hyperparameter tuning, including learning rate adjustments. Tools such as TensorBoard, Weights & Biases, or MLflow can record extensive details about training runs, such as:\n• Learning rate progression and its impact on validation loss.\n• Plots of loss curves over epochs, which can be annotated with the learning rate schedule in use.\n• Comparisons between different learning rate strategies (e.g., grid search vs. cyclical learning rates).\n\nFor instance, integrating TensorBoard in a TensorFlow workflow may involve:\n\nExperiment tracking not only provides insights into which learning rate strategies are most effective but also facilitates reproducibility and collaborative analysis, leading to more robust research outcomes.\n\nWhen working with large deep learning models or datasets, scaling the learning rate tuning process becomes imperative. Here are a few tips on how to effectively manage this:\n• Distributed Training:\n\n Frameworks like Horovod (for TensorFlow and PyTorch) can distribute training across multiple GPUs or even multiple nodes. This distribution minimizes the downtime per experiment and helps in effectively testing a broader set of hyperparameters concurrently.\n• Automated Hyperparameter Optimization:\n\n Tools such as Hyperopt, Ray Tune, or Optuna can significantly streamline the process of hyperparameter optimization, including learning rate tuning. These tools often implement Bayesian optimization or random search strategies that can explore the parameter space more efficiently than a manual grid search.\n• Cloud-based Solutions:\n\n Leveraging cloud platforms (e.g., AWS SageMaker, Google AI Platform) can provide resources on-demand, thereby reducing the training time for large-scale experiments. The flexibility to run multiple experiments in parallel can be a game-changer when fine-tuning learning rate schedules on extensive datasets.\n\nA systematic approach combining these tools and strategies can lead to an accelerated model development cycle, enabling rapid experimentation and iteration.\n\nTo illustrate the concepts discussed, let’s consider a couple of hypothetical case studies where learning rate tuning played a pivotal role in model performance.\n\nIn this study, a team was tasked with classifying images from a large dataset using a deep convolutional neural network. The initial experiments with a fixed learning rate resulted in the model plateauing without converging to optimal performance. By employing an exponential decay schedule, the team observed:\n• Enhanced precision in later epochs as the learning rate decreased, allowing the model to fine-tune its weights.\n\nMathematically, the adjustments in the learning rate can be viewed as modifying the gradient descent update:\n\nThis case study underscores the vital role of learning rate decay in achieving balanced exploration and exploitation of the loss landscape.\n\nIn another scenario, researchers working on training transformer-based models for language translation deployed cyclical learning rates. The benefits that emerged included:\n• Rapid escape from saddle points: The periodic increases in the learning rate enabled the optimizer to break free from local traps.\n• Improved generalization: The cyclic nature of the learning rate introduced variability, which helped in avoiding overfitting.\n• Robust training dynamics: Even in the presence of very deep network architectures, the cyclical method maintained consistent performance across several experimental runs.\n\nThis approach can be mathematically viewed as creating a non-monotonic progression of ηt​ that oscillates between ηmin​ and ηmax​, ensuring that the optimizer periodically revisits higher learning rates to re-energize the training process.\n\nOptimizing the learning rate is both an art and a science. While theoretical formulas provide a solid foundation, empirical tuning often leads to insights that are tailored to the specific characteristics of your model and data.\n• Start Simple:\n\n Begin with simple methods such as grid search or a constant learning rate. Use these initial experiments as a baseline.\n• Monitor Continuously:\n\n Use tracking tools like TensorBoard to visualize how learning rate adjustments affect the loss curves and model performance. Real-time insights often prompt timely adjustments.\n• Automate and Scale:\n\n Integrate automation tools for hyperparameter optimization to ease the burden of manual tuning, especially when scaling across large datasets and complex models.\n• Experiment with Dynamic Schedules:\n\n Explore advanced strategies such as exponential decay and cyclical learning rates. Often, a hybrid approach that leverages multiple strategies might be the optimal solution.\n• Document and Reproduce:\n\n Always record the experiment configurations, including the learning rate schedules and decay parameters, to ensure reproducibility. This documentation is crucial for iterative improvement and collaborative work.\n\nOptimizing the learning rate is a cornerstone for successful deep learning model training. A well-chosen learning rate accelerates convergence, reduces training time, and improves overall model performance. Whether you choose a static value or adopt dynamic schedules like exponential decay or cyclical learning rates, the key is to understand the underlying mechanisms driving convergence.\n\nBy combining theoretical insights—illustrated by equations such as:\n\nand practical tools available through Python libraries, you’re well-equipped to navigate the complex landscape of deep learning training. In the rapidly evolving field of artificial intelligence, mastering this optimization not only enhances performance but also paves the way for innovative research and applications.\n\nAs you continue to explore and experiment, remember that every model is unique. What works best for one model or dataset might require adjustments for another. Keep a flexible mindset, leverage robust experiment tracking, and always be ready to iterate on your approach.\n\nHappy tuning, and may your models converge faster and more efficiently than ever before!\n• SPSS and SAS: For professionals and academics, IBM SPSS and SAS provide powerful statistical analysis tools that simplify complex data processing with advanced diagnostic capabilities. Both software packages feature a traditional menu-driven user interface (UI/UX), making them accessible for users who prefer a point-and-click approach over coding-based workflows.\n• Number Analytics: Number Analytics is an AI-powered statistical software that automates statistical model selection, result interpretation, and report documentation. Designed for business professionals with limited statistical background, it simplifies complex analyses with an intuitive, user-friendly approach. Try Number Analytics. (Number Analytics)"
    },
    {
        "link": "https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks",
        "document": "The weights of a neural network cannot be calculated using an analytical method. Instead, the weights must be discovered via an empirical optimization procedure called stochastic gradient descent.\n\nThe optimization problem addressed by stochastic gradient descent for neural networks is challenging and the space of solutions (sets of weights) may be comprised of many good solutions (called global optima) as well as easy to find, but low in skill solutions (called local optima).\n\nThe amount of change to the model during each step of this search process, or the step size, is called the “learning rate” and provides perhaps the most important hyperparameter to tune for your neural network in order to achieve good performance on your problem.\n\nIn this tutorial, you will discover the learning rate hyperparameter used when training deep learning neural networks.\n\nAfter completing this tutorial, you will know:\n• How to configure the learning rate with sensible defaults, diagnose behavior, and develop a sensitivity analysis.\n• How to further improve performance with learning rate schedules, momentum, and adaptive learning rates.\n\nKick-start your project with my new book Better Deep Learning, including step-by-step tutorials and the Python source code files for all examples.\n\nThis tutorial is divided into six parts; they are:\n• What Is the Learning Rate?\n\nWhat Is the Learning Rate?\n\nDeep learning neural networks are trained using the stochastic gradient descent algorithm.\n\nStochastic gradient descent is an optimization algorithm that estimates the error gradient for the current state of the model using examples from the training dataset, then updates the weights of the model using the back-propagation of errors algorithm, referred to as simply backpropagation.\n\nThe amount that the weights are updated during training is referred to as the step size or the “learning rate.”\n\nSpecifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0.\n\nThe learning rate is often represented using the notation of the lowercase Greek letter eta (n).\n\nDuring training, the backpropagation of error estimates the amount of error for which the weights of a node in the network are responsible. Instead of updating the weight with the full amount, it is scaled by the learning rate.\n\nThis means that a learning rate of 0.1, a traditionally common default value, would mean that weights in the network are updated 0.1 * (estimated weight error) or 10% of the estimated weight error each time the weights are updated.\n\nA neural network learns or approximates a function to best map inputs to outputs from examples in the training dataset.\n\nThe learning rate hyperparameter controls the rate or speed at which the model learns. Specifically, it controls the amount of apportioned error that the weights of the model are updated with each time they are updated, such as at the end of each batch of training examples.\n\nGiven a perfectly configured learning rate, the model will learn to best approximate the function given available resources (the number of layers and the number of nodes per layer) in a given number of training epochs (passes through the training data).\n\nGenerally, a large learning rate allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights. A smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train.\n\nAt extremes, a learning rate that is too large will result in weight updates that will be too large and the performance of the model (such as its loss on the training dataset) will oscillate over training epochs. Oscillating performance is said to be caused by weights that diverge (are divergent). A learning rate that is too small may never converge or may get stuck on a suboptimal solution.\n\nIn the worst case, weight updates that are too large may cause the weights to explode (i.e. result in a numerical overflow).\n\nTherefore, we should not use a learning rate that is too large or too small. Nevertheless, we must configure the model in such a way that on average a “good enough” set of weights is found to approximate the mapping problem as represented by the training dataset.\n\nIt is important to find a good value for the learning rate for your model on your training dataset.\n\nThe learning rate may, in fact, be the most important hyperparameter to configure for your model.\n\nIn fact, if there are resources to tune hyperparameters, much of this time should be dedicated to tuning the learning rate.\n\nUnfortunately, we cannot analytically calculate the optimal learning rate for a given model on a given dataset. Instead, a good (or good enough) learning rate must be discovered via trial and error.\n\nThe range of values to consider for the learning rate is less than 1.0 and greater than 10^-6.\n\nThe learning rate will interact with many other aspects of the optimization process, and the interactions may be nonlinear. Nevertheless, in general, smaller learning rates will require more training epochs. Conversely, larger learning rates will require fewer training epochs. Further, smaller batch sizes are better suited to smaller learning rates given the noisy estimate of the error gradient.\n\nA traditional default value for the learning rate is 0.1 or 0.01, and this may represent a good starting point on your problem.\n\nDiagnostic plots can be used to investigate how the learning rate impacts the rate of learning and learning dynamics of the model. One example is to create a line plot of loss over training epochs during training. The line plot can show many properties, such as:\n• The rate of learning over training epochs, such as fast or slow.\n• Whether model has learned too quickly (sharp rise and plateau) or is learning too slowly (little or no change).\n• Whether the learning rate might be too large via oscillations in loss.\n\nConfiguring the learning rate is challenging and time-consuming.\n\nAn alternative approach is to perform a sensitivity analysis of the learning rate for the chosen model, also called a grid search. This can help to both highlight an order of magnitude where good learning rates may reside, as well as describe the relationship between learning rate and performance.\n\nIt is common to grid search learning rates on a log scale from 0.1 to 10^-5 or 10^-6.\n\nWhen plotted, the results of such a sensitivity analysis often show a “U” shape, where loss decreases (performance improves) as the learning rate is decreased with a fixed number of training epochs to a point where loss sharply increases again because the model fails to converge.\n\nIf you need help experimenting with the learning rate for your model, see the post:\n• Understand the Impact of Learning Rate on Model Performance With Deep Learning Neural Networks\n\nTraining a neural network can be made easier with the addition of history to the weight update.\n\nSpecifically, an exponentially weighted average of the prior updates to the weight can be included when the weights are updated. This change to stochastic gradient descent is called “momentum” and adds inertia to the update procedure, causing many past updates in one direction to continue in that direction in the future.\n\nMomentum can accelerate learning on those problems where the high-dimensional “weight space” that is being navigated by the optimization process has structures that mislead the gradient descent algorithm, such as flat regions or steep curvature.\n\nThe amount of inertia of past updates is controlled via the addition of a new hyperparameter, often referred to as the “momentum” or “velocity” and uses the notation of the Greek lowercase letter alpha (a).\n\nIt has the effect of smoothing the optimization process, slowing updates to continue in the previous direction instead of getting stuck or oscillating.\n\nMomentum is set to a value greater than 0.0 and less than one, where common values such as 0.9 and 0.99 are used in practice.\n\nMomentum does not make it easier to configure the learning rate, as the step size is independent of the momentum. Instead, momentum can improve the speed of the optimization process in concert with the step size, improving the likelihood that a better set of weights is discovered in fewer training epochs.\n\nAn alternative to using a fixed learning rate is to instead vary the learning rate over the training process.\n\nThe way in which the learning rate changes over time (training epochs) is referred to as the learning rate schedule or learning rate decay.\n\nPerhaps the simplest learning rate schedule is to decrease the learning rate linearly from a large initial value to a small value. This allows large weight changes in the beginning of the learning process and small changes or fine-tuning towards the end of the learning process.\n\nIn fact, using a learning rate schedule may be a best practice when training neural networks. Instead of choosing a fixed learning rate hyperparameter, the configuration challenge involves choosing the initial learning rate and a learning rate schedule. It is possible that the choice of the initial learning rate is less sensitive than choosing a fixed learning rate, given the better performance that a learning rate schedule may permit.\n\nThe learning rate can be decayed to a small value close to zero. Alternately, the learning rate can be decayed over a fixed number of training epochs, then kept constant at a small value for the remaining training epochs to facilitate more time fine-tuning.\n\nThe performance of the model on the training dataset can be monitored by the learning algorithm and the learning rate can be adjusted in response.\n\nThis is called an adaptive learning rate.\n\nPerhaps the simplest implementation is to make the learning rate smaller once the performance of the model plateaus, such as by decreasing the learning rate by a factor of two or an order of magnitude.\n\nAlternately, the learning rate can be increased again if performance does not improve for a fixed number of training epochs.\n\nAn adaptive learning rate method will generally outperform a model with a badly configured learning rate.\n\nAlthough no single method works best on all problems, there are three adaptive learning rate methods that have proven to be robust over many types of neural network architectures and problem types.\n\nThey are AdaGrad, RMSProp, and Adam, and all maintain and adapt learning rates for each of the weights in the model.\n\nPerhaps the most popular is Adam, as it builds upon RMSProp and adds momentum.\n\nA robust strategy may be to first evaluate the performance of a model with a modern version of stochastic gradient descent with adaptive learning rates, such as Adam, and use the result as a baseline. Then, if time permits, explore whether improvements can be achieved with a carefully selected learning rate or simpler learning rate schedule.\n\nThis section provides more resources on the topic if you are looking to go deeper.\n• Understand the Impact of Learning Rate on Model Performance With Deep Learning Neural Networks\n• What learning rate should be used for backprop?, Neural Network FAQ.\n\nIn this tutorial, you discovered the learning rate hyperparameter used when training deep learning neural networks.\n• How to configure the learning rate with sensible defaults, diagnose behavior, and develop a sensitivity analysis.\n• How to further improve performance with learning rate schedules, momentum, and adaptive learning rates.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://cameronrwolfe.substack.com/p/the-best-learning-rate-schedules",
        "document": "This newsletter is supported by Alegion. As a research scientist at Alegion, I work on a range of problems from online learning to diffusion models. Feel free to check out our data annotation platform or contact me about potential collaboration/opportunities!\n\nWelcome to the Deep (Learning) Focus newsletter. Each issue picks a single topic in deep learning research and comprehensively overviews related research. Feel free to subscribe to the newsletter, share it, or follow me on twitter if you enjoy it!\n\nAnybody that has trained a neural network knows that properly setting the learning rate during training is a pivotal aspect of getting the neural network to perform well. Additionally, the learning rate is typically varied along the training trajectory according to some learning rate schedule. The choice of this schedule also has a large impact on the quality of training.\n\nMost practitioners adopt a few, widely-used strategies for the learning rate schedule during training; e.g., step decay or cosine annealing. Many of these schedules are curated for a particular benchmark, where they have been determined empirically to maximize test accuracy after years of research. But, these strategies often fail to generalize to other experimental settings, raising an important question: what are the most consistent and useful learning rate schedules for training neural networks?\n\nWithin this overview, we will look at recent research into various learning rate schedules that can be used to train neural networks. Such research has discovered numerous strategies for the learning rate that are both highly effective and easy to use; e.g., cyclical or triangular learning rate schedules. By studying these methods, we will arrive at several practical takeaways, providing simple tricks that can be immediately applied to improving neural network training.\n\nTo supplement this overview, I have implemented the main learning rate schedules that we will explore within a repository found here. These code examples are somewhat minimal, but they are sufficient to implement any of the learning rate schedules discussed in this overview without much effort.\n\nIn a supervised learning setting, the goal of neural network training is to produce a neural network that, given some data as input, can predict the ground truth label associated with that data. One example of this would be training a neural network to correctly predict whether an image contains a cat or a dog based upon a large dataset of labeled images of cats and dogs.\n\nThe basic components of neural network training, depicted above, are as follows:\n• None Neural Network: takes some data as input and transforms this data based on its internal parameters/weights to produce some output.\n• None Dataset: a large set of examples of input-output data pairs (e.g., images and their corresponding classifications).\n• None Optimizer: used to update the neural network’s internal parameters such that its predictions become more accurate.\n• None Hyperparameters: external parameters that are set by the deep learning practitioner to control relevant details of the training process.\n\nUsually, a neural network begins training with all of its parameters randomly initialized. To learn more meaningful parameters, the neural network is shown samples of data from the dataset. For each of these samples, the neural network attempts to predict the correct output, then the optimizer updates the neural network’s parameters to improve this prediction.\n\nThis process of updating the neural network’s parameters such that it can better match the known outputs within a dataset is referred to as training. The process repeats iteratively, typically until the neural network has looped over the entire dataset – referred to as an epoch of training – multiple times.\n\nAlthough this description of neural network training is not comprehensive, it should provide enough intuition to make it through this overview. Many extensive tutorials on neural network training exist online. My favorite tutorial by-far is from the “Practical Deep Learning for Coders” course by Jeremy Howard and fast.ai; see the link to the video below.\n\nModel parameters are updated by the optimizer during training. Hyperparameters, in contrast, are “extra” parameters that we, the deep learning practitioner, have control over. But, what can we actually control with hyperparameters? One common hyperparameter, which is relevant to this overview, is the learning rate.\n\nwhat is the learning rate? Put simply, each time the optimizer updates the neural network’s parameters, the learning rate controls the size of this update. Should we update the parameters a lot, a little bit, or somewhere in the middle? We make this choice by setting the learning rate.\n\nselecting a good learning rate. Setting the learning rate is one of the most important aspects of training a neural network. If we choose a value that is too large, training will diverge. On the other hand, a learning rate that is too small can yield poor performance and slow training. We must choose a learning rate that is large enough to provide regularization benefits to the training process and converge quickly, while not being too large such that the training process becomes unstable.\n\nHyperparameters like the learning rate are typically selected using a simple approach called grid search. The basic idea is to:\n• None Define a range of potential values for each hyperparameter\n• None Select a discrete set of values to test within this range\n• None Test all combinations of possible hyperparameter values\n• None Choose the best hyperparameter setting based on validation set performance\n\nGrid search is a simple, exhaustive search for the best hyperparameters. See the illustration below for an example of grid search over potential learning rate values.\n\nA similar approach can be applied to many hyperparameters at once by following a similar approach and testing all possible combinations of hyperparameter values.\n\nGrid search is computationally inefficient, as it requires the neural network to be retrained for each hyperparameter setting. To avoid this cost, many deep learning practitioners adopt a “guess and check” approach of trying several hyperparameters within a reasonable range and seeing what works. Alternative methodologies for selecting optimal hyperparameters have been proposed [5], but grid search or guess and check procedures are commonly used due to their simplicity.\n\nAfter selecting a learning rate, we typically should not maintain this same learning rate throughout the entire training process. Rather, conventional wisdom suggests that we should (i) select an initial learning rate, then (ii) decay this learning rate throughout the training process [1]. The function by which we perform this decay is referred to as the learning rate schedule.\n\nMany different learning rate schedules have been proposed over the years; e.g., step decay (i.e., decaying the learning rate by 10X a few times during training) or cosine annealing; see the figure below. In this overview, we will explore a number of recently proposed schedules that perform especially well.\n\nadaptive optimization techniques. Neural network training according to stochastic gradient descent (SGD) selects a single, global learning rate that is used for updating all model parameters. Beyond SGD, adaptive optimization techniques have been proposed (e.g., RMSProp or Adam [6]), which use training statistics to dynamically adjust the learning rate used for each of a model’s parameters. Most of the results outlined within this overview apply to both adaptive and SGD-style optimizers.\n\nIn this section, we will see several examples of recently proposed learning rate schedules. These include strategies like cyclical or triangular learning rates, as well as different profiles for learning rate decay. The optimal learning rate strategy is highly-dependent upon the domain and experimental settings, but we will see that several high-level takeaways can be drawn by studying the empirical results of many different learning rate strategies.\n\nAuthors in [1] propose a new method for handling the learning rate during neural network training: cyclically varying it between a minimum and a maximum value according to a smooth schedule. Prior to this work, most practitioners adopted the popular strategy of (i) setting the learning rate to an initially large value, then (ii) decaying the learning rate as training proceeds.\n\nIn [1], we throw away this rule-of-thumb in favor of a cyclical strategy. Cycling the learning rate in this way is somewhat counterintuitive — increasing the learning rate during training damages model performance, right? Despite temporarily degrading network performance as the learning rate increases, cyclical learning rate schedules actually provide a lot of benefits over the full course of training, as we will see in [1].\n\nCyclical learning rates introduce three new hyperparameters: stepsize, minimum learning rate, and maximum learning rate. The resulting schedule is “triangular”, meaning that the learning rate is increased/decreased in adjacent cycles; see above. The stepsize can be set somewhere between 2-10 training epochs, while the range for the learning rate is typically discovered via a learning rate range test (see Section 3.3 of [1]).\n\nIncreasing the learning rate temporarily degrades model performance. Once the learning rate has decayed again, however, the model’s performance will recover and improve. With this in mind, we see in the experimental results of [1] that models trained with cyclical learning rates follow a cyclical pattern in their performance. Model performance peaks at the end of each cycle (i.e., when the learning rate decays back to the minimum value) and becomes somewhat worse at intermediate stages of the cycle (i.e., when the learning rate is increased); see below.\n\nThe results in [1] reveal that cyclical learning rates benefit model performance over the course of training. Models trained via cyclical learning rates reach higher levels of performance faster than models trained with other learning rate strategies; see the figure below. In other words, the anytime performance of models trained with cyclical learning rates is really good!\n\nIn larger-scale experiments on ImageNet, cyclical learning rates still provide benefits, though they are a bit less pronounced.\n\nThe authors in [2] propose a simple restarting technique for the learning rate, called stochastic gradient descent with restarts (SGDR), in which the learning rate is periodically reset to its original value and scheduled to decrease. This technique employs the following steps:\n• None Decay the learning rate according to some fixed schedule\n• None Reset the learning rate to its original value after the end of the decay schedule\n• None Return to step #1 (i.e., decay the learning rate again)\n\nA depiction of different schedules that follow this strategy is provided below.\n\nWe can notice a few things about the schedules above. First, a cosine decay schedule is always used in [2] (the plot’s y-axis is in log scale). Additionally, the length of each decay schedule may increase as training progresses. Concretely, authors in [2] define the length of the first decay cycle as , then multiply this length by during each successive decay cycle; see below for a depiction.\n\nTo follow the terminology of [1], the stepsize of SGDR may increase after each cycle. Unlike [1], however, SGDR is not triangular (i.e., each cycle just decays the learning rate).\n\nIn experiments on CIFAR10/100, we can see that SGDR learning rate schedules yield good model performance more quickly than step decay schedules — SGDR has good anytime performance. The models obtained after each decay cycle perform well and continue to get better in successive decay cycles.\n\nGoing beyond these initial results, we can study model ensembles formed by taking “snapshots” at the end of each decay cycle. In particular, we can save a copy of the model’s state after each decay cycle within an SGDR schedule. Then, after training is complete, we can average the predictions of each of these models at inference time, forming an ensemble/group of models; see the link below for more details on the idea of ensembles.\n\nBy forming model ensembles in this way, we can achieve pretty significant reductions in test error on CIFAR10; see below.\n\nAdditionally, the snapshots from SGDR seem to provide a set of models with diverse predictions. Forming an ensemble in this way actually outperforms the normal approach of adding independent, fully-trained models into an ensemble.\n\nSuper-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\n\nThe authors in [3] study an interesting approach for training neural networks that allows the speed of training to be increased by an order of magnitude. The basic approach — originally outlined in [8] — is to perform a single, triangular learning rate cycle with a large maximum learning rate, then allow the learning rate to decay below the minimum value of this cycle at the end of training; see below for an illustration.\n\nIn addition, the momentum is cycled in the opposite direction of the learning rate (typically in the range [0.85, 0.95]). This approach of jointly cycling the learning rate and momentum is referred to as “1cycle”. The authors in [3] show that it can be used to achieve “super-convergence” (i.e., extremely fast convergence to a high-performing solution).\n\nFor example, we see in experiments on CIFAR10 that 1cycle can achieve better performance than baseline learning rate strategies with 8X fewer training iterations. Using different 1cycle step sizes can yield even further speedups in training, though the accuracy level varies depending on the step size.\n\nWe can observe similar results on a few different architectures and datasets. See the table below, where 1cycle again yields good performance in a surprisingly small number of training epochs.\n\nCurrently, it is not clear whether super-convergence is achievable in a wide number of experimental settings, as experiments provided in [3] are somewhat limited in scale and variety. Nonetheless, we can probably all agree that the super-convergence phenomenon is quite interesting. In fact, the result was so interesting that it was even popularized and studied in depth by the fast.ai community.\n\nWithin [4], authors (including myself) consider the problem of properly scheduling the learning rate given different budget regimes (i.e., small, medium, or large number of training epochs). You might be thinking: why would we consider this setting? Well, oftentimes the optimal number of training epochs is not known ahead of time. Plus, we might be working with a fixed monetary budget that limits the number of training epochs we can perform.\n\nTo find the best budget-agnostic learning rate schedules, we must first define the space of possible learning rate schedules that will be considered. In [4], we do this by decomposing a learning rate schedule into two components:\n• None Profile: the function according to which the learning rate is varied throughout training.\n• None Sampling Rate: the frequency with which the learning rate is updated according to the chosen profile.\n\nSuch a decomposition can be used to describe nearly all fixed-structure learning rate schedules. Different profile and sampling rate combinations are depicted below. Higher sampling rates cause the schedule to match the underlying profile more closely.\n\nAuthors in [4] consider learning rate schedules formed with different sampling rates and three function profiles — exponential (i.e., produces step schedules), linear, and REX (i.e., a novel profile defined in [4]); see the figure above.\n\nFrom here, the authors train a Resnet20/38 on CIFAR10 with different sampling rate and profile combinations. In these experiments, we see that step decay schedules (i.e., exponential profile with a low sampling rate) only perform well given a low sampling rate and many training epochs. REX schedules with every iteration sampling perform well in all different epoch settings.\n\nPrior work indicated that a linear decay schedule is best for low-budget training settings (i.e., training with fewer epochs) [9]. In [4], we can see that REX is actually a better choice, as it avoids decaying the learning rate too early during training.\n\nFrom here, authors in [4] consider a variety of popular learning rate schedules, as shown in the figure below.\n\nThese schedules are tested across a variety of domains and training epoch budgets. When the performance is aggregated across all experiments, we get the results shown below.\n\nImmediately, we see that REX achieves shockingly consistent performance across different budget regimes and experimental domains. No other learning rate schedule achieves close to the same ratio of top-1/3 finishes across experiments, revealing that REX is a good domain/budget-agnostic learning rate schedule.\n\nBeyond the consistency of REX, these results teach us something more general: commonly-used learning rate strategies don’t generalize well across experimental settings. Each schedule (even REX, though to a lesser degree) performs best in only a small number of cases, revealing that selecting the proper learning rate strategy for any particular setting is incredibly important.\n\nProperly handling the learning rate is arguably the most important aspect of neural network training. Within this overview, we have learned about several practical learning rate schedules for training deep networks. Studying this line of work provides takeaways that are simple to understand, easy to implement, and highly effective. Some of these basic takeaways are outlined below.\n\nChoose a good learning rate. Properly setting the learning rate is one of the most important aspects of training a high-performing neural network. Choosing a poor initial learning rate or using the wrong learning rate schedule drastically deteriorates model performance.\n\nThe “default” schedule isn’t always best. Many experimental settings have a “default” learning rate schedule that we tend to adopt without much thought; e.g., step decay schedules for training CNNs for image classification. We should be aware that the performance of these schedules may deteriorate drastically as experimental settings change; e.g., for budgeted settings, REX-based schedules significantly outperform step decay. As practitioners, we should always be mindful of our chosen learning rate schedule to truly maximize our model’s performance.\n\nCyclical schedules are awesome. Cyclical or triangular learning rate schedules (e.g., as in [2] or [3]) are really useful because:\n• None They often match or exceed state-of-the-art performance\n\nUsing cyclical learning rate strategies, models reach their best performance at the end of each decay cycle. We can simply continue training for any given number of cycles until we are happy with the network’s performance. The optimal amount of training need not be known a priori, which is often useful in practice.\n\nThere’s a lot to explore out there. Although learning rate strategies have been widely studied, it seems like there is still more out there to be discovered. For example, we have seen that adopting alternative decay profiles benefits budgeted settings [4] and cyclical strategies may even be used to achieve super-convergence in some cases [3]. My question is: what more can be discovered? It seems like there are really interesting strategies (e.g., fractal learning rates [7]) that are yet to be explored.\n\nAs a supplement to this overview, I created a lightweight code repository for reproducing some of the different learning rate schedules, which includes:\n• None Functions for adjusting the learning rate/momentum in PyTorch optimizers\n• None Working examples for common learning rate schedules we have seen in this overview\n\nAlthough a bit minimal, this code provides everything that’s needed to implement and use any of the learning rate strategies we have studied so far. A link to the repository is provided below.\n\nIf you’re not interested in using this code, you can also use the learning rate schedulers directly implemented within PyTorch.\n\nNew to the newsletter?\n\nHello! I am Cameron R. Wolfe, a research scientist at Alegion and PhD student at Rice University studying the empirical and theoretical foundations of deep learning. This is the Deep (Learning) Focus newsletter, where I pick a single, bi-weekly topic in deep learning research, provide an understanding of relevant background information, then overview a handful of popular papers on the topic. If you like this newsletter, please subscribe, share it with your friends, or follow me on twitter!\n\n[1] Smith, Leslie N. \"Cyclical learning rates for training neural networks.\" 2017 IEEE winter conference on applications of computer vision (WACV). IEEE, 2017.\n\n[3] Smith, Leslie N., and Nicholay Topin. \"Super-convergence: Very fast training of neural networks using large learning rates.\" Artificial intelligence and machine learning for multi-domain operations applications. Vol. 11006. SPIE, 2019.\n\n[4] Chen, John, Cameron Wolfe, and Tasos Kyrillidis. \"REX: Revisiting Budgeted Training with an Improved Schedule.\" Proceedings of Machine Learning and Systems 4 (2022): 64-76.\n\n[5] Yu, Tong, and Hong Zhu. \"Hyper-parameter optimization: A review of algorithms and applications.\" arXiv preprint arXiv:2003.05689 (2020).\n\n[7] Agarwal, Naman, Surbhi Goel, and Cyril Zhang. \"Acceleration via fractal learning rate schedules.\" International Conference on Machine Learning. PMLR, 2021."
    }
]