[
    {
        "link": "https://stats.libretexts.org/Bookshelves/Probability_Theory/Introductory_Probability_(Grinstead_and_Snell)/06%3A_Expected_Value_and_Variance/6.01%3A_Expected_Value_of_Discrete_Random_Variables",
        "document": "When a large collection of numbers is assembled, as in a census, we are usually interested not in the individual numbers, but rather in certain descriptive quantities such as the average or the median. In general, the same is true for the probability distribution of a numerically-valued random variable. In this and in the next section, we shall discuss two such descriptive quantities: the expected value and the variance. Both of these quantities apply only to numerically-valued random variables, and so we assume, in these sections, that all random variables have numerical values. To give some intuitive justification for our definition, we consider the following game.\n\nA die is rolled. If an odd number turns up, we win an amount equal to this number; if an even number turns up, we lose an amount equal to this number. For example, if a two turns up we lose 2, and if a three comes up we win 3. We want to decide if this is a reasonable game to play. We first try simulation. The program Die carries out this simulation. The program prints the frequency and the relative frequency with which each outcome occurs. It also calculates the average winnings. We have run the program twice. The results are shown in Table \\(\\PageIndex{1}\\). In the first run we have played the game 100 times. In this run our average gain is \\(-.57\\). It looks as if the game is unfavorable, and we wonder how unfavorable it really is. To get a better idea, we have played the game 10,000 times. In this case our average gain is \\(-.4949\\). We note that the relative frequency of each of the six possible outcomes is quite close to the probability 1/6 for this outcome. This corresponds to our frequency interpretation of probability. It also suggests that for very large numbers of plays, our average gain should be \\[ \n\nonumber \\begin{aligned} \\mu & = & 1 \\Bigl(\\frac 16\\Bigr) - 2\\Bigl(\\frac 16\\Bigr) + 3 \\Bigl(\\frac 16\\Bigr) - 4 \\Bigl(\\frac 16\\Bigr) + 5 \\Bigl(\\frac 16\\Bigr) - 6 \\Bigl(\\frac 16\\Bigr) \\\\ & = & \\frac 96 - \\frac {12}6 = -\\frac 36 = -.5\\ .\\end{aligned}\\] This agrees quite well with our average gain for 10,000 plays. We note that the value we have chosen for the average gain is obtained by taking the possible outcomes, multiplying by the probability, and adding the results. This suggests the following definition for the expected outcome of an experiment.\n\nWith the Law of Large Numbers to bolster the frequency interpretation of probability, we find it natural to justify the definition of expected value in terms of the average outcome over a large number of repetitions of the experiment. The concept of expected value was used before it was formally defined; and when it was used, it was considered not as an average value but rather as the appropriate value for a gamble. For example recall, from the Historical Remarks section of Chapter 1, Section 1.2, Pascal’s way of finding the value of a three-game series that had to be called off before it is finished. Pascal first observed that if each player has only one game to win, then the stake of 64 pistoles should be divided evenly. Then he considered the case where one player has won two games and the other one. Then consider, Sir, if the first man wins, he gets 64 pistoles, if he loses he gets 32. Thus if they do not wish to risk this last game, but wish to separate without playing it, the first man must say: “I am certain to get 32 pistoles, even if I lose I still get them; but as for the other 32 pistoles, perhaps I will get them, perhaps you will get them, the chances are equal. Let us then divide these 32 pistoles in half and give one half to me as well as my 32 which are mine for sure.\" He will then have 48 pistoles and the other 16.2 Note that Pascal reduced the problem to a symmetric bet in which each player gets the same amount and takes it as obvious that in this case the stakes should be divided equally. The first systematic study of expected value appears in Huygens’ book. Like Pascal, Huygens find the value of a gamble by assuming that the answer is obvious for certain symmetric situations and uses this to deduce the expected for the general situation. He does this in steps. His first proposition is Prop. I. If I expect \\(a\\) or \\(b\\), either of which, with equal probability, may fall to me, then my Expectation is worth \\((a + b)/2\\), that is, the half Sum of \\(a\\) and \\(b\\).3 Huygens proved this as follows: Assume that two player A and B play a game in which each player puts up a stake of \\((a + b)/2\\) with an equal chance of winning the total stake. Then the value of the game to each player is \\((a + b)/2\\). For example, if the game had to be called off clearly each player should just get back his original stake. Now, by symmetry, this value is not changed if we add the condition that the winner of the game has to pay the loser an amount \\(b\\) as a consolation prize. Then for player A the value is still \\((a + b)/2\\). But what are his possible outcomes for the modified game? If he wins he gets the total stake \\(a + b\\) and must pay B an amount \\(b\\) so ends up with \\(a\\). If he loses he gets an amount \\(b\\) from player B. Thus player A wins \\(a\\) or \\(b\\) with equal chances and the value to him is \\((a + b)/2\\). Huygens illustrated this proof in terms of an example. If you are offered a game in which you have an equal chance of winning 2 or 8, the expected value is 5, since this game is equivalent to the game in which each player stakes 5 and agrees to pay the loser 3 — a game in which the value is obviously 5. Prop. II. If I expect \\(a\\), \\(b\\), or \\(c\\), either of which, with equal facility, may happen, then the Value of my Expectation is \\((a + b + c)/3\\), or the third of the Sum of \\(a\\), \\(b\\), and \\(c\\).4 His argument here is similar. Three players, A, B, and C, each stake \\[(a+b+c)/3\\] in a game they have an equal chance of winning. The value of this game to player A is clearly the amount he has staked. Further, this value is not changed if A enters into an agreement with B that if one of them wins he pays the other a consolation prize of \\(b\\) and with C that if one of them wins he pays the other a consolation prize of \\(c\\). By symmetry these agreements do not change the value of the game. In this modified game, if A wins he wins the total stake \\(a + b + c\\) minus the consolation prizes \\(b + c\\) giving him a final winning of \\(a\\). If B wins, A wins \\(b\\) and if C wins, A wins \\(c\\). Thus A finds himself in a game with value \\((a + b + c)/3\\) and with outcomes \\(a\\), \\(b\\), and \\(c\\) occurring with equal chance. This proves Proposition II. More generally, this reasoning shows that if there are \\(n\\) outcomes \\[a_1,\\ a_2,\\ \\ldots,\\ a_n\\ ,\\] all occurring with the same probability, the expected value is \\[\\frac {a_1 + a_2 +\\cdots+ a_n}n\\ .\\] In his third proposition Huygens considered the case where you win \\(a\\) or \\(b\\) but with unequal probabilities. He assumed there are \\(p\\) chances of winning \\(a\\), and \\(q\\) chances of winning \\(b\\), all having the same probability. He then showed that the expected value is \\[E = \\frac p{p + q} \\cdot a + \\frac q{p + q} \\cdot b\\ .\\] This follows by considering an equivalent gamble with \\(p + q\\) outcomes all occurring with the same probability and with a payoff of \\(a\\) in \\(p\\) of the outcomes and \\(b\\) in \\(q\\) of the outcomes. This allowed Huygens to compute the expected value for experiments with unequal probabilities, at least when these probablities are rational numbers. Thus, instead of defining the expected value as a weighted average, Huygens assumed that the expected value of certain symmetric gambles are known and deduced the other values from these. Although this requires a good deal of clever manipulation, Huygens ended up with values that agree with those given by our modern definition of expected value. One advantage of this method is that it gives a justification for the expected value in cases where it is not reasonable to assume that you can repeat the experiment a large number of times, as for example, in betting that at least two presidents died on the same day of the year. (In fact, three did; all were signers of the Declaration of Independence, and all three died on July 4.) In his book, Huygens calculated the expected value of games using techniques similar to those which we used in computing the expected value for roulette at Monte Carlo. For example, his proposition XIV is: Prop. XIV. If I were playing with another by turns, with two Dice, on this Condition, that if I throw 7 I gain, and if he throws 6 he gains allowing him the first Throw: To find the proportion of my Hazard to his.5 A modern description of this game is as follows. Huygens and his opponent take turns rolling a die. The game is over if Huygens rolls a 7 or his opponent rolls a 6. His opponent rolls first. What is the probability that Huygens wins the game? To solve this problem Huygens let \\(x\\) be his chance of winning when his opponent threw first and \\(y\\) his chance of winning when he threw first. Then on the first roll his opponent wins on 5 out of the 36 possibilities. Thus, \\[x = \\frac {31}{36} \\cdot y\\ .\\] But when Huygens rolls he wins on 6 out of the 36 possible outcomes, and in the other 30, he is led back to where his chances are \\(x\\). Thus \\[y = \\frac 6{36} + \\frac {30}{36} \\cdot x\\ .\\] From these two equations Huygens found that \\(x = 31/61\\). Another early use of expected value appeared in Pascal’s argument to show that a rational person should believe in the existence of God.6 Pascal said that we have to make a wager whether to believe or not to believe. Let \\(p\\) denote the probability that God does not exist. His discussion suggests that we are playing a game with two strategies, believe and not believe, with payoffs as shown in Table [table 6.4]. Here \\(-u\\) represents the cost to you of passing up some worldly pleasures as a consequence of believing that God exists. If you do not believe, and God is a vengeful God, you will lose \\(x\\). If God exists and you do believe you will gain v. Now to determine which strategy is best you should compare the two expected values \\[p(-u) + (1 - p)v \\qquad {\\rm and} \\qquad p0 + (1 - p)(-x),\\] and choose the larger of the two. In general, the choice will depend upon the value of \\(p\\). But Pascal assumed that the value of \\(v\\) is infinite and so the strategy of believing is best no matter what probability you assign for the existence of God. This example is considered by some to be the beginning of decision theory. Decision analyses of this kind appear today in many fields, and, in particular, are an important part of medical diagnostics and corporate business decisions. Another early use of expected value was to decide the price of annuities. The study of statistics has its origins in the use of the bills of mortality kept in the parishes in London from 1603. These records kept a weekly tally of christenings and burials. From these John Graunt made estimates for the population of London and also provided the first mortality data,7 shown in Table [table 6.5]. As Hacking observes, Graunt apparently constructed this table by assuming that after the age of 6 there is a constant probability of about 5/8 of surviving for another decade.8 For example, of the 64 people who survive to age 6, 5/8 of 64 or 40 survive to 16, 5/8 of these 40 or 25 survive to 26, and so forth. Of course, he rounded off his figures to the nearest whole person. Clearly, a constant mortality rate cannot be correct throughout the whole range, and later tables provided by Halley were more realistic in this respect.9 A terminal annuity provides a fixed amount of money during a period of \\(n\\) years. To determine the price of a terminal annuity one needs only to know the appropriate interest rate. A life annuity provides a fixed amount during each year of the buyer’s life. The appropriate price for a life annuity is the expected value of the terminal annuity evaluated for the random lifetime of the buyer. Thus, the work of Huygens in introducing expected value and the work of Graunt and Halley in determining mortality tables led to a more rational method for pricing annuities. This was one of the first serious uses of probability theory outside the gambling houses. Although expected value plays a role now in every branch of science, it retains its importance in the casino. In 1962, Edward Thorp’s book Beat the Dealer10 provided the reader with a strategy for playing the popular casino game of blackjack that would assure the player a positive expected winning. This book forevermore changed the belief of the casinos that they could not be beat.\n\nA card is drawn at random from a deck consisting of cards numbered 2 through 10. A player wins 1 dollar if the number on the card is odd and loses 1 dollar if the number if even. What is the expected value of his winnings? A card is drawn at random from a deck of playing cards. If it is red, the player wins 1 dollar; if it is black, the player loses 2 dollars. Find the expected value of the game. In a class there are 20 students: 3 are 5’ 6\", 5 are 5’8\", 4 are 5’10\", 4 are 6’, and 4 are 6’ 2\". A student is chosen at random. What is the student’s expected height? In Las Vegas the roulette wheel has a 0 and a 00 and then the numbers 1 to 36 marked on equal slots; the wheel is spun and a ball stops randomly in one slot. When a player bets 1 dollar on a number, he receives 36 dollars if the ball stops on this number, for a net gain of 35 dollars; otherwise, he loses his dollar bet. Find the expected value for his winnings. In a second version of roulette in Las Vegas, a player bets on red or black. Half of the numbers from 1 to 36 are red, and half are black. If a player bets a dollar on black, and if the ball stops on a black number, he gets his dollar back and another dollar. If the ball stops on a red number or on 0 or 00 he loses his dollar. Find the expected winnings for this bet. A die is rolled twice. Let \\(X\\) denote the sum of the two numbers that turn up, and \\(Y\\) the difference of the numbers (specifically, the number on the first roll minus the number on the second). Show that \\(E(XY) = E(X)E(Y)\\). Are \\(X\\) and \\(Y\\) independent? Show that, if \\(X\\) and \\(Y\\) are random variables taking on only two values each, and if \\(E(XY) = E(X)E(Y)\\), then \\(X\\) and \\(Y\\) are independent. A royal family has children until it has a boy or until it has three children, whichever comes first. Assume that each child is a boy with probability 1/2. Find the expected number of boys in this royal family and the expected number of girls. If the first roll in a game of craps is neither a natural nor craps, the player can make an additional bet, equal to his original one, that he will make his point before a seven turns up. If his point is four or ten he is paid off at \\(2 : 1\\) odds; if it is a five or nine he is paid off at odds \\(3 : 2\\); and if it is a six or eight he is paid off at odds \\(6 : 5\\). Find the player’s expected winnings if he makes this additional bet when he has the opportunity. In Example \\(\\PageIndex{16}\\) assume that Mr. Ace decides to buy the stock and hold it until it goes up 1 dollar and then sell and not buy again. Modify the program StockSystem to find the distribution of his profit under this system after a twenty-day period. Find the expected profit and the probability that he comes out ahead. On September 26, 1980, the New York Times reported that a mysterious stranger strode into a Las Vegas casino, placed a single bet of 777,000 dollars on the “don’t pass\" line at the crap table, and walked away with more than 1.5 million dollars. In the “don’t pass\" bet, the bettor is essentially betting with the house. An exception occurs if the roller rolls a 12 on the first roll. In this case, the roller loses and the “don’t pass\" better just gets back the money bet instead of winning. Show that the “don’t pass\" bettor has a more favorable bet than the roller. Recall that in the martingales doubling system (see Exercise 1.1.10 ), the player doubles his bet each time he loses. Suppose that you are playing roulette in a where there are no 0’s, and you bet on red each time. You then win with probability 1/2 each time. Assume that you enter the casino with 100 dollars, start with a 1-dollar bet and employ the martingale system. You stop as soon as you have won one bet, or in the unlikely event that black turns up six times in a row so that you are down 63 dollars and cannot make the required 64-dollar bet. Find your expected winnings under this system of play. You have 80 dollars and play the following game. An urn contains two white balls and two black balls. You draw the balls out one at a time without replacement until all the balls are gone. On each draw, you bet half of your present fortune that you will draw a white ball. What is your expected final fortune? In the hat check problem (see Example 3.2.8. ), it was assumed that \\(N\\) people check their hats and the hats are handed back at random. Let \\(X_j = 1\\) if the \\(j\\)th person gets his or her hat and 0 otherwise. Find \\(E(X_j)\\) and \\(E(X_j \\cdot X_k)\\) for \\(j\\) not equal to \\(k\\). Are \\(X_j\\) and \\(X_k\\) independent? A box contains two gold balls and three silver balls. You are allowed to choose successively balls from the box at random. You win 1 dollar each time you draw a gold ball and lose 1 dollar each time you draw a silver ball. After a draw, the ball is not replaced. Show that, if you draw until you are ahead by 1 dollar or until there are no more gold balls, this is a favorable game. Gerolamo Cardano in his book, The Gambling Scholar written in the early 1500s, considers the following carnival game. There are six dice. Each of the dice has five blank sides. The sixth side has a number between 1 and 6—a different number on each die. The six dice are rolled and the player wins a prize depending on the total of the numbers which turn up.\n• Find, as Cardano did, the expected total without finding its distribution.\n• Large prizes were given for large totals with a modest fee to play the game. Explain why this could be done. Let \\(X\\) be the first time that a occurs in an infinite sequence of Bernoulli trials with probability \\(p\\) for success. Let \\(p_k = P(X = k)\\) for \\(k = 1\\), 2, …. Show that \\(p_k = p^{k - 1}q\\) where \\(q = 1 - p\\). Show that \\(\\sum_k p_k = 1\\). Show that \\(E(X) = 1/q\\). What is the expected number of tosses of a coin required to obtain the first tail? Exactly one of six similar keys opens a certain door. If you try the keys, one after another, what is the expected number of keys that you will have to try before success? A multiple choice exam is given. A problem has four possible answers, and exactly one answer is correct. The student is allowed to choose a subset of the four possible answers as his answer. If his chosen subset contains the correct answer, the student receives three points, but he loses one point for each wrong answer in his chosen subset. Show that if he just guesses a subset uniformly and randomly his expected score is zero. You are offered the following game to play: a fair coin is tossed until heads turns up for the first time (see Example 6.1.3). If this occurs on the first toss you receive 2 dollars, if it occurs on the second toss you receive \\(2^2 = 4\\) dollars and, in general, if heads turns up for the first time on the \\(n\\)th toss you receive \\(2^n\\) dollars.\n• Show that the expected value of your winnings does not exist (i.e., is given by a divergent sum) for this game. Does this mean that this game is favorable no matter how much you pay to play it?\n• Assume that you only receive \\(2^{10}\\) dollars if any number greater than or equal to ten tosses are required to obtain the first head. Show that your expected value for this modified game is finite and find its value.\n• Assume that you pay 10 dollars for each play of the original game. Write a program to simulate 100 plays of the game and see how you do.\n• Now assume that the utility of \\(n\\) dollars is \\(\\sqrt n\\). Write an expression for the expected utility of the payment, and show that this expression has a finite value. Estimate this value. Repeat this exercise for the case that the utility function is \\(\\log(n)\\). Let \\(X\\) be a random variable which is Poisson distributed with parameter \\(\\lambda\\). Show that \\(E(X) = \\lambda\\). Hint : Recall that \\[e^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots\\,.\\] Recall that in Exercise =1.1.4, we considered a town with two hospitals. In the large hospital about 45 babies are born each day, and in the smaller hospital about 15 babies are born each day. We were interested in guessing which hospital would have on the average the largest number of days with the property that more than 60 percent of the children born on that day are boys. For each hospital find the expected number of days in a year that have the property that more than 60 percent of the children born on that day were boys. An insurance company has 1,000 policies on men of age 50. The company estimates that the probability that a man of age 50 dies within a year is .01. Estimate the number of claims that the company can expect from beneficiaries of these men within a year. Using the life table for 1981 in Appendix C, write a program to compute the expected lifetime for males and females of each possible age from 1 to 85. Compare the results for males and females. Comment on whether life insurance should be priced differently for males and females. A deck of ESP cards consists of 20 cards each of two types: say ten stars, ten circles (normally there are five types). The deck is shuffled and the cards turned up one at a time. You, the alleged percipient, are to name the symbol on each card before it is turned up. Suppose that you are really just guessing at the cards. If you do not get to see each card after you have made your guess, then it is easy to calculate the expected number of correct guesses, namely ten. If, on the other hand, you are guessing with information, that is, if you see each card after your guess, then, of course, you might expect to get a higher score. This is indeed the case, but calculating the correct expectation is no longer easy. But it is easy to do a computer simulation of this guessing with information, so we can get a good idea of the expectation by simulation. (This is similar to the way that skilled blackjack players make blackjack into a favorable game by observing the cards that have already been played. See Exercise \\(\\PageIndex{29}\\).)\n• First, do a simulation of guessing without information, repeating the experiment at least 1000 times. Estimate the expected number of correct answers and compare your result with the theoretical expectation.\n• What is the best strategy for guessing with information?\n• Do a simulation of guessing with information, using the strategy in (b). Repeat the experiment at least 1000 times, and estimate the expectation in this case. Consider the ESP problem as described in Exercise \\(\\PageIndex{25}\\). You are again guessing with information, and you are using the optimal guessing strategy of guessing star if the remaining deck has more stars, circle if more circles, and tossing a coin if the number of stars and circles are equal. Assume that \\(S \\geq C\\), where \\(S\\) is the number of stars and \\(C\\) the number of circles. We can plot the results of a typical game on a graph, where the horizontal axis represents the number of steps and the vertical axis represents the difference between the number of stars and the number of circles that have been turned up. A typical game is shown in Figure \\(\\PageIndex{6}\\. In this particular game, the order in which the cards were turned up is \\((C,S,S,S,S,C,C,S,S,C)\\). Thus, in this particular game, there were six stars and four circles in the deck. This means, in particular, that every game played with this deck would have a graph which ends at the point \\((10, 2)\\). We define the line \\(L\\) to be the horizontal line which goes through the ending point on the graph (so its vertical coordinate is just the difference between the number of stars and circles in the deck).\n• Show that, when the random walk is below the line \\(L\\), the player guesses right when the graph goes up (star is turned up) and, when the walk is above the line, the player guesses right when the walk goes down (circle turned up). Show from this property that the subject is sure to have at least \\(S\\) correct guesses.\n• When the walk is at a point \\((x,x)\\) the line \\(L\\) the number of stars and circles remaining is the same, and so the subject tosses a coin. Show that the probability that the walk reaches \\((x,x)\\) is \\[ \\frac{\\binom{S}{x} \\binom{C}{x} }{ \\binom{S + C}{2x} } \\]Hint : The outcomes of \\(2x\\) cards is a hypergeometric distribution (see Section [sec 5.1]).\n• Using the results of (a) and (b) show that the expected number of correct guesses under intelligent guessing is \\[S + \\sum_{x = 1}^C \\frac{1}{2} \\frac{ \\binom{S}{x} \\binom{C}{x} }{ \\binom{S + C}{2x} } \\] It has been said12 that a Dr. B. Muriel Bristol declined a cup of tea stating that she preferred a cup into which milk had been poured first. The famous statistician R. A. Fisher carried out a test to see if she could tell whether milk was put in before or after the tea. Assume that for the test Dr. Bristol was given eight cups of tea—four in which the milk was put in before the tea and four in which the milk was put in after the tea.\n• What is the expected number of correct guesses the lady would make if she had no information after each test and was just guessing?\n• Using the result of Exercise [exer 6.1.26] find the expected number of correct guesses if she was told the result of each guess and used an optimal guessing strategy. In a popular computer game the computer picks an integer from 1 to \\(n\\) at random. The player is given \\(k\\) chances to guess the number. After each guess the computer responds “correct,\" “too small,\" or “too big.\"\n• Show that if \\(n \\leq 2^k - 1\\), then there is a strategy that guarantees you will correctly guess the number in \\(k\\) tries.\n• Show that if \\(n \\geq 2^k - 1\\), there is a strategy that assures you of identifying one of \\(2^k - 1\\) numbers and hence gives a probability of \\((2^k - 1)/n\\) of winning. Why is this an optimal strategy? Illustrate your result in terms of the case \\(n = 9\\) and \\(k = 3\\). In the casino game of blackjack the dealer is dealt two cards, one face up and one face down, and each player is dealt two cards, both face down. If the dealer is showing an ace the player can look at his down cards and then make a bet called an bet. (Expert players will recognize why it is called insurance.) If you make this bet you will win the bet if the dealer’s second card is a : namely, a ten, jack, queen, or king. If you win, you are paid twice your insurance bet; otherwise you lose this bet. Show that, if the only cards you can see are the dealer’s ace and your two cards and if your cards are not ten cards, then the insurance bet is an unfavorable bet. Show, however, that if you are playing two hands simultaneously, and you have no ten cards, then it is a favorable bet. (Thorp13 has shown that the game of blackjack is favorable to the player if he or she can keep good enough track of the cards that have been played.) Assume that, every time you buy a box of Wheaties, you receive a picture of one of the \\(n\\) players for the New York Yankees (see Exercise [sec 3.2].[exer 3.2.34]). Let \\(X_k\\) be the number of additional boxes you have to buy, after you have obtained \\(k - 1\\) different pictures, in order to obtain the next new picture. Thus \\(X_1 = 1\\), \\(X_2\\) is the number of boxes bought after this to obtain a picture different from the first pictured obtained, and so forth.\n• Show that \\(X_k\\) has a geometric distribution with \\(p = (n - k + 1)/n\\).\n• Simulate the experiment for a team with 26 players (25 would be more accurate but we want an even number). Carry out a number of simulations and estimate the expected time required to get the first 13 players and the expected time to get the second 13. How do these expectations compare?\n• Show that, if there are \\(2n\\) players, the expected time to get the first half of the players is \\[2n \\left( \\frac 1{2n} + \\frac 1{2n - 1} +\\cdots+ \\frac 1{n + 1} \\right)\\ ,\\] and the expected time to get the second half is \\[2n \\left( \\frac 1n + \\frac 1{n - 1} +\\cdots+ 1 \\right)\\ .\\]\n• In Example [exam 6.5] we stated that \\[1 + \\frac 12 + \\frac 13 +\\cdots+ \\frac 1n \\sim \\log n + .5772 + \\frac 1{2n}\\ .\\] Use this to estimate the expression in (c). Compare these estimates with the exact values and also with your estimates obtained by simulation for the case \\(n = 26\\). (Feller14) A large number, \\(N\\), of people are subjected to a blood test. This can be administered in two ways: (1) Each person can be tested separately, in this case \\(N\\) test are required, (2) the blood samples of \\(k\\) persons can be pooled and analyzed together. If this test is this one test suffices for the \\(k\\) people. If the test is each of the \\(k\\) persons must be tested separately, and in all, \\(k + 1\\) tests are required for the \\(k\\) people. Assume that the probability \\(p\\) that a test is positive is the same for all people and that these events are independent.\n• Find the probability that the test for a pooled sample of \\(k\\) people will be positive.\n• What is the expected value of the number \\(X\\) of tests necessary under plan (2)? (Assume that \\(N\\) is divisible by \\(k\\).)\n• For small \\(p\\), show that the value of \\(k\\) which will minimize the expected number of tests under the second plan is approximately \\(1/\\sqrt p\\). Write a program to add random numbers chosen from \\([0,1]\\) until the first time the sum is greater than one. Have your program repeat this experiment a number of times to estimate the expected number of selections necessary in order that the sum of the chosen numbers first exceeds 1. On the basis of your experiments, what is your estimate for this number? The following related discrete problem also gives a good clue for the answer to Exercise [exer 6.1.32]. Randomly select with replacement \\(t_1\\), \\(t_2\\), …, \\(t_r\\) from the set \\((1/n, 2/n, \\dots, n/n)\\). Let \\(X\\) be the smallest value of \\(r\\) satisfying \\[t_1 + t_2 +\\cdots+ t_r > 1\\ .\\] Then \\(E(X) = (1 + 1/n)^n\\). To prove this, we can just as well choose \\(t_1\\), \\(t_2\\), …, \\(t_r\\) randomly with replacement from the set \\((1, 2, \\dots, n)\\) and let \\(X\\) be the smallest value of \\(r\\) for which \\[t_1 + t_2 +\\cdots+ t_r > n\\ .\\]\n• Use Exercise [sec 3.2].[exer 3.2.35.5] to show that \\[P(X \\geq j + 1) = {n \\choose j}{\\Bigl(\\frac {1}{n}\\Bigr)^j}\\ .\\] A coin is tossed until the first time a head turns up. If this occurs on the \\(n\\)th toss and \\(n\\) is odd you win \\(2^n/n\\), but if \\(n\\) is even then you lose \\(2^n/n\\). Then if your expected winnings exist they are given by the convergent series \\[1 - \\frac 12 + \\frac 13 - \\frac 14 +\\cdots\\] called the alternating It is tempting to say that this should be the expected value of the experiment. Show that if we were to do this, the expected value of an experiment would depend upon the order in which the outcomes are listed. Suppose we have an urn containing \\(c\\) yellow balls and \\(d\\) green balls. We draw \\(k\\) balls, without replacement, from the urn. Find the expected number of yellow balls drawn. : Write the number of yellow balls drawn as the sum of \\(c\\) random variables. The reader is referred to Example [exam 6.7] for an explanation of the various options available in Monte Carlo roulette.\n• Compute the expected winnings of a 1 franc bet on red under option (a).\n• Compare the expected winnings for all three options. (from Pittel17) Telephone books, \\(n\\) in number, are kept in a stack. The probability that the book numbered \\(i\\) (where \\(1 \\le i \\le n\\)) is consulted for a given phone call is \\(p_i > 0\\), where the \\(p_i\\)’s sum to 1. After a book is used, it is placed at the top of the stack. Assume that the calls are independent and evenly spaced, and that the system has been employed indefinitely far into the past. Let \\(d_i\\) be the average depth of book \\(i\\) in the stack. Show that \\(d_i \\le d_j\\) whenever \\(p_i \\ge p_j\\). Thus, on the average, the more popular books have a tendency to be closer to the top of the stack. : Let \\(p_{ij}\\) denote the probability that book \\(i\\) is above book \\(j\\). Show that \\(p_{ij} = p_{ij}(1 - p_j) + p_{ji}p_i\\). (from Propp18) In the previous problem, let \\(P\\) be the probability that at the present time, each book is in its proper place, i.e., book \\(i\\) is \\(i\\)th from the top. Find a formula for \\(P\\) in terms of the \\(p_i\\)’s. In addition, find the least upper bound on \\(P\\), if the \\(p_i\\)’s are allowed to vary. Hint : First find the probability that book 1 is in the right place. Then find the probability that book 2 is in the right place, given that book 1 is in the right place. Continue. (from H. Shultz and B. Leonard19) A sequence of random numbers in \\([0, 1)\\) is generated until the sequence is no longer monotone increasing. The numbers are chosen according to the uniform distribution. What is the expected length of the sequence? (In calculating the length, the term that destroys monotonicity is included.) : Let \\(a_1,\\ a_2,\\ \\ldots\\) be the sequence and let \\(X\\) denote the length of the sequence. Then \\[P(X > k) = P(a_1 < a_2 < \\cdots < a_k)\\ ,\\] and the probability on the right-hand side is easy to calculate. Furthermore, one can show that \\[E(X) = 1 + P(X > 1) + P(X > 2) + \\cdots\\ .\\] Let \\(T\\) be the random variable that counts the number of 2-unshuffles performed on an \\(n\\)-card deck until all of the labels on the cards are distinct. This random variable was discussed in Section 3.3. Using Equation [eq 3.3.1] in that section, together with the formula \\[E(T) = \\sum_{s = 0}^\\infty P(T > s)\\] that was proved in Exercise \\(\\PageIndex{33}\\), show that Show that for \\(n = 52\\), this expression is approximately equal to 11.7. (As was stated in Chapter 3, this means that on the average, almost 12 riffle shuffles of a 52-card deck are required in order for the process to be considered random.)"
    },
    {
        "link": "https://online.stat.psu.edu/stat414/book/export/html/675",
        "document": "Toss a fair, six-sided die many times. In the long run (do you notice that it is bolded and italicized?!), what would the average (or \"mean\") of the tosses be? That is, if we have the following, for example: what is the average of the tosses? This example lends itself to a couple of notes.\n• In reality, one-sixth of the tosses will equal \\(x\\) only in the long run (there's that bolding again).\n• The mean is a weighted average, that is, an average of the values weighted by their respective individual probabilities.\n• The mean is called the expected value of \\(X\\), denoted \\(E(X)\\) or by \\(\\mu\\), the greek letter mu (read \"mew\"). If \\(f(x)\\) is the p.m.f. of the discrete random variable \\(X\\) with support \\(S\\), and if the summation: exists (that is, it is less than \\(\\infty\\)), then the resulting sum is called the mathematical expectation, or the expected value of the function \\(u(X)\\). The expectation is denoted \\(E[u(X)]\\). That is: What is the average toss of a fair six-sided die? If the random variable \\(X\\) is the top face of a tossed, fair, six-sided die, then the p.m.f. of \\(X\\) is: for \\(x=1, 2, 3, 4, 5, \\text{and } 6\\). Therefore, the average toss, that is, the expected value of \\(X\\), is: Hmm... if we toss a fair, six-sided die once, should we expect the toss to be 3.5? No, of course not! All the expected value tells us is what we would expect the average of a large number of tosses to be in the long run. If we toss a fair, six-sided die a thousand times, say, and calculate the average of the tosses, will the average of the 1000 tosses be exactly 3.5? No, probably not! But, we can certainly expect it to be close to 3.5. It is important to keep in mind that the expected value of \\(X\\) is a theoretical average, not an actual, realized one! Hannah's House of Gambling has a roulette wheel containing 38 numbers: zero (0), double zero (00), and the numbers 1, 2, 3, ..., 36. Let \\(X\\) denote the number on which the ball lands and \\(u(X)\\) denote the amount of money paid to the gambler, such that: \\begin{array}{lcl} u(X) &=& \\$5 \\text{ if } X=0\\\\ u(X) &=& \\$10 \\text{ if } X=00\\\\ u(X) &=& \\$1 \\text{ if } X \\text{ is odd}\\\\ u(X) &=& \\$2 \\text{ if } X \\text{ is even} \\end{array} How much would I have to charge each gambler to play in order to ensure that I made some money? Assuming that the ball has an equally likely chance of landing on each number, the p.m.f of \\(X\\) is: for \\(x=0, 00, 1, 2, 3, \\ldots, 36\\). Therefore, the expected value of \\(u(X)\\) is: Note that the 18 that is multiplied by the \\$1 and \\$2 is because there are 18 odd and 18 even numbers on the wheel. Our calculation tells us that, in the long run, Hannah's House of Gambling would expect to have to pay out \\$1.82 for each spin of the roulette wheel. Therefore, in order to ensure that the House made money, the House would have to charge at least \\$1.82 per play. Imagine a game in which, on any play, a player has a 20% chance of winning \\$3 and an 80% chance of losing \\$1. The probability mass function of the random variable \\(X\\), the amount won or lost on a single play is: and so the average amount won (actually lost, since it is negative) — in the long run — is: What does \"in the long run\" mean? If you play, are you guaranteed to lose no more than 20 cents? If you play and lose, you are guaranteed to lose \\$1! An expected loss of 20 cents means that if you played the game over and over and over and over .... again, the average of your \\$3 winnings and your \\$1 losses would be a 20 cent loss. \"In the long run\" means that you can't draw conclusions about one or two plays, but rather thousands and thousands of plays. What is the expected value of a discrete random variable \\(X\\) with the following probability mass function: where \\(c\\) is a constant and the support is \\(x=1, 2, 3, \\ldots\\)? The expected value is calculated as follows: The first equal sign arises from the definition of the expected value. The second equal sign just involves replacing the generic p.m.f. notation \\(f(x)\\) with the given p.m.f. And, the third equal sign is because the constant \\(c\\) can be pulled through the summation sign, because it does not depend on the value of \\(x\\). Now, to finalize our calculation, all we need to do is determine what the summation: equals. Oops! You might recognize this quantity from your calculus studies as the divergent harmonic series, whose sum is infinity. Therefore, as the above definition of expectation suggests, we say in this case that the expected value of \\(X\\) doesn't exist. This is the first example where the summation is not absolutely convergent. That is, we cannot get a finite answer here. The expectation for a random variable may not always exist. In this course, we will not encounter nonexistent expectations very often. However, when you encounter more sophisticated distributions in your future studies, you may find that the expectation does not exist.\n\nSuppose the p.m.f. of the discrete random variable \\(X\\) is: What is \\(E(2)\\)? What is \\(E(X)\\)? And, what is \\(E(2X)\\)? This example leads us to a very helpful theorem.\n• If \\(c\\) is a constant, then \\(E(c)=c\\)\n• If \\(c\\) is a constant and \\(u\\) is a function, then: When it exists, the mathematical expectation \\(E\\) satisfies the following properties: Let's return to the same discrete random variable \\(X\\). That is, suppose the p.m.f. of the random variable \\(X\\) is: It can be easily shown that \\(E(X^2)=4.4\\). What is \\(E(2X+3X^2)\\)? This example again leads us to a very helpful theorem. Let \\(c_1\\) and \\(c_2\\) be constants and \\(u_1\\) and \\(u_2\\) be functions. Then, when the mathematical expectation \\(E\\) exists, it satisfies the following property: Before we look at the proof, it should be noted that the above property can be extended to more than two terms. That is: Suppose the p.m.f. of the discrete random variable \\(X\\) is: In the previous examples, we determined that \\(E(X)=1.8\\) and \\(E(X^2)=4.4\\). Knowing that, what is \\(E(4X^2)\\) and \\(E(3X+2X^2)\\)? Using part (b) of the first theorem, we can determine that: And using the second theorem, we can determine that: Let \\(u(X)=(X-c)^2\\) where \\(c\\) is a constant. Suppose \\(E[(X-c)^2]\\) exists. Find the value of \\(c\\) that minimizes \\(E[(X-c)^2]\\). Note that the expectations \\(E(X)\\) and \\(E[(X-E(X))^2]\\) are so important that they deserve special attention.\n\nIn the previous pages, we concerned ourselves with finding the expectation of any general function \\(u(X)\\) of the discrete random variable \\(X\\). Here, we'll focus our attention on one particular function, namely: Let's jump right in, and give the expectation in this situation a special name! First Moment about the Origin When the function \\(u(X)=X\\), the expectation of \\(u(X)\\), when it exists: is called the expected value of \\(X\\), and is denoted \\(E(X)\\). Or, it is called the mean of \\(X\\), and is denoted as \\(\\mu\\) (the greek letter mu, read \"mew\"). That is, \\(\\mu=E(X)\\). The expected value of \\(X\\) can also be called the first moment about the origin. The maximum patent life for a new drug is 17 years. Subtracting the length of time required by the Food and Drug Administration for testing and approval of the drug provides the actual patent life for the drug — that is, the length of time that the company has to recover research and development costs and to make a profit. The distribution of the lengths of actual patent lives for new drugs is as follows: What is the mean patent life for a new drug? Answer The mean can be calculated as: That is, the average patent life for a new drug is 7.9 years. Let \\(X\\) follow a hypergeometric distribution in which n objects are selected from \\(N\\) objects with \\(m\\) of the objects being one type, and \\(N-m\\) of the objects being a second type. What is the mean of \\(X\\)? Recalling the p.m.f. of a hypergeometric distribution and using the definition of the expected value of \\(X\\), we have: You should be getting the idea already that this is going to be messy! So, we're going to work on it in parts. First, note that the first term of the summation equals 0 when \\(x=0\\). And, note that some of the terms can be written differently: Therefore, replacing these quantities in our formula for \\(E(X)\\), we have: My voice gets caught off at the end there, but we still managed to finish the proof in the nick of time! We've shown that, in general, the mean of a hypergeometric random variable \\(X\\), in which \\(n\\) objects are selected from \\(N\\) objects with \\(m\\) of the objects being one type, is: Suppose the random variable \\(X\\) follows the uniform distribution on the first \\(m\\) positive integers. That is, suppose the p.m.f. of \\(X\\) is: What is the mean of \\(X\\)?\n\nConsider two probability mass functions. The first: It is a straightforward calculation to show that the mean of \\(X\\) and the mean of \\(Y\\) are the same: Let's draw a picture that illustrates the two p.m.f.s and their means. Again, the pictures illustrate (at least) two things:\n• The \\(X\\) and \\(Y\\) means are at the fulcrums in which their axes don't tilt (\"a balanced seesaw\").\n• The second p.m.f. exhibits greater variability than the first p.m.f. That second point suggests that the means of \\(X\\) and \\(Y\\) are not sufficient in summarizing their probability distributions. Hence, the following definition! Definition. When \\(u(X)=(X-\\mu)^2\\), the expectation of \\(u(X)\\): is called the variance of \\(X\\), and is denoted as \\(\\text{Var}(X)\\) or \\(\\sigma^2\\) (\"sigma-squared\"). The variance of \\(X\\) can also be called the second moment of \\(X\\) about the mean \\(\\mu\\). The positive square root of the variance is called the standard deviation of \\(X\\), and is denoted \\(\\sigma\\) (\"sigma\"). That is: Although most students understand that \\(\\mu=E(X)\\) is, in some sense, a measure of the middle of the distribution of \\(X\\), it is much more difficult to get a feeling for the meaning of the variance and the standard deviation. The next example (hopefully) illustrates how the variance and standard deviation quantifies the spread or dispersion of the values in the support \\(S\\). Let's return to the probability mass functions of the previous example. The first: What is the variance and standard deviation of \\(X\\)? How does it compare to the variance and standard deviation of \\(Y\\)? The variance of \\(X\\) is calculated as: And, therefore, the standard deviation of \\(X\\) is: Now, the variance of \\(Y\\) is calculated as: And, therefore, the standard deviation of \\(Y\\) is: As you can see, the expected variation in the random variable \\(Y\\), as quantified by its variance and standard deviation, is much larger than the expected variation in the random variable \\(X\\). Given the p.m.f.s of the two random variables, this result should not be surprising. As you might have noticed, the formula for the variance of a discrete random variable can be quite cumbersome to use. Fortunately, there is a slightly easier-to-work-with alternative formula. An easier way to calculate the variance of a random variable \\(X\\) is: Use the alternative formula to verify that the variance of the random variable \\(X\\) with the following probability mass function: is 0.6, as we calculated earlier. First, we need to calculate the expected value of \\(X^2\\): Earlier, we determined that \\(\\mu\\), the mean of \\(X\\), is 4. Therefore, using the shortcut formula for the variance, we verify that indeed the variance of \\(X\\) is 0.6: Suppose the random variable \\(X\\) follows the uniform distribution on the first \\(m\\) positive integers. That is, suppose the p.m.f. of \\(X\\) is: What is the variance of \\(X\\)? On the previous page, we determined that the mean of the discrete uniform random variable \\(X\\) is: If we can calculate \\(E(X^2)\\), we can use the shortcut formula to calculate the variance of \\(X\\). Let's do that: The following theorem can be useful in calculating the mean and variance of a random variable \\(Y\\) that is a linear function of a random variable \\(X\\). If the mean and variance of the random variable \\(X\\) is: respectively, then the mean, variance and standard deviation of the random variable \\(Y=aX+b\\) is: The mean temperature in Victoria, B.C. is 50 degrees Fahrenheit with standard deviation 8 degrees Fahrenheit. What is the mean temperature in degrees Celsius? What is the standard deviation in degrees Celsius? First, recall that the conversion from Fahrenheit (F) to Celsius (C) is: Therefore, the mean temperature in degrees Celsius is calculated as: And, the standard deviation in degrees Celsius is calculated as:\n\nLet's now spend some time clarifying the distinction between a population mean and a sample mean, and between a population variance and a sample variance. Suppose we are interested in determining \\(\\mu\\), the mean number of hours slept nightly by American college students. Because the population of American college students is so large, we can't possibly record the number of hours slept by each American college student. How can we determine the value of the population mean if the population is too large to measure it? We could take a random sample of American college students, calculate the average for the students in the sample, and use that sample mean as an estimate of the population mean. Similarly, we could calculate the sample variance and use it to estimate the population variance \\(\\sigma^2\\) Now, all we need to do is define the sample mean and sample variance! The sample mean, denoted \\(\\bar{x}\\) and read “x-bar,” is simply the average of the \\(n\\) data points \\(x_1, x_2, \\ldots, x_n\\): The sample mean summarizes the \"location\" or \"center\" of the data. A random sample of 10 American college students reported sleeping 7, 6, 8, 4, 2, 7, 6, 7, 6, 5 hours, respectively. What is the sample mean? The sample variance, denoted \\(s^2\\) and read \"s-squared,\" summarizes the \"spread\" or \"variation\" of the data: The sample standard deviation, denoted \\(s\\) is simply the positive square root of the sample variance. That is: A random sample of 10 American college students reported sleeping 7, 6, 8, 4, 2, 7, 6, 7, 6, 5 hours, respectively. What is the sample standard deviation? Therefore, the sample standard deviation is: An easier way to calculate the sample variance is: A random sample of 10 American college students reported sleeping 7, 6, 8, 4, 2, 7, 6, 7, 6, 5 hours, respectively. What is the sample standard deviation? Therefore, the sample standard deviation is: We will get a better feel for what the sample standard deviation tells us later on in our studies. For now, you can roughly think of it as the average distance of the data values \\(x_1, x_2, \\ldots, x_n\\) from their sample mean."
    },
    {
        "link": "https://statisticsbyjim.com/probability/expected-value",
        "document": "What is the Expected Value?\n\nThe expected value in statistics is the long-run average outcome of a random variable based on its possible outcomes and their respective probabilities. Essentially, if an experiment (like a game of chance) were repeated, the expected value tells us the average result we’d see in the long run. Statisticians denote it as E(X), where E is “expected value,” and X is the random variable.\n\nUnderstanding the expected value isn’t just a mathematical exercise—it offers a predictive lens through which we can forecast likely outcomes in uncertain scenarios, from financial investments to decision-making in various fields.\n\nIts origins trace back to the 17th century when famed mathematicians Blaise Pascal and Pierre de Fermat corresponded about gambling problems. They sought a rational way to divide stakes in a game interrupted before its conclusion.\n\nThe concept of expected value emerged from these early deliberations on fair bets and equitable decisions, providing gamblers, scholars, investors, and decision-makers with a robust tool to predict, strategize, and optimize in the face of uncertainty.\n\nIn this post, learn how to find an expected value for different cases and calculate it using formulas for various probability distributions. We’ll work through example calculations for expected values in several contexts.\n\nHow to Find an Expected Value\n\nUnderstanding the general process for calculating the expected value for a discrete random variable is the best place to start. A discrete random variable represents specific, distinct values, often arising from countable outcomes like flipping coins, rolling dice, or counting objects.\n\nTo find the expected value, multiply each possible value of your discrete variable by its probability and then sum all these products.\n\nThe expected value formula for a discrete variable is the following:\n• i is the index variable from 1 to n, all possible values of the discrete variable.\n• n is the number of possible outcomes.\n• P(x ) is the probability of x .\n\nThis method is a fancy version of a weighted average, where each outcome’s weight is its probability. Because the probabilities sum to 1, there is no need to divide. Learn more about Weighted Averages: Formula & Calculation Examples.\n\nImagine a game show where you have a 0.5 chance to win $100, a 0.4 chance to win $500, and a 0.1 probability to lose $100 (negative because you lose money).\n\nTo find the expected value for the game show, we’ll take each outcome (the winnings and loss), multiply it by its probability, and sum them.\n\nHence, the expected value for that game is $240. As more people play this game, the average outcome will converge on this value according to the law of large numbers.\n\nHaving understood the basics, let’s delve into some specific contexts that use variations of the general process I describe above.\n\nWhen outcomes are equally likely (like rolling a fair die), the expected value formula is a straightforward average of the results because all the weights (i.e., probabilities) are equal.\n\nFor example, when you roll a die, each outcome (1 through 6) has an equal 1/6 chance.\n\nTherefore, the expected value of rolling a die is 3.5. When you roll a die many times, the average will converge on this value.\n\nOne of the classic applications of an expected value lies within the realm of the binomial distribution. The binomial distribution models the number of successes in a fixed number of independent Bernoulli trials (think “yes-no” scenarios, like flipping a coin). The expected value formula for a binomial distribution is the following:\n• n is the number of trials.\n• p is the probability of success on a single trial.\n\nImagine you’re taking a 10-question multiple-choice quiz where each question has five choices, and you’re guessing on every question. The probability of getting a question right by guessing is 1 out of 5 options or 0.2.\n\nUsing the expected value formula for the binomial distribution:\n\nSo, if you were to guess randomly on this quiz, you’d expect to answer two questions correctly on average.\n\nThe expected value for a continuous probability distribution is the mean of the random variable. As you take larger random samples from a continuous probability distribution, the sample averages will tend to converge on the expected value thanks to the law of large numbers.\n\nHowever, calculating the mean for a continuous probability distribution is more complex because continuous probabilities apply to a range rather than a distinct value. Consequently, we need to use integration, which you can think of as summing infinitely tiny products of values and their probabilities.\n\nFortunately, these solutions reduce to simple expected value formulas. The table below summarizes them for various discrete and continuous distributions.\n\nBefore we wrap up, here’s a quick cheat sheet on expected value formulas for common probability distributions. Click the links to learn more about each distribution.\n\nWith this guide, you’re well-equipped to make informed predictions and understand the average outcomes of various experiments and scenarios."
    },
    {
        "link": "https://bookdown.org/pkaldunn/DistTheory/Expectation.html",
        "document": ""
    },
    {
        "link": "https://coconino.edu/resources/files/pdfs/academics/sabbatical-reports/kate-kozak/chapter_5.pdf",
        "document": ""
    },
    {
        "link": "https://brilliant.org/wiki/distinct-objects-into-distinct-bins",
        "document": "Distinct objects into distinct bins is a type of problem in combinatorics in which the goal is to count the number of possible distributions of objects into bins. A distribution of objects into bins is an arrangement of those objects such that each object is placed into one of the bins. In this type of problem, the objects and bins are distinct. This means that it matters which objects go into which bin when counting distributions. Derrick is eating lunch with his friends, Edward and Francine. Derrick has an apple, a banana, and a cherry in his lunch that he is thinking about sharing. Derrick can give some, all, or none of his fruit away (e.g., he can keep fruit for himself). In how many ways can Derrick distribute his fruit? In this problem, the \"distinct objects\" are the fruit, and the \"distinct bins\" they are being distributed to are the people. This problem can be solved by listing out all the possibilities, but a more efficient way to solve this problem is to use the rule of product. There are 3 possible destinations for the apple, 3 possible destinations for the banana, and 3 possible destinations for the cherry. Because these objects are distributed at the same time, the rule of product is used, and thus there are \\(3\\times 3\\times 3=\\boxed{27}\\) possible distributions of the fruit. \\(_\\square\\)\n\nIn the previous example, there was no regard for fairness when determining the possible distributions of fruit. One of the possible distributions had Derrick keeping all the fruit for himself. Another possible distribution had Derrick giving all the fruit to Francine. In the base case of the \"distinct objects into distinct bins\" problem, each object is placed independently, and this allows for the distributions to be counted efficiently using the rule of product. Suppose there are \\(n\\) distinct objects that are to be distributed among \\(r\\) distinct bins. This can be done in precisely \\(r^n\\) ways. For each object, there are \\(r\\) bins it can be placed into. This placement occurs for each of the \\(n\\) objects. By the rule of product, this can be done in \\(\\underbrace{r \\times r \\times r \\times \\cdots \\times r} _{n \\text{ times}} = r^n\\) ways. \\(_\\square\\) We have 8 carrots of different sizes and 2 cute bunnies. In how many ways can we feed the bunnies? (The bunnies are very hungry, so they will eat all 8 carrots.) In this question, we can model the carrots as objects and the bunnies as bins. Since they are all distinct, the above formula holds true. Therefore, the total number of ways in which the bunnies can be fed is \\(2^8 = 256\\). \\(_\\square\\) Note: If you are familiar with binary numbers, you can think of it this way. Suppose we have an 8-digit binary number--the number of digits corresponds to the number of carrots, and the base of the number corresponds to the number of bunnies. If a digit is 0, it means that a carrot is given to the first bunny; if a digit is 1, it means that a carrot is given to the second bunny. For example, the number 01100001 means that the \\(2^\\text{nd}, 3^\\text{rd},\\) and \\(8^\\text{th}\\) carrots are given to the second bunny. There are a total of \\(2^8\\) 8-digit binary numbers, so the answer is 256. In how many ways can \\(5\\) balls, each of a different color, be distributed among \\(3\\) distinct urns? In this question, the balls are the objects and the urns are the bins. Since each ball has a different color, they are distinct, and the urns are distinct as well. Therefore, we can use the above formula and see this can be done in \\(3^5 = 243\\) ways. \\(_\\square\\) There are 20 distinct people signed up for a raffle. There are 3 prizes that can be won: a 100 cm HDTV, a signed football jersey, and an envelope filled with gift cards. It is possible for a person to win more than one prize. How many possible prize distributions are there? Discount Al's Thrift Shop is having a one-day going-out-of-business sale in which everything must go! Al only has four distinct things left in his shop. All the years that Al has been in business, he's only ever had four distinct customers, and he does not expect that to change today At the end of the day, the bank representative will come to foreclose the shop, and he will take whatever is left. How many ways can Al's things be sold to his customers (or taken by the bank)?\n\nThe previous examples and problems covered situations in which all of the objects were distributed. However, what if only some of the objects are distributed? This opens up some interesting possibilities. Suppose there are \\(n\\) distinct objects, of which \\(k\\) are to be distributed among \\(r\\) distinct bins. This can be done in precisely \\(\\binom{n}{k}r^k\\) ways. Note: \\(\\binom{n}{k}\\) is notation for the binomial coefficient. There are \\(\\binom{n}{k}\\) combinations of \\(k\\) objects chosen from \\(n\\) distinct objects. These \\(k\\) objects are then distributed among \\(r\\) distinct bins. For each combination, there are \\(r^k\\) distributions of the \\(k\\) objects among the \\(r\\) bins. Thus, there are a total of \\(\\binom{n}{k}r^k\\) distributions of \\(k\\) objects, chosen from \\(n\\) distinct objects, into \\(r\\) distinct bins. \\(_\\square\\) Grandpa Joe has \\(8\\) different presents that he is considering giving to his \\(5\\) grandchildren. However, he decides that he'd like to keep \\(2\\) presents for himself. How many ways can he distribute the remaining presents? In this problem, there are \\(8\\) distinct objects, of which \\(6\\) are to be distributed among \\(5\\) distinct bins. Using the above theorem, this can be done in precisely \\(\\binom{8}{6}5^6=28\\times15625=\\boxed{437500}\\) ways. \\(_\\square\\) This result is actually more than the number of distributions of \\(8\\) distinct objects into \\(5\\) distinct bins, \\(5^8=390625\\). At a farm auction, various pieces of farm equipment and livestock are bid on and sold. Today, there are 7 distinct items up for auction, and there are 3 farmers bidding on them. One of the farmers declares that he will be bid on at least 2 of the items (not specifying which ones), and he won't be outbid. If this farmer is telling the truth, then how many ways can the items be distributed among farmers? Assume that all items are sold.\n\nMore interesting and challenging problems can be made by imposing additional conditions. Generic formula for this type, In how many ways can \\(5\\) balls, each of a different color, be distributed among \\(3\\) distinct urns such that no urn remains empty? The restriction that no urn is left empty seems harmless, but it makes the problem much more complicated than previous problems and examples in this wiki. If we use the generic formula we directly get \\(3^5-3*(3-1)^5+3+0\\) . therefore \\(243-3*32+3 = \\boxed{150}\\) . Other way is to solve this problem is by using the principle of inclusion and exclusion. First, consider what the problem would be without the restriction. Let \\(U\\) be the set of all distributions of \\(5\\) distinct objects into \\(3\\) distinct bins. \\(|U|=3^5=243\\). Now let \\(A\\) be the set of all distributions of \\(5\\) distinct objects into the \\(1^\\text{st}\\) and \\(2^\\text{nd}\\) bins. Likewise, let \\(B\\) be the set of all distributions into the \\(1^\\text{st}\\) and \\(3^\\text{rd}\\) bins, and let \\(C\\) be the set of all distributions into the \\(2^\\text{nd}\\) and \\(3^\\text{rd}\\) bins. \\(A\\cup B\\cup C\\) will be the set of all distributions in which at least one bin is left empty. Our goal is to find the number of distributions in which no bin is left empty. This will be \\(|U|-|A\\cup B\\cup C|\\). By the principle of inclusion and exclusion, \\[|A\\cup B \\cup C| = |A| + |B| + |C| - |A \\cap B| - |A \\cap C| - |B \\cap C| + |A \\cap B \\cap C|.\\] \\(|A|\\) is the number of distributions of \\(5\\) distinct objects into \\(2\\) distinct bins. This is \\(|A|=2^5=32\\). Without loss of generality, \\(|A|=|B|=|C|\\). \\(|A\\cap B|\\) is the number of distributions of \\(5\\) distinct objects into \\(1\\) distinct bin. This is \\(|A\\cap B|=1\\). Without loss of generality, \\(|A\\cap B|=|A\\cap C|=|B\\cap C|\\). It is not possible for all three bins to be empty, so \\(|A\\cap B\\cap C|=0\\). Using the principle of inclusion and exclusion formula, Therefore, the number of distributions of \\(5\\) distinct balls into \\(3\\) distinct urns in which no urn is left empty is Sri and Godfrey are marooned on a desert island. Together, they have a toothbrush, a calculator, a volleyball, an mp3 player, a kite, and a shovel. They decide to distribute these objects randomly among themselves. However, they agree that each person should get at least one object. How many ways can the objects be distributed among Sri and Godfrey? It is the end of the school year, and a teacher is giving out awards to her 3 students. She has 6 distinct awards (for grades, attendance, generosity, etc.) to give out, and she will give each award to the student who is most deserving. However, she knows that her students can be rather immature, and one of them might throw a fit if he or she doesn't get an award. She secretly decides to make sure that each student gets at least one award (even if he or she doesn't deserve it). How many ways can the awards be distributed among the students if all of the awards are given?"
    },
    {
        "link": "https://math.stackexchange.com/questions/1892883/distributing-k-objects-in-n-containers-probability-distribution-for-combin",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://geeksforgeeks.org/discrete-mathematics-the-pigeonhole-principle",
        "document": "The Pigeonhole Principle is a fundamental concept in combinatorics and mathematics that states if more items are put into fewer containers than the number of items, at least one container must contain more than one item. This seemingly simple principle has profound implications and applications in various fields, including mathematics, computer science, and engineering.\n\nThis article explores the Pigeonhole Principle, its mathematical formulation, examples, and applications.\n\nThe Pigeonhole Principle can be formally stated as follows:\n\nThe pigeonhole principle is useful in counting methods. In order to apply the principle, one has to decide which objects will play the role of pigeon and which objects will play the role of pigeonholes.\n\nSuppose that a flock of 20 pigeons flies into a set of 19 pigeonholes to roost. Because there are 20 pigeons but only 19 pigeonholes, at least one of these 19 pigeonholes must have at least two pigeons in it. To see why this is true, note that if each pigeonhole had at most one pigeon, at most 19 pigeons, one per hole, could be accommodated. This illustrates a general principle called the pigeonhole principle, which states that if there are more pigeons than pigeonholes, there must be at least one pigeonhole with at least two pigeons.\n\nTheorem -I) If “A” is the average number of pigeons per hole, where A is not an integer then\n• None At least one pigeon hole contains ceil[A] (smallest integer greater than or equal to A) pigeons.\n• None The remaining pigeon holes contain at most floor[A] (largest integer less than or equal to A) pigeons.\n\nOr II) We can say that, if n + 1 objects are put into n boxes, then at least one box contains two or more objects. The abstract formulation of the principle: Let X and Y be finite sets and let f: A→B be a function.\n• None If X has more elements than Y, then f is not one-to-one.\n• None If X and Y have the same number of elements and f is onto, then f is one-to-one.\n• None If X and Y have the same number of elements and f is one-to-one, then f is onto.\n\nThe pigeonhole principle is one of the simplest but most useful ideas in mathematics. We will see more applications that proof of this theorem.\n\nExample 1: If (Kn+1) pigeons are kept in n pigeon holes where K is a positive integer, what is the average no. of pigeons per pigeon hole?\n\nExample 2: A bag contains 10 red marbles, 10 white marbles, and 10 blue marbles. What is the minimum no. of marbles you have to choose randomly from the bag to ensure that we get 4 marbles of same color?\n\nLet q , q , . . . , q be positive integers. If q + q + . . . + q – n + 1 objects are put into n boxes, then either the 1st box contains at least q objects, or the 2nd box contains at least q objects, . . ., the nth box contains at least q objects. Application of this theorem is more important, so let us see how we apply this theorem in problem solving.\n\nExample 1:In a computer science department, a student club can be formed with either 10 members from first year or 8 members from second year or 6 from third year or 4 from final year. What is the minimum no. of students we have to choose randomly from department to ensure that a student club is formed?\n\nExample 2:A box contains 6 red, 8 green, 10 blue, 12 yellow and 15 white balls. What is the minimum no. of balls we have to choose randomly from the box to ensure that we get 9 balls of same color?\n\n1. In a group of 13 people, at least two people must share the same birth month, since there are only 12 months in a year.\n\n2. If 11 people each shake hands with 12 others, at least one person must have shaken hands with at least 2 others, because 12 handshakes among 11 people means some people must have more than one handshake.\n\n3. If you have 10 pairs of black socks and 10 pairs of white socks randomly distributed in a drawer, pulling out 3 socks guarantees that at least one pair is of the same color (assuming pairs are distinguishable).\n\nThe Pigeonhole Principle is used in various fields to solve problems that involve distributing objects into containers. Here are some notable applications:\n\n1. Computer Science: In computer science, the Pigeonhole Principle is used in hashing algorithms, where data items (keys) are assigned to hash values (buckets). The principle guarantees that if there are more items than buckets, collisions will occur, necessitating strategies to handle them.\n\n2. Coding Theory: In coding theory, the Pigeonhole Principle helps in understanding error detection and correction. It implies that if you encode more messages than the number of unique codewords, some messages will share the same codeword, leading to potential errors.\n\n3. Number Theory: In number theory, the Pigeonhole Principle can prove the existence of certain properties within a set of numbers. For instance, it can show that within any set of n+1n+1n+1 integers, there exists a pair of integers whose difference is divisible by nnn.\n\n4. Graph Theory: In graph theory, the Pigeonhole Principle is used to prove properties of graphs, such as the existence of a subgraph with certain characteristics. For example, in any group of people, some subset of individuals will have mutual friendships or mutual non-friendships.\n\n1. Prove that among any 52 integers, there are always two whose difference is divisible by 51.\n\n2. In a group of 1001 people, show that there are at least two people with the same number of friends within the group.\n\n3. Prove that if 5 points are placed inside a unit square, there must be two points that are at most 1/√2 units apart.\n\n4. Show that in any sequence of 101 distinct real numbers, there is always an increasing or decreasing subsequence of length at least 11.\n\n5. Prove that for any set of 10 integers, there are always two distinct subsets with the same sum of elements.\n\n6. In a 8×8 chessboard, prove that if 65 squares are colored, at least one rectangle formed by the grid lines must have all four\n\n7. Show that among any 100 integers, there are always two whose difference is divisible by 99.\n\n8. Prove that in any group of 6 people, there are either 3 mutual friends or 3 mutual strangers.\n\n9. Given 2n+1 integers, prove that there are always two whose sum or difference is divisible by 2n.\n\n10.In a room of 367 people, prove that at least two people have the same birthday, assuming no leap years.\n\n1. Prove or disprove that all non-trivial zeros of the Riemann zeta function have a real part equal to 1/2.\n\n2. Determine whether every problem whose solution can be quickly verified by a computer can also be solved quickly by a computer.\n\n3. Prove that for projective algebraic varieties, certain topological classes called Hodge classes are actually algebraic cycles.\n\n4. Relate the rank of an elliptic curve to the order of zero of its L-function at s = 1.\n\n5. Prove that for any positive integer n, the sequence defined by n → n/2 (if n is even) and n → 3n + 1 (if n is odd) always reaches 1.\n\n6. Prove that every even integer greater than 2 can be expressed as the sum of two prime numbers.\n\n7. Prove that there are infinitely many pairs of prime numbers that differ by 2.\n\n8. Prove or disprove this conjecture about the computational hardness of approximately solving certain types of constraint satisfaction problems.\n\n9. Prove that for every positive integer n ≥ 2, the fraction 4/n can be expressed as the sum of three unit fractions.\n\n10. Prove or disprove that every bounded linear operator on a complex Hilbert space has a non-trivial invariant subspace.\n\nThe Pigeonhole Principle is a powerful and intuitive concept in mathematics, offering a straightforward method for proving the existence of certain properties within sets. Its applications span across various disciplines, providing solutions to problems in computer science, coding theory, number theory, and graph theory. Understanding and applying this principle can simplify complex problems and yield valuable insights.\n\nWhat is the Pigeonhole Principle?\n\nHow is the Pigeonhole Principle used in computer science?\n\nCan the Pigeonhole Principle be applied to infinite sets?\n\nWhat are some real-life examples of the Pigeonhole Principle?\n\nHow does the Pigeonhole Principle help in proving mathematical theorems?"
    },
    {
        "link": "https://math.stackexchange.com/questions/4046823/how-to-distribute-10-distinct-envelopes-into-3-identical-mailboxes",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://bookdown.org/jgscott/DSGI/probability-models.html",
        "document": ""
    }
]