[
    {
        "link": "https://datacarpentry.github.io/R-genomics/04-dplyr.html",
        "document": ""
    },
    {
        "link": "https://listendata.com/2016/08/dplyr-tutorial.html",
        "document": "This tutorial explains how to use the dplyr package for data analysis, along with several examples. It's a complete tutorial on data manipulation and data wrangling with R.\n\nThe dplyr package is one of the most powerful and popular package in R. This package was written by the most popular R programmer Hadley Wickham who has written many useful R packages such as ggplot2, tidyr etc.\n\nThe dplyr is a powerful R-package to manipulate, clean and summarize unstructured data. In short, it makes data exploration and data analysis easy and fast in R.\n\nThe package \"dplyr\" comprises many functions that perform mostly used data manipulation operations such as applying filter, selecting specific columns, sorting data, adding or deleting columns and aggregating data. Another most important advantage of this package is that it's very easy to learn and use dplyr functions. Also it's easy to remember these functions. For example, filter() is used to filter rows.\n\nHow to install and load dplyr package\n\nTo install the dplyr package, type the following command.\n\nTo load dplyr package, type the command below:\n\ndplyr functions process faster than base R functions. It is because dplyr functions were written in a computationally efficient manner. They are also more stable in the syntax and better supports data frames than vectors.\n\nPeople have been using SQL for analyzing data for decades. Every modern data analysis software such as Python, R, SAS etc supports SQL commands. But SQL was never designed to perform data analysis. It was rather designed for querying and managing data. There are many data analysis operations where SQL fails or makes simple things difficult. For example, calculating median for multiple variables, converting wide format data to long format etc. Whereas, dplyr package was designed to perform data analysis.\n\nThe names of dplyr functions are similar to SQL commands such as select() for selecting variables, group_by() - group data by grouping variable, join() - joining two data sets. Also includes inner_join() and left_join(). It also supports sub queries for which SQL was popular for.\n\nIn this tutorial, we are using the following data which contains income generated by states from year 2002 to 2015. Note : This data do not contain actual income figures of the states. To download the dataset, click on this link - Dataset and then right click and hit 'Save as' option.\n\nThis dataset contains 51 observations (rows) and 16 variables (columns). The snapshot of first 6 rows of the dataset is shown below.\n\nSubmit the following code to load data directly from link. If you want to load the data from your local drive, you need to change the file path in the code below.\n\nThe sample_n function selects random rows from a data frame (or table). The second parameter of the function tells R the number of rows to select.\n\nThe sample_frac function returns randomly N% of rows. In the example below, it returns randomly 10% of rows.\n\nExample 3 : Remove Duplicate Rows based on all the variables (Complete Row)\n\nThe distinct function is used to eliminate duplicates.\n\nIn this dataset, there is not a single duplicate row so it returned same number of rows as in mydata.\n\nThe .keep_all function is used to retain all other variables in the output data frame.\n\nIn the example below, we are using two variables - Index, Y2010 to determine uniqueness.\n\nThe select() function is used to select only desired variables.\n\nSuppose you are asked to select only a few variables. The code below selects variables \"Index\", columns from \"State\" to \"Y2008\".\n\nThe minus sign before a variable tells R to drop the variable.\n\nThe above code can also be written like :\n\nExample 8 : Selecting or Dropping Variables starts with 'Y'\n\nThe starts_with() function is used to select variables starts with an alphabet.\n\nAdding a negative sign before starts_with() implies dropping the variables starts with 'Y'.\n\nExample 9 : Selecting Variables contain 'I' in their names\n\nThe code below keeps variable 'State' in the front and the remaining variables follow that.\n\nNew order of variables are displayed below -\n\nThe rename() function is used to change variable name.\n\nThe rename function can be used to rename variables.\n\nIn the following code, we are renaming 'Index' variable to 'Index1'.\n\nThe filter() function is used to subset data with matching logical conditions.\n\nSuppose you need to subset data. You want to filter rows and retain only those values in which Index is equal to A.\n\nThe %in% operator can be used to select multiple items. In the following program, we are telling R to select rows against 'A' and 'C' in column 'Index'.\n\nExample 14 : 'AND' Condition in Selection Criteria\n\nSuppose you need to apply 'AND' condition. In this case, we are picking data for 'A' and 'C' in the column 'Index' and income greater than 1.3 million in Year 2002.\n\nExample 15 : 'OR' Condition in Selection Criteria\n\nThe 'I' denotes OR in the logical condition. It means any of the two conditions.\n\nThe \"!\" sign is used to reverse the logical condition.\n\nThe grepl function is used to search for pattern matching. In the following code, we are looking for records wherein column state contains 'Ar' in their name.\n\nThe summarise() function is used to summarize data.\n\nIn the example below, we are calculating mean and median for the variable Y2015.\n\nIn the following example, we are calculating number of records, mean and median for variables Y2005 and Y2006. The summarise_at function allows us to select multiple variables by their names.\n\nhas been soft-deprecated (dropped) from dplyr 0.8.0. Instead we should use . The equivalent code is stated below -\n\nAnother way of using it without stating names is through formula instead of function. This is function and this is formula.\n\nYou must be wondering about and symbols. It's a way to pass purrr style anonymous function. See the base R method as compared to purrr style below. Both returns the same output. purrr style provides a shortcut to define anonymous function.\n\nSymbolsandmeans the same thing. You can try the above code by replacingwith\n\nIncase you want to add additional arguments for the functions mean and median (for example na.rm = TRUE), you can do it like the code below.\n\nWe can also use custom functions in the summarise function. In this case, we are computing the number of records, number of missing values, mean and median for variables Y2011 and Y2012. The dot (.) denotes each variables specified in the second argument of the function.\n\nInstead of funs( ), you should make a habit of using list( ) as funs( ) can be dropped in future versions of dplyr package.\n\nSuppose you want to subtract mean from its original value and then calculate variance of it.\n\nEquivalent purrr style method can be written like this :\n\nThe summarise_if function allows you to summarise conditionally.\n\nstore data for all the numeric variablesthefunction calculates summary statistics for all the columns in a data frameWe are checking theandin a categorical (factor) variable.\n\nThe arrange() function is used to sort data.\n\nTo sort a variable in descending order, use\n\nThe default sorting order of arrange() function is ascending. In this example, we are sorting data by multiple variables.\n\nSuppose you need to sort one variable by descending order and other variable by ascending oder.\n\nIt is important to understand the pipe (%>%) operator before knowing the other functions of dplyr package. dplyr utilizes pipe operator from another package (magrittr). It allows you to write sub-queries like we do it in sql.\n\nNote : All the functions in dplyr package can be used without the pipe operator. The question arises \"Why to use pipe operator %>%\". The answer is it lets to wrap multiple functions together with the use of %>%.\n\nThe code below demonstrates the usage of pipe %>% operator. In this example, we are selecting 10 random observations of two variables \"Index\" \"State\" from the data frame \"mydata\".\n\nThe group_by() function is used to group data by categorical variable(s).\n\nWe are calculating count and mean of variables Y2011 and Y2012 by variable Index.\n\nThe above code can also be written likeSince dplyr >= 1.0.0 version you may get the following warnings.To suppress this warning you can use the following command.\n\nThe do() function is used to compute within groups\n\nSuppose you need to pull top 2 rows from 'A', 'C' and 'I' categories of variable Index.\n\nExample 26 : Selecting 3rd Maximum Value by Categorical Variable\n\nThe slice() function is used to select rows by position.\n\nExample 27 : Summarize, Group and Sort Together\n\nWe are calculating third maximum value of variable Y2015 by variable Index. The following code first selects only two variables Index and Y2015. Then it filters the variable Index with 'A', 'C' and 'I' and then it groups the same variable and sorts the variable Y2015 in descending order. At last, it selects the third row.Like SQL, dplyr uses window functions that are used to subset data within a group. It returns a vector of values. We could usethat calculates rank in the preceding example,In this case, we are computing mean of variables Y2014 and Y2015 by variable Index. Then sort the result by calculated mean variable Y2015.\n\nThe mutate() function is used to create new variables.\n\nThe following code calculates division of Y2015 by Y2014 and name it \"change\".\n\nExample 29 : Multiply all the variables by 1000\n\nIt creates new variables and name them with suffix \"_new\".\n\nThe output shown in the image above is truncated due to high number of variables.Note - The above code returns the following error messages -1: In Ops.factor(c(1L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 4L, 5L, 6L, :‘*’ not meaningful for factors2: In Ops.factor(1:51, 1000) : ‘*’ not meaningful for factorsIt implies you are multiplying 1000 to string(character) values which are stored as factor variables. These variables are 'Index', 'State'. It does not make sense to apply multiplication operation on character variables. For these two variables, it creates newly created variables which contain only NA.Suppose you need to calculate rank for variables Y2008 to Y2010.\n\nBy default, min_rank() assigns 1 to the smallest value and high number to the largest value. In case, you need to assign rank 1 to the largest value of a variable, use min_rank(desc(.))\n\nExample 31 : Select State that generated highest income among the variable 'Index'\n\nThecalculates cumulative sum of a variable. Withwe insert a new variable called 'Total' which contains values of cumulative income of variable Index.\n\nThe join() function is used to join two datasets.\n\nExample 33 : Common rows in both the tables\n\nExample 37 : Rows appear in one table but not in other table\n\nExample 38 : IF ELSE Statement\n\ndatasets (or tables) to merge / joincommon variable (primary key) to join by.returns rows when there is a match in both tables. In this example, we are merging df1 and df2 with ID as common variable (primary key).If the primary key does not have same name in both the tables, try the following way:It returns all rows from the left table, even if there are no matches in the right table.Rows that appear in both x and y.Rows that appear in either or both x and y.Rows that appear in x but not y.selects unique rows that are common to both the data frames.displays all rows from both the tables and removes duplicate records from the combined dataset. By using, it allows duplicate rows in the combined dataset.true : Value if condition meetsfalse : Value if condition does not meetmissing : Value if missing cases.It will be used to replace missing values (Default : NULL)If a value is less than 5, add it to 1 and if it is greater than or equal to 5, add it to 2. Otherwise 0.Multiple IF ELSE statement can be written using if_else() function. See the example below -We can usefunction to write nested if-else queries. In case_when(), you can use variables directly within case_when() wrapper.refers to ELSE statement.Suppose you want to find maximum value in each row of variables 2012, 2013, 2014, 2015. Thefunction allows you to apply functions to rows.Suppose you are asked to combine two data frames. Let's first create two sample datasets.Thecombine two datasets with rows. So combined dataset would containIt is equivalent to base R function rbind.Thecombine two datasets with columns. So combined dataset would containThe output is shown below-\n\nThe quantile() function is used to determine Nth percentile value. In this example, we are computing percentile values by variable Index.\n\nExample 44 : Number of levels in factor variables\n\nExample 45 : Multiply by 1000 to numeric variables\n\nExample 46 : Convert value to NA\n\nExample 47 : Use of pull( ) function\n\nExample 48 : How to deal with Quotation\n\nThefunction is used to divide the data into N bins.This example explains the advanced usage of. In this example, we are building linear regression model for each level of a categorical variable. There are 3 levels in variable cyl of dataset mtcars.It includes functions like select_if, mutate_if, summarise_if. They come into action only when logical condition meets. See examples below.Thefunction returns only those columns where logical condition is TRUE. Therefers to retain only numeric variables.Similarly, you can use the following code for selecting factor columns -Like select_if() function, summarise_if() function lets you to summarise only for variables where logical condition holds.It returns 19 levels for variable Index and 51 levels for variable State.In this example, we are converting \"\" to NA usingfunction.\"a\" \"b\" NA \"d\"is equivalent to writingorIf you want output to be in vector rather than data frame (default method), you can use pull( ) function.\n\nLet's understand with example. You want to use a variable which is in quotes. In the example below, Species is in quotes. If you use quoted variable directly, it would return zero rows. To make it work, you need to use operator which unquotes its argument and gets evaluated immediately in the surrounding context. The final thing we need to do is turn the character string \"Species\" into Species, a symbol by using function.\n\nis used to quote its argument. Here we are asking user to define variable name without quotes.\n\nExample 49 : How to use SQL rank() over(partition by)\n\nIn SQL, is used to compute rank by a grouping variable. In dplyr, it can be achieved very easily with a single line of code. See the example below. Here we are calculating rank of variable Y2015 by variable Index.\n\nIn dplyr, there are many functions to compute rank other than . These are , , .\n\nThe across( ) function was added starting dplyr version 1.0. It helps analyst to perform same operation on multiple columns. Let's take a sample data.frame and calculate mean on variables from 'mpg' through 'qsec' by 'carb'.\n\nThe code below calculates average on numeric variables. It identifies numeric variables using where() function.\n\nHere we are using two summary statistics - mean and no. of distinct values in two different set of variables.\n\ncan also be applied with mutate function\n\nThere are hundreds of packages that are dependent on this package. The main benefit it offers is to take off fear of R programming and make coding effortless and lower processing time. However, some R programmers prefer data.table package for its speed. I would recommend learn both the packages. The data.table package wins over dplyr in terms of speed if data size greater than 1 GB."
    },
    {
        "link": "https://dplyr.tidyverse.org",
        "document": "dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges:\n• adds new variables that are functions of existing variables\n• changes the ordering of the rows.\n\nThese all combine naturally with which allows you to perform any operation “by group”. You can learn more about them in . As well as these single-table verbs, dplyr also provides a variety of two-table verbs, which you can learn about in .\n\nIf you are new to dplyr, the best place to start is the data transformation chapter in R for Data Science."
    },
    {
        "link": "https://finley-lab.com/files/ifdar/dplyr",
        "document": "This book is in. We want your feedback to make the book better for you and other readers. To add your annotation,and then click theon the pop-up menu. To see the annotations of others, click thein the upper right hand corner of the page\n\nThe Minnesota tree growth dataset introduced in Section 6.1 and datasets presented in Section 1.2, provide a look at different ways to organize data in a single table. In most cases, dataset rows correspond to observational units (e.g., plots, trees, or years) and columns correspond to variables. This is the data organization format envisioned when applying functions.\n\nAccording to the package’s author Hadley Wickham, is designed to be “a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges.” The functions do, in fact, capture data analysis actions (i.e., verbs). These verbs are best organized into three categories based on the component of the dataset upon which they operate:\n• Columns:\n• subset columns based on name or characteristics.\n\nA detailed cheat sheet is available from within RStudio and more resources are found by running on the console. Also, Hadley Wickham wrote a vignette that compares functions to their base R equivalents. Run in the console to access this vignette. If you’re new to we suggest working through this chapter first, then return to the vignette if you want a crosswalk between and base R operations.\n\nSections 7.1 through 7.8 introduce verb functions that operate on a single tibble. We then cover several support functions designed to extend the verb functions’ usefulness and flexibility. Topics in each section are illustrated using a minimal example based on a small toy tibble called , created in Section 6.2. The toy dataset example is followed by a more thorough exposition using the Minnesota tree growth dataset introduced in Section 6.1. Extensive workflow examples using the Minnesota tree growth dataset are provided in Section 7.13. Finally, Section 7.14 covers a set of verb functions to combine information from two or more tibbles.\n\ncreates new columns and adds them to the right side of an existing tibble. The first argument in is the tibble to be augmented and subsequent arguments define the new columns. This function is particularly useful because exiting columns in the tibble can be referenced when defining new columns. This feature is illustrated in Figure 7.6 using the existing column to create a basal area ( ) column. (ref:mutate-fig) Using to add a basal area ( ) column to . Sometimes, we want a tibble that includes only newly created columns, or a combination of original and new columns. In this case we use . For example, as illustrated in Figure 7.7, say we want to make using in , but this time we want the new tibble to include only the columns , , and (i.e., we don’t want to retain ). Also notice in Figure 7.7, we can carryover columns unchanged by just listing their names in transmute arguments. (ref:transmute-fig) Using to create basal area ( ) and keep and columns. Let’s use to add a basal area column to . Given DBH in cm, we calculate BA (m\\(^2\\)) = \\(0.00007854\\cdot \\text{DBH}^2\\), which is implemented in the code below to create . The above shows is now the last column, which is the default placement for columns created using and . Take a look at and optional and arguments that allow you to override this default and control column placement. If you feel sad for —being in the last column and all—then check out in the next section, which also allows you to change column order for an existing tibble. Often the values we assign to a new column using are conditional on the values in one or more existing columns. For example, say we want to add a species column to and we know tree 1 is Larix laricina, 2 is Picea glauca, and 3 is Populus tremuloides. So, how do we go about creating such a column? One approach is to use the ’s function, which is defined as , where is a logical vector created using one or more vectors and logical operators in Section 4.7, is the value to use if the is , is the value to use if the is , and is the value to use if the is neither nor , e.g., . One important point about ’s function is the values given in the and arguments must be of the same type (see data types in Section 4.2.1 and for more details). If this requirement annoys you, then consider base R’s less strict function that will coerce argument values to the same type (see ). While is a standalone function (meaning it can be used on its own to create a vector), we often use it in and/or other functions where vectors are created or values changed. Let’s begin by using within to create a column and populate rows with “Larix laricina” and, for demonstration, all other rows with . All rows in the column can be filled in as follows. Notice above rows are incrementally filled, first for 1, then 2, and then 3. Alternatively, you can use nested conditional statements, where the argument holds another . is a powerful tool in some situations, particularly when we need to test only one condition. However, as illustrated above, coding can be onerous when we need to test multiple conditions. The alternative to writing multiple nested statements, is (see ). Like , is a standalone function but is often used in to create or modify columns. takes one or more arguments, where each argument, or case, is a two-sided formula where the left side is separated by the right side by a tilde symbol . The left side is a logical vector created using one or more vectors and logical operators in Section 4.7. The right side is the value to use when the left side is . If no cases are (i.e., none of the arguments’ left sides are ) then a is returned. Like, , requires values on the left side of cases to be the same type. As one final example, let’s add a diameter class column to . We’ll use to return only and our new columns. Diameter class criteria and associated naming are given on the left and right side, respectively, within .\n\nMany datasets we encounter have a nested structure (e.g., measurements on trees within plots, plots within stands, stands within a forest) and/or populations of interest (e.g., species, geographic region, product type). When dataset rows correspond to observational units and columns correspond to variables, observation placement within nested levels or population membership is defined by values in one or more discrete, nominal, or ordinal variables (Figure 10.1). In this way, observations (i.e., rows) can be grouped by common nesting and/or population values. adds such row grouping information to a tibble and returns a grouped tibble. The first argument in is the tibble to group and subsequent arguments identify the columns, or combination of columns, that define row membership to groups. As we’ll see, some verb functions behave differently when passed grouped data. Let’s return to the non-stylized data created in Section 6.2. The code below groups by , assigns the result to , then prints . The second printed line, , tells us this is a grouped tibble, groups are defined by values in the column, and there are 3 groups. Multiple columns can be used to specify the grouping structure. The code below groups first by then with the result being 5 groups (i.e., 5 groups because every row in has a unique combination of and values). Additional grouping variables can be added to an already grouped tibble by setting the argument to . For example, is equivalent to . If you want to know the grouping columns, but don’t want all the other information provided by or , call on the grouped tibble. See for other functions used to extract group information. We often want to remove or change a tibble’s grouping structure. Use to remove grouping information, i.e., make grouped data ungrouped. For example, remove all group information with , or remove individual grouping columns by passing them as subsequent arguments as illustrated below. So how do functions behave given a grouped tibble? We’ll cover the various functions in order of their initial presentation and, hence, start with functions that operate on tibble rows (i.e., , and ). If is passed a grouped tibble and the filtering condition uses a summary function or group characteristic (e.g., number of rows computed using the , see Section 7.10), the result will be group specific. For example, the code below selects each tree’s largest DBH measurement row. Here, recognizes is grouped by and hence applies the summary function and condition separately using each tree’s values. Given there are three groups, the result comprises three rows each holding a group-specific result. Figure 7.11 is a stylized take on the code and output above, and useful for highlighting how is applied separately to each group of rows. (ref:filter-group-fig) Using grouped to subset rows with the maximum by tree from . (ref:filter-group-fig) Using groupedto subset rows with the maximumby treefrom Next, let’s consider a grouped that uses a group characteristic in the filtering condition. Say we want all trees that only have one measurement. Recall, each row in represents a measurement and the function returns the number of rows, so applied to the grouped will return the number of measurements for each tree. This logic is used in the code below to return our desired result. (trees_by_id, () ) # Trees with only one measurement. Similarly, and its various flavors , , , , and subset rows within each group. For example, the code below, returns the row with the minimum year for each tree. Passing grouped and ungrouped data to gives the same result, unless you set the function’s optional argument to , in which case it will order first by the grouping. Recall, the functions , , , , and operate on tibble columns. Because and only affect the column name and position, their behavior is the same given grouped or ungrouped data. Similarly, a grouped is the same as an ungrouped , except grouping columns are always included in the resulting column subset. Similar to a grouped , a grouped , or grouped , has different behavior if your new column definition uses a summary function or some other group specific characteristic. For example, the code below adds a new column to called that holds the maximum DBH recorded for each tree. Notice the call to above uses the grouped tibble, hence the summary function returns the maximum DBH found among each tree’s measurements and recycles that value to match the number of rows in the given group. Breaking this down a bit more, for tree 1 the maximum is 2.1, i.e., , hence 2.1 is repeated for the first two rows of . The values for 2 and 3 follow the same logic. Let’s consider a slightly more involved example. Say we want to add an annual DBH increment ( ) column to . Given is the tree’s diameter at the end of a growing season, is the difference between a tree’s current and previous year DBH measurements, computed as follows. Because we pass the grouped to above, the (see ) function is applied to each tree’s vector. Specifically, for tree 1, the first two values in the column equal . If it’s not immediately apparent how works or why s appear in the column, spend a little time reading the manual page and work some simple examples, e.g., run . As mentioned at the end of Section 7.8, is most useful when applied to grouped data. As illustrated in Figure 7.12, given a grouped tibble, column summaries defined in are applied separately to each group of rows. It’s useful to compare Figures 7.12 and 7.10, and note the only difference is the code in Figure 7.12 is passed the grouped tibble. Again, it might be instructive to break down the steps leading to the output in Figure 7.12. For example, the mean and standard deviation values for tree 1 are and , respectively. Check your understanding about the grouped and computing the standard deviation by figuring out why the appears in the last row of . (ref:summarize-group-fig) Use to compute tree specific mean and standard deviation of . (ref:summarize-group-fig) Useto compute tree specific mean and standard deviation of Each time you apply to a grouped tibble, the default behavior is to remove the last grouping level (i.e., the right most group specified by and shown in , or ). The optional argument , which is by default set to , controls if and how the output tibble is grouped. In most cases this default behavior is sensible, because the output of is a summary of rows defined by the last grouping variable. However, when there are multiple grouping columns, you might want the output of to be ungrouped, which is accomplished by overriding the default behavior using or by passing the grouped output to the function. We’ll return to these very important points in Section 7.13 and Chapter 12.5 when working with tibbles grouped by multiple columns. A key strength is its ability to work with grouped data. For example, the grouped is so handy that it appears in nearly all subsequent estimation problems in Chapter 12 and beyond.\n\nThere’s often a need to apply one or more functions to a set of columns in a tibble, which is accomplished using . has two primary arguments. The first, , uses syntax (see Section 7.4) to select columns to which a function(s) will be applied. The second, , is a function or list of functions to apply to selected columns. While can be used in several functions, we use it most often in and . To motivate some examples, let’s add a tree height (ft) column to the tibble. Now, say we want to round and to whole numbers. One option is to apply the function to each column individually within , i.e., . This works well, but it could get tedious if there are more than a few columns to round. The alternative is to use within , which is applied in the code below. Say you want to apply a function to all columns of a certain data type. In such cases, the syntax function (see Section 7.4) is useful, e.g., . If you want to specify non-default arguments in a function passed to , or you’d like the columns defined in to be passed to something other than the first argument in a function, then you need to use an anonymous function. An anonymous function is just a function without a name. Continuing with the rounding example, say we want to round the and columns to one digit (the default of is ). This is accomplished using an anonymous function as follows. The anonymous function calls with set as a placeholder for and . A less verbose alternative to anonymous function syntax is to use the package’s lambda syntax. Following the example above, we can replace the anonymous function with equivalent lambda syntax where the tilde says we’re providing a function with set as a placeholder for columns given in . Let’s use within to compute the mean for and using base R’s function, first using an anonymous function then using the equivalent lambda syntax. Now, let’s compute the mean and standard deviation for and by passing a list of lambda syntax calls to the and functions to . In the output above, the numbers appended to column names, i.e., and , correspond to function list element indexes. We can explicitly label output columns by passing a function list with element names to as illustrated below. Use the argument in for finer control over column names, see for details. See the end of Section 12.5 for a more extensive within example. Also, there’s an vignette, accessed by running on the console, that provides extensive illustrations and details.\n\nYou might have noticed that is the first argument for all verb functions. This is an intentional design feature that, when combined with the package pipe operator , facilitates efficient and intuitive workflows. The pipe operator is available automatically when or other packages are loaded. The pipe operator places the object on its left into a function on its right. By default, the operator places the object in the function’s first argument. Here’s an initial non- example that pipes the vector into the function. The pipe operator implicitly adds the object to the function’s argument list (i.e., the object is added to the function’s argument list, but you just don’t see it). Additional function arguments follow the implicit argument. For example, let’s redefine to include an then add the optional argument to . To appreciate the beauty of using pipes with , consider the following example. Returning to the data, say we want to keep only tree 1 measurements, remove the column that was recently added in Section 7.11, and sort rows by in descending order. One possible approach to this three-step process is to create intermediate tibbles as illustrated below. If you plan to use these intermediate tibbles for something else and they can be meaningfully named, then perhaps this approach is a good one. However, if the intermediate tibbles are just a means to an end, then there are two good reasons this approach should be avoided. First, the environment and code becomes cluttered with unimportant objects, e.g., and . Second, if you’re like us, you’ll inevitably make an error incrementing tibble number suffixes or inputting the wrong tibble in a subsequent function. You could do away with the intermediate tibbles by simply overwriting a tibble name. This too is a less than desirable approach. It’s difficult to debug, because you’ll need to re-run all lines each time you make a change to the code. Also, it’s hard to follow how is modified by each line. Another approach is to nest the calls to , , and . The downside of this approach is that it’s challenging to write and even harder to read. Using pipes is the preferred solution to move the result of one verb function to the next. Notice, when written using pipes, the code reads like an action-packed sentence. In your mind, replace the pipe operator with “then” to say “take the tibble then filter then select then arrange.” Let’s try another piped operation using . Recall back in Section 7.9 we selected each tree’s first measurement year using , where is grouped by . Here’s that same query performed using pipes. Again, read it like a sentence, “take then group it by then slice.” We’ll make extensive use of pipes throughout this book, as we find them to be intuitive and a simple way to make code more readable. The pipe operator has several features beyond its default behavior. For example, as illustrated in the code below, use the dot symbol to indicate a specific location in the function argument list to place the object (think of the dot as the object’s placeholder). Run on the console to learn more about pipe functionality.\n\nThis section combines verb and helper functions in piped workflows to answer a series of questions about the Minnesota tree growth dataset, which was introduced in Section 6.1. In most cases, we pose a question about the data, then offer code to answer the question. Use this section to build understanding by methodically picking apart our code and referencing preceding sections, function manual pages, and other resources. Before proceeding, it’s important to fully understand how tree measurements are organized in . Recall, from Section 6.1, a tree’s set of annual measurements are uniquely identified by the combination of their , , and values (i.e., tree numbers are unique within plot, and plot numbers are unique within stand). For example, in Figure 6.1, the annual and measurements that define a given tree’s line share common , , and values. Say \\(y\\) is some variable (i.e., column) in . For example, \\(y\\) may be , , , , or . Then, using subscript notation that we will introduce formally in Section 10.2.1, a measurement for \\(y\\) (i.e., row value for column \\(y\\)) is indexed using \\(y_{ijkl}\\) for (\\(i\\)), (\\(j\\)), (\\(k\\)), and (\\(l\\)). Given the data comprise three plots within each of five stands, index values are: \\[\\begin{align*} i &= (1, 2, \\ldots, 5),\\\\ j &= (1, 2, 3),\\\\ k &= (1,2,\\ldots, n_{ij}), \\text{and}\\\\ l &= \\left(\\min(\\text{years}_{ijk}), \\min(\\text{years}_{ijk})+1, \\ldots, \\max(\\text{years}_{ijk})\\right), \\end{align*}\\] where there are \\(n_{ij}\\) trees in the \\(j\\)-th plot in the \\(i\\)-th stand, and \\(\\text{years}_{ijk}\\) is the set of years measured for the \\(k\\)-th tree in the \\(j\\)-th plot in the \\(i\\)-th stand. Take a scroll through the rows to confirm this index notation makes sense (e.g., run to print all rows). It might also be instructive to see all combinations of the indexes using the code below (again, add to the line below if you want to see all rows). returns distinct, i.e., unique, row value combinations given one or more columns. The output from tells us that for 1 (\\(i = 1\\)) and 1 (\\(j = 1\\)) there are \\(n_{ij} = 7\\) trees measured. How many total tree measurements were taken? How many individual trees were measured?. How many plots within each stand? How many trees within each plot (note, this is the \\(k\\) index’s \\(n_{ij}\\))? #> `summarise()` has grouped output by 'stand_id'. You #> can override using the `.groups` argument. The query above requires grouping by both and to count the number of distinct values for each plot. Recall from Section 7.9, the output from a grouped is a tibble with the last grouping level removed. Hence, in this case, the output is grouped by only . This makes sense because each row in the output is the number of distinct values for each . The message printed directly following the call to and prior to the tibble values (i.e., “ has grouped output by . You can override using the argument.”) is a friendly reminder the resulting tibble is still grouped. Also, as noted in Section 7.9 and as suggested in the printed message, you can change this default grouping behavior using the argument (see for specifics). It’s often useful to add the grouping argument to (even if it’s the default argument value, i.e., ) because it reinforces your awareness and, doing so, suppresses the message printed when the output is grouped. We’ll add the argument to all subsequent calls to . How many years were measured for each tree? Also, what was the minimum and maximum measurement year for each tree (note, these are the \\(l\\) index’s \\(\\min(\\text{years}_{ijk})\\) and \\(\\max(\\text{years}_{ijk})\\))? As described toward the end of Section 7.9, setting in removes and grouping from the output tibble. Alternatively, we can remove any unneeded or unwanted grouping by piping output to . How many trees are within each stand? Given trees have unique values within , and is unique within , some care is needed when crafting code to answer this question. Let’s begin by making a mistake and group only by and count distinct . The above is wrong because there will be some values that are common among the three plots within each stand (e.g., 1 occurs on all three plots but it will only be counted once). We get the correct tree count within each stand using the code below, which first counts the trees on each plot, then sums these counts within each stand. Importantly, after the first , is grouped by only and hence ready for the second . By species, how many trees were measured and how many measurements were taken? Sort the result by the number of trees measured. The first line in the output above says there’s only one Populus tremuloides ( ) in . Let’s double check this is correct. Visual inspection of the raw data is often an easy way to check results. We’ll limit the columns viewed to the first five. Given the output above has 71 rows and the same , and for each row (which you can see in its entirety by adding after ), we can be confident there is only one Populus tremuloides tree in the dataset. The code below is another approach to confirm there’s only one Populus tremuloides recorded. Use this same approach to confirm there are 2 Thuja occidentalis ( ) in . The next series of questions look at each individual tree’s chronology (i.e., measurements on a given tree). Which tree has the longest chronology and what’s its age and species? The output above tells us three trees share the longest chronology of 111 years. Two trees are 119 years old and the third is 125 years old. All three trees are Betula papyrifera. Notice in the above we used the helper function, which returns the first value in a vector (there is also a and helper function, see ). Because the value is the same for each trees’ measurements, it doesn’t matter which element in is selected, i.e., the first, last, or \\(n\\)-th value will be the same. Try running the code above without (i.e., just set in ), you’ll see a lot more rows are returned and a lot of repeated values in the first five columns. This is because returns all the column values and duplicates the other columns’ values to get the number of rows to match that of (this is not what you want). Accidentally creating a vector in a calculation is a common error, which is often diagnosed when you end up with more output rows than anticipated. In the code above, it’s important to first ungroup the output (via either the argument or piping through ), otherwise is applied to the grouped data (i.e., grouped by and ) and hence you get the tree with the longest chronology within each plot (which, again, is not what you want). Which tree has the largest annual growth increment ( ) and what’s its expressed as annual DBH increment (cm)? () # So that all values can be seen in the output. Given each tree’s basal area over time ( ), how do we compute an annual basal area increment? Recall, from Section 6.1, is the annual radial growth increment (i.e., annual growth by the end of growing season). Similarly, is the end of growing season DBH inside bark. In Section 7.6, we used to create an end of growing season inside bark basal area column ( ). The code below computes the desired basal area increment column ( ) by subtracting current from previous year basal area. A few things to notice in the code above. First, there is no need to group because calculations only involve values within a given row. Second, creating temporary columns improves readability and facilitates debugging: end of the previous year’s DBH ( ) and its associated basal area ( ). Third, the desired column ( ) is the difference between the current and previous year basal area. Finally, we removed our temporary columns using , prior to assigning the result back to . Tidyverse workflows, e.g., piped expressions seen above, are powerful data manipulation and analysis tools. When using such tools for our work, it’s important to keep the following sage words present in our mind. To ensure our work is accurate and delivered efficiently, it’s our responsibility to become proficient with the tools of our given profession. As professionals, we conduct analyses for clients or publicly funded research projects, and hence have an obligation to deliver high-quality analysis and results. A firm grasp on quantitative fundamentals and software tools of our trade is critical, but what really ensures consistent high-quality output is methodical and obsessive checking, and rechecking, code and results. As you’ve already seen, it’s easy to generate output using R and tools like , and sometimes it’s even easy to generate output that’s not accompanied by warnings or errors. Especially for the latter case, it’s critical there is a mind (your mind) checking the output is sensible and correct. Consider the following example. Say the first step in your analysis is to compute the total tree-ring width for each value given in the tibble below (i.e., each tree’s inside bark radius). Seems simple enough. We can use a grouped and even take care of those pesky values using the function’s argument. Looks good, no errors, and numbers came out. Next, we’ll extend this code to compute the mean over the two trees, using an additional . Okay, mean tree radius (inside bark) for the two trees is 0.15! Unfortunately, despite the fact the code is perfectly correct and there were clearly no warnings or errors, the answer is completely wrong. Perhaps you spotted the impending issue after the first bit of code yielded for a of and not the expected , after all, the sum of values is (try it, ). However, again, the code is correct. So what happened? The sum of an empty set of numbers is zero (try it, ). When all values passed to are and then all values are removed resulting in an empty set, hence the result of (i.e., ). This is subtle, but is a good example of the type of errors to be on the lookout for. Not all errors are coding errors and it’s important to stay constantly mindful and test your intuition against what the software produces. Okay, so what’s a fix to this specific example? One approach is to write a new sum function that returns for an empty set, but that might cause other code to break. Perhaps the simplest fix is to avoid calling for empty sets, which is done in the following revised code with the help of that returns if all values are and the previously used summation code, otherwise.\n\nMost analyses use variables and associated observations held in a single tibble. This tibble, however, is typically constructed from multiple data sources. Hence, a common pre-analysis task is to combine, or join, information from various sources into a single tibble. Fortunately, as part of its “grammar of data manipulation,” provides functions to simplify this task. We’ll focus initially on two suites of function to join tibbles.\n• None Mutating joins: combine the columns of tibbles and . A mutating join adds new columns to from matching rows of .\n• None Filtering joins: matches rows of tibbles and in the same way as mutating joins, but affects rows of , not columns. Notice how the term “filter” applies to manipulating rows and “mutate” applies to adding columns, which lines up with our previous experience with ’s and functions. In the next two sections, we introduce functions for mutating and filtering joins. In addition to each function’s description, we provide an example using toy data provided in Figure 7.13. After covering mutating and filtering joins, we’ll introduce set operations and a few other functions for combining information in tibbles. Species codes like those in Table 6.1 are commonly used in the field when recording species occurrence. When field measurements are encoded in data files and then analysed, results are presented using either common and/or scientific names. This means, at some point, species codes need to be replaced with species names. Typically the intended audience determines if common or scientific names are used in a report (e.g., peer reviewed publications use scientific names and inventory summaries given to landowners use common names). As a result, we often find ourselves joining species codes, common names, and scientific names, each of which might reside in a different tibble. This is the motivation behind the toy data in Figure 7.13. If you’d like to follow along with the examples, code to create Figure 7.13 tibbles and is given below. For all join functions, the first two arguments, and respectively, define the tibbles to combine. All joins also return a new tibble with the same type as . This consistent function input and output facilitates writing efficient piped workflows using all functions. In addition to and arguments, join functions take an argument that controls which columns are used to match row values across the two tibbles. If the argument is not specified, then all columns with names common to and are used (this is referred to as a natural join). If you want to use only a subset of common column names, then set equal to a character vector of those names. For example, using the tibbles in Figure 7.13, to join and by , set . To join by different column names in and , use a named vector to match columns. For example, say in was named , then will match rows in to . After presenting the various mutating and filtering joins below, we’ll work through some applied examples. Mutating joins add columns from to based on row matching. If a row in matches multiple rows in , the multiple matching rows in will be included in the output. There are four mutating joins.\n• includes all rows in , regardless of whether they match values in . If a given row in does not have a match in , an value is assigned to that row in the new columns. Figure 7.14 illustrates the left join using data from Figure 7.13. Here, and in subsequent examples using these data, we’re joining by the column. Because is not in the resulting tibble has a for the new column and row.\n• includes all rows in . It’s equivalent to , but the columns and rows will be ordered differently. Figure 7.15 illustrates the right join using data from Figure 7.13. Because is in but not , the resulting tibble has an for the new column and row.\n• includes only rows that match in and . For data given in Figure 7.13, and are the only common values in and , hence their inner join results in Figure 7.16.\n• includes all rows from and . Figure 7.17 illustrates the full join using data from Figure 7.13, notice rows for all non-matching values receive a value. See for more details, optional arguments, and examples. Filtering joins filter rows from based on the presence or absence of row matches in . Filtering joins match rows in the same way as mutating joins, but the result is a row subset of (i.e., same columns but, possibly, fewer rows of ). Filtering joins are used most often to diagnose join mismatches that might cause unwanted or unexpected results when performing mutating joins. There are two filtering joins.\n• keeps all rows in that have a match in . For data given in Figure 7.13, and are the only common values in and , hence their semi join results in Figure 7.18.\n• removes all rows in that have a match in . For data given in Figure 7.13, is the only row in that does not occur in , hence the anti join result in Figure 7.19. See for more details, optional arguments, and examples. Tree measurements for a typical timber cruise include species, DBH, number of logs, and log/stem quality. While height might be measured for a subset of trees, we rarely collect it for all measurement trees. Stem volume is typically an important variable that comes from a timber cruise; however, we don’t measure stem volume directly on a tree. Stem volume and similar quantities such as biomass are computed using allometric equations (Chapter 5). Allometric equations are regression equations that relate measurements like species, DBH, and perhaps height to more difficult and expensive to measure quantities such as stem volume or biomass. Data to inform these allometric models is expensive to collect because trees must be harvested and meticulously partitioned to facilitate accurate dimension and weight measurements (see, e.g., Section 1.2.1). These efforts result in published allometric equations that take the inexpensive measurements as input (e.g., species and DBH) and return estimates of the expensive quantity (e.g., height, volume, biomass). In this section we’ll work with species specific height models (e.g., given tree species and DBH, the model returns tree height) and volume models (e.g., given tree species, DBH, and height, the model returns tree volume). It’s not necessary to understand model formulation and estimation for this section. Let’s begin by looking at a model used by the USDA Forest Service to estimate tree height (ft) given DBH (in) and species. The model, published in FVS (2021) Equation 4.1.2, is \\[\\begin{equation} \\text{height} = 4.5 + \\exp\\left(\\beta_1 + \\frac{\\beta_2}{\\left(\\text{DBH} + 1.0\\right)} \\right), \\tag{7.1} \\end{equation}\\] where regression coefficient values for \\(\\beta_1\\) and \\(\\beta_2\\) are provided in Table 4.1.2 of FVS (2021) for trees in the northeastern US. For convenience, we copied the coefficients from FVS (2021) to “datasets/FVS_NE_coefficients.csv”, which is read in below and named . #> Rows: 108 Columns: 3 #> ── Column specification ────────────────────────────────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (1): USFS_FVS_code #> dbl (2): beta_1, beta_2 #> #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Each row in holds species specific ( ) regression coefficient values for \\(\\beta_1\\) ( ) and \\(\\beta_2\\) ( ) used to implement (7.1). For example, say you have a 10 (in) DBH balsam fir (Abies balsamea) and want to estimate its height (ft). The for balsam fir is and associated coefficients are \\(\\beta_1\\) = 4.5084 and \\(\\beta_2\\) = -6.0116. The code below applies (7.1) using the given DBH, \\(\\beta_1\\), and \\(\\beta_2\\) values to produce the desired height estimate. Now, say we want to use (7.1) to estimate the height for all trees measured in some cruise data where only DBH and species were collected. For illustration, we use a version of the two stand plot data introduced later in Section ?? and shown in Figure 12.7. As seen below, these data provide a stand and plot identification value and overstory tree DBH and species codes. #> Rows: 15 Columns: 4 #> ── Column specification ────────────────────────────────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (1): scientific_name #> dbl (3): stand_id, plot_id, DBH_in #> #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. We begin by joining coefficient columns in to by species. Once that’s complete, we’ll use to add a new column with values defined by (7.1) and column values , , . There’s one mild technical issue though—notice the in are two letter species codes and in are scientific names. So before we can join to by species, we need to get a common species identification column. Fortunately, this isn’t the first time we’ve encountered this issue. Columns in the “datasets/USFS_species_codes.csv” file, read in below, allow crosswalk between species common name, scientific name, and three common USFS code types. #> Rows: 108 Columns: 5 #> ── Column specification ────────────────────────────────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (4): common_name, scientific_name, USFS_FVS_cod... #> dbl (1): USFS_FIA_code #> #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. The column is common to the and tibbles. Also, the column is common to the and tibbles. So, joining the column from to by will let us subsequently join to by . The first step in this sequence of joins is given below. # Remove the columns we don't need. # Check that all species have a scientific_name match. ( (stands USFS_FVS_code)) # No NAs mean all matched (that's good)! Let’s step through the code above before looking at the resulting . adds columns from to by matching to . After the , all columns are added to . The call to removes unneeded columns (i.e., we only need ). The final line of code checks that there are no values in . This is an important step, because presence means not all the species in were in , which would present a problem as we move forward. Fortunately, no values were found, which means all rows in now have a valid value. An alternative, and perhaps better, approach to doing this check via after doing the is to do it before the using an . Recall, removes all rows in that have a match in . So, if the below returns a tibble with no rows, we know all species in have a match in . We’ll use anti joins for join diagnostics in subsequent code. # Alternative way to check all species had a scientific_name match. Okay, let’s take a look at our work. The output from above shows that now has the column and, hence, we’re ready to join it with . However, before proceeding with the left join of and by , we first use an anti join to check that all values are in . # Check that all USFS_FVS_code species in stands are in ht_coeffs. The empty tibble returned by the anti join above tells us that all species in have a match in and we can proceed with the left join to attach the regression coefficients in to . The output from above shows that and are now added to and we can proceed with adding the new tree height column ( ) to via . # Remove the now unnecessary coefficients and There we go! We now have an estimate of tree height in our tibble. Next, let’s add stem volume estimates to our tree data. We’ll follow the same process as adding height estimates, which starts with finding a suitable allometric model in the literature. Given we have species, DBH (in), and now height (ft), we can look for models that take these variables as input and return volume (m\\(^3\\)). A suitable model for this setting is given in Honer (1967) and defined as \\[\\begin{equation} \\text{volume} = \\frac{\\text{DBH}^2}{\\alpha_1 + \\frac{\\alpha_2}{\\text{height}}}. \\tag{7.2} \\end{equation}\\] We copied species specific regression coefficient values for \\(\\alpha_1\\) and \\(\\alpha_2\\) from Honer (1967) to “datasets/Honer_coefficients.csv”, which is read in below and named . #> Rows: 34 Columns: 3 #> ── Column specification ────────────────────────────────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (1): USFS_FVS_code #> dbl (2): alpha_1, alpha_2 #> #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Columns , , and , in hold values for species, \\(\\alpha_1\\), and \\(\\alpha_2\\), respectively. Because and use the same species codes (i.e., ) we can continue directly with checking that all species in are present in via the anti join below. The empty anti join tibble above confirms all species are present and we can proceed with the left join, compute tree volumes using (7.2) within , remove subsequently unneeded coefficient columns, and take a look at our new tree and species specific column. As mentioned previously, applying allometric equations to estimate tree characteristics is a common task, and one greatly simplified using a join and verb function workflow. We often want to manipulate rows in two tibbles that have the same columns. For example, we might want to identify rows with identical values in two tibbles. Or perhaps combine two tibbles, but keep only one instance of duplicate rows. While such tasks could be accomplished using joins, provides functions that treat rows like sets (i.e., in the mathematical sense). Like joins, these functions accept two tibbles, and , and provide the following set operations.\n• : return unique rows in that are not in .\n• : return all rows (even duplicates) in or . Let’s try these functions out using the following example tibbles. Notice rows one and two in are duplicate (i.e., their tree and are the same). Also, the fourth row of is the same as the first row of . The result from identifies both and have a row with and in common. The call to shows the two unique rows in that are not in . Because set functions, with the exception of , return unique instances of rows, only one instance of ’s and row is returned. The result of is all rows in and , except the duplicates for rows with and . Finally, is identical to , but its result includes the duplicates. #> # A tibble: 7 × 2 #> id value #> <dbl> <chr> #> 1 1 a #> 2 1 a #> 3 2 b #> 4 3 c #> 5 3 c #> 6 4 d #> 7 5 e Aside from joins and set operations that follow row matching rules, we frequently want simply to bind together two or more tibbles either by rows or columns. These operations are done using the and functions. Both functions take two or more tibbles to bind as separate arguments (or in a single list). The function has an optional argument that, if specified, will add a new column with values that link each row to the original tibbles. See for additional details about both functions. We’ll illustrate the bind functions using the tibble created in Section 6.2, which recall looks like the following. Say we collected core data for a few more trees and recorded it in the tibble below. We can combine and into a single tibble as shown below. The argument is added below, so we can keep track of the original tibbles. Say we also have species for . This information could be added when was formed, or afterwards using as follows. If column names don’t match, then fills in missing values with . For example, the column does not exist in but does in , hence the result below.\n\nFigure 7.20 provides timber cruise data collected over three inventory plots and recorded in a field notebook. Rows in the notebook correspond to measurement trees and columns are defined as follows.\n• plot number on which the tree was measured. Data provided in Figure 7.20 are used for Exercises 7.1 through 7.24. For most exercises, we provide the desired output and it’s your job to write the code to reproduce the output. Use the pipe operator when possible. Exercise 7.1 Create a tibble called that holds the cruise data provided in Figure 7.20. To do this, copy the data into a text file (e.g., separate columns using comma, space, or tab delimiters) or enter it into a spreadsheet then export it as a text file with approprite column delimiters. Then read the file into R using a function, see Section 6.3. Once complete, your tibble should match the output below. Hint, to get your column data types to match those shown below, use the argument in your chosen read function. Exercise 7.2 Given your output from Exercise 7.1 answer the following questions about (place your answer to each question behind a comment in your code).\n• How many rows and columns?\n• What are the column names?\n• What is the data type of each column?\n• Are there any values? If so, in which columns and which rows? Exercise 7.4 Print rows with trees that do not have for DBH. Exercise 7.5 Print the row with the largest DBH. Exercise 7.6 Print rows with species code “SS” and DBH greater than or equal to 40 cm. Exercise 7.7 Print rows that: 1) do not have species code “SS”; 2) are live; 3) have DBH greater than 50 cm. Exercise 7.8 Change column names and to and . Be sure to assign your result back to to make the change perminant. Exercise 7.9 Use to add a new column called that holds each tree’s basal area in square meters. Place this new column to the right of . Given DBH in centimeters, basal area in square meters is \\[\\begin{equation} \\frac{\\pi}{10000}\\cdot\\left(\\frac{\\text{DBH}}{2}\\right)^2. \\end{equation}\\] Your new tibble should look like the following. Exercise 7.10 Use the function to add a new column called . Values in should be when equals and when equals . Hint, consider using the or function within as described in Section 7.6. Exercise 7.11 Use the function to remove the column. Exercise 7.12 How many trees (i.e., rows) in each plot? Call your count variable . Hint, use in your piped expression. Exercise 7.13 How many distinct species across all plots? Call your count variable . Exercise 7.14 How many distinct species within each plot? Call your count variable . Exercise 7.15 What are the distinct species codes across all plots? Exercise 7.16 What are the distinct species codes within each plot? Exercise 7.17 Print rows with the largest DBH on each plot. Exercise 7.18 What is the total BA on each plot? Call this total . Hint, use in your piped expression that also involves calling the function in . Exercise 7.19 What is the total BA on each plot and by species? Call this total . Hint, this should require modification of your 7.18 code’s function and consultation with Section 7.13.1. #> `summarise()` has grouped output by 'Plot'. You can #> override using the `.groups` argument. Exercise 7.20 Next, let’s add stem volume estimates using the tree measurements. These data were collected in costal British Columbia, Canada. Given we have species, DBH (cm), and total height (m), we can look for models that take these variables as input and return volume (ft\\(^3\\)). A suitable model for this setting is defined as \\[\\begin{equation} \\text{volume} = \\exp(\\beta_0) + \\text{DBH}^{\\beta_1} + \\text{Ht}^{b_2}, \\tag{7.3} \\end{equation}\\] where \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\) regression coefficient estimates for merchantable volume are provided in Table 6 of Nigh (2016). For convenience, we copied these coefficients from Nigh (2016) to “datasets/Nigh_coefficients.csv”. Add a new column to called that holds merchantable volume for each tree and do some clean up when you’re done. Write the code to replicate the following steps.\n• Make a coefficients tibble by reading in the species specific regression coefficients held in “Nigh_coefficients.csv”. Hint, open “Nigh_coefficients.csv” in a text editor to see what it contains and how the columns are delimited.\n• Add these regression coefficients as new columns to via a join by column and coefficients tibble .\n• Specify (7.3) within to add the desired column. Place to the right of . Your resulting should look like the following. Exercise 7.21 What is the total for each plot? Call your summarized variable . Exercise 7.22 The area of each plot on which trees were measured is 0.04 ha. So your result from Exercise ?? represents tree volume on each plot per 0.04 ha. Scale your result from Exercise ?? to represent tree volume on each plot per ha. Call your summarized variable . Hint, use the area expansion factor, introduced in Section XX, within your Exercise ?? code. Exercise 7.23 What is the mean value across the three plots’ computed in Exercise 7.22? Call your summarized variable . Hint, pipe the result of your Exercise 7.22 into another . As noted briefly in Section XX and covered in greater detail in Section XX, this mean is our best estimate for the forest population volume per ha—assuming the distribution of volume on the three plots represent well the distribution of volume in the population. Exercise 7.24 Say the total forest area was 25 ha. Modify your code from Exercise 7.23 to compute the total forest volume, in addition to . Call your resulting total forest volume variable . Hint, compute these two desired quantities within a single call to ."
    },
    {
        "link": "http://wvview.org/ds/r/05_Summarization_Stats.html",
        "document": "There are many packages available in R that provide functions to summarize data. Here, I will primarily demonstrate dplyr, which we explored in the data manipulation module, and psych. I tend to use dplyr for data manipulation and summarization tasks. However, pysch is also very useful as it allows you to obtain a variety of summary statistics quickly. First, you will need to read in the required packages. If you haven’t installed them, you will need to do so. Also, you will need to load in some example data. We will work with the high_plains_data.csv file. The elevation (“elev”), temperature (“temp”), and precipitation (“precip2”) data were extracted from digital map data provided by the PRISM Climate Group at Oregon State University. The elevation data are provided in meters, the temperature data represent 30-year annual normal mean temperature in Celsius, and the precipitation data represent 30-year annual normal precipitation in millimeters. The original digital map data have a resolution of 4-by-4 kilometers and can be obtained here. I also summarized percent forest (“per_for”) from the 2011 National Land Cover Database (NLCD). NLCD data can be obtained from the Multi-Resolution Land Characteristics Consortium (MRLC). The link at the bottom of the page provides the example data and R Markdown file used to generate this module. I do not need all of the columns to work through the examples. So, I am using dplyr to select out only the required variables into a new data frame. Once the required columns have been extracted, I use the summary() function from base R to obtain summary information for each column. The minimum, 1st quartile, median, mean, 3rd quartile, and maximum values are returned for continuous variables. For factors, the factor levels and associated counts are returned. Calling levels() on a factor column will print the list of factor levels. This data set contains county-level data across eight states. Generally, I find that it is a good idea to use summary() to inspect data prior to performing any analyses. Mean Mean Mean Mean The base R head() and tail() functions can be used to print the first five or last five rows of data, respectively. If you would like to print a different number of rows, you can use the optional n argument. Similar to summary(), these functions are useful for inspecting data prior to an analysis. The describe() function from the psych package can be used to obtain a variety of summary statistics for all or individual columns of data. By default it will return the mean, standard deviation, median, trimmed mean, median absolute deviation, minimum, maximum, range, skewness, kurtosis, and standard error for continuous data. To obtain individual statistics, you can use the summarize() function from dplyr. In the examples below, I am obtaining the mean, standard deviation, and median for the elevation data. The result can be saved to a variable for later use. The base R hist() function can be used to generate a histogram to visualize the distribution of a single continuous variable. In a later module, you will learn to make more refined histograms using the ggplot2 package; however, this base R function can be useful for simple data exploration. As opposed to obtaining global summary statistics, you may be interested in summarizing data by group for comparison. The describeBy() function from the psych package expands upon describe() to allow for grouping. In the example below, I have obtained summary statistics by state. By default the result will be returned as a list object; however, you can set the mat argument to TRUE to obtain the output as a data frame. Alternatively, you can use the dplyr group_by() function to obtain summary statistics by group. In the example below I am piping the data frame into group_by() followed by summarize() to obtain the mean county percent forest by state. If you would like to calculate a summary statistic for more than one column at once, you can use the summarize_all() function as opposed to summarize() as demonstrated in the second code block.\n\nWe will now explore some common statistical tests. Since this is not a statistics course, I will not demonstrate complex tests; however, if you need to conduct more complex tests than those demonstrated here, I think you will find that the syntax will be similar to the examples provided. The base R cor() function allows for the calculation of correlation between pairs of continuous variables. The following methods are available:\n• Pearson Correlation Coefficient: parametric and assesses the linear correlation between two variables (similar to R-squared)\n• Kendal Rank Correlation Coefficient: nonparametric measure of monotonic correlation based on ranks and reported as probabilities Parametric tests have assumptions relating to data distribution. For example, the data may be assumed to be normally distributed. If input data do not meet the distribution requirements, the tests may be invalid or misleading. In contrast, nonparametric tests do not have distribution assumptions and often rely on the ranks or order of data point values as opposed to their actual value or magnitude. They can serve as a more appropriate test when distribution assumptions are violated. In the example below, I have compared the correlation between population density, percent forest cover, temperature, elevation, and precipitation using all three methods. Large positive values indicate direct or positive correlation between the two variables while negative values indicate indirect or inverse correlation between the two variables. You may have noticed that the diagonal values are 1 for all methods. This is because a variable is perfectly correlated with itself. If you would like to visualize or graph variable correlations, I highly recommend the corrplot package. A T-test is used to statistically assess whether the means of two groups are different. The null hypothesis is that the means are not different, and the alternative hypothesis is that they are different. In the example, I first use dplyr to subset out only counties in Utah or North Dakota since we can only compare two groups with a T-test. Since statistical inference is used to infer something about a population from a sampled drawn from that population, I am also randomly sampling 15 counties from each of the two states. I also set a random seed so that the random sampling is reproducible. I then use the base R t.test() function to perform the test. Specifically, I assess whether the mean annual county temperatures are different for the two states. I provide a formula and a data set. Once the test executes, the result will be printed to the console. A p-value of 0.000227 is obtained, which is much lower than 0.05. This indicates to reject the null hypothesis in favor of the alternative hypothesis that the average of mean annual temperature by county in these two states is different. alternative hypothesis true difference means is not equal to alternative hypothesistrue differencemeans is not equal to A T-test is a parametric test. As discussed above, this means that it has some assumptions. Specifically, the T-test assumes that the two groups are independent of each other, samples within groups are independent, the dependent variable is normally distributed, and the two groups have similar variance. It is likely that at least some of these assumptions are violated. For example, due to spatial autocorrelation, it is unlikely that samples are independent. The Mann-Whitney U Test offers a nonparametric alternative to the T-test with relaxed distribution assumptions. However, it is still assumed that the samples are independent. The example below provides the result for the comparison of Utah and North Dakota. A statistically significant p-value is obtained, again suggesting a rejection of the null hypothesis in favor of the alternative hypothesis that the median of mean annual temperature by county is different between the two states. alternative hypothesis true location shift is not equal to alternative hypothesistrue location shift is not equal to What if you would like to compare more than two groups? This can be accomplished using analysis of variance or ANOVA. There are many different types of analysis of variance that vary based on study design, such as One-Way ANOVA, Two-Way ANOVA, MANOVA, Factorial ANOVA, ANCOVA, and MANCOVA. Here, I will simply demonstrate One-Way ANOVA, which is used to compare the means of a single variable between groups. However, if you need to implement one of the more complex methods, you can use similar syntax. Similar to the example above, I am assessing whether the mean annual temperature by county is different between states. However, now I am assessing all states in the data set as oppose to only two. Again, I subset out 15 counties for each state to represent a sample from the larger population. The null hypothesis is that no pair of means are different while the alternative hypothesis is that at least one pair of means is different. The test results in a statistically significant p-value, suggesting to reject the null hypothesis in favor of the alternative hypothesis that at least one pair of states have statistically different mean annual temperature by county. However, the test does not tell us what pair or pairs are different. Following a statistically significant ANOVA result, it is common to use a pair-wise test to compare each pair combination. There are different methods available. Here, I am using the Tukey’s Honest Significant Difference method, made available by the TukeyHSD(), function to perform the comparisons. If the p-value is lower than 0.05 and the confidence interval does not include zero within its range, the states are suggested to be statistically significantly different at the 95% confidence level. Since ANOVA is a parametric method, it has assumptions, similar to the T-test. Specifically, ANOVA assumes that samples are independent, the data and model residuals (i.e., error terms) are normally distributed, and that variance is consistent between groups. A Q-Q plot can be used to visually assess data normality by comparing the quantiles of the data to theoretical quantiles, or the quantiles the data would have if they were normally distributed. Divergence from the line suggests that the data are not normally distributed. In the examples below I have created Q-Q plots using two different functions: qqnorm() and qqPlot(). The first plot represents the distribution of the temperature data while the second plot represents the distribution of the model residuals. Since the data points diverge from the line, this suggests that the data are not normally distributed and that an assumption of ANOVA is violated. The Bartlett Test of Homogeneity of Variance is used to assess whether variance is the same between groups. A statistically significant result, as obtained here, suggests that this assumption is violated. ANOVA is also sensitive to outliers. The Bonferroni Outlier Test is used to determine whether outliers are present. The results in this case do suggest the presence of outliers. Since many of the assumptions of ANOVA are violated, as highlighted by the analysis above, it would be good to assess difference between groups using a nonparametric alternative. The Kruskal-Wallis Rank Sum Test provides this alternative. Similar to ANOVA, a statistically significant result, as obtained here, suggests that at least one pair of groups are different. It does not tell you what pair or pairs. So, I use the pairw.kw() function from the asbio package to perform a nonparametric pair-wise comparison. A p-value lower than 0.05 and a confidence interval that does not include zero in its range suggest difference between the two groups.\n\nAgain, my goal here was to provide a demonstration of data summarization and statistical tests that can be performed in R. As this is not a statistics course, I did not explain the methods in detail or provide examples of more complex techniques. If you are interested in these topics, I would suggest taking a statistics course. If you need to perform more complex statistical analyses in R, you will find that they can be performed using similar syntax to the examples provided here. We will discuss more data summarization in later modules including data visualization and graphing with ggplot2. In regards to statistical methods, we will explore linear regression and machine learning later in the course."
    },
    {
        "link": "https://cran.r-project.org/web/packages/broom/vignettes/broom.html",
        "document": "The broom package takes the messy output of built-in functions in R, such as , , or , and turns them into tidy tibbles.\n\nThe concept of “tidy data”, as introduced by Hadley Wickham, offers a powerful framework for data manipulation and analysis. That paper makes a convincing statement of the problem this package tries to solve (emphasis mine):\n\nWhile model inputs usually require tidy inputs, such attention to detail doesn’t carry over to model outputs. Outputs such as predictions and estimated coefficients aren’t always tidy. This makes it more difficult to combine results from multiple models. For example, in R, the default representation of model coefficients is not tidy because it does not have an explicit variable that records the variable name for each estimate, they are instead recorded as row names. In R, row names must be unique, so combining coefficients from many models (e.g., from bootstrap resamples, or subgroups) requires workarounds to avoid losing important information. This knocks you out of the flow of analysis and makes it harder to combine the results from multiple models. I’m not currently aware of any packages that resolve this problem.\n\nbroom is an attempt to bridge the gap from untidy outputs of predictions and estimations to the tidy data we want to work with. It centers around three S3 methods, each of which take common objects produced by R statistical functions ( , , , etc) and convert them into a tibble. broom is particularly designed to work with Hadley’s dplyr package (see the broom+dplyr vignette for more).\n\nbroom should be distinguished from packages like reshape2 and tidyr, which rearrange and reshape data frames into different forms. Those packages perform critical tasks in tidy data analysis but focus on manipulating data frames in one specific format into another. In contrast, broom is designed to take format that is not in a tabular data format (sometimes not anywhere close) and convert it to a tidy tibble.\n\nTidying model outputs is not an exact science, and it’s based on a judgment of the kinds of values a data scientist typically wants out of a tidy analysis (for instance, estimates, test statistics, and p-values). You may lose some of the information in the original object that you wanted, or keep more information than you need. If you think the tidy output for a model should be changed, or if you’re missing a tidying function for an S3 class that you’d like, I strongly encourage you to open an issue or a pull request.\n\nThis package provides three S3 methods that do three distinct kinds of tidying.\n• : constructs a tibble that summarizes the model’s statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for functions.\n• : add columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments.\n• : construct a concise one-row summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire model. Note that some classes may have only one or two of these methods defined. Consider as an illustrative example a linear fit on the built-in dataset. ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 37.2851 1.8776 19.858 < 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 This summary output is useful enough if you just want to read it. However, converting it to tabular data that contains all the same information, so that you can combine it with other models or do further analysis, is not trivial. You have to do to get a matrix of coefficients, the terms are still stored in row names, and the column names are inconsistent with other packages (e.g. compared to ). Instead, you can use the function, from the broom package, on the fit: This gives you a tabular data representation. Note that the row names have been moved into a column called , and the column names are simple and consistent (and can be accessed using ). Instead of viewing the coefficients, you might be interested in the fitted values and residuals for each of the original points in the regression. For this, use , which augments the original data with information from the model: Note that each of the new columns begins with a (to avoid overwriting any of the original columns). Finally, several summary statistics are computed for the entire regression, such as R^2 and the F-statistic. These can be accessed with the function: This distinction between the , and functions is explored in a different context in the k-means vignette."
    },
    {
        "link": "https://broom.tidymodels.org/articles/broom.html",
        "document": "The broom package takes the messy output of built-in functions in R, such as , , or , and turns them into tidy tibbles.\n\nThe concept of “tidy data”, as introduced by Hadley Wickham, offers a powerful framework for data manipulation and analysis. That paper makes a convincing statement of the problem this package tries to solve (emphasis mine):\n\nWhile model inputs usually require tidy inputs, such attention to detail doesn’t carry over to model outputs. Outputs such as predictions and estimated coefficients aren’t always tidy. This makes it more difficult to combine results from multiple models. For example, in R, the default representation of model coefficients is not tidy because it does not have an explicit variable that records the variable name for each estimate, they are instead recorded as row names. In R, row names must be unique, so combining coefficients from many models (e.g., from bootstrap resamples, or subgroups) requires workarounds to avoid losing important information. This knocks you out of the flow of analysis and makes it harder to combine the results from multiple models. I’m not currently aware of any packages that resolve this problem.\n\nbroom is an attempt to bridge the gap from untidy outputs of predictions and estimations to the tidy data we want to work with. It centers around three S3 methods, each of which take common objects produced by R statistical functions ( , , , etc) and convert them into a tibble. broom is particularly designed to work with Hadley’s dplyr package (see the broom+dplyr vignette for more).\n\nbroom should be distinguished from packages like reshape2 and tidyr, which rearrange and reshape data frames into different forms. Those packages perform critical tasks in tidy data analysis but focus on manipulating data frames in one specific format into another. In contrast, broom is designed to take format that is not in a tabular data format (sometimes not anywhere close) and convert it to a tidy tibble.\n\nTidying model outputs is not an exact science, and it’s based on a judgment of the kinds of values a data scientist typically wants out of a tidy analysis (for instance, estimates, test statistics, and p-values). You may lose some of the information in the original object that you wanted, or keep more information than you need. If you think the tidy output for a model should be changed, or if you’re missing a tidying function for an S3 class that you’d like, I strongly encourage you to open an issue or a pull request.\n\nThis package provides three S3 methods that do three distinct kinds of tidying.\n• : constructs a tibble that summarizes the model’s statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for functions.\n• : add columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments.\n• : construct a concise one-row summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire model. Note that some classes may have only one or two of these methods defined. Consider as an illustrative example a linear fit on the built-in dataset. This summary output is useful enough if you just want to read it. However, converting it to tabular data that contains all the same information, so that you can combine it with other models or do further analysis, is not trivial. You have to do to get a matrix of coefficients, the terms are still stored in row names, and the column names are inconsistent with other packages (e.g. compared to ). Instead, you can use the function, from the broom package, on the fit: This gives you a tabular data representation. Note that the row names have been moved into a column called , and the column names are simple and consistent (and can be accessed using ). Instead of viewing the coefficients, you might be interested in the fitted values and residuals for each of the original points in the regression. For this, use , which augments the original data with information from the model: Note that each of the new columns begins with a (to avoid overwriting any of the original columns). Finally, several summary statistics are computed for the entire regression, such as R^2 and the F-statistic. These can be accessed with the function: This distinction between the , and functions is explored in a different context in the k-means vignette."
    },
    {
        "link": "https://rdocumentation.org/packages/broom/versions/0.4.2",
        "document": "The broom package takes the messy output of built-in functions in R, such as , , or , and turns them into tidy data frames.\n\nThe concept of \"tidy data\", as introduced by Hadley Wickham, offers a powerful framework for data manipulation and analysis. That paper makes a convincing statement of the problem this package tries to solve (emphasis mine):\n\nWhile model inputs usually require tidy inputs, such attention to detail doesn't carry over to model outputs. Outputs such as predictions and estimated coefficients aren't always tidy. This makes it more difficult to combine results from multiple models. For example, in R, the default representation of model coefficients is not tidy because it does not have an explicit variable that records the variable name for each estimate, they are instead recorded as row names. In R, row names must be unique, so combining coefficients from many models (e.g., from bootstrap resamples, or subgroups) requires workarounds to avoid losing important information. This knocks you out of the flow of analysis and makes it harder to combine the results from multiple models. I'm not currently aware of any packages that resolve this problem.\n\nbroom is an attempt to bridge the gap from untidy outputs of predictions and estimations to the tidy data we want to work with. It centers around three S3 methods, each of which take common objects produced by R statistical functions ( , , , etc) and convert them into a data frame. broom is particularly designed to work with Hadley's dplyr package (see the \"broom and dplyr\" vignette for more).\n\nbroom should be distinguished from packages like reshape2 and tidyr, which rearrange and reshape data frames into different forms. Those packages perform critical tasks in tidy data analysis but focus on manipulating data frames in one specific format into another. In contrast, broom is designed to take format that is not in a data frame (sometimes not anywhere close) and convert it to a tidy data frame.\n\nTidying model outputs is not an exact science, and it's based on a judgment of the kinds of values a data scientist typically wants out of a tidy analysis (for instance, estimates, test statistics, and p-values). You may lose some of the information in the original object that you wanted, or keep more information than you need. If you think the tidy output for a model should be changed, or if you're missing a tidying function for an S3 class that you'd like, I strongly encourage you to open an issue or a pull request.\n\nThe broom package is available on CRAN:\n\nYou can also install the development version of the broom package using devtools:\n\nFor additional documentation, please browse the vignettes:\n\nThis package provides three S3 methods that do three distinct kinds of tidying.\n• : constructs a data frame that summarizes the model's statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for functions.\n• : add columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments.\n• : construct a concise one-row summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire model.\n\nNote that some classes may have only one or two of these methods defined.\n\nConsider as an illustrative example a linear fit on the built-in dataset.\n\nThis summary output is useful enough if you just want to read it. However, converting it to a data frame that contains all the same information, so that you can combine it with other models or do further analysis, is not trivial. You have to do to get a matrix of coefficients, the terms are still stored in row names, and the column names are inconsistent with other packages (e.g. compared to ).\n\nInstead, you can use the function, from the broom package, on the fit:\n\nThis gives you a data.frame representation. Note that the row names have been moved into a column called , and the column names are simple and consistent (and can be accessed using ).\n\nInstead of viewing the coefficients, you might be interested in the fitted values and residuals for each of the original points in the regression. For this, use , which augments the original data with information from the model:\n\nNote that each of the new columns begins with a (to avoid overwriting any of the original columns).\n\nFinally, several summary statistics are computed for the entire regression, such as R^2 and the F-statistic. These can be accessed with the function:\n\nThis distinction between the , and functions is explored in a different context in the k-means vignette.\n\nThese functions apply equally well to the output from :\n\nNote that the statistics computed by are different for objects than for (e.g. deviance rather than R^2):\n\nThese functions also work on other fits, such as nonlinear models ( ):\n\nThe function can also be applied to objects, such as those output by popular built-in functions like , , and .\n\nSome cases might have fewer columns (for example, no confidence interval):\n\nSince the output is already only one row, returns the same output:\n\nThere is no function for objects, since there is no meaningful sense in which a hypothesis test produces output about each initial data point.\n\nCurrently broom provides tidying methods for many S3 objects from the built-in stats package, including\n\nIt also provides methods for S3 objects in popular third-party packages, including\n\nA full list of the , and methods available for each class is as follows:\n\nIn order to maintain consistency, we attempt to follow some conventions regarding the structure of returned data.\n• The output of the , and functions is always a data frame.\n• The output never has rownames. This ensures that you can combine it with other tidy outputs without fear of losing information (since rownames in R cannot contain duplicates).\n• Some column names are kept consistent, so that they can be combined across different models and so that you know what to expect (in contrast to asking \"is it or ?\" every time). The examples below are not all the possible column names, nor will all tidy output contain all or even any of these columns.\n• Each row in a output typically represents some well-defined concept, such as one term in a regression, one test, or one cluster/class. This meaning varies across models but is usually self-evident. The one thing each row cannot represent is a point in the initial data (for that, use the method).\n• Common column names include:\n• : the term in a regression or model that is being estimated.\n• : this spelling was chosen (over common alternatives such as , , or ) to be consistent with functions in R's built-in package\n• a test statistic, usually the one used to compute the p-value. Combining these across many sub-groups is a reliable way to perform (e.g.) bootstrap hypothesis testing\n• estimate of an effect size, slope, or other value\n• the low end of a confidence interval on the\n• the high end of a confidence interval on the\n• adds columns to the original data.\n• If the argument is missing, attempts to reconstruct the data from the model (note that this may not always be possible, and usually won't contain columns not used in the model).\n• Each row in an output matches the corresponding row in the original data.\n• If the original data contained rownames, turns them into a column called .\n• Newly added column names begin with to avoid overwriting columns in the original data.\n• Common column names include:\n• : the predicted values, on the same scale as the data.\n• always returns a one-row data frame.\n• The only exception is that returns an empty data frame.\n• We avoid including arguments that were given to the modeling function. For example, a glance output does not need to contain a field for , since that is decided by the user calling rather than the modeling function itself.\n• Common column names include:\n• the fraction of variance explained by the model\n• R^2 adjusted based on the degrees of freedom\n• the square root of the estimated variance of the residuals\n\nPlease note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms."
    },
    {
        "link": "https://cran.r-project.org/web/packages/broom/broom.pdf",
        "document": ""
    },
    {
        "link": "https://broom.tidymodels.org",
        "document": "summarizes key information about models in tidy s. provides three verbs to make it convenient to interact with model objects: For a detailed introduction, please see . tidies 100+ models from popular modelling packages and almost all of the model objects in the package that comes with base R. lists method availability. If you aren’t familiar with tidy data structures and want to know how they can make your life easier, we highly recommend reading Hadley Wickham’s Tidy Data.\n\nproduces a where each row contains information about an important component of the model. For regression models, this often corresponds to regression coefficients. This is can be useful if you want to inspect a model or create custom visualizations. returns a tibble with exactly one row of goodness of fitness measures and related statistics. This is useful to check for model misspecification and to compare many models. adds columns to a dataset, containing information such as fitted values, residuals or cluster assignments. All columns added to a dataset have prefix to prevent existing columns from being overwritten. We welcome contributions of all types! For questions and discussions about tidymodels packages, modeling, and machine learning, please post on Posit Community. If you think you have encountered a bug, please submit an issue. Either way, learn how to create and share a reprex (a minimal, reproducible example), to clearly communicate about your code. Check out further details on contributing guidelines for tidymodels packages and how to get help. If you have never directly contributed to an R package before, is an excellent place to start. Find an issue with the Beginner Friendly tag and comment that you’d like to take it on and we’ll help you get started. Generally, too, we encourage typo corrections, bug reports, bug fixes and feature requests. Feedback on the clarity of the documentation is especially valuable! If you are interested in adding tidier methods for new model objects, please read this article on the tidymodels website. We have a Contributor Code of Conduct. By participating in you agree to abide by its terms."
    },
    {
        "link": "https://uc-r.github.io/tidyr",
        "document": "Although many fundamental data processing functions exist in R, they have been a bit convoluted to date and have lacked consistent coding and the ability to easily flow together. This leads to difficult-to-read nested functions and/or choppy code. R Studio is driving a lot of new packages to collate data management tasks and better integrate them with other analysis activities. As a result, a lot of data processing tasks are becoming packaged in more cohesive and consistent ways, which leads to:\n\nis a one such package which was built for the sole purpose of simplifying the process of creating tidy data. This tutorial provides you with the basic understanding of the four fundamental functions of data tidying that tidyr provides:\n\nAlthough not required, the tidyr and dplyr packages make use of the pipe operator developed by Stefan Milton Bache in the R package magrittr. Although all the functions in tidyr and dplyr can be used without the pipe operator, one of the great conveniences these packages provide is the ability to string multiple functions together by incorporating .\n\nThis operator will forward a value, or the result of an expression, into the next function call/expression. For instance a function to filter data can be written as:\n\nBoth functions complete the same task and the benefit of using is not evident; however, when you desire to perform multiple functions its advantage becomes obvious. For more info check out the tutorial.\n\nDescription: There are times when our data is considered unstacked and a common attribute of concern is spread out across columns. To reformat the data such that these common attributes are gathered together as a single variable, the function will take multiple columns and collapse them into key-value pairs, duplicating all other columns as needed.\n\n☛ This function is a complement to\n\nWe’ll start with the following data set:\n\nThis data is considered wide since the variable (represented as quarters) is structured such that each quarter represents a variable. To re-structure the time component as an individual variable, we can gather each quarter within one column variable and also gather the values associated with each quarter in a second column variable.\n\nThese all produce the same results:\n\nAlso note that if you do not supply arguments for na.rm or convert values then the defaults are used.\n\nDescription: Many times a single column variable will capture multiple variables, or even parts of a variable you just don’t care about. Some examples include:\n\nIn each of these cases, our objective may be to separate characters within the variable string. This can be accomplished using the function which turns a single character column into multiple columns.\n\n☛ This function is a complement to\n\nWe can go back to our long_DF dataframe we created above in which way may desire to clean up or separate the Quarter variable.\n\nBy applying the function we get the following:\n\nThese produce the same results:\n\nObjective: Merging two variables into one\n\nDescription: There may be a time in which we would like to combine the values of two variables. The function is a convenience function to paste together multiple variable values into one. In essence, it combines two variables of a single observation into one variable.\n\n☛ This function is a complement to\n\nUsing the separate_DF dataframe we created above, we can re-unite the Time_Interval and Interval_ID variables we created and re-create the original Quarter variable we had in the long_DF dataframe.\n\nThese produce the same results:\n\nDescription: There are times when we are required to turn long formatted data into wide formatted data. The function spreads a key-value pair across multiple columns.\n\n☛ This function is a complement to"
    },
    {
        "link": "https://sthda.com/english/wiki/tidyr-crucial-step-reshaping-data-with-r-for-easier-analyses",
        "document": ""
    },
    {
        "link": "https://ademos.people.uic.edu/Chapter9.html",
        "document": ""
    },
    {
        "link": "https://tidyr.tidyverse.org",
        "document": "The goal of tidyr is to help you create tidy data. Tidy data is data where:\n• Each variable is a column; each column is a variable.\n• Each observation is a row; each row is an observation.\n• Each value is a cell; each cell is a single value. Tidy data describes a standard way of storing data that is used wherever possible throughout the tidyverse. If you ensure that your data is tidy, you’ll spend less time fighting with the tools and more time working on your analysis. Learn more about tidy data in .\n• None “Pivoting” which converts between long and wide forms. tidyr 1.0.0 introduces and , replacing the older and functions. See for more details.\n• None “Rectangling”, which turns deeply nested lists (as from JSON) into tidy tibbles. See , , , and for more details.\n• None Nesting converts grouped data to a form where each group becomes a single row containing a nested data frame, and unnesting does the opposite. See , , and for more details.\n• None Splitting and combining character columns. Use , , and to pull a single character column into multiple columns; use to combine multiple columns into a single character column.\n• None Make implicit missing values explicit with ; make explicit missing values implicit with ; replace missing values with next/previous value with , or a known value with .\n\nIf you encounter a clear bug, please file a minimal reproducible example on github. For questions and other discussion, please use community.rstudio.com. Please note that the tidyr project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms."
    },
    {
        "link": "https://linkedin.com/pulse/how-use-spread-function-tidyr-package-tuhin-tapadar",
        "document": "In the tidyr package in R, the spread() function is used to reshape data frames from a \"long\" format to a \"wide\" format. This can be especially useful when working with data that has been collected in a \"long\" format, such as survey data, where multiple observations are recorded for each individual.\n\nHere, we have an example of a data frame, which shows five different European cities, and the number of buses for different years.\n\nAnd as we can see it is not really very easy to understand the number of buses for the different years for each of the cities and this is an example of a \"long\" table, converting them to a wide format will be much easier to read. So, we will use the spread() from the tidyr package to create a wider table from the long table. Here's the code of how to use the spread() function in tidyr:\n\n# The general sytax of the spread() function to reshape the data frame data_wide <- spread(data, variable, value) # This is how we will convert our data from wide to long format. spread(df, key = \"Year\", value = \"Value\")\n\nAs you can see, the spread() function reshapes the data frame from a long to a wide format and now it is much easy to understand the information regarding each of the cities for each of the years. It is quite useful when you want to pivot data based on one column to get another column as headers. It's quite simple, intuitive, and easy to use with tidyr package.\n\nWhen working with survey data or other types of data that are recorded in a \"long\" format, the spread() function is a great way to reshape the data into a more easily readable format. Additionally, you can use the gather() function from tidyr package to convert the wide format to the long format, which can be helpful when working with data that has been spread. We will talk about the gather() in the next post.\n\nWhen using the spread() function, it's important to pay attention to the input arguments, the first argument is the column that you want to pivot on and the second argument is the column that will be spread. Also, you should check for missing values, since the spread function will make the data frame more sparse."
    }
]