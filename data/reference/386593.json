[
    {
        "link": "https://postgresql.org/docs/current/tutorial-fk.html",
        "document": "Recall the and tables from Chapter 2. Consider the following problem: You want to make sure that no one can insert rows in the table that do not have a matching entry in the table. This is called maintaining the referential integrity of your data. In simplistic database systems this would be implemented (if at all) by first looking at the table to check if a matching record exists, and then inserting or rejecting the new records. This approach has a number of problems and is very inconvenient, so PostgreSQL can do this for you.\n\nThe new declaration of the tables would look like this:\n\nNow try inserting an invalid record:\n\nThe behavior of foreign keys can be finely tuned to your application. We will not go beyond this simple example in this tutorial, but just refer you to Chapter 5 for more information. Making correct use of foreign keys will definitely improve the quality of your database applications, so you are strongly encouraged to learn about them."
    },
    {
        "link": "https://postgresql.org/docs/current/sql-createtable.html",
        "document": "The optional clause specifies a list of tables from which the new table automatically inherits all columns. Parent tables can be plain tables or foreign tables. Use of creates a persistent relationship between the new child table and its parent table(s). Schema modifications to the parent(s) normally propagate to children as well, and by default the data of the child table is included in scans of the parent(s). If the same column name exists in more than one parent table, an error is reported unless the data types of the columns match in each of the parent tables. If there is no conflict, then the duplicate columns are merged to form a single column in the new table. If the column name list of the new table contains a column name that is also inherited, the data type must likewise match the inherited column(s), and the column definitions are merged into one. If the new table explicitly specifies a default value for the column, this default overrides any defaults from inherited declarations of the column. Otherwise, any parents that specify default values for the column must all specify the same default, or an error will be reported. constraints are merged in essentially the same way as columns: if multiple parent tables and/or the new table definition contain identically-named constraints, these constraints must all have the same check expression, or an error will be reported. Constraints having the same name and expression will be merged into one copy. A constraint marked in a parent will not be considered. Notice that an unnamed constraint in the new table will never be merged, since a unique name will always be chosen for it. Column settings are also copied from parent tables. If a column in the parent table is an identity column, that property is not inherited. A column in the child table can be declared identity column if desired.\n\nCreates the table as a partition of the specified parent table. The table can be created either as a partition for specific values using or as a default partition using . Any indexes, constraints and user-defined row-level triggers that exist in the parent table are cloned on the new partition. The must correspond to the partitioning method and partition key of the parent table, and must not overlap with any existing partition of that parent. The form with is used for list partitioning, the form with and is used for range partitioning, and the form with is used for hash partitioning. is any variable-free expression (subqueries, window functions, aggregate functions, and set-returning functions are not allowed). Its data type must match the data type of the corresponding partition key column. The expression is evaluated once at table creation time, so it can even contain volatile expressions such as . When creating a list partition, can be specified to signify that the partition allows the partition key column to be null. However, there cannot be more than one such list partition for a given parent table. cannot be specified for range partitions. When creating a range partition, the lower bound specified with is an inclusive bound, whereas the upper bound specified with is an exclusive bound. That is, the values specified in the list are valid values of the corresponding partition key columns for this partition, whereas those in the list are not. Note that this statement must be understood according to the rules of row-wise comparison (Section 9.25.5). For example, given , a partition bound allows with any , with any non-null , and with any . The special values and may be used when creating a range partition to indicate that there is no lower or upper bound on the column's value. For example, a partition defined using allows any values less than 10, and a partition defined using allows any values greater than or equal to 10. When creating a range partition involving more than one column, it can also make sense to use as part of the lower bound, and as part of the upper bound. For example, a partition defined using allows any rows where the first partition key column is greater than 0 and less than or equal to 10. Similarly, a partition defined using allows any rows where the first partition key column starts with \"a\". Note that if or is used for one column of a partitioning bound, the same value must be used for all subsequent columns. For example, is not a valid bound; you should write . Also note that some element types, such as , have a notion of \"infinity\", which is just another value that can be stored. This is different from and , which are not real values that can be stored, but rather they are ways of saying that the value is unbounded. can be thought of as being greater than any other value, including \"infinity\" and as being less than any other value, including \"minus infinity\". Thus the range is not an empty range; it allows precisely one value to be stored — \"infinity\". If is specified, the table will be created as the default partition of the parent table. This option is not available for hash-partitioned tables. A partition key value not fitting into any other partition of the given parent will be routed to the default partition. When a table has an existing partition and a new partition is added to it, the default partition must be scanned to verify that it does not contain any rows which properly belong in the new partition. If the default partition contains a large number of rows, this may be slow. The scan will be skipped if the default partition is a foreign table or if it has a constraint which proves that it cannot contain rows which should be placed in the new partition. When creating a hash partition, a modulus and remainder must be specified. The modulus must be a positive integer, and the remainder must be a non-negative integer less than the modulus. Typically, when initially setting up a hash-partitioned table, you should choose a modulus equal to the number of partitions and assign every table the same modulus and a different remainder (see examples, below). However, it is not required that every partition have the same modulus, only that every modulus which occurs among the partitions of a hash-partitioned table is a factor of the next larger modulus. This allows the number of partitions to be increased incrementally without needing to move all the data at once. For example, suppose you have a hash-partitioned table with 8 partitions, each of which has modulus 8, but find it necessary to increase the number of partitions to 16. You can detach one of the modulus-8 partitions, create two new modulus-16 partitions covering the same portion of the key space (one with a remainder equal to the remainder of the detached partition, and the other with a remainder equal to that value plus 8), and repopulate them with data. You can then repeat this -- perhaps at a later time -- for each modulus-8 partition until none remain. While this may still involve a large amount of data movement at each step, it is still better than having to create a whole new table and move all the data at once. A partition must have the same column names and types as the partitioned table to which it belongs. Modifications to the column names or types of a partitioned table will automatically propagate to all partitions. constraints will be inherited automatically by every partition, but an individual partition may specify additional constraints; additional constraints with the same name and condition as in the parent will be merged with the parent constraint. Defaults may be specified separately for each partition. But note that a partition's default value is not applied when inserting a tuple through a partitioned table. Rows inserted into a partitioned table will be automatically routed to the correct partition. If no suitable partition exists, an error will occur. Operations such as which normally affect a table and all of its inheritance children will cascade to all partitions, but may also be performed on an individual partition. Note that creating a partition using requires taking an lock on the parent partitioned table. Likewise, dropping a partition with requires taking an lock on the parent table. It is possible to use to perform these operations with a weaker lock, thus reducing interference with concurrent operations on the partitioned table.\n\nThe clause specifies a table from which the new table automatically copies all column names, their data types, and their not-null constraints. Unlike , the new table and original table are completely decoupled after creation is complete. Changes to the original table will not be applied to the new table, and it is not possible to include data of the new table in scans of the original table. Also unlike , columns and constraints copied by are not merged with similarly named columns and constraints. If the same name is specified explicitly or in another clause, an error is signaled. The optional clauses specify which additional properties of the original table to copy. Specifying copies the property, specifying omits the property. is the default. If multiple specifications are made for the same kind of object, the last one is used. The available options are: Comments for the copied columns, constraints, and indexes will be copied. The default behavior is to exclude comments, resulting in the copied columns and constraints in the new table having no comments. Compression method of the columns will be copied. The default behavior is to exclude compression methods, resulting in columns having the default compression method. constraints will be copied. No distinction is made between column constraints and table constraints. Not-null constraints are always copied to the new table. Default expressions for the copied column definitions will be copied. Otherwise, default expressions are not copied, resulting in the copied columns in the new table having null defaults. Note that copying defaults that call database-modification functions, such as , may create a functional linkage between the original and new tables. Any generation expressions of copied column definitions will be copied. By default, new columns will be regular base columns. Any identity specifications of copied column definitions will be copied. A new sequence is created for each identity column of the new table, separate from the sequences associated with the old table. Indexes, , , and constraints on the original table will be created on the new table. Names for the new indexes and constraints are chosen according to the default rules, regardless of how the originals were named. (This behavior avoids possible duplicate-name failures for the new indexes.) Extended statistics are copied to the new table. settings for the copied column definitions will be copied. The default behavior is to exclude settings, resulting in the copied columns in the new table having type-specific default settings. For more on settings, see Section 65.2. is an abbreviated form selecting all the available individual options. (It could be useful to write individual clauses after to select all but some specific options.) The clause can also be used to copy column definitions from views, foreign tables, or composite types. Inapplicable options (e.g., from a view) are ignored.\n\nThe constraint specifies that a group of one or more columns of a table can contain only unique values. The behavior of a unique table constraint is the same as that of a unique column constraint, with the additional capability to span multiple columns. The constraint therefore enforces that any two rows must differ in at least one of these columns. For the purpose of a unique constraint, null values are not considered equal, unless is specified. Each unique constraint should name a set of columns that is different from the set of columns named by any other unique or primary key constraint defined for the table. (Otherwise, redundant unique constraints will be discarded.) When establishing a unique constraint for a multi-level partition hierarchy, all the columns in the partition key of the target partitioned table, as well as those of all its descendant partitioned tables, must be included in the constraint definition. Adding a unique constraint will automatically create a unique btree index on the column or group of columns used in the constraint. The created index has the same name as the unique constraint. The optional clause adds to that index one or more columns that are simply “payload”: uniqueness is not enforced on them, and the index cannot be searched on the basis of those columns. However they can be retrieved by an index-only scan. Note that although the constraint is not enforced on included columns, it still depends on them. Consequently, some operations on such columns (e.g., ) can cause cascaded constraint and index deletion.\n\nThe clause defines an exclusion constraint, which guarantees that if any two rows are compared on the specified column(s) or expression(s) using the specified operator(s), not all of these comparisons will return . If all of the specified operators test for equality, this is equivalent to a constraint, although an ordinary unique constraint will be faster. However, exclusion constraints can specify constraints that are more general than simple equality. For example, you can specify a constraint that no two rows in the table contain overlapping circles (see Section 8.8) by using the operator. The operator(s) are required to be commutative. Exclusion constraints are implemented using an index that has the same name as the constraint, so each specified operator must be associated with an appropriate operator class (see Section 11.10) for the index access method . Each defines a column of the index, so it can optionally specify a collation, an operator class, operator class parameters, and/or ordering options; these are described fully under CREATE INDEX. The access method must support (see Chapter 62); at present this means cannot be used. Although it's allowed, there is little point in using B-tree or hash indexes with an exclusion constraint, because this does nothing that an ordinary unique constraint doesn't do better. So in practice the access method will always be or . The allows you to specify an exclusion constraint on a subset of the table; internally this creates a partial index. Note that parentheses are required around the predicate.\n\nThese clauses specify a foreign key constraint, which requires that a group of one or more columns of the new table must only contain values that match values in the referenced column(s) of some row of the referenced table. If the list is omitted, the primary key of the is used. Otherwise, the list must refer to the columns of a non-deferrable unique or primary key constraint or be the columns of a non-partial unique index. The user must have permission on the referenced table (either the whole table, or the specific referenced columns). The addition of a foreign key constraint requires a lock on the referenced table. Note that foreign key constraints cannot be defined between temporary tables and permanent tables. A value inserted into the referencing column(s) is matched against the values of the referenced table and referenced columns using the given match type. There are three match types: , , and (which is the default). will not allow one column of a multicolumn foreign key to be null unless all foreign key columns are null; if they are all null, the row is not required to have a match in the referenced table. allows any of the foreign key columns to be null; if any of them are null, the row is not required to have a match in the referenced table. is not yet implemented. (Of course, constraints can be applied to the referencing column(s) to prevent these cases from arising.) In addition, when the data in the referenced columns is changed, certain actions are performed on the data in this table's columns. The clause specifies the action to perform when a referenced row in the referenced table is being deleted. Likewise, the clause specifies the action to perform when a referenced column in the referenced table is being updated to a new value. If the row is updated, but the referenced column is not actually changed, no action is done. Referential actions other than the check cannot be deferred, even if the constraint is declared deferrable. There are the following possible actions for each clause: Produce an error indicating that the deletion or update would create a foreign key constraint violation. If the constraint is deferred, this error will be produced at constraint check time if there still exist any referencing rows. This is the default action. Produce an error indicating that the deletion or update would create a foreign key constraint violation. This is the same as except that the check is not deferrable. Delete any rows referencing the deleted row, or update the values of the referencing column(s) to the new values of the referenced columns, respectively. Set all of the referencing columns, or a specified subset of the referencing columns, to null. A subset of columns can only be specified for actions. Set all of the referencing columns, or a specified subset of the referencing columns, to their default values. A subset of columns can only be specified for actions. (There must be a row in the referenced table matching the default values, if they are not null, or the operation will fail.) If the referenced column(s) are changed frequently, it might be wise to add an index to the referencing column(s) so that referential actions associated with the foreign key constraint can be performed more efficiently."
    },
    {
        "link": "https://stackoverflow.com/questions/28558920/postgresql-foreign-key-syntax",
        "document": "I have 2 tables as you will see in my PosgreSQL code below. The first table students has 2 columns, one for and the other which is the Primary Key.\n\nIn my second table called tests, this has 4 columns, one for , one for the , then one for a student with the highest score in a subject which is . am trying to make refer to in my students table. This is the code I have below, am not sure if the syntax is correct:\n\nis the syntax correct? because i have seen another one like\n\nWhat would be the correct way of creating the foreign key in PostgreSQL please?"
    },
    {
        "link": "https://postgresql.org/docs/7.1/sql-createtable.html",
        "document": "This documentation is for an unsupported version of PostgreSQL.You may want to view the same page for the current version, or one of the other supported versions listed above instead.\n\nCREATE [ TEMPORARY | TEMP ] TABLE ( { [ [ ... ] ] | } [, ... ] ) [ INHERITS ( [, ... ] ) ] where can be: [ CONSTRAINT ] { NOT NULL | NULL | UNIQUE | PRIMARY KEY | DEFAULT | CHECK ( ) | REFERENCES [ ( ) ] [ MATCH FULL | MATCH PARTIAL ] [ ON DELETE ] [ ON UPDATE ] [ DEFERRABLE | NOT DEFERRABLE ] [ INITIALLY DEFERRED | INITIALLY IMMEDIATE ] } and can be: [ CONSTRAINT ] { UNIQUE ( [, ... ] ) | PRIMARY KEY ( [, ... ] ) | CHECK ( ) | FOREIGN KEY ( [, ... ] ) REFERENCES [ ( [, ... ] ) ] [ MATCH FULL | MATCH PARTIAL ] [ ON DELETE ] [ ON UPDATE ] [ DEFERRABLE | NOT DEFERRABLE ] [ INITIALLY DEFERRED | INITIALLY IMMEDIATE ] } If specified, the table is created only for this session, and is automatically dropped on session exit. Existing permanent tables with the same name are not visible (in this session) while the temporary table exists. Any indexes created on a temporary table are automatically temporary as well. The name of the new table to be created. The name of a column to be created in the new table. The type of the column. This may include array specifiers. Refer to the PostgreSQL User's Guide for further information about data types and arrays. The optional INHERITS clause specifies a list of table names from which this table automatically inherits all fields. An optional name for a column or table constraint. If not specified, the system generates a name. A default value for a column. See the DEFAULT clause for more information. CHECK clauses specify integrity constraints or tests which new or updated rows must satisfy for an insert or update operation to succeed. Each constraint must be an expression producing a boolean result. A condition appearing within a column definition should reference that column's value only, while a condition appearing as a table constraint may reference multiple columns. The name of an existing table to be referenced by a foreign key constraint. The name of a column in an existing table to be referenced by a foreign key constraint. If not specified, the primary key of the existing table is assumed. A keyword indicating the action to take when a foreign key constraint is violated. Message returned if table creation failed. This is usually accompanied by some descriptive text, such as: , which occurs at runtime if the table specified already exists in the database.\n\nCREATE TABLE will enter a new, initially empty table into the current database. The table will be \"owned\" by the user issuing the command. Each may be a simple type, a complex type (set) or an array type. Each attribute may be specified to be non-null and each may have a default value, specified by the DEFAULT Clause. Note: Consistent array dimensions within an attribute are not enforced. This will likely change in a future release. CREATE TABLE also automatically creates a data type that represents the tuple type (structure type) corresponding to one row of the table. Therefore, tables can't have the same name as any existing datatype. A table can have no more than 1600 columns (in practice, the effective limit is lower because of tuple-length constraints). A table cannot have the same name as a system catalog table.\n\nThe optional INHERITS clause specifies a list of table names from which the new table automatically inherits all fields. If the same field name appears in more than one parent table, Postgres reports an error unless the field definitions match in each of the parent tables. If there is no definition conflict, then the duplicate fields are merged to form a single field of the new table. If the new table's own field list contains a field name that is also inherited, this declaration must likewise match the inherited field(s), and the field definitions are merged into one. Inherited and new field declarations of the same name must specify exactly the same data type to avoid an error. They need not specify identical constraints --- all constraints provided from any declaration are merged together and all are applied to the new table. If the new table explicitly specifies a default value for the field, this default overrides any defaults from inherited declarations of the field. Otherwise, any parents that specify default values for the field must all specify the same default, or an error will be reported. Postgres automatically allows the created table to inherit functions on tables above it in the inheritance hierarchy; that is, if we create table inheriting from , then functions that accept the tuple type can also be applied to instances of . (Currently, this works reliably for functions on the first or only parent table, but not so well for functions on additional parents.)\n\nThe DEFAULT clause assigns a default data value for the column whose column definition it appears within. The value is any variable-free expression (note that sub-selects and cross-references to other columns in the current table are not supported). The data type of a default value must match the column definition's data type. The DEFAULT expression will be used in any INSERT operation that does not specify a value for the column. If there is no DEFAULT clause, then the default is NULL. CREATE TABLE distributors ( name VARCHAR(40) DEFAULT 'luso films', did INTEGER DEFAULT NEXTVAL('distributors_serial'), modtime TIMESTAMP DEFAULT now() ); The above assigns a literal constant default value for the column , and arranges for the default value of column to be generated by selecting the next value of a sequence object. The default value of will be the time at which the row is inserted. The above assigns a literal constant default value for the column, and arranges for the default value of columnto be generated by selecting the next value of a sequence object. The default value ofwill be the time at which the row is inserted. It is worth remarking that would produce a result that is probably not the intended one: the string will be coerced to a timestamp value immediately, and so the default value of will always be the time of table creation. This difficulty is avoided by specifying the default value as a function call. would produce a result that is probably not the intended one: the stringwill be coerced to a timestamp value immediately, and so the default value ofwill always be the time of table creation. This difficulty is avoided by specifying the default value as a function call.\n\n[ CONSTRAINT ] { NULL | NOT NULL | UNIQUE | PRIMARY KEY | CHECK | REFERENCES [ ( ) ] [ MATCH ] [ ON DELETE ] [ ON UPDATE ] [ [ NOT ] DEFERRABLE ] [ INITIALLY ] } An arbitrary name given to a constraint clause. The column is allowed to contain NULL values. This is the default. The column is not allowed to contain NULL values. This is equivalent to the column constraint CHECK ( NOT NULL). The column must have unique values. In Postgres this is enforced by automatic creation of a unique index on the column. This column is a primary key, which implies that other tables may rely on this column as a unique identifier for rows. Both UNIQUE and NOT NULL are implied by PRIMARY KEY. See PRIMARY KEY for more information. The optional constraint clauses specify constraints or tests which new or updated rows must satisfy for an insert or update operation to succeed. A constraint is a named rule: an SQL object which helps define valid sets of values by putting limits on the results of INSERT, UPDATE or DELETE operations performed on a table. There are two ways to define integrity constraints: table constraints, covered later, and column constraints, covered here. A column constraint is an integrity constraint defined as part of a column definition, and logically becomes a table constraint as soon as it is created. The column constraints available are: The NOT NULL constraint specifies a rule that a column may contain only non-null values. This is a column constraint only, and not allowed as a table constraint. ERROR: ExecAppend: Fail to add null value in not null attribute \" \". This error occurs at runtime if one tries to insert a null value into a column which has a NOT NULL constraint. Define two NOT NULL column constraints on the table , one of which is explicitly given a name: CREATE TABLE distributors ( did DECIMAL(3) CONSTRAINT no_null NOT NULL, name VARCHAR(40) NOT NULL ); An arbitrary name given to a constraint clause. This error occurs at runtime if one tries to insert a duplicate value into a column. The UNIQUE constraint specifies a rule that a group of one or more distinct columns of a table may contain only unique values. The column definitions of the specified columns do not have to include a NOT NULL constraint to be included in a UNIQUE constraint. Having more than one null value in a column without a NOT NULL constraint, does not violate a UNIQUE constraint. (This deviates from the SQL92 definition, but is a more sensible convention. See the section on compatibility for more details.) Each UNIQUE column constraint must name a column that is different from the set of columns named by any other UNIQUE or PRIMARY KEY constraint defined for the table. Note: Postgres automatically creates a unique index for each UNIQUE constraint, to assure data integrity. See CREATE INDEX for more information. which is equivalent to the following specified as a table constraint: which is equivalent to the following specified as a table constraint: An arbitrary name given to a constraint clause. This error occurs at runtime if one tries to insert an illegal value into a column subject to a CHECK constraint. The CHECK constraint specifies a generic restriction on allowed values within a column. The CHECK constraint is also allowed as a table constraint. CHECK specifies a general boolean expression involving one or more columns of a table. A new row will be rejected if the boolean expression evaluates to FALSE when applied to the row's values. Currently, CHECK expressions cannot contain sub-selects nor refer to variables other than fields of the current row. The SQL92 standard says that CHECK column constraints may only refer to the column they apply to; only CHECK table constraints may refer to multiple columns. Postgres does not enforce this restriction. It treats column and table CHECK constraints alike. An arbitrary name given to a constraint clause. This occurs at runtime if one tries to insert a duplicate value into a column subject to a PRIMARY KEY constraint. The PRIMARY KEY column constraint specifies that a column of a table may contain only unique (non-duplicate), non-NULL values. The definition of the specified column does not have to include an explicit NOT NULL constraint to be included in a PRIMARY KEY constraint. Only one PRIMARY KEY can be specified for a table, whether as a column constraint or a table constraint. The PRIMARY KEY constraint should name a set of columns that is different from other sets of columns named by any UNIQUE constraint defined for the same table, since it will result in duplication of equivalent indexes and unproductive additional runtime overhead. However, Postgres does not specifically disallow this. [ CONSTRAINT ] REFERENCES [ ( ) ] [ MATCH ] [ ON DELETE ] [ ON UPDATE ] [ [ NOT ] DEFERRABLE ] [ INITIALLY ] The REFERENCES constraint specifies a rule that a column value is checked against the values of another column. REFERENCES can also be specified as part of a FOREIGN KEY table constraint. An arbitrary name given to a constraint clause. The table that contains the data to check against. The column in to check the data against. If this is not specified, the PRIMARY KEY of the is used. There are three match types: MATCH FULL, MATCH PARTIAL, and a default match type if none is specified. MATCH FULL will not allow one column of a multi-column foreign key to be NULL unless all foreign key columns are NULL. The default MATCH type allows some foreign key columns to be NULL while other parts of the foreign key are not NULL. MATCH PARTIAL is currently not supported. The action to do when a referenced row in the referenced table is being deleted. There are the following actions. Produce error if foreign key violated. This is the default. Set the referencing column values to their default value. The action to do when a referenced column in the referenced table is being updated to a new value. If the row is updated, but the referenced column is not changed, no action is done. There are the following actions. Produce error if foreign key violated. This is the default. Update the value of the referencing column to the new value of the referenced column. Set the referencing column values to their default value. This controls whether the constraint can be deferred to the end of the transaction. If DEFERRABLE, SET CONSTRAINTS ALL DEFERRED will cause the foreign key to be checked only at the end of the transaction. NOT DEFERRABLE is the default. has two possible values which specify the default time to check the constraint. Check constraint only at the end of the transaction. Check constraint after each statement. This is the default. ERROR: referential integrity violation - key referenced from not found in This error occurs at runtime if one tries to insert a value into a column which does not have a matching column in the referenced table. The REFERENCES column constraint specifies that a column of a table must only contain values which match against values in a referenced column of a referenced table. A value added to this column is matched against the values of the referenced table and referenced column using the given match type. In addition, when the referenced column data is changed, actions are run upon this column's matching data. Currently Postgres only supports MATCH FULL and a default match type. In addition, the referenced columns are supposed to be the columns of a UNIQUE constraint in the referenced table, however Postgres does not enforce this.\n\n[ CONSTRAINT name ] { PRIMARY KEY | UNIQUE } ( [, ... ] ) [ CONSTRAINT name ] CHECK ( ) [ CONSTRAINT name ] FOREIGN KEY ( [, ... ] ) REFERENCES [ ( [, ... ] ) ] [ MATCH ] [ ON DELETE ] [ ON UPDATE ] [ [ NOT ] DEFERRABLE ] [ INITIALLY ] An arbitrary name given to a constraint clause. The column name(s) for which to define a unique index and, for PRIMARY KEY, a NOT NULL constraint. A boolean expression to be evaluated as the constraint. The possible outputs for the table constraint clause are the same as for the corresponding portions of the column constraint clause. A table constraint is an integrity constraint defined on one or more columns of a table. The four variations of \"Table Constraint\" are: An arbitrary name given to a constraint clause. A name of a column in a table. This error occurs at runtime if one tries to insert a duplicate value into a column. The UNIQUE constraint specifies a rule that a group of one or more distinct columns of a table may contain only unique values. The behavior of the UNIQUE table constraint is the same as that for column constraints, with the additional capability to span multiple columns. See the section on the UNIQUE column constraint for more details. An arbitrary name given to a constraint clause. The names of one or more columns in the table. This occurs at run-time if one tries to insert a duplicate value into a column subject to a PRIMARY KEY constraint. The PRIMARY KEY constraint specifies a rule that a group of one or more distinct columns of a table may contain only unique (nonduplicate), non-null values. The column definitions of the specified columns do not have to include a NOT NULL constraint to be included in a PRIMARY KEY constraint. The PRIMARY KEY table constraint is similar to that for column constraints, with the additional capability of encompassing multiple columns. Refer to the section on the PRIMARY KEY column constraint for more information. [ CONSTRAINT ] FOREIGN KEY ( [, ... ] ) REFERENCES [ ( [, ... ] ) ] [ MATCH ] [ ON DELETE ] [ ON UPDATE ] [ [ NOT ] DEFERRABLE ] [ INITIALLY ] The REFERENCES constraint specifies a rule that a column value or set of column values is checked against the values in another table. An arbitrary name given to a constraint clause. The names of one or more columns in the table. The table that contains the data to check against. One or more columns in the to check the data against. If this is not specified, the PRIMARY KEY of the is used. There are three match types: MATCH FULL, MATCH PARTIAL, and a default match type if none is specified. MATCH FULL will not allow one column of a multi-column foreign key to be NULL unless all foreign key columns are NULL. The default MATCH type allows some foreign key columns to be NULL while other parts of the foreign key are not NULL. MATCH PARTIAL is currently not supported. The action to do when a referenced row in the referenced table is being deleted. There are the following actions. Produce error if foreign key violated. This is the default. Set the referencing column values to their default value. The action to do when a referenced column in the referenced table is being updated to a new value. If the row is updated, but the referenced column is not changed, no action is done. There are the following actions. Produce error if foreign key violated. This is the default. Update the value of the referencing column to the new value of the referenced column. Set the referencing column values to their default value. This controls whether the constraint can be deferred to the end of the transaction. If DEFERRABLE, SET CONSTRAINTS ALL DEFERRED will cause the foreign key to be checked only at the end of the transaction. NOT DEFERRABLE is the default. has two possible values which specify the default time to check the constraint. Check constraint after each statement. This is the default. Check constraint only at the end of the transaction. ERROR: referential integrity violation - key referenced from not found in This error occurs at runtime if one tries to insert a value into a column which does not have a matching column in the referenced table. The FOREIGN KEY constraint specifies a rule that a group of one or more distinct columns of a table is related to a group of distinct columns in the referenced table. The FOREIGN KEY table constraint is similar to that for column constraints, with the additional capability of encompassing multiple columns. Refer to the section on the FOREIGN KEY column constraint for more information.\n\nIn addition to the locally visible temporary table, SQL92 also defines a CREATE GLOBAL TEMPORARY TABLE statement, and optionally an ON COMMIT clause: For temporary tables, the CREATE GLOBAL TEMPORARY TABLE statement names a new table visible to other clients and defines the table's columns and constraints. The optional ON COMMIT clause of CREATE TEMPORARY TABLE specifies whether or not the temporary table should be emptied of rows whenever COMMIT is executed. If the ON COMMIT clause is omitted, SQL92 specifies that the default is ON COMMIT DELETE ROWS. However, Postgres' behavior is always like ON COMMIT PRESERVE ROWS. The NULL \"constraint\" (actually a non-constraint) is a Postgres extension to SQL92 that is included for symmetry with the NOT NULL clause (and for compatibility with some other RDBMSes). Since it is the default for any column, its presence is simply noise. SQL92 specifies some additional capabilities for NOT NULL: [ CONSTRAINT ] NOT NULL [ {INITIALLY DEFERRED | INITIALLY IMMEDIATE} ] [ [ NOT ] DEFERRABLE ] SQL92 specifies some additional capabilities for constraints, and also defines assertions and domain constraints. Note: Postgres does not yet support either domains or assertions. An assertion is a special type of integrity constraint and shares the same namespace as other constraints. However, an assertion is not necessarily dependent on one particular table as constraints are, so SQL-92 provides the CREATE ASSERTION statement as an alternate method for defining a constraint: Domain constraints are defined by CREATE DOMAIN or ALTER DOMAIN statements: [ CONSTRAINT ] { NOT NULL | PRIMARY KEY | FOREIGN KEY | UNIQUE | CHECK } [ {INITIALLY DEFERRED | INITIALLY IMMEDIATE} ] [ [ NOT ] DEFERRABLE ] A CONSTRAINT definition may contain one deferment attribute clause and/or one initial constraint mode clause, in any order. The constraint must be checked at the end of each statement. SET CONSTRAINTS ALL DEFERRED will have no effect on this type of constraint. This controls whether the constraint can be deferred to the end of the transaction. If SET CONSTRAINTS ALL DEFERRED is used or the constraint is set to INITIALLY DEFERRED, this will cause the foreign key to be checked only at the end of the transaction. Note: SET CONSTRAINTS changes the foreign key constraint mode only for the current transaction. Check constraint after each statement. This is the default. Check constraint only at the end of the transaction. SQL92 specifies some additional capabilities for CHECK in either table or column constraints. [ CONSTRAINT ] CHECK ( VALUE ) [ {INITIALLY DEFERRED | INITIALLY IMMEDIATE} ] [ [ NOT ] DEFERRABLE ] [ CONSTRAINT ] CHECK ( VALUE ) [ {INITIALLY DEFERRED | INITIALLY IMMEDIATE} ] [ [ NOT ] DEFERRABLE ] Multiple inheritance via the INHERITS clause is a Postgres language extension. SQL99 (but not SQL92) defines single inheritance using a different syntax and different semantics. SQL99-style inheritance is not yet supported by Postgres."
    },
    {
        "link": "https://neon.tech/postgresql/postgresql-tutorial/postgresql-foreign-key",
        "document": "Summary: in this tutorial, you will learn about the PostgreSQL foreign key and how to add foreign keys to tables using foreign key constraints.\n\nIn PostgreSQL, a foreign key is a column or a group of columns in a table that uniquely identifies a row in another table.\n\nA foreign key establishes a link between the data in two tables by referencing the primary key or a unique constraint of the referenced table.\n\nThe table containing a foreign key is referred to as the referencing table or child table. Conversely, the table referenced by a foreign key is known as the referenced table or parent table.\n\nThe main purpose of foreign keys is to maintain referential integrity in a relational database, ensuring that relationships between the parent and child tables are valid.\n\nFor example, a foreign key prevents the insertion of values that do not have corresponding values in the referenced table.\n\nAdditionally, a foreign key maintains consistency by automatically updating or deleting related rows in the child table when changes occur in the parent table.\n\nA table can have multiple foreign keys depending on its relationships with other tables.\n\nTo define a foreign key, you can use a foreign key constraint.\n• First, specify the name for the foreign key constraint after the keyword. The clause is optional. If you omit it, PostgreSQL will assign an auto-generated name.\n• Second, specify one or more foreign key columns in parentheses after the keywords.\n• Third, specify the parent table and parent key columns referenced by the foreign key columns in the clause.\n• Finally, specify the desired delete and update actions in the and clauses.\n\nThe delete and update actions determine the behaviors when the primary key in the parent table is deleted and updated.\n\nSince the primary key is rarely updated, the is infrequently used in practice. We’ll focus on the action.\n\nThe following statements create the and tables:\n\nIn this example, the table is the parent table and the table is the child table.\n\nEach customer has zero or many contacts and each contact belongs to zero or one customer.\n\nThe column in the table is the foreign key column that references the primary key column with the same name in the table.\n\nThe following foreign key constraint in the table defines the as the foreign key:\n\nBecause the foreign key constraint does not have the and action, they default to .\n\nThe following inserts data into the and tables:\n\nThe following statement deletes the customer id 1 from the table:\n\nBecause of the , PostgreSQL issues a constraint violation because the referencing rows of the customer id 1 still exist in the table:\n\nThe action is similar to the . The difference only arises when you define the foreign key constraint as with an or mode. We’ll discuss more on this in the upcoming tutorial.\n\nThe automatically sets to the foreign key columns in the referencing rows of the child table when the referenced rows in the parent table are deleted.\n\nFirst, drop the sample tables and re-create them with the foreign key that uses the action in the clause:\n\nSecond, insert data into the and tables:\n\nThird, delete the customer with id 1 from the table:\n\nBecause of the action, the referencing rows in the table are set to NULL.\n\nFinally, display the data in the table:\n\nThe output indicates that the values of customer id 1 changed to .\n\nThe automatically deletes all the referencing rows in the child table when the referenced rows in the parent table are deleted. In practice, the is the most commonly used option.\n\nThe following statements recreate the sample tables with the delete action of the changes to :\n\nThe following statement deletes the customer id 1:\n\nBecause of the action, all the referencing rows in the table are automatically deleted:\n\nThe sets the default value to the foreign key column of the referencing rows in the child table when the referenced rows from the parent table are deleted.\n\nTo add a foreign key constraint to the existing table, you use the following form of the ALTER TABLE statement:\n\nWhen adding a foreign key constraint with option to an existing table, you need to follow these steps:\n\nSecond, add a new foreign key constraint with action:\n• Use foreign keys to ensure the referential integrity and consistency of data between two tables.\n• Use the constraint to define a foreign key constraint when creating a table.\n• Use the to add a foreign key constraint to an existing table."
    },
    {
        "link": "https://stackoverflow.com/questions/60607910/how-to-manage-postgresql-foreign-keys",
        "document": "I need some advice on SQL structure on Postgresql.\n\nI have those two tables :\n\nI added no constraints on purpose. I need a foreign key on the child table product_attribute.attribute_value_id referencing the parent table attribute_value.id. The best practice is to create a primary key on the field attribute_value.id (maybe with a sequence), or to CREATE UNIQUE INDEX on attribute_value.id ? I first thought indexes were only special lookup tables that the database search engine can use to speed up data retrieval. But when I played with foreign keys, I found that creating an unique index allowed me to avoid error \"there is no unique constraint matching given keys for referenced table blablabla\" because a foreign key is not supposed to point to a non unique value. Should indexes be used to create foreign keys then ?\n\nI also need a foreign key on the child table product_attribute.attribute_id referencing parent table attribute_value.attribute_id. The problem is that attribute_value.attribute_id is not unique. But all the rows in product_attribute.attribute_id must not take any value out of attribute_value.attribute_id's possible values. How should I do ?"
    },
    {
        "link": "https://neon.tech/postgresql/postgresql-tutorial/postgresql-foreign-key",
        "document": "Summary: in this tutorial, you will learn about the PostgreSQL foreign key and how to add foreign keys to tables using foreign key constraints.\n\nIn PostgreSQL, a foreign key is a column or a group of columns in a table that uniquely identifies a row in another table.\n\nA foreign key establishes a link between the data in two tables by referencing the primary key or a unique constraint of the referenced table.\n\nThe table containing a foreign key is referred to as the referencing table or child table. Conversely, the table referenced by a foreign key is known as the referenced table or parent table.\n\nThe main purpose of foreign keys is to maintain referential integrity in a relational database, ensuring that relationships between the parent and child tables are valid.\n\nFor example, a foreign key prevents the insertion of values that do not have corresponding values in the referenced table.\n\nAdditionally, a foreign key maintains consistency by automatically updating or deleting related rows in the child table when changes occur in the parent table.\n\nA table can have multiple foreign keys depending on its relationships with other tables.\n\nTo define a foreign key, you can use a foreign key constraint.\n• First, specify the name for the foreign key constraint after the keyword. The clause is optional. If you omit it, PostgreSQL will assign an auto-generated name.\n• Second, specify one or more foreign key columns in parentheses after the keywords.\n• Third, specify the parent table and parent key columns referenced by the foreign key columns in the clause.\n• Finally, specify the desired delete and update actions in the and clauses.\n\nThe delete and update actions determine the behaviors when the primary key in the parent table is deleted and updated.\n\nSince the primary key is rarely updated, the is infrequently used in practice. We’ll focus on the action.\n\nThe following statements create the and tables:\n\nIn this example, the table is the parent table and the table is the child table.\n\nEach customer has zero or many contacts and each contact belongs to zero or one customer.\n\nThe column in the table is the foreign key column that references the primary key column with the same name in the table.\n\nThe following foreign key constraint in the table defines the as the foreign key:\n\nBecause the foreign key constraint does not have the and action, they default to .\n\nThe following inserts data into the and tables:\n\nThe following statement deletes the customer id 1 from the table:\n\nBecause of the , PostgreSQL issues a constraint violation because the referencing rows of the customer id 1 still exist in the table:\n\nThe action is similar to the . The difference only arises when you define the foreign key constraint as with an or mode. We’ll discuss more on this in the upcoming tutorial.\n\nThe automatically sets to the foreign key columns in the referencing rows of the child table when the referenced rows in the parent table are deleted.\n\nFirst, drop the sample tables and re-create them with the foreign key that uses the action in the clause:\n\nSecond, insert data into the and tables:\n\nThird, delete the customer with id 1 from the table:\n\nBecause of the action, the referencing rows in the table are set to NULL.\n\nFinally, display the data in the table:\n\nThe output indicates that the values of customer id 1 changed to .\n\nThe automatically deletes all the referencing rows in the child table when the referenced rows in the parent table are deleted. In practice, the is the most commonly used option.\n\nThe following statements recreate the sample tables with the delete action of the changes to :\n\nThe following statement deletes the customer id 1:\n\nBecause of the action, all the referencing rows in the table are automatically deleted:\n\nThe sets the default value to the foreign key column of the referencing rows in the child table when the referenced rows from the parent table are deleted.\n\nTo add a foreign key constraint to the existing table, you use the following form of the ALTER TABLE statement:\n\nWhen adding a foreign key constraint with option to an existing table, you need to follow these steps:\n\nSecond, add a new foreign key constraint with action:\n• Use foreign keys to ensure the referential integrity and consistency of data between two tables.\n• Use the constraint to define a foreign key constraint when creating a table.\n• Use the to add a foreign key constraint to an existing table."
    },
    {
        "link": "https://stackoverflow.com/questions/76226855/is-it-good-practise-in-postgresql-to-add-extra-foreign-keys-columns-for-a-speedy",
        "document": "When I make ER diagram in the software makes automaticly multiple foreign key columns where I would have expected just one foreign key:\n\nIn the image you see the following foreign keys with multiple columns:\n\nFrom this post I learned that does this for index purposes.\n\nWhen I make such an ER diagram in these extra foreign key columns aren't added. Is this about how advanced is the tooling or isn't it a good practise in ? With other words: is it a good idea to add these extra columns as foreign keys and put an on them?"
    },
    {
        "link": "https://enterprisedb.com/blog/how-to-secure-postgresql-security-hardening-best-practices-checklist-tips-encryption-authentication-vulnerabilities",
        "document": "Securing data is mission-critical for the success of any enterprise and the safety of its customers. This article is intended as a comprehensive overview that will help you examine the security of your Postgres deployment from end to end.\n\nThe vast majority of the discussion will focus on features, functionality, and techniques that apply equally to PostgreSQL and EDB Postgres Advanced Server (EPAS); however, it will also touch on a couple of features that are only available in EPAS. These will be clearly noted. This article references the latest version of Postgres currently available: 12.3.\n\nAs part of the review of the components and process of securing Postgres, we'll look at the following sections:\n\nThe first part of any security review is to look at how the server is connected to and accessed. As with any security configuration, follow the principle of least privilege when considering how to configure your system; that is, only allow as much access as is required to implement a working system and no more.\n\nIt can be extremely difficult to prevent someone with physical access to a server from obtaining the data, but there are several measures that can be taken, both physical and technological.\n\nFirst, physical access should be limited as much as possible by ensuring the server is located in a secure facility. This may be a privately owned server room, in which case measures can be taken to ensure that only authorized personnel can enter the room and that monitoring such as CCTV is employed. In the case that a co-location facility is used, ensure that the chosen provider has a strictly enforced security policy appropriately designed to prevent unauthorized access, and in facilities that allow users to enter, that locking racks and cages are available to keep other customers away from your hardware.\n\nThere's little that can be done in this regard with the major cloud providers other than to trust that they do implement the high levels of physical security that they claim. However for both cloud providers and co-location facilities, it is essential to check that they have appropriate documentation attesting to the level of security they provide, such as SOC 2 or 3.\n\nThere are two ways to connect to a Postgres server; via a Unix Domain Socket or a TCP/IP Socket\n• None Unix Domain Sockets (UDS) are the default method for connecting to a Postgres database on Unix-like platforms. On Windows they are not available at present but will be in Postgres v13 and later. UDS are only accessible from the machine on which they are present (and therefore are not subject to direct remote attacks) and appear as special files on the file system. This means that access to them is subject to the same access controls as other files (though only write permission is needed to use the socket) and can be controlled by managing the permissions and group ownership of the socket through the unix_socket_permissions and unix_socket_group configuration options, as well as the permissions on the directory in which the socket is created. Sockets are always owned by the user that the Postgres server is running as. To offer even more flexibility, Postgres can create multiple sockets (though by default, only one is created) using the unix_socket_directories configuration option, each of whose directories can have different permissions as required to segregate different users or applications and help to apply the principle of least privilege. If your application is running on the same host as the database server, seriously consider allowing access to the server via one or more UDS only.\n• None If you need to access your Postgres server from a remote system, as is often the case when implementing applications with multiple tiers or services, or just for remote administration using tools such as pgAdmin, you will need to use a TCP/IP network socket. As is generally the case when it comes to security, we want to minimize the potential attack area for anyone attempting to gain access to the system. How this is done depends on how the server is hosted on the network. If it's inside a corporate network, it may be hosted on multiple VLANs or physical networks, which can be used for different purposes, such as applications, management, and storage access, for example. The system should only be configured to listen for and accept connections on the networks that are required; by default, a source code build of Postgres will listen only on the localhost or loopback address, which prevents connections from other machines. However, some pre-packaged builds of Postgres override this, so you should check your installation. Use the listen_addresses configuration parameter in postgresql.conf to ensure Postgres only listens and accepts connections on the required network addresses, thus preventing access from, say, the storage network.\n\nFirewalls are an important tool to prevent unauthorized access to network ports. Many also offer logging facilities that can be used as part of a broader initiative to proactively detect intrusion attempts, helping mitigate them before they succeed.\n• None Most modern operating systems include firewalls, including the Windows Defender Firewall on Windows and iptables on Linux; plus, there are also a number of third-party products you might choose. Typical firewalls will allow you to define inbound and outbound rules that specify the traffic that is allowed. These rules will consist of several common parameters:\n• The local port, e.g. 5432 (the default port for PostgreSQL)\n• The source address; i.e., where the connection attempt is coming from. Some firewalls offer additional options to give far greater flexibility; for example, Windows Defender Firewall allows you to specify a program instead of port number. As always, we want to minimize access to Postgres, so it would be quite normal to create a rule for TCP (and/or IPv6) traffic arriving on port 5432 to be rejected (or black-holed) unless it's coming from the address of our application server. The source address can usually be a list of addresses or subnets. If your server has any Foreign Data Wrappers or similar extensions installed, it may also be desirable to create outbound rules to prevent them from being used to connect to anything other than a predefined set of servers. While configuring Windows Defender Firewall is quite straightforward, configuring iptables is much more complex. Linux distributions such as Redhat and Ubuntu offer management tools to make this easier, and there are also other open source tools available, such as Ferm and Shorewall.\n\nMinimize access to your server as much as possible using a firewall.\n• None Most cloud providers recommend against using firewalls in virtual instances, suggesting instead that users make use of the firewalls built into the platform. This typically makes management much easier, allowing rule sets to be created that can be reused and attached to multiple servers and allowing management through their web and command line interfaces and REST APIs. Firewalls at the cloud providers are implemented as part of their network infrastructure and generally work in much the same way as the host firewalls described in the previous section, i.e., specify the source addresses, protocol, and destination port for traffic to allow. Most cloud providers also offer Virtual Private Clouds (VPC), in which a number of servers can coexist in a single virtual environment with its own private network or networks. This type of configuration has become the default and makes it very easy to deploy a multi-tiered system on the public cloud while keeping the non-public tiers segregated from the internet in general. The use of multiple subnets within a VPC can make it easy to further segregate servers, keeping the public tiers in a \"DMZ\" subnet, with only minimal access to the database servers that are in a private subnet with no direct internet connection.\n\nIf traffic to the database server is flowing across the network, it is good (arguably essential) practice to encrypt that traffic. Postgres uses OpenSSL to provide transport security – though work has been underway for some time to add support for Microsoft Secure Channel or Schannel and Apple Secure Transport – through the use of TLS (previously SSL).\n\nTo encrypt connections in Postgres you will need at least a server certificate and key, ideally protected with a passphrase that can be securely entered at server startup either manually or using a script that can retrieve the passphrase on behalf of the server, as specified using the ssl_passphrase_command configuration parameter. Passphrases are not supported on Windows, at least as of Postgres 12. The server certificate and key are specified using the ssl_cert_file and ssl_key_file, respectively.\n\nIf you have an existing Certification Authority (CA) in use, you can use certificates provided with Postgres. The configuration parameters ssl_ca_file and ssl_crl_file allow you to provide the CA (and intermediate) certificates and the certificate revocation list to the server. This gives you the flexibility to revoke certificates in response to security incidents and have the server reject client certificates or the client reject server certificates. It also allows you to configure the client and server to reject each other if the identity of either cannot be verified through the chain of trust to prevent as-yet undetected spoofing. The use of certificates for client authentication is discussed below.\n\nIt's important to ensure that your use of TLS is secure as well. There are several configuration parameters that can be set to ensure that you're not using ciphers or other options that may no longer be considered secure. It is recommended that you check and appropriately configure the following configuration parameters in your postgresql.conf configuration file:\n\nNo recommendation is made in this article on what those parameters should be set to, as inevitably, they will change over time. You should periodically check to ensure you're using options that continue to be regarded as secure and update them when appropriate.\n\nIf traffic to your server flows over the network, ensure it's encrypted using the strongest possible ciphers and other options.\n\nAfter access, the next security component to be considered is client authentication, which is how we authenticate users and control whether they can connect to the server successfully through the pg_hba.conf configuration file.\n\nThe pg_hba.conf file (typically found in the Postgres data directory) defines the access rules and authentication methods for the data server. Lines in the file are processed sequentially when a connection is established, and the first line that matches the properties of the connection is used to determine the authentication method.\n\nThere are seven different possible formats for lines in the file (as well as comments, which start with a #), of which there are three main variants, the rest following the same structure as one of the others but with a different connection type in the first field. Here are some examples:\n\nIn this example, a connection attempt from my_user to the database my_db over a local (UDS) connection using scram-sha-256 will be accepted.\n\nIn this example, a connection attempt from my_user to the database my_db from 172.16.253.47 using md5 as the authentication method will be accepted.\n\nNote that the address shown in the example has /32 on the end to denote that all 32 of the high-order bits must be matched. To allow connections to match from anywhere on that subnet, we could also write it as 172.16.0.0/16 or 172.16.0.0 255.255.0.0 (the third format variant).\n\nThe fields in each line are always the connection type, the database name(s), the user name(s), the client network address/subnet (where needed), and the authentication method, plus any options that may be applicable. See the documentation for more information.\n\nAs a rule of thumb, any network connections should use either the hostssl or hostgssenc connection types to ensure that connections are encrypted.\n\nIn the following subsections we will examine several of the most used authentication methods.\n\nThe trust authentication method should only be used in exceptional circumstances, if at all, as it allows a matching client to connect to the server with no further authentication. Trust is useful for testing and development work on the local machine when connecting via a UDS, and when only fully trusted users have access to the machine and data security is not a concern.\n\nIt is also a useful mechanism for resetting passwords in the server if there is no other way to log in; temporarily allow trust access to connections from a UDS, connect to the server and reset the password, and then disable the trust access again.\n\nUse trust with extreme care. It can be very dangerous!\n\nPeer and ident are both methods of allowing users to be authenticated by the underlying operating system. Many Postgres packages come pre-configured to use peer authentication.\n\nThe peer authentication method is only available for local connections. When peer is used, the server gets the client’s username from the operating system and checks that it matches the requested database username.\n\nThe ident authentication method is only available for network connections. It works in a similar way to peer authentication, except that it relies on an ident server running on the client to confirm the username.\n\nBoth peer and ident allow the use of connection maps to handle acceptable mismatches between the username known to the client and that known to the database server.\n\nNote that ident should not be relied upon, as the client running the ident server is unlikely to be guaranteed trustworthy.\n\nFor many years md5 was the preferred hashing mechanism for use with Postgres, and though still widely used it's strongly recommended that users move to the scram-sha-256, where password authentication is required.\n\nBoth md5 and scram-sha-256 use a challenge-response mechanism to prevent sniffing and store hashed passwords on the server. However, scram-sha-256 stores the hashes in what is currently considered to be a cryptographically secure form to avoid issues if an attacker gains access to the hash.\n\nIf you need to support password authentication with a standalone Postgres server, you should be using scram-sha-256 as the authentication method. Do not use md5 in new deployments!\n\nLDAP and Kerberos are often utilized in corporate environments when integrated with Single Sign On (SSO) systems. In such systems, the Postgres server is configured to authenticate the user through an LDAP directory or Kerberos infrastructure.\n\nIn LDAP systems, there are various ways that user access can be controlled, for example, by only granting access to users that are members of a specific organizational unit or group. When setting up your pg_hba.conf file, additional options can be specified at the end of the line, including an LDAP search filter, which will only allow users that match the filter to connect to the database.\n\nKerberos authentication is available through the gssapi authentication method in Postgres. Setting it up can be a little more daunting than LDAP and other authentication methods, but it is considered secure and offers automatic authentication for client software that supports it. There is a related Windows-specific sspi authentication method that can be used in Windows domains.\n\nThough LDAP authentication is very popular based on user feedback the author has received, Kerberos authentication should always be preferred as, unlike LDAP, the user's password is never sent to the Postgres server.\n\nTLS (sometimes referred to as SSL) certificates can be used for authentication and as a requirement for TLS encryption, as discussed in part one of this blog series. Certificate authentication works by trusting a top-level certificate (or one of its children or “intermediate” certificates) to issue certificates only to trusted clients. Clients in possession of a certificate and key issued by the higher authority who also issued the server certificate and key can be considered trusted.\n\nIn a simple example, you would first create a Certificate Authority (CA) certificate and key. This is extremely valuable and sensitive, so it must be kept completely secure. Then, you create a certificate and key for the Postgres server and sign it using the CA certificate and key.\n\nBoth the server certificate and key are then installed into the Postgres server, along with a copy of the CA's certificate (but not the CA's key).\n\nClient certificates and keys can then also be created and signed by the CA as required.\n\nWhen the client connects to Postgres and the cert authentication method is used, the Postgres server will check that the certificate presented by the client is trusted and that the Common Name (CN) field of the certificate matches the username for the client. Username mapping can also be done as with peer and ident.\n\nThe client can also specify several options when connecting to the server, including whether to (and to what extent) verify the trustworthiness of the server's certificate. This gives protection against spoofing.\n\nBoth the client and the server can use certificate revocation lists to keep track of any certificates that should no longer be trusted.\n\nCertificates are an ideal way to authenticate automated systems that need to connect across the network to a Postgres server.\n\nThere are two additional configuration options that are worth considering:\n\nauthentication_timeout is a parameter that can be set in postgresql.conf. Its purpose is to set the maximum amount of time in which authentication must be completed before the server closes the connection. This is to ensure that incomplete connection attempts don't occupy a connection slot indefinitely.\n\nauth_delay is a contrib module for Postgres that can be loaded through the shared_preload_libraries configuration option in postgresql.conf. Its purpose is to pause briefly when an authentication attempt fails before failure is reported to make brute force attacks much more difficult.\n\nThese are some of the more popular authentication methods available in Postgres. There are a number of other authentication methods available, but these are less widely used and have more specialized applications.\n\nThe next critical component in securing a Postgres deployment is the creation and setting of roles, which can limit database access for specified users.\n\nVery old (practically prehistoric) versions of PostgreSQL offered users and user groups as ways to group user accounts together. In PostgreSQL 8.1 this system was replaced with the SQL Standard compliant roles system.\n\nA role can be a member of other roles or have roles that are members of it. We sometimes refer to this as \"granting\" a role to another role. Roles have several attributes that can be set, including ones that effectively make them user accounts that can be used to log in to the database server. An example of granting a role to another role is shown below:\n\nThis makes the nagios role a member of pg_monitor, thereby giving nagios access to the extended functionality reserved for superusers and members of the pg_monitor role.\n\nRoles have several fixed attributes that can be set:\n• LOGIN: Can this role be used to login to the database server?\n• CREATEROLE: Can this role create new roles?\n• PASSWORD: The password for the role, if set.\n• VALID UNTIL: An optional timestamp after which time the password will no longer be valid.\n\nRoles with the SUPERUSER flag set automatically bypass all permission checks except the right to log in.\n\nThere are several other less commonly used role attributes that can be set. See the documentation for more information.\n\nGrant SUPERUSER (and potentially dangerous attributes such as CREATEDB and CREATEROLE) with great care. Do not use a role with SUPERUSER privileges for day-to-day work.\n\nPostgreSQL (as opposed to EDB Postgres Advanced Server) doesn't include any password complexity enforcement functionality by default. It does include a hook that can be used to plug in a module to do password complexity checks, but this will have no effect if the user changes their password using a pre-hashed string.\n\nA sample password check module can be found in Postgres' contrib directory in the source tree and is included with most package sets. This module can be used as an example for developing something more complex that meets an organization's specific requirements, though it does require C development work.\n\nThe most effective way to enforce password complexity in Postgres is to use an external identity service for authentication, such as LDAP or Kerberos, as described above.\n\nEDB Postgres Advanced Server offers a password profile feature that can be used with the password (never use this, as the password will be transferred in plain text!), md5, and scram-sha-256 authentication methods configured in pg_hba.conf. Password profiles can be configured by the superuser and applied to one or more roles. A profile allows you to define the following options:\n• FAILED_LOGIN_ATTEMPTS: The number of failed login attempts that may occur before the role is locked out for the amount of time specified in the PASSWORD_LOCK_TIME parameter.\n• PASSWORD_LIFE_TIME: The number of days a password can be used before the user is prompted to change it.\n• PASSWORD_GRACE_TIME: The length of the grace period after a password expires until the user is forced to change their password. When the grace period expires, a user will be allowed to connect but will not be allowed to execute any command until they update their expired password.\n• PASSWORD_REUSE_TIME: The number of days a user must wait before reusing a password.\n• PASSWORD_REUSE_MAX: The number of password changes that must occur before a password can be reused.\n• PASSWORD_VERIFY_FUNCTION: The name of a PL/SQL function that can check password complexity.\n\nNote that if the user changes their password by supplying a new one in a pre-hashed form, then it is not possible to verify the complexity with the PASSWORD_VERIFY_FUNCTION option or re-use with the PASSWORD_REUSE_MAX option. To mitigate this, the PASSWORD_ALLOW_HASHED option may be set to false in the password profile.\n\nIf you're running EDB Postgres Advanced Server and not using an external authentication provider such as LDAP or Kerberos, consider using password profiles to ensure your users maintain strong, regularly changed passwords.\n\nThe SET ROLE SQL command may be applied by a user to change the user identifier of the current session to the name of any role of which they are a member. This may be used to either add or restrict privileges from the session and may be reset using RESET ROLE (thus making SET ROLE unsuitable for use as a multi-tenancy solution).\n\nSET ROLE is like using the sudo su - on a Unix-like system. It essentially allows you to run SQL commands as that other user.\n\nWhen a role is a member of another role, it will automatically inherit the privileges of that role. To use SET ROLE effectively, the NOINHERIT keyword should be used when creating the role to prevent it from inheriting privileges automatically, requiring the use of SET ROLE to explicitly gain them when needed.\n\nIn addition to SET ROLE, there is also a SET SESSION AUTHORIZATION command, which is only available to superusers. The high-level difference between them is that SET ROLE will change the current_user value but not session_user, whilst SET SESSION AUTHORIZATION will change both.\n\nIn practical terms, this means that after running SET SESSION AUTHORIZATION, any subsequent SET ROLE commands will be restricted to those that the session_user could perform, regardless of the fact that the original session_user was a superuser. This allows superusers to more accurately imitate another user.\n\nConsider using SET ROLE to allow users to temporarily elevate their privileges only when required to perform more potentially dangerous tasks.\n\nPostgres comes with several built-in monitoring roles (originally developed by your humble author!) that have access to functionality that was restricted to superusers only in earlier versions of Postgres. These roles allow you to grant specific privileges to roles that are used to monitor the system without having to give them full superuser access:\n• pg_monitor: A role which combines all the following roles:\n• pg_read_all_settings: Read all configuration variables, even those normally visible only to superusers.\n• pg_read_all_stats: Read all pg_stat_* views and use various statistics-related extensions, even those normally visible only to superusers.\n• pg_stat_scan_tables: Execute monitoring functions that may take ACCESS SHARE locks on tables, potentially for a long time.\n\nUse the monitoring roles to give elevated privileges to the roles you use to monitor your database servers to avoid the need to give them superuser access. Ensure that your roles have the minimum privileges required to do what you need.\n\nAs part of examining the security setup of your Postgres deployment, it is important to look at data access control and how we can prevent users from accessing data they should not be able to.\n\nAccess Control Lists or ACLs are somewhat cryptic strings attached to objects such as tables, functions, views, and even columns in Postgres. They contain a list of privileges such as select, insert, execute, and so on that are granted to each role, as well as an additional optional flag (*) for each privilege that, if present, denotes that the role has the ability to grant this privilege to other roles, and the name of the role that granted the privileges.\n\nAn example of an ACL for a table created by Joe might be as follows:\n\nThe first section tells us that Joe has all the available privileges on the table (INSERT, SELECT, UPDATE, DELETE, TRUNCATE, REFERENCES, and TRIGGER), originally granted by Joe (when he created the table).\n\nThe second section tells us that read access has been granted to PUBLIC (a special pseudo role that means everyone) by Joe, and the third section tells us that the Sales Team has been granted INSERT, SELECT, and UPDATE privileges, again, by Joe.\n\nThe privilege flags in ACLs vary quite significantly based on the type of object in question; please review the documentation for further details.\n\nIt's useful to understand how ACLs are written in Postgres, particularly if you prefer working with command line tools, which will typically show them in the internal format. Graphical tools such as pgAdmin will parse and display the ACL in a visual format that is much easier to read.\n\nAny well-designed system should use roles in conjunction with ACLs to protect the schema and data in the database. It is good practice to have the schema (i.e., the tables and other objects) be owned by a non-superuser role that is not a role that the application uses to connect to the database or to grant other privileges to login roles.\n\nCreate group roles that reflect the permissions or roles within your application that have the required database privileges and grant those roles to login roles as required. It is not usually a good idea to grant privileges directly to login roles used by end users, as that can quickly become difficult to manage.\n\nSpend time fully understanding the privileges required in your system for users and applications to be able to do their jobs. Minimize privileges to only those required, separate schema ownership from data, and make use of group roles to simplify privilege management for individual login roles.\n\nACLs are managed on objects in Postgres using the GRANT and REVOKE SQL commands. In most cases, when an object is created, only the owner has any privileges to use or work with that object in any way, exceptions being that PUBLIC is granted EXECUTE permission on functions and procedures, CONNECT and TEMPORARY permission on databases, and USAGE permission on languages, data types, and domains. Any of these privileges can be revoked if required.\n\nPermission to modify or drop an object is always reserved for the owner of the object and superusers. The object ownership can be reassigned using the ALTER SQL command.\n\nDefault privileges can be overridden using the ALTER DEFAULT PRIVILEGES command for some object types. This allows you to configure the system such that certain privileges are automatically granted to roles when new objects are created. For instance, Joe, in the previous example, could issue a command such as the one below to grant the Sales Team insert, select, and update privileges on any new tables (but not preexisting ones, which may need to be updated manually):\n\nAssuming that when a new object is created it doesn't automatically include the required privileges in the ACL, we can use GRANT and REVOKE to set up the ACL as required. To continue our previous example, Joe might use the following SQL command to grant the Sales Team permissions on the orders table:\n\nTo revoke any automatically granted privileges or to revoke previously granted privileges to meet changing business needs, we can use the REVOKE SQL command:\n\nAssuming the Sales Team previously had the INSERT, SELECT, and UPDATE privileges, as seen in the earlier example, this would remove the UPDATE privilege, allowing them to view and add orders but not modify them.\n\nIt is worth noting that the use of ACLs on columns can sometimes catch people out because the wildcard in a SELECT * FROM query will not exclude the columns that users don't have access to and will return an access denied message for the table. In such cases, the user should explicitly list the columns from which they have permission to SELECT.\n\nHaving created group roles to organize login users, use the GRANT and REVOKE SQL commands to give the group roles the minimum level of privilege required to work. Use default privileges where appropriate as a time-saver, but be careful that doing so doesn't give more privileges than are appropriate in the future. Use GRANT to give the privileges to the required login roles by making them members of the group roles.\n\nRow Level Security or RLS is a technology available in Postgres that allows you to define policies that limit the visibility of rows in a table to certain roles. Before we dive into the details of how an RLS policy can be set up, there are two important caveats to note:\n• Superusers and roles with the BYPASSRLS attribute always bypass row level security policies, as do table owners, unless they force the policy on themselves.\n• The existence of a row may be inferred by a user through \"covert channels.\" For example, a unique constraint on a field such as a social security number might prevent the user inserting another row with the same value. The user cannot access the row, but they can infer that a record with that social security number already exists.\n\nBy default, row level security is turned off on tables in Postgres. It can be enabled with a command such as ALTER TABLE...ENABLE ROW LEVEL SECURITY, which will enable a restrictive policy preventing access to all data unless or until other policies have been created.\n\nThe policy itself consists of a name, the table to which the policy applies, the optional role to which it applies, and the USING clause, which defines how matching or allowed rows will be identified. For example, we might limit access to orders to the Sales Team member who created them:\n\nWe can also specify operations to which the policy applies. The following example would allow all members of the Sales Team to select any orders, but only the original salesperson would be able to update or delete an order:\n\nBy default, permissive policies are used, meaning that where there are multiple policies that apply, they are combined using a boolean OR. It is also possible to use restrictive policies, where a boolean AND is used when evaluating whether access to a row satisfies the combined policies.\n\nRow Level Security policies can take some effort to set up, and index design must also consider them, but there are cases when it may be essential to do so, such as in a medical records system where it may be a legal requirement to restrict access to patient records to the medical staff that are directly responsible for the patient's care.\n\nConsider the legal and ethical requirements to restrict access to specific rows in each of your tables, and design and implement RLS policies to meet those requirements where necessary. Take care to minimize covert channels by avoiding the use of sensitive data in constraints.\n\nViews are obviously useful for encapsulating commonly executed queries into an object that can be queried as if it were also a table, but they can also be useful for preventing unauthorized access to data by ensuring that roles do not have the ability to select from the underlying tables and have to access the data from the view instead. A classic example is part of Postgres; the pg_catalog.pg_authid table contains a row for each role in the database, including a column containing the hash of the password for the role if it's been set. Because the hash is considered sensitive information, the table does not have SELECT privileges for any roles other than the superuser that the database was initialized with.\n\nA view (pg_catalog.pg_roles) is provided instead, which can be selected from by any user. When selecting from the view, the password is always returned as ********. This is arguably more convenient than simply using an ACL on the password column in the underlying table as that would cause a permissions error if queried with SELECT * FROM.\n\nWhen using updateable views, a CHECK OPTION is available when defining the view. When omitted, the view will allow the user to insert or update records such that they wouldn't be visible through the view. Otherwise the insert or update will only be allowed if the row would be visible to the user. If LOCAL CHECK OPTION is specified, row visibility is checked only against conditions on the view being used directly, but when CASCADED CHECK OPTION is used (the default, if CHECK OPTION is specified), row visibility is checked against the view being used directly as well as any other underlying views.\n\nConsider using views over secured tables as a method of allowing access to a limited subset of the columns in the underlying table to appropriate roles.\n\nUsing views to restrict access to a column is quite common, but people often also use them to restrict access to certain rows. While there is certainly value in doing that, one must be mindful of one particularly nasty side effect; it's possible to trick the Postgres optimizer into leaking the hidden data!\n\nThis is not actually a bug; it's the way the system is intended to work. Essentially, what can happen is that when a query against a view is executed by the user, and the user includes a call to a very low-cost function in that outer query, the optimizer may choose to run the query for every row in the data underlying the view, before it applies the selectivity clauses in the view, thus allowing the function to access the restricted data. This is demonstrated nicely in a blog post by my colleague Robert Haas.\n\nTo solve this problem, we use a security barrier, which is basically an option that is passed when the view is created. This tells Postgres to always execute the qualifiers on the view first, thus ensuring that the function never sees the hidden rows.\n\nRelated to security barriers is the LEAKPROOF parameter for functions. This can only be used by superusers when creating a function and serves to certify that the function doesn't leak any information besides the intended return value. This allows Postgres to better optimize queries where a function is used with a security barrier view, safe in the knowledge that the function won't leak any information.\n\nBe careful when using views to hide rows to ensure that they are marked as security barriers to avoid leaking data. Consider whether RLS might be a better solution for limiting access to specific rows.\n\nBy default, functions and procedures in Postgres are what we call SECURITY INVOKER functions. That means that when they are called, they execute with the privileges of the calling role.\n\nPassing the SECURITY DEFINER option when creating the function means that whenever the function is called, it will be executed with the privileges of the owner instead of the calling role. This is similar to the setuid bit in a Unix file ACL, which when set will allow an executable to run with the permissions of its owner instead of the user that executed it.\n\nThis ability can be useful in various situations. One example might be a function that is called by a trigger on a table to write a record to an audit log, which prevents all login and group roles from accessing in any way. It is important to carefully consider the consequences of using SECURITY DEFINER functions though – in particular, ensure that they are kept as simple as possible and perform only a single task without taking any parameters that may allow them to be used for other purposes for which they were not intended.\n\nConsider using SECURITY DEFINER functions to provide specific functionality to roles that cannot perform that task directly themselves. Be careful to consider the possible ramifications and ways in which such functions could be misused, and ensure they are limited to performing only the intended task.\n\nData redaction is a technique that hides specific pieces of sensitive information from users by dynamically changing the value that is displayed. While this can be done to some extent with views in Postgres as described above, EDB Postgres Advanced Server includes native data redaction functionality.\n\nRedaction is implemented in EPAS using data redaction policies on tables. In short, these policies specify one or more columns on a table to operate on, an expression that determines whether the policy should be applied, a function to perform the redaction, a scope, and any exceptions to the policy. See the documentation link above for an example of how policies can be created.\n\nWhen using EDB Postgres Advanced Server and working with sensitive data such as credit card numbers, consider using data redaction policies to dynamically change the data displayed to a redacted form such as \"XXXX XXXX XXXX XXXX 8397\" to prevent users from having access to sensitive data unnecessarily.\n\nThe final component to consider when executing end-to-end security for Postgres is encrypting sensitive data. There are several methods and extensions available that can be configured to provide additional security through encryption.\n\npgcrypto is a standard extension of Postgres and EPAS that is included as a contrib module in the source tree and most binary distributions. Its purpose is to provide SQL functions for encryption and hashing that can be utilized as part of the logic in your database design.\n\nWith most binary distributions of Postgres, pgcrypto can be installed by first ensuring that the contrib modules are installed on the server. Installer-based distributions such as those for Windows and macOS are typically installed as part of the database server itself. Linux packages such as Debian/Ubuntu's DEBs and Redhat/SUSE RPMs may include them in a sub-package. For example, the PostgreSQL Community's packages from yum.postgresql.org have a package called postgresql12-contrib for PostgreSQL 12.\n\nOnce the package is installed on your server, simply run the CREATE EXTENSION command in the desired database as a superuser:\n\nConsider using the pgcrypto extension in your databases when you require the ability to hash or encrypt individual pieces of data to meet regulatory and similar requirements.\n\nHashing is a method of generating a cryptographically secure representation of a piece of data, typically of a fixed length (the size of which is dependent on the algorithm used). Importantly, it is non-reversible; that is, the original data cannot be extracted from the hash value – however because the hashed value is unique to the original data, it can be used as a checksum to see if the data has been changed or to see if a user-provided value matches the original value.\n\nHashing is mostly used to store passwords and other sensitive information that may need to be verified but not returned.\n\nAs an example, we can use pgcrypto to hash a password that the user will use in the future:\n\nTo verify this password later we can SELECT the user record from the table:\n\nIf a record is returned, the password was entered correctly – otherwise, it was incorrect.\n\nIt is important to note that when passwords are included in SQL commands, as shown above, they may end up being written to log files on the database server. Network communications may also leak commands like these if not protected with encryption.\n\nNever store user passwords in plain text or obfuscated form in the database, and never use a reversible encrypted form unless the functionality of the application absolutely requires it (for example, if writing a password manager application). Use non-reversible hashing wherever possible for application passwords and other information that must be verified but not returned.\n\npgcrypto also provides functionality for encrypting data that is useful when storing information that needs to be retrieved but should be stored in a secure form. There are \"raw\" encryption/decryption functions provided with pgcrypto as well as PGP functions.\n\nThe PGP functions are strongly encouraged over use of the raw functions, which use a user-provided key directly as the cipher key, provide no integrity checking, expect the user to manage all encryption parameters, and work with bytea data, not text.\n\nSymmetric key encryption is the easiest to use as it doesn't require a PGP key. For example, we can demonstrate encryption and decryption of data as shown with this simple SQL command in which the inner function call encrypts the data and the outer one decrypts it:\n\nNote that the cipher text returned by the encryption function and passed to the decryption function is in bytea format.\n\nTo use public key functionality, a key is first required. This can be generated using GnuPG with a command such as:\n\nThe PostgreSQL documentation suggests that the preferred key type is \"DSA and Elgamal.\" Once the key is generated, you'll need to export it:\n\nThe public key can now be used to encrypt data using the SQL encryption function:\n\nSimilarly, the data can later be decrypted using:\n\nAgain, note that the cipher text is in bytea format.\n\nEncryption is used when storing sensitive data in the database that need to be retrieved in the future. Consider carefully whether symmetric or public key encryption is most appropriate for your use case. Public key generally makes more sense when exchanging data with others (because there's no shared secret), while symmetric may make more sense for a self-contained application.\n\nOne major issue with the use of encryption in a database is key management. In its simplest form, an application may have a hard-coded or centrally configured key that it uses when encrypting and decrypting data. Unless the application can change the key (which may also be expensive if there's a lot of data to re-encrypt), then that key will be valid for the lifetime of the application, and it also means that all users are sharing one single key. These factors greatly increase the chances of that key being known to multiple people (e.g., the administrators of the application), some of whom may leave the organization, taking that knowledge with them.\n\nKey management systems alleviate some of these problems by offering ways to store keys in a secure service separately from the database and application and to potentially use different keys for different users or purposes. Some, such as Bruce Momjian's pgcryptokey extension, also offer functionality for re-encrypting through SQL commands. That can still be expensive, of course, but the extension does make it trivial to do.\n\nKey management systems can also avoid the need for users to ever see the actual keys; their access to the keys can be controlled through a password or passphrase (which may be authenticated using Kerberos or a similar enterprise identity management system), with the key itself being passed directly to the database server or application as needed.\n\nAt the time of writing there is an ongoing discussion in the PostgreSQL community about the development of a key management system as a feature of the database server. Those who are interested in this feature or interested in seeing how features are discussed and added to PostgreSQL might want to read the original and current email threads.\n\nConsider whether the use of a key management system may be appropriate for managing your cryptographic keys to avoid the use of shared keys or to separate their storage from the application.\n\nWhen using file system encryption (or full disk encryption, as the benefits are essentially the same for the scope of this blog), we typically encrypt the volumes that are used to store the database and write-ahead log, or often the entire system. These types of encryption are transparent to the database server and require no configuration in Postgres.\n\nIt is important to note that the file system and data encryption in Postgres protect against different attack vectors. The operating system may make use of a password or key management system very early on in the boot phase to ensure that keys are kept externally, but once a server with file system encryption is booted and running with filesystems mounted, all the data is accessible in the same way as it would be on a machine without encryption.\n\nThis gives us protection against physical attacks on non-running hardware, such as a stolen hard disk. File system or full disk encryption does not protect against attacks on a system that is up and running, nor do they enable us to control the visibility of data in the database for different users.\n\nThere are different file system or full disk system encryption options available bundled with most operating systems, commercially and as open source products. Among the most common options are FileVault, which is included with Apple macOS, BitLocker for use with Microsoft Windows, and LUKS on Linux systems.\n\nEncrypted volumes are also available on all major cloud providers to protect data. For example, Amazon's Elastic Block Service (EBS) provides an option for creating encrypted volumes, which can use a default key or one provided through their key management system.\n\nIt's worth noting that Amazon does, of course, have access to both your keys and the physical devices on which the volumes are provisioned, but they go to lengths to ensure that there is the separation of duties between the staff that may have access to the keys and staff that may have access to the hardware.\n\nIt is a good idea to use a file system or full disk encryption on any computer to protect against physical loss of hardware, in your humble author's opinion. All popular operating systems have options available to allow this built in.\n\nIn this article, we've looked at the security of your Postgres implementation, from the client perspective to the on-disk storage. We saw how several factors related to server access can affect the security of your Postgres servers and that the following should be considered as part of any deployment or review:\n• Server access via Unix Domain Sockets and the network\n\nWe also saw how determining user authentication mechanisms to authenticate different connection attempts is critical to securing your Postgres deployment. Roles are also an important part of security in Postgres, and by configuring and securing them properly, we can use them to minimize the risk to our database servers using the principle of least privilege.\n\nTechniques for securing and minimizing access to sensitive data in Postgres require planning and careful design but can significantly improve the security of your data. Finally, we saw how encryption security for your Postgres deployment encompasses both data and file system/full disk encryption; it may also be desirable to integrate with a key management system.\n\nIn all aspects of this journey, there are options that are appropriate for one deployment, and other options that are appropriate for others. Additional functionality should also be considered, such as sepgsql – which can work with SELinux and could take an entire blog series to describe on its own!\n\nHopefully, this overview is helpful for reviewing the security of your deployments, but do remember that each deployment scenario is unique, and the suggestions made here are not a \"one size fits all\" solution."
    },
    {
        "link": "https://dbvis.com/thetable/a-definitive-guide-to-postgres-foreign-key",
        "document": "Great! We’re done experimenting with how some of these actions work. Time to look at some of the best practices for working with PostgreSQL foreign keys!\n\nBest Practices for Working With Postgres Foreign Key\n\nBelow are some of the best practices that should be taken into consideration when working with a Postgres foreign key:\n\nTo better appreciate its capabilities, you need a tool that helps you manage databases and visually explore query results. This is where a full-featured database client like DbVisualizer comes in. In addition to being able to connect to several DBMSs, it offers advanced query optimization functionality, and full support for all database features, including , and actions. Download DbVisualizer for free now!\n\nWhat are foreign keys, and why use them?\n\nForeign keys are like glue in your database, connecting related tables and preventing data inconsistencies. They ensure that, for example, an order always references a valid product, and a customer always has a valid address. Without them, you'd end up with \"orphaned\" records and messy data.\n\nHow do you define a foreign key in Postgres?\n\nYou use the constraint in your or statement. Just tell Postgres which columns in your table link to specific columns in another table (like referencing ).\n\nCan I delete a row in a parent table if it has child rows?\n\nIt depends on what you set for when defining the foreign key. You can choose to automatically delete child rows ( ), prevent deletion ( ), or even set the foreign key columns in child rows to null.\n\nWhat are the available actions for handling updates and deletions with foreign keys in PostgreSQL?\n• None : When a referenced row is updated or deleted, the foreign key column in the child table(s) is set to . This means that if a referenced row is updated or deleted, the corresponding foreign key values in the child table(s) will be set to .\n• None : This is the default action in PostgreSQL. If a referenced row is updated or deleted and there are still dependent rows in the child table(s), an error is raised, and the update or deletion is not allowed.\n• None : When a referenced row is updated or deleted, the changes are automatically propagated to the related rows in the child table(s). This means that if a referenced row is updated or deleted, all associated rows in the child table(s) will also be updated or deleted.\n• None : Similar to , if a referenced row is updated or deleted and there are still dependent rows in the child table(s), an error is raised, and the update or deletion is not allowed. is the same as and is included for compatibility with other database systems.\n• None : When a referenced row is updated or deleted, the foreign key column in the child table(s) is set to its default value. This means that if a referenced row is updated or deleted, the corresponding foreign key values in the child table(s) will be set to their default values, if any are defined."
    }
]