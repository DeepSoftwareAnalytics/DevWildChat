[
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html",
        "document": "The levels in the pivot table will be stored in MultiIndex objects (hierarchical indexes) on the index and columns of the result DataFrame.\n\nindex column, Grouper, array, or list of the previous Keys to group by on the pivot table index. If a list is passed, it can contain any of the other types (except list). If an array is passed, it must be the same length as the data and will be used in the same manner as column values. columns column, Grouper, array, or list of the previous Keys to group by on the pivot table column. If a list is passed, it can contain any of the other types (except list). If an array is passed, it must be the same length as the data and will be used in the same manner as column values. If a list of functions is passed, the resulting pivot table will have hierarchical columns whose top level are the function names (inferred from the function objects themselves). If a dict is passed, the key is column to aggregate and the value is function or list of functions. If , aggfunc will be used to calculate the partial aggregates. Value to replace missing values with (in the resulting pivot table, after aggregation). If , special columns and rows will be added with partial group aggregates across the categories on the rows and columns. Do not include columns whose entries are all NaN. If True, rows with a NaN value in any column will be omitted before computing margins. Name of the row / column that will contain the totals when margins is True. This only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. Deprecated since version 2.2.0: The default value of is deprecated and will change to in a future version of pandas. Specifies if the result should be sorted.\n\nReference the user guide for more examples.\n\nThis first example aggregates values by taking the sum.\n\nWe can also fill missing values using the parameter.\n\nThe next example aggregates by taking the mean across multiple columns.\n\nWe can also calculate multiple types of aggregations for any given value column."
    },
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot_table.html",
        "document": "The levels in the pivot table will be stored in MultiIndex objects (hierarchical indexes) on the index and columns of the result DataFrame.\n\nindex column, Grouper, array, or list of the previous Keys to group by on the pivot table index. If a list is passed, it can contain any of the other types (except list). If an array is passed, it must be the same length as the data and will be used in the same manner as column values. columns column, Grouper, array, or list of the previous Keys to group by on the pivot table column. If a list is passed, it can contain any of the other types (except list). If an array is passed, it must be the same length as the data and will be used in the same manner as column values. If a list of functions is passed, the resulting pivot table will have hierarchical columns whose top level are the function names (inferred from the function objects themselves). If a dict is passed, the key is column to aggregate and the value is function or list of functions. If , aggfunc will be used to calculate the partial aggregates. Value to replace missing values with (in the resulting pivot table, after aggregation). If , special columns and rows will be added with partial group aggregates across the categories on the rows and columns. Do not include columns whose entries are all NaN. If True, rows with a NaN value in any column will be omitted before computing margins. Name of the row / column that will contain the totals when margins is True. This only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. Deprecated since version 2.2.0: The default value of is deprecated and will change to in a future version of pandas. Specifies if the result should be sorted.\n\nReference the user guide for more examples.\n\nThis first example aggregates values by taking the sum.\n\nWe can also fill missing values using the parameter.\n\nThe next example aggregates by taking the mean across multiple columns.\n\nWe can also calculate multiple types of aggregations for any given value column."
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/reshaping.html",
        "document": "pandas provides methods for manipulating a and to alter the representation of the data for further data processing or data summarization.\n• None and : Group unique values within one or more discrete categories.\n• None and : Pivot a column or row level to the opposite axis respectively.\n\nData is often stored in so-called “stacked” or “record” format. In a “record” or “wide” format, typically there is one row for each subject. In the “stacked” or “long” format there are multiple rows for each subject where applicable. To perform time series operations with each unique variable, a better representation would be where the are the unique variables and an of dates identifies individual observations. To reshape the data into this form, we use the method (also implemented as a top level function ): If the argument is omitted, and the input has more than one column of values which are not used as column or index inputs to , then the resulting “pivoted” will have hierarchical columns whose topmost level indicates the respective value column: You can then select subsets from the pivoted : Note that this returns a view on the underlying data in the case where the data are homogeneously-typed. can only handle unique rows specified by and . If you data contains duplicates, use . While provides general purpose pivoting with various data types, pandas also provides or for pivoting with aggregation of numeric data. The function can be used to create spreadsheet-style pivot tables. See the cookbook for some advanced strategies. A one three ... three two A one three two one three two The result is a potentially having a on the index or column. If the column name is not given, the pivot table will include all of the data in an additional level of hierarchy in the columns: Also, you can use for and keywords. For detail of , see Grouping with a Grouper specification. Passing to will add a row and column with an label with partial group aggregates across the categories on the rows and columns: C bar foo All bar foo All Additionally, you can call to display a pivoted DataFrame as having a multi-level index:\n\nClosely related to the method are the related and methods available on and . These methods are designed to work together with objects (see the section on hierarchical indexing).\n• None : “pivot” a level of the (possibly hierarchical) column labels, returning a with an index with a new inner-most level of row labels.\n• None : (inverse operation of ) “pivot” a level of the (possibly hierarchical) row index to the column axis, producing a reshaped with a new inner-most level of column labels. The function “compresses” a level in the columns to produce either:\n• None A , in the case of a in the columns.\n• None A , in the case of a in the columns. If the columns have a , you can choose which level to stack. The stacked level becomes the new lowest level in a on the columns: With a “stacked” or (having a as the ), the inverse operation of is , which by default unstacks the last level: If the indexes have names, you can use the level names instead of specifying the level numbers: Notice that the and methods implicitly sort the index levels involved. Hence a call to and then , or vice versa, will result in a sorted copy of the original or : You may also stack or unstack more than one level at a time by passing a list of levels, in which case the end result is as if each level in the list were processed individually. The list of levels can contain either level names or level numbers but not a mixture of the two. # from above is equivalent to: Unstacking can result in missing values if subgroups do not have the same set of labels. By default, missing values will be replaced with the default fill value for that data type. The missing value can be filled with a specific value with the argument.\n\nThe top-level function and the corresponding are useful to massage a into a format where one or more columns are identifier variables, while all other columns, considered measured variables, are “unpivoted” to the row axis, leaving just two non-identifier columns, “variable” and “value”. The names of those columns can be customized by supplying the and parameters. first last variable value first last quantity value When transforming a DataFrame using , the index will be ignored. The original index values can be kept by setting the parameter to (default is ). will however duplicate index values. first last variable value first last variable value is similar to with more customization for column matching.\n\nTo convert categorical variables of a into a “dummy” or “indicator”, creates a new with columns of the unique variables and the values representing the presence of those variables per row. adds a prefix to the the column names which is useful for merging the result with the original : This function is often used along with discretization functions like : also accepts a . By default, , , or type columns are encoded as dummy variables with other columns unaltered. Specifying the keyword will encode a column of any type. As with the version, you can pass values for the and . By default the column name is used as the prefix and as the prefix separator. You can specify and in 3 ways:\n• None string: Use the same value for or for each column to be encoded.\n• None list: Must be the same length as the number of columns being encoded. To avoid collinearity when feeding the result to statistical models, specify . When a column contains only one level, it will be omitted in the result. The values can be cast to a different type using the argument. converts the output of back into a of categorical values from indicator values. Dummy coded data only requires categories to be included, in this case the last category is the default category. The default category can be modified with .\n\nFor a column with nested, list-like values, will transform each list-like value to a separate row. The resulting will be duplicated corresponding to the index label from the original row: can also explode the column in the . will replace empty lists with a missing value indicator and preserve scalar entries. A comma-separated string value can be split into individual values in a list and then exploded to a new row.\n\nUse to compute a cross-tabulation of two (or more) factors. By default computes a frequency table of the factors unless an array of values and an aggregation function are passed. Any passed will have their name attributes used unless row or column names for the cross-tabulation are specified If receives only two , it will provide a frequency table. can also summarize to data. For data, to include all of data categories even if the actual data does not contain any instances of a particular category, use . Frequency tables can also be normalized to show percentages rather than counts using the argument: can also normalize values within each row or within each column: can also accept a third and an aggregation function ( ) that will be applied to the values of the third within each group defined by the first two : will add a row and column with an label with partial group aggregates across the categories on the rows and columns:"
    },
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html",
        "document": "Reshape data (produce a “pivot” table) based on column values. Uses unique values from specified / to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the User Guide for more on reshaping.\n\ncolumns str or object or a list of str Column to use to make new frame’s columns. index str or object or a list of str, optional Column to use to make new frame’s index. If not given, uses existing index. values str, object or a list of the previous, optional Column(s) to use for populating new frame’s values. If not specified, all remaining columns will be used and the result will have hierarchically indexed columns. When there are any , combinations with multiple values. when you need to aggregate.\n\nFor finer-tuned control, see hierarchical indexing documentation along with the related stack/unstack methods.\n\nReference the user guide for more examples.\n\nYou could also assign a list of column names or a list of index names.\n\nA ValueError is raised if there are any duplicates.\n\nNotice that the first two rows are the same for our and arguments."
    },
    {
        "link": "https://builtin.com/data-science/pandas-pivot-tables",
        "document": "The pandas library is a popular Python package for data analysis. When initially working with a data set in pandas, the structure will be two-dimensional, consisting of rows and columns, which are also known as a DataFrame. An important part of data analysis is the process of grouping, summarizing, aggregating and calculating statistics about this data. Pandas pivot tables provide a powerful tool to perform these analysis techniques with Python.\n\nIf you are a spreadsheet user then you may already be familiar with the concept of pivot tables. Pandas pivot tables work in a very similar way to those found in spreadsheet tools such as Microsoft Excel. The pivot table function takes in a data frame and the parameters detailing the shape you want the data to take. Then it outputs summarized data in the form of a pivot table.\n\nI will give a brief introduction with code examples to the pandas pivot table tool. I’ll then use a data set called “autos,” which contains a range of features about cars, such as the make, price, horsepower and miles per gallon.\n\nYou can download the data from OpenML, or the code can be imported directly into your code using the scikit-learn API as shown below.\n• Values: These are the numerical values you are looking to summarize.\n\nThe code used to create the pivot table can be seen below. In the function, we specify the DataFrame we are summarizing, and then the column names for the values, index and columns. Additionally, we specify the type of calculation we want to use. In this case, we’re computing the mean.\n\nPivot tables can be multi-level. We can use multiple indexes and column level groupings to create more powerful summaries of a data set.\n\nMore on PandasSorting Data Frames in Pandas: A Hands-On Guide\n\nHow to Plot with Pandas Pivot Table\n\nPandas pivot tables can be used in conjunction with the pandas plotting functionality to create useful data visualizations.\n\nSimply adding to the end of your pivot table code will create a plot of the data. As an example, the below code creates a bar chart showing the mean car price by make and number of doors.\n\nHow to Calculate With Pandas Pivot Table\n\nThe argument in the pivot table function can take in one or more standard calculations.\n\nThe following code calculates the mean and median price for car body style and the number of doors.\n\nYou can add the argument to add totals to columns and rows. You can also specify a name for the totals using .\n\nHow to Style Your Pandas Pivot Table\n\nWhen summarizing data, styling is important. We want to ensure that the patterns and insights that the pivot table is providing are easy to read and understand. In the pivot tables used in earlier parts of the article, very little styling has been applied. As a result, the tables are not easy to understand or visually appealing.\n\nWe can use another Pandas method, known as the style method to make the tables look prettier and easier to draw insights from. The code below adds appropriate formatting and units of measurement to each of the values used in this pivot table. It is now much easier to distinguish between the two columns and to comprehend what the data is telling you.\n\nWe can combine different formats using the styler and use the pandas built-in styles to summarize data in a way that instantly draws insights out. In the code and pivot table shown below, we have ordered the make of the car by price from high to low value, added appropriate formatting to the numbers and added a bar chart overlaying the values in both columns. This makes it easier to draw conclusions from the table, such as which make of car is the most expensive and how horsepower relates to the price for each car make.\n\nMore on PandasA Beginner’s Guide to Using Pandas for Text Data Wrangling With Python\n\nPivot tables have been in use since the early ’90s with Microsoft patenting the famous Excel version known as “PivotTable” in 1994. They are still widely used today because they are such a powerful tool for analyzing data. The Pandas pivot table brings this tool out of the spreadsheet and into the hands of Python users.\n\nThis guide gave a brief introduction to the usage of the pivot table tool in Pandas. It is meant to give a beginner a quick tutorial to get up and running with the tool but I suggest digging into the pandas documentation, which gives a more in-depth guide to this function."
    },
    {
        "link": "https://stackoverflow.com/questions/22923775/calculate-time-difference-between-two-pandas-columns-in-hours-and-minutes",
        "document": "I have two columns, and , in a dataframe.\n\nI add a new column, , to find the difference between the two dates using\n\nI get the column, but it contains , when there's more than 24 hours.\n\nHow do I convert my results to only hours and minutes (i.e. days are converted to hours)?"
    },
    {
        "link": "https://stackoverflow.com/questions/29091468/pandas-time-series-time-between-events",
        "document": "How can I calculate the time (number of days) between \"events\" in a Pandas time series? For example, if I have the below time series I'd like to know on each day in the series how many days have passed since the last\n\nThe way I've done it seems overcomplicated, so I'm hoping for something more elegant. Obviously a for loop iterating over the rows would work, but I'm looking for a vectorized (scalable) solution ideally. My current attempt below:"
    },
    {
        "link": "https://pandas.pydata.org/docs/getting_started/intro_tutorials/09_timeseries.html",
        "document": "Aggregate the current hourly time series values to the monthly maximum value in each of the stations.\n\nA very powerful method on time series data with a datetime index, is the ability to time series to another frequency (e.g., converting secondly data into 5-minutely data)."
    },
    {
        "link": "https://saturncloud.io/blog/how-to-calculate-the-time-difference-between-two-consecutive-rows-in-pandas",
        "document": "How to Calculate the Time Difference Between Two Consecutive Rows in Pandas\n\nIn this blog, discover how to tackle the complex task of calculating time differences between consecutive rows in a pandas DataFrame, a common challenge for data scientists and software engineers when working with extensive datasets.\n\nAs a data scientist or software engineer, you may come across a situation where you need to calculate the time difference between two consecutive rows in a pandas DataFrame. This can be a challenging task, especially when dealing with large datasets. In this article, we will explore how to calculate the time difference between two consecutive rows in pandas.\n\nPandas is a popular open-source Python library used for data manipulation and analysis. It provides data structures for efficiently storing and manipulating large datasets, including data frames and series. Pandas is widely used in data science and machine learning applications.\n\nSuppose you have a pandas DataFrame that contains a timestamp column. You want to calculate the time difference between two consecutive rows in this DataFrame. For example, you have a DataFrame that contains the timestamps of user logins, and you want to calculate the time difference between each login.\n\nTo solve this problem, we can use pandas function. The function calculates the difference between two consecutive rows in a DataFrame. We can use this function to calculate the time difference between consecutive timestamps in a DataFrame.\n\nHere’s an example of how to use the function to calculate the time difference between two consecutive rows in a pandas DataFrame:\n\nIn this example, we create a sample DataFrame with a timestamp column and a value column. We convert the timestamp column to datetime format using pandas' function. Then we use the function to calculate the time difference between consecutive timestamps and store the result in a new column called .\n\nThe output of this code will be:\n\nAs you can see, the function calculates the time difference between consecutive timestamps and stores the result in the column. The first row of the column is (Not a Time), which indicates that there is no time difference between the first and second rows.\n\nIn some cases, the function may return missing values (NaN) if there is a missing value in the original DataFrame. To deal with missing values, we can use the function to replace them with a default value.\n\nHere’s an example of how to use the function to replace missing values with a default value:\n\nIn this example, we create a sample DataFrame with a missing value in the timestamp column. After using pandas function, it yeilds values as show above. To solve this problem, we can use the function to replace missing values with a default value of 0 seconds.\n\nAs you can see, the missing value in the timestamp column is replaced with a default value of 0 seconds.\n\nIn this article, we have explored how to calculate the time difference between two consecutive rows in a pandas DataFrame. We have learned how to use the function to calculate the time difference between consecutive timestamps and how to deal with missing values using the function. By using these techniques, you can efficiently calculate the time difference between consecutive rows in a pandas DataFrame and perform various time-based analyses.\n\nSaturn Cloud is your all-in-one solution for data science & ML development, deployment, and data pipelines in the cloud. Spin up a notebook with 4TB of RAM, add a GPU, connect to a distributed cluster of workers, and more. Request a demo today to learn more."
    },
    {
        "link": "https://datacamp.com/tutorial/timedelta-python-time-intervals",
        "document": "Master the basics of data analysis with Python in just four hours. This online course will introduce the Python interface and explore popular packages."
    }
]