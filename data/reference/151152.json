[
    {
        "link": "https://developer.mozilla.org/en-US/docs/WebAssembly/Guides/C_to_Wasm",
        "document": "When you've written a new code module in a language like C/C++, you can compile it into WebAssembly using a tool like Emscripten. Let's look at how it works.\n\nWith the environment set up, let's look at how to use it to compile a C example to Wasm. There are a number of options available when compiling with Emscripten, but the main two scenarios we'll cover are:\n• Compiling to Wasm and creating HTML to run our code in, plus all the JavaScript \"glue\" code needed to run the Wasm in the web environment.\n• Compiling to Wasm and just creating the JavaScript. We will look at both below.\n\nThis is the simplest case we'll look at, whereby you get emscripten to generate everything you need to run your code, as WebAssembly, in the browser.\n• First we need an example to compile. Take a copy of the following simple C example, and save it in a file called in a new directory on your local drive:\n• Now, using the terminal window you used to enter the Emscripten compiler environment, navigate to the same directory as your file, and run the following command: The options we've passed in with the command are as follows:\n• — Specifies that we want Emscripten to generate an HTML page to run our code in (and a filename to use), as well as the Wasm module and the JavaScript \"glue\" code to compile and instantiate the Wasm so it can be used in the web environment. At this point in your source directory you should have:\n• A JavaScript file containing glue code to translate between the native C functions, and JavaScript/Wasm ( )\n• An HTML file to load, compile, and instantiate your Wasm code, and display its output in the browser ( )\n\nNow all that remains is for you to load the resulting in a browser that supports WebAssembly. It is enabled by default from Firefox 52, Chrome 57, Edge 57, Opera 44. Note: If you try to open generated HTML file ( ) directly from your local hard drive (e.g. ), you will end up with an error message along the lines of both async and sync fetching of the wasm failed . You need to run your HTML file through an HTTP server ( ) — see How do you set up a local testing server? for more information. If everything has worked as planned, you should see \"Hello world\" output in the Emscripten console appearing on the web page, and your browser's JavaScript console. Congratulations, you've just compiled C to WebAssembly and run it in your browser!\n\nSometimes you will want to use a custom HTML template. Let's look at how we can do this.\n• First of all, save the following C code in a file called , in a new directory:\n• Search for the file in your emsdk repo. Copy it into a subdirectory called inside your previous new directory.\n• Now navigate into your new directory (again, in your Emscripten compiler environment terminal window), and run the following command: The options we've passed are slightly different this time:\n• We've specified , meaning that the compiler will still output the JavaScript glue code and .\n• We've specified , which is used to optimize the code. Emcc has optimization levels like any other C compiler, including: (no optimization), , , , , , and . is a good setting for release builds.\n• We've also specified — this provides the path to the HTML template you want to use to create the HTML you will run your example through.\n• Now let's run this example. The above command will have generated , which will have much the same content as the template with some glue code added into load the generated Wasm, run it, etc. Open it in your browser and you'll see much the same output as the last example. Note: You could specify outputting just the JavaScript \"glue\" file* rather than the full HTML by specifying a .js file instead of an HTML file in the flag, e.g. . You could then build your custom HTML completely from scratch, although this is an advanced approach; it is usually easier to use the provided HTML template.\n• Emscripten requires a large variety of JavaScript \"glue\" code to handle memory allocation, memory leaks, and a host of other problems\n\nIf you want to call a function defined in your C code from JavaScript, you can use the Emscripten function and the declaration, which adds your functions to the exported functions list (see Why do functions in my C/C++ source code vanish when I compile to JavaScript, and/or I get No functions to process?). Let's look at how this works.\n• To start with, save the following code as in a new directory: By default, Emscripten-generated code always just calls the function, and other functions are eliminated as dead code. Putting before a function name stops this from happening. You also need to import the library to use . Note: We are including the blocks so that if you are trying to include this in C++ code, the example will still work. Due to C versus C++ name mangling rules, this would otherwise break, but here we are setting it so that it treats it as an external C function if you are using C++.\n• Now add with as content into this new directory too, just for convenience (you'd obviously put this in a central place in your real dev environment).\n• Now let's run the compilation step again. From inside your latest directory (and while inside your Emscripten compiler environment terminal window), compile your C code with the following command. Note that we need to compile with : otherwise, when exits, the runtime would be shut down and it wouldn't be valid to call compiled code. This is necessary for proper C emulation: for example, to ensure that functions are called.\n• If you load the example in your browser again, you'll see the same thing as before!\n• Now we need to run our new function from JavaScript. First of all, open up your hello3.html file in a text editor.\n• Add a element as shown below, just above the first opening tag.\n• Now add the following code at the end of the first element: document.getElementById(\"my-button\").addEventListener(\"click\", () => { alert(\"check console\"); const result = Module.ccall( \"myFunction\", // name of C function null, // return type null, // argument types null, // arguments ); }); This illustrates how is used to call the exported function."
    },
    {
        "link": "https://emscripten.org",
        "document": "Emscripten is a complete compiler toolchain to WebAssembly, using LLVM, with a special focus on speed, size, and the Web platform.\n\nCompile your existing projects written in C or C++ — or any language that uses LLVM — to browsers, Node.js, or wasm runtimes. Emscripten converts OpenGL into WebGL, and has support for familiar APIs like SDL, pthreads, and POSIX, as well as Web APIs and JavaScript. Thanks to the combination of LLVM, Emscripten, Binaryen, and WebAssembly, the output is compact and runs at near-native speed.\n\nInterested to learn more? Read our About Page!\n\nReady to get started? Download and install the SDK and then proceed to the Tutorial!"
    },
    {
        "link": "https://emscripten.org/docs/compiling/WebAssembly.html",
        "document": "WebAssembly is a binary format for executing code on the web, allowing fast start times (smaller download and much faster parsing in browsers when compared to JS or asm.js). Emscripten compiles to WebAssembly by default, but you can also compile to JS for older browsers.\n\nFor some historical background, see these slides and this blogpost.\n\nWebAssembly is emitted by default, without the need for any special flags. If you don’t want WebAssembly, you can disable it with something like Deciding to compile to Wasm or JS can be done at the linking stage: it doesn’t affect the object files. Emscripten emits WebAssembly using the upstream LLVM Wasm backend, since version (October 2019). Previously emscripten also supported the old fastcomp backend which was removed in (August 2020). There are some differences you may notice between the two backends, if you upgrade from fastcomp to upstream:\n• None The Wasm backend is strict about linking files with different features sets - for example, if one file was built with atomics but another was not, it will error at link time. This prevents possible bugs, but may mean you need to make some build system fixes.\n• None behaves differently in the two backends. In fastcomp we emit asm.js, while in upstream we emit JS (since not all Wasm constructs can be expressed in asm.js). Also, the JS support implements the same external API, so in particular startup will be async just like Wasm by default, and you can control that with (even though ).\n• None The Wasm backend uses Wasm object files by default. That means that it does codegen at the compile step, which makes the link step much faster - like a normal native compiler. For comparison, in fastcomp the compile step emits LLVM IR in object files.\n• None You normally wouldn’t notice this, but some compiler flags affect codegen, like . Such flags must be passed during codegen. The simple and safe thing is to pass all flags at both compile and link time.\n• None You can enable Link Time Optimization (LTO) with the usual llvm flags ( , , , at both compile and link times; note, however, that thin LTO is not heavily tested atm and so regular LTO is recommended).\n• None With fastcomp, LTO optimization passes were not be run by default; for that it was necessary to pass . With the llvm backend LTO passes will be run on any object files that are in bitcode format.\n• None Another thing you might notice is that fastcomp’s link stage is able to perform some minor types of link time optimization even without LTO being set. The LLVM backend requires actually setting LTO for those things.\n• None , the linker used by the Wasm backend, requires libraries ( archives) to contain symbol indexes. This matches the behaviour the native GNU linker. While will create such indexes by default, native tools such as GNU and GNU are not aware of the WebAssembly object format and cannot create archive indexes. In particular, if you run GNU on an archive file that contains WebAssembly object files it will remove the index which makes the archive unusable at link time.\n• None Also see the blocker bugs on the Wasm backend, and the Wasm backend tagged issues.\n\nWebAssembly can trap - throw an exception - on things like division by zero, rounding a very large float to an int, and so forth. In asm.js such things were silently ignored, as in JavaScript they do not throw, so this is a difference between JavaScript and WebAssembly that you may notice, with the browser reporting an error like , , , or . The LLVM Wasm backend avoids traps by adding more code around each possible trap (basically clamping the value if it would trap). This can increase code size and decrease speed, if you don’t need that extra code. The proper solution for this is to use newer Wasm instructions that do not trap, by calling emcc or clang with . That code may not run in older VMs, though.\n\nWhen using to build to WebAssembly, you will see a file containing that code, as well as the usual file that is the main target of compilation. Those two are built to work together: run the (or , if that’s what you asked for) file, and it will load and set up the WebAssembly code for you, properly setting up imports and exports for it, etc. Basically, you don’t need to care about whether the compiled code is asm.js or WebAssembly, it’s just a compiler flag, and otherwise everything should just work (except the WebAssembly should be faster).\n• None Note that the file is not standalone - it’s not easy to manually run it without that code, as it depends on getting the proper imports that integrate with JS. For example, it receives imports for syscalls so that it can do things like print to the console. There is work in progress towards ways to create standalone files, see the WebAssembly Standalone page. You may also see additional files generated, like a file if you are preloading files into the virtual filesystem. All that is exactly the same as when building to asm.js. One difference you may notice is the lack of a , which for asm.js contains the static memory initialization data, which in WebAssembly we can pack more efficiently into the WebAssembly binary itself.\n\nSince its original launch, WebAssembly has been expanded with various feature extensions, which have been implmented in browsers. A list of features (including already-shipped and in-progress) and details about browser versions that support them can be found on webassembly.org. Several of these features can be used by Emscripten (or are by default) and can be enabled or disabled individually (using either Clang or emscripten flags) or by selecting which version of browsers Emscripten should target.\n• None Exception handling (see C++ Exceptions Support for details).\n• None SIMD (see Using SIMD with WebAssembly for details).\n• None Nontrapping float-to-int conversion (enabled by default, use to disable). Clang will generate nontrapping (saturating) float-to-int conversion instructions for C typecasts. This should have no effect on programs that do not have undefined behavior but if the casted floating-point value is outside the range of the target integer type, the result will be a number of the max or min value instead of a trap. This also results in a small code size improvement because of details of the LLVM IR semantics.\n• None Bulk memory operations (enabled by default, use to disable). and instructions are used in the implementation of C and , and Clang may generate them elsewhere.\n• None JS BigInt integration (enabled by default, use the setting to disable). This has the effect that Wasm i64 values are passed and returned between Wasm and JS as BigInt values rather than being split by Binaryen into pairs of Numbers.\n• None Sign-extension operators (enabled by default, use to disable). For the features that are enabled by default (or will be when sufficient browser support exists), it’s also possible to control them by specifying which browser versions you want to target. You can use the setting (and also , and ). Setting a value lower than the default version will disable features not supported by the specified version. Some features (e.g. Exception handling and threads) are not enabled by default because they have tradeoffs (e.g. binary size costs or restrictions on how the resulting wasm can be used such as COEP headers). These are not controlled by the browser version flags and must be enabled explicitly. See the settings page for details of the default browser versions Emscripten targets.\n\nWebAssembly code is prepared somewhat differently than asm.js. asm.js can be bundled inside the main JS file, while as mentioned earlier WebAssembly is a binary file on the side, so you will have more than one file to distribute. Another noticeable effect is that WebAssembly is compiled asynchronously by default, which means you must wait for compilation to complete before calling compiled code (by waiting for , or the callback, etc., which you also need to do when you have anything else that makes startup async, like a file for asm.js, or preloaded file data, etc.). You can turn off async compilation by setting , but that may not work in Chrome due to current limitations there.\n• None Note that even with async compilation turned off, fetching the WebAssembly binary may need to be an asynchronous operation (since the Web does not allow synchronous binary downloads on the main thread). If you can fetch the binary yourself, you can set and it will be used from there, and then (with async compilation off) compilation should be synchronous."
    },
    {
        "link": "https://emscripten.org/docs",
        "document": "This comprehensive documentation set contains everything you need to know to use Emscripten.\n• None Introducing Emscripten explains what Emscripten does, why it is needed, its limitations and its licensing. It will help you understand whether Emscripten is the right tool for you.\n• None Getting Started walks you through downloading, installing and using the Emscripten SDK.\n• None Porting illustrates the main differences between the native and Emscripten runtime environments, and explains the changes you need to make to prepare your C/C++ code for the Web.\n• None Optimizing Code shows how to optimise your code for size and performance.\n• None Optimizing WebGL gives tips for how to maximize WebGL rendering performance for your page.\n• None Compiling and Running Projects demonstrates how to integrate Emscripten into your existing project build system.\n• None Contributing to Emscripten explains how you can contribute to the project.\n• None Building Emscripten from Source explains how to build Emscripten from sources on GitHub (this is useful for contributors).\n• None About this site describes the documentation tools and writing conventions used to create this site.\n• None API Reference is a reference for the Emscripten toolchain.\n• None Emscripten Compiler Settings is a reference of all the Emscripten compiler settings.\n• None Tools Reference is a reference for the Emscripten integration APIs.\n• None Debugging with Sanitizers shows how to debug with sanitizers.\n• None Module Splitting is a guide to splitting modules and deferring the loading of code to improve startup time.\n\nThe full hierarchy of articles, opened to the second level, is shown below."
    },
    {
        "link": "https://github.com/skypjack/entt/issues/131",
        "document": "Many C++ libraries have been converted to Javascript / WebAssembly using Emscripten, such as Bullet physics https://github.com/kripken/ammo.js/, Box2D, ect. Which has resulted in great performance boosts for JS versions.\n\nI think this library would be a great candidate for it, being that is seems to be the fastest ECS library available in C++, and I would very much appreciate a WebAssembly version to use in my projects.\n\nAny thoughts on this?"
    },
    {
        "link": "https://js.tensorflow.org/api/latest",
        "document": "Tensors are the core datastructure of TensorFlow.js They are a generalization of vectors and matrices to potentially higher dimensions. We have utility functions for common cases like Scalar, 1D, 2D, 3D and 4D tensors, as well a number of functions to initialize tensors in ways useful for machine learning. Creates a tf.Tensor with the provided values, shape and dtype. ( ) { bytesPerElement = ; sizeInBytes = data. * bytesPerElement; gpuWriteBuffer = device. ({ : , : sizeInBytes, : . | . }); arrayBuffer = gpuWriteBuffer. (); (dtype === ) { (arrayBuffer). (data); } (dtype === ) { (arrayBuffer). (data); } { ( + `'float32'|'int32' dtype, while the dtype is .` ); } gpuWriteBuffer. (); gpuReadBuffer = device. ({ : , : sizeInBytes, : . | . | . }); copyEncoder = device. (); copyEncoder. ( gpuWriteBuffer, , gpuReadBuffer, , sizeInBytes); copyCommands = copyEncoder. (); device. . ([copyCommands]); gpuWriteBuffer. (); gpuReadBuffer; } savedBackend = tf. (); tf. ( ). ( { ( 'Failed to use WebGPU backend. Please use Chrome Canary to run.' )}); dtype = ; device = tf. (). ; aData = [ , , , , , , , , , , , , , , , ]; bData = [ , , , , , , , , , , , , , , , ]; expected = [ , , , , , , , , , , , , , , , ]; aBuffer = (device, aData, dtype); shape = [aData. ]; a = tf. ({ : aBuffer}, shape, dtype); b = tf. (bData, shape, dtype); result = tf. (a, b); result. (); a. (); b. (); result. (); aBuffer. (); tf. (savedBackend);\n• object, or a object. If the values are strings, they will be encoded as utf-8 and kept as . If the values is a object, the dtype could only be 'float32' or 'int32' and the object has to have: 1. texture, a , the texture must share the same with TFJS's WebGL backend (you could create a custom WebGL backend from your texture's canvas) and the internal texture format for the input texture must be floating point or normalized integer; 2. height, the height of the texture; 3. width, the width of the texture; 4. channels, a non-empty subset of 'RGBA', indicating the values of which channels will be passed to the tensor, such as 'R' or 'BR' (The order of the channels affect the order of tensor values. ). (If the values passed from texture is less than the tensor size, zeros will be padded at the rear.). If the values is a object, the dtype could only be 'float32' or 'int32 and the object has to have: buffer, a . The buffer must:\n• share the same with TFJS's WebGPU backend; 2. buffer.usage should at least support GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC; 3. buffer.size should not be smaller than the byte size of tensor shape. WebGPUData optionally supports zero copy by flag zeroCopy. When zeroCopy is false or undefined(default),this passing GPUBuffer can be destroyed after tensor is created. When zeroCopy is true, this GPUBuffer is bound directly by the tensor, so do not destroy this GPUBuffer until all access is done. The values of the tensor. Can be nested array of numbers, or a flat array, or a TypedArray (At the moment it supports Uint8Array, Uint8ClampedArray, Int32Array, Float32Array) data types, or aobject, or aobject. If the values are strings, they will be encoded as utf-8 and kept as. If the values is aobject, the dtype could only be 'float32' or 'int32' and the object has to have: 1. texture, a, the texture must share the samewith TFJS's WebGL backend (you could create a custom WebGL backend from your texture's canvas) and the internal texture format for the input texture must be floating point or normalized integer; 2. height, the height of the texture; 3. width, the width of the texture; 4. channels, a non-empty subset of 'RGBA', indicating the values of which channels will be passed to the tensor, such as 'R' or 'BR' (The order of the channels affect the order of tensor values. ). (If the values passed from texture is less than the tensor size, zeros will be padded at the rear.). If the values is aobject, the dtype could only be 'float32' or 'int32 and the object has to have: buffer, a. The buffer must:\n• The shape of the tensor. Optional. If not provided, it is inferred from . Creates rank-0 tf.Tensor (scalar) with the provided value and dtype. The same functionality can be achieved with tf.tensor(), but in general we recommend using tf.scalar() as it makes the code more readable. Creates rank-1 tf.Tensor with the provided values, shape and dtype. The same functionality can be achieved with tf.tensor(), but in general we recommend using tf.tensor1d() as it makes the code more readable.\n• The values of the tensor. Can be array of numbers, or a TypedArray Creates rank-2 tf.Tensor with the provided values, shape and dtype. The same functionality can be achieved with tf.tensor(), but in general we recommend using tf.tensor2d() as it makes the code more readable.\n• The values of the tensor. Can be nested array of numbers, or a flat array, or a TypedArray\n• The shape of the tensor. If not provided, it is inferred from . Creates rank-3 tf.Tensor with the provided values, shape and dtype. The same functionality can be achieved with tf.tensor(), but in general we recommend using tf.tensor3d() as it makes the code more readable.\n• The values of the tensor. Can be nested array of numbers, or a flat array, or a TypedArray\n• The shape of the tensor. If not provided, it is inferred from . Creates rank-4 tf.Tensor with the provided values, shape and dtype. The same functionality can be achieved with tf.tensor(), but in general we recommend using tf.tensor4d() as it makes the code more readable.\n• The values of the tensor. Can be nested array of numbers, or a flat array, or a TypedArray\n• The shape of the tensor. Optional. If not provided, it is inferred from . Creates rank-5 tf.Tensor with the provided values, shape and dtype. The same functionality can be achieved with tf.tensor(), but in general we recommend using tf.tensor5d() as it makes the code more readable.\n• The values of the tensor. Can be nested array of numbers, or a flat array, or a TypedArray\n• The shape of the tensor. Optional. If not provided, it is inferred from . Creates rank-6 tf.Tensor with the provided values, shape and dtype. The same functionality can be achieved with tf.tensor(), but in general we recommend using tf.tensor6d() as it makes the code more readable.\n• The values of the tensor. Can be nested array of numbers, or a flat array, or a TypedArray\n• The shape of the tensor. Optional. If not provided, it is inferred from . Creates an empty tf.TensorBuffer with the specified and . The values are stored in CPU as TypedArray. Fill the buffer using , or by modifying directly . When done, call to get an immutable tf.Tensor with those values.\n• An array of integers defining the output tensor shape.\n• The dtype of the buffer. Defaults to 'float32'.\n• The values of the buffer as TypedArray . Defaults to zeros. Creates a new tensor with the same values and shape as the specified tensor. Given a tensor representing the real part of a complex number, and a tensor representing the imaginary part of a complex number, this operation returns complex numbers elementwise of the form [r0, i0, r1, i1], where r represents the real part and i represents the imag part. The input tensors real and imag must have the same shape. Given a diagonal, this operation returns a tensor with the diagonal and everything else padded with zeros. Assume the input has dimensions , then the output is a tensor of rank 2k with dimensions\n• If provided, will add the batch shape to the beginning of the shape of the returned tf.Tensor by repeating the identity matrix.\n• An array of integers defining the output tensor shape.\n• The scalar value to fill the tensor with.\n• The type of an element in the resulting tensor. Defaults to 'float32' if the given param value is a number, otherwise 'string'. Returns the imaginary part of a complex (or real) tensor. Given a tensor input, this operation returns a tensor of type float that is the imaginary part of each element in input considered as a complex number. If input is real, a tensor of all zeros is returned. Return an evenly spaced sequence of numbers over the given interval.\n• The start value of the sequence.\n• The end value of the sequence.\n• The number of values to generate. Creates a one-hot tf.Tensor. The locations represented by take value (defaults to 1), while all other locations take value (defaults to 0). If is rank , the output has rank with the last axis of size . used to encode prediction class must start from 0. For example, if you have 3 classes of data, class 1 should be encoded as 0, class 2 should be 1, and class 3 should be 2.\n• of indices with dtype . Indices must start from 0. tf.Tensor of indices with dtype. Indices must start from 0.\n• The depth of the one hot dimension.\n• A number used to fill in the output when the index matches the location.\n• A number used to fill in the output when the index does not match the location.\n• The dtype of the output tensor, default to 'int32'. Creates a tf.Tensor with all elements set to 1.\n• An array of integers defining the output tensor shape.\n• The type of an element in the resulting tensor. Defaults to 'float'. Creates a tf.Tensor with all elements set to 1 with the same shape as the given tensor. Prints information about the tf.Tensor including its data.\n• The tensor to be printed.\n• Whether to print verbose information about the , including dtype and size. Creates a new tf.Tensor1D filled with the numbers in the range provided. The tensor is a half-open interval meaning it includes start, but excludes stop. Decrementing ranges and negative step values are also supported.\n• An integer increment (will default to 1 or -1)\n• The data type of the output tensor. Defaults to 'float32'. Returns the real part of a complex (or real) tensor. Given a tensor input, this operation returns a tensor of type float that is the real part of each element in input considered as a complex number. If the input is real, it simply makes a clone. The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.\n• An array of integers defining the output tensor shape.\n• The mean of the normal distribution.\n• The standard deviation of the normal distribution.\n• The data type of the output tensor.\n• The seed for the random number generator. Creates a new variable with the provided initial value.\n• Initial value for the tensor.\n• If true, optimizers are allowed to update it.\n• Name of the variable. Defaults to a unique id.\n• If set, initialValue will be converted to the given type. Creates a tf.Tensor with all elements set to 0.\n• An array of integers defining the output tensor shape.\n• The type of an element in the resulting tensor. Can be 'float32', 'int32' or 'bool'. Defaults to 'float'. Creates a tf.Tensor with all elements set to 0 with the same shape as the given tensor. This section shows the main Tensor related classes in TensorFlow.js and the methods we expose on them. A tf.Tensor object represents an immutable, multidimensional array of numbers that has a shape and a data type. For performance reasons, functions that create tensors do not necessarily perform a copy of the data passed to them (e.g. if the data is passed as a ), and changes to the data will change the tensor. This is not a feature and is not supported. To avoid this behavior, use the tensor before changing the input data or create a copy with . See tf.tensor() for details on how to create a tf.Tensor. Returns a promise of tf.TensorBuffer that holds the underlying data. Returns the tensor data as a nested array. The transfer of data is done asynchronously. Returns the tensor data as a nested array. The transfer of data is done synchronously. Asynchronously downloads the values from the tf.Tensor. Returns a promise of TypedArray that resolves when the computation has finished. Copy the tensor's data to a new GPU resource. Comparing to the and , this method prevents data from being downloaded to CPU. For WebGL backend, the data will be stored on a densely packed texture. This means that the texture will use the RGBA channels to store value. For WebGPU backend, the data will be stored on a buffer. There is no parameter, so can not use a user-defined size to create the buffer.\n• \n• customTexShape: Optional. If set, will use the user defined texture shape to create the texture. Synchronously downloads the values from the tf.Tensor. This blocks the UI thread until the values are ready, which can cause performance issues. Prints the tf.Tensor. See tf.print() for details.\n• Whether to print verbose information about the tensor, including dtype and size. Returns a copy of the tensor. See tf.clone() for details. Returns a human-readable description of the tensor. Useful for logging. A mutable tf.Tensor, useful for persisting state, e.g. for training. Assign a new tf.Tensor to this variable. The new tf.Tensor must have the same shape and dtype as the old tf.Tensor.\n• New tensor to be assigned to this variable. A mutable object, similar to tf.Tensor, that allows users to set values at locations before converting to an immutable tf.Tensor. Sets a value in the buffer at a given location. Returns the value in the buffer at the provided location. Creates an immutable tf.Tensor object from the buffer. This section describes some common Tensor transformations for reshaping and type-casting. This operation reshapes the \"batch\" dimension 0 into dimensions of shape , interleaves these blocks back into the grid defined by the spatial dimensions , to obtain a result with the same rank as the input. The spatial dimensions of this intermediate result are then optionally cropped according to to produce the output. This is the reverse of tf.spaceToBatchND(). See below for a precise description.\n• = , where spatialShape has dimensions. tf.Tensor . N-D with, where spatialShape hasdimensions.\n• A 1-D array. Must have shape , all values must be >= 1.\n• , all values must be >= 0. specifies the amount to crop from input dimension , which corresponds to spatial dimension . It is required that A 2-D array. Must have shape, all values must be >= 0.specifies the amount to crop from input dimension, which corresponds to spatial dimension. It is required that This operation is equivalent to the following steps:\n• Permute dimensions of to produce of shape\n• Crop the start and end of dimensions of according to to produce the output of shape: Return the shape of s0 op s1 with broadcast. compute r0, the broadcasted shape as a tensor. s0, s1 and r0 are all integer vectors. This function returns the shape of the result of an operation between two tensors of size s0 and s1 performed with broadcast. The tensor's shape is compared to the broadcast shape from end to beginning. Ones are prepended to the tensor's shape until it has the same length as the broadcast shape. If input.shape[i]==shape[i], the (i+1)-th axis is already broadcast-compatible. If input.shape[i]==1 and shape[i]==N, then the input tensor is tiled N times along that axis (using tf.tile).\n• The input is to be broadcast to this shape.\n• The input tensor to be casted.\n• The dtype to cast the input tensor to. Rearranges data from depth into blocks of spatial data. More specifically, this op outputs a copy of the input tensor where values from the dimension are moved in spatial blocks to the and dimensions. The attr indicates the input block size and how the data is moved.\n• Chunks of data of size from depth are rearranged into non-overlapping blocks of size\n• The width the output tensor is , whereas the height is\n• The Y, X coordinates within each block of the output image are determined by the high order component of the input channel index\n• The depth of the input tensor must be divisible by The attr specifies the layout of the input and output tensors with the following options: \"NHWC\": [ ] \"NCHW\": [ ]\n• An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\" Checks the input tensor mathes the given shape. Given an input tensor, returns a new tensor with the same values as the input tensor with shape . The method supports the null value in tensor. It will still check the shapes, and null is a placeholder.\n• The input tensor to be ensured.\n• A TensorShape representing the shape of this tensor, an array or null. Returns a tf.Tensor that has expanded rank, by inserting a dimension into the tensor's shape.\n• The input tensor whose dimensions are to be expanded.\n• The dimension index at which to insert shape of . Defaults to 0 (the first dimension). This operation implements the and modes of pad.\n• An array of length (the rank of the tensor), where each element is a length-2 tuple of ints , specifying how much to pad along each dimension of the tensor. In \"reflect\" mode, the padded regions do not include the borders, while in \"symmetric\" mode the padded regions do include the borders. For example, if the input is and paddings is , then the output is in \"reflect\" mode, and in \"symmetric\" mode. If is \"reflect\" then both and must be no greater than . If mode is \"symmetric\" then both and must be no greater than\n• String to specify padding mode. Can be Pads a tf.Tensor with a given value and paddings. This operation implements mode. For and , refer to tf.mirrorPad(). Also available are stricter rank-specific methods with the same signature as this method that assert that is of given length.\n• An array of length (the rank of the tensor), where each element is a length-2 tuple of ints , specifying how much to pad along each dimension of the tensor.\n• The pad value to use. Defaults to 0. Given an input tensor, returns a new tensor with the same values as the input tensor with shape . If one component of shape is the special value -1, the size of that dimension is computed so that the total size remains constant. In particular, a shape of [-1] flattens into 1-D. At most one component of shape can be -1. If shape is 1-D or higher, then the operation returns a tensor with shape shape filled with the values of tensor. In this case, the number of elements implied by shape must be the same as the number of elements in tensor.\n• The input tensor to be reshaped.\n• An array of integers defining the output tensor shape. Computes the difference between two lists of numbers. Given a Tensor and a Tensor , this operation returns a Tensor that represents all values that are in but not in . The returned Tensor is sorted in the same order that the numbers appear in (duplicates are preserved). This operation also returns a Tensor indices that represents the position of each out element in . In other words:\n• 1-D Tensor. Must have the same type as x. Values to exclude in the output. This operation divides \"spatial\" dimensions of the input into a grid of blocks of shape , and interleaves these blocks with the \"batch\" dimension (0) such that in the output, the spatial dimensions correspond to the position within the grid, and the batch dimension combines both the position within a spatial block and the original batch position. Prior to division into blocks, the spatial dimensions of the input are optionally zero padded according to . See below for a precise description.\n• = , where spatialShape has dimensions. tf.Tensor . N-D with, where spatialShape hasdimensions.\n• A 1-D array. Must have shape , all values must be >= 1.\n• , all values must be >= 0. specifies the amount to zero-pad from input dimension , which corresponds to spatial dimension . It is required that A 2-D array. Must have shape, all values must be >= 0.specifies the amount to zero-pad from input dimension, which corresponds to spatial dimension. It is required that This operation is equivalent to the following steps:\n• Zero-pad the start and end of dimensions of the input according to to produce of shape paddedShape.\n• Permute dimensions of to produce of shape:\n• Reshape to flatten into the batch dimension, producing an output tensor of shape: Removes dimensions of size 1 from the shape of a tf.Tensor.\n• The input tensor to be squeezed.\n• An optional list of numbers. If specified, only squeezes the dimensions listed. The dimension index starts at 0. It is an error to squeeze a dimension that is not 1. TensorFlow.js provides several operations to slice or extract parts of a tensor, or join multiple tensors together.\n• K-D boolean tensor, K <= N and K must be known statically.\n• A 0-D int Tensor representing the axis in tensor to mask from. By default, axis is 0 which will mask from the first dimension. Otherwise K + axis <= N. Concatenates a list of tf.Tensors along a given axis. The tensors ranks and types must match, and their sizes must match in all dimensions except . Also available are stricter rank-specific methods that assert that are of the given rank: Except (which does not have axis param), all methods have same signature as this method.\n• The axis to concatenate along. Defaults to 0 (the first dim). Gather slices from tensor 's axis according to .\n• The input tensor whose slices are to be gathered.\n• The indices of the values to extract.\n• The axis over which to select values. Defaults to 0.\n• Optional. The number of batch dimensions. It must be less than or equal to rank(indices). Defaults to 0. The output tensor will have shape of Also available are stricter rank-specific methods that assert that is of the given rank: Except (which does not have axis param), all methods have same signature as this method.\n• The input tensor to be reversed.\n• The set of dimensions to reverse. Must be in the range [-rank(x), rank(x)). Defaults to all axes. Extracts a slice from a tf.Tensor starting at coordinates and is of size . Also available are stricter rank-specific methods with the same signature as this method that assert that is of the given rank:\n• The input tf.Tensor to slice from.\n• The coordinates to start the slice from. The length can be less than the rank of x - the rest of the axes will have implicit 0 as start. Can also be a single number, in which case it specifies the first axis.\n• The size of the slice. The length can be less than the rank of x - the rest of the axes will have implicit -1. A value of -1 requests the rest of the dimensions in the axis. Can also be a single number, in which case it specifies the size of the first axis. If is a number, splits along dimension into smaller tensors. Requires that evenly divides . If is a number array, splits into pieces. The shape of the -th piece has the same size as except along dimension where the size is .\n• Either an integer indicating the number of splits along the axis or an array of integers containing the sizes of each output tensor along the axis. If a number then it must evenly divide ; otherwise the sum of sizes must match . Can contain one -1 indicating that dimension is to be inferred.\n• The dimension along which to split. Defaults to 0 (the first dim). Stacks a list of rank- tf.Tensors into one rank- tf.Tensor.\n• A list of tensor objects with the same shape and dtype.\n• The axis to stack along. Defaults to 0 (the first dim). Construct a tensor by repeating it the number of times given by reps. This operation creates a new tensor by replicating times. The output tensor's th dimension has elements, and the values of are replicated times along the th dimension. For example, tiling by produces .\n• Determines the number of replications per dimension. Unstacks a tf.Tensor of rank- into a list of rank- tf.Tensors.\n• The axis to unstack along. Defaults to 0 (the first dim). Tensor contraction over specified indices and outer product. allows defining Tensors by defining their element-wise computation. This computation is based on Einstein summation. This implementation of einsum has the following limitations:\n• Does not support duplicate axes for any given input tensor. E.g., equation 'ii->' is not supported.\n• The notation is not supported.\n• a string describing the contraction, in the same format as numpy.einsum\n• the input(s) to contract (each one a Tensor), whose shapes should be consistent with equation.\n• 1D array with unnormalized log-probabilities, or 2D array of shape . See the parameter.\n• Number of samples to draw for each row slice.\n• Whether the provided are normalized true probabilities (sum to 1). Defaults to false. Creates a tf.Tensor with values sampled from a random number generator function defined by the user.\n• An array of integers defining the output tensor shape.\n• A random number generator function which is called for each element in the output tensor.\n• The data type of the output tensor. Defaults to 'float32'.\n• An array of integers defining the output tensor shape.\n• The shape parameter of the gamma distribution.\n• The inverse scale parameter of the gamma distribution. Defaults to 1.\n• The data type of the output. Defaults to float32.\n• The seed for the random number generator.\n• An array of integers defining the output tensor shape.\n• The mean of the normal distribution.\n• The standard deviation of the normal distribution.\n• The data type of the output.\n• The seed for the random number generator. The generated values will have mean 0 and standard deviation 1.\n• An array of integers defining the output tensor shape.\n• The data type of the output.\n• The seed for the random number generator. The generated values follow a uniform distribution in the range [minval, maxval). The lower bound minval is included in the range, while the upper bound maxval is excluded.\n• An array of integers defining the output tensor shape.\n• The lower bound on the range of random values to generate. Defaults to 0.\n• The upper bound on the range of random values to generate. Defaults to 1.\n• The data type of the output tensor. Defaults to 'float32'.\n• An optional int. Defaults to 0. If seed is set to be non-zero, the random number generator is seeded by the given seed. Otherwise, it is seeded by a random seed. The generated values are uniform integers in the range [minval, maxval). The lower bound minval is included in the range, while the upper bound maxval is excluded.\n• An array of integers defining the output tensor shape.\n• An optional int. Defaults to 0. If seed is set to be non-zero, the random number generator is seeded by the given seed. Otherwise, it is seeded by a random seed.\n\nModels are one of the primary abstractions used in TensorFlow.js Layers. Models can be trained, evaluated, and used for prediction. A model's state (topology, and optionally, trained weights) can be restored from various formats. Models are a collection of Layers, see Model Creation for details about how Layers can be connected. There are two primary ways of creating models.\n• Sequential — Easiest, works if the models is a simple stack of each layer's input resting on the top of the previous layer's output.\n• Model — Offers more control if the layers need to be wired together in graph-like ways — multiple 'towers', layers that skip a layer, etc. Creates a tf.Sequential model. A sequential model is any model where the outputs of one layer are the inputs to the next layer, i.e. the model topology is a simple 'stack' of layers, with no branching or skipping. This means that the first layer passed to a tf.Sequential model should have a defined input shape. What that means is that it should have received an or argument, or for some type of layers (recurrent, Dense...) an argument. The key difference between tf.model() and tf.sequential() is that tf.sequential() is less generic, supporting only a linear stack of layers. tf.model() is more generic and supports an arbitrary graph (without cycles) of layers. It is also possible to specify a batch size (with potentially undetermined batch dimension, denoted by \"null\") for the first layer using the key. The following example is equivalent to the above: You can also use an of already-constructed s to create a tf.Sequential model:\n• Stack of layers for the model. A model is a data structure that consists of and defines inputs and outputs. The key difference between tf.model() and tf.sequential() is that tf.model() is more generic, supporting an arbitrary graph (without cycles) of layers. tf.sequential() is less generic and supports only a linear stack of layers. When creating a tf.LayersModel, specify its input(s) and output(s). Layers are used to wire input(s) to output(s). For example, the following code snippet defines a model consisting of two layers, with 10 and 4 units, respectively. Used to instantiate an input to a model as a tf.SymbolicTensor. Users should call the factory function for consistency with other generator functions. Note: is only necessary when using . When using , specify for the first layer or use as the first layer.\n• A shape, not including the batch size. For instance, indicates that the expected input will be batches of 32-dimensional vectors.\n• A shape tuple (integer), including the batch size. For instance, indicates that the expected input will be batches of 10 32-dimensional vectors. indicates batches of an arbitrary number of 32-dimensional vectors.\n• An optional name string for the layer. Should be unique in a model (do not reuse the same name twice). It will be autogenerated if it isn't provided.\n• A boolean specifying whether the placeholder to be created is sparse. Load a graph model given a URL to the model definition. Example of loading MobileNetV2 from a URL and making a prediction with a zeros input: Example of loading MobileNetV2 from a TF Hub URL and making a prediction with a zeros input:\n• The url or an that loads the model.\n• Options for the HTTP request, which allows to send credentials and custom headers.\n• For detailed information on the supported fields, see https://developer.mozilla.org/en-US/docs/Web/API/Request/Request\n• A function used to override the function.\n• . Strict loading model: whether extraneous weights or missing weights should trigger an If , require that the provided weights exactly match those required by the layers. means that both extra weights and missing weights will be silently ignored.\n• Path prefix for weight files, by default this is calculated from the path of the model JSON file. For instance, if the path to the model JSON file is , then the default path prefix will be . If a weight file has the path value in the weight manifest, then the weight file will be loaded from by default. However, if you provide a value of , then the weight file will be loaded from the path instead.\n• Whether the module or model is to be loaded from TF Hub. Setting this to allows passing a TF-Hub module URL, omitting the standard model file name and the query parameters.\n• An async function to convert weight file name to URL. The weight file names are stored in model.json's weightsManifest.paths field. By default we consider weight files are colocated with the model.json file. For example: model.json URL: https://www.google.com/models/1/model.json group1-shard1of1.bin url: https://www.google.com/models/1/group1-shard1of1.bin With this func you can convert the weight file name to any URL.\n• Whether to stream the model directly to the backend or cache all its weights on CPU first. Useful for large models. Load a model composed of Layer objects, including its topology and optionally weights. See the Tutorial named \"How to import a Keras Model\" for usage examples. This method is applicable to:\n• Models created with the , tf.sequential(), and tf.model() APIs of TensorFlow.js and later saved with the tf.LayersModel.save() method.\n• Models converted from Keras or TensorFlow tf.keras using the tensorflowjs_converter. This mode is not applicable to TensorFlow s or their converted forms. For those models, use tf.loadGraphModel(). Example 1. Load a model from an HTTP server. Example 2: Save 's topology and weights to browser local storage; then load it back. Example 3. Saving 's topology and weights to browser IndexedDB; then load it back. Example 4. Load a model from user-selected files from HTML file input elements.\n• \n• A string path to the JSON describing the model in the canonical TensorFlow.js format. For file:// (tfjs-node-only), http:// and https:// schemas, the path can be either absolute or relative. The content of the JSON file is assumed to be a JSON object with the following fields and values:\n• 'modelTopology': A JSON object that can be either of:\n• a model architecture JSON consistent with the format of the return value of\n• a full model JSON in the format of .\n• 'weightsManifest': A TensorFlow.js weights manifest. See the Python converter function for more details. It is also assumed that model weights can be accessed from relative paths described by the fields in weights manifest.\n• A object that loads model artifacts with its method. Can be either of the two formats\n• \n• : Require that the provided weights exactly match those required by the layers. Default true. Passing false means that both extra weights and missing weights will be silently ignored.\n• : A progress callback of the form: . This callback can be used to monitor the model-loading process.\n• For detailed information on the supported fields, see https://developer.mozilla.org/en-US/docs/Web/API/Request/Request\n• A function used to override the function.\n• . Strict loading model: whether extraneous weights or missing weights should trigger an If , require that the provided weights exactly match those required by the layers. means that both extra weights and missing weights will be silently ignored.\n• Path prefix for weight files, by default this is calculated from the path of the model JSON file. For instance, if the path to the model JSON file is , then the default path prefix will be . If a weight file has the path value in the weight manifest, then the weight file will be loaded from by default. However, if you provide a value of , then the weight file will be loaded from the path instead.\n• Whether the module or model is to be loaded from TF Hub. Setting this to allows passing a TF-Hub module URL, omitting the standard model file name and the query parameters.\n• An async function to convert weight file name to URL. The weight file names are stored in model.json's weightsManifest.paths field. By default we consider weight files are colocated with the model.json file. For example: model.json URL: https://www.google.com/models/1/model.json group1-shard1of1.bin url: https://www.google.com/models/1/group1-shard1of1.bin With this func you can convert the weight file name to any URL.\n• Whether to stream the model directly to the backend or cache all its weights on CPU first. Useful for large models. Creates an IOHandler that triggers file downloads from the browser. The returned instance can be used as model exporting methods such as and supports only saving.\n• , should follow either of the following two formats:\n• or , in which case the default file names will be used:\n• 'model.json' for the JSON file containing the model topology and weights manifest.\n• 'model.weights.bin' for the binary file containing the binary weight values.\n• A single string or an Array of a single string, as the file name prefix. For example, if is provided, the downloaded JSON file and binary weights file will be named 'foo.json' and 'foo.weights.bin', respectively. Prefix name of the files to be downloaded. For use withshould follow either of the following two formats: Creates an IOHandler that loads model artifacts from user-selected files. This method can be used for loading from files such as user-selected files in the browser. When used in conjunction with tf.loadLayersModel(), an instance of tf.LayersModel (Keras-style) can be constructed from the loaded artifacts.\n• s to load from. Currently, this function supports only loading from files that contain Keras-style models (i.e., s), for which an of s is expected (in that order):\n• A JSON file containing the model topology and weight manifest.\n• Optionally, one or more binary files containing the binary weights. These files must have names that match the paths in the contained by the aforementioned JSON file, or errors will be thrown during loading. These weights files have the same format as the ones generated by that comes with the Python PIP package. If no weights files are provided, only the model topology will be loaded from the JSON file above. s to load from. Currently, this function supports only loading from files that contain Keras-style models (i.e.,s), for which anofs is expected (in that order): Creates an IOHandler subtype that sends model artifacts to HTTP server. An HTTP request of the mime type will be sent to the URL. The form data includes artifacts that represent the topology and/or weights of the model. In the case of Keras-style , two blobs (files) exist in form-data:\n• A binary weights file consisting of the concatenated weight values. These files are in the same format as the one generated by tfjs_converter. The following code snippet exemplifies the client-side code that uses this function: If the default method is to be used, without any custom parameters such as headers, you can simply pass an HTTP or HTTPS URL to : The following GitHub Gist https://gist.github.com/dsmilkov/1b6046fd6132d7408d5257b0976f7864 implements a server based on flask that can receive the request. Upon receiving the model artifacts via the request, this particular server reconstitutes instances of Keras Models in memory.\n• A URL path to the model. Can be an absolute HTTP path (e.g., 'http://localhost:8000/model-upload)') or a relative path (e.g., './model-upload').\n• \n• weightPathPrefix Optional, this specifies the path prefix for weight files, by default this is calculated from the path param.\n• fetchFunc Optional, custom function. E.g., in Node.js, the from node-fetch can be used here.\n• onProgress Optional, progress callback function, fired periodically before the load is completed. Optional configuration for the loading. It includes the following fields:\n• The that loads the model, or the that encode the model, or a tuple of of which the first element encodes the model and the second contains the weights. Copy a model from one URL to another. For a web browser environment, the registered mediums are Local Storage and IndexedDB. Move a model from one URL to another. Remove a model specified by URL from a registered storage medium. Register a class with the serialization map of TensorFlow.js. This is often used for registering custom Layers, so they can be serialized and deserialized. Example 1. Register the class without package name and specified name. Example 2. Register the class with package name: \"Package\" and specified name: \"MyLayer\". Example 3. Register the class with specified name: \"MyLayer\". Example 4. Register the class with specified package name: \"Package\".\n• The class to be registered. It must have a public static member called defined and the value must be a non-empty string.\n• The package name that this class belongs to. This used to define the key in GlobalCustomObject. If not defined, it defaults to .\n• The name that user specified. It defaults to the actual name of the class as specified by its static property. A tf.Functional is an alias to tf.LayersModel. A tf.GraphModel is a directed, acyclic graph built from a SavedModel GraphDef and allows inference execution. A tf.GraphModel can only be created by loading from a model converted from a TensorFlow SavedModel using the command line converter tool and loaded via tf.loadGraphModel(). Synchronously construct the in memory weight map and compile the inference graph. Save the configuration and/or weights of the GraphModel. An is an object that has a method of the proper signature defined. The method manages the storing or transmission of serialized data (\"artifacts\") that represent the model's topology and weights onto or via a specific medium, such as file downloads, local storage, IndexedDB in the web browser and HTTP requests to a server. TensorFlow.js provides implementations for a number of frequently used saving mediums, such as tf.io.browserDownloads() and . See for more details. This method also allows you to refer to certain types of s as URL-like string shortcuts, such as 'localstorage://' and 'indexeddb://'. Example 1: Save 's topology and weights to browser local storage; then load it back.\n• An instance of or a URL-like, scheme-based string shortcut for .\n• Whether to save only the trainable weights of the model, ignoring the non-trainable ones.\n• Whether the optimizer will be saved (if exists). Execute the inference for the input tensors.\n• Prediction configuration for specifying the batch size. Currently the batch size option is ignored for graph model.\n• Optional. Batch size (Integer). If unspecified, it will default to 32. Execute the inference for the input tensors in async fashion, use this method when your model contains control flow ops.\n• Prediction configuration for specifying the batch size. Currently the batch size option is ignored for graph model.\n• Optional. Batch size (Integer). If unspecified, it will default to 32. Executes inference for the model for given input tensors.\n• tensor, tensor array or tensor map of the inputs for the model, keyed by the input node names.\n• output node name from the TensorFlow model, if no outputs are specified, the default outputs of the model would be used. You can inspect intermediate nodes of the model by adding them to the outputs array. Executes inference for the model for given input tensors in async fashion, use this method when your model contains control flow ops.\n• tensor, tensor array or tensor map of the inputs for the model, keyed by the input node names.\n• output node name from the TensorFlow model, if no outputs are specified, the default outputs of the model would be used. You can inspect intermediate nodes of the model by adding them to the outputs array. Get intermediate tensors for model debugging mode (flag KEEP_INTERMEDIATE_TENSORS is true). Releases the memory used by the weight tensors and resourceManager. A tf.LayersModel is a directed, acyclic graph of s plus methods for training, evaluation, prediction and saving. tf.LayersModel is the basic unit of training, inference and evaluation in TensorFlow.js. To create a tf.LayersModel, use tf.LayersModel.\n• Name and type of all layers that comprise the model.\n• Number of weight parameters of each layer\n• If the model has non-sequential-like topology, the inputs each layer receives\n• The total number of trainable and non-trainable parameters of the model.\n• Custom widths of each of the columns, as either fractions of (e.g., ) or absolute number of characters (e.g., ). Each number corresponds to right-most (i.e., ending) position of a column.\n• Custom print function. Can be used to replace the default . For example, you can use to mute the printed messages in the console. Configures and prepares the model for training and evaluation. Compiling outfits the model with an optimizer, loss, and/or metrics. Calling or on an un-compiled model will throw an error.\n• a specifying the loss, optimizer, and metrics to be used for fitting and evaluating this model.\n• An instance of tf.train.Optimizer or a string name for an Optimizer.\n• Object function(s) or name(s) of object function(s). If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or an Array of losses. The loss value that will be minimized by the model will then be the sum of all individual losses.\n• List of metrics to be evaluated by the model during training and testing. Typically you will use . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary. Returns the loss value & metrics values for the model in test mode. Loss and metrics are specified during , which needs to happen before calls to . Computation is done in batches.\n• of test data, or an of tf.Tensor of test data, or anof tf.Tensor s if the model has multiple inputs.\n• of target data, or an of tf.Tensor of target data, or anof tf.Tensor s if the model has multiple outputs.\n• Batch size (Integer). If unspecified, it will default to 32.\n• Tensor of weights to weight the contribution of different samples to the loss and metrics.\n• integer: total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of .\n• method is expected to generate a dataset iterator object, the method of which is expected to produce data batches for evaluation. The return value of the call ought to contain a boolean field and a field. The field is expected to be an array of two A dataset object. Itsmethod is expected to generate a dataset iterator object, themethod of which is expected to produce data batches for evaluation. The return value of thecall ought to contain a booleanfield and afield. Thefield is expected to be an array of two tf.Tensor s or an array of two nested tf.Tensor structures. The former case is for models with exactly one input and one output (e.g. a sequential model). The latter case is for models with multiple inputs and/or multiple outputs. Of the two items in the array, the first is the input feature(s) and the second is the output target(s).\n• Number of batches to draw from the dataset object before ending the evaluation. Computation is done in batches. Note: the \"step\" mode of predict() is currently not supported. This is because the TensorFlow.js core backend is imperative only.\n• of The input data, as a Tensor, or anof tf.Tensor s if the model has multiple inputs.\n• Optional. Batch size (Integer). If unspecified, it will default to 32.\n• : Input samples, as a Tensor (for models with exactly one input) or an array of Tensors (for models with more than one input). Trains the model for a fixed number of epochs (iterations on a dataset).\n• of training data, or an array of tf.Tensor of training data, or an array of tf.Tensor s if the model has multiple inputs. If all inputs in the model are named, you can also pass a dictionary mapping input names to tf.Tensor s.\n• of target (label) data, or an array of tf.Tensor of target (label) data, or an array of tf.Tensor s if the model has multiple outputs. If all outputs in the model are named, you can also pass a dictionary mapping output names to tf.Tensor s.\n• Number of samples per gradient update. If unspecified, it will default to 32.\n• Integer number of times to iterate over the training data arrays.\n• Expected to be 0, 1, or 2. Default: 1. 0 - No printed message during fit() call. 1 - In Node.js (tfjs-node), prints the progress bar, together with real-time updates of loss and metric values and training speed. In the browser: no action. This is the default. 2 - Not implemented yet.\n• \n• : called at the start of every epoch.\n• : called at the end of every epoch.\n• : called at the start of every batch.\n• : called at the end of every batch.\n• : called every milliseconds with the current epoch, batch and logs. The logs are the same as in . Note that can skip batches or epochs. See also docs for below. List of callbacks to be called during training. Can have one or more of the following callbacks:\n• Float between 0 and 1: fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the and data provided, before shuffling.\n• Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. This could be a tuple [xVal, yVal] or a tuple [xVal, yVal, valSampleWeights]. The model will not be trained on this data. will override .\n• Whether to shuffle the training data before each epoch. Has no effect when is not .\n• Optional object mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. If the model has multiple outputs, a class weight can be specified for each of the outputs by setting this field an array of weight object or an object that maps model output names (e.g., ) to weight objects.\n• Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequenceLength), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sampleWeightMode=\"temporal\" in compile().\n• Epoch at which to start training (useful for resuming a previous training run). When this is used, is the index of the \"final epoch\". The model is not trained for a number of iterations given by , but merely until the epoch of index is reached.\n• Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with Input Tensors such as TensorFlow data tensors, the default is equal to the number of unique samples in your dataset divided by the batch size, or 1 if that cannot be determined.\n• Only relevant if is specified. Total number of steps (batches of samples) to validate before stopping.\n• Configures the frequency of yielding the main thread to other tasks. In the browser environment, yielding the main thread can improve the responsiveness of the page during training. In the Node.js environment, it can ensure tasks queued in the event loop can be handled in a timely manner. The value can be one of the following:\n• : The yielding happens at a certain frame rate (currently set at 125ms). This is the default.\n• : never yield. (yielding can still happen through calls in custom callbacks.)\n• method is expected to generate a dataset iterator object, the method of which is expected to produce data batches for training. The return value of the call ought to contain a boolean field and a field. The field is expected to be an array of two A dataset object. Itsmethod is expected to generate a dataset iterator object, themethod of which is expected to produce data batches for training. The return value of thecall ought to contain a booleanfield and afield. Thefield is expected to be an array of two tf.Tensor s or an array of two nested tf.Tensor structures. The former case is for models with exactly one input and one output (e.g. a sequential model). The latter case is for models with multiple inputs and/or multiple outputs. Of the two items in the array, the first is the input feature(s) and the second is the output target(s).\n• (Optional) Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of samples of your dataset divided by the batch size, so that () call can utilize the entire dataset. If it is not provided, use return value in as signal to finish an epoch.\n• Integer number of times to iterate over the training dataset.\n• Expected to be 0, 1, or 2. Default: 1. 0 - No printed message during fit() call. 1 - In Node.js (tfjs-node), prints the progress bar, together with real-time updates of loss and metric values and training speed. In the browser: no action. This is the default. 2 - Not implemented yet.\n• \n• : called at the start of every epoch.\n• : called at the end of every epoch.\n• : called at the start of every batch.\n• : called at the end of every batch.\n• : called every milliseconds with the current epoch, batch and logs. The logs are the same as in . Note that can skip batches or epochs. See also docs for below. List of callbacks to be called during training. Can have one or more of the following callbacks:\n• \n• An array , where the two values may be tf.Tensor, an array of Tensors, or a map of string to Tensor.\n• Similarly, an array (not implemented yet).\n• a object with elements of the form , where and are the feature and label tensors, respectively. Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. This could be any of the following: If is an Array of Tensor objects, each tf.Tensor will be sliced into batches during validation, using the parameter (which defaults to 32). The entirety of the tf.Tensor objects will be used in the validation. If is a dataset object, and the parameter is specified, the validation will use batches drawn from the dataset object. If parameter is not specified, the validation will stop when the dataset is exhausted. The model will not be trained on this data.\n• Used only if is an array of tf.Tensor objects, i.e., not a dataset object. If not specified, its value defaults to 32.\n• is specified and is a dataset object. (Optional) Only relevant ifis specified and is a dataset object. Total number of batches of samples to draw from for validation purpose before stopping at the end of every epoch. If not specified, will use as signal to stop validation.\n• Configures the frequency of yielding the main thread to other tasks. In the browser environment, yielding the main thread can improve the responsiveness of the page during training. In the Node.js environment, it can ensure tasks queued in the event loop can be handled in a timely manner. The value can be one of the following:\n• : The yielding happens at a certain frame rate (currently set at 125ms). This is the default.\n• : never yield. (But yielding can still happen through calls in custom callbacks.)\n• Epoch at which to start training (useful for resuming a previous training run). When this is used, is the index of the \"final epoch\". The model is not trained for a number of iterations given by , but merely until the epoch of index is reached.\n• Optional object mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. If the model has multiple outputs, a class weight can be specified for each of the outputs by setting this field an array of weight object or an object that maps model output names (e.g., ) to weight objects. This method differs from and in the following regards:\n• It operates on exactly one batch of data.\n• It returns only the loss and metric values, instead of returning the batch-by-batch loss and metric values.\n• It doesn't support fine-grained options such as verbosity and callbacks.\n• \n• A tf.Tensor, or an Array of tf.Tensors (in case the model has multiple inputs).\n• An Object mapping input names to corresponding tf.Tensor (if the model has named inputs). Input data. It could be one of the following:\n• . Target data. It could be either a tf.Tensor or multiple tf.Tensor s. It should be consistent with Save the configuration and/or weights of the LayersModel. An is an object that has a method of the proper signature defined. The method manages the storing or transmission of serialized data (\"artifacts\") that represent the model's topology and weights onto or via a specific medium, such as file downloads, local storage, IndexedDB in the web browser and HTTP requests to a server. TensorFlow.js provides implementations for a number of frequently used saving mediums, such as tf.io.browserDownloads() and . See for more details. This method also allows you to refer to certain types of s as URL-like string shortcuts, such as 'localstorage://' and 'indexeddb://'. Example 1: Save 's topology and weights to browser local storage; then load it back. Example 2. Saving 's topology and weights to browser IndexedDB; then load it back. Example 3. Saving 's topology and weights as two files ( and ) downloaded from browser. Example 4. Send 's topology and weights to an HTTP server. See the documentation of tf.io.http() for more details including specifying request parameters and implementation of the server.\n• An instance of or a URL-like, scheme-based string shortcut for .\n• Whether to save only the trainable weights of the model, ignoring the non-trainable ones.\n• Whether the optimizer will be saved (if exists). Retrieves a layer based on either its name (unique) or index. Indices are based on order of horizontal graph traversal (bottom-up). If both and are specified, takes precedence. Retrieves a layer based on either its name (unique) or index. Indices are based on order of horizontal graph traversal (bottom-up). If both and are specified, takes precedence. Retrieves a layer based on either its name (unique) or index. Indices are based on order of horizontal graph traversal (bottom-up). If both and are specified, takes precedence. Retrieves a layer based on either its name (unique) or index. Indices are based on order of horizontal graph traversal (bottom-up). If both and are specified, takes precedence. A model with a stack of layers, feeding linearly from one to the next. tf.sequential() is a factory function that creates an instance of tf.Sequential. Adds a layer instance on top of the layer stack. tf.SymbolicTensor is a placeholder for a Tensor without any concrete value. They are most often encountered when building a graph of s for a tf.LayersModel and the input data's shape, but not values are known. Retrieve the OpMapper object for the registered op. Register an Op for graph model executor. This allows you to register TensorFlow custom op or override existing op. Here is an example of registering a new MatMul Op. The inputs and attrs of the node object are based on the TensorFlow op registry.\n• \n• attr: A map from attribute name to its value An op function which is called with the current graph node during execution and needs to return a tensor or a list of tensors. The node has the following attributes:"
    },
    {
        "link": "https://js.tensorflow.org/api_tasks/0.0.1-alpha.8",
        "document": "TFJS Task API groups models into different tasks. To use a specific model, you first need to load it, then call the method on the model to run the inference. To load a model, use a model loader as follows. Do not construct the model manually. Please refer to a specific model below for details about the exact model loader to use and the corrsponding options. All loaded models have a predict method defined. Call it with model-specific input and options to get the inference result. Please refer to a specific model below for details about the input and the corresponding options. All loaded models have a method defined to clean up resources. The model should not be used after this call.\n\nThe task of classifying images into a preset of labels. The base class for all ImageClassification task models. Performs classification on the given image-like input, and returns result.\n• The image-like element to run classification on.\n• Inference options. Different models have different inference options. See individual model for more details. A custom TFLite image classification model loaded from a model url or an in memory. The underlying image classifier is built on top of the TFLite Task Library. As a result, the custom model needs to meet the metadata requirements. Refer to tfTask.ImageClassifier for the and method.\n• . The model url, or the model content stored in an You can use TFLite model urls from directly. For model compatibility, see comments in the corresponding model class.\n• Maximum number of top scored results to return. If < 0, all results will be returned. If 0, an invalid argument error is returned.\n• Score threshold in [0,1), overrides the ones provided in the model metadata (if any). Results below this value are rejected.\n• The number of threads to be used for TFLite ops that support multi-threading when running inference with CPU. num_threads should be greater than 0 or equal to -1. Setting num_threads to -1 has the effect to let TFLite runtime set the value. Default to number of physical CPU cores, or -1 if WASM multi-threading is not supported by user's browser. Refer to tfTask.ImageClassifier for the and method.\n• The backend to use to run TFJS models. Default to 'webgl'. Refer to tfTask.ImageClassifier for the and method.\n• The MobileNet version number. Use 1 for MobileNetV1, and 2 for MobileNetV2. Defaults to 1.\n• Controls the width of the network, trading accuracy for performance. A smaller alpha decreases accuracy and increases performance. Defaults to 1.0.\n• Maximum number of top scored results to return. If < 0, all results will be returned. If 0, an invalid argument error is returned.\n• Score threshold in [0,1), overrides the ones provided in the model metadata (if any). Results below this value are rejected.\n• The number of threads to be used for TFLite ops that support multi-threading when running inference with CPU. num_threads should be greater than 0 or equal to -1. Setting num_threads to -1 has the effect to let TFLite runtime set the value. Default to number of physical CPU cores, or -1 if WASM multi-threading is not supported by user's browser.\n\nThe task of predicting associated class for each pixel of an image. The base class for all ImageSegmentation task models. Performs segmentation on the given image-like input, and returns result.\n• The image-like element to run segmentation on.\n• Inference options. Different models have different inference options. See individual model for more details.\n• The width of the returned segmentation map.\n• The height of the returned segmentation map.\n• The colored segmentation map as which can be fed into and mapped to a canvas.\n• The red color component for the label, in the [0, 255] range.\n• The green color component for the label, in the [0, 255] range.\n• The blue color component for the label, in the [0, 255] range. Refer to tfTask.ImageSegmenter for the and method.\n• The backend to use to run TFJS models. Default to 'webgl'. Refer to tfTask.ImageSegmenter for the and method.\n• The number of threads to be used for TFLite ops that support multi-threading when running inference with CPU. num_threads should be greater than 0 or equal to -1. Setting num_threads to -1 has the effect to let TFLite runtime set the value. Default to number of physical CPU cores, or -1 if WASM multi-threading is not supported by user's browser. A custom TFLite image segmentation model loaded from a model url or an in memory. The underlying image segmenter is built on top of the TFLite Task Library. As a result, the custom model needs to meet the metadata requirements. Refer to tfTask.ImageSegmenter for the and method.\n• . The model url, or the model content stored in an You can use TFLite model urls from directly. For model compatibility, see comments in the corresponding model class.\n• The number of threads to be used for TFLite ops that support multi-threading when running inference with CPU. num_threads should be greater than 0 or equal to -1. Setting num_threads to -1 has the effect to let TFLite runtime set the value. Default to number of physical CPU cores, or -1 if WASM multi-threading is not supported by user's browser.\n\nThe task of localizing and identifing multiple objects in a single image. The base class for all ObjectDetection task models. Detects objects on the given image-like input, and returns result.\n• The image-like element to detect objects on.\n• Inference options. Different models have different inference options. See individual model for more details.\n• The bounding box of the object.\n• The X coordinate of the top-left corner of the bounding box.\n• The Y coordinate of the top-left corner of the bounding box. Refer to tfTask.ObjectDetector for the and method.\n• The backend to use to run TFJS models. Default to 'webgl'.\n• The maximum number of bounding boxes of detected objects. There can be multiple objects of the same class, but at different locations. Defaults to 20.\n• The minimum score of the returned bounding boxes of detected objects. Value between 0 and 1. Defaults to 0.5. Refer to tfTask.ObjectDetector for the and method.\n• Maximum number of top scored results to return. If < 0, all results will be returned. If 0, an invalid argument error is returned.\n• Score threshold in [0,1), overrides the ones provided in the model metadata (if any). Results below this value are rejected.\n• The number of threads to be used for TFLite ops that support multi-threading when running inference with CPU. num_threads should be greater than 0 or equal to -1. Setting num_threads to -1 has the effect to let TFLite runtime set the value. Default to number of physical CPU cores, or -1 if WASM multi-threading is not supported by user's browser. A custom TFLite object detection model loaded from a model url or an in memory. The underlying object detector is built on top of the TFLite Task Library. As a result, the custom model needs to meet the metadata requirements. Refer to tfTask.ObjectDetector for the and method.\n• . The model url, or the model content stored in an You can use TFLite model urls from directly. For model compatibility, see comments in the corresponding model class.\n• Maximum number of top scored results to return. If < 0, all results will be returned. If 0, an invalid argument error is returned.\n• Score threshold in [0,1), overrides the ones provided in the model metadata (if any). Results below this value are rejected.\n• The number of threads to be used for TFLite ops that support multi-threading when running inference with CPU. num_threads should be greater than 0 or equal to -1. Setting num_threads to -1 has the effect to let TFLite runtime set the value. Default to number of physical CPU cores, or -1 if WASM multi-threading is not supported by user's browser.\n\nThe task of detecting sentiments in a given paragraph of text. The base class for all SentimentDetection task models. Detects sentiment on the given text, and returns result.\n• The text to detect sentiment on.\n• Inference options. Different models have different inference options. See individual model for more details.\n• A map from sentiment labels to their detection result along with the raw probabilities ([negative probability, positive probability]).\n• Whether the sentiment is considered true or false. It is set to null when the result cannot be determined (e.g. below a threshold).\n• The raw probabilities for this sentiment. It detects whether the review text is positive or negetive. The model returns the prediction results of the following sentiment labels: Refer to tfTask.SentimentDetector for the and method, and more details about the result interface.\n• A prediction is considered valid only if its confidence exceeds the threshold. Defaults to 0.65. It detects whether text contains toxic content such as threatening language, insults, obscenities, identity-based hate, or sexually explicit language. By default, the model returns the prediction results of the following sentiment labels: Refer to for the and method, and more details about the result interface.\n• An array of strings indicating which types of toxicity to detect. Labels must be one of | | | | | | . Defaults to all labels.\n• The backend to use to run TFJS models. Default to 'webgl'.\n• A prediction is considered valid only if its confidence exceeds the threshold. Defaults to 0.65.\n\nThe task of answering questions based on the content of a given passage. The base class for all Q&A task models. Gets the answer to the given question based on the content of a given passage.\n• Context where the answer are looked up from.\n• The index of the starting character of the answer in the passage.\n• The index of the last character of the answer text. Refer to tfTask.QuestionAnswerer for the and method.\n• The backend to use to run TFJS models. Default to 'webgl'. Refer to tfTask.QuestionAnswerer for the and method. A custom TFLite Q&A model loaded from a model url or an in memory. The underlying question answerer is built on top of the TFLite Task Library. As a result, the custom model needs to meet the metadata requirements. Refer to tfTask.QuestionAnswerer for the and method.\n• . The model url, or the model content stored in an You can use TFLite model urls from directly. For model compatibility, see comments in the corresponding model class."
    },
    {
        "link": "https://js.tensorflow.org/api_tasks/0.0.1-alpha.6",
        "document": "TFJS Task API groups models into different tasks. To use a specific model, you first need to load it, then call the method on the model to run the inference. To load a model, use a model loader as follows. Do not construct the model manually. Please refer to a specific model below for details about the exact model loader to use and the corrsponding options. All loaded models have a predict method defined. Call it with model-specific input and options to get the inference result. Please refer to a specific model below for details about the input and the corresponding options. All loaded models have a method defined to clean up resources. The model should not be used after this call.\n\nThe task of classifying images into a preset of labels. The base class for all ImageClassification task models. Performs classification on the given image-like input, and returns result.\n• The image-like element to run classification on.\n• Inference options. Different models have different inference options. See individual model for more details. A custom TFLite image classification model loaded from a model url or an in memory. The underlying image classifier is built on top of the TFLite Task Library. As a result, the custom model needs to meet the metadata requirements. Refer to tfTask.ImageClassifier for the and method.\n• . The model url, or the model content stored in an You can use TFLite model urls from directly. For model compatibility, see comments in the corresponding model class.\n• Maximum number of top scored results to return. If < 0, all results will be returned. If 0, an invalid argument error is returned.\n• Score threshold in [0,1), overrides the ones provided in the model metadata (if any). Results below this value are rejected.\n• The number of threads to be used for TFLite ops that support multi-threading when running inference with CPU. num_threads should be greater than 0 or equal to -1. Setting num_threads to -1 has the effect to let TFLite runtime set the value. Default to number of physical CPU cores, or -1 if WASM multi-threading is not supported by user's browser. Refer to tfTask.ImageClassifier for the and method.\n• The backend to use to run TFJS models. Default to 'webgl'. Refer to tfTask.ImageClassifier for the and method.\n• The MobileNet version number. Use 1 for MobileNetV1, and 2 for MobileNetV2. Defaults to 1.\n• Controls the width of the network, trading accuracy for performance. A smaller alpha decreases accuracy and increases performance. Defaults to 1.0.\n• Maximum number of top scored results to return. If < 0, all results will be returned. If 0, an invalid argument error is returned.\n• Score threshold in [0,1), overrides the ones provided in the model metadata (if any). Results below this value are rejected.\n• The number of threads to be used for TFLite ops that support multi-threading when running inference with CPU. num_threads should be greater than 0 or equal to -1. Setting num_threads to -1 has the effect to let TFLite runtime set the value. Default to number of physical CPU cores, or -1 if WASM multi-threading is not supported by user's browser.\n\nThe task of predicting associated class for each pixel of an image. The base class for all ImageSegmentation task models. Performs segmentation on the given image-like input, and returns result.\n• The image-like element to run segmentation on.\n• Inference options. Different models have different inference options. See individual model for more details.\n• The width of the returned segmentation map.\n• The height of the returned segmentation map.\n• The colored segmentation map as which can be fed into and mapped to a canvas.\n• The red color component for the label, in the [0, 255] range.\n• The green color component for the label, in the [0, 255] range.\n• The blue color component for the label, in the [0, 255] range. Refer to tfTask.ImageSegmenter for the and method.\n• The backend to use to run TFJS models. Default to 'webgl'. Refer to tfTask.ImageSegmenter for the and method.\n• The number of threads to be used for TFLite ops that support multi-threading when running inference with CPU. num_threads should be greater than 0 or equal to -1. Setting num_threads to -1 has the effect to let TFLite runtime set the value. Default to number of physical CPU cores, or -1 if WASM multi-threading is not supported by user's browser. A custom TFLite image segmentation model loaded from a model url or an in memory. The underlying image segmenter is built on top of the TFLite Task Library. As a result, the custom model needs to meet the metadata requirements. Refer to tfTask.ImageSegmenter for the and method.\n• . The model url, or the model content stored in an You can use TFLite model urls from directly. For model compatibility, see comments in the corresponding model class.\n• The number of threads to be used for TFLite ops that support multi-threading when running inference with CPU. num_threads should be greater than 0 or equal to -1. Setting num_threads to -1 has the effect to let TFLite runtime set the value. Default to number of physical CPU cores, or -1 if WASM multi-threading is not supported by user's browser.\n\nThe task of localizing and identifing multiple objects in a single image. The base class for all ObjectDetection task models. Detects objects on the given image-like input, and returns result.\n• The image-like element to detect objects on.\n• Inference options. Different models have different inference options. See individual model for more details.\n• The bounding box of the object.\n• The X coordinate of the top-left corner of the bounding box.\n• The Y coordinate of the top-left corner of the bounding box. Refer to tfTask.ObjectDetector for the and method.\n• The backend to use to run TFJS models. Default to 'webgl'.\n• The maximum number of bounding boxes of detected objects. There can be multiple objects of the same class, but at different locations. Defaults to 20.\n• The minimum score of the returned bounding boxes of detected objects. Value between 0 and 1. Defaults to 0.5. Refer to tfTask.ObjectDetector for the and method.\n• Maximum number of top scored results to return. If < 0, all results will be returned. If 0, an invalid argument error is returned.\n• Score threshold in [0,1), overrides the ones provided in the model metadata (if any). Results below this value are rejected.\n• The number of threads to be used for TFLite ops that support multi-threading when running inference with CPU. num_threads should be greater than 0 or equal to -1. Setting num_threads to -1 has the effect to let TFLite runtime set the value. Default to number of physical CPU cores, or -1 if WASM multi-threading is not supported by user's browser. A custom TFLite object detection model loaded from a model url or an in memory. The underlying object detector is built on top of the TFLite Task Library. As a result, the custom model needs to meet the metadata requirements. Refer to tfTask.ObjectDetector for the and method.\n• . The model url, or the model content stored in an You can use TFLite model urls from directly. For model compatibility, see comments in the corresponding model class.\n• Maximum number of top scored results to return. If < 0, all results will be returned. If 0, an invalid argument error is returned.\n• Score threshold in [0,1), overrides the ones provided in the model metadata (if any). Results below this value are rejected.\n• The number of threads to be used for TFLite ops that support multi-threading when running inference with CPU. num_threads should be greater than 0 or equal to -1. Setting num_threads to -1 has the effect to let TFLite runtime set the value. Default to number of physical CPU cores, or -1 if WASM multi-threading is not supported by user's browser."
    },
    {
        "link": "https://tensorflow.org/js/guide/models_and_layers",
        "document": "New to machine learning? Watch a video course to get practical working knowledge of ML using web technologies View series\n\nStay organized with collections Save and categorize content based on your preferences.\n\nIn machine learning, a model is a function with learnable parameters that maps an input to an output. The optimal parameters are obtained by training the model on data. A well-trained model will provide an accurate mapping from the input to the desired output.\n\nIn TensorFlow.js there are two ways to create a machine learning model:\n• using the Layers API where you build a model using layers.\n• using the Core API with lower-level ops such as , , etc.\n\nFirst, we will look at the Layers API, which is a higher-level API for building models. Then, we will show how to build the same model using the Core API.\n\nThere are two ways to create a model using the Layers API: A sequential model, and a functional model. The next two sections look at each type more closely.\n\nThe most common type of model is the model, which is a linear stack of layers. You can create a model by passing a list of layers to the function:\n\nOr via the method:\n\nYou can access the layers of the model via , and more specifically and .\n\nAnother way to create a is via the function. The key difference between and is that allows you to create an arbitrary graph of layers, as long as they don't have cycles.\n\nHere is a code snippet that defines the same model as above using the API:\n\nWe call on each layer in order to connect it to the output of another layer. The result of in this case is a , which acts like a but without any concrete values.\n\nNote that unlike the sequential model, we create a via instead of providing an to the first layer.\n\ncan also give you a concrete , if you pass a concrete to it:\n\nThis can be useful when testing layers in isolation and seeing their output.\n\nJust like in a sequential model, you can access the layers of the model via , and more specifically and .\n\nBoth the sequential model and the functional model are instances of the class. One of the major benefits of working with a is validation: it forces you to specify the input shape and will use it later to validate your input. The also does automatic shape inference as the data flows through the layers. Knowing the shape in advance allows the model to automatically create its parameters, and can tell you if two consecutive layers are not compatible with each other.\n\nCall to print a useful summary of the model, which includes:\n• Name and type of all layers in the model.\n• Number of weight parameters of each layer.\n• If the model has general topology (discussed below), the inputs each layer receives\n• The total number of trainable and non-trainable parameters of the model.\n\nFor the model we defined above, we get the following output on the console:\n\nNote the values in the output shapes of the layers: a reminder that the model expects the input to have a batch size as the outermost dimension, which in this case can be flexible due to the value.\n\nOne of the major benefits of using a over the lower-level API is the ability to save and load a model. A knows about:\n• the architecture of the model, allowing you to re-create the model.\n• the weights of the model\n• the state of the optimizer, allowing you to resume training.\n\nTo save or load a model is just 1 line of code:\n\nThe example above saves the model to local storage in the browser. See the and the save and load guide for how to save to different mediums (e.g. file storage, , trigger a browser download, etc.)\n\nLayers are the building blocks of a model. If your model is doing a custom computation, you can define a custom layer, which interacts well with the rest of the layers. Below we define a custom layer that computes the sum of squares:\n\nTo test it, we can call the method with a concrete tensor:\n\nIn the beginning of this guide, we mentioned that there are two ways to create a machine learning model in TensorFlow.js.\n\nThe general rule of thumb is to always try to use the Layers API first, since it is modeled after the well-adopted Keras API which follows best practices and reduces cognitive load. The Layers API also offers various off-the-shelf solutions such as weight initialization, model serialization, monitoring training, portability, and safety checking.\n\nYou may want to use the Core API whenever:\n• You need maximum flexibility or control.\n• You don't need serialization, or can implement your own serialization logic.\n\nModels in the Core API are just functions that take one or more and return a . The same model as above written using the Core API looks like this:\n\nNote that in the Core API we are responsible for creating and initializing the weights of the model. Every weight is backed by a which signals to TensorFlow.js that these tensors are learnable. You can create a using tf.variable() and passing in an existing .\n\nIn this guide you have familiarized yourself with the different ways to create a model using the Layers and the Core API. Next, see the training models guide for how to train a model."
    },
    {
        "link": "https://stackoverflow.com/questions/61096228/loading-keras-model-into-tensorflow-js-locally",
        "document": "I had this problem too.\n\nUsing generated the same error you had. I tried changing the import to find file , which didn't exist yet gave the same error (I notice there's an outstanding issue with TensorFlow.js to give a more meaningful error).\n\nLooking at my Parcel setup, I realised that the and files were being copied into the root of the folder. So I removed the directory from the import and now it works with:\n\nNote that my files are still in ."
    }
]