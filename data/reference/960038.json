[
    {
        "link": "https://pandas.pydata.org/pandas-docs/version/1.5.0/reference/api/pandas.DataFrame.pct_change.html",
        "document": "Percentage change between the current and a prior element.\n\nComputes the percentage change from the immediately previous row by default. This is useful in comparing the percentage of change in a time series of elements.\n\nSee the percentage change in a Series where filling NAs with last valid observation forward to next valid.\n\nPercentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01.\n\nPercentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns."
    },
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html",
        "document": "Fractional change between the current and a prior element.\n\nComputes the fractional change from the immediately previous row by default. This is useful in comparing the fraction of change in a time series of elements.\n\nDespite the name of this method, it calculates fractional change (also known as per unit change or relative change) and not percentage change. If you need the percentage change, multiply these values by 100.\n\nHow to handle NAs before computing percent changes. Deprecated since version 2.1: All options of are deprecated except . The number of consecutive NAs to fill before stopping. Increment to use from time series API (e.g. ‘ME’ or BDay()). Additional keyword arguments are passed into or . The same type as the calling object.\n\nCompute the difference of two elements in a Series. Compute the difference of two elements in a DataFrame. Shift the index by some number of periods. Shift the index by some number of periods.\n\nSee the percentage change in a Series where filling NAs with last valid observation forward to next valid.\n\nPercentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01.\n\nPercentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns."
    },
    {
        "link": "https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.pct_change.html",
        "document": "Fractional change between the current and a prior element.\n\nComputes the fractional change from the immediately previous row by default. This is useful in comparing the fraction of change in a time series of elements.\n\nDespite the name of this method, it calculates fractional change (also known as per unit change or relative change) and not percentage change. If you need the percentage change, multiply these values by 100.\n\nSee the percentage change in a Series where filling NAs with last valid observation forward to next valid.\n\nPercentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01.\n\nPercentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns."
    },
    {
        "link": "https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.Series.pct_change.html",
        "document": "Percentage change between the current and a prior element.\n\nComputes the percentage change from the immediately previous row by default. This is useful in comparing the percentage of change in a time series of elements.\n\nSee the percentage change in a Series where filling NAs with last valid observation forward to next valid.\n\nPercentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01.\n\nPercentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns."
    },
    {
        "link": "https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/1.19.0/modin/pandas_api/snowflake.snowpark.modin.pandas.DataFrame.pct_change",
        "document": "Fractional change between the current and a prior element.\n\nComputes the fractional change from the immediately previous row by default. This is useful in comparing the fraction of change in a time series of elements.\n\nDespite the name of this method, it calculates fractional change (also known as per unit change or relative change) and not percentage change. If you need the percentage change, multiply these values by 100.\n• None How to handle NAs before computing percent changes. Deprecated since version 2.1: All options of are deprecated except .\n• None The number of consecutive NAs to fill before stopping. Snowpark pandas does not yet support this parameter.\n• None Increment to use from time series API (e.g. ‘ME’ or BDay()). Snowpark pandas does not yet support this parameter.\n• None Additional keyword arguments are passed into or . Unlike pandas, Snowpark pandas does not use under the hood, and thus may not yet support the passed keyword arguments. The same type as the calling object.\n\nCompute the difference of two elements in a Series. Compute the difference of two elements in a DataFrame. Shift the index by some number of periods. Shift the index by some number of periods.\n\nSee the percentage change in a Series where filling NAs with last valid observation forward to next valid.\n\nPercentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01.\n\nPercentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns."
    },
    {
        "link": "https://stackoverflow.com/questions/23981601/format-certain-floating-dataframe-columns-into-percentage-in-pandas",
        "document": "The accepted answer suggests to modify the raw data for presentation purposes, something you generally do not want. Imagine you need to make further analyses with these columns and you need the precision you lost with rounding.\n\nYou can modify the formatting of individual columns in data frames, in your case:\n\nFor your information yields , so no need for multiplying by 100.\n\nYou don't have a nice HTML table anymore but a text representation. If you need to stay with HTML use the function instead.\n\nAs of pandas 0.17.1, life got easier and we can get a beautiful html table right away:"
    },
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html",
        "document": "Fractional change between the current and a prior element.\n\nComputes the fractional change from the immediately previous row by default. This is useful in comparing the fraction of change in a time series of elements.\n\nDespite the name of this method, it calculates fractional change (also known as per unit change or relative change) and not percentage change. If you need the percentage change, multiply these values by 100.\n\nHow to handle NAs before computing percent changes. Deprecated since version 2.1: All options of are deprecated except . The number of consecutive NAs to fill before stopping. Increment to use from time series API (e.g. ‘ME’ or BDay()). Additional keyword arguments are passed into or . The same type as the calling object.\n\nCompute the difference of two elements in a Series. Compute the difference of two elements in a DataFrame. Shift the index by some number of periods. Shift the index by some number of periods.\n\nSee the percentage change in a Series where filling NAs with last valid observation forward to next valid.\n\nPercentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01.\n\nPercentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns."
    },
    {
        "link": "https://stackoverflow.com/questions/64389558/percentage-format-does-not-changes-to-float-pandas",
        "document": "I'm having a problem with converting my data to fro dataframe to percentage format and keep it as a float.\n\nI prepared a simple code thats reflects the code from my actual project:\n\nIn my actual project I need to choose ONLY columns that contain specific string and do some calculation on their values. At the end I have to change the formatting to percentage with 4 decimal places. Eventhough I use my values are still . Consequently, when I save dataframe to excel file, values are pasted as text and not as number. In addition, while creating a line chart from this dataframe, I get an error 'unhashable type: 'numpy.ndarray'.\n\nPlease advise on how to succefully convert data to percentage format and keep it as a float in order to get accurate paste in excel file and creating a line chart with matplotlib."
    },
    {
        "link": "https://medium.com/@tubelwj/how-to-remove-non-numeric-data-from-a-pandas-dataframe-c506ab9dbbbd",
        "document": "During the process of data analysis, it’s common to encounter DataFrames that contain non-numeric data, such as strings, dates, boolean values, and so on. These non-numeric data can impact our calculations and statistics, leading to inaccurate results or errors. Therefore, it’s essential to clean, remove, or convert these data to numeric types.\n\nIn this post, we will explore how to achieve this using Pandas, a powerful Python data analysis library. We’ll discuss three different methods:\n\n1. Using the `pd.to_numeric` function\n\n2. Using the `str.isdecimal` method\n\n3. Using regular expressions\n\nThe post will cover the principles, advantages, disadvantages, and code implementations for each of these methods.\n\nBasically, for each column in the DataFrame, we will check its data type. If it is a non-numeric type, attempt to convert it to a numeric type or delete rows containing non-numeric data.\n\nThe `pd.to_numeric` function can convert data of string or object types to numeric types, returning NaN if the conversion is not possible. We can then use the `notna` method to filter out rows containing NaN. The advantage of this method is its ability to handle special numeric formats such as scientific notation, currency symbols, percentages, etc. The drawback is that it may result in the loss of some original data information, such as decimal places or other characters in strings.\n\nAs seen below, we successfully removed non-numeric data, retaining only numeric data.\n\nThe `str.isdecimal` method can determine whether a string represents a decimal number, returning a boolean value. We can use this method to check if each cell’s content is a number, then use boolean indexing to select rows that meet the criteria. The advantage of this method is its more accurate identification of numeric types, avoiding the accidental deletion of numbers with decimal points or negative signs. The drawback is that it may miss some special numeric formats like scientific notation, currency symbols, percentages, etc.\n\nHere’s an example code: Suppose we have a DataFrame with a column named “score” storing student grades, but some data is non-numeric, such as A, B, C, or 100%. We want to delete this non-numeric data, keeping only numeric data.\n\nYou can see that we have successfully removed non-numeric data, keeping only numeric data.\n\nRegular expressions are patterns used to match strings. We can use them to define the format of numeric types, such as whether they contain a negative sign, decimal point, comma, etc. Then, we can use the `str.match` method to match strings based on the regular expression. If the match is successful, it returns True; otherwise, it returns False. We can then use boolean indexing to select the matching rows. The advantage of this method is its flexibility in defining the format of numeric types, covering more cases. The drawback is that it may require writing complex regular expressions or multiple matches for different formats.\n\nHere’s an example code: Suppose we have a DataFrame with a column named “price” storing the prices of goods, but some data is non-numeric, such as Free, N/A, or $10.00. We want to remove this non-numeric data, keeping only numeric data.\n\nYou can see that we have successfully removed non-numeric data, retaining only numeric data.\n\nDuring the process of removing non-numeric data using Pandas, it’s essential to consider the following:\n\n1. Choose an appropriate method based on the specific characteristics of the data. Different methods may yield different results and have varying applicability.\n\n2. Check the data types to ensure that the data is of string or object type before applying the mentioned methods. If the data is of other types, such as dates, booleans, or numeric types, it may be necessary to convert them to string type first or use alternative methods.\n\n3. Be caution when deleting non-numeric data, as it may lead to data loss or bias. Sometimes, non-numeric data may contain valuable information, such as categories, labels, comments, etc. The decision to delete or retain such data should be based on the analysis goals and whether the information is relevant to the analysis.\n\nThat’s all tips on how to remove non-numeric data from a Pandas dataframe. Thakns for your reading."
    },
    {
        "link": "https://pbpython.com/pandas_dtypes.html",
        "document": "When doing data analysis, it is important to make sure you are using the correct data types; otherwise you may get unexpected results or errors. In the case of pandas, it will correctly infer data types in many cases and you can move on with your analysis without any further thought on the topic. Despite how well pandas works, at some point in your data analysis processes, you will likely need to explicitly convert data from one type to another. This article will discuss the basic pandas data types (aka ), how they map to python and numpy data types and the options for converting from one pandas type to another.\n\nA data type is essentially an internal construct that a programming language uses to understand how to store and manipulate data. For instance, a program needs to understand that you can add two numbers together like 5 + 10 to get 15. Or, if you have two strings such as “cat” and “hat” you could concatenate (add) them together to get “cathat.” A possible confusing point about pandas data types is that there is some overlap between pandas, python and numpy. This table summarizes the key points: For the most part, there is no need to worry about determining if you should try to explicitly force the pandas type to a corresponding to NumPy type. Most of the time, using pandas default and types will work. The only reason I included in this table is that sometimes you may see the numpy types pop up on-line or in your own analysis. For this article, I will focus on the follow pandas types: The and types are better served in an article of their own if there is interest. However, the basic approaches outlined in this article apply to these types as well. One other item I want to highlight is that the data type can actually contain multiple different types. For instance, the a column could include integers, floats and strings which collectively are labeled as an . Therefore, you may need some additional techniques to handle mixed data types in columns. Refer to this article for an example the expands on the currency cleanups described below.\n\nWhy do we care? Data types are one of those things that you don’t tend to care about until you get an error or some unexpected results. It is also one of the first things you should check once you load a new data into pandas for further analysis. I will use a very simple CSV file to illustrate a couple of common errors you might see in pandas if the data type is not correct. Additionally, an example notebook is up on github. Upon first glance, the data looks ok so we could try doing some operations to analyze the data. Let’s try adding together the 2016 and 2017 sales: This does not look right. We would like to get totals added together but pandas is just concatenating the two values together to create one long string. A clue to the problem is the line that says An is a string in pandas so it performs a string operation instead of a mathematical one. If we want to see what all the data types are in a dataframe, use Additionally, the function shows even more useful info. After looking at the automatically assigned data types, there are several concerns:\n• The is a but it should be an\n• The and columns are stored as objects, not numerical values such as a or\n• and are also stored as objects not numerical values\n• We have , and columns that should be converted to\n• The column should be a boolean Until we clean up these data types, it is going to be very difficult to do much additional analysis on this data. In order to convert data types in pandas, there are three basic options:\n• Use to force an appropriate\n• Use pandas functions such as or\n\nThe simplest way to convert a pandas column of data to a different type is to use . For instance, to convert the to an integer we can call it like this: In order to actually change the customer number in the original dataframe, make sure to assign it back since the functions returns a copy. And here is the new data frame with the Customer Number as an integer: This all looks good and seems pretty simple. Let’s try to do the same thing to our column and convert it to a floating point number: In a similar manner, we can try to conver the column to an integer: Both of these return exceptions which mean that the conversions did not work. In each of the cases, the data included values that could not be interpreted as numbers. In the sales columns, the data includes a currency symbol as well as a comma in each value. In the columnm the last value is “Closed” which is not a number; so we get the exception. So far it’s not looking so good for as a tool. We should give it one more try on the column. At first glance, this looks ok but upon closer inspection, there is a big problem. All values were interpreted as but the last customer has an Active flag of so this does not seem right. The takeaway from this section is that will only work if:\n• the data is clean and can be simply interpreted as a number\n• you want to convert a numeric value to a string object If the data has non-numeric characters or is not homogeneous, then will not be a good choice for type conversion. You will need to do additional transforms for the type change to work correctly.\n\nSince this data is a little more complex to convert, we can build a custom function that we apply to each value and convert to the appropriate data type. For currency conversion (of this specific data set), here is a simple function we can use: Convert the string number value to a float The code uses python’s string functions to strip out the ‘$” and ‘,’ and then convert the value to a floating point number. In this specific case, we could convert the values to integers as well but I’m choosing to use floating point in this case. I also suspect that someone will recommend that we use a type for currency. This is not a native data type in pandas so I am purposely sticking with the float approach. Also of note, is that the function converts the number to a python but pandas internally converts it to a As mentioned earlier, I recommend that you allow pandas to convert to specific size or as it determines appropriate. There is no need for you to try to downcast to a smaller or upcast to a larger byte size unless you really know why you need to do it. Now, we can use the pandas function to apply this to all the values in the 2016 column. Success! All the values are showing as so we can do all the math functions we need to. I’m sure that the more experienced readers are asking why I did not just use a lambda function? Before I answer, here is what we could do in 1 line with a function: Using we can streamline the code into 1 line which is a perfectly valid approach. I have three main concerns with this approach:\n• If you are just learning python/pandas or if someone new to python is going to be maintaining code, I think the longer function is more readable. The primary reason is that it includes comments and can be broken down into a couple of steps. functions are a little more difficult for the new user to grasp.\n• Secondly, if you are going to be using this function on multiple columns, I prefer not to duplicate the long lambda function.\n• Finally, using a function makes it easy to clean up the data when using I will cover usage at the end of the article. Some may also argue that other lambda-based approaches have performance improvements over the custom function. That may be true but for the purposes of teaching new users, I think the function approach is preferrable. Here’s a full example of converting the data in both sales columns using the function. For another example of using vs. a function, we can look at the process for fixing the column. Doing the same thing with a custom function: Convert the percentage string to an actual floating point percent Both produce the same value: The final custom function I will cover is using to convert the active column to a boolean. There are several possible ways to solve this specific problem. The approach is useful for many types of problems so I’m choosing to include it here. The basic idea is to use the function to convert all “Y” values to and everything else assigned Which results in the following dataframe: The dtype is appropriately set to . Whether you choose to use a function, create a more standard python function or use another approach like , these approaches are very flexible and can be customized for your own unique data needs.\n\nPandas has a middle ground between the blunt function and the more complex custom functions. These helper functions can be very useful for certain data type conversions. If you have been following along, you’ll notice that I have not done anything with the date columns or the column. Both of these can be converted simply using built in pandas functions such as and . The reason the conversion is problematic is the inclusion of a non-numeric value in the column. If we tried to use we would get an error (as described earlier). The function can handle these values more gracefully: There are a couple of items of note. First, the function easily processes the data and creates a column. Additionally, it replaces the invalid “Closed” value with a value because we passed . We can leave that value there or fill it in with a 0 using : The final conversion I will cover is converting the separate month, day and year columns into a . The pandas function is quite configurable but also pretty smart by default. In this case, the function combines the columns into a new series of the appropriate dtype. We need to make sure to assign these values back to the dataframe:\n\nNow the data is properly converted to all the types we need:\n\nThe dataframe is ready for analysis!\n\nBringing it all together The basic concepts of using and custom functions can be included very early in the data intake process. If you have a data file that you intend to process repeatedly and it always comes in the same format, you can define the and to be applied when reading the data. It is helpful to think of as performing on the data. The arguments allow you to apply functions to the various input columns similar to the approaches outlined above. It is important to note that you can only apply a or a function to a specified column once using this approach. If you try to apply both to the same column, then the dtype will be skipped. Here is a streamlined example that does almost all of the conversion at the time the data is read into the dataframe: As mentioned earlier, I chose to include a example as well as the function example for converting data. The only function that can not be applied here is the conversion of the , and columns to the corresponding column. Still, this is a powerful convention that can help improve your data processing pipeline."
    }
]