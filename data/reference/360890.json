[
    {
        "link": "https://stackoverflow.com/questions/3286525/return-sql-table-as-json-in-python",
        "document": "I'm playing around with a little web app in web.py, and am setting up a url to return a JSON object. What's the best way to convert a SQL table to JSON using python?\n\nHere is a really nice example of a pythonic way to do that: import json import psycopg2 def db(database_name='pepe'): return psycopg2.connect(database=database_name) def query_db(query, args=(), one=False): cur = db().cursor() cur.execute(query, args) r = [dict((cur.description[i][0], value) \\ for i, value in enumerate(row)) for row in cur.fetchall()] cur.connection.close() return (r[0] if r else None) if one else r my_query = query_db(\"select * from majorroadstiger limit %s\", (3,)) json_output = json.dumps(my_query) You get an array of JSON objects:\n\nMore information about how you'll be working with your data before transferring it would help a ton. The json module provides dump(s) and load(s) methods that'll help if you're using 2.6 or newer: http://docs.python.org/library/json.html. Without knowing which libraries you're using I can't tell you for sure if you'll find a method like that. Normally, I'll process query results like this (examples with kinterbasdb because it's what we're currently working with): qry = \"Select Id, Name, Artist, Album From MP3s Order By Name, Artist\" # Assumes conn is a database connection. cursor = conn.cursor() cursor.execute(qry) rows = [x for x in cursor] cols = [x[0] for x in cursor.description] songs = [] for row in rows: song = {} for prop, val in zip(cols, row): song[prop] = val songs.append(song) # Create a string representation of your array of songs. songsJSON = json.dumps(songs) There are undoubtedly better experts out there who'll have list comprehensions to eliminate the need for written out loops, but this works and should be something you could adapt to whatever library you're retrieving records with.\n\nI knocked together a short script that dumps all data from all tables, as dicts of column name : value. Unlike other solutions, it doesn't require any info about what the tables or columns are, it just finds everything and dumps it. Hope someone finds it useful! from contextlib import closing from datetime import datetime import json import MySQLdb DB_NAME = 'x' DB_USER = 'y' DB_PASS = 'z' def get_tables(cursor): cursor.execute('SHOW tables') return [r[0] for r in cursor.fetchall()] def get_rows_as_dicts(cursor, table): cursor.execute('select * from {}'.format(table)) columns = [d[0] for d in cursor.description] return [dict(zip(columns, row)) for row in cursor.fetchall()] def dump_date(thing): if isinstance(thing, datetime): return thing.isoformat() return str(thing) with closing(MySQLdb.connect(user=DB_USER, passwd=DB_PASS, db=DB_NAME)) as conn, closing(conn.cursor()) as cursor: dump = { table: get_rows_as_dicts(cursor, table) for table in get_tables(cursor) } print(json.dumps(dump, default=dump_date, indent=2))\n\nReturn a single row of values from a select query like below. cur.execute(f\"select name,userid, address from table1 where userid = 1 \") row = cur.fetchone() desc = list(zip(*cur.description))[0] #To get column names rowdict = dict(zip(desc,row)) jsondict = jsonify(rowdict) #Flask jsonify is a tuple of tuples as below. and to combine column name with values\n\nOne simple example for return SQL table as formatted JSON and fix error as he had @Whitecat I get the error datetime.datetime(1941, 10, 31, 0, 0) is not JSON serializable In that example you should use JSONEncoder. import json import pymssql # subclass JSONEncoder class DateTimeEncoder(JSONEncoder): #Override the default method def default(self, obj): if isinstance(obj, (datetime.date, datetime.datetime)): return obj.isoformat() def mssql_connection(): try: return pymssql.connect(server=\"IP.COM\", user=\"USERNAME\", password=\"PASSWORD\", database=\"DATABASE\") except Exception: print(\"\n\nERROR: Unable to connect to the server.\") exit(-1) def query_db(query): cur = mssql_connection().cursor() cur.execute(query) r = [dict((cur.description[i][0], value) for i, value in enumerate(row)) for row in cur.fetchall()] cur.connection.close() return r def write_json(query_path): # read sql from file with open(\"../sql/my_sql.txt\", 'r') as f: sql = f.read().replace('\n\n', ' ') # creating and writing to a json file and Encode DateTime Object into JSON using custom JSONEncoder with open(\"../output/my_json.json\", 'w', encoding='utf-8') as f: json.dump(query_db(sql), f, ensure_ascii=False, indent=4, cls=DateTimeEncoder) if __name__ == \"__main__\": write_json() # You get formatted my_json.json, for example: [ { \"divroad\":\"N\", \"featcat\":null, \"countyfp\":\"001\", \"date\":\"2020-08-28\" } ]"
    },
    {
        "link": "https://geeksforgeeks.org/working-with-json-in-sql",
        "document": "JSON stands for Javascript Object Notation. It is mainly used in storing and transporting data. Mostly all NoSQL databases like MongoDB, CouchDB, etc., use JSON format data. Whenever your data from one server has to be transferred to a web page, JSON format is the preferred format for front-end applications like Android, iOS, React, Angular, etc.\n\nIn this article, we will learn how to store, retrieve, and manipulate JSON data in SQL Server using various SQL functions. We will learn how JSON fits into SQL, demonstrate how to store JSON data in SQL tables and cover the most common JSON functions like ISJSON(), JSON_VALUE(), JSON_MODIFY(), and more.\n\nWhat is JSON in SQL Server?\n\nJSON is a lightweight data-interchange format that is easy for humans to read and write. SQL Server introduced native support for JSON handling starting from SQL Server 2016. This allows you to store JSON data in NVARCHAR columns and use SQL functions to parse, query, and modify JSON data.\n\nIn SQL Server, you can store JSON data as a string in an NVARCHAR column. SQL Server treats JSON data as a string, allowing you to parse it when necessary.\n\nNow let us create a table named “Authors” and let us insert some data into it as shown below:\n\nJSON is a beautiful option for bridging NoSQL and relational worlds. Hence, in case if you have the data got exported from MongoDB and need to import them in SQL Server, we can follow below approaches\n\nJSON documents can be stored as-is in NVARCHAR columns either in LOB storage format or Relational storage format. Raw JSON documents have to be parsed, and they may contain Non-English text. By using nvarchar(max) data type, we can store JSON documents with a max capacity of 2 GB in size. If the JSON data is not huge, we can go for NVARCHAR(4000), or else we can go for NVARCHAR(max) for performance reasons.\n\nThe main reason for keeping the JSON document in NVARCHAR format is for Cross feature compatibility. NVARCHAR works with X feature i.e. all the SQL server components such as Hekaton(OLTP), temporal, or column store tables, etc. As JSON behavior is also in that way, it is represented as NVARCHAR datatype.\n\nBefore SQL Server 2016, JSON was stored in the database as text. Hence, there was a need to change the database schema and migration occurred as JSON type in NVarchar format\n\nJSON is just treated as an Object in JavaScript and hence called as Javascript Object Notation. There is no specific standardized JSON object type on client-side available similar to XmlDom object.\n\nLet us see the important functionalities available in SQL Server which can be used with JSON data.\n\nThis function is used to check whether the given input json string is in JSON format or not. If it is in JSON format, it returns 1 as output or else 0. i.e. it returns either 1 or 0 in INT format.\n\nThe output will be a scalar value from the given JSON string. Parsing of JSON string is done and there are some specific formats are there for providing the path. For example\n\nUsed to extract an array of data or objects from the JSON string.\n\nThere is an option called “JSON_MODIFY” in (Transact-SQL) function is available to update the value of a property in a JSON string and return the updated JSON string. Whenever there is a requirement to change JSON text, we can do that\n\nThis function is used for Exporting SQL Server data as JSON format. This is a useful function to export SQL data into JSON format. There are two options available with FOR JSON\n• AUTO: As it is nested JSON sub-array is created based on the table hierarchy.\n• PATH: By using this we can define the structure of JSON in a customized way.\n\nThis function is used for importing JSON as String data. We can import JSON as a text file by using OPENROWSET function and in that the BULK option should be enabled. It returns a single string field with BulkColumn as its column name.\n\nNote: Even large data also can be placed. As a sample, we showed only a single row.\n\nSINGLE_BLOB, which reads a file as varbinary(max). SINGLE_NCLOB, which reads a file as nvarchar(max) — If the contents are in Non-English text like Japanese or Chinese etc., data, we need to go in this pattern. We used SINGLE_CLOB, which reads a file as varchar(max).\n\nIt will generate a relational table with its contents from the JSON string. Each row is created which can be got by iterating through JSON object elements, OPENJSON can be used to parse the JSON as a text. Let us have a JSON placed in an external file and its contents are\n\nWe can see that for “Strings” key like “authorname” and “skills” got type as 1 and “int” key like “id” and “age” got type as 2. Similarly, for boolean, the type is 3. For arrays, it is 4 and for object, it is 5. OPENJSON parses only the root level of the JSON.\n\nIn case if the JSON is nested, we need to use Path variables\n\nWe can even make the skillsets as columns of data as\n\nSaving the rowset into Table: Here the number of columns should match the count that is present inside with:\n\nThere is an option called “JSON_MODIFY” in (Transact-SQL) function is available to update the value of a property in a JSON string and return the updated JSON string. Whenever there is a requirement to change JSON text, we can do that\n\nHandling JSON in SQL Server enables seamless interaction with modern web applications and NoSQL databases. The ability to store, query, and manipulate JSON data directly in SQL Server enhances the flexibility and efficiency of your data management system. SQL Server’s native JSON functions—such as ISJSON(), JSON_VALUE(), JSON_QUERY(), and JSON_MODIFY()—make it easier to integrate and work with JSON data without needing a separate NoSQL system."
    },
    {
        "link": "https://datacamp.com/tutorial/json-data-python",
        "document": "In this course, you'll learn the basics of relational databases and how to interact with them."
    },
    {
        "link": "https://learn.microsoft.com/en-us/sql/relational-databases/json/json-data-sql-server?view=sql-server-ver16",
        "document": "Applies to: SQL Server 2016 (13.x) and later versions Azure SQL Database Azure SQL Managed Instance Azure Synapse Analytics SQL database in Microsoft Fabric\n\nJSON is a popular textual data format that's used for exchanging data in modern web and mobile applications. JSON is also used for storing unstructured data in log files or NoSQL databases such as Microsoft Azure Cosmos DB. Many REST web services return results that are formatted as JSON text or accept data that's formatted as JSON. For example, most Azure services, such as Azure Search, Azure Storage, and Azure Cosmos DB, have REST endpoints that return or consume JSON. JSON is also the main format for exchanging data between webpages and web servers by using AJAX calls.\n\nJSON functions, first introduced in SQL Server 2016 (13.x), enable you to combine NoSQL and relational concepts in the same database. You can combine classic relational columns with columns that contain documents formatted as JSON text in the same table, parse and import JSON documents in relational structures, or format relational data to JSON text.\n\nHere's an example of JSON text:\n\nBy using SQL Server built-in functions and operators, you can do the following things with JSON text:\n• Run any Transact-SQL query on the converted JSON objects.\n• Format the results of Transact-SQL queries in JSON format.\n\nThe next sections discuss the key capabilities that SQL Server provides with its built-in JSON support.\n\nThe new json data type that stores JSON documents in a native binary format that provides the following benefits over storing JSON data in varchar/nvarchar:\n• More efficient reads, as the document is already parsed\n• More efficient writes, as the query can update individual values without accessing the entire document\n• No change in compatibility with existing code\n\nUsing the JSON same functions described in this article remain the most efficient way to query the json data type. For more information on the native json data type, see JSON data type.\n\nExtract values from JSON text and use them in queries\n\nIf you have JSON text that's stored in database tables, you can read or modify values in the JSON text by using the following built-in functions:\n• JSON_QUERY (Transact-SQL) extracts an object or an array from a JSON string.\n• JSON_MODIFY (Transact-SQL) changes a value in a JSON string.\n\nIn the following example, the query uses both relational and JSON data (stored in a column named ) from a table called :\n\nApplications and tools see no difference between the values taken from scalar table columns and the values taken from JSON columns. You can use values from JSON text in any part of a Transact-SQL query (including WHERE, ORDER BY, or GROUP BY clauses, window aggregates, and so on). JSON functions use JavaScript-like syntax for referencing values inside JSON text.\n\nFor more information, see Validate, Query, and Change JSON Data with Built-in Functions (SQL Server), JSON_VALUE (Transact-SQL), and JSON_QUERY (Transact-SQL).\n\nIf you must modify parts of JSON text, you can use the JSON_MODIFY (Transact-SQL) function to update the value of a property in a JSON string and return the updated JSON string. The following example updates the value of a property in a variable that contains JSON:\n\nYou don't need a custom query language to query JSON in SQL Server. To query JSON data, you can use standard T-SQL. If you must create a query or report on JSON data, you can easily convert JSON data to rows and columns by calling the rowset function. For more information, see Parse and Transform JSON Data with OPENJSON.\n\nThe following example calls and transforms the array of objects that is stored in the variable to a rowset that can be queried with a standard Transact-SQL statement:\n\ntransforms the array of JSON objects into a table in which each object is represented as one row, and key/value pairs are returned as cells. The output observes the following rules:\n• converts JSON values to the types that are specified in the clause.\n• can handle both flat key/value pairs and nested, hierarchically organized objects.\n• You don't have to return all the fields that are contained in the JSON text.\n• You can optionally specify a path after the type specification to reference a nested property or to reference a property by a different name.\n• The optional prefix in the path specifies that values for the specified properties must exist in the JSON text.\n\nFor more information, see Parse and Transform JSON Data with OPENJSON and OPENJSON (Transact-SQL).\n\nJSON documents might have sub-elements and hierarchical data that can't be directly mapped into the standard relational columns. In this case, you can flatten JSON hierarchy by joining parent entity with sub-arrays.\n\nIn the following example, the second object in the array has sub-array representing person skills. Every sub-object can be parsed using additional function call:\n\nThe array is returned in the first as original JSON text fragment and passed to another function using operator. The second function parses JSON array and return string values as single column rowset that will be joined with the result of the first .\n\njoins first-level entity with sub-array and return flatten resultset. Due to JOIN, the second row is repeated for every skill.\n\nFormat SQL Server data or the results of SQL queries as JSON by adding the clause to a statement. Use to delegate the formatting of JSON output from your client applications to SQL Server. For more information, see Format query results as JSON with FOR JSON.\n\nThe following example uses PATH mode with the clause:\n\nThe clause formats SQL results as JSON text that can be provided to any app that understands JSON. The PATH option uses dot-separated aliases in the SELECT clause to nest objects in the query results.\n\nFor more information, see Format query results as JSON with FOR JSON and FOR Clause (Transact-SQL).\n\nJSON aggregate functions enable construction of JSON objects or arrays based on an aggregate from SQL data.\n• JSON_OBJECTAGG constructs a JSON object from an aggregation of SQL data or columns.\n• JSON_ARRAYAGG constructs a JSON array from an aggregation of SQL data or columns.\n\nUse cases for JSON data in SQL Server\n\nJSON support in SQL Server and Azure SQL Database lets you combine relational and NoSQL concepts. You can easily transform relational to semi-structured data and vice-versa. JSON isn't a replacement for existing relational models, however. Here are some specific use cases that benefit from the JSON support in SQL Server and in SQL Database.\n\nConsider denormalizing your data model with JSON fields in place of multiple child tables.\n\nStore info about products with a wide range of variable attributes in a denormalized model for flexibility.\n\nLoad, query, and analyze log data stored as JSON files with all the power of the Transact-SQL language.\n\nWhen you need real-time analysis of IoT data, load the incoming data directly into the database instead of staging it in a storage location.\n\nTransform relational data from your database easily into the JSON format used by the REST APIs that support your web site.\n\nSQL Server provides a hybrid model for storing and processing both relational and JSON data by using standard Transact-SQL language. You can organize collections of your JSON documents in tables, establish relationships between them, combine strongly typed scalar columns stored in tables with flexible key/value pairs stored in JSON columns, and query both scalar and JSON values in one or more tables by using full Transact-SQL.\n\nJSON text is stored in or columns and is indexed as plain text. Any SQL Server feature or component that supports text supports JSON, so there are almost no constraints on interaction between JSON and other SQL Server features. You can store JSON in In-memory or Temporal tables, apply Row-Level Security predicates on JSON text, and so on.\n\nHere are some use cases that show how you can use the built-in JSON support in SQL Server.\n\nJSON is a textual format so the JSON documents can be stored in columns in a SQL Database. Since type is supported in all SQL Server subsystems you can put JSON documents in tables with clustered columnstore indexes, memory optimized tables, or external files that can be read using OPENROWSET or PolyBase.\n\nTo learn more about your options for storing, indexing, and optimizing JSON data in SQL Server, see the following articles:\n\nYou can format information that's stored in files as standard JSON or line-delimited JSON. SQL Server can import the contents of JSON files, parse it by using the or functions, and load it into tables.\n• None If your JSON documents are stored in local files, on shared network drives, or in Azure Files locations that can be accessed by SQL Server, you can use bulk import to load your JSON data into SQL Server.\n• None If your line-delimited JSON files are stored in Azure Blob storage or the Hadoop file system, you can use PolyBase to load JSON text, parse it in Transact-SQL code, and load it into tables.\n\nIf you must load JSON data from an external service into SQL Server, you can use to import the data into SQL Server instead of parsing the data in the application layer.\n\nIn supported platforms, use the native json data type instead of nvarchar(max) for improved performance and more efficient storage.\n\nYou can provide the content of the JSON variable by an external REST service, send it as a parameter from a client-side JavaScript framework, or load it from external files. You can easily insert, update, or merge results from JSON text into a SQL Server table.\n\nIf you must filter or aggregate JSON data for reporting purposes, you can use to transform JSON to relational format. You can then use standard Transact-SQL and built-in functions to prepare the reports.\n\nYou can use both standard table columns and values from JSON text in the same query. You can add indexes on the expression to improve the performance of the query. For more information, see Index JSON data.\n\nIf you have a web service that takes data from the database layer and returns it in JSON format, or if you have JavaScript frameworks or libraries that accept data formatted as JSON, you can format JSON output directly in a SQL query. Instead of writing code or including a library to convert tabular query results and then serialize objects to JSON format, you can use to delegate the JSON formatting to SQL Server.\n\nFor example, you might want to generate JSON output that's compliant with the OData specification. The web service expects a request and response in the following format:\n\nThis OData URL represents a request for the ProductID and ProductName columns for the product with 1. You can use to format the output as expected in SQL Server.\n\nThe output of this query is JSON text that's fully compliant with the OData spec. Formatting and escaping are handled by SQL Server. SQL Server can also format query results in any format, such as OData JSON or GeoJSON.\n\nTo get the AdventureWorks sample database, download at least the database file and the samples and scripts file from GitHub.\n\nAfter you restore the sample database to an instance of SQL Server, extract the samples file, and then open the file from the JSON folder. Run the scripts in this file to reformat some existing data as JSON data, test sample queries and reports over the JSON data, index the JSON data, and import and export JSON.\n\nHere's what you can do with the scripts that are included in the file:\n• None Denormalize the existing schema to create columns of JSON data.\n• None Store information from , , , , and other tables that contain information related to sales order into JSON columns in the table.\n• None Store information from and tables in the table as arrays of JSON objects.\n• None Import and export JSON. Create and run procedures that export the content of the and the tables as JSON results, and import and update the and the tables by using JSON input.\n• None Run query examples. Run some queries that call the stored procedures and views that you created in steps 2 and 4.\n• None Clean up scripts. Don't run this part if you want to keep the stored procedures and views that you created in steps 2 and 4."
    },
    {
        "link": "https://stackoverflow.com/questions/2097475/how-to-safely-generate-a-sql-like-statement-using-python-db-api",
        "document": "I am trying to assemble the following SQL statement using python's db-api:\n\nwhere BEGINNING_OF_STRING should be a python var to be safely filled in through the DB-API. I tried\n\nI am out of ideas; what is the correct way to do this?"
    },
    {
        "link": "https://reddit.com/r/webdev/comments/1fvcox2/best_practices_for_generating_nested_json",
        "document": "Hey all! I wanted to kick off a bit of a discussion on creating nested JSON responses for an API.\n\nI'd like to produce a nested response to an API request. Let's imagine something like this from `/api/books/:id`\n\nMy main question is, what's the best way to produce this data from our relational database?\n\nWe could build a single SQL query that makes this, and the API end point could just pass along the response.\n• Let's the database handle the data and does it in a single query\n• The shape of a response is kinda sorta business logic so maybe it doesn't belong at the database level?\n• Typing is thrown out the window using the JSON function\n• Depending on the nesting complexity, performance of query could suffer and hog a connection\n\nWe could keep the queries separate and simple, and assemble them in the API\n• Endpoint performance and maintainability suffer by some amount\n• May require more management of DB connections/clients, using transactions or otherwise, and async management (Promise.all, etc)"
    },
    {
        "link": "https://stackoverflow.com/questions/12806386/is-there-any-standard-for-json-api-response-format",
        "document": "Do standards or best practices exist for structuring JSON responses from an API? Obviously, every application's data is different, so that much I'm not concerned with, but rather the \"response boilerplate\", if you will. An example of what I mean:\n\nAssuming you question is about REST webservices design and more precisely concerning success/error. I think there are 3 different types of design.\n• None Use only HTTP Status code to indicate if there was an error and try to limit yourself to the standard ones (usually it should suffice).\n• Pros: It is a standard independent of your api.\n• Cons: Less information on what really happened.\n• None Use HTTP Status + json body (even if it is an error). Define a uniform structure for errors (ex: code, message, reason, type, etc) and use it for errors, if it is a success then just return the expected json response.\n• Pros: Still standard as you use the existing HTTP status codes and you return a json describing the error (you provide more information on what happened).\n• Cons: The output json will vary depending if it is a error or success.\n• None Forget the http status (ex: always status 200), always use json and add at the root of the response a boolean responseValid and a error object (code,message,etc) that will be populated if it is an error otherwise the other fields (success) are populated.\n• None Pros: The client deals only with the body of the response that is a json string and ignores the status(?). It's up to you to choose :) Depending on the API I would choose 2 or 3 (I prefer 2 for json rest apis). Another thing I have experienced in designing REST Api is the importance of documentation for each resource (url): the parameters, the body, the response, the headers etc + examples. I would also recommend you to use jersey (jax-rs implementation) + genson (java/json databinding library). You only have to drop genson + jersey in your classpath and json is automatically supported.\n• None Solution 2 is the hardest to implement but the advantage is that you can nicely handle exceptions and not only business errors, initial effort is more important but you win on the long term.\n• None Solution 3 is the easy to implement on both, server side and client but it's not so nice as you will have to encapsulate the objects you want to return in a response object containing also the responseValid + error.\n\nI will not be as arrogant to claim that this is a standard so I will use the \"I prefer\" form. I prefer terse response (when requesting a list of /articles I want a JSON array of articles). In my designs I use HTTP for status report, a 200 returns just the payload. 400 returns a message of what was wrong with request: If there was error with processing on my side, I return 501 with a message: {\"message\" : \"Could not connect to data store.\"} From what I've seen quite a few REST-ish frameworks tend to be along these lines. JSON is supposed to be a payload format, it's not a session protocol. The whole idea of verbose session-ish payloads comes from the XML/SOAP world and various misguided choices that created those bloated designs. After we realized all of it was a massive headache, the whole point of REST/JSON was to KISS it, and adhere to HTTP. I don't think that there is anything remotely standard in either JSend and especially not with the more verbose among them. XHR will react to HTTP response, if you use jQuery for your AJAX (like most do) you can use / and / callbacks to capture errors. I can't see how encapsulating status reports in JSON is any more useful than that.\n\nFor what it's worth I do this differently. A successful call just has the JSON objects. I don't need a higher level JSON object that contains a success field indicating true and a payload field that has the JSON object. I just return the appropriate JSON object with a 200 or whatever is appropriate in the 200 range for the HTTP status in the header. However, if there is an error (something in the 400 family) I return a well-formed JSON error object. For example, if the client is POSTing a User with an email address and phone number and one of these is malformed (i.e. I cannot insert it into my underlying database) I will return something like this: Important bits here are that the \"field\" property must match the JSON field exactly that could not be validated. This allows clients to know exactly what went wrong with their request. Also, \"message\" is in the locale of the request. If both the \"emailAddress\" and \"phoneNumber\" were invalid then the \"errors\" array would contain entries for both. A 409 (Conflict) JSON response body might look like this: { \"description\" : \"Already Exists\" \"errors\" : [ { \"field\" : \"phoneNumber\", \"message\" : \"Phone number already exists for another user.\" } ], } With the HTTP status code and this JSON the client has all they need to respond to errors in a deterministic way and it does not create a new error standard that tries to complete replace HTTP status codes. Note, these only happen for the range of 400 errors. For anything in the 200 range I can just return whatever is appropriate. For me it is often a HAL-like JSON object but that doesn't really matter here. The one thing I thought about adding was a numeric error code either in the the \"errors\" array entries or the root of the JSON object itself. But so far we haven't needed it.\n\nThe point of JSON is that it is completely dynamic and flexible. Bend it to whatever whim you would like, because it's just a set of serialized JavaScript objects and arrays, rooted in a single node. What the type of the rootnode is is up to you, what it contains is up to you, whether you send metadata along with the response is up to you, whether you set the mime-type to or leave it as is up to you (as long as you know how to handle the edge cases). Build a lightweight schema that you like.\n\n Personally, I've found that analytics-tracking and mp3/ogg serving and image-gallery serving and text-messaging and network-packets for online gaming, and blog-posts and blog-comments all have very different requirements in terms of what is sent and what is received and how they should be consumed. So the last thing I'd want, when doing all of that, is to try to make each one conform to the same boilerplate standard, which is based on XML2.0 or somesuch. That said, there's a lot to be said for using schemas which make sense to you and are well thought out.\n\n Just read some API responses, note what you like, criticize what you don't, write those criticisms down and understand why they rub you the wrong way, and then think about how to apply what you learned to what you need.\n\nI used to follow this standard, was pretty good, easy, and clean on the client layer. Normally, the HTTP status 200, so that's a standard check which I use at the top. and I normally use the following JSON I also use a template for the API's dynamic response; try { // query and what not. response.payload = new { data = new { pagination = new Pagination(), customer = new Customer(), notifications = 5 } } // again something here if we get here success has to be true // I follow an exit first strategy, instead of building a pyramid // of doom. response.success = true; } catch(Exception exception){ response.success = false; response.message = exception.GetStackTrace(); _logger.Fatal(exception, this.GetFacadeName()) } return response; { \"success\": boolean, \"message\": \"some message\", \"payload\": { \"data\" : [] \"message\": \"\" ... // put whatever you want to here. } } on the client layer I would use the following: if(response.code != 200) { // woops something went wrong. return; } if(!response.success){ console.debug ( response.message ); return; } // if we are here then success has to be true. if(response.payload) { .... } notice how I break early avoiding the pyramid of doom.\n\nThere is no lawbreaking or outlaw standard other than common sense. If we abstract this like two people talking, the standard is the best way they can accurately understand each other in minimum words in minimum time. In our case, 'minimum words' is optimizing bandwidth for transport efficiency and 'accurately understand' is the structure for parser efficiency; which ultimately ends up with the less the data, and the common the structure; so that it can go through a pin hole and can be parsed through a common scope (at least initially). Almost in every cases suggested, I see separate responses for 'Success' and 'Error' scenario, which is kind of ambiguity to me. If responses are different in these two cases, then why do we really need to put a 'Success' flag there? Is it not obvious that the absence of 'Error' is a 'Success'? Is it possible to have a response where 'Success' is TRUE with an 'Error' set? Or the way, 'Success' is FALSE with no 'Error' set? Just one flag is not enough? I would prefer to have the 'Error' flag only, because I believe there will be less 'Error' than 'Success'. Also, should we really make the 'Error' a flag? What about if I want to respond with multiple validation errors? So, I find it more efficient to have an 'Error' node with each error as child to that node; where an empty (counts to zero) 'Error' node would denote a 'Success'."
    },
    {
        "link": "https://blog.dreamfactory.com/stored-procedures-data-integration-rest",
        "document": "System administrators, DBAs, and application developers all know about the need for data integration. That’s when you have to pull data from one database, application, or web service to another. Fortunately, this task is a lot less challenging than it used to be because APIs (application programming interfaces) are so popular today. APIs provide a standard way to share the data and services of one system with another, even if the two systems are otherwise incompatible.\n\nThe REST API protocol in particular is frequently used for data integration needs. It’s easy to create API calls using RESTful principles and send data back and forth in JSON, a data format similar to XML but more versatile. And when it comes to storing data, Microsoft SQL Server has been widely adopted by enterprises everywhere.\n\nSQL Server stored procedures make it easy to run code on the database when needed. Developers with data integration needs have worked out many ways to do data integration with REST API code and SQL Server stored procedures. Here’s a look at some of the best REST API calls for data integration.\n\nHere's the key things to know about SQL Server Stored Procedures:\n• Stored Procedures: These are like small programs run on a SQL Server database, capable of accepting input parameters and returning multiple values, including status values to indicate success or failure.\n• SQL Server Configuration for HTTP Requests: To make REST API calls from stored procedures, SQL Server needs configuration to enable HTTP requests using OLE Automation Procedures.\n• Using OLE Automation Procedures: SQL Server offers predefined procedures like sp_OACreate, sp_OAMethod, and sp_OADestroy for making API calls, allowing the creation, method execution, and destruction of OLE objects.\n• REST API with GET and POST Methods: Stored procedures can be used to call REST API endpoints, both for retrieving (GET) and sending (POST) data, handling API responses in JSON format and updating SQL Server tables.\n• Stored Procedures and REST APIs Limitations: While useful for simple data transfers, more complex tasks might require advanced scripting languages. API management platforms can offer a more scalable, secure, and efficient solution for SQL Server data integration needs.\n\nHow Stored Procedures and REST APIs Work Together\n\nStored procedures are like small programs you can run on a SQL Server database. According to Microsoft, stored procedures can:\n• Accept input parameters and return multiple values in the form of output parameters to the calling program.\n• Contain programming statements that perform operations in the database. These include calling other procedures.\n• Return a status value to a calling program to indicate success or failure (and the reason for failure).\n\nLike other small programming languages, stored procedures are perfect for running bits of code on a regular basis. Repetitive and regularly scheduled tasks are handled easily by stored procedures. They’re also handled more efficiently, as each stored procedure is compiled the first time it’s run. This means they run fast and use less overhead on the server.\n\nMost data integration tasks will need to be run more than once. And since you can use code in stored procedures, REST API calls are a perfect fit for stored procedures. Before you get started though, you need to properly configure SQL Server to make HTTP requests from within stored procedures. That’s because RESTful APIs are built upon the HTTP commands that power web services.\n\nTo enable HTTP requests, an administrator will need to enable OLE Automation Procedures on the SQL Server instance. An administrator with the appropriate permissions needs to run these commands:\n\nNow you can make HTTP requests from within stored procedures, enabling you to connect to REST API endpoints.\n\nOnce OLE Automation Procedures have been enabled, SQL Server offers a few predefined procedures that can be used for making API calls. First up is the sp_OACreate method.\n\nsp_OACreate is a simple method for creating an instance of an OLE object. The newly created object is destroyed after all SQL statements have been run. In SQL Server terms, this is known as the completion of the Transact-SQL batch.\n\nThe format for sp_OACreate looks like this:\n\nThe variable OUTPUT is the returned object. It will identify the newly created OLE object and be used to call other OLE Automation stored procedures.\n\nsp_OAMethod is used to call a method of an OLE object. Its format follows this convention:\n\nHere’s what each part of this code means:\n• objecttoken is the token of the OLE object created earlier with sp_OACreate.\n• methodname is the name of the method to be called.\n• returnvalue OUTPUT is the object for the returned value of the method.\n\nEach OLE object is destroyed at the end of a Transact-SQL batch. The sp_OADestroy procedure allows you to destroy the object at a different point. Here’s how to call this procedure:\n\nobjecttoken is, of course, the OLE object created earlier by sp_OACreate.\n\nREST API in SQL Server Stored Procedure: The GET Method\n\nNow that you’re familiar with the OLE predefined procedures, you can put them to use by calling a REST API endpoint in a stored procedure. Here’s an example using the HTTP GET method for retrieving customer data from an API endpoint:\n\nThis stored procedure code does the following:\n• Create an OLE object with the sp_OACreate procedure\n• Pass the OLE object to an HTTP request call\n• Parse the JSON data and insert (or update) records in the customer table\n\nNow that you’ve created a stored procedure to retrieve data via a REST API, the next part of this tutorial is how to do the same with sending data to an API endpoint.\n\nSending data to an API endpoint using the POST HTTP method is similar to doing the reverse with the GET method. The main difference is that you can issue SQL SELECT statements to retrieve the data to send, or you can include static JSON in your stored procedure. This example uses static JSON to create an OLE object:\n\nAnd that’s how you use OLE objects—and their support for HTTP requests—to utilize REST APIs in stored procedures.\n\nFor simple data transfer needs, like updating reporting dashboards with data from SQL Server tables, the OLE automation procedures work well. The SQL Server stored procedure language has support for functions that you can use for other API functions, such as parameters, authentication with an API key or OAuth, or advanced HTTP headers.\n\nHowever, more complex tasks might call for the use of a more advanced scripting language than the stored procedures language. You might consider Python or other languages that have good support for working with Javascript/JSON and SQL server schemas like Java.\n\nJava has strong integration with SQL Server, thanks to the many features of the JDBC driver. On the other hand, Java has a high overhead in terms of system resources—and the time it takes to learn the connection string format of JDBC.\n\nMigrating legacy data integration processes to REST APIs offers a modern approach to data exchange, enhancing scalability, real-time access, and flexibility. Traditional methods like flat file transfers and ETL (Extract, Transform, Load) processes often rely on batch operations, which can introduce latency and complexity in data pipelines. By transitioning to REST API-based integration, you can achieve more efficient and responsive data handling within your SQL Server environment.\n• Identify all legacy processes, such as scheduled flat file transfers or ETL jobs, that need modernization.\n• Analyze the data sources, formats, and frequency of these processes to determine the required API endpoints.\n• Define RESTful API endpoints that replicate the functionality of existing processes, ensuring they handle the necessary CRUD operations (Create, Read, Update, Delete).\n• For data previously handled by ETL processes, design APIs that allow direct access to source systems, reducing the need for intermediate staging.\n• Update or create new stored procedures in SQL Server to interact with the designed REST APIs.\n• Use SQL Server’s OLE Automation Procedures (e.g., , ) to make HTTP requests, replacing file-based operations with API calls.\n• Incorporate data transformation logic directly within stored procedures or through API responses, eliminating the need for separate ETL tools.\n• Use JSON functions in SQL Server to parse and manipulate API responses, converting data into the required format.\n• Utilize SQL Server Agent to schedule stored procedures that call the REST APIs, ensuring regular and automated data synchronization.\n• Replace legacy job schedulers with SQL Server Agent tasks, maintaining the existing automation but with modernized data flows.\n• Implement robust error handling in stored procedures to manage API response failures or data discrepancies.\n• Secure API calls with appropriate authentication methods (e.g., API keys, OAuth), replacing unsecured file transfers with encrypted API transactions.\n• Perform comprehensive testing to ensure that the new API-based processes replicate or improve upon the functionality of the legacy systems.\n• Validate that data accuracy, completeness, and performance meet or exceed the standards of the old processes.\n\nREST API-based integration offers immediate data access, minimizing latency compared to batch processing. It enhances scalability by facilitating seamless integration with cloud services and distributed systems. The consolidation of data integration into API calls simplifies system architecture, reducing maintenance complexity. Additionally, it improves security through modern authentication methods and encrypted communications, surpassing the protection offered by legacy file-based approaches.\n\nAn easier and more full-featured way to take care of your SQL Server data integration needs is an API management platform. These comprehensive apps offer several advantages when working with REST APIs and SQL Server.\n\nFirst, an API management platform is more scalable. Since the platform provides an abstraction layer between SQL Server and web services, the workload on your database is greatly reduced. Management platforms tend to offer more security than opening up your SQL server instance to all the connections required for stored procedures or other hand-coded solutions. And that's just one of many benefits of these platforms.\n\nDreamFactory: A Complete Management Platform for SQL Server (And All Your APIs)\n\nDreamFactory has excellent data integration capabilities with SQL Server. Generating and documenting a SQL Server API takes less than five minutes. That’s much less than the time it takes to create stored procedures, test them out, and manually document your API. And DreamFactory can handle more than just SQL Server. It’s the complete platform for all your data sources and API management needs. The DreamFactory iPaaS (Internet Platform as a Service) allows you to get up and running right away. You can even get a 14-day free trial to try it out for yourself. Sign up today to see why DreamFactory is the only API management platform you need."
    },
    {
        "link": "https://stackoverflow.com/questions/44705592/best-practice-for-json-response-to-restful-api-call",
        "document": "I'm trying to ascertain what best practice might be for returning a JSON response for a Restful API call. I've read multiple blog and tutorial articles that offer different opinions.\n\nOne approach I've seen proposes that any Restful call return a JSON response that includes both meta data and result data, like the representation shown below:\n\nOther articles poo-poo this approach. I'm about to start a new API project and I'd like to get feedback from experts about the best approach on this subject."
    },
    {
        "link": "https://developer.ibm.com/tutorials/creating-rest-apis-based-on-sql-statements",
        "document": ""
    }
]