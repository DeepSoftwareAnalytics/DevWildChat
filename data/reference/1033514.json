[
    {
        "link": "https://data-flair.training/blogs/hive-join",
        "document": "In Apache Hive, for combining specific fields from two tables by using values common to each one we use Hive Join – HiveQL Select Joins Query. However, we need to know the syntax of Hive Join for implementation purpose.\n\nSo, in this article, “Hive Join – HiveQL Select Joins Query and its types” we will cover syntax of joins in hive. Also, we will learn an example of Hive Join to understand well.\n\nMoreover, there are several types of Hive join – HiveQL Select Joins: Hive inner join, hive left outer join, hive right outer join, and hive full outer join. We will also learn Hive Join tables in depth.\n\nBasically, for combining specific fields from two tables by using values common to each one we use Hive JOIN clause.\n\nIn other words, to combine records from two or more tables in the database we use JOIN clause. However, it is more or less similar to SQL JOIN. Also, we use it to combine rows from multiple tables.\n\nMoreover, there are some points we need to observe about Hive Join:\n• None In Joins, only Equality joins are allowed.\n• None However, in the same query more than two tables can be joined.\n• None Basically, to offer more control over ON Clause for which there is no match LEFT, RIGHT, FULL OUTER joins exist in order.\n• None Also, note that Hive Joins are not Commutative\n• None Whether they are LEFT or RIGHT joins in Hive, even then Joins are left-associative.\n\nFollowing is a syntax of Hive Join – HiveQL Select Clause.\n\n join_table:\n\n table_reference JOIN table_factor [join_condition]\n\n | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference\n\n join_condition\n\n | table_reference LEFT SEMI JOIN table_reference join_condition\n\n | table_reference CROSS JOIN table_reference [join_condition]\n\nb. Example of Join in Hive\n\nExample of Hive Join – HiveQL Select Clause.\n\n However, to understand well let’s suppose the following table named CUSTOMERS.\n\n Table.1 Hive Join Example\n\nAlso, suppose another table ORDERS as follows:\n\n Table.2 – Hive Join Example\n\nBasically, there are 4 types of Hive Join. Such as:\n\nSo, let’s discuss each Hive join in detail.\n\nBasically, to combine and retrieve the records from multiple tables we use Hive Join clause. Moreover, in SQL JOIN is as same as OUTER JOIN. Moreover, by using the primary keys and foreign keys of the tables JOIN condition is to be raised.\n\nFurthermore, the below query executes JOIN the CUSTOMER and ORDER tables. Then further retrieves the records:\n\n hive> SELECT c.ID, c.NAME, c.AGE, o.AMOUNT \n\n FROM CUSTOMERS c JOIN ORDERS o \n\n ON (c.ID = o.CUSTOMER_ID);\n\nMoreover, we get to see the following response, on the successful execution of the query:\n\nOn defining HiveQL Left Outer Join, even if there are no matches in the right table it returns all the rows from the left table.\n\nTo be more specific, even if the ON clause matches 0 (zero) records in the right table, then also this Hive JOIN still returns a row in the result. Although, it returns with NULL in each column from the right table.\n\nIn addition, it returns all the values from the left table. Also, the matched values from the right table, or NULL in case of no matching JOIN predicate.\n\n However, the below query shows LEFT OUTER JOIN between CUSTOMER as well as ORDER tables:\n\nMoreover, we get to see the following response, on the successful execution of the HiveQL Select query:\n\nBasically, even if there are no matches in the left table, HiveQL Right Outer Join returns all the rows from the right table.\n\nTo be more specific, even if the ON clause matches 0 (zero) records in the left table, then also this Hive JOIN still returns a row in the result. Although, it returns with NULL in each column from the left table\n\nIn addition, it returns all the values from the right table. Also, the matched values from the left table or NULL in case of no matching join predicate.\n\nHowever, the below query shows RIGHT OUTER JOIN between the CUSTOMER as well as ORDER tables.\n\nMoreover, we get to see the following response, on the successful execution of the HiveQL Select query:\n\n Table.5 – Right Outer Join in Hive\n\nThe major purpose of this HiveQL Full outer Join is it combines the records of both the left and the right outer tables which fulfills the Hive JOIN condition. Moreover, this joined table contains either all the records from both the tables or fills in NULL values for missing matches on either side.\n\nHowever, the below query shows FULL OUTER JOIN between CUSTOMER as well as ORDER tables:\n\nMoreover, we get to see the following response, on the successful execution of the query:\n\nThis was all about HiveQL Select – Apache Hive Join Tutorial. Hope you like our explanation of Types of Joins in Hive.\n\nAs a result, we have seen what is Apache Hive Join and possible types of Join in Hive- HiveQL Select. Also, we have seen several examples to understand Joins in Hive well. Moreover, we hope this article “Hive Join” will help a lot. Still, if any doubt occurs feel free to ask in the comment section."
    },
    {
        "link": "https://stackoverflow.com/questions/60399994/hive-sql-full-outer-join-with-where-clause",
        "document": "I am creating a full outer join with a where clause. However, it can only generate inner join result. I suspect that it is due to the where clause, but I do need this where condition being added. So how can I create a query with both needs meet (both the where condition and full outer join)? Here is my query.\n\nThese are all the scenarios that I can think of as now. I can add in if I find any missing scenarios. My point here is the fact that the following results should be included:\n• if the two tables meet all those conditions set up with the keys and date, then it is included as shown in line 1 and 2 in the desired result.\n• if any of those conditions is not met, then we will keep one table's information in the result as shown in line 3, 4, 5, and 6 in the desired result.\n\nEDIT: Based on @Gordon Linoff 's suggestion, I used a union all to resolve the issue. Please see my solution in my answer post below."
    },
    {
        "link": "https://stackoverflow.com/questions/66884611/using-three-or-more-joins-within-a-single-hive-query",
        "document": "The result should be the same but performance may be different. Joins can duplicate or reduce the number of rows.\n\nAll kind of joins can duplicate rows (except left semi join, which may or may not reduce the number of rows) if the join key is not unique. This is normal behavior and can be done intentionally. Each subsequent join after the duplicating one will process more rows and this will definitely affect performance.\n\nCheck that join key is unique in the table you are joining with to eliminate duplication.\n\nInner join can also reduce the number of rows (like left-semi-join) if keys are not matching. Subsequent joins will process less rows and this will improve performance. At the same time inner join can duplicate rows(see above).\n\nJoins which reduce the dataset should be done before other joins if you want to improve performance. Filter in join ON condition if possible because it more efficient than WHERE clause, which is being applied after all joins. Joins duplicating rows(intentionally) should be executed last.\n\nNormally CBO decides which joins should be done first based on statistics. Alternatively you can explicitly group joins into subqueries to make sure that joins reducing the dataset are being executed first. For example like this:\n\nAlso Map joins can be combined and executed on single vertex. Read this answer."
    },
    {
        "link": "https://tutorialspoint.com/hive/hiveql_joins.htm",
        "document": "JOIN is a clause that is used for combining specific fields from two tables by using values common to each one. It is used to combine records from two or more tables in the database.\n\nWe will use the following two tables in this chapter. Consider the following table named CUSTOMERS..\n\nConsider another table ORDERS as follows:\n\nThere are different types of joins given as follows:\n\nJOIN clause is used to combine and retrieve the records from multiple tables. JOIN is same as OUTER JOIN in SQL. A JOIN condition is to be raised using the primary keys and foreign keys of the tables.\n\nThe following query executes JOIN on the CUSTOMER and ORDER tables, and retrieves the records:\n\nOn successful execution of the query, you get to see the following response:\n\nThe HiveQL LEFT OUTER JOIN returns all the rows from the left table, even if there are no matches in the right table. This means, if the ON clause matches 0 (zero) records in the right table, the JOIN still returns a row in the result, but with NULL in each column from the right table.\n\nA LEFT JOIN returns all the values from the left table, plus the matched values from the right table, or NULL in case of no matching JOIN predicate.\n\nThe following query demonstrates LEFT OUTER JOIN between CUSTOMER and ORDER tables:\n\nOn successful execution of the query, you get to see the following response:\n\nThe HiveQL RIGHT OUTER JOIN returns all the rows from the right table, even if there are no matches in the left table. If the ON clause matches 0 (zero) records in the left table, the JOIN still returns a row in the result, but with NULL in each column from the left table.\n\nA RIGHT JOIN returns all the values from the right table, plus the matched values from the left table, or NULL in case of no matching join predicate.\n\nThe following query demonstrates RIGHT OUTER JOIN between the CUSTOMER and ORDER tables.\n\nnotranslate\"> hive> SELECT c.ID, c.NAME, o.AMOUNT, o.DATE FROM CUSTOMERS c RIGHT OUTER JOIN ORDERS o ON (c.ID = o.CUSTOMER_ID);\n\nOn successful execution of the query, you get to see the following response:\n\nThe HiveQL FULL OUTER JOIN combines the records of both the left and the right outer tables that fulfil the JOIN condition. The joined table contains either all the records from both the tables, or fills in NULL values for missing matches on either side.\n\nThe following query demonstrates FULL OUTER JOIN between CUSTOMER and ORDER tables:\n\nOn successful execution of the query, you get to see the following response:"
    },
    {
        "link": "https://docs.cloudera.com/runtime/7.3.1/using-hiveql/topics/hive_hive_query_language_basics.html",
        "document": "Using Apache Hive, you can query distributed data storage. You need to know ANSI SQL to view, maintain, or analyze Hive data. Examples of the basics, such as how to insert, update, and delete data from a table, helps you get started with Hive.\n\nHive supports ANSI SQL and atomic, consistent, isolated, and durable (ACID) transactions. For updating data, you can use the MERGE statement, which meets ACID standards. Materialized views optimize queries based on access patterns. Hive supports tables up to 300PB in Optimized Row Columnar (ORC) format. Other file formats are also supported. You can create tables that resemble those in a traditional relational database. You use familiar insert, update, delete, and merge SQL statements to query table data. The insert statement writes data to tables. Update and delete statements modify and delete values already written to Hive. The merge statement streamlines updates, deletes, and changes data capture operations by drawing on co-existing tables. These statements support auto-commit that treats each statement as a separate transaction and commits it after the SQL statement is executed."
    },
    {
        "link": "https://docs.cloudera.com/runtime/7.3.1/using-hiveql/topics/hive-manage-partitions.html",
        "document": "You run the MSCK (metastore consistency check) Hive command: every time you need to synchronize a partition with the file system.\n\nThis command triggers a full repair which might take a longer time if the number of partitions are more. Instead, you can use operators in the command, such as EQUAL, NOTEQUAL, LESSTHANOREQUALTO, LESSTHAN, GREATERTHANOREQUALTO, GREATERTHAN, or LIKE in the partition column so that a larger subset of partitions can be recovered (added/removed) without triggering a full repair."
    },
    {
        "link": "http://docs.cloudera.com.s3-website-us-east-1.amazonaws.com/HDPDocuments/HDP3/HDP-3.1.0/using-hiveql/content/hive-manage-partitions.html",
        "document": "You can discover file system changes related to partitions and synchronize Hive metadata automatically. Performing synchronization automatically as opposed to manually can save substantial time, especially when partitioned data, such as logs, changes frequently.You can also configure how long to retain partition data and metadata.\n\nAfter creating a partitioned table, Hive does not update metadata about corresponding directories on the file system or object store that you add or drop. The partition metadata in the Hive metastore becomes stale after corresponding directories are added or deleted. You need to synchronize the metastore and the file system.\n• Manually You run the MSCK (metastore consistency check) Hive command: every time you need to synchronize a partition with your file system.\n• Automatically You set up partition discovery to occur periodically. You can refresh Hive metastore partition information manually or automatically. table property is automatically created and enabled for external partitioned tables. When is enabled for a table, Hive performs an automatic refresh as follows:\n• Adds corresponding partitions that are in the file system, but not in metastore, to the metastore.\n• Removes partition schema information from metastore if you removed the corresponding partitions from the file system. Thetable property is automatically created and enabled for external partitioned tables. Whenis enabled for a table, Hive performs an automatic refresh as follows: You can configure how long to keep partition metadata and data and remove it after the retention period elapses.\n• Generally, partition discovery and retention is not recommended for use on managed tables.\n• You must deploy a remote Hive metastore for your cluster by installing and configuring a supported database on the cluster. The Hive metastore acquires an exclusive lock on a table that enables partition discovery that can slow down other queries. Using the default metastore, which is embedded in the HiveServer process and installed by Ambari, you cannot manage a partition automatically."
    },
    {
        "link": "https://docs.aws.amazon.com/athena/latest/ug/partitions.html",
        "document": "By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but that is loaded only once per day, might partition by a data source identifier and date.\n\nAthena can use Apache Hive style partitions, whose data paths contain key value pairs connected by equal signs (for example, or ). Thus, the paths include both the names of the partition keys and the values that each path represents. To load new Hive partitions into a partitioned table, you can use the MSCK REPAIR TABLE command, which works only with Hive-style partitions.\n\nAthena can also use non-Hive style partitioning schemes. For example, CloudTrail logs and Firehose delivery streams use separate path components for date parts such as . For such non-Hive style partitions, you use ALTER TABLE ADD PARTITION to add the partitions manually.\n\nWhen using partitioning, keep in mind the following points:\n• If you query a partitioned table and specify the partition in the clause, Athena scans the data only from that partition.\n• If you issue queries against Amazon S3 buckets with a large number of objects and the data is not partitioned, such queries may affect the request rate limits in Amazon S3 and lead to Amazon S3 exceptions. To prevent errors, partition your data. Additionally, consider tuning your Amazon S3 request rates. For more information, see Best practices design patterns: Optimizing Amazon S3 performance .\n• Partition locations to be used with Athena must use the protocol (for example, ). In Athena, locations that use other protocols (for example, ) will result in query failures when queries are run on the containing tables.\n• Make sure that the Amazon S3 path is in lower case instead of camel case (for example, instead of ). If the S3 path is in camel case, doesn't add the partitions to the AWS Glue Data Catalog. For more information, see MSCK REPAIR TABLE.\n• Because scans both a folder and its subfolders to find a matching partition scheme, be sure to keep data for separate tables in separate folder hierarchies. For example, suppose you have data for table 1 in and data for table 2 in . If both tables are partitioned by string, will add the partitions for table 2 to table 1. To avoid this, use separate folder structures like and instead. Note that this behavior is consistent with Amazon EMR and Apache Hive.\n• If you are using the AWS Glue Data Catalog with Athena, see AWS Glue endpoints and quotas for service quotas on partitions per account and per table.\n• Although Athena supports querying AWS Glue tables that have 10 million partitions, Athena cannot read more than 1 million partitions in a single scan. In such scenarios, partition indexing can be beneficial. For more information, see the AWS Big Data Blog article Improve Amazon Athena query performance using AWS Glue Data Catalog partition indexes .\n• To request a partitions quota increase if you are using the AWS Glue Data Catalog, visit the Service Quotas console for AWS Glue .\n\nTo create a table that uses partitions, use the clause in your CREATE TABLE statement. The clause defines the keys on which to partition data, as in the following example. The clause specifies the root location of the partitioned data.\n\nAfter you create the table, you load the data in the partitions for querying. For Hive style partitions, you run MSCK REPAIR TABLE. For non-Hive style partitions, you use ALTER TABLE ADD PARTITION to add the partitions manually.\n\nThe following sections show how to prepare Hive style and non-Hive style data for querying in Athena.\n\nIn this scenario, partitions are stored in separate folders in Amazon S3. For example, here is the partial listing for sample ad impressions output by the command, which lists the S3 objects under a specified prefix: Here, logs are stored with the column name (dt) set equal to date, hour, and minute increments. When you give a DDL with the location of the parent folder, the schema, and the name of the partitioned column, Athena can query data in those subfolders. To make a table from this data, create a partition along 'dt' as in the following Athena DDL statement: This table uses Hive's native JSON serializer-deserializer to read JSON data stored in Amazon S3. For more information about the formats supported, see Choose a SerDe for your data. After you run the query, run the command in the Athena query editor to load the partitions, as in the following example. After you run this command, the data is ready for querying. Query the data from the impressions table using the partition column. Here's an example: SELECT dt,impressionid FROM impressions WHERE dt<'2009-04-12-14-00' and dt>='2009-04-12-13-00' ORDER BY dt DESC LIMIT 100 This query should show results similar to the following:\n\nIn the following example, the command shows ELB logs stored in Amazon S3. Note how the data layout does not use pairs and therefore is not in Hive format. (The option for the command specifies that all files or objects under the specified directory or prefix be listed.) Because the data is not in Hive format, you cannot use the command to add the partitions to the table after you create it. Instead, you can use the ALTER TABLE ADD PARTITION command to add each partition manually. For example, to load the data in s3://athena-examples- /elb/plaintext/2015/01/01/, you can run the following query. Note that a separate partition column for each Amazon S3 folder is not required, and that the partition key value can be different from the Amazon S3 key. If a partition already exists, you receive the error . To avoid this error, you can use the clause. For more information, see ALTER TABLE ADD PARTITION. To remove a partition, you can use ALTER TABLE DROP PARTITION.\n\nTo avoid having to manage partitions yourself, you can use partition projection. Partition projection is an option for highly partitioned tables whose structure is known in advance. In partition projection, partition values and locations are calculated from table properties that you configure rather than read from a metadata repository. Because the in-memory calculations are faster than remote look-up, the use of partition projection can significantly reduce query runtimes.\n\nFor more information, see Use partition projection with Amazon Athena."
    },
    {
        "link": "https://hive.apache.org/docs/latest/sql-standard-based-hive-authorization_40509928",
        "document": "The default authorization in Hive is not designed with the intent to protect against malicious users accessing data they should not be accessing. It only helps in preventing users from accidentally doing operations they are not supposed to do. It is also incomplete because it does not have authorization checks for many operations including the grant statement. The authorization checks happen during Hive query compilation. But as the user is allowed to execute dfs commands, user-defined functions and shell commands, it is possible to bypass the client security checks.\n\nHive also has support for storage based authorization, which is commonly used to add authorization to metastore server API calls (see Storage Based Authorization in the Metastore Server). As of Hive 0.12.0 it can be used on the client side as well. While it can protect the metastore against changes by malicious users, it does not support fine grained access control (column or row level).\n\nThe default authorization model in Hive can be used to provide fine grained access control by creating views and granting access to views instead of the underlying tables.\n\nThe SQL standards based authorization option (introduced in Hive 0.13) provides a third option for authorization in Hive. This is recommended because it allows Hive to be fully SQL compliant in its authorization model without causing backward compatibility issues for current users. As users migrate to this more secure model, the current default authorization could be deprecated.\n\nFor an overview of this authorization option, see SQL Standards Based Authorization in HiveServer2.\n\nThis authorization mode can be used in conjunction with storage based authorization on the metastore server. Like the current default authorization in Hive, this will also be enforced at query compilation time. To provide security through this option, the client will have to be secured. This can be done by allowing users access only through Hive Server2, and by restricting the user code and non-SQL commands that can be run. The checks will happen against the user who submits the request, but the query will run as the Hive server user. The directories and files for input data would have read access for this Hive server user. For users who don’t have the need to protect against malicious users, this could potentially be supported through the Hive command line as well.\n\nThe goal of this work has been to comply with the SQL standard as far as possible, but there are deviations from the standard in the implementation. Some deviations were made to make it easier for existing Hive users to migrate to this authorization model, and some were made considering ease of use (in such cases we also looked at what many widely used databases do).\n\nUnder this authorization model, users who have access to the Hive CLI, HDFS commands, Pig command line, ‘hadoop jar’ command, etc., are considered privileged users. In an organization, it is typically only the teams that work on ETL workloads that need such access. These tools don’t access the data through HiveServer2, and as a result their access is not authorized through this model. For Hive CLI, Pig, and MapReduce users access to Hive tables can be controlled using storage based authorization enabled on the metastore server.\n\nMost users such as business analysts tend to use SQL and ODBC/JDBC through HiveServer2 and their access can be controlled using this authorization model.\n\nCommands such as dfs, add, delete, compile, and reset are disabled when this authorization is enabled.\n\nThe set commands used to change Hive configuration are restricted to a smaller safe set. This is controlled using the hive.security.authorization.sqlstd.confwhitelist configuration parameter. If this set needs to be customized, the HiveServer2 administrator can set a value for this configuration parameter in its hive-site.xml.\n\nPrivileges to add or drop functions and macros are restricted to the admin role.\n\nTo enable users to use functions, the ability to create permanent functions has been added. A user in the admin role can run commands to create these functions, which all users can then use.\n\nThe Hive transform clause is also disabled when this authorization is enabled.\n\n● SELECT privilege – gives read access to an object.\n\n● INSERT privilege – gives ability to add data to an object (table).\n\n● UPDATE privilege – gives ability to run update queries on an object (table).\n\n● DELETE privilege – gives ability to delete data in an object (table).\n\n● ALL PRIVILEGES – gives all privileges (gets translated into all the above privileges).\n• The privileges apply to table and views. The above privileges are not supported on databases.\n• Database ownership is considered for certain actions.\n• URI is another object in Hive, as Hive allows the use of URI in SQL syntax. The above privileges are not applicable on URI objects. URI used are expected to point to a file/directory in a file system. Authorization is done based on the permissions the user has on the file/directory.\n\nFor certain actions, the ownership of the object (table/view/database) determines if you are authorized to perform the action.\n\nThe user who creates the table, view or database becomes its owner. In the case of tables and views, the owner gets all the privileges with grant option.\n\nA role can also be the owner of a database. The “ ” command can be used to set the owner of a database to a role.\n\nPrivileges can be granted to users as well as roles.\n\n Users can belong to one or more roles.\n\nThere are two roles with special meaning – public and admin.\n\n All users belong to the public role. You use this role in your grant statement to grant a privilege to all users.\n\nWhen a user runs a Hive query or command, the privileges granted to the user and her “current roles” are checked. The current roles can be seen using the “ ” command. All of the user’s roles except for the admin role will be in the current roles by default, although you can use the “ ” command to set a specific role as the current role. See the command descriptions for details.\n\nUsers who do the work of a database administrator are expected to be added to the admin role.\n\n They have privileges for running additional commands such as “ ” and “ ”. They can also access objects that they haven’t been given explicit access to. However, a user who belongs to the admin role needs to run the “ ” command before getting the privileges of the admin role, as this role is not in current roles by default.\n\nRole names are case insensitive. That is, “marketing” and “MarkEting” refer to same role.\n\nUser names are case sensitive. This is because, unlike role names, user names are not managed within Hive. The user can be any user that the hiveserver2 authentication mode supports.\n\nUser and role names may optionally be surrounded by backtick characters ( hive.support.quoted.identifiers column (default value). All [Unicode](http://en.wikipedia.org/wiki/List_of_Unicode_characters) characters are permitted in the quoted identifiers, with double backticks (``) representing a backtick character. However when hive.support.quoted.identifiers none`, only alphanumeric and underscore characters are permitted in user names and role names.\n\nFor details, see HIVE-6013 and Supporting Quoted Identifiers in Column Names.\n\nAs of Hive 0.14, user may be optionally surrounded by backtick characters ( hive.support.quoted.identifiers` setting.\n\nCreates a new role. Only the admin role has privilege for this.\n\nThe role names ALL, DEFAULT and NONE are reserved.\n\nDrops the given role. Only the admin role has privilege for this.\n\nShows the list of the user’s current roles. All actions of the user are authorized by looking at the privileges of the user and all current roles of the user.\n\nThe default current roles has all roles for the user except for the admin role (even if the user belongs to the admin role as well).\n\nAny user can run this command.\n\nIf a role_name is specified, then that role becomes the only role in current roles.\n\nSetting role_name to ALL refreshes the list of current roles (in case new roles were granted to the user) and sets them to the default list of roles.\n\nSetting role_name to NONE will remove all current roles from the current user. (It’s introduced in HIVE-11780 and will be included in the upcoming versions 1.3.0 and 1.2.2.)\n\nIf a role the user does not belong to is specified as the role_name, it will result in an error.\n\nOnly the admin role has privilege for this.\n\nGrant one or more roles to other roles or users.\n\nIf “WITH ADMIN OPTION” is specified, then the user gets privileges to grant the role to other users/roles.\n\nIf the grant statement ends up creating a cycling relationship between roles, the command will fail with an error.\n\nRevokes the membership of the roles from the user/roles in the FROM clause.\n\nAs of Hive 0.14.0, revoking just the ADMIN OPTION is possible with the use of REVOKE ADMIN OPTION FOR (HIVE-6252).\n\nwhere is the name of a user or role.\n\nLists all roles the given user or role has been granted.\n\nCurrently any user can run this command. But this is likely to change in future to allow users to see only their own role grants, and additional privileges would be needed to see role grants of other users.\n\nLists all roles and users who belong to this role.\n\nOnly the admin role has privilege for this.\n\nIf a user is granted a privilege WITH GRANT OPTION on a table or view, then the user can also grant/revoke privileges of other users and roles on those objects. As of Hive 0.14.0, the grant option for a privilege can be removed while still keeping the privilege by using REVOKE GRANT OPTION FOR (HIVE-7404).\n\nNote that in case of the REVOKE statement, the DROP-BEHAVIOR option of CASCADE is not currently supported (which is in SQL standard). As a result, the revoke statement will not drop any dependent privileges. For details on CASCADE behavior, you can check the Postgres revoke documentation.\n\nNotice that in Hive, unlike in standard SQL, USER or ROLE must be specified in the principal_specification.\n\nCurrently any user can run this command. But this is likely to change in the future to allow users to see only their own privileges, and additional privileges would be needed to see privileges of other users.\n\nFind out the privileges user ashutosh has on table hivejiratable:\n\nFind out the privileges user ashutosh has on all objects:\n\nFind out the privileges all users have on table hivejiratable:\n\nAs of Hive 3.0.0 (HIVE-12408), Ownership is not required for the URI Privilege.\n\nSet the following in hive-site.xml:\n• hive.users.in.admin.role to the list of comma-separated users who need to be added to admin role. Note that a user who belongs to the admin role needs to run the “ ” command before getting the privileges of the admin role, as this role is not in current roles by default.\n\nStart HiveServer2 with the following additional command-line options:\n\nSet the following in hive-site.xml:\n• hive.users.in.admin.role to the list of comma-separated users who need to be added to admin role. Note that a user who belongs to the admin role needs to run the “ ” command before getting the privileges of the admin role, as this role is not in current roles by default.\n• Add org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly to hive.security.metastore.authorization.manager. (It takes a comma separated list, so you can add it along with StorageBasedAuthorization parameter, if you want to enable that as well).\n\n This setting disallows any of the authorization api calls to be invoked in a remote metastore. HiveServer2 can be configured to use embedded metastore, and that will allow it to invoke metastore authorization api. Hive cli and any other remote metastore users would be denied authorization when they try to make authorization api calls. This restricts the authorization api to privileged HiveServer2 process. You should also ensure that the metastore rdbms access is restricted to the metastore server and hiverserver2.\n• hive.security.authorization.manager to org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory. This will ensure that any table or views created by hive-cli have default privileges granted for the owner.\n\nSet the following in hiveserver2-site.xml:\n\nHIVE-6985 – SQL std auth - privileges grants to public role not being honored\n\nHIVE-6957 – SQL authorization does not work with HS2 binary mode and Kerberos auth\n\nThe known issues noted above under Hive 0.13.0 have been fixed in 0.13.1 release.\n\nFor information on the SQL standard for security see:\n\nProblem: My user name is in hive.users.in.admin.role in hive-site.xml, but I still get the error that user is not an admin. What could be wrong?\n\nDo This: Ensure that you have restarted HiveServer2 after a configuration change and that you have used the HiveServer2 command line options as described in Configuration above.\n\nDo This: Ensure that you have run a ‘ ’ command to get the admin role."
    },
    {
        "link": "https://nexocode.com/blog/posts/what-is-apache-hive",
        "document": "Apache Hive is a popular data warehousing tool built on top of the Hadoop ecosystem, designed to make data querying and analysis easy and efficient. With its SQL-like query language, Hive allows users to perform ad-hoc querying, summarization, and data analysis on large datasets stored in Hadoop Distributed File System (HDFS) or other compatible file systems.\n\nIn this article, we will take a deep dive into the architecture of Apache Hive, from data storage to data analysis, and examine the key components that make Hive such a powerful tool for big data processing. By the end of this article, you will have a thorough understanding of how Hive manages and processes data, as well as its capabilities and limitations, allowing you to make informed decisions when designing your big data processing pipeline.\n\nBig data is more than just a buzzword- it’s a reality for businesses of all sizes. And to take advantage of big data, you need a modern big data ecosystem.\n\nA modern big data ecosystem includes hardware, software, and services that work together to process and analyze large volumes of data. The goal is to enable businesses to make better decisions faster and improve their bottom line.\n\nSeveral components are essential to a thriving big data ecosystem:\n• Data variety: Different data types from multiple sources are ingested and outputted (structured data, unstructured data, semi-structured data).\n• Velocity: Fast ingest and processing of data in real-time.\n• Volume: Scalable storage and processing of large amounts of data.\n• Cheap raw storage: Ability to store data affordably in its original form.\n• Flexible processing: Ability to run various processing engines on the same data.\n• Support for streaming analytics: Streaming analytics refers to providing low latency to process real-time data streams in near-real-time.\n• Support for modern applications: Ability to power new types of applications that require fast, flexible data processing like BI tools, machine learning systems, log analysis, and more.\n\nBatch processing is a type of computing process that involves collecting data and running it through a set of tasks in batches. Data is collected, sorted, and there are usually multiple steps involved in the process. The result of the batch process is typically compressed data stored for future use.\n\nBatch processing has been used for decades to manage large volumes of data and still has many applications. Still, it isn’t suitable for real-time applications where near-instantaneous results are required.\n\nApache Hive is an open-source data warehousing and analysis system built on top of the Apache Hadoop ecosystem. It provides a SQL-like interface for querying large data sets stored in Hadoop’s HDFS or other Hadoop-supported storage systems. Hive translates SQL-like queries into MapReduce jobs for execution on Hadoop clusters, allowing users to perform ad hoc analysis and aggregate queries on big data without having to write complex MapReduce code. Hive also provides tools for managing, storing, and retrieving large data sets, making it a popular choice for data warehousing and business intelligence workloads.\n\nThe architecture of Apache Hive is built around several key components that work together to enable efficient data querying and analysis. These components include the Hive driver, compiler, execution engine, and storage handler, among others.\n\nLet’s take a closer look at each of these components and how they contribute to the overall functionality of Hive.\n\nThe driver acts as the entry point for all Hive operations, managing the lifecycle of a HiveQL query. It is responsible for parsing, compiling, optimizing, and executing the query.\n\nThe compiler takes the Hive queries and translates them into a series of MapReduce jobs or Tez tasks, depending on the configured execution engine. It also performs query optimization, such as predicate pushdown and join reordering, to improve performance.\n\nThe execution engine is responsible for running the generated MapReduce jobs or Tez tasks on the Hadoop cluster. It manages all the data, flow, resource allocation, and task scheduling.\n\nThe storage handler is an interface between Hive and the underlying storage system, such as HDFS or Amazon S3. It is responsible for reading and writing data to and from the storage system in a format that Hive can understand.\n\nHive clients provide a way for users to interact with the Hive ecosystem and submit queries. Different clients offer various interfaces and protocols to connect to Hive, ensuring compatibility with a wide range of applications and systems. Some of the most common Hive clients include:\n• Hive Thrift: The Hive Thrift client is based on the Apache Thrift framework, which is a scalable cross-language service development framework. Thrift allows Hive to communicate with clients using different programming languages, such as Java, Python, C++, and more. The Hive Thrift client provides a comprehensive API to interact with Hive and execute HiveQL queries, making it a popular choice for developers looking to integrate Hive with their applications.\n• Hive JDBC: The Hive JDBC client is a Java-based client that uses the Java Database Connectivity (JDBC) API to connect to Hive Server2. JDBC is a widely used standard for connecting Java applications to databases, and the Hive JDBC driver allows users to leverage existing JDBC-based tools and applications to interact with Hive. With the Hive JDBC client, users can submit queries, fetch results, and manage resources using familiar JDBC methods and interfaces.\n• Hive ODBC: The Hive ODBC client uses the Open Database Connectivity (ODBC) API to connect to Hive Server2. ODBC is a standard programming interface that enables applications to access data in relational databases, regardless of the specific database management system. The Hive ODBC driver allows users to connect Hive to a variety of ODBC-compliant tools, such as Microsoft Excel, Tableau, and other data analytics and visualization applications. With the Hive ODBC client, users can easily integrate Hive with their existing data analysis workflows and tools.\n\nHive also provides a simple user interface (UI) to query results and monitor processes.\n\nThe metastore is a centralized repository for storing metadata about the data stored in the Hadoop cluster, such as the location of data files, the schema of tables, and the structure of partitions. The metadata storage is used by Hive to keep track of the data being analyzed.\n\nHive Server 2 is the service component of Hive that provides a Thrift API for accessing Hive from remote clients. HS2 supports multi-client concurrency and authentication. It is designed to provide better support for open API clients like JDBC and ODBC to submit HiveQL queries and retrieve results from the Hadoop cluster.\n\nThe Hive Command Line Interface or command line tool (CLI) is a shell-based interface for interacting with Hive. It provides an easy way to execute the structured query language - HiveQL queries - and perform other operations on the data stored in the Apache Hadoop cluster. The CLI is suitable for ad hoc queries and debugging, but it doesn’t support concurrency and may not be ideal for production use cases.\n\nBeeline Shell is an alternative to the Hive CLI that connects to Hive Server 2 using JDBC. It is a more robust and flexible option, as it supports concurrency and multi-user access. Beeline can execute HiveQL queries and manage resources just like the Hive CLI, but it is better suited for production environments due to its more advanced features and improved stability.\n\nHive UDFs are custom functions created by users to extend the capabilities of the SQL-like Hive Query Language. These functions can be used to perform complex calculations, data transformations, or other tasks that are not supported by the built-in Hive functions. UDFs can be written in Java, and once created, they can be registered with Hive and used in queries just like any other built-in function.\n• Scalar UDFs: These functions take one or more input values and return a single output value. Scalar UDFs can be used in SELECT, WHERE, and HAVING clauses of a query.\n• Aggregate UDFs: These functions perform calculations on a group of rows and return a single output value. Aggregate UDFs can be used in GROUP BY clauses.\n• Table-Generating UDFs: These functions take one or more input values and return a table as output. Table-generating UDFs can be used in the FROM clause of a query.\n\nHive supports extending the UDFs, by creating custom UDFs, users can extend Hive’s functionality and tailor it to their specific data engineering needs, making it a highly versatile tool for data analysis and processing in the Hadoop ecosystem.\n\nApache Hive works as a data warehouse software with its data storage system built on top of the Hadoop Distributed File System or other compatible file systems, such as Amazon S3 or Azure Blob Storage. Hive does not have its own storage layer; instead, it leverages Hadoop’s storage infrastructure to manage and store data.\n\nWhen data is ingested into Hive, it is typically stored as tables, which are organized into databases. Each table consists of rows and columns, similar to a traditional relational database system. However, unlike traditional databases, Hive tables can be stored in various file formats, such as CSV, Avro, Parquet, ORC, and others. Users can choose the file format that best suits their data and query requirements, with each format offering different trade-offs in terms of storage efficiency, query performance, and compatibility.\n\nHive’s data storage system also supports partitioning, which is a technique used to divide a table into smaller, more manageable pieces based on the values of one or more columns. Partitioning can significantly improve query performance, as it allows Hive to read only the relevant partitions when processing a query, instead of scanning the entire table. This is particularly beneficial when dealing with large datasets.\n\nAdditionally, Hive allows users to create external tables, which are tables that reference data stored outside the Hive warehouse directory. External tables allow Hive to manage metadata and query data stored in other locations, such as HDFS directories or cloud storage, providing flexibility in how data is organized and stored within the Hadoop ecosystem.\n\nHiveQL (Hive Query Language) is a SQL-like data query language developed for Apache Hive to simplify querying and analyzing structured data stored in HDFS or other compatible storage systems. It provides users with a familiar syntax and semantics, making it easier for those with SQL queries experience to work with big data stored in Hadoop.\n\nHow Hive Works With Hadoop and Hadoop Distributed File System (HDFS)\n\nApache Hive works closely with Hadoop and the Hadoop Distributed File System to provide a powerful data warehousing and querying solution for large datasets. Let’s revise it step-by-step and see how Hive interacts with Hadoop:\n• Data storage: As mentioned above, Hive does not have its own storage layer. When users create tables in Hive, the data is stored as files within HDFS directories. Users can choose from various file formats, based on their specific requirements and use cases.\n• Metadata management: Hive maintains metadata about the tables, such as schema information, partitioning details, and data locations, in a component called the Hive Metastore. The metadata helps Hive track the structure and location of the data stored in HDFS, enabling it to process queries efficiently.\n• Query execution: When users submit HiveQL queries, Hive translates these SQL-like queries into a series of MapReduce, Tez, or Apache Spark jobs that can be executed on the Hadoop cluster. The query execution engine processes the data stored in HDFS and returns the results to the user.\n\nApache Hive is a versatile data processing solution for a variety of big data use cases, ranging from data analysis and warehousing to ETL processing and machine learning. Its SQL-like query language and distributed processing capabilities make it an attractive choice for organizations looking to harness the power of big data. Here are some popular use cases of applying Hive in modern big data infrastructures:\n• Data analysis and reporting: Hive enables users to perform ad hoc querying, summarization, and data analysis on large datasets starting by applying structure to unstructured data sets. Its SQL-like query language makes it easy for analysts and data scientists to extract insights from big data using familiar SQL syntax, without the need to write complex MapReduce code. It also supports work for data science teams and non-programmers to process and analyze petabytes of data.\n• ETL (Extract, Transform, Load) processing: Hive can be used to perform ETL tasks on large datasets, such as data cleaning, transformation, aggregation, and loading into other systems or storage formats. Its support for User-Defined Functions (UDFs) allows users to extend Hive’s functionality and perform custom data processing tasks as needed.\n• Data integration: Hive’s support for external tables enables users to manage metadata and query data stored in other HDFS directories or cloud storage systems. This allows for seamless data integration and access to data stored in various locations within the Hadoop ecosystem.\n• Machine learning and data mining: Hive can be used as a preprocessing step for machine learning and data mining tasks, as it enables users to prepare and transform large datasets into suitable formats for further analysis using machine learning algorithms or data mining techniques.\n• Log and event data analysis: Hive is well-suited for analyzing log and event data generated by web servers, applications, or IoT devices. Users can store and process large volumes of log data in Hive, and perform various batch analysis tasks such as anomaly detection, trend analysis, and user behavior analysis.\n\nApache Hive offers several advantages for organizations dealing with large datasets and looking for an efficient data warehousing and analysis solution. Some key advantages of using Apache Hive include the following:\n• SQL-like Query Language: HiveQL enables users with SQL experience to easily query and analyze big data.\n• Flexibility: Hive supports various file formats, partitioning, and bucketing techniques for optimized storage and query performance.\n• Data Integration: Hive’s external tables allow querying data stored in other HDFS directories or cloud storage systems.\n• Ecosystem Compatibility: Hive integrates seamlessly with other big data tools and frameworks within the Hadoop ecosystem.\n\nThere are also some limitations and disadvantages to using Apache Hive, including the following:\n• Latency: As Hive translates queries into MapReduce, Tez, or Apache Spark jobs, it may not be suitable for real-time or low-latency data processing. Hive is more suited for batch processing and analytical workloads where query response times in the order of minutes or hours are acceptable.\n• Limited support for transactional operations: Hive’s support for transactional operations like UPDATE and DELETE is limited compared to traditional relational databases. Additionally, its ACID (Atomicity, Consistency, Isolation, Durability) properties support is relatively recent and may not be as mature as in other databases.\n• No support for stored procedures and triggers: Unlike traditional relational databases, Hive does not support stored procedures and triggers, which can be a limitation for users who rely on these features for complex business logic and data processing tasks.\n• Resource management: Hive relies on the underlying Hadoop cluster for resource management, and it may require tuning and optimization to achieve the desired performance. Users may need to invest time and effort in configuring and managing Hadoop resources for optimal Hive performance.\n• Limited indexing: Hive does not offer extensive indexing options like traditional databases, which can lead to slower query performance in some cases. Users may need to rely on partitioning and bucketing strategies to optimize query performance.\n\nApache Hive as Part of the Big Data Architecture Stack\n\nApache Hive serves as an essential component in the big data architecture stack, providing data warehousing and analytics capabilities. Apart from already thoroughly explained Hadoop and HDFS integrations, Hive integrates seamlessly with other Hadoop ecosystem tools such as Pig, HBase, and Spark, enabling organizations to build a comprehensive big data processing pipeline tailored to their specific needs.\n\nWho is Using Apache Hive Project?\n\nApache Hive has been adopted by many organizations across various industries for their big data processing needs. As one of the initial developers of Apache Hive, Facebook uses it extensively for data warehousing and analytics to optimize their advertising platform and gain insights into user behavior. Other notable companies, including Netflix, Airbnb, Uber, and LinkedIn, also use Apache Hive for processing and analyzing large amounts of data related to user behavior, preferences, and platform optimization. These examples illustrate the widespread adoption of Apache Hive by major organizations worldwide, showcasing its versatility and effectiveness in processing and analyzing big data across various industries.\n\nIn addition to being an open-source project, Apache Hive is also available as a fully-managed service through various cloud providers, including Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). These cloud services provide a fully-managed Hive environment, which means that users do not have to worry about configuring, provisioning, and managing the underlying infrastructure.\n• Amazon Web Services (AWS): Amazon EMR (Elastic MapReduce) is a fully-managed service that provides a Hadoop cluster environment, Apache Hive included, along with other big data processing tools. Users can quickly provision EMR clusters with the desired configuration and scale them up or down as needed.\n• Microsoft Azure: Azure HDInsight is a fully-managed cloud service that provides a Hadoop cluster environment, which includes Apache Hive, along with other big data processing tools. HDInsight also integrates with other Azure services such as Azure Data Lake Storage, Azure SQL Database, and Azure Machine Learning.\n• Google Cloud Platform (GCP): Google Cloud Dataproc is a fully-managed service that provides a Hadoop cluster environment, which includes Apache Hive, along with other big data processing tools. Dataproc also integrates with other GCP services such as BigQuery, Cloud Storage, and Cloud AI Platform.\n\nManaged Hive services offer several advantages over running Hive on-premises or in a self-managed Hadoop cluster, including simplified deployment, reduced maintenance, and lower operational costs. Users can quickly provision Hive clusters with the desired configuration and scale them up or down as needed to handle changes in workload or data volume.\n\nApache Hive is a powerful tool for processing and analyzing big data, enabling users to execute complex data queries and run ad hoc SQL queries efficiently. With its support for compressed data storage, optimized row columnar format, and integration with other data mining tools, Hive has become a popular choice for organizations handling large datasets. Data analysts and data scientists can leverage Hive’s SQL-like query language and graphical user interfaces to extract insights and create interactive visualizations from huge datasets stored in Hadoop data nodes.\n\nWhile Apache Hive is a data warehouse software well suited for analytics, it is not designed for online transaction processing (OLTP) or real-time data processing. Organizations requiring real-time data processing or OLTP should consider other tools or frameworks, such as Apache Flink or Apache Kafka.\n\nIn comparison to more traditional database and data warehouse systems, Apache Hive offers a cost-effective and scalable solution for processing big data. Its ability to work with various file formats and integrate with other Hadoop ecosystem tools makes it a versatile and powerful tool for big data processing.\n\nTo sum all this up, Apache Hive has revolutionized big data processing by enabling organizations to manage and analyze massive amounts of data efficiently and cost-effectively, making it an essential tool for any modern big data architecture. And if you need help getting started, don’t hesitate to contact our team of experts. We’d be happy to walk you through the basics and help get your Hive implementation up and running in no time!"
    }
]