[
    {
        "link": "https://geeksforgeeks.org/paging-in-operating-system",
        "document": "Paging is a memory management scheme that eliminates the need for a contiguous allocation of physical memory. The process of retrieving processes in the form of pages from the secondary storage into the main memory is known as paging. The basic purpose of paging is to separate each procedure into pages.\n\nThe mapping between logical pages and physical page frames is maintained by the page table, which is used by the memory management unit to translate logical addresses into physical addresses. The page table maps each logical page number to a physical page frame number. By using a Page Table, the operating system keeps track of the mapping between logical addresses (used by programs) and physical addresses (actual locations in memory).\n\nWhy Paging is used for memory Management?\n\nPaging is a memory management technique that addresses common challenges in allocating and managing memory efficiently. Here we can understand why paging is needed as a Memory Management technique:\n• Memory isn’t always available in a single block: Programs often need more memory than what is available in a single continuous block. Paging breaks memory into smaller, fixed-size pieces, making it easier to allocate scattered free spaces.\n• Processes size can increase or decrease: programs don’t need to occupy continuous memory, so they can grow dynamically without the need to be moved.\n• Logical Address or Virtual Address: , also known as the Virtual Address, is the address generated by the CPU when a program accesses memory.\n• Logical Address Space or Virtual Address Space: The Logical Address Space, also known as the Virtual Address Space, refers to the set of all possible logical addresses that a process can generate during its execution. It is a conceptual range of memory addresses used by a program and is independent of the actual physical memory (RAM).\n• Physical Address: is the actual location in the computer’s physical memory (RAM) where data or instructions are stored. It is used by the memory hardware to access specific data in the system’s memory.\n• Physical Address Space: The Physical Address Space refers to the total range of addresses available in a computer’s physical memory (RAM). It represents the actual memory locations that can be accessed by the system hardware to store or retrieve data.\n• Logical to bodily address mapping: In paging, the logical address area of a technique is divided into constant-sized pages, and each web page is mapped to a corresponding physical body within the main reminiscence. This permits the working gadget to manipulate the memory in an extra flexible way, as it is able to allocate and deallocate frames as needed.\n• Fixed web page and frame length: Paging makes use of a set web page length, which is usually identical to the size of a frame within the most important memory. This facilitates simplifying the reminiscence control technique and improves device performance.\n• Page desk entries: Each page within the logical address area of a method is represented through a , which contains facts approximately the corresponding bodily body in the predominant memory. This consists of the frame range, in addition to other manipulate bits which can be used by the running machine to manage the reminiscence.\n• A number of page desk entries: The range of page desk entries in a manner’s page desk is identical to the wide variety of pages inside the logical deal with the area of the technique.\n• Page table stored in important memory: The web page desk for each system is typically saved in important reminiscence, to allow for green get right of entry to and change by the operating device. However, this could additionally introduce overhead, because the web page table must be updated on every occasion a system is swapped in or out of the main memory.\n\nPaging is a method used by operating systems to manage memory efficiently. In paging, the physical memory is divided into fixed-size blocks called page frames, which are the same size as the pages used by the process. The process’s logical address space is also divided into fixed-size blocks called pages, which are the same size as the page frames.\n\nWhen a process requests memory, the operating system allocates one or more page frames to the process and maps the process’s logical pages to the physical page frames. When a program runs, its pages are loaded into any available frames in the physical memory.\n\nThis approach prevents fragmentation issues by keeping memory allocation uniform. Each program has a page table, which the operating system uses to keep track of where each page is stored in physical memory. When a program accesses data, the system uses this table to convert the program’s address into a physical memory address.\n\nPaging allows for better memory use and makes it easier to manage. It also supports virtual memory, letting parts of programs be stored on disk and loaded into memory only when needed. This way, even large programs can run without fitting entirely into main memory.\n\nThe mapping from virtual to physical address is done by the Memory Management Unit (MMU) which is a hardware device and this mapping is known as the paging technique.\n• None The Physical Address Space is conceptually divided into a number of fixed-size blocks, called frames\n• None The Logical Address Space is also split into fixed-size blocks, called pages\n\nThe address generated by the CPU is divided into\n• Page number(p): Number of bits required to represent the pages in\n• Page offset(d): Number of bits required to represent a particular word in a page or page size of Logical Address Space or word number of a page or page offset.\n\nA Physical Address is divided into two main parts:\n• Frame Number(f): Number of bits required to represent the frame of Physical Address Space or Frame number frame\n• Frame Offset(d): Number of bits required to represent a particular word in a frame or frame size of Physical Address Space or word number of a frame or frame offset.\n\nSo, a physical address in this scheme may be represented as follows:\n• None Each entry in TLB consists of two parts: a tag and a value.\n• None When this memory is used, then an item is compared with all tags simultaneously. If the item is found, then the corresponding value is returned.\n\nPaging is a memory management technique used in operating systems to manage memory and allocate memory to processes. In paging, memory is divided into fixed-size blocks called pages, and processes are allocated memory in terms of these pages. Each page is of the same size, and the size is typically a power of 2, such as 4KB or 8 KB.\n\nThe hardware implementation of the page table can be done by using dedicated registers. But the usage of the register for the page table is satisfactory only if the page table is small. If the page table contains a large number of entries then we can use TLB(translation Look-aside buffer), a special, small, fast look-up hardware cache.\n• None Each entry in TLB consists of two parts: a tag and a value.\n• None When this memory is used, then an item is compared with all tags simultaneously. If the item is found, then the corresponding value is returned.\n\nRead more about – TLB hit and miss\n• Eliminates External Fragmentation: Paging divides memory into fixed-size blocks (pages and frames), so processes can be loaded wherever there is free space in memory. This prevents wasted space due to fragmentation.\n• Efficient Memory Utilization: Since pages can be placed in non-contiguous memory locations, even small free spaces can be utilized, leading to better memory allocation.\n• Supports Virtual Memory: Paging enables the implementation of virtual memory, allowing processes to use more memory than physically available by swapping pages between RAM and secondary storage.\n• Ease of Swapping: Individual pages can be moved between physical memory and disk (swap space) without affecting the entire process, making swapping faster and more efficient.\n• Improved Security and Isolation: Each process works within its own set of pages, preventing one process from accessing another’s memory space.\n• Internal Fragmentation: If the size of a process is not a perfect multiple of the page size, the unused space in the last page results in internal fragmentation.\n• Increased Overhead: Table requires additional memory and processing. For large processes, the page table can grow significantly, consuming valuable memory resources.\n• Page Table Lookup Time: Accessing memory requires translating logical addresses to physical addresses using the page table. This additional step increases memory access time, although Translation Lookaside Buffers (TLBs) can help reduce the impact.\n• I/O Overhead During Page Faults: When a required page is not in physical memory (page fault), it needs to be fetched from secondary storage, causing delays and increased I/O operations.\n• Complexity in Implementation: Paging requires sophisticated hardware and software support, including the Memory Management Unit (MMU) and algorithms for which add complexity to the system.\n\nA memory management unit (MMU) is a technique used to convert logical address to physical address. Logical address is the address generated by CPU for each page and physical address is the real address of the frame where page is going to be stored.\n\nWhenever a page has to be accessed by CPU using the logical address, it requires physical address for accessing the page. Logical address comprises of two parts: Page Number and Offset.\n\nAlso read – Multilevel Paging in Operating System\n\nAlso read – Paged Segmentation and Segmented Paging\n\nIn conclusion, paging is a memory management technique that helps computers in storing data efficiently and it also helps in retrieve data as it breaks the memory into small, fixed size chunks called pages. It helps in handling larger amount of data without the issue of fragmentation that improves the performance and usability.\n\nWhat is the use of Paging in an Operating System?\n\nWhat is the basic advantage of Paging?\n\nWhat is the effect of Paging?"
    },
    {
        "link": "https://kernel.org/doc/gorman/html/understand/understand006.html",
        "document": "Linux layers the machine independent/dependent layer in an unusual manner in comparison to other operating systems�[ ]. Other operating systems have objects which manage the underlying physical pages such as the object in BSD. Linux instead maintains the concept of a three-level page table in the architecture independent code even if the underlying architecture does not support it. While this is conceptually easy to understand, it also means that the distinction between different types of pages is very blurry and page types are identified by their flags or what lists they exist on rather than the objects they belong to.\n\nArchitectures that manage their Memory Management Unit (MMU) differently are expected to emulate the three-level page tables. For example, on the x86 without PAE enabled, only two page table levels are available. The Page Middle Directory (PMD) is defined to be of size 1 and “folds back” directly onto the Page Global Directory (PGD) which is optimised out at compile time. Unfortunately, for architectures that do not manage their cache or Translation Lookaside Buffer (TLB) automatically, hooks for machine dependent have to be explicitly left in the code for when the TLB and CPU caches need to be altered and flushed even if they are null operations on some architectures like the x86. These hooks are discussed further in Section 3.8.\n\nThis chapter will begin by describing how the page table is arranged and what types are used to describe the three separate levels of the page table followed by how a virtual address is broken up into its component parts for navigating the table. Once covered, it will be discussed how the lowest level entry, the Page Table Entry (PTE) and what bits are used by the hardware. After that, the macros used for navigating a page table, setting and checking attributes will be discussed before talking about how the page table is populated and how pages are allocated and freed for the use with page tables. The initialisation stage is then discussed which shows how the page tables are initialised during boot strapping. Finally, we will cover how the TLB and CPU caches are utilised.\n\nEach process a pointer ( → ) to its own Page Global Directory (PGD) which is a physical page frame. This frame contains an array of type which is an architecture specific type defined in < >. The page tables are loaded differently depending on the architecture. On the x86, the process page table is loaded by copying → into the register which has the side effect of flushing the TLB. In fact this is how the function is implemented in the architecture dependent code.\n\nEach active entry in the PGD table points to a page frame containing an array of Page Middle Directory (PMD) entries of type which in turn points to page frames containing Page Table Entries (PTE) of type , which finally points to page frames containing the actual user data. In the event the page has been swapped out to backing storage, the swap entry is stored in the PTE and used by during page fault to find the swap entry containing the page data. The page table layout is illustrated in Figure 3.1.\n\nAny given linear address may be broken up into parts to yield offsets within these three page table levels and an offset within the actual page. To help break up the linear address into its component parts, a number of macros are provided in triplets for each page table level, namely a , a and a macro. The macros specifies the length in bits that are mapped by each level of the page tables as illustrated in Figure 3.2.\n\nThe values can be ANDd with a linear address to mask out all the upper bits and is frequently used to determine if a linear address is aligned to a given level within the page table. The macros reveal how many bytes are addressed by each entry at each level. The relationship between the and macros is illustrated in Figure 3.3.\n\nFor the calculation of each of the triplets, only is important as the other two are calculated based on it. For example, the three macros for page level on the x86 are:\n\nis the length in bits of the offset part of the linear address space which is 12 bits on the x86. The size of a page is easily calculated as 2PAGE_SHIFT which is the equivalent of the code above. Finally the mask is calculated as the negation of the bits which make up the . If a page needs to be aligned on a page boundary, is used. This macro adds to the address before simply ANDing it with the to zero out the page offset bits.\n\nis the number of bits in the linear address which are mapped by the second level part of the table. The and are calculated in a similar way to the page level macros.\n\nis the number of bits which are mapped by the top, or first level, of the page table. The and are calculated in the same manner as above.\n\nThe last three macros of importance are the which determine the number of entries in each level of the page table. is the number of pointers in the PGD, 1024 on an x86 without PAE. is for the PMD, 1 on the x86 without PAE and is for the lowest level, 1024 on the x86.\n\nAs mentioned, each entry is described by the structs , and for PTEs, PMDs and PGDs respectively. Even though these are often just unsigned integers, they are defined as structs for two reasons. The first is for type protection so that they will not be used inappropriately. The second is for features like PAE on the x86 where an additional 4 bits is used for addressing more than 4GiB of memory. To store the protection bits, is defined which holds the relevant flags and is usually stored in the lower bits of a page table entry.\n\nFor type casting, 4 macros are provided in , which takes the above types and returns the relevant part of the structs. They are , , and . To reverse the type casting, 4 more macros are provided , , and .\n\nWhere exactly the protection bits are stored is architecture dependent. For illustration purposes, we will examine the case of an x86 architecture without PAE enabled but the same principles apply across architectures. On an x86 with no PAE, the is simply a 32 bit integer within a struct. Each points to an address of a page frame and all the addresses pointed to are guaranteed to be page aligned. Therefore, there are (12) bits in that 32 bit value that are free for status bits of the page table entry. A number of the protection and status bits are listed in Table ?? but what bits exist and what they mean varies between architectures.\n\nThese bits are self-explanatory except for the which we will discuss further. On the x86 with Pentium III and higher, this bit is called the Page Attribute Table (PAT) while earlier architectures such as the Pentium II had this bit reserved. The PAT bit is used to indicate the size of the page the PTE is referencing. In a PGD entry, this same bit is instead called the Page Size Exception (PSE) bit so obviously these bits are meant to be used in conjunction.\n\nAs Linux does not use the PSE bit for user pages, the PAT bit is free in the PTE for other purposes. There is a requirement for having a page resident in memory but inaccessible to the userspace process such as when a region is protected with with the flag. When the region is to be protected, the bit is cleared and the bit is set. The macro checks if either of these bits are set and so the kernel itself knows the PTE is present, just inaccessible to userspace which is a subtle, but important point. As the hardware bit is clear, a page fault will occur if the page is accessed so Linux can enforce the protection while still knowing the page is resident if it needs to swap it out or the process exits.\n\nMacros are defined in < > which are important for the navigation and examination of page table entries. To navigate the page directories, three macros are provided which break up a linear address space into its component parts. takes an address and the for the process and returns the PGD entry that covers the requested address. takes a PGD entry and an address and returns the relevant PMD. takes a PMD and returns the relevant PTE. The remainder of the linear address provided is the offset within the page. The relationship between these fields is illustrated in Figure 3.1.\n\nThe second round of macros determine if the page table entries are present or may be used.\n• , and return 1 if the corresponding entry does not exist;\n• , and return 1 if the corresponding page table entries have the bit set;\n• , and will clear the corresponding page table entry;\n• and are used to check entries when passed as input parameters to functions that may change the value of the entries. Whether it returns 1 varies between the few architectures that define these macros but for those that actually define it, making sure the page entry is marked as present and accessed are the two most important checks.\n\nThere are many parts of the VM which are littered with page table walk code and it is important to recognise it. A very simple example of a page table walk is the function in . The following is an excerpt from that function, the parts unrelated to the page table walk are omitted:\n\nIt simply uses the three offset macros to navigate the page tables and the and macros to make sure it is looking at a valid page table.\n\nThe third set of macros examine and set the permissions of an entry. The permissions determine what a userspace process can and cannot do with a particular page. For example, the kernel page table entries are never readable by a userspace process.\n• The read permissions for an entry are tested with , set with and cleared with ;\n• The write permissions are tested with , set with and cleared with ;\n• The execute permissions are tested with , set with and cleared with . It is worth nothing that with the x86 architecture, there is no means of setting execute permissions on pages so these three macros act the same way as the read macros;\n• The permissions can be modified to a new value with but its use is almost non-existent. It is only used in the function in .\n\nThe fourth set of macros examine and set the state of an entry. There are only two bits that are important in Linux, the dirty bit and the accessed bit. To check these bits, the macros and macros are used. To set the bits, the macros and are used. To clear them, the macros and are available.\n\nThis set of functions and macros deal with the mapping of addresses and pages to PTEs and the setting of the individual entries.\n\nThe macro takes a and protection bits and combines them together to form the that needs to be inserted into the page table. A similar macro exists which takes a physical page address as a parameter.\n\nThe macro returns the which corresponds to the PTE entry. returns the containing the set of PTEs.\n\nThe macro takes a such as that returned by and places it within the processes page tables. is the reverse operation. An additional function is provided called which clears an entry from the process page table and returns the . This is important when some modification needs to be made to either the PTE protection or the itself.\n\nThe last set of functions deal with the allocation and freeing of page tables. Page tables, as stated, are physical pages containing an array of entries and the allocation and freeing of physical pages is a relatively expensive operation, both in terms of time and the fact that interrupts are disabled during page allocation. The allocation and deletion of page tables, at any of the three levels, is a very frequent operation so it is important the operation is as quick as possible.\n\nHence the pages used for the page tables are cached in a number of different lists called quicklists. Each architecture implements these caches differently but the principles used are the same. For example, not all architectures cache PGDs because the allocation and freeing of them only happens during process creation and exit. As both of these are very expensive operations, the allocation of another page is negligible.\n\nPGDs, PMDs and PTEs have two sets of functions each for the allocation and freeing of page tables. The allocation functions are , and respectively and the free functions are, predictably enough, called , and .\n\nBroadly speaking, the three implement caching with the use of three caches called , and . Architectures implement these three lists in different ways but one method is through the use of a LIFO type structure. Ordinarily, a page table entry contains points to other pages containing page tables or data. While cached, the first element of the list is used to point to the next free page table. During allocation, one page is popped off the list and during free, one is placed as the new head of the list. A count is kept of how many pages are used in the cache.\n\nThe quick allocation function from the is not externally defined outside of the architecture although is a common choice for the function name. The cached allocation function for PMDs and PTEs are publicly defined as and .\n\nIf a page is not available from the cache, a page will be allocated using the physical page allocator (see Chapter 6). The functions for the three levels of page tables are , and .\n\nObviously a large number of pages may exist on these caches and so there is a mechanism in place for pruning them. Each time the caches grow or shrink, a counter is incremented or decremented and it has a high and low watermark. is called in two places to check these watermarks. When the high watermark is reached, entries from the cache will be freed until the cache size returns to the low watermark. The function is called after when a large number of page tables are potentially reached and is also called by the system idle task.\n\nWhen the system first starts, paging is not enabled as page tables do not magically initialise themselves. Each architecture implements this differently so only the x86 case will be discussed. The page table initialisation is divided into two phases. The bootstrap phase sets up page tables for just 8MiB so the paging unit can be enabled. The second phase initialises the rest of the page tables. We discuss both of these phases below.\n\nThe assembler function is responsible for enabling the paging unit in . While all normal kernel code in is compiled with the base address at , the kernel is actually loaded beginning at the first megabyte (0x00100000) of memory. The first megabyte is used by some devices for communication with the BIOS and is skipped. The bootstrap code in this file treats 1MiB as its base address by subtracting from any address until the paging unit is enabled so before the paging unit is enabled, a page table mapping has to be established which translates the 8MiB of physical memory to the virtual address .\n\nInitialisation begins with statically defining at compile time an array called which is placed using linker directives at 0x00101000. It then establishes page table entries for 2 pages, and . If the processor supports the Page Size Extension (PSE) bit, it will be set so that pages will be translated are 4MiB pages, not 4KiB as is the normal case. The first pointers to and are placed to cover the region the second pointers to and are placed at . This means that when paging is enabled, they will map to the correct pages using either physical or virtual addressing for just the kernel image. The rest of the kernel page tables will be initialised by .\n\nOnce this mapping has been established, the paging unit is turned on by setting a bit in the register and a jump takes places immediately to ensure the Instruction Pointer (EIP register) is correct.\n\nThe function responsible for finalising the page tables is called . The call graph for this function on the x86 can be seen on Figure 3.4.\n\nThe function first calls to initialise the page tables necessary to reference all physical memory in and . Remember that high memory in cannot be directly referenced and mappings are set up for it temporarily. For each used by the kernel, the boot memory allocator (see Chapter 5) is called to allocate a page for the PMDs and the PSE bit will be set if available to use 4MiB TLB entries instead of 4KiB. If the PSE bit is not supported, a page for PTEs will be allocated for each . If the CPU supports the PGE flag, it also will be set so that the page table entry will be global and visible to all processes.\n\nNext, calls to setup the fixed address space mappings at the end of the virtual address space starting at . These mappings are used for purposes such as the local APIC and the atomic kmappings between and required by . Finally, the function calls to initialise the page table entries required for normal high memory mappings with .\n\nOnce returns, the page tables for kernel space are now full initialised so the static PGD ( ) is loaded into the CR3 register so that the static table is now being used by the paging unit.\n\nThe next task of the is responsible for calling to initialise each of the PTEs with the protection flags. The final task is to call which initialises all the zone structures used.\n\nThere is a requirement for Linux to have a fast method of mapping virtual addresses to physical addresses and for mapping s to their physical address. Linux achieves this by knowing where, in both virtual and physical memory, the global array is as the global array has pointers to all s representing physical memory in the system. All architectures achieve this with very similar mechanisms but for illustration purposes, we will only examine the x86 carefully. This section will first discuss how physical addresses are mapped to kernel virtual addresses and then what this means to the array.\n\nAs we saw in Section 3.6, Linux sets up a direct mapping from the physical address 0 to the virtual address at 3GiB on the x86. This means that any virtual address can be translated to the physical address by simply subtracting which is essentially what the function with the macro does:\n\nObviously the reverse operation involves simply adding which is carried out by the function with the macro . Next we see how this helps the mapping of s to physical addresses.\n\nAs we saw in Section 3.6.1, the kernel image is located at the physical address 1MiB, which of course translates to the virtual address and a virtual region totaling about 8MiB is reserved for the image which is the region that can be addressed by two PGDs. This would imply that the first available memory to use is located at but that is not the case. Linux tries to reserve the first 16MiB of memory for so first virtual area used for kernel allocations is actually . This is where the global is usually located. will be still get used, but only when absolutely necessary.\n\nPhysical addresses are translated to s by treating them as an index into the array. Shifting a physical address bits to the right will treat it as a PFN from physical address 0 which is also an index within the array. This is exactly what the macro does which is declared as follows in < >:\n\nThe macro takes the virtual address , converts it to the physical address with , converts it into an array index by bit shifting it right bits and indexing into the by simply adding them together. No macro is available for converting s to physical addresses but at this stage, it should be obvious to see how it could be calculated.\n\nInitially, when the processor needs to map a virtual address to a physical address, it must traverse the full page directory searching for the PTE of interest. This would normally imply that each assembly instruction that references memory actually requires several separate memory references for the page table traversal�[ ]. To avoid this considerable overhead, architectures take advantage of the fact that most processes exhibit a locality of reference or, in other words, large numbers of memory references tend to be for a small number of pages. They take advantage of this reference locality by providing a Translation Lookaside Buffer (TLB) which is a small associative memory that caches virtual to physical page table resolutions.\n\nLinux assumes that the most architectures support some type of TLB although the architecture independent code does not cares how it works. Instead, architecture dependant hooks are dispersed throughout the VM code at points where it is known that some hardware with a TLB would need to perform a TLB related operation. For example, when the page tables have been updated, such as after a page fault has completed, the processor may need to be update the TLB for that virtual address mapping.\n\nNot all architectures require these type of operations but because some do, the hooks have to exist. If the architecture does not require the operation to be performed, the function for that TLB operation will a null operation that is optimised out at compile time.\n\nA quite large list of TLB API hooks, most of which are declared in < >, are listed in Tables 3.2 and ?? and the APIs are quite well documented in the kernel source by �[ ]. It is possible to have just one TLB flush function but as both TLB flushes and TLB refills are very expensive operations, unnecessary TLB flushes should be avoided if at all possible. For example, when context switching, Linux will avoid loading new page tables using Lazy TLB Flushing, discussed further in Section 4.3.\n\nAs Linux manages the CPU Cache in a very similar fashion to the TLB, this section covers how Linux utilises and manages the CPU cache. CPU caches, like TLB caches, take advantage of the fact that programs tend to exhibit a locality of reference�[ ]�[ ]. To avoid having to fetch data from main memory for each reference, the CPU will instead cache very small amounts of data in the CPU cache. Frequently, there is two levels called the Level 1 and Level 2 CPU caches. The Level 2 CPU caches are larger but slower than the L1 cache but Linux only concerns itself with the Level 1 or L1 cache.\n\nCPU caches are organised into lines. Each line is typically quite small, usually 32 bytes and each line is aligned to it's boundary size. In other words, a cache line of 32 bytes will be aligned on a 32 byte address. With Linux, the size of the line is which is defined by each architecture.\n\nHow addresses are mapped to cache lines vary between architectures but the mappings come under three headings, direct mapping, associative mapping and set associative mapping. Direct mapping is the simpliest approach where each block of memory maps to only one possible cache line. With associative mapping, any block of memory can map to any cache line. Set associative mapping is a hybrid approach where any block of memory can may to any line but only within a subset of the available lines. Regardless of the mapping scheme, they each have one thing in common, addresses that are close together and aligned to the cache size are likely to use different lines. Hence Linux employs simple tricks to try and maximise cache usage\n• Frequently accessed structure fields are at the start of the structure to increase the chance that only one line is needed to address the common fields;\n• Unrelated items in a structure should try to be at least cache size bytes apart to avoid false sharing between CPUs;\n• Objects in the general caches, such as the cache, are aligned to the L1 CPU cache to avoid false sharing.\n\nIf the CPU references an address that is not in the cache, a cache missccurs and the data is fetched from main memory. The cost of cache misses is quite high as a reference to cache can typically be performed in less than 10ns where a reference to main memory typically will cost between 100ns and 200ns. The basic objective is then to have as many cache hits and as few cache misses as possible.\n\nJust as some architectures do not automatically manage their TLBs, some do not automatically manage their CPU caches. The hooks are placed in locations where the virtual to physical mapping changes, such as during a page table update. The CPU cache flushes should always take place first as some CPUs require a virtual to physical mapping to exist when the virtual address is being flushed from the cache. The three operations that require proper ordering are important is listed in Table 3.4.\n\nThe API used for flushing the caches are declared in < > and are listed in Tables 3.5. In many respects, it is very similar to the TLB flushing API.\n\nIt does not end there though. A second set of interfaces is required to avoid virtual aliasing problems. The problem is that some CPUs select lines based on the virtual address meaning that one physical address can exist on multiple lines leading to cache coherency problems. Architectures with this problem may try and ensure that shared mappings will only use addresses as a stop-gap measure. However, a proper API to address is problem is also supplied which is listed in Table 3.6.\n\nMost of the mechanics for page table management are essentially the same for 2.6 but the changes that have been introduced are quite wide reaching and the implementations in-depth.\n\nA new file has been introduced called . This source file contains replacement code for functions that assume the existence of a MMU like for example. This is to support architectures, usually microcontrollers, that have no MMU. Much of the work in this area was developed by the uCLinux Project (http://www.uclinux.org).\n\nThe most significant and important change to page table management is the introduction of Reverse Mapping (rmap). Referring to it as “rmap” is deliberate as it is the common usage of the “acronym” and should not be confused with the -rmap tree developed by Rik van Riel which has many more alterations to the stock VM than just the reverse mapping.\n\nIn a single sentence, rmap grants the ability to locate all PTEs which map a particular page given just the . In 2.4, the only way to find all PTEs which map a shared page, such as a memory mapped shared library, is to linearaly search all page tables belonging to all processes. This is far too expensive and Linux tries to avoid the problem by using the swap cache (see Section 11.4). This means that with many shared pages, Linux may have to swap out entire processes regardless of the page age and usage patterns. 2.6 instead has a PTE chain associated with every which may be traversed to remove a page from all page tables that reference it. This way, pages in the LRU can be swapped out in an intelligent manner without resorting to swapping entire processes.\n\nAs might be imagined by the reader, the implementation of this simple concept is a little involved. The first step in understanding the implementation is the that is a field in . This has union has two fields, a pointer to a called and a called . The union is an optisation whereby is used to save memory if there is only one PTE mapping the entry, otherwise a chain is used. The type varies between architectures but whatever its type, it can be used to locate a PTE, so we will treat it as a for simplicity.\n\nThe is a little more complex. The struct itself is very simple but it is compact with overloaded fields and a lot of development effort has been spent on making it small and efficient. Fortunately, this does not make it indecipherable.\n\nFirst, it is the responsibility of the slab allocator to allocate and manage s as it is this type of task the slab allocator is best at. Each can hold up to pointers to PTE structures. Once that many PTEs have been filled, a is allocated and added to the chain.\n\nThe has two fields. The first is which has two purposes. When is ANDed with , it returns the number of PTEs currently in this indicating where the next free slot is. When is ANDed with the negation of (i.e. ∼ ), a pointer to the next in the chain is returned1. This is basically how a PTE chain is implemented.\n\nTo give a taste of the rmap intricacies, we'll give an example of what happens when a new PTE needs to map a page. The basic process is to have the caller allocate a new with . This allocated chain is passed with the and the PTE to . If the existing PTE chain associated with the page has slots available, it will be used and the allocated by the caller returned. If no slots were available, the allocated will be added to the chain and NULL returned.\n\nThere is a quite substantial API associated with rmap, for tasks such as creating chains and adding and removing PTEs to a chain, but a full listing is beyond the scope of this section. Fortunately, the API is confined to and the functions are heavily commented so their purpose is clear.\n\nThere are two main benefits, both related to pageout, with the introduction of reverse mapping. The first is with the setup and tear-down of pagetables. As will be seen in Section 11.4, pages being paged out are placed in a swap cache and information is written into the PTE necessary to find the page again. This can lead to multiple minor faults as pages are put into the swap cache and then faulted again by a process. With rmap, the setup and removal of PTEs is atomic. The second major benefit is when pages need to paged out, finding all PTEs referencing the pages is a simple operation but impractical with 2.4, hence the swap cache.\n\nReverse mapping is not without its cost though. The first, and obvious one, is the additional space requirements for the PTE chains. Arguably, the second is a CPU cost associated with reverse mapping but it has not been proved to be significant. What is important to note though is that reverse mapping is only a benefit when pageouts are frequent. If the machines workload does not result in much pageout or memory is ample, reverse mapping is all cost with little or no benefit. At the time of writing, the merits and downsides to rmap is still the subject of a number of discussions.\n\nThe reverse mapping required for each page can have very expensive space requirements. To compound the problem, many of the reverse mapped pages in a VMA will be essentially identical. One way of addressing this is to reverse map based on the VMAs rather than individual pages. That is, instead of having a reverse mapping for each page, all the VMAs which map a particular page would be traversed and unmap the page from each. Note that objects in this case refers to the VMAs, not an object in the object-orientated sense of the word2. At the time of writing, this feature has not been merged yet and was last seen in kernel 2.5.68-mm1 but there is a strong incentive to have it available if the problems with it can be resolved. For the very curious, the patch for just file/device backed objrmap at this release is available 3 but it is only for the very very curious reader.\n\nThere are two tasks that require all PTEs that map a page to be traversed. The first task is which checks all PTEs that map a page to see if the page has been referenced recently. The second task is when a page needs to be unmapped from all processes with . To complicate matters further, there are two types of mappings that must be reverse mapped, those that are backed by a file or device and those that are anonymous. In both cases, the basic objective is to traverse all VMAs which map a particular page and then walk the page table for that VMA to get the PTE. The only difference is how it is implemented. The case where it is backed by some sort of file is the easiest case and was implemented first so we'll deal with it first. For the purposes of illustrating the implementation, we'll discuss how is implemented.\n\ncalls which is the top level function for finding all PTEs within VMAs that map the page. As the page is mapped for a file or device, → contains a pointer to a valid . The has two linked lists which contain all VMAs which use the mapping with the → and → fields. For every VMA that is on these linked lists, is called with the VMA and the page as parameters. The function first checks if the page is in an address managed by this VMA and if so, traverses the page tables of the using the VMA ( → ) until it finds the PTE mapping the page for that .\n\nAnonymous page tracking is a lot trickier and was implented in a number of stages. It only made a very brief appearance and was removed again in 2.5.65-mm4 as it conflicted with a number of other changes. The first stage in the implementation was to use → and → fields to track and pairs. These fields previously had been used to store a pointer to and a pointer to the (See Chapter 11). Exactly how it is addressed is beyond the scope of this section but the summary is that is stored in →\n\nworks in a similar fashion but obviously, all the PTEs that reference a page with this method can do so without needing to reverse map the individual pages. There is a serious search complexity problem that is preventing it being merged. The scenario that describes the problem is as follows;\n\nTake a case where 100 processes have 100 VMAs mapping a single file. To unmap a single page in this case with object-based reverse mapping would require 10,000 VMAs to be searched, most of which are totally unnecessary. With page based reverse mapping, only 100 slots need to be examined, one for each process. An optimisation was introduced to order VMAs in the by virtual address but the search for a single page is still far too expensive for object-based reverse mapping to be merged.\n\nIn 2.4, page table entries exist in as the kernel needs to be able to address them directly during a page table walk. This was acceptable until it was found that, with high memory machines, was being consumed by the third level page table PTEs. The obvious answer is to move PTEs to high memory which is exactly what 2.6 does.\n\nAs we will see in Chapter 9, addressing information in high memory is far from free, so moving PTEs to high memory is a compile time configuration option. In short, the problem is that the kernel must map pages from high memory into the lower address space before it can be used but there is a very limited number of slots available for these mappings introducing a troublesome bottleneck. However, for applications with a large number of PTEs, there is little other option. At time of writing, a proposal has been made for having a User Kernel Virtual Area (UKVA) which would be a region in kernel space private to each process but it is unclear if it will be merged for 2.6 or not.\n\nTo take the possibility of high memory mapping into account, the macro from 2.4 has been replaced with in 2.6. If PTEs are in low memory, this will behave the same as and return the address of the PTE. If the PTE is in high memory, it will first be mapped into low memory with so it can be used by the kernel. This PTE must be unmapped as quickly as possible with .\n\nIn programming terms, this means that page table walk code looks slightly different. In particular, to find the PTE for a given address, the code now reads as (taken from );\n\nAdditionally, the PTE allocation API has changed. Instead of , there is now a for use with kernel PTE mappings and for userspace mapping. The principal difference between them is that will never use high memory for the PTE.\n\nIn memory management terms, the overhead of having to map the PTE from high memory should not be ignored. Only one PTE may be mapped per CPU at a time, although a second may be mapped with . This introduces a penalty when all PTEs need to be examined, such as during when all PTEs in a given range need to be unmapped.\n\nAt time of writing, a patch has been submitted which places PMDs in high memory using essentially the same mechanism and API changes. It is likely that it will be merged.\n\nMost modern architectures support more than one page size. For example, on many x86 architectures, there is an option to use 4KiB pages or 4MiB pages. Traditionally, Linux only used large pages for mapping the actual kernel image and no where else. As TLB slots are a scarce resource, it is desirable to be able to take advantages of the large pages especially on machines with large amounts of physical memory.\n\nIn 2.6, Linux allows processes to use “huge pages”, the size of which is determined by . The number of available huge pages is determined by the system administrator by using the proc interface which ultimatly uses the function . As the success of the allocation depends on the availability of physically contiguous memory, the allocation should be made during system startup.\n\nThe root of the implementation is a Huge TLB Filesystem (hugetlbfs) which is a pseudo-filesystem implemented in . Basically, each file in this filesystem is backed by a huge page. During initialisation, registers the file system and mounts it as an internal filesystem with .\n\nThere are two ways that huge pages may be accessed by a process. The first is by using to setup a shared region backed by huge pages and the second is the call on a file opened in the huge page filesystem.\n\nWhen a shared memory region should be backed by huge pages, the process should call and pass as one of the flags. This results in being called which creates a new file in the root of the internal hugetlb filesystem. A file is created in the root of the internal filesystem. The name of the file is determined by an atomic counter called which is incremented every time a shared region is setup.\n\nTo create a file backed by huge pages, a filesystem of type hugetlbfs must first be mounted by the system administrator. Instructions on how to perform this task are detailed in . Once the filesystem is mounted, files can be created as normal with the system call . When is called on the open file, the struct ensures that is called to setup the region properly.\n\nHuge TLB pages have their own function for the management of page tables, address space operations and filesystem operations. The names of the functions for page table management can all be seen in < > and they are named very similar to their “normal” page equivalents. The implementation of the hugetlb functions are located near their normal page equivalents so are easy to find.\n\nThe changes here are minimal. The API function has being totally removed and a new API has been introduced."
    },
    {
        "link": "https://www2.cs.uregina.ca/~hamilton/courses/330/notes/memory/paging.html",
        "document": "The increased hit rate produces only a 22-percent slowdown in memory access time. \n\n The hit ratio is clearly related to the number of associative registers. With the number of associative registers ranging between 16 and 512, a hit ratio of 80 to 98 percent can be obtained."
    },
    {
        "link": "https://stackoverflow.com/questions/17646754/determining-page-numbers-and-offsets-for-given-addresses",
        "document": "Consider a computer system with a 32-bit logical address and 4KB page size. The system supports up to 512MB of physical memory.\n\nWhy did I have to to get the answer?\n\nHow many entries are there in an inverted page table? An inverted page table needs as many entries as there are page frames in memory.\n\nWhy did I have to to get the inverted page table entries?\n\nHow do I figure out these page numbers and offsets based on the hex addresses?\n\nI know the answers and but I want to understand WHY and HOW. Can someone please explain in detail :)"
    },
    {
        "link": "https://byjus.com/gate/paging-in-operating-system-notes",
        "document": "Paging is a method of gaining access to data more quickly. When a program requires a page, it is available in the main memory because the OS copies a set number of pages from the storage device into the main memory. Paging permits a process’s physical address space to be noncontiguous. Paging refers to a memory management strategy that does away with the need for the allocation of contiguous physical memory.\n\nIn this article, we will look more into the Paging in OS according to the GATE Syllabus for (Computer Science Engineering) CSE. Let us read ahead to find out more about it.\n• What is Paging in the OS?\n\nWhat is Paging in the OS?\n\nPaging is a storage mechanism used in OS to retrieve processes from secondary storage to the main memory as pages. The primary concept behind paging is to break each process into individual pages. Thus the primary memory would also be separated into frames.\n\nOne page of the process must be saved in one of the given memory frames. These pages can be stored in various memory locations, but finding contiguous frames/holes is always the main goal. Process pages are usually only brought into the main memory when they are needed; else, they are stored in the secondary storage.\n\nThe frame sizes may vary depending on the OS. Each frame must be of the same size. Since the pages present in paging are mapped on to the frames, the page size should be similar to the frame size.\n\nAssuming that the main memory is 16 KB and the frame size is 1 KB, the main memory will be partitioned into a collection of 16 1 KB frames. P1, P2, P3, and P4 are the four processes in the system, each of which is 4 KB in size. Each process is separated into 1 KB pages, allowing one page to be saved in a single frame.\n\nBecause all of the frames are initially empty, the pages of the processes will be stored in a continuous manner. The graphic below depicts frames, pages, and the mapping between them.\n\nConsider the case when P2 and P4 are shifted to the waiting state after a period of time. Eight frames are now empty, allowing other pages to be loaded in their stead. Inside the ready queue is the process P5, which is 8 KB (8 pages) in size.\n\nGiven that we have 8 noncontiguous frames accessible in memory, paging allows us to store the process in many locations. As a result, we can load the process P5 page instead of P2 and P4.\n\nThe Memory Management Unit (MMU) is responsible for converting logical addresses to physical addresses. The physical address refers to the actual address of a frame in which each page will be stored, whereas the logical address refers to the address that is generated by the CPU for each page.\n\nWhen the CPU accesses a page using its logical address, the OS must first collect the physical address in order to access that page physically. There are two elements to the logical address:\n\nThe OS’s memory management unit must convert the page numbers to the frame numbers.\n\nLet’s say the CPU requests the 10th word of the 4th page of process P3 in the image above. Because page number 4 of process P1 is stored at frame number 9, the physical address will be returned as the 10th word of the 9th frame.\n\nLet’s consider another example:\n• If the physical address is 12 bits, then the physical address space would be 4 K words\n• If the logical address is 13 bits, then the logical address space would be 8 K words\n• If the page size is equal to the frame size, which is equal to 1 K words (assumption),\n\nThe address generated by the CPU is divided into the following:\n• Page offset(d): It refers to the number of bits necessary to represent a certain word on a page, page size in Logical Address Space, or page word number or page offset.\n• Page number(p): It is the number of bits needed to represent the pages in the Logical Address Space or the page number.\n\nThe Physical Address is divided into the following:\n• Frame offset(d): It refers to the number of bits necessary to represent a certain word in a frame, or the Physical Address Space frame size, the word number of a frame, or the frame offset.\n• Frame number(f): It’s the number of bits needed to indicate a frame of the Physical Address Space or a frame number.\n\nDedicated registers can be used to implement the page table in hardware. However, using a register for the page table is only useful if the page table is tiny. We can employ TLB (translation look-aside buffer), a particular, tiny, fast look-up hardware cache if the page table has a significant number of entries.\n• LB entries are made up of two parts: a value and a tag.\n• When this memory is accessed, an item is compared to all tags at the same time.\n• If the object is located, the value associated with it is returned.\n\nIn case the page table is kept in the main memory,\n\nthen the effective access time would be = m(page table) + m(page in page table)\n\n1. The logical memory gets broken into various blocks of similar sizes known as _______.\n\nd) none of the above\n\n2. The physical memory gets broken into various fixed-sized blocks known as _________.\n\nd) none of the above\n\n3. Which of these is used in the form of an index in a given page table?\n\n4. The smaller page tables can be implemented in the form of a set of which of these?\n\nd) all of the above\n\nKeep learning and stay tuned to get the latest updates on GATE Exam along with GATE Eligibility Criteria, GATE 2023, GATE Admit Card, GATE Syllabus, GATE Previous Year Question Paper, and more."
    },
    {
        "link": "https://geeksforgeeks.org/paging-in-operating-system",
        "document": "Paging is a memory management scheme that eliminates the need for a contiguous allocation of physical memory. The process of retrieving processes in the form of pages from the secondary storage into the main memory is known as paging. The basic purpose of paging is to separate each procedure into pages.\n\nThe mapping between logical pages and physical page frames is maintained by the page table, which is used by the memory management unit to translate logical addresses into physical addresses. The page table maps each logical page number to a physical page frame number. By using a Page Table, the operating system keeps track of the mapping between logical addresses (used by programs) and physical addresses (actual locations in memory).\n\nWhy Paging is used for memory Management?\n\nPaging is a memory management technique that addresses common challenges in allocating and managing memory efficiently. Here we can understand why paging is needed as a Memory Management technique:\n• Memory isn’t always available in a single block: Programs often need more memory than what is available in a single continuous block. Paging breaks memory into smaller, fixed-size pieces, making it easier to allocate scattered free spaces.\n• Processes size can increase or decrease: programs don’t need to occupy continuous memory, so they can grow dynamically without the need to be moved.\n• Logical Address or Virtual Address: , also known as the Virtual Address, is the address generated by the CPU when a program accesses memory.\n• Logical Address Space or Virtual Address Space: The Logical Address Space, also known as the Virtual Address Space, refers to the set of all possible logical addresses that a process can generate during its execution. It is a conceptual range of memory addresses used by a program and is independent of the actual physical memory (RAM).\n• Physical Address: is the actual location in the computer’s physical memory (RAM) where data or instructions are stored. It is used by the memory hardware to access specific data in the system’s memory.\n• Physical Address Space: The Physical Address Space refers to the total range of addresses available in a computer’s physical memory (RAM). It represents the actual memory locations that can be accessed by the system hardware to store or retrieve data.\n• Logical to bodily address mapping: In paging, the logical address area of a technique is divided into constant-sized pages, and each web page is mapped to a corresponding physical body within the main reminiscence. This permits the working gadget to manipulate the memory in an extra flexible way, as it is able to allocate and deallocate frames as needed.\n• Fixed web page and frame length: Paging makes use of a set web page length, which is usually identical to the size of a frame within the most important memory. This facilitates simplifying the reminiscence control technique and improves device performance.\n• Page desk entries: Each page within the logical address area of a method is represented through a , which contains facts approximately the corresponding bodily body in the predominant memory. This consists of the frame range, in addition to other manipulate bits which can be used by the running machine to manage the reminiscence.\n• A number of page desk entries: The range of page desk entries in a manner’s page desk is identical to the wide variety of pages inside the logical deal with the area of the technique.\n• Page table stored in important memory: The web page desk for each system is typically saved in important reminiscence, to allow for green get right of entry to and change by the operating device. However, this could additionally introduce overhead, because the web page table must be updated on every occasion a system is swapped in or out of the main memory.\n\nPaging is a method used by operating systems to manage memory efficiently. In paging, the physical memory is divided into fixed-size blocks called page frames, which are the same size as the pages used by the process. The process’s logical address space is also divided into fixed-size blocks called pages, which are the same size as the page frames.\n\nWhen a process requests memory, the operating system allocates one or more page frames to the process and maps the process’s logical pages to the physical page frames. When a program runs, its pages are loaded into any available frames in the physical memory.\n\nThis approach prevents fragmentation issues by keeping memory allocation uniform. Each program has a page table, which the operating system uses to keep track of where each page is stored in physical memory. When a program accesses data, the system uses this table to convert the program’s address into a physical memory address.\n\nPaging allows for better memory use and makes it easier to manage. It also supports virtual memory, letting parts of programs be stored on disk and loaded into memory only when needed. This way, even large programs can run without fitting entirely into main memory.\n\nThe mapping from virtual to physical address is done by the Memory Management Unit (MMU) which is a hardware device and this mapping is known as the paging technique.\n• None The Physical Address Space is conceptually divided into a number of fixed-size blocks, called frames\n• None The Logical Address Space is also split into fixed-size blocks, called pages\n\nThe address generated by the CPU is divided into\n• Page number(p): Number of bits required to represent the pages in\n• Page offset(d): Number of bits required to represent a particular word in a page or page size of Logical Address Space or word number of a page or page offset.\n\nA Physical Address is divided into two main parts:\n• Frame Number(f): Number of bits required to represent the frame of Physical Address Space or Frame number frame\n• Frame Offset(d): Number of bits required to represent a particular word in a frame or frame size of Physical Address Space or word number of a frame or frame offset.\n\nSo, a physical address in this scheme may be represented as follows:\n• None Each entry in TLB consists of two parts: a tag and a value.\n• None When this memory is used, then an item is compared with all tags simultaneously. If the item is found, then the corresponding value is returned.\n\nPaging is a memory management technique used in operating systems to manage memory and allocate memory to processes. In paging, memory is divided into fixed-size blocks called pages, and processes are allocated memory in terms of these pages. Each page is of the same size, and the size is typically a power of 2, such as 4KB or 8 KB.\n\nThe hardware implementation of the page table can be done by using dedicated registers. But the usage of the register for the page table is satisfactory only if the page table is small. If the page table contains a large number of entries then we can use TLB(translation Look-aside buffer), a special, small, fast look-up hardware cache.\n• None Each entry in TLB consists of two parts: a tag and a value.\n• None When this memory is used, then an item is compared with all tags simultaneously. If the item is found, then the corresponding value is returned.\n\nRead more about – TLB hit and miss\n• Eliminates External Fragmentation: Paging divides memory into fixed-size blocks (pages and frames), so processes can be loaded wherever there is free space in memory. This prevents wasted space due to fragmentation.\n• Efficient Memory Utilization: Since pages can be placed in non-contiguous memory locations, even small free spaces can be utilized, leading to better memory allocation.\n• Supports Virtual Memory: Paging enables the implementation of virtual memory, allowing processes to use more memory than physically available by swapping pages between RAM and secondary storage.\n• Ease of Swapping: Individual pages can be moved between physical memory and disk (swap space) without affecting the entire process, making swapping faster and more efficient.\n• Improved Security and Isolation: Each process works within its own set of pages, preventing one process from accessing another’s memory space.\n• Internal Fragmentation: If the size of a process is not a perfect multiple of the page size, the unused space in the last page results in internal fragmentation.\n• Increased Overhead: Table requires additional memory and processing. For large processes, the page table can grow significantly, consuming valuable memory resources.\n• Page Table Lookup Time: Accessing memory requires translating logical addresses to physical addresses using the page table. This additional step increases memory access time, although Translation Lookaside Buffers (TLBs) can help reduce the impact.\n• I/O Overhead During Page Faults: When a required page is not in physical memory (page fault), it needs to be fetched from secondary storage, causing delays and increased I/O operations.\n• Complexity in Implementation: Paging requires sophisticated hardware and software support, including the Memory Management Unit (MMU) and algorithms for which add complexity to the system.\n\nA memory management unit (MMU) is a technique used to convert logical address to physical address. Logical address is the address generated by CPU for each page and physical address is the real address of the frame where page is going to be stored.\n\nWhenever a page has to be accessed by CPU using the logical address, it requires physical address for accessing the page. Logical address comprises of two parts: Page Number and Offset.\n\nAlso read – Multilevel Paging in Operating System\n\nAlso read – Paged Segmentation and Segmented Paging\n\nIn conclusion, paging is a memory management technique that helps computers in storing data efficiently and it also helps in retrieve data as it breaks the memory into small, fixed size chunks called pages. It helps in handling larger amount of data without the issue of fragmentation that improves the performance and usability.\n\nWhat is the use of Paging in an Operating System?\n\nWhat is the basic advantage of Paging?\n\nWhat is the effect of Paging?"
    },
    {
        "link": "https://geeksforgeeks.org/logical-and-physical-address-in-operating-system",
        "document": "A logical address is generated by the CPU while a program is running. The logical address is a virtual address as it does not exist physically, therefore, it is also known as a Virtual Address. The physical address describes the precise position of necessary data in a memory. Before they are used, the MMU must map the logical address to the physical address. In operating systems, logical and physical addresses are used to manage and access memory. Here is an overview of each in detail.\n\nA logical address, also known as a virtual address, is an address generated by the CPU during program execution. It is the address seen by the process and is relative to the program’s address space. The process accesses memory using logical addresses, which are translated by the operating system into physical addresses. An address that is created by the CPU while a program is running is known as a logical address. Because the logical address is virtual—that is, it doesn’t exist physically—it is also referred to as such. The CPU uses this address as a reference to go to the actual memory location. All logical addresses created from a program’s perspective are referred to as being in the “logical address space”. This address is used as a reference to access the physical memory location by CPU. The term Logical Address Space is used for the set of all logical addresses generated by a program’s perspective.\n\nA physical address is the actual address in the main memory where data is stored. It is a location in physical memory, as opposed to a virtual address. Physical addresses are used by the Memory Management Unit (MMU) to translate logical addresses into physical addresses. The user must use the corresponding logical address to go to the physical address rather than directly accessing the physical address. For a computer program to function, physical memory space is required. Therefore, the logical address and physical address need to be mapped before the program is run.\n\nThe term “physical address” describes the precise position of necessary data in a memory. Before they are used, the MMU must map the logical address to the physical address. This is because the user program creates the logical address and believes that the program is operating in this logical address. However, the program requires physical memory to execute. All physical addresses that match the logical addresses in a logical address space are collectively referred to as the “physical address space”\n\nThe translation from logical to physical addresses is performed by the operating system’s memory management unit (MMU) within the computer’s hardware architecture. The MMU uses a page table to translate logical addresses into physical addresses. The page table maps each logical page number to a physical frame number. While the operating system plan this process, it’s important to note that the MMU itself is a hardware component separate from the software-based elements of the operating system.\n\nSimilarities Between Logical and Physical Addresses in the Operating System\n• None Both logical and physical addresses are used to identify a specific location in memory.\n• None Both logical and physical addresses can be represented in different formats, such as binary, hexadecimal, or decimal.\n• None Both logical and physical addresses have a finite range, which is determined by the number of bits used to represent them.\n\nImportant Points about Logical and Physical Addresses in Operating Systems\n• None The use of logical addresses provides a layer of abstraction that allows processes to access memory without knowing the physical memory location.\n• None Logical addresses are mapped to physical addresses using a page table. The page table contains information about the mapping between logical and physical addresses.\n• None The MMU translates logical addresses into physical addresses using the page table. This translation is transparent to the process and is performed by hardware.\n• None The use of logical and physical addresses allows the operating system to manage memory more efficiently by using techniques such as paging and segmentation.\n\nThe physical hardware of a computer that manages its virtual memory and caching functions is called the memory management unit (MMU). The MMU is sometimes housed in a separate Integrated Chip (IC), but it is typically found inside the central processing unit (CPU) of the computer. The MMU receives all inputs for data requests and decides whether to retrieve the data from ROM or RAM storage.\n\nSome reference books on operating system concepts that cover logical and physical addressing include:\n• None “Operating Systems: Three Easy Pieces” by Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau. \n\n These books provide detailed coverage of operating system concepts, including memory management and addressing techniques.\n\nHow does the operating system map logical addresses to physical addresses?\n\nCan two processes have the same logical address?\n\nWhy do we need logical address?"
    },
    {
        "link": "https://phoenixnap.com/kb/paging",
        "document": "Paging is a memory management technique in operating systems that enables processes to access more memory than is physically available. The system improves performance and resource utilization and reduces the risk of page faults. This method is also known as swapping in Unix-like systems.\n\nThis article explains what paging in operating systems is and how it works.\n\nPaging is a memory management method that enables processes to use virtual storage. A process has access to the pages it needs without waiting for them to be loaded into physical memory. The technique stores and retrieves data from a computer's secondary or virtual storage (hard drive, SSD, etc.) to the primary storage (RAM).\n\nWhen a process tries to access a page that is not in RAM, the OS brings in the page from the virtual memory.\n\nPaging improves the efficiency of memory management. By dividing memory into pages, the operating system moves pages in and out of memory as needed. Keeping only the frequently used pages reduces the number of page faults, which improves system performance and responsiveness.\n\nPaging enables an OS to transfer data between secondary (virtual) and primary (physical) memory. Both storage types are divided into fixed-size blocks. The blocks in primary memory are frames, while those in secondary storage are pages.\n\nWhenever a program executes, it splits into pages, and the OS automatically stores them in secondary memory.\n\nWhen the process requests memory, the OS allocates page frames from the primary memory to the process. Next, the OS moves program pages from the secondary memory to the primary memory frames.\n\nThe logical pages and physical page frames maintain the relationship using a structure called the page table. The memory management unit (MMU) uses the page table to translate logical addresses (virtual memory) into physical addresses (physical memory).\n\nUse the following command to print the page table in the terminal/command prompt in Linux:\n\nFor the command to work, find the process ID first using the , top, , or command. For instance, if the PID for the running process of choice is 17422, print the table with the following command:\n\nThe command output consists of multiple lines, each with four columns except for the first. The first line shows the process ID (PID) and the name. The remaining lines list the mapped virtual page addresses for the process.\n\nThe columns in the output have the following information:\n• Mode. The permissions for the page.\n• Mapping. The name of the file or object the page is mapped to.\n\nKnowing the associated terminology is essential to understanding the paging process. Below is a list of common terminology when working with paging:\n• Pages. Fixed-sized blocks of logical/secondary memory. Pages represent a unit of information transfer between main memory and secondary storage. A page set is a process working set. A working set constantly changes as the process accesses different parts of memory.\n• Page table. A data structure that enables the memory management unit (MMU) to translate logical addresses into physical addresses.\n• Physical memory. The actual memory available to the computer, usually in the form of RAM. In paging, the physical, or main, memory splits into fixed-size blocks or page frames, each with a unique address. The frames are the same size as the pages used by the process.\n• Virtual memory (logical/secondary memory). A hard disk or solid-state drive that serves as an extension of the main memory. It compensates for physical memory shortages by temporarily bringing in data from RAM. The logical memory is divided into the same size blocks, called pages. Each page has a logical address within the virtual memory.\n• Memory management unit (MMU). A hardware component that translates logical addresses into physical addresses. The MMU also manages the allocation and deallocation of pages in memory.\n• Logical address (virtual address). When a program runs, the CPU generates a logical address for each page. The MMU translates logical into physical addresses.\n• Physical address. Each frame within a physical memory has a unique physical address. Addresses represent a specific location of a process in actual memory.\n• Physical address space. The range of all possible physical addresses a system is able to reference.\n\nPaging involves the systematic retrieval and display of segmented information. The paging workflow either ends with a page hit or encounters a page fault.\n\nThe workflow follows these steps:\n\n2. The OS allocates the process segments into the same-sized pages in the virtual memory. It also creates corresponding page frames in physical memory.\n\n3. The OS creates a page table, which maps the process's logical pages to the physical page frames.\n\n4. Another process requests access to the process frame or page.\n\n5a. The requested page is already in the main memory, and the process continues execution, creating a page hit.\n\n5b. If the requested page is not in the main memory, the system is dealing with the page fault or page miss. The paging process continues to resolve the issue:\n\n6. The memory management unit (MMU) uses the page table to identify the process's correct location (correct page frame) in the physical memory.\n\n7. The page table updates with the correct physical address.\n\n8. The OS brings the page into page frames in the primary memory,\n\n9. The process resumes from the point of interruption, and can now access the page. The page fault resolves.\n\nThe paging implementation offers multiple methods, and the choice of a specific technique depends on several factors. This includes the OS type, available memory, and the process's requirements.\n\nFor example, demand paging is a good choice for operating systems that need to be efficient. On the other hand, anticipatory paging is a better choice for operating systems that need to be responsive.\n\nThe text below explains the different paging techniques:\n• Demand paging. The most common memory management technique in modern operating systems like Windows, Linux, and macOS. It optimizes memory usage by keeping only the currently used page in memory.\n• Anticipatory paging. A more aggressive form of demand paging that preloads the pages near the requested page as soon as possible. The goal is to reduce page faults and latency by predicting which pages not in RAM are likely to be needed soon.\n• Prepaging. Prepaging is a less aggressive form of anticipatory paging. The system preloads any pages that might be needed in the near future (but not immediately or soon).\n• Free page queue, stealing, and reclamation. These techniques manage memory by keeping track of free frames and reallocating the frames as needed.\n• The free page queue. A list of page frames available for allocation. By ensuring the queue never empties, the process minimizes dealing with page faults.\n• Page stealing. A process of freeing up page frames. Pages that haven't been referenced recently are removed from memory. The free page frames are then added to the free page queue.\n• Reclaiming. Reclaiming (in conjunction with stealing) frees up page frames no longer in use. However, reclaiming is more aggressive. The OS can even reclaim the pages that are likely to restart soon.\n• Pre-cleaning. The process saves the contents of modified pages back to disk, even if those pages are likely to be modified again soon. When a new program runs, the system does not have to wait for the pages to be read from disk, which improves the start-up time.\n• Copy-on-write paging. When a process attempts to write to a page that is already in use by another process, the OS creates a copy of the page and gives the process exclusive access to the copy. This ensures that the two processes cannot overwrite each other's data.\n• Segmented paging. Combines paging with segmentation by splitting the main memory into variable-sized segments, each with a page table. The segments divide into even smaller, fixed-sized pages. Unlike virtual memory, which employs page-based division, segments handle the mapping of virtual to physical memory addresses.\n• Inverted paging. A technique where the page table is in memory, and the physical memory is divided into frames. When a process needs to access a page, it looks up the page table to find the physical frame.\n\nA page fault occurs when a process tries to access a page not present in physical memory. The page is unavailable either because it has not been in use recently or because there is insufficient memory.\n• The page has never been loaded into memory before.\n• The page has been removed from memory to make room for another page.\n• The page has been modified in memory and needs to be written back to disk.\n\nThe best strategy for dealing with page faults depends on several factors, including the OS type, the available memory, and the processes. The OS handles page faults using either a paging method or by terminating the process.\n\nPossible solutions are to do one of the following:\n• Terminate the program if the page is not important or if the program is not in a critical section. Different page replacement algorithms determine which page to evict when a page fault occurs. The most common page replacement algorithms include Least Recently Used (LRU), First In, First Out (FIFO), and Optimal.\n• Move an unused page to the secondary memory and bring in the page causing the page fault to the primary memory. The goal is to ensure the program is able to continue running.\n• Preload pages that the operating system predicts are going to be needed soon (anticipatory paging or prepaging). This helps reduce page faults but also leads to wasting memory if the preloaded pages are unused.\n\nThrashing occurs in virtual memory when the page fault rate becomes so high that the system spends more time servicing page faults and not enough time executing user code.\n\nThe feedback loop that causes thrashing begins when a process experiences a page fault. The operating system (OS) must then load the requested page from the disk into memory, which is time-consuming.\n\nIf the OS receives multiple page faults in a brief period, it spends all the time servicing page faults and none executing user code.\n\nIf thrashing occurs, the system continues to slow down until it becomes unusable. The only way to fix thrashing is to address the underlying causes. Some of the causes include:\n• Too many processes. When the operating system runs too many processes, it can't keep up with the demands. This leads to a high page faults rate, as the operating system has to swap pages in and out of memory frequently.\n• Too much memory usage. Processes that use a lot of memory prevent the operating system from simultaneously keeping all the pages in memory. This also leads to a high rate of page faults.\n• Poor page replacement algorithm. The page replacement algorithm decides which page to evict when a page fault occurs. If the page replacement algorithm is not good, it evicts frequently used pages, which leads to a high rate of page faults.\n• A high degree of multiprogramming. When the multiprogramming degree is high, processes run simultaneously in the system. This puts a strain on the operating system and leads to thrashing.\n• Lack of frames: If there are not enough frames available in memory, the operating system has to evict pages from memory when new pages arrive.\n\nDifferent methods to prevent thrashing are:\n\nPaging allows operating systems to use physical memory more efficiently and improves system performance. Key paging advantages are:\n• Reducing external fragmentation. Paging allows the operating system to use more memory than possible if all pages are simultaneously resident in memory. The OS moves pages not currently in use onto the disk and then swaps them back in when needed.\n• Improving memory usage. Paging improves performance by reducing the time the CPU waits for pages to load from the disk. The operating system keeps the most frequently used pages in memory and only swaps out the pages not used as often.\n• Simplifying memory management. Paging streamlines memory management for the operating system. Therefore, the OS only has to keep track of the pages currently in memory rather than the entire address space of each process.\n• Efficient swapping. With paging, the operating system does not have to consider fragmentation when swapping out a page. Moreover, the OS chooses the page least likely to be used.\n• Supports virtual memory. Virtual memory support means each process has its own address space, even if the physical memory is not large enough to accommodate all the processes.\n\nPaging is a very effective way to use physical memory, but it also adds complexity to the OS and causes page faults. While the advantages outweigh the disadvantages, the following list represents key drawbacks:\n• Increased overhead. Paging introduces overhead, as the operating system has to track which pages are in memory and which are on disk. Therefore, this overhead reduces performance, especially on systems with slow disks.\n• Page faults. Paging can lead to page faults. The faults occur when the operating system needs to load a page from the disk into memory. When they happen frequently, the system slows down.\n• Complexity. Paging is a complex and challenging system to implement and debug.\n• Internal fragmentation. Paging increases internal fragmentation, as the last page of a process is not fully used, leading to wasted memory.\n• Page table overhead: Paging requires page tables, which consume significant memory. Multilevel page tables and variable page sizes (super-pages) mitigate the issue.\n\nAfter reading this text, you now understand how paging in operating systems works, and the advantages and disadvantages.\n\nNext, learn how to check memory usage in Linux to monitor the system efficiently."
    },
    {
        "link": "https://workat.tech/core-cs/tutorial/paging-in-operating-system-os-knbcthp3w8o7",
        "document": ""
    },
    {
        "link": "https://lenovo.com/us/en/glossary/paging?srsltid=AfmBOoo_2IsXHtIAA561CyKyqjX2QYxevvuh2tzz6X_bLkumrxaAXd9A",
        "document": "Paging is a memory management scheme that eliminates the need for contiguous allocation of physical memory. This means your computer's physical memory can be divided into equal-sized blocks, called \"pages,\" that can be easily managed and swapped. When you're running applications or programs, paging helps your system efficiently use its random-access memory (RAM) by ensuring that only the necessary parts of an application are loaded into memory at any given time, significantly speeding up your computing experience.\n\nHow does paging improve my computer's performance?\n\nPaging can significantly improve your computer's performance by managing how memory is used and accessed. Instead of loading an entire program into RAM, which can be space-consuming and slow, paging only loads portions of the program currently needed. This means you can run multiple applications simultaneously without running out of memory, leading to a smoother and more efficient computing experience.\n\nWhat determines the size of a page in paging?\n\nThe size of a page in paging is determined by your operating system and the hardware of your computer. Most systems use a standard page size, which is typically 4 kilobytes (KB), though this can vary depending on the specific architecture and needs of the system.\n\nDoes every program use paging?\n\nYes, every program on a system that implements paging will use it, albeit indirectly. The operating system handles paging for all programs, so developers and users don't need to worry about the details. Every time a program is executed, the operating system manages its memory using paging, irrespective of the application size or complexity.\n\nWhen is paging most beneficial for my computer?\n\nPaging is most beneficial when you run multiple applications simultaneously or use applications that require more memory than physically available on your system. It allows your computer to handle larger workloads by allocating virtual memory spaces, ensuring that applications can run smoothly without constant crashes or slowdowns due to insufficient memory.\n\nCan I adjust the paging settings on my computer?\n\nYes, you can adjust paging settings, commonly referred to as \"virtual memory\" settings, on your computer. This allows you to allocate a certain amount of disk space to be used as virtual memory, supplementing the physical random-access memory (RAM). Adjusting these settings can help optimize your system's performance, especially if you frequently run memory-intensive applications. However, it's important to use discretion, because setting the virtual memory too high or too low can negatively impact system performance.\n\nWhat's the difference between paging and segmentation?\n\nPaging and segmentation are both memory management techniques, but they work differently. Paging divides the memory into fixed-size blocks, while segmentation divides memory into variable-sized segments based on logical divisions, like modules or functions. Paging is more about managing physical memory efficiently, without regard to the programs' logical structure. Segmentation attempts to align the physical and logical organization of memory, making it easier for programs to be managed and protected.\n\nHow does paging interact with the CPU’s memory management unit?\n\nThe CPU's MMU plays a crucial role in the paging process. It translates logical addresses generated by a program into physical addresses in hardware. When an application accesses a memory address, the MMU uses a page table to map the logical address to the corresponding physical address. If the data is not in memory, and a page fault occurs, the MMU signals the operating system to load the required page from disk into physical memory. This interaction ensures efficient memory access and utilization, significantly influencing system performance.\n\nIs paging used in mobile devices, and how does it affect performance?\n\nYes, paging is used in mobile devices, similar to how it is implemented in desktop and server environments. Mobile operating systems use paging to manage memory efficiently, especially given the limited physical memory resources compared to traditional computers. Paging allows mobile devices to multitask effectively, running multiple applications by swapping pages in and out of memory as needed. However, because mobile devices often use storage with slower access times, heavy reliance on paging can impact performance, leading to lag or slower application responses if the system becomes too dependent on swapping pages to and from slower storage.\n\nCan I see which pages are in memory and which are paged out?\n\nYes, operating systems often provide tools or commands that allow users to see detailed memory usage, including which pages are currently in physical memory and which have been paged out to disk. For example, in Windows, the Task Manager can show you a range of information about the processes and their memory usage, including page faults and the current working set of a process, which indicates the pages in physical memory.\n\nIs paging the same across all operating systems?\n\nWhile the fundamental concept of paging is consistent across different operating systems, the implementation details can vary. Factors such as page size, the algorithm used for page replacement, and the handling of page faults can differ between Unix/Linux, Windows, and other operating systems (OSs). Each operating system has optimized its paging mechanism to align with its kernel's goals and hardware, leading to variations in how paging is managed and utilized.\n\nHow does paging affect the lifespan of SSDs in modern computers?\n\nFrequent paging can impact the lifespan of SSDs due to the write operations required to swap pages between physical memory and disk. SSDs have a limited number of write cycles before the storage cells begin to degrade, and excessive paging involves continuous read-write operations. However, modern SSDs are designed to handle high write volumes, and operating systems often include optimizations to reduce unnecessary writes, mitigating the potential impact of paging on SSD lifespan.\n\nCan paging be disabled on a computer system?\n\nDisabling paging entirely on a computer system is generally not advisable or practical, as modern operating systems are built around virtual memory, of which paging is a crucial component. Attempting to disable paging could lead to system instability, performance issues, or even failure to boot, as the system would not be able to handle memory-intensive tasks or multitask effectively. Operating systems rely on paging to manage memory resources efficiently, ensuring that applications have access to the memory they need, while optimizing the use of physical random-access memory (RAM).\n\nHow does paging relate to Virtual Memory?\n\nPaging is a fundamental technology that enables the implementation of virtual memory in modern computing systems. Virtual memory extends the available memory on a computer by using part of the hard drive as additional random-access memory (RAM), effectively allowing the system to run more applications simultaneously than physical RAM alone. Paging facilitates this by mapping virtual addresses used by a program to physical addresses in the memory, allowing the system to swap pages between RAM and the hard disk as needed. This seamless interaction between physical and virtual memory through paging allows for more efficient use of system resources, enhancing performance."
    }
]