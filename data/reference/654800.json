[
    {
        "link": "https://docs.gitlab.com/ci/variables",
        "document": "CI/CD variables are a type of environment variable. You can use them to:\n• Control the behavior of jobs and pipelines.\n• Store values you want to re-use.\n\nYou can override variable values for a specific pipeline when you run a pipeline manually, run a manual job, or have them prefilled in manual pipelines.\n\nVariable names are limited by the shell the runner uses to execute scripts. Each shell has its own set of reserved variable names.\n\nTo ensure consistent behavior, you should always put variable values in single or double quotes. Variables are internally parsed by the Psych YAML parser, so quoted and unquoted variables might be parsed differently. For example, is interpreted as an octal value, so the value becomes , but is parsed as a string with a value of .\n\nFor more information about advanced use of GitLab CI/CD, see 7 advanced GitLab CI workflow hacks shared by GitLab engineers.\n\nGitLab CI/CD makes a set of predefined CI/CD variables available for use in pipeline configuration and job scripts. These variables contain information about the job, pipeline, and other values you might need when the pipeline is triggered or running.\n\nYou can use predefined CI/CD variables in your without declaring them first. For example:\n\nThe script in this example outputs .\n\nTo create a CI/CD variable in the file, define the variable and value with the keyword.\n\nVariables saved in the file are visible to all users with access to the repository, and should store only non-sensitive project configuration. For example, the URL of a database saved in a variable. Sensitive variables containing values like secrets or keys should be added in the UI.\n\nYou can define in:\n• A job: The variable is only available in that job’s , , or sections, and with some job keywords.\n• The top-level of the file: The variable is available as a default for all jobs in a pipeline, unless a job defines a variable with the same name. The job’s variable takes precedence.\n\nIn both cases, you cannot use these variables with global keywords.\n\nIn this example:\n• outputs: Variables are 'Different value than default', 'Job 2 variable', and ''\n\nUse the and keywords to define variables that are prefilled for manually-triggered pipelines.\n\nIf you don’t want default variables to be available in a job, set to :\n\nSensitive variables like tokens or passwords should be stored in the settings in the UI, not in the file. Add CI/CD variables in the UI:\n• For a project in the project’s settings.\n• For all projects in a group in the group’s setting.\n• For all projects in a GitLab instance in the instance’s settings.\n\nAlternatively, these variables can be added by using the API:\n\nBy default, pipelines from forked projects can’t access the CI/CD variables available to the parent project. If you run a merge request pipeline in the parent project for a merge request from a fork, all variables become available to the pipeline.\n\nYou can add CI/CD variables to a project’s settings.\n• You must be a project member with the Maintainer role.\n\nTo add or update variables in the project settings:\n• On the left sidebar, select Search or go to and find your project.\n• Select Add variable and fill in the details:\n• Key: Must be one line, with no spaces, using only letters, numbers, or .\n• Protect variable Optional. If selected, the variable is only available in pipelines that run on protected branches or protected tags.\n• Visibility: Select Visible (default), Masked, or Masked and hidden (only available for new variables).\n\nAfter you create a variable, you can use it in the pipeline configuration or in job scripts.\n\nYou can make a CI/CD variable available to all projects in a group.\n• You must be a group member with the Owner role.\n• On the left sidebar, select Search or go to and find your group.\n• Select Add variable and fill in the details:\n• Key: Must be one line, with no spaces, using only letters, numbers, or .\n• Protect variable Optional. If selected, the variable is only available in pipelines that run on protected branches or protected tags.\n• Visibility: Select Visible (default), Masked, or Masked and hidden (only available for new variables).\n\nThe group variables that are available in a project are listed in the project’s Settings > CI/CD > Variables section. Variables from subgroups are recursively inherited.\n\nTo set a group CI/CD variable to only be available for certain environments:\n• On the left sidebar, select Search or go to and find your group.\n• To the right of the variable, select Edit ( ).\n• For Environment scope, select All (default) ( ), a specific environment, or a wildcard environment scope.\n\nYou can make a CI/CD variable available to all projects and groups in a GitLab instance.\n• You must have administrator access to the instance.\n• On the left sidebar, at the bottom, select Admin.\n• Select Add variable and fill in the details:\n• Key: Must be one line, with no spaces, using only letters, numbers, or .\n• Value: The value is limited to 10,000 characters, but also bounded by any limits in the runner’s operating system.\n• Protect variable Optional. If selected, the variable is only available in pipelines that run on protected branches or protected tags.\n• Visibility: Select Visible (default), Masked, or Masked and hidden (only available for new variables).\n\nCode pushed to the file could compromise your variables. Variables could be accidentally exposed in a job log, or maliciously sent to a third party server.\n\nReview all merge requests that introduce changes to the file before you:\n• Run a pipeline in the parent project for a merge request submitted from a forked project.\n\nReview the file of imported projects before you add files or run pipelines against them.\n\nThe following example shows malicious code in a file:\n\nTo help reduce the risk of accidentally leaking secrets through scripts like in , all variables containing sensitive information should always be masked in job logs. You can also limit a variable to protected branches and tags only.\n\nAlternatively, use one of the native GitLab integrations to connect with third party secrets manager providers to store and retrieve secrets:\n\nYou can also use OpenID Connect (OIDC) authentication for secrets managers which do not have a native integration.\n\nMalicious scripts like in must be caught during the review process. Reviewers should never trigger a pipeline when they find code like this, because malicious code can compromise both masked and protected variables.\n\nVariable values are encrypted using and stored in the database. This data can only be read and decrypted with a valid secrets file.\n\nYou can mask a project, group, or instance CI/CD variable so the value of the variable does not display in job logs. When a masked CI/CD variable would be displayed in a job log, the value is replaced with to prevent the value from being exposed.\n• You must have the same role or access level as required to add a CI/CD variable in the UI.\n• For the group, project, or in the Admin area, select Settings > CI/CD.\n• Next to the variable you want to protect, select Edit.\n\nThe method used to mask variables limits what can be included in a masked variable. The value of the variable must:\n• Be a single line with no spaces.\n• Not match the name of an existing predefined or custom CI/CD variable.\n• Not include non-alphanumeric characters other than , , , , or .\n\nAdditionally, if variable expansion is enabled, the value can contain only:\n\nMasking a variable automatically masks the value anywhere in a job log. If another variable has the same value, that value is also masked, including when a variable references a masked variable. The string is shown instead of the value, possibly with some trailing characters.\n\nDifferent versions of GitLab Runner have different masking limitations:\n\nIn addition to masking, you can also prevent the value of CI/CD variables from being revealed in the CI/CD settings page. Hiding a variable is only possible when creating a new variable, you cannot update an existing variable to be hidden.\n• You must have the same role or access level as required to add a CI/CD variable in the UI.\n• The variable value must match the requirements for masked variables.\n\nTo hide a variable, select Masked and hidden in the Visibility section when you add a new CI/CD variable in the UI. After you save the variable, the variable can be used in CI/CD pipelines, but cannot be revealed in the UI again.\n\nYou can configure a project, group, or instance CI/CD variable to be available only to pipelines that run on protected branches or protected tags.\n\nMerged results pipelines and merge request pipelines do not have access to these variables.\n• You must have the same role or access level as required to add a CI/CD variable in the UI.\n• For the project or group, go to Settings > CI/CD.\n• Next to the variable you want to protect, select Edit.\n\nThe variable is available for all subsequent pipelines.\n\nAll predefined CI/CD variables and variables defined in the file are “variable” type ( of in the API). Variable type variables:\n• Consist of a key and value pair.\n• Are made available in jobs as environment variables, with:\n• The CI/CD variable key as the environment variable name.\n• The CI/CD variable value as the environment variable value.\n\nProject, group, and instance CI/CD variables are “variable” type by default, but can optionally be set as a “file” type ( of in the API). File type variables:\n• Consist of a key, value, and file.\n• Are made available in jobs as environment variables, with:\n• The CI/CD variable key as the environment variable name.\n• The CI/CD variable value saved to a temporary file.\n• The path to the temporary file as the environment variable value.\n\nUse file type CI/CD variables for tools that need a file as input. The AWS CLI and are both tools that use type variables for configuration.\n\nFor example, if you are using with:\n• A variable with a key of and as the value.\n• A file type variable with a key of and a certificate as the value.\n\nPass as a option, which accepts a variable, and pass as a option, which accepts a path to a file:\n\nYou cannot set a CI/CD variable defined in the file as a file type variable. If you have a tool that requires a file path as an input, but you want to use a variable defined in the :\n• Run a command that saves the value of the variable in a file.\n• Use that file with your tool.\n\nAll CI/CD variables are set as environment variables in the job’s environment. You can use variables in job scripts with the standard formatting for each environment’s shell.\n\nTo access environment variables, use the syntax for your runner executor’s shell.\n\nTo access environment variables in Bash, , and similar shells, prefix the CI/CD variable with ( ):\n\nTo access variables in a Windows PowerShell environment, including environment variables set by the system, prefix the variable name with or :\n\nIn some cases environment variables must be surrounded by quotes to expand properly:\n\nTo access CI/CD variables in Windows Batch, surround the variable with :\n\nYou can also surround the variable with for delayed expansion. Delayed expansion might be needed for variables that contain white spaces or newlines:\n\nService containers can use CI/CD variables, but by default can only access variables saved in the file. Variables added in the GitLab UI are not available to service containers, because service containers are not trusted by default.\n\nTo make a UI-defined variable available in a service container, you can re-assign it to another variable in your :\n\nThe re-assigned variable cannot have the same name as the original variable. Otherwise it does not get expanded.\n\nPass an environment variable to another job\n\nYou can create a new environment variable in a job, and pass it to another job in a later stage. These variables cannot be used as CI/CD variables to configure a pipeline (for example with the keyword), but they can be used in job scripts.\n\nTo pass a job-created environment variable to other jobs:\n• In the job script, save the variable as a file.\n• The format of the file must be one variable definition per line.\n• Each line must be formatted as: .\n• Values can be wrapped in quotes, but cannot contain newline characters.\n• Save the file as an artifact.\n• Jobs in later stages can then use the variable in scripts, unless jobs are configured not to receive variables.\n\nVariables from reports take precedence over certain types of new variable definitions such as job defined variables.\n\nYou can also pass variables to downstream pipelines.\n\nYou can use the or keywords to control which jobs receive the artifacts.\n\nTo have no environment variables from a artifact:\n• Set to only list jobs that do not have a artifact.\n\nPass an environment variable from the section to another section in the same job\n\nUse to pass environment variables defined in the section to another section.\n\nTo also reference the variable in other stages, write the variable to both the and files:\n\nYou cannot create a CI/CD variable that is an array of values, but you can use shell scripting techniques for similar behavior.\n\nFor example, you can store multiple values separated by a space in a variable, then loop through the values with a script:\n\nYou can use variables as part of a string. You can surround the variables with curly brackets ( ) to help distinguish the variable name from the surrounding text. Without curly brackets, the adjacent text is interpreted as part of the variable name. For example:\n\nUse CI/CD variables in other variables\n\nYou can use variables inside other variables:\n\nUse the character in CI/CD variables\n\nIf you do not want the character interpreted as the start of another variable, use instead:\n\nExpanded variables treat values with the character as a reference to another variable. CI/CD variables are expanded by default. To treat variables with a character as raw strings, disable variable expansion for the variable\n• You must have the same role or access level as required to add a CI/CD variable in the UI.\n\nTo disable variable expansion for the variable:\n• For the project or group, go to Settings > CI/CD.\n• Next to the variable you want to do not want expanded, select Edit.\n\nYou can use CI/CD variables with the same name in different places, but the values can overwrite each other. The type of variable and where they are defined determines which variables take precedence.\n\nThe order of precedence for variables is (from highest to lowest):\n• Pipeline variables. These variables all have the same precedence:\n• Variables added when creating a pipeline with the API.\n• Group variables. If the same variable name exists in a group and its subgroups, the job uses the value from the closest subgroup. For example, if you have , the variable defined in takes precedence.\n• Job variables, defined in jobs in the file.\n• Default variables for all jobs, defined at the top-level of the file.\n\nIn this example, outputs because variables defined in jobs in the file have higher precedence than default variables.\n\nPipeline variables are variables that are specified when running a new pipeline.\n• You must have the Developer role in the project.\n\nYou can specify a pipeline variable when you:\n• Create a pipeline by using the API endpoint.\n• Create a pipeline by using the API endpoint.\n• Pass variables to a downstream pipeline by using either the keyword, keyword or variables.\n\nThese variables have higher precedence and can override other defined variables, including predefined variables.\n\nYou can limit who can run pipelines with pipeline variables to specific user roles. When users with a lower role try to use pipeline variables, they receive an error message.\n• You must have the Maintainer role in the project. If the minimum role was previously set to or , then you must have the Owner role in the project.\n\nTo limit the use of pipeline variables to only the Maintainer role and higher:\n• Under Minimum role to use pipeline variables, select one of:\n• : No pipelines can run with pipeline variables. Default for new projects in new namespaces on GitLab.com.\n• : Only users with the Owner role can run pipelines with pipeline variables. You must have the Owner role for the project to change the setting to this value.\n• : Only users with at least the Maintainer role can run pipelines with pipeline variables. Default when not specified on GitLab Self-Managed and GitLab Dedicated.\n• : Only users with at least the Developer role can run pipelines with pipeline variables.\n\nYou can also use the projects API to set the role for the setting.\n\nThis restriction does not affect the use of CI/CD variables from the project or group settings. Most jobs can still use the keyword in the YAML configuration, but not jobs that use the keyword to trigger downstream pipelines. Trigger jobs pass variables to a downstream pipelines as pipeline variables, which is also controlled by this setting.\n\nScripts executed in separate shell contexts do not share exports, aliases, local function definitions, or any other local shell updates.\n\nThis means that if a job fails, variables created by user-defined scripts are not exported.\n• Scripts specified in and the main script are executed together in a single shell context, and are concatenated.\n• Scripts specified in run in a shell context completely separate to the and the specified scripts.\n\nRegardless of the shell the scripts are executed in, the runner output includes:\n• Variables defined in:\n• The file in the section.\n• The file in the section.\n\nThe runner cannot handle manual exports, shell aliases, and functions executed in the body of the script, like .\n\nFor example, in the following file, the following scripts are defined:\n\nWhen the runner executes the job:\n• is executed:\n• Prints the value of .\n• Prints the value of .\n• Prints the value of .\n• is executed in a new, separate shell context:\n• Prints the value of .\n• Prints the value of .\n• Prints an empty value of . The variable value cannot be detected because is in a separate shell context to .\n• None You can configure Auto DevOps to pass CI/CD variables to a running application. To make a CI/CD variable available as an environment variable in the running application’s container, prefix the variable key with .\n• None The Managing the Complex Configuration Data Management Monster Using GitLab video is a walkthrough of the Complex Configuration Data Monorepo working example project. It explains how multiple levels of group CI/CD variables can be combined with environment-scoped project variables for complex configuration of application builds or deployments. The example can be copied to your own group or instance for testing. More details on what other GitLab CI patterns are demonstrated are available at the project page.\n• None You can pass CI/CD variables to downstream pipelines. Use keyword to specify what type of variables to pass to the downstream pipeline.\n\nYou can list all variables available to a script with the command in Bash or in PowerShell. This exposes the values of all available variables, which can be a security risk. Masked variables display as .\n\nFor example, with Bash:\n\nYou can use debug logging to help troubleshoot problems with pipeline configuration or job scripts. Debug logging exposes job execution details that are usually hidden by the runner and makes job logs more verbose. It also exposes all variables and secrets available to the job.\n\nBefore you enable debug logging, make sure only team members can view job logs. You should also delete job logs with debug output before you make logs public again.\n\nTo enable debug logging, set the variable to :\n\nAccess to debug logging is restricted to users with at least the Developer role. Users with a lower role cannot see the logs when debug logging is enabled with a variable in:\n• The CI/CD variables set in the GitLab UI.\n\nThis issue occurs when the combined length of all CI/CD variables defined for a job exceeds the limit imposed by the shell where the job executes. This includes the names and values of pre-defined and user defined variables. This limit is typically referred to as , and is shell and operating system dependent. This issue also occurs when the content of a single File-type variable exceeds .\n\nFor more information, see issue 392406.\n\nAs a workaround you can either:\n• Use File-type CI/CD variables for large environment variables where possible.\n• If a single large variable is larger than , try using Secure Files, or bring the file to the job through some other mechanism.\n\nDefault variable doesn’t expand in job variable of the same name\n\nYou cannot use a default variable’s value in a job variable of the same name. A default variable is only made available to a job when the job does not have a variable defined with the same name. If the job has a variable with the same name, the job’s variable takes precedence and the default variable is not available in the job.\n\nFor example, these two samples are equivalent:\n• None In this sample, has no value because it’s not defined anywhere:\n• None In this sample, has no value because the default variable with the same name is not available in the job:\n\nIn both cases, the echo command outputs .\n\nIn general, you should use the default variable directly in a job rather than reassigning its value to a new variable. If you need to do this, use variables with different names instead. For example:"
    },
    {
        "link": "https://docs.gitlab.com/ci/variables/predefined_variables",
        "document": "Predefined CI/CD variables are available in every GitLab CI/CD pipeline.\n\nAvoid overriding predefined variables, as it can cause the pipeline to behave unexpectedly.\n\nPredefined variables become available at three different phases of pipeline execution:\n• Pre-pipeline: Pre-pipeline variables are available before the pipeline is created. These variables are the only variables that can be used with to control which configuration files to use when creating the pipeline.\n• Pipeline: Pipeline variables become available when GitLab is creating the pipeline. Along with pre-pipeline variables, pipeline variables can be used to configure defined in jobs, to determine which jobs to add to the pipeline.\n• Job-only: These variables are only made available to each job when a runner picks up the job and runs it, and:\n• Can be used in job scripts.\n• Cannot be used with trigger jobs.\n• Cannot be used with , or .\n\nThese variables are available before GitLab creates the pipeline (Pre-pipeline). These variables can be used with and as environment variables in jobs.\n\nThe pipeline must be a merge request pipeline, and the merge request must be open.\n\nThese variables are only available when:\n\nIntegrations that are responsible for deployment configuration can define their own predefined variables that are set in the build environment. These variables are only defined for deployment jobs.\n\nFor example, the Kubernetes integration defines deployment variables that you can use with the integration.\n\nThe documentation for each integration explains if the integration has any deployment variables available.\n\nWhen Auto DevOps is enabled, some additional pre-pipeline variables are made available:\n• : Has a value of to indicate Auto DevOps is enabled.\n\nSome integrations make variables available in jobs. These variables are available as job-only predefined variables:\n\nYou can output the values of all variables available for a job with a command."
    },
    {
        "link": "https://gitlab.com/gitlab-org/gitlab/-/blob/v13.1.3-ee/doc/ci/variables/README.md",
        "document": ""
    },
    {
        "link": "https://forum.gitlab.com/t/usage-of-ci-pipeline-id-variables-in-environment-name/78668",
        "document": "I have this configuration in :\n• … that the variable is actually set in the job ( )\n\nI would now expect to see an environment called “ ”. but gitlab creates an environment with the name “ ”.\n\nI already found this topic which seems to confirm that i can use gitlab CI variables in the environment names … but it seems this one is missing.\n\ncan someone confirm that or tell me what i’m doing wrong?\n• why do i want this? - I want a temporary environment for this pipeline run.\n• do i insist on that? - nope, i just don’t know what else to do …"
    },
    {
        "link": "https://pradappandiyan.medium.com/understanding-gitlab-ci-cd-variables-for-pip-c0ef42eef37c",
        "document": "GitLab CI/CD uses various types of variables to make pipelines dynamic, adaptable, and secure. Variables can store values such as paths, URLs, or secret credentials. You can create more flexible and reusable pipelines by utilizing predefined, global, and local variables. In this article, we will explore different types of GitLab CI/CD variables, how they are defined, and their use cases in a typical pipeline.\n\nGlobal variables are defined at the top level of the file and are accessible throughout the entire pipeline, across all stages and jobs. These variables can store common values that need to be used in multiple jobs, ensuring consistency and reducing redundancy.\n• GLOBAL_VARIABLE: This variable is available across all stages and jobs.\n• COMMON_URL: A URL that could be used across different jobs, for instance, for making API requests.\n\nGitLab CI provides a rich set of predefined variables that give information about the pipeline, project, or job. These variables are automatically set and can be accessed directly in the pipeline.\n• CI_COMMIT_SHA: The commit hash of the current commit.\n• CI_JOB_ID: The unique ID of the current job.\n• CI_PROJECT_NAME: The name of the project.\n\nPredefined variables are useful for retrieving metadata about the pipeline without manually defining them. They can be used in scripts, logging, or reporting.\n\nWhen a pipeline runs in response to a merge request (MR), special variables are available to give information about the MR itself. These variables allow access to MR-specific data, such as the branch names, project IDs, and labels.\n• CI_MERGE_REQUEST_ID: The ID of the merge request.\n• CI_MERGE_REQUEST_TITLE: The title of the merge request.\n\nThese variables can be handy when you want to customize your pipeline behavior based on the context of the merge request.\n\nLocal variables are defined at the job level and are only available within that specific job. This allows for customizing job behavior without affecting the entire pipeline.\n\nIn this example:\n• LOCAL_VARIABLE is defined only for the and is not accessible in other jobs.\n• GLOBAL_VARIABLE, however, is accessible because it was defined at the global level.\n\nYou can define variables that are specific to certain environments, such as deployment environments. This is useful when you want to tailor your job for a specific environment, for example, using different URLs for staging and production.\n\nHere, DEPLOY_URL is specific to the and can contain deployment-related URLs or credentials.\n\nGitLab CI/CD variables have a clear precedence order, which dictates which variable value will be used if there are conflicts. From highest to lowest priority:\n• Global variables: Variables defined at the top level in .\n• Environment variables: Variables defined in GitLab project settings or in the group.\n\nHere is the data which I have tested with the gitlab job to validate most of the variables\n\nUsing different types of GitLab CI variables enables you to create more dynamic, reusable, and flexible pipelines. By combining global variables for consistency, job-specific variables for custom behavior, and predefined or merge request variables for environment-specific tasks, you can ensure that your pipeline runs smoothly across different scenarios.\n\nUnderstanding the various types of variables and how to use them effectively is key to mastering GitLab CI/CD and creating scalable, efficient pipelines.\n\nI have created a project on Gitlab and added the code here.\n\nFeel free to hit clap if you like the content. Happy Automation Testing :) Cheers. 👏"
    },
    {
        "link": "https://docs.gitlab.com/ci/yaml",
        "document": "This document lists the configuration options for the GitLab file. This file is where you define the CI/CD jobs that make up your pipeline.\n• If you are already familiar with basic CI/CD concepts, try creating your own file by following a tutorial that demonstrates a simple or complex pipeline.\n• For a collection of examples, see GitLab CI/CD examples.\n• To view a large file used in an enterprise, see the file for .\n\nWhen you are editing your file, you can validate it with the CI Lint tool.\n\nIf you are editing content on this page, follow the instructions for documenting keywords.\n• None The names and order of the pipeline stages.\n• None Override a set of commands that are executed after job. Allow job to fail. A failed job does not cause the pipeline to fail. List of files and directories to attach to a job on success. Override a set of commands that are executed before job. List of files that should be cached between subsequent runs. Use configuration from DAST profiles on a job level. Restrict which artifacts are passed to a specific job by providing a list of jobs to fetch artifacts from. Name of an environment to which the job deploys. Configuration entries that this job inherits from. Authenticate with third party services using identity federation. Defines if a job can be canceled when made redundant by a newer run. Upload the result of a job to use with GitLab Pages. How many instances of a job should be run in parallel. When and how many times a job can be auto-retried in case of a failure. List of conditions to evaluate and determine selected attributes of a job, and whether or not it’s created. Shell script that is executed by a runner. Run configuration that is executed by a runner. The CI/CD secrets the job needs. List of tags that are used to select a runner. Define a custom job-level timeout that takes precedence over the project-wide setting.\n• None Define default CI/CD variables for all jobs in the pipeline.\n\nSome keywords are not defined in a job. These keywords control pipeline behavior or import additional pipeline configuration.\n\nYou can set global defaults for some keywords. Each default keyword is copied to every job that doesn’t already have it defined. If the job already has a keyword defined, that default is not used.\n\nSupported values: These keywords can have custom defaults:\n• , though due to issue 213634 this keyword has no effect.\n\nIn this example:\n• and are the default keywords for all jobs in the pipeline.\n• The job does not have or defined, so it uses the defaults of and .\n• The job does not have defined, but it does have explicitly defined. It uses the default , but ignores the default and uses the defined in the job.\n• Control inheritance of default keywords in jobs with .\n• Global defaults are not passed to downstream pipelines, which run independently of the upstream pipeline that triggered the downstream pipeline.\n\nUse to include external YAML files in your CI/CD configuration. You can split one long file into multiple files to increase readability, or reduce duplication of the same configuration in multiple places.\n\nYou can also store template files in a central repository and include them in projects.\n• Merged with those in the file.\n• Always evaluated first and then merged with the content of the file, regardless of the position of the keyword.\n\nThe time limit to resolve all files is 30 seconds.\n• Only certain CI/CD variables can be used with keywords.\n• Use merging to customize and override included CI/CD configurations with local\n• You can override included configuration by having the same job name or global keyword in the file. The two configurations are merged together, and the configuration in the file takes precedence over the included configuration.\n• If you rerun a:\n• Job, the files are not fetched again. All jobs in a pipeline use the configuration fetched when the pipeline was created. Any changes to the source files do not affect job reruns.\n• Pipeline, the files are fetched again. If they changed after the last pipeline run, the new pipeline uses the changed configuration.\n• You can have up to 150 includes per pipeline by default, including nested. Additionally:\n• In GitLab 16.0 and later users on GitLab Self-Managed can change the maximum includes value.\n• In GitLab 15.10 and later you can have up to 150 includes. In nested includes, the same file can be included multiple times, but duplicated includes count towards the limit.\n• From GitLab 14.9 to GitLab 15.9, you can have up to 100 includes. The same file can be included multiple times in nested includes, but duplicates are ignored.\n\nUse to add a CI/CD component to the pipeline configuration.\n\nSupported values: The full address of the CI/CD component, formatted as .\n\nUse to include a file that is in the same repository and branch as the configuration file containing the keyword. Use instead of symbolic links.\n• The YAML file must have the extension or .\n• You can use and wildcards in the file path.\n• You can use certain CI/CD variables.\n\nYou can also use shorter syntax to define the path:\n• The file and the local file must be on the same branch.\n• configuration is always evaluated based on the location of the file containing the keyword, not the project running the pipeline. If a nested is in a configuration file in a different project, checks that other project for the file.\n\nTo include files from another private project on the same GitLab instance, use and .\n• A full file path, or array of file paths, relative to the root directory ( ). The YAML files must have the or extension.\n• : Optional. The ref to retrieve the file from. Defaults to the of the project when not specified.\n• You can use certain CI/CD variables.\n\nYou can also specify a :\n• configuration is always evaluated based on the location of the file containing the keyword, not the project running the pipeline. If a nested is in a configuration file in a different project, checks that other project for the file.\n• When the pipeline starts, the file configuration included by all methods is evaluated. The configuration is a snapshot in time and persists in the database. GitLab does not reflect any changes to the referenced file configuration until the next pipeline starts.\n• When you include a YAML file from another private project, the user running the pipeline must be a member of both projects and have the appropriate permissions to run pipelines. A error may be displayed if the user does not have access to any of the included files.\n• Be careful when including another project’s CI/CD configuration file. No pipelines or notifications trigger when CI/CD configuration files change. From a security perspective, this is similar to pulling a third-party dependency. For the , consider:\n• Using a specific SHA hash, which should be the most stable option. Use the full 40-character SHA hash to ensure the desired commit is referenced, because using a short SHA hash for the might be ambiguous.\n• Applying both protected branch and protected tag rules to the in the other project. Protected tags and branches are more likely to pass through change management before changing.\n\nUse with a full URL to include a file from a different location.\n• Authentication with the remote URL is not supported.\n• The YAML file must have the extension or .\n• You can use certain CI/CD variables.\n• All nested includes are executed without context as a public user, so you can only include public projects or templates. No variables are available in the section of nested includes.\n• Be careful when including another project’s CI/CD configuration file. No pipelines or notifications trigger when the other project’s files change. From a security perspective, this is similar to pulling a third-party dependency. To verify the integrity of the included file, consider using the keyword. If you link to another GitLab project you own, consider the use of both protected branches and protected tags to enforce change management rules.\n• All templates can be viewed in . Not all templates are designed to be used with , so check template comments before using one.\n• You can use certain CI/CD variables.\n• All nested includes are executed without context as a public user, so you can only include public projects or templates. No variables are available in the section of nested includes.\n\nUse to set the values for input parameters when the included configuration uses and is added to the pipeline.\n\nIn this example:\n• The configuration contained in is added to the pipeline, with a input set to a value of for the included configuration.\n• If the included configuration file uses , the input value must match the defined type.\n• If the included configuration file uses , the input value must match one of the listed options.\n\nYou can use with to conditionally include other configuration files.\n\nIn this example, if the variable is:\n• , the configuration is included in the pipeline.\n• Not or does not exist, the configuration is not included in the pipeline.\n• Examples of using with:\n\nUse with to specifiy a SHA256 hash of the included remote file. If does not match the actual content, the remote file is not processed and the pipeline fails.\n\nUse to define stages that contain groups of jobs. Use in a job to configure the job to run in a specific stage.\n\nIf is not defined in the file, the default pipeline stages are:\n\nThe order of the items in defines the execution order for jobs:\n• Jobs in the same stage run in parallel.\n• Jobs in the next stage run after the jobs from the previous stage complete successfully.\n\nIf a pipeline contains only jobs in the or stages, it does not run. There must be at least one other job in a different stage.\n\nIn this example:\n• All jobs in execute in parallel.\n• If all jobs in succeed, the jobs execute in parallel.\n• If all jobs in succeed, the jobs execute in parallel.\n• If all jobs in succeed, the pipeline is marked as .\n\nIf any job fails, the pipeline is marked as and jobs in later stages do not start. Jobs in the current stage are not stopped and continue to run.\n• If a job does not specify a , the job is assigned the stage.\n• If a stage is defined but no jobs use it, the stage is not visible in the pipeline, which can help compliance pipeline configurations:\n• Stages can be defined in the compliance configuration but remain hidden if not used.\n• The defined stages become visible when developers use them in job definitions.\n• To make a job start earlier and ignore the stage order, use the keyword.\n\nYou can use some predefined CI/CD variables in configuration, but not variables that are only defined when jobs start.\n\nUse to configure the behavior of the auto-cancel redundant pipelines feature.\n• : Cancel the pipeline, but only if no jobs with have started yet. Default when not defined.\n• : Do not auto-cancel any jobs.\n\nIn this example:\n• When a new commit is pushed to a branch, GitLab creates a new pipeline and and start.\n• If a new commit is pushed to the branch before the jobs complete, only is canceled.\n\nUse to configure which jobs should be canceled as soon as one job fails.\n• : Cancel the pipeline and all running jobs as soon as one job fails.\n• : Do not auto-cancel any jobs.\n\nIn this example, if fails, is canceled if it is still running and does not start.\n\nYou can use in to define a name for pipelines.\n\nAll pipelines are assigned the defined name. Any leading or trailing spaces in the name are removed.\n\nA configuration with different pipeline names depending on the pipeline conditions:\n• If the name is an empty string, the pipeline is not assigned a name. A name consisting of only CI/CD variables could evaluate to an empty string if all the variables are also empty.\n• become default variables available in all jobs, including jobs which forward variables to downstream pipelines by default. If the downstream pipeline uses the same variable, the variable is overwritten by the upstream variable value. Be sure to either:\n• Use a unique variable name in every project’s pipeline configuration, like .\n• Use in the trigger job and list the exact variables you want to forward to the downstream pipeline.\n\nThe keyword in is similar to defined in jobs, but controls whether or not a whole pipeline is created.\n\nWhen no rules evaluate to true, the pipeline does not run.\n\nSupported values: You can use some of the same keywords as job-level :\n• , can only be or when used with .\n\nIn this example, pipelines run if the commit title (first line of the commit message) does not end with and the pipeline is for either:\n• If your rules match both branch pipelines (other than the default branch) and merge request pipelines, duplicate pipelines can occur.\n• , , and are not supported in , but do not cause a syntax violation. Though they have no effect, do not use them in as it could cause syntax failures in the future. See issue 436473 for more details.\n\nYou can use in to define variables for specific pipeline conditions.\n\nWhen the condition matches, the variable is created and can be used by all jobs in the pipeline. If the variable is already defined at the top level as a default variable, the variable takes precedence and overrides the default variable.\n\nSupported values: Variable name and value pairs:\n• The name can use only numbers, letters, and underscores ( ).\n• The value must be a string.\n\nWhen the branch is the default branch:\n\nWhen the branch is :\n• job1’s is , and is .\n• job2’s is , and is .\n\nWhen the branch is something else:\n• become default variables available in all jobs, including jobs which forward variables to downstream pipelines by default. If the downstream pipeline uses the same variable, the variable is overwritten by the upstream variable value. Be sure to either:\n• Use unique variable names in every project’s pipeline configuration, like .\n• Use in the trigger job and list the exact variables you want to forward to the downstream pipeline.\n\nUse to configure the behavior of the or the features.\n\nIn this example, is set to and is set to for all jobs by default. But if a pipeline runs for a protected branch, the rule overrides the default with and . For example, if a pipeline is running for:\n• A non-protected branch and a new commit is pushed, continues to run and is canceled.\n• A protected branch and a new commit is pushed, both and continue to run.\n\nSome keywords must be defined in a header section of a YAML configuration file. The header must be at the top of the file, separated from the rest of the configuration with .\n\nAdd a section to the header of a YAML file to configure the behavior of a pipeline when a configuration is added to the pipeline with the keyword.\n\nYou can use to define input parameters for the CI/CD configuration you intend to add to a pipeline with . Use to define the values to use when the pipeline runs.\n\nUse the inputs to customize the behavior of the configuration when included in CI/CD configuration.\n\nUse the interpolation format to reference the values outside of the header section. Inputs are evaluated and interpolated when the configuration is fetched during pipeline creation, but before the configuration is merged with the contents of the file.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n• Inputs are mandatory unless you use to set a default value.\n• Inputs expect strings unless you use to set a different input type.\n• A string containing an interpolation block must not exceed 1 MB.\n• The string inside an interpolation block must not exceed 1 KB.\n\nInputs are mandatory when included, unless you set a default value with .\n\nUse to have no default value.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nSupported values: A string representing the default value, or .\n\nIn this example:\n• is mandatory and must be defined.\n• is optional. If not defined, the value is .\n• is optional. If not defined, it has no value.\n• The pipeline fails with a validation error when the input:\n• Uses both and , but the default value is not one of the listed options.\n• Uses both and , but the default value does not match the regular expression.\n• Value does not match the .\n\nUse to give a description to a specific input. The description does not affect the behavior of the input and is only used to help users of the file understand the input.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nInputs can use to specify a list of allowed values for an input. The limit is 50 options per input.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nIn this example:\n• is mandatory and must be defined with one of the values in the list.\n• The pipeline fails with a validation error when:\n• The input uses both and , but the default value is not one of the listed options.\n• Any of the input options do not match the , which can be either or , but not when using .\n\nUse to specify a regular expression that the input must match.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nIn this example, inputs of or match the regular expression and pass validation. An input of does not match the regular expression and fails validation.\n• can only be used with a of , not or .\n• Do not enclose the regular expression with the character. For example, use , not .\n\nBy default, inputs expect strings. Use to set a different required type for inputs.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nSupported values: Can be one of:\n• , to accept an array of inputs.\n• , to accept string inputs (default when not defined).\n• , to only accept or inputs.\n\nThe following topics explain how to use keywords to configure CI/CD pipelines.\n\nUse to define an array of commands to run last, after a job’s and sections complete. commands also run when:\n• The job is canceled while the or sections are still running.\n• The job fails with failure type of , but not other failure types.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nScripts you specify in execute in a new shell, separate from any or commands. As a result, they:\n• Have the current working directory set back to the default (according to the variables which define how the runner processes Git requests).\n• Don’t have access to changes done by commands defined in the or , including:\n• Changes outside of the working tree (depending on the runner executor), like software installed by a or script.\n• Have a separate timeout. For GitLab Runner 16.4 and later, this defaults to 5 minutes, and can be configured with the variable. In GitLab 16.3 and earlier, the timeout is hard-coded to 5 minutes.\n• Don’t affect the job’s exit code. If the section succeeds and the times out or fails, the job exits with code ( ).\n• There is a known issue with using CI/CD job tokens with . You can use a job token for authentication in commands, but the token immediately becomes invalid if the job is canceled. See issue for more details.\n\nFor jobs that time out:\n• commands do not execute by default.\n• You can configure timeout values to ensure runs by setting appropriate and values that don’t exceed the job’s timeout.\n• Use with to define a default array of commands that should run after all jobs.\n• You can configure a job to skip commands if the job is canceled.\n• Use color codes with to make job logs easier to review.\n• You can ignore errors in .\n\nUse to determine whether a pipeline should continue running when a job fails.\n• To let the pipeline continue running subsequent jobs, use .\n• To stop the pipeline from running subsequent jobs, use .\n\nWhen jobs are allowed to fail ( ) an orange warning ( ) indicates that a job failed. However, the pipeline is successful and the associated commit is marked as passed with no warnings.\n\nThis same warning is displayed when:\n• All other jobs in the stage are successful.\n• All other jobs in the pipeline are successful.\n\nThe default value for is:\n• for jobs that use inside .\n• in all other cases.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nIn this example, and run in parallel:\n• If fails, jobs in the stage do not start.\n• If fails, jobs in the stage can still start.\n• You can use as a subkey of .\n• If is set, the job is always considered successful, and later jobs with don’t start if this job fails.\n• You can use with a manual job to create a blocking manual job. A blocked pipeline does not run any jobs in later stages until the manual job is started and completes successfully.\n\nUse to control when a job should be allowed to fail. The job is for any of the listed exit codes, and false for any other exit code.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nUse to specify which files to save as job artifacts. Job artifacts are a list of files and directories that are attached to the job when it succeeds, fails, or always.\n\nThe artifacts are sent to GitLab after the job finishes. They are available for download in the GitLab UI if the size is smaller than the maximum artifact size.\n\nBy default, jobs in later stages automatically download all the artifacts created by jobs in earlier stages. You can control artifact download behavior in jobs with .\n\nWhen using the keyword, jobs can only download artifacts from the jobs defined in the configuration.\n\nJob artifacts are only collected for successful jobs by default, and artifacts are restored after caches.\n\nPaths are relative to the project directory ( ) and can’t directly link outside it.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of file paths, relative to the project directory.\n• You can use Wildcards that use glob patterns and:\n• In GitLab Runner 13.0 and later, .\n• For GitLab Pages job:\n• In GitLab 17.10 and later, the path is automatically appended to , so you don’t need to specify it again.\n• In GitLab 17.10 and later, when the path is not specified, the directory is automatically appended to .\n\nThis example creates an artifact with and all the files in the directory.\n• If not used with , the artifacts file is named , which becomes when downloaded.\n• To restrict which jobs a specific job fetches artifacts from, see .\n\nUse to prevent files from being added to an artifacts archive.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of file paths, relative to the project directory.\n• You can use Wildcards that use glob or patterns.\n\nThis example stores all files in , but not files located in subdirectories of .\n• Files matched by can be excluded using too.\n\nUse to specify how long job artifacts are stored before they expire and are deleted. The setting does not affect:\n• Artifacts from the latest job, unless keeping the latest job artifacts is disabled at the project level or instance-wide.\n\nAfter their expiry, artifacts are deleted hourly by default (using a cron job), and are not accessible anymore.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: The expiry time. If no unit is provided, the time is in seconds. Valid values include:\n• The expiration time period begins when the artifact is uploaded and stored on GitLab. If the expiry time is not defined, it defaults to the instance wide setting.\n• To override the expiration date and protect artifacts from being automatically deleted:\n• Select Keep on the job page.\n• Set the value of to .\n• If the expiry time is too short, jobs in later stages of a long pipeline might try to fetch expired artifacts from earlier jobs. If the artifacts are expired, jobs that try to fetch them fail with a could not retrieve the needed artifacts error. Set the expiry time to be longer, or use in later jobs to ensure they don’t try to fetch expired artifacts.\n\nUse the keyword to expose job artifacts in the merge request UI.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• The name to display in the merge request UI for the artifacts download link. Must be combined with .\n• Artifacts are saved, but do not display in the UI if the values:\n• Define a directory, but do not end with . For example, works with , but does not.\n• Start with . For example, works with , but does not.\n• A maximum of 10 job artifacts per merge request can be exposed.\n• If a directory is specified and there is more than one file in the directory, the link is to the job artifacts browser.\n• If GitLab Pages is enabled, GitLab automatically renders the artifacts when the artifacts is a single file with one of these extensions:\n\nUse the keyword to define the name of the created artifacts archive. You can specify a unique name for every archive.\n\nIf not defined, the default name is , which becomes when downloaded.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• The name of the artifacts archive. CI/CD variables are supported. Must be combined with .\n\nTo create an archive with a name of the current job:\n• Use CI/CD variables to define the artifacts configuration\n\nUse to determine whether the job artifacts should be publicly available.\n\nWhen is (default), the artifacts in public pipelines are available for download by anonymous, guest, and reporter users.\n\nTo deny read access to artifacts in public pipelines for anonymous, guest, and reporter users, set to :\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• (default if not defined) or .\n\nUse to determine who can access the job artifacts from the GitLab UI or API. This option does not prevent you from forwarding artifacts to downstream pipelines.\n\nYou cannot use and in the same job.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• (default): Artifacts in a job in public pipelines are available for download by anyone, including anonymous, guest, and reporter users.\n• : Artifacts in the job are only available for download by users with the Developer role or higher.\n• : Artifacts in the job are not available for download by anyone.\n• affects all too, so you can also restrict access to artifacts for reports.\n\nUse to collect artifacts generated by included templates in jobs.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• See list of available artifacts reports types.\n• Combining reports in parent pipelines using artifacts from child pipelines is not supported. Track progress on adding support in this issue.\n• To be able to browse and download the report output files, include the keyword. This uploads and stores the artifact twice.\n• Artifacts created for are always uploaded, regardless of the job results (success or failure). You can use to set an expiration date for the artifacts.\n\nUse to add all Git untracked files as artifacts (along with the paths defined in ). ignores configuration in the repository’s , so matching artifacts in are included.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• or (default if not defined).\n\nUse to upload artifacts on job failure or despite the failure.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• (default): Upload artifacts only when the job succeeds.\n• : Upload artifacts only when the job fails.\n• : Always upload artifacts (except when jobs time out). For example, when uploading artifacts required to troubleshoot failing tests.\n• The artifacts created for are always uploaded, regardless of the job results (success or failure). does not change this behavior.\n\nUse to define an array of commands that should run before each job’s commands, but after artifacts are restored.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• Scripts you specify in are concatenated with any scripts you specify in the main . The combined scripts execute together in a single shell.\n• Using at the top level, but not in the section, is deprecated.\n• Use with to define a default array of commands that should run before the commands in all jobs.\n• Use color codes with to make job logs easier to review.\n\nUse to specify a list of files and directories to cache between jobs. You can only use paths that are in the local working copy.\n• By default, not shared between protected and unprotected branches.\n• Limited to a maximum of four different caches.\n\nYou can disable caching for specific jobs, for example to override:\n• The configuration for a job added with .\n\nFor more information about caches, see Caching in GitLab CI/CD.\n\nUse the keyword to choose which files or directories to cache.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of paths relative to the project directory ( ). You can use wildcards that use glob patterns:\n• In GitLab Runner 13.0 and later, .\n\nCache all files in that end in and the file:\n• The keyword includes files even if they are untracked or in your file.\n• See the common use cases for more examples.\n\nUse the keyword to give each cache a unique identifying key. All jobs that use the same cache key use the same cache, including in different pipelines.\n\nIf not set, the default key is . All jobs with the keyword but no share the cache.\n\nMust be used with , or nothing is cached.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• None If you use Windows Batch to run your shell scripts you must replace with . For example:\n• None The value can’t contain:\n• The character, or the equivalent URI-encoded .\n• Only the character (any number), or the equivalent URI-encoded .\n• None The cache is shared between jobs, so if you’re using different paths for different jobs, you should also set a different . Otherwise cache content can be overwritten.\n• You can specify a fallback cache key to use if the specified is not found.\n• You can use multiple cache keys in a single job.\n• See the common use cases for more examples.\n\nUse the keyword to generate a new key when one or two specific files change. lets you reuse some caches, and rebuild them less often, which speeds up subsequent pipeline runs.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of one or two file paths.\n\nThis example creates a cache for Ruby and Node.js dependencies. The cache is tied to the current versions of the and files. When one of these files changes, a new cache key is computed and a new cache is created. Any future job runs that use the same and with use the new cache, instead of rebuilding the dependencies.\n• The cache is a SHA computed from the most recent commits that changed each listed file. If neither file is changed in any commits, the fallback key is .\n\nUse to combine a prefix with the SHA computed for .\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nFor example, adding a of causes the key to look like . If a branch changes , that branch has a new SHA checksum for . A new cache key is generated, and a new cache is created for that key. If is not found, the prefix is added to , so the key in the example would be .\n• If no file in is changed in any commits, the prefix is added to the key.\n\nUse to cache all files that are untracked in your Git repository. Untracked files include files that are:\n• Created, but not added to the checkout with .\n\nCaching untracked files can create unexpectedly large caches if the job downloads:\n• Dependencies, like gems or node modules, which are usually untracked.\n• Artifacts from a different job. Files extracted from the artifacts are untracked by default.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• None You can combine with to cache all untracked files, as well as files in the configured paths. Use to cache any specific files, including tracked files, or files that are outside of the working directory, and use to also cache all untracked files. For example: In this example, the job caches all untracked files in the repository, as well as all the files in . If there are untracked files in , they are covered by both keywords.\n\nUse to set a cache to be shared between protected and unprotected branches.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nUse to define when to save the cache, based on the status of the job.\n\nMust be used with , or nothing is cached.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• (default): Save the cache only when the job succeeds.\n• : Save the cache only when the job fails.\n\nThis example stores the cache whether or not the job fails or succeeds.\n\nTo change the upload and download behavior of a cache, use the keyword. By default, the job downloads the cache when the job starts, and uploads changes to the cache when the job ends. This caching style is the policy (default).\n\nTo set a job to only download the cache when the job starts, but never upload changes when the job finishes, use .\n\nTo set a job to only upload a cache when the job finishes, but never download the cache when the job starts, use .\n\nUse the policy when you have many jobs executing in parallel that use the same cache. This policy speeds up job execution and reduces load on the cache server. You can use a job with the policy to build the cache.\n\nMust be used with , or nothing is cached.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• You can use a variable to control a job’s cache policy.\n\nUse to specify a list of keys to try to restore cache from if there is no cache found for the . Caches are retrieved in the order specified in the section.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nUse with a custom regular expression to configure how code coverage is extracted from the job output. The coverage is shown in the UI if at least one line in the job output matches the regular expression.\n\nTo extract the code coverage value from the match, GitLab uses this smaller regular expression: .\n• An RE2 regular expression. Must start and end with . Must match the coverage number. May match surrounding text as well, so you don’t need to use a regular expression character group to capture the exact number. Because it uses RE2 syntax, all groups must be non-capturing.\n\nIn this example:\n• GitLab checks the job log for a match with the regular expression. A line like would match.\n• GitLab then checks the matched fragment to find a match to . The sample matching line above gives a code coverage of .\n• You can find regex examples in Code Coverage.\n• If there is more than one matched line in the job output, the last line is used (the first result of reverse search).\n• If there are multiple matches in a single line, the last match is searched for the coverage number.\n• If there are multiple coverage numbers found in the matched fragment, the first number is used.\n• Coverage output from child pipelines is not recorded or displayed. Check the related issue for more details.\n\nUse the keyword to specify a site profile and scanner profile to be used in a CI/CD configuration. Both profiles must first have been created in the project. The job’s stage must be .\n\nKeyword type: Job keyword. You can use only as part of a job.\n\nSupported values: One each of and .\n• Use to specify the site profile to be used in the job.\n• Use to specify the scanner profile to be used in the job.\n\nIn this example, the job extends the configuration added with the keyword to select a specific site profile and scanner profile.\n• Settings contained in either a site profile or scanner profile take precedence over those contained in the DAST template.\n\nUse the keyword to define a list of specific jobs to fetch artifacts from. The specified jobs must all be in earlier stages. You can also set a job to download no artifacts at all.\n\nWhen is not defined in a job, all jobs in earlier stages are considered dependent and the job fetches all artifacts from those jobs.\n\nTo fetch artifacts from a job in the same stage, you must use . You should not combine with in the same job.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The names of jobs to fetch artifacts from.\n• An empty array ( ), to configure the job to not download any artifacts.\n\nIn this example, two jobs have artifacts: and . When is executed, the artifacts from are downloaded and extracted in the context of the build. The same thing happens for and artifacts from .\n\nThe job downloads artifacts from all previous jobs because of the stage precedence.\n• The job status does not matter. If a job fails or it’s a manual job that isn’t triggered, no error occurs.\n• If the artifacts of a dependent job are expired or deleted, then the job fails.\n\nUse to define the environment that a job deploys to.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: The name of the environment the job deploys to, in one of these formats:\n• CI/CD variables, including predefined, project, group, instance, or variables defined in the file. You can’t use variables defined in a section.\n• If you specify an and no environment with that name exists, an environment is created.\n\nSet a name for an environment.\n\nCommon environment names are , , and , but you can use any name.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: The name of the environment the job deploys to, in one of these formats:\n• CI/CD variables, including predefined, project, group, instance, or variables defined in the file. You can’t use variables defined in a section.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: A single URL, in one of these formats:\n• CI/CD variables, including predefined, project, group, instance, or variables defined in the file. You can’t use variables defined in a section.\n• After the job completes, you can access the URL by selecting a button in the merge request, environment, or deployment pages.\n\nClosing (stopping) environments can be achieved with the keyword defined under . It declares a different job that runs to close the environment.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• See for more details and an example.\n\nUse the keyword to specify how the job interacts with the environment.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: One of the following keywords:\n\nThe keyword specifies the lifetime of the environment. When an environment expires, GitLab automatically stops it.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: A period of time written in natural language. For example, these are all equivalent:\n\nWhen the environment for is created, the environment’s lifetime is set to . Every time the review app is deployed, that lifetime is also reset to .\n\nThe keyword can be used for all environment actions except . Some actions can be used to reset the scheduled stop time for the environment. For more information, see Access an environment for preparation or verification purposes.\n\nUse the keyword to configure the dashboard for Kubernetes for an environment.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : A string specifying the GitLab agent for Kubernetes. The format is .\n• : A string representing the Kubernetes namespace. It needs to be set together with the keyword.\n• : A string representing the path to the Flux resource. This must be the full resource path. It needs to be set together with the and keywords.\n\nThis configuration sets up the job to deploy to the environment, associates the agent named with the environment, and configures the dashboard for Kubernetes for an environment with the namespace and the set to .\n• To use the dashboard, you must install the GitLab agent for Kubernetes and configure for the environment’s project or its parent group.\n• The user running the job must be authorized to access the cluster agent. Otherwise, it will ignore , and attributes.\n\nUse the keyword to specify the tier of the deployment environment.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: One of the following:\n• Environments created from this job definition are assigned a tier based on this value.\n• Existing environments don’t have their tier updated if this value is added later. Existing environments must have their tier updated via the Environments API.\n\nUse CI/CD variables to dynamically name environments.\n\nThe job is marked as a deployment to dynamically create the environment. is a CI/CD variable set by the runner. The variable is based on the environment name, but suitable for inclusion in URLs. If the job runs in a branch named , this environment would be accessible with a URL like .\n\nThe common use case is to create dynamic environments for branches and use them as review apps. You can see an example that uses review apps at https://gitlab.com/gitlab-examples/review-apps-nginx/.\n\nUse to reuse configuration sections. It’s an alternative to YAML anchors and is a little more flexible and readable.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The name of another job in the pipeline.\n• A list (array) of names of other jobs in the pipeline.\n\nIn this example, the job uses the configuration from the template job. When creating the pipeline, GitLab:\n• Merges the content with the job.\n• Doesn’t merge the values of the keys.\n\nThe combined configuration is equivalent to these jobs:\n• You can use multiple parents for .\n• The keyword supports up to eleven levels of inheritance, but you should avoid using more than three levels.\n• In the example above, is a hidden job, but you can extend configuration from regular jobs as well.\n• Use to reuse configuration from included configuration files.\n\nUse to specify lists of commands to execute on the runner at certain stages of job execution, like before retrieving the Git repository.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• A hash of hooks and their commands. Available hooks: .\n\nUse to specify a list of commands to execute on the runner before cloning the Git repository and any submodules. You can use it for example to:\n\nThis feature is in beta.\n\nUse to authenticate with third party services using identity federation.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• : Google Cloud. Must be configured with the Google Cloud IAM integration.\n\nUse to create JSON web tokens (JWT) to authenticate with third party services. All JWTs created this way support OIDC authentication. The required sub-keyword is used to configure the claim for the JWT.\n\nUse to specify a Docker image that the job runs in.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: The name of the image, including the registry path if needed, in one of these formats:\n\nIn this example, the image is the default for all jobs in the pipeline. The job does not use the default, because it overrides the default with a job-specific section.\n\nThe name of the Docker image that the job runs in. Similar to used by itself.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: The name of the image, including the registry path if needed, in one of these formats:\n\nCommand or script to execute as the container’s entry point.\n\nWhen the Docker container is created, the is translated to the Docker option. The syntax is similar to the Dockerfile directive, where each shell token is a separate string in the array.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• Override the entrypoint of an image.\n\nUse to pass options to the Docker executor runner. This keyword does not work with other executor types.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nA hash of options for the Docker executor, which can include:\n• : Selects the architecture of the image to pull. When not specified, the default is the same platform as the host runner.\n• : Specify the username or UID to use when running the container.\n\nThe pull policy that the runner uses to fetch the Docker image.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• A single pull policy, or multiple pull policies in an array. Can be , , or .\n• If the runner does not support the defined pull policy, the job fails with an error similar to: ERROR: Job failed (system failure): the configured PullPolicies ([always]) are not allowed by AllowedPullPolicies ([never]) .\n\nUse to control inheritance of default keywords and variables.\n\nUse to control the inheritance of default keywords.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• (default) or to enable or disable the inheritance of all default keywords.\n• You can also list default keywords to inherit on one line:\n\nUse to control the inheritance of default variables keywords.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• (default) or to enable or disable the inheritance of all default variables.\n• You can also list default variables to inherit on one line:\n\nUse to configure the auto-cancel redundant pipelines feature to cancel a job before it completes if a new pipeline on the same ref starts for a newer commit. If the feature is disabled, the keyword has no effect. The new pipeline must be for a commit with new changes. For example, the Auto-cancel redundant pipelines feature has no effect if you select New pipeline in the UI to run a pipeline for the same commit.\n\nThe behavior of the Auto-cancel redundant pipelines feature can be controlled by the setting.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nExample of with the default behavior:\n\nIn this example, a new pipeline causes a running pipeline to be:\n• Canceled, if only is running or pending.\n\nExample of with the setting:\n\nIn this example, a new pipeline causes a running pipeline to cancel and if they are running or pending.\n• Only set if the job can be safely canceled after it has started, like a build job. Deployment jobs usually shouldn’t be canceled, to prevent partial deployments.\n• When using the default behavior or :\n• A job that has not started yet is always considered , regardless of the job’s configuration. The configuration is only considered after the job starts.\n• Running pipelines are only canceled if all running jobs are configured with or no jobs configured with have started at any time. After a job with starts, the entire pipeline is no longer considered interruptible.\n• If the pipeline triggered a downstream pipeline, but no job with in the downstream pipeline has started yet, the downstream pipeline is also canceled.\n• You can add an optional manual job with in the first stage of a pipeline to allow users to manually prevent a pipeline from being automatically canceled. After a user starts the job, the pipeline cannot be canceled by the Auto-cancel redundant pipelines feature.\n• When using with a trigger job:\n• The triggered downstream pipeline is never affected by the trigger job’s configuration.\n• If is set to , the trigger job’s configuration has no effect.\n• If is set to , a trigger job with can be automatically canceled.\n\nUse to execute jobs out-of-order. Relationships between jobs that use can be visualized as a directed acyclic graph.\n\nYou can ignore stage ordering and run some jobs without waiting for others to complete. Jobs in multiple stages can run concurrently.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• An array of jobs (maximum of 50 jobs).\n• An empty array ( ), to set the job to start as soon as the pipeline is created.\n\nThis example creates four paths of execution:\n• Linter: The job runs immediately without waiting for the stage to complete because it has no needs ( ).\n• Linux path: The job runs as soon as the job finishes, without waiting for to finish.\n• macOS path: The jobs runs as soon as the job finishes, without waiting for to finish.\n• The job runs as soon as all previous jobs finish: , , , , .\n• The maximum number of jobs that a single job can have in the array is limited:\n• For GitLab.com, the limit is 50. For more information, see issue 350398.\n• For GitLab Self-Managed, the default limit is 50. This limit can be changed.\n• If refers to a job that uses the keyword, it depends on all jobs created in parallel, not just one job. It also downloads artifacts from all the parallel jobs by default. If the artifacts have the same name, they overwrite each other and only the last one downloaded is saved.\n• To have refer to a subset of parallelized jobs (and not all of the parallelized jobs), use the keyword.\n• You can refer to jobs in the same stage as the job you are configuring.\n• If refers to a job that might not be added to a pipeline because of , , or , the pipeline might fail to create. Use the keyword to resolve a failed pipeline creation.\n• If a pipeline has jobs with and jobs in the stage, they will all start as soon as the pipeline is created. Jobs with start immediately, and jobs in the stage also start immediately.\n\nWhen a job uses , it no longer downloads all artifacts from previous stages by default, because jobs with can start before earlier stages complete. With you can only download artifacts from the jobs listed in the configuration.\n\nUse (default) or to control when artifacts are downloaded in jobs that use .\n\nKeyword type: Job keyword. You can use it only as part of a job. Must be used with .\n\nIn this example:\n• The job does not download the artifacts.\n• The job downloads the artifacts from all three , because is , or defaults to , for all three needed jobs.\n• You should not combine with in the same job.\n\nUse to download artifacts from up to five jobs in other pipelines. The artifacts are downloaded from the latest successful specified job for the specified ref. To specify multiple jobs, add each as separate array items under the keyword.\n\nIf there is a pipeline running for the ref, a job with does not wait for the pipeline to complete. Instead, the artifacts are downloaded from the latest successful run of the specified job.\n\nmust be used with , , and .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : The job to download artifacts from.\n• : The ref to download artifacts from.\n• : Must be to download artifacts.\n\nIn this example, downloads the artifacts from the latest successful and jobs on the branches in the and projects.\n\nYou can use CI/CD variables in , for example:\n• To download artifacts from a different pipeline in the current project, set to be the same as the current project, but use a different ref than the current pipeline. Concurrent pipelines running on the same ref could override the artifacts.\n• The user running the pipeline must have at least the Reporter role for the group or project, or the group/project must have public visibility.\n• You can’t use in the same job as .\n• When using to download artifacts from another pipeline, the job does not wait for the needed job to complete. Using to wait for jobs to complete is limited to jobs in the same pipeline. Make sure that the needed job in the other pipeline completes before the job that needs it tries to download the artifacts.\n• You can’t download artifacts from jobs that run in .\n• To download artifacts between parent-child pipelines, use .\n\nA child pipeline can download artifacts from a job in its parent pipeline or another child pipeline in the same parent-child pipeline hierarchy.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : A pipeline ID. Must be a pipeline present in the same parent-child pipeline hierarchy.\n• : The job to download artifacts from.\n\nIn this example, the job in the parent pipeline creates some artifacts. The job triggers a child pipeline, and passes the variable to the child pipeline as a new variable. The child pipeline can use that variable in to download artifacts from the parent pipeline.\n• The attribute does not accept the current pipeline ID ( ). To download artifacts from a job in the current pipeline, use .\n• You cannot use in a trigger job, or to fetch artifacts from a multi-project pipeline. To fetch artifacts from a multi-project pipeline use .\n\nTo need a job that sometimes does not exist in the pipeline, add to the configuration. If not defined, is the default.\n\nJobs that use , , or and that are added with might not always be added to a pipeline. GitLab checks the relationships before starting a pipeline:\n• If the entry has and the needed job is present in the pipeline, the job waits for it to complete before starting.\n• If the needed job is not present, the job can start when all other needs requirements are met.\n• If the section contains only optional jobs, and none are added to the pipeline, the job starts immediately (the same as an empty entry: ).\n• If a needed job has , but it was not added to the pipeline, the pipeline fails to start with an error similar to: 'job1' job needs 'job2' job, but it was not added to the pipeline .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nIn this example:\n• When the branch is the default branch, is added to the pipeline, so:\n• waits for both and to complete.\n• When the branch is not the default branch, is not added to the pipeline, so:\n• waits for only to complete, and does not wait for the missing .\n• has no other needed jobs and starts immediately (at the same time as ), like .\n\nYou can mirror the pipeline status from an upstream pipeline to a job by using the keyword. The latest pipeline status from the default branch is replicated to the job.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• A full project path, including namespace and group. If the project is in the same group or namespace, you can omit them from the keyword. For example: or .\n• If you add the keyword to , the job no longer mirrors the pipeline status. The behavior changes to .\n\nJobs can use to run a job multiple times in parallel in a single pipeline, but with different variable values for each instance of the job.\n\nUse to execute jobs out-of-order depending on parallelized jobs.\n\nKeyword type: Job keyword. You can use it only as part of a job. Must be used with .\n\nSupported values: An array of hashes of variables:\n• The variables and values must be selected from the variables and values defined in the job.\n\nThe above example generates the following jobs:\n\nThe job runs as soon as the job finishes.\n• Specify a parallelized job using needs with multiple parallelized jobs.\n• None The order of the matrix variables in must match the order of the matrix variables in the needed job. For example, reversing the order of the variables in the job in the earlier example above would be invalid: - : app1 # The variable order does not match `linux:build` and is invalid.\n\nUse to define a GitLab Pages job that uploads static content to GitLab. The content is then published as a website.\n• Alternatively, define if want to use a different content directory.\n\nKeyword type: Job keyword or Job name (deprecated). You can use it only as part of a job.\n• A boolean. Uses the default configuration when set to\n• A hash of configuration options, see the following sections for details.\n\nThis example renames the directory to . This directory is exported as an artifact and published with GitLab Pages.\n\nThis example does not move the directory, but uses the property directly. It also configures the pages deployment to be unpublished after a week.\n\nDeprecated: Use as a job name\n\nUsing as a job name results in the same behavior as specifying the Pages property . This method is available for backwards compatibility, but might not receive all future improvements to the Pages job configuration.\n\nExample using as a job name:\n\nTo use as a job name without triggering a Pages deployment, set the property to false:\n\nUse to configure the content directory of a job. The top-level keyword is deprecated as of GitLab 17.9 and must now be nested under the keyword.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: A path to a directory containing the Pages content. In GitLab 17.10 and later, if not specified, the default directory is used and if specified, this path is automatically appended to .\n\nThis example uses Eleventy to generate a static website and output the generated HTML files into a the directory. This directory is exported as an artifact and published with GitLab Pages.\n\nIt is also possible to use variables in the field. For example:\n\nThe publish path specified must be relative to the build root.\n\nUse to configure a path prefix for parallel deployments of GitLab Pages.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nThe given value is converted to lowercase and shortened to 63 bytes. Everything except alphanumeric characters or periods is replaced with a hyphen. Leading and trailing hyphens or periods are not permitted.\n\nIn this example, a different pages deployment is created for each branch.\n\nUse to specify how long a deployment should be available before it expires. After the deployment is expired, it’s deactivated by a cron job running every 10 minutes.\n\nBy default, parallel deployments expire automatically after 24 hours. To disable this behavior, set the value to .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: The expiry time. If no unit is provided, the time is in seconds. Valid values include:\n\nUse to run a job multiple times in parallel in a single pipeline.\n\nMultiple runners must exist, or a single runner must be configured to run multiple jobs concurrently.\n\nParallel jobs are named sequentially from to .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• A numeric value from to .\n\nThis example creates 5 jobs that run in parallel, named to .\n• Every parallel job has a and predefined CI/CD variable set.\n• A pipeline with jobs that use might:\n• Create more jobs running in parallel than available runners. Excess jobs are queued and marked while waiting for an available runner.\n• Create too many jobs, and the pipeline fails with a error. The maximum number of jobs that can exist in active pipelines is limited at the instance-level.\n\nUse to run a job multiple times in parallel in a single pipeline, but with different variable values for each instance of the job.\n\nMultiple runners must exist, or a single runner must be configured to run multiple jobs concurrently.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: An array of hashes of variables:\n• The variable names can use only numbers, letters, and underscores ( ).\n• The values must be either a string, or an array of strings.\n• The number of permutations cannot exceed 200.\n\nThe example generates 10 parallel jobs, each with different values for and :\n• None jobs add the variable values to the job names to differentiate the jobs from each other, but large values can cause names to exceed limits:\n• Job names must be 255 characters or fewer.\n• When using , job names must be 128 characters or fewer.\n• None You cannot create multiple matrix configurations with the same variable values but different variable names. Job names are generated from the variable values, not the variable names, so matrix entries with identical values generate identical job names that overwrite each other. For example, this configuration would try to create two series of identical jobs, but the versions overwrite the versions:\n• There’s a known issue when using tags with .\n• Select different runner tags for each parallel matrix job.\n\nThe release job must have access to the , which must be in the .\n\nIf you use the Docker executor, you can use this image from the GitLab container registry:\n\nIf you use the Shell executor or similar, install on the server where the runner is registered.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• When you add a Git tag in the UI at Code > Tags.\n• None All release jobs, except trigger jobs, must include the keyword. A release job can use the output from script commands. If you don’t need the script, you can use a placeholder: An issue exists to remove this requirement.\n• None The section executes after the keyword and before the .\n• None A release is created only if the job’s main script succeeds.\n• None If the release already exists, it is not updated and the job with the keyword fails.\n• CI/CD example of the keyword.\n\nRequired. The Git tag for the release.\n\nIf the tag does not exist in the project yet, it is created at the same time as the release. New tags use the SHA associated with the pipeline.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nTo create a release when a new tag is added to the project:\n• Use the CI/CD variable as the .\n• Use to configure the job to run only for new tags.\n\nTo create a release and a new tag at the same time, your should not configure the job to run only for new tags. A semantic versioning example:\n\nIf the tag does not exist, the newly created tag is annotated with the message specified by . If omitted, a lightweight tag is created.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nThe release name. If omitted, it is populated with the value of .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nThe long description of the release.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The path to a file that contains the description.\n• The file location must be relative to the project directory ( ).\n• If the file is a symbolic link, it must be in the .\n• The and filename can’t contain spaces.\n• The is evaluated by the shell that runs . You can use CI/CD variables to define the description, but some shells use different syntax to reference variables. Similarly, some shells might require special characters to be escaped. For example, backticks ( ) might need to be escaped with a backslash ( ).\n\nThe for the release, if the doesn’t exist yet.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• A commit SHA, another tag name, or a branch name.\n\nThe title of each milestone the release is associated with.\n\nThe date and time when the release is ready.\n• A date enclosed in quotes and expressed in ISO 8601 format.\n• If it is not defined, the current date and time is used.\n\nUse to include asset links in the release.\n\nUse to create a resource group that ensures a job is mutually exclusive across different pipelines for the same project.\n\nFor example, if multiple jobs that belong to the same resource group are queued simultaneously, only one of the jobs starts. The other jobs wait until the is free.\n\nResource groups behave similar to semaphores in other programming languages.\n\nYou can choose a process mode to strategically control the job concurrency for your deployment preferences. The default process mode is . To change the process mode of a resource group, use the API to send a request to edit an existing resource group.\n\nYou can define multiple resource groups per environment. For example, when deploying to physical devices, you might have multiple physical devices. Each device can be deployed to, but only one deployment can occur per device at any given time.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• Only letters, digits, , , , , , , , and spaces. It can’t start or end with . CI/CD variables are supported.\n\nIn this example, two jobs in two separate pipelines can never run at the same time. As a result, you can ensure that concurrent deployments never happen to the production environment.\n\nUse to configure how many times a job is retried if it fails. If not defined, defaults to and jobs do not retry.\n\nWhen a job fails, the job is processed up to two more times, until it succeeds or reaches the maximum number of retries.\n\nBy default, all failure types cause the job to be retried. Use or to select which failures to retry on.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nwill be retried up to 2 times if the exit code is or if it had a runner system failure.\n\nUse with to retry jobs for only specific failure cases. is the maximum number of retries, like , and can be , , or .\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• A single failure type, or an array of one or more failure types:\n• : Retry when the failure reason is unknown.\n• : Retry when:\n• The runner failed to pull the Docker image. For , , executors.\n• : Retry when the job got stuck or timed out.\n• : Retry if there is a runner system failure (for example, job setup failed).\n• : Retry if the runner is unsupported.\n• : Retry if a delayed job could not be executed.\n• : Retry if the script exceeded the maximum execution time set for the job.\n• : Retry if the job is archived and can’t be run.\n• : Retry if the job failed to complete prerequisite tasks.\n• : Retry if the scheduler failed to assign the job to a runner.\n• : Retry if there is an unknown job problem.\n\nIf there is a failure other than a runner system failure, the job is not retried.\n\nExample of (array of failure types):\n\nUse with to retry jobs for only specific failure cases. is the maximum number of retries, like , and can be , , or .\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nYou can specify the number of retry attempts for certain stages of job execution using variables.\n\nUse to include or exclude jobs in pipelines.\n\nRules are evaluated when the pipeline is created, and evaluated in order. When a match is found, no more rules are checked and the job is either included or excluded from the pipeline depending on the configuration. If no rules match, the job is not added to the pipeline.\n\naccepts an array of rules. Each rules must have at least one of:\n\nRules can also optionally be combined with:\n\nYou can combine multiple keywords together for complex rules.\n\nThe job is added to the pipeline:\n• If an , , or rule matches, and is configured with (default if not defined), , or .\n• If a rule is reached that is only , , or .\n\nThe job is not added to the pipeline:\n• If a rule matches and has .\n\nFor additional examples, see Specify when jobs run with .\n\nUse clauses to specify when to add a job to a pipeline:\n• If an statement is true, add the job to the pipeline.\n• If an statement is true, but it’s combined with , do not add the job to the pipeline.\n• If an statement is false, check the next item (if any more exist).\n• Based on the values of CI/CD variables or predefined CI/CD variables, with some exceptions.\n\nKeyword type: Job-specific and pipeline-specific. You can use it as part of a job to configure the job behavior, or with to configure the pipeline behavior.\n• You cannot use nested variables with . See issue 327780 for more details.\n• If a rule matches and has no defined, the rule uses the defined for the job, which defaults to if not defined.\n• You can mix at the job-level with in rules. configuration in takes precedence over at the job-level.\n• Unlike variables in sections, variables in rules expressions are always formatted as .\n• You can use with to conditionally include other configuration files.\n• CI/CD variables on the right side of and expressions are evaluated as regular expressions.\n\nUse to specify when to add a job to a pipeline by checking for changes to specific files.\n\nFor new branch pipelines or when there is no Git event, always evaluates to true and the job always runs. Pipelines like tag pipelines, scheduled pipelines, and manual pipelines, all do not have a Git event associated with them. To cover these cases, use to specify the branch to compare against the pipeline ref.\n\nIf you do not use , you should use only with branch pipelines or merge request pipelines, though still evaluates to true when creating a new branch. With:\n• Merge request pipelines, compares the changes with the target MR branch.\n• Branch pipelines, compares the changes with the previous commit on the branch.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nAn array including any number of:\n• Paths to files. The file paths can include CI/CD variables.\n• Wildcard paths for:\n• A directory and all its subdirectories, for example .\n• Wildcard glob paths for all files with the same extension or multiple extensions, for example or .\n• Wildcard paths to files in the root directory, or all directories, wrapped in double quotes. For example or .\n\nIn this example:\n• If the pipeline is a merge request pipeline, check and the files in for changes.\n• If has changed, add the job to the pipeline as a manual job, and the pipeline continues running even if the job is not triggered ( ).\n• If a file in has changed, add the job to the pipeline.\n• If no listed files have changed, do not add either job to any pipeline (same as ).\n• Glob patterns are interpreted with Ruby’s with the flags .\n• A maximum of 50 patterns or file paths can be defined per section.\n• resolves to if any of the matching files are changed (an operation).\n• For additional examples, see Specify when jobs run with .\n• You can use the character for both variables and paths. For example, if the variable exists, its value is used. If it does not exist, the is interpreted as being part of a path.\n• Jobs or pipelines can run unexpectedly when using .\n\nUse to specify that a job only be added to a pipeline when specific files are changed, and use to specify the files.\n\nis the same as using without any subkeys. All additional details and related topics are the same.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• Same as above.\n\nIn this example, both jobs have the same behavior.\n\nUse to specify which ref to compare against for changes to the files listed under .\n\nKeyword type: Job keyword. You can use it only as part of a job, and it must be combined with .\n• A branch name, like , , or .\n• A tag name, like or .\n\nIn this example, the job is only included when the has changed relative to and the pipeline source is a merge request event.\n• Using with merged results pipelines can cause unexpected results, because the comparison base is an internal commit that GitLab creates.\n• You can use to skip a job if the branch is empty.\n\nUse to run a job when certain files exist in the repository.\n\nKeyword type: Job keyword. You can use it as part of a job or an .\n• An array of file paths. Paths are relative to the project directory ( ) and can’t directly link outside it. File paths can use glob patterns and CI/CD variables.\n\nIn this example:\n• runs if a exists in the root directory of the repository.\n• runs if a exists anywhere in the repository.\n• Glob patterns are interpreted with Ruby’s with the flags .\n• For performance reasons, GitLab performs a maximum of 50,000 checks against patterns or file paths. After the 50,000th check, rules with patterned globs always match. In other words, the rule always assumes a match in projects with more than 50,000 files, or if there are fewer than 50,000 files but the rules are checked more than 50,000 times.\n• If there are multiple patterned globs, the limit is 50,000 divided by the number of globs. For example, a rule with 5 patterned globs has file limit of 10,000.\n• A maximum of 50 patterns or file paths can be defined per section.\n• resolves to if any of the listed files are found (an operation).\n• With job-level , GitLab searches for the files in the project and ref that runs the pipeline. When using with , GitLab searches for the files in the project and ref of the file that contains the section. The project containing the section can be different than the project running the pipeline when using:\n• cannot search for the presence of artifacts, because evaluation happens before jobs run and artifacts are fetched.\n\nis the same as using without any subkeys. All additional details are the same.\n\nKeyword type: Job keyword. You can use it as part of a job or an .\n\nIn this example, both jobs have the same behavior.\n• In some cases you cannot use or in a CI/CD variable with . See issue 386595 for more details.\n\nUse to specify the location in which to search for the files listed under . Must be used with .\n\nKeyword type: Job keyword. You can use it as part of a job or an , and it must be combined with .\n• : Optional. The commit ref to use to search for the file. The ref can be a tag, branch name, or SHA. Defaults to the of the project when not specified.\n\nIn this example, the job is only included when the exists in the project on the commit tagged with .\n\nUse alone or as part of another rule to control conditions for adding a job to a pipeline. is similar to , but with slightly different input options.\n\nIf a rule is not combined with , , or , it always matches if reached when evaluating a job’s rules.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• (default): Run the job only when no jobs in earlier stages fail.\n• : Run the job only when at least one job in an earlier stage fails.\n• : Don’t run the job regardless of the status of jobs in earlier stages.\n• : Run the job regardless of the status of jobs in earlier stages.\n• : Add the job to the pipeline as a manual job. The default value for changes to .\n• : Add the job to the pipeline as a delayed job.\n\nIn this example, is added to pipelines:\n• For the default branch, with which is the default behavior when is not defined.\n• In all other cases as a manual job.\n• When evaluating the status of jobs for and :\n• Jobs with in earlier stages are considered successful, even if they failed.\n• Skipped jobs in earlier stages, for example manual jobs that have not been started, are considered successful.\n• When using to add a manual job:\n• becomes by default. This default is the opposite of using to add a manual job.\n• To achieve the same behavior as defined outside of , set to .\n\nUse in to allow a job to fail without stopping the pipeline.\n\nYou can also use with a manual job. The pipeline continues running without waiting for the result of the manual job. combined with in rules causes the pipeline to wait for the manual job to run before continuing.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• or . Defaults to if not defined.\n\nIf the rule matches, then the job is a manual job with .\n• The rule-level overrides the job-level , and only applies when the specific rule triggers the job.\n\nUse in rules to update a job’s for specific conditions. When a condition matches a rule, the job’s configuration is completely replaced with the in the rule.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• An array of job names as strings.\n• A hash with a job name, optionally with additional attributes.\n• An empty array ( ), to set the job needs to none when the specific condition is met.\n\nIn this example:\n• If the pipeline runs on a branch that is not the default branch, and therefore the rule matches the first condition, the job needs the job.\n• If the pipeline runs on the default branch, and therefore the rule matches the second condition, the job needs the job.\n• in rules override any defined at the job-level. When overridden, the behavior is same as job-level .\n• in rules can accept and .\n\nUse in to define variables for specific conditions.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• A hash of variables in the format .\n\nUse in rules to update a job’s value for specific conditions.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• The rule-level overrides the job-level , and only applies when the specific rule triggers the job.\n\nUse to define a series of steps to be executed in a job. Each step can be either a script or a predefined step.\n\nYou can also provide optional environment variables and inputs.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• An array of hashes, where each hash represents a step with the following possible keys:\n• : A string representing the name of the step.\n• : A string or array of strings containing shell commands to execute.\n• : Optional. A hash of environment variables specific to this step.\n\nEach array entry must have a , and one or (but not both).\n\nIn this example, the job has two steps:\n• uses a predefined step with an environment variable and an input parameter.\n• A step can have either a or a key, but not both.\n• A configuration cannot be used together with existing keyword.\n• Multi-line scripts can be defined using YAML block scalar syntax.\n\nUse to specify commands for the runner to execute.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• When you use these special characters in , you must use single quotes ( ) or double quotes ( ).\n• Use color codes with to make job logs easier to review.\n\nUse to specify CI/CD secrets to:\n• Make available in the job as CI/CD variables ( type by default).\n\nUse to specify secrets provided by a HashiCorp Vault.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : Name of the secrets engine. Can be one of (default), , or .\n• : Name of the field where the password is stored.\n\nTo specify all details explicitly and use the KV-V2 secrets engine:\n\nYou can shorten this syntax. With the short syntax, and both default to :\n\nTo specify a custom secrets engine path in the short syntax, add a suffix that starts with :\n\nUse to specify secrets provided by GCP Secret Manager.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : Name of the secret.\n\nUse to specify secrets provided by a Azure Key Vault.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : Name of the secret.\n\nUse to configure the secret to be stored as either a or type CI/CD variable\n\nBy default, the secret is passed to the job as a type CI/CD variable. The value of the secret is stored in the file and the variable contains the path to the file.\n\nIf your software can’t use type CI/CD variables, set to store the secret value directly in the variable.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The keyword is a setting for the CI/CD variable and must be nested under the CI/CD variable name, not in the section.\n\nUse to explicitly select a token to use when authenticating with Vault by referencing the token’s CI/CD variable.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The name of an ID token\n• When the keyword is not set, the first ID token is used to authenticate.\n\nUse to specify any additional Docker images that your scripts require to run successfully. The image is linked to the image specified in the keyword.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: The name of the services image, including the registry path if needed, in one of these formats:\n\nCI/CD variables are supported, but not for .\n\nIn this example, GitLab launches two containers for the job:\n• A PostgreSQL container. The commands in the Ruby container can connect to the PostgreSQL database at the hostname.\n\nUse to pass options to the Docker executor of a GitLab Runner.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nA hash of options for the Docker executor, which can include:\n• : Selects the architecture of the image to pull. When not specified, the default is the same platform as the host runner.\n• : Specify the username or UID to use when running the container.\n\nThe pull policy that the runner uses to fetch the Docker image.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• A single pull policy, or multiple pull policies in an array. Can be , , or .\n• If the runner does not support the defined pull policy, the job fails with an error similar to: ERROR: Job failed (system failure): the configured PullPolicies ([always]) are not allowed by AllowedPullPolicies ([never]) .\n\nUse to define which stage a job runs in. Jobs in the same can execute in parallel (see Additional details).\n\nIf is not defined, the job uses the stage by default.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: A string, which can be a:\n• The stage name must be 255 characters or fewer.\n• Jobs can run in parallel if they run on different runners.\n• If you have only one runner, jobs can run in parallel if the runner’s setting is greater than .\n\nUse the stage to make a job run at the start of a pipeline. By default, is the first stage in a pipeline. User-defined stages execute after . You do not have to define in .\n\nIf a pipeline contains only jobs in the or stages, it does not run. There must be at least one other job in a different stage.\n\nKeyword type: You can only use it with a job’s keyword.\n• If a pipeline has jobs with and jobs in the stage, they will all start as soon as the pipeline is created. Jobs with start immediately, ignoring any stage configuration.\n• A pipeline execution policy can define a stage which runs before .\n\nUse the stage to make a job run at the end of a pipeline. By default, is the last stage in a pipeline. User-defined stages execute before . You do not have to define in .\n\nIf a pipeline contains only jobs in the or stages, it does not run. There must be at least one other job in a different stage.\n\nKeyword type: You can only use it with a job’s keyword.\n• A pipeline execution policy can define a stage which runs after .\n\nUse to select a specific runner from the list of all runners that are available for the project.\n\nWhen you register a runner, you can specify the runner’s tags, for example , , or . To pick up and run a job, a runner must be assigned every tag listed in the job.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of tag names, which are case-sensitive.\n\nIn this example, only runners with both the and tags can run the job.\n• The number of tags must be less than .\n• Use tags to control which jobs a runner can run\n• Select different runner tags for each parallel matrix job\n\nUse to configure a timeout for a specific job. If the job runs for longer than the timeout, the job fails.\n\nThe job-level timeout can be longer than the project-level timeout, but can’t be longer than the runner’s timeout.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: A period of time written in natural language. For example, these are all equivalent:\n\nUse to declare that a job is a “trigger job” which starts a downstream pipeline that is either:\n\nTrigger jobs can use only a limited set of GitLab CI/CD configuration keywords. The keywords available for use in trigger jobs are:\n• (only with a value of , , or ).\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• For multi-project pipelines, the path to the downstream project. CI/CD variables are supported in GitLab 15.3 and later, but not job-only variables. Alternatively, use .\n• You can use in the same job as , but you cannot use the API to start trigger jobs. See issue 284086 for more details.\n• You cannot manually specify CI/CD variables before running a manual trigger job.\n• CI/CD variables defined in a top-level section (globally) or in the trigger job are forwarded to the downstream pipeline as trigger variables.\n• Pipeline variables are not passed to downstream pipelines by default. Use trigger:forward to forward these variables to downstream pipelines.\n• Job-only variables are not available in trigger jobs.\n• Environment variables defined in the runner’s are not available to trigger jobs and are not passed to downstream pipelines.\n• You cannot use in a trigger job.\n• To run a pipeline for a specific branch, tag, or commit, you can use a trigger token to authenticate with the pipeline triggers API. The trigger token is different than the keyword.\n\nUse to declare that a job is a “trigger job” which starts a child pipeline.\n• to set the inputs when the downstream pipeline configuration uses .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The path to the child pipeline’s configuration file.\n\nUse to declare that a job is a “trigger job” which starts a multi-project pipeline.\n\nBy default, the multi-project pipeline triggers for the default branch. Use to specify a different branch.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The path to the downstream project. CI/CD variables are supported in GitLab 15.3 and later, but not job-only variables.\n\nExample of for a different branch:\n• To run a pipeline for a specific branch, tag, or commit, you can also use a trigger token to authenticate with the pipeline triggers API. The trigger token is different than the keyword.\n\nUse to force the job to wait for the downstream pipeline to complete before it is marked as success.\n\nThis behavior is different than the default, which is for the job to be marked as success as soon as the downstream pipeline is created.\n\nThis setting makes your pipeline execution linear rather than parallel.\n\nIn this example, jobs from subsequent stages wait for the triggered pipeline to successfully complete before starting.\n• Optional manual jobs in the downstream pipeline do not affect the status of the downstream pipeline or the upstream trigger job. The downstream pipeline can complete successfully without running any optional manual jobs.\n• Blocking manual jobs in the downstream pipeline must run before the trigger job is marked as successful or failed. The trigger job shows pending ( ) if the downstream pipeline status is waiting for manual action ( ) due to manual jobs. By default, jobs in later stages do not start until the trigger job completes.\n• If the downstream pipeline has a failed job, but the job uses , the downstream pipeline is considered successful and the trigger job shows success.\n\nUse to set the inputs when the downstream pipeline configuration uses .\n\nUse to specify what to forward to the downstream pipeline. You can control what is forwarded to both parent-child pipelines and multi-project pipelines.\n\nForwarded variables do not get forwarded again in nested downstream pipelines by default, unless the nested downstream trigger job also uses .\n• : (default), or . When , variables defined in the trigger job are passed to downstream pipelines.\n• : or (default). When , pipeline variables are passed to the downstream pipeline.\n\nRun this pipeline manually, with the CI/CD variable :\n• CI/CD variables forwarded to downstream pipelines with are pipeline variables, which have high precedence. If a variable with the same name is defined in the downstream pipeline, that variable is usually overwritten by the forwarded variable.\n\nUse to configure the conditions for when jobs run. If not defined in a job, the default value is .\n\nKeyword type: Job keyword. You can use it as part of a job. and can also be used in .\n• (default): Run the job only when no jobs in earlier stages fail.\n• : Run the job only when at least one job in an earlier stage fails.\n• : Don’t run the job regardless of the status of jobs in earlier stages. Can only be used in a section or .\n• : Run the job regardless of the status of jobs in earlier stages.\n• : Add the job to the pipeline as a manual job.\n• : Add the job to the pipeline as a delayed job.\n\nIn this example, the script:\n• Always executes as the last step in pipeline regardless of success or failure.\n• Executes when you run it manually in the GitLab UI.\n• When evaluating the status of jobs for and :\n• Jobs with in earlier stages are considered successful, even if they failed.\n• Skipped jobs in earlier stages, for example manual jobs that have not been started, are considered successful.\n• The default value for is with . The default value changes to with .\n• can be used with for more dynamic job control.\n• can be used with to control when a pipeline can start.\n\nUse with to define a custom confirmation message for manual jobs. If there is no manual job defined with , this keyword has no effect.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nVariables can be defined in a CI/CD job, or as a top-level (global) keyword to define default CI/CD variables for all jobs.\n• All YAML-defined variables are also set to any linked Docker service containers.\n• YAML-defined variables are meant for non-sensitive project configuration. Store sensitive information in protected variables or CI/CD secrets.\n• Manual pipeline variables and scheduled pipeline variables are not passed to downstream pipelines by default. Use trigger:forward to forward these variables to downstream pipelines.\n• Predefined variables are variables the runner automatically creates and makes available in the job.\n• You can configure runner behavior with variables.\n\nYou can use job variables in commands in the job’s , , or sections, and also with some job keywords. Check the Supported values section of each job keyword to see if it supports variables.\n\nYou cannot use job variables as values for global keywords like .\n\nSupported values: Variable name and value pairs:\n• The name can use only numbers, letters, and underscores ( ). In some shells, the first character must be a letter.\n• The value must be a string.\n\nIn this example:\n• has and job variables defined. Both job variables can be used in the section.\n\nVariables defined in a top-level section act as default variables for all jobs.\n\nEach default variable is made available to every job in the pipeline, except when the job already has a variable defined with the same name. The variable defined in the job takes precedence, so the value of the default variable with the same name cannot be used in the job.\n\nLike job variables, you cannot use default variables as values for other global keywords, like .\n\nSupported values: Variable name and value pairs:\n• The name can use only numbers, letters, and underscores ( ). In some shells, the first character must be a letter.\n• The value must be a string.\n\nIn this example:\n• has no variables defined. The default variable is copied to the job and can be used in the section.\n• already has a variable defined, so the default is not copied to the job. The job also has a job variable defined. Both job variables can be used in the section.\n\nUse the keyword to define a description for a default variable. The description displays with the prefilled variable name when running a pipeline manually.\n\nKeyword type: You can only use this keyword with default , not job .\n• When used without , the variable exists in pipelines that were not triggered manually, and the default value is an empty string ( ).\n\nUse the keyword to define a pipeline-level (default) variable’s value. When used with , the variable value is prefilled when running a pipeline manually.\n\nKeyword type: You can only use this keyword with default , not job .\n• If used without , the behavior is the same as .\n\nUse to define an array of values that are selectable in the UI when running a pipeline manually.\n\nMust be used with , and the string defined for :\n• Must also be one of the strings in the array.\n\nIf there is no , this keyword has no effect.\n\nKeyword type: You can only use this keyword with default , not job .\n\nUse the keyword to configure a variable to be expandable or not.\n\nKeyword type: You can use this keyword with both default and job .\n• : The variable is not expandable.\n• The result of is .\n• The result of is .\n• The keyword can only be used with default and job keywords. You can’t use it with or .\n\nThe following keywords are deprecated.\n\nDefining , , , , and globally is deprecated. Using these keywords at the top level is still possible to ensure backwards compatibility, but could be scheduled for removal in a future milestone.\n\nUse instead. For example:\n\nYou can use and to control when to add jobs to pipelines.\n• Use to define when a job runs.\n• Use to define when a job does not run.\n\nYou can use the and keywords to control when to add jobs to a pipeline based on branch names or pipeline types.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: An array including any number of:\n• Branch names, for example or .\n• Regular expressions that match against branch names, for example .\n• The following keywords: For pipelines triggered by the pipelines API. When the Git reference for a pipeline is a branch. For pipelines created by using a GitLab ChatOps command. When you use CI services other than GitLab. When an external pull request on GitHub is created or updated (See Pipelines for external pull requests). For pipelines created when a merge request is created or updated. Enables merge request pipelines, merged results pipelines, and merge trains. For multi-project pipelines created by using the API with , or the keyword. For pipelines triggered by a event, including for branches and tags. When the Git reference for a pipeline is a tag. For pipelines created by using a trigger token. For pipelines created by selecting New pipeline in the GitLab UI, from the project’s Build > Pipelines section.\n\nExample of and :\n• None Scheduled pipelines run on specific branches, so jobs configured with run on scheduled pipelines too. Add to prevent jobs with from running on scheduled pipelines.\n• None or used without any other keywords are equivalent to or . For example, the following two jobs configurations have the same behavior:\n• None If a job does not use , , or , then is set to and by default. For example, and are equivalent:\n\nYou can use the or keywords to control when to add jobs to a pipeline, based on the status of CI/CD variables.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nUse the keyword with to run a job, or with to skip a job, when a Git push event modifies a file.\n\nUse in pipelines with the following refs:\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: An array including any number of:\n• Wildcard paths for:\n• A directory and all its subdirectories, for example .\n• Wildcard glob paths for all files with the same extension or multiple extensions, for example or .\n• Wildcard paths to files in the root directory, or all directories, wrapped in double quotes. For example or .\n• resolves to if any of the matching files are changed (an operation).\n• Glob patterns are interpreted with Ruby’s with the flags .\n• If you use refs other than , , or , can’t determine if a given file is new or old and always returns .\n• If you use with other refs, jobs ignore the changes and always run.\n• If you use with other refs, jobs ignore the changes and never run.\n• Jobs or pipelines can run unexpectedly when using .\n\nUse or to control if jobs are added to the pipeline when the Kubernetes service is active in the project.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• The strategy accepts only the keyword.\n\nIn this example, the job runs only when the Kubernetes service is active in the project."
    },
    {
        "link": "https://docs.gitlab.com/ci",
        "document": "CI/CD is a continuous method of software development, where you continuously build, test, deploy, and monitor iterative code changes.\n\nThis iterative process helps reduce the chance that you develop new code based on buggy or failed previous versions. GitLab CI/CD can catch bugs early in the development cycle, and help ensure that the code deployed to production complies with your established code standards.\n\nThis process is part of a larger workflow:\n\nTo use GitLab CI/CD, you start with a file at the root of your project. This file specifies the stages, jobs, and scripts to be executed during your CI/CD pipeline. It is a YAML file with its own custom syntax.\n\nIn this file, you define variables, dependencies between jobs, and specify when and how each job should be executed.\n\nYou can name this file anything you want, but is the most common name, and the product documentation refers to it as the file or the CI/CD configuration file.\n\nFor more information, see:\n• The CI/CD YAML syntax reference, which lists all possible keywords\n\nRunners are the agents that run your jobs. These agents can run on physical machines or virtual instances. In your file, you can specify a container image you want to use when running the job. The runner loads the image, clones your project, and runs the job either locally or in the container.\n\nIf you use GitLab.com, runners on Linux, Windows, and macOS are already available for use. And you can register your own runners on GitLab.com if you’d like.\n\nIf you don’t use GitLab.com, you can:\n• Register runners or use runners already registered for your GitLab Self-Managed instance.\n\nFor more information, see:\n\nA pipeline is what you’re defining in the file, and is what happens when the contents of the file are run on a runner.\n\nPipelines are made up of jobs and stages:\n• Stages define the order of execution. Typical stages might be , , and .\n• Jobs specify the tasks to be performed in each stage. For example, a job can compile or test code.\n\nPipelines can be triggered by various events, like commits or merges, or can be on schedule. In your pipeline, you can integrate with a wide range of tools and platforms.\n\nFor more information, see:\n• Pipeline editor, which you use to edit your configuration\n\nStep 4: Use CI/CD variables as part of jobs\n\nGitLab CI/CD variables are key-value pairs you use to store and pass configuration settings and sensitive information, like passwords or API keys, to jobs in a pipeline.\n\nUse CI/CD variables to customize jobs by making values defined elsewhere accessible to jobs. You can hard-code CI/CD variables in your file, set them in your project settings, or generate them dynamically. You can define them for the project, group, or instance.\n\nTwo types of variables exist: custom variables and predefined.\n• Custom variables are user-defined. Create and manage them in the GitLab UI, API, or in configuration files.\n• Predefined variables are automatically set by GitLab and provide information about the current job, pipeline, and environment.\n\nVariables can be marked as “protected” or “masked” for added security.\n• Protected variables are only available to jobs running on protected branches or tags.\n• Masked variables have their values hidden in job logs to prevent sensitive information from being exposed.\n\nFor more information, see:\n\nA CI/CD component is a reusable pipeline configuration unit. Use a CI/CD component to compose an entire pipeline configuration or a small part of a larger pipeline.\n\nYou can add a component to your pipeline configuration with .\n\nReusable components help reduce duplication, improve maintainability, and promote consistency across projects. Create a component project and publish it to the CI/CD Catalog to share your component across multiple projects.\n\nGitLab also has CI/CD component templates for common tasks and integrations.\n\nFor more information, see:"
    },
    {
        "link": "https://docs.gitlab.com/ci/yaml/yaml_optimization",
        "document": "You can reduce complexity and duplicated configuration in your GitLab CI/CD configuration files by using:\n• YAML-specific features like anchors ( ), aliases ( ), and map merging ( ). Read more about the various YAML features.\n• The keyword, which is more flexible and readable. You should use where possible.\n\nYAML has a feature called ‘anchors’ that you can use to duplicate content across your document.\n\nUse anchors to duplicate or inherit properties. Use anchors with hidden jobs to provide templates for your jobs. When there are duplicate keys, the latest included key wins, overriding the other keys.\n\nIn certain cases (see YAML anchors for scripts), you can use YAML anchors to build arrays with multiple components defined elsewhere. For example:\n\nYou can’t use YAML anchors across multiple files when using the keyword. Anchors are only valid in the file they were defined in. To reuse configuration from different YAML files, use tags or the keyword.\n\nThe following example uses anchors and map merging. It creates two jobs, and , that inherit the configuration, each with their own custom defined:\n\nsets up the name of the anchor ( ), means “merge the given hash into the current one,” and includes the named anchor ( again). The expanded version of this example is:\n\nYou can use anchors to define two sets of services. For example, and share the defined in , but use different , defined in and :\n\nYou can see that the hidden jobs are conveniently used as templates, and overwrites .\n\nYou can use YAML anchors with script, , and to use predefined commands in multiple jobs:\n\nYou can use the keyword to reuse configuration in multiple jobs. It is similar to YAML anchors, but simpler and you can use with .\n\nsupports multi-level inheritance. You should avoid using more than three levels, due to the additional complexity, but you can use as many as eleven. The following example has two levels of inheritance:\n\nTo exclude a key from the extended content, you must assign it to , for example:\n\nUse and together\n\nTo reuse configuration from different configuration files, combine and .\n\nIn the following example, a is defined in the file. Then, in the file, refers to the contents of the :\n\nYou can use to merge hashes but not arrays. The algorithm used for merge is “closest scope wins”. When there are duplicate keys, GitLab performs a reverse deep merge based on the keys. Keys from the last member always override anything defined on other levels. For example:\n\nThe result is this job:\n\nIn this example:\n• does not merge, but overwrites . You can use YAML anchors to merge arrays.\n\nUse the custom YAML tag to select keyword configuration from other job sections and reuse it in the current section. Unlike YAML anchors, you can use tags to reuse configuration from included configuration files as well.\n\nIn the following example, a and an from two different locations are reused in the job:\n\nIn the following example, reuses all the variables in , while selects a specific variable and reuses it as a new variable.\n\nThere’s a known issue when using tags with the keyword.\n\nYou can nest tags up to 10 levels deep in , , and sections. Use nested tags to define reusable sections when building more complex scripts. For example:\n\nIn this example, the job runs all three commands.\n\nThe pipeline editor supports tags. However, the schema rules for custom YAML tags like might be treated as invalid by your editor by default. You can configure some editors to accept tags. For example:\n• None In VS Code, you can set to parse in your file:\n• None In Sublime Text, if you are using the package, you can set in your user settings:"
    },
    {
        "link": "https://tinplavec.medium.com/gitlab-ci-cd-best-practices-i-recommend-after-2-years-of-experience-dfc0e349e4d6",
        "document": "I’ve been writing pipelines for GitLab CI/CD for more than 2 years now. Here are the practices I follow, or at least try to. They will help speed up your pipeline and make it more readable and easier to manage.\n\nI’m sure there are number of occasions when you’re writing a new CI/CD job, but you realize you’re missing a package to run some command. You do what’s easiest: add , , or similar.\n\nHowever, this will slow down your pipeline because every time this job is run, it would install all these packages again and again. Instead, look for the image that already has what you need on the Docker Hub, or build your own Docker image.\n\nMost often, we end up building our custom Docker image since it’s hard to find the exact image you need on the Internet, and even if you find it, using it brings security risks. So, we developed a CI/CD pipeline and established a special directory in our Docker repo just for these images. Basically, it’s CI/CD automation for building CI/CD images 😄.\n\nMaybe it doesn’t seem so important at first, but this standardization will increase code readability, especially for complex CI/CD jobs. When checking someone else’s code, you’ll instantly see if a keyword is missing or wrongly defined, and it will be much easier to compare two jobs. A standard practice would be to go alphabetical, but this is not intuitive actually. For example, this way keyword would be at the front, while would be near the end of the job definition. In reality, however, is run first, and then are saved. Therefore, alphabetical keyword order doesn’t look good visually and may confuse developers new to CI/CD. So, after some thought, I developed a more intuitive order based on the chronological order of job execution.\n\nThe keyword is the first one. It gives a clue that the following keywords will override it. Then , and define where and if the job will be scheduled in the pipeline. After the job is scheduled, it is run on the qualified runner with using the specified Docker . The and are then set, and finally is run.\n\nAfter we started using this order, writing and reading jobs became more understandable and easier to explain to juniors.\n\nGitLab offers you a cool feature to include CI/CD configuration from a local file, remote file, another repo, or a template. For example:\n\nHowever, this always includes the latest version of the file on the default branch. At first, this may seem simplest, but it’s not a good practice the same way using to tag Docker images is not a good practice. The second you create another repo that includes this file, you’ll realize you need to version your CI files. Therefore tag the CI repo with and do something like this:\n\nThis way you can easily update by creating a new tag, without breaking repos that include the file. can be tag, branch name or literally commit SHA. If including remote YAML file, you obviously can’t use . In that case you can put file version at the end of file name.\n\nNo matter if your CI/CD pipeline is easy or complex, it’s gonna be much more readable if you write comments. When we started with GitLab CI/CD, we didn’t write any comments. Sooner than later only the author of the code would know what the code does, and if the code was written a while ago, no one could easily understand it. Our pipeline structure is complex with a lot of including, extending, and advance rules to trigger jobs. One simple way to make extending clearer is to write down the origin. Also, be sure to explain rules clauses, and how you use variables.\n\nPerhaps you don’t have to be so thorough as this example is, but still, take some time to write helpful comments, or else you will have to explain your code time and time again to every colleague every time they try to read it. Also, it’s useful to write explanations in job's , just as you would when writing some Python script.\n\nFinally, if your pipeline becomes too complex, it’s helpful to write docs that explain it on high level. There’s never enough of documentation 😅.\n\n5. Using alpine images is not such a benefit actually\n\nIt is often recommended to use small alpine images to speed up your pipeline jobs. While these images will be pulled faster, there are often unexpected errors related to them. If you have a simple application to build, alpine will work, but when you start upgrading your app with complex functionalities, something will likely break and you will have to switch to full image instead of alpine. Also, , alpine’s package manager, is known to drop old versions of packages after some time. The only benefit of using alpine images for CI/CD jobs is faster image download. However, this benefit is not so significant because of 2 reasons. Firstly, the time difference for image download is usually 10 – 20 seconds. GitLab Runners run in the cloud with fast Internet access. So, to download 1 GB more does not take significantly more time. Secondly, GitLab Runners have cache for images, especially if you run your own runners. For example, we run our runners in the Kubernetes cluster where one job is spawned as one pod. When the first job triggers pulling of an image, that image is cached inside the cluster. So, when the job is triggered again, pulling of the image is instant, as the image is not actually pulled from the Internet, but from the cache. Setting up your own GitLab Runners is free and easier than you think, and they can perfectly run on a low-cost server. For these reasons I believe it makes no sense to use alpine images for CI/CD jobs. However, if you’re experienced and know all the problems that could arise when using alpine, it may make sense to you to use it.\n\nSince GitLab 14.2 it is possible to create stageless pipelines. This pipeline actually runs all jobs in one single stage, while the job order is determined based solely on keyword.\n\nThe example above is simple, but it shows how improves your pipeline. If you would use instead, build jobs would be in the first stage, while deploy jobs would be in the second stage. This way needs (no pun intended) to wait for both and to finish. With it only needs to wait for , which makes sense.\n\nFurthermore, you have the ability to use optional . If the needed job exists in the pipeline, main job will wait for it to finish. If the needed job doesn’t exist in the current pipeline, main job will just run immediately.\n\nIf you’re deploying to Kubernetes, like we do, you’ll love GitLab’s agent for Kubernetes. The agent connects GitLab repos with Kubernetes clusters. Setting it up is actually fairly easy. You need to create the agent on GitLab for a project or a group, get the token, and then install the agent with Helm into a Kubernetes cluster. And that’s it. The agent will automatically inject Kubernetes context (kubeconfig) you select to your CI/CD jobs. You can now just simply do and as much as you want.\n\nThere is also another way to use the agent. It’s called GitOps workflow, where you just push manifests to the repo and the agent will automatically scan and deploy them. This workflow, however, requires more setup on the cluster side.\n\nMost of my knowledge of GitLab CI/CD doesn’t come from tutorials, it comes from reading the documentation. Simple tutorials may be good for starters, but if you read the documentation you can truly become an expert in the field. GitLab Keyword reference explains almost all of the pipeline functionalities available. It is frequently updated as new GitLab versions are released. Just try to glance at this doc, I’m sure you’ll find some new cool functionality you were not aware of."
    },
    {
        "link": "https://docs.gitlab.com/ci/pipelines",
        "document": "CI/CD pipelines are the fundamental component of GitLab CI/CD. Pipelines are configured in a file by using YAML keywords.\n\nPipelines can run automatically for specific events, like when pushing to a branch, creating a merge request, or on a schedule. When needed, you can also run pipelines manually.\n• Global YAML keywords that control the overall behavior of the project’s pipelines.\n• Jobs that execute commands to accomplish a task. For example, a job could compile, test, or deploy code. Jobs run independently from each other, and are executed by runners.\n• Stages, which define how to group jobs together. Stages run in sequence, while the jobs in a stage run in parallel. For example, an early stage could have jobs that lint and compile code, while later stages could have jobs that test and deploy code. If all jobs in a stage succeed, the pipeline moves on to the next stage. If any job in a stage fails, the next stage is not (usually) executed and the pipeline ends early.\n\nA small pipeline could consist of three stages, executed in the following order:\n• A stage, with a job called that compiles the project’s code.\n• A stage, with two jobs called and that run various tests on the code. These tests would only run if the job completed successfully.\n• A stage, with a job called . This job would only run if both jobs in the stage started and completed successfully.\n\nTo get started with your first pipeline, see Create and run your first GitLab CI/CD pipeline.\n\nPipelines can be configured in many different ways:\n• Basic pipelines run everything in each stage concurrently, followed by the next stage.\n• Pipelines that use the keyword run based on dependencies between jobs and can run more quickly than basic pipelines.\n• Merge request pipelines run for merge requests only (rather than for every commit).\n• Merged results pipelines are merge request pipelines that act as though the changes from the source branch have already been merged into the target branch.\n• Merge trains use merged results pipelines to queue merges one after the other.\n• Parent-child pipelines break down complex pipelines into one parent pipeline that can trigger multiple child sub-pipelines, which all run in the same project and with the same SHA. This pipeline architecture is commonly used for mono-repos.\n• Multi-project pipelines combine pipelines for different projects together.\n\nPipelines and their component jobs and stages are defined with YAML keywords in the CI/CD pipeline configuration file for each project. When editing CI/CD configuration in GitLab, you should use the pipeline editor.\n\nYou can also configure specific aspects of your pipelines through the GitLab UI:\n\nIf you use VS Code to edit your GitLab CI/CD configuration, the GitLab Workflow extension for VS Code helps you validate your configuration and view your pipeline status.\n\nPipelines can be manually executed, with predefined or manually-specified variables.\n\nYou might do this if the results of a pipeline (for example, a code build) are required outside the standard operation of the pipeline.\n• On the left sidebar, select Search or go to and find your project.\n• In the Run for branch name or tag field, select the branch or tag to run the pipeline for.\n• Enter any CI/CD variables required for the pipeline to run. You can set specific variables to have their values prefilled in the form.\n\nThe pipeline now executes the jobs as configured.\n\nYou can use the and keywords to define pipeline-level (global) variables that are prefilled when running a pipeline manually. Use the description to explain information such as what the variable is used for, and what the acceptable values are.\n\nIn manually-triggered pipelines, the New pipeline page displays all pipeline-level variables that have a defined in the file. The description displays below the variable.\n\nYou can change the prefilled value, which overrides the value for that single pipeline run. Any variables overridden by using this process are expanded and not masked. If you do not define a for the variable in the configuration file, the variable name is still listed, but the value field is blank.\n\nIn this example:\n• is listed in the New pipeline page, but with no value set. The user is expected to define the value each time the pipeline is run manually.\n• is pre-filled in the New pipeline page with as the default value, and the message explains the other options.\n\nYou can define an array of CI/CD variable values the user can select from when running a pipeline manually. These values are in a dropdown list in the New pipeline page. Add the list of value options to and set the default value with . The string in must also be included in the list.\n\nYou can use a query string to pre-populate the New pipeline page. For example, the query string pre-populates the New pipeline page with:\n\nThe format of the URL is:\n\nThe following parameters are supported:\n• : specify the branch to populate the Run for field with.\n\nFor each or , a key and value are required.\n\nManual jobs, allow you to require manual interaction before moving forward in the pipeline.\n\nYou can do this straight from the pipeline graph. Select Run ( ) to execute that particular job.\n\nFor example, your pipeline can start automatically, but require a manual action to deploy to production. In the example below, the stage has a job with a manual action:\n\nIf a stage contains only manual jobs, you can start all the jobs at the same time by selecting Run all manual ( ) above the stage. If the stage contains non-manual jobs, the option is not displayed.\n\nTo push a commit without triggering a pipeline, add or , using any capitalization, to your commit message.\n\nAlternatively, with Git 2.10 or later, use the Git push option. The push option does not skip merge request pipelines.\n\nUsers with the Owner role for a project can delete a pipeline:\n• On the left sidebar, select Search or go to and find your project.\n• Select either the pipeline ID (for example ) or the pipeline status icon (for example Passed) of the pipeline to delete.\n• In the top right of the pipeline details page, select Delete.\n\nDeleting a pipeline does not automatically delete its child pipelines. See issue 39503 for more details.\n\nA strict security model is enforced when pipelines are executed on protected branches.\n\nThe following actions are allowed on protected branches if the user is allowed to merge or push to that specific branch:\n• Run manual pipelines (using the Web UI or pipelines API).\n• Retry or cancel existing jobs (using the Web UI or pipelines API).\n\nVariables marked as protected are accessible to jobs that run in pipelines for protected branches. Only assign users the right to merge to protected branches if they have permission to access sensitive information like deployment credentials and tokens.\n\nRunners marked as protected can run jobs only on protected branches, preventing untrusted code from executing on the protected runner and preserving deployment keys and other credentials from being unintentionally accessed. To ensure that jobs intended to be executed on protected runners do not use regular runners, they must be tagged accordingly.\n\nReview the deployment safety page for additional security recommendations for securing your pipelines.\n\nTrigger a pipeline when an upstream project is rebuilt (deprecated)\n\nYou can set up your project to automatically trigger a pipeline based on tags in a different project. When a new tag pipeline in the subscribed project finishes, it triggers a pipeline on your project’s default branch, regardless of the tag pipeline’s success, failure, or cancellation.\n• The upstream project must be public.\n• The user must have the Developer role in the upstream project.\n\nTo trigger the pipeline when the upstream project is rebuilt:\n• On the left sidebar, select Search or go to and find your project.\n• Enter the project you want to subscribe to, in the format . For example, if the project is , use .\n\nThe maximum number of upstream pipeline subscriptions is 2 by default, for both the upstream and downstream projects. On GitLab Self-Managed, an administrator can change this limit.\n\nThe total running time for a given pipeline excludes:\n• The duration of the initial run for any job that is retried or manually re-run.\n\nThat means that if a job is retried or manually re-run, only the duration of the latest run is included in the total running time.\n\nEach job is represented as a , which consists of:\n\nIn the example:\n• A begins at 0 and ends at 2.\n• A’ begins at 2 and ends at 4.\n• B begins at 1 and ends at 3.\n• C begins at 6 and ends at 7.\n\nVisually, it can be viewed as:\n\nBecause A is retried, we ignore it and count only job A’. The union of B, A’, and C is (1, 4) and (6, 7). Therefore, the total running time is:\n\nTo view all the pipelines that ran for your project:\n• On the left sidebar, select Search or go to and find your project.\n\nYou can filter the Pipelines page by:\n\nSelect Pipeline ID in the dropdown list in the top right to display the pipeline IDs (unique ID across the instance). Select pipeline IID to display the pipeline IIDs (internal ID, unique across the project only).\n\nTo view the pipelines that relate to a specific merge request, go to the Pipelines tab in the merge request.\n\nSelect a pipeline to open the pipeline details page which shows every job in the pipeline. From this page you can cancel a running pipeline, retry failed jobs, or delete a pipeline.\n\nThe pipeline details page displays a graph of all the jobs in the pipeline:\n\nYou can use a standard URL to access the details for specific pipelines:\n• : The details page for the latest pipeline for the most recent commit on the default branch in the project.\n• : The details page for the latest pipeline for the most recent commit on branch in the project.\n\nWhen you configure jobs with the keyword, you have two options for how to group the jobs in the pipeline details page. To group the jobs by stage configuration, select stage in the Group jobs by section:\n\nTo group the jobs by configuration, select Job dependencies. You can optionally select Show dependencies to render lines between dependent jobs.\n\nJobs in the leftmost column run first, and jobs that depend on them are grouped in the next columns. In this example:\n• is configured with and depends on no jobs, so it displays in the first column, despite being in the stage.\n• depends on , and depends on both and , so both test jobs display in the second column.\n• Both jobs depend on jobs in second column (which themselves depend on other earlier jobs), so the deploy jobs display in the third column.\n\nWhen you hover over a job in the Job dependencies view, every job that must run before the selected job is highlighted:\n\nPipeline mini graphs take less space and can tell you at a quick glance if all jobs passed or something failed. They show all related jobs for a single commit and the net result of each stage of your pipeline. You can quickly see what failed and fix it.\n\nThe pipeline mini graph always group jobs by stage, and display throughout GitLab when displaying pipeline or commit details.\n\nStages in pipeline mini graphs are expandable. Hover your mouse over each stage to see the name and status, and select a stage to expand its jobs list.\n\nWhen a pipeline contains a job that triggers a downstream pipeline, you can see the downstream pipeline in the pipeline details view and mini graphs.\n\nIn the pipeline details view, a card displays for every triggered downstream pipeline on the right of the pipeline graph. Hover over a card to see which job triggered the downstream pipeline. Select a card to display the downstream pipeline to the right of the pipeline graph.\n\nIn the pipeline mini graph, the status of every triggered downstream pipeline displays as additional status icons to the right of the mini graph. Select a downstream pipeline status icon to go to the detail page of that downstream pipeline.\n\nPipeline analytics are available on the CI/CD Analytics page.\n\nPipeline status and test coverage report badges are available and configurable for each project. For information on adding pipeline badges to projects, see Pipeline badges.\n• Perform basic functions. For more information, see Pipelines API.\n• Maintain pipeline schedules. For more information, see Pipeline schedules API.\n• Trigger pipeline runs. For more information, see:\n\nWhen a runner picks a pipeline job, GitLab provides that job’s metadata. This includes the Git refspecs, which indicate which ref (such as branch or tag) and commit (SHA1) are checked out from your project repository.\n\nThis table lists the refspecs injected for each pipeline type:\n\nThe refs and exist in your project repository. GitLab generates the special ref during a running pipeline job. This ref can be created even after the associated branch or tag has been deleted. It’s therefore useful in some features such as automatically stopping an environment, and merge trains that might run pipelines after branch deletion.\n\nWhen a user deletes their GitLab.com account, the deletion does not occur for seven days. During this period, any pipeline subscriptions created by that user continue to run with the user’s original permissions. To prevent unauthorized pipeline executions, immediately update pipeline subscription settings for the deleted user."
    }
]