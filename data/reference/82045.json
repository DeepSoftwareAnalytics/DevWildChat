[
    {
        "link": "https://medium.com/ms-club-of-sliit/spring-boot-microservices-best-practices-and-coding-style-guidelines-d48aa371b75e",
        "document": "Microservices architecture has gained significant popularity due to its ability to build scalable and modular applications. Spring Boot, a powerful Java framework, provides a comprehensive set of tools and features to develop microservices. However, to ensure the effectiveness and maintainability of your Spring Boot microservices, it’s essential to follow best practices and adhere to coding style guidelines. In this article, we will discuss some key best practices and coding style guidelines for Spring Boot microservices development.\n\nThe Single Responsibility Principle (SRP) and modularization are essential components of creating Spring Boot microservices. Let’s explore these ideas in more detail:\n\nBreaking down your application into smaller, independent modules or microservices is known as modularization. Each microservice ought to be able to run independently and have a clearly defined role. This strategy has various advantages, such as:\n\n· Scalability: Modularization enables you to grow individual microservices in accordance with their unique needs, improving resource allocation and performance.\n\n· Maintainability: It is simpler to comprehend, maintain, and troubleshoot smaller, more focused microservices. Developers can individually work on several microservices without affecting others.\n\n· Reusability: Well-designed microservices can be joined to provide new functionality or reused across several applications.\n\nAccording to the Single Responsibility Principle (SRP), a class or component should only have one reason to change. It implies that each microservice should have a particular goal or set of capabilities when applied to microservices. This idea encourages reusability, maintainability, and loose coupling. Following SRP has several advantages, including:\n\nmore logical code organization A single responsibility makes the code more concentrated and simpler to comprehend, which reduces complexity.\n\nEasy testing: Since you can test each microservice independently using fewer, more focused microservices, unit testing becomes more manageable.\n\nIndependence: Microservices adhering to SRP are flexible and agile because they may be developed, deployed, and scaled separately.\n\nConsider using the following procedures in your Spring Boot microservices to successfully integrate modularization and SRP:\n\n· Based on each microservice’s responsibilities and features, identify, and define distinct boundaries.\n\n· Make sure that each microservice can store its own data, handle its own business logic, and interact with other microservices.\n\n· Align microservices with certain business domains by using domain-driven design (DDD) approaches.\n\n· Use separation of concerns to make sure that each microservice handles a certain piece of the functioning of the entire program.\n\n· You can make your Spring Boot microservices more scalable, manageable, and flexible by embracing modularization and SRP. This will allow for better development procedures and effective team cooperation.\n\n- config: spring Configuration classes for all the modules\n\n- dapr-components: dapr components file, such as for pushhubs, if you are using dapr for your app.\n\n- persistence: entities and responsibilities for the database.\n\nIt is recommended to follow the following package structure within the module,\n\n- src/main/java — Contains packages and classes of Java source code.\n\n- src/test/resources — Contains non-Java resources like property files and Spring configuration.\n\nUse the customary packet sparingly. Make sure that everything is contained in a package with a meaningful name, including the entry point. This prevents wiring and component scan-related surprises.\n\n· To make it simpler to upgrade and test newer versions, including the version numbers of any third-party dependent libraries that are NOT included in the Spring Boot BOM in the pom file’s properties/> section.\n\n· DO list the versions of each plugin.\n\n· Version numbers of the dependent libraries that are included in the Spring Boot bill of materials should NOT be specified.\n\n· DON’T mix versions of libraries that are transitively dependent and libraries that are directly dependent in the same project. As an illustration, slf4j 1.5 and slf4j 1.6 are incompatible, hence we must prevent the project from compiling with mixed dependency versions. Running mvn dependency: tree to find incompatible versions of dependent libraries is one approach to be certain.\n\nHowever, it is crucial to follow industry best practices to fully utilize the capabilities of Spring Boot and guarantee the success of your projects. Following these guidelines boosts overall development productivity while also enhancing the maintainability and scalability of your apps.\n\nWe are able to manage these external dependencies thanks to the numerous open-source projects that the Spring Boot project itself utilizes and integrates. However, some are not utilized in the project itself, necessitating the maintenance of the project version. It will be very difficult to sustain a big project with many unfinished modules. How do you do it? Spring IO Platform accomplishes that. It supports additional external open-source libraries while being a part of the Spring Boot project itself. To develop our own basic project platform-BOM, we might take inspiration from the Spring IO Platform. All business module projects should be presented as BOMs. As a result, you simply need to update the dependence’s version when upgrading a third-party dependency.\n\nThe application of auto-configuration is a key component of Spring Boot. This aspect of Spring Boot makes your code simpler and more functional. When a particular jar file is found on the classpath, autoconfiguration is initiated.\n\nTo use it, you should rely on Spring Boot Starters. Therefore, you may start by including: if you want to integrate with Redis.\n\nIf you want to integrate with MongoDB, you need this:\n\nThese tedious configurations are properly integrated and function as a unit with the aid of these starters, and they have all been tried and tested. This will help you stay out of the dreaded Jar hell.\n\no Use Spring initializer to start a new Spring boot project.\n\nA new Spring Boot project can be easily created with the help of Spring Initializer, which can also load any necessary dependencies ( https://start.spring.io/ ).\n\nYou can be sure that the dependencies you receive when building an application using Initializer are tested and proven to operate with Spring auto-configuration. Even new integrations that you may not have known about could be found.\n\nAlthough you have a lot of freedom in how you build your source code organization, there are some fundamental guidelines to keep in mind.\n\nStay away from default packages. To prevent surprises during component scanning and assembly, make sure everything is packaged clearly, including your entrance point.\n\nKeep the application’s entry class, Application.java, in the top-level source directory.\n\nAlthough it is not required, I advise placing controllers and services in function-oriented modules. Several excellent developers advise combining all the controls. Always keep to one style!\n\no Ensure that the database is independent of the main business logic.\n\nBefore, I wasn’t sure how to handle database interactions in Spring Boot in the best way possible. It became much more evident to me after reading “Clear Architecture” by Robert C. Martin.\n\nYou want to keep the service and your database logic distinct. In an ideal world, you wouldn’t want the service to know which database it’s communicating with, therefore object persistence needs to be abstracted.\n\n“Robert C. Martin strongly states that your database is a “detail”, which means not coupling your application to a specific database. Few people would switch databases in the past, and I’ve noticed that using Spring Boot and modern microservice development makes things much faster.”\n\no Keep business logic out of spring boot code.\n\nYou should safeguard your business logic by considering the principles learned from “Clear Architecture”. It’s incredibly tempting to combine different types of Spring Boot code. Avoid doing it. Your business logic will remain reusable if you can resist the urge.\n\nUsually, the service includes a library. without having to remove many Spring annotations from your code, and easier to design.\n\nUtilizing constructor injection is one method to protect your business logic from Spring Boot code. The @Autowired annotation on the constructor is not only optional, but it also makes it simple to instantiate beans without Spring.\n\nYou need to handle exceptions in a consistent manner. Spring Boot offers two primary methods:\n\n2. The @ExceptionHandler annotation, which may be helpful in some cases, can also be added to the controller.\n\nYou presumably already know this, but instead of manually using System.out.println() for logging, you should be utilizing Logger. With little to no setting, this is simple to accomplish with Spring Boot. Obtain only the class’s logger instance:\n\nThis is crucial since it enables you to specify various logging levels in accordance with your requirements.\n\nYou’ve probably heard of the Lombok project if you’re a Java developer.\n\nWith the help of its annotations, the Java library Lombok can help you write cleaner, shorter code.\n\nFor example, in some classes like entities, request/response objects, dtos, etc., you may need a lot of lines for getters and setters.\n\nHowever, if you use Lombok, it only takes one line, and you may use @Data, @Getter, or @Setter depending on your needs.\n\nAnnotations from the Lombok logger are also available. @Slf4j is advised.\n\nTest your code! Although this is not exclusive to Spring Boot, it must be emphasized. You write legacy code from the beginning if you don’t write tests.\n\nYour codebase will become dangerous to change if someone else uses it. When several services are interdependent, this can become extremely riskier.\n\nSince there are Spring Boot best practices, you should think about adopting Spring Cloud Contract for your consumer-driven contracts because it will facilitate your service integrations with other platforms.\n\nWriting Spring-based microservices has never been simpler thanks to Spring Boot. I hope that by using these best practices, your implementation process will eventually become more robust and successful in addition to being faster."
    },
    {
        "link": "https://github.com/abhisheksr01/spring-boot-microservice-best-practices",
        "document": "\n• What to expect Next!\n\nThis repository combines the best practices and essential integrations for building robust Spring Boot-based microservices, all in one place. It serves as a template, allowing developers to easily create their own microservices by adding or removing dependencies based on their needs.\n\nIn the sections below, I'll walk you through the various integrations included in the project and guide you on how to use them effectively.\n\nCurrently, the microservice exposes a simple GET API that accepts a company reference as a path parameter. It then queries the Companies House API to retrieve and return detailed company information.\n• You must have >= Java 21 Installed. You can use SDKMAN for maintaining different JDK's in your system.\n• You should register for a free Companies House account and then generate a REST API key. This is required for invoking the GET API exposed by this service else the API invocation will fail at runtime with auth error.\n• Replace the in the application.yaml under main dir with the newly created API Key.\n\nLet us get started by Cloning or Downloading repository in your local workstation.\n\nOnce cloned/downloaded import the project in your favourite IDE (IntelliJ, Eclipse etc.).\n\nWe are using Gradle Wrapper for dependency management so that you do not need to explicitly configure Gradle or Maven.\n\nExecute below gradlew command to download all the dependencies specified in the gradle.build.\n\nWe are following Classic Microservice \"Separation of Concerns\" pattern having Controller <--> Service <--> Connector layers.\n\nThe three different takes the responsibilities as below:\n• Controller: Controller layer allows access and handles requests coming from the client.\n\n Then invoke a business class to process business-related tasks and then finally respond.\n\n Additionally Controller may take responsibility of validating the incoming request Payload thus ensuring that any invalid or malicious data do not pass this layer.\n• Service: The business logic is implemented within this layer, thus keeping the logic separate and secure from the controller layer. This layer may further call a Connector or Repository/DAO layer to get the Data to process and act accordingly.\n• Connector/Repository: The only responsibility of this layer is to fetch data which is required by the Service layer to perform the business logic to serve the request.\n\n When our Microservice makes a call to another Service we would like to name it as Connector (as in our case) layer whereas when interacting with a DB commonly it's known as Repository.\n\nAt the core of the Cloud Native Practices in Software Engineering lies the Behavior Driven Development(BDD) and Test-Driven Development (TDD).\n\n While developing the code I followed BDD first approach where I wrote a failing feature/acceptance criteria thus driving our development through behavior and then followed by Test Driven Development.\n\n A feature is not considered as developed until all the Unit Tests (TDD) and feature (BDD) passes.\n\nWe are using JUnit 5 for running our unit test cases.\n\nOnce executed a report as below will be generated at local path\n\nWe are using one of the most famous BDD implementation i.e., Cucumber.\n\nOpen Class in package and execute CucumberTest from the class.\n\nOnce the test execution completes you can see the Cucumber Test Report at :\n\nPitest is used for performing mutation testing. To execute the mutation test run :\n\nonce the test execution completes report should be accessible at:\n\nThe application can be started in the IDE through CompaniesHouseApplication.java or by executing below command in the terminal\n\nOnce the application starts the application can be accessed through below URL:\n\nIf valid Rest API Key is added in then we should receive all the companies details that has in their registered title\n\nAn excellent* library for converting VO to DAO objects and vice versa.\n\nProvides excellent annotations based support for Auto generation of methods, logging, Builders, Validation etc. We will be using below annotations during this exercise:\n\n @Data: Auto generates setters, getters, hashcode and toString methods\n\n @Slf4j: Just add this annotation on top of any Spring Bean and start using the log\n\nClick here to see implementation.\n\nCheckstyle is a static code analysis tool used in software development for checking if Java source code complies with coding rules.\n\nCode coverage is a preliminary step to know whether our test covers all the scenarios we have developed so far.\n\nJacoco is a free Java code coverage library distributed under the Eclipse Public License.\n\nA smarter Dockerfile linter that helps you build best practice Docker images.\n\nWith the latest version of SpringDoc-API you just need to include a single dependency as below & that's it.\n\nTo test it locally start the application then Swagger UI documentation can be accessed by URL:\n\nOnce application is deployed in a Platform the same documentation will be accessible by below URL:\n\nwhere \"companieshouse\" is the context path.\n\nDevSecOps stands for development, security, and operations. It's an approach to culture, automation, and platform design that integrates security as a shared responsibility throughout the entire application and infrastructure security lifecycle.\n\nIn this repo we are looking at some of the key practices to secure the application and infrastructure.\n\nIntroduction:\n\n A vulnerability is a hole or a weakness in the application, which can be a design flaw or an implementation bug, that allows an attacker to cause harm to the stakeholders of an application.\n\nIn this section we are focusing on identifying vulnerabilities within dependencies used in the code base.\n\nDocker image security scanning should be a core part of your Docker security strategy.\n\nAlthough image scanning won't protect you from all possible security vulnerabilities, it's the primary means of defense against security flaws or insecure code within container images. It's therefore a foundational part of overall Docker security.\n\nTo learn how to containerize application click here.\n\nStatic analysis can be conducted on the IAC, just like it can be done on any other software programming language.\n\nThe IAC static analysis can be done on configuration and code such as:\n• Dockerfile: It is possible to scan a Dockerfile to determine if the file contains a potential vulnerability, such as the use of root, a vulnerable base docker image, etc.\n• Kubernetes Configuration: Helm charts or standard k8s YAML configurations can pose security risks, exposing internal information such as NGINX Ingress through annotations, running containers with allowPrivilegeEscalation etc.\n• Terraform: For deploying Cloud Infrastructure, it is the most commonly used form of IAC. As a result, there is an increased risk of introducing vulnerabilities into the cloud infrastructure. Keeping the infrastructure secure is crucial since it sits at the bottom of the application architecture.\n• Easy to integrate with other tooling such as CI/CD pipeline, Monitoring tool, Repositories etc.\n• Developer Adaptability and ease of use: The tool should be easy to use and can educate on CVE's identified.\n• Less Noise and efficient mechanism to control it\n• Relevancy to the solution we are developing\n\nA penetration test, colloquially known as a pen test, pentest or ethical hacking, is an authorized simulated cyberattack on a computer system, performed to evaluate the security of the system. Not to be confused with a vulnerability assessment.\n\nContinuous Integration: It's a software development practise where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day.\n\n Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible.\n\nContinuous Delivery: It's software engineering approach in which teams produce software in short cycles, ensuring that the software can be reliably released at any time and, when releasing the software, doing so manually.\n\nContinuous Deployment: It means that every change goes through the pipeline and automatically gets put into production, resulting in many production deployments every day.\n\n To do Continuous Deployment you must be doing Continuous Delivery.\n\nPictorial representation of the above two approaches:\n\nNow let us look at the key building blocks for achieving CI/CD.\n\nContainerization is the process of distributing and deploying applications by packaging the application components & its dependencies into a standardized, isolated, lightweight process environments called containers.\n\nDocker is the widely adapted Containerization platform developed to simplify and standardize deployment in various such environments using a specially packaged files known as Docker Images.\n\nBefore we continue further make sure you have docker installed, if not click here to see the instructions.\n\nIt uses an intermediate gradle:8.5-jdk21 container for building an executable jar and then openjdk:21-slim as a base image by copying the jar for application docker image.\n\n This Dockerfile will be handy and is an example of using intermediate containers when we do not have respective runtime (JAVA) & package manager (gradle) installed locally for building the executable application.\n\nTypically, in a Pipeline we would build an executable jar as part of initial task and store it as an artifact. Once the code meets all the quality requirements instead of rebuilding the jar this Dockerfile will copy the existing one from the specified path while creating the image.\n\nYou must have Java installed in your workstation if not click here to download.\n\nNow let us focus on distribution aspect of docker, So far we have learnt how to create docker image & its basic lifecycle but how we can share it with other developers or use in the application deployment.\n\nFor this purpose we will be using Docker Hub which is the free Docker Registry for storing the docker images.\n• CircleCI is a Software as a Service (SaaS) CI/CD pipeline tool chain in the DevOps world.\n• Personally I found it quite easy to integrate with the GitHub repo and start building my CI/CD pipeline.\n• CircleCI runs each job in a separate container or VM. That is, each time your job runs CircleCI spins up a container or VM to run the job in.\n• At the time of writing this article CircleCI's free-tier provides 2500 free credits to run GitHub & Bit Bucket repositories based pipelines.\n• It is highly recommended to go through CircleCI's official Getting Started. \n\n To start using CircleCI follow below steps:\n• Create a folder .circle and a pipeline config file inside it as config.yml (beware the extension should strictly be .yml rather than .yaml, I hope CircleCI fixes this in upcoming releases).\n• \n• Jobs: Where individual stages in our pipeline is executed.\n• Steps: A collection of executable commands which are run during a job.\n• Workflows: It orchestrates the jobs as a pipeline.\n• Executors: Underlying technology or environment in which to run a job. Example: Docker images, Linux virtual machine (VM) image, macOS VM image, Windows VM image\n• Orbs: Reusable & Shareable packages of configs which can be imported to ease the pipeline configurable.\n• Click here to open the CircleCI config file for this project. When this config runs for the \"workflow-all-jobs\" the output pipeline is shown below and deploys the app to AWS EKS Cluster.\n• If you wish to use this config file in your project you must create a context \"credentials\" and add below Environment Variables with respective values. Follow link to learn how to do it. AWS_ACCESS_KEY_ID AWS_DEFAULT_REGION AWS_SECRET_ACCESS_KEY DOCKER_USER (Your docker username) DOCKER_PASS (Your docker user EKS_NAMESPACE (Kubernetes namespace to which the aws user has access to and you would like to deploy)\n• Scheduled Workflows: CircleCI supports scheduled execution of the workflow, look for scheduled-vulnerability-check in the config.yml.\n\n Here I am checking for vulnerabilities within my code libs and docker images at scheduled intervals. \n\nWhat I like about this feature is we do not need to create a separate pipeline file or steps to run the jobs.\n• Built on the simple mechanics of resources, tasks, and jobs, Concourse presents a general approach to automation that makes it great for CI/CD. \n\n Let us quickly spin a concourse pipeline locally and before proceeding make sure you have Docker installed. From the root of the project change the directory: Access the local concourse using the URL http://127.0.0.1:8080/ and then download FLY CLI utility from the concourse home page. Update the credentials in the secrets/vars.yml Now execute below command to configure the pipeline job. Authenticate the pipeline by clicking link highlighted in CLI with USERNAME/PASSWORD as admin/admin. Then approve the pipeline configuration by typing \"y\" and hit enter. By default, all the pipelines in concourse are paused, execute below command to un pause the same. The pipeline can be accessed using the URL http://127.0.0.1:8080/teams/main/pipelines/spring-boot-best-practices\n• Jenkins is one of the most widely used CI/CD Build Tool preferred by many Organizations especially within Enterprises.\n• Jenkins Image (Recommended): If you have docker installed, the easiest way to get started is with Jenkins public image. Execute below command to start jenkins at http://localhost:9090 Get the jenkins container name: After replacing the jenkins container name execute below command to get the default admin password: [!NOTE] Jenkins by default starts at port 8080 & our spring boot application also uses the same port.\n\n Hence to avoid the Binding Exception we are using an environment variable -e JENKINS_OPTS=\"--httpPort=9090\". Access local Jenkins at http://localhost:9090/ and install all the recommended plugins. To learn more about this approach click here.\n• Jenkins War: Follow the instructions to install the Jenkins and run on a Web Server. Click here to see implementation.\n• Cloud Build is a service that executes your builds on Google Cloud Platform infrastructure.\n• Cloud Build can import source code from Google Cloud Storage, Cloud Source Repositories, GitHub, or Bitbucket, execute a build to your specifications, and produce artifacts such as Docker containers or Java archives. \n\n Cloud Build executes your build as a series of build steps, where each build step is run in a Docker container. We can either trigger Cloud Build through CLI or by declaring the steps in . Click here to see implementation. To perform this exercise you must have signed for Google Cloud Platform account and gcloud SDK configured. You can register here for GCP Free Tier. We are going to deploy our Microservice container to Cloud Run (Fully Managed through Cloud Build). Execute below command to use local to perform steps\n\nAs software engineering continues to evolve, we do too.\n\nAll planned tasks for this repository are tracked in the GitHub Issues section.\n\nIf you'd like to propose new work or enhancements for this repository, please create a New Issue using the appropriate issue type.\n\nWe utilize Cocogitto to automate the process of creating Semantic SemVer GitHub releases for this repository.\n\nFor information on the latest releases, please visit the Releases section of the repository.\n\nThe release process is automated through GitHub Actions (GHA CI), following Cocogitto's conventional commit types.\n\nThis project is licensed under the MIT License - see the LICENSE file for details"
    },
    {
        "link": "https://dzone.com/articles/implementation-best-practices-microservice-api-wit",
        "document": "First, let's turn to the architecture, which will be explained in detail. Let's look at each of these tiers in detail.\n\nLet me explain the architecture in detail. These components are commonly associated with the architecture of applications that follow the principles of Domain-Driven Design (DDD) and Model-View-Controller (MVC) or similar architectural patterns. Let me cover this one by one:\n\nEntities represent the core business objects or concepts in your application. They encapsulate data related to the business domain. For example, in an Employee Management System, an employee entity might have attributes like name, email, and salary related to an employee.\n\nRepositories are responsible for handling the data access logic. They provide an abstraction over the data storage, allowing the application to interact with the data without worrying about the underlying storage details. For example, an EmployeeRepository would handle operations like storing, retrieving, updating, and deleting employee records in the database.\n\nServices contain business logic that doesn't naturally fit within the methods of an entity. They orchestrate interactions between entities and repositories to fulfill higher-level use cases. For example, an EmployeeService might have methods to calculate bonuses, process employee transfers, or handle complex business rules involving multiple entities.\n\nMappers are responsible for transforming data between different layers of the application. They convert data from database entities to domain objects and vice versa. For example, an EmployeeMapper might convert an Employee entity to a data transfer object (EmployeeRequest) that can be sent over the network or used by the presentation layer.\n\nControllers handle incoming requests from the user interface or external systems. They interpret user input, invoke the necessary services or business logic, and prepare the response to be sent back. In a web application, a controller receives an HTTP request, extracts the necessary data, and delegates the request to the appropriate service. It then formats the service response and sends it back to the client.\n\nFrontend: You have the option of building Native Apps like Android and iOS. Desktop browser apps or mobile browser apps can be built using React or Angular frameworks.\n\nBest Practices for Implementation of the Architecture\n• Name the package as “entities” under the feature name\n• Set id as Long and generation type as identity\n• Name the class and table in plurals like users\n• Use Lombok for constructor and getter/setter code\n• Have a length for every String field\n• Use references to other tables like . Remember, the table created is automatic, and what you write in the entity matters.\n• Use bidirectional if you wish to save the values in multiple tables in one call.\n• Use to join tables. Create a separate Join class if there are fields in the join table apart from join id columns.\n• Identify the right inheritance type for is-a relationship. Pick between single table, class table, and concrete table inheritance based on the number of fields in every class.\n• Name the package as “repositories” under the feature name\n• Extend the with the entity name and id as Long\n• As much as possible, use the style as a method for querying the entity like\n• Use batch operations for multiple entries to the database, like\n• Use Optional for the return type as much as possible\n• Name the package as “services” under the feature name\n• Create an interface for all the operations within the service and create an implementation class\n• Accept the Request object and return the Response object from the model’s package.\n• If multiple repositories need to be called, it should be called in a transaction unless you wish to start a new transaction.\n• If you wish to call multiple services, the particular service has to be named as an aggregate service and within a transaction.\n• Do not return the ResponseEntity from the service; it is the job of the controller tier.\n• Name the package as “mappers” under the feature name\n• Create an interface called Mapper using Generics and convert the entity to a model and vice versa using this mapper\n• Do not use the entity as a return object in the controller tier\n• Name the package as “models” under the feature name\n• All requests and response objects will be stored here\n• Use annotation for the model classes\n• The model should act as a frontend for the API, and the service should convert the model to an entity for talking to the repository.\n• Name the package as “controllers” under the feature name\n• Try to create an API for every resource under a bounded context\n• Make the resource names in plural, like /API/users\n• For CRUD Operations:\n• Use HTTP POST for create an operation with Request as the body\n• Use HTTP GET for retrieve all records\n• Use HTTP GET with /{id} for retrieve with an identifier\n• Use HTTP DELETE with /{id} to delete the record\n• For operation other, the CRUD try avoiding a verb as much as possible\n• Implement validation at the controller tier with @Valid\n• Realize the difference between API thinking and RPC thinking. It is key to understanding APIs.\n\nIn a long-term vision, as and when the code base becomes bulky, you might need to use the Strangler pattern to take out some of the services and deploy them as a separate microservice. This kind of coding structure will help then. If you get your basics right from the very beginning, then later on, the ride will be smooth."
    },
    {
        "link": "https://medium.com/@rocky.bhatia86/microservices-best-practices-ccc6706f46c1",
        "document": "Micro services have become a buzzword in the world of software development. But what exactly are micro services? Simply put, micro services are a software architectural style that structures an application as a collection of loosely coupled services. Each service is designed to execute a specific business function, which can be developed, deployed, and maintained independently.\n\nIn today’s fast-paced tech environment, the ability to rapidly develop and scale applications is crucial. Microservices allow teams to break down complex systems into manageable, independent components, making it easier to build and maintain applications that are scalable, resilient, and easier to manage.\n\nMicro services bring a plethora of advantages to the table:\n\n- Scalability: Since each microservice operates independently, scaling individual components of an application becomes more straightforward.\n\n- Flexibility: Different microservices can be developed using different technologies, enabling teams to choose the best tools for each task.\n\n- Resilience: If one microservice fails, it doesn’t bring down the entire system, thus enhancing the overall system’s resilience.\n\nThe Single Responsibility Principle (SRP) is a key tenet in microservices architecture. Each microservice should be responsible for a single piece of functionality or a specific domain within the application. This principle simplifies development and maintenance because each microservice is easier to understand, test, and deploy.\n\nBy adhering to the SRP, you minimize the risk of creating a monolithic service disguised as microservices. This approach ensures that each microservice does one thing and does it well, which reduces dependencies and fosters independent development and scaling.\n\nOne of the golden rules in microservices architecture is that each microservice should have its own data store. This isolation means that each service can evolve independently, without being tied to a shared database schema that could limit flexibility.\n\n- NoSQL Databases: Useful for services dealing with large volumes of unstructured data.\n\nThis separation allows each service to use the most appropriate data storage technology for its needs, enhancing performance and scalability.\n\nIn a microservices architecture, communication between services is vital. Asynchronous communication is a preferred method because it decouples services, making them more resilient and fault-tolerant. Unlike synchronous communication, where services wait for responses, asynchronous communication allows services to continue processing other tasks, thus enhancing the overall efficiency.\n\nUsing these tools, microservices can communicate without depending on the availability or speed of other services, leading to more robust and scalable applications.\n\nContainerization is the practice of bundling a microservice along with its dependencies into a single package, which can run consistently across different environments. Docker is the most popular containerization technology, enabling developers to create lightweight, portable, and self-sufficient containers.\n\nBenefits of Using Docker for Microservices\n\n- Consistency: Ensures that a microservice runs the same way across development, testing, and production environments.\n\n- Isolation: Keeps each microservice isolated, reducing the risk of conflicts and simplifying deployment.\n\n- Scalability: Makes it easier to scale microservices by replicating containers across multiple instances.\n\nWhile Docker handles the creation of containers, Kubernetes takes care of orchestrating them. Orchestration involves managing, scaling, and deploying containers in a way that optimizes resource use and ensures reliability.\n\nKubernetes automates the deployment, scaling, and operations of containers, making it easier to manage microservices at scale. It provides features like load balancing, automatic scaling, and self-healing, ensuring that microservices run smoothly even under heavy loads.\n\nIn a microservices architecture, it’s crucial to separate the build and deploy processes. The build process should produce a deployable artifact, like a Docker image, which can then be deployed across different environments without modification. This separation ensures consistency and reduces the chances of deployment issues caused by environment-specific configurations.\n\nTools like Jenkins, GitLab CI/CD, and CircleCI can be used to automate the build process, creating artifacts that can be reliably deployed in staging, testing, and production environments.\n\nDomain-Driven Design (DDD) is a strategic approach to software development that emphasises defining software architecture based on the business domains it serves. In the context of microservices, DDD helps in defining the boundaries of each microservice, ensuring that each service aligns with a specific business capability.\n\nBy applying DDD, you can create microservices that are more cohesive and aligned with business needs. This approach also facilitates better communication between development teams and business stakeholders, leading to more effective and efficient software development.\n\nStatelessness is a principle that states microservices should not retain any session state between requests. Instead, any necessary state should be stored in a database or an external data store. Stateless microservices are easier to scale because they don’t need to worry about replicating session data across multiple instances.\n\nWhen state management is unavoidable, consider using distributed caching solutions like Redis or Memcached to store session data, ensuring it’s accessible across different instances of a microservice.\n\nMicro frontends extend the concept of microservices to the frontend of web applications. Instead of building a monolithic frontend, the application is broken down into smaller, independently deployable components that can be developed and maintained by separate teams.\n\nIsolation: Each micro frontend should be isolated to avoid conflicts.\n\nIndependent Deployment: Allow each micro frontend to be deployed independently, reducing the risk of system-wide failures.\n\nMonitoring and observability are critical in a microservices architecture, where multiple independent services interact with each other. Effective monitoring allows you to detect and diagnose issues quickly, ensuring the system remains reliable and performant.\n\nGrafana: A visualisation tool that works well with Prometheus.\n\nImplementing observability involves collecting and analysing data from logs, metrics, and traces to gain insights into the system’s performance and behaviour.\n\nSecurity is a significant concern in microservices architectures due to the increased number of services and their interactions. Each microservice needs to be secured individually, with proper authentication, authorization, and encryption mechanisms.\n\nAPI Gateway: Use an API Gateway to manage authentication, rate limiting, and security policies.\n\nEncryption: Ensure data in transit is encrypted using TLS/SSL.\n\nAutomated testing is essential in microservices architectures, where changes to one service can impact others. By automating tests, you can quickly identify issues and ensure that your microservices work correctly before they reach production.\n\nVersioning is crucial in a microservices architecture, where multiple versions of a service may need to coexist. Proper versioning strategies help manage backward compatibility and ensure that updates don’t break the system.\n\nURI Versioning: Include the version in the API endpoint (e.g., /api/v1/).\n\nHeader Versioning: Specify the version in the HTTP headers.\n\nContent Negotiation: Allow clients to request specific versions through content negotiation.\n\nDocumentation plays a vital role in microservices development, providing clear guidelines for developers, testers, and operations teams. Comprehensive documentation helps ensure that everyone involved in the project understands how each microservice works and how they interact.\n\nBest Practices for Creating Useful Documentation\n\nKeep It Updated: Regularly update documentation to reflect changes in the codebase.\n\nUse Clear and Concise Language: Make it easy to understand.\n\nInclude Examples: Provide examples and use cases to illustrate concepts.\n\nIn conclusion, microservices offer a powerful approach to building scalable, maintainable, and flexible applications. By following best practices like adhering to the Single Responsibility Principle, using separate data stores, embracing containerization, and implementing robust security measures, you can ensure that your microservices architecture is resilient, efficient, and ready to meet the demands of modern software development.\n\nWhat are the key advantages of using microservices?\n\nMicroservices offer scalability, flexibility, and resilience, allowing for independent development and deployment of application components.\n\nHow does asynchronous communication differ from synchronous communication?\n\nAsynchronous communication allows services to interact without waiting for immediate responses, enhancing system resilience and efficiency.\n\nWhy is containerization crucial for microservices?\n\nContainerization ensures consistency across environments, simplifies deployment, and enhances scalability by isolating microservices in portable containers.\n\nWhat is the significance of Domain-Driven Design in microservices?\n\nDomain-Driven Design helps define clear boundaries for microservices, ensuring they align with specific business capabilities and improving maintainability.\n\nHow can I ensure the security of my microservices architecture?\n\nImplement security best practices like using an API Gateway, token-based authentication, and encryption to protect your microservices architecture."
    },
    {
        "link": "https://piotrminkowski.com/2019/12/06/spring-boot-best-practices-for-microservices",
        "document": "In this article I’m going to propose my list of “golden rules” for building Spring Boot applications, which are a part of a microservices-based system. I’m basing on my experience in migrating monolithic SOAP applications running on JEE servers into REST-based small applications built on top of Spring Boot. This list of Spring Boot best practices assumes you are running many microservices on the production under huge incoming traffic. Let’s begin.\n\nIt is just amazing how metrics visualization can change an approach to the systems monitoring in the organization. After setting up monitoring in Grafana we are able to recognize more than 90% of bigger problems in our systems before they are reported by customers to our support team. Thanks to those two monitors with plenty of diagrams and alerts we may react much faster than earlier. If you have microservices-based architecture metrics become even more important than for monoliths.\n\n The good news for us is that Spring Boot comes with a built-in mechanism for collecting the most important metrics. In fact, we just need to set some configuration properties to expose a predefined set of metrics provided by the Spring Boot Actuator. To use it we need to include Actuator starter as dependency:\n\nTo enable metrics endpoint we have to set property to . Now you may check out the full list of generated metrics by calling endpoint . One of the most important metrics for us is , which provides statistics with the number of incoming requests and response time. It is automatically tagged with method type (POST, GET, etc.), HTTP status, and URI.\n\n Metrics have to be stored somewhere. The most popular tools for that are InfluxDB and Prometheus. They are representing two different models of collecting data. Prometheus periodically retrieves data from the endpoint exposed by the application, while InfluxDB provides REST API that has to be called by the application. The integration with those two tools and several others is realized with the Micrometer library. To enable support for InfluxDB we have to include the following dependency.\n\nWe also have to provide at least URL and Influx database name inside file.\n\nTo enable Prometheus HTTP endpoint we first need to include the appropriate Micrometer module and also set property to .\n\nBy default, Prometheus tries to collect data from defined target endpoint once a minute. A rest of configuration has to be provided inside Prometheus. A section is responsible for specifying a set of targets and parameters describing how to connect with them.\n\nSometimes it is useful to provide additional tags to metrics, especially if we have many instances of a single microservice that logs to a single Influx database. Here’s the sample of tagging for applications running on Kubernetes.\n\nHere’s a diagram from Grafana created for metric of a single application.\n\nLogging is something that is not very important during development, but is the key point during maintenance. It is worth to remember that in the organization your application would be viewed through the logs quality. Usually, an application is maintenanced by the support team, so your logs should be significant. Don’t try to put everything there, only the most important events should be logged.\n\n It is also important to use the same standard of logging for all the microservices. For example, if you are logging information in JSON format, do the same for every single application. If you use tag for indicating application name or to distinguish different instances of the same application do it everywhere. Why? You usually want to store the logs collected from all microservices in a single, central place. The most popular tool for that (or rather the collection of tools) is Elastic Stack (ELK). To take advantage of storing logs in a central place, you should ensure that query criteria and response structure would be the same for all the applications, especially that you will correlate the logs between different microservices. How is that? Of course by using the external library. I can recommend my library for Spring Boot logging. To use it you should include it to your dependencies.\n\nThis library will force you to use some good logging practices and automatically integrate with Logstash (one of three ELK tools responsible for collecting logs). Its main features are:\n• an ability to log all incoming HTTP requests and outgoing HTTP responses with full body, and send those logs to Logstash with the proper tags indicating calling method name or response HTTP status\n• it is able to calculate and store an execution time for each request\n• an ability to generate and propagate correlationId for downstream services calling with Spring\n\nTo enable sending logs to Logstash we should at least provide its address and property to .\n\nAfter including the library you may take advantage of logs tagging in Logstash. Here’s the screen from Kibana for single response log entry.\n\nWe may also include Spring Cloud Sleuth library to our dependencies.\n\nSpring Cloud Sleuth propagates headers compatible with Zipkin – a popular tool for distributed tracing. Its main features are:\n• adding trace (correlating requests) and span IDs to the Slf4J MDC\n• it modifies a pattern of log entry to add some informations like additional MDC fields\n• it provides integration with other Spring components like OpenFeign, RestTemplate or Spring Cloud Netflix Zuul\n\nIn most cases, your application will be called by other applications through REST-based API. Therefore, it is worth taking care of proper and clear documentation. The documentation should be generated along with the code. Of course there are some tools for that. One of the most popular of them is Swagger. You can easily integrate Swagger 2 with your Spring Boot application using SpringFox project. In order to expose a Swagger HTML site with API documentation we need to include the following dependencies. The first library is responsible for generating Swagger descriptor from Spring MVC controllers code, while the second embeds Swagger UI to display representation of Swagger YAML descriptor in your web browser.\n\nIt’s not all. We also have to provide some beans to customize default Swagger generation behaviour. It should document only methods implemented inside our controllers, for example not the methods provided by Spring Boot automatically like endpoints. We may also customize UI appearance by defining bean.\n\nHere’s an example of Swagger 2 UI for a single microservice.\n\nThe next case is to define the same REST API guideline for all microservices. If you are building an API of your microservices consistently, it is much simpler to integrate with it for both external and internal clients. The guideline should contain instructions on how to build your API, which headers need to be set on the request and response, how to generate error codes etc. Such a guideline should be shared with all developers and vendors in your organization. For more detailed explanation of generating Swagger documentation for Spring Boot microservices including exposing it for all the application on API gateway you may refer to my article Microservices API Documentation with Swagger2.\n\nIf you are using Spring cloud for communication between microservices, you may leverage Spring Cloud Netflix Hystrix or Spring Cloud Circuit Breaker to implement circuit breaking. However, the first solution has been already moved to the maintenance mode by Pivotal team, since Netflix does not develop Hystrix anymore. The recommended solution is the new Spring Cloud Circuit Breaker built on top of resilience4j project.\n\nThen we need to configure required settings for circuit breaker by defining bean that is passed a . We are using default values as shown below.\n\nFor more details about integrating Hystrix circuit breaker with Spring Boot application you may refer to my article Part 3: Creating Microservices: Circuit Breaker, Fallback and Load Balancing with Spring Cloud.\n\nAnother important rule amongst Spring Boot best practices is transparency. We should not forget that one of the most important reasons for migration into microservices architecture is a requirement of continuous delivery. Today, the ability to deliver changes fast gives the advantage on the market. You should be able even to deliver changes several times during a day. Therefore, it is important what’s the current version, where it has been released and what changes it includes.\n\n When working with Spring Boot and Maven we may easily publish such information like a date of last changes, Git commit id or numerous version of application. To achieve that we just need to include following Maven plugins to our .\n\nAssuming you have already included Spring Boot Actuator (see Section 1), you have to enable endpoint to be able to display all interesting data.\n\nOf course, we have many microservices consisting of our system, and there are a few running instances of every single microservice. It is desirable to monitor our instances in a single, central place – the same as with collecting metrics and logs. Fortunately, there is a tool dedicated for Spring Boot application, that is able to collect data from all Actuator endpoints and display them in UI. It is Spring Boot Admin developed by Codecentric. The most comfortable way to run it is by creating a dedicated Spring Boot application that includes Spring Boot Admin dependencies and integrates with a discovery server, for example Spring Cloud Netflix Eureka.\n\nThen we should enable it for Spring Boot application by annotating the main class with .\n\nWith Spring Boot Admin we may easily browse a list of applications registered in the discovery server and check out the version or commit info for each of them.\n\nWe can expand details to see all elements retrieved from endpoint and much more data collected from other Actuator endpoints.\n\nConsumer Driven Contract (CDC) testing is one of the methods that allows you to verify integration between applications within your system. The number of such interactions may be really large especially if you maintain microservices-based architecture. It is relatively easy to start with contract testing in Spring Boot thanks to the Spring Cloud Contract project. There are some other frameworks designed especially for CDC like Pact, but Spring Cloud Contract would probably be the first choice, since we are using Spring Boot.\n\n To use it on the producer side we need to include Spring Cloud Contract Verifier.\n\nOn the consumer side we should include Spring Cloud Contract Stub Runner.\n\nThe first step is to define a contract. One of the options to write it is by using Groovy language. The contract should be verified on the both producer and consumer side. Here’s\n\nThe contract is packaged inside the JAR together with stubs. It may be published to a repository manager like Artifactory or Nexus, and then consumers may download it from there during the JUnit test. Generated JAR file is suffixed with .\n\nContract testing will not verify sophisticated use cases in your microservices-based system. However, it is the first phase of testing interaction between microservices. Once you ensure the API contracts between applications are valid, you proceed to more advanced integration or end-to-end tests. For more detailed explanation of continuous integration with Spring Cloud Contract you may refer to my article Continuous Integration with Jenkins, Artifactory and Spring Cloud Contract.\n\nSpring Boot and Spring Cloud relatively often release the new versions of their framework. Assuming that your microservices have a small codebase it is easy to up a version of used libraries. Spring Cloud releases new versions of projects using release train pattern, to simplify dependencies management and avoid problems with conflicts between incompatible versions of libraries.\n\n Moreover, Spring Boot systematically improves startup time and memory footprint of applications, so it is worth updating it just because of that. Here’s the current stable release of Spring Boot and Spring Cloud.\n\nI showed you that it is not hard to follow best practices with Spring Boot features and some additional libraries being a part of Spring Cloud. These Spring Boot best practices will make it easier for you to migrate into microservices-based architecture and also to run your applications in containers."
    },
    {
        "link": "https://spring.io/microservices",
        "document": "With Spring Boot, your microservices can start small and iterate fast. That’s why it has become the de facto standard for Java™ microservices. Quickstart your project with Spring Initializr and then package as a JAR. With Spring Boot’s embedded server model, you’re ready to go in minutes."
    },
    {
        "link": "https://spring.io/guides/gs/spring-boot",
        "document": "This guide provides a sampling of how Spring Boot helps you accelerate application development. As you read more Spring Getting Started guides, you will see more use cases for Spring Boot. This guide is meant to give you a quick taste of Spring Boot. If you want to create your own Spring Boot-based project, visit Spring Initializr, fill in your project details, pick your options, and download a bundled up project as a zip file.\n\nLearn What You Can Do with Spring Boot Spring Boot offers a fast way to build applications. It looks at your classpath and at the beans you have configured, makes reasonable assumptions about what you are missing, and adds those items. With Spring Boot, you can focus more on business features and less on infrastructure. The following examples show what Spring Boot can do for you:\n• Is Spring MVC on the classpath? There are several specific beans you almost always need, and Spring Boot adds them automatically. A Spring MVC application also needs a servlet container, so Spring Boot automatically configures embedded Tomcat.\n• Is Jetty on the classpath? If so, you probably do NOT want Tomcat but instead want embedded Jetty. Spring Boot handles that for you.\n• Is Thymeleaf on the classpath? If so, there are a few beans that must always be added to your application context. Spring Boot adds them for you. These are just a few examples of the automatic configuration Spring Boot provides. At the same time, Spring Boot does not get in your way. For example, if Thymeleaf is on your path, Spring Boot automatically adds a to your application context. But if you define your own with your own settings, Spring Boot does not add one. This leaves you in control with little effort on your part. Spring Boot does not generate code or make edits to your files. Instead, when you start your application, Spring Boot dynamically wires up beans and settings and applies them to your application context.\n\nYou can use this pre-initialized project and click Generate to download a ZIP file. This project is configured to fit the examples in this tutorial.\n• Navigate to https://start.spring.io. This service pulls in all the dependencies you need for an application and does most of the setup for you.\n• Choose either Gradle or Maven and the language you want to use. This guide assumes that you chose Java.\n• Download the resulting ZIP file, which is an archive of a web application that is configured with your choices. If your IDE has the Spring Initializr integration, you can complete this process from your IDE. You can also fork the project from Github and open it in your IDE or other editor. For Spring 3.0 you need Java 17 or later, regardless of whether you use Spring Initializr.\n\nThe Spring Initializr creates a simple application class for you. However, in this case, it is too simple. You need to modify the application class to match the following listing (from ): package com.example.springboot; import java.util.Arrays; import org.springframework.boot.CommandLineRunner; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.ApplicationContext; import org.springframework.context.annotation.Bean; @SpringBootApplication public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } @Bean public CommandLineRunner commandLineRunner(ApplicationContext ctx) { return args -> { System.out.println(\"Let's inspect the beans provided by Spring Boot:\"); String[] beanNames = ctx.getBeanDefinitionNames(); Arrays.sort(beanNames); for (String beanName : beanNames) { System.out.println(beanName); } }; } } is a convenience annotation that adds all of the following:\n• : Tags the class as a source of bean definitions for the application context.\n• : Tells Spring Boot to start adding beans based on classpath settings, other beans, and various property settings. For example, if is on the classpath, this annotation flags the application as a web application and activates key behaviors, such as setting up a .\n• : Tells Spring to look for other components, configurations, and services in the package, letting it find the controllers. The method uses Spring Boot’s method to launch an application. Did you notice that there was not a single line of XML? There is no file, either. This web application is 100% pure Java and you did not have to deal with configuring any plumbing or infrastructure. There is also a method marked as a , and this runs on start up. It retrieves all the beans that were created by your application or that were automatically added by Spring Boot. It sorts them and prints them out.\n\nYou will want to add a test for the endpoint you added, and Spring Test provides some machinery for that. If you use Gradle, add the following dependency to your file: If you use Maven, add the following to your file: Now write a simple unit test that mocks the servlet request and response through your endpoint, as the following listing (from ) shows: comes from Spring Test and lets you, through a set of convenient builder classes, send HTTP requests into the and make assertions about the result. Note the use of and to inject a instance. Having used , we are asking for the whole application context to be created. An alternative would be to ask Spring Boot to create only the web layers of the context by using . In either case, Spring Boot automatically tries to locate the main application class of your application, but you can override it or narrow it down if you want to build something different. As well as mocking the HTTP request cycle, you can also use Spring Boot to write a simple full-stack integration test. For example, instead of (or as well as) the mock test shown earlier, we could create the following test (from ): The embedded server starts on a random port because of , and the actual port is configured automatically in the base URL for the .\n\nIf you are building a web site for your business, you probably need to add some management services. Spring Boot provides several such services (such as health, audits, beans, and more) with its actuator module. If you use Gradle, add the following dependency to your file: If you use Maven, add the following dependency to your file: Then restart the application. If you use Gradle, run the following command in a terminal window (in the directory): If you use Maven, run the following command in a terminal window (in the directory): You should see that a new set of RESTful end points have been added to the application. These are management services provided by Spring Boot. The following listing shows typical output: The actuator exposes the following: There is also an endpoint, but, by default, it is visible only through JMX. To enable it as an HTTP endpoint, add to your file and expose it with . However, you probably should not enable the shutdown endpoint for a publicly available application. You can check the health of the application by running the following command: You can try also to invoke shutdown through curl, to see what happens when you have not added the necessary line (shown in the preceding note) to : Because we did not enable it, the requested endpoint is not available (because the endpoint does not exist). For more details about each of these REST endpoints and how you can tune their settings with an file (in ), see the the documentation about the endpoints."
    },
    {
        "link": "https://medium.com/@J_vishal/microservices-architecture-a-practical-guide-with-java-spring-boot-implementation-7fd0e5bf8752",
        "document": "Microservices have evolved as a popular architectural style in the software development industry. By breaking down large applications into smaller, independent services, micro-services offer a more flexible, scalable, and maintainable approach. In this blog post, We will go into the principles of microservices, compare them to traditional monolithic and SOA architectures, and explore how to implement them using Java Spring Boot.\n\nMonolithic Architecture: In a monolithic architecture, all components of an application — are bundled together in a single package. This package is deployed as a whole, and scaling involves increasing the capacity of the entire application. For instance, if you have an e-commerce application where both product and order functionalities are integrated into one system, and you scale the entire application when needed, when you are working with a monolithic architecture.\n\nService-Oriented Architecture (SOA): SOA divides an application into separate services based on functional domains or enterprise needs. In our e-commerce example, you might have distinct services for Payment and Order Management and shipping management These services operate independently but are interconnected to create a cohesive application. SOA focuses on integrating these services to work together within a larger enterprise scope.\n\nMicroservices Architecture: Microservices take the modular approach a step further by breaking down the application into even smaller, more specialized components. Each microservice handles a specific task or functionality, such as a product service, an order service, or a inventory service. This architectural style allows each microservice to be developed, deployed, and scaled independently. For instance, in our e-commerce application, if the product search service experiences high traffic, it can be scaled independently of the order service, which might have different traffic patterns.\n\nPlease refer to the diagram below for a side-by-side comparison of monolithic and microservice architectures.\n\nTo demonstrate the concepts, we will create a simplified e-commerce platform with two microservices, as shown in the diagram above:\n• Product Service: Handles product search, inventory management, and other product-related functionalities. Depending on the requirements and load, you can further divide these services into smaller, more specialized units like separate service for product search & inventory etc.\n• Order Service: Processes customer orders, manages order statuses, and interacts with the product service to verify product availability & checkout.\n\nEureka is a service registry that enables microservices to discover and communicate with each other dynamically. By registering themselves with Eureka, services can easily locate and interact with other services without hardcoding their addresses & ports.\n• Dynamic discovery: Services can be added or removed from the registry without manual configuration.\n• Load balancing: Eureka can distribute traffic across multiple instances of a service to improve performance and availability.\n• Failover: If a service instance becomes unavailable, Eureka can automatically redirect requests to a healthy instance.\n\nFeignClient is a declarative REST client that simplifies the process of making HTTP requests between microservices. By defining an interface, you can specify the HTTP method, URL, and request/response parameters. FeignClient takes care of the underlying HTTP implementation, making it easier to focus on the business logic.\n• Declarative API: No need to write HTTP client code.\n• Integration with Eureka: FeignClient can use Eureka to discover service instances and load balance requests.\n• Create Spring Boot projects: Set up two separate Spring Boot projects for the product service and order services. As below\n• Add dependencies: Include the necessary dependencies for Spring Web, Eureka, and FeignClient in both projects. In above example all dependencies are already added.\n• Configure Eureka: Register your services with Eureka by adding the appropriate configuration.\n• Create FeignClient interfaces: Define FeignClient interfaces in each service to interact with the other service’s APIs.\n• Implement service logic: Write the business logic for each service, using FeignClient to call the other service’s APIs as needed.\n\nAfter starting all the services and enabling the Eureka server, the service will appear on the Eureka dashboard as shown below.\n\nThe best architectural style depends on factors such as the size and complexity of the application, development team’s expertise, and business requirements. Here’s a general guideline:\n• Medium-sized applications with moderate complexity: SOA can be a good choice.\n• Large-scale, complex applications with high scalability and flexibility requirements: Microservices architecture is often preferred.\n\nBy carefully considering these factors, you can select the most appropriate architectural style for your application.\n\nMicroservices offer a significant advantage over monolithic architectures by enabling independent development, deployment, and scaling. This not only improves scalability and resilience but also simplifies the overall management of complex applications. When combined with tools like Eureka and FeignClient, microservices further streamline development by providing a centralized service registry and a declarative HTTP client. This abstracts away the complexity of HTTP calls, allowing developers to focus on core business logic.\n\nWhile microservices can be highly beneficial, careful design is crucial to avoid potential challenges that may be difficult to address later. Before adopting a microservice architecture, thoroughly consider factors such as transactional requirements, data and reporting needs, technology, maintenance considerations, team size, and cost."
    },
    {
        "link": "https://reddit.com/r/SpringBoot/comments/1if5vis/how_do_i_build_a_microservice_architecture",
        "document": "As per title, I've done about three Spring boot projects so far and I'm starting to get comfortable. I'm wondering how do I go about creating a microservice architecture?\n\nAlong with it I have many questions and new things to learn like Kafka or an API gateway and so on\n\nI have two questions I would appreciate some guidance\n• Where's a good place to start, the docs or is there a tutorial you've learn from. Would love to get recommendations from anyone, based on your experience\n• Will I have trouble hosting it on a budget? For context, I have a 8GB VPS that's already hosting one small full stack application (spring + react), I wonder if It can handle a bunch of microservices more. I don't really understand how it works but my idea of it is each microservives has it's own java run time which consumes quite a lot of ram"
    },
    {
        "link": "https://geeksforgeeks.org/spring-boot-architecture",
        "document": "Spring Boot is built on top of the core Spring framework. It simplifies and automates Spring-based application development by reducing the need for manual configuration. Spring Boot follows a layered architecture, where each layer interacts with other layers in a hierarchical order. The official Spring Boot documentation defines it as:\n\nThe main goal of Spring Boot is to eliminate XML and annotation-based configuration complexities. It provides several benefits, including opinionated defaults, convention over configuration, stand-alone applications, and production readiness.\n\nSpring Boot consists of the following four layers:\n\nThe Presentation Layer is the topmost layer of the Spring Boot architecture. It primarily consists of REST controllers that handle HTTP requests (GET, POST, PUT, DELETE). It performs authentication, request validation, and JSON serialization/deserialization (conversion of JSON to Java objects and vice versa). After processing the request, it forwards the request to the business layer.\n\nThe Business Layer is responsible for implementing the application’s core logic. It consists of service classes that:\n• None Interact with the Persistence Layer to store or retrieve data.\n\nThe Persistence Layer manages database transactions and storage logic. It consists of repository classes using Spring Data JPA, Hibernate, or R2DBC for data access. It is responsible for:\n\nThe Database Layer contains the actual database where the application data is stored. It can support:\n• None The client (frontend or API consumer) sends an HTTP request (GET, POST, PUT, DELETE) to the application.\n• None The request is handled by the Controller Layer, which maps the request to a specific handler method.\n• None The Service Layer processes business logic and communicates with the Persistence Layer to fetch or modify data.\n• None The Persistence Layer interacts with the Database Layer using Spring Data JPA or R2DBC, often through a Repository Class\n• None The processed response is returned as JSON.\n• None Spring Boot Actuator can be used for monitoring and health checks."
    }
]