[
    {
        "link": "https://speakwrite.com/blog/phonetic-transcription",
        "document": ""
    },
    {
        "link": "https://ecampusontario.pressbooks.pub/essentialsoflinguistics2/chapter/3-6-the-international-phonetic-alphabet",
        "document": "Note that we have been talking about phones as if it were obvious what they are, but this is not always the case. It is sometimes easy to find a clear separation between the phones in a given word, that is, to segment the word into its component phones, but sometimes, it can be very difficult. We can see this difference by looking at waveforms, which are special pictures that graphically represent the air vibrations of sound waves. The two waveforms in Figure 3.19 show a notable difference in how easy it is to segment the English words nab and wool.\n\nThe waveform for nab contains abrupt transitions between three very different regions, corresponding to three phones. In comparison, the waveform for wool has smooth transitions from beginning to end, with no obvious divisions between phones. For the purposes of this textbook, all words will be segmented for you, but it is important to remember that when working with raw data from a spoken language, it may not be so clear where the boundaries are between phones.\n\nWhen we can identify the individual phones in a word, we want to have a suitable way to notate them that can be easily and consistently understood, so that the relevant information about the pronunciation can be conveyed in an unambiguous way to other linguists. Such notation is called a transcription, which may be very broad (giving only the minimal information needed to contrast one word with another), or it may be very narrow (giving a large amount of fine-grained phonetic detail), or somewhere in between. Whether broad, narrow, or in between, phonetic transcription is conventionally given in square brackets [ ], so that, for example, the consonant at the beginning of the English word nab could be transcribed as [n], with the understanding that the symbol [n] is intended to represent a voiced alveolar nasal stop.\n\nAs linguists, we are interested in studying and describing as many languages as we can, so for spoken languages, we ideally want to use a transcription system that can be used for all possible phones in any spoken language. This means we cannot simply use one existing language’s writing system, because it would be optimized for representing the phones of that language and would not have easy ways to represent phones from other languages.\n\nIn addition, many writing systems are filled with inconsistencies and irregularities that make them unsuitable for any kind of rigorous and unambiguous transcription, even for their associated spoken language. For example, the letter <a> in the English writing system is used to represent different phones, such as the low front unrounded vowel in nab, the low back unrounded vowel in father, the mid front tense unrounded vowel in halo, and the mid central unrounded vowel in diva. Conversely, the high front unrounded tense vowel in English can be represented by different letters and letter combinations: <i> in diva, <ee> in meet, <ea> in meat, <e> in me, and <y> in mummy. That is, then English writing system does not have a one-to-one relationship between phones and letters.\n\nNote that symbols from a writing system are represented here with surrounding angle brackets < >. This is a common notational convention in linguistics that helps visually distinguish symbols in a writing system from symbols used for the transcription of phones, which are enclosed in square brackets [ ].\n\nFurthermore, even if English spelling were perfectly regular, many specific English words have different equally valid pronunciations, such as either, data, and route. But even words that seem to have only one consistent pronunciation may in fact be pronounced differently by different speakers in more subtle ways. For example, in Los Angeles and London, the vowel in the word mop normally has a low back tongue position, with the London vowel also having some lip rounding that is not used in the Los Angeles pronunciation. In Chicago, the vowel in mop is articulated more in the centre of the mouth, making mop sound nearly like map to other speakers. If we tried to describe in writing how to pronounce a vowel from another language, and we said that it was pronounced the same as the vowel in the English word mop, we could not guarantee that the reader would know whether the vowel in question is back and unrounded (as in Los Angeles mop), back and round (as in London mop), or central and unrounded (as in Chicago mop).\n\nTo avoid these problems, linguists have devised more suitable transcription systems for spoken languages, each with their own strengths and weaknesses. In this textbook, we will use a widespread standard transcription system called the International Phonetic Alphabet (abbreviated IPA). The IPA was created by the International Phonetic Association (unhelpfully also abbreviated IPA). The IPA organization was founded in 1886, and the first version of their transcription system was published shortly after. Since then, the IPA transcription system has undergone many revisions as our understanding of the world’s spoken languages has evolved. The most recent symbol was added to the IPA in 2005: [ⱱ] for the labiodental tap, a phone found in many languages of central Africa, such as Mono (a Central Banda language of the Ubangian family, spoken in the Democratic Republic of the Congo; Olson and Hajek 1999).\n\nFor reference, the full chart for the IPA is given in Figure 3.20. This chart is available under a Creative Commons Attribution-Sharealike 3.0 Unported License, copyright © 2020 by the International Phonetic Association. It is also available online at the IPA’s homepage, and there are also some online versions that are accessible for screenreaders, such as this one created by Weston Ruter.\n\nLearning the IPA takes a lot of time, practice, and guidance, and it is not just about memorizing symbols. The underlying structure and principles behind the organization of the table are what really matter. In this way, the IPA is like the periodic table of elements in chemistry. So, while it is helpful to know that Na is the chemical symbol for the element sodium with atomic number 11 and that [m] is the IPA symbol for a voiced bilabial nasal stop, it is much more important to know what these concepts are and what those terms mean. What is sodium? What does it mean for an element to have an atomic number of 11? What does it mean for a phone to be voiced? How is the vocal tract configured for a bilabial nasal stop?\n\nThis is why this chapter focuses on defining concepts, so that you can build a solid foundation in understanding how phones are articulated. The notation is also important, but it has no value without the corresponding conceptual understanding.\n\nA full discussion of how to use the IPA for transcription is beyond the scope of an introductory textbook like this one. Here, we discuss a few guidelines and some concrete examples from English. For any transcription, it is important to keep in mind who your audience is and what the purpose of the transcription is. Most of the time, we normally only need a fairly broad transcription to get across a basic idea of the most important aspects of the articulation.\n\nOne important guiding recommendation from the IPA for broad transcription is to use the typographically simplest notation that still conveys the most crucial information. For example, when possible, we should choose symbols like upright [a] and [r] rather than their rotated counterparts [ɐ] and [ɹ]. These upright symbols are easier to type, easier to read, and more reliable in how they are displayed in different fonts.\n\nAnother aspect of typographic simplicity is to avoid diacritics, which are special marks like [ ̪ ] and [ʰ] that are placed above, below, through, or next to a symbol to give it a slightly different meaning. These are often necessary for certain contexts, but sometimes, they are superfluous and can hinder the reader’s understanding. Thus, you should use diacritics when their meaning is crucial, but avoid them otherwise.\n\nTypographic simplicity is also good practice when dealing with a lot of variation between speakers that is not relevant to the main point. For example, the English consonant typically spelled by the letter <r> is pronounced in many different ways by different speakers, sometimes even differently by the same speaker (DeLattre and Freeman 1968). Many North Americans have some sort of central approximant, but it varies from alveolar [ɹ], to postalveolar [ɹ̱], to retroflex [ɻ]. In addition to the primary place of articulation, some speakers may also have secondary pharyngealization, with a constriction between the tongue root and pharyngeal wall, which is indicated in the IPA with a [ˁ] diacritic after the symbol. Some speakers may have some amount secondary lip rounding, which is indicated in the IPA with a [ʷ] diacritic after the symbol. Some speakers may have both pharyngealization and rounding!\n\nThat results in at least twelve different possible articulations, each with its own transcription in the IPA, depending on the place of articulation, whether or not there is pharyngealization, and whether or not there is lip rounding. The IPA symbols for these twelve possibilities are given in the list below. For each, the symbols are in order by place of articulation: alveolar, postalveolar, and retroflex.\n• no pharyngealization and no rounding: [ɹ], [ɹ̱], or [ɻ]\n• pharyngealization and no rounding: [ɹˁ], [ɹ̱ˁ], or [ɻˁ]\n• rounding and no pharyngealization: [ɹʷ], [ɹ̱ʷ], or [ɻʷ]\n• both pharyngealization and rounding: [ɹˁʷ], [ɹ̱ˁʷ], or [ɻˁʷ]\n\nFurthermore, there are many other rhotic pronunciations beyond those in North American varieties, such as an alveolar tap [ɾ] or trill [r] in Scotland (Johnston 1997), a voiced uvular fricative [ʁ] in Northumbria (Maguire 2017), and a labiodental approximant [ʋ] in southeast England (Foulkes and Docherty 2000).\n\nThus, when transcribing English in general, there is no one single symbol that accurately represents the pronunciation of this consonant, so for broad transcription, a plain upright [r] with no diacritics is a reasonable choice that follows the IPA’s recommendation for typographic simplicity. Of course, when transcribing a specific articulation from a specific speaker, it may make sense to use a more precise symbol, especially if the details of the articulation are important. But generally speaking, a plain upright [r] is normally fine for English, though some linguists may prefer to use [ɹ] or [ɻ] for North American English, even though there are at least a dozen equally valid North American pronunciations. If you are taking a course in linguistics, be sure to follow the standards and conventions set by your instructor.\n\nWhy is there so much variation in the pronunciation of English <r>? These consonants belong to an unusual class of phones called rhotics, named after the Greek letter rho <ρ>, which itself represents a rhotic consonant in Greek. Across the world’s spoken languages, we find a lot of variation in rhotics. Many languages only have one rhotic, but which particular rhotic they have can be very different even between closely related languages. The pronunciation of rhotics in a language can also shift over time, especially if the language only has one. There seems to be no single overarching phonetic similarity in the various rhotics, and linguists are still trying to figure out what makes this class of consonants so special.\n\nEven when the pronunciation of a given phone is fairly consistent across speakers, many linguists still choose a typographically simpler transcription. Consider the consonant at the beginning of the English word chin, which is a voiceless postalveolar affricate. Affricates in the IPA are normally transcribed by writing the corresponding plosive symbol to represent the stop closure, followed by the corresponding fricative symbol to represent the fricated release, both united under a curved tie-bar [ ͡ ] to show they are unified as a single phone.\n\nSo, to represent the voiceless postalveolar affricate in chin, we need to select the correct plosive and fricative symbols. First, let us consider the voiceless postalveolar fricative. We can find its symbol in the IPA chart by looking in the section devoted to consonants. Places of articulation are listed across the top, while manners of articulation are listed down the left. Within a given cell, if there are two symbols, the one on the left is voiceless, and the one on the right is voiced. So looking in the postalveolar column and the fricative row, we find the symbols [ʃ] and [ʒ], and since we are interested in the voiceless fricative, we pick the symbol [ʃ].\n\nHowever, there is no similar basic symbol for a voiceless postalveolar plosive in the IPA. That part of the chart is blank, so we have to create our own symbol by using the base symbol for a similar consonant and adding one or more diacritics. In this case, we can use alveolar [t] and put a retraction diacritic [ ̱ ] under it to indicate that its place of articulation is slightly farther back, as we did for the postalveolar central approximant [ɹ̱] before. Thus, we get [ṯ] as the symbol for a voiceless postalveolar plosive.\n\nSo, we would begin by putting these two symbols together under a tie-bar: [ṯ͡ʃ]. However, most English speakers also pronounce this affricate with some amount of lip rounding, so a fully accurate transcription would be something more like [ṯ͡ʃʷ], with the [ʷ] diacritic to indicate rounding.\n\nBut hardly any linguist transcribes this affricate with that much phonetic detail. It is almost never relevant to indicate that it is round, and the postalveolar location of the stop closure is implied by the fact that it has a postalveolar release. You cannot release a stop closure in a position different from where it is made: if there is a postalveolar release, it necessarily must come from a postalveolar closure. So for typographic simplicity, the affricate is more commonly transcribed simply as [t͡ʃ], with neither of the two diacritics. As with [r] for the English rhotic, [t͡ʃ] is not technically an accurate transcription for most speakers, but it is typographically simpler and conveys of all the crucial information needed to understand the basics of the articulation. The tie-bar on the affricate may also sometimes be left off in transcriptions, so [tʃ] is also a common transcription for this affricate, making it even more typographically simple.\n\nEven without these issues, there is still usually no such thing as “the” correct transcription of a word. Two pronunciations of the same word by the same speaker will always have some differences, because we live in a physical world where we cannot avoid slight imperfections and fluctuations. Even if we wanted to capture all of those possible differences with the IPA, it is simply not designed for that level of phonetic detail. When such detail is important, it needs to be conveyed in other ways, such as with pictures (like waveforms and midsagittal diagrams) and numerical measurements (like loudness in decibels and duration in milliseconds). Thus, an IPA transcription is always inherently missing some details, so we have to decide how much detail is needed and how much should be left out for simplicity.\n\nDespite all these pitfalls, it is still important to get some basic skill in transcription for future work in linguistics. Since this textbook is presented in English, English is a good starting point to give you something concrete in which to ground your understanding of how to do transcription. However, there is much dialectal variation in English, so the transcriptions offered here are very general and may differ from the varieties of English you are familiar with.\n\nWe begin with consonants, where there tends to be less variation across dialects. Table 3.2 lists some plosives and affricates of English, with their IPA symbol (keeping in mind typographic simplicity) and example words containing each consonant in various positions, where possible. For each word, The portion of the spelling that corresponds to the phone is in bold. Finally, a phonetic description of each consonant is also given.\n\nMost of these are straightforward, but as discussed in Section 3.3, the alveolar consonants are normally apicoalveolar, though some speakers may pronounce them with the blade of the tongue. If that detail is necessary, these consonants can be transcribed as [t̻] and [d̻], using the laminal diacritic [ ̻ ]. Regardless of the active articulator, some speakers may pronounce these consonants on the back of the teeth rather than on the alveolar ridge, in which case, they would be transcribed as [t̪] and [d̪], using the dental diacritic [ ̪ ].\n\nThe glottal plosive (also frequently called a glottal stop) is only a marginal consonant in English. It can be found as the catch in the throat in the middle of the interjection uh-oh. Some speakers also have it elsewhere, such as in the middle of some British English pronunciations of the word bottle. It is articulated by making a full stop closure with the vocal folds, blocking all airflow through the glottis.\n\nThe most notable variation here is that some speakers do not have [θ] and [ð], and instead used [t] and [d] or [f] and [v], depending on the dialect and the position in the word. As with the postalveolar affricates mentioned before, the postalveolar fricatives are also usually somewhat rounded, so they could be more narrowly transcribed as [ʃʷ] and [ʒʷ]. The voiced postalveolar fricative [ʒ] is also one of the rarest consonants in English, and many speakers pronounce it as an affricate in some positions instead of as a fricative. For example, you may hear speakers pronounce the final consonant of garage as the affricate [d͡ʒ] rather than the fricative [ʒ].\n\nTable 3.4 lists some sonorants of English. Across the world’s spoken languages, sonorants tend to be voiced by default, because their high degree of airflow causes the vocal folds to spontaneously vibrate, unless extra effort is put in to keep them from vibrating. This is true for English, so the phonation of the sonorants is not listed here.\n\nA few of these sonorants warrant extra discussion. The alveolar nasal stop [n] has much of the same variation as the alveolar plosives, with some speakers having a laminoalveolar articulation [n̻] and some having a dental articulation [n̪]. The velar nasal stop is often one of the most surprising phones of English to English speakers who are new to phonetics, because is not easily identifiable as its own phone. Many people are misled by the spelling and think they say words like singer with a [ɡ], but in fact, most speakers have only a nasal stop there, so that singer differs from finger, with singer having only [ŋ] and finger having [ŋɡ]. However, there are speakers who do genuinely pronounce all words like these with a [ɡ] after the nasal stop, but even then, the nasal stop they have is still velar [ŋ], not alveolar [n].\n\nA notable consonant here is [w], which is special among the consonants of English in being doubly articulated, which means that it has two equal places of articulation. It is both bilabial (with an approximant constriction between the two lips) and velar (with a second approximant constriction between the tongue back and the velum). Its place of articulation is usually called labial-velar. English used to consistently have two labial-velar approximants, a voiced [w] and a voiceless [ʍ]. Very few speakers today have both of these, but those who do pronounce the words witch and which differently, with voiced [w] in witch and voiceless [ʍ] in which.\n\nNow we can move on to the vowels. This is where much of the variation in pronunciation occurs across English dialects, and fully describing all of the vowels in English would take up an entire textbook of its own. Note that this is not a general property of spoken languages overall. Some are like English, with most dialectal variation in the vowels, but others have much more dialectal variation in the consonants, while others may have a relatively even mixture of variation in both consonants and vowels.\n\nTable 3.5 lists some monophthongs of English, with a focus on the English vowels as they are broadly pronounced across North American dialects. However, there is still much variation just in North America, and this discussion should not be taken to represent any particular speaker or region, let alone any sort of idealized standard or target pronunciation. This is simply a convenient abstraction that provides a useful baseline, though it is still only a very rough guide, and individual speakers can vary quite a lot from what is discussed here. Note also that unstressed vowels are very unstable, especially in fast speech, so for example, unstressed [u] could be pronounced more like [ʊ] or [ə], even for the same speaker saying the same word. As in most spoken languages, the vowels of English are generally all voiced, so their phonation is not listed here. Example words are given that show the vowel in a stressed syllable, an unstressed syllable, and at the end of the word (see Sections 3.10 and 3.11 for more about syllables and stress).\n\nAs noted before, there is a lot of variation that cannot be adequately discussed here, so we only cover a few notable deviations. First, while many speakers pronounce the four tense vowels as monophthongs as transcribed here, most speakers pronounce some or all of them as diphthongs instead, perhaps even having an approximant at the end rather than a vowel. For example, high front unrounded tense [i] may be pronounced more like [ɪi] or [ij] by some speakers. It is especially common for the two tense mid vowels to be pronounced as diphthongs, something like [eɪ] and [oʊ] or perhaps [ej] and [ow].\n\nMany of the back round vowels, especially [ʊ], are fronter and/or unrounded for some speakers in some dialects.\n\nThe back vowels in bore and bought are pronounced similarly to each other by some North Americans, and so they are often represented with the same symbol [ɔ], though note there may still be some differences, with [ɔ] before a rhotic often pronounced somewhat higher, closer to [o]. However, many speakers in Canada and in the western United States have a very different vowel in bought from bore. Their bought vowel is much lower, and for some speakers, it is also unrounded. These speakers use the same low vowel in bought that they use in bot. For most North Americans, the low vowels in bot and balm are pronounced the same, either as back round [ɒ] or back unrounded [ɑ]; in some dialects, it may be central unrounded [a]. Others have two different vowels for these words, usually [ɒ] in bot and [ɑ] or [a] in balm. Needless to say, this part of the vowel system of English is particularly troublesome, and even many expert linguists get aspects of it wrong.\n\nThe two mid central vowels [ʌ] and [ə] are often treated as related pronunciations of the same vowel, based on whether or not they occur in a stressed syllable (again, see Sections 3.10 and 3.11 for more about syllables and stress). For now, just note that some vowels of English are pronounced louder and longer than others, which we call stressed, while the other softer and shorter vowels are said to be unstressed. We can see the difference in stress in pairs like billow and below, which differ mostly in which syllable is stressed: the first syllable in billow and the second syllable in below. The two central vowels of English differ in stress: the first syllable of the name Bubba is stressed, and the second is unstressed, so we might transcribe this name as [bʌbə]. Although these two vowels sound very similar for many speakers and could easily be notated with the same symbol, there is a long tradition of notating the unstressed mid central vowel of English with [ə] and the stressed mid central vowel with [ʌ], based on historical pronunciations in which the stressed vowel used to be pronounced much farther back (and still is, in some dialects).\n\nThe symbol [ə] has a special name, schwa. A common joke among linguists is that their favourite vowel is schwa or that they wish they could be more like schwa, because it is unstressed, referring to a desire to avoid the typically high amounts of stress in academia. However, while schwa is always unstressed in English and in some other spoken languages, in many others, [ə] is a normal vowel like any other vowel and can be stressed. For example, stressed [ə] can be found in languages like Sḵwx̱wú7mesh (a.k.a. Squamish, a Coast Salish language of the Salishan family, spoken in British Columbia; Demers and Horn 1978), Romanian (a western Romance language of the Indo-European family, spoken in Romania; Chitoran 2002), and Mandarin (a Sinitic language of the Sino-Tibetan family, spoken in China and nearby areas; Cheng 1973). Thus, there is nothing that requires [ə] to be unstressed in general, so you should not think of it an inherently unstressed vowel.\n\nFinally, we consider diphthongs and syllabic consonants, which are phones that have consonant-like constrictions in the vocal tract but which function more like vowels within English. Some diphthongs and syllabic consonants of English are given in Table 3.5.\n\nFor the diphthongs, the symbols used here represent a rough average over where they typically start and end, but the actual pronunciation varies quite a lot from speaker to speaker and even for the same speaker. The low starting point for [aɪ] and [aʊ] may be closer to back [ɑ] or front [æ], while the mid back starting point for [ɔɪ] may be closer to tense [o]. Additionally, the hiɡh front ending point for [aɪ] and [ɔɪ] may be closer to tense [i] or the approximant [j], while the hiɡh back ending point for [aʊ] may similarly be closer to tense [u] or the approximant [w].\n\nSyllabic consonants are transcribed by using the syllabic diacritic [ˌ] under the relevant consonant symbol. However, sometimes these are transcribed with a preceding [ə] instead, so that hazelnut could be transcribed either as [hezl̩nʌt] or as [hezəlnʌt]. Syllabic rhotics (also called rhotacized vowels or r-coloured vowels) are so common that they have their own dedicated symbols: [ɝ] in stressed syllables and [ɚ] in unstressed syllables. Thus, burning could be transcribed as [br̩nɪŋ] or [bɝnɪŋ], while interval could be transcribed as [ɪntr̩vl̩] or [ɪntɚvl̩].\n\nWith all of this variation, not just in pronunciation by different speakers, but in transcription choices by different linguists, it can be difficult to figure out what is really intended by a given transcription. This is why when exact phonetic details matter, it is a good idea not to rely solely on the IPA, but to include prose descriptions, midsagittal diagrams, and other tools that can help clarify exactly what is meant.\n\nCheng, Chin-Chuan. 1973. A synchronic phonology of Mandarin Chinese. Monographs on Linguistic Analysis. The Hague: Mouton.\n\nChitoran, Ioana. 2002. The phonology and morphology of Romanian diphthongization. Probus 14(2): 205–246.\n\nDeLattre, Pierre, and Donald C. Freeman. 1968. A dialect study of American r’s by x-ray motion picture. Linguistics 44: 29–68\n\nDemers, Richard A. and George M. Horn. 1978. Stress assignment in Squamish. International Journal of American Linguistics 44(3): 180–191.\n\nFoulkes, Paul, and Gerard J. Docherty. 2000. Another chapter in the story of /r/: Labiodental variants in British English. Journal of Sociolinguistics 4(1): 30-59.\n\nJohnston, Paul. 1997. Regional variation. In Charles Jones, ed. The Edinburgh history of the Scots language. Edinburgh: Edinburgh University Press. 433–513.\n\nMaguire, Warren. 2017. Variation and change in the realisation of /r/ in an isolated Northumbrian dialect. In Chris Montgomery and Emma Moore, eds. Language and a sense of place: Studies in language and region. Cambridge: Cambridge University Press. 87–104.\n\nOlson, Kenneth S., and John Hajek. 1999. The phonetic status of the labial flap. Journal of the International Phonetic Association 29(2): 101–114."
    },
    {
        "link": "https://en.wikipedia.org/wiki/International_Phonetic_Alphabet",
        "document": "The International Phonetic Alphabet (IPA) is an alphabetic system of phonetic notation based primarily on the Latin script. It was devised by the International Phonetic Association in the late 19th century as a standard written representation for the sounds of speech.[1] The IPA is used by lexicographers, foreign language students and teachers, linguists, speech–language pathologists, singers, actors, constructed language creators, and translators.[2][3]\n\nThe IPA is designed to represent those qualities of speech that are part of lexical (and, to a limited extent, prosodic) sounds in oral language: phones, intonation and the separation of syllables.[1] To represent additional qualities of speech – such as tooth gnashing, lisping, and sounds made with a cleft palate – an extended set of symbols may be used.[2]\n\nSegments are transcribed by one or more IPA symbols of two basic types: letters and diacritics. For example, the sound of the English digraph ⟨ch⟩ may be transcribed in IPA with a single letter: [c], or with multiple letters plus diacritics: [t̠̺͡ʃʰ], depending on how precise one wishes to be. Slashes are used to signal phonemic transcription; therefore, /tʃ/ is more abstract than either [t̠̺͡ʃʰ] or [c] and might refer to either, depending on the context and language.[note 1]\n\nOccasionally, letters or diacritics are added, removed, or modified by the International Phonetic Association. As of the most recent change in 2005,[4] there are 107 segmental letters, an indefinitely large number of suprasegmental letters, 44 diacritics (not counting composites), and four extra-lexical prosodic marks in the IPA. These are illustrated in the current IPA chart, posted below in this article and on the International Phonetic Association's website.[5]\n\nIn 1886, a group of French and English language teachers, led by the French linguist Paul Passy, formed what would be known from 1897 onwards as the International Phonetic Association (in French, l'Association phonétique internationale).[6] The idea of the alphabet had been suggested to Passy by Otto Jespersen. It was developed by Passy along with other members of the association, principally Daniel Jones. The original IPA alphabet was based on the Romic alphabet, an English spelling reform created by Henry Sweet that in turn was based on the Palaeotype alphabet of Alexander John Ellis, but to make it usable for other languages the values of the symbols were allowed to vary from language to language.[note 2] For example, the sound [ʃ] (the sh in shoe) was originally represented with the letter ⟨c⟩ for English but with ⟨x⟩ for French and German; with German, ⟨c⟩ was used for the [x] sound of Bach.[6] With a growing number of transcribed languages this proved impractical, and in 1888 the values of the letters were made uniform across languages. This would provide the base for all future revisions.[6][8]\n\nSince its creation, the IPA has undergone a number of revisions. After relatively frequent revisions and expansions from the 1890s to the 1940s, the IPA remained nearly static until the Kiel Convention in 1989, which substantially revamped the alphabet. A smaller revision took place in 1993 with the resurrection of letters for mid central vowels[2] and the retirement of letters for voiceless implosives.[9] The alphabet was last revised in May 2005 with the addition of a letter for a labiodental flap.[10] Apart from the addition and removal of symbols, changes to the IPA have consisted largely of renaming symbols and categories and in modifying typefaces.[2]\n\nExtensions to the International Phonetic Alphabet for speech pathology (extIPA) were created in 1990 and were officially adopted by the International Clinical Phonetics and Linguistics Association in 1994.[11] They were substantially revised in 2015.\n\nThe general principle of the IPA is to provide one letter for each distinctive sound (speech segment).[note 3] This means that:\n• It does not normally use combinations of letters to represent single sounds, the way English does with , and , nor single letters to represent multiple sounds, the way represents or in English.\n• There are no letters that have context-dependent sound values, the way and in several European languages have a \"hard\" or \"soft\" pronunciation.\n• The IPA does not usually have separate letters for two sounds if no known language makes a distinction between them, a property known as \"selectiveness\". 2 note 4 However, if a large number of phonemically distinct letters can be derived with a diacritic, that may be used instead. note 5\n\nThe alphabet is designed for transcribing sounds (phones), not phonemes, though it is used for phonemic transcription as well. A few letters that did not indicate specific sounds have been retired – ⟨ˇ⟩, once used for the \"compound\" tone of Swedish and Norwegian, and ⟨ƞ⟩, once used for the moraic nasal of Japanese – though one remains: ⟨ɧ⟩, used for the sj-sound of Swedish. When the IPA is used for broad phonetic or for phonemic transcription, the letter–sound correspondence can be rather loose. The IPA has recommended that more 'familiar' letters be used when that would not cause ambiguity.[13] For example, ⟨e⟩ and ⟨o⟩ for [ɛ] and [ɔ], ⟨t⟩ for [t̪] or [ʈ], ⟨f⟩ for [ɸ], etc. Indeed, in the illustration of Hindi in the IPA Handbook, the letters ⟨c⟩ and ⟨ɟ⟩ are used for /t͡ʃ/ and /d͡ʒ/.\n\nAmong the symbols of the IPA, 107 letters represent consonants and vowels, 31 diacritics are used to modify these, and 17 additional signs indicate suprasegmental qualities such as length, tone, stress, and intonation.[note 6] These are organized into a chart; the chart displayed here is the official chart as posted at the website of the IPA.\n\nThe letters chosen for the IPA are meant to harmonize with the Latin alphabet.[note 7] For this reason, most letters are either Latin or Greek, or modifications thereof. Some letters are neither: for example, the letter denoting the glottal stop, ⟨ʔ⟩, originally had the form of a question mark with the dot removed. A few letters, such as that of the voiced pharyngeal fricative, ⟨ʕ⟩, were inspired by other writing systems (in this case, the Arabic letter ⟨ﻉ⟩, ʿayn, via the reversed apostrophe).[9]\n• The right-swinging tail, as in ⟨ ⟩, indicates retroflex articulation. It originates from the hook of an r.\n• The top hook, as in ⟨ ⟩, indicates implosion.\n• Several nasal consonants are based on the form ⟨ ⟩: ⟨ ⟩. ⟨ ⟩ and ⟨ ⟩ derive from ligatures of gn and ng, and ⟨ ⟩ is an ad hoc imitation of ⟨ ⟩.\n• Letters turned 180 degrees for suggestive shapes, such as ⟨ ⟩ from ⟨ ⟩. note 8 Either the original letter may be reminiscent of the target sound, e.g., ⟨ ⟩ – or the turned one, e.g., ⟨ ⟩. Rotation was popular in the era of mechanical typesetting, as it had the advantage of not requiring the casting of special type for IPA symbols, much as the sorts had traditionally often pulled double duty for and , and , and , and to reduce cost. An example of a font that uses turned small-capital omega for the vowel letter ʊ. The symbol had originally been a small-capital .\n• Among consonant letters, the small capital letters ⟨ ⟩, and also ⟨ ⟩ in extIPA, indicate more guttural sounds than their base letters – ⟨ ⟩ is a late exception. Among vowel letters, small capitals indicate lax vowels. Most of the original small-cap vowel letters have been modified into more distinctive shapes – e.g. ⟨ ⟩ from [ ] – with only ⟨ ⟩ remaining as small capitals.\n\nThe International Phonetic Alphabet is based on the Latin script, and uses as few non-Latin letters as possible.[6] The Association created the IPA so that the sound values of most letters would correspond to \"international usage\" (approximately Classical Latin).[6] Hence, the consonant letters ⟨b⟩, ⟨d⟩, ⟨f⟩, ⟨ɡ⟩, ⟨h⟩, ⟨k⟩, ⟨l⟩, ⟨m⟩, ⟨n⟩, ⟨p⟩, ⟨s⟩, ⟨t⟩, ⟨v⟩, ⟨w⟩, and ⟨z⟩ have more or less their word-initial values in English (g as in gill, h as in hill, though p t k are unaspirated as in spill, still, skill); and the vowel letters ⟨a⟩, ⟨e⟩, ⟨i⟩, ⟨o⟩, ⟨u⟩ correspond to the (long) sound values of Latin: [i] is like the vowel in machine, [u] is as in rule, etc. Other Latin letters, particularly ⟨j⟩, ⟨r⟩ and ⟨y⟩, differ from English, but have their IPA values in Latin or other European languages.\n\nThis basic Latin inventory was extended by adding small-capital and cursive forms, diacritics and rotation. The sound values of these letters are related to those of the original letters, and their derivation may be iconic.[note 9] For example, letters with a rightward-facing hook at the bottom represent retroflex equivalents of the source letters, and small capital letters usually represent uvular equivalents of their source letters.\n\nThere are also several letters from the Greek alphabet, though their sound values may differ from Greek. For most Greek letters, subtly different glyph shapes have been devised for the IPA, specifically ⟨ɑ⟩, ⟨ꞵ⟩, ⟨ɣ⟩, ⟨ɛ⟩, ⟨ɸ⟩, ⟨ꭓ⟩ and ⟨ʋ⟩, which are encoded in Unicode separately from their parent Greek letters. One, however – ⟨θ⟩ – has only its Greek form, while for ⟨ꞵ ~ β⟩ and ⟨ꭓ ~ χ⟩, both Greek and Latin forms are in common use.[16] The tone letters are not derived from an alphabet, but from a pitch trace on a musical scale.\n\nBeyond the letters themselves, there are a variety of secondary symbols which aid in transcription. Diacritic marks can be combined with the letters to add tone and phonetic detail such as secondary articulation. There are also special symbols for prosodic features such as stress and intonation.\n\nThere are two principal types of brackets used to set off (delimit) IPA transcriptions:\n\nAll three of the above are provided by the IPA Handbook. The following are not, but may be seen in IPA transcription or in associated material (especially angle brackets):\n\nSome examples of contrasting brackets in the literature:\n\nIPA letters have cursive forms designed for use in manuscripts and when taking field notes, but the Handbook recommended against their use, as cursive IPA is \"harder for most people to decipher\". A braille representation of the IPA for blind or visually impaired professionals and students has also been developed.[35]\n\nThe International Phonetic Alphabet is occasionally modified by the Association. After each modification, the Association provides an updated simplified presentation of the alphabet in the form of a chart. (See History of the IPA.) Not all aspects of the alphabet can be accommodated in a chart of the size published by the IPA. The alveolo-palatal and epiglottal consonants, for example, are not included in the consonant chart for reasons of space rather than of theory (two additional columns would be required, one between the retroflex and palatal columns and the other between the pharyngeal and glottal columns), and the lateral flap would require an additional row for that single consonant, so they are listed instead under the catchall block of \"other symbols\".[36] The indefinitely large number of tone letters would make a full accounting impractical even on a larger page, and only a few examples are shown, and even the tone diacritics are not complete; the reversed tone letters are not illustrated at all.\n\nThe procedure for modifying the alphabet or the chart is to propose the change in the Journal of the IPA. (See, for example, December 2008 on an open central unrounded vowel[37] and August 2011 on central approximants.)[38] Reactions to the proposal may be published in the same or subsequent issues of the Journal (as in August 2009 on the open central vowel).[39][better source needed] A formal proposal is then put to the Council of the IPA[40][clarification needed] – which is elected by the membership[41] – for further discussion and a formal vote.[42][43]\n\nMany users of the alphabet, including the leadership of the Association itself, deviate from its standardized usage.[note 13] The Journal of the IPA finds it acceptable to mix IPA and extIPA symbols in consonant charts in their articles. (For instance, including the extIPA letter ⟨𝼆⟩, rather than ⟨ʎ̝̊⟩, in an illustration of the IPA.)[44]\n\nOf more than 160 IPA symbols, relatively few will be used to transcribe speech in any one language, with various levels of precision. A precise phonetic transcription, in which sounds are specified in detail, is known as a narrow transcription. A coarser transcription with less detail is called a broad transcription. Both are relative terms, and both are generally enclosed in square brackets.[1] Broad phonetic transcriptions may restrict themselves to easily heard details, or only to details that are relevant to the discussion at hand, and may differ little if at all from phonemic transcriptions, but they make no theoretical claim that all the distinctions transcribed are necessarily meaningful in the language.\n\nFor example, the English word little may be transcribed broadly as [ˈlɪtəl], approximately describing many pronunciations. A narrower transcription may focus on individual or dialectical details: [ˈɫɪɾɫ] in General American, [ˈlɪʔo] in Cockney, or [ˈɫɪːɫ] in Southern US English.\n\nPhonemic transcriptions, which express the conceptual counterparts of spoken sounds, are usually enclosed in slashes (/ /) and tend to use simpler letters with few diacritics. The choice of IPA letters may reflect theoretical claims of how speakers conceptualize sounds as phonemes or they may be merely a convenience for typesetting. Phonemic approximations between slashes do not have absolute sound values. For instance, in English, either the vowel of pick or the vowel of peak may be transcribed as /i/, so that pick, peak would be transcribed as /ˈpik, ˈpiːk/ or as /ˈpɪk, ˈpik/; and neither is identical to the vowel of the French pique, which would also be transcribed /pik/. By contrast, a narrow phonetic transcription of pick, peak, pique could be: [pʰɪk], [pʰiːk], [pikʲ].\n\nIPA is popular for transcription by linguists. Some American linguists, however, use a mix of IPA with Americanist phonetic notation or Sinological phonetic notation or otherwise use nonstandard symbols for various reasons.[45] Authors who employ such nonstandard use are encouraged to include a chart or other explanation of their choices, which is good practice in general, as linguists differ in their understanding of the exact meaning of IPA symbols and common conventions change over time.\n\nMany British dictionaries, including the Oxford English Dictionary and some learner's dictionaries such as the Oxford Advanced Learner's Dictionary and the Cambridge Advanced Learner's Dictionary, now use the International Phonetic Alphabet to represent the pronunciation of words.[46] However, most American (and some British) volumes use one of a variety of pronunciation respelling systems, intended to be more comfortable for readers of English and to be more acceptable across dialects, without the implication of a preferred pronunciation that the IPA might convey. For example, the respelling systems in many American dictionaries (such as Merriam-Webster) use ⟨y⟩ for IPA [ j] and ⟨sh⟩ for IPA [ ʃ ], reflecting the usual spelling of those sounds in English.[47][48][note 14] (In IPA, [y] represents the sound of the French ⟨u⟩, as in tu, and [sh] represents the sequence of consonants in grasshopper.)\n\nThe IPA is also not universal among dictionaries in languages other than English. Monolingual dictionaries of languages with phonemic orthographies generally do not bother with indicating the pronunciation of most words, and tend to use respelling systems for words with unexpected pronunciations. Dictionaries produced in Israel use the IPA rarely and sometimes use the Hebrew alphabet for transcription of foreign words.[note 15] Bilingual dictionaries that translate from foreign languages into Russian usually employ the IPA, but monolingual Russian dictionaries occasionally use pronunciation respelling for foreign words.[note 16] The IPA is more common in bilingual dictionaries, but there are exceptions here too. Mass-market bilingual Czech dictionaries, for instance, tend to use the IPA only for sounds not found in Czech.[note 17]\n\nIPA letters have been incorporated into the alphabets of various languages, notably via the Africa Alphabet in many sub-Saharan languages such as Hausa, Fula, Akan, Gbe languages, Manding languages, Lingala, etc. Capital case variants have been created for use in these languages. For example, Kabiyè of northern Togo has Ɖ ɖ, Ŋ ŋ, Ɣ ɣ, Ɔ ɔ, Ɛ ɛ, Ʋ ʋ. These, and others, are supported by Unicode, but appear in Latin ranges other than the IPA extensions.\n\nIn the IPA itself, however, only lower-case letters are used. The 1949 edition of the IPA handbook indicated that an asterisk ⟨*⟩ might be prefixed to indicate that a word was a proper name,[50] but this convention was not included in the 1999 Handbook, which notes the contrary use of the asterisk as a placeholder for a sound or feature that does not have a symbol.[51]\n\nThe IPA has widespread use among classical singers during preparation as they are frequently required to sing in a variety of foreign languages. They are also taught by vocal coaches to perfect diction and improve tone quality and tuning.[52] Opera librettos are authoritatively transcribed in IPA, such as Nico Castel's volumes[53] and Timothy Cheek's book Singing in Czech.[54] Opera singers' ability to read IPA was used by the site Visual Thesaurus, which employed several opera singers \"to make recordings for the 150,000 words and phrases in VT's lexical database ... for their vocal stamina, attention to the details of enunciation, and most of all, knowledge of IPA\".[55]\n\nThe International Phonetic Association organizes the letters of the IPA into three categories: pulmonic consonants, non-pulmonic consonants, and vowels.[note 18][57][58]\n\nPulmonic consonant letters are arranged singly or in pairs of voiceless (tenuis) and voiced sounds, with these then grouped in columns from front (labial) sounds on the left to back (glottal) sounds on the right. In official publications by the IPA, two columns are omitted to save space, with the letters listed among \"other symbols\" even though theoretically they belong in the main chart.[note 19] They are arranged in rows from full closure (occlusives: stops and nasals) at top, to brief closure (vibrants: trills and taps), to partial closure (fricatives), and finally minimal closure (approximants) at bottom, again with a row left out to save space. In the table below, a slightly different arrangement is made: All pulmonic consonants are included in the pulmonic-consonant table, and the vibrants and laterals are separated out so that the rows reflect the common lenition pathway of stop → fricative → approximant, as well as the fact that several letters pull double duty as both fricative and approximant; affricates may then be created by joining stops and fricatives from adjacent cells. Shaded cells represent articulations that are judged to be impossible or not distinctive.\n\nVowel letters are also grouped in pairs – of unrounded and rounded vowel sounds – with these pairs also arranged from front on the left to back on the right, and from maximal closure at top to minimal closure at bottom. No vowel letters are omitted from the chart, though in the past some of the mid central vowels were listed among the \"other symbols\".\n\nA pulmonic consonant is a consonant made by obstructing the glottis (the space between the vocal folds) or oral cavity (the mouth) and either simultaneously or subsequently letting out air from the lungs. Pulmonic consonants make up the majority of consonants in the IPA, as well as in human language. All consonants in English fall into this category.[60]\n\nThe pulmonic consonant table, which includes most consonants, is arranged in rows that designate manner of articulation, meaning how the consonant is produced, and columns that designate place of articulation, meaning where in the vocal tract the consonant is produced. The main chart includes only consonants with a single place of articulation.\n• In rows where some letters appear in pairs (the obstruents), the letter to the right represents a voiced consonant, except breathy-voiced . 61 In the other rows (the sonorants), the single letter represents a voiced consonant.\n• While IPA provides a single letter for the coronal places of articulation (for all consonants but fricatives), these do not always have to be used exactly. When dealing with a particular language, the letters may be treated as specifically dental, alveolar, or post-alveolar, as appropriate for that language, without diacritics.\n• Shaded areas indicate articulations judged to be impossible.\n• The letters are canonically voiced fricatives but may be used for approximants. note 20\n• In many languages, such as English, and are not actually glottal, fricatives, or approximants. Rather, they are bare phonation. 63\n• It is primarily the shape of the tongue rather than its position that distinguishes the fricatives , , and .\n• are defined as epiglottal fricatives under the \"Other symbols\" section in the official IPA chart, but they may be treated as trills at the same place of articulation as because trilling of the aryepiglottic folds typically co-occurs. 64\n• Some listed phones are not known to exist as phonemes in any language.\n\nNon-pulmonic consonants are sounds whose airflow is not dependent on the lungs. These include clicks (found in the Khoisan languages and some neighboring Bantu languages of Africa), implosives (found in languages such as Sindhi, Hausa, Swahili and Vietnamese), and ejectives (found in many Amerindian and Caucasian languages).\n• Clicks have traditionally been described as consisting of a forward place of articulation, commonly called the click \"type\" or historically the \"influx\", and a rear place of articulation, which when combined with the quality of the click is commonly called the click \"accompaniment\" or historically the \"efflux\". The IPA click letters indicate only the click type (forward articulation and release). Therefore, all clicks require two letters for proper notation: ⟨ ⟩, etc., or with the order reversed if both the forward and rear releases are audible. The letter for the rear articulation is frequently omitted, in which case a ⟨ ⟩ may usually be assumed. However, some researchers dispute the idea that clicks should be analyzed as doubly articulated, as the traditional transcription implies, and analyze the rear occlusion as solely a part of the airstream mechanism. 65 In transcriptions of such approaches, the click letter represents both places of articulation, with the different letters representing the different click types, and diacritics are used for the elements of the accompaniment: ⟨ ⟩, etc.\n• Letters for the voiceless implosives ⟨ ⟩ are no longer supported by the IPA, though they remain in Unicode. Instead, the IPA typically uses the voiced equivalent with a voiceless diacritic: ⟨ ⟩, etc.\n• The letter for the retroflex implosive, , is not \"explicitly IPA approved\", 66 but has the expected form if such a symbol were to be approved.\n• The ejective diacritic is placed at the right-hand margin of the consonant, rather than immediately after the letter for the stop: ⟨ ⟩, ⟨ ⟩. In imprecise transcription, it often stands in for a superscript glottal stop in glottalized but pulmonic sonorants, such as , , , – also transcribable as creaky , , , .\n\nAffricates and co-articulated stops are represented by two letters joined by a tie bar, either above or below the letters with no difference in meaning.[note 21] Affricates are optionally represented by ligatures – e.g. ⟨ʧ, ʤ ⟩ – though this is no longer official IPA usage[1] because a great number of ligatures would be required to represent all affricates this way. Alternatively, a superscript notation for a consonant release is sometimes used to transcribe affricates, for example ⟨tˢ⟩ for [t͜s], paralleling [kˣ] ~ [k͜x]. The letters for the palatal plosives ⟨c⟩ and ⟨ɟ⟩ are often used as a convenience for [t͜ʃ] and [d͜ʒ] or similar affricates, even in official IPA publications, so they must be interpreted with care.\n\nCo-articulated consonants are sounds that involve two simultaneous places of articulation (are pronounced using two parts of the vocal tract). In English, the [w] in \"went\" is a coarticulated consonant, being pronounced by rounding the lips and raising the back of the tongue. Similar sounds are [ʍ] and [ɥ]. In some languages, plosives can be double-articulated, for example in the name of Laurent Gbagbo.\n• , the Swedish sj-sound, is described by the IPA as a \"simultaneous and \", but it is unlikely such a simultaneous fricative actually exists in any language. 68\n• Multiple tie bars can be used: ⟨ ⟩ or ⟨ ⟩. For instance, a pre-voiced velar affricate may be transcribed as ⟨ ⟩\n• If a diacritic needs to be placed on or under a tie bar, the combining grapheme joiner (U+034F) needs to be used, as in 'chewed' (Margi). Font support is spotty, however.\n\nThe IPA defines a vowel as a sound which occurs at a syllable center.[69] Below is a chart depicting the vowels of the IPA. The IPA maps the vowels according to the position of the tongue.\n\nThe vertical axis of the chart is mapped by vowel height. Vowels pronounced with the tongue lowered are at the bottom, and vowels pronounced with the tongue raised are at the top. For example, [ɑ] (the first vowel in father) is at the bottom because the tongue is lowered in this position. [i] (the vowel in \"meet\") is at the top because the sound is said with the tongue raised to the roof of the mouth.\n\nIn a similar fashion, the horizontal axis of the chart is determined by vowel backness. Vowels with the tongue moved towards the front of the mouth (such as [ɛ], the vowel in \"met\") are to the left in the chart, while those in which it is moved to the back (such as [ʌ], the vowel in \"but\") are placed to the right in the chart.\n\nIn places where vowels are paired, the right represents a rounded vowel (in which the lips are rounded) while the left is its unrounded counterpart.\n\nDiphthongs are typically specified with a non-syllabic diacritic, as in ⟨ui̯⟩ or ⟨u̯i⟩, or with a superscript for the on- or off-glide, as in ⟨uⁱ⟩ or ⟨ᵘi⟩. Sometimes a tie bar is used: ⟨u͜i⟩, especially when it is difficult to tell if the diphthong is characterized by an on-glide or an off-glide or when it is variable.\n• ⟨ ⟩ officially represents a front vowel, but there is little if any distinction between front and central open vowels (see Vowel § Acoustics), and ⟨ ⟩ is frequently used for an open central vowel. 45 If disambiguation is required, the retraction diacritic or the centralized diacritic may be added to indicate an open central vowel, as in ⟨ ⟩ or ⟨ ⟩.\n\nDiacritics are used for phonetic detail. They are added to IPA letters to indicate a modification or specification of that letter's normal pronunciation.[70]\n\nBy being made superscript, any IPA letter may function as a diacritic, conferring elements of its articulation to the base letter. Those superscript letters listed below are specifically provided for by the IPA Handbook; other uses can be illustrated with ⟨tˢ⟩ ([t] with fricative release), ⟨ᵗs⟩ ([s] with affricate onset), ⟨ⁿd⟩ (prenasalized [d]), ⟨bʱ⟩ ([b] with breathy voice), ⟨mˀ⟩ (glottalized [m]), ⟨sᶴ⟩ ([s] with a flavor of [ʃ], i.e. a voiceless alveolar retracted sibilant), ⟨oᶷ⟩ ([o] with diphthongization), ⟨ɯᵝ⟩ (compressed [ɯ]). Superscript diacritics placed after a letter are ambiguous between simultaneous modification of the sound and phonetic detail at the end of the sound. For example, labialized ⟨kʷ⟩ may mean either simultaneous [k] and [w] or else [k] with a labialized release. Superscript diacritics placed before a letter, on the other hand, normally indicate a modification of the onset of the sound (⟨mˀ⟩ glottalized [m], ⟨ˀm⟩ [m] with a glottal onset). (See § Superscript IPA.)\n\nSubdiacritics (diacritics normally placed below a letter) may be moved above a letter to avoid conflict with a descender, as in voiceless ⟨ŋ̊⟩.[70] The raising and lowering diacritics have optional spacing forms ⟨˔⟩, ⟨˕⟩ that avoid descenders.\n\nThe state of the glottis can be finely transcribed with diacritics. A series of alveolar plosives ranging from open-glottis to closed-glottis phonation is:\n\nAdditional diacritics are provided by the Extensions to the IPA for speech pathology.\n\nThese symbols describe the features of a language above the level of individual consonants and vowels, that is, at the level of syllable, word or phrase. These include prosody, pitch, length, stress, intensity, tone and gemination of the sounds of a language, as well as the rhythm and intonation of speech.[72] Various ligatures of pitch/tone letters and diacritics are provided for by the Kiel Convention and used in the IPA Handbook despite not being found in the summary of the IPA alphabet found on the one-page chart.\n\nUnder capital letters below we will see how a carrier letter may be used to indicate suprasegmental features such as labialization or nasalization. Some authors omit the carrier letter, for e.g. suffixed [kʰuˣt̪s̟]ʷ or prefixed [ʷkʰuˣt̪s̟],[note 22] or place a spacing variant of a diacritic such as ⟨˔⟩ or ⟨˜⟩ at the beginning or end of a word to indicate that it applies to the entire word.[note 23]\n\nThe old staveless tone letters, which are effectively obsolete, include high ⟨ˉe⟩, mid ⟨˗e⟩, low ⟨ˍe⟩, rising ⟨ˊe⟩, falling ⟨ˋe⟩, low rising ⟨ˏe⟩ and low falling ⟨ˎe⟩.\n\nOfficially, the stress marks ⟨ˈ ˌ⟩ appear before the stressed syllable, and thus mark the syllable boundary as well as stress (though the syllable boundary may still be explicitly marked with a period).[75] Occasionally the stress mark is placed immediately before the nucleus of the syllable, after any consonantal onset.[76] In such transcriptions, the stress mark does not mark a syllable boundary. The primary stress mark may be doubled ⟨ˈˈ⟩ for extra stress (such as prosodic stress). The secondary stress mark is sometimes seen doubled ⟨ˌˌ⟩ for extra-weak stress, but this convention has not been adopted by the IPA.[75] Some dictionaries place both stress marks before a syllable, ⟨¦⟩, to indicate that pronunciations with either primary or secondary stress are heard, though this is not IPA usage.[note 28]\n\nThere are three boundary markers: ⟨.⟩ for a syllable break, ⟨|⟩ for a minor prosodic break and ⟨‖⟩ for a major prosodic break. The tags 'minor' and 'major' are intentionally ambiguous. Depending on need, 'minor' may vary from a foot break to a break in list-intonation to a continuing–prosodic unit boundary (equivalent to a comma), and while 'major' is often any intonation break, it may be restricted to a final–prosodic unit boundary (equivalent to a period). The 'major' symbol may also be doubled, ⟨‖‖⟩, for a stronger break.[note 29]\n\nAlthough not part of the IPA, the following additional boundary markers are often used in conjunction with the IPA: ⟨μ⟩ for a mora or mora boundary, ⟨σ⟩ for a syllable or syllable boundary, ⟨+⟩ for a morpheme boundary, ⟨#⟩ for a word boundary (may be doubled, ⟨##⟩, for e.g. a breath-group boundary),[78] ⟨$⟩ for a phrase or intermediate boundary and ⟨%⟩ for a prosodic boundary. For example, C# is a word-final consonant, %V a post-pausa vowel, and σC a syllable-initial consonant.\n\n⟨ꜛ ꜜ⟩ are defined in the Handbook as \"upstep\" and \"downstep\", concepts from tonal languages. However, the upstep symbol can also be used for pitch reset, and the IPA Handbook uses it for prosody in the illustration for Portuguese, a non-tonal language.\n\nPhonetic pitch and phonemic tone may be indicated by either diacritics placed over the nucleus of the syllable – e.g., high-pitch ⟨é⟩ – or by Chao tone letters placed either before or after the word or syllable. There are three graphic variants of the tone letters: with or without a stave, and facing left or facing right from the stave. The stave was introduced with the 1989 Kiel Convention, as was the option of placing a staved letter after the word or syllable, while retaining the older conventions. There are therefore six ways to transcribe pitch/tone in the IPA: i.e., ⟨é⟩, ⟨˦e⟩, ⟨e˦⟩, ⟨꜓e⟩, ⟨e꜓⟩ and ⟨ˉe⟩ for a high pitch/tone.[75][79][80] Of the tone letters, only left-facing staved letters and a few representative combinations are shown in the summary on the Chart, and in practice it is currently more common for tone letters to occur after the syllable/word than before, as in the Chao tradition. Placement before the word is a carry-over from the pre-Kiel IPA convention, as is still the case for the stress and upstep/downstep marks. The IPA endorses the Chao tradition of using the left-facing tone letters, ⟨˥ ˦ ˧ ˨ ˩⟩, for underlying tone, and the right-facing letters, ⟨꜒ ꜓ ꜔ ꜕ ꜖⟩, for surface tone, as occurs in tone sandhi, and for the intonation of non-tonal languages.[note 30] In the Portuguese illustration in the 1999 Handbook, tone letters are placed before a word or syllable to indicate prosodic pitch (equivalent to [↗︎] global rise and [↘︎] global fall, but allowing more precision), and in the Cantonese illustration they are placed after a word/syllable to indicate lexical tone. Theoretically therefore prosodic pitch and lexical tone could be simultaneously transcribed in a single text, though this is not a formalized distinction.\n\nRising and falling pitch, as in contour tones, are indicated by combining the pitch diacritics and letters in the table, such as grave plus acute for rising [ě] and acute plus grave for falling [ê]. Only six combinations of two diacritics are supported, and only across three levels (high, mid, low), despite the diacritics supporting five levels of pitch in isolation. The four other explicitly approved rising and falling diacritic combinations are high/mid rising [e᷄], low rising [e᷅], high falling [e᷇], and low/mid falling [e᷆].[note 31]\n\nThe Chao tone letters, on the other hand, may be combined in any pattern, and are therefore used for more complex contours and finer distinctions than the diacritics allow, such as mid-rising [e˨˦], extra-high falling [e˥˦], etc. There are 20 such possibilities. However, in Chao's original proposal, which was adopted by the IPA in 1989, he stipulated that the half-high and half-low letters ⟨˦ ˨⟩ may be combined with each other, but not with the other three tone letters, so as not to create spuriously precise distinctions. With this restriction, there are 8 possibilities.[81]\n\nThe old staveless tone letters tend to be more restricted than the staved letters, though not as restricted as the diacritics. Officially, they support as many distinctions as the staved letters,[note 32] but typically only three pitch levels are distinguished. Unicode supports default or high-pitch ⟨ˉ ˊ ˋ ˆ ˇ ˜ ˙⟩ and low-pitch ⟨ˍ ˏ ˎ ꞈ ˬ ˷⟩. Only a few mid-pitch tones are supported – such as ⟨˗ ˴⟩ – and then only accidentally.\n\nAlthough tone diacritics and tone letters are presented as equivalent on the chart, \"this was done only to simplify the layout of the chart. The two sets of symbols are not comparable in this way.\"[82] Using diacritics, a high tone is ⟨é⟩ and a low tone is ⟨è⟩; in tone letters, these are ⟨e˥⟩ and ⟨e˩⟩. One can double the diacritics for extra-high ⟨e̋⟩ and extra-low ⟨ȅ⟩; there is no parallel to this using tone letters. Instead, tone letters have mid-high ⟨e˦⟩ and mid-low ⟨e˨⟩; again, there is no equivalent among the diacritics. Thus in a three-register tone system, ⟨é ē è⟩ are equivalent to ⟨e˥ e˧ e˩⟩, while in a four-register system, ⟨e̋ é è ȅ⟩ may be equivalent to ⟨e˥ e˦ e˨ e˩⟩.[75]\n\nThe correspondence breaks down even further once they start combining. For more complex tones, one may combine three or four tone diacritics in any permutation,[75] though in practice only generic peaking (rising-falling) e᷈ and dipping (falling-rising) e᷉ combinations are used. Chao tone letters are required for finer detail (e˧˥˧, e˩˨˩, e˦˩˧, e˨˩˦, etc.). Although only 10 peaking and dipping tones were proposed in Chao's original, limited set of tone letters, phoneticians often make finer distinctions, and indeed an example is found on the IPA Chart.[note 33] The system allows the transcription of 112 peaking and dipping pitch contours, including tones that are level for part of their length.\n\nMore complex contours are possible. Chao gave an example of [꜔꜒꜖꜔] (mid-high-low-mid) from English prosody.[81]\n\nChao tone letters generally appear after each syllable, for a language with syllable tone – ⟨a˧vɔ˥˩⟩ – or after the phonological word, for a language with word tone (⟨avɔ˧˥˩⟩). The IPA gives the option of placing the tone letters before the word or syllable – ⟨˧a˥˩vɔ⟩, ⟨˧˥˩avɔ⟩ – but this is rare for lexical tone. Reversed tone letters may be used to clarify that they apply to the following rather than to the preceding syllable – ⟨꜔a꜒꜖vɔ⟩, ⟨꜔꜒꜖avɔ⟩. The staveless letters are not directly supported by Unicode, but some fonts allow the stave in Chao tone letters to be suppressed.\n\nIPA diacritics may be doubled to indicate an extra degree (greater intensity) of the feature indicated.[83] This is a productive process, but apart from extra-high and extra-low tones being marked by doubled high- and low-tone diacritics, ⟨ə̋, ə̏⟩, the major prosodic break ⟨‖⟩ being marked as a doubled minor break ⟨|⟩, and a couple other instances, such usage is not enumerated by the IPA.\n\nFor example, the stress mark may be doubled (or even tripled, etc.) to indicate an extra degree of stress, such as prosodic stress in English.[84] An example in French, with a single stress mark for normal prosodic stress at the end of each prosodic unit (marked as a minor prosodic break), and a double or even triple stress mark for contrastive/emphatic stress: [ˈˈɑ̃ːˈtre | məˈsjø ‖ ˈˈvwala maˈdam ‖] Entrez monsieur, voilà madame.[85] Similarly, a doubled secondary stress mark ⟨ˌˌ⟩ is commonly used for tertiary (extra-light) stress, though a proposal to officially adopt this was rejected.[86] In a similar vein, the effectively obsolete staveless tone letters were once doubled for an emphatic rising intonation ⟨˶⟩ and an emphatic falling intonation ⟨˵⟩.[87]\n\nLength is commonly extended by repeating the length mark, which may be phonetic, as in [ĕ e eˑ eː eːˑ eːː] etc., as in English shhh! [ʃːːː], or phonemic, as in the \"overlong\" segments of Estonian:\n\nDelimiters are similar: double slashes indicate extra phonemic (morpho-phonemic), double square brackets especially precise transcription, and double parentheses especially unintelligible.\n• Nasalization, as in Palantla Chinantec lightly nasalized vs heavily nasalized , 89 though some care can be needed to distinguish this from the extIPA diacritic for velopharyngeal frication in disordered speech, , which has also been analyzed as extreme nasalization.\n• Especially lowered, e.g. (or , if the former symbol does not display properly) for as a weak fricative in some pronunciations of register. 91\n• Especially retracted, e.g. or , note 37 83 92 though some care might be needed to distinguish this from indications of alveolar or alveolarized articulation in extIPA, e.g. .\n• The transcription of strident and harsh voice as extra-creaky may be motivated by the similarities of these phonations.\n\nThe extIPA provides combining parentheses for weak intensity, which when combined with a doubled diacritic indicate an intermediate degree. For instance, increasing degrees of nasalization of the vowel [e] might be written ⟨e ẽ᪻ ẽ ẽ̃᪻ ẽ̃⟩.\n\nAs noted above, IPA letters are often used quite loosely in broad transcription if no ambiguity would arise in a particular language. Because of that, IPA letters have not generally been created for sounds that are not distinguished in individual languages. A distinction between voiced fricatives and approximants is only partially implemented by the IPA, for example. Even with the relatively recent addition of the palatal fricative ⟨ʝ⟩ and the velar approximant ⟨ɰ⟩ to the alphabet, other letters, though defined as fricatives, are often ambiguous between fricative and approximant. For forward places, ⟨β⟩ and ⟨ð⟩ can generally be assumed to be fricatives unless they carry a lowering diacritic. Rearward, however, ⟨ʁ⟩ and ⟨ʕ⟩ are perhaps more commonly intended to be approximants even without a lowering diacritic. ⟨h⟩ and ⟨ɦ⟩ are similarly either fricatives or approximants, depending on the language, or even glottal \"transitions\", without that often being specified in the transcription.\n\nAnother common ambiguity is among the letters for palatal consonants. ⟨c⟩ and ⟨ɟ⟩ are not uncommonly used as a typographic convenience for affricates, typically [t͜ʃ] and [d͜ʒ], while ⟨ɲ⟩ and ⟨ʎ⟩ are commonly used for palatalized alveolar [n̠ʲ] and [l̠ʲ]. To some extent this may be an effect of analysis, but it is common to match up single IPA letters to the phonemes of a language, without overly worrying about phonetic precision.\n\nIt has been argued that the lower-pharyngeal (epiglottal) fricatives ⟨ʜ⟩ and ⟨ʢ⟩ are better characterized as trills, rather than as fricatives that have incidental trilling.[94] This has the advantage of merging the upper-pharyngeal fricatives [ħ, ʕ] together with the epiglottal plosive [ʡ] and trills [ʜ ʢ] into a single pharyngeal column in the consonant chart. However, in Shilha Berber the epiglottal fricatives are not trilled.[95][96] Although they might be transcribed ⟨ħ̠ ʢ̠⟩ to indicate this, the far more common transcription is ⟨ʜ ʢ⟩, which is therefore ambiguous between languages.\n\nAmong vowels, ⟨a⟩ is officially a front vowel, but is more commonly treated as a central vowel. The difference, to the extent it is even possible, is not phonemic in any language.\n\nFor all phonetic notation, it is good practice for an author to specify exactly what they mean by the symbols that they use.\n\nSuperscript IPA letters are used to indicate secondary aspects of articulation. These may be aspects of simultaneous articulation that are considered to be in some sense less dominant than the basic sound, or may be transitional articulations that are interpreted as secondary elements.[97] Examples include secondary articulation; onsets, releases, aspiration and other transitions; shades of sound; light epenthetic sounds and incompletely articulated sounds. Morphophonemically, superscripts may be used for assimilation, e.g. ⟨aʷ⟩ for the effect of labialization on a vowel /a/, which may be realized as phonemic /o/.[98] The IPA and ICPLA endorse Unicode encoding of superscript variants of all contemporary segmental letters in the IPA proper and of all additional fricatives in extIPA, including the \"implicit\" IPA retroflex letters ⟨ꞎ 𝼅 𝼈 ᶑ 𝼊 ⟩.[44][99][100]\n\nSuperscripts are often used as a substitute for the tie bar, for example ⟨tᶴ⟩ for [t͜ʃ] and ⟨kᵖ⟩ or ⟨ᵏp⟩ for [k͜p]. However, in precise notation there is a difference between a fricative release in [tᶴ] and the affricate [t͜ʃ], between a velar onset in [ᵏp] and doubly articulated [k͜p].[101]\n\nSuperscript letters can be meaningfully modified by combining diacritics, just as baseline letters can. For example, a superscript dental nasal in ⟨ⁿ̪d̪⟩, a superscript voiceless velar nasal in ⟨ᵑ̊ǂ⟩, and labial-velar prenasalization in ⟨ᵑ͡ᵐɡ͡b⟩. Although the diacritic may seem a bit oversized compared to the superscript letter it modifies, e.g. ⟨ᵓ̃⟩, this can be an aid to legibility, just as it is with the composite superscript c-cedilla ⟨ᶜ̧⟩ and rhotic vowels ⟨ᵊ˞ ᶟ˞⟩. Superscript length marks can be used to indicate the length of aspiration of a consonant, e.g. [pʰ tʰ𐞂 kʰ𐞁]. Another option is to use extIPA parentheses and a doubled diacritic: ⟨p⁽ʰ⁾ tʰ kʰʰ⟩.[44]\n\nA number of IPA letters and diacritics have been retired or replaced over the years. This number includes duplicate symbols, symbols that were replaced due to user preference, and unitary symbols that were rendered with diacritics or digraphs to reduce the inventory of the IPA. The rejected symbols are now considered obsolete, though some are still seen in the literature.\n\nThe IPA once had several pairs of duplicate symbols from alternative proposals, but eventually settled on one or the other. An example is the vowel letter ⟨ɷ⟩, rejected in favor of ⟨ʊ⟩. Affricates were once transcribed with ligatures, such as ⟨ʧ ʤ ⟩ (and others, some of which not found in Unicode). These have been officially retired but are still used. Letters for specific combinations of primary and secondary articulation have also been mostly retired, with the idea that such features should be indicated with tie bars or diacritics: ⟨ƍ⟩ for [zʷ] is one. In addition, the rare voiceless implosives, ⟨ƥ ƭ ƈ ƙ ʠ ⟩, were dropped soon after their introduction and are now usually written ⟨ɓ̥ ɗ̥ ʄ̊ ɠ̊ ʛ̥ ⟩. The original set of click letters, ⟨ʇ, ʗ, ʖ, ʞ⟩, was retired but is still sometimes seen, as the current pipe letters ⟨ǀ, ǃ, ǁ, ǂ⟩ can cause problems with legibility, especially when used with brackets ([ ] or / /), the letter ⟨l⟩ (small L), or the prosodic marks ⟨|, ‖⟩. (For this reason, some publications which use the current IPA pipe letters disallow IPA brackets.)[102]\n\nIndividual non-IPA letters may find their way into publications that otherwise use the standard IPA. This is especially common with:\n• Affricates, such as the Americanist barred lambda ⟨ ⟩ for or ⟨ ⟩ for . note 38\n• Digits for tonal phonemes that have conventional numbers in a local tradition, such as the four tones of Standard Chinese. This may be more convenient for comparison between related languages and dialects than a phonetic transcription would be, because tones vary more unpredictably than segmental phonemes do.\n• Digits for tone levels, which are simpler to typeset, though the lack of standardization can cause confusion (e.g. ⟨ ⟩ is high tone in some languages but low tone in others; ⟨ ⟩ may be high, medium or low tone, depending on the local convention).\n• Iconic extensions of standard IPA letters that are implicit in the alphabet, such as retroflex ⟨ ⟩ and ⟨ ⟩. These are referred to in the Handbook and have been included in Unicode at IPA request.\n• Even presidents of the IPA have used para-IPA notation, such as resurrecting the old diacritic ⟨ ⟩ for purely labialized sounds (not simultaneously velarized), the lateral fricative letter ⟨ ⟩, and either the old dot diacritic ⟨ ⟩ or the novel letters ⟨ ⟩ for the not-quite-retroflex fricatives of Polish sz, ż and of Russian ш ж.\n\nIn addition, it is common to see ad hoc typewriter substitutions, generally capital letters, for when IPA support is not available, e.g. S for ⟨ ʃ ⟩. (See also SAMPA and X-SAMPA substitute notation.)\n\nThe Extensions to the International Phonetic Alphabet for Disordered Speech, commonly abbreviated \"extIPA\" and sometimes called \"Extended IPA\", are symbols whose original purpose was to accurately transcribe disordered speech. At the Kiel Convention in 1989, a group of linguists drew up the initial extensions,[note 39] which were based on the previous work of the PRDS (Phonetic Representation of Disordered Speech) Group in the early 1980s.[104] The extensions were first published in 1990, then modified, and published again in 1994 in the Journal of the International Phonetic Association, when they were officially adopted by the ICPLA.[105] While the original purpose was to transcribe disordered speech, linguists have used the extensions to designate a number of sounds within standard communication, such as hushing, gnashing teeth, and smacking lips,[2] as well as regular lexical sounds such as lateral fricatives that do not have standard IPA symbols.\n\nIn addition to the Extensions to the IPA for disordered speech, there are the conventions of the Voice Quality Symbols, which include a number of symbols for additional airstream mechanisms and secondary articulations in what they call \"voice quality\".\n\nCapital letters and various characters on the number row of the keyboard are commonly used to extend the alphabet in various ways.\n\nThere are various punctuation-like conventions for linguistic transcription that are commonly used together with IPA. Some of the more common are:\n\nFull capital letters are not used as IPA symbols, except as typewriter substitutes (e.g. N for ⟨ŋ⟩, S for ⟨ ʃ ⟩, O for ⟨ɔ⟩ – see SAMPA). They are, however, often used in conjunction with the IPA in two cases:\n• for (archi)phonemes and for natural classes of sounds (that is, as wildcards). The extIPA chart, for example, uses capital letters as wildcards in its illustrations.\n• as carrying letters for the Voice Quality Symbols.\n\nWildcards are commonly used in phonology to summarize syllable or word shapes, or to show the evolution of classes of sounds. For example, the possible syllable shapes of Mandarin can be abstracted as ranging from /V/ (an atonic vowel) to /CGVNᵀ/ (a consonant-glide-vowel-nasal syllable with tone), and word-final devoicing may be schematized as C → C̥/_#. They are also used in historical linguistics for a sound that is posited but whose nature has not been determined beyond some generic category such as {nasal} or {uvular}. In speech pathology, capital letters represent indeterminate sounds, and may be superscripted to indicate they are weakly articulated: e.g. [ᴰ] is a weak indeterminate alveolar, [ᴷ] a weak indeterminate velar.[109]\n\nThere is a degree of variation between authors as to the capital letters used, but ⟨C⟩ for {consonant}, ⟨V⟩ for {vowel} and ⟨N⟩ for {nasal} are ubiquitous in English-language material. Other common conventions are ⟨T⟩ for {tone/accent} (tonicity), ⟨P⟩ for {plosive}, ⟨F⟩ for {fricative}, ⟨S⟩ for {sibilant},[note 40] ⟨G⟩ for {glide/semivowel}, ⟨L⟩ for {lateral} or {liquid}, ⟨R⟩ for {rhotic} or {resonant/sonorant},[note 41] ⟨₵⟩ for {obstruent}, ⟨Ʞ⟩ for {click}, ⟨A, E, O, Ɨ, U⟩ for {open, front, back, close, rounded vowel}[note 42] and ⟨B, D, Ɉ, K, Q, Φ, H⟩ for {labial, alveolar, post-alveolar/palatal, velar, uvular, pharyngeal, glottal[note 43] consonant}, respectively, and ⟨X⟩ for {any sound}, as in ⟨CVX⟩ for a heavy syllable {CVC, CVV̯, CVː}. The letters can be modified with IPA diacritics, for example ⟨Cʼ⟩ for {ejective}, ⟨Ƈ ⟩ for {implosive}, ⟨N͡C⟩ or ⟨ᴺC⟩ for {prenasalized consonant}, ⟨Ṽ⟩ for {nasal vowel}, ⟨CʰV́⟩ for {aspirated CV syllable with high tone}, ⟨S̬⟩ for {voiced sibilant}, ⟨N̥⟩ for {voiceless nasal}, ⟨P͡F⟩ or ⟨Pꟳ⟩ for {affricate}, ⟨Cᴳ⟩ for a consonant with a glide as secondary articulation (e.g. ⟨Cʲ⟩ for {palatalized consonant} and ⟨Cʷ⟩ for {labialized consonant}) and ⟨D̪⟩ for {dental consonant}. ⟨H⟩, ⟨M⟩, ⟨L⟩ are also commonly used for high, mid and low tone, with ⟨LH⟩ for rising tone and ⟨HL⟩ for falling tone, rather than transcribing them overly precisely with IPA tone letters or with ambiguous digits.[note 44]\n\nTypical examples of archiphonemic use of capital letters are ⟨I⟩ for the Turkish harmonic vowel set {i y ɯ u};[note 45] ⟨D⟩ for the conflated flapped middle consonant of American English writer and rider; ⟨N⟩ for the homorganic syllable-coda nasal of languages such as Spanish and Japanese (essentially equivalent to the wild-card usage of the letter); and ⟨R⟩ in cases where a phonemic distinction between trill /r/ and flap /ɾ/ is conflated, as in Spanish enrejar /eNreˈxaR/ (the n is homorganic and the first r is a trill, but the second r is variable).[110] Similar usage is found for phonemic analysis, where a language does not distinguish sounds that have separate letters in the IPA. For instance, Castillian Spanish has been analyzed as having phonemes /Θ/ and /S/, which surface as [θ] and [s] in voiceless environments and as [ð] and [z] in voiced environments (e.g. hazte /ˈaΘte/ → [ˈaθte], vs hazme /ˈaΘme/ → [ˈaðme], or las manos /laS ˈmanoS/ → [lazˈmanos]).[111]\n\n⟨V⟩, ⟨F⟩ and ⟨C⟩ have completely different meanings as Voice Quality Symbols, where they stand for \"voice\" (VoQS jargon for secondary articulation),[note 46] \"falsetto\" and \"creak\". These three letters may take diacritics to indicate what kind of voice quality an utterance has, and may be used as carrier letters to extract a suprasegmental feature that occurs on all susceptible segments in a stretch of IPA. For instance, the transcription of Scottish Gaelic [kʷʰuˣʷt̪ʷs̟ʷ] 'cat' and [kʷʰʉˣʷt͜ʃʷ] 'cats' (Islay dialect) can be made more economical by extracting the suprasegmental labialization of the words: Vʷ[kʰuˣt̪s̟] and Vʷ[kʰʉˣt͜ʃ].[112] The conventional wildcards ⟨X⟩ or ⟨C⟩ might be used instead of VoQS ⟨V⟩ so that the reader does not misinterpret ⟨Vʷ⟩ as meaning that only vowels are labialized (i.e. Xʷ[kʰuˣt̪s̟] for all segments labialized, Cʷ[kʰuˣt̪s̟] for all consonants labialized), or the carrier letter may be omitted altogether (e.g. ʷ[kʰuˣt̪s̟], [ʷkʰuˣt̪s̟] or [kʰuˣt̪s̟]ʷ). (See § Suprasegmentals for other transcription conventions.)\n\nThis summary is to some extent valid internationally, but linguistic material written in other languages may have different associations with capital letters used as wildcards. For example, in German ⟨K⟩ and ⟨V⟩ are used for Konsonant (consonant) and Vokal (vowel); in Russian, ⟨С⟩ and ⟨Г⟩ are used for согласный (soglasnyj, consonant) and гласный (glasnyj, vowel). In French, tone may be transcribed with ⟨H⟩ and ⟨B⟩ for haut (high) and bas (low).[113]\n\nThe blank cells on the summary IPA chart can be filled without much difficulty if the need arises.\n\nThe missing retroflex letters, namely ⟨ᶑ ꞎ 𝼅 𝼈 𝼊 ⟩, are \"implicit\" in the alphabet, and the IPA supported their adoption into Unicode.[44] Attested in the literature are the retroflex implosive ⟨ᶑ ⟩, the voiceless retroflex lateral fricative ⟨ꞎ ⟩, the retroflex lateral flap ⟨𝼈 ⟩ and the retroflex click ⟨𝼊 ⟩; the first is also mentioned in the IPA Handbook, and the lateral fricatives are provided for by the extIPA.\n\nThe epiglottal trill is arguably covered by the generally trilled epiglottal \"fricatives\" ⟨ʜ ʢ⟩. Ad hoc letters for near-close central vowels, ⟨ᵻ ᵿ⟩, are used in some descriptions of English, though those are specifically reduced vowels – forming a set with the IPA reduced vowels ⟨ə ɐ⟩ – and the simple points in vowel space are easily transcribed with diacritics: ⟨ɪ̈ ʊ̈⟩ or ⟨ɨ̞ ʉ̞⟩. Diacritics are able to fill in most of the remainder of the charts.[note 47] If a sound cannot be transcribed, an asterisk ⟨*⟩ may be used, either as a letter or as a diacritic (as in ⟨k*⟩ sometimes seen for the Korean \"fortis\" velar).\n\nRepresentations of consonant sounds outside of the core set are created by adding diacritics to letters with similar sound values. The Spanish bilabial and dental approximants are commonly written as lowered fricatives, [β̞] and [ð̞] respectively.[note 48] Similarly, voiced lateral fricatives can be written as raised lateral approximants, [ɭ˔ ʎ̝ ʟ̝], though the extIPA also provides ⟨𝼅⟩ for the first of these. A few languages such as Banda have a bilabial flap as the preferred allophone of what is elsewhere a labiodental flap. It has been suggested that this be written with the labiodental flap letter and the advanced diacritic, [ⱱ̟].[115] Similarly, a labiodental trill would be written [ʙ̪] (bilabial trill and the dental sign), and the labiodental plosives are now universally ⟨p̪ b̪⟩ rather than the ad hoc letters ⟨ȹ ȸ⟩ once found in Bantuist literature. Other taps can be written as extra-short plosives or laterals, e.g. [ ɟ̆ ɢ̆ ʟ̆], though in some cases the diacritic would need to be written below the letter. A retroflex trill can be written as a retracted [r̠], just as non-subapical retroflex fricatives sometimes are. The remaining pulmonic consonants – the uvular laterals ([ʟ̠ 𝼄̠ ʟ̠˔]) and the palatal trill – while not strictly impossible, are very difficult to pronounce and are unlikely to occur even as allophones in the world's languages.\n\nThe vowels are similarly manageable by using diacritics for raising, lowering, fronting, backing, centering, and mid-centering.[note 49] For example, the unrounded equivalent of [ʊ] can be transcribed as mid-centered [ɯ̽], and the rounded equivalent of [æ] as raised [ɶ̝] or lowered [œ̞] (though for those who conceive of vowel space as a triangle, simple [ɶ] already is the rounded equivalent of [æ]). True mid vowels are lowered [e̞ ø̞ ɘ̞ ɵ̞ ɤ̞ o̞] or raised [ɛ̝ œ̝ ɜ̝ ɞ̝ ʌ̝ ɔ̝], while centered [ɪ̈ ʊ̈] and [ä] (or, less commonly, [ɑ̈]) are near-close and open central vowels, respectively.\n\nThe only known vowels that cannot be represented in this scheme are vowels with unexpected roundedness. For unambiguous transcription, such sounds would require dedicated diacritics. Possibilities include ⟨ʏʷ⟩ or ⟨ɪʷ⟩ for protrusion and ⟨uᵝ⟩ (or VoQS ⟨ɯᶹ⟩) for compression. However, these transcriptions suggest that the sounds are diphthongs, and so while they may be clear for a language like Swedish where they are diphthongs, they may be misleading for languages such as Japanese where they are monophthongs. The extIPA 'spread' diacritic ⟨◌͍⟩ is sometimes seen for compressed ⟨u͍⟩, ⟨o͍⟩, ⟨ɔ͍⟩, ⟨ɒ͍⟩, though again the intended meaning would need to be explained or they would be interpreted as being spread the way that cardinal ⟦i⟧ is. For protrusion (w-like labialization without velarization), Ladefoged & Maddieson use the old IPA omega diacritic for labialization, ⟨◌̫⟩, for protruded ⟨y᫇⟩, ⟨ʏ̫⟩, ⟨ø̫⟩, ⟨œ̫⟩. This is an adaptation of an old IPA convention of rounding an unrounded vowel letter like i with a subscript omega (⟨◌̫⟩) and unrounding a rounded letter like u with a subscript turned omega.[117] As of 2024 , the turned omega diacritic is in the pipeline for Unicode, and is under consideration for compression in extIPA.[118] Kelly & Local use a combining w diacritic ⟨◌ᪿ⟩ for protrusion (e.g. ⟨yᷱ øᪿ⟩) and a combining ʍ diacritic ⟨◌ᫀ⟩ for compression (e.g. ⟨uᫀ oᫀ⟩).[119] Because their transcriptions are manuscript, these are effectively the same symbols as the old IPA diacritics, which indeed are historically cursive w and ʍ. However, the more angular ⟨◌ᫀ⟩ of typescript might misleadingly suggest the vowel is protruded and voiceless (like [ʍ]) rather than compressed and voiced.\n\nIn both print and speech, an IPA symbol is often distinguished from the sound it transcribes because IPA letters very often do not have their cardinal IPA values in practice. This is commonly the case in phonemic and broad phonetic transcription, making articulatory descriptions of IPA letters, such as \"mid front rounded vowel\" or \"voiced velar stop\", inappropriate as names for those letters. While the Handbook of the International Phonetic Association states that no official names exist for its symbols, it admits the presence of one or two common names for each.[120] The symbols also have nonce names in the Unicode standard. In many cases, the names in Unicode and the IPA Handbook differ. For example, the Handbook calls ⟨ɛ⟩ \"epsilon\", while Unicode calls it \"small letter open e\".\n\nThe traditional names of the Latin and Greek letters are usually used for unmodified letters.[note 50] Letters which are not directly derived from these alphabets, such as ⟨ʕ⟩, may have a variety of names, sometimes based on the appearance of the symbol or on the sound that it represents. In Unicode, some of the letters of Greek origin have Latin forms for use in IPA; the others use the characters from the Greek block.\n\nFor diacritics, there are two methods of naming. For traditional diacritics, the IPA notes the name in a well known language; for example, ⟨é⟩ is \"e-acute\", based on the name of the diacritic in English and French. Non-traditional diacritics are often named after objects they resemble, so ⟨d̪⟩ is called \"d-bridge\".\n\nGeoffrey Pullum and William Ladusaw [d] list a variety of names in use for both current and retired IPA symbols in their Phonetic Symbol Guide. Many of them found their way into Unicode.[9]\n\nUnicode supports nearly all of the IPA. Apart from basic Latin and Greek and general punctuation, the primary blocks are IPA Extensions, Spacing Modifier Letters and Combining Diacritical Marks, with lesser support from Phonetic Extensions, Phonetic Extensions Supplement, Combining Diacritical Marks Supplement, and scattered characters elsewhere. The extended IPA is supported primarily by those blocks and Latin Extended-G.\n\nAfter the Kiel Convention in 1989, most IPA symbols were assigned an identifying number to prevent confusion between similar characters during the printing of manuscripts. The codes were never much used and have been superseded by Unicode.\n\nMany typefaces have support for IPA characters, but good diacritic rendering remains rare.[122] Web browsers generally do not need any configuration to display IPA characters, provided that a typeface capable of doing so is available to the operating system.\n\nTypefaces that provide full IPA and nearly full extIPA support, including properly rendering the diacritics, include Gentium Plus, Charis SIL, Doulos SIL, and Andika developed by SIL International. Indeed, the IPA chose Doulos to publish their chart in Unicode format. In addition to the level of support found in commercial and system fonts, these fonts support the full range of old-style (pre-Kiel) staveless tone letters, through a character variant option that suppresses the stave of the Chao tone letters. They also have an option to maintain the [a] ~ [ɑ] vowel distinction in italics. The only notable gaps are with the extIPA: the combining parentheses, which enclose diacritics, are not supported, nor is the enclosing circle that marks unidentified sounds, and which Unicode considers to be a copy-edit mark and thus not eligible for Unicode support.\n\nThe basic Latin Noto fonts commissioned by Google also have significant IPA support, including diacritic placement, only failing with the more obscure IPA and extIPA characters and superscripts of the Latin Extended-F and Latin Extended-G blocks. The extIPA parentheses are included, but they do not enclose diacritics as they are supposed to.\n\nDejaVu is the second free Unicode font chosen by the IPA to publish their chart. It was last updated in 2016 and so does not support the Latin F or G blocks. Stacked diacritics tend to overstrike each other.\n\nAs of 2018 , the IPA was developing their own font, unitipa, based on TIPA.[123]\n\nCalibri, the default font of Microsoft Office, has nearly complete IPA support with good diacritic rendering, though it is not as complete as some free fonts (see image at right). Other widespread Microsoft fonts, such as Arial and Times New Roman, have poor support.\n\nThe Apple system fonts Geneva, Lucida Grande and Hiragino (certain weights) have only basic IPA support.\n\nBrill has complete IPA and extIPA coverage of characters added to Unicode by 2020, with good diacritic and tone-letter support. It is a commercial font but is freely available for non-commercial use.[124]\n\nSeveral systems have been developed that map the IPA symbols to ASCII characters. Notable systems include SAMPA and X-SAMPA. The usage of mapping systems in on-line text has to some extent been adopted in the context input methods, allowing convenient keying of IPA characters that would be otherwise unavailable on standard keyboard layouts.\n\nIETF language tags have registered fonipa as a variant subtag identifying text as written in IPA.[125] Thus, an IPA transcription of English could be tagged as en-fonipa. For the use of IPA without attribution to a concrete language, und-fonipa is available.\n\nOnline IPA keyboard utilities are available, though none of them cover the complete range of IPA symbols and diacritics. Examples are the IPA 2018 i-charts hosted by the IPA,[126] IPA character picker by Richard Ishida at GitHub,[127] Type IPA phonetic symbols at TypeIt.org,[128] and an IPA Chart keyboard by Weston Ruter also at GitHub.[129] In April 2019, Google's Gboard for Android added an IPA keyboard to its platform.[130][131] For iOS there are multiple free keyboard layouts available, such as the IPA Phonetic Keyboard.[132]"
    },
    {
        "link": "https://custom-writing.org/blog/phonetics",
        "document": "A branch of linguistics that studies all human sounds is called phonetics. It analyses the production (articulation), transmission (sound), and perception (hearing) of sounds. The phonetic system of a language represents the way people use sounds in their speech. A language’s phonology classifies these sounds into vowels and consonants, long and short sounds, and many other language-specific parameters.\n\nYou can find detailed information on each of these aspects in this article by custom-writing.org experts, including how the English phonetic system relates to the IPA (International Phonetic Alphabet), phonetics definition, types of vowels, and more.\n\nThe English phonetic system comprises the four components: speech sounds, syllabic word structure, stress, and intonation. To make it simple, it describes the way we produce and perceive the sounds of speech. Most ESL textbooks explain these components using the International Phonetic Alphabet (IPA) which is described below.\n\nSometimes the meaning of phonics is limited to a simplified definition of phonetics. But it is incorrect. Phonetics is the academic study of the sounds of a language. Hence, this science is a branch of linguistics. Phonics is a method of teaching to read when each letter is pronounced as in the alphabet.\n\nThe phonetics and phonology difference can be explained by their approaches and methods as a science. The former is a descriptive discipline that analyzes separate sounds we use in a language. The latter is more theoretical and explores the patterns of sounds, their system, and combination.\n\nThe International Phonetic Alphabet is a system of symbols representing each sound used in the English language. English language learning widely uses IPA. Linguists transcribe words in this alphabet for their research. Dictionaries use IPA to present the correct pronunciation of words. However, not some of the above use their own alphabets for various reasons. In most cases, they provide a reference table. Many of the best American English dictionaries transcribe words in a phonetic respelling system, which could be more comfortable for an unprepared reader.\n\nIPA was developed in the XIX century but is presently used for the modern language. If you know how to pronounce each of the symbols, you will be able to use the transcription in a dictionary.\n\nWhat Is an IPA Chart & How to Use It?\n\nThe IPA chart is a unique classification of sounds according to different aspects. There are 107 phonetic symbols and 52 diacritics in this phonemic transcription chart. Each of them represents its place in the mouth or throat. So everyone can reproduce the sound quickly.\n\nThe sounds in phonetics also vary by the manner of pronouncing them. What’s important here is how lips, tongue, and teeth work to produce one or another sound. The way you use breath is also essential.\n\nThe first thing to know about IPA is that there are two broad categories:\n• Vowels – these speech sounds in English are produced with air moving freely in different directions.\n• Consonants – these phonetic sounds are produced by air too, but are stopped by various parts of the mouth like tongue or teeth.\n\nBelow you’ll find consonant and vowel IPA charts. Note that you can check the phonetic symbols with audio, so you can always have an example before your eyes. Continue reading to know all the secrets of learning the transcription alphabet!\n\nThere aren’t so many vowel letters in the English language, but their phonetic spelling can be challenging to master.\n\nUsually, vowels in phonetics have the following classification:\n\nYou can listen to any of them with the help of our IPA vowel chart below.\n\nSometimes it’s challenging to distinguish long phonetic vowel sounds from short ones. A foreigner may think they sound the same, but the difference is noticeable for a native speaker. For that purpose, IPA and other alphabets use phonetic signs, such as the symbol /:/.\n\nStill, it’s troublesome for many ESL speakers to understand that the same phonetic letters can sound differently. And it’s okay because a lot of languages don’t have such difficult letter pronunciation.\n\nThe difference between those phonetic sounds is easy to notice while pronouncing them. Long /i:/ takes more time than short /i/ to produce, and you have to tense your tongue more. At the same time, short /i/ is pronounced without any tension.\n\nIPA phonetics regarding consonants is harder to remember. There are many types of phonetic consonants according to the manner of pronouncing them.\n\nUsually, consonant sounds of the English alphabet are divided into the following categories:\n• Fricative sounds – produced when the tongue rubs teeth or the roof of the mouth.\n• Plosive sounds – produced by stopping airflow with an explosive sound.\n\nOur interactive chart will help you with pronunciation if you have any difficulties.\n\nIf you’re an ESL speaker, you may wonder whether you should pronounce or omit the phonetic sound /r/. Well, it depends on the variant of English you use.\n\nIn the American variant of English, /r/ is always pronounced. In the British variant of English, you pronounce /r/ phonetic sound only if it comes before a vowel. In other cases, omit it. For example, in ‘print,’ you pronounce /r/ because a vowel sound follows, while in ‘park,’ you omit /r/.\n\nAlso, make sure to use linking /r/. It appears when /r/ letter pronunciation at the end of the word is omitted, but a vowel sound follows next in a word combination. For example, in the word ‘bear’ the phonetic sound /r/ isn’t pronounced. But if there is the word combination ‘bear eats’, then the sound /r/ appears in British English spelling.\n\nMost languages have stress in their pronunciation. The English phonetic system has an element like this, too.\n\nIn IPA transcription, you mark it with a symbol /ˈ/. But what’s interesting is that English phonetic spelling can have two stresses at once. Primary stress is a regular phonetic symbol used in words with three syllables. Secondary stress appears when there are more than two syllables in a word. It emphasizes the syllable that is weaker than the primary stressed one but stronger than the last syllable.\n\nThe IPA symbols list represents secondary stress as /ˌ/. For example, the IPA transcription of the word ‘alphabetical’ looks like /ˌælfəˈbetɪkl/. The first syllable here is secondarily stressed.\n\nYou may wonder if it is challenging to learn IPA pronunciation symbols. Well, it depends on how much time you spend on it. Almost every teacher would advise you to look up every new word in a dictionary. It can help you understand the English phonetic system in a short time.\n\nHowever, students often ignore IPA transcription and trust their ears instead. It’s nice if you have excellent listening skills, but remember that you’re more likely to see words for the first in the text rather than hearing them somewhere else. Moreover, native speakers can use many different variants of the language! That’s why it’s useful to know how to read new words without the help of native speakers.\n\nNow that you know how to avoid pronunciation mistakes, you may be interested in this article. It’ll teach you how to write correctly."
    },
    {
        "link": "https://scholar.harvard.edu/files/adam/files/phonetics.ppt.pdf",
        "document": ""
    },
    {
        "link": "https://speakwrite.com/blog/phonetic-transcription",
        "document": ""
    },
    {
        "link": "https://researchgate.net/publication/231774047_On_the_status_of_diacritics",
        "document": "On the status of diacritics\n\nIn this article we note that diacritics, both in terms of their de®nition by the IPA,\n\nand in studies of transcriber reliability, are treated as a single group. Further, they\n\nare usually treated as being used purely to re®ne the meaning of a sound and, as\n\nsuch, as having less status phonetically than full symbols. It is argued here that\n\ndiacritics should be classi®ed into at least two major categories, and it is shown\n\nhow one of these categories is the equivalent of a `full' symbo l. Apart from the\n\nimplications this has for reliability measures, it is argued in conclusion that a more\n\nneutral de®nition of diacritic by the IPA is required.\n\nDiacritics are described in IPA (1999) as `small letters or other mar ks which can be\n\nadded to a vowel or consonant letter to modify or re®ne its meaning in various ways'.\n\n`Modify' and `re®ne' clearly imply that a symbol `modi®ed' with a diacritic represents\n\nonly a slight sound difference as compared with the unadorned symbol. By implic ation,\n\nthen, diacritics represent slighter sound differences than do full symbols. Despite the\n\nexpansion of this statement that `a letter and any diacritic or diacritics attached to it\n\nare regarded as a single (complex) symbol' (p. 15), it has generally been the ca se that\n\ndiacritic usage has been seen as a separate part of the transcription process: something\n\nseemingly more dif®cult to undertake than the choice of symbols becau se you are\n\nhaving to listen for only slight sound differences. This sort of approach is evident in\n\nmuch of the literature on reliability in phonetic transcription or, indeed, simp ly in\n\naccounts of reliability measures in phonetic work in general. It is the grouping together\n\nof all diacritics in reliability measures that motivated this investigation of the status of\n\nIn this note I will look at a recent examination of transcriber reliability. I will also\n\nexamine the status of the various IPA diacritics to see how far they can be grouped into\n\na single category, and, further, a category that seems to present `problem s' in the"
    },
    {
        "link": "https://degruyter.com/document/doi/10.1515/9780748691012-012/html?lang=en&srsltid=AfmBOorK6gmmPGTzypKmRmsPpW_mcvDEiDw30oZcBXZxDsJVVj1SzLlB",
        "document": "Your purchase has been completed. Your documents are now available to view."
    },
    {
        "link": "https://en.wikipedia.org/wiki/International_Phonetic_Alphabet",
        "document": "The International Phonetic Alphabet (IPA) is an alphabetic system of phonetic notation based primarily on the Latin script. It was devised by the International Phonetic Association in the late 19th century as a standard written representation for the sounds of speech.[1] The IPA is used by lexicographers, foreign language students and teachers, linguists, speech–language pathologists, singers, actors, constructed language creators, and translators.[2][3]\n\nThe IPA is designed to represent those qualities of speech that are part of lexical (and, to a limited extent, prosodic) sounds in oral language: phones, intonation and the separation of syllables.[1] To represent additional qualities of speech – such as tooth gnashing, lisping, and sounds made with a cleft palate – an extended set of symbols may be used.[2]\n\nSegments are transcribed by one or more IPA symbols of two basic types: letters and diacritics. For example, the sound of the English digraph ⟨ch⟩ may be transcribed in IPA with a single letter: [c], or with multiple letters plus diacritics: [t̠̺͡ʃʰ], depending on how precise one wishes to be. Slashes are used to signal phonemic transcription; therefore, /tʃ/ is more abstract than either [t̠̺͡ʃʰ] or [c] and might refer to either, depending on the context and language.[note 1]\n\nOccasionally, letters or diacritics are added, removed, or modified by the International Phonetic Association. As of the most recent change in 2005,[4] there are 107 segmental letters, an indefinitely large number of suprasegmental letters, 44 diacritics (not counting composites), and four extra-lexical prosodic marks in the IPA. These are illustrated in the current IPA chart, posted below in this article and on the International Phonetic Association's website.[5]\n\nIn 1886, a group of French and English language teachers, led by the French linguist Paul Passy, formed what would be known from 1897 onwards as the International Phonetic Association (in French, l'Association phonétique internationale).[6] The idea of the alphabet had been suggested to Passy by Otto Jespersen. It was developed by Passy along with other members of the association, principally Daniel Jones. The original IPA alphabet was based on the Romic alphabet, an English spelling reform created by Henry Sweet that in turn was based on the Palaeotype alphabet of Alexander John Ellis, but to make it usable for other languages the values of the symbols were allowed to vary from language to language.[note 2] For example, the sound [ʃ] (the sh in shoe) was originally represented with the letter ⟨c⟩ for English but with ⟨x⟩ for French and German; with German, ⟨c⟩ was used for the [x] sound of Bach.[6] With a growing number of transcribed languages this proved impractical, and in 1888 the values of the letters were made uniform across languages. This would provide the base for all future revisions.[6][8]\n\nSince its creation, the IPA has undergone a number of revisions. After relatively frequent revisions and expansions from the 1890s to the 1940s, the IPA remained nearly static until the Kiel Convention in 1989, which substantially revamped the alphabet. A smaller revision took place in 1993 with the resurrection of letters for mid central vowels[2] and the retirement of letters for voiceless implosives.[9] The alphabet was last revised in May 2005 with the addition of a letter for a labiodental flap.[10] Apart from the addition and removal of symbols, changes to the IPA have consisted largely of renaming symbols and categories and in modifying typefaces.[2]\n\nExtensions to the International Phonetic Alphabet for speech pathology (extIPA) were created in 1990 and were officially adopted by the International Clinical Phonetics and Linguistics Association in 1994.[11] They were substantially revised in 2015.\n\nThe general principle of the IPA is to provide one letter for each distinctive sound (speech segment).[note 3] This means that:\n• It does not normally use combinations of letters to represent single sounds, the way English does with , and , nor single letters to represent multiple sounds, the way represents or in English.\n• There are no letters that have context-dependent sound values, the way and in several European languages have a \"hard\" or \"soft\" pronunciation.\n• The IPA does not usually have separate letters for two sounds if no known language makes a distinction between them, a property known as \"selectiveness\". 2 note 4 However, if a large number of phonemically distinct letters can be derived with a diacritic, that may be used instead. note 5\n\nThe alphabet is designed for transcribing sounds (phones), not phonemes, though it is used for phonemic transcription as well. A few letters that did not indicate specific sounds have been retired – ⟨ˇ⟩, once used for the \"compound\" tone of Swedish and Norwegian, and ⟨ƞ⟩, once used for the moraic nasal of Japanese – though one remains: ⟨ɧ⟩, used for the sj-sound of Swedish. When the IPA is used for broad phonetic or for phonemic transcription, the letter–sound correspondence can be rather loose. The IPA has recommended that more 'familiar' letters be used when that would not cause ambiguity.[13] For example, ⟨e⟩ and ⟨o⟩ for [ɛ] and [ɔ], ⟨t⟩ for [t̪] or [ʈ], ⟨f⟩ for [ɸ], etc. Indeed, in the illustration of Hindi in the IPA Handbook, the letters ⟨c⟩ and ⟨ɟ⟩ are used for /t͡ʃ/ and /d͡ʒ/.\n\nAmong the symbols of the IPA, 107 letters represent consonants and vowels, 31 diacritics are used to modify these, and 17 additional signs indicate suprasegmental qualities such as length, tone, stress, and intonation.[note 6] These are organized into a chart; the chart displayed here is the official chart as posted at the website of the IPA.\n\nThe letters chosen for the IPA are meant to harmonize with the Latin alphabet.[note 7] For this reason, most letters are either Latin or Greek, or modifications thereof. Some letters are neither: for example, the letter denoting the glottal stop, ⟨ʔ⟩, originally had the form of a question mark with the dot removed. A few letters, such as that of the voiced pharyngeal fricative, ⟨ʕ⟩, were inspired by other writing systems (in this case, the Arabic letter ⟨ﻉ⟩, ʿayn, via the reversed apostrophe).[9]\n• The right-swinging tail, as in ⟨ ⟩, indicates retroflex articulation. It originates from the hook of an r.\n• The top hook, as in ⟨ ⟩, indicates implosion.\n• Several nasal consonants are based on the form ⟨ ⟩: ⟨ ⟩. ⟨ ⟩ and ⟨ ⟩ derive from ligatures of gn and ng, and ⟨ ⟩ is an ad hoc imitation of ⟨ ⟩.\n• Letters turned 180 degrees for suggestive shapes, such as ⟨ ⟩ from ⟨ ⟩. note 8 Either the original letter may be reminiscent of the target sound, e.g., ⟨ ⟩ – or the turned one, e.g., ⟨ ⟩. Rotation was popular in the era of mechanical typesetting, as it had the advantage of not requiring the casting of special type for IPA symbols, much as the sorts had traditionally often pulled double duty for and , and , and , and to reduce cost. An example of a font that uses turned small-capital omega for the vowel letter ʊ. The symbol had originally been a small-capital .\n• Among consonant letters, the small capital letters ⟨ ⟩, and also ⟨ ⟩ in extIPA, indicate more guttural sounds than their base letters – ⟨ ⟩ is a late exception. Among vowel letters, small capitals indicate lax vowels. Most of the original small-cap vowel letters have been modified into more distinctive shapes – e.g. ⟨ ⟩ from [ ] – with only ⟨ ⟩ remaining as small capitals.\n\nThe International Phonetic Alphabet is based on the Latin script, and uses as few non-Latin letters as possible.[6] The Association created the IPA so that the sound values of most letters would correspond to \"international usage\" (approximately Classical Latin).[6] Hence, the consonant letters ⟨b⟩, ⟨d⟩, ⟨f⟩, ⟨ɡ⟩, ⟨h⟩, ⟨k⟩, ⟨l⟩, ⟨m⟩, ⟨n⟩, ⟨p⟩, ⟨s⟩, ⟨t⟩, ⟨v⟩, ⟨w⟩, and ⟨z⟩ have more or less their word-initial values in English (g as in gill, h as in hill, though p t k are unaspirated as in spill, still, skill); and the vowel letters ⟨a⟩, ⟨e⟩, ⟨i⟩, ⟨o⟩, ⟨u⟩ correspond to the (long) sound values of Latin: [i] is like the vowel in machine, [u] is as in rule, etc. Other Latin letters, particularly ⟨j⟩, ⟨r⟩ and ⟨y⟩, differ from English, but have their IPA values in Latin or other European languages.\n\nThis basic Latin inventory was extended by adding small-capital and cursive forms, diacritics and rotation. The sound values of these letters are related to those of the original letters, and their derivation may be iconic.[note 9] For example, letters with a rightward-facing hook at the bottom represent retroflex equivalents of the source letters, and small capital letters usually represent uvular equivalents of their source letters.\n\nThere are also several letters from the Greek alphabet, though their sound values may differ from Greek. For most Greek letters, subtly different glyph shapes have been devised for the IPA, specifically ⟨ɑ⟩, ⟨ꞵ⟩, ⟨ɣ⟩, ⟨ɛ⟩, ⟨ɸ⟩, ⟨ꭓ⟩ and ⟨ʋ⟩, which are encoded in Unicode separately from their parent Greek letters. One, however – ⟨θ⟩ – has only its Greek form, while for ⟨ꞵ ~ β⟩ and ⟨ꭓ ~ χ⟩, both Greek and Latin forms are in common use.[16] The tone letters are not derived from an alphabet, but from a pitch trace on a musical scale.\n\nBeyond the letters themselves, there are a variety of secondary symbols which aid in transcription. Diacritic marks can be combined with the letters to add tone and phonetic detail such as secondary articulation. There are also special symbols for prosodic features such as stress and intonation.\n\nThere are two principal types of brackets used to set off (delimit) IPA transcriptions:\n\nAll three of the above are provided by the IPA Handbook. The following are not, but may be seen in IPA transcription or in associated material (especially angle brackets):\n\nSome examples of contrasting brackets in the literature:\n\nIPA letters have cursive forms designed for use in manuscripts and when taking field notes, but the Handbook recommended against their use, as cursive IPA is \"harder for most people to decipher\". A braille representation of the IPA for blind or visually impaired professionals and students has also been developed.[35]\n\nThe International Phonetic Alphabet is occasionally modified by the Association. After each modification, the Association provides an updated simplified presentation of the alphabet in the form of a chart. (See History of the IPA.) Not all aspects of the alphabet can be accommodated in a chart of the size published by the IPA. The alveolo-palatal and epiglottal consonants, for example, are not included in the consonant chart for reasons of space rather than of theory (two additional columns would be required, one between the retroflex and palatal columns and the other between the pharyngeal and glottal columns), and the lateral flap would require an additional row for that single consonant, so they are listed instead under the catchall block of \"other symbols\".[36] The indefinitely large number of tone letters would make a full accounting impractical even on a larger page, and only a few examples are shown, and even the tone diacritics are not complete; the reversed tone letters are not illustrated at all.\n\nThe procedure for modifying the alphabet or the chart is to propose the change in the Journal of the IPA. (See, for example, December 2008 on an open central unrounded vowel[37] and August 2011 on central approximants.)[38] Reactions to the proposal may be published in the same or subsequent issues of the Journal (as in August 2009 on the open central vowel).[39][better source needed] A formal proposal is then put to the Council of the IPA[40][clarification needed] – which is elected by the membership[41] – for further discussion and a formal vote.[42][43]\n\nMany users of the alphabet, including the leadership of the Association itself, deviate from its standardized usage.[note 13] The Journal of the IPA finds it acceptable to mix IPA and extIPA symbols in consonant charts in their articles. (For instance, including the extIPA letter ⟨𝼆⟩, rather than ⟨ʎ̝̊⟩, in an illustration of the IPA.)[44]\n\nOf more than 160 IPA symbols, relatively few will be used to transcribe speech in any one language, with various levels of precision. A precise phonetic transcription, in which sounds are specified in detail, is known as a narrow transcription. A coarser transcription with less detail is called a broad transcription. Both are relative terms, and both are generally enclosed in square brackets.[1] Broad phonetic transcriptions may restrict themselves to easily heard details, or only to details that are relevant to the discussion at hand, and may differ little if at all from phonemic transcriptions, but they make no theoretical claim that all the distinctions transcribed are necessarily meaningful in the language.\n\nFor example, the English word little may be transcribed broadly as [ˈlɪtəl], approximately describing many pronunciations. A narrower transcription may focus on individual or dialectical details: [ˈɫɪɾɫ] in General American, [ˈlɪʔo] in Cockney, or [ˈɫɪːɫ] in Southern US English.\n\nPhonemic transcriptions, which express the conceptual counterparts of spoken sounds, are usually enclosed in slashes (/ /) and tend to use simpler letters with few diacritics. The choice of IPA letters may reflect theoretical claims of how speakers conceptualize sounds as phonemes or they may be merely a convenience for typesetting. Phonemic approximations between slashes do not have absolute sound values. For instance, in English, either the vowel of pick or the vowel of peak may be transcribed as /i/, so that pick, peak would be transcribed as /ˈpik, ˈpiːk/ or as /ˈpɪk, ˈpik/; and neither is identical to the vowel of the French pique, which would also be transcribed /pik/. By contrast, a narrow phonetic transcription of pick, peak, pique could be: [pʰɪk], [pʰiːk], [pikʲ].\n\nIPA is popular for transcription by linguists. Some American linguists, however, use a mix of IPA with Americanist phonetic notation or Sinological phonetic notation or otherwise use nonstandard symbols for various reasons.[45] Authors who employ such nonstandard use are encouraged to include a chart or other explanation of their choices, which is good practice in general, as linguists differ in their understanding of the exact meaning of IPA symbols and common conventions change over time.\n\nMany British dictionaries, including the Oxford English Dictionary and some learner's dictionaries such as the Oxford Advanced Learner's Dictionary and the Cambridge Advanced Learner's Dictionary, now use the International Phonetic Alphabet to represent the pronunciation of words.[46] However, most American (and some British) volumes use one of a variety of pronunciation respelling systems, intended to be more comfortable for readers of English and to be more acceptable across dialects, without the implication of a preferred pronunciation that the IPA might convey. For example, the respelling systems in many American dictionaries (such as Merriam-Webster) use ⟨y⟩ for IPA [ j] and ⟨sh⟩ for IPA [ ʃ ], reflecting the usual spelling of those sounds in English.[47][48][note 14] (In IPA, [y] represents the sound of the French ⟨u⟩, as in tu, and [sh] represents the sequence of consonants in grasshopper.)\n\nThe IPA is also not universal among dictionaries in languages other than English. Monolingual dictionaries of languages with phonemic orthographies generally do not bother with indicating the pronunciation of most words, and tend to use respelling systems for words with unexpected pronunciations. Dictionaries produced in Israel use the IPA rarely and sometimes use the Hebrew alphabet for transcription of foreign words.[note 15] Bilingual dictionaries that translate from foreign languages into Russian usually employ the IPA, but monolingual Russian dictionaries occasionally use pronunciation respelling for foreign words.[note 16] The IPA is more common in bilingual dictionaries, but there are exceptions here too. Mass-market bilingual Czech dictionaries, for instance, tend to use the IPA only for sounds not found in Czech.[note 17]\n\nIPA letters have been incorporated into the alphabets of various languages, notably via the Africa Alphabet in many sub-Saharan languages such as Hausa, Fula, Akan, Gbe languages, Manding languages, Lingala, etc. Capital case variants have been created for use in these languages. For example, Kabiyè of northern Togo has Ɖ ɖ, Ŋ ŋ, Ɣ ɣ, Ɔ ɔ, Ɛ ɛ, Ʋ ʋ. These, and others, are supported by Unicode, but appear in Latin ranges other than the IPA extensions.\n\nIn the IPA itself, however, only lower-case letters are used. The 1949 edition of the IPA handbook indicated that an asterisk ⟨*⟩ might be prefixed to indicate that a word was a proper name,[50] but this convention was not included in the 1999 Handbook, which notes the contrary use of the asterisk as a placeholder for a sound or feature that does not have a symbol.[51]\n\nThe IPA has widespread use among classical singers during preparation as they are frequently required to sing in a variety of foreign languages. They are also taught by vocal coaches to perfect diction and improve tone quality and tuning.[52] Opera librettos are authoritatively transcribed in IPA, such as Nico Castel's volumes[53] and Timothy Cheek's book Singing in Czech.[54] Opera singers' ability to read IPA was used by the site Visual Thesaurus, which employed several opera singers \"to make recordings for the 150,000 words and phrases in VT's lexical database ... for their vocal stamina, attention to the details of enunciation, and most of all, knowledge of IPA\".[55]\n\nThe International Phonetic Association organizes the letters of the IPA into three categories: pulmonic consonants, non-pulmonic consonants, and vowels.[note 18][57][58]\n\nPulmonic consonant letters are arranged singly or in pairs of voiceless (tenuis) and voiced sounds, with these then grouped in columns from front (labial) sounds on the left to back (glottal) sounds on the right. In official publications by the IPA, two columns are omitted to save space, with the letters listed among \"other symbols\" even though theoretically they belong in the main chart.[note 19] They are arranged in rows from full closure (occlusives: stops and nasals) at top, to brief closure (vibrants: trills and taps), to partial closure (fricatives), and finally minimal closure (approximants) at bottom, again with a row left out to save space. In the table below, a slightly different arrangement is made: All pulmonic consonants are included in the pulmonic-consonant table, and the vibrants and laterals are separated out so that the rows reflect the common lenition pathway of stop → fricative → approximant, as well as the fact that several letters pull double duty as both fricative and approximant; affricates may then be created by joining stops and fricatives from adjacent cells. Shaded cells represent articulations that are judged to be impossible or not distinctive.\n\nVowel letters are also grouped in pairs – of unrounded and rounded vowel sounds – with these pairs also arranged from front on the left to back on the right, and from maximal closure at top to minimal closure at bottom. No vowel letters are omitted from the chart, though in the past some of the mid central vowels were listed among the \"other symbols\".\n\nA pulmonic consonant is a consonant made by obstructing the glottis (the space between the vocal folds) or oral cavity (the mouth) and either simultaneously or subsequently letting out air from the lungs. Pulmonic consonants make up the majority of consonants in the IPA, as well as in human language. All consonants in English fall into this category.[60]\n\nThe pulmonic consonant table, which includes most consonants, is arranged in rows that designate manner of articulation, meaning how the consonant is produced, and columns that designate place of articulation, meaning where in the vocal tract the consonant is produced. The main chart includes only consonants with a single place of articulation.\n• In rows where some letters appear in pairs (the obstruents), the letter to the right represents a voiced consonant, except breathy-voiced . 61 In the other rows (the sonorants), the single letter represents a voiced consonant.\n• While IPA provides a single letter for the coronal places of articulation (for all consonants but fricatives), these do not always have to be used exactly. When dealing with a particular language, the letters may be treated as specifically dental, alveolar, or post-alveolar, as appropriate for that language, without diacritics.\n• Shaded areas indicate articulations judged to be impossible.\n• The letters are canonically voiced fricatives but may be used for approximants. note 20\n• In many languages, such as English, and are not actually glottal, fricatives, or approximants. Rather, they are bare phonation. 63\n• It is primarily the shape of the tongue rather than its position that distinguishes the fricatives , , and .\n• are defined as epiglottal fricatives under the \"Other symbols\" section in the official IPA chart, but they may be treated as trills at the same place of articulation as because trilling of the aryepiglottic folds typically co-occurs. 64\n• Some listed phones are not known to exist as phonemes in any language.\n\nNon-pulmonic consonants are sounds whose airflow is not dependent on the lungs. These include clicks (found in the Khoisan languages and some neighboring Bantu languages of Africa), implosives (found in languages such as Sindhi, Hausa, Swahili and Vietnamese), and ejectives (found in many Amerindian and Caucasian languages).\n• Clicks have traditionally been described as consisting of a forward place of articulation, commonly called the click \"type\" or historically the \"influx\", and a rear place of articulation, which when combined with the quality of the click is commonly called the click \"accompaniment\" or historically the \"efflux\". The IPA click letters indicate only the click type (forward articulation and release). Therefore, all clicks require two letters for proper notation: ⟨ ⟩, etc., or with the order reversed if both the forward and rear releases are audible. The letter for the rear articulation is frequently omitted, in which case a ⟨ ⟩ may usually be assumed. However, some researchers dispute the idea that clicks should be analyzed as doubly articulated, as the traditional transcription implies, and analyze the rear occlusion as solely a part of the airstream mechanism. 65 In transcriptions of such approaches, the click letter represents both places of articulation, with the different letters representing the different click types, and diacritics are used for the elements of the accompaniment: ⟨ ⟩, etc.\n• Letters for the voiceless implosives ⟨ ⟩ are no longer supported by the IPA, though they remain in Unicode. Instead, the IPA typically uses the voiced equivalent with a voiceless diacritic: ⟨ ⟩, etc.\n• The letter for the retroflex implosive, , is not \"explicitly IPA approved\", 66 but has the expected form if such a symbol were to be approved.\n• The ejective diacritic is placed at the right-hand margin of the consonant, rather than immediately after the letter for the stop: ⟨ ⟩, ⟨ ⟩. In imprecise transcription, it often stands in for a superscript glottal stop in glottalized but pulmonic sonorants, such as , , , – also transcribable as creaky , , , .\n\nAffricates and co-articulated stops are represented by two letters joined by a tie bar, either above or below the letters with no difference in meaning.[note 21] Affricates are optionally represented by ligatures – e.g. ⟨ʧ, ʤ ⟩ – though this is no longer official IPA usage[1] because a great number of ligatures would be required to represent all affricates this way. Alternatively, a superscript notation for a consonant release is sometimes used to transcribe affricates, for example ⟨tˢ⟩ for [t͜s], paralleling [kˣ] ~ [k͜x]. The letters for the palatal plosives ⟨c⟩ and ⟨ɟ⟩ are often used as a convenience for [t͜ʃ] and [d͜ʒ] or similar affricates, even in official IPA publications, so they must be interpreted with care.\n\nCo-articulated consonants are sounds that involve two simultaneous places of articulation (are pronounced using two parts of the vocal tract). In English, the [w] in \"went\" is a coarticulated consonant, being pronounced by rounding the lips and raising the back of the tongue. Similar sounds are [ʍ] and [ɥ]. In some languages, plosives can be double-articulated, for example in the name of Laurent Gbagbo.\n• , the Swedish sj-sound, is described by the IPA as a \"simultaneous and \", but it is unlikely such a simultaneous fricative actually exists in any language. 68\n• Multiple tie bars can be used: ⟨ ⟩ or ⟨ ⟩. For instance, a pre-voiced velar affricate may be transcribed as ⟨ ⟩\n• If a diacritic needs to be placed on or under a tie bar, the combining grapheme joiner (U+034F) needs to be used, as in 'chewed' (Margi). Font support is spotty, however.\n\nThe IPA defines a vowel as a sound which occurs at a syllable center.[69] Below is a chart depicting the vowels of the IPA. The IPA maps the vowels according to the position of the tongue.\n\nThe vertical axis of the chart is mapped by vowel height. Vowels pronounced with the tongue lowered are at the bottom, and vowels pronounced with the tongue raised are at the top. For example, [ɑ] (the first vowel in father) is at the bottom because the tongue is lowered in this position. [i] (the vowel in \"meet\") is at the top because the sound is said with the tongue raised to the roof of the mouth.\n\nIn a similar fashion, the horizontal axis of the chart is determined by vowel backness. Vowels with the tongue moved towards the front of the mouth (such as [ɛ], the vowel in \"met\") are to the left in the chart, while those in which it is moved to the back (such as [ʌ], the vowel in \"but\") are placed to the right in the chart.\n\nIn places where vowels are paired, the right represents a rounded vowel (in which the lips are rounded) while the left is its unrounded counterpart.\n\nDiphthongs are typically specified with a non-syllabic diacritic, as in ⟨ui̯⟩ or ⟨u̯i⟩, or with a superscript for the on- or off-glide, as in ⟨uⁱ⟩ or ⟨ᵘi⟩. Sometimes a tie bar is used: ⟨u͜i⟩, especially when it is difficult to tell if the diphthong is characterized by an on-glide or an off-glide or when it is variable.\n• ⟨ ⟩ officially represents a front vowel, but there is little if any distinction between front and central open vowels (see Vowel § Acoustics), and ⟨ ⟩ is frequently used for an open central vowel. 45 If disambiguation is required, the retraction diacritic or the centralized diacritic may be added to indicate an open central vowel, as in ⟨ ⟩ or ⟨ ⟩.\n\nDiacritics are used for phonetic detail. They are added to IPA letters to indicate a modification or specification of that letter's normal pronunciation.[70]\n\nBy being made superscript, any IPA letter may function as a diacritic, conferring elements of its articulation to the base letter. Those superscript letters listed below are specifically provided for by the IPA Handbook; other uses can be illustrated with ⟨tˢ⟩ ([t] with fricative release), ⟨ᵗs⟩ ([s] with affricate onset), ⟨ⁿd⟩ (prenasalized [d]), ⟨bʱ⟩ ([b] with breathy voice), ⟨mˀ⟩ (glottalized [m]), ⟨sᶴ⟩ ([s] with a flavor of [ʃ], i.e. a voiceless alveolar retracted sibilant), ⟨oᶷ⟩ ([o] with diphthongization), ⟨ɯᵝ⟩ (compressed [ɯ]). Superscript diacritics placed after a letter are ambiguous between simultaneous modification of the sound and phonetic detail at the end of the sound. For example, labialized ⟨kʷ⟩ may mean either simultaneous [k] and [w] or else [k] with a labialized release. Superscript diacritics placed before a letter, on the other hand, normally indicate a modification of the onset of the sound (⟨mˀ⟩ glottalized [m], ⟨ˀm⟩ [m] with a glottal onset). (See § Superscript IPA.)\n\nSubdiacritics (diacritics normally placed below a letter) may be moved above a letter to avoid conflict with a descender, as in voiceless ⟨ŋ̊⟩.[70] The raising and lowering diacritics have optional spacing forms ⟨˔⟩, ⟨˕⟩ that avoid descenders.\n\nThe state of the glottis can be finely transcribed with diacritics. A series of alveolar plosives ranging from open-glottis to closed-glottis phonation is:\n\nAdditional diacritics are provided by the Extensions to the IPA for speech pathology.\n\nThese symbols describe the features of a language above the level of individual consonants and vowels, that is, at the level of syllable, word or phrase. These include prosody, pitch, length, stress, intensity, tone and gemination of the sounds of a language, as well as the rhythm and intonation of speech.[72] Various ligatures of pitch/tone letters and diacritics are provided for by the Kiel Convention and used in the IPA Handbook despite not being found in the summary of the IPA alphabet found on the one-page chart.\n\nUnder capital letters below we will see how a carrier letter may be used to indicate suprasegmental features such as labialization or nasalization. Some authors omit the carrier letter, for e.g. suffixed [kʰuˣt̪s̟]ʷ or prefixed [ʷkʰuˣt̪s̟],[note 22] or place a spacing variant of a diacritic such as ⟨˔⟩ or ⟨˜⟩ at the beginning or end of a word to indicate that it applies to the entire word.[note 23]\n\nThe old staveless tone letters, which are effectively obsolete, include high ⟨ˉe⟩, mid ⟨˗e⟩, low ⟨ˍe⟩, rising ⟨ˊe⟩, falling ⟨ˋe⟩, low rising ⟨ˏe⟩ and low falling ⟨ˎe⟩.\n\nOfficially, the stress marks ⟨ˈ ˌ⟩ appear before the stressed syllable, and thus mark the syllable boundary as well as stress (though the syllable boundary may still be explicitly marked with a period).[75] Occasionally the stress mark is placed immediately before the nucleus of the syllable, after any consonantal onset.[76] In such transcriptions, the stress mark does not mark a syllable boundary. The primary stress mark may be doubled ⟨ˈˈ⟩ for extra stress (such as prosodic stress). The secondary stress mark is sometimes seen doubled ⟨ˌˌ⟩ for extra-weak stress, but this convention has not been adopted by the IPA.[75] Some dictionaries place both stress marks before a syllable, ⟨¦⟩, to indicate that pronunciations with either primary or secondary stress are heard, though this is not IPA usage.[note 28]\n\nThere are three boundary markers: ⟨.⟩ for a syllable break, ⟨|⟩ for a minor prosodic break and ⟨‖⟩ for a major prosodic break. The tags 'minor' and 'major' are intentionally ambiguous. Depending on need, 'minor' may vary from a foot break to a break in list-intonation to a continuing–prosodic unit boundary (equivalent to a comma), and while 'major' is often any intonation break, it may be restricted to a final–prosodic unit boundary (equivalent to a period). The 'major' symbol may also be doubled, ⟨‖‖⟩, for a stronger break.[note 29]\n\nAlthough not part of the IPA, the following additional boundary markers are often used in conjunction with the IPA: ⟨μ⟩ for a mora or mora boundary, ⟨σ⟩ for a syllable or syllable boundary, ⟨+⟩ for a morpheme boundary, ⟨#⟩ for a word boundary (may be doubled, ⟨##⟩, for e.g. a breath-group boundary),[78] ⟨$⟩ for a phrase or intermediate boundary and ⟨%⟩ for a prosodic boundary. For example, C# is a word-final consonant, %V a post-pausa vowel, and σC a syllable-initial consonant.\n\n⟨ꜛ ꜜ⟩ are defined in the Handbook as \"upstep\" and \"downstep\", concepts from tonal languages. However, the upstep symbol can also be used for pitch reset, and the IPA Handbook uses it for prosody in the illustration for Portuguese, a non-tonal language.\n\nPhonetic pitch and phonemic tone may be indicated by either diacritics placed over the nucleus of the syllable – e.g., high-pitch ⟨é⟩ – or by Chao tone letters placed either before or after the word or syllable. There are three graphic variants of the tone letters: with or without a stave, and facing left or facing right from the stave. The stave was introduced with the 1989 Kiel Convention, as was the option of placing a staved letter after the word or syllable, while retaining the older conventions. There are therefore six ways to transcribe pitch/tone in the IPA: i.e., ⟨é⟩, ⟨˦e⟩, ⟨e˦⟩, ⟨꜓e⟩, ⟨e꜓⟩ and ⟨ˉe⟩ for a high pitch/tone.[75][79][80] Of the tone letters, only left-facing staved letters and a few representative combinations are shown in the summary on the Chart, and in practice it is currently more common for tone letters to occur after the syllable/word than before, as in the Chao tradition. Placement before the word is a carry-over from the pre-Kiel IPA convention, as is still the case for the stress and upstep/downstep marks. The IPA endorses the Chao tradition of using the left-facing tone letters, ⟨˥ ˦ ˧ ˨ ˩⟩, for underlying tone, and the right-facing letters, ⟨꜒ ꜓ ꜔ ꜕ ꜖⟩, for surface tone, as occurs in tone sandhi, and for the intonation of non-tonal languages.[note 30] In the Portuguese illustration in the 1999 Handbook, tone letters are placed before a word or syllable to indicate prosodic pitch (equivalent to [↗︎] global rise and [↘︎] global fall, but allowing more precision), and in the Cantonese illustration they are placed after a word/syllable to indicate lexical tone. Theoretically therefore prosodic pitch and lexical tone could be simultaneously transcribed in a single text, though this is not a formalized distinction.\n\nRising and falling pitch, as in contour tones, are indicated by combining the pitch diacritics and letters in the table, such as grave plus acute for rising [ě] and acute plus grave for falling [ê]. Only six combinations of two diacritics are supported, and only across three levels (high, mid, low), despite the diacritics supporting five levels of pitch in isolation. The four other explicitly approved rising and falling diacritic combinations are high/mid rising [e᷄], low rising [e᷅], high falling [e᷇], and low/mid falling [e᷆].[note 31]\n\nThe Chao tone letters, on the other hand, may be combined in any pattern, and are therefore used for more complex contours and finer distinctions than the diacritics allow, such as mid-rising [e˨˦], extra-high falling [e˥˦], etc. There are 20 such possibilities. However, in Chao's original proposal, which was adopted by the IPA in 1989, he stipulated that the half-high and half-low letters ⟨˦ ˨⟩ may be combined with each other, but not with the other three tone letters, so as not to create spuriously precise distinctions. With this restriction, there are 8 possibilities.[81]\n\nThe old staveless tone letters tend to be more restricted than the staved letters, though not as restricted as the diacritics. Officially, they support as many distinctions as the staved letters,[note 32] but typically only three pitch levels are distinguished. Unicode supports default or high-pitch ⟨ˉ ˊ ˋ ˆ ˇ ˜ ˙⟩ and low-pitch ⟨ˍ ˏ ˎ ꞈ ˬ ˷⟩. Only a few mid-pitch tones are supported – such as ⟨˗ ˴⟩ – and then only accidentally.\n\nAlthough tone diacritics and tone letters are presented as equivalent on the chart, \"this was done only to simplify the layout of the chart. The two sets of symbols are not comparable in this way.\"[82] Using diacritics, a high tone is ⟨é⟩ and a low tone is ⟨è⟩; in tone letters, these are ⟨e˥⟩ and ⟨e˩⟩. One can double the diacritics for extra-high ⟨e̋⟩ and extra-low ⟨ȅ⟩; there is no parallel to this using tone letters. Instead, tone letters have mid-high ⟨e˦⟩ and mid-low ⟨e˨⟩; again, there is no equivalent among the diacritics. Thus in a three-register tone system, ⟨é ē è⟩ are equivalent to ⟨e˥ e˧ e˩⟩, while in a four-register system, ⟨e̋ é è ȅ⟩ may be equivalent to ⟨e˥ e˦ e˨ e˩⟩.[75]\n\nThe correspondence breaks down even further once they start combining. For more complex tones, one may combine three or four tone diacritics in any permutation,[75] though in practice only generic peaking (rising-falling) e᷈ and dipping (falling-rising) e᷉ combinations are used. Chao tone letters are required for finer detail (e˧˥˧, e˩˨˩, e˦˩˧, e˨˩˦, etc.). Although only 10 peaking and dipping tones were proposed in Chao's original, limited set of tone letters, phoneticians often make finer distinctions, and indeed an example is found on the IPA Chart.[note 33] The system allows the transcription of 112 peaking and dipping pitch contours, including tones that are level for part of their length.\n\nMore complex contours are possible. Chao gave an example of [꜔꜒꜖꜔] (mid-high-low-mid) from English prosody.[81]\n\nChao tone letters generally appear after each syllable, for a language with syllable tone – ⟨a˧vɔ˥˩⟩ – or after the phonological word, for a language with word tone (⟨avɔ˧˥˩⟩). The IPA gives the option of placing the tone letters before the word or syllable – ⟨˧a˥˩vɔ⟩, ⟨˧˥˩avɔ⟩ – but this is rare for lexical tone. Reversed tone letters may be used to clarify that they apply to the following rather than to the preceding syllable – ⟨꜔a꜒꜖vɔ⟩, ⟨꜔꜒꜖avɔ⟩. The staveless letters are not directly supported by Unicode, but some fonts allow the stave in Chao tone letters to be suppressed.\n\nIPA diacritics may be doubled to indicate an extra degree (greater intensity) of the feature indicated.[83] This is a productive process, but apart from extra-high and extra-low tones being marked by doubled high- and low-tone diacritics, ⟨ə̋, ə̏⟩, the major prosodic break ⟨‖⟩ being marked as a doubled minor break ⟨|⟩, and a couple other instances, such usage is not enumerated by the IPA.\n\nFor example, the stress mark may be doubled (or even tripled, etc.) to indicate an extra degree of stress, such as prosodic stress in English.[84] An example in French, with a single stress mark for normal prosodic stress at the end of each prosodic unit (marked as a minor prosodic break), and a double or even triple stress mark for contrastive/emphatic stress: [ˈˈɑ̃ːˈtre | məˈsjø ‖ ˈˈvwala maˈdam ‖] Entrez monsieur, voilà madame.[85] Similarly, a doubled secondary stress mark ⟨ˌˌ⟩ is commonly used for tertiary (extra-light) stress, though a proposal to officially adopt this was rejected.[86] In a similar vein, the effectively obsolete staveless tone letters were once doubled for an emphatic rising intonation ⟨˶⟩ and an emphatic falling intonation ⟨˵⟩.[87]\n\nLength is commonly extended by repeating the length mark, which may be phonetic, as in [ĕ e eˑ eː eːˑ eːː] etc., as in English shhh! [ʃːːː], or phonemic, as in the \"overlong\" segments of Estonian:\n\nDelimiters are similar: double slashes indicate extra phonemic (morpho-phonemic), double square brackets especially precise transcription, and double parentheses especially unintelligible.\n• Nasalization, as in Palantla Chinantec lightly nasalized vs heavily nasalized , 89 though some care can be needed to distinguish this from the extIPA diacritic for velopharyngeal frication in disordered speech, , which has also been analyzed as extreme nasalization.\n• Especially lowered, e.g. (or , if the former symbol does not display properly) for as a weak fricative in some pronunciations of register. 91\n• Especially retracted, e.g. or , note 37 83 92 though some care might be needed to distinguish this from indications of alveolar or alveolarized articulation in extIPA, e.g. .\n• The transcription of strident and harsh voice as extra-creaky may be motivated by the similarities of these phonations.\n\nThe extIPA provides combining parentheses for weak intensity, which when combined with a doubled diacritic indicate an intermediate degree. For instance, increasing degrees of nasalization of the vowel [e] might be written ⟨e ẽ᪻ ẽ ẽ̃᪻ ẽ̃⟩.\n\nAs noted above, IPA letters are often used quite loosely in broad transcription if no ambiguity would arise in a particular language. Because of that, IPA letters have not generally been created for sounds that are not distinguished in individual languages. A distinction between voiced fricatives and approximants is only partially implemented by the IPA, for example. Even with the relatively recent addition of the palatal fricative ⟨ʝ⟩ and the velar approximant ⟨ɰ⟩ to the alphabet, other letters, though defined as fricatives, are often ambiguous between fricative and approximant. For forward places, ⟨β⟩ and ⟨ð⟩ can generally be assumed to be fricatives unless they carry a lowering diacritic. Rearward, however, ⟨ʁ⟩ and ⟨ʕ⟩ are perhaps more commonly intended to be approximants even without a lowering diacritic. ⟨h⟩ and ⟨ɦ⟩ are similarly either fricatives or approximants, depending on the language, or even glottal \"transitions\", without that often being specified in the transcription.\n\nAnother common ambiguity is among the letters for palatal consonants. ⟨c⟩ and ⟨ɟ⟩ are not uncommonly used as a typographic convenience for affricates, typically [t͜ʃ] and [d͜ʒ], while ⟨ɲ⟩ and ⟨ʎ⟩ are commonly used for palatalized alveolar [n̠ʲ] and [l̠ʲ]. To some extent this may be an effect of analysis, but it is common to match up single IPA letters to the phonemes of a language, without overly worrying about phonetic precision.\n\nIt has been argued that the lower-pharyngeal (epiglottal) fricatives ⟨ʜ⟩ and ⟨ʢ⟩ are better characterized as trills, rather than as fricatives that have incidental trilling.[94] This has the advantage of merging the upper-pharyngeal fricatives [ħ, ʕ] together with the epiglottal plosive [ʡ] and trills [ʜ ʢ] into a single pharyngeal column in the consonant chart. However, in Shilha Berber the epiglottal fricatives are not trilled.[95][96] Although they might be transcribed ⟨ħ̠ ʢ̠⟩ to indicate this, the far more common transcription is ⟨ʜ ʢ⟩, which is therefore ambiguous between languages.\n\nAmong vowels, ⟨a⟩ is officially a front vowel, but is more commonly treated as a central vowel. The difference, to the extent it is even possible, is not phonemic in any language.\n\nFor all phonetic notation, it is good practice for an author to specify exactly what they mean by the symbols that they use.\n\nSuperscript IPA letters are used to indicate secondary aspects of articulation. These may be aspects of simultaneous articulation that are considered to be in some sense less dominant than the basic sound, or may be transitional articulations that are interpreted as secondary elements.[97] Examples include secondary articulation; onsets, releases, aspiration and other transitions; shades of sound; light epenthetic sounds and incompletely articulated sounds. Morphophonemically, superscripts may be used for assimilation, e.g. ⟨aʷ⟩ for the effect of labialization on a vowel /a/, which may be realized as phonemic /o/.[98] The IPA and ICPLA endorse Unicode encoding of superscript variants of all contemporary segmental letters in the IPA proper and of all additional fricatives in extIPA, including the \"implicit\" IPA retroflex letters ⟨ꞎ 𝼅 𝼈 ᶑ 𝼊 ⟩.[44][99][100]\n\nSuperscripts are often used as a substitute for the tie bar, for example ⟨tᶴ⟩ for [t͜ʃ] and ⟨kᵖ⟩ or ⟨ᵏp⟩ for [k͜p]. However, in precise notation there is a difference between a fricative release in [tᶴ] and the affricate [t͜ʃ], between a velar onset in [ᵏp] and doubly articulated [k͜p].[101]\n\nSuperscript letters can be meaningfully modified by combining diacritics, just as baseline letters can. For example, a superscript dental nasal in ⟨ⁿ̪d̪⟩, a superscript voiceless velar nasal in ⟨ᵑ̊ǂ⟩, and labial-velar prenasalization in ⟨ᵑ͡ᵐɡ͡b⟩. Although the diacritic may seem a bit oversized compared to the superscript letter it modifies, e.g. ⟨ᵓ̃⟩, this can be an aid to legibility, just as it is with the composite superscript c-cedilla ⟨ᶜ̧⟩ and rhotic vowels ⟨ᵊ˞ ᶟ˞⟩. Superscript length marks can be used to indicate the length of aspiration of a consonant, e.g. [pʰ tʰ𐞂 kʰ𐞁]. Another option is to use extIPA parentheses and a doubled diacritic: ⟨p⁽ʰ⁾ tʰ kʰʰ⟩.[44]\n\nA number of IPA letters and diacritics have been retired or replaced over the years. This number includes duplicate symbols, symbols that were replaced due to user preference, and unitary symbols that were rendered with diacritics or digraphs to reduce the inventory of the IPA. The rejected symbols are now considered obsolete, though some are still seen in the literature.\n\nThe IPA once had several pairs of duplicate symbols from alternative proposals, but eventually settled on one or the other. An example is the vowel letter ⟨ɷ⟩, rejected in favor of ⟨ʊ⟩. Affricates were once transcribed with ligatures, such as ⟨ʧ ʤ ⟩ (and others, some of which not found in Unicode). These have been officially retired but are still used. Letters for specific combinations of primary and secondary articulation have also been mostly retired, with the idea that such features should be indicated with tie bars or diacritics: ⟨ƍ⟩ for [zʷ] is one. In addition, the rare voiceless implosives, ⟨ƥ ƭ ƈ ƙ ʠ ⟩, were dropped soon after their introduction and are now usually written ⟨ɓ̥ ɗ̥ ʄ̊ ɠ̊ ʛ̥ ⟩. The original set of click letters, ⟨ʇ, ʗ, ʖ, ʞ⟩, was retired but is still sometimes seen, as the current pipe letters ⟨ǀ, ǃ, ǁ, ǂ⟩ can cause problems with legibility, especially when used with brackets ([ ] or / /), the letter ⟨l⟩ (small L), or the prosodic marks ⟨|, ‖⟩. (For this reason, some publications which use the current IPA pipe letters disallow IPA brackets.)[102]\n\nIndividual non-IPA letters may find their way into publications that otherwise use the standard IPA. This is especially common with:\n• Affricates, such as the Americanist barred lambda ⟨ ⟩ for or ⟨ ⟩ for . note 38\n• Digits for tonal phonemes that have conventional numbers in a local tradition, such as the four tones of Standard Chinese. This may be more convenient for comparison between related languages and dialects than a phonetic transcription would be, because tones vary more unpredictably than segmental phonemes do.\n• Digits for tone levels, which are simpler to typeset, though the lack of standardization can cause confusion (e.g. ⟨ ⟩ is high tone in some languages but low tone in others; ⟨ ⟩ may be high, medium or low tone, depending on the local convention).\n• Iconic extensions of standard IPA letters that are implicit in the alphabet, such as retroflex ⟨ ⟩ and ⟨ ⟩. These are referred to in the Handbook and have been included in Unicode at IPA request.\n• Even presidents of the IPA have used para-IPA notation, such as resurrecting the old diacritic ⟨ ⟩ for purely labialized sounds (not simultaneously velarized), the lateral fricative letter ⟨ ⟩, and either the old dot diacritic ⟨ ⟩ or the novel letters ⟨ ⟩ for the not-quite-retroflex fricatives of Polish sz, ż and of Russian ш ж.\n\nIn addition, it is common to see ad hoc typewriter substitutions, generally capital letters, for when IPA support is not available, e.g. S for ⟨ ʃ ⟩. (See also SAMPA and X-SAMPA substitute notation.)\n\nThe Extensions to the International Phonetic Alphabet for Disordered Speech, commonly abbreviated \"extIPA\" and sometimes called \"Extended IPA\", are symbols whose original purpose was to accurately transcribe disordered speech. At the Kiel Convention in 1989, a group of linguists drew up the initial extensions,[note 39] which were based on the previous work of the PRDS (Phonetic Representation of Disordered Speech) Group in the early 1980s.[104] The extensions were first published in 1990, then modified, and published again in 1994 in the Journal of the International Phonetic Association, when they were officially adopted by the ICPLA.[105] While the original purpose was to transcribe disordered speech, linguists have used the extensions to designate a number of sounds within standard communication, such as hushing, gnashing teeth, and smacking lips,[2] as well as regular lexical sounds such as lateral fricatives that do not have standard IPA symbols.\n\nIn addition to the Extensions to the IPA for disordered speech, there are the conventions of the Voice Quality Symbols, which include a number of symbols for additional airstream mechanisms and secondary articulations in what they call \"voice quality\".\n\nCapital letters and various characters on the number row of the keyboard are commonly used to extend the alphabet in various ways.\n\nThere are various punctuation-like conventions for linguistic transcription that are commonly used together with IPA. Some of the more common are:\n\nFull capital letters are not used as IPA symbols, except as typewriter substitutes (e.g. N for ⟨ŋ⟩, S for ⟨ ʃ ⟩, O for ⟨ɔ⟩ – see SAMPA). They are, however, often used in conjunction with the IPA in two cases:\n• for (archi)phonemes and for natural classes of sounds (that is, as wildcards). The extIPA chart, for example, uses capital letters as wildcards in its illustrations.\n• as carrying letters for the Voice Quality Symbols.\n\nWildcards are commonly used in phonology to summarize syllable or word shapes, or to show the evolution of classes of sounds. For example, the possible syllable shapes of Mandarin can be abstracted as ranging from /V/ (an atonic vowel) to /CGVNᵀ/ (a consonant-glide-vowel-nasal syllable with tone), and word-final devoicing may be schematized as C → C̥/_#. They are also used in historical linguistics for a sound that is posited but whose nature has not been determined beyond some generic category such as {nasal} or {uvular}. In speech pathology, capital letters represent indeterminate sounds, and may be superscripted to indicate they are weakly articulated: e.g. [ᴰ] is a weak indeterminate alveolar, [ᴷ] a weak indeterminate velar.[109]\n\nThere is a degree of variation between authors as to the capital letters used, but ⟨C⟩ for {consonant}, ⟨V⟩ for {vowel} and ⟨N⟩ for {nasal} are ubiquitous in English-language material. Other common conventions are ⟨T⟩ for {tone/accent} (tonicity), ⟨P⟩ for {plosive}, ⟨F⟩ for {fricative}, ⟨S⟩ for {sibilant},[note 40] ⟨G⟩ for {glide/semivowel}, ⟨L⟩ for {lateral} or {liquid}, ⟨R⟩ for {rhotic} or {resonant/sonorant},[note 41] ⟨₵⟩ for {obstruent}, ⟨Ʞ⟩ for {click}, ⟨A, E, O, Ɨ, U⟩ for {open, front, back, close, rounded vowel}[note 42] and ⟨B, D, Ɉ, K, Q, Φ, H⟩ for {labial, alveolar, post-alveolar/palatal, velar, uvular, pharyngeal, glottal[note 43] consonant}, respectively, and ⟨X⟩ for {any sound}, as in ⟨CVX⟩ for a heavy syllable {CVC, CVV̯, CVː}. The letters can be modified with IPA diacritics, for example ⟨Cʼ⟩ for {ejective}, ⟨Ƈ ⟩ for {implosive}, ⟨N͡C⟩ or ⟨ᴺC⟩ for {prenasalized consonant}, ⟨Ṽ⟩ for {nasal vowel}, ⟨CʰV́⟩ for {aspirated CV syllable with high tone}, ⟨S̬⟩ for {voiced sibilant}, ⟨N̥⟩ for {voiceless nasal}, ⟨P͡F⟩ or ⟨Pꟳ⟩ for {affricate}, ⟨Cᴳ⟩ for a consonant with a glide as secondary articulation (e.g. ⟨Cʲ⟩ for {palatalized consonant} and ⟨Cʷ⟩ for {labialized consonant}) and ⟨D̪⟩ for {dental consonant}. ⟨H⟩, ⟨M⟩, ⟨L⟩ are also commonly used for high, mid and low tone, with ⟨LH⟩ for rising tone and ⟨HL⟩ for falling tone, rather than transcribing them overly precisely with IPA tone letters or with ambiguous digits.[note 44]\n\nTypical examples of archiphonemic use of capital letters are ⟨I⟩ for the Turkish harmonic vowel set {i y ɯ u};[note 45] ⟨D⟩ for the conflated flapped middle consonant of American English writer and rider; ⟨N⟩ for the homorganic syllable-coda nasal of languages such as Spanish and Japanese (essentially equivalent to the wild-card usage of the letter); and ⟨R⟩ in cases where a phonemic distinction between trill /r/ and flap /ɾ/ is conflated, as in Spanish enrejar /eNreˈxaR/ (the n is homorganic and the first r is a trill, but the second r is variable).[110] Similar usage is found for phonemic analysis, where a language does not distinguish sounds that have separate letters in the IPA. For instance, Castillian Spanish has been analyzed as having phonemes /Θ/ and /S/, which surface as [θ] and [s] in voiceless environments and as [ð] and [z] in voiced environments (e.g. hazte /ˈaΘte/ → [ˈaθte], vs hazme /ˈaΘme/ → [ˈaðme], or las manos /laS ˈmanoS/ → [lazˈmanos]).[111]\n\n⟨V⟩, ⟨F⟩ and ⟨C⟩ have completely different meanings as Voice Quality Symbols, where they stand for \"voice\" (VoQS jargon for secondary articulation),[note 46] \"falsetto\" and \"creak\". These three letters may take diacritics to indicate what kind of voice quality an utterance has, and may be used as carrier letters to extract a suprasegmental feature that occurs on all susceptible segments in a stretch of IPA. For instance, the transcription of Scottish Gaelic [kʷʰuˣʷt̪ʷs̟ʷ] 'cat' and [kʷʰʉˣʷt͜ʃʷ] 'cats' (Islay dialect) can be made more economical by extracting the suprasegmental labialization of the words: Vʷ[kʰuˣt̪s̟] and Vʷ[kʰʉˣt͜ʃ].[112] The conventional wildcards ⟨X⟩ or ⟨C⟩ might be used instead of VoQS ⟨V⟩ so that the reader does not misinterpret ⟨Vʷ⟩ as meaning that only vowels are labialized (i.e. Xʷ[kʰuˣt̪s̟] for all segments labialized, Cʷ[kʰuˣt̪s̟] for all consonants labialized), or the carrier letter may be omitted altogether (e.g. ʷ[kʰuˣt̪s̟], [ʷkʰuˣt̪s̟] or [kʰuˣt̪s̟]ʷ). (See § Suprasegmentals for other transcription conventions.)\n\nThis summary is to some extent valid internationally, but linguistic material written in other languages may have different associations with capital letters used as wildcards. For example, in German ⟨K⟩ and ⟨V⟩ are used for Konsonant (consonant) and Vokal (vowel); in Russian, ⟨С⟩ and ⟨Г⟩ are used for согласный (soglasnyj, consonant) and гласный (glasnyj, vowel). In French, tone may be transcribed with ⟨H⟩ and ⟨B⟩ for haut (high) and bas (low).[113]\n\nThe blank cells on the summary IPA chart can be filled without much difficulty if the need arises.\n\nThe missing retroflex letters, namely ⟨ᶑ ꞎ 𝼅 𝼈 𝼊 ⟩, are \"implicit\" in the alphabet, and the IPA supported their adoption into Unicode.[44] Attested in the literature are the retroflex implosive ⟨ᶑ ⟩, the voiceless retroflex lateral fricative ⟨ꞎ ⟩, the retroflex lateral flap ⟨𝼈 ⟩ and the retroflex click ⟨𝼊 ⟩; the first is also mentioned in the IPA Handbook, and the lateral fricatives are provided for by the extIPA.\n\nThe epiglottal trill is arguably covered by the generally trilled epiglottal \"fricatives\" ⟨ʜ ʢ⟩. Ad hoc letters for near-close central vowels, ⟨ᵻ ᵿ⟩, are used in some descriptions of English, though those are specifically reduced vowels – forming a set with the IPA reduced vowels ⟨ə ɐ⟩ – and the simple points in vowel space are easily transcribed with diacritics: ⟨ɪ̈ ʊ̈⟩ or ⟨ɨ̞ ʉ̞⟩. Diacritics are able to fill in most of the remainder of the charts.[note 47] If a sound cannot be transcribed, an asterisk ⟨*⟩ may be used, either as a letter or as a diacritic (as in ⟨k*⟩ sometimes seen for the Korean \"fortis\" velar).\n\nRepresentations of consonant sounds outside of the core set are created by adding diacritics to letters with similar sound values. The Spanish bilabial and dental approximants are commonly written as lowered fricatives, [β̞] and [ð̞] respectively.[note 48] Similarly, voiced lateral fricatives can be written as raised lateral approximants, [ɭ˔ ʎ̝ ʟ̝], though the extIPA also provides ⟨𝼅⟩ for the first of these. A few languages such as Banda have a bilabial flap as the preferred allophone of what is elsewhere a labiodental flap. It has been suggested that this be written with the labiodental flap letter and the advanced diacritic, [ⱱ̟].[115] Similarly, a labiodental trill would be written [ʙ̪] (bilabial trill and the dental sign), and the labiodental plosives are now universally ⟨p̪ b̪⟩ rather than the ad hoc letters ⟨ȹ ȸ⟩ once found in Bantuist literature. Other taps can be written as extra-short plosives or laterals, e.g. [ ɟ̆ ɢ̆ ʟ̆], though in some cases the diacritic would need to be written below the letter. A retroflex trill can be written as a retracted [r̠], just as non-subapical retroflex fricatives sometimes are. The remaining pulmonic consonants – the uvular laterals ([ʟ̠ 𝼄̠ ʟ̠˔]) and the palatal trill – while not strictly impossible, are very difficult to pronounce and are unlikely to occur even as allophones in the world's languages.\n\nThe vowels are similarly manageable by using diacritics for raising, lowering, fronting, backing, centering, and mid-centering.[note 49] For example, the unrounded equivalent of [ʊ] can be transcribed as mid-centered [ɯ̽], and the rounded equivalent of [æ] as raised [ɶ̝] or lowered [œ̞] (though for those who conceive of vowel space as a triangle, simple [ɶ] already is the rounded equivalent of [æ]). True mid vowels are lowered [e̞ ø̞ ɘ̞ ɵ̞ ɤ̞ o̞] or raised [ɛ̝ œ̝ ɜ̝ ɞ̝ ʌ̝ ɔ̝], while centered [ɪ̈ ʊ̈] and [ä] (or, less commonly, [ɑ̈]) are near-close and open central vowels, respectively.\n\nThe only known vowels that cannot be represented in this scheme are vowels with unexpected roundedness. For unambiguous transcription, such sounds would require dedicated diacritics. Possibilities include ⟨ʏʷ⟩ or ⟨ɪʷ⟩ for protrusion and ⟨uᵝ⟩ (or VoQS ⟨ɯᶹ⟩) for compression. However, these transcriptions suggest that the sounds are diphthongs, and so while they may be clear for a language like Swedish where they are diphthongs, they may be misleading for languages such as Japanese where they are monophthongs. The extIPA 'spread' diacritic ⟨◌͍⟩ is sometimes seen for compressed ⟨u͍⟩, ⟨o͍⟩, ⟨ɔ͍⟩, ⟨ɒ͍⟩, though again the intended meaning would need to be explained or they would be interpreted as being spread the way that cardinal ⟦i⟧ is. For protrusion (w-like labialization without velarization), Ladefoged & Maddieson use the old IPA omega diacritic for labialization, ⟨◌̫⟩, for protruded ⟨y᫇⟩, ⟨ʏ̫⟩, ⟨ø̫⟩, ⟨œ̫⟩. This is an adaptation of an old IPA convention of rounding an unrounded vowel letter like i with a subscript omega (⟨◌̫⟩) and unrounding a rounded letter like u with a subscript turned omega.[117] As of 2024 , the turned omega diacritic is in the pipeline for Unicode, and is under consideration for compression in extIPA.[118] Kelly & Local use a combining w diacritic ⟨◌ᪿ⟩ for protrusion (e.g. ⟨yᷱ øᪿ⟩) and a combining ʍ diacritic ⟨◌ᫀ⟩ for compression (e.g. ⟨uᫀ oᫀ⟩).[119] Because their transcriptions are manuscript, these are effectively the same symbols as the old IPA diacritics, which indeed are historically cursive w and ʍ. However, the more angular ⟨◌ᫀ⟩ of typescript might misleadingly suggest the vowel is protruded and voiceless (like [ʍ]) rather than compressed and voiced.\n\nIn both print and speech, an IPA symbol is often distinguished from the sound it transcribes because IPA letters very often do not have their cardinal IPA values in practice. This is commonly the case in phonemic and broad phonetic transcription, making articulatory descriptions of IPA letters, such as \"mid front rounded vowel\" or \"voiced velar stop\", inappropriate as names for those letters. While the Handbook of the International Phonetic Association states that no official names exist for its symbols, it admits the presence of one or two common names for each.[120] The symbols also have nonce names in the Unicode standard. In many cases, the names in Unicode and the IPA Handbook differ. For example, the Handbook calls ⟨ɛ⟩ \"epsilon\", while Unicode calls it \"small letter open e\".\n\nThe traditional names of the Latin and Greek letters are usually used for unmodified letters.[note 50] Letters which are not directly derived from these alphabets, such as ⟨ʕ⟩, may have a variety of names, sometimes based on the appearance of the symbol or on the sound that it represents. In Unicode, some of the letters of Greek origin have Latin forms for use in IPA; the others use the characters from the Greek block.\n\nFor diacritics, there are two methods of naming. For traditional diacritics, the IPA notes the name in a well known language; for example, ⟨é⟩ is \"e-acute\", based on the name of the diacritic in English and French. Non-traditional diacritics are often named after objects they resemble, so ⟨d̪⟩ is called \"d-bridge\".\n\nGeoffrey Pullum and William Ladusaw [d] list a variety of names in use for both current and retired IPA symbols in their Phonetic Symbol Guide. Many of them found their way into Unicode.[9]\n\nUnicode supports nearly all of the IPA. Apart from basic Latin and Greek and general punctuation, the primary blocks are IPA Extensions, Spacing Modifier Letters and Combining Diacritical Marks, with lesser support from Phonetic Extensions, Phonetic Extensions Supplement, Combining Diacritical Marks Supplement, and scattered characters elsewhere. The extended IPA is supported primarily by those blocks and Latin Extended-G.\n\nAfter the Kiel Convention in 1989, most IPA symbols were assigned an identifying number to prevent confusion between similar characters during the printing of manuscripts. The codes were never much used and have been superseded by Unicode.\n\nMany typefaces have support for IPA characters, but good diacritic rendering remains rare.[122] Web browsers generally do not need any configuration to display IPA characters, provided that a typeface capable of doing so is available to the operating system.\n\nTypefaces that provide full IPA and nearly full extIPA support, including properly rendering the diacritics, include Gentium Plus, Charis SIL, Doulos SIL, and Andika developed by SIL International. Indeed, the IPA chose Doulos to publish their chart in Unicode format. In addition to the level of support found in commercial and system fonts, these fonts support the full range of old-style (pre-Kiel) staveless tone letters, through a character variant option that suppresses the stave of the Chao tone letters. They also have an option to maintain the [a] ~ [ɑ] vowel distinction in italics. The only notable gaps are with the extIPA: the combining parentheses, which enclose diacritics, are not supported, nor is the enclosing circle that marks unidentified sounds, and which Unicode considers to be a copy-edit mark and thus not eligible for Unicode support.\n\nThe basic Latin Noto fonts commissioned by Google also have significant IPA support, including diacritic placement, only failing with the more obscure IPA and extIPA characters and superscripts of the Latin Extended-F and Latin Extended-G blocks. The extIPA parentheses are included, but they do not enclose diacritics as they are supposed to.\n\nDejaVu is the second free Unicode font chosen by the IPA to publish their chart. It was last updated in 2016 and so does not support the Latin F or G blocks. Stacked diacritics tend to overstrike each other.\n\nAs of 2018 , the IPA was developing their own font, unitipa, based on TIPA.[123]\n\nCalibri, the default font of Microsoft Office, has nearly complete IPA support with good diacritic rendering, though it is not as complete as some free fonts (see image at right). Other widespread Microsoft fonts, such as Arial and Times New Roman, have poor support.\n\nThe Apple system fonts Geneva, Lucida Grande and Hiragino (certain weights) have only basic IPA support.\n\nBrill has complete IPA and extIPA coverage of characters added to Unicode by 2020, with good diacritic and tone-letter support. It is a commercial font but is freely available for non-commercial use.[124]\n\nSeveral systems have been developed that map the IPA symbols to ASCII characters. Notable systems include SAMPA and X-SAMPA. The usage of mapping systems in on-line text has to some extent been adopted in the context input methods, allowing convenient keying of IPA characters that would be otherwise unavailable on standard keyboard layouts.\n\nIETF language tags have registered fonipa as a variant subtag identifying text as written in IPA.[125] Thus, an IPA transcription of English could be tagged as en-fonipa. For the use of IPA without attribution to a concrete language, und-fonipa is available.\n\nOnline IPA keyboard utilities are available, though none of them cover the complete range of IPA symbols and diacritics. Examples are the IPA 2018 i-charts hosted by the IPA,[126] IPA character picker by Richard Ishida at GitHub,[127] Type IPA phonetic symbols at TypeIt.org,[128] and an IPA Chart keyboard by Weston Ruter also at GitHub.[129] In April 2019, Google's Gboard for Android added an IPA keyboard to its platform.[130][131] For iOS there are multiple free keyboard layouts available, such as the IPA Phonetic Keyboard.[132]"
    },
    {
        "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9796287",
        "document": "Phonetic transcription using the International Phonetic Alphabet (IPA, 1999) is an essential component in the training of all Speech and Language Therapists (SLTs), and is included in the Health and Care Professionals Council's Standards of Proficiency for SLTs (HCPC, 2014). The Child Speech Disorder Research Network notes that ‘transcription ability is a skill unique to SLTs […] no‐one else can provide this information about a child's speech’ (CSDRN, 2017: 2). Thus, SLTs are unique amongst healthcare and primary/secondary education professionals in their ability to transcribe speech phonetically, and indeed to listen to speech objectively. There are numerous approaches and levels of detail that can be employed in transcription: Heselwood (2013) lists several (albeit not mutually exclusive) dichotomies such as specific versus generic, speaker versus listener orientated, and systematic versus impressionistic, along with other categories such as phonemic, allophonic and segmental transcription. A key distinction is the broad–narrow continuum, which ranges from narrow phonetic transcription capturing specific details of physical utterances to broad phonetic transcription capturing physical utterances in less detail, through to phonemic transcription which captures only the contrastive categories used rather than the specific realizations. One approach is not inherently superior to any other; rather, Heselwood (2013: 25) proposes that the quality of a transcription may be judged by how well it fulfils its intended purpose. In clinical practice, phonetic transcription and subsequent analysis is used to inform differential diagnoses of particular categories of speech sound disorder (SSD), which have different aetiologies, for example those which are phonological in nature compared with those which are secondary to organic causes such as cleft palate or hearing impairment (e.g., Dodd, 2014). Transcription is also used to select clinical targets and intervention approaches, and to provide a baseline against which progress can be monitored (CSDRN, 2017). Recommendations for transcription are provided in the Good Practice Guidelines for the Transcription of Child Speech (CSDRN, 2017)—there is no equivalent guidance for adult speech, although many of the recommendations are applicable to both groups. The level of detail employed is at the discretion of the individual clinician: the guidelines note that SLTs may use either broad or narrow transcription as needed; a position which supports Heselwood's (2013) view, described above. In a review of 320 SSD referrals over a 15‐month period, over 87.5% were identified as phonological rather than articulatory in nature (Broomfield & Dodd, 2004). The practice of using broad transcription for phonological errors is endorsed by the CSDRN (2017: 8), and in a survey of SLTs in the UK (Knight et al., 2018: 780) 40.6% reported using only broad transcription with an exemplar rationale being, ‘I use broad transcription as I find it meets my needs’. For speech errors that are not found in typical development, Howard and Heselwood (2002: 373) believe this ‘will clearly require transcription at a phonetic, rather than phonemic, level’, implying that atypical speech by definition cannot be adequately captured by a phonemic transcription. This is not necessarily the case, however: some subtypes of SSD such as consistent phonological disorder and inconsistent phonological disorder are characterized by errors which, although atypical, are phonological rather than articulatory, such as initial consonant deletion, or inconsistent phonological substitutions (Dodd, 2014). Conversely, typical speech errors (either age appropriate or late‐persisting) may require more detailed transcription for errors such as feature synthesis in consonant clusters such as [n̥] for /sn/ (e.g., Chin & Dinnsen, 1992), suggesting a false dichotomy between the practice of using broad transcription for typical errors versus narrow transcription for atypical errors. For clients with atypical articulation (as opposed to atypical phonological) errors, much of the literature advocates the use of narrow transcription, including use of the ExtIPA symbols for disordered speech (IPA, 1999). Muller and Papakyritsis (2011) and Ball et al. (2009) present examples of clinically significant features captured by a narrow transcription, demonstrating how transcription can inform the subsequent intervention. Narrow transcription is recommended for cleft palate or hearing‐impaired speech in clinical guidance (CSDRN, 2017), and was reportedly used by specialist SLTs working with those populations in Knight et al.’s (2018) survey. Studies involving SLT students have similarly found that their experience of seeing or using narrow transcription is almost exclusively restricted to cleft palate and hearing impairment contexts (e.g., Shaw & Yanushevskaya, 2021; Windsor, 2011). When recommending narrow transcription for these populations, however, the CSDRN guidelines do not specify whether all segments should be transcribed narrowly (including, for example, acceptable allophonic realizations) or only the disordered realizations, and in Knight et al. (2018) those who used narrow transcription did not specify whether this applied to all segments. As an alternative to narrow transcription, Meyer and Munson (2021) propose using a perceptual rating scale, for example, on a spectrum between a ‘[t]‐like sound’ and a ‘[k]‐like sound’, while Roxburgh et al. (2016) found that SLTs’ perceptual evaluations (without using traditional transcription) were adequate for assessing the speech of children with repaired sub‐mucous cleft palate. Thus, the question remains about whether (and how) clinicians might make judgements about which aspects of speech require more detailed transcription and in which situations they should transcribe more tokens to facilitate analysis. Howard and Heselwood (2002) warn that clinicians should avoid judging clinical relevance too early, and with reference to ophthalmology, Hussain and Oestreicher (2018: 120) describe a number of cognitive biases in clinical decision‐making and diagnosis, whereby ‘a failure of heuristics may lead to diagnostic error’. An early judgement of clinical relevance may therefore cause the listener to miss details which may not be immediately salient, yet have implications for the diagnosis and subsequent therapy input for clients. Different SSDs require specific intervention approaches; for example a phonological intervention such as Multiple Oppositions Therapy would be inappropriate for children with non‐phonological SSDs such as childhood apraxia of speech. Accurate diagnosis, which is informed by accurate transcription, therefore holds significant implications for the efficacy of clinical interventions, as well as considerations relating to time‐ and cost‐effectiveness. Further to its application in speech assessment and diagnosis, transcription is also used to monitor progress (CSDRN, 2017: 8) and to evidence therapy outcomes (e.g., Enderby et al., 2009). Routinely incorporating transcription into reassessment—a common practice reported by SLTs in Knight et al.’s (2018) survey—could demonstrate progress between two points in time (pre‐ and post‐intervention) and, in a broader context, demonstrate the value and effectiveness of SLT interventions service to the commissioning body (Children's Commissioner, 2019: 7). However, in a review of 174 sets of clinical case notes, Morgan et al. (2021) found that SLTs did not consistently record or analyse pre‐ and post‐intervention data in sufficient detail to adequately monitor progress and to evidence outcomes for children with SSD. Research describing the clinical use of transcription has primarily focused on organic SSDs such as cleft palate speech (e.g., Roxburgh et al., 2016; Sell, 2004) or hearing‐impaired speech (e.g., Teoh & Chin, 2009). Much less has been published about transcription (and subsequent analysis and decisions about clinical pathways) for other client groups, such as children with other types of SSD, or adult clients. In a survey of 333 SLPs, Skahan et al. (2007) report that many favoured particular published screening assessments and were over‐reliant on these tools to make decisions about clinical pathways for children, without sufficient analysis of speech alongside the screening assessment. A survey of 231 Australian paediatric speech–language pathologists (SLPs) found that typical assessment consisted of single‐word sampling, stimulability testing and judgement of intelligibility (Baker & McLeod 2014). The study did not investigate transcription specifically, although 85.6% reported that their assessment ‘sometimes’ or ‘always’ included consideration of the client's phonetic inventory. Joffe and Pring (2008) found that the majority of 98 participating SLTs rated themselves as ‘confident’ or ‘very confident’ in selecting SSD interventions, but again did not investigate the role of transcription in this decision‐making process. In a survey of SLTs working with a range of client groups, Knight et al. (2018) found that many lacked confidence in using transcription, particularly narrow transcription. This cohort also reported limited opportunities to maintain skills after pre‐registration training, although 75% expressed an interest in opportunities for continuing professional development (CPD). Both Windsor (2011) and Knight et al. (2018) hypothesize a ‘theory to practice’ gap in SLTs’ use of transcription, suggesting a difference between the advanced level at which transcription is taught to SLT students and the more basic level which is subsequently used in clinical practice. In the context of limited guidance from professional bodies about the transcription skills needed upon qualification, Titterington and Bates (2021) make a case for developing benchmark competencies at various levels, from newly qualified practitioner (NQP) to generalist and finally specialist SLTs (e.g., those working with cleft palate and hearing impairment caseloads), in order to support the maintenance and development of transcription skills in clinical practice. Practical barriers to transcription, such as limited time for detailed transcription or for CPD, have been highlighted previously (Titterington & Bates, 2021; Windsor, 2011), and recently a further challenge has arisen following the increase in the use of telehealth. During the COVID‐19 pandemic, a survey by the Royal College of Speech and Language Therapists (RCSLT, 2020) found that 63.1% of SLT respondents were using online platforms for client‐facing sessions. The inherent challenges of telehealth models—including connectivity, technical difficulties and distortions of the acoustic signal—have been noted by Sevitz et al. (2021) and Campbell and Goldstein (2022), but neither study considers transcription specifically. Little is known about SLTs’ ability to transcribe effectively via telehealth, and the RCSLT (2020) identified a need for further research into the use of telehealth with particular client groups (although did not specify which groups). To date, there is little in‐depth qualitative research into SLTs’ practices and views about transcription, particularly amongst generalist SLTs (those whose caseloads are mixed rather than solely SSD clients). Knight et al.’s (2018) online survey provides a useful insight into the views and practices of SLTs working with a range of client groups: while this study design enabled sampling of a large number of SLTs (n = 759) it could not facilitate opportunities for in‐depth discussions or probing clinicians’ views and practices in detail. A further gap in the literature concerns service‐wide considerations such as explicitly adopting a consistent approach to transcription, implementing and promoting existing professional guidance, or service‐wide barriers and facilitators to transcription. The shift towards telehealth also requires studies to establish whether and how clinicians’ views of and practices in transcription have changed. The present study seeks to provide further in‐depth information about these areas through the following research questions: What are SLTs’ views about clinical phonetic transcription? What are the working practices of SLTs and SLT services in relation to transcription, and what factors influence these practices?\n\nThree broad themes were generated from the analysis: (1) division and unity; (2) one small part of a big job; and (3) fit for purpose. Figure 1 shows the themes and sub‐themes. Themes and sub‐themes [Colour figure can be viewed at wileyonlinelibrary.com] SLTs identified that transcription is essential to the work of SLTs, with 10 participants stating, unprompted, that it was vital for their role, for example: ‘I wouldn't be able to do my job, I don't think, without transcription.’ One SLT who worked with a general paediatric caseload stated, ‘it's rare that a day would go by when I don't transcribe’. There was also a recognition that transcription is unique to the SLT profession within healthcare more broadly, and many expressed pride in this unique skill: It is definitely something that's so unique to us and we sort of take it for granted cos even when you find it difficult […] we've got at least some knowledge of it and that is a very unique skill. Some also discussed feeling excitement and pride in the process of learning to transcribe as a student: ‘look, I can do funny symbols […] I felt like a bit of superiority’. Thus, SLTs felt that transcription was an inherent part of their identity, and that this skill both unites them and also distinguishes them from other healthcare professionals. However, SLTs working with general paediatric caseloads described a clear divide between their own role and that of specialist SLTs such as SSD, cleft palate or hearing impairment specialists. SLTs appeared to identify both themselves and their service as either specialized or not specialized in using transcription, indicating that despite their view of transcription as a specialist skill for the profession, they have different perceptions about their skills in using transcription and its application to their own role. Non‐specialist (generalist) SLTs frequently discussed ‘the specialist’, framing the specialist as an inherently different type of SLT, with the onward referral marking a clear boundary between the two roles. Onward referrals were made when it became apparent that more in‐depth transcription and more specialist management was required, for example, for children with multiple atypical speech errors: If I feel like narrow transcription is needed, I try, I try my hardest but usually I'm like ‘OK and now I'm gonna pass you on to our specialist’ because it's a child that, they're gonna be too complicated for me. If a child comes in the door and the first thing that it says is a bunch of vowels, you're instantly going [sitting up and widening eyes]—cos you know you're going to transfer them, you know you're getting a specialist involved [laughing]. Similarly, those who identified as specialists in transcription also perceived inherent differences between themselves and generalist SLTs: one cleft palate specialist described referrals from generalists where ‘they are wrong’ (although she acknowledged that ‘we'd prefer that than not getting the referral at all’). There were also differences in the use (or not) of specific symbols used by SLTs depending on which populations they usually work with. One generalist SLT felt that repeatedly using the CLEAR phonology assessment (Keeling & Keeling, 2006)—which was consistently described by generalists as their ‘go‐to’ assessment—with similar clients had led to her becoming accustomed to completing the assessment as if by rote, using the same English phoneme symbols each time to capture typical phonological errors: ‘the majority of my transcription is the CLEAR, and it's almost now become muscle memory. And then I feel like I've lost that wider skill of being able to transcribe’. Indeed, the CLEAR assessment does not necessarily require transcription at all, and some SLTs reported using ticks for correct realizations and circles around errored sounds on the printed orthographic words: ‘we'll circle the sound as we're doing it live, so that we can quickly see what sounds they're not producing’. On the whole, generalist SLTs were confident in identifying errors such as phoneme substitution or omission through objective listening (with or without transcribing), and some were also able to identify certain sounds not typically found in English speech that they encounter frequently: ‘I think there's a few now that I know, like bilabial fricative, I find that quite a lot with kids producing an <f> as a bilabial fricative so I know that now’. However, many expressed doubts about their ability to perceive less common errors: ‘I'm constantly thinking, “I think that's what I heard? I'm not sure, I think that was it.”’ One SLT described feeling ‘consciously incompetent’ at transcription, adding ‘I always feel if something more disordered walks through the door I can't transcribe it as accurately as I want to […] I'm constantly just waiting for that person that's going to challenge that.’ Others found certain symbols difficult to remember: I do have ones that I always forget, like […] voiced palatal fricative, I can always remember the voiceless but then I have to really think about the voiced one and I guess the same for vowels as well. Generalist SLTs employed strategies such as annotating assessment forms to indicate and describe sounds that were difficult to transcribe: I'd write it down as best I can and I'd put some type of mark on this sound that I know I've not transcribed to be the sound it is [laughing], and then […] write down something along the lines of, mouth isn't open enough, or tongue's too far back. SLTs working with cleft palate or hearing impairment clients were more likely to use ExtIPA symbols and/or additional diacritics, and to view this as essential for their role, but again tended to describe this in terms of specific symbols to capture the features they commonly encounter within their specialist population: A common one they, for <f>, it looks as though they're saying [f] cos they mark the place but it's still stopped, there's no fricative so it's just like [silently demonstrating labiodental plosive] and how to transcribe that is now a new fave of mine cos I just see it all the time, but I know how to transcribe it. (hearing impairment specialist) I'll note down the kind of resonance features or airflow features […] I would use diacritics all the time to indicate nasality, nasal emission, nasal turbulence. We would use weakness quite a lot, by virtue of VPI, but also for dysarthric patients. (cleft palate specialist) Similar to generalist SLTs, specialists did not necessarily feel confident about transcribing features not commonly found in the populations they usually work with: a hearing impairment specialist commented, ‘I think I'd still feel nervous if you gave me a cleft child’, and a cleft palate specialist described feeling unsure about symbols for certain vowels. Four SLTs in total described difficulties with vowel transcription, for example, ‘I rarely get a child that makes vowel errors but when I do it does throw me a little bit’—although it was unclear whether ‘errors’ referred to distortions or substitutions. SLTs identified that transcription is a skill which requires an investment of time, both for the quality of transcription in each session and also for the maintenance of one's own skills over the course of a career. The length of a session is often limited by service‐wide protocols: ‘we have to try and do the assessment and do the notes and write a report within an hour’ (one SLT reported an even shorter limit of 30 min). For SLTs with more flexibility, time was still a consideration when planning clinical activity across a caseload: ‘I can't spend too long on transcribing one particular piece of work when I know that I've got a backlog of other bits that need to be done.’ Time limitations were also described in relation to schools who directly commission SLT input: Being able to evidence that [time] back to people who are commissioning you in school being like ‘so where were you all afternoon?’ like, ‘oh you know that one boy? Well I wanted to sit and listen to everything that he was doing’ and they look at you like, ‘are you mad?’. The need to remain mindful of service protocols, caseload management and commissioner satisfaction suggests that there is often a tension between the desire to complete clinical processes to a high standard whilst also meeting the day‐to‐day demands of one's employer. Thus, the time required to complete a detailed transcription, or to transcribe more tokens, means that transcription is not always a priority for individual SLTs within a session. SLTs also recognized that within a clinical session there are numerous clinical aspects to observe and record. Many did not transcribe speech as part of a language assessment, for example if clients produce phonemic paraphasias or incorrect realizations: ‘I'm more focused on their language […] speech, it just goes a bit out the window for me’—although some reported using phonemic transcription if the target word or the scoring was ambiguous: If it's not pronounced properly I will transcribe it sometimes, because you know sometimes when they say /skɛlɪskəʊp/ [telescope …] I think you can get a point [on the assessment] for /skɛlɪskəʊp/ but I need to look back in the manual to see how near it was to the target word in order to get the point. Transcribing can also prove difficult to manage whilst also engaging the client, particularly when working with young children: At university you'd have minimal distractions [when transcribing], then in a clinic room, the child could be throwing the foam dice around the room, they could be hiding under the table, so I think in clinic there's more distractions. Most generalist SLTs reported that they transcribe single words, in formal assessments only, rather than transcribing connected speech in assessment sessions or transcribing at any level in therapy (as opposed to assessment or reassessment) sessions: I know I probably should transcribe during therapy sessions as well, but I must say I do that less. On top of everything else you've got to remember that's sort of a second thought for me. One SLT summarized all of these difficulties succinctly, saying ‘it's one small part of a big job’, capturing the notion that, for many reasons, transcription is not always a priority for SLTs within clinical sessions. Maintaining or improving transcription skills also requires investment, and many SLTs discussed their experiences of and their interest in CPD for transcription. All SLTs reported that they value reassurance from others, such as colleagues, CPD tutors or fellow CPD delegates, and reflected on the benefits of consensus transcription: If I transcribe something similar to whoever is sat near me [in a CPD session], I'm like, ‘oh yeah, I got it right’ […] I just like knowing I've got a similar thought to everybody else around me and I think when you work by yourself so much you don't have that with anybody. The transition from student to NQP, in terms of opportunities for reassurance, was also commented on by one NQP: ‘you had so much support at uni and then someone's trusting you to transcribe’. Generalist SLTs did not routinely attend transcription CPD sessions, with the exception of the first author's regional group (attended by nine participants, of which seven were generalist SLTs), which was free of charge and held out with working hours. SLTs made suggestions for how CPD could be delivered: one proposed ‘a phonetic transcription revision session, like an online thing, and we can log in in once a year and you could have like a revision package’, while another suggested ‘we could all bring case studies’. A hearing impairment specialist reflected that students can also provide reassurance to qualified practitioners: ‘of all the things to do together with a student, transcribing a speech assessment together is a really nice activity and a really good learning activity […] it's really helpful for us as well’, suggesting that both novice and experienced transcribers value the opportunity to calibrate one's own transcription with others. However, the investment of time for CPD was, again, not always a priority at a whole‐service level: We had some conversations about trying to set something up in the team where we did more joint transcribing as a CPD thing […] but it's just trying to fit it in with everything else in the service, it's quite hard. Two SLTs in who worked in the same service discussed the difficulties they faced in asking their employer to fund CPD courses: You'd have to link it into your appraisal and your learning objectives, and it's quite a specific thing to put down if you're not in that specialist area, so I don't know if they'd let you access it. Supporting the notion of a divide between specialists and generalists as discussed under Theme 1, transcription appeared to hold a higher value in cleft palate and hearing impairment services, where investment in practices such as audio‐recording, longer clinical sessions and attending CPD courses was more common: I've been working with hearing impaired children for two years now, so I've been on some courses and I've annotated my IPA chart and I have that next to me so I just spend a lot longer doing it in more detail. Similarly, consistent whole‐team approaches to transcription were described by one cleft palate specialist: We record all of our VPI assessments so that's where children come where there's a concern about the palate and they'll have x‐rays as well, so on those days they'll have recordings and our audit clinics are also recorded. A lack of investment in transcription practices in generalist services was also evident in SLTs’ accounts of technical difficulties in producing and storing electronic transcriptions. Many described electronic patient record (EPR) systems which do not support IPA symbols, meaning that SLTs must write or type transcriptions and attach these documents separately. Some SLTs therefore attempt to capture clients’ realizations in the EPR using orthography, while others use descriptive labels for sounds: one acknowledged ‘that's weakened my knowledge of symbols, because I'll sometimes, especially for more obscure ones I'll just think to myself how to describe it [in the EPR] rather than get the symbol’. For SLTs who use EPRs which do allow IPA symbols, this was often limited to base symbols: ‘if you have to put extra diacritic bits in, I mean, I've no idea how you'd do that’. Generalist services had not prioritized investigations into how to facilitate the use of IPA symbols in EPRs (‘some therapist said that you could get [a function for IPA symbols] but we don't have it’), while conversely in specialist services such as cleft palate, ‘we put a huge bid forward for being able to transcribe [electronically …] it allows when I copy and paste in from the IPA’. Similarly, many services had not established clear policies to enable SLTs to make and store audio‐recordings of clients’ speech for subsequent analysis: one SLT reported that ‘we're not allowed to record in my trust, so it's always live […] I'd love to be able to record’, while another commented that ‘[there's] just red tape everywhere you look’. Theme 1 (‘Division and Unity’) identified that generalist SLTs were confident in using broad transcription to capture phonological processes. Most felt this level was sufficient to identify therapy targets, and therefore judged their transcription skills to be good enough for their purposes: I would say more than 90% of my transcription is only at a broad sort of phonemic level, rarely do I go beyond that actually. However I find that it's enough to do what I need to do with the vast majority of the children. One SLT, who works with adults with Apraxia of Speech, reported that her colleagues generally use orthographic transcription, arguing that this is sufficient for identifying patterns of substitutions or omissions at a phonemic level and therefore for setting targets: Because it's just the English symbols really, you probably could do that with orthographic transcription […] I think people have been getting along just fine without it to be honest and working with apraxia of speech and just using spelling. This SLT acknowledged, however, that she did use IPA symbols to capture non‐English realizations made by bi‐ or multilingual clients: ‘he was a Tamil speaker so it was useful to get a straight‐up, what's his phonological inventory, to start off with’, and she also went on to reflect that transcription can invoke an underlying knowledge of place, manner and voice categories and therefore guide clinical decisions when setting targets: I can see by using the IPA whether that is one sound or a type of sound, so in that way it would set goals like ok we're gonna work on bilabials first, maybe […] so it can help me categorise what type of sounds to go for. This view was shared by another SLT who felt that transcription inherently reflected SLTs’ training and their ability to listen to speech: ‘it's not just learning a new alphabet, is it, which people might think it is […] we've had loads of training around using it’. Another reflected that ‘even just being able to listen and to pick out the sounds is a skill in itself, isn't it?’. When planning intervention for clients, some specialist SLTs described how their transcription might suggest a particular diagnosis and therefore a specific care pathway or intervention: At that prognostic level of deciding what route does a patient need to go down, which is you know, is applicable to our population, the transcription is sort of key to signposting them (cleft palate specialist). However, generalist SLTs rarely discussed how their transcription might inform diagnosis, and actually suggested that the reverse might occur—that is, their early judgement about the type of SSD informs the transcription: As soon as they start talking I guess I'd make some type of judgement going ‘oh okay you're just fronting yeah alright then’ […] whereas if they open their mouth and they sounded like there was something more disordered going on then I would tune in more and I would transcribe more. I'd be making a judgement about how long‐term or short‐term it was and therefore how much time I would be spending on doing the transcription in the first place […] with a simple speech delay it would seem like not really worth doing loads of transcription. Instead of using their transcription to inform their target‐setting, therefore, generalist SLTs used clinical judgement to anticipate what their targets might be, and then disregarded in their transcription the features that they judged to be less relevant: ‘I'd probably go for more clinically relevant or clinically not relevant [… if] I'm not going to set a target around that, it's not clinically relevant, it doesn't really matter’. Many felt that transcription held limited value as a communication system between clinicians. While they acknowledged that clients are frequently transferred between SLTs, there is often a long period of time between the initial transcription and the subsequent intervention: ‘there would usually be a quite significant gap of time that had passed and so you probably want to do some reassessment anyway’. For this reason, many generalist SLTs reported that they did not usually read transcriptions completed by their colleagues. Others were mindful of colleagues’ confidence and ability in using and reading transcription, and felt that less common symbols might not be understood by others, suggesting that transcription is not always perceived as a core skill for SLTs: Other therapists might not have an awareness of the diacritic, so it's almost trying to make sure that your transcription is readable for other therapists. If that's for another therapist to look at, are they going to know what you've put […] if you've put some fancy whatever? I don't know. In a discussion about service‐wide protocols and shared approaches to transcription, one SLT suggested that ‘a protocol for the team I think would make reading other people's transcriptions easier’. A total of 11 SLTs described the value of transcription as a means of monitoring progress and evaluating therapy outcomes for children: ‘I've been doing a lot of therapy with a boy, I think he's got Developmental Verbal Dyspraxia, and I think the transcription has been key because it allows me to monitor his progress.’ Two others commented on the role of transcription in demonstrating the effectiveness of intervention approaches to commissioners: We're trying to prove that Multiple Oppositions Therapy is effective and if [we] didn't transcribe we wouldn't have anything to statistically show that […] We can say ‘look how many speech sounds they were missing pre‐intervention, look at their inventory now’ and that's all [from the] transcription so I think it's really important. SLTs reported many specific challenges associated with telehealth. Some reflected that typical phonemic substitutions were easy to perceive: ‘where it worked OK was if they were doing quite an obvious and quite clear process’; but others lacked confidence in their ability to perceive even common phonological substitutions via telehealth: ‘if there's a slight time lag, it really throws it off. And especially with [k] and [t] like fronting or backing, I just cannot tell’. Two SLTs discussed feeling more confident when using telehealth to reassess a child who they had previously met in person, because they had prior expectations about the client's speech production, although one reflected that this in itself caused some bias: They could have totally changed their errors and you're thinking last time we saw him, ‘oh yeah, you could never get a [k], so yeah, that sounds right’. And actually they have. You just have this thought of what they're going to sound like when you go into the call. SLTs working with all client groups felt less confident about telehealth transcription compared to live transcription, with three specialist therapists (cleft palate and hearing impairment) noting particular features that were hard to perceive: We'd realise that mostly it was airflow errors that we weren't picking up on, and active nasal fricatives, so the fricative sounds were the ones that we were the most vulnerable on, just because of the audio link (cleft palate specialist). One hearing impairment specialist concluded that ‘I can't do speech assessment over telehealth […] I really can't reliably transcribe’, suggesting that transcription via telehealth is not always fit for purpose, particularly with certain populations.\n\nThis study investigated the views and working practices of 19 SLTs in relation to phonetic transcription, and the factors that influence these. Three themes were generated: (1) division and unity; (2) one small part of a big job; and (3) fit for purpose. Here, these themes are summarized and discussed, and their collective implications considered. SLTs in this study recognized transcription as a specialist skill which they felt proud of, supporting the CSDRN's (2017) position that transcription is unique, within healthcare, to SLT. While almost all SLTs working with speech saw transcription as an inherent part of their role, clear differences in views and working practices exist between generalist and specialist SLTs. The limited use of narrow transcription by generalist SLTs in this study (and their lack of confidence in using it) is consistent with previous research (e.g., Knight et al., 2018; Windsor, 2011); however, some had evidently retained the ability to perceive sub‐phonemic details even when they could not recall or did not know the symbol, using annotations and descriptions instead. In cases where SLTs’ transcription skills were not adequate for capturing details, their management (such as making onward referrals to specialist services) was still appropriate, suggesting that SLTs’ decision‐making, if not the actual transcription, is nonetheless ‘fit for purpose’. Many SLTs made early judgements about clients’ overall intelligibility, their ‘typicalness’ for the clinical population and their prognosis in order to choose which features to capture in transcription. SLTs often had prior expectations about what they might hear and what their focus should be, both in transcribing and in setting targets: while some discussed the role of bias specifically in telehealth reassessment, this could also apply to assessment more broadly, regardless of method. The practice of making an early judgement, therefore, must still be regarded with caution: Hussain and Oestreicher (2018: 120) note that heuristics are ‘useful in assimilating a large amount of information and distillating this into salient points’, but warn that time and workload pressures, along with being less familiar with the presenting problem, can cause bias in clinical decision‐making. This observation is particularly relevant in light of SLTs’ views that transcription is ‘one small part of a big job’ and their descriptions of strict time limits for clinical sessions. Existing recommendations for transcription (e.g., CSDRN, 2017) recognize that SLTs may employ different levels of detail in transcription: this is borne out by reported clinical practices in this study and supports Heselwood's (2013) view that a transcription is detailed enough if it fulfils its intended purpose. However, the practices reported in this study suggest that details are often overlooked and existing clinical guidance for transcription is not followed—indeed, transcription is not always used, with SLTs sometimes using orthographic transcription or a ‘tick‐box’ system. This has significant implications for accurate analysis and diagnosis, including specific considerations relating to bi‐ and multilingual clients for whom SLTs may need to capture realizations not typically found in an English phonetic inventory. Analysis of speech without adequate transcription (either due to lack of detail or limited tokens transcribed) also has the potential to affect the selection of evidence‐based interventions for specific types of SSD, and the ability to monitor and evidence outcomes for children (e.g., Morgan et al., 2021), as well as increased likelihood of re‐referral of clients if an unsuitable intervention is chosen. While listening to speech and transcribing at any level is a skill in itself, if narrow transcription is viewed as the exclusive domain of specialist SLTs then generalist therapists risk missing potentially significant details, and being unable to transcribe and, through analysis, identify more unusual errors when needed. SLTs cannot monitor clients’ progress without transcribing, particularly as most SLTs in this study did not transcribe routinely in therapy sessions. There is a further danger, in exclusively using broad transcription and relying on one favoured assessment tool, that SLTs risk diminishing the specialist skill which they are manifestly proud of and which delineates their role from that of other professionals. Participants in this study who had attended CPD sessions valued the opportunity to learn from others and to gain reassurance about their ability to transcribe—similarly, Knight et al. (2018) reported that peer support was suggested by many SLTs as a CPD activity. However, many SLTs rarely, if ever, attend any CPD for transcription (Knight et al., 2018), and the current study provides insight into some of the reasons for this: some generalist services are unlikely to fund CPD for transcription, which both reinforces the notion of the divide between generalist and specialist, and indicates that transcription may be viewed as a skill for which only specialist SLTs need to undertake CPD. Theme two identified that transcription is seen as ‘one small part of a big job’, and therefore CPD is not prioritized by services or individual SLTs. With few opportunities for post‐qualification CPD in transcription, SLTs may lack the skills needed to progress to specialist posts, or to succeed in such posts. The views and practices reported in this study support the case, made by Titterington and Bates (2021), for developing benchmark competencies for transcription skills at various levels, whereby targeted CPD could be undertaken to work towards each level. These data also highlight the barriers presented by technology, including the use of adequate EPR systems and clear policies around recording clients’ speech, as well as the challenges of telehealth which has recently become significantly more widespread. These challenges include identifying nasality and place of articulation, which impacts on SLTs’ confidence when transcribing via telehealth. As this method of assessment appears likely to continue (RCSLT, 2020), the results from this study raise tentative questions about transcribing via telehealth and about whether, in the case of phonetic distortions particularly, alternatives such as a continuous rating scale (e.g., Meyer & Munson 2021) may be beneficial. Finally, the results reported here represent wider considerations within the profession. Concerns about lack of time and funding for CPD, conflicting demands and the need to be accountable to service commissioners are not specific to the issue of transcription, and the importance of promoting the value of SLT‐specific skills could also be applied to other practices (e.g., grammatical analysis) as well as transcription. Similarly, overreliance on screening tools (e.g., Skahan et al., 2007), the use of heuristics, and the possibility of cognitive bias, might also be applied to all SLTs’ decision‐making, and indeed that of other healthcare professionals. Thus, transcription is ‘one small part’ but also reflects the ‘big job’ more broadly. Although questions were used as a basis for broader discussion, some responses were prompted by key words in the questions such as ‘confidence’ and ‘barriers’. Prior relationships between researchers and participants also influenced the discussions—for example, some participants were aware of the moderator's views about transcription, which may have prompted comments such as ‘I know I probably should transcribe more in therapy sessions’. However, prior relationships were primarily a supportive factor in establishing rapport. Participants in this study were, by definition, likely to have an interest in transcription (indeed some explicitly stated this) and to use transcription routinely. Adult SLTs were underrepresented, while service managers were not represented at all, which could have provided a useful perspective about policies as well as views about the profile and value of transcription at a whole‐service level. Future research could specifically elicit the views of adult SLTs and service managers in order to explore their perceptions. SLTs in this study felt their transcription skills were fit for purpose; however, this study elicited self‐reported practices rather than objectively analysing SLTs’ transcription, and also did not clarify participants’ understanding of terms such as broad and narrow transcription. Future studies could analyse the actual transcription practices of SLTs in order to consider the validity of their perceptions about being fit for purpose, and to investigate the role of transcription in clinical case studies. While some SLTs suggested CPD practices such as online packages or discussion of case studies, identifying a consensus about what CPD should include was beyond the scope of this study. This would be a valuable area to investigate in the future, and could potentially inform the development of CPD programmes, supported by some suggestions made by SLTs in this study (e.g., a focus on vowels). Similarly, establishing a consensus about service‐wide protocols for transcription by generalist SLTs (and developing this for use within services) would be a useful direction for future research, with a tangible and clinically relevant output. SLTs do not all use transcription in the same way, and establishing consistency across the profession is neither achievable nor, perhaps, desirable. However increased investment in good transcription practice could afford SLTs the confidence to transcribe in more detail when needed and to trust that transcriptions will be mutually understood between colleagues. While SLTs’ skills are broadly fit for purpose, this does not always apply when they encounter clients who differ from their usual populations, whose speech contains less common errors which SLTs are not always confident in perceiving and transcribing. SLTs would value and benefit from external opportunities to improve transcription skills, in order to support more reliable onward referrals and to improve diagnostic accuracy and appropriate management. Service managers could therefore promote and facilitate transcription by allowing more time when necessary, removing ‘red tape’ to enable audio‐recording and storage, investigating the use of IPA symbols in EPRs (which could also be considered by professional bodies at national level) and encouraging and funding peer support and CPD for transcription. Investing in transcription has the potential not only to promote the profession by placing value on SLTs’ unique skill, but also to inform accurate analysis of speech. This, in turn, supports appropriate management including the use of evidence‐based interventions and, ultimately, improved outcomes for clients.\n\nIn your everyday practice, what does transcription mean to you? How often do you transcribe clients’ speech? What sort of utterances do you transcribe? Prompts: Formal/informal assessments for the purposes of explicitly assessing speech, spontaneous speech that you want to capture, errors made in language assessments (e.g. a phonemic paraphasia)? Do you audio or video record clients, or do you always transcribe live? Or both at the same time? Prompts: why/why not? If so, do you set aside additional time for transcribing or for reviewing your transcription after the session? Does this help with transcribing? Why/why not? How do you store transcriptions? Prompts: handwritten into case notes / handwritten transcriptions scanned into or attached to EPR? Do you use an EPR that supports IPA? Are there any barriers to transcribing in everyday practice? Does everyone in your service use transcription in the same way? Prompts: do you discuss this explicitly (e.g. CPD sessions)? Does your service have any guidelines for transcription best practice? How confident are you in transcribing Prompts: confidence in broad/narrow transcription? Are there any areas that you would like to improve? Has your confidence changed since qualifying? Since moving into a specialist team? What do you do if you are transcribing a client's speech and you're not sure about a sound that they used? When you transcribe a client's speech, what are you really interested in capturing? Tell me about a client whose speech you have transcribed recently. Prompts: did you plan to transcribe their speech before you saw them? Did you use a formal assessment tool? Was their speech appropriate or delayed/disordered? How much of their speech did you transcribe? Did you transcribe sounds, words or phrases/sentences? How much do you rely on transcription to support with target‐setting? Prompts: link to previous question – talk about whether/how the transcription informed the target‐setting. What will you do next with this client? What was your experience of learning transcription as a student? Prompts: did you have any prior experience of transcription? Did you enjoy it? How easy or difficult did you find it? How well did it relate to your clinical learning on placement? Do you feel able to support students to develop transcription skills on placement? Prompt: how do you do this? Do you think it's important to do this as a clinical educator? Ability to phonetically transcribe is one of the HCPC Standards Of Proficiency. How important do you think it is in practice? Do you have any other thoughts that you would like to share? Can you tell me why you were interested in taking part in this group?"
    }
]