[
    {
        "link": "https://requests.readthedocs.io/en/latest/user/advanced",
        "document": "This document covers some of Requests more advanced features.\n\nThe Session object allows you to persist certain parameters across requests. It also persists cookies across all requests made from the Session instance, and will use ’s connection pooling. So if you’re making several requests to the same host, the underlying TCP connection will be reused, which can result in a significant performance increase (see HTTP persistent connection). A Session object has all the methods of the main Requests API. Sessions can also be used to provide default data to the request methods. This is done by providing data to the properties on a Session object: # both 'x-test' and 'x-test2' are sent Any dictionaries that you pass to a request method will be merged with the session-level values that are set. The method-level parameters override session parameters. Note, however, that method-level parameters will not be persisted across requests, even if using a session. This example will only send the cookies with the first request, but not the second: If you want to manually add cookies to your session, use the Cookie utility functions to manipulate . Sessions can also be used as context managers: This will make sure the session is closed as soon as the block is exited, even if unhandled exceptions occurred. Sometimes you’ll want to omit session-level keys from a dict parameter. To do this, you simply set that key’s value to in the method-level parameter. It will automatically be omitted. All values that are contained within a session are directly available to you. See the Session API Docs to learn more.\n\nWhenever you receive a object from an API call or a Session call, the attribute is actually the that was used. In some cases you may wish to do some extra work to the body or headers (or anything else really) before sending a request. The simple recipe for this is the following: # do something with prepped.body 'No, I want exactly this as the body.' # do something with prepped.headers Since you are not doing anything special with the object, you prepare it immediately and modify the object. You then send that with the other parameters you would have sent to or . However, the above code will lose some of the advantages of having a Requests object. In particular, -level state such as cookies will not get applied to your request. To get a with that state applied, replace the call to with a call to , like this: # do something with prepped.body 'Seriously, send exactly these bytes.' # do something with prepped.headers When you are using the prepared request flow, keep in mind that it does not take into account the environment. This can cause problems if you are using environment variables to change the behaviour of requests. For example: Self-signed SSL certificates specified in will not be taken into account. As a result an is thrown. You can get around this behaviour by explicitly merging the environment settings into your session:\n\nIf you need to use a proxy, you can configure individual requests with the argument to any request method: Alternatively you can configure it once for an entire : Setting may behave differently than expected. Values provided will be overwritten by environmental proxies (those returned by urllib.request.getproxies). To ensure the use of proxies in the presence of environmental proxies, explicitly specify the argument on all individual requests as initially explained above. When the proxies configuration is not overridden per request as shown above, Requests relies on the proxy configuration defined by standard environment variables , , , and . Uppercase variants of these variables are also supported. You can therefore set them to configure Requests (only set the ones relevant to your needs): To use HTTP Basic Auth with your proxy, use the syntax in any of the above configuration entries: Storing sensitive username and password information in an environment variable or a version-controlled file is a security risk and is highly discouraged. To give a proxy for a specific scheme and host, use the form for the key. This will match for any request to the given scheme and exact hostname. Note that proxy URLs must include the scheme. Finally, note that using a proxy for https connections typically requires your local machine to trust the proxy’s root certificate. By default the list of certificates trusted by Requests can be found with: You override this default certificate bundle by setting the (or ) environment variable to another file path: In addition to basic HTTP proxies, Requests also supports proxies using the SOCKS protocol. This is an optional feature that requires that additional third-party libraries be installed before use. You can get the dependencies for this feature from : Once you’ve installed those dependencies, using a SOCKS proxy is just as easy as using a HTTP one: Using the scheme causes the DNS resolution to happen on the client, rather than on the proxy server. This is in line with curl, which uses the scheme to decide whether to do the DNS resolution on the client or proxy. If you want to resolve the domains on the proxy server, use as the scheme.\n\nRequests provides access to almost the full range of HTTP verbs: GET, OPTIONS, HEAD, POST, PUT, PATCH and DELETE. The following provides detailed examples of using these various verbs in Requests, using the GitHub API. We will begin with the verb most commonly used: GET. HTTP GET is an idempotent method that returns a resource from a given URL. As a result, it is the verb you ought to use when attempting to retrieve data from a web location. An example usage would be attempting to get information about a specific commit from GitHub. Suppose we wanted commit on Requests. We would get it like so: We should confirm that GitHub responded correctly. If it has, we want to work out what type of content it is. Do this like so: So, GitHub returns JSON. That’s great, we can use the method to parse it into Python objects. So far, so simple. Well, let’s investigate the GitHub API a little bit. Now, we could look at the documentation, but we might have a little more fun if we use Requests instead. We can take advantage of the Requests OPTIONS verb to see what kinds of HTTP methods are supported on the url we just used. Uh, what? That’s unhelpful! Turns out GitHub, like many API providers, don’t actually implement the OPTIONS method. This is an annoying oversight, but it’s OK, we can just use the boring documentation. If GitHub had correctly implemented OPTIONS, however, they should return the allowed methods in the headers, e.g. Turning to the documentation, we see that the only other method allowed for commits is POST, which creates a new commit. As we’re using the Requests repo, we should probably avoid making ham-handed POSTS to it. Instead, let’s play with the Issues feature of GitHub. This documentation was added in response to Issue #482. Given that this issue already exists, we will use it as an example. Let’s start by getting it. Cool, we have three comments. Let’s take a look at the last of them. Probably in the \"advanced\" section Well, that seems like a silly place. Let’s post a comment telling the poster that he’s silly. Who is the poster, anyway? OK, so let’s tell this Kenneth guy that we think this example should go in the quickstart guide instead. According to the GitHub API doc, the way to do this is to POST to the thread. Let’s do it. \"Sounds great! I'll get right on it!\" Huh, that’s weird. We probably need to authenticate. That’ll be a pain, right? Wrong. Requests makes it easy to use many forms of authentication, including the very common Basic Auth. Sounds great! I'll get right on it. Brilliant. Oh, wait, no! I meant to add that it would take me a while, because I had to go feed my cat. If only I could edit this comment! Happily, GitHub allows us to use another HTTP verb, PATCH, to edit this comment. Let’s do that. \"Sounds great! I'll get right on it once I feed my cat.\" Excellent. Now, just to torture this Kenneth guy, I’ve decided to let him sweat and not tell him that I’m working on this. That means I want to delete this comment. GitHub lets us delete comments using the incredibly aptly named DELETE method. Let’s get rid of it. Excellent. All gone. The last thing I want to know is how much of my ratelimit I’ve used. Let’s find out. GitHub sends that information in the headers, so rather than download the whole page I’ll send a HEAD request to get the headers. Excellent. Time to write a Python program that abuses the GitHub API in all kinds of exciting ways, 4995 more times.\n\nAs of v1.0.0, Requests has moved to a modular internal design. Part of the reason this was done was to implement Transport Adapters, originally described here. Transport Adapters provide a mechanism to define interaction methods for an HTTP service. In particular, they allow you to apply per-service configuration. Requests ships with a single Transport Adapter, the . This adapter provides the default Requests interaction with HTTP and HTTPS using the powerful urllib3 library. Whenever a Requests is initialized, one of these is attached to the object for HTTP, and one for HTTPS. Requests enables users to create and use their own Transport Adapters that provide specific functionality. Once created, a Transport Adapter can be mounted to a Session object, along with an indication of which web services it should apply to. The mount call registers a specific instance of a Transport Adapter to a prefix. Once mounted, any HTTP request made using that session whose URL starts with the given prefix will use the given Transport Adapter. The adapter will be chosen based on a longest prefix match. Be mindful prefixes such as will also match or . It’s recommended to terminate full hostnames with a . Many of the details of implementing a Transport Adapter are beyond the scope of this documentation, but take a look at the next example for a simple SSL use- case. For more than that, you might look at subclassing the . The Requests team has made a specific choice to use whatever SSL version is default in the underlying library (urllib3). Normally this is fine, but from time to time, you might find yourself needing to connect to a service-endpoint that uses a version that isn’t compatible with the default. You can use Transport Adapters for this by taking most of the existing implementation of HTTPAdapter, and adding a parameter ssl_version that gets passed-through to . We’ll make a Transport Adapter that instructs the library to use SSLv3: \"\"\"\"Transport adapter\" that allows us to use SSLv3.\"\"\" By default, Requests does not retry failed connections. However, it is possible to implement automatic retries with a powerful array of features, including backoff, within a Requests using the urllib3.util.Retry class:\n\nMost requests to external servers should have a timeout attached, in case the server is not responding in a timely manner. By default, requests do not time out unless a timeout value is set explicitly. Without a timeout, your code may hang for minutes or more. The connect timeout is the number of seconds Requests will wait for your client to establish a connection to a remote machine (corresponding to the connect()) call on the socket. It’s a good practice to set connect timeouts to slightly larger than a multiple of 3, which is the default TCP packet retransmission window. Once your client has connected to the server and sent the HTTP request, the read timeout is the number of seconds the client will wait for the server to send a response. (Specifically, it’s the number of seconds that the client will wait between bytes sent from the server. In 99.9% of cases, this is the time before the server sends the first byte). If you specify a single value for the timeout, like this: The timeout value will be applied to both the and the timeouts. Specify a tuple if you would like to set the values separately: If the remote server is very slow, you can tell Requests to wait forever for a response, by passing None as a timeout value and then retrieving a cup of coffee. The connect timeout applies to each connection attempt to an IP address. If multiple addresses exist for a domain name, the underlying will try each address sequentially until one successfully connects. This may lead to an effective total connection timeout multiple times longer than the specified time, e.g. an unresponsive server having both IPv4 and IPv6 addresses will have its perceived timeout doubled, so take that into account when setting the connection timeout. Neither the connect nor read timeouts are wall clock. This means that if you start a request, and look at the time, and then look at the time when the request finishes or times out, the real-world time may be greater than what you specified."
    },
    {
        "link": "https://realpython.com/python-requests",
        "document": "The Requests library is the de facto standard for making HTTP requests in Python. It abstracts the complexities of making requests behind a beautiful, simple API so that you can focus on interacting with services and consuming data in your application.\n\nThroughout this tutorial, you’ll see some of the most useful features that Requests has to offer as well as ways to customize and optimize those features for different situations that you may come across. You’ll also learn how to use Requests in an efficient way as well as how to prevent requests to external services from slowing down your application.\n\nIn this tutorial, you’ll learn how to:\n• Make requests using the most common HTTP methods\n• Customize your requests’ headers and data using the query string and message body\n• Inspect data from your requests and responses\n• Configure your requests to help prevent your application from backing up or slowing down\n\nFor the best experience working through this tutorial, you should have basic general knowledge of HTTP. That said, you still may be able to follow along fine without it.\n\nIn the upcoming sections, you’ll see how you can install and use in your application. If you want to play with the code examples that you’ll see in this tutorial, as well as some additional ones, then you can download the code examples and work with them locally:\n\nEven though the Requests library is a common staple for many Python developers, it’s not included in Python’s standard library. There are good reasons for that decision, primarily that the library can continue to evolve more freely as a self-standing project. Note: Requests doesn’t support asynchronous HTTP requests directly. If you need async support in your program, you should try out AIOHTTP or HTTPX. The latter library is broadly compatible with Requests’ syntax. Because Requests is a third-party library, you need to install it before you can use it in your code. As a good practice, you should install external packages into a virtual environment, but you may choose to install into your global environment if you’re planning to use it across multiple projects. Whether you’re working in a virtual environment or not, you’ll need to install : Once has finished installing , you can use it in your application. Importing looks like this: Now that you’re all set up, it’s time to begin your journey through Requests. Your first goal will be learning how to make a request.\n\nA is a powerful object for inspecting the results of the request. Make that same request again, but this time store the return value in a variable so that you can get a closer look at its attributes and behaviors: In this example, you’ve captured the return value of , which is an instance of , and stored it in a variable called . You can now use to see a lot of information about the results of your request. The first bit of information that you can gather from is the status code. A status code informs you of the status of the request. For example, a status means that your request was successful, whereas a status means that the resource you were looking for wasn’t found. There are many other possible status codes as well to give you specific insights into what happened with your request. By accessing , you can see the status code that the server returned: returned , which means that your request was successful and the server responded with the data that you were requesting. Sometimes, you might want to use this information to make decisions in your code: With this logic, if the server returns a status code, then your program will print . If the result is a , then your program will print . Requests goes one step further in simplifying this process for you. If you use a instance in a conditional expression, then it’ll evaluate to if the status code was smaller than , and otherwise. Therefore, you can simplify the last example by rewriting the statement: In the code snippet above, you implicitly check whether the of is between and . If it’s not, then you raise an exception that includes the non-success status code in an f-string. Note: This truth value test is possible because is an overloaded method on . This means that the adapted default behavior of takes the status code into account when determining the truth value of the object. Keep in mind that this method is not verifying that the status code is equal to . The reason for this is that other status codes within the to range, such as and , are also considered successful in the sense that they provide some workable response. For example, the status code tells you that the response was successful, but there’s no content to return in the message body. So, make sure you use this convenient shorthand only if you want to know if the request was generally successful. Then, if necessary, you’ll need to handle the response appropriately based on the status code. Let’s say you don’t want to check the response’s status code in an statement. Instead, you want to use Request’s built-in capacities to raise an exception if the request was unsuccessful. You can do this using : If you invoke , then Requests will raise an for status codes between and . If the status code indicates a successful request, then the program will proceed without raising that exception. Now, you know a lot about how to deal with the status code of the response that you got back from the server. However, when you make a request, you rarely only care about the status code of the response. Usually, you want to see more. Next, you’ll see how to view the actual data that the server sent back in the body of the response. The response of a request often has some valuable information, known as a payload, in the message body. Using the attributes and methods of , you can view the payload in a variety of different formats. To see the response’s content in , you use : While gives you access to the raw bytes of the response payload, you’ll often want to convert them into a string using a character encoding such as UTF-8. will do that for you when you access : Because the decoding of to a requires an encoding scheme, Requests will try to guess the encoding based on the response’s headers if you don’t specify one. You can provide an explicit encoding by setting before accessing : If you take a look at the response, then you’ll see that it’s actually serialized JSON content. To get a dictionary, you could take the that you retrieved from and deserialize it using . However, a simpler way to accomplish this task is to use : The of the return value of is a dictionary, so you can access values in the object by key: You can do a lot with status codes and message bodies. But, if you need more information, like metadata about the response itself, then you’ll need to look at the response’s headers. The response headers can give you useful information, such as the content type of the response payload and a time limit on how long to cache the response. To view these headers, access : returns a dictionary-like object, allowing you to access header values by key. For example, to see the content type of the response payload, you can access : There’s something special about this dictionary-like headers object, though. The HTTP specification defines headers as case-insensitive, which means that you’re able to access these headers without worrying about their capitalization: Whether you use the key or , you’ll get the same value. Now that you’ve seen the most useful attributes and methods of in action, you already have a good overview of Requests’ basic usage. You can get content from the Internet and work with the response that you receive. But there’s more to the Internet than plain and straightforward URLs. In the next section, you’ll take a step back and see how your responses change when you customize your requests to account for query string parameters.\n\nAccording to the HTTP specification, , , and the less common requests pass their data through the message body rather than through parameters in the query string. Using Requests, you’ll pass the payload to the corresponding function’s parameter. takes a dictionary, a list of tuples, bytes, or a file-like object. You’ll want to adapt the data that send in the body of your request to the specific needs of the service that you’re interacting with. For example, if your request’s content type is , then you can send the form data as a dictionary: You can also send that same data as a list of tuples: If, however, you need to send JSON data, then you can use the parameter. When you pass JSON data via , Requests will serialize your data and add the correct header for you. Like you learned earlier, the httpbin service accepts test requests and responds with data about the requests. For instance, you can use it to inspect a basic request: You can see from the response that the server received your request data and headers as you sent them. Requests also provides this information to you in the form of a that you’ll inspect in more detail in the next section.\n\nAuthentication helps a service understand who you are. Typically, you provide your credentials to a server by passing data through the header or a custom header defined by the service. All the functions of Requests that you’ve seen to this point provide a parameter called , which allows you to pass your credentials: The request succeeds if the credentials that you pass in the tuple to are valid. When you pass your credentials in a tuple to the parameter, Requests applies the credentials using HTTP’s Basic access authentication scheme under the hood. You may wonder where the string that Requests set as the value for your header comes from. In short, it’s a Base64-encoded string of the username and password with the prefix :\n• First, Requests combines the username and password that you provided, putting a colon in between them. So for the username and password , this becomes .\n• Then, Requests encodes this string in Base64 using . The encoding converts the string to .\n• Finally, Requests adds in front of this Base64 string. This is how the final value for the header becomes in the example shown above. HTTP Basic authentication isn’t very secure, because you can decode the username and password from the Base64 string. That’s why it’s important to always send these requests over HTTPS, which provides an additional layer of security by encrypting the entire HTTP request. You could make the same request by passing explicit Basic authentication credentials using : Though you don’t need to be explicit for Basic authentication, you may want to authenticate using another method. Requests provides other methods of authentication out of the box, such as and . A real-world example of an API that requires authentication is GitHub’s authenticated user API. This endpoint provides information about the authenticated user’s profile. If you try to make a request without credentials, then you’ll see that the status code is : If you don’t provide authentication credentials when accessing a service that requires them, then you’ll get an HTTP error code as a response. To make a request to GitHub’s authenticated user API, you first need to generate a personal access token with the read:user scope. Then you can pass this token as the second element in a tuple to : Like you learned previously, this approach passes the credentials to , which expects a username and a password and sends the credentials as a Base64-encoded string with the prefix : This works, but it’s not the right way to authenticate with a Bearer token—and using an empty string input for the superfluous username is awkward. With Requests, you can supply your own authentication mechanism to fix that. To try this out, create a subclass of and implement : \"\"\"Attach an API token to the Authorization header.\"\"\" Here, your custom mechanism receives a token, then includes that token in the header of your request, also setting the recommended prefix to the string. You can now use this custom token authentication to make your call to GitHub’s authenticated user API: Your custom created a well-formatted string for the header. You can now use this more intuitive way of interacting with a token-based authentication scheme such as the one that parts of GitHub’s API require. Note: While you could construct the authentication string outside of a custom authentication class and pass it directly with , this appoach is discouraged because it can lead to unexpected behavior. When you attempt to set your authentication credentials directly using , then Requests may internally overwrite your input. This can happen, for example, if you have a file that provides authentication credentials. Requests will attempt to get the credentials from the file if you don’t provide an authentication method using . Bad authentication mechanisms can lead to security vulnerabilities. Unless a service requires a custom authentication mechanism for some reason, you’ll always want to use a tried-and-true auth scheme like the built-in Basic authentication or OAuth, for example through Requests-OAuthlib. While you’re thinking about security, consider dealing with SSL certificates using Requests.\n\nAnytime the data that you’re trying to send or receive is sensitive, security is important. The way that you communicate with secure sites over HTTP is by establishing an encrypted connection using SSL, which means that verifying the target server’s SSL certificate is critical. The good news is that Requests does this for you by default. However, there are some cases where you might want to change this behavior. If you want to disable SSL certificate verification, then you pass to the parameter of the request function: InsecureRequestWarning: Unverified HTTPS request is being made to host Requests even warns you when you’re making an insecure request to help you keep your data safe! Note: Requests uses a package called to provide certificate authorities. This lets Requests know which authorities it can trust. Therefore, you should update frequently to keep your connections as secure as possible. Now that you know how to make all sorts of HTTP requests using Requests, authenticated or not, you may wonder about how you can make sure that your program works as quickly as possible. In the next section, you’ll learn about a few ways that you can improve performance with the help of Requests.\n\nWhen using Requests, especially in a production application environment, it’s important to consider performance implications. Features like timeout control, sessions, and retry limits can help you keep your application running smoothly. When you make an inline request to an external service, your system will need to wait for the response before moving on. If your application waits too long for that response, requests to your service could back up, your user experience could suffer, or your background jobs could hang. By default, Requests will wait indefinitely on the response, so you should almost always specify a timeout duration to prevent these issues from happening. To set the request’s timeout, use the parameter. can be an integer or float representing the number of seconds to wait on a response before timing out: In the first request, the request will time out after 1 second. In the second request, the request will time out after 3.05 seconds. You can also pass a tuple to with the following two elements:\n• Connect timeout: The time it allows for the client to establish a connection to the server\n• Read timeout: The time it’ll wait on a response once your client has established a connection Both of these elements should be numbers, and can be of type or : If the request establishes a connection within 3.05 seconds and receives data within 5 seconds of the connection being established, then the response will be returned as it was before. If the request times out, then the function will raise a exception: \"The request did not time out\" Your program can catch the exception and respond accordingly. Until now, you’ve been dealing with high-level APIs such as and . These functions are abstractions of what’s going on when you make your requests. They hide implementation details, such as how connections are managed, so that you don’t have to worry about them. Underneath those abstractions is a class called . If you need to fine-tune your control over how requests are being made or improve the performance of your requests, you may need to use a instance directly. Sessions are used to persist parameters across requests. For example, if you want to use the same authentication across multiple requests, then you can use a session: In this code example, you use a context manager to ensure that the session releases the resources when it doesn’t need them anymore. In line 7, you log in using your custom . You only need to log in once per session, and then you can make multiple authenticated requests. Requests will persist the credentials while the session exists. You then make two requests to the authenticated user API in lines 9 and 10 using instead of . The primary performance optimization of sessions comes in the form of persistent connections. When your app makes a connection to a server using a , it keeps that connection around in a connection pool. When your app wants to connect to the same server again, it’ll reuse a connection from the pool rather than establishing a new one. When a request fails, you may want your application to retry the same request. However, Requests won’t do this for you by default. To apply this functionality, you need to implement a custom transport adapter. Transport adapters let you define a set of configurations for each service that you’re interacting with. For example, say you want all requests to to retry two times before finally raising a . You’d build a transport adapter, set its parameter, and mount it to an existing : In this example, you’ve set up your session so that it’ll retry a maximum of two times when your request to GitHub’s API doesn’t work as expected. When you mount the —in this case, —to , then will adhere to its configuration for each request to . Note: While the implementation shown above works, you won’t see any effect of the retry behavior unless there’s something wrong with your network connection or GitHub’s servers. If you want to play around with code that builds on top of this example, and you’d like to inspect when the retries happen, then you’re in luck. You can download the materials of this tutorial and take a look at : Get Your Code: Click here to download the free sample code that shows you how to use Python’s Requests library. The code in this file improves on the example shown above by using the underlying to further customize the retry functionality. It also adds logging to display debugging output, which gives you a chance to monitor when Python attempted the retries. Requests comes packaged with intuitive implementations for timeouts, transport adapters, and sessions that can help you keep your code efficient and your application resilient."
    },
    {
        "link": "https://requests.readthedocs.io",
        "document": "Requests is an elegant and simple HTTP library for Python, built for human beings.\n\nRequests allows you to send HTTP/1.1 requests extremely easily. There’s no need to manually add query strings to your URLs, or to form-encode your POST data. Keep-alive and HTTP connection pooling are 100% automatic, thanks to urllib3."
    },
    {
        "link": "https://medium.com/@vishalbty/introduction-to-python-requests-de74d5203931",
        "document": "Python Requests is a popular, open-source library designed to simplify the process of making HTTP requests in Python. Its user-friendly interface, combined with powerful features, makes it a go-to choice for developers when dealing with web-based data and services. The primary purpose of the Requests library is to enable easy and efficient interaction with web servers, APIs, and other HTTP-based resources.\n\nHTTP requests are a fundamental aspect of web development, as they enable communication between clients and servers. When users visit websites, interact with web applications, or use APIs, they are making HTTP requests. Web developers need a reliable way to create, send, and process these requests in their applications. Python Requests provides a straightforward and efficient solution for managing HTTP requests, allowing developers to focus on building robust and scalable applications without worrying about low-level details of network communication.\n\nBefore Python Requests, developers relied on built-in libraries like urllib and httplib for handling HTTP requests. While these libraries are functional, they often require more complex and verbose code to achieve the same tasks compared to Python Requests. The Requests library simplifies the process with a more intuitive and user-friendly interface. Some of the advantages of Python Requests over urllib and httplib include:\n• Easier handling of redirects and following links\n• Better support for handling various data formats, like JSON\n• Advanced features like timeouts, retries, and authentication As a result, Python Requests has become the preferred choice for many developers when working with HTTP requests.\n\nInstalling the Requests library The Python Requests library can be easily installed using the pip package manager. To install the library, open your command prompt or terminal and run the following command:\n\nFor users with multiple Python versions installed, it might be necessary to use instead of :\n\nAfter the installation is complete, the Requests library is ready to be imported and used in your Python scripts or applications.\n\nUsing the Requests library is straightforward. First, you’ll need to import the library in your Python script or application:\n\nTo make an HTTP request, simply call the appropriate method (e.g., , , , ) on the module, passing the URL as an argument. For example, to make a GET request to a specific URL:\n\nThe object returned by the request contains useful information about the server's response, such as the status code, headers, and content. Here are some common properties and methods of the object:\n• : Returns the HTTP status code of the response (e.g., 200 for success, 404 for not found).\n• : Decodes and returns the JSON content of the response, if applicable.\n\nA basic example of using Python Requests to fetch and process JSON data:\n\nSimplicity and ease of use\n• Simple syntax for HTTP methods (GET, POST, PUT, DELETE): Python Requests makes it easy to perform various HTTP methods with straightforward functions like , , , and .\n• Accessing response data (headers, content, status codes): The library provides an intuitive way to access response data, such as status codes, headers, and content, through the object.\n• Handling redirects and following links: Requests automatically handles redirects by default, making it easy to follow links and navigate through websites.\n• Using cookies and sessions: The library provides support for managing cookies and sessions, allowing developers to maintain state and user authentication across multiple requests.\n• Customizing headers and authentication: Requests enables customization of headers and authentication, allowing users to set custom headers, use different authentication schemes, and send tokens or API keys with ease.\n• Setting timeouts for requests: Developers can set timeouts for requests to prevent long-running or unresponsive requests from stalling the application.\n• Implementing retries with exponential backoff: Requests does not include a built-in retry mechanism, but it can be easily implemented using third-party libraries like or custom logic, helping to make requests more resilient to temporary network issues.\n• JSON support: Requests makes it easy to send and receive JSON data with built-in encoding and decoding methods.\n• Sending and receiving binary data: The library supports sending and receiving binary data, such as images and files, using the attribute of the object and the parameter in request methods.\n• Dealing with form data and files: Requests simplifies the process of sending form data and files by automatically encoding and decoding when necessary.\n• Certificate verification: By default, Requests verifies SSL certificates to ensure secure communication between the client and the server.\n• Custom SSL configurations: The library allows developers to customize SSL configurations, such as providing custom certificate authorities or disabling certificate verification when necessary.\n• Secure connection options (e.g., proxies, VPN): Requests supports using proxies and VPNs to establish secure connections and bypass network restrictions.\n• Importance of non-blocking requests: Asynchronous requests are crucial for performance, especially in web applications and services, as they allow multiple requests to be made simultaneously without blocking the application.\n• Implementing async requests with aiohttp library: While Requests does not natively support asynchronous requests, the library provides similar functionality with an async interface, making it easy to combine the power of Requests with the benefits of asynchronous programming.\n\nProper error handling and exception management are crucial when working with HTTP requests. The Requests library raises exceptions for certain types of errors, such as network issues, timeouts, or invalid URLs. Developers should use and blocks to handle these exceptions gracefully and prevent their applications from crashing due to unexpected errors.\n\nWhen making multiple requests to the same server or service, it is a good practice to reuse sessions for connection pooling. This can improve performance by reducing the overhead of establishing a new connection for each request. The object in Requests can be used to create and manage persistent connections:\n\nWhen interacting with APIs, it is essential to respect their rate limits and usage guidelines. Exceeding the rate limits can result in temporary or permanent bans. Developers should carefully read the API documentation and implement mechanisms to prevent exceeding these limits, such as using delays or caching responses when appropriate.\n\nPython Requests can be extended with various third-party libraries and integrations to enhance its functionality. Some popular extensions include:\n• : A library that simplifies OAuth authentication, making it easy to work with APIs that require OAuth 1.0 or 2.0.\n• : A library that provides transparent caching for Requests, helping to reduce the number of API calls and improve performance.\n\nUsing these and other third-party libraries can help developers build more powerful and efficient applications while leveraging the simplicity and ease of use provided by Python Requests.\n\nPython Requests can be combined with the BeautifulSoup library to scrape and parse web pages efficiently. Developers can use Requests to fetch the HTML content of a web page and then use BeautifulSoup to extract relevant information, such as links, images, or text.\n\nPython Requests makes it easy to interact with RESTful APIs by providing simple functions for HTTP methods like GET, POST, PUT, and DELETE. Developers can use these functions to send requests to API endpoints, submit or update data, and retrieve responses.\n\nRequests can be used to automate form submissions and user authentication processes in web applications. Developers can submit form data using the function and manage cookies or sessions to maintain user authentication across multiple requests.\n\nPython Requests can be used to test and monitor the functionality, performance, and reliability of web applications and services. Developers can create scripts that send HTTP requests to various endpoints, check the response status codes and content, and report any issues or discrepancies.\n\nPython Requests is a powerful and easy-to-use library for handling HTTP requests in Python. With its simple syntax, advanced features, and user-friendly interface, Requests has become the go-to choice for many developers when working with web-based data and services. Some of the key benefits of using Python Requests include:\n• Simplicity and ease of use\n• Advanced HTTP features, such as handling redirects, cookies, and authentication\n• Support for various data formats, including JSON and binary data\n• SSL and security features, such as certificate verification and custom SSL configurations\n• Compatibility with third-party libraries and integrations, like and\n\nResources for further learning and improvement\n\nTo learn more about Python Requests and improve your skills, consider exploring the following resources:\n• Python Requests : Discover answers and solutions to all your Python Requests issues and questions\n• Python Requests official documentation: The official documentation is an excellent starting point for learning about the library’s features and best practices.\n• Real Python: Real Python offers various articles and tutorials on Python Requests, web scraping, and interacting with APIs.\n• Python for Data Science Handbook: This book by Jake VanderPlas provides an introduction to Python Requests and other essential tools for data science and web development.\n• GitHub repositories: Search for GitHub repositories related to Python Requests to discover real-world examples, open-source projects, and libraries that extend or complement the functionality of Requests.\n\nEncouragement to explore and utilize Python Requests in projects\n\nPython Requests is a valuable tool for any developer working with web-based data and services. We encourage you to explore the features of Python Requests and incorporate it into your projects. As you gain experience with the library, you’ll be better equipped to handle the complexities of modern web development and create more efficient, reliable, and secure applications. Happy coding!\n\nTo learn more about python requests"
    },
    {
        "link": "https://geeksforgeeks.org/python-requests-tutorial",
        "document": "Python Requests library is used for making HTTP requests to a specified URL. Python requests provide inbuilt functionalities for managing both the request and response.\n\nPython requests module has several built-in methods to make HTTP requests to specified URL using GET, POST, PUT, PATCH or HEAD requests. HTTP request is meant to either retrieve data from a specified URI or to push data to a server. It works as a request-response protocol between a client and a server.\n• None Requests is an Apache2 Licensed HTTP library, that allows to send HTTP/1.1 requests using Python.\n• None To play with web, Python Requests is must. Whether it be hitting APIs, downloading entire facebook pages, and much more cool stuff, one will have to make a request to the URL.\n• None Checkout an Example Python Script using Requests and Web Scraping –\n\nIn this tutorial, we will explore What is Python Request Library, How to make GET requests through Python Requests, Response objects and Methods, Authentication using Python Requests, and so on.\n\nLet’s demonstrate how to make a GET request to an endpoint. GET method is used to retrieve information from the given server using a given URI. The GET method sends the encoded user information appended to the page request. The page and the encoded information are separated by the ‘?’ character. For example:\n\nLet’s try making a request to github’s APIs for example purposes.\n\nsave this file as request.py and through terminal run,\n\nGET method is used to retrieve information from the given server using a given URI. POST request method requests that a web server accepts the data enclosed in the body of the request message, most likely for storing it The PUT method requests that the enclosed entity be stored under the supplied URI. If the URI refers to an already existing resource, it is modified and if the URI does not point to an existing resource, then the server can create the resource with that URI. The DELETE method deletes the specified resource The HEAD method asks for a response identical to that of a GET request, but without the response body. It is used for modify capabilities. The PATCH request only needs to contain the changes to the resource, not the complete resource\n\nWhen one makes a request to a URL, it returns a response. This Response object in terms of python is returned by requests.method(), method being – get, post, put, etc. Response is a powerful object with lots of functions and attributes that assist in normalizing data or creating ideal portions of code.\n\nFor example, response.status_code returns the status code from the headers itself, and one can check if the request was processed successfully or not. Response object can be used to imply lots of features, methods, and functionalities.\n\nSave this file as request.py, and run using below command\n\nStatus code 200 indicates that request was made successfully.\n\nresponse.encoding returns the encoding used to decode response.content. response.elapsed returns a timedelta object with the time elapsed from sending the request to the arrival of the response. response.close() closes the connection to the server. response.content returns the content of the response, in bytes. response.cookies returns a CookieJar object with the cookies sent back from the server. response.history returns a list of response objects holding the history of request (url). response.is_permanent_redirect returns True if the response is the permanent redirected url, otherwise False. response.is_redirect returns True if the response was redirected, otherwise False. response.json() returns a JSON object of the result (if the result was written in JSON format, if not it raises an error). response.url returns the URL of the response. response.text returns the content of the response, in unicode. response.status_code returns a number that indicates the status (200 is OK, 404 is Not Found). response.request returns the request object that requested this response. response.reason returns a text corresponding to the status code. response.raise_for_status() returns an HTTPError object if an error has occurred during the process. response.ok returns True if status_code is less than 200, otherwise False.\n\nAuthentication refers to giving a user permissions to access a particular resource. Since, everyone can’t be allowed to access data from every URL, one would require authentication primarily. To achieve this authentication, typically one provides authentication data through Authorization header or a custom header defined by server.\n\nReplace “user” and “pass” with your username and password. It will authenticate the request and return a response 200 or else it will return error 403.\n\nRequests verifies SSL certificates for HTTPS requests, just like a web browser. SSL Certificates are small data files that digitally bind a cryptographic key to an organization’s details. Often, an website with a SSL certificate is termed as secure website. By default, SSL verification is enabled, and Requests will throw a SSLError if it’s unable to verify the certificate.\n\nLet us try to access a website with an invalid SSL certificate, using Python requests\n\nThis website doesn’t have SSL setup so it raises this error. one can also pass the link to the certificate for validation via python requests only.\n\nThis would work in case the path provided is correct for SSL certificate for github.com.\n\nSession object allows one to persist certain parameters across requests. It also persists cookies across all requests made from the Session instance and will use urllib3’s connection pooling. So if several requests are being made to the same host, the underlying TCP connection will be reused, which can result in a significant performance increase. A session object all the methods as of requests.\n\nLet us illustrate use of session objects by setting a cookie to a url and then making a request again to check if cookie is set."
    },
    {
        "link": "https://geeksforgeeks.org/beautifulsoup-error-handling",
        "document": "Sometimes, during scraping data from websites we all have faced several types of errors in which some are out of understanding and some are basic syntactical errors. Here we will discuss on types of exceptions that are faced during coding the script.\n\nWhen we are fetching any website content we need to aware of some of the errors that occur during fetching. These errors may be HTTPError, URLError, AttributeError, or XMLParserError. Now we will discuss each error one by one.\n\nHTTPError occurs when we’re performing web scraping operations on a website that is not present or not available on the server. When we provide the wrong link during requesting to the server then and we execute the program is always shows an Error “Page Not Found” on the terminal.\n\nThe link we provide to the URL is running correctly there is no Error occurs. Now we see HTTPError by changing the link.\n\nWhen we request the wrong website from the server it means that URL which we are given for requesting is wrong then URLError will occur. URLError always responds as a server not found an error.\n\nHere we see that the program executes correct and print output “No Error”. Now we change the URL link for showing the URLError :-\n\nThe AttributeError in BeautifulSoup is raised when an invalid attribute reference is made, or when an attribute assignment fails. When during the execution of code we pass the wrong attribute to a function that attribute doesn’t have a relation with that function then AttributeError occurs. When we try to access the Tag using BeautifulSoup from a website and that tag is not present on that website then BeautifulSoup always gives an AttributeError.\n\nWe take a good example to explain the concept of AttributeError with web scraping using BeautifulSoup:\n\nWe all are gone through XML parser error during coding the web scraping scripts, by the help of BeautifulSoup we parse the document into HTML very easily. If we stuck on the parser error then we easily overcome this error by using BeautifulSoup, and it is very easy to use.\n\nWhen we’re parsing the HTML content from the website we generally use ‘ xml ‘ or ‘ xml-xml ‘ in the parameter of BeautifulSoup constructor. It was written as the second parameter after the HTML document.\n\nXML parser error generally happens when we’re not passing any element in the find() and find_all() function or element is missing from the document. It sometimes gives the empty bracket [] or None as their output."
    },
    {
        "link": "https://realpython.com/beautiful-soup-web-scraper-python",
        "document": "Beautiful Soup is a Python library designed for parsing HTML and XML documents. It creates parse trees that make it straightforward to extract data from HTML documents you’ve scraped from the internet. Beautiful Soup is a useful tool in your web scraping toolkit, allowing you to conveniently extract specific information from HTML, even from complex static websites.\n\nIn this tutorial, you’ll learn how to build a web scraper using Beautiful Soup along with the Requests library to scrape and parse job listings from a static website. Static websites provide consistent HTML content, while dynamic sites may require handling JavaScript. For dynamic websites, you’ll need to incorporate additional tools that can execute JavaScript, such as Scrapy or Selenium.\n\nBy the end of this tutorial, you’ll understand that:\n• You can use Beautiful Soup for parsing HTML and XML documents to extract data from web pages.\n• Beautiful Soup is named after a song in Alice’s Adventures in Wonderland by Lewis Carroll, based on its ability to tackle poorly structured HTML known as tag soup.\n• You’ll often use Beautiful Soup in your web scraping pipeline when scraping static content, while you’ll need additional tools such as Selenium to handle dynamic, JavaScript-rendered pages.\n• Using Beautiful Soup is legal because you only use it for parsing documents. Web scraping in general is also legal if you respect a website’s terms of service and copyright laws.\n\nWorking through this project will give you the knowledge and tools that you need to scrape any static website out there on the World Wide Web. If you like learning with hands-on examples and have a basic understanding of Python and HTML, then this tutorial is for you! You can download the project source code by clicking on the link below:\n\nWeb scraping is the process of gathering information from the internet. Even copying and pasting the lyrics of your favorite song can be considered a form of web scraping! However, the term “web scraping” usually refers to a process that involves automation. While some websites don’t like it when automatic scrapers gather their data, which can lead to legal issues, others don’t mind it. If you’re scraping a page respectfully for educational purposes, then you’re unlikely to have any problems. Still, it’s a good idea to do some research on your own to make sure you’re not violating any Terms of Service before you start a large-scale web scraping project. Say that you like to surf—both in the ocean and online—and you’re looking for employment. It’s clear that you’re not interested in just any job. With a surfer’s mindset, you’re waiting for the perfect opportunity to roll your way! You know about a job site that offers precisely the kinds of jobs you want. Unfortunately, a new position only pops up once in a blue moon, and the site doesn’t provide an email notification service. You consider checking up on it every day, but that doesn’t sound like the most fun and productive way to spend your time. You’d rather be outside surfing real-life waves! Thankfully, Python offers a way to apply your surfer’s mindset. Instead of having to check the job site every day, you can use Python to help automate the repetitive parts of your job search. With automated web scraping, you can write the code once, and it’ll get the information that you need many times and from many pages. Note: In contrast, when you try to get information manually, you might spend a lot of time clicking, scrolling, and searching, especially if you need large amounts of data from websites that are regularly updated with new content. Manual web scraping can take a lot of time and be highly repetitive and error-prone. There’s so much information on the internet, with new information constantly being added. You’ll probably be interested in some of that data, and much of it is out there for the taking. Whether you’re actually on the job hunt or just want to automatically download all the lyrics of your favorite artist, automated web scraping can help you accomplish your goals. The internet has grown organically out of many sources. It combines many different technologies, styles, and personalities, and it continues to grow every day. In other words, the internet is a hot mess! Because of this, you’ll run into some challenges when scraping the web:\n• Variety: Every website is different. While you’ll encounter general structures that repeat themselves, each website is unique and will need personal treatment if you want to extract the relevant information.\n• Durability: Websites constantly change. Say you’ve built a shiny new web scraper that automatically cherry-picks what you want from your resource of interest. The first time you run your script, it works flawlessly. But when you run the same script a while later, you run into a discouraging and lengthy stack of tracebacks! Unstable scripts are a realistic scenario because many websites are in active development. If a site’s structure changes, then your scraper might not be able to navigate the sitemap correctly or find the relevant information. The good news is that changes to websites are often small and incremental, so you’ll likely be able to update your scraper with minimal adjustments. Still, keep in mind that the internet is dynamic and keeps on changing. Therefore, the scrapers you build will probably require maintenance. You can set up continuous integration to run scraping tests periodically to ensure that your main script doesn’t break without your knowledge. Some website providers offer application programming interfaces (APIs) that allow you to access their data in a predefined manner. With APIs, you can avoid parsing HTML. Instead, you can access the data directly using formats like JSON and XML. HTML is primarily a way to visually present content to users. When you use an API, the data collection process is generally more stable than it is through web scraping. That’s because developers create APIs to be consumed by programs rather than by human eyes. The front-end presentation of a site might change often, but a change in the website’s design doesn’t affect its API structure. The structure of an API is usually more permanent, which means it’s a more reliable source of the site’s data. However, APIs can change as well. The challenges of both variety and durability apply to APIs just as they do to websites. Additionally, it’s much harder to inspect the structure of an API by yourself if the provided documentation lacks quality. The approach and tools you need to gather information using APIs is outside the scope of this tutorial. To learn more about it, check out API Integration in Python.\n\nBefore you write any Python code, you need to get to know the website that you want to scrape. Getting to know the website should be your first step for any web scraping project that you want to tackle. You’ll need to understand the site structure to extract the information relevant for you. Start by opening the site that you want to scrape with your favorite browser. Click through the site and interact with it just like any typical job searcher would. For example, you can scroll through the main page of the website: On that page, you can see many job postings in a card format. Each of them has two buttons. If you click on Learn, then you’ll visit Real Python’s home page. If you click on Apply, then you’ll see a new page that contains more detailed descriptions of the job on that card. You might also notice that the URL in your browser’s address bar changes when you navigate to one of those pages. You can encode a lot of information in a URL. Becoming familiar with how URLs work and what they’re made of will help you on your web scraping journey. For example, you might find yourself on a details page that has the following URL: You can deconstruct the above URL into two main parts:\n• The base URL points to the main location of the web resource. In the example above, the base URL is .\n• The path to a specific resource location points to a unique job description. In the example above, the path is . Any job posted on this website will share the same base URL. However, the location of the unique resources will be different depending on the job posting that you view. Usually, similar resources on a website will share a similar location, such as the folder structure . However, the final part of the path points to a specific resource and will be different for each job posting. In this case, it’s a static HTML file named . URLs can hold more information than just the location of a file. Some websites use query parameters to encode values that you submit when performing a search. You can think of them as query strings that you send to the database to retrieve specific records. You’ll find query parameters at the end of a URL. For example, if you go to Indeed and search for “software developer” in “Australia” through the site’s search bar, you’ll see that the URL changes to include these values as query parameters: The query parameters in this URL are . Query parameters consist of three parts:\n• Start: You can identify the beginning of the query parameters by looking for the question mark ( ).\n• Information: You’ll find the pieces of information that constitute one query parameter encoded in key-value pairs, where related keys and values are joined together by an equal sign ( ).\n• Separator: You’ll see an ampersand symbol ( ) separating multiple query parameters if there are more than one. Equipped with this information, you can separate the URL’s query parameters into two key-value pairs:\n• selects the location of the job. Try to change the search parameters and observe how that affects your URL. Go ahead and enter new values in the search bar of the Indeed job board: Change these values to observe the changes in the URL. Next, try to change the values directly in your URL. See what happens when you paste the following URL into your browser’s address bar: If you change and submit the values in the website’s search box, then it’ll be directly reflected in the URL’s query parameters and vice versa. If you change either of them, then you’ll see different results on the website. As you can see, exploring the URLs of a site can give you insight into how to retrieve data from the website’s server. Head back to Fake Python jobs and continue to explore it. This site is a static website containing hardcoded information. It doesn’t operate on top of a database, which is why you won’t have to work with query parameters in this scraping tutorial. Next, you’ll want to learn more about how the data is structured for display. You’ll need to understand the page structure to pick what you want from the HTML response that you’ll collect in one of the upcoming steps. Developer tools can help you understand the structure of a website. All modern browsers come with developer tools installed. In this section, you’ll learn how to work with the developer tools in Chrome. The process will be very similar on other modern browsers. In Chrome on macOS, you can open up the developer tools through the menu by selecting View → Developer → Developer Tools. On Windows and Linux, you can access them by clicking the top-right menu button ( ) and selecting More Tools → Developer Tools. You can also access your developer tools by right-clicking on the page and selecting the Inspect option or using a keyboard shortcut: Developer tools allow you to interactively explore the site’s document object model (DOM) to better understand your source. To dig into your page’s DOM, select the Elements tab in developer tools. You’ll see a structure with clickable HTML elements. You can expand, collapse, and even edit elements right in your browser: The HTML on the right represents the structure of the page you can see on the left. You can think of the text displayed in your browser as the HTML structure of the page. If you’re interested, then you can read more about the difference between the DOM and HTML. When you right-click elements on the page, you can select Inspect to zoom to their location in the DOM. You can also hover over the HTML text on your right and see the corresponding elements light up on the page. Click to expand the exercise block for a specific task to practice using your developer tools: Find a single job posting. What HTML element is it wrapped in, and what other HTML elements does it contain? Play around and explore! The more you get to know the page you’re working with, the easier it’ll be to scrape. But don’t get too overwhelmed with all that HTML text. You’ll use the power of programming to step through this maze and cherry-pick the information that’s relevant to you.\n\nNow that you have an idea of what you’re working with, it’s time to start using Python. First, you’ll want to get the site’s HTML code into your Python script so that you can interact with it. For this task, you’ll use Python’s Requests library. Before you install any external package, you’ll need to create a virtual environment for your project. Activate your new virtual environment, then type the following command in your terminal to install the Requests library: Then open up a new file in your favorite text editor and call it . You only need a few lines of code to retrieve the HTML: When you run this code, it issues an HTTP request to the given URL. It retrieves the HTML data that the server sends back and stores that data in a Python object you called . If you print the attribute of , then you’ll notice that it looks just like the HTML you inspected earlier with your browser’s developer tools. You’ve successfully fetched the static site content from the internet! You now have access to the site’s HTML from within your Python script. The website that you’re scraping in this tutorial serves static HTML content. In this scenario, the server that hosts the site sends back HTML documents that already contain all the data a user gets to see. When you inspected the page with developer tools earlier on, you discovered that a single job posting consists of the following long and messy-looking HTML: It can be challenging to wrap your head around a long block of HTML code. To make it easier to read, you can use an HTML formatter to clean up the HTML automatically. Good readability can help you better understand the structure of any block of code. While improved HTML formatting may or may not help, it’s always worth a try. Note: Keep in mind that every website looks different. That’s why it’s necessary to inspect and understand the structure of the site you’re working with before moving forward. The HTML you’ll encounter will sometimes be confusing. Luckily, the HTML of this job board has descriptive class names on the elements that you’re interested in:\n• contains the title of the job posting.\n• contains the name of the company that offers the position.\n• contains the location where you’d be working. If you ever get lost in a large pile of HTML, remember that you can always go back to your browser and use the developer tools to further explore the HTML structure interactively. By now, you’ve successfully harnessed the power and user-friendly design of Python’s Requests library. With only a few lines of code, you managed to scrape static HTML content from the web and make it available for further processing. While this was a breeze, you may encounter more challenging situations when working on your own web scraping projects. Before you learn how to select the relevant information from the HTML that you just scraped, you’ll take a quick look at two more challenging situations. Some pages contain information that’s hidden behind a login. This means you’ll need an account to be able to scrape anything from the page. Just like you need to log in on your browser when you want to access content on such a page, you’ll also need to log in from your Python script. The Requests library comes with the built-in capacity to handle authentication. With these techniques, you can log in to websites when making the HTTP request from your Python script and then scrape information that’s hidden behind a login. You won’t need to log in to access the job board information, so this tutorial won’t cover authentication. Many modern websites don’t send back static HTML content like this practice site does. If you’re dealing with a dynamic website, then you could receive JavaScript code as a response. This code will look completely different from what you see when you inspect the same page with your browser’s developer tools. Note: In this tutorial, the term dynamic website refers to a website that doesn’t return the same HTML that you see when viewing the page in your browser. Dynamic websites are designed to provide their functionality in collaboration with the clients’ browsers. Instead of sending HTML pages, these apps send JavaScript code that instructs your browser to create the desired HTML. Web apps deliver dynamic content this way to offload work from the server to the clients’ machines, as well as to avoid page reloads and improve the overall user experience. Your browser will diligently execute the JavaScript code it receives from a server and create the DOM and HTML for you locally. However, if you request a dynamic website in your Python script, then you won’t get the HTML page content. When you use Requests, you receive only what the server sends back. In the case of a dynamic website, you’ll end up with JavaScript code without the relevant data. The only way to go from that code to the content that you’re interested in is to execute the code, just like your browser does. The Requests library can’t do that for you, but there are other solutions that can:\n• Requests-HTML is a project created by the author of the Requests library that allows you to render JavaScript using syntax that’s similar to the syntax in Requests. It also includes capabilities for parsing the data by using Beautiful Soup under the hood.\n• Selenium is another popular choice for scraping dynamic content. Selenium automates a full browser and can execute JavaScript, allowing you to interact with and retrieve the fully rendered HTML response for your script. You won’t go deeper into scraping dynamically-generated content in this tutorial. If you need to scrape a dynamic website, then you can look into one of the options mentioned above.\n\nYou’ve successfully scraped some HTML from the internet, but when you look at it, it looks like a mess. There are tons of HTML elements here and there, thousands of attributes scattered around—and maybe there’s some JavaScript mixed in as well? It’s time to parse this lengthy code response with the help of Python to make it more accessible so you can pick out the data that you want. Beautiful Soup is a Python library for parsing structured data. It allows you to interact with HTML in a similar way to how you interact with a web page using developer tools. The library exposes intuitive methods that you can use to explore the HTML you received. Note: The name Beautiful Soup originates from the Lewis Carroll song Beautiful Soup in Alice’s Adventures in Wonderland, where a character sings about beautiful soup. This name reflects the library’s ability to parse poorly formed HTML that’s also known as tag soup. To get started, use your terminal to install Beautiful Soup into your virtual environment: Then, import the library in your Python script and create a object: When you add the two highlighted lines of code, then you create a object that takes as input, which is the HTML content that you scraped earlier. Note: You’ll want to pass instead of to avoid problems with character encoding. The attribute holds raw bytes, which Python’s built-in HTML parser can decode better than the text representation you printed earlier using the attribute. The second argument that you pass to the class constructor, , makes sure that you use an appropriate parser for HTML content. At this point, you’re set up with a object that you named . You can now run your script using Python’s interactive mode: When you use the command-option to run a script, then Python executes the code and drops you into a REPL environment. This can be a good way to continue exploring the scraped HTML through the user-friendly lens of Beautiful Soup. In an HTML web page, every element can have an attribute assigned. As the name already suggests, that attribute makes the element uniquely identifiable on the page. You can begin to parse your page by selecting a specific element by its ID. Switch back to developer tools and identify the HTML object that contains all the job postings. Explore by hovering over parts of the page and using right-click to Inspect. Note: It helps to periodically switch back to your browser and explore the page interactively using developer tools. You’ll get a better idea of where and how to find the exact elements that you’re looking for. In this case, the element that you’re looking for is a with an attribute that has the value . It has some other attributes as well, but below is the gist of what you’re looking for: Beautiful Soup allows you to find that specific HTML element by its ID: For easier viewing, you can prettify any object when you print it out. If you call on the variable that you assigned above, then you’ll see all the HTML contained within the neatly structured: When you find an element by its ID, you can pick out one specific element from among the rest of the HTML, no matter how large the source code of the website is. Now you can focus on working with only this part of the page’s HTML. It looks like your soup just got a little thinner! Nevertheless, it’s still quite dense. You’ve seen that every job posting is wrapped in a element with the class . Now you can work with your new object called and select only the job postings in it. These are, after all, the parts of the HTML that you’re interested in! You can pick out all job cards in a single line of code: Here, you call on , which is a object. It returns an iterable containing all the HTML for all the job listings displayed on that page. Take a look at all of them: That’s pretty neat already, but there’s still a lot of HTML! You saw earlier that your page has descriptive class names on some elements. You can pick out those child elements from each job posting with : Each is another object. Therefore, you can use the same methods on it as you did on its parent element, . With this code snippet, you’re getting closer and closer to the data that you’re actually interested in. Still, there’s a lot going on with all those HTML tags and attributes floating around: Next, you’ll learn how to narrow down this output to access only the text content that you’re interested in. You only want to see the title, company, and location of each job posting. And behold! Beautiful Soup has got you covered. You can add to a object to return only the text content of the HTML elements that the object contains: Run the above code snippet, and you’ll see the text of each element displayed. However, you’ll also get some extra whitespace. But no worries, because you’re working with Python strings so you can the superfluous whitespace. You can also apply any other familiar Python string methods to further clean up your text: The results finally look much better! You’ve now got a readable list of jobs, associated company names, and each job’s location. However, you’re specifically looking for a position as a software developer, and these results contain job postings in many other fields as well. Find Elements by Class Name and Text Content Not all of the job listings are developer jobs. Instead of printing out all the jobs listed on the website, you’ll first filter them using keywords. You know that job titles in the page are kept within elements. To filter for only specific jobs, you can use the argument: This code finds all elements where the contained string matches exactly. Note that you’re directly calling the method on your first variable. If you go ahead and the output of the above code snippet to your console, then you might be disappointed because it’ll be empty: There was a Python job in the search results, so why isn’t it showing up? When you use as you did above, your program looks for that string exactly. Any variations in the spelling, capitalization, or whitespace will prevent the element from matching. In the next section, you’ll find a way to make your search string more general. In addition to strings, you can sometimes pass functions as arguments to Beautiful Soup methods. You can change the previous line of code to use a function instead: Now you’re passing an anonymous function to the argument. The lambda function looks at the text of each element, converts it to lowercase, and checks whether the substring is found anywhere. You can check whether you managed to identify all the Python jobs with this approach: Your program has found ten matching job posts that include the word in their job title! Finding elements based on their text content is a powerful way to filter your HTML response for specific information. Beautiful Soup allows you to use exact strings or functions as arguments for filtering text in objects. However, when you try to print the information of the filtered Python jobs like you’ve done before, you run into an error: This traceback message is a common error that you’ll run into a lot when you’re scraping information from the internet. Inspect the HTML of an element in your list. What does it look like? Where do you think the error is coming from? When you look at a single element in , you’ll see that it consists of only the element that contains the job title: When you revisit the code you used to select the items, you’ll notice that’s what you targeted. You filtered for only the title elements of the job postings that contain the word . As you can see, these elements don’t include the rest of the information about the job. The error message you received earlier was related to this: You tried to find the job title, the company name, and the job’s location in each element in , but each element contains only the job title text. Your diligent parsing library still looks for the other ones, too, and returns because it can’t find them. Then, fails with the shown error message when you try to extract the attribute from one of these objects. The text you’re looking for is nested in sibling elements of the elements that your filter returns. Beautiful Soup can help you select sibling, child, and parent elements of each object. One way to get access to all the information for a job is to step up in the hierarchy of the DOM starting from the elements that you identified. Take another look at the HTML of a single job posting, for example, using your developer tools. Then, find the element that contains the job title and its closest parent element that contains the information you’re interested in: The element with the class contains all the information you want. It’s a third-level parent of the title element that you found using your filter. With this information in mind, you can now use the elements in and fetch their great-grandparent elements to get access to all the information you want: You added a list comprehension that operates on each of the title elements in that you got by filtering with the lambda expression. You’re selecting the parent element of the parent element of the parent element of each title element. That’s three generations up! When you were looking at the HTML of a single job posting, you identified that this specific parent element with the class name contains all the information you need. Now you can adapt the code in your loop to iterate over the parent elements instead: When you run your script another time, you’ll see that your code once again has access to all the relevant information. That’s because you’re now looping over the elements instead of just the title elements. Using the attribute that each object comes with gives you an intuitive way to step through your DOM structure and address the elements you need. You can also access child elements and sibling elements in a similar manner. Read up on navigating the tree for more information. At this point, you’ve already written code that scrapes the site and filters its HTML for relevant job postings. Well done! However, what’s still missing is fetching the link to apply for a job. While inspecting the page, you found two links at the bottom of each card. If you use on the link elements in the same way you did for the other elements, then you won’t get the URLs that you’re interested in: If you execute the code shown above, then you’ll get the link text for and instead of the associated URLs. That’s because the attribute leaves only the visible content of an HTML element. It strips away all HTML tags, including the HTML attributes containing the URL, and leaves you with just the link text. To get the URL instead, you need to extract the value of one of the HTML attributes instead of discarding it. The URL of a link element is associated with the HTML attribute. The specific URL that you’re looking for is the value of the attribute of the second tag at the bottom of the HTML for a single job posting: Start by fetching all the elements in a job card. Then, extract the value of their attributes using square-bracket notation: In this code snippet, you first fetch all the links from each of the filtered job postings. Then, you extract the attribute, which contains the URL, using and print it to your console. Each job card has two links associated with it. However, you’re only looking for the second link, so you’ll apply a small edit to the code: In the updated code snippet, you use indexing to pick the second link element from the results of using its index ( ). Then, you directly extract the URL using the square-bracket notation with the key, thereby fetching the value of the attribute. You can use the same square-bracket notation to extract other HTML attributes as well."
    },
    {
        "link": "https://stackoverflow.com/questions/48071598/parsing-through-html-with-beautifulsoup-in-python",
        "document": "Currently my code is as follows:\n\nAs of now, I get a list containing the correct information but its still inside of HTML tags. An example of an element of the list is given below:\n\nfrom this item I want to extract both the href link and also the following string separately, but I am having trouble doing this and I really don't think getting this info should require a whole new set of operations. How do?"
    },
    {
        "link": "https://crummy.com/software/BeautifulSoup/bs4/doc",
        "document": "Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. But you'll only ever have to deal with about four kinds of objects: , , , and . These objects represent the HTML elements that comprise the page. A object corresponds to an XML or HTML tag in the original document. Tags have a lot of attributes and methods, and I'll cover most of them in Navigating the tree and Searching the tree. For now, the most important methods of a tag are for accessing its name and attributes. If you change a tag's name, the change will be reflected in any markup generated by Beautiful Soup down the line: An HTML or XML tag may have any number of attributes. The tag has an attribute \"id\" whose value is \"boldest\". You can access a tag's attributes by treating the tag like a dictionary: You can access the dictionary of attributes directly as : You can add, remove, and modify a tag's attributes. Again, this is done by treating the tag as a dictionary: HTML 4 defines a few attributes that can have multiple values. HTML 5 removes a couple of them, but defines a few more. The most common multi-valued attribute is (that is, a tag can have more than one CSS class). Others include , , , , and . By default, Beautiful Soup stores the value(s) of a multi-valued attribute as a list: When you turn a tag back into a string, the values of any multi-valued attributes are consolidated: If an attribute looks like it has more than one value, but it's not a multi-valued attribute as defined by any version of the HTML standard, Beautiful Soup stores it as a simple string: You can force all attributes to be stored as strings by passing as a keyword argument into the constructor: You can use to always return the value in a list container, whether it's a string or multi-valued attribute value: If you parse a document as XML, there are no multi-valued attributes: Again, you can configure this using the argument: You probably won't need to do this, but if you do, use the defaults as a guide. They implement the rules described in the HTML specification: A tag can contain strings as pieces of text. Beautiful Soup uses the class to contain these pieces of text: A is just like a Python Unicode string, except that it also supports some of the features described in Navigating the tree and Searching the tree. You can convert a to a Unicode string with : You can't edit a string in place, but you can replace one string with another, using replace_with(): supports most of the features described in Navigating the tree and Searching the tree, but not all of them. In particular, since a string can't contain anything (the way a tag may contain a string or another tag), strings don't support the or attributes, or the method. If you want to use a outside of Beautiful Soup, you should call on it to turn it into a normal Python Unicode string. If you don't, your string will carry around a reference to the entire Beautiful Soup parse tree, even when you're done using Beautiful Soup. This is a big waste of memory. The object represents the parsed document as a whole. For most purposes, you can treat it as a object. This means it supports most of the methods described in Navigating the tree and Searching the tree. You can also pass a object into one of the methods defined in Modifying the tree, just as you would a . This lets you do things like combine two parsed documents: Since the object doesn't correspond to an actual HTML or XML tag, it has no name and no attributes. But sometimes it's useful to reference its (such as when writing code that works with both and objects), so it's been given the special \"[document]\": , , and cover almost everything you'll see in an HTML or XML file, but there are a few leftover bits. The main one you'll probably encounter is the . \"<b><!--Hey, buddy. Want to buy a used parser?--></b>\" The object is just a special type of : # 'Hey, buddy. Want to buy a used parser' But when it appears as part of an HTML document, a is displayed with special formatting: # <!--Hey, buddy. Want to buy a used parser?--> Beautiful Soup defines a few subclasses to contain strings found inside specific HTML tags. This makes it easier to pick out the main body of the page, by ignoring strings that probably represent programming directives found within the page. (These classes are new in Beautiful Soup 4.9.0, and the html5lib parser doesn't use them.) A subclass that represents embedded CSS stylesheets; that is, any strings found inside a tag during document parsing. A subclass that represents embedded Javascript; that is, any strings found inside a tag during document parsing. A subclass that represents embedded HTML templates; that is, any strings found inside a tag during document parsing. Beautiful Soup defines some classes for holding special types of strings that can be found in XML documents. Like , these classes are subclasses of that add something extra to the string on output. A subclass representing the declaration at the beginning of an XML document. A subclass representing the document type declaration which may be found near the beginning of an XML document. A subclass that represents the contents of an XML processing instruction.\n\nHere's the \"Three sisters\" HTML document again: <p class=\"story\">Once upon a time there were three little sisters; and their names were and they lived at the bottom of a well.</p> I'll use this as an example to show you how to move from one part of a document to another. Tags may contain strings and more tags. These elements are the tag's children. Beautiful Soup provides a lot of different attributes for navigating and iterating over a tag's children. Note that Beautiful Soup strings don't support any of these attributes, because a string can't have children. The simplest way to navigate the parse tree is to find a tag by name. To do this, you can use the method: For convenience, just saying the name of the tag you want is equivalent to (if no built-in attribute has that name). If you want the <head> tag, just say : You can use this trick again and again to zoom in on a certain part of the parse tree. This code gets the first <b> tag beneath the <body> tag: (and its convenience equivalent) gives you only the first tag by that name: If you need to get all the <a> tags, you can use : For more complicated tasks, such as pattern-matching and filtering, you can use the methods described in Searching the tree. A tag's children are available in a list called : The object itself has children. In this case, the <html> tag is the child of the object.: A string does not have , because it can't contain anything: Instead of getting them as a list, you can iterate over a tag's children using the generator: If you want to modify a tag's children, use the methods described in Modifying the tree. Don't modify the the list directly: that can lead to problems that are subtle and difficult to spot. The and attributes consider only a tag's direct children. For instance, the <head> tag has a single direct child—the <title> tag: But the <title> tag itself has a child: the string \"The Dormouse's story\". There's a sense in which that string is also a child of the <head> tag. The attribute lets you iterate over all of a tag's children, recursively: its direct children, the children of its direct children, and so on: The <head> tag has only one child, but it has two descendants: the <title> tag and the <title> tag's child. The object only has one direct child (the <html> tag), but it has a whole lot of descendants: If a tag has only one child, and that child is a , the child is made available as : If a tag's only child is another tag, and that tag has a , then the parent tag is considered to have the same as its child: If a tag contains more than one thing, then it's not clear what should refer to, so is defined to be : If there's more than one thing inside a tag, you can still look at just the strings. Use the generator to see all descendant strings: # 'Once upon a time there were three little sisters; and their names were\n\n' # ';\n\nand they lived at the bottom of a well.' Newlines and spaces that separate tags are also strings. You can remove extra whitespace by using the generator instead: # 'Once upon a time there were three little sisters; and their names were' # ';\n\n and they lived at the bottom of a well.' Here, strings consisting entirely of whitespace are ignored, and whitespace at the beginning and end of strings is removed. Continuing the \"family tree\" analogy, every tag and every string has a parent: the tag that contains it. You can access an element's parent with the attribute. In the example \"three sisters\" document, the <head> tag is the parent of the <title> tag: The title string itself has a parent: the <title> tag that contains it: The parent of a top-level tag like <html> is the object itself: And the of a object is defined as None: You can iterate over all of an element's parents with . This example uses to travel from an <a> tag buried deep within the document, to the very top of the document: The generator is a variant of which gives you the entire ancestry of an element, including the element itself: Consider a simple document like this: The <b> tag and the <c> tag are at the same level: they're both direct children of the same tag. We call them siblings. When a document is pretty-printed, siblings show up at the same indentation level. You can also use this relationship in the code you write. You can use and to navigate between page elements that are on the same level of the parse tree: The <b> tag has a , but no , because there's nothing before the <b> tag on the same level of the tree. For the same reason, the <c> tag has a but no : The strings \"text1\" and \"text2\" are not siblings, because they don't have the same parent: In real documents, the or of a tag will usually be a string containing whitespace. Going back to the \"three sisters\" document: You might think that the of the first <a> tag would be the second <a> tag. But actually, it's a string: the comma and newline that separate the first <a> tag from the second: The second <a> tag is then the of the comma string: You can iterate over a tag's siblings with or : # '; and they lived at the bottom of a well.' # 'Once upon a time there were three little sisters; and their names were\n\n' Take a look at the beginning of the \"three sisters\" document: An HTML parser takes this string of characters and turns it into a series of events: \"open an <html> tag\", \"open a <head> tag\", \"open a <title> tag\", \"add a string\", \"close the <title> tag\", \"open a <p> tag\", and so on. The order in which the opening tags and strings are encountered is called document order. Beautiful Soup offers tools for searching a document's elements in document order. The attribute of a string or tag points to whatever was parsed immediately after the opening of the current tag or after the current string. It might be the same as , but it's usually drastically different. Here's the final <a> tag in the \"three sisters\" document. Its is a string: the conclusion of the sentence that was interrupted by the start of the <a> tag: # ';\n\nand they lived at the bottom of a well.' But the of that <a> tag, the thing that was parsed immediately after the <a> tag, is not the rest of that sentence: it's the string \"Tillie\" inside it: That's because in the original markup, the word \"Tillie\" appeared before that semicolon. The parser encountered an <a> tag, then the word \"Tillie\", then the closing </a> tag, then the semicolon and rest of the sentence. The semicolon is on the same level as the <a> tag, but the word \"Tillie\" was encountered first. The attribute is the exact opposite of . It points to the opening tag or string that was parsed immediately before this one: You should get the idea by now. You can use these iterators to move forward or backward in the document as it was parsed: # ';\n\nand they lived at the bottom of a well.'\n\nBeautiful Soup's main strength is in searching the parse tree, but you can also modify the tree and write your changes as a new HTML or XML document. I covered this earlier, in , but it bears repeating. You can rename a tag, change the values of its attributes, add new attributes, and delete attributes: If you set a tag's attribute to a new string, the tag's contents are replaced with that string: Be careful: if the tag contained other tags, they and all their contents will be destroyed. You can add to a tag's contents with . It works just like calling on a Python list: Starting in Beautiful Soup 4.7.0, also supports a method called , which adds every element of a list to a , in order: If you need to add a string to a document, no problem—you can pass a Python string in to , or you can call the constructor: If you want to create a comment or some other subclass of , just call the constructor: # ['Hello', ' there', 'Nice to see you.'] What if you need to create a whole new tag? The best solution is to call the factory method : Only the first argument, the tag name, is required. Because insertion methods return the newly inserted element, you can create, insert, and obtain an element in one step: is just like , except the new element doesn't necessarily go at the end of its parent's . It will be inserted at whatever numeric position you say, similar to on a Python list: # <a href=\"http://example.com/\">I linked to but did not endorse <i>example.com</i></a> # ['I linked to ', 'but did not endorse ', <i>example.com</i>] You can pass more than one element into . All the elements will be inserted, starting at the numeric position you provide. The method inserts tags or strings immediately before something else in the parse tree: The method inserts tags or strings immediately after something else in the parse tree: Both methods return the list of newly inserted elements. removes a tag or string from the tree. It returns the tag or string that was extracted: At this point you effectively have two parse trees: one rooted at the object you used to parse the document, and one rooted at the tag that was extracted. You can go on to call on a child of the element you extracted: removes a tag from the tree, then completely destroys it and its contents: The behavior of a decomposed or is not defined and you should not use it for anything. If you're not sure whether something has been decomposed, you can check its property (new in Beautiful Soup 4.9.0): extracts a tag or string from the tree, then replaces it with one or more tags or strings of your choice: returns the tag or string that got replaced, so that you can examine it or add it back to another part of the tree. The ability to pass multiple arguments into replace_with() is new in Beautiful Soup 4.10.0. wraps an element in the object you specify. It returns the new wrapper: This method is new in Beautiful Soup 4.0.5. is the opposite of . It replaces a tag with whatever's inside that tag. It's good for stripping out markup: Like , returns the tag that was replaced. After calling a bunch of methods that modify the parse tree, you may end up with two or more objects next to each other. Beautiful Soup doesn't have any problems with this, but since it can't happen in a freshly parsed document, you might not expect behavior like the following: You can call to clean up the parse tree by consolidating adjacent strings: This method is new in Beautiful Soup 4.8.0.\n\nAny HTML or XML document is written in a specific encoding like ASCII or UTF-8. But when you load that document into Beautiful Soup, you'll discover it's been converted to Unicode: It's not magic. (That sure would be nice.) Beautiful Soup uses a sub-library called Unicode, Dammit to detect a document's encoding and convert it to Unicode. The autodetected encoding is available as the attribute of the object: If is , that means the document was already Unicode when it was passed into Beautiful Soup: Unicode, Dammit guesses correctly most of the time, but sometimes it makes mistakes. Sometimes it guesses correctly, but only after a byte-by-byte search of the document that takes a very long time. If you happen to know a document's encoding ahead of time, you can avoid mistakes and delays by passing it to the constructor as . Here's a document written in ISO-8859-8. The document is so short that Unicode, Dammit can't get a lock on it, and misidentifies it as ISO-8859-7: We can fix this by passing in the correct : If you don't know what the correct encoding is, but you know that Unicode, Dammit is guessing wrong, you can pass the wrong guesses in as : Windows-1255 isn't 100% correct, but that encoding is a compatible superset of ISO-8859-8, so it's close enough. ( is a new feature in Beautiful Soup 4.4.0.) In rare cases (usually when a UTF-8 document contains text written in a completely different encoding), the only way to get Unicode may be to replace some characters with the special Unicode character \"REPLACEMENT CHARACTER\" (U+FFFD, �). If Unicode, Dammit needs to do this, it will set the attribute to on the or object. This lets you know that the Unicode representation is not an exact representation of the original—some data was lost. If a document contains �, but is , you'll know that the � was there originally (as it is in this paragraph) and doesn't stand in for missing data. When you write out an output document from Beautiful Soup, you get a UTF-8 document, even if the input document wasn't in UTF-8 to begin with. Here's a document written in the Latin-1 encoding: Note that the <meta> tag has been rewritten to reflect the fact that the document is now in UTF-8. If you don't want UTF-8, you can pass an encoding into : You can also call encode() on the object, or any element in the soup, just as if it were a Python string: Any characters that can't be represented in your chosen encoding will be converted into numeric XML entity references. Here's a document that includes the Unicode character SNOWMAN: The SNOWMAN character can be part of a UTF-8 document (it looks like ☃), but there's no representation for that character in ISO-Latin-1 or ASCII, so it's converted into \"☃\" for those encodings: You can use Unicode, Dammit without using Beautiful Soup. It's useful whenever you have data in an unknown encoding and you just want it to become Unicode: Unicode, Dammit's guesses will get a lot more accurate if you install one of these Python libraries: , , or . The more data you give Unicode, Dammit, the more accurately it will guess. If you have your own suspicions as to what the encoding might be, you can pass them in as a list: Unicode, Dammit has two special features that Beautiful Soup doesn't use. You can use Unicode, Dammit to convert Microsoft smart quotes to HTML or XML entities: You can also convert Microsoft smart quotes to ASCII quotes: Hopefully you'll find this feature useful, but Beautiful Soup doesn't use it. Beautiful Soup prefers the default behavior, which is to convert Microsoft smart quotes to Unicode characters along with everything else: Sometimes a document is mostly in UTF-8, but contains Windows-1252 characters such as (again) Microsoft smart quotes. This can happen when a website includes data from multiple sources. You can use to turn such a document into pure UTF-8. Here's a simple example: This document is a mess. The snowmen are in UTF-8 and the quotes are in Windows-1252. You can display the snowmen or the quotes, but not both: Decoding the document as UTF-8 raises a , and decoding it as Windows-1252 gives you gibberish. Fortunately, will convert the string to pure UTF-8, allowing you to decode it to Unicode and display the snowmen and quote marks simultaneously: only knows how to handle Windows-1252 embedded in UTF-8 (or vice versa, I suppose), but this is the most common case. Note that you must know to call on your data before passing it into or the constructor. Beautiful Soup assumes that a document has a single encoding, whatever it might be. If you pass it a document that contains both UTF-8 and Windows-1252, it's likely to think the whole document is Windows-1252, and the document will come out looking like . is new in Beautiful Soup 4.1.0.\n\nIf you're having trouble understanding what Beautiful Soup does to a document, pass the document into the function. (This function is new in Beautiful Soup 4.2.0.) Beautiful Soup will print out a report showing you how different parsers handle the document, and tell you if you're missing a parser that Beautiful Soup could be using: # I noticed that html5lib is not installed. Installing it may help. # Trying to parse your data with html.parser # Here's what html.parser did with the document: Just looking at the output of diagnose() might show you how to solve the problem. Even if not, you can paste the output of when asking for help. There are two different kinds of parse errors. There are crashes, where you feed a document to Beautiful Soup and it raises an exception (usually an ). And there is unexpected behavior, where a Beautiful Soup parse tree looks a lot different than the document used to create it. These problems are almost never problems with Beautiful Soup itself. This is not because Beautiful Soup is an amazingly well-written piece of software. It's because Beautiful Soup doesn't include any parsing code. Instead, it relies on external parsers. If one parser isn't working on a certain document, the best solution is to try a different parser. See Installing a parser for details and a parser comparison. If this doesn't help, you might need to inspect the document tree found inside the object, to see where the markup you're looking for actually ended up.\n• None (on the line ): Caused by running an old Python 2 version of Beautiful Soup under Python 3, without converting the code.\n• None - Caused by running an old Python 2 version of Beautiful Soup under Python 3.\n• None - Caused by running the Python 3 version of Beautiful Soup under Python 2.\n• None - Caused by running Beautiful Soup 3 code in an environment that doesn't have BS3 installed. Or, by writing Beautiful Soup 4 code without knowing that the package name has changed to .\n• None - Caused by running Beautiful Soup 4 code in an environment that doesn't have BS4 installed. By default, Beautiful Soup parses documents as HTML. To parse a document as XML, pass in \"xml\" as the second argument to the constructor: You'll need to have lxml installed.\n• None If your script works on one computer but not another, or in one virtual environment but not another, or outside the virtual environment but not inside, it's probably because the two environments have different parser libraries available. For example, you may have developed the script on a computer that has lxml installed, and then tried to run it on a computer that only has html5lib installed. See Differences between parsers for why this matters, and fix the problem by mentioning a specific parser library in the constructor.\n• None Because HTML tags and attributes are case-insensitive, all three HTML parsers convert tag and attribute names to lowercase. That is, the markup <TAG></TAG> is converted to <tag></tag>. If you want to preserve mixed-case or uppercase tags and attributes, you'll need to parse the document as XML.\n• None (or just about any other ) - This problem shows up in two main situations. First, when you try to print a Unicode character that your console doesn't know how to display. (See this page on the Python wiki for help.) Second, when you're writing to a file and you pass in a Unicode character that's not supported by your default encoding. In this case, the simplest solution is to explicitly encode the Unicode string into UTF-8 with .\n• None - Caused by accessing when the tag in question doesn't define the attribute. The most common errors are and . Use if you're not sure is defined, just as you would with a Python dictionary.\n• None - This usually happens because you expected to return a single tag or string. But returns a list of tags and strings—a object. You need to iterate over the list and look at the of each one. Or, if you really only want one result, you need to use instead of .\n• None - This usually happens because you called and then tried to access the attribute of the result. But in your case, didn't find anything, so it returned , instead of returning a tag or a string. You need to figure out why your call isn't returning anything.\n• None - This usually happens because you're treating a string as though it were a tag. You may be iterating over a list, expecting that it contains nothing but tags, when it actually contains both tags and strings. Beautiful Soup will never be as fast as the parsers it sits on top of. If response time is critical, if you're paying for computer time by the hour, or if there's any other reason why computer time is more valuable than programmer time, you should forget about Beautiful Soup and work directly atop lxml. That said, there are things you can do to speed up Beautiful Soup. If you're not using lxml as the underlying parser, my advice is to start. Beautiful Soup parses documents significantly faster using lxml than using html.parser or html5lib. You can speed up encoding detection significantly by installing the cchardet library. Parsing only part of a document won't save you much time parsing the document, but it can save a lot of memory, and it'll make searching the document much faster.\n\nBeautiful Soup 3 is the previous release series, and is no longer supported. Development of Beautiful Soup 3 stopped in 2012, and the package was completely discontinued in 2021. There's no reason to install it unless you're trying to get very old software to work, but it's published through PyPi as : You can also download a tarball of the final release, 3.2.2. If you ran or , but your code doesn't work, you installed Beautiful Soup 3 by mistake. You need to run . The documentation for Beautiful Soup 3 is archived online. Most code written against Beautiful Soup 3 will work against Beautiful Soup 4 with one simple change. All you should have to do is change the package name from to . So this:\n• None If you get the \"No module named BeautifulSoup\", your problem is that you're trying to run Beautiful Soup 3 code, but you only have Beautiful Soup 4 installed.\n• None If you get the \"No module named bs4\", your problem is that you're trying to run Beautiful Soup 4 code, but you only have Beautiful Soup 3 installed. Although BS4 is mostly backward-compatible with BS3, most of its methods have been deprecated and given new names for PEP 8 compliance. There are numerous other renames and changes, and a few of them break backward compatibility. Here's what you'll need to know to convert your BS3 code and habits to BS4: Beautiful Soup 3 used Python's , a module that was deprecated and removed in Python 3.0. Beautiful Soup 4 uses by default, but you can plug in lxml or html5lib and use that instead. See Installing a parser for a comparison. Since is not the same parser as , you may find that Beautiful Soup 4 gives you a different parse tree than Beautiful Soup 3 for the same markup. If you swap out for lxml or html5lib, you may find that the parse tree changes yet again. If this happens, you'll need to update your scraping code to process the new tree. I renamed three attributes to avoid using words that have special meaning to Python. Unlike my changes to method names (which you'll see in the form of deprecation warnings), these changes did not preserve backwards compatibility. If you used these attributes in BS3, your code will break in BS4 until you change them. Some of the generators used to yield after they were done, and then stop. That was a bug. Now the generators just stop. There is no longer a class for parsing XML. To parse XML you pass in \"xml\" as the second argument to the constructor. For the same reason, the constructor no longer recognizes the argument. Beautiful Soup's handling of empty-element XML tags has been improved. Previously when you parsed XML you had to explicitly say which tags were considered empty-element tags. The argument to the constructor is no longer recognized. Instead, Beautiful Soup considers any empty tag to be an empty-element tag. If you add a child to an empty-element tag, it stops being an empty-element tag. An incoming HTML or XML entity is always converted into the corresponding Unicode character. Beautiful Soup 3 had a number of overlapping ways of dealing with entities, which have been removed. The constructor no longer recognizes the or arguments. (Unicode, Dammit still has , but its default is now to turn smart quotes into Unicode.) The constants , , and have been removed, since they configure a feature (transforming some but not all entities into Unicode characters) that no longer exists. If you want to turn Unicode characters back into HTML entities on output, rather than turning them into UTF-8 characters, you need to use an output formatter. Tag.string now operates recursively. If tag A contains a single tag B and nothing else, then A.string is the same as B.string. (Previously, it was None.) Multi-valued attributes like have lists of strings as their values, not simple strings. This may affect the way you search by CSS class. objects now implement the method, such that two objects are considered equal if they generate the same markup. This may change your script's behavior if you put objects into a dictionary or set. If you pass one of the methods both string and a tag-specific argument like name, Beautiful Soup will search for tags that match your tag-specific criteria and whose Tag.string matches your string value. It will not find the strings themselves. Previously, Beautiful Soup ignored the tag-specific arguments and looked for strings. The constructor no longer recognizes the argument. It's now the parser's responsibility to handle markup correctly. The rarely-used alternate parser classes like and have been removed. It's now the parser's decision how to handle ambiguous markup. The method now returns a Unicode string, not a bytestring."
    },
    {
        "link": "https://stackoverflow.com/questions/21570780/using-python-and-beautifulsoup-saved-webpage-source-codes-into-a-local-file",
        "document": "I am trying to use Python and BeautifulSoup to pick up information on a webpage. Because the webpage is in the company website and requires login and redirection, I copied the target page's source code page into a file and saved it as “example.html” in C:\\ for the convenience of practicing.\n\nThis is a part of the original code:\n\nThe code I worked out so far is:\n\nThis is just the first stage of testing, so it's somewhat incomplete.\n\nHowever, when I run it, it gives an error message. Seems it’s improper to use to open a local file.\n\nHow can I practice using a local file?"
    }
]