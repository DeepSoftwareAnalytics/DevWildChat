[
    {
        "link": "https://sciencedirect.com/science/article/pii/S131915782300006X",
        "document": ""
    },
    {
        "link": "https://restack.io/p/natural-language-understanding-answer-nlp-flowchart-cat-ai",
        "document": "Explore the NLP flowchart for Natural Language Understanding, detailing processes and methodologies in a concise format.\n\nArtificial Intelligence and Natural Language Processing and Understanding in Space: A Methodological Framework and Four ESA Case Studies Mapping Use Cases to NLP Tasks in Space Applications In the realm of space applications, Natural Language Processing (NLP) technologies are increasingly being utilized to enhance the efficiency and effectiveness of various tasks. This section delves into the specific use cases where NLP can be applied, particularly in the context of space mission documentation and data management. NLP technologies can be leveraged in several key areas:\n• Information Extraction: Extracting relevant information from extensive scientific documents is crucial. NLP can automate the identification of key data points, enabling researchers to focus on analysis rather than data gathering.\n• Semantic Metadata Generation: By producing semantic metadata, NLP facilitates the development of sophisticated information retrieval systems. This aligns with the FAIR principles, ensuring that research data is Findable, Accessible, Interoperable, and Reusable.\n• Automated Question Answering: Systems can be designed to automatically answer queries related to space mission documents, such as ESA’s Concurrent Design Facility reports. This capability enhances accessibility to critical information. To visualize the integration of NLP in space applications, consider the following flowchart: This flowchart illustrates how NLP transforms unstructured data into actionable insights, streamlining workflows in space missions. Recent implementations of NLP at ESA demonstrate its potential:\n• Open Space Innovation Platform (OSIP): NLP assists in evaluating the innovation potential of submitted ideas, ensuring that promising concepts are identified and developed.\n• Quality Assurance Procedures: By automating the analysis of quality management documents, NLP contributes to maintaining high standards in space operations. Looking ahead, the evolution of NLP technologies in space is promising. With advancements in transformer models and domain-specific training, we anticipate:\n• Enhanced Language Models: New models tailored for space data will improve comprehension and reasoning capabilities.\n• Collaborative AI Systems: Future AI systems will not only assist but also engage in complex problem-solving alongside human experts, potentially leading to groundbreaking discoveries. In conclusion, the integration of NLP in space applications is set to revolutionize how information is processed and utilized, paving the way for more efficient and innovative approaches to space exploration and research.\n• None Explore detailed NLP course outlines focusing on Natural Language Understanding concepts and applications.\n• None Explore key steps for achieving effective natural language understanding in your applications and systems.\n\nArtificial Intelligence and Natural Language Processing and Understanding in Space: A Methodological Framework and Four ESA Case Studies In the realm of Natural Language Processing (NLP) for space applications, understanding the workflow is crucial for effective implementation. The process begins with defining the use case, which involves identifying the problem to be addressed through language technologies. Key factors include the objectives, types of documents to process, expected outputs, and user interaction. This information is summarized in a use case name and description, as illustrated in Table 1. Once the use case is defined, the next step is to assess its technical feasibility by mapping it against known NLP tasks. Two main categories of NLP tasks are particularly relevant for space applications:\n• Information Extraction: This involves automatically extracting specified types of facts from unstructured documents, producing structured metadata. The metadata may include:\n• Main topics or domains related to the text The output from these tasks is often utilized in information retrieval systems, enhancing search and recommendation engines to provide more accurate access to data for space scientists and engineers. The data access layer is responsible for preprocessing tasks, which include cleaning and formatting text from various sources, such as PDF documents or web scraping. This step is essential for generating dense representations of text documents using language models. For instance, the use of models like BERT or RoBERTa can significantly improve the quality of data representation. In the domain logic layer, software components are orchestrated according to the specific functionalities required for the use case. This includes: An example of orchestration is generating questions using a language model like T5 and answering them with a RoBERTa model. The presentation layer encompasses user interface components necessary for the use case. Tools like Kibana dashboards are invaluable for visualizing results from information extraction tasks, allowing users to search and explore large document collections effectively. Additionally, frameworks such as Streamlit and Dash facilitate rapid app prototyping and demonstration. Provisioning the necessary resources identified during the analysis phase is critical. Resources can vary widely and include data, corpora, annotation datasets, software, and hardware. It is advisable to develop a testing set to evaluate the NLP components effectively. If resource provisioning is constrained by budget or time, the use case may need to be reconsidered. In summary, the NLP application development workflow for space projects involves a structured approach that integrates various layers of architecture, ensuring that each component works harmoniously to achieve the desired outcomes.\n• None Explore NLP tools for automated text analysis using Natural Language Understanding to enhance data insights and streamline processes."
    },
    {
        "link": "https://kdnuggets.com/3-easy-ways-create-flowcharts-diagrams-using-llms",
        "document": "Creating diagrams doesn’t have to be hard! With just a simple text description, LLMs can help you generate flowcharts and diagrams in no time.\n\nYou’ve probably already explored generating images with LLMs. But what about flowcharts and diagrams? These are essential for visualizing processes, workflows, and system architectures. Normally, drawing them manually on online tools can be time-consuming, but did you know that you can automate the process using LLMs with simple text descriptions? It saves you time and effort! In this article, we’ll look at three easy ways to create flowcharts and diagrams using LLMs, complete with practical examples and prompts you can use. So, let’s get started.\n\n\n\n Mermaid.js is a simple, JavaScript-based diagramming tool that allows you to create flowcharts, sequence diagrams, and more using a Markdown-like syntax.\n• graph TD / graph LR: Defines the diagram direction—TD (Top Down) or LR (Left to Right).\n• Describe Your Diagram: Start by providing a clear description of the diagram you want to create. For example: \"Create a flowchart for a user login process. The steps are: User enters credentials, system validates credentials, if valid, grant access; if invalid, show error message.”\n• Ask the LLM to Generate Mermaid Code: Request the LLM (like GPT) to convert your description into Mermaid syntax. For example: Prompt: \"Convert the following process into Mermaid flowchart syntax: [Your Description]\"\n• Render the Diagram: Use a Mermaid-compatible editor like:\n• VS Code with the Mermaid extension. Paste the generated Mermaid code into the editor to see the flowchart. Simple, right?\n\n \n\n \n\n\n\n\n\n PlantUML is widely used opensource tool for creating UML diagrams, including class diagrams, sequence diagrams, and activity diagrams.\n• @startuml / @enduml: Marks the start and end of the code.\n• actor: Defines an actor (a person or external system) in the diagram.\n• ->: Indicates the flow from one element to another (e.g., from an actor to a system).\n• Describe Your Diagram: Provide a detailed description of the diagram you want. For example: \"Create a sequence diagram for an online shopping process. The steps are: User adds item to cart, system updates cart, user checks out, system processes payment, and order is confirmed.\"\n• Ask the LLM to Generate PlantUML Code: Use the LLM to convert your description into PlantUML syntax. For example: Prompt: \"Convert the following process into PlantUML syntax: [Your Description]\" @startuml actor User participant \"Shopping System\" as System User -> System: Add Item to Cart System --> User: Cart Updated User -> System: Check Out System -> System: Process Payment System --> User: Order Confirmed @enduml\n• Render the Diagram: Use a PlantUML-compatible editor like:\n• VS Code with the PlantUML extension. Paste the generated PlantUML code into the editor to visualize the diagram.\n\n \n\n \n\n\n\n\n\n Graphviz is a powerful graph visualization software that uses the DOT language to describe graphs. It’s flexible and supports a wide range of graph types, including flowcharts, network diagrams, and hierarchical structures.\n• digraph: Indicates that this is a directed graph (i.e., edges have a direction).\n• [label=\"...\"]: Adds a label to an edge to specify the relationship between nodes.\n• Describe Your Diagram:Give a clear description of the diagram you want. For example: \"Create a flowchart for a decision-making process. The steps are: Start, analyze data, if data is valid, proceed; if invalid, retry.\"\n• Ask the LLM to Generate DOT Code: Use the LLM to convert your description into DOT syntax. For example: Prompt: \"Convert the following process into Graphviz DOT syntax: [Your Description]\"\n• Render the Diagram: Use a Graphviz-compatible editor or tool like:\n• VS Code with the Graphviz extension. Paste the generated DOT code into the editor to visualize the diagram.\n• Be Specific: The more detail you provide in your description, the more accurate the results will be.\n• Iterate: Don’t be afraid to tweak the generated code or structure based on feedback and adjust it as needed.\n• Combine Tools: Use different tools for different needs. For example, use Mermaid for quick drafts and Graphviz for more complex diagrams.\n• Automate: Consider integrating LLMs with scripts to automate diagram generation, especially for repetitive tasks.\n\nNo matter which tool you choose—Mermaid, PlantUML, or Graphviz DOT—these methods will make diagramming faster, easier, and more accessible. Experiment with these tools, find the workflow that works best for you.\n\n \n\n\n\nKanwal Mehreen Kanwal is a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook \"Maximizing Productivity with ChatGPT\". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields."
    },
    {
        "link": "https://lexalytics.com/blog/machine-learning-natural-language-processing",
        "document": "\n• \n• Background: Machine Learning in the Context of Natural Language Processing\n• ML vs NLP and Using Machine Learning on Natural Language Sentences\n\nBefore we dive deep into how to apply machine learning and AI for NLP and text analytics, let’s clarify some basic ideas.\n\nMost importantly, “machine learning” really means “machine teaching.” We know what the machine needs to learn, so our task is to create a learning framework and provide properly-formatted, relevant, clean data for the machine to learn from.\n\nWhen we talk about a “model,” we’re talking about a mathematical representation. Input is key. A machine learning model is the sum of the learning that has been acquired from its training data. The model changes as more learning is acquired.\n\nUnlike algorithmic programming, a machine learning model is able to generalize and deal with novel cases. If a case resembles something the model has seen before, the model can use this prior “learning” to evaluate the case. The goal is to create a system where the model continuously improves at the task you’ve set it.\n\nMachine learning for NLP and text analytics involves a set of statistical techniques for identifying parts of speech, entities, sentiment, and other aspects of text. The techniques can be expressed as a model that is then applied to other text, also known as supervised machine learning. It also could be a set of algorithms that work across large sets of data to extract meaning, which is known as unsupervised machine learning. It’s important to understand the difference between supervised and unsupervised learning, and how you can get the best of both in one system.\n\nText data requires a special approach to machine learning. This is because text data can have hundreds of thousands of dimensions (words and phrases) but tends to be very sparse. For example, the English language has around 100,000 words in common use. But any given tweet only contains a few dozen of them. This differs from something like video content where you have very high dimensionality, but you have oodles and oodles of data to work with, so, it’s not quite as sparse.\n\nIn supervised machine learning, a batch of text documents are tagged or annotated with examples of what the machine should look for and how it should interpret that aspect. These documents are used to “train” a statistical model, which is then given un-tagged text to analyze.\n\nLater, you can use larger or better datasets to retrain the model as it learns more about the documents it analyzes. For example, you can use supervised learning to train a model to analyze movie reviews, and then later train it to factor in the reviewer’s star rating.\n\nThe most popular supervised NLP machine learning algorithms are:\n\nAll you really need to know if come across these terms is that they represent a set of data scientist guided machine learning algorithms.\n\nLexalytics uses supervised machine learning to build and improve our core text analytics functions and NLP features.\n\nTokenization involves breaking a text document into pieces that a machine can understand, such as words. Now, you’re probably pretty good at figuring out what’s a word and what’s gibberish. English is especially easy. See all this white space between the letters and paragraphs? That makes it really easy to tokenize. So, NLP rules are sufficient for English tokenization.\n\nBut how do you teach a machine learning algorithm what a word looks like? And what if you’re not working with English-language documents? Logographic languages like Mandarin Chinese have no whitespace.\n\nThis is where we use machine learning for tokenization. Chinese follows rules and patterns just like English, and we can train a machine learning model to identify and understand them.\n\nPart of Speech Tagging (PoS tagging) means identifying each token’s part of speech (noun, adverb, adjective, etc.) and then tagging it as such. PoS tagging forms the basis of a number of important Natural Language Processing tasks. We need to correctly identify Parts of Speech in order to recognize entities, extract themes, and to process sentiment. Lexalytics has a highly-robust model that can PoS tag with >90% accuracy, even for short, gnarly social media posts.\n\nAt their simplest, named entities are people, places, and things (products) mentioned in a text document. Unfortunately, entities can also be hashtags, emails, mailing addresses, phone numbers, and Twitter handles. In fact, just about anything can be an entity if you look at it the right way. And don’t get us stated on tangential references.\n\nAt Lexalytics, we’ve trained supervised machine learning models on large amounts pre-tagged entities. This approach helps us to optimize for accuracy and flexibility. We’ve also trained NLP algorithms to recognize non-standard entities (like species of tree, or types of cancer).\n\nIt’s also important to note that Named Entity Recognition models rely on accurate PoS tagging from those models.\n\nSentiment analysis is the process of determining whether a piece of writing is positive, negative or neutral, and then assigning a weighted sentiment score to each entity, theme, topic, and category within the document. This is an incredibly complex task that varies wildly with context. For example, take the phrase, “sick burn” In the context of video games, this might actually be a positive statement.\n\nCreating a set of NLP rules to account for every possible sentiment score for every possible word in every possible context would be impossible. But by training a machine learning model on pre-scored data, it can learn to understand what “sick burn” means in the context of video gaming, versus in the context of healthcare. Unsurprisingly, each language requires its own sentiment classification model.\n\n\n\n Categorization means sorting content into buckets to get a quick, high-level overview of what’s in the data. To train a text classification model, data scientists use pre-sorted content and gently shepherd their model until it’s reached the desired level of accuracy. The result is accurate, reliable categorization of text documents that takes far less time and energy than human analysis.\n\nUnsupervised machine learning involves training a model without pre-tagging or annotating. Some of these techniques are surprisingly easy to understand.\n\nClustering means grouping similar documents together into groups or sets. These clusters are then sorted based on importance and relevancy (hierarchical clustering).\n\nAnother type of unsupervised learning is Latent Semantic Indexing (LSI). This technique identifies on words and phrases that frequently occur with each other. Data scientists use LSI for faceted searches, or for returning search results that aren’t the exact search term.\n\nFor example, the terms “manifold” and “exhaust” are closely related documents that discuss internal combustion engines. So, when you Google “manifold” you get results that also contain “exhaust”.\n\nMatrix Factorization is another technique for unsupervised NLP machine learning. This uses “latent factors” to break a large matrix down into the combination of two smaller matrices. Latent factors are similarities between the items.\n\nThink about the sentence, “I threw the ball over the mountain.” The word “threw” is more likely to be associated with “ball” than with “mountain”.\n\nIn fact, humans have a natural ability to understand the factors that make something throwable. But a machine learning NLP algorithm must be taught this difference.\n\nUnsupervised learning is tricky, but far less labor- and data-intensive than its supervised counterpart. Lexalytics uses unsupervised learning algorithms to produce some “basic understanding” of how language works. We extract certain important patterns within large sets of text documents to help our models understand the most likely interpretation.\n\nThe Lexalytics Concept Matrix™ is, in a nutshell, unsupervised learning applied to the top articles on Wikipedia™. Using unsupervised machine learning, we built a web of semantic relationships between the articles. This web allows our text analytics and NLP to understand that “apple” is close to “fruit” and is close to “tree”, but is far away from “lion”, and that it is closer to “lion” than it is to “linear algebra.” Unsupervised learning, through the Concept Matrix™, forms the basis of our understanding of semantic information (remember our discussion above).\n\n\n\n Our Syntax Matrix™ is unsupervised matrix factorization applied to a massive corpus of content (many billions of sentences). The Syntax Matrix™ helps us understand the most likely parsing of a sentence – forming the base of our understanding of syntax (again, recall our discussion earlier in this article).\n\nNatural Language Processing broadly refers to the study and development of computer systems that can interpret speech and text as humans naturally speak and type it. Human communication is frustratingly vague at times; we all use colloquialisms, abbreviations, and don’t often bother to correct misspellings. These inconsistencies make computer analysis of natural language difficult at best. But in the last decade, both NLP techniques and machine learning algorithms have progressed immeasurably.\n\nThere are three aspects to any given chunk of text:\n\nSemantic information is the specific meaning of an individual word. A phrase like “the bat flew through the air” can have multiple meanings depending on the definition of bat: winged mammal, wooden stick, or something else entirely? Knowing the relevant definition is vital for understanding the meaning of a sentence.\n\nAnother example: “Billy hit the ball over the house.” As the reader, you may assume that the ball in question is a baseball, but how do you know? The ball could be a volleyball, a tennis ball, or even a bocce ball. We assume baseball because they are the type of balls most often “hit” in such a way, but without natural language machine learning a computer wouldn’t know to make the connection.\n\nThe second key component of text is sentence or phrase structure, known as syntax information. Take the sentence, “Sarah joined the group already with some search experience.” Who exactly has the search experience here? Sarah, or the group? Depending on how you read it, the sentence has very different meaning with respect to Sarah’s abilities.\n\n\n\n Finally, you must understand the context that a word, phrase, or sentence appears in. What is the concept being discussed? If a person says that something is “sick”, are they talking about healthcare or video games? The implication of “sick” is often positive when mentioned in a context of gaming, but almost always negative when discussing healthcare.\n\nML vs NLP and Using Machine Learning on Natural Language Sentences\n\nLet’s return to the sentence, “Billy hit the ball over the house.” Taken separately, the three types of information would return:\n• : person – act of striking an object with another object – spherical play\n• : this sentence is about a child playing with a ball\n\nThese aren’t very helpful by themselves. They indicate a vague idea of what the sentence is about, but full understanding requires the successful combination of all three components.\n\nThis analysis can be accomplished in a number of ways, through machine learning models or by inputting rules for a computer to follow when analyzing text. Alone, however, these methods don’t work so well.\n\nMachine learning models are great at recognizing entities and overall sentiment for a document, but they struggle to extract themes and topics, and they’re not very good at matching sentiment to individual entities or themes.\n\nAlternatively, you can teach your system to identify the basic rules and patterns of language. In many languages, a proper noun followed by the word “street” probably denotes a street name. Similarly, a number followed by a proper noun followed by the word “street” is probably a street address. And people’s names usually follow generalized two- or three-word formulas of proper nouns and nouns.\n\nUnfortunately, recording and implementing language rules takes a lot of time. What’s more, NLP rules can’t keep up with the evolution of language. The Internet has butchered traditional conventions of the English language. And no static NLP codebase can possibly encompass every inconsistency and meme-ified misspelling on social media.\n\nVery early text mining systems were entirely based on rules and patterns. Over time, as natural language processing and machine learning techniques have evolved, an increasing number of companies offer products that rely exclusively on machine learning. But as we just explained, both approaches have major drawbacks.\n\nThat’s why at Lexalytics, we utilize a hybrid approach. We’ve trained a range of supervised and unsupervised models that work in tandem with rules and patterns that we’ve been refining for over a decade.\n\nOur text analysis functions are based on patterns and rules. Each time we add a new language, we begin by coding in the patterns and rules that the language follows. Then our supervised and unsupervised machine learning models keep those rules in mind when developing their classifiers. We apply variations on this system for low-, mid-, and high-level text functions.\n\nLow-level text functions are the initial processes through which you run any text input. These functions are the first step in turning unstructured text into structured data. They form the base layer of information that our mid-level functions draw on. Mid-level text analytics functions involve extracting the real content of a document of text. This means who is speaking, what they are saying, and what they are talking about.\n\nThe high-level function of sentiment analysis is the last step, determining and applying sentiment on the entity, theme, and document levels.\n• Intentions: ML + Rules “What are you going to do?”\n• Intentions uses the syntax matrix to extract the intender, intendee, and intent\n• We use ML to train models for the different types of intent\n• We use rules to whitelist or blacklist certain words\n• Multilayered approach to get you the best accuracy\n• Apply Sentiment: ML + Rules “How do you feel about that?”\n\nYou can see how this system pans out in the chart below:"
    },
    {
        "link": "https://yunhefeng.me/material/GenFlowchart.pdf",
        "document": ""
    },
    {
        "link": "https://graphviz.org/documentation",
        "document": "Various algorithms for projecting abstract graphs into a space for visualization.\n\nVarious graphic and data formats for end user, web, documents and other applications.\n\nInstructions to customise the layout of Graphviz nodes, edges, graphs, subgraphs, and clusters.\n\nCatalogue of the schemas/types/grammars expected by attributes.\n\nAttributes you can set on graphs\n\nAttributes you can set on graph nodes\n\nAttributes you can set on subgraph clusters\n\nAttributes you can set on graph edges"
    },
    {
        "link": "https://graphviz.org/doc/info/lang.html",
        "document": "Terminals are shown in bold font and nonterminals in italics. Literal characters are given in single quotes. Parentheses and indicate grouping when needed. Square brackets and enclose optional items. Vertical bars separate alternatives.\n\nThe keywords node, edge, graph, digraph, subgraph, and strict are case-independent. Note also that the allowed compass point values are not keywords, so these strings can be used elsewhere as ordinary identifiers and, conversely, the parser will actually accept any identifier.\n\nAn ID is one of the following:\n• Any string of alphabetic ( ) characters, underscores ( ) or digits( ), not beginning with a digit;\n\nAn ID is just a string; the lack of quote characters in the first two forms is just for simplicity. There is no semantic difference between and , or between and . Obviously, to use a keyword as an ID, it must be quoted.\n\nNote that, in HTML strings, angle brackets must occur in matched pairs, and newlines and other formatting whitespace characters are allowed. In addition, the content must be legal XML, so that the special XML escape sequences for \", &, <, and > may be necessary in order to embed these characters in attribute values or raw text. As an ID, an HTML string can be any legal XML string. However, if used as a label attribute, it is interpreted specially and must follow the syntax for HTML-like labels.\n\nBoth quoted strings and HTML strings are scanned as a unit, so any embedded comments will be treated as part of the strings.\n\nAn edgeop is in directed graphs and in undirected graphs.\n\nThe language supports C++-style comments: and . In addition, a line beginning with a '#' character is considered a line output from a C preprocessor (e.g., # 34 to indicate line 34 ) and discarded.\n\nSemicolons and commas aid readability but are not required. Also, any amount of whitespace may be inserted between terminals.\n\nAs another aid for readability, dot allows double-quoted strings to span multiple physical lines using the standard C convention of a backslash immediately preceding a newline character². In addition, double-quoted strings can be concatenated using a '+' operator. As HTML strings can contain newline characters, which are used solely for formatting, the language does not allow escaped newlines or concatenation operators to be used within them.\n\nSubgraphs play three roles in Graphviz. First, a subgraph can be used to represent graph structure, indicating that certain nodes and edges should be grouped together. This is the usual role for subgraphs and typically specifies semantic information about the graph components. It can also provide a convenient shorthand for edges. An edge statement allows a subgraph on both the left and right sides of the edge operator. When this occurs, an edge is created from every node on the left to every node on the right. For example, the specification\n\nIn the second role, a subgraph can provide a context for setting attributes. For example, a subgraph could specify that blue is the default color for all nodes defined in it. In the context of graph drawing, a more interesting example is:\n\nThis (anonymous) subgraph specifies that the nodes A, B and C should all be placed on the same rank if drawn using dot.\n\nThe third role for subgraphs directly involves how the graph will be laid out by certain layout engines. If the name of the subgraph begins with , Graphviz notes the subgraph as a special cluster subgraph. If supported, the layout engine will do the layout so that the nodes belonging to the cluster are drawn together, with the entire drawing of the cluster contained within a bounding rectangle. Note that, for good and bad, cluster subgraphs are not part of the DOT language, but solely a syntactic convention adhered to by certain layout engines.\n\nA graph must be specified as either a digraph or a graph. Semantically, this indicates whether or not there is a natural direction from one of the edge's nodes to the other. Lexically, a digraph must specify an edge using the edge operator while a undirected graph must use . Operationally, the distinction is used to define different default rendering attributes. For example, edges in a digraph will be drawn, by default, with an arrowhead pointing to the head node. For ordinary graphs, edges are drawn without any arrowheads by default.\n\nA graph may also be described as strict. This forbids the creation of multi-edges, i.e., there can be at most one edge with a given tail node and head node in the directed case. For undirected graphs, there can be at most one edge connected to the same two nodes. Subsequent edge statements using the same two nodes will identify the edge with the previously defined one and apply any attributes given in the edge statement. For example, the graph\n\nwill have a single edge connecting nodes and , whose color is blue.\n\nIf a default attribute is defined using a node, edge, or graph statement, or by an attribute assignment not attached to a node or edge, any object of the appropriate type defined afterwards will inherit this attribute value. This holds until the default attribute is set to a new value, from which point the new value is used. Objects defined before a default attribute is set will have an empty string value attached to the attribute once the default attribute definition is made.\n\nNote, in particular, that a subgraph receives the attribute settings of its parent graph at the time of its definition. This can be useful; for example, one can assign a font to the root graph and all subgraphs will also use the font. For some attributes, however, this property is undesirable. If one attaches a label to the root graph, it is probably not the desired effect to have the label used by all subgraphs. Rather than listing the graph attribute at the top of the graph, and the resetting the attribute as needed in the subgraphs, one can simply defer the attribute definition in the graph until the appropriate subgraphs have been defined.\n\nIf an edge belongs to a cluster, its endpoints belong to that cluster. Thus, where you put an edge can effect a layout, as clusters are sometimes laid out recursively.\n\nThere are certain restrictions on subgraphs and clusters. First, at present, the names of a graph and it subgraphs share the same namespace. Thus, each subgraph must have a unique name. Second, although nodes can belong to any number of subgraphs, it is assumed clusters form a strict hierarchy when viewed as subsets of nodes and edges.\n\nThe DOT language assumes at least the ASCII character set. Quoted strings, both ordinary and HTML-like, may contain non-ASCII characters. In most cases, these strings are uninterpreted: they simply serve as unique identifiers or values passed through untouched. Labels, however, are meant to be displayed, which requires that the software be able to compute the size of the text and determine the appropriate glyphs. For this, it needs to know what character encoding is used.\n\nBy default, DOT assumes the UTF-8 character encoding. It also accepts the Latin1 (ISO-8859-1) character set, assuming the input graph uses the charset attribute to specify this. For graphs using other character sets, there are usually programs, such as , which will translate from one character set to another.\n\nAnother way to avoid non-ASCII characters in labels is to use HTML entities for special characters. During label evaluation, these entities are translated into the underlying character. This table shows the supported entities, with their Unicode value, a typical glyph, and the HTML entity name. Thus, to include a lower-case Greek beta into a string, one can use the ASCII sequence . In general, one should only use entities that are allowed in the output character set, and for which there is a glyph in the font.\n• In quoted strings in DOT, the only escaped character is double-quote . That is, in quoted strings, the dyad is converted to ; all other characters are left unchanged. In particular, remains . Layout engines may apply additional escape sequences.\n• Previous to 2.30, the language allowed escaped newlines to be used anywhere outside of HTML strings. The new lex-based scanner makes this difficult to implement. Given the perceived lack of usefulness of this generality, we have restricted this feature to double-quoted strings, where it can actually be helpful."
    },
    {
        "link": "https://graphviz.org/pdf/dotguide.pdf",
        "document": ""
    },
    {
        "link": "https://graphviz.readthedocs.io/en/stable/api.html",
        "document": "Help on class Digraph in module graphviz.graphs: | name: Graph name used in the source code. | comment: Comment added to the first line of the source. | graph_attr: Mapping of ``(attribute, value)`` pairs for the graph. | node_attr: Mapping of ``(attribute, value)`` pairs set for all nodes. | edge_attr: Mapping of ``(attribute, value)`` pairs set for all edges. | to add to the graph ``body``. | All parameters are `optional` and can be changed under their | corresponding attribute name after instance creation. | Data and other attributes defined here: | list of weak references to the object | Initialize self. See help(type(self)) for accurate signature. | Yield the DOT source code line by line (as graph or subgraph). | attrs: Attributes to be set (must be strings, may be empty). | See the :ref:`usage examples in the User Guide <attributes>`. | Create an edge between two nodes. | label: Caption to be displayed near the edge. | attrs: Any additional edge attributes (must be strings). | The ``tail_name`` and ``head_name`` strings are separated | by (optional) colon(s) into ``node`` name, ``port`` name, | See :ref:`details in the User Guide <node-ports-compass>`. | and strings of the form ``<...>`` have a special meaning. | See the sections :ref:`backslash-escapes` and | :ref:`quoting-and-html-like-labels` in the user guide for details. | The ``tail_name`` and ``head_name`` strings are separated | by (optional) colon(s) into ``node`` name, ``port`` name, | See :ref:`details in the User Guide <node-ports-compass>`. | name: Unique identifier for the node inside the source. | label: Caption to be displayed (defaults to the node ``name``). | attrs: Any additional node attributes (must be strings). | and strings of the form ``<...>`` have a special meaning. | See the sections :ref:`backslash-escapes` and | :ref:`quoting-and-html-like-labels` in the user guide for details. | Add the current content of the given sole ``graph`` argument | created with the given (``name``, ``comment``, etc.) arguments | whose content is added as subgraph | graph: An instance of the same kind | name: Subgraph name (``with``-block use). | body: Verbatim lines to add to the subgraph ``body`` | See the :ref:`usage examples in the User Guide <subgraphs-clusters>`. | When used as a context manager, the returned new graph instance | uses ``strict=None`` and the parent graph's values | for ``directory``, ``format``, ``engine``, and ``encoding`` by default. | If the ``name`` of the subgraph begins with | the layout engine will treat it as a special cluster subgraph. | Save the source to file and render with the Graphviz engine. | format: The output format used for rendering | renderer: The output renderer used for rendering | formatter: The output formatter used for rendering | overwrite_source: Allow ``dot`` to write to the file it reads from. | The (possibly relative) path of the rendered file. | graphviz.RequiredArgumentError: If ``formatter`` is given | but ``renderer`` is None. | ValueError: If ``outfile`` is the same file as the source file | of the rendering ``dot`` subprocess is non-zero. | RuntimeError: If viewer opening is requested but not supported. | The layout command is started from the directory of ``filepath``, | so that references to external files | can be given as paths relative to the DOT source file. | Save the source to file, open the rendered result in a viewer. | from the viewer process (ineffective on Windows). | The (possibly relative) path of the rendered file. | graphviz.CalledProcessError: If the exit status is non-zero. | RuntimeError: If opening the viewer is not supported. | There is no option to wait for the application to close, | and no way to retrieve the application's exit status. | Save the DOT source to file. Ensure the file ends with a newline. | filename: Filename for saving the source (defaults to ``name`` + ``'.gv'``) | The (possibly relative) path of the saved source file. | The target path for saving the DOT source file. | Data and other attributes inherited from graphviz.saving.Save: | Return the source piped through the Graphviz layout command. | format: The output format used for rendering | renderer: The output renderer used for rendering | formatter: The output formatter used for rendering | Bytes or if encoding is given decoded string | graphviz.RequiredArgumentError: If ``formatter`` is given | but ``renderer`` is None. | of the rendering ``dot`` subprocess is non-zero. | Return a new :class:`.Source` instance with the source | of leaf edges between 1 and this small integer. | of up to this many nodes. | graphviz.RequiredArgumentError: If ``fanout`` is given | but ``stagger`` is None. | of the unflattening 'unflatten' subprocess is non-zero. | The encoding for the saved source file. | The layout engine used for rendering | The output format used for rendering | The output renderer used for rendering | The output formatter used for rendering | An independent copy of the current object."
    },
    {
        "link": "https://graphviz.readthedocs.io/en/stable/manual.html",
        "document": "graphviz provides a simple pure-Python interface for the Graphviz graph-drawing software. It runs under Python 3.8+. To install it with pip, run the following: For a system-wide install, this typically requires administrator access. For an isolated install, you can run the same inside a or a virtualenv. The only dependency is a working installation of Graphviz (download page, archived versions, installation procedure for Windows). After installing Graphviz, make sure that its subdirectory containing the layout command for rendering graph descriptions is on your systems’ (sometimes done by the installer; setting on Linux, Mac, and Windows): On the command-line, should print the version of your Graphiz installation. Windows users might want to check the status of known issues (gvedit.exe, sfdp, commands) and consider trying an older archived version as a workaround (e.g. graphviz-2.38.msi). See the downstream conda-forge distribution conda-forge/python-graphviz (feedstock), which should automatically conda-forge/graphviz (feedstock) as dependency.\n\nThe graphviz package provides two main classes: and . They create graph descriptions in the DOT language for undirected and directed graphs respectively. They have the same API. and produce different DOT syntax and have different values for . Create a graph by instantiating a new or object: Their constructors allow to set the graph’s identifier, the for the DOT source and the rendered graph, an optional for the first source code line, etc. Add nodes and edges to the graph object using its and or methods: The method takes a identifier as first argument and an optional . The method takes the names of start node and end node, while takes an iterable of name pairs. Keyword arguments are turned into (node and edge) attributes (see extensive Graphviz docs on available attributes). Use the method to save the DOT source code and render it with the default layout engine (see below for using other layout engines). Passing will automatically open the resulting (PDF, SVG, PNG, etc.) file with your system’s default viewer application for the rendered file type. Backslash-escapes and strings of the form have a special meaning in the DOT language and are currently passed on as is by this library. If you need to render arbitrary strings literally (e.g. from user input), consider wrapping them with the function first. See the sections on Backslash escapes and Quoting and HTML-like labels below for details.\n\nThe Graphviz layout engines support a number of escape sequences such as , , (for placement of multi-line labels: centered, left-justified, right-justified) and , , (expanded to the current node name, graph name, object label). To be able to use them from this library (e.g., for labels), backslashes in strings are (mostly) passed on as is. This means that literal backslashes need to be escaped (doubled) by the user. As the backslash is also special in Python literals a second level of doubling is needed. E.g. for a label that is rendered as single literal backlash: . Doubling of backslashes can be avoided by using raw string literals ( ) instead. This is similar to the solution proposed for the stdlib module. See also https://en.wikipedia.org/wiki/Leaning_toothpick_syndrome. To disable any special character meaning in a string (e.g. from user input to be rendered literally), use the function (similar to the function): To prevent breaking the internal quoting mechanism, the special meaning of as a backslash-escaped quote has been disabled since version of this library. E.g. both and now produce the same DOT source (a label that renders as a literal quote). See also examples/graphviz-escapes.ipynb (nbviewer).\n\nThe graph-building methods of and objects automatically take care of quoting (and escaping quotes) where needed (whitespace, keywords, double quotes, etc.): If a string starts with and ends with , it is passed on as is, i.e. without quoting/escaping: The content between the angle brackets is treated by the Graphviz layout engine as special HTML string that can be used for HTML-like labels: For strings that should literally begin with and end with , use the function to disable the special meaning of angled parenthesis and apply normal quoting/escaping: Before version , the only workaround was to add leading or trailing space ( ):\n\nand objects have a method for adding a subgraph to the instance. There are two ways to use it: Either with a ready-made instance of the same kind as the only argument (whose content is added as a subgraph) or omitting the argument (returning a context manager for defining the subgraph content more elegantly within a -block). First option, with as the only argument: Second usage, with a -block (omitting the argument): Both produce the same result: If the of a subgraph begins with (all lowercase), the layout engine treats it as a special cluster subgraph (example ). See the section in DOT language. When is used as a context manager, the new graph instance is created with copying the parent graph values for , , , , , and : These copied attributes are only relevant for rendering the subgraph independently (i.e. as a stand-alone graph) from within the -block.\n\nOn platforms such as Windows, viewer programs opened by with (or equivalently with the shortcut-method) might lock the (PDF, PNG, etc.) file for as long as the viewer is open (blocking re-rendering it with a error). You can use the function from the stdlib module to render to a different file for each invocation. This avoids needing to close the viewer window each time within such an incremental workflow (and also serves to preserves the intermediate steps).\n• None use the Jupyter notebook or Qt Console (display the current version of the rendered graph in repeated add/render/view cycles)"
    },
    {
        "link": "https://analyticsvidhya.com/blog/2020/08/a-simple-introduction-to-sequence-to-sequence-models",
        "document": "Are you struggling to tackle complex language tasks like machine translation, text summarization, or chatbot creation? Look no further than the groundbreaking seq2seq models – the neural network architectures taking the deep learning world by storm. These innovative encoder-decoder models, built with recurrent neural networks (RNNs) like LSTMs and GRUs, have revolutionized how we process and generate sequential data.\n\nAt the core of seq2seq models lies the attention mechanism, a game-changer that allows the decoder to focus dynamically on the most relevant parts of the input sequence. This attention-based approach boosts accuracy and provides valuable insights into the model’s decision-making process. While classical seq2seq models faced hurdles with long sequences, the advent of transformers using self-attention has pushed the boundaries further. From neural machine translation to image captioning and beyond, seq2seq models have left an indelible mark on natural language processing (NLP) and computer vision. In this tutorial, we will unlock their full potential with large datasets, optimized architectures like the encoder-decoder network, and cutting-edge optimizers – endless possibilities!\n\nSeq2Seq (Sequence-to-Sequence) models are a type of neural network, an exceptional Recurrent Neural Network architecture, designed to transform one data sequence into another. They are handy for tasks where the input and output are sequences of varying lengths, which traditional neural networks struggle to handle, such as solving complex language problems like machine translation, question answering, creating chatbots, text summarization, etc.\n\nUse Cases of the Sequence to Sequence Models\n• Machine Translation: One of the most prominent applications of Seq2Seq models is translating text from one language to another, such as converting English sentences into French sentences.\n• Text Summarization: Seq2Seq models can generate concise summaries of longer documents, capturing the essential information while omitting less relevant details.\n• Speech Recognition: Converting spoken language into written text. Seq2Seq models can be trained to map audio signals (sequences of sound) to their corresponding transcriptions (sequences of words).\n• Chatbots and Conversational AI: These models can generate human-like responses in a conversation, taking the previous sequence of user inputs and generating appropriate replies.\n• Image Captioning: Seq2Seq models can describe the content of an image in natural language. The encoder processes the image (often using Convolutional Neural Networks, CNNs) to produce a context vector, which the decoder converts into a descriptive sentence.\n• Video Captioning: Similar to image captioning but with videos, Seq2Seq models generate descriptive texts for video content, capturing the sequence of actions and scenes.\n• Time Series Prediction involves predicting the future values of a sequence based on past observations. This application is expected in finance (stock prices), meteorology (weather forecasting), and more.\n• Code Generation: This process generates code snippets or entire programs from natural language descriptions, which is helpful in programming assistants and automated software engineering tools.\n• Flexibility with Input and Output Sequences: Seq2Seq models can handle variable-length input and output sequences, particularly those using the encoder-decoder architecture. This makes them suitable for tasks like machine translation, where the length of the input sentence (e.g., English) and the output sequence (e.g., French) can differ significantly.\n• Effective Handling of Sequential Data: Utilizing recurrent neural networks (RNNs) such as Long Short Term Memory (LSTM) and GRU in the encoder-decoder structure allows Seq2Seq models to capture long-range dependencies within the input sequence. This is crucial for understanding context and meaning in tasks like text summarization and neural machine translation.\n• Attention Mechanism: Introducing the attention mechanism enhances the performance of Seq2Seq models by allowing the decoder to focus on relevant parts of the input sequence at each time step. This addresses the limitation of compressing all input information into a single context vector and significantly improves accuracy in tasks requiring nuanced understanding, such as image captioning and natural language processing (NLP) applications.\n• Versatility in Application: Seq2Seq models are not limited to text-based tasks. They are also employed in speech recognition, video captioning, and time series prediction. Their ability to process and generate sequences makes them a powerful tool in various deep-learning applications.\n• Computational Complexity: Training Seq2Seq models can be computationally intensive, especially those using Long Short-Term Memory (LSTM) or GRU networks. The requirement for significant training data and large batch sizes can lead to high computational costs and longer epochs.\n• Difficulty in Handling Long Sequences: While RNNs and their variants (LSTM, GRU) are designed to handle sequential data, they can struggle with long sequences due to the vanishing gradient problem, which impacts the learning of long-range dependencies. Even with attention mechanisms, this can remain challenging in tasks requiring detailed context over extended sequences.\n• Dependency on Large Datasets: Seq2Seq models require extensive and diverse datasets for practical training. Insufficient or poor-quality training data can lead to overfitting and reduced generalization capacity of the model, impacting the model’s performance on unseen data.\n• Performance Variability with Architecture Choices: The performance of Seq2Seq models can vary significantly based on architectural choices and hyperparameters, such as the number of layers in the encoder-decoder, the size of the hidden state, and the specific optimizer used (e.g., Adam). Fine-tuning these parameters is often necessary but can be complex and time-consuming.\n• Emerging Competition from Transformers: Transformers and their variants (like BERT and GPT) have been shown to outperform traditional Seq2Seq models in many tasks by eliminating the need for sequential processing and better handling of long-range dependencies. This has led to a shift in focus within natural language processing (NLP).\n\nThe most common architecture used to build Seq2Seq models is Encoder-Decoder architecture.\n\nAs the name implies, there are two components — an encoder and a decoder.\n• Both the encoder and the decoder are Long short-term memory (LSTM) models (or sometimes GRU models)\n• The encoder reads the input sequence and summarizes the information in something called the internal state vectors or context vectors (in the case of LSTM, these are called the hidden state and cell state vectors). We discard the outputs of the encoder and only preserve the internal states. This context vector aims to encapsulate the information for all input elements to help the decoder make accurate predictions.\n• The hidden states h_i are computed using the formula:\n\nThe LSTM reads the data, one sequence after the other. Thus if the input is a sequence of length ‘t’, we say that LSTM reads it in ‘t’ time steps.\n\n2. hi and ci = The LSTM maintains two states (‘h’ for hidden state and ‘c’ for cell state) at each time step. Combined, these are the internal states of the LSTM at time step i.\n\n3. Yi = Output sequence at time step i. Yi is actually a probability distribution over the entire vocabulary generated by using a softmax activation. Thus, each Yi is a vector of size “vocab_size” representing a probability distribution.\n• The decoder is a long-short-term memory (LSTM) whose initial states are initialized to the final states of the Encoder LSTM. In other words, the encoder sector of the encoder’s final cell is input to the first cell of the decoder network. Using these initial states, the decoder starts generating the output sequence, and these outputs are also considered for future outputs.\n• A stack of several LSTM units where each predicts an output y_t at a time step t.\n• Each recurrent unit accepts a hidden state from the previous unit and produces an output and its hidden state.\n• Any hidden state h_i is computed using the formula:\n• The output y_t at time step t is computed using the formula:\n• We calculate the outputs using the hidden state at the current time step and the respective weight W(S). Softmax creates a probability vector to help us determine the final output (e.g., word in the question-answering problem).\n\nThere are two primary drawbacks to this architecture, both related to length.\n• Firstly, as with humans, this architecture has minimal memory. The ultimate hidden state of the Long Short Term Memory (LSTM), S or W, is tasked with encapsulating the entire sentence for translation. Typically, S or W comprises only a few hundred units (i.e., floating-point numbers). However, cramming too much into this fixed-dimensional vector increases glossiness in the neural network. Thinking of neural net”orks in terms of the “lossy compression” they must perform is sometimes quite useful.\n• Second, as a general rule of thumb, the deeper a neural network is, the harder it is to train. For recurrent neural networks, the longer the sequence is, the deeper the neural network is along the time dimension. This results in vanishing gradients, where the gradient signal from the objective that the recurrent neural network learns from disappears as it travels backward. Even with RNNs specifically made to help prevent vanishing gradients, such as the LSTM, this is still a fundamental problem.\n\nFurthermore, we have models like Attention Models and Transformers for more robust and lengthy sentences.\n\nThe attention mechanism is crucial in many modern neural network architectures, especially in tasks involving sequences like natural language processing and computer vision. Its primary function is to allow the model to dynamically focus on different parts of the input sequence (or image) while processing the output sequence. Here are some key aspects and benefits of attention mechanisms:\n\nCheckout this article about the Machine Learning Algorithms\n• Dynamic Weighting: Instead of relying on a fixed-length context vector to encode the entire input sequence, attention mechanisms assign different weights to different parts of the input sequence based on their relevance to the current step of the output sequence. This dynamic weighting enables the model to focus more on relevant information and ignore irrelevant parts.\n• Soft Alignment: Attention mechanisms create a soft alignment between the input and output sequences by computing a distribution of attention weights over the input sequence. This allows the model to consider multiple input elements simultaneously, unlike hard alignment methods that force the model to choose only one input element at each step.\n• Scalability: Attention mechanisms are scalable to sequences of varying lengths. They can adapt to longer input sequences without significantly increasing the computational complexity, unlike fixed-length context vectors, which may struggle with long sequences.\n• Interpretable Representations: Attention weigh’s represent the model’s decision-making process. By visualizing these weights, researchers and practitioners can gain insights into which parts of the input sequence are most relevant for generating specific parts of the output sequence.\n\nTransformers, a breakthrough in deep learning, have significantly impacted Seq2Seq models, particularly in natural language processing (NLP) tasks. Unlike traditional Seq2Seq models based on recurrent neural networks (RNNs), transformers rely on self-attention mechanisms to capture long-range dependencies in input sequences, making them highly effective for sequence-to-sequence tasks.\n\nThe architecture of transformer-based Seq2Seq models consists of an encoder-decoder framework, similar to traditional Seq2Seq models. However, instead of recurrent layers, transformers employ self-attention layers, enabling the model to dynamically weigh the importance of different input tokens.\n• Encoder: The encoder comprises multiple layers of self-attention mechanisms and feed-forward neural networks. It processes the input sequence in parallel, allowing the model to efficiently capture dependencies across the entire sequence.\n• Decoder: Similar to the encoder, the decoder consists of self-attention layers and feed-forward networks. It generates the output sequence token by token, attending to relevant parts of the input sequence using the attention mechanism.\n• Self-Attention Mechanism: This mechanism allows each token in the input sequence to attend to all other tokens, effectively capturing contextual information. By assigning different attention weights to each token, the model can focus on relevant information and ignore irrelevant parts.\n• Positional Encoding: Since transformers do not inherently understand the sequential order of tokens, positional encodings are added to the input embeddings to provide information about token positions. This ensures that the model can differentiate between tokens based on their positions in the sequence.\n• Multi-Head Attention: Transformers typically use multi-head attention mechanisms, where attention is calculated multiple times in parallel with different learned linear projections. This allows the model to capture diverse relationships between tokens.\n• Feed-Forward Neural Networks: These networks are applied after the self-attention layers to perform nonlinear transformations on the encoded representations, facilitating the learning of complex patterns in the data.\n\nSeq2Seq (Sequence-to-Sequence) models find applications in a wide range of tasks where the input and output are sequences of varying lengths. One prominent example of Seq2Seq models in action is in machine translation, where they excel at translating text from one language to another. Let’s explore this application using an example:\n\nConsider a scenario where we have a Seq2Seq model trained to translate English sentences into French. Here’s how the process works:\n\nNow, let’s break down how the Seq2Seq model translates the input sequence into the target sequence:\n• The input sequence “How are you today?” is fed into the encoder part of the Seq2Seq model.\n• The encoder, typically composed of LSTM or GRU layers, processes the input sequence token by token, generating a fixed-size representation known as the context vector or hidden state.\n• Each token in the input sequence is encoded into a high-dimensional vector representation, capturing its semantic meaning and context within the sentence.\n• The final hidden state of the encoder contains the summarized information from the entire input sequence, encapsulating the sentence’s meaning.\n• The context vector generated by the encoder is passed to the decoder as the initial hidden state.\n• The decoder, also composed of LSTM or GRU layers, generates the output sequence token by token.\n• The decoder predicts the next token in the target sequence based on the context vector and previously generated tokens at each time step.\n• The model utilizes an attention mechanism to focus on relevant parts of the input sequence while generating each token of the output sequence. This allows the model to effectively align words in the input and output sentences.\n• The process continues until the decoder predicts an end-of-sequence token or reaches a maximum predefined length for the output sequence.\n\nCheckout this article about the Attention Mechanism in Deep Learning\n• The model generates the target sequence token by token, producing the translated sentence “Comment vas-tu aujourd’hui ?” in French.\n• Each token in the output sequence is predicted based on the information encoder-decoder’s context vector and the decoder’s internal state.\n• The final output is a fluent and contextually accurate translation of the input sentence, demonstrating the Seq2Seq model’s ability to handle complex language tasks like machine translation.\n\nHere’s a simplified Python code example demonstrating how to implement a Seq2Seq model for English-to-French translation using PyTorch:\n\nThis tutorial has served as a comprehensive guide for Seq2Seq models. Encoder-decoder architectures power them, and recurrent neural networks like Long Short Term Memory (LSTM) and GRU have emerged as indispensable tools in natural language processing (NLP) and beyond. With optimized architectures, large datasets, and cutting-edge techniques, the potential of Seq2Seq models remains vast. Their versatility drives machine learning and artificial intelligence innovation, from language translation to code generation.\n\nAs researchers and practitioners continue to explore and refine these models, Seq2Seq remains at the forefront of advancements in deep learning, paving the way for more sophisticated and intelligent systems.\n\nIf you want to learn more about artificial intelligence, machine learning algorithms, Language models, and Python, follow Analytics Vidhya’s Blog.\n\nQ1. Which transformer architecture is best? A. Determining the “best” transformer architecture depends on the task and dataset. Models like BERT, GPT, and T5 each excel in different areas. For instance, BERT is excellent for natural language understanding tasks, while GPT is renowned for text generation, and T5 offers a unified framework for various NLP tasks. Q2. How does the attention mechanism improve seq2seq models? A. The attention mechanism enhances Seq2Seq models by dynamically allowing the decoder to focus on relevant parts of the input sequence. Instead of compressing all input information into a single context vector, attention assigns different weights to different parts of the sequence, enabling the model to capture dependencies effectively and significantly improving accuracy. Q3. How do Seq2Seq models handle variable-length input sequences? A. Seq2Seq models handle variable-length input sequences using techniques like padding and masking. The padding ensures that all sequences in a batch have the same length, while masking prevents the model from attending to padding tokens. This allows the model to process sequences of varying lengths without losing important information. Q4. Is ChatGPT seq2seq? A. ChatGPT, like its predecessor GPT, is a transformer-based model, not a Seq2Seq model. While both architectures are used for natural language processing tasks, GPT generates text autoregressively, while Seq2Seq models typically perform tasks like translation and summarization by encoding and decoding sequences. Q5. Is seq2seq supervised or unsupervised? A. Seq2Seq models are typically supervised learning models. During training, they require input-output pairs, where the input sequence and corresponding output sequence are provided. The model learns to map input sequences to output sequences based on the training data, optimizing its parameters to minimize a predefined loss function."
    },
    {
        "link": "https://appliednetsci.springeropen.com/articles/10.1007/s41109-019-0174-8",
        "document": "We propose unsupervised and supervised architectures for learning representations of labeled and unlabeled graphs. In our unsupervised architecture, the goal is to learn graph embeddings such that graphs with similar structure lie close to one another in the embedding space. We seek to learn a mapping function \\(\\Phi :G \\to \\mathbb {R}^{k}\\) that embeds a graph G into a k-dimensional space. We are interested in methods that scale linearly in the number of graphs in a dataset (as opposed to graph kernel methods that require quadratic time). Our approach uses encoder-decoder models, particularly autoencoders (Hinton and Zemel 1993), which form one of the principal frameworks of unsupervised learning. Autoencoders are typically trained to reconstruct their input in a way that learns useful properties of the data. There are two parts to an autoencoder: an encoder that maps the input to some intermediate representation, and a decoder that attempts to reconstruct the input from this intermediate representation. We need to decide how to represent graphs in a form that can be encoded and then reconstructed. We do this by extracting ode sequences with various levels of granularity (the increasing order of node neighborhoods) from the graphs. “Generating sequences from graphs” section describes several methods for doing this. Given node sequences from a graph, we then need an encoding-decoding framework that can handle variable-length sequences. We choose LSTMs for both the encoder and decoder, forming an LSTM autoencoder (Li et al. 2015a). LSTM autoencoders use one LSTM to read the input sequence and encode it to a fixed dimensional vector, and then use another LSTM to decode the output sequence from the vector. We consider several variations for the encoder and decoder, described in “Sequence-to-sequence encoder-decoder” section. We experiment with two training objectives, described in “Training” section. Given the trained encoder LSTM , we define the graph embedding function Φ(G) as the mean of the vectors output by the encoder over Seq(G), the set of graph sequences extracted from G: Using the mean outperformed max pooling in our experiments so we only report results using the mean in this paper. We use Φ to represent graphs in our experiments in “Experiments” section, demonstrating state-of-the-art performance for several graph classification tasks. Our supervised model uses a sequence-to-sequence framework to capture the graph structure with the supervision of a graph classification task. We train our model to predict the label of a graph using randomly selected sets of node sequences over multiple iterations. The model leverages the node embeddings learned by our unsupervised encoder-decoder models in learning to predict graph labels. We describe our supervised approach in “Supervised graph representation learning” section and compare it with other supervised and unsupervised approaches in “Experiments” section.\n\nThe input of our model is a set of node sequences generated from different types of graph substructures such as Random Walks, Shortest Paths, and Breadth-First Search. Different substructures capture varying aspects of local vs global topology of the graph. Moreover, we incorporate information about the neighborhood of a node sequence in order to preserve more information about the region of the graph that is traversed by the sequence. The neighborhood of a sequence consists of the neighbors of nodes in that sequence. In order to incorporate the neighborhood of a sequence in graph representation learning, we assign a label to each node in the sequence based on its neighborhood. The order of the neighborhood (which we call granularity) determines whether it is the immediate neighbors (1st order neighborhood) or the neighbors of neighbors as well (2nd order neighborhood), etc. For instance, the kth-order granularity for sequence s:v ,…,v , considers all the nodes in the graph that have a distance of k or less from a v ∈s. We examine the impact of various substructures and orders of granularity on our models. Random Walks (RW): Given a source node u, we generate a random walk w with fixed length m. Let v denote the ith node in w , starting with v =u. v is a node from the Nbrs(v ) that is selected with probability 1/deg(v ), where deg(v ) is the degree of v . (This is a 0-order Random Walk in our context. A higher order Random Walk replaces nodes with higher order neighborhoods–see the following “Substructure granularity” section). Shortest Paths (SP): We generate all the shortest paths between each pair of nodes in the graph using the Floyd-Warshall algorithm (Floyd 1962). Breadth-First Search (BFS): We run the BFS algorithm at each node to generate graph sequences for that node. The graph sequences for the graph include the BFS sequences starting at each node in the graph, limited to a maximum number of edges from the starting node. We give details on the maximum used in our experiments below. Each of the sequences defined in the previous section can be sequences of nodes at different orders of granularity. For example, a Random Walk or a Shortest Path can be a sequence of nodes with 0-order granularity, a sequence of nodes with first-order granularity, or a sequence of nodes with the second-order granularity, etc. We use Weisfeiler-Lehman (WL) algorithm (Weisfeiler and Lehman 1968) to generate labels for nodes, encoding the node neighborhood (of the correpsonding order of granularity) information in the label. This algorithm is typically used as a graph isomorphism test. It is known as an iterative node classification or node refinement procedure (Shervashidze et al. 2011). The WL algorithm uses multiset labels to encode the local structure of the graphs. The idea is to create a multiset label for each node using the sorted list of its neighbors’ labels. Then, the sorted list is compressed into a new value. The WL algorithm can iterate this labeling process and add higher order neighbors to the neighborhood list at each iteration. This labeling process continues until the new multiset labels of graphs in the dataset are different or the number of iterations reaches a specified limit. Each iteration increases the order of substructure granularity by expanding the node neighborhood. The WL labels can enrich the information provided by a node, regardless of whether the original graph is labeled or unlabeled. Each unique WL label is converted to a parameter vector in a k-dimensional space where each entry is initialized with a draw from a uniform distribution with range [−1,1]. Overall, depending on the granularity that we consider for nodes, we may include different number of parameter vectors in the model. WL parameter vectors are updated during the training time in addition to the model parameters. For each node v in a sequence, Emb(v) denotes the parameter vector assigned to the corresponding WL label of node v.\n\nWe discussed three types of substructures used for extracting sequences from graphs and our approach for embedding nodes considering the order of their neighborhoods. We now describe how we will learn our graph embedding functions in our unsupervised setting. We formulate graph representation learning as training an encoder-decoder on node sequences generated from graphs. The most common type of encoder-decoder is a feed-forward deep neural network, but those suffer from the limitation of requiring fixed-length inputs and an inability to model sequential data. Therefore, we focus in this paper on sequence-to-sequence encoder-decoder architecture, which can support arbitrary-length sequences. These encoder-decoder models are based on the sequence-to-sequence learning framework of Sutskever et al. (2014), an LSTM-based architecture in which both the inputs and outputs are sequences of variable length. The architecture uses one LSTM as the encoder LSTM and another LSTM as the decoder LSTM . An input sequence s with length m is given to LSTM and its elements are processed one per time step. The hidden vector h at the last time step m is the fixed-length representation of the input sequence. This vector is provided as the initial vector to LSTM to generate the output sequence. We suggested four different versions of sequence-to-sequence encoder-decoder models to investigate their ability to capture the graph structure. Three out of four encoder-decoder models adapt the sequence-to-sequence learning framework for autoencoding simply by using the same sequence for both the input and output. The autoencoders are trained so that LSTM reconstructs the input using the final hidden vector from LSTM . In our experiments, we use several graph datasets. We train a single encoder-decoder for each graph dataset. The encoder-decoder is trained on a training set of graph sequences pooled across all graphs in the dataset. After training the encoder-decoder, we obtain the representation Φ(G) for a single graph G by encoding its sequences s∈Seq(G) using LSTM , then averaging its encoding vectors, as in Eq. (3). S2S-AE: This is the standard sequence-to-sequence autoencoder inspired by (Li et al. 2015a), which we customize for embedding graphs. Figure 2 shows an overview of this model. We use \\(h_{t}^{enc}\\) to denote the hidden vector at time step t in LSTM and \\(h_{t}^{dec}\\) to denote the hidden vector at time step t in LSTM . We define shorthand for Eq. 1 as follows: where Emb(v ) takes the role of x in Eq. 1. The hidden vector at the last time step \\(h_{last}^{enc} \\in \\mathbb {R}^{m}\\) denotes the representation of the input sequence, and is used as the hidden vector of the decoder at its first time step: The last cell vector of the encoder is copied over in an analogous way. Then each decoder hidden vector \\(h_{t}^{dec}\\) is computed based on the hidden vector and the node embedding from the previous time step: The decoder uses \\(h_{t}^{dec}\\) to predict the next node embedding \\(\\overline {Emb({v_{t}})}\\) as in Eq. 2. We have two different loss functions to test with this model. First, we consider the node embeddings fixed and compute a loss based on the difference between the predicted node embedding \\(\\overline {Emb({v_{t}})}\\) and the true one Emb(v ). Second, we consider a parameter vector for each embedding and update the node embeddings in addition to the model parameters using a cross entropy function. We discuss training in “Training” section. For Emb(v ), we use a vector of all zeroes. S2S-AE-PP: In the previous model, LSTM predicts the embedding of the node at time step t using \\(h_{t-1}^{dec}\\) as well as Emb(v ), the true node embedding at time step t−1. However, this may enable the decoder to rely too heavily on the previous true node in the sequence, thereby making it easier to reconstruct the input and reducing the need for the encoder to learn an effective representation of the input sequence. We consider a variation (“S2S-AE-PP: sequence-to-sequence autoencoder previous predicted”) in which we use the previous predicted node \\({Emb}(\\overline {v_{t-1}})\\) instead of the previous true one: This forces the encoder and decoder to work harder to respectively encode and decode the graph sequences. This variation is related to scheduled sampling (Bengio et al. 2015), in which the training process is changed gradually from using true previous symbols to using predicted previous symbols more and more during training. The difference of S2S-AE-PP with previous model is indicated in Fig. 3. S2S-AE-PP-WL1,2: This model is similar to S2S-AE-PP except that, for each node in the sequence, we use two different levels of neighborhood granularity. We incorporate the first-order neighborhoods and second-order neighborhoods (neighbors of neighbors) of nodes in learning graph representation. We use \\(x_{1_{t}}\\) to denote the embedding of the label produced by one iteration of WL (for the first-order neighborhood) and \\(x_{2_{t}}\\) for that produced by two iterations of WL (for second-order neighborhood). Equation 1 is modified to receive both as inputs. For example, the first line of Eq. 1 becomes: The other equations are changed analogously. The embeddings for both the first-order and second-order neighborhoods are learned. Figure 4 shows how this model integrates the information from neighborhoods in order to learn the graph representation. S2S-N2N-PP: This model is a “neighbors-to-node” (N2N) prediction model and uses random walks as graph sequences (Fig. 5). The idea is to explore the neighborhood of a sequence by an encoder and predict the sequence itself by the decoder using the gathered information via the encoder. The encoder is encouraged to collect the information from the neighborhood which is distinguishable from other graph substructures and decoder can reconstruct the original sequence from that. That is, each item in the input sequence is the set of neighbors (their embeddings are averaged) for the corresponding node in the output sequence: where Nbrs(v) returns the set of neighbors of v and we predict the nodes in the random walk via the decoder as in Eq. 7. Unlike the other models, this model is not an autoencoder because the input and output sequences are not the same. Let S be a sequence training set generated from a set of graphs. The representations of the sequences s∈S are computed using the encoders described in the previous section. We use two different loss functions to train our models: squared error and categorical cross-entropy. The goal is to minimize the following loss functions, summed over all examples s∈S, where s:v ,…,v . We used the squared error (SE) loss function for the embeddings that are fixed and are not considered as the trainable parameters of the model. We include a nonlinear transformation to estimate the embedding of the tth node in s using the hidden vector of the decoder at time step t: where ReLU is the rectified linear unit activation function and W and b are additional parameters. Given the predicted node embeddings for the sequence s, the squared error loss function computes the average of the elementwise squared differences between the input and output sequences: We use the categorical cross entropy (CE) loss function for experiments in which we update the node embeddings during training. We predict the tth node as follows: where L is the set of labels. The loss computes the categorical cross entropy between the input embeddings and the predicted output embeddings: where l denotes the true label of node v , and the predicted probability of the true label is computed as follows:\n\nIn this setting, the graph representation learning is guided by a task-specific supervision, incorporating the supervision information into the learning process. Given a dataset D:{G ,...,G }, the goal is to learn a function GrLabel:D→L that assigns a label from a set of labels L to each graph in the dataset. We introduce our supervised method inspired by the most effective unsupervised method proposed in “Sequence-to-sequence encoder-decoder” section. As we will show in “Experiments” section, that S2S-N2N-PP outperforms the other methods in almost all experiments. Therefore, we design the foundation of our supervised method based on the S2S-N2N-PP. We utilize one LSTM for processing a sequence (LSTM ) and one bidrectional LSTM for processing the neighborhoods of the sequence (BiLSTM ). The neighborhoods of a sequence consist of the neighbors of nodes in the sequence. The hidden representation obtained by the BiLSTM from the neighborhoods is given to the LSTM in order to incorporate it in graph representation learning. The BiLSTM contains an attention mechanism to select the most informative neighborhoods for the purpose of graph classification. Recently, several approaches focused on the application of attention mechanisms in graph-structured data (Lee et al. 2018a, b; Veličković et al. 2018). The attention mechanism in these methods is mainly designed to explore the neighbors of a node and focus on the most informative neighbors. However, we utilize a two-level attention mechanism to improve our classification performance by leveraging local as well as global information from the neighborhoods of nodes in a sequence. The first-level attention mechanism, Att , attends over the neighbors of a node to capture more relevant information from its neighbors. The second-level attention module, Att , attends over the neighborhoods along a sequence of nodes to focus on the overall more informative neighborhoods. Our approach aims to capture more globally relevant information from the graph by the Att in comparison with the Att . Figure 6 shows the difference between the two levels of attention mechanism. Overall, given a sequence s, our approach includes two main components: (1) Neighborhood embedding to gather information from the neighborhoods of nodes in sequence s, and (2) Sequence embedding to represent the sequence s by incorporating information from its neighborhoods. We discuss these two components in the following. Figure 7 shows an overview of the proposed approach. For each sequence s∈S, where s:v ,…,v , extracted in “Generating sequences from graphs” section, information from the neighbors of nodes in the sequence is gathered and passed through the BiLSTM . An attention module Att is utilized in order to gather local information from the neighbors of v ∈s. The Att (Nbrs(v )) function decides to which nodes in Nbrs(v ) to pay attention in order to enhance their impact during training. We rely on the attention mechanism in our model to relieve the classification task from the burden of considering all the nodes in Nbrs(v ) to be equally informative. For each neighbor n∈Nbrs(v ): where f is a neural network that computes the attention coefficient for node n. The normalized coefficient, a , is obtained by a softmax function. The resulting attention readout for node v is represented by r . The information about the neighborhood of v is passed through the BiLSTM . Finally, BiLSTM provides a hidden representation from the neighborhood of a sequence. After gathering information from the neighborhood of sequence s, the last hidden representation of the neighborhood, \\(h_{s}^{Nbh}\\), is used by LSTM to find a representation for the sequence for the purpose of graph classification. LSTM processes the sequence whose neighborhood has been already processed by BiLSTM : The second-level attention module, Att is applied over the output of BiLSTM in order to assign weights to parts of the neighborhood along sequence s and reflect the importance of each part for the task-specific prediction: where f is a neural network that computes a single scalar as the attention coefficient for each hidden state of BiLSTM . The attention coefficient of Att is represented by a . An attention mechanism on the outputs of BiLSTM provides the capability for the model to focus on the parts of the neighborhoods along the sequence that are influential in the end-to-end training of the model. The concatenation of sequence embedding and attention module is given to a simple feed-forward neural network, g, to find the subgraph representation, rep , by incorporating sequence s and its neighborhood. In order to train a scalable end-to-end model, we train the model on a set of extracted sequences from “Generating sequences from graphs” section. The model is trained so that it learns to predict the graph label using the set of chosen sequences. A batch of sequences, b, is selected from a graph G ∈D, and the graph representation is obtained as follows: Finally, the probability distribution over labels for G is computed by: where W is a weight matrix reducing the dimension of the graph representation and b is a bias. The loss function computes the categorical cross entropy between the predicted distribution \\(\\overline {y}\\) and the true label of the graph \\(l_{G_{k}} \\in L_{G}\\):"
    },
    {
        "link": "https://geeksforgeeks.org/seq2seq-model-in-machine-learning",
        "document": "Seq2Seq model or Sequence-to-Sequence model, is a machine learning architecture designed for tasks involving sequential data. It takes an input sequence, processes it, and generates an output sequence. The architecture consists of two fundamental components: an encoder and a decoder. Seq2Seq models have significantly improved the quality of machine translation systems making them an important technology. The article aims to explore the fundamentals of the seq2seq model and its applications along with its advantages and disadvantages.\n\nThe seq2Seq model is a kind of machine learning model that takes sequential data as input and generates also sequential data as output. Before the arrival of Seq2Seq models, the machine translation systems relied on statistical methods and phrase-based approaches. The most popular approach was the use of phrase-based statistical machine translation (SMT) systems. That was not able to handle long-distance dependencies and capture global context.\n\nSeq2Seq models addressed the issues by leveraging the power of neural networks, especially recurrent neural networks (RNN). The concept of seq2seq model was introduced in the paper titled “Sequence to Sequence Learning with Neural Networks” by Google. The architecture discussed in this research paper is fundamental framework for natural language processing tasks. The seq2seq models are encoder-decoder models. The encoder processes the input sequence and transforms it into a fixed-size hidden representation. The decoder uses the hidden representation to generate output sequence. The encoder-decoder structure allows them to handle input and output sequences of different lengths, making them capable to handle sequential data. Seq2Seq models are trained using a dataset of input-output pairs, where the input is a sequence of tokens, and the output is also a sequence of tokens. The model is trained to maximize the likelihood of the correct output sequence given the input sequence.\n\nThe advancement in neural networks architectures led to the development of more capable seq2seq model named transformers. “Attention is all you need! ” was a research paper that first introduced the transformer model in the era of Deep Learning after which language-related models have taken a huge leap. The main idea behind the transformers model was that of attention layers and different encoder and decoder stacks which were highly efficient to perform language-related tasks.\n\nSeq2Seq models have been widely used in NLP tasks due to their ability to handle variable-length input and output sequences. Additionally, the attention mechanism is often used in Seq2Seq models to improve performance and it allows the decoder to focus on specific parts of the input sequence when generating the output.\n\nWhat is Encoder and Decoder in Seq2Seq model?\n\nIn the seq2seq model, the Encoder and the Decoder architecture plays a vital role in converting input sequences into output sequences. Let’s explore about each block:\n\nThe main purpose of the encoder block is to process the input sequence and capture information in a fixed-size context vector.\n• None The input sequence is put into the encoder.\n• None The encoder processes each element of the input sequence using neural networks (or transformer architecture).\n• None Throughout this process, the encoder keeps an internal state, and the ultimate hidden state functions as the context vector that encapsulates a compressed representation of the entire input sequence. This context vector captures the semantic meaning and important information of the input sequence.\n\nThe final hidden state of the encoder is then passed as the context vector to the decoder.\n\nThe decoder block is similar to encoder block. The decoder processes the context vector from encoder to generate output sequence incrementally.\n• None In the training phase, the decoder receives both the context vector and the desired target output sequence (ground truth).\n• None During inference, the decoder relies on its own previously generated outputs as inputs for subsequent steps.\n\nThe decoder uses the context vector to comprehend the input sequence and create the corresponding output sequence. It engages in autoregressive generation, producing individual elements sequentially. At each time step, the decoder uses the current hidden state, the context vector, and the previous output token to generate a probability distribution over the possible next tokens. The token with the highest probability is then chosen as the output, and the process continues until the end of the output sequence is reached.\n\nThe decoder and encoder architecture utilizes RNNs to generate desired outputs. Let’s look at the simplest seq2seq model.\n\nFor a given sequence of inputs [Tex](x_1,x_2, …, x_T) [/Tex], a RNN generates a sequence of outputs [Tex](y_1, y_2, …, y_T) [/Tex]through iterative computation based on the following equation:\n• [Tex]W^{hx} [/Tex] represents the weight matrix for the input\n• [Tex]W^{yh} [/Tex] represents the wight matrix for the output\n• None T is the length of the sequence.\n\nRecurrent Neural Networks can easily map sequences to sequences when the alignment between the inputs and the outputs are known in advance. Although the vanilla version of RNN is rarely used, its more advanced version i.e. LSTM or GRU is used. This is because RNN suffers from the problem of vanishing gradient. LSTM develops the context of the word by taking 2 inputs at each point in time. One from the user and the other from its previous output, hence the name recurrent (output goes as input).\n• Flexibility : Seq2Seq models can handle a wide range of tasks such as machine translation, text summarization, and image captioning, as well as variable-length input and output sequences.\n• Handling Sequential Data: Seq2Seq models are well-suited for tasks that involve sequential data such as natural language, speech, and time series data.\n• Handling Context: The encoder-decoder architecture of Seq2Seq models allows the model to capture the context of the input sequence and use it to generate the output sequence.\n• Attention Mechanism: Using attention mechanisms allows the model to focus on specific parts of the input sequence when generating the output, which can improve performance for long input sequences.\n• Computationally Expensive: Seq2Seq models require significant computational resources to train and can be difficult to optimize.\n• Limited Interpretability: The internal workings of Seq2Seq models can be difficult to interpret, which can make it challenging to understand why the model is making certain decisions.\n• Overfitting : Seq2Seq models can overfit the training data if they are not properly regularized, which can lead to poor performance on new data.\n• Handling Rare Words: Seq2Seq models can have difficulty handling rare words that are not present in the training data.\n• Handling Long input Sequences: Seq2Seq models can have difficulty handling input sequences that are very long, as the context vector may not be able to capture all the information in the input sequence.\n\nThroughout the article, we have discovered the machine translation is the real-world application of seq2seq model. Let’s explore more applications:\n• Text Summarization: The seq2seq model effectively understands the input text which makes it suitable for news and document summarization.\n• Speech Recognition: Seq2Seq model, especially those with attention mechanisms, excel in processing audio waveform for ASR. They are able to capture spoken language patterns effectively.\n• Image Captioning: The seq2seq model integrate image features from CNNs with textual generation capabilities for image captioning. They are capable to describe images in a human readable format."
    },
    {
        "link": "https://deeplearningmath.org/sequence-models",
        "document": "See the latest book content here.\n\nSequence Models have been motivated by the analysis of sequential data such text sentences, time-series and other discrete sequences data. These models are especially designed to handle sequential information while Convolutional Neural Network are more adapted for process spatial information.\n\nThe key point for sequence models is that the data we are processing are not anymore independently and identically distributed (i.i.d.) samples and the data carry some dependency due to the sequential order of the data.\n\nSequence Models is very popular for speech recognition, voice recognition, time series prediction, and natural language processing.\n\nRNN is especially designed to deal with sequential data which is not i.i.d. RNN can tackle the following challenges from sequence data:\n• None to keep track of long-term dependencies\n• None to share parameters across the sequence The following figure exhibits the difference between the classical classic feed forward neural network architecture (left figure) and the Recurrent Neural Network:\n• None Feed forward neural network also called vanilla neural network presents one input and provides one output without taking into account the sequence of the data.\n• None Recurrent Neural Network introduce an internal loop wich allows information to be passed from one step of the network to the next. It apply a recurrence relation at every time step to process a sequence of data. The unfold representation of the previous RNN is presented in the following figure which make more clear how the RNN can handle sequence data using the recurrence relation \\[\\Large{\\boxed{\\underbrace{h^{<t>}}_{\\textrm{cell state}} = f_W(\\underbrace{h^{<t-1>}}_{\\textrm{old state}}, \\underbrace{x^{<t>}}_{\\textrm{input vector}})}}\\] where \\(f_W\\) is a function parameterized by the weight \\(W\\). It is important to note that at every time step \\(t\\) the same function \\(f_W\\) is used and same set of weight parameters. Let introduce the weight parameters in the previous figure and precise some notation. Below are the details associated with each parameter in the network.\n• None \\(x^{<1>}, \\ldots, x^{<t-1>}, x^{<t>}, x^{<t+1>}, ... x^{<m>}\\): the input data. In natural langage processing (NLP), for example, we can consider that the sequence input is a sentence of \\(m\\) words \\(x^{<1>}, \\ldots, x^{<t-1>}, x^{<t>}, x^{<t+1>}, ... x^{<m>}\\).\n• None \\(x^{<t>} \\in \\mathbb{R}^{|V|}\\): input vector at time \\(t\\). For example each word \\(x^{<t>}\\) in the sentence will be input as 1-hot vector of size \\(|\\mathcal{V}|\\) (where \\(\\mathcal V\\) is the vocabulary and \\(|V|\\) denotes the size).\n• None \\(h^{<t>} = \\sigma(W_{hh} h^{<t-1>} + W_{hx} x^{<t>})\\): the relationship to compute the hidden layer output features at each time-step \\(t\\).\n• None \\(W_{hx} \\in \\mathbb{R}^{D_h \\times |\\mathcal{V}| }\\): weight matrix used to condition the input vector, \\(x^{<t>}\\). The number of neurons in each layer is \\(D_h\\)\n• None \\(W_{hh} \\in \\mathbb{R}^{D_h \\times D_h}\\): weights matrix used to condition the output of the previous time-step, \\(h^{<t-1>}\\)\n• None \\(h^{<t-1>} \\in \\mathbb{R}^{D_h}\\): output of the non-linear function at the previous time-step, \\(t-1\\). \\(h^{<0>} \\in \\mathbb{R}^{D_h}\\) is an initialization vector for the hidden layer at time-step \\(t = 0\\).\n• None Each desired output \\(y^{<t>}\\) is a 1-hot vector of size \\(|\\mathcal{V}|\\) with 1 at index of \\(x^{<t+1>}\\) in \\(\\mathcal V\\) and 0 elsewhere.\n• None \\(W_{yh} \\in \\in \\mathbb{R}^{|\\mathcal{V}| \\times D_h}\\): weights matrix used to compute the prediction at each time \\(t\\). For example, \\(\\hat{y}^{<t>} = softmax (W_{yh}h_t)\\) and \\(\\hat{y} \\in \\mathbb{R}^{|\\mathcal{V}|}\\). Different tasks can be achieved using RNN:\n• None In theory for each time \\(t\\) can use information from many steps back\n• None Same weights applied on every timestep However RNN could be very slow to train and in practice it is difficult to access information from many steps back. Let consider training a RNN for language model example:\n• None We first have to get access to a big corpus of text which is a sequence of words \\(x^{<1>}, \\ldots, x^{<t-1>}, x^{<t>}, x^{<t+1>}, ... x^{<T>}\\)\n• None Forward pass into the RNN model and compute the output distribution \\(\\hat{y}^{<t>}\\) for every time \\(t\\) which represent the predict probability distribution of every word , given words so far\n• None Compute the Loss fuction on each step \\(t\\). In this case the cross-entropy between predicted probability distribution \\(\\hat{y}^{<t>}\\) and the true next word \\(y^{<t>}\\) (one-hot for \\(\\hat{x}^{<t+1>}\\)):\n• Average this to get overall loss for entire training set: Computiong the loss and the gradients accross entire corpus is too expensive. Then , in practice we use a batch of sentences to compute the loss and Stochastic Gradient Descent is exploited to compute the gradients for samll chunk of data, and then update. Let explicit the derivative of \\(L\\) w.r.t. the repeated weight matrix \\(W_{hh}\\). The backpropagation over time as two summations: \\[\\Large{\\boxed{ \\frac{\\partial L}{\\partial W_{hh}}=\\sum_{j=1}^{T} \\frac{\\partial L^{<j>}}{\\partial W_{hh}}}} \\] - Second summations showing that each \\(L^{<t>}\\) depends on the weight matrices before it: \\[\\Large{\\boxed{ \\frac{\\partial L^{<t>}}{\\partial W_{hh}}=\\sum_{i=1}^{t} \\frac{\\partial L^{<t>}}{\\partial W_{hh}}}} \\] Now using the chain rule differentiation over all hidden layers whitin the time interval \\([k,t]\\): where each \\(\\dfrac{\\partial h^{<j>}}{\\partial h^{<j-1>}}\\) is the Jacobian matrix for \\(h\\): Repeated matrix multiplication is required to get \\(\\dfrac{\\partial h^{<t>}}{\\partial h^{<k>}}\\). It leads to a vanishing or exploding gradients issues. Remind us that the RNN is based on this recursive relation \\[h^{<t>} = \\sigma(W_{hh} h^{<t-1>} + W_{hx} x^{<t>}).\\] Let consider the identidy function \\(\\sigma(x)=x\\), to help to understand why we have an issue with the computation of the gradient. Let consider the gradient of the loss \\(L^{<i>}\\) on time \\(i\\), with respect to the hidden state \\(h^{<j>}\\) on some previous time \\(j\\) and denote \\(\\ell=i-j\\). Then, \\[\\begin{eqnarray*} \\dfrac{\\partial L^{<i>}}{\\partial h^{<j>}}&=&\\dfrac{\\partial L^{<i>}}{\\partial h^{<i>}} \\prod_{j<t\\leq i}\\dfrac{\\partial h^{<t>}}{\\partial h^{<t-1>}}\\\\ &=&\\dfrac{\\partial L^{<i>}}{\\partial h^{<i>}} \\prod_{j<t\\leq i} W_{hh}=\\dfrac{\\partial L^{<i>}}{\\partial h^{<i>}} \\prod_{j<t\\leq i}W_{hh}^{\\ell} \\end{eqnarray*}\\] Then if \\(W_{hh}\\) is “small”, then this term gets exponentially problematic as \\(\\ell\\) becomes large. Indeed, if the weight matrix \\(W_{hh}\\) is diagonalizable: \\[W_{hh}=Q^{-1}\\times \\Delta \\times Q,\\] where \\(Q\\) is composed of the eigenvectors and \\(\\Delta\\) is a diagonal matrix with the eigenvalues on the diagonal. Computing the power of \\(W_{hh}\\) is then given by: Thus eigenvalues lower than 1 will lead to vanishing gradient while eigenvalues greater than 1 will lead to exploding gradient. Note that gradient signal from faraway is lost because it’s much smaller than gradient signal from close-by. Thus weights are updated only with respect to near effects, not long-term effects. For example in langage modelling, the contribution of faraway words to predicting the next word at time-step \\(t\\) diminishes when the gradient vanishes early on. The gradient can be viewed as a measure of the effect of the past on the future. Thus if gradient is small, the model cannot learn this dependency and the model unable to predict similar long-distance dependencies at test time. Exploding gradient is also a big issue for updating the weight and can cause too big step during the stochastic gradient descent. Moreover, once the gradient value grows extremely large, it causes an overflow (i.e. NaN). A solution to solve the problem of exploding gradients has been first introduced by Thomas Mikolov who proposed the Gradient clipping. The principle is to scale down the gradient before applying an update when the norm of the gradient is greater than some threshold.\n\nLSTM is used to take into account long-term dependencies and was introduced by Hochreiter and Schmidhuber in 1997 to offer a solution to the vanishing gradients problem. LSTM models make each node a more complex unit with gates controlling what information is passed through rather each node being just a simple RNN cell. At each time \\(t\\), LSTM provides a hidden state (\\(h^{(t)}\\)) and a cell state (\\(c^{(t)}\\)) which are both vectors of length \\(n\\). The cell has the ability to stores long-tern information. Further, the LSTM model can erase, write and read information from the cell. Gates are defined to get the ability to selection which information to either erased, written or read. The gates are also vectors of length \\(n\\). At each step \\(t\\) the gates can be open (1), closed (0) or somewhere in-between. Note that the gates are dynamic. From a sequence of inputs \\(x^{(t)}\\), LSTM computes a sequence of hidden states \\(h^{(t)}\\), and cell state \\(c^{(t)}\\):\n• controls what is kept versus forgotten, from previous cell state\n• controls what parts of the new cell content are written to cell\n• controls what parts of cell are output to hidden state\n• this is the new content to be written to the cell\n• erase (“forget”) some content from last cell state, and write (“input”) some new cell content\n• read (“output”) some content from the cell\n• None LSTM have been largely exploited for handwritting recognition, speech recognition, machine translation, parsing, image captioning.\n• None The architecture of LSTM model is especially designed to preserve information over many timesteps. Indeed, LST maintains a separate cell state from what of outputted.\n• None LSTM uses gates to control the flow of information\n• None Backpropagating from \\(c^{(t)}\\) to \\(c^{(t-1)}\\) is only element-wise multiplication by the \\(f\\) gate, and there is no matrix multiplication by \\(W\\). The \\(f\\) gate is different at every time step, ranged between 0 and 1 due to sigmoid property, thus it overcome the issue of multiplying the same thing over and over again.\n• None Backpropagating from \\(h^{(t)}\\) to \\(h^{(t-1)}\\) is going through only one single tanh nonlinearity rather than tanh for every single step.\n• None The Backpropagation through time with uninterrupted gradient flow helps for the vanishing gradient issue even if LSTM does not guarantee vanishing/exploding gradient issues.\n\nMachine translation (MT) is one of the main active research area in Natural Language Processing (NLP). The goal is to provide a fast and reliable computer program that translates a text in one language (source) into another language (the target) Using neural network model, the main architecture used for MT is the encoder–decoder model:\n• None encoder part which summarizes the information in the source sentence\n• None decoder part based on the encoding, generate the target-language output in a step-by-step fashion The encoder–decoder architecture has been introduced by Cho et al. (2014). A variant called sequence-to-sequence model has been introduced by Sutskever et al., (2014) Principle. In this model the encoder and decoder are both GRUs. The final state of the encoder is used as the summary \\(c\\) and then this summary is accessed by all steps in the decoder Principle. In this model the encoder and decoder are multilayered LSTMs. The final state of the encoder becomes the initial state of the decoder. So the source sentence has to be reversed. Limitation of these models are exposed in the paper “On the Properties of Neural Machine Translation: Encoder-Decoder Approaches”. They showed that the performance of the encoder-decoder network degrades rapidly as the length of the input sentence increases. The main drawback of the previous models is that the encoded vector need to capture the entire phrase (sentence) and might skipped many important details. Moreover the information needs to “flow” through many RNN steps which is quite difficult for long sentence. Bahdanau et al. (2015) have proposed to include attention layer which consist to include attention mechanisms to give more importance to some of the input words compared to others while translating the sentence. Figure 8.18: The encoder-decoder model with additive attention (see Bahdanau et al. (2015)) A survey of the different implementations of attention model is presented by Galassi et al. (2019)"
    },
    {
        "link": "https://medium.com/towards-data-science/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d",
        "document": "If we take a high-level view, a seq2seq model has encoder, decoder and intermediate step as its main components:\n\nWe use embedding, so we have to first compile a “vocabulary” list containing all the words we want our model to be able to use or read. The model inputs will have to be tensors containing the IDs of the words in the sequence.\n\nThere are four symbols, however, that we need our vocabulary to contain. Seq2seq vocabularies usually reserve the first four spots for these elements:\n• <PAD>: During training, we’ll need to feed our examples to the network in batches. The inputs in these batches all need to be the same width for the network to do its calculation. Our examples, however, are not of the same length. That’s why we’ll need to pad shorter inputs to bring them to the same width of the batch\n• <EOS>: This is another necessity of batching as well, but more on the decoder side. It allows us to tell the decoder where a sentence ends, and it allows the decoder to indicate the same thing in its outputs as well.\n• <UNK>: If you’re training your model on real data, you’ll find you can…"
    }
]