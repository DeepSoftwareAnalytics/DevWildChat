[
    {
        "link": "https://metadesignsolutions.com/concurrency-and-multithreading-in-java-best-practices-and-pitfalls",
        "document": "Java is widely used for developing high-performance applications that require efficient handling of multiple tasks. With the increasing demand for faster and more scalable applications, Java concurrency and multithreading have become critical concepts for Java developers.\n\nCompanies providing Java development services focus on writing efficient multithreaded code to improve application responsiveness, performance, and resource utilization. However, writing concurrent programs in Java can lead to several challenges, such as race conditions, deadlocks, and thread starvation.\n\nIf you want to hire Java developers who can handle concurrency efficiently, they must follow best practices while avoiding common pitfalls. In this article, we will discuss concurrency and multithreading in Java, covering best practices and pitfalls that every Java development company should be aware of.\n\nWhat is Concurrency in Java?\n\nConcurrency in Java refers to executing multiple tasks simultaneously to improve an application’s efficiency. Instead of processing tasks sequentially, concurrency enables better CPU utilization by running independent tasks concurrently.\n\nWhat is Multithreading in Java?\n\nMultithreading is a subset of concurrency where multiple threads execute independently within a single process. It allows tasks to run in parallel, improving performance and responsiveness in Java applications.\n\nJava provides built-in support for multithreading and concurrency through classes like Thread, Runnable, and ExecutorService. Many Java development companies rely on these features to build scalable applications.\n\nWhy Use Concurrency and Multithreading in Java?\n\nMultithreading allows multiple tasks to execute in parallel, reducing response time. For example, in a web application, one thread can handle user requests while another processes background tasks.\n\nModern processors have multiple cores, and multithreading ensures that all available CPU resources are utilized efficiently.\n\nConcurrent programming allows independent tasks, such as database queries and file operations, to run simultaneously, reducing bottlenecks.\n\nApplications handling large workloads can scale better by leveraging concurrency and multithreading techniques.\n\nBest Practices for Concurrency and Multithreading in Java\n\n1. Use the Right Thread Pool\n\nCreating a new thread for every task is inefficient and can lead to excessive resource consumption. Instead, use thread pools to manage threads efficiently."
    },
    {
        "link": "https://medium.com/@AlexanderObregon/java-multithreading-and-concurrency-best-practices-39c86602e666",
        "document": "In computing, performance and efficiency are paramount. Java, one of the most widely used programming languages, offers strong features for multithreading and concurrency. Multithreading allows a program to operate more efficiently by performing multiple tasks simultaneously. However, with great power comes great responsibility. Properly managing threads and concurrency in Java is crucial for building efficient, error-free, and scalable applications.\n\nIn Java, a thread is a lightweight sub-process, the smallest unit of processing. It’s a path of execution within a program. The Java Virtual Machine (JVM) allows an application to have multiple threads running concurrently, each performing different tasks. This simultaneous execution of multiple threads can improve the performance of applications.\n\nThreads share the same process’s memory and resources but execute independently. This enables efficient use of CPU resources, as threads can be executed in parallel on multicore processors.\n• Improved Application Performance: By allowing multiple operations to run concurrently, threads can significantly improve the performance of high-load applications.\n• Better Resource Utilization: Threads utilize the CPU more efficiently by performing background operations while the main thread continues its primary tasks.\n• Asynchronous Behavior: Threads enable asynchronous processing, where a task can run in the background and notify the main thread upon completion.\n\nUnderstanding the thread lifecycle is crucial for proper thread management. A thread in Java goes through several states:\n• New: When a thread instance is created, it’s in the new state.\n• Runnable: Once the method is invoked, the thread enters the runnable state, where it's ready to run and waiting for CPU allocation.\n• Running: The thread is currently executing.\n• Blocked/Waiting: The thread might be waiting for resources or for another thread to perform a task.\n• Terminated: The thread’s run method completes, and the thread is terminated.\n\nAs previously mentioned, there are two primary ways to create a thread in Java: extending the class and implementing the interface. Here’s a deeper look:\n• Extending the Thread Class: When you extend the class, your class inherits all the thread properties and methods. It's a straightforward approach but less flexible as it doesn't allow extending any other class.\n• Implementing the Runnable Interface: Implementing the interface is more flexible. It allows your class to extend other classes. Also, it's a better choice for objects that perform complex operations and need to extend other classes.\n\nOnce a thread is created, it doesn’t start running immediately. To start the thread, you need to call its method. This method is a request to the JVM to perform a system-specific procedure to start a new thread.\n\nJava provides a mechanism for prioritizing threads. You can set the priority of each thread relative to others using the method. The JVM tries to schedule threads according to their priorities as much as possible.\n\nWhile powerful, threads come with challenges:\n• Thread Safety: When threads share resources, ensuring that they don’t interfere with each other is crucial.\n• Deadlocks: A deadlock can occur when two or more threads are waiting indefinitely for each other’s resources.\n• Resource Management: Creating too many threads can lead to excessive consumption of memory and processing power, slowing down the entire system.\n\nSynchronization in Java is a important concept for ensuring that multiple threads can work together without interfering with each other’s operations, especially when they access shared resources. The primary purpose of synchronization is to avoid thread interference and memory consistency errors.\n• Thread Interference: Occurs when multiple threads try to access and modify the same resource simultaneously, leading to inconsistent or erroneous results.\n• Memory Consistency Errors: These errors happen when different threads have inconsistent views of what should be the same data, leading to unpredictable results.\n\nTo avoid these issues, Java provides synchronized methods:\n• Method-Level Synchronization: By using the keyword with a method, you ensure that only one thread can access this method at a time. It's a simple way to prevent thread interference.\n• Object Locks: When a thread enters a synchronized method, it acquires an intrinsic lock (or monitor lock) for that method’s object. Other threads attempting to enter any of the object’s synchronized methods are blocked until the first thread exits the synchronized method.\n• Block-Level Synchronization: Instead of locking the entire method, Java allows you to synchronize a block of statements within a method. This approach provides better performance as it reduces the duration for which the lock is held.\n• Specific Object Locks: Synchronized blocks can lock on any object, not just . This flexibility allows finer control over which resources are locked.\n• Intrinsic Locks: In Java, every object comes with an intrinsic lock. When a thread acquires an intrinsic lock, no other thread can acquire the same lock until it’s released.\n• Reentrant Synchronization: Java’s intrinsic locks are reentrant. If a thread already holds a certain lock, it can enter another block of code synchronized on the same lock without any issues.\n• Memory Visibility: To ensure that changes made by one thread to a shared variable are visible to other threads, Java introduces the keyword. A variable's value is always read from and written to the main memory, ensuring that every thread sees the latest value.\n• Thread-Safe Class Design: A class is considered thread-safe if it behaves correctly when accessed from multiple threads, regardless of the scheduling or interleaving of the execution by the runtime.\n• Immutable Objects: One of the simplest ways to achieve thread safety is by designing immutable classes. An immutable object, once created, cannot be modified, thus inherently thread-safe.\n• Deadlocks: A deadlock can occur when two or more threads are blocked forever, each waiting for the other’s lock.\n• Performance Impact: Excessive synchronization can lead to performance issues, as it forces threads to spend time waiting to acquire locks.\n\nThe Java Concurrency API includes a set of thread-safe collection classes, which are designed to improve scalability and performance when dealing with concurrent access.\n• ConcurrentHashMap: Perhaps the most commonly used, is a thread-safe variant of . Unlike or synchronized wrappers of , it allows concurrent read access and maintains performance by segmenting the map.\n• CopyOnWriteArrayList: This is a thread-safe variant of . It's particularly useful in scenarios where you iterate over the list more often than you modify it. On each modification, it creates a fresh copy of the underlying array.\n• Blocking Queues: Classes like and implement the interface, which is designed to handle typical producer-consumer scenarios.\n\nManaging threads manually (creating, starting, and coordinating their lifecycle) can be complex and error-prone. The Executor framework abstracts the creation and management of threads, providing a powerful toolbox for asynchronous processing.\n• ExecutorService: It provides methods to manage termination and methods that can produce a for tracking progress of one or more asynchronous tasks.\n• ScheduledExecutorService: This is used to schedule tasks with a delay or to execute repeatedly with a fixed interval.\n\nWhile tasks can be executed by executors, they don't return a result. and come into play when you need to get results from your threads.\n• Callable Interface: Unlike , a can return a value and throw checked exceptions. It represents a task that returns a result.\n• Future Interface: A represents the result of an asynchronous computation. Methods are provided to check if the computation is complete, to wait for its completion, and to retrieve the result.\n\nIntroduced in Java 7, the Fork/Join framework is designed to recursively divide parallelizable tasks into smaller tasks and then combine the results of these subtasks.\n• Work-Stealing: This framework follows the work-stealing algorithm where idle threads can ‘steal’ work from those that are busy.\n• RecursiveTask: A subclass of , can return a result and is useful for tasks that can be broken down into smaller, independent subtasks.\n\nWhile these tools significantly ease the handling of concurrency in Java, they come with their own set of challenges:\n• Complexity in Debugging: Debugging concurrent programs can be more complex due to the non-deterministic nature of thread execution.\n• Resource Management: Properly managing resources and avoiding excessive thread creation is vital to maintain system performance.\n\nUnderstanding Java’s multithreading and concurrency is essential for building efficient and scalable applications. The key is to balance performance gains with the complexities of thread management. By adhering to best practices such as thread safety, effective use of synchronization, and leveraging advanced concurrency tools like Executor frameworks and concurrent collections, developers can harness the full potential of Java’s concurrency model. Remember, the goal is not only to improve application performance but also to maintain code quality and reliability in a concurrent environment."
    },
    {
        "link": "https://stackoverflow.com/questions/66611118/what-is-best-way-of-implementing-multithreading-in-java",
        "document": "I have been following several YouTube demos and tutorials on implementing multi-threaded operations in Java. However, all the tutorials show this following procedure:\n\nSome demos extends Thread class instead of Runnable, however, they follow a similar concept.\n\nHowever, one problem that I reckon is that what if I want to have multiple logics in a class that I want to run concurrently, then, I have a issue. Well, Java veterans may know some trick to do so. However, I tried to implement such logic in a different way. Here is the code that I wrote:\n\nNow, I can launch several threads and call the particular method that I want run concurrently.\n\nNow, I want to know is there any benefit of implementing multi threaded operation using the first approach and is there any shortcomings of the latter one?"
    },
    {
        "link": "https://stackoverflow.com/questions/19512867/what-is-the-correct-way-to-create-threads-in-an-enterprise-java-application",
        "document": "The correct way to do this is by using the Concurrency Utils API, which is part of the Java EE7 release. Creating threads this way ensures that the thread has access to all the other enterprise services. Using the Concurrency Utils ensures that your thread is created and managed by the container.\n\nPlease see here and here for examples"
    },
    {
        "link": "https://alibabacloud.com/blog/java-thread-pool-implementation-and-best-practices-in-business-applications_601528",
        "document": "The article discusses the implementation principles and source code analysis of Java thread pools, as well as best practices for using thread pools in business applications.\n\nA thread pool is a mechanism for managing and reusing threads.\n\nThe core idea of a thread pool is to pre-create a certain number of threads and keep them in the pool. When a task needs to be executed, the thread pool takes an idle thread from the pool to execute the task. Once the task is completed, the thread is not destroyed but returned to the thread pool, ready to be used immediately or later for executing other tasks. This mechanism can avoid the performance overhead caused by frequently creating and destroying threads, while also controlling the number of concurrently running threads, thereby improving system performance and resource utilization.\n\nThe main components of the thread pool include worker threads, task queues, thread managers, and so on. The design of thread pools helps to optimize the performance and resource utilization of multi-threaded programs, while also simplifying the complexity of thread management and reuse.\n\n2. What Are the Benefits of Thread Pools?\n• A thread pool can reduce the overhead of thread creation and destruction. Creating and destroying threads consumes system resources, but by reusing threads, a thread pool avoids frequent resource operations, thereby enhancing system performance.\n• A thread pool controls and optimizes system resource utilization. By controlling the number of threads, it maximizes machine performance and improves the efficiency of resource usage.\n• A thread pool can improve response time. By pre-creating threads and handling tasks concurrently with multi-threading, a thread pool enhances task response speed and system concurrency performance.\n\nThe core implementation class of the Java thread pool is ThreadPoolExecutor, and its class inheritance relationship is illustrated in the figure, in which the core method is as follows:\n\nPart of the core method of ThreadPoolExecutor\n\n2. The status of the thread pool\n• RUNNING: Once the thread pool is created, it is in the RUNNING state, the number of tasks is 0, and it can receive new tasks and process queued tasks.\n• SHUTDOWN: It can not receive new tasks but can process queued tasks. When the shutdown() method of the thread pool is called, the status of the thread pool changes from RUNNING to SHUTDOWN.\n• STOP: It can not receive new tasks, process queued tasks, but can interrupt tasks that are being processed. When the shutdownNow() method of the thread pool is called, the status of the thread pool changes from RUNNING or SHUTDOWN to STOP.\n• TIDYING: When the thread pool is in the SHUTDOWN state, the task queue is empty and the task in execution is empty. When the thread pool is in the STOP state and the task in execution in the thread pool is empty, the status of the thread pool changes to the TIDYING state and the terminated() method will be executed. This method is an empty implementation in the thread pool and can be overridden to handle it accordingly.\n• TERMINATED: The thread pool is terminated completely. After the terminated() method is executed in the TIDYING state, the status of the thread pool changes from TIDYING to TERMINATED.\n• Can the core threads of the thread pool be recycled?\n\nThe ThreadPoolExecutor does not recycle the core thread by default but provides an allowCoreThreadTimeOut(boolean value) method. When the parameter is true, you can recycle the core thread after reaching the thread idle time. In the business code, if the thread pool is used periodically, you can consider setting this parameter to true.\n• Can I create a thread in the thread pool before I submit a task?\n\nprestartCoreThread(): Start a thread and wait for the task. If the number of core threads has been reached, this method returns false, otherwise, it returns true.\n\nprestartAllCoreThreads(): All core threads are started, and the number of successfully started core threads is returned.\n\nWith this setting, the creation of the core thread can be completed before the task is submitted, thus realizing the effect of preheating the thread pool.\n\nFirst, the ctl is obtained. The ctl consists of 32 bits. The upper 3 bits record the status of the thread pool, and the lower 29 bits record the number of working threads in the thread pool. After obtaining the ctl, it determines whether the current number of working threads is less than the number of core threads. If it is less than the number of core threads, it creates a core thread to execute tasks. Otherwise, it attempts to add tasks to create non-core threads.\n\nIn the double-layer endless loop, ctl is still obtained to check the status of the current thread pool. After the check is passed, cas will be tried to increase the number of working threads in the inner endless loop. Only if the increase is successful can cas jump out of the outer for loop and actually start to create threads.\n\nTo create a thread, you must lock the thread to ensure safe concurrency. The Worker class is used to encapsulate the Thread object in the thread pool. When the thread pool is running or in the shutdown state, you can create a thread and execute tasks in the blocking queue.\n\nAfter the thread is successfully created and added to the thread pool, the start() method is called to start the thread and execute the task.\n\nIn the runWorker() method, the first task is initially the one encapsulated in the Worker object, and then, within a while() loop, tasks are continuously fetched from the blocking queue for execution, achieving thread reuse. One detail to note is that the source code directly throws exceptions from the task.run() method without catching them.\n\nCheck the status of the thread pool. If it is not in a running state, and if it is not in shutdown and the blocking queue is not empty, decrement the number of worker threads by 1 and return null. Note that this only decrements the number of worker threads and does not actually destroy the thread; the logic for destroying threads is handled in processWorkerExit().\n\nProvide two blocking methods to obtain the tasks in the blocking queue, depending on whether timeout control is required.\n\nWhen a thread encounters an exception during execution or fails to retrieve a blocking task, it will enter this method.\n\n4. Best Practices for Using Thread Pools in Business Applications\n\n1. How to Select the Appropriate Thread Pool Parameters\n\nIt is recommended to use a custom thread factory that overrides the thread creation method, allowing for customization of thread names, priorities, and other attributes. This facilitates easier troubleshooting.\n• How do I select the appropriate thread pool parameters?\n\n1) Select according to the task scenarios\n\nCPU-intensive tasks (N +1): These tasks consume CPU resources. You can set the number of threads to N (number of CPU cores) +1. An extra thread beyond the number of CPU cores is used to prevent the impact of occasional page faults or other issues that might cause task interruptions. Once a task is paused, the CPU will be idle. In such cases, the extra thread can make full use of the CPU's idle time.\n\nI/O intensive task (2N): In such tasks, the system spends most of its time handling I/O interactions. During the periods when a thread is handling I/O, it does not occupy the CPU. This allows the CPU to be allocated to other threads during these times. Therefore, for I/O-intensive tasks, you can configure additional threads, with a specific calculation method being 2N.\n\n2) Select according to the purposes of the thread pool\n\nFor example, when a user queries a product detail page, it involves retrieving a series of related information such as price, discounts, inventory, and basic details. From a user experience perspective, it's desirable to minimize the response time of the product detail page. In this case, you can consider using a thread pool to concurrently query information such as price, discounts, and inventory, and then aggregate the results before returning them, thereby reducing the overall API response time. In this case, where the purpose of the thread pool is to achieve the fastest response time, you might consider not setting a queue to buffer concurrent tasks. Instead, configure a larger corePoolSize and maxPoolSize to handle as many tasks concurrently as possible.\n\nFor example, when synchronizing product supply with channels in a project, where a large volume of product data needs to be queried and synchronized with the channels, you might consider using a thread pool to efficiently handle the batch tasks. The purpose of this thread pool is to use limited machine resources to process as many tasks as possible per unit time and improve system throughput. Therefore, it is necessary to set blocking queue buffer tasks and adjust the appropriate corePoolSize according to the task scenario.\n\nUse Executors to create a specific thread pool. The thread pool parameters are relatively fixed and are not recommended.\n\nExecutors is a utility class in a java.util.concurrent package that makes it easy for us to create several thread pools with specific parameters.\n• CachedThreadPool: the number of threads can be dynamically scaled. The maximum number of threads is Integer.MAX_VALUE.\n• SingleThreadPool: the thread of a single thread, the number of core threads, and the maximum number of threads are both 1, unbounded blocking queue.\n\n...\n\nIt is recommended to use the hungry singleton pattern to create a thread pool object. It supports flexible parameter configuration, creates the thread pool object during class loading, and only instantiates a single object. It then encapsulates a unified method for obtaining the thread pool object, exposing it for use in business code. See the example code below:\n• Can a thread pool object defined by a local variable be collected as garbage after the method ends?\n\nTo begin with the conclusion, in the code shown above, obj is a local variable defined within the test1() method. Normally, local variables are stored on the stack. When the method ends, the stack frame is popped, and the local variables within that stack frame are also destroyed. At this point, no variables point to the new Object() instance in the heap, so the heap object created by new Object() can be garbage collected. Similarly, executorService is also a local variable defined within the method, but after the method ends, active threads still exist in the thread pool. According to the GC Roots reachability analysis principle, objects that can serve as GC Roots include:\n• Objects referenced in the virtual machine stack (local variable table in the stack frame).\n• The object referenced by the class static property in the method area.\n• The object referenced by the constant in the method area.\n• The object referenced by the native JNI method in the native stack.\n\nTherefore, after the test2() method completes, the threads in the thread pool will enter a blocked state in getTask(), but they remain active threads. At this point, the thread pool object in the heap is still reachable through GC Roots, so it will not be garbage collected.\n\nTo prove the above conclusion, we only need to prove that the running thread object holds a reference to the thread pool object. In source code parsing, we know that the threads in the thread pool are encapsulated by Worker objects, so we only need to find the reference relationship between Worker and ThreadPoolExecutor. In the source code, Worker is an inner class in ThreadPoolExecutor class.\n\nHowever, we do not see that the Worker class directly references the external thread pool object. Could it be that if the thread pool object is unreachable through GC Roots, it can be garbage collected? However, if the thread pool is garbage collected while the threads within it are still alive, this is quite contradictory.\n\nIn fact, this issue involves the reference relationship between Java's inner and outer classes. You can refer to the following demo:\n\nIn a non-static inner class, the method of the outer class can be directly called without instantiating the outer class object. The reason is that in Java, the non-static inner class will hold a reference to the outer class. Javac decompilation can be used to verify this conclusion. In the Outer$Inner.class file generated by decompilation, the outer class will be used as the parameter of the non-static inner class construction method. That is, the non-static inner class will hold a reference to the outer class,\n\nWhy emphasize non-static inner classes? Because static inner classes do not hold a reference to the outer class.\n\nThe javac decompiled Outer$Inner.class file does not reference the outer class\n\nThis issue leads to two insights:\n\n1) Avoid defining thread pool objects as local variables in your code. This not only leads to the frequent creation of thread pool objects, which goes against the design principle of thread reuse but also potentially causes memory leaks if the local variable's thread pool object cannot be garbage collected in a timely manner.\n\n2) In the business code, the priority is to define static inner classes rather than non-static inner classes, and can effectively prevent memory leakage risk.\n\n3. Interdependent Subtasks Avoid Using the Same Thread Pool\n\nInterdependent tasks are submitted to the same thread pool. The parent task depends on the execution result of the subtask. When the parent task get() executes the result, the thread may be blocked because the subtask has not been completed. If there are too many submitted tasks, the threads in the thread pool are occupied and blocked by similar parent tasks, resulting in no threads to execute the subtasks in the task queue, resulting in a deadlock of thread starvation.\n• Use different thread pools to isolate tasks that are interdependent.\n• Call the future.get() method to set the timeout period. This prevents thread blocking but still causes a large number of timeout exceptions.\n• execute(Runnable r): No return value, just submit a task to the thread pool for processing, lightweight method, suitable for processing tasks that do not need to return results.\n• submit(Runnable r): The return value is Future type. The future can be used to check whether the task has been completed and obtain the result of the task. It is suitable for tasks that need to process the returned result.\n\nFor example, the following code is used to push pricing information for calendar rooms to third-party channels. Due to the extensive range of calendar room supply, thread pool technology is employed to improve the efficiency of the pricing push. Additionally, since the entire push task takes a considerable amount of time, and to prevent task interruption, it is necessary to record the execution progress of the push task and implement a \"resume from breakpoint\" feature. To achieve this, the submit() method is used to submit subtasks, and then the execution results of all subtasks are blocked and retrieved to update the progress of the push task.\n\n5. Please Capture the Code Exception of the Subtask in the Thread Pool\n• If the thread executing the task in the thread pool is abnormal, will the abnormal thread be destroyed? Can other tasks be performed normally?\n\nThe preceding code execution result is shown in the following figure:\n\nWhat can be found is that:\n\n1) The thread that executes the task in the thread pool is abnormal but does not affect the execution of other tasks. In addition, execute() submits the task and directly prints the exception information. If you use submit() to submit a task, what will the console output? Students who are interested in this can explore it.\n\n2) Note that the printed thread names are 1,2,5,3. Why is there no printed thread name 4?\n\nFirst of all, we can confirm that a new thread 5 was created after the thread exception. As for how thread 3 is handled and why thread 4 is not printed, we can analyze it from the source code. After executing the task.run() method, if there is an exception, the runWorker() method will throw an exception and enter three finally code blocks in turn.\n\nIn the processWorkerExit(w, completedAbruptly) method, you can see that if the running thread pool has a thread execution exception, it will call workers.remove() to remove the current thread and call addWorker() to recreate a new thread.\n\nTherefore, the two actions of destroying the thread and recreating the thread in task 3 and creating the thread in task 4 have timing issues. For more information, see the following figure:\n\nSo where does the abnormal information that controls printing come from?\n\nAfter runWorker() is executed, since the exception executed by task.run() is not caught, the jvm calls back java.lang.Thread#dispatchUncaughtException method will eventually process the uncaught exception information in the thread pool and finally call the java.lang.ThreadGroup#uncaughtException. As you can see, we are supported here to customize the uncaught exception handler UncaughtExceptionHandler, otherwise, the exception information will be printed directly by default.\n\nTherefore, in the business code, please catch exceptions in subtasks. Otherwise, worker threads in the thread pool will be frequently destroyed and created, resulting in a waste of resources and violating the design principle of thread reuse.\n\nDisclaimer: The views expressed herein are for reference only and don't necessarily represent the official views of Alibaba Cloud."
    },
    {
        "link": "https://redis.io/solutions/caching",
        "document": ""
    },
    {
        "link": "https://redis.io/blog/what-is-enterprise-caching",
        "document": ""
    },
    {
        "link": "https://medium.com/factset/multi-layered-caching-strategies-4427025cae6e",
        "document": "Using Redis to Supercharge Your Data Delivery\n\nIf you are looking to improve performance as a software developer, caching is one of the best paths forward. Caching hastens the delivery of content using previously saved information, and there are many ways to take advantage of caching strategies. To deliver the best performance, a multi-layered caching architecture can smooth out variability in client latency or backend service response times. The term multi-layered caching architecture refers to the practice of utilizing multiple caching strategies that, when combined, offer the greatest performance improvement. For example, web browsers provide one form of caching when building web applications but combining browser-based caching with server and/or data layer caching will deliver better performance than any individual technique.\n\nBelow is a nonexhaustive list of potential locations where a development team may leverage caching today. The diagram’s scale represents the proximity of the cached information to the end user’s machine, with the ones on the left being on the end user’s machine itself and the one to the right located at the source data system. If the flow of requests travels from the user to the data source, and back to the user, a cache at any point allows for information to be returned early, delivering more timely results to the end user.\n\nLet us focus on the server-adjacent layer and dive a bit deeper into one caching technology, Redis. Redis is an open-source, in-memory, data storage system that lends itself well to a caching solution. Redis offers flexible installation mechanisms including manual installation, Docker instructions, AWS via Elasticache, Azure via Azure Cache for Redis, or Google Cloud via Memorystore. The popularity of Redis ensures that language SDKs are readily available and easy to leverage in any project. The command documentation available on Redis’ website is comprised of easy to digest information along with code examples which can be manipulated directly within the documentation.\n\nAt FactSet, our applications cater to financial professionals whose workflows require answers and insight as fast as possible to ensure timely responses to their clients. In line with the theme of multi-layered caching strategies, our applications take advantage of multiple caching strategies to ensure optimal performance of our solutions. To supplement caching in one area of our product, Redis was introduced to provide low latency responses for data which does not change frequently.\n\nWhen researching a particular financial instrument, FactSet needs to determine what content sets contain relevant data and information. Due to the breadth of the content sets available on our platform, the identification of relevant content sources may involve making many network requests for different services. Making dozens of calls delays the user from obtaining critical information. While any individual call may be fast, in aggregate they could take multiple seconds and form a bottleneck, degrading user experience. Enter Redis, which can help reduce data fetching frequency and smooth out the overall response times.\n\nIn our application architecture, requests to obtain the universe of content are routed through a middle tier layer. This layer provides the perfect opportunity to attach a server-adjacent cache that collects information for faster future delivery.\n\nOur application layer makes a request to the middle -tier to obtain the content universe. The middle tier quickly checks Redis for a cached result that can be instantly delivered. If the cache does not have an answer for the request, the middle tier issues requests to the various backends to collect the content universe. Returned universe data is added to the cache with an appropriate expiration time to ensure the information does not become stale. Subsequent requests to the middle -tier to obtain the universe for the same instrument can instantly return the response from the cache, saving precious time.\n\nRedis can facilitate a caching workflow in several ways and a combination of storage structures, rather than a singular approach, optimizes performance. To store the raw data model, we leverage SET/GET calls to hold onto JSON based for a given key. For the code examples that you will see, they will contain a mix of JavaScript and Redis CLI commands to avoid promoting any specific Redis JS SDK library.\n\nIf you will be setting multiple key/value pairs in the cache at one time, make sure to leverage a pipeline construct within the SDK you are utilizing. The pipeline will avoid round trips to Redis for each operation by sending the workload in bulk.\n\nAt this point, we have a basic caching structure for JSON content that we could retrieve via GET operations.\n\nTo avoid an expensive SCAN operation to gather insights about the entire cache, we also collect information relating to usage within a sorted set structure. The usage information benefits other workflows. The usage information is important as a seed for background jobs that keep the cache up to date. The sorted set is maintained via Redis’ ZINCRBY command. This command simply increments a numerical value for a given “member” of the set. As our middle tier layer receives requests for a given instrument, we increment usage with one simple command:\n\nRedis gracefully handles potential edge cases, such as if the member does not exist in the sorted set already. This sorted set builds up usage/hits as requests come through the middle -tier. Unlike the general SET command, the sorted set does not have a built-in mechanism to automatically expire the data. While it may be valuable to know what the usage may be over a particular date range, say one week or one month, it would not be too useful to hold indefinitely. This brings us to an accessory to the cache mentioned briefly above, background jobs.\n\nGoing beyond with automation\n\nNow that we have our basic cache in place, it is time to pair the cache with a background process that keeps it updated, thus extending its life. One downside to the basic implementation above is that we had defined the expiration time for any instrument in the cache to be one day. Considering a cache which expires every day, let us say that the following day every user accesses a symbol from yesterday which is no longer cached during their first several hits to the application. If the user never makes it past these first several hits, they may feel that the product is always slow as they were never able to experience the power of the cache.\n\nTo solve this challenge, we extend the initial approach by leveraging multiple sorted sets. These extra sorted sets help define weighted usage over a given date range. We combine the weighted usage to build a rolling picture of the popularity of instruments which helps feed the background job or other systems.\n\nLet us continue with the daily theme and extend the sorted set storage into one table per day for an entire week. This means we would create seven sorted sets in Redis to store seven rolling days of traffic. We then need to adjust the usage incrementation logic to incorporate the current day, from the server’s perspective, that the usage relates to.\n\nOnce this code has been in place for at least a week. We have seven sorted sets collecting usage information. Since these are unique sets, we would need to combine them in some way to get the complete picture of the most popular symbols over the rolling week. Redis makes this easy with a command called ZUNION, or ZUNIONSTORE. This command can be slow since it needs to scan every set being merged. We will come back to these commands in a moment but, to overcome the need to run ZUNION frequently, we can materialize the union and keep it updated as we track usage from our middle tier. We achieved this with the creation of one more sorted set that contains the rolling seven-day union. This union set will store a weighted value of the usage to help us monitor any shift in popularity of the instruments.\n\nIn the snippet above, we start to introduce some weightings to the stored usage information. For the ZUNION function, weightings can be applied to each set during the merger to augment the resulting set’s rankings. The perfect weighted ranking algorithm involves a great deal of complexity, but we can keep this simple by assigning the current day a weight of 10 points and each subsequent day further back in history a weighting of one less until we reach four points for the weight six days ago in this rolling seven-day scenario.\n\nNow, onto the scheduled job. This daily operation is responsible for three main tasks. First it will obtain the weighted usage across all sorted sets. Then it will clear the current day’s usage set to provide a fresh location to gather data for today. Lastly, it will process over the instruments in the sorted union and refresh the cached JSON data. For the first piece, obtaining the weighted usage, we utilize the ZUNIONSTORE command to compile the combined usage and store it in the union set shown above.\n\nThis command is a bit wordy, but it takes in N-number of sorted sets with N-number of weighting values to apply and combines the data into one output set. After this command has run, we have an up-to-date union table that represents weighted rankings for all the traffic over the past seven days. We can use this information to sequence a scheduled job more accurately such that it processes instruments in order of popularity when refreshing the cached JSON. The introduction of the scheduled job allows us to extend the expiration time for any given instrument from one to seven days. With a cache that now lasts for a week, we would no longer have the scenario where users are accessing instruments from the previous day which are not in the cache. The existence of the scheduled job allows for data to remain cached for a much longer period, providing the greatest potential for cache hits and near instantaneous responses.\n\nWhen we first started this project, we encountered lookup times within the range of seconds. We needed to significantly reduce that response time to improve the user experience. We have found a massive reduction with the introduction of this cache. Excluding latency and focusing on pure server execution and lookup/processing times, here are the results for various percentiles:\n\nThis implementation helped us significantly improve performance within a key area of FactSet with low to moderate engineering and maintenance effort. A solution such as the one described here can buy engineering teams more time to make larger improvements to a data model or system. Out of the box approaches and implementations of server-adjacent caching systems can supercharge response delivery. Despite the significant improvements that we achieved; we can continue to expand upon the optimizations within the server-adjacent caching layer of our applications."
    },
    {
        "link": "https://docs.aws.amazon.com/whitepapers/latest/database-caching-strategies-using-redis/caching-patterns.html",
        "document": "When you are caching data from your database, there are caching patterns for Redis and Memcached that you can implement, including proactive and reactive approaches. The patterns you choose to implement should be directly related to your caching and application objectives.\n\nTwo common approaches are cache-aside or lazy loading (a reactive approach) and write-through (a proactive approach). A cache-aside cache is updated after the data is requested. A write-through cache is updated immediately when the primary database is updated. With both approaches, the application is essentially managing what data is being cached and for how long.\n\nThe following diagram is a typical representation of an architecture that uses a remote distributed cache .\n\nA cache-aside cache is the most common caching strategy available. The fundamental data retrieval logic can be summarized as follows:\n\nThis approach has a couple of advantages:\n\nA disadvantage when using cache-aside as the only caching pattern is that because the data is loaded into the cache only after a cache miss, some overhead is added to the initial response time because additional roundtrips to the cache and database are needed.\n\nA write-through cache reverses the order of how the cache is populated. Instead of lazy-loading the data in the cache after a cache miss, the cache is proactively updated immediately following the primary database update. The fundamental data retrieval logic can be summarized as follows:\n• The application, batch, or backend process updates the primary database.\n• Immediately afterward, the data is also updated in the cache. The write-through pattern is almost always implemented along with lazy loading. If the application gets a cache miss because the data is not present or has expired, the lazy loading pattern is performed to update the cache. The write-through approach has a couple of advantages:\n• Because the cache is up-to-date with the primary database, there is a much greater likelihood that the data will be found in the cache. This, in turn, results in better overall application performance and user experience.\n• The performance of your database is optimal because fewer database reads are performed. A disadvantage of the write-through approach is that infrequently-requested data is also written to the cache, resulting in a larger and more expensive cache. A proper caching strategy includes effective use of both write-through and lazy loading of your data and setting an appropriate expiration for the data to keep it relevant and lean."
    },
    {
        "link": "https://codedamn.com/news/backend/advanced-redis-caching-techniques",
        "document": "Redis is an open-source, in-memory data structure store used as a database, cache, and message broker. It is well known for its performance and flexibility, which makes it an ideal choice for implementing caching strategies. In this blog post, we will explore advanced Redis caching techniques that will help you optimize your application's performance. You will learn about various caching patterns, how to handle cache eviction, and how to use Lua scripting for more advanced use cases. We will also provide code examples and detailed explanations to ensure that even beginners can follow along and implement these techniques in their applications.\n\nCaching is a technique that stores data in a fast-access medium, such as RAM, to reduce the time taken to access the data. Redis is particularly well-suited for caching because of its in-memory storage capabilities and its support for various data structures. In this section, we will explore different caching patterns that you can use in Redis.\n\nThe cache-aside pattern is one of the most commonly used caching patterns. In this pattern, the application first checks if the data is available in the cache. If the data is not present in the cache, the application retrieves the data from the primary data store and updates the cache before returning the data to the caller.\n\nHere's an example using Redis and Python:\n\nIn the read-through pattern, the cache is responsible for fetching the data from the primary data store when a cache miss occurs. This pattern simplifies the application code, as the application only interacts with the cache and does not need to handle cache misses explicitly.\n\nTo implement the read-through pattern in Redis, you can use a custom cache implementation that fetches the data from the primary data store when necessary:\n\nThe write-through pattern ensures that the cache is always up-to-date by updating the cache whenever the primary data store is updated. In this pattern, the application writes data to both the cache and the primary data store. This ensures that the cache always contains the latest data, minimizing the chances of stale data.\n\nHere's an example of how you can implement the write-through pattern with Redis and Python:\n\nThe write-behind pattern is an optimization of the write-through pattern. In this pattern, the application writes data to the cache and asynchronously updates the primary data store. This improves the application's write performance, as it does not need to wait for the primary data store to acknowledge the write operation.\n\nTo implement the write-behind pattern, you can use a message queue or a background worker to handle the asynchronous updates:\n\nCache eviction strategies determine how and when items are removed from the cache to make room for new items. Redis supports several eviction strategies that can be configured based on your application's requirements.\n\nThe LRU strategy removes the least recently used items from the cache when the cache is full. This strategy prioritizes items that have been accessed recently, ensuring that frequently accessed items are retained in the cache.\n\nTo configure Redis to use the LRU eviction strategy, set the configuration option to or :\n\nRedis allows you to set a time-to-live (TTL) value for cache items, which specifies the duration for which the item will remain in the cache. Once the TTL expires, the item is automatically removed from the cache.\n\nHere's an example of how to set a TTL for a cache item using Python and Redis:\n\nRedis supports Lua scripting, which allows you to run custom scripts on the Redis server. This can be useful for implementing advanced caching techniques that require atomic operations or multiple commands to be executed in a single transaction.\n\nFor example, you can use a Lua script to implement an atomic \"get or create\" operation that retrieves a cache item or creates it if it doesn't exist:\n\nHere's how you can use this Lua script in Python:\n\nRedis is an open-source, in-memory data structure store that can be used as a database, cache, and message broker. It is known for its performance, flexibility, and support for various data structures.\n\nHow do I choose the best cache eviction strategy for my application?\n\nThe choice of cache eviction strategy depends on your application's requirements and access patterns. The LRU strategy is a good default choice, as it prioritizes frequently accessed items. However, you may also want to consider using a TTL-based strategy to ensure that cache items are automatically removed after a certain period.\n\nWhen should I use Lua scripting in Redis?\n\nLua scripting can be useful for implementing advanced caching techniques that require atomic operations or multiple commands to be executed in a single transaction. If your caching logic is complex and requires multiple Redis commands to be executed together, Luascripting can help you ensure atomicity and improve performance by reducing the number of round trips between your application and the Redis server.\n\nHow can I monitor the performance of my Redis cache?\n\nYou can monitor the performance of your Redis cache by using the built-in tool or various third-party monitoring solutions. The tool provides real-time statistics about your Redis instance, such as cache hits and misses, memory usage, and command execution times. You can also use third-party monitoring solutions like Datadog or Grafana to monitor and visualize your Redis cache's performance over time.\n\nWhat are the benefits of using Redis for caching?\n\nUsing Redis for caching has several benefits, including:\n• Improved application performance: Redis stores data in memory, which enables fast access times and reduces the load on your primary data store.\n• Scalability: Redis can be easily scaled horizontally by using clustering or partitioning techniques.\n• Flexibility: Redis supports various data structures, such as strings, lists, sets, and hashes, which makes it suitable for a wide range of caching scenarios.\n• Persistence: Redis can be configured to persist data to disk, allowing you to recover your cache state in case of a server restart or failure.\n\nCan I use Redis as a primary data store, or should it only be used for caching?\n\nWhile Redis is primarily used for caching, it can also be used as a primary data store in certain scenarios. Redis provides data persistence and replication features that allow you to store and retrieve data reliably. However, if your application requires complex queries, transactions, or relational data modeling, a traditional relational database might be a better choice."
    }
]