[
    {
        "link": "https://developer.ibm.com/tutorials/awb-implementing-linear-discriminant-analysis-python",
        "document": ""
    },
    {
        "link": "https://machinelearningmastery.com/linear-discriminant-analysis-for-dimensionality-reduction-in-python",
        "document": "Reducing the number of input variables for a predictive model is referred to as dimensionality reduction.\n\nFewer input variables can result in a simpler predictive model that may have better performance when making predictions on new data.\n\nLinear Discriminant Analysis, or LDA for short, is a predictive modeling algorithm for multi-class classification. It can also be used as a dimensionality reduction technique, providing a projection of a training dataset that best separates the examples by their assigned class.\n\nThe ability to use Linear Discriminant Analysis for dimensionality reduction often surprises most practitioners.\n\nIn this tutorial, you will discover how to use LDA for dimensionality reduction when developing predictive models.\n\nAfter completing this tutorial, you will know:\n• Dimensionality reduction involves reducing the number of input variables or columns in modeling data.\n• LDA is a technique for multi-class classification that can be used to automatically perform dimensionality reduction.\n• How to evaluate predictive models that use an LDA projection as input and make predictions with new raw data.\n\nKick-start your project with my new book Data Preparation for Machine Learning, including step-by-step tutorials and the Python source code files for all examples.\n\nThis tutorial is divided into four parts; they are:\n• Worked Example of LDA for Dimensionality\n\nDimensionality reduction refers to reducing the number of input variables for a dataset.\n\nIf your data is represented using rows and columns, such as in a spreadsheet, then the input variables are the columns that are fed as input to a model to predict the target variable. Input variables are also called features.\n\nWe can consider the columns of data representing dimensions on an n-dimensional feature space and the rows of data as points in that space. This is a useful geometric interpretation of a dataset.\n\nHaving a large number of dimensions in the feature space can mean that the volume of that space is very large, and in turn, the points that we have in that space (rows of data) often represent a small and non-representative sample.\n\nThis can dramatically impact the performance of machine learning algorithms fit on data with many input features, generally referred to as the “curse of dimensionality.”\n\nTherefore, it is often desirable to reduce the number of input features. This reduces the number of dimensions of the feature space, hence the name “dimensionality reduction.”\n\nA popular approach to dimensionality reduction is to use techniques from the field of linear algebra. This is often called “feature projection” and the algorithms used are referred to as “projection methods.”\n\nProjection methods seek to reduce the number of dimensions in the feature space whilst also preserving the most important structure or relationships between the variables observed in the data.\n\nThe resulting dataset, the projection, can then be used as input to train a machine learning model.\n\nIn essence, the original features no longer exist and new features are constructed from the available data that are not directly comparable to the original data, e.g. don’t have column names.\n\nAny new data that is fed to the model in the future when making predictions, such as test dataset and new datasets, must also be projected using the same technique.\n\nLinear Discriminant Analysis, or LDA, is a linear machine learning algorithm used for multi-class classification.\n\nIt should not be confused with “Latent Dirichlet Allocation” (LDA), which is also a dimensionality reduction technique for text documents.\n\nLinear Discriminant Analysis seeks to best separate (or discriminate) the samples in the training dataset by their class value. Specifically, the model seeks to find a linear combination of input variables that achieves the maximum separation for samples between classes (class centroids or means) and the minimum separation of samples within each class.\n\nThere are many ways to frame and solve LDA; for example, it is common to describe the LDA algorithm in terms of Bayes Theorem and conditional probabilities.\n\nIn practice, LDA for multi-class classification is typically implemented using the tools from linear algebra, and like PCA, uses matrix factorization at the core of the technique. As such, it is good practice to perhaps standardize the data prior to fitting an LDA model.\n\nFor more information on how LDA is calculated in detail, see the tutorial:\n\nNow that we are familiar with dimensionality reduction and LDA, let’s look at how we can use this approach with the scikit-learn library.\n\nWe can use LDA to calculate a projection of a dataset and select a number of dimensions or components of the projection to use as input to a model.\n\nThe scikit-learn library provides the LinearDiscriminantAnalysis class that can be fit on a dataset and used to transform a training dataset and any additional dataset in the future.\n\nThe outputs of the LDA can be used as input to train a model.\n\nPerhaps the best approach is to use a Pipeline where the first step is the LDA transform and the next step is the learning algorithm that takes the transformed data as input.\n\nIt can also be a good idea to standardize data prior to performing the LDA transform if the input variables have differing units or scales; for example:\n\nNow that we are familiar with the LDA API, let’s look at a worked example.\n\nWorked Example of LDA for Dimensionality\n\nFirst, we can use the make_classification() function to create a synthetic 10-class classification problem with 1,000 examples and 20 input features, 15 inputs of which are meaningful.\n\nThe complete example is listed below.\n\nRunning the example creates the dataset and summarizes the shape of the input and output components.\n\nNext, we can use dimensionality reduction on this dataset while fitting a naive Bayes model.\n\nWe will use a Pipeline where the first step performs the LDA transform and selects the five most important dimensions or components, then fits a Naive Bayes model on these features. We don’t need to standardize the variables on this dataset, as all variables have the same scale by design.\n\nThe pipeline will be evaluated using repeated stratified cross-validation with three repeats and 10 folds per repeat. Performance is presented as the mean classification accuracy.\n\nThe complete example is listed below.\n\nRunning the example evaluates the model and reports the classification accuracy.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nIn this case, we can see that the LDA transform with naive bayes achieved a performance of about 31.4 percent.\n\nHow do we know that reducing 20 dimensions of input down to five is good or the best we can do?\n\nWe don’t; five was an arbitrary choice.\n\nA better approach is to evaluate the same transform and model with different numbers of input features and choose the number of features (amount of dimensionality reduction) that results in the best average performance.\n\nLDA is limited in the number of components used in the dimensionality reduction to between the number of classes minus one, in this case, (10 – 1) or 9\n\nThe example below performs this experiment and summarizes the mean classification accuracy for each configuration.\n\nRunning the example first reports the classification accuracy for each number of components or features selected.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nWe can see a general trend of increased performance as the number of dimensions is increased. On this dataset, the results suggest a trade-off in the number of dimensions vs. the classification accuracy of the model.\n\nThe results suggest using the default of nine components achieves the best performance on this dataset, although with a gentle trade-off as fewer dimensions are used.\n\nA box and whisker plot is created for the distribution of accuracy scores for each configured number of dimensions.\n\nWe can see the trend of increasing classification accuracy with the number of components, with a limit at nine.\n\nWe may choose to use an LDA transform and Naive Bayes model combination as our final model.\n\nThis involves fitting the Pipeline on all available data and using the pipeline to make predictions on new data. Importantly, the same transform must be performed on this new data, which is handled automatically via the Pipeline.\n\nThe code below provides an example of fitting and using a final model with LDA transforms on new data.\n\nRunning the example fits the Pipeline on all available data and makes a prediction on new data.\n\nHere, the transform uses the nine most important components from the LDA transform as we found from testing above.\n\nA new row of data with 20 columns is provided and is automatically transformed to 15 components and fed to the naive bayes model in order to predict the class label.\n\nThis section provides more resources on the topic if you are looking to go deeper.\n\nIn this tutorial, you discovered how to use LDA for dimensionality reduction when developing predictive models.\n• Dimensionality reduction involves reducing the number of input variables or columns in modeling data.\n• LDA is a technique for multi-class classification that can be used to automatically perform dimensionality reduction.\n• How to evaluate predictive models that use an LDA projection as input and make predictions with new raw data.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://geeksforgeeks.org/ml-linear-discriminant-analysis",
        "document": "When working with high-dimensional datasets it is important to apply dimensionality reduction techniques to make data exploration and modeling more efficient. One such technique is Linear Discriminant Analysis (LDA) which helps in reducing the dimensionality of data while retaining the most significant features for classification tasks. It works by finding the linear combinations of features that best separate the classes in the dataset. In this article we will learn about it and how to implement it in python.\n\nLinear Discriminant Analysis (LDA) also known as Normal Discriminant Analysis is supervised classification problem that helps separate two or more classes by converting higher-dimensional data space into a lower-dimensional space. It is used to identify a linear combination of features that best separates classes within a dataset.\n\nFor example we have two classes that need to be separated efficiently. Each class may have multiple features and using a single feature to classify them may result in overlapping. To solve this LDA is used as it uses multiple features to improve classification accuracy.\n\nLDA works by some assumptions and we are required to understand them so that we have a better understanding of its working.\n\nFor LDA to perform effectively certain assumptions are made:\n• Gaussian Distribution : Data within each class should follow a\n• Equal Covariance Matrices of the different classes should be equal.\n• Linear Separability : A linear decision boundary should be sufficient to separate the classes.\n\nFor example, when data points belonging to two classes are plotted if they are not linearly separable LDA will attempt to find a projection that maximizes class separability.\n\n\n\n\n\nImage shows an example where the classes (black and green circles) are not linearly separable. LDA attempts to separate them using red dashed line. It uses both axes (X and Y) to generate a new axis in such a way that it maximizes the distance between the means of the two classes while minimizing the variation within each class. This transforms the dataset into a space where the classes are better separated.\n\nAfter transforming the data points along a new axis LDA maximizes the class separation. This new axis allows for clearer classification by projecting the data along a line that enhances the distance between the means of the two classes.\n\nPerpendicular distance between the decision boundary and the data points helps us to visualize how LDA works by reducing class variation and increasing separability.\n\nAfter generating this new axis using the above-mentioned criteria, all the data points of the classes are plotted on this new axis and are shown in the figure given below.\n\nIt shows how LDA creates a new axis to project the data and separate the two classes effectively along a linear path. But it fails when the mean of the distributions are shared as it becomes impossible for LDA to find a new axis that makes both classes linearly separable. In such cases we use non-linear discriminant analysis.\n\nLDA works by finding directions in the feature space that best separate the classes. It does this by maximizing the difference between the class means while minimizing the spread within each class.\n\nLet’s assume we have two classes with -dimensional samples such as [Tex]x_1, x_2, … x_n[/Tex] where:\n\nIf [Tex]x_i[/Tex]​ represents a data point, its projection onto the line represented by the unit vector v is [Tex]v^T x_i[/Tex]​.\n\nLet the means of class [Tex]c_1[/Tex] and class [Tex]c_2[/Tex]​ before projection be μ1​ and μ2 respectively. After projection the new means are [Tex]\\hat{\\mu}_1 = v^T \\mu_1[/Tex]and [Tex]\\hat{\\mu}_2 = v^T \\mu_2[/Tex]​.\n\nWe aim to normalize the difference [Tex]|\\hat{\\mu}_1 – \\hat{\\mu}_2|[/Tex]to maximize the class separation.\n\nThe scatter for samples of class c1c_1c1​ is calculated as:\n\nThe goal is to maximize the ratio of the between-class scatter to the within-class scatter, which leads us to the following criterion:\n\nFor the best separation, we calculate the eigenvector corresponding to the highest eigenvalue of the scatter matrices [Tex]s_w^{-1} s_b[/Tex].\n• Quadratic Discriminant Analysis (QDA): Each class uses its own estimate of variance (or covariance) allowing it to handle more complex relationships.\n• Flexible Discriminant Analysis (FDA): Uses non-linear combinations of inputs such as splines to handle non-linear separability.\n• Regularized Discriminant Analysis (RDA): into the covariance estimate to prevent overfitting.\n\nIn this implementation we will perform linear discriminant analysis using the Scikit-learn library on the Iris dataset.\n• StandardScaler() : Standardizes the features to ensure they have a mean of 0 and a standard deviation of 1 removing the influence of different scales.\n• fit_transform() : Standardizes the feature data by applying the transformation learned from the training data ensuring each feature contributes equally.\n• LabelEncoder() : Converts categorical labels into numerical values that machine learning models can process.\n• fit_transform() on y : Transforms the target labels into numerical values for use in classification models.\n• LinearDiscriminantAnalysis() : Reduces the dimensionality of the data by projecting it into a lower-dimensional space while maximizing the separation between classes.\n• transform() on X_test : Applies the learned LDA transformation to the test data to maintain consistency with the training data.\n• None Works well even when the number of features is much larger than the number of training samples.\n• None Assumes Gaussian distribution of data which may not always be the case.\n• None Assumes equal covariance matrices for different classes which may not hold in all datasets.\n• None Assumes linear separability which is not always true.\n• None May not always perform well in high-dimensional feature spaces.\n• Face Recognition : It is used to reduce the high-dimensional feature space of pixel values in face recognition applications helping to identify faces more efficiently.\n• Medical Diagnosis : It classifies disease severity in mild, moderate or severe based on patient parameters helping in decision-making for treatment.\n• Customer Identification : It can help identify customer segments most likely to purchase a specific product based on survey data.\n\nLinear Discriminant Analysis (LDA) is a technique for dimensionality reduction that not only simplifies high-dimensional data but also enhances the performance of models by maximizing class separability. By converting data into a lower-dimensional space it helps us to improve accuracy of classification task."
    },
    {
        "link": "https://github.com/xbeat/Machine-Learning/blob/main/Building%20a%20Linear%20Discriminant%20Analysis%20(LDA)%20Algorithm%20from%20Scratch%20in%20Python.md",
        "document": "Linear Discriminant Analysis is a powerful technique for dimensionality reduction and classification. It aims to find a linear combination of features that best separates two or more classes of objects or events. LDA is particularly useful when dealing with multi-class classification problems and can be implemented from scratch using Python.\n\nBefore implementing LDA, we need to prepare our data. This involves splitting the dataset into features (X) and labels (y), as well as separating the data by class. We'll use a simple example with two classes and two features.\n\nThe first step in LDA is to calculate the mean of each class. These means will be used to compute the between-class scatter matrix.\n\nThe within-class scatter matrix represents the spread of samples around their respective class means. It's calculated by summing the covariance matrices of each class.\n\nThe between-class scatter matrix represents the spread of class means around the overall mean. It's calculated using the difference between class means and the overall mean.\n\nLDA seeks to maximize the ratio of between-class scatter to within-class scatter. This is achieved by solving the generalized eigenvalue problem: S_W^(-1) * S_B * w = λ * w.\n\nOnce we have the LDA direction (w), we can project our data onto this direction to reduce dimensionality and visualize the separation between classes.\n\nWe can use the LDA direction to classify new data points. The decision boundary is perpendicular to the LDA direction and positioned at the midpoint between the projected class means.\n\nTo assess the performance of our LDA implementation, we can split our data into training and testing sets, train the LDA on the training data, and evaluate its accuracy on the test data.\n\nLet's apply our LDA implementation to the famous Iris dataset, which contains measurements of three different species of iris flowers.\n\nLet's apply LDA to classify wines based on their chemical properties. We'll use a subset of the Wine Quality dataset from UCI Machine Learning Repository.\n\nLet's visualize the LDA projection for the Wine Quality dataset to see how well it separates the classes.\n• Dimensionality reduction: LDA can project high-dimensional data onto a lower-dimensional space while preserving class separability.\n• Interpretability: The LDA components provide insight into which features are most important for classification.\n• Efficiency: LDA can be computationally efficient, especially for large datasets.\n• Linearity assumption: LDA assumes that classes are linearly separable, which may not always be the case in real-world data.\n• Gaussian distribution assumption: LDA assumes that the data within each class follows a Gaussian distribution.\n• Sensitivity to outliers: LDA can be affected by outliers, as it relies on mean and covariance calculations.\n\nLinear Discriminant Analysis is a powerful technique for dimensionality reduction and classification. We've implemented LDA from scratch in Python and applied it to real-world datasets. To improve upon basic LDA, consider exploring:\n• Incremental LDA: Allows for online learning with streaming data.\n• Multi-class LDA: Extends LDA to handle more than two classes simultaneously.\n\nFor those interested in delving deeper into Linear Discriminant Analysis and its applications, here are some valuable resources:\n• Fisher, R. A. (1936). The Use of Multiple Measurements in Taxonomic Problems. Annals of Eugenics. (Original paper introducing LDA)\n\nThese resources provide a mix of theoretical foundations and practical implementations of LDA and its variants."
    },
    {
        "link": "https://medium.com/@famn594/understanding-linear-discriminant-analysis-lda-in-python-programming-cb2a6fd774b5",
        "document": "Linear Discriminant Analysis (LDA) is a powerful technique in the field of pattern recognition, machine learning, and statistics. It is used for dimensionality reduction, feature extraction, and classification. LDA is particularly useful when you have a labeled dataset and want to find the linear combinations of features that best separate different classes. In this article, we’ll delve into the principles of LDA and demonstrate its implementation in Python.\n\nLDA is a supervised dimensionality reduction technique. Unlike Principal Component Analysis (PCA), which focuses on capturing the maximum variance in the data, LDA emphasizes class separability. It aims to project the data points onto a lower-dimensional subspace while maximizing the distance between the class means and minimizing the spread within each class.\n\nThe main steps involved in LDA are as follows\n\n1.Compute the mean vectors for each class\n\nCalculate the mean vector for each class by averaging the feature vectors of the data points belonging to that class.\n\nWithin-class scatter matrix (SW): It measures the spread of data within each class and is calculated as the sum of the covariance matrices for individual classes.\n\nBetween-class scatter matrix (SB): It quantifies the separation between classes and is computed as a weighted sum of the covariance matrices between the classes.\n\nTo find the linear discriminants, we perform an eigendecomposition of the matrix SW^(-1) * SB. The resulting eigenvalues and eigenvectors help us determine the optimal directions in the new feature space.\n\nSort the eigenvalues in descending order and choose the top k eigenvectors corresponding to the k largest eigenvalues. These eigenvectors will define the new subspace.\n\nForm a projection matrix by stacking the selected eigenvectors as columns. This matrix will be used to project the data onto the new subspace.\n\n6.Project data onto the new subspace:\n\nFinally, the data points are projected onto the new subspace created by the projection matrix.\n\nLet’s take a practical look at implementing Linear Discriminant Analysis in Python. We’ll use the scikit-learn library, which provides a convenient interface for LDA.\n\n# Scatter plot of the three classes (Original Data)\n\nplt.figure(figsize=(12, 5))\n\n# Plot the projected data in the original 2D space showing projection direction\n\ntheta = np.arctan2(w[1], w[0])\n\nz1x, z1y = np.multiply(np.cos(theta), y1), np.multiply(np.sin(theta), y1)\n\nz2x, z2y = np.multiply(np.cos(theta), y2), np.multiply(np.sin(theta), y2)\n\nz3x, z3y = np.multiply(np.cos(theta), y3), np.multiply(np.sin(theta), y3)\n\nz4x, z4y = np.multiply(np.cos(theta), max([max(y1), max(y2), max(y3)]) + 2), np.multiply(np.sin(theta), max([max(y1), max(y2), max(y3)]) + 2)"
    },
    {
        "link": "https://machinelearningmastery.com/face-recognition-using-principal-component-analysis",
        "document": "Recent advance in machine learning has made face recognition not a difficult problem. But in the previous, researchers have made various attempts and developed various skills to make computer capable of identifying people. One of the early attempt with moderate success is eigenface, which is based on linear algebra techniques.\n\nIn this tutorial, we will see how we can build a primitive face recognition system with some simple linear algebra technique such as principal component analysis.\n\nAfter completing this tutorial, you will know:\n• How to use principal component analysis to extract characteristic images from an image dataset\n• How to express any image as a weighted sum of the characteristic images\n• How to compare the similarity of images from the weight of principal components\n\nThis tutorial is divided into 3 parts; they are:\n\nIn computer, pictures are represented as a matrix of pixels, with each pixel a particular color coded in some numerical values. It is natural to ask if computer can read the picture and understand what it is, and if so, whether we can describe the logic using matrix mathematics. To be less ambitious, people try to limit the scope of this problem to identifying human faces. An early attempt for face recognition is to consider the matrix as a high dimensional detail and we infer a lower dimension information vector from it, then try to recognize the person in lower dimension. It was necessary in the old time because the computer was not powerful and the amount of memory is very limited. However, by exploring how to compress image to a much smaller size, we developed a skill to compare if two images are portraying the same human face even if the pictures are not identical.\n\nIn 1987, a paper by Sirovich and Kirby considered the idea that all pictures of human face to be a weighted sum of a few “key pictures”. Sirovich and Kirby called these key pictures the “eigenpictures”, as they are the eigenvectors of the covariance matrix of the mean-subtracted pictures of human faces. In the paper they indeed provided the algorithm of principal component analysis of the face picture dataset in its matrix form. And the weights used in the weighted sum indeed correspond to the projection of the face picture into each eigenpicture.\n\nIn 1991, a paper by Turk and Pentland coined the term “eigenface”. They built on top of the idea of Sirovich and Kirby and use the weights and eigenpictures as characteristic features to recognize faces. The paper by Turk and Pentland laid out a memory-efficient way to compute the eigenpictures. It also proposed an algorithm on how the face recognition system can operate, including how to update the system to include new faces and how to combine it with a video capture system. The same paper also pointed out that the concept of eigenface can help reconstruction of partially obstructed picture.\n\nBefore we jump into the code, let’s outline the steps in using eigenface for face recognition, and point out how some simple linear algebra technique can help the task.\n\nAssume we have a bunch of pictures of human faces, all in the same pixel dimension (e.g., all are r×c grayscale images). If we get M different pictures and vectorize each picture into L=r×c pixels, we can present the entire dataset as a L×M matrix (let’s call it matrix $A$), where each element in the matrix is the pixel’s grayscale value.\n\nRecall that principal component analysis (PCA) can be applied to any matrix, and the result is a number of vectors called the principal components. Each principal component has the length same as the column length of the matrix. The different principal components from the same matrix are orthogonal to each other, meaning that the vector dot-product of any two of them is zero. Therefore the various principal components constructed a vector space for which each column in the matrix can be represented as a linear combination (i.e., weighted sum) of the principal components.\n\nThe way it is done is to first take $C=A – a$ where $a$ is the mean vector of the matrix $A$. So $C$ is the matrix that subtract each column of $A$ with the mean vector $a$. Then the covariance matrix is\n\nfrom which we find its eigenvectors and eigenvalues. The principal components are these eigenvectors in decreasing order of the eigenvalues. Because matrix $S$ is a L×L matrix, we may consider to find the eigenvectors of a M×M matrix $C^T\\cdot C$ instead as the eigenvector $v$ for $C^T\\cdot C$ can be transformed into eigenvector $u$ of $C\\cdot C^T$ by $u=C\\cdot v$, except we usually prefer to write $u$ as normalized vector (i.e., norm of $u$ is 1).\n\nThe physical meaning of the principal component vectors of $A$, or equivalently the eigenvectors of $S=C\\cdot C^T$, is that they are the key directions that we can construct the columns of matrix $A$. The relative importance of the different principal component vectors can be inferred from the corresponding eigenvalues. The greater the eigenvalue, the more useful (i.e., holds more information about $A$) the principal component vector. Hence we can keep only the first K principal component vectors. If matrix $A$ is the dataset for face pictures, the first K principal component vectors are the top K most important “face pictures”. We call them the eigenface picture.\n\nFor any given face picture, we can project its mean-subtracted version onto the eigenface picture using vector dot-product. The result is how close this face picture is related to the eigenface. If the face picture is totally unrelated to the eigenface, we would expect its result is zero. For the K eigenfaces, we can find K dot-product for any given face picture. We can present the result as weights of this face picture with respect to the eigenfaces. The weight is usually presented as a vector.\n\nConversely, if we have a weight vector, we can add up each eigenfaces subjected to the weight and reconstruct a new face. Let’s denote the eigenfaces as matrix $F$, which is a L×K matrix, and the weight vector $w$ is a column vector. Then for any $w$ we can construct the picture of a face as\n\nwhich $z$ is resulted as a column vector of length L. Because we are only using the top K principal component vectors, we should expect the resulting face picture is distorted but retained some facial characteristic.\n\nSince the eigenface matrix is constant for the dataset, a varying weight vector $w$ means a varying face picture. Therefore we can expect the pictures of the same person would provide similar weight vectors, even if the pictures are not identical. As a result, we may make use of the distance between two weight vectors (such as the L2-norm) as a metric of how two pictures resemble.\n\nNow we attempt to implement the idea of eigenface with numpy and scikit-learn. We will also make use of OpenCV to read picture files. You may need to install the relevant package with command:\n\nThe dataset we use are the ORL Database of Faces, which is quite of age but we can download it from Kaggle:\n\nThe file is a zip file of around 4MB. It has pictures of 40 persons and each person has 10 pictures. Total to 400 pictures. In the following we assumed the file is downloaded to the local directory and named as .\n\nWe may extract the zip file to get the pictures, or we can also make use of the package in Python to read the contents from the zip file directly:\n\nThe above is to read every PGM file in the zip. PGM is a grayscale image file format. We extract each PGM file into a byte string through and convert it into a numpy array of bytes. Then we use OpenCV to decode the byte string into an array of pixels using . The file format will be detected automatically by OpenCV. We save each picture into a Python dictionary for later use.\n\nHere we can take a look on these picture of human faces, using matplotlib:\n\nWe can also find the pixel size of each picture:\n\nThe pictures of faces are identified by their file name in the Python dictionary. We can take a peek on the filenames:\n\nand therefore we can put faces of the same person into the same class. There are 40 classes and totally 400 pictures:\n\nTo illustrate the capability of using eigenface for recognition, we want to hold out some of the pictures before we generate our eigenfaces. We hold out all the pictures of one person as well as one picture for another person as our test set. The remaining pictures are vectorized and converted into a 2D numpy array:\n\nNow we can perform principal component analysis on this dataset matrix. Instead of computing the PCA step by step, we make use of the PCA function in scikit-learn, which we can easily retrieve all results we needed:\n\nWe can identify how significant is each principal component from the explained variance ratio:\n\nor we can simply make up a moderate number, say, 50, and consider these many principal component vectors as the eigenface. For convenience, we extract the eigenface from PCA result and store it as a numpy array. Note that the eigenfaces are stored as rows in a matrix. We can convert it back to 2D if we want to display it. In below, we show some of the eigenfaces to see how they look like:\n\nFrom this picture, we can see eigenfaces are blurry faces, but indeed each eigenfaces holds some facial characteristics that can be used to build a picture.\n\nSince our goal is to build a face recognition system, we first calculate the weight vector for each input picture:\n\nThe above code is using matrix multiplication to replace loops. It is roughly equivalent to the following:\n\nUp to here, our face recognition system has been completed. We used pictures of 39 persons to build our eigenface. We use the test picture that belongs to one of these 39 persons (the one held out from the matrix that trained the PCA model) to see if it can successfully recognize the face:\n\nAbove, we first subtract the vectorized image by the average vector that retrieved from the PCA result. Then we compute the projection of this mean-subtracted vector to each eigenface and take it as the weight for this picture. Afterwards, we compare the weight vector of the picture in question to that of each existing picture and find the one with the smallest L2 distance as the best match. We can see that it indeed can successfully find the closest match in the same class:\n\nand we can visualize the result by comparing the closest match side by side:\n\nWe can try again with the picture of the 40th person that we held out from the PCA. We would never get it correct because it is a new person to our model. However, we want to see how wrong it can be as well as the value in the distance metric:\n\nWe can see that it’s best match has a greater L2 distance:\n\nbut we can see that the mistaken result has some resemblance to the picture in question:\n\n\n\nIn the paper by Turk and Petland, it is suggested that we set up a threshold for the L2 distance. If the best match’s distance is less than the threshold, we would consider the face is recognized to be the same person. If the distance is above the threshold, we claim the picture is someone we never saw even if a best match can be find numerically. In this case, we may consider to include this as a new person into our model by remembering this new weight vector.\n\nActually, we can do one step further, to generate new faces using eigenfaces, but the result is not very realistic. In below, we generate one using random weight vector and show it side by side with the “average face”:\n\nHow good is eigenface? It is surprisingly overachieved for the simplicity of the model. However, Turk and Pentland tested it with various conditions. It found that its accuracy was “an average of 96% with light variation, 85% with orientation variation, and 64% with size variation.” Hence it may not be very practical as a face recognition system. After all, the picture as a matrix will be distorted a lot in the principal component domain after zoom-in and zoom-out. Therefore the modern alternative is to use convolution neural network, which is more tolerant to various transformations.\n\nPutting everything together, the following is the complete code:\n\nThis section provides more resources on the topic if you are looking to go deeper.\n• L. Sirovich and M. Kirby (1987). “Low-dimensional procedure for the characterization of human faces“. Journal of the Optical Society of America A. 4(3): 519–524.\n• M. Turk and A. Pentland (1991). “Eigenfaces for recognition“. Journal of Cognitive Neuroscience. 3(1): 71–86.\n\nIn this tutorial, you discovered how to build a face recognition system using eigenface, which is derived from principal component analysis.\n• How to extract characteristic images from the image dataset using principal component analysis\n• How to use the set of characteristic images to create a weight vector for any seen or unseen images\n• How to use the weight vectors of different images to measure for their similarity, and apply this technique to face recognition\n• How to generate a new random image from the characteristic images"
    },
    {
        "link": "https://hassan-id-mansour.medium.com/face-recognition-using-eigenfaces-python-b857b2599ed0",
        "document": "In various fields and in our daily lives, facial recognition (FR) technology has become one of the basic necessities. For example, you can easily unlock your mobile phone only with your face without resorting to other methods, and this is done by comparing a human face to a database of known faces. In this article, we will discuss how we can implement face recognition in Python by using the Eigenface technique.\n• Understanding the dimensionality-reduction method “Principal Component Analysis (PCA)” is very important, so for that, I find this YouTube video very helpful. StatQuest : Principal Component Analysis (PCA) (étape par étape) — YouTube.\n\nThe strategy of the Eigenfaces method consists of efficiently using Principal Component Analysis (PCA) for projecting the face in question in facespace (eigenspace), so we can represent it as a linear combination of eigenvectors called eigenfaces.\n\nIV. Computation of Eigenfaces and application of FR\n\n1. Obtain face images I = [ I1, I2, I3, ….,Im ] (training faces) and must be centered with the same size (N, N). 2. Represent every image Ii as a vector Γi , Γ = [ Γ1, Γ2, …., Γm]. dim (Ii) = (N, N) and dim (Γi) = (m, N*N) 4. Subtract the mean face from every vector: φ = [ Γ1 - Ψ, Γ2 - Ψ, …,Γm - Ψ] dim(φ) = (m, N*N) 5. Compute the covariance matrix C: C = (1/m) * φφT where φT is the transpose of φ. dim(C) = (m, m) 6. Compute the eigenvectors(eigenfaces) U of C. dim (U) = (K, N*N) 7. Finding the vector of wights Wi for each image vector φi in the database by using this formula:\n\nThis function is used for showing the images.\n\nThis dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling), and facial details (glasses / no glasses). All the images were taken against a dark, homogeneous background, with the subjects in an upright, frontal position (with tolerance for some side movement).\n\n4. Showing all images of the same face\n\nFirst, we convert our query image and dataset image each to a single vector, subtracting the average face to keep the specific characteristics of each face.\n\nFind the new transformation or the projection of our images in the eigenspace by calculating the eigenfaces.\n\nThe shape of the original image set:\n\nThe shape of the new projection of our image set:\n\nAs we can see from the result for the first 10 faces, the precision is 0.8.\n\nAll available images of the same face:\n\nThe search result below gives a precision of 0.9 for the first 10 faces.\n• Easy to implement and computationally less expensive.\n• No knowledge (such as facial feature) of the image required (except id).\n• The algorithm is sensitive to lightning, shadows and also scale of face in the image.\n• Front view of the face is required for this algorithm to work properly.\n\nIn this article we had applied eigenface technique for facial recognition in python."
    },
    {
        "link": "https://github.com/mohamedmahmoud97/FaceRecognition",
        "document": "The Project is structred into a helpers module containg helpers used to load the images data from disk in numpy arrays, As for PCA, LDA and KNN they all reside in a folder named classifiers with PCA and LDA exposing the methods train and project and KNN exposing the methods train and predict, back at the root of the project alongside the helpers module there is the fisher faces module that uses both LDA and PCA to implement the fisher faces algorithm descirbed in section 2.As for the actual usage of these modules, we use three jupyter notebooks, one for testing pca named PCA, another for testing lda named LDA and a last one for testing the fisher faces with the name FisherFaces, all residing at the root of the project.\n\nThe Principal Component Analysis (PCA) is one of the most successful techniques that have been used in image recognition and compression. The main objective of PCA is to reduce the large dimensionality of the data space to the smaller dimensionality of feature space, which are needed to describe the data economically. One of the simplest and most effective PCA approaches used in face recognition systems is the so called eigenface approach. This approach transforms faces into a small set of essential characteristics, eigenfaces, which are the main components of the initial set of learning images (training set). Recognition is done by projecting a new image in the eigenface subspace, after which the person is classified by comparing its position in eigenface space with the position of known individuals.\n\nIn our assignment firstly, we separated the data set into two parts, the training set and the test set. Then we used the PCA algorithm that computes the mean vectors of each dimension, normalize the data, computes the covariance matrix, eigen values and vectors. We sort the eigen values descendingly and take the corresponding eigen vectors (eigen faces) to it. We used four values for the alpha [0.8,0.85,0.9,0.95], that will change the number of dimensions of the eigen faces according to the explained variance. The higher the alpha the bigger the dimensionality. Then we project the training data and test data on our projection matrix which is the eigenface subspace, Figure 1 shows one of the eigenfaces produced by the algorithm.\n\nFinally we used the knn classifier to determine the class labels with k = 1.\n\nThe accuracy of all values of alpha: testing accuracy (0.8) = 0.74 testing accuracy (0.85) = 0.70 testing accuracy (0.9) = 0.68 testing accuracy (0.95) = 0.63\n\nThe lower the alpha the higher the accuracy as the training data is too small and maybe if we increase the alpha it overshoots.\n\nLinear Discriminant Analysis (LDA) is a method used to reduce dimensions in the aim of maximizing the between class separation and the within class variance for better classification between different classes, the method was first used to classify between 2 different classes now we are using in to classify between more than two classes.\n\nIn our Face Recognition example we are having a small set of examples per class relative to the number of features representing each example, this cause our within class scatter matrix S to be a low rank matrix (the rank of S is at most N - c) and this results in S being singular.\n\nIn order to solve this problem we are no heading to calculate the Fisherfaces rather than the Eigenfaces. So now instead of using LDA directly we will use PCA first to reduce the matrix from where N is the number of examples, d is number of dimensions and c is the number of classes.So now the PCA projection matrix is of size and data matrix after PCA is passing this data matrix to the LDA will result and taking the most dominant C-1 fisherfaces will cause a projection matrix of size which s the required projection matrix, the columns of this matrix are the fisher faces, figure 1 shows the first fisher face.\n\nThe bulk of the implementation lies in the predict method (Listing 1) which relies on calculating the euclidean distances between all test and train pairs, sorting the distances ascendingly and then substituting the first k distances with the class of the points they represent and finally pick the class that has appeared the most in the first k points as the predicted class.\n\nTo test how the accuracy of classification varies with the value of K when using both Fisherfaces and EigenFaces.\n\n As for Fisherfaces we plotted the accuracy for values ranging from 1 to 20 (Figure 2), since the graph didn’t seem to decerease in an exponentially decaying fashion as in the case with PCA, so we increased the range to make sure we get the best value of K, however after plotting the graph it turned out the best accuracy of 77% was obtained with k equaling 3.\n\n For EigenFaces we plotted four graphs similar to the one plotted for Fisherfaces, one for each value of alpha, Figure 3 shows the alpha equaling 0.8.Here we only plotted for values of k from 1 to 7 since the graph seemed to decrease exponentially.\n\nHow many dominant eigenvectors will you use for the LDA solution? The number of dominant eigenvectors will be equal to number of classes - 1, we are using CIFAR-10, of 10 classes, so 9 dominant eigenvectors will be used for the LDA"
    },
    {
        "link": "https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html",
        "document": "Go to the end to download the full example code. or to run this example in your browser via JupyterLite or Binder\n\nFaces recognition example using eigenfaces and SVMs#\n\nThe dataset used in this example is a preprocessed excerpt of the “Labeled Faces in the Wild”, aka LFW: http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n\nDownload the data, if not already on disk and load it as numpy arrays\n\n# introspect the images arrays to find the shapes (for plotting) # for machine learning we use the 2 data directly (as relative pixel # positions info is ignored by this model) # the label to predict is the id of the person\n\nSplit into a training set and a test and keep 25% of the data for testing.\n\nCompute a PCA (eigenfaces) on the face dataset (treated as unlabeled dataset): unsupervised feature extraction / dimensionality reduction\n\nQuantitative evaluation of the model quality on the test set\n\nQualitative evaluation of the predictions using matplotlib\n\nplot the result of the prediction on a portion of the test set\n\nplot the gallery of the most significative eigenfaces\n\nFace recognition problem would be much more effectively solved by training convolutional neural networks but this family of models is outside of the scope of the scikit-learn library. Interested readers should instead try to use pytorch or tensorflow to implement such models."
    },
    {
        "link": "https://github.com/Elzawawy/FaceRecoginition",
        "document": "Face recognition has become a major field of interest these days. Face recognition algorithms are used in a wide range of applications such as security control,crime investigation, and entrance control in buildings, access control at automatic teller machines, passport verification, identifying the faces in a given databases. Formally, Face recognition is the bio-metric identification by scanning a person’s face and matching it against a library of known faces. In this assignment, we implement two different image recognition approaches. The first is Eigenfaces which is one of the most widely used representations, which are based on principal component analysis and use a nearest neighbour classifier. The second is Fisherfaces which use linear/Fisher discriminant analysis (FLD/LDA) for best discriminating the face images of same class and also use a nearest neighbour classifier.\n\nPCA is also known as Karhunen Loeve projection.Principal component analy- sis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncor- related variables called principal components and refrred to as Eigenfaces. In PCA, the main idea to re-express the available dataset to extract the relevant information by reducing the redundancy and minimize the noise. A nearest neighbour classifier works afterwards by comparing compare a face’s position in Eigenfaces subspace with the position of known individuals and the distances between them.\n\nPCA algorithm steps and procedure is provided in Figure 1 which is taken from [1] from Chapter 7.\n\nPCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on. PCA calculates the Eigenvectors of the co-variance matrix, and projects the original data onto a lower dimensional feature subspace, which is defined by Eigenvectors with large Eigenvalues. The coordinates of the data points in the lower subspace dimension is computed and fed to the classifier after this algorithm job ends.\n\nWe implemented the above algorithm in Figure 1 using Python and Numpy arrays. The results shown in this section are related to the 50-50 data split test, results on the 70-30 data split test can be obtained from the source code attached and referred to later on. Computing the mean vectors of each dimen- sion, normalizing the data, and computing the co-variance matrix, Eigenvalues and Eigenvectors. After Projection of data, we obtain the coordinates A matrix and use finally the simple (i.e k = 1) k-nearest neighbour classifier to determine the class labels of testing data. We established four trials when choosing the di- mensionrbased on four different values ofα: 0.8 , 0.85, 0.9, 0.95 which yielded different values ofrand recomputed the accuracy of the classifier for each trial. Figures 2 and 3 show a plot of the relation betweenrandαand betweenαand accuracy of the KNN classifier respectively. From figure 2, it can be deduced\n\nthat as alpha increases, the number of reduced dimensions required to achieve that alpha value increases. It is then obvious from figure 3, that after a certain alpha value there is no clear or big change in accuracy and it actually decreases when alpha is set to 0.95 as it is over-fits. So, we can deduce the best alpha value here is 0.8, which achieves very good accuracy with no or little redundant dimensions in the subspace.\n\nThe aim of this section is to better grasp the concept of Eigenfaces in action rather than in mathematical formulas and codes. First,the dataset we worked on in this assignment is the ATT face database (formerly the ORL database)[2]. It is heavily used in publications as an example of an easy set of images as the pictures are taken under very similar conditions. There are ten different images of each of 40 distinct classes(i.e persons). For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous\n\nbackground with the subjects in an upright, frontal position (with tolerance for some side movement). A preview image of the Database of Faces is shown in Figure 4. Second, we split the photos into training and test sets using two different splits in two different files ( 50-50 splits and 70-30 splits). The mean face is the face that resembles roughly most of the dataset faces features, the mean face of the train set, in case of 50-50 is shown in Figure 5.\n\nThen, we completed the procedure as explained in 2.2 and the Eigenfaces\n\nproduced as those shown in Figure 6. Finally, to complete the purpose of this section, we show the reconstruction of a sample face via the Eigenfaces obtained. Figure 7(a) shows the original face while 7(b) shows the reconstructed one.\n\nSo, to summarize our work in the first approach, Eigenfaces along with a simple KNN Classifier did a very good job in faces classification as well as face reconstruction experiment conducted on a sample test face.\n\nLinear discriminant analysis (LDA), also called normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher’s linear discriminant, a method used in statistics, pattern recognition and machine learn- ing to find a linear combination of features that characterizes or separates two or more classes of objects or events. Ronald A. Fisher formulated the Linear Discriminant in 1936 in [3], and it also has some practical uses as classifier. The original Linear discriminant was de- scribed for a 2-class problem, and it was then later generalized as “multi-class Linear Discriminant Analysis” in [4]. The goal of LDA is to project a feature space (a dataset n-dimensional samples) onto a smaller subspace k (where k is smaller than or equal to n-1) while main- taining the class-discriminatory information. In other words, it seeks to reduce dimensionality while class separation in mind.\n\nLDA algorithm steps and procedure is provided in Figure 8 which is taken from [1] from Chapter 20.\n\nAs the algorithm lists, the steps involved in the algorithm is as follows. Step one, compute d-dimensional mean vectors for different classes from the dataset, where d is the dimension of feature space.Step two, compute in-between class and with-in class scatter matrices.Step three, compute Eigenvectors and corre- sponding Eigenvalues for the scatter matrices. Step four, choose k Eigenvectors corresponding to top k Eigenvalues to form a transformation matrix of dimension dxk. Step five, transform the d-dimensional feature space X to k-dimensional feature subspace Y via the transformation matrix.\n\nAgain, we implemented the above algorithm in Figure 8 using Python and Numpy arrays. In order to find a good projection vector, we need to define a measure of separation between the projections. However, the distance between the projected means is not a very good measure since it does not take into account the standard deviation within the classes. The solution proposed by Fisher is to maximize a function that represents the difference between the means, normalized by a measure of the within-class variability, or the so-called scatter.The Fisher linear discriminant is defined as the linear functionwTxthat maximizes the criterion function: (the distance between the projected means normalized by the within-class scatter of the projected samples. Therefore, we will be looking for a projection where examples from the same class are projected very close to each other and, at the same time, the projected means are as farther apart as possible. Thus, It can be shown that the optimal projection matrix W∗is the one whose columns are the eigenvectors corresponding to the largest eigenvalues of generalized eigenvalue problem. The main resulting matrices of the implementation is shown in figures below, more can be found in the source code though.\n\nFinally, we note the KNN classifier scores using LDA on (50-50) and (70-30) splits were 0.55 and 0.64 respectively.\n\nAlthough, LDA sounds fancy and a good option it is wise to consider it’s prob- lems and whether it is going to be a good choice for you when applying it on your data or not. Firstly, LDA produces at most C-1 feature projections. If the classification error estimates establish that more features are needed, some other method must be employed to provide those additional features. Secondly,LDA is a parametric method since it assumes uni-modal Gaussian like- lihoods. If the distributions are significantly non-Gaussian, the LDA projections will not be able to preserve any complex structure of the data, which may be needed for classification. Lastly, LDA will fail when the discriminatory information is not in the mean but rather in the variance of the data.\n\nIn any of these cases do occur, you can consider other variants of LDA that solve some of those limitations mentioned above such as Non-parametric LDA, Orthonormal LDA , Generalized LDA and Multilayer perceptrons. Moreover, you can be directed otherwise to use a totally different dimension reduction technique such as PCA, Exploratory Projection Pursuit or Sammon’s non-linear mapping.\n\nIn general, dimensionality reduction does not only help reducing computational costs for a given classification task, but it can also be helpful to avoid over-fitting by minimizing the error in parameter estimation (“curse of dimensionality”). In this work, we worked in the aim to implement a face recognition software based\n\non KNN Classification and using dimensionality reduction before applying it to the classifier with two approaches, one was LDA and the other was PCA.\n\nLDA is also closely related to principal component analysis (PCA) and in that they both look for linear combinations of variables which best explain the data.LDA explicitly attempts to model the difference between the classes of data. PCA on the other hand does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities.\n\nBoth Linear Discriminant Analysis (LDA) and Principal Component Anal- ysis (PCA) are linear transformation techniques that are commonly used for di- mensionality reduction. PCA can be described as an “unsupervised” algorithm, since it “ignores” class labels and its goal is to find the directions (the so-called principal components) that maximize the variance in a dataset. In contrast to PCA, LDA is “supervised” and computes the directions (“linear discriminant”) that will represent the axes that that maximize the separation between multiple classes. Although it might sound intuitive that LDA is superior to PCA for a multi-class classification task where the class labels are known, this might not always the case. For example, comparisons between classification accuracies for image recognition after using PCA or LDA show that PCA tends to outperform LDA if the number of samples per class is relatively small as shown in [5]. In practice, it is also common to use both LDA and PCA in combination: E.g., PCA for dimensionality reduction followed by an LDA, this approach is called the Fisher Faces and was proposed in [6]. As we mention later on, one of our\n\nfuture improvement tasks is to implement this approach and compare it with our work here. Also in case of uniformly distributed data, LDA almost always performs better than PCA. However if the data is highly skewed (irregularly distributed) then it is advised to use PCA since LDA can be biased towards the majority class.\n\nIn this section we would like to point out some final notes that we would like to do futuristic improvements in our source code and implementation. One of those being is implementing the Fisher Faces approach (Applying PCA first then feeding it as input to the LDA) and comparing it with the two approaches already implemented in order to get a full view of the idea. Also, we would like to apply some OOP and modularity in our code by breaking down the source code into various python files in which there is a Utility class for the helper methods and also a main class for the ML algorithms applied. Also, it is important to point out that the accuracy scores achieved by LDA was unsatisfactory like those obtained from PCA so maybe a last goal is to ensure that our implementations are correct and that this is just the nature’s work.\n\n[1]Data Mining and Analysis: Fundamental Concepts and Algorithms MO- HAMMED J. ZAKI Rensselaer Polytechnic Institute, Troy, New York WAG- NER MEIRA JR. Universidade Federal de Minas Gerais, Brazil\n\n[3]”The Use of Multiple Measurements in Taxonomic Problems” by Ronald A. Fisher in 1936\n\n[4]”Multiple Discriminant Analysis” by C. R. Rao in 1948\n\n[5]PCA vs. LDA, A.M. Martinez et al., 2001\n\n[6]”Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Pro- jection” by Peter N. Belhumeur, Joao P. Hespanha, and David J. Kriegman\n\n[7]FACE RECOGNITION USING PCA, LDA AND VARIOUS DISTANCE CLASSIFIERS"
    }
]