[
    {
        "link": "https://cython.readthedocs.io/en/latest/src/userguide/source_files_and_compilation.html",
        "document": "Cython source file names consist of the name of the module followed by a extension, for example a module called primes would have a source file named .\n\nCython code, unlike Python, must be compiled. This happens in two stages:\n\nOnce you have written your / file, there are a couple of ways how to turn it into an extension module.\n\nThe following sub-sections describe several ways to build your extension modules, and how to pass directives to the Cython compiler.\n\nThere are also a number of tools that process files apart from Cython, e.g.\n\nThere are two ways of compiling from the command line.\n• None The cython command takes a or file and compiles it into a C/C++ file.\n• None The cythonize command takes a or file and compiles it into a C/C++ file. It then compiles the C/C++ file into an extension module which is directly importable from Python. One way is to compile it manually with the Cython compiler, e.g.: This will produce a file called , which then needs to be compiled with the C compiler using whatever options are appropriate on your platform for generating an extension module. For these options look at the official Python documentation. The other, and probably better, way is to use the extension provided with Cython. The benefit of this method is that it will give the platform specific compilation options, acting like a stripped down autotools. Run the cythonize compiler command with your options and list of files to generate an extension module. For example: This creates a file (or in C++ mode), compiles it, and puts the resulting extension module ( or , depending on your platform) next to the source file for direct import ( builds “in place”). The switch additionally produces an annotated html file of the source code. The cythonize command accepts multiple source files and glob patterns like as argument and also understands the common option for running multiple parallel build jobs. When called without further options, it will only translate the source files to or files. Pass the flag for a complete list of supported options. There simpler command line tool cython only invokes the source code translator. In the case of manual compilation, how to compile your files will vary depending on your operating system and compiler. The Python documentation for writing extension modules should have some details for your system. On a Linux system, for example, it might look similar to this: After compilation, a ( for Windows) file is written into the target directory and your module, , is available for you to import as with any other Python module. Note that if you are not relying on cythonize or , you will not automatically benefit from the platform specific file extension that CPython generates for disambiguation, such as on a regular 64bit Linux installation of CPython 3.5.\n\nThe setuptools extension provided with Cython allows you to pass files directly to the constructor in your setup file. If you have a single Cython file that you want to turn into a compiled extension, say with filename the associated would be: If your build depends directly on Cython in this way, then you may also want to inform pip that is required for to execute, following PEP 518, creating a file containing, at least: To understand the more fully look at the official setuptools documentation. To compile the extension for use in the current directory use: setuptools 74.1.0 adds experimental support for extensions in (instead of ): In this case, you can use any build frontend - e.g. build More details on building Cython modules that use cimport numpy can be found in the Numpy section of the user guide. If you have Cython include files or Cython definition files in non-standard places you can pass an parameter to : If you need to specify compiler options, libraries to link with or other linker options you will need to create instances manually (note that glob syntax can still be used to specify multiple extensions in one line): # Everything but primes.pyx is included here. Some useful options to know about are\n• None - list of directories to search for C/C++ header files (in Unix form for portability),\n• None - list of library names (not filenames or paths) to link against,\n• None - list of directories to search for C/C++ libraries at link time. Note that when using setuptools, you should import it before Cython, otherwise, both might disagree about the class to use here. Often, Python packages that offer a C-level API provide a way to find the necessary C header files: If your options are static (for example you do not need to call a tool like to determine them) you can also provide them directly in your or source file using a special comment block at the start of the file: If you cimport multiple .pxd files defining libraries, then Cython merges the list of libraries, so this works as expected (similarly with other options, like above). If you have some C files that have been wrapped with Cython and you want to compile them into your extension, you can define the setuptools parameter: Note that these sources are added to the list of sources of the current extension module. Spelling this out in the file looks as follows: The class takes many options, and a fuller explanation can be found in the setuptools documentation. Sometimes this is not enough and you need finer customization of the setuptools . To do this, you can provide a custom function to create the final object after Cython has processed the sources, dependencies and directives but before the file is actually Cythonized. This function takes 2 arguments and , where is the object given as input to Cython and is a with all keywords which should be used to create the . The function must return a 2-tuple , where is the created and is metadata which will be written as JSON at the top of the generated C files. This metadata is only used for debugging purposes, so you can put whatever you want in there (as long as it can be converted to JSON). The default function (defined in ) is: In case that you pass a string instead of an to , the will be an without sources. For example, if you do , the will be . Just as an example, this adds as library to every extension: If you Cythonize in parallel (using the argument), then the argument to must be pickleable. In particular, it cannot be a lambda function. The function can take extra arguments which will allow you to customize your build. Compile a set of source modules into C/C++ files and return a list of distutils Extension objects for them.\n• None module_list – As module list, pass either a glob pattern, a list of glob patterns or a list of Extension objects. The latter allows you to configure the extensions separately through the normal distutils options. You can also pass Extension objects that have glob patterns as their sources. Then, cythonize will resolve the pattern and create a copy of the Extension for every matching file.\n• None exclude – When passing glob patterns as , you can exclude certain module names explicitly by passing them into the option.\n• None nthreads – The number of concurrent builds for parallel compilation (requires the module).\n• None aliases – If you want to use compiler directives like but can only know at compile time (when running the ) which values to use, you can use aliases and pass a dictionary mapping those aliases to Python strings when calling . As an example, say you want to use the compiler directive but this path isn’t always fixed and you want to find it when running the . You can then do , find the value of in the , put it in a python variable called as a string, and then call .\n• None quiet – If True, Cython won’t print error, warning, or status messages during the compilation.\n• None force – Forces the recompilation of the Cython modules, even if the timestamps don’t indicate that a recompilation is necessary.\n• None language – To globally enable C++ mode, you can pass . Otherwise, this will be determined at a per-file level based on compiler directives. This affects only modules found based on file names. Extension instances passed into will not be changed. It is recommended to rather use the compiler directive than this option.\n• None exclude_failures – For a broad ‘try to compile’ mode that ignores compilation failures and simply excludes the failed extensions, pass . Note that this only really makes sense for compiling files which can also be used without compilation.\n• None show_all_warnings – By default, not all Cython warnings are printed. Set to true to show all warnings.\n• None annotate – If , will produce a HTML file for each of the or files compiled. The HTML file gives an indication of how much Python interaction there is in each of the source code lines, compared to plain C code. It also allows you to see the C/C++ code generated for each line of Cython code. This report is invaluable when optimizing a function for speed, and for determining when to release the GIL: in general, a block may contain only “white” code. See examples in Determining where to add types or Primes.\n• None annotate-fullc – If will produce a colorized HTML version of the source which includes entire generated C/C++-code.\n• None compiler_directives – Allow to set compiler directives in the like this: . See Compiler directives.\n• None depfile – produce depfiles for the sources if True.\n• None cache – If the cache enabled with default path. If the value is a path to a directory, then the directory is used to cache generated / files. By default cache is disabled. See Cython cache.\n\nIn some scenarios, it can be useful to link multiple Cython modules (or other extension modules) into a single binary, e.g. when embedding Python in another application. This can be done through the inittab import mechanism of CPython. Create a new C file to integrate the extension modules and add this macro to it: If you are only targeting Python 3.x, just use as prefix. Then, for each of the modules, declare its module init function as follows, replacing with the name of the module: In C++, declare them as . If you are not sure of the name of the module init function, refer to your generated module source file and look for a function name starting with . Next, before you start the Python runtime from your application code with , you need to initialise the modules at runtime using the C-API function, again inserting the name of each of the modules: This enables normal imports for the embedded extension modules. In order to prevent the joined binary from exporting all of the module init functions as public symbols, Cython 0.28 and later can hide these symbols if the macro is defined while C-compiling the module C files. Also take a look at the cython_freeze tool. It can generate the necessary boilerplate code for linking one or more modules into a single Python executable.\n\nFor building Cython modules during development without explicitly running after each change, you can use : This allows you to automatically run Cython on every that Python is trying to import. You should use this for simple Cython builds only where no extra C libraries and no special building setup is needed. It is also possible to compile new modules that are being imported (including the standard library and installed packages). For using this feature, just tell that to : In the case that Cython fails to compile a Python module, will fall back to loading the source modules instead. Note that it is not recommended to let build code on end user side as it hooks into their import system. The best way to cater for end users is to provide pre-built binary packages in the wheel packaging format. The function can take several arguments to influence the compilation of Cython or Python files. Call this to install the import hook in your meta-path for a single Python process. If you want it to be installed whenever you use Python, add it to your (as described above).\n• None pyximport – If set to False, does not try to import files.\n• None pyimport – You can pass to also install the import hook in your meta-path. Note, however, that it is rather experimental, will not work at all for some files and packages, and will heavily slow down your imports due to search and compilation. Use at your own risk.\n• None build_dir – By default, compiled modules will end up in a directory in the user’s home directory. Passing a different path as will override this.\n• None build_in_temp – If , will produce the C files locally. Working with complex dependencies and debugging becomes more easy. This can principally interfere with existing files of the same name.\n• None setup_args – Dict of arguments for Distribution. See .\n• None reload_support – Enables support for dynamic , e.g. after a change in the Cython code. Additional files may arise on that account, when the previously loaded module file cannot be overwritten.\n• None load_py_module_on_import_failure – If the compilation of a file succeeds, but the subsequent import fails for some reason, retry the import with the normal module instead of the compiled module. Note that this may lead to unpredictable results for modules that change the system state during their import, as the second import will rerun these modifications in whatever state the system was left after the import of the compiled module failed.\n• None inplace – Install the compiled module ( for Linux and Mac / for Windows) next to the source file.\n• None language_level – The source language level to use: 2 or 3. The default is to use the language level of the current Python runtime for .py files and Py2 for files. Since does not use internally, it currently requires a different setup for dependencies. It is possible to declare that your module depends on multiple files, (likely and files). If your Cython module is named and thus has the filename then you should create another file in the same directory called . The file can be a list of filenames or “globs” (like or ). Each filename or glob must be on a separate line. Pyximport will check the file date for each of those files before deciding whether to rebuild the module. In order to keep track of the fact that the dependency has been handled, Pyximport updates the modification time of your “.pyx” source file. Future versions may do something more sophisticated like informing setuptools of the dependencies directly. does not use . Thus it is not possible to do things like using compiler directives at the top of Cython files or compiling Cython code to C++. Pyximport does not give you any control over how your Cython file is compiled. Usually the defaults are fine. You might run into problems if you wanted to write your program in half-C, half-Cython and build them into a single library. Pyximport does not hide the setuptools/GCC warnings and errors generated by the import process. Arguably this will give you better feedback if something went wrong and why. And if nothing went wrong it will give you the warm fuzzy feeling that pyximport really did rebuild your module as it was supposed to. Basic module reloading support is available with the option . Note that this will generate a new module filename for each build and thus end up loading multiple shared libraries into memory over time. CPython has limited support for reloading shared libraries as such, see PEP 489. Pyximport puts both your file and the platform-specific binary into a separate build directory, usually . To copy it back into the package hierarchy (usually next to the source file) for manual reuse, you can pass the option .\n\nIt’s possible to compile code in a notebook cell with Cython. For this you need to load the Cython magic: Then you can define a Cython cell by writing on top of it. Like this: Note that each cell will be compiled into a separate extension module. So if you use a package in a Cython cell, you will have to import this package in the same cell. It’s not enough to have imported the package in a previous cell. Cython will tell you that there are “undefined global names” at compilation time if you don’t comply. The global names (top level functions, classes, variables and modules) of the cell are then loaded into the global namespace of the notebook. So in the end, it behaves as if you executed a Python cell. Additional allowable arguments to the Cython magic are listed below. You can see them also by typing in IPython or a Jupyter notebook. Produce a colorized HTML version of the source which includes entire generated C/C++-code. Output a C++ rather than C file. Force the compilation of a new module, even if the source has been previously compiled. Extra flags to pass to compiler via the extra_compile_args. Extra flags to pass to linker via the extra_link_args. Add a library to link the extension against (can be specified multiple times). Add a path to the list of library directories (can be specified multiple times). Add a path to the list of include directories (can be specified multiple times). Add a path to the list of src files (can be specified multiple times). Specify a name for the Cython module. Enable profile guided optimisation in the C compiler. Compiles the cell twice and executes it in between to generate a runtime profile.\n\nCompiler options can be set in the , before calling , like this: Here are the options that are available: Whether or not to include docstring in the Python extension. If False, the binary size will be smaller, but the attribute of any class or function will be an empty string. Embed the source code position in the docstrings of functions and classes. Decref global variables in each module on exit for garbage collection. 0: None, 1+: interned objects, 2+: cdef globals, 3+: types objects Mostly for reducing noise in Valgrind as it typically executes at process exit (when all memory will be reclaimed anyways). Note that directly or indirectly executed cleanup code that makes use of global variables or types may no longer be safe when enabling the respective level since there is no guaranteed order in which the (reference counted) objects will be cleaned up. The order can change due to live references and reference cycles. Should tp_clear() set object fields to None instead of clearing them to NULL? Generate an annotated HTML version of the input source files for debugging and optimisation purposes. This has the same effect as the argument in . This will abort the compilation on the first error occurred rather than trying to keep going and printing further error messages. Make unknown names an error. Python raises a NameError when encountering unknown names at runtime, whereas this option makes them a compile time error. If you want full Python compatibility, you should disable this option and also ‘cache_builtins’. Make uninitialized local variable reference a compile time error. Python raises UnboundLocalError at runtime, whereas this option makes them a compile time error. Note that this option affects only variables of “python object” type. This will convert statements of the form to when is a C integer type, and the direction (i.e. sign of step) can be determined. WARNING: This may change the semantics if the range causes assignment to i to overflow. Specifically, if this option is set, an error will be raised before the loop is entered, whereas without this option the loop will execute until an overflowing value is encountered. Perform lookups on builtin names only once, at module initialisation time. This will prevent the module from getting imported if a builtin name that it uses cannot be found during initialisation. Default is True. Note that some legacy builtins are automatically remapped from their Python 2 names to their Python 3 names by Cython when building in Python 3.x, so that they do not get in the way even if this option is enabled. Generate branch prediction hints to speed up error handling etc. Enable this to allow one to write to overwrite the definition if the cpdef function foo, at the cost of an extra dictionary lookup on every call. If this is false it generates only the Python wrapper and no override check. Whether or not to embed the Python interpreter, for use in making a standalone executable or calling from external libraries. This will provide a C function which initialises the interpreter and executes the body of this module. See this demo for a concrete example. If true, the initialisation function is the C main() function, but this option can also be set to a non-empty string to provide a function name explicitly. Default is False. Allows cimporting from a pyx file without a pxd file. Maximum number of dimensions for buffers – set lower than number of dimensions in numpy, as slices are passed by value and involve a lot of copying. Number of function closure instances to keep in a freelist (0: no freelists)\n\nCompiler directives are instructions which affect the behavior of Cython code. Here is the list of currently supported directives: Controls whether free functions behave more like Python’s CFunctions (e.g. ) or, when set to True, more like Python’s functions. When enabled, functions will bind to an instance when looked up as a class attribute (hence the name) and will emulate the attributes of Python functions, including introspections like argument names and annotations. Changed in version 3.0.0: Default changed from False to True If set to False, Cython is free to assume that indexing operations ([]-operator) in the code will not cause any IndexErrors to be raised. Lists, tuples, and strings are affected only if the index can be determined to be non-negative (or if is False). Conditions which would normally trigger an IndexError may instead cause segfaults or data corruption if this is set to False. In Python, arrays and sequences can be indexed relative to the end. For example, A[-1] indexes the last value of a list. In C, negative indexing is not supported. If set to False, Cython is allowed to neither check for nor correctly handle negative indices, possibly causing segfaults or data corruption. If bounds checks are enabled (the default, see above), negative indexing will usually raise an for indices that Cython evaluates itself. However, these cases can be difficult to recognise in user code to distinguish them from indexing or slicing that is evaluated by the underlying Python array or sequence object and thus continues to support wrap-around indices. It is therefore safest to apply this option only to code that does not process negative indices at all. If set to True, Cython checks that\n• None a memoryview is initialized whenever its elements are accessed or assigned to.\n• None a C++ class is initialized when it is accessed (only when is on) Setting this to False disables these checks. If set to False, Cython is free to assume that native field accesses on variables typed as an extension type, or buffer accesses on a buffer variable, never occurs when the variable is set to . Otherwise a check is inserted and the appropriate exception is raised. This is off by default for performance reasons. If set to True, Cython sets the slot to to signal that the module is safe to run without an active GIL and prevent the GIL from being enabled when the module is imported. Otherwise the slot is set to which will cause the GIL to be automatically enabled. Setting this to True does not itself make the module safe to run without the GIL; it merely confirms that you have checked the logic and consider it safe to run. Since free-threading support is still experimental itself, this is also an experimental directive that might be changed or removed in future releases. If set to True, raise errors on overflowing C integer arithmetic operations. Incurs a modest runtime penalty, but is much faster than using Python ints. If set to True, and overflowcheck is True, check the overflow bit for nested, side-effect-free arithmetic expressions once rather than at every step. Depending on the compiler, architecture, and optimization settings, this may help or hurt performance. A simple suite of benchmarks can be found in . If set to True, Cython will embed a textual copy of the call signature in the docstring of all Python visible functions and classes. Tools like IPython and epydoc can thus display the signature, which cannot otherwise be retrieved after compilation. If set to , Cython will generate signatures preserving C type declarations and Python type annotations. If set to , Cython will do a best attempt to use pure-Python type annotations in embedded signatures. For arguments without Python type annotations, the C type is mapped to the closest Python type equivalent (e.g., C is mapped to Python type and C is mapped to Python type). The specific output and type mapping are experimental and may change over time. The format generates signatures that are compatible with those understood by CPython’s Argument Clinic tool. The CPython runtime strips these signatures from docstrings and translates them into a attribute. This is mainly useful when using , since the Cython functions generated with do not have (nor need) a attribute. If set to False, Cython will adjust the remainder and quotient operators C types to match those of Python ints (which differ when the operands have opposite signs) and raise a when the right operand is 0. This has up to a 35% speed penalty. If set to True, no checks are performed. See CEP 516. If set to True, Cython will emit a runtime warning whenever division is performed with negative operands. See CEP 516. modifies the return type of , as shown in the table below: C integer (known to be >= 0 at compile time) C integer (may be negative) Return type is C double (note that Python would dynamically pick or here, while Cython doesn’t) C floating point (or C integer) Return type is floating point, result is NaN if the result would be complex Either a C real or complex number at cost of some speed The behaviour largely keeps the result type the same as the operand types, while the behaviour follows Python and returns a flexible type depending on the inputs. Introduced in Cython 3.0 with a default of False; before that, the behaviour matched the version. When disabled, uses the and signatures when constructing functions/methods which take zero or one arguments. Has no effect on special methods and functions with more than one argument. The and signatures provide slightly faster calling conventions but disallow the use of keywords. When enabled, makes the special binary operator methods ( , etc.) behave according to the low-level C-API slot semantics, i.e. only a single method implements both the normal and reversed operator. This used to be the default in Cython 0.x and was now replaced by Python semantics, i.e. the default in Cython 3.x and later is . Write hooks for Python profilers into the compiled C code. Write line tracing hooks for Python profilers or coverage reporting into the compiled C code. This also enables profiling. Note that the generated module will not actually use line tracing, unless you additionally pass the C macro definition to the C compiler (e.g. using the setuptools option ). Define to also include functions and sections. Infer types of untyped variables in function bodies. Default is None, indicating that only safe (semantically-unchanging) inferences are allowed. In particular, inferring integral types for variables used in arithmetic expressions is considered unsafe (due to possible overflow) and must be explicitly requested. Globally set the Python language level to be used for module compilation. Default is None, indicating compatibility with Python 3 in Cython 3.x and with Python 2 in Cython 0.x. To enable Python 3 source code semantics, set this to 3 (or 3str) at the start of a module or pass the “-3” or “–3str” command line options to the compiler. For Python 2 semantics, use 2 and “-2” accordingly. Before Cython 3.1, the option enabled Python 3 semantics but did not change the type and unprefixed string literals to when the compiled code runs in Python 2.x. In Cython 3.1, is an alias for . Language level 2 ignores type annotations due to the int/long ambiguity. Note that cimported files inherit this setting from the module being compiled, unless they explicitly set their own language level. Included source files always inherit this setting. Globally set the type of an implicit coercion from char* or std::string. Globally set the encoding to use when implicitly coercing char* or std:string to a unicode object. Coercion from a unicode object to C type is only allowed when set to or , the latter being utf-8 in Python 3 and nearly-always ascii in Python 2. Enables the attribute cache for extension types in CPython by setting the type flag . Default is True, meaning that the cache is enabled for Cython implemented types. To disable it explicitly in the rare cases where a type needs to juggle with its internally without paying attention to cache consistency, this option can be set to False. Whether to print tracebacks when suppressing unraisable exceptions. PEP 492 specifies that async-def coroutines must not be iterable, in order to prevent accidental misuse in non-async contexts. However, this makes it difficult and inefficient to write backwards compatible code that uses async-def coroutines in Cython but needs to interact with async Python code that uses the older yield-from syntax, such as asyncio before Python 3.5. This directive can be applied in modules or selectively as decorator on an async-def coroutine to make the affected coroutine(s) iterable and thus directly interoperable with yield-from. Uses function argument annotations to determine the type of variables. Since Python does not enforce types given in annotations, setting to False gives greater compatibility with Python code. From Cython 3.0, can be set on a per-function or per-class basis. Copy the original source code line by line into C code comments in the generated code file to help with understanding the output. This is also required for coverage analysis. Make C++ variables behave more like Python variables by allowing them to be “unbound” instead of always default-constructing them at the start of a function. See cpp_locals directive for more detail. When enabled, functions will not propagate raised exceptions by default. Hence, the function will behave in the same way as if declared with keyword. See Error return values for details. Setting this directive to will cause Cython 3.0 to have the same semantics as Cython 0.x. This directive was solely added to help migrate legacy code written before Cython 3. It will be removed in a future release. Whether to expand chained if-else statements (including statements like ) into C switch statements. This can have performance benefits if there are lots of values but cause compiler errors if there are any duplicate values (which may not be detectable at Cython compile time for all C constants). Cython can generate code that optimistically checks for Python method objects at call time and unpacks the underlying function to call it directly. This can substantially speed up method calls, especially for builtins, but may also have a slight negative performance impact in some cases where the guess goes completely wrong. Disabling this option can also reduce the code size. All warning directives take True / False as options to turn the warning on / off. Warns about any variables that are implicitly declared without a declaration Warns about code paths that are statically determined to be unreachable, e.g. returning twice unconditionally. Warns about use of variables that are conditionally uninitialized. Warns about unused assignment to the same name, such as Warns about multiple variables declared on the same line with at least one pointer type. For example - which, as in C, declares as a pointer, as a value type, but could be mininterpreted as declaring two pointers. Warns about use of the deprecated statement in Cython code, see Conditional Compilation and Deprecation of DEF / IF. Warns about use of the deprecated statement in Cython code, see Conditional Compilation and Deprecation of DEF / IF. Show performance hints during compilation pointing to places in the code which can yield performance degradation. Note that performance hints are not warnings and hence the directives starting with above do not affect them and they will not trigger a failure when “error on warnings” is enabled. One can set compiler directives through a special header comment near the top of the file, like this: The comment must appear before any code (but can appear after other comments or whitespace). One can also pass a directive on the command line by using the -X switch: Directives passed on the command line will override directives set in header comments. For local blocks, you need to cimport the special builtin module: Then you can use the directives either as decorators or in a with statement, like this: # turn off boundscheck for this function # turn it temporarily on again for this block These two methods of setting directives are not affected by overriding the directive on the command-line using the -X option. Compiler directives can also be set in the file by passing a keyword argument to : This will override the default directives as specified in the dictionary. Note that explicit per-file or local directives as explained above take precedence over the values passed to .\n\nCython has a number of C macros that can be used to control compilation. Typically, these would be set using in (for example ), however they can also be set in other ways like using the environmental variable. These macros are set automatically by Cython to sensible default values unless you chose to explicitly override them, so they are a tool that most users can happily ignore. Not all combinations of macros are compatible or tested, and some change the default value of other macros. They are listed below in rough order from most important to least important: Turns on Cython’s experimental Limited API support, meaning that one compiled module can be used by many Python interpreter versions (at the cost of some performance). At this stage many features do not work in the Limited API. You should set this macro to be the version hex for the minimum Python version you want to support (>=3.7). will support Python 3.7 upwards. Note that this is a :external+python:c:macro:`Python macro <Py_LIMITED_API>`_, rather than just a Cython macro, and so it changes what parts of the Python headers are visible too. See The Limited API and Stable ABI for more details about this feature. Uses multi-phase module initialization as described in PEP 489. This improves Python compatibility, especially when running the initial import of the code when it makes attributes such as available. It is therefore on by default where supported. Stores module data on a struct associated with the module object rather than as C global variables. The advantage is that it should be possible to import the same module more than once (e.g. in different sub-interpreters). At the moment this is experimental and not all data has been moved. Specifically, globals have not been moved. Defines es as Heap Types rather than “static types”. Practically this does not change a lot from a user point of view, but it is needed to implement Limited API support. These control the inclusion of profiling and line tracing calls in the module. See the and Compiler directives. Slightly different to the other macros, this controls how functions appear to C++ code. See C++ public declarations for full details. Controls whether C lines numbers appear in tracebacks. See C line numbers in tracebacks for a complete description. Passes complex numbers using the C or C++ language standard library types instead of an internal type defined by Cython. Turning it on maximizes compatibility with external libraries. However, MSVC has poor standards support (especially in C mode) and so struggles to use the standard library types. It is on by default on platforms where we think it’s likely to work. There is a further list of macros which turn off various optimizations or language features. Under normal circumstance Cython enables these automatically based on the version of Python you are compiling for so there is no need to use them to try to enable extra optimizations - all supported optimizations are enabled by default. These are mostly relevant if you’re tying to get Cython working in a new and unsupported Python interpreter where you will typically want to set them to 0 to disable optimizations. They are listed below for completeness but hidden by default since most users will be uninterested in changing them. If enabled, Cython will directly access members of the struct. Use the internal function for more efficient access to properties of C classes. Enable optimizations based on direct access into the internals of Python / / objects respectively. Use a faster (but internal) mechanism for building unicode strings, for example in f-strings. Avoid using “borrowed references” and ensure that Cython always holds a reference to objects it manipulates. Most useful for non-reference-counted implementations of Python, like PyPy (where it is enabled by default). Avoid using APIs that return unsafe “borrowed references” and instead use the equivalent APIs that return “strong references”. Most useful for the free-threaded build of CPython, where incrementing the reference count of borrowed references to items in mutable containers might introduce thread safety issues. Borrowed references to items in immutable containers are still allowed with this setting. Use some C-API macros that increase performance by skipping error checking, which may not be safe on all Python implementations (e.g. PyPy). Prefer the C-API macros / inline-functions for builtin types over their counterparts if errors are not expected. On some Python versions this speeds up getting/releasing the GIL. Try to speed up method calls at the cost of code-size. Linked to the compiler directive - this macro is used to selectively enable the compiler directive only on versions of Python that support it. These are used internally to incrementally enable the vectorcall calling mechanism on older Python versions (<3.8). Use the type-slot instead of , as described in PEP 442. Try to optimize attribute lookup by using versioned dictionaries where supported. Use an internal structure to track exception state, used in CPython 3.7 and later. Attempt to provide docstrings also for special (double underscore) methods. Enable the use of freelists on extension types with the @cython.freelist decorator. Enable the use of atomic reference counting (as opposed to locking then reference counting) in Cython typed memoryviews. Debug option for including constant (string/integer/code/…) objects in . By default, Cython avoids GC traversing these objects because they can never participate in reference cycles, and thus would uselessly waste time during garbage collection runs. Makes module state lookup thread-safe (when and are both enabled). This is on by default where it would be helpful, however it can be disabled if you are sure that one interpreter will not be importing your module at the same time as another is using it. Values greater than 1 can be used to select a specific implementation for debugging purposes."
    },
    {
        "link": "https://cython.readthedocs.io/en/latest/src/quickstart/build.html",
        "document": "Cython code must, unlike Python, be compiled. This happens in two stages:\n\nTo understand fully the Cython + setuptools build process, one may want to read more about distributing Python modules.\n\nThere are several ways to build Cython code:\n• None Write a setuptools . This is the normal and recommended way.\n• None Run the command-line utility. This is a good approach for compiling a single Cython source file directly to an extension. A source file can be built “in place” (so that the extension module is created next to the source file, ready to be imported) with .\n• None Use Pyximport, importing Cython files as if they were files (using setuptools to compile and build in the background). This method is easier than writing a , but is not very flexible. So you’ll need to write a if, for example, you need certain compilations options.\n• None Run the command-line utility manually to produce the file from the file, then manually compiling the file into a shared object library or DLL suitable for import from Python. (These manual steps are mostly for debugging and experimentation.)\n• None Use the [Jupyter] notebook or the [Sage] notebook, both of which allow Cython code inline. This is the easiest way to get started writing Cython code and running it.\n\nCurrently, using setuptools is the most common way Cython files are built and distributed. The other methods are described in more detail in the Source Files and Compilation section of the reference manual.\n\nCython can be used conveniently and interactively from a web browser through the Jupyter notebook. To install Jupyter notebook, e.g. into a virtualenv, use pip: To enable support for Cython compilation, install Cython as described in the installation guide and load the extension from within the Jupyter notebook: Then, prefix a cell with the marker to compile it You can show Cython’s code analysis by passing the option: For more information about the arguments of the magic, see Compiling with a Jupyter Notebook."
    },
    {
        "link": "https://cython.org",
        "document": "What users have to say about Cython: »You would expect a whole lot of organizations and people to fancy a language that's about as high-level as Python, yet almost as fast and down-to-the-metal as C. Add to that the ability to seamlessly integrate with both your existing C/++ codebase and your Python codebase, easily mix very high level abstractions with very low-level machine access... clear winner.« → Dun Peal on c.l.py »You guys rock! In scikit-learn, we have decided early on to do Cython, rather than C or C++. That decision has been a clear win because the code is way more maintainable. We have had to convince new contributors that Cython was better for them, but the readability of the code, and the capacity to support multiple Python versions, was worth it.« → Gaël Varoquaux »The biggest surprise (and of course this is Cython's selling point) is how simple the interfacing between high level and low level code becomes, and the fact that it is all very robust. It's exiciting to see that there are several active projects around that attempt to speed up Python. The nice thing about Cython is that it doesn't give you \"half the speed of C\" or \"maybe nearly the speed of C, 3 years from now\" -- it gives the real deal, -O3 C, and it works right now.« → Fredrik Johansson »SciPy is approximately 50% Python, 25% Fortran, 20% C, 3% Cython and 2% C++ … The distribution of secondary programming languages in SciPy is a compromise between a powerful, performance-enhancing language that interacts well with Python (that is, Cython) and the usage of languages (and their libraries) that have proven reliable and performant over many decades. For implementing new functionality, Python is still the language of choice. If Python performance is an issue, then we prefer the use of Cython followed by C, C++ or Fortran (in that order). The main motivation for this is maintainability: Cython has the highest abstraction level, and most Python developers will understand it. C is also widely known, and easier for the current core development team to manage than C++ and especially Fortran.« → Pauli Virtanen et al., SciPy »Not to mention that the generated C often makes use of performance tricks that are too tedious or arcane to write by hand, partially motivated by scientific computing’s constant push. And through all that, Cython code maintains a high level of integration with Python itself, right down to the stack trace and line numbers. PayPal has certainly benefitted from their efforts through high-performance Cython users like gevent, lxml, and NumPy. While our first go with Cython didn’t stick in 2011, since 2015, all native extensions have been written and rewritten to use Cython.« → Mahmoud Hashemi »Cython produces binaries much like C++, Go, and Rust do. Now with GitHub Actions the cross-platform build and release process can be automated for free for Open Source projects. This is an enormous opportunity to make the Python ecosystem 20-50% faster with a single pull request.« → Grant Jenks »I'm honestly never going back to writing C again. Cython gives me all the expressiveness of Python combined with all the performance and close-to-the-metal-godlike-powers of C. I've been using it to implement high-performance graph traversal and routing algorithms and to interface with C/C++ libraries, and it's been an absolute amazing productivity boost.« → Andrew Tipton »A general rule of thumb is that your program spends 80% of its time running 20% of the code. Thus a good strategy for efficient coding is to write everything, profile your code, and optimize the parts that need it. Python’s profilers are great, and Cython allows you to do the latter step with minimal effort.« → Hoyt Koepke »The question was, in auto-generated code, to what extent there were bugs there, to what extent there were bugs in the generators. The first time I did this, I got lots and lots of warnings from the tool for code generated by both SWIG and Cython [...] Basically, everything I found Cython emitting was a false positive and a bug in my checker tool [CPyChecker].« → David Malcolm »Basically, Cython is about 7x times faster than Boost.Python, which astonished me.« → Chris Chou »Using Cython allows you to just put effort into speeding up the parts of code you need to work on, and to do so without having to change very much. This is vastly different from ditching all the code and reimplementing it another language. It also requires you to learn a pretty minimal amount of stuff. You also get to keep the niceness of the Python syntax which may Python coders have come to appreciate.« → Craig Macomber »If you have a piece of Python that you need to run fast, then I would recommend you used Cython immediately. This means that I can exploit the beauty of Python and the speed of C together, and that’s a match made in heaven.« → Stavros »From 85 seconds (at the beginning of this post) down to 0.8 seconds: a reduction by a factor of 100 ...thank you cython! :-)« → André Roberge »Writing a full-on CPython module from scratch would probably offer better performance than Cython if you know the quirks and are disciplined. But to someone who doesn't already drip CPython C modules, Cython is a godsend. Ultimately, there's 5 commonly used ways (CPython [C-API], Boost::Python, SWIG, Cython, ctypes) to integrate C into Python, and right now you'd be crazy not to give Cython a shot, if that's your need. It's very easy to learn for anyone familiar with both C and Python.« → ashika »What I loved about the Cython code is that I use a Python list to manage the Vortex objects. This shows that we can use the normal Python containers to manage objects. This is extremely convenient. [...] Clearly, if you are building code from scratch and need speed, Cython is an excellent option. For this I really must congratulate the Cython and Pyrex developers.« → Prabhu Ramachandran »I wrote a script that compute a distance matrix (O^2) in Python with Numpy arrays and the same script in Cython. It took me 10 minutes to figure it out how Cython works and I gained a speed up of 550 times !!! Amazing« → kfrancoi »I would like to report on a successful Cython project. Successful in the sense that it was much faster than all code written by my predecessors mainly because the speed scales almost linearly with the number of cores. Also, the code is shorter and much easier to read and maintain. [...] Making it this fast & short & readable & maintainable would have been pretty hard without Cython.« → Alex van Houten »At work, we’ve started using Cython with excellent success. We rewrote one particular Perl script as Cython and achieved a 600% speed improvement. As a Perl lover, this was impressive. We still get all the benefits of Python such as rapid development and clean object-oriented design patterns but with the speed of C.« → Wim Kerkhoff »The reason that I was interested in Cython was the long calculation times I encountered while doing a multi-variable optimization with a function evaluation that involved solving a differential equation with scipy.integrate.odeint. By simply replacing the class that contained the differential equation with a Cython version the calculation time dropped by a factor 5. Not bad for half a Sunday afternoons work.« → Korbinin »I was surprised how simple it was to get it working both under Windows and Linux. I did not have to mess with make files or configure the compiles. Cython integrated well with NumPy and SciPy. This expands the programming tasks you can do with Python substantially.« → Sami Badawi »This is why the Scipy folks keep harping about Cython – it’s rapidly becoming (or has already become) the lingua franca of exposing legacy libraries to Python. Their user base has tons of legacy code or external libraries that they need to interface, and most of the reason Python has had such a great adoption curve in that space is because Numpy has made the data portion of that interface easy. Cython makes the code portion quite painless, as well.« → Peter Z. Wang »Added an optional step of compiling fastavro with Cython. Just doing that, with no Cython specific code reduced the time of processing 10K records from 2.9sec to 1.7sec. Not bad for that little work.« → Miki Tebeka »fastavro compiles the Python code without any specific Cython code. This way on machines that do not have a compiler users can still use fastavro. The end result is a package that reads Avro faster than Java and supports both Python 2 and Python 3. Using Cython and a little bit of work th[is] was achieved without too much effort.« → Miki Tebeka »... the binding needed to be rewritten, mainly because the current binding is directly written in C++ and is a maintenance nightmare. This new binding is written in Cython« → Bastien Léonard » Code generation via Cython allows the production of smaller and more maintainable bindings, including increased compatibility with all supported Python releases without additional burden for NEST developers. « This approach resulted in a reduction of the code footprint of around 50% and a significant increase in the cohesiveness of the code related to the Python bindings: whereas previously seven core files and 22 additional files were involved, the new approach requires merely two core files. The new implementation also removes the compile-time dependency on NumPy and provides numerous additional maintainability benefits by reducing complexity and increasing comprehensibility of the code. The re-write of the build system also resulted in a 50% reduction of code, and resolved multiple issues with its usability and robustness. « This approach resulted in a reduction of the code footprint of around 50% and a significant increase in the cohesiveness of the code related to the Python bindings: whereas previously seven core files and 22 additional files were involved, the new approach requires merely two core files. The new implementation also removes the compile-time dependency on NumPy and provides numerous additional maintainability benefits by reducing complexity and increasing comprehensibility of the code. The re-write of the build system also resulted in a 50% reduction of code, and resolved multiple issues with its usability and robustness. « » In conclusion, we hope that through a more widespread use of Cython, neuroscientific software developers will be able to focus their creative energy on refining their algorithms and implementing new features, instead of working to pay off the interest on the accumulating technical debt. « → Yury V. Zaytsev and Abigail Morrison » The Cython version took about 30 minutes to write, and it runs just as fast as the C code — because, why wouldn’t it? It *is* C code, really, with just some syntactic sugar. And you don’t even have to learn or think about a foreign, complicated C API…You just, write C. Or C++ — although that’s a little more awkward. Both the Cython version and the C version are about 70x faster than the pure Python version, which uses Numpy arrays. « → Matthew Honnibal » I love this project. Fantastic way to write Python bindings for native libs or speed up computationally intensive code without having to write C yourself. « → schmichael » I use a lot of pyrex/cython to bind to libraries - it's so much faster to code in python. It's been a huge boon. Having used swig, hand writing wrappers, and pyrex before i can say i much prefer cython. Thank you for the hard work. « → jnazario » I am not good with C so I mostly do pure python for my research. However, now dealing with clusters of 1000+ molecules, there was huge bottlenecks in my code. Using cython it went from running single calculation in hours to seconds, focking nice... « → fishtickler » Cython saves you from a great many of the gotchas [that C has]. The worst you'll usually get is a lack of performance gain (at which point cython -a is your friend). Wringing out all the performance you can get can require a reasonable working knowledge of C -- but you don't have to know it that well to do pretty darn well. « → lmcinnes » [spaCy is] written in clean but efficient Cython code, which allows us to manage both low level details and the high-level Python API in a single codebase. « → Matthew Honnibal » [uvloop] is written in Cython, and by the way, Cython is just amazing. It's unfortunate that it's not as wide-spread and I think it's kind-a underappreciated what you can do in Cython. Essentially, it's a superset of the Python language, you can strictly type it and it will compile to C and you will have C speed. You can easily achieve it, with a syntax more similar to Python. Definitely check out Cython. « → Yury Selivanov (video@22:50) » 300.000 req/sec is a number comparable to Go's built-in web server (I'm saying this based on a rough test I made some years ago). Given that Go is designed to do exactly that, this is really impressive. My kudos to your choice to use Cython. « → beertown » Cython is one of the best kept secrets of Python. It extends Python in a direction that addresses many of the shortcomings of the language and the platform « → Ulaş Türkmen"
    },
    {
        "link": "https://stackoverflow.com/questions/46303106/compiling-multiple-c-files-with-cython",
        "document": "How do I compile using Cython, a C function from file , where this also depends on further C files and . This seems like it must be a common use of Cython but after reading through the documentation, I still cannot figure out how to do it. My directory contains the following files\n\nThe and files contain standalone functionality. The file uses both of these and contains the function I want to be exposed to Python, . The file tests that all the C functionality runs correctly i.e. I can execute\n\nto compile into a executable that works.\n\nMy issue now is trying to compile this with Cython. My file is\n\nI then use the basic\n\nHowever if I try and compile this using\n\nwhere structure is declared in .\n\nHow do I tell the compiler we must also consider the and files when compiling.\n\nIf doing as said in the comments, I include and in then the program now compiles. However when I try to import the module in Python I get an :\n\nFurther, if I change the extern declaration line in to\n\nI then get an import error\n\nwhich is a function in"
    },
    {
        "link": "https://packaging-guide.openastronomy.org/en/latest/extensions.html",
        "document": "Python packages can include compiled extensions in a variety of languages, most commonly C and Cython (Cython is a language close to Python that can be automatically translated into C). An extension, once compiled, looks just like a regular Python module/sub-module.\n\nThere are a number of reasons why you might want to include compiled extensions including for example to speed up code that is otherwise slow in Python, or because you want to include an existing stable library without having to re-implement it in Python.\n\nTo define an extension, we need to create an instance of inside the file. For a simple case with a single file, this would look like: Here is the final name the compiled extension will have, which means that if the extension defines a function it can be imported as: The argument should be set to a list of source files to compile and link together to create the extension, and the filenames should be relative to the file. If your extension uses the Numpy C API, you should also specify the Numpy include directory using: There are a number of other options that can be passed to set for example what other libraries to link to, flags or macros to pass to the compiler, and so on. For more information about these, see the in the Python documentation. Once your extension has been defined, you should pass a list of extensions to the keyword argument to the function in the file: If you want to build a Cython extension instead of a C extension, specify the file(s) in the argument: And make sure you also add to your build-time dependencies:\n\nFor packages with many extensions, you might want to consider using the extension-helpers package. This package serves two main purposes:\n• None For single-file Cython extensions, it will automatically discover and define these extensions.\n• None For other extensions, it allows you to define extensions inside files which can be anywhere in your package. These files should contain a single function called that returns a list of extensions. The idea is then to make it easier to manage extensions for large packages by placing the files close to the extension code. To use extension-helpers, first make sure it is included in your file as a build-time dependency: Then adjust your to include: Finally, if needed, create files in sub-modules where you have extensions, add a function, and make sure that it returns a list of objects."
    },
    {
        "link": "http://docs.aiohttp.org/en/stable/client_advanced.html",
        "document": "is the heart and the main entry point for all client API operations. Create the session first, use the instance for performing HTTP requests and initiating WebSocket connections. The session contains a cookie storage and connection pool, thus cookies and connections are shared between HTTP requests sent by the same session.\n\nIf you need to add HTTP headers to a request, pass them in a to the headers parameter. For example, if you want to specify the content-type directly: You also can set default headers for all session requests: Typical use case is sending JSON body. You can specify content type directly as shown above, but it is more convenient to use special keyword :\n\nInstead of setting the header directly, and individual request methods provide an argument. An instance of can be passed in like this: Note that if the request is redirected and the redirect URL contains credentials, those credentials will supersede any previously set credentials. In other words, if redirects to , the second request will be authenticated as . Providing both the parameter and authentication in the initial URL will result in a . For other authentication flows, the header can be set directly: The authentication header for a session may be updated as and when required. For example: Note that a copy of the headers dictionary is set as an attribute when creating a instance (as a object). Updating the original dictionary does not have any effect. In cases where the authentication header value expires periodically, an task may be used to update the session’s default headers in the background. header will be removed if you get redirected to a different host or protocol.\n\nWe can view the server’s response using a : The dictionary is special, though: it’s made just for HTTP headers. According to RFC 7230, HTTP Header names are case-insensitive. It also supports multiple values for the same key as HTTP protocol does. So, we can access the headers using any capitalization we want: All headers are converted from binary data using UTF-8 with option. That works fine on most cases but sometimes unconverted data is needed if a server uses nonstandard encoding. While these headers are malformed from RFC 7230 perspective they may be retrieved by using property: If a response contains some HTTP Cookies, you can quickly access them: Response cookies contain only values, that were in headers of the last request in redirection chain. To gather cookies between all redirection requests please use aiohttp.ClientSession object.\n\nThe execution flow of a specific request can be followed attaching listeners coroutines to the signals provided by the instance, this instance will be used as a parameter for the constructor having as a result a client that triggers the different signals supported by the . By default any instance of class comes with the signals ability disabled. The following snippet shows how the start and the end signals of a request flow can be followed: The is a list that can contain instances of class that allow run the signals handlers coming from different instances. The following example shows how two different that have a different nature are installed to perform their job in each signal handle: All signals take as a parameters first, the instance used by the specific request related to that signals and second, a instance called . The object can be used to share the state through to the different signals that belong to the same request and to the same class, perhaps: The param is by default a that is initialized at the beginning of the request flow. However, the factory used to create this object can be overwritten using the constructor param of the class. The param can given at the beginning of the request execution, accepted by all of the HTTP verbs, and will be passed as a keyword argument for the factory. This param is useful to pass data that is only available at request time, perhaps: Tracing Reference section for more information about the different signals supported.\n\nTo tweak or change transport layer of requests you can pass a custom connector to and family. For example: By default session object takes the ownership of the connector, among other things closing the connections once the session is closed. If you are keen on share the same connector through different session instances you must give the connector_owner parameter as False for each session instance. Connectors section for more information about different connector types and configuration options. To limit amount of simultaneously opened connections you can pass limit parameter to connector: The example limits total amount of parallel connections to . If you explicitly want not to have limits, pass . For example: To limit amount of simultaneously opened connection to the same endpoint ( triple) you can pass limit_per_host parameter to connector: The example limits amount of parallel connections to the same to . The default is (no limit on per host bases). By default comes with the DNS cache table enabled, and resolutions will be cached by default for seconds. This behavior can be changed either to change of the TTL for a resolution, as can be seen in the following example: or disabling the use of the DNS cache table, meaning that all requests will end up making a DNS resolution, as the following example shows: In order to specify the nameservers to when resolving the hostnames, aiodns is required: If your HTTP server uses UNIX domain sockets you can use : If your HTTP server uses Named pipes you can use : It will only work with the ProactorEventLoop\n\nBy default aiohttp uses strict checks for HTTPS protocol. Certification checks can be relaxed by setting ssl to : If you need to setup custom ssl parameters (use own certification files for example) you can create a instance and pass it into the methods or set it for the entire session with . There are explicit errors when ssl verification fails If you need to skip both ssl related errors By default, Python uses the system CA certificates. In rare cases, these may not be installed or Python is unable to find them, resulting in a error like One way to work around this problem is to use the package: If you need to verify self-signed certificates, you need to add a call to with the key pair: You may also verify certificates via SHA256 fingerprint: Note that this is the fingerprint of the DER-encoded certificate. If you have the certificate in PEM format, you can convert it to DER with e.g: Tip: to convert from a hexadecimal digest to a binary byte-string, you can use . ssl parameter could be passed to as default, the value from and others override default.\n\naiohttp supports plain HTTP proxies and HTTP proxies that can be upgraded to HTTPS via the HTTP CONNECT method. aiohttp has a limited support for proxies that must be connected to via — see the info box below for more details. To connect, use the proxy parameter: Authentication credentials can be passed in proxy URL: And you may set default proxy: Contrary to the library, it won’t read environment variables by default. But you can do so by passing into constructor.: aiohttp uses for reading the proxy configuration (e.g. from the HTTP_PROXY etc. environment variables) and applies them for the HTTP, HTTPS, WS and WSS schemes. Hosts defined in will bypass the proxy. Added in version 3.8: WS_PROXY and WSS_PROXY are supported since aiohttp v3.8. Proxy credentials are given from file if present (see for more details). As of now (Python 3.10), support for TLS in TLS is disabled for the transports that uses. If the further release of Python (say v3.11) toggles one attribute, it’ll just work™. aiohttp v3.8 and higher is ready for this to happen and has code in place supports TLS-in-TLS, hence sending HTTPS requests over HTTPS proxy tunnels. ⚠️ For as long as your Python runtime doesn’t declare the support for TLS-in-TLS, please don’t file bugs with aiohttp but rather try to help the CPython upstream enable this feature. Meanwhile, if you really need this to work, there’s a patch that may help you make it happen, include it into your app’s code base: https://github.com/aio-libs/aiohttp/discussions/6044#discussioncomment-1432443. When supplying a custom instance, bear in mind that it will be used not only to establish a TLS session with the HTTPS endpoint you’re hitting but also to establish a TLS tunnel to the HTTPS proxy. To avoid surprises, make sure to set up the trust chain that would recognize TLS certificates used by both the endpoint and the proxy.\n\nWhen closes at the end of an block (or through a direct call), the underlying connection remains open due to asyncio internal details. In practice, the underlying connection will close after a short while. However, if the event loop is stopped before the underlying connection is closed, a warning is emitted (when warnings are enabled). To avoid this situation, a small delay must be added before closing the event loop to allow any open underlying connections to close. For a without SSL, a simple zero-sleep ( ) will suffice: # Zero-sleep to allow underlying connections to close For a with SSL, the application must wait a short duration before closing: # Wait 250 ms for the underlying SSL connections to close Note that the appropriate amount of time to wait will vary from application to application. All if this will eventually become obsolete when the asyncio internals are changed so that aiohttp itself can wait on the underlying connection to close. Please follow issue #1925 for the progress on this."
    },
    {
        "link": "http://docs.aiohttp.org/en/stable/client_reference.html",
        "document": "Client session is the recommended interface for making HTTP requests. Session encapsulates a connection pool (connector instance) and supports keepalives by default. Unless you are connecting to a large, unknown number of different servers over the lifetime of your application, it is suggested you use a single session for the lifetime of your application to benefit from connection pooling. The client session supports the context manager protocol for self closing. The class for creating client sessions and making requests.\n• None Base part of the URL (optional) If set, allows to join a base part to relative URLs in request calls. If the URL has a path it must have a trailing (as in https://docs.aiohttp.org/en/stable/). Note that URL joining follows RFC 3986. This means, in the most common case the request URLs should have no leading slash, e.g.:\n• None cookies (dict) – Cookies to send with the request (optional)\n• None HTTP Headers to send with every request (optional). May be either iterable of key-value pairs or (e.g. , ).\n• None set of headers for which autogeneration should be skipped. aiohttp autogenerates headers like or if these headers are not explicitly passed. Using parameter allows to skip that generation. Note that autogeneration can’t be skipped.\n• None auth (aiohttp.BasicAuth) – an object that represents HTTP Basic Authorization (optional). It will be included with any request. However, if the parameter is set, the request URL’s origin must match the base URL’s origin; otherwise, the default auth will not be included.\n• None request_class (aiohttp.ClientRequest) – Custom class to use for client requests.\n• None response_class (ClientResponse) – Custom class to use for client responses.\n• None ws_response_class (ClientWebSocketResponse) – Custom class to use for websocket responses.\n• By default every session instance has own private cookie jar for automatic cookies processing but user may redefine this behavior by providing own jar implementation. One example is not processing cookies at all when working in proxy mode. If no cookie processing is needed, a instance can be provided.\n• Setting the parameter to allows to share connection pool between sessions without sharing session state: cookies etc.\n• None Automatically call for each response, by default. This parameter can be overridden when making a request, e.g.: Set the parameter to if you need for most of cases but override for those requests where you need to handle responses with status 400 or higher. You can also provide a coroutine which takes the response as an argument and can raise an exception based on custom logic, e.g.: 'I wanted to see \"apple pie\" in response' As with boolean values, you’re free to set this on the session and/or overwrite it on a per-request basis.\n• Changed in version 3.10.9: The default value for the timeout has been changed to 30 seconds.\n• None Trust environment settings for proxy configuration if the parameter is ( by default). See Proxy support for more information. Get proxy credentials from file if present. Get HTTP Basic Auth credentials from file if present. If environment variable is set, read from file specified there rather than from . Changed in version 3.9: Added support for reading HTTP Basic Auth credentials from file.\n• None trace_configs – A list of instances used for client tracing. (default) is used for request tracing disabling. See Tracing Reference for more information.\n• None A callable that accepts a and the contents, and returns a which will be used as the encoding parameter to . This function will be called when the charset is not known (e.g. not specified in the Content-Type header). The default function simply defaults to . if the session has been closed, otherwise. derived instance used for the session. Gives access to cookie jar’s content and modifiers. aiohttp re quote’s redirect urls by default, but some servers require exact url from location header. To disable re-quote system set attribute to . Deprecated since version 3.5: The attribute modification is deprecated. Default client timeouts, instance. The value can be tuned by passing timeout parameter to constructor. HTTP Headers that sent with every request May be either iterable of key-value pairs or (e.g. , ). Set of headers for which autogeneration skipped. Should connector be closed on session closing Should be called for each response Should the body response be automatically decompressed Trust environment settings for proxy configuration or ~/.netrc file if present. See Proxy support for more information. A list of instances used for client tracing. (default) is used for request tracing disabling. See Tracing Reference for more information. Performs an asynchronous HTTP request. Returns a response object that should be used as an async context manager.\n• None url – Request URL, or that will be encoded with (see to skip encoding).\n• None Mapping, iterable of tuple of key/value pairs or string to be sent as parameters in the query string of the new request. Ignored for subsequent redirected requests (optional)\n• None with preferably url-encoded content (Warning: content will not be encoded by aiohttp)\n• None data – The data to send in the body of the request. This can be a object or anything that can be passed into , e.g. a dictionary, bytes, or file-like object. (optional)\n• None json – Any json compatible python object (optional). json and data parameters could not be used at the same time.\n• Global session cookies and the explicitly set cookies will be merged when sending the request.\n• None headers (dict) – HTTP Headers to send with the request (optional)\n• None set of headers for which autogeneration should be skipped. aiohttp autogenerates headers like or if these headers are not explicitly passed. Using parameter allows to skip that generation.\n• None allow_redirects (bool) – Whether to process redirects or not. When , redirects are followed (up to times) and logged into and . When , the original response is returned. by default (optional).\n• None max_redirects (int) – Maximum number of redirects to follow. is raised if the number is exceeded. Ignored when . by default.\n• None compress (bool) – Set to if request has to be compressed with deflate encoding. If can not be combined with a Content-Encoding and Content-Length headers. by default (optional).\n• None chunked (int) – Enable chunked transfer encoding. It is up to the developer to decide how to chunk data streams. If chunking is enabled, aiohttp encodes the provided chunks in the “Transfer-encoding: chunked” format. If chunked is set, then the Transfer-encoding and content-length headers are disallowed. by default (optional).\n• None response if set to . If set to value from will be used. by default (optional).\n• None read_until_eof (bool) – Read response until EOF if response does not have Content-Length header. by default (optional).\n• Changed in version 3.3: The parameter is instance, is still supported for sake of backward compatibility. If is passed it is a total timeout (in seconds).\n• None ( is used), for skip SSL certificate validation, for fingerprint validation, for custom SSL certificate validation.\n• None Sets or overrides the host name that the target server’s certificate will be matched against. See for more information.\n• None HTTP headers to send to the proxy if the parameter proxy has been provided.\n• None Object used to give as a kw param for each new object instantiated, used to give information to the tracers that is only available at request time.\n• None by default, it means that the session global value is used.\n• None auto_decompress (bool) – Automatically decompress response body. Overrides . May be used to enable/disable auto decompression on a per-request basis. In order to modify inner parameters, provide .\n• None allow_redirects (bool) – Whether to process redirects or not. When , redirects are followed and logged into . When , the original response is returned. by default (optional). In order to modify inner parameters, provide .\n• None data – Data to send in the body of the request; see for details (optional) In order to modify inner parameters, provide .\n• None data – Data to send in the body of the request; see for details (optional) In order to modify inner parameters, provide . In order to modify inner parameters, provide .\n• None allow_redirects (bool) – Whether to process redirects or not. When , redirects are followed and logged into . When , the original response is returned. by default (optional). In order to modify inner parameters, provide .\n• None allow_redirects (bool) – Whether to process redirects or not. When , redirects are followed and logged into . When , the original response is returned. by default (optional). In order to modify inner parameters, provide .\n• None data – Data to send in the body of the request; see for details (optional)\n• None url – Websocket server url, or that will be encoded with (see to skip encoding).\n• None timeout – a timeout for websocket. By default, the value is used ( seconds for the websocket to close). means no timeout will be used.\n• None autoclose (bool) – Automatically close websocket connection on close message from server. If autoclose is False then close procedure has to be handled manually. by default\n• None autoping (bool) – automatically send pong on ping message from server. by default\n• None heartbeat (float) – Send ping message every heartbeat seconds and wait pong response, if pong response is not received then close connection. The timer is reset on any data reception.(optional)\n• None Mapping, iterable of tuple of key/value pairs or string to be sent as parameters in the query string of the new request. Ignored for subsequent redirected requests (optional)\n• None with preferably url-encoded content (Warning: content will not be encoded by aiohttp)\n• None headers (dict) – HTTP Headers to send with the request (optional)\n• None ( is used), for skip SSL certificate validation, for fingerprint validation, for custom SSL certificate validation.\n• None Perform SSL certificate validation for HTTPS requests (enabled by default). May be disabled to skip validation for sites with invalid certificates.\n• None Pass the SHA256 digest of the expected certificate in DER format to verify that the certificate the server presents matches. Useful for certificate pinning. Note: use of MD5 or SHA1 digests is insecure and deprecated.\n• ssl_context may be used for configuring certification authority channel, supported SSL options etc.\n• None HTTP headers to send to the proxy if the parameter proxy has been provided.\n• None 0 for disable, 9 to 15 for window bit support. Default value is 0.\n• None 4 MB by default. To disable the size limit use . Detach connector from session without closing the former. Session is switched to closed state anyway.\n\nWhile we encourage usage we also provide simple coroutines for making HTTP requests. Basic API is good for performing simple HTTP requests without keepaliving, cookies and complex connection stuff like properly configured SSL certification chaining. Asynchronous context manager for performing an asynchronous HTTP request. Returns a response object. Use as an async context manager.\n• None url – Request URL, or that will be encoded with (see to skip encoding).\n• None Mapping, iterable of tuple of key/value pairs or string to be sent as parameters in the query string of the new request. Ignored for subsequent redirected requests (optional)\n• None data – The data to send in the body of the request. This can be a object or anything that can be passed into , e.g. a dictionary, bytes, or file-like object. (optional)\n• None json – Any json compatible python object (optional). json and data parameters could not be used at the same time.\n• None cookies (dict) – HTTP Cookies to send with the request (optional)\n• None headers (dict) – HTTP Headers to send with the request (optional)\n• None set of headers for which autogeneration should be skipped. aiohttp autogenerates headers like or if these headers are not explicitly passed. Using parameter allows to skip that generation.\n• None allow_redirects (bool) – Whether to process redirects or not. When , redirects are followed (up to times) and logged into and . When , the original response is returned. by default (optional).\n• None max_redirects (int) – Maximum number of redirects to follow. is raised if the number is exceeded. Ignored when . by default.\n• None compress (bool) – Set to if request has to be compressed with deflate encoding. If can not be combined with a Content-Encoding and Content-Length headers. by default (optional).\n• None chunked (int) – Enables chunked transfer encoding. It is up to the developer to decide how to chunk data streams. If chunking is enabled, aiohttp encodes the provided chunks in the “Transfer-encoding: chunked” format. If chunked is set, then the Transfer-encoding and content-length headers are disallowed. by default (optional).\n• None for response if set to . If set to value from will be used. by default (optional).\n• None read_until_eof (bool) – Read response until EOF if response does not have Content-Length header. by default (optional).\n• None SSL validation mode. for default SSL check ( is used), for skip SSL certificate validation, for fingerprint validation, for custom SSL certificate validation.\n• None Sets or overrides the host name that the target server’s certificate will be matched against. See for more information.\n• None proxy_headers (collections.abc.Mapping) – HTTP headers to send to the proxy if the parameter proxy has been provided.\n• None trace_request_ctx – Object used to give as a kw param for each new object instantiated, used to give information to the tracers that is only available at request time.\n• None by default, it means that the session global value is used.\n• None auto_decompress (bool) – Automatically decompress response body. May be used to enable/disable auto decompression on a per-request basis.\n• None for regular TCP sockets (both HTTP and HTTPS schemes supported).\n• None for connecting via UNIX socket (it’s used mostly for testing purposes). All connector classes should be derived from . By default all connectors support keep-alive connections (behavior is controlled by force_close constructor’s parameter).\n• None keepalive_timeout (float) – timeout for connection reusing after releasing (optional). Values . For disabling keep-alive feature use flag.\n• None limit (int) – total number simultaneous connections. If limit is the connector has no limit (default: 100).\n• None limit_per_host (int) – limit simultaneous connections to the same endpoint. Endpoints are the same if they are have equal triple. If limit is the connector has no limit (default: 0).\n• None some SSL servers do not properly complete SSL shutdown process, in that case asyncio leaks SSL connections. If this parameter is set to True, aiohttp additionally aborts underlining transport after 2 seconds. It is off by default. For Python version 3.12.7+, or 3.13.1 and later, this parameter is ignored because the asyncio SSL connection leak is fixed in these versions of Python.\n• None event loop used for handling connections. If param is , is used for getting default event loop. Read-only property, if connector should ultimately close connections on releasing. The total number for simultaneous connections. If limit is 0 the connector has no limit. The default limit size is 100. The limit for simultaneous connections to the same endpoint. Endpoints are the same if they are have equal triple. If limit_per_host is the connector has no limit per host. Get a free connection from pool or create new one if connection is absent in the pool. The call may be paused if is exhausted until used connections returns to pool. Abstract method for actual connection establishing, should be overridden in subclasses. Connector for working with HTTP and HTTPS via TCP sockets. The most common transport. When you don’t know what connector type to use, use a instance. Constructor accepts all parameters suitable for plus several TCP-specific ones: ( is used), for skip SSL certificate validation, for fingerprint validation, for custom SSL certificate validation.\n• None perform SSL certificate validation for HTTPS requests (enabled by default). May be disabled to skip validation for sites with invalid certificates. Deprecated since version 2.3: Pass verify_ssl to etc.\n• None pass the SHA256 digest of the expected certificate in DER format to verify that the certificate the server presents matches. Useful for certificate pinning. Note: use of MD5 or SHA1 digests is insecure and deprecated. Deprecated since version 2.3: Pass verify_ssl to etc.\n• None use internal cache for DNS lookups, by default. Enabling an option may speedup connection establishing a bit but may introduce some side effects also.\n• None expire after some seconds the DNS entries, means cached forever. By default 10 seconds (optional). In some environments the IP addresses related to a specific HOST can change after a specific time. Use this option to keep the DNS cache updated refreshing each entry after N seconds.\n• None limit (int) – total number simultaneous connections. If limit is the connector has no limit (default: 100).\n• None limit_per_host (int) – limit simultaneous connections to the same endpoint. Endpoints are the same if they are have equal triple. If limit is the connector has no limit (default: 0).\n• None custom resolver instance to use. by default (asynchronous if is installed). Custom resolvers allow to resolve hostnames differently than the way the host is configured. The resolver is by default, asynchronous version is pretty robust but might fail in very rare cases.\n• None TCP socket family, both IPv4 and IPv6 by default. For IPv4 only use , for IPv6 only – . family is by default, that means both IPv4 and IPv6 are accepted. To specify only concrete version please pass or explicitly.\n• ssl_context may be used for configuring certification authority channel, supported SSL options etc.\n• None local_addr (tuple) – tuple of used to bind socket locally if specified.\n• None enable_cleanup_closed (bool) – Some ssl servers do not properly complete SSL shutdown process, in that case asyncio leaks SSL connections. If this parameter is set to True, aiohttp additionally aborts underlining transport after 2 seconds. It is off by default.\n• None The amount of time in seconds to wait for a connection attempt to complete, before starting the next attempt in parallel. This is the “Connection Attempt Delay” as defined in RFC 8305. To disable Happy Eyeballs, set this to . The default value recommended by the RFC is 0.25 (250 milliseconds).\n• None controls address reordering when a host name resolves to multiple IP addresses. If or unspecified, no reordering is done, and addresses are tried in the order returned by the resolver. If a positive integer is specified, the addresses are interleaved by address family, and the given integer is interpreted as “First Address Family Count” as defined in RFC 8305. The default is if happy_eyeballs_delay is not specified, and if it is. Use quick lookup in internal DNS cache for host names if . The cache of resolved hosts if is enabled. Remove specific entry if both host and port are specified, clear all cache otherwise. Use for sending HTTP/HTTPS requests through UNIX Sockets as underlying transport. UNIX sockets are handy for writing tests and making very fast connections between processes on the same host. Constructor accepts all parameters suitable for plus UNIX-specific one: End user should never create instances manually but get it by coroutine. read-only property, if connection was closed, released or detached. Underlying socket is not closed, the connection may be reused later if timeout (30 seconds by default) for connection was not expired.\n\nUser never creates the instance of ClientResponse class but gets it from API calls. After exiting from block response object will be released (see method). Boolean representation of HTTP status code ( ). if is less than ; otherwise, . Payload stream, which contains response’s BODY ( ). It supports various reading methods depending on the expected format. When chunked transfer encoding is used by the server, allows retrieving the actual http chunks. Reading from the stream may raise if the response object is closed before response receives all data or in case if any transfer encoding related errors like malformed chunked encoding of broken compression data. Unmodified HTTP headers of response as unconverted bytes, a sequence of pairs. For each link, key is link param when it exists, or link url as otherwise, and value is of link params and url at key as instance. Returns value is if no Content-Type header present in HTTP headers according to RFC 2616. To make sure Content-Type header is not present in the server reply, use or , e.g. . Read-only property that specifies the encoding for the request’s BODY. The value is parsed from the Content-Type HTTP header. Returns like or if no Content-Type header present in HTTP headers or it has no charset information. Read-only property that specified the Content-Disposition HTTP header. Instance of or if no Content-Disposition header present in HTTP headers. A of objects of preceding requests (earliest request first) if there were redirects, an empty sequence otherwise. Read the whole response’s body as . Close underlying connection if data reading gets an error, release connection otherwise. Raise an if the data can’t be read. It is not required to call on the response object. When the client fully receives the payload, the underlying connection automatically returns back to pool. If the payload is not fully read, the connection is closed Raise an if the response status is 400 or higher. Do nothing for success responses (less than 400). Read response’s body and return decoded using specified encoding parameter. If encoding is content encoding is determined from the Content-Type header, or using the function. Close underlying connection if data reading gets an error, release connection otherwise. encoding (str) – text encoding used for BODY decoding, or for encoding autodetection (default). if decoding fails. See also . Read response’s body as JSON, return using specified encoding and loader. If data is not still available a call will be done. If response’s does not match parameter get raised. To disable content type check pass value.\n• None text encoding used for BODY decoding, or for encoding autodetection (default). By the standard JSON encoding should be but practice beats purity: some servers return non-UTF responses. Autodetection works pretty fine anyway.\n• None loads (collections.abc.Callable) – callable used for loading JSON data, by default.\n• None content_type (str) – specify response’s content-type, if content type does not match raise . To disable check, pass as value. (default: ). BODY as JSON data parsed by loads parameter or if BODY is empty or contains white-spaces only. A with request URL and headers from object, instance. Retrieve content encoding using info in HTTP header. If no charset is present or the charset is not understood by Python, the function associated with the is called.\n\nTotal number of seconds for the whole request. Maximal number of seconds for acquiring a connection from pool. The time consists connection establishment for a new connection or waiting for a free connection from a pool if pool connection limits are exceeded. Maximal number of seconds for connecting to a peer for a new connection, not given from a pool. See also . Maximal number of seconds for reading a portion of data from a peer. A timeout for the websocket to close. Timeouts of 5 seconds or more are rounded for scheduling on the next second boundary (an absolute time where microseconds part is zero) for the sake of performance. E.g., assume a timeout is , absolute time when timeout should expire is , and it points to which is equal to . The absolute time for the timeout cancellation is . It leads to grouping all close scheduled timeout expirations to exactly the same time to reduce amount of loop wakeups. Changed in version 3.7: Rounding to the next seconds boundary is disabled for timeouts smaller than 5 seconds for the sake of easy debugging. In turn, tiny timeouts can lead to significant performance degradation on production environment. Value of corresponding etag without quotes. Flag indicates that etag is weak (has prefix). A data class to represent the Content-Disposition header, available as attribute. A instance. Value of Content-Disposition header itself, e.g. . A instance. Content filename extracted from parameters. May be . A with request URL and headers from object, available as attribute. Should be used for specifying authorization data in client API, e.g. auth parameter for . credentials data, or is credentials are not provided. Encode credentials into string suitable for header etc. The cookie jar instance is available as . The jar contains items for storing internal cookie data. These cookies may be iterated over:\n• None unsafe (bool) – (optional) Whether to accept cookies from IPs.\n• None (optional) Whether to quote cookies according to RFC 2109. Some backend systems (not compatible with RFC mentioned above) does not support quoted cookies.\n• None for cookies marked as Secured. Possible types are\n• None or of or\n• None cookies – a (e.g. , ) or iterable of pairs with cookies returned by server’s response.\n• None response_url (URL) – URL of response, for shared cookies. Regular cookies are coupled with server’s URL and are sent only to this server, shared ones are sent in every client request. Return jar’s cookies acceptable for URL and available in header for sending client requests for given URL. response_url (URL) – request’s URL for which cookies are asked. with filtered cookies for given URL. Write a pickled representation of cookies into the file at provided path. file_path – Path to file where cookies will be serialized, or instance. Load a pickled representation of cookies from the file at provided path. file_path – Path to file from where cookies will be imported, or instance. Removes all cookies from the jar if the predicate is . Otherwise remove only those that returns . callable that gets as a parameter and returns if this must be deleted from the jar. Remove all cookies from the jar that belongs to the specified domain or its subdomains. domain (str) – domain for which cookies must be deleted from the jar. Dummy cookie jar which does not store cookies but ignores them. Could be useful e.g. for web crawlers to iterate over Internet without blowing up with saved cookies information. To install dummy cookie jar pass it into session instance: digest (bytes) – SHA256 digest for certificate in DER-encoded binary form (see ). To check fingerprint pass the object into call, e.g.: A object contains the form data and also handles encoding it into a body that is either or . is used if at least one field is an object or was added with at least one optional argument to ( , , or ). Otherwise, is used. instances are callable and return a on being called. A container for the key/value pairs of this form. If it is a or , it must be a valid argument for . For , , and , the keys and values must be valid and arguments to , respectively.\n• None name (str) – Name of the field\n• If this is not set and is a , , or object, the argument is used as the filename unless is specified. If is not set and is an object, the filename is extracted from the object if possible. Add one or more fields to the form.\n• None or of length two, containing a name-value pair\n\nException hierarchy has been significantly modified in version 2.0. aiohttp defines only exceptions that covers connection handling and server response misbehaviors. For developer specific mistakes, aiohttp uses python standard exceptions like or . Reading a response content may raise a exception. This exception indicates errors specific to the payload encoding. Such as invalid compressed data, malformed chunked-encoded chunks or not enough data that satisfy the content-length header. All exceptions are available as members of aiohttp module. This exception can only be raised while reading the response payload if one of these errors occurs:\n• None not enough data that satisfy HTTP header. URL used for fetching is malformed, e.g. it does not contain host part. Base class for all errors related to client url. Base class for all errors related to client redirects. Base class for all errors related to non http client urls. Redirect URL is malformed, e.g. it does not contain host part. Redirect URL does not contain http schema. These exceptions could happen after we get response from server. Instance of object, contains information about request. History from failed response, if available, else empty tuple. A of objects used for handle redirection responses. Client was redirected too many times. Maximum number of redirects can be configured by using parameter in . Subset of connection errors that are initiated by an exception. To catch all timeouts, including the timeout, use ."
    },
    {
        "link": "https://docs.aiohttp.org/en/v3.8.4/client_reference.html",
        "document": "Client session is the recommended interface for making HTTP requests. Session encapsulates a connection pool (connector instance) and supports keepalives by default. Unless you are connecting to a large, unknown number of different servers over the lifetime of your application, it is suggested you use a single session for the lifetime of your application to benefit from connection pooling. The client session supports the context manager protocol for self closing. The class for creating client sessions and making requests.\n• None Base part of the URL (optional) If set it allows to skip the base part in request calls.\n• If loop is the constructor borrows it from connector if specified. is used for getting default event loop otherwise.\n• None cookies (dict) – Cookies to send with the request (optional)\n• None HTTP Headers to send with every request (optional). May be either iterable of key-value pairs or (e.g. , ).\n• None set of headers for which autogeneration should be skipped. aiohttp autogenerates headers like or if these headers are not explicitly passed. Using parameter allows to skip that generation. Note that autogeneration can’t be skipped.\n• By default every session instance has own private cookie jar for automatic cookies processing but user may redefine this behavior by providing own jar implementation. One example is not processing cookies at all when working in proxy mode. If no cookie processing is needed, a instance can be provided.\n• None Automatically call for each response, by default. This parameter can be overridden when you making a request, e.g.: Set the parameter to if you need for most of cases but override for those requests where you need to handle responses with status 400 or higher.\n• None Request operations timeout. is cumulative for all request operations (request, redirects, responses, data consuming). By default, the read timeout is 5*60 seconds. Use or to disable timeout checks. Deprecated since version 3.3: Use parameter instead.\n• None timeout for connection establishing (optional). Values or mean no timeout. Deprecated since version 3.3: Use parameter instead.\n• Setting the parameter to allows to share connection pool between sessions without sharing session state: cookies etc.\n• None Get proxies information from HTTP_PROXY / HTTPS_PROXY environment variables if the parameter is ( by default). Get proxy credentials from file if present.\n• None trace_configs – A list of instances used for client tracing. (default) is used for request tracing disabling. See Tracing Reference for more information. if the session has been closed, otherwise. derived instance used for the session. Gives access to cookie jar’s content and modifiers. aiohttp re quote’s redirect urls by default, but some servers require exact url from location header. To disable re-quote system set attribute to . Deprecated since version 3.5: The attribute modification is deprecated. Default client timeouts, instance. The value can be tuned by passing timeout parameter to constructor. HTTP Headers that sent with every request May be either iterable of key-value pairs or (e.g. , ). Set of headers for which autogeneration skipped. Should connector be closed on session closing Should be called for each response Should the body response be automatically decompressed Should get proxies information from HTTP_PROXY / HTTPS_PROXY environment variables or ~/.netrc file if present A list of instances used for client tracing. (default) is used for request tracing disabling. See Tracing Reference for more information.\n• None Mapping, iterable of tuple of key/value pairs or string to be sent as parameters in the query string of the new request. Ignored for subsequent redirected requests (optional)\n• None with preferably url-encoded content (Warning: content will not be encoded by aiohttp)\n• None data – The data to send in the body of the request. This can be a object or anything that can be passed into , e.g. a dictionary, bytes, or file-like object. (optional)\n• None json – Any json compatible python object (optional). json and data parameters could not be used at the same time.\n• Global session cookies and the explicitly set cookies will be merged when sending the request.\n• None headers (dict) – HTTP Headers to send with the request (optional)\n• None set of headers for which autogeneration should be skipped. aiohttp autogenerates headers like or if these headers are not explicitly passed. Using parameter allows to skip that generation.\n• None allow_redirects (bool) – If set to , do not follow redirects. by default (optional).\n• None max_redirects (int) – Maximum number of redirects to follow. by default.\n• None compress (bool) – Set to if request has to be compressed with deflate encoding. If can not be combined with a Content-Encoding and Content-Length headers. by default (optional).\n• None chunked (int) – Enable chunked transfer encoding. It is up to the developer to decide how to chunk data streams. If chunking is enabled, aiohttp encodes the provided chunks in the “Transfer-encoding: chunked” format. If chunked is set, then the Transfer-encoding and content-length headers are disallowed. by default (optional).\n• None response if set to . If set to value from will be used. by default (optional).\n• None read_until_eof (bool) – Read response until EOF if response does not have Content-Length header. by default (optional).\n• None by default, it means that the session global value is used.\n• Changed in version 3.3: The parameter is instance, is still supported for sake of backward compatibility. If is passed it is a total timeout (in seconds).\n• None ( is used), for skip SSL certificate validation, for fingerprint validation, for custom SSL certificate validation.\n• None Perform SSL certificate validation for HTTPS requests (enabled by default). May be disabled to skip validation for sites with invalid certificates.\n• None Pass the SHA256 digest of the expected certificate in DER format to verify that the certificate the server presents matches. Useful for certificate pinning. Warning: use of MD5 or SHA1 digests is insecure and removed.\n• ssl_context may be used for configuring certification authority channel, supported SSL options etc.\n• None HTTP headers to send to the proxy if the parameter proxy has been provided.\n• None Object used to give as a kw param for each new object instantiated, used to give information to the tracers that is only available at request time. In order to modify inner parameters, provide .\n• None allow_redirects (bool) – If set to , do not follow redirects. by default (optional). In order to modify inner parameters, provide .\n• None data – Data to send in the body of the request; see for details (optional) In order to modify inner parameters, provide .\n• None data – Data to send in the body of the request; see for details (optional) In order to modify inner parameters, provide . In order to modify inner parameters, provide .\n• None allow_redirects (bool) – If set to , do not follow redirects. by default (optional). In order to modify inner parameters, provide .\n• None allow_redirects (bool) – If set to , do not follow redirects. by default (optional). In order to modify inner parameters, provide .\n• None data – Data to send in the body of the request; see for details (optional)\n• None timeout (float) – Timeout for websocket to close. seconds by default\n• None receive_timeout (float) – Timeout for websocket to receive complete message. (unlimited) seconds by default\n• None autoclose (bool) – Automatically close websocket connection on close message from server. If autoclose is False then close procedure has to be handled manually. by default\n• None autoping (bool) – automatically send pong on ping message from server. by default\n• None heartbeat (float) – Send ping message every heartbeat seconds and wait pong response, if pong response is not received then close connection. The timer is reset on any data reception.(optional)\n• None Mapping, iterable of tuple of key/value pairs or string to be sent as parameters in the query string of the new request. Ignored for subsequent redirected requests (optional)\n• None with preferably url-encoded content (Warning: content will not be encoded by aiohttp)\n• None headers (dict) – HTTP Headers to send with the request (optional)\n• None ( is used), for skip SSL certificate validation, for fingerprint validation, for custom SSL certificate validation.\n• None Perform SSL certificate validation for HTTPS requests (enabled by default). May be disabled to skip validation for sites with invalid certificates.\n• None Pass the SHA256 digest of the expected certificate in DER format to verify that the certificate the server presents matches. Useful for certificate pinning. Note: use of MD5 or SHA1 digests is insecure and deprecated.\n• ssl_context may be used for configuring certification authority channel, supported SSL options etc.\n• None HTTP headers to send to the proxy if the parameter proxy has been provided.\n• None 0 for disable, 9 to 15 for window bit support. Default value is 0.\n• None 4 MB by default. To disable the size limit use . Detach connector from session without closing the former. Session is switched to closed state anyway.\n• None for regular TCP sockets (both HTTP and HTTPS schemes supported).\n• None for connecting via UNIX socket (it’s used mostly for testing purposes). All connector classes should be derived from . By default all connectors support keep-alive connections (behavior is controlled by force_close constructor’s parameter).\n• None keepalive_timeout (float) – timeout for connection reusing after releasing (optional). Values . For disabling keep-alive feature use flag.\n• None limit (int) – total number simultaneous connections. If limit is the connector has no limit (default: 100).\n• None limit_per_host (int) – limit simultaneous connections to the same endpoint. Endpoints are the same if they are have equal triple. If limit is the connector has no limit (default: 0).\n• None enable_cleanup_closed (bool) – some SSL servers do not properly complete SSL shutdown process, in that case asyncio leaks ssl connections. If this parameter is set to True, aiohttp additionally aborts underlining transport after 2 seconds. It is off by default.\n• None event loop used for handling connections. If param is , is used for getting default event loop. Read-only property, if connector should ultimately close connections on releasing. The total number for simultaneous connections. If limit is 0 the connector has no limit. The default limit size is 100. The limit for simultaneous connections to the same endpoint. Endpoints are the same if they are have equal triple. If limit_per_host is the connector has no limit per host. Get a free connection from pool or create new one if connection is absent in the pool. The call may be paused if is exhausted until used connections returns to pool. Abstract method for actual connection establishing, should be overridden in subclasses. Connector for working with HTTP and HTTPS via TCP sockets. The most common transport. When you don’t know what connector type to use, use a instance. Constructor accepts all parameters suitable for plus several TCP-specific ones: ( is used), for skip SSL certificate validation, for fingerprint validation, for custom SSL certificate validation.\n• None perform SSL certificate validation for HTTPS requests (enabled by default). May be disabled to skip validation for sites with invalid certificates. Deprecated since version 2.3: Pass verify_ssl to etc.\n• None pass the SHA256 digest of the expected certificate in DER format to verify that the certificate the server presents matches. Useful for certificate pinning. Note: use of MD5 or SHA1 digests is insecure and deprecated. Deprecated since version 2.3: Pass verify_ssl to etc.\n• None use internal cache for DNS lookups, by default. Enabling an option may speedup connection establishing a bit but may introduce some side effects also.\n• None expire after some seconds the DNS entries, means cached forever. By default 10 seconds (optional). In some environments the IP addresses related to a specific HOST can change after a specific time. Use this option to keep the DNS cache updated refreshing each entry after N seconds.\n• None limit (int) – total number simultaneous connections. If limit is the connector has no limit (default: 100).\n• None limit_per_host (int) – limit simultaneous connections to the same endpoint. Endpoints are the same if they are have equal triple. If limit is the connector has no limit (default: 0).\n• None custom resolver instance to use. by default (asynchronous if is installed). Custom resolvers allow to resolve hostnames differently than the way the host is configured. The resolver is by default, asynchronous version is pretty robust but might fail in very rare cases.\n• None TCP socket family, both IPv4 and IPv6 by default. For IPv4 only use , for IPv6 only – . family is by default, that means both IPv4 and IPv6 are accepted. To specify only concrete version please pass or explicitly.\n• ssl_context may be used for configuring certification authority channel, supported SSL options etc.\n• None local_addr (tuple) – tuple of used to bind socket locally if specified.\n• None enable_cleanup_closed (bool) – Some ssl servers do not properly complete SSL shutdown process, in that case asyncio leaks SSL connections. If this parameter is set to True, aiohttp additionally aborts underlining transport after 2 seconds. It is off by default. Use quick lookup in internal DNS cache for host names if . The cache of resolved hosts if is enabled. Remove specific entry if both host and port are specified, clear all cache otherwise. Use for sending HTTP/HTTPS requests through UNIX Sockets as underlying transport. UNIX sockets are handy for writing tests and making very fast connections between processes on the same host. Constructor accepts all parameters suitable for plus UNIX-specific one: End user should never create instances manually but get it by coroutine. read-only property, if connection was closed, released or detached. Underlying socket is not closed, the connection may be reused later if timeout (30 seconds by default) for connection was not expired.\n\nUser never creates the instance of ClientResponse class but gets it from API calls. After exiting from block response object will be released (see coroutine). Boolean representation of HTTP status code ( ). if is less than ; otherwise, . Payload stream, which contains response’s BODY ( ). It supports various reading methods depending on the expected format. When chunked transfer encoding is used by the server, allows retrieving the actual http chunks. Reading from the stream may raise if the response object is closed before response receives all data or in case if any transfer encoding related errors like misformed chunked encoding of broken compression data. Unmodified HTTP headers of response as unconverted bytes, a sequence of pairs. For each link, key is link param when it exists, or link url as otherwise, and value is of link params and url at key as instance. Returns value is if no Content-Type header present in HTTP headers according to RFC 2616. To make sure Content-Type header is not present in the server reply, use or , e.g. . Read-only property that specifies the encoding for the request’s BODY. The value is parsed from the Content-Type HTTP header. Returns like or if no Content-Type header present in HTTP headers or it has no charset information. Read-only property that specified the Content-Disposition HTTP header. Instance of or if no Content-Disposition header present in HTTP headers. A of objects of preceding requests (earliest request first) if there were redirects, an empty sequence otherwise. Read the whole response’s body as . Close underlying connection if data reading gets an error, release connection otherwise. Raise an if the data can’t be read. It is not required to call on the response object. When the client fully receives the payload, the underlying connection automatically returns back to pool. If the payload is not fully read, the connection is closed Raise an if the response status is 400 or higher. Do nothing for success responses (less than 400). Read response’s body and return decoded using specified encoding parameter. If encoding is content encoding is autocalculated using HTTP header and charset-normalizer tool if the header is not provided by server. cchardet is used with fallback to charset-normalizer if cchardet is not available. Close underlying connection if data reading gets an error, release connection otherwise. encoding (str) – text encoding used for BODY decoding, or for encoding autodetection (default). LookupError – if the encoding detected by cchardet is unknown by Python (e.g. VISCII). If response has no info in HTTP header cchardet / charset-normalizer is used for content encoding autodetection. It may hurt performance. If page encoding is known passing explicit encoding parameter might help: Read response’s body as JSON, return using specified encoding and loader. If data is not still available a call will be done, If encoding is content encoding is autocalculated using cchardet or charset-normalizer as fallback if cchardet is not available. if response’s does not match parameter get raised. To disable content type check pass value.\n• None text encoding used for BODY decoding, or for encoding autodetection (default). By the standard JSON encoding should be but practice beats purity: some servers return non-UTF responses. Autodetection works pretty fine anyway.\n• None loads (collections.abc.Callable) – callable used for loading JSON data, by default.\n• None content_type (str) – specify response’s content-type, if content type does not match raise . To disable check, pass as value. (default: ). BODY as JSON data parsed by loads parameter or if BODY is empty or contains white-spaces only. A namedtuple with request URL and headers from object, instance. Automatically detect content encoding using info in HTTP header. If this info is not exists or there are no appropriate codecs for encoding then cchardet / charset-normalizer is used. Beware that it is not always safe to use the result of this function to decode a response. Some encodings detected by cchardet are not known by Python (e.g. VISCII). charset-normalizer is not concerned by that issue. RuntimeError – if called before the body has been read, for cchardet usage\n\nTotal number of seconds for the whole request. Maximal number of seconds for acquiring a connection from pool. The time consists connection establishment for a new connection or waiting for a free connection from a pool if pool connection limits are exceeded. Maximal number of seconds for connecting to a peer for a new connection, not given from a pool. See also . Maximal number of seconds for reading a portion of data from a peer. Timeouts of 5 seconds or more are rounded for scheduling on the next second boundary (an absolute time where microseconds part is zero) for the sake of performance. E.g., assume a timeout is , absolute time when timeout should expire is , and it points to which is equal to . The absolute time for the timeout cancellation is . It leads to grouping all close scheduled timeout expirations to exactly the same time to reduce amount of loop wakeups. Changed in version 3.7: Rounding to the next seconds boundary is disabled for timeouts smaller than 5 seconds for the sake of easy debugging. In turn, tiny timeouts can lead to significant performance degradation on production environment. Value of corresponding etag without quotes. Flag indicates that etag is weak (has prefix). A data class with request URL and headers from object, available as attribute. Should be used for specifying authorization data in client API, e.g. auth parameter for . credentials data, or is credentials are not provided. Encode credentials into string suitable for header etc. The cookie jar instance is available as . The jar contains items for storing internal cookie data. These cookies may be iterated over:\n• None unsafe (bool) – (optional) Whether to accept cookies from IPs.\n• None (optional) Whether to quote cookies according to RFC 2109. Some backend systems (not compatible with RFC mentioned above) does not support quoted cookies.\n• None for cookies marked as Secured. Possible types are\n• None or of or\n• None cookies – a (e.g. , ) or iterable of pairs with cookies returned by server’s response.\n• None response_url (URL) – URL of response, for shared cookies. Regular cookies are coupled with server’s URL and are sent only to this server, shared ones are sent in every client request. Return jar’s cookies acceptable for URL and available in header for sending client requests for given URL. response_url (URL) – request’s URL for which cookies are asked. with filtered cookies for given URL. Write a pickled representation of cookies into the file at provided path. file_path – Path to file where cookies will be serialized, or instance. Load a pickled representation of cookies from the file at provided path. file_path – Path to file from where cookies will be imported, or instance. Removes all cookies from the jar if the predicate is . Otherwise remove only those that returns . callable that gets as a parameter and returns if this must be deleted from the jar. Remove all cookies from the jar that belongs to the specified domain or its subdomains. domain (str) – domain for which cookies must be deleted from the jar. Dummy cookie jar which does not store cookies but ignores them. Could be useful e.g. for web crawlers to iterate over Internet without blowing up with saved cookies information. To install dummy cookie jar pass it into session instance: digest (bytes) – SHA256 digest for certificate in DER-encoded binary form (see ). To check fingerprint pass the object into call, e.g.: A object contains the form data and also handles encoding it into a body that is either or . is used if at least one field is an object or was added with at least one optional argument to ( , , or ). Otherwise, is used. instances are callable and return a on being called. A container for the key/value pairs of this form. If it is a or , it must be a valid argument for . For , , and , the keys and values must be valid and arguments to , respectively.\n• None name (str) – Name of the field\n• If this is not set and is a , , or object, the argument is used as the filename unless is specified. If is not set and is an object, the filename is extracted from the object if possible. Add one or more fields to the form.\n• None or of length two, containing a name-value pair\n\nException hierarchy has been significantly modified in version 2.0. aiohttp defines only exceptions that covers connection handling and server response misbehaviors. For developer specific mistakes, aiohttp uses python standard exceptions like or . Reading a response content may raise a exception. This exception indicates errors specific to the payload encoding. Such as invalid compressed data, malformed chunked-encoded chunks or not enough data that satisfy the content-length header. All exceptions are available as members of aiohttp module. This exception can only be raised while reading the response payload if one of these errors occurs:\n• None not enough data that satisfy HTTP header. URL used for fetching is malformed, e.g. it does not contain host part. A instance. Value of Content-Disposition header itself, e.g. . A instance. Content filename extracted from parameters. May be . These exceptions could happen after we get response from server. Instance of object, contains information about request. History from failed response, if available, else empty tuple. A of objects used for handle redirection responses. Client was redirected too many times. Maximum number of redirects can be configured by using parameter in . Subset of connection errors that are initiated by an exception."
    },
    {
        "link": "https://webshare.io/academy-article/aiohttp-proxy",
        "document": "AIOHTTP is a versatile Python library that provides an asynchronous framework for handling HTTP requests and building web servers. Built on Python's asyncio, it enables non-blocking I/O operations, allowing developers to perform multiple tasks simultaneously without waiting for each operation to complete. This makes it an ideal choice for scenarios that demand high performance and scalability, such as real-time applications, API integrations, and large-scale web scraping.\n\nIn AIOHTTP, a proxy acts as an intermediary between your client and the server, masking your original IP address and ensuring anonymity. In this article, we’ll guide you through three effective ways to set up proxies with AIOHTTP: using a static HTTP proxy, an HTTP proxy list, and a SOCKS5 proxy. We’ll also cover a bonus method for configuring residential proxies and discuss advanced proxy configurations and troubleshooting common issues.\n\nBefore setting up proxies in AIOHTTP, ensure you have the following prerequisites ready:\n• Python Installed: Ensure you have Python 3.7 or later installed on your system. You can download it from the Python website.\n• AIOHTTP Library: Install the AIOHTTP library using pip if you haven’t already. Run the following command in your terminal or command prompt:\n• Proxy Service: AIOHTTP supports proxy configurations for both HTTP and SOCKS5 proxies. For an easy start, Webshare provides a free plan that includes 10 shared datacenter proxies. These proxies come with a 1GB monthly bandwidth limit and offer options for both rotating and static configurations. Once you sign up, you can find the required proxy details, including username, password, host, and port, in your Webshare account dashboard.\n\nBelow is the structure of a proxy URL required to connect to a proxy server in AIOHTTP:\n\nOnly the <PROTOCOL> and <HOST> parts are mandatory. However, <PORT> is often needed to establish the connection, and <USERNAME>:<PASSWORD> is only necessary for authenticated proxies.\n• Python IDE or Text Editor: Use an environment like VS Code, PyCharm, or any text editor of your choice to write and run your Python scripts.\n\nA static HTTP proxy is a single proxy server used for all requests. This method is straightforward and ideal for scenarios where a single proxy suffices. Here's how to set it up in AIOHTTP.\n\nUse the proxy URL format discussed earlier to define your proxy settings. Here’s a basic example of configuring an HTTP proxy:\n• Proxy URL: Replace proxyhost, port, username, and password with the credentials of your HTTP proxy. Omit username:password@ if authentication isn’t required.\n• URL: Use any endpoint for testing; https://httpbin.org/ip is a common choice as it returns your current public IP.\n\nHow to test the static proxy configuration\n\nRun the script. If the proxy is working, the output will show the IP address of the proxy server instead of your original IP.\n\nUsing a list of HTTP proxies allows you to dynamically switch between proxies during your requests. This is particularly useful for tasks like web scraping, where you might want to avoid rate-limiting or bans.\n\nFirst, prepare a list of proxy URLs in the correct format. Then, use a random or sequential method to pick proxies from the list. Here’s an example implementation:\n• Proxy List: Replace the placeholders in the proxy URLs with your actual proxy details.\n• Random Selection: The random.choice(proxies) ensures a different proxy is used for each request. You can replace this with a sequential approach if needed.\n\nHow to test the HTTP proxy list configuration\n\nRun the script in your interactive environment. The output will show the corresponding IP address returned by the server.\n\nA SOCKS5 proxy provides advanced functionality, such as UDP support and enhanced security. AIOHTTP supports SOCKS5 proxies through the aiohttp_socks library. This method explains how to configure and use SOCKS5 proxies in your requests.\n\nFirst, install aiohttp socks to enable SOCKS5 support using the below command:\n\nPrepare your SOCKS5 proxy URL. The format is:\n\nThe aiohttp_socks library provides a ProxyConnector to handle SOCKS5 proxies. Here’s how to configure it:\n• ProxyConnector: The ProxyConnector is initialized using the SOCKS5 proxy URL.\n• Session Handling: The connector is passed to aiohttp.ClientSession, ensuring all requests use the configured proxy.\n\nHow to test the SOCKS5 proxy configuration\n\nRun the script in your interactive environment. If the proxy is correctly set up, the output will display the IP address of the proxy server.\n\nResidential proxies provide IP addresses assigned by Internet Service Providers (ISPs) to real devices, making them less likely to be detected or blocked compared to datacenter proxies. This method explains how to integrate residential proxies with AIOHTTP.\n\nHow to setup residential proxy in AIOHTTP\n\nBefore setting up, ensure you have access to a residential proxy service. Typically, you'll receive credentials, such as:\n\nHere’s how to configure AIOHTTP to use a residential proxy:\n• Proxy Parameter: The proxy parameter in session.get() explicitly defines the proxy to use for the request.\n• Authentication: The credentials (username:password) in the proxy URL handle authentication automatically.\n\nHow to test the residential proxy configuration\n\nRun the script to verify the connection through the residential proxy. The output should display the residential proxy’s IP address.\n\nAdvanced proxy configurations can significantly improve the efficiency and reliability of your scraping setup. Below are additional advanced techniques for managing proxies and optimizing scraping tasks.\n\nUsing the same user-agent for multiple requests can lead to detection. Combining user-agent rotation with proxies can mimic different devices or browsers, reducing the chance of being blocked.\n• User-Agent Rotation: The USER_AGENTS list contains various User-Agent strings that represent different browsers and operating systems. By randomly selecting a user-agent for each request, you can make your scraping activity appear more like that of a real user.\n• Fetch Function: The fetch_with_user_agent_and_proxy function takes a URL and a proxy as input. It randomly selects a user-agent from the USER_AGENTS list and sets it in the request headers. The function then makes an asynchronous GET request to the specified URL using the provided proxy.\n• Main Function: The main function defines the URL to be accessed (https://httpbin.org/ip) and specifies a proxy. It calls the fetch_with_user_agent_and_proxy function and prints the response.\n\nCertain websites serve content based on the user's IP location. Implementing a proxy selection strategy based on geolocation ensures accurate data collection.\n• Geolocation-Based Proxy Dictionary: The GEO_PROXIES dictionary maps regions to their respective proxy URLs. This allows for easy selection of proxies based on the desired location.\n• Fetch Function: The fetch_by_region function takes a URL and a region as input. It retrieves the appropriate proxy from the GEO_PROXIES dictionary and makes an asynchronous GET request to the specified URL using that proxy. If no proxy is available for the specified region, it raises a ValueError.\n• Main Function: The main function defines the URL to be accessed and a list of regions. It creates a list of tasks to fetch data from the URL for each region concurrently using asyncio.gather.\n\nWhile using proxies with AIOHTTP, you might encounter various challenges. Below are the common issues and how to resolve them.\n\nSymptom: Errors occur like Invalid URL or ValueError: Proxy URL is not valid.\n\nCause: The proxy URL provided doesn’t follow the correct format.\n\nSymptom: Requests take too long and eventually fail with a timeout error.\n\nCause: The proxy server is slow or unresponsive.\n\nSolution: Test the proxy's responsiveness before using it. Adjust the timeout setting in AIOHTTP using ClientTimeout:\n\nSymptom: Despite setting a custom User-Agent, requests are identified as bots.\n\nCause: The custom User-Agent header isn’t applied to requests or is overridden.\n\nSolution: Ensure the User-Agent is set explicitly in the request:\n\nIntegrating proxies with AIOHTTP is a powerful way to optimize your web scraping, data fetching, and API interactions. By configuring proxies properly, you can enhance security, improve performance, and avoid detection. With the right setup, proxies can help you overcome challenges like IP blocking, rate limiting, and captchas, making your web requests more reliable and efficient."
    },
    {
        "link": "https://proxiesapi.com/articles/using-aiohttp-for-easy-and-powerful-reverse-proxying-in-python",
        "document": "Reverse proxying is an incredibly useful technique for forwarding requests from one server to another in a transparent way. This opens up all sorts of possibilities like load balancing, centralized authentication, caching, and more.\n\nThe Python aiohttp library makes setting up a reverse proxy simple and easy, while still providing a powerful and customizable solution. In this article, I'll walk through how to use aiohttp to build a basic reverse proxy, explain the core concepts, and show some more advanced usage examples.\n\nWhat Exactly is a Reverse Proxy?\n\nSimply put, a reverse proxy is a server that forwards requests to one or more backend servers transparently. When a client sends a request to a reverse proxy, the proxy forwards it to the appropriate backend server, gets the response, and then sends it back to the client.\n\nThis allows the backend servers to focus on serving application logic while the proxy handles tasks like security, caching, compression, etc. The client has no knowledge that it's talking to a proxy, not the real server.\n\nSome common examples where reverse proxies are used:\n\nThe aiohttp library includes a class that makes building proxies straightforward. Here is a simple example:\n\nAnd that's it! Any requests get proxied transparently to the backend server.\n\nRight now this proxies everything to one backend URL. To support multiple backends, you can dynamically set the based on the request path, headers, etc.\n\nFor example, routes could proxy to one API server, while routes proxy to another blog application server.\n\nSometimes you may want to stream a response instead of loading it all into memory. This can be done by creating an aiohttp , like:\n\nThis streams the content from the proxy response through to the client response.\n\nWhile building a basic reverse proxy is easy, aiohttp provides ways to construct more advanced proxies too.\n\nThe uses a under the hood to determine how to connect to backends.\n\nThe default simple resolver uses plain TCP sockets. But you can also create a custom resolver to add connection pooling, UNIX domain socket support, SSH tunneling, and more.\n\nFor example, here is resolver that uses a HTTP connection pool:\n\nNow all proxy requests will reuse HTTP connections from the pool.\n\nFor ultimate control, you can subclass and override methods like , , etc.\n\nThis allows implementing custom caching, authentication logic, request rewriting, and more.\n\nHere is a simple example that logs every proxied request:\n\nThe possibilities are endless when subclassing ProxyConnector!\n\nHopefully this gives you a good overview of how to leverage aiohttp for building Python reverse proxy applications, both simple and advanced.\n\nThe aiohttp documentation goes into more depth on all the configuration options and customization possible around proxying."
    },
    {
        "link": "https://stackoverflow.com/questions/53021448/multiple-async-requests-simultaneously",
        "document": "Here is a typical pattern that accomplishes what you're trying to do. (Python 3.7+.)\n\nOne major change is that you will need to move from , which is built for synchronous IO, to a package such as that is built specifically to work with / (native coroutines):\n\nThere are two distinct elements to this, one being the asynchronous aspect of the coroutines and one being the concurrency introduced on top of that when you specify a container of tasks (futures):\n• You create one coroutine that uses with two awaitables: the first being and the second being . This is the async aspect. The purpose of ing these IO-bound responses is to tell the event loop that other calls can take turns running through that same routine.\n• The concurrent aspect is encapsulated in . This maps the awaitable call to each of your . The result is an aggregate list of returned values. Note that this wrapper will wait until all of your responses come in and call . If, alternatively, you want to process them greedily as they are ready, you can loop over : each Future object returned represents the earliest result from the set of the remaining awaitables.\n\nLastly, take note that is a high-level \"porcelain\" function introduced in Python 3.7. In earlier versions, you can mimic it (roughly) like:\n\nThere are a number of ways to limit the rate of concurrency. For instance, see in async-await function or large numbers of tasks with limited concurrency."
    },
    {
        "link": "https://laac.dev/blog/concurrent-http-requests-python-asyncio",
        "document": "Python 3.4 added the asyncio module to the standard library. Asyncio allows us to run IO-bound tasks asynchronously to increase the performance of our program. Common IO-bound tasks include calls to a database, reading and writing files to disk, and sending and receiving HTTP requests. A Django web application is a common example of an IO-bound application.\n\nWe’ll demonstrate the usage of concurrent HTTP requests by fetching prices for stock tickers. The only third party package we’ll use is httpx. Httpx is very similar to the popular requests package, but httpx supports asyncio.\n• Copy the below example code into a python file named async_http.py\n\nWith the newly created virtual environment activated and python file ready, let’s run the program to test our setup.\n\nIf you look at the output, the requests do not finish sequentially. In a synchronous program, the request for VTSAX would be made first and finish first. Afterward, the next request for VTIAX would start. In our asynchronous program, the requests are made back to back and finish out of order whenever the API responds. Let’s run the script again with the same arguments and see what the order of results are.\n\nAs you can see in the first request we received results for IJS first, but in the second request, the results for IJS returned fourth. Let’s walk through the code to see what our program does.\n\nLet’s start with the function. The function starts by creating an that we’ll pass in every time we call .\n\nCreating a client allows us to take advantage of HTTP connection pooling, which reuses the same TCP connection for each request. This increases the performance for each HTTP request. Additionally, we’re using a with statement to automatically close our client when the function finishes.\n\nNext, let’s look at our return statement.\n\nFirst, we’re running which accepts asyncio futures or coroutines. In our case, we’re expanding, using an asterisk, a map of functions which are our coroutines. To create our map of functions, we’re using the list of tickers and using , which passes in our client to every function for each ticker. Once our map call is done, we have a function for each ticker which we can pass to to run concurrently.\n\nNow let’s look at our function.\n\nWe’re using the that we passed in to make an HTTP GET request to Yahoo Finance. We use the await keyword here because this is where the IO happens. Once the program reaches this line, it makes the HTTP GET request and yields control to the event loop while the request finishes.\n\nOnce the request finishes, we extract the json from the response and return the price along with the ticker to identify which price is associated with a ticker. Finally before returning the price, we turn it into a decimal and round it to the nearest two decimal points.\n\nThe package ecosystem around the asyncio module is still maturing. Httpx looks like a quality replacement for requests. Starlette and FastAPI are two promising ASGI based web servers. As of version 3.1, Django has support for ASGI. Finally, more libraries are being released with asyncio in mind. As of this writing, asyncio has not seen widespread usage, but over the next few years, I predict asyncio will see a lot more adoption within the Python community."
    },
    {
        "link": "https://apidog.com/blog/aiohttp-concurrent-request",
        "document": "In today's fast-paced world, web applications are constantly striving for efficiency. One key aspect of achieving this is handling multiple requests simultaneously. This is where AIOHTTP, a powerful Python library, shines. By enabling concurrent requests, AIOHTTP empowers developers to unlock the true potential of asynchronous programming.\n\nThis article delves into the practicalities of making concurrent requests with AIOHTTP, providing a step-by-step guide and exploring its benefits for optimizing application performance.\n\nLet's first refresh our memory with brief descriptions on a couple of important concepts that we will encounter through this article:\n\nAIOHTTP concurrent requests referes to the the ability to initiate and manage multiple HTTP requests asynchronously within a single Python application utilizing the AIOHTTP library.\n\nThis approach leverages the power of asyncio to avoid blocking on individual requests, allowing the application to handle them simultaneously and improve overall performance.\n\nAIOHTTP builds upon the foundation of asyncio, a Python library for asynchronous programming. Asynchronous code allows your application to handle multiple tasks seemingly at once.\n\nIn the context of AIOHTTP, this translates to sending and receiving HTTP requests concurrently without being blocked by individual responses. This maximizes CPU utilization and improves application responsiveness.\n\nAIOHTTP uses asyncio concepts like tasks and coroutines to manage concurrent requests. A task represents a unit of asynchronous work, and a coroutine is a function designed to be used with asyncio. AIOHTTP allows you to create coroutines that handle individual HTTP requests. These coroutines are then launched as tasks, enabling them to run concurrently.\n\n3. Async with and Async for Loops:\n\nAIOHTTP provides asynchronous versions of common loop constructs like and . These are used to manage asynchronous operations like making requests and handling responses within your coroutines.\n\nThe statement is used for asynchronous context management (e.g., opening and closing connections), while loops are ideal for iterating through asynchronous results (e.g., processing multiple responses).\n\nAIOHTTP offers an class for making asynchronous HTTP requests. This client provides methods like , , , and that take URLs and parameters as arguments. These methods return coroutines that can be launched as tasks to initiate concurrent requests.\n\nOnce a concurrent request completes, the corresponding coroutine receives the response object. You can use methods like , , and on the response object to access the status code, response body as text, or decoded JSON data, respectively.\n\nRobust error handling is crucial for managing concurrent requests. AIOHTTP allows you to handle exceptions raised during request execution or response processing within your coroutines.\n\nThis ensures that your application doesn't crash due to unexpected errors and can gracefully handle failed requests.\n• Improved Performance: Concurrent requests significantly enhance performance by utilizing resources efficiently and reducing overall waiting time for responses.\n• Increased Scalability: AIOHTTP applications can handle a high volume of concurrent requests, making them suitable for web applications with heavy traffic.\n• Enhanced Responsiveness: Applications remain responsive even under heavy load as they're not blocked waiting for individual responses.\n• API Rate Limits: Be mindful of rate limits imposed by APIs when making concurrent requests to avoid overloading their servers.\n• Error Handling Complexity: Managing errors across multiple concurrent requests can become complex. Implement robust error handling strategies to ensure graceful operation.\n• Resource Management: While AIOHTTP improves resource utilization, monitor resource usage to avoid overwhelming your application's processing power.\n\nThe code example above defines an function that takes a URL as input. It uses an statement to manage the and makes a GET request to the provided URL. The response is then handled, and the data or error message is printed.\n\nThe function creates a list of URLs and uses to create tasks for each URL. Finally, is used to run all tasks concurrently and wait for them to complete.\n\nThe code example builds upon the previous one, demonstrating file downloads. The function takes a URL and filename as input. It retrieves the content and writes it chunk-by-chunk to a file with the specified name.\n\nThe function creates a list of URL-filename pairs and launches tasks for each download using . Similar to the previous example, is used for concurrent execution.\n\nImportant Note: Remember to replace the example URLs with the actual URLs you want to fetch or download. These examples serve as a foundation for building your own concurrent request functionalities using AIOHTTP.\n\nThe AIOHTTP framework is based on thhe Python programming language, therefore you may have to learn the said programming language if you have no prior exposure to it. Luckily, there is an omni-potent API development tool that can assist you with code generation for client code called Apidog.\n\nWith Apidog, you can have the necessary Python client code for creating AIOHTTP-based applications. Proceed to the next section to find out how.\n\nTo utilize Apidog's code generation feature, begin by clicking the button found on the top right corner of the Apidog window, and press .\n\nNext, select the section, where you can find different frameworks for the JavaScript language. In this step, select , and copy the code. You can then paste it over to your IDE to implement the AIOHTTP framework!\n\nYou may need to create more than one requests for your application, more so significant if your application is going to be complex.\n\nFirst, begin by initializing a new request on Apidog.\n\nNext, select the HTTP method, and craft a proper REST API URL. You can use a mix of path and query parameters, along with multiple IDs to create a more specific API URL.\n\nOnce you have finished including all the details, you can save the progress by clicking the button.\n\nThis exploration of AIOHTTP and concurrent requests in Python has equipped you with the skills to significantly enhance the performance and efficiency of your web applications. By embracing the power of asynchronous programming with asyncio, AIOHTTP allows you to send and receive multiple HTTP requests simultaneously. This eliminates the traditional wait times associated with sequential requests, leading to faster data retrieval and a more responsive user experience.\n\nThe provided examples have demonstrated the practical applications of AIOHTTP for fetching data and downloading files concurrently. Remember to keep API rate limits in mind when crafting your concurrent request strategies. Additionally, implementing robust error handling practices will ensure your applications gracefully handle unexpected situations and maintain smooth operation under heavy loads. With the knowledge gained from this guide, you can confidently leverage AIOHTTP concurrent requests to build high-performing and scalable web applications in Python.\n\nApidog is an API development platform that provides API developers with the necessary tools for the entire API lifecycle. Aside from code generation, you can also utilize Apidog's API Hub feature if you ever get stuck on a coder's \"mental block\"."
    },
    {
        "link": "https://calybre.global/post/asynchronous-api-calls-in-python-with-asyncio",
        "document": "In the world of Python, where efficiency and responsiveness are paramount, leveraging asynchronous programming has become a game-changer. One of the notable features empowering asynchronous programming in Python is the `asyncio` library. In this post, we'll explore how to use the power of `asyncio` to make asynchronous API calls, delving into its advantages, disadvantages, and the underlying mechanics.\n\nUnder the hood, `asyncio` uses an event loop to manage and execute asynchronous tasks. The event loop is responsible for scheduling tasks, handling I/O operations, and switching between tasks efficiently. When an asynchronous function encounters an I/O operation, it yields control to the event loop, allowing other tasks to run.\n\nHere is a quick illustration to showcase the idea of Asynchronous load:\n\n`asyncio` is a Python library that provides a framework for writing asynchronous code using coroutines, event loops, and tasks. At its core, `asyncio` enables the execution of asynchronous functions in a non-blocking manner, allowing developers to write highly concurrent code that efficiently handles I/O-bound operations.\n\nFor more information on the ‘asyncio’ library, please follow this link.\n\nLet's delve into the process of making asynchronous API calls using `asyncio`. Consider the following example where we have a list of API endpoints, and we want to fetch data from each endpoint asynchronously.\n\nComponents of the Example:\n• This asynchronous coroutine is responsible for making API requests using the aiohttp library.\n• It takes a session object and a URL as parameters, performs a GET request, and returns the JSON content of the response.\n• This is the main asynchronous coroutine that orchestrates the API calls.\n• It defines a list of API endpoints (urls) that we want to fetch data from concurrently.\n• It creates an aiohttp.ClientSession to manage the HTTP client session for making asynchronous requests.\n• It creates a list of tasks using a list comprehension, where each task corresponds to calling the fetch_data coroutine with a specific URL.\n• The asyncio.gather(*tasks) function is used to concurrently execute all the tasks and await their results.\n• The if name == \"main\": block ensures that the main coroutine is executed when the script is run.\n• The asyncio.run(main()) function runs the main coroutine, initiating the asynchronous execution.\n• The main coroutine starts by defining a list of API endpoints (urls).\n• An aiohttp.ClientSession is created within an async with block to manage the HTTP client session. This ensures proper resource cleanup after API calls.\n• A list of tasks is created, where each task corresponds to fetching data from a specific API endpoint using the fetch_data coroutine.\n• asyncio.gather(*tasks) concurrently executes all the tasks, allowing multiple API requests to be made simultaneously.\n• The results of the API calls are collected in the results variable\n• Improved Performance: By making API calls asynchronously, your program can efficiently utilize CPU resources, avoiding unnecessary waiting time. This can significantly improve the overall performance of your application, especially in scenarios with multiple concurrent API calls.\n• Responsiveness: Asynchronous programming allows your application to remain responsive while waiting for I/O operations, such as API requests. This is crucial for creating smooth user experiences, especially in web applications.\n• Concurrent Execution: enables the concurrent execution of asynchronous tasks. This concurrency is achieved through cooperative multitasking, allowing your application to switch between tasks without waiting for one to complete.\n• Complexity: Asynchronous programming introduces a level of complexity, especially for developers who are new to asynchronous concepts. Debugging asynchronous code might be more challenging compared to synchronous code.\n• Not Always Applicable: Asynchronous programming shines in scenarios with I/O-bound operations. However, for CPU-bound tasks, the benefits might be less pronounced, and the complexity introduced may not be justified.\n• Limited Libraries: While major libraries and frameworks support asynchronous programming, not all third-party libraries may be asynchronous-friendly. This can limit your choices when building asynchronous applications.\n\nis a powerful tool for building highly concurrent and responsive applications in Python. When it comes to making asynchronous API calls, ` provides a robust framework for harnessing the benefits of asynchronous programming. However, it's crucial to weigh the advantages against the added complexity, especially in scenarios where synchronous code might suffice.\n\nIn the ever-evolving landscape of Python development, ` stands as a testament to the language's adaptability and commitment to staying at the forefront of modern programming paradigms."
    },
    {
        "link": "https://medium.com/@rspatel031/handling-large-requests-with-aiohttp-and-asyncio-in-python-d603b2de5c69",
        "document": "In the fast-paced world of web development, handling large numbers of requests concurrently is a common challenge. Python’s library, coupled with the framework, provides an efficient and elegant solution to tackle this issue. In this blog, we'll explore how to leverage these tools to execute large requests seamlessly.\n• aiohttp: A powerful asynchronous HTTP client/server framework for Python. It allows you to perform HTTP requests asynchronously, making it suitable for handling numerous requests simultaneously.\n• asyncio: Python’s asynchronous I/O library that facilitates concurrent code execution. It introduces the and keywords to simplify asynchronous programming.\n\nEnsure you have both and installed. You can install them using pip:\n\nBasic example demonstrating asynchronous HTTP requests using and . This example will REST API data from multiple URLs simultaneously.\n\nIn this example:\n• The function is an asynchronous function that makes a GET request to a given URL using the library and uses to manage the HTTP sessions.\n• The function creates a list of URLs to fetch.\n• It creates a list of tasks for each URL using the function.\n• is used to run all the tasks concurrently, and it returns a list of responses.\n• The processing of responses can occur once all APIs have been executed.\n\nThere are numerous possibilities and functionalities available with this module. For additional information, kindly explore the website: https://docs.aiohttp.org/en/stable/\n\nUsing and in Python allows you to efficiently handle large numbers of requests concurrently, improving the performance of your applications. With these tools, you can build scalable and responsive applications that can handle demanding workloads."
    }
]