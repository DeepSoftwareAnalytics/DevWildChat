[
    {
        "link": "https://docs.python.org/3/library/random.html",
        "document": "This module implements pseudo-random number generators for various distributions.\n\nFor integers, there is uniform selection from a range. For sequences, there is uniform selection of a random element, a function to generate a random permutation of a list in-place, and a function for random sampling without replacement.\n\nOn the real line, there are functions to compute uniform, normal (Gaussian), lognormal, negative exponential, gamma, and beta distributions. For generating distributions of angles, the von Mises distribution is available.\n\nAlmost all module functions depend on the basic function , which generates a random float uniformly in the half-open range . Python uses the Mersenne Twister as the core generator. It produces 53-bit precision floats and has a period of 2**19937-1. The underlying implementation in C is both fast and threadsafe. The Mersenne Twister is one of the most extensively tested random number generators in existence. However, being completely deterministic, it is not suitable for all purposes, and is completely unsuitable for cryptographic purposes.\n\nThe functions supplied by this module are actually bound methods of a hidden instance of the class. You can instantiate your own instances of to get generators that don’t share state.\n\nClass can also be subclassed if you want to use a different basic generator of your own devising: see the documentation on that class for more details.\n\nThe module also provides the class which uses the system function to generate random numbers from sources provided by the operating system.\n\nReturn a random element from the non-empty sequence seq. If seq is empty, raises . Return a k sized list of elements chosen from the population with replacement. If the population is empty, raises . If a weights sequence is specified, selections are made according to the relative weights. Alternatively, if a cum_weights sequence is given, the selections are made according to the cumulative weights (perhaps computed using ). For example, the relative weights are equivalent to the cumulative weights . Internally, the relative weights are converted to cumulative weights before making selections, so supplying the cumulative weights saves work. If neither weights nor cum_weights are specified, selections are made with equal probability. If a weights sequence is supplied, it must be the same length as the population sequence. It is a to specify both weights and cum_weights. The weights or cum_weights can use any numeric type that interoperates with the values returned by (that includes integers, floats, and fractions but excludes decimals). Weights are assumed to be non-negative and finite. A is raised if all weights are zero. For a given seed, the function with equal weighting typically produces a different sequence than repeated calls to . The algorithm used by uses floating-point arithmetic for internal consistency and speed. The algorithm used by defaults to integer arithmetic with repeated selections to avoid small biases from round-off error. Changed in version 3.9: Raises a if all weights are zero. To shuffle an immutable sequence and return a new shuffled list, use instead. Note that even for small , the total number of permutations of x can quickly grow larger than the period of most random number generators. This implies that most permutations of a long sequence can never be generated. For example, a sequence of length 2080 is the largest that can fit within the period of the Mersenne Twister random number generator. Return a k length list of unique elements chosen from the population sequence. Used for random sampling without replacement. Returns a new list containing elements from the population while leaving the original population unchanged. The resulting list is in selection order so that all sub-slices will also be valid random samples. This allows raffle winners (the sample) to be partitioned into grand prize and second place winners (the subslices). Members of the population need not be hashable or unique. If the population contains repeats, then each occurrence is a possible selection in the sample. Repeated elements can be specified one at a time or with the optional keyword-only counts parameter. For example, is equivalent to . To choose a sample from a range of integers, use a object as an argument. This is especially fast and space efficient for sampling from a large population: . If the sample size is larger than the population size, a is raised. Changed in version 3.11: The population must be a sequence. Automatic conversion of sets to lists is no longer supported.\n\nThe following functions generate specific real-valued distributions. Function parameters are named after the corresponding variables in the distribution’s equation, as used in common mathematical practice; most of these equations can be found in any statistics text. Return the next random floating-point number in the range Return a random floating-point number N such that for and for . The end-point value may or may not be included in the range depending on floating-point rounding in the expression . Return a random floating-point number N such that and with the specified mode between those bounds. The low and high bounds default to zero and one. The mode argument defaults to the midpoint between the bounds, giving a symmetric distribution. Beta distribution. Conditions on the parameters are and . Returned values range between 0 and 1. Exponential distribution. lambd is 1.0 divided by the desired mean. It should be nonzero. (The parameter would be called “lambda”, but that is a reserved word in Python.) Returned values range from 0 to positive infinity if lambd is positive, and from negative infinity to 0 if lambd is negative. Changed in version 3.12: Added the default value for . Gamma distribution. (Not the gamma function!) The shape and scale parameters, alpha and beta, must have positive values. (Calling conventions vary and some sources define ‘beta’ as the inverse of the scale). Normal distribution, also called the Gaussian distribution. mu is the mean, and sigma is the standard deviation. This is slightly faster than the function defined below. Multithreading note: When two threads call this function simultaneously, it is possible that they will receive the same return value. This can be avoided in three ways. 1) Have each thread use a different instance of the random number generator. 2) Put locks around all calls. 3) Use the slower, but thread-safe function instead. Changed in version 3.11: mu and sigma now have default arguments. Log normal distribution. If you take the natural logarithm of this distribution, you’ll get a normal distribution with mean mu and standard deviation sigma. mu can have any value, and sigma must be greater than zero. Normal distribution. mu is the mean, and sigma is the standard deviation. Changed in version 3.11: mu and sigma now have default arguments. mu is the mean angle, expressed in radians between 0 and 2*pi, and kappa is the concentration parameter, which must be greater than or equal to zero. If kappa is equal to zero, this distribution reduces to a uniform random angle over the range 0 to 2*pi. Weibull distribution. alpha is the scale parameter and beta is the shape parameter.\n\n# Even integer from 0 to 100 inclusive ['four', 'two', 'ace', 'three'] # of 52 playing cards, and determine the proportion of cards # Estimate the probability of getting 5 or more heads from 7 spins # of a biased coin that settles on heads 60% of the time. # Probability of the median of 5 samples being in middle two quartiles Example of statistical bootstrapping using resampling with replacement to estimate a confidence interval for the mean of a sample: Example of a resampling permutation test to determine the statistical significance or p-value of an observed difference between the effects of a drug versus a placebo: # Example from \"Statistics is Easy\" by Dennis Shasha and Manda Wilson 'at least as extreme as the observed difference of leads us to reject the null' 'hypothesis that there is no difference between the drug and the placebo.' Simulation of arrival times and service deliveries for a multiserver queue: # time when each server becomes available Statistics for Hackers a video tutorial by Jake Vanderplas on statistical analysis using just a few fundamental concepts including simulation, sampling, shuffling, and cross-validation. Economics Simulation a simulation of a marketplace by Peter Norvig that shows effective use of many of the tools and distributions provided by this module (gauss, uniform, sample, betavariate, choice, triangular, and randrange). A Concrete Introduction to Probability (using Python) a tutorial by Peter Norvig covering the basics of probability theory, how to write simulations, and how to perform data analysis using Python.\n\nThese recipes show how to efficiently make random selections from the combinatoric iterators in the module: \"Choose r elements with replacement. Order the result to match the iterable.\" # Result will be in set(itertools.combinations_with_replacement(iterable, r)). The default returns multiples of 2⁻⁵³ in the range 0.0 ≤ x < 1.0. All such numbers are evenly spaced and are exactly representable as Python floats. However, many other representable floats in that interval are not possible selections. For example, isn’t an integer multiple of 2⁻⁵³. The following recipe takes a different approach. All floats in the interval are possible selections. The mantissa comes from a uniform distribution of integers in the range 2⁵² ≤ mantissa < 2⁵³. The exponent comes from a geometric distribution where exponents smaller than -53 occur half as often as the next larger exponent. All real valued distributions in the class will use the new method: The recipe is conceptually equivalent to an algorithm that chooses from all the multiples of 2⁻¹⁰⁷⁴ in the range 0.0 ≤ x < 1.0. All such numbers are evenly spaced, but most have to be rounded down to the nearest representable Python float. (The value 2⁻¹⁰⁷⁴ is the smallest positive unnormalized float and is equal to .) Generating Pseudo-random Floating-Point Values a paper by Allen B. Downey describing ways to generate more fine-grained floats than normally generated by ."
    },
    {
        "link": "https://stackoverflow.com/questions/68689719/getting-a-unique-number-using-random-module-in-python",
        "document": "Another way of doing this is to notice that you condition excludes one option, e.g.: if the previous item was 7, then it only makes sense to choose one of the remaining 9 options. Hence make the max = 9, and add one if the variate is equal to or greater than 7.\n\nFor example, gives:\n\nwhen the RNG seeded with 42.\n\nThis gives the same result as the other rejection sampling based approaches, but can be more efficient. Specifically it'll get more efficient as the as the number of options deceases, because there's a higher chance of randomly choosing the same value twice and the other approaches having to loop around and draw another value from the RNG.\n\nAlso, the probability of picking any value is only 1/10 the first time around, every time after that you can't pick the same value again so the probability of any specific value goes up to 1/9 (this is what the rejection is doing).\n\nThe way this algorithm shuffles up based on (e.g. rather than just ) means that the resulting distribution is uniform. For example, imagine the first time we picked 4 the adjustment would work as follows:\n\nnotice that every value is uniquely mapped to a value (i.e. it's a bijection). So this algorithm will give uniform variates if the underlying RNG is uniform (the same as the rejection sampling based approaches)."
    },
    {
        "link": "https://w3schools.com/python/module_random.asp",
        "document": "Python has a built-in module that you can use to make random numbers.\n\nThe module has a set of methods:"
    },
    {
        "link": "https://stackoverflow.com/questions/22842289/generate-n-unique-random-numbers-within-a-range",
        "document": "I know how to generate a random number within a range in Python.\n\nAnd I know I can put this in a loop to generate n amount of these numbers\n\nHowever, I need to make sure each number in that list is unique. Other than a load of conditional statements, is there a straightforward way of generating n number of unique random numbers?\n\nThe important thing is that each number in the list is different to the others..\n\n[12, 5, 5, 1] = bad, because the number 5 occurs twice."
    },
    {
        "link": "https://geeksforgeeks.org/python-random-module",
        "document": "Python Random module generates random numbers in Python. These are pseudo-random numbers means they are not truly random.\n\nThis module can be used to perform random actions such as generating random numbers, printing random a value for a list or string, etc. It is an in-built function in Python.\n\nList of all the functions Python Random Module\n\nThere are different random functions in the Random Module of Python. Look at the table below to learn more about these functions:\n\nLet’s discuss some common operations performed by Random module in Python.\n\nExample 1: Printing a random value from a list in Python.\n\nThis code uses the module to select a random element from the list using the function. It prints a random element from the list, demonstrating how to pick a random item from a sequence in Python.\n\nExample 2: Creating random numbers with Python seed() in Python.\n\nAs stated above random module creates pseudo-random numbers. Random numbers depend on the seeding value. For example, if the seeding value is 5 then the output of the below program will always be the same. Therefore, it must not be used for encryption.\n\nThe code sets the random number generator’s seed to 5 using , ensuring reproducibility. It then prints two random floating-point numbers between 0 and 1 using . The seed makes these numbers the same every time you run the code with a seed of 5, providing consistency in the generated random values.\n\nrandom.randint() method is used to generate random integers between the given range.\n\nThis code uses the ‘ module to generate random integers within specific ranges. It first generates a random integer between 5 and 15 (inclusive) and then between -10 and -2 (inclusive). The generated integers are printed with appropriate formatting.\n\nA random.random() method is used to generate random floats between 0.0 to 1.\n\nIn this code, we are using the function from the ‘ module in Python. It prints a random floating-point number between 0 and 1 when you call .\n\nRandom sampling from a list in Python (random.choice, and sample)\n\nExample 1: Python random.choice() function is used to return a random item from a list, tuple, or string.\n\nThe code uses the function from the module to randomly select elements from different data types. It demonstrates selecting a random element from a list, a string, and a tuple. The chosen elements will vary each time you run the code, making it useful for random selection from various data structures.\n\nExample 2: Python random.sample() function is used to return a random item from a list, tuple, or string.\n\nThis code utilizes the function from the ‘ module to obtain random samples from various data types. It selects three random elements without replacement from a list, a tuple, and a string, demonstrating its versatility in generating distinct random samples. With each execution, the selected elements will differ, providing random subsets from the input data structures.\n\nA random.shuffle() method is used to shuffle a sequence (list). Shuffling means changing the position of the elements of the sequence. Here, the shuffling operation is inplace.\n\nThis code uses the function from the ‘ module to shuffle the elements of a list named ‘ . It first prints the original order of the list, then shuffles it twice. The second shuffle creates a new random order, and the list’s content is displayed after each shuffle. This demonstrates how the elements are rearranged randomly in the list with each shuffle operation.\n\nIn this article we discussed about Python Random module, and also saw some examples of functions in random module in Python. Random module in Python is very important and contains very useful functions.\n\nHope this helps you in using Python Random module functions.\n\nWhat is a random module in Python?\n\nWhat is random vs Randint in Python?\n\nHow many functions are there in the random module?\n\nWhat is the syntax of the random function?\n\nWhat is the difference between the math module and the random module?"
    },
    {
        "link": "https://geeksforgeeks.org/open-addressing-collision-handling-technique-in-hashing",
        "document": "Open Addressing is a method for handling collisions. In Open Addressing, all elements are stored in the hash table itself. So at any point, the size of the table must be greater than or equal to the total number of keys (Note that we can increase table size by copying old data if needed). This approach is also known as closed hashing. This entire procedure is based upon probing. We will understand the types of probing ahead:\n\nIn linear probing, the hash table is searched sequentially that starts from the original location of the hash. If in case the location that we get is already occupied, then we check for the next location.\n\nFor example, The typical gap between two probes is 1 as seen in the example below:\n\nExample: Let us consider a simple hash function as “key mod 5” and a sequence of keys that are to be inserted are 50, 70, 76, 85, 93.\n\n\n\nImplementation : Please refer Program to implement Hash Table using Open Addressing\n\nIf you observe carefully, then you will understand that the interval between probes will increase proportionally to the hash value. Quadratic probing is a method with the help of which we can solve the problem of clustering that was discussed above. This method is also known as the mid-square method. In this method, we look for the i2‘th slot in the ith iteration. We always start from the original hash location. If only the location is occupied then we check the other slots.\n\nExample: Let us consider table Size = 7, hash function as Hash(x) = x % 7 and collision resolution strategy to be f(i) = i2 . Insert = 22, 30, and 50.\n\nImplementation : Please refer Program for Quadratic Probing in Hashing\n\nThe intervals that lie between probes are computed by another hash function. Double hashing is a technique that reduces clustering in an optimized way. In this technique, the increments for the probing sequence are computed by using another hash function. We use another hash function hash2(x) and look for the i*hash2(x) slot in the ith rotation.\n\nExample: Insert the keys 27, 43, 692, 72 into the Hash Table of size 7. where first hash-function is h1​(k) = k mod 7 and second hash-function is h2(k) = 1 + (k mod 5)\n\nComparison of the above three:\n\nOpen addressing is a collision handling technique used in hashing where, when a collision occurs (i.e., when two or more keys map to the same slot), the algorithm looks for another empty slot in the hash table to store the collided key.\n• linear probing , the algorithm simply looks for the next available slot in the hash table and places the collided key there. If that slot is also occupied, the algorithm continues searching for the next available slot until an empty slot is found. This process is repeated until all collided keys have been stored. Linear probing has the best cache performance but suffers from clustering. One more advantage of Linear probing is easy to compute.\n• quadratic probing , the algorithm searches for slots in a more spaced-out manner. When a collision occurs, the algorithm looks for the next slot using an equation that involves the original hash value and a quadratic function. If that slot is also occupied, the algorithm increments the value of the quadratic function and tries again. This process is repeated until an empty slot is found. Quadratic probing lies between the two in terms of cache performance and clustering.\n• double hashing , the algorithm uses a second hash function to determine the next slot to check when a collision occurs. The algorithm calculates a hash value using the original hash function, then uses the second hash function to calculate an offset. The algorithm then checks the slot that is the sum of the original hash value and the offset. If that slot is occupied, the algorithm increments the offset and tries again. This process is repeated until an empty slot is found. Double hashing has poor cache performance but no clustering. Double hashing requires more computation time as two hash functions need to be computed.\n\nThe choice of collision handling technique can have a significant impact on the performance of a hash table. Linear probing is simple and fast, but it can lead to clustering (i.e., a situation where keys are stored in long contiguous runs) and can degrade performance. Quadratic probing is more spaced out, but it can also lead to clustering and can result in a situation where some slots are never checked. Double hashing is more complex, but it can lead to more even distribution of keys and can provide better performance in some cases.\n\nIn chaining, Hash table never fills up, we can always add more elements to chain. In open addressing, table may become full. Chaining is Less sensitive to the hash function or load factors. Chaining is mostly used when it is unknown how many and how frequently keys may be inserted or deleted. Open addressing is used when the frequency and number of keys is known. Cache performance of chaining is not good as keys are stored using linked list. Open addressing provides better cache performance as everything is stored in the same table. Wastage of Space (Some Parts of hash table in chaining are never used). In Open addressing, a slot can be used even if an input doesn’t map to it.\n\nNote: Cache performance of chaining is not good because when we traverse a Linked List, we are basically jumping from one node to another, all across the computer’s memory. For this reason, the CPU cannot cache the nodes which aren’t visited yet, this doesn’t help us. But with Open Addressing, data isn’t spread, so if the CPU detects that a segment of memory is constantly being accessed, it gets cached for quick access.\n\nLike Chaining, the performance of hashing can be evaluated under the assumption that each key is equally likely to be hashed to any slot of the table (simple uniform hashing)"
    },
    {
        "link": "https://geeksforgeeks.org/quadratic-probing-in-hashing",
        "document": "Hashing is an improvement technique over the Direct Access Table. The idea is to use a hash function that converts a given phone number or any other key to a smaller number and uses the small number as the index in a table called a hash table.\n\nQuadratic probing is an open-addressing scheme where we look for the i2‘th slot in the i’th iteration if the given hash value x collides in the hash table. We have already discussed linear probing implementation.\n\nHow Quadratic Probing is done?\n\nLet hash(x) be the slot index computed using the hash function.\n• None If the slot hash(x) % S is full, then we try (hash(x) + 1*1) % S.\n• None If (hash(x) + 1*1) % S is also full, then we try (hash(x) + 2*2) % S.\n• None If (hash(x) + 2*2) % S is also full, then we try (hash(x) + 3*3) % S.\n• None This process is repeated for all the values of i until an empty slot is found.\n\nFor example: Let us consider a simple hash function as “key mod 7” and sequence of keys as 22, 30 and 50.\n\nBelow is the implementation of the above approach:\n\n// Insert in the table if there // If there is a collision // Computing the new hash value // Insert in the table if there // If there is a collision // Computing the new hash value # Insert in the table if there # If there is a collision # Computing the new hash value // C# implementation of the Quadratic Probing // Insert in the table if there // If there is a collision // Computing the new hash value // This code is contributed by Rajput-Ji // Insert in the table if there // If there is a collision // Computing the new hash value\n\nTime Complexity: O(N * L), where N is the length of the array and L is the size of the hash table.\n\nAuxiliary Space: O(1)\n\nThe above implementation of quadratic probing does not guarantee that we will always be able to use a hast table empty slot. It might happen that some entries do not get a slot even if there is a slot available. For example consider the input array {21, 10, 32, 43, 54, 65, 87, 76} and table size 11, we get the output as {10, -1, 65, 32, 54, -1, -1, -1, 43, -1, 21} which means the items 87 and 76 never get a slot. To make sure that elements get filled, we need to have a higher table size.\n\nA hash table can be fully utilized using the below idea.\n\nIterate over the hash table to next power of 2 of table size. For example if table size is 11, then iterate 16 times. And iterate over the hash table using the below formula\n\nhash(x) = [hash(x) + (j + j*j)/2] % (Next power of 2 of table size)\n\nBelow is the implementation of this idea.\n\n// Insert in the table if there // If there is a collision // Computing the new hash value // Function to calculate the next power of 2 greater than or equal to m // Insert in the table if there is no collision // If there is a collision, iterate through possible quadratic values # Function to calculate the next power of 2 greater than or equal to m # Insert in the table if there is no collision # If there is a collision, iterate through possible quadratic values // Function to calculate the next power of 2 greater than or equal to m // Insert in the table if there is no collision // If there is a collision, iterate through possible quadratic values // Function to calculate the next power of 2 greater than or equal to m // Insert in the table if there is no collision // If there is a collision, iterate through possible quadratic values"
    },
    {
        "link": "https://python.plainenglish.io/implementing-a-hash-table-in-python-step-by-step-716f61323a4d",
        "document": "Hash tables are a cornerstone of efficient data storage and retrieval in software development. By providing rapid access to data through unique keys, hash tables enable high-speed lookups, inserts, and deletions, making them indispensable in scenarios where performance is critical, such as database indexing and caching solutions.\n\nThe essence of a hash table lies in its hashing mechanism, which converts a key into an array index using a hash function. This chosen index determines where the corresponding value is stored in the array. By ensuring that this function distributes keys uniformly across the array and employing advanced collision resolution techniques such as double hashing and quadratic probing, hash tables can minimize collisions and optimize data retrieval times. These methods enhance the hash table’s ability to maintain quick access even under high load factors, proving crucial in maintaining performance across a variety of applications.\n\n2.1 Explanation of the Key Components of a Hash Table\n\nA hash table consists of several fundamental components that work together to store and manage data efficiently:\n\nHash Function: This is the backbone of a hash table. The hash function takes an input key and computes an index into an array of buckets or slots, where the corresponding value is stored. The efficiency of a hash function is critical as it affects the distribution of data within the table. A good hash function minimizes collisions and distributes entries uniformly across the buckets.\n\nBuckets or Slots: These are the positions in the array where data entries are stored. Each bucket can store one or multiple entries. In the simplest form, a bucket holds a single key-value pair. However, depending on the collision handling strategy, it might also hold a list of entries or even a more complex data structure.\n\nHandling Collisions: Collisions occur when two keys hash to the same index. Efficiently managing collisions is vital to maintaining the performance of a hash table. There are several methods to handle collisions, including:\n• Chaining: This method involves storing multiple elements at the same index using a more complex data structure (like a linked list or another hash table). Each bucket or slot at a particular index points to the head of a list of entries that share the same hash index.A hash table consists of several fundamental components that work together to store and manage data efficiently:\n• Open Addressing: In open addressing, all elements are stored directly in the array. When a collision occurs, the hash table probes or searches for the next available slot according to a predefined sequence. Common strategies include linear probing, quadratic probing, and double hashing. Each method offers different benefits in terms of ease of implementation and effectiveness at reducing collisions.\n\n2.2 Overview of Common Uses of Hash Tables in Software Development\n\nHash tables are ubiquitous in software development, valued for their efficiency and versatility. Here are some common uses:\n• Database Indexing: Hash tables provide quick data retrieval, which is essential for the performance of database indexing systems.\n• Caching: Hash tables are ideal for caching applications where quick lookup of cached data is crucial. They allow for efficient inserts, lookups, and deletions.\n• Data Deduplication: In scenarios where data redundancy must be minimized, hash tables can help quickly identify duplicate data.\n• Associative Arrays: Many programming languages use hash tables to implement associative arrays (also known as maps or dictionaries), which can retrieve and store data based on user-defined keys.\n• Unique Data Representation: Hash tables are useful for maintaining sets of unique items and are extensively used in implementations that require checks against repetition.\n\nTo implement a hash table in Python, you need a basic setup that includes a Python interpreter and a text editor or an Integrated Development Environment (IDE). Python’s standard libraries are sufficient for building a hash table, so no additional libraries are necessary for this basic implementation. However, if you plan to enhance your hash table with more advanced features or for specific applications, you might consider using libraries such as for performance optimizations or for testing your implementation.\n\nEnsure that you have Python installed on your system. Python 3.8 or higher is recommended due to its improved features and support. You can download the latest version of Python from the official Python website or use a package manager like on macOS or on Ubuntu Linux.\n\nInitial Setup of a Python Script or Module for the Hash Table\n• Create a New Python File: Start by creating a new Python file named . This file will contain all your code related to the hash table implementation.\n• Define the Structure of Your Hash Table Class: Begin by defining a class named . This class will encapsulate all the functionalities of your hash table, including methods for insertion, deletion, and lookup. Here’s a basic structure to start with:\n• Setup Testing: It’s a good practice to set up a simple testing mechanism to validate your hash table as you build it. You can use Python’s built-in framework to write test cases:\n• Run Your Script: You can run your script using the command line by navigating to the directory containing your file and running . This command will execute your script and any tests you have written.\n\nThe HashTable class serves as the blueprint for our hash table. It is designed to manage the storage, retrieval, and deletion of key-value pairs efficiently. Below is a basic structure for our class, which includes initialization, methods for handling data, and a method for applying the hash function:\n• : Initialization of the Hash Table. This method sets up the hash table with a specified size and initializes the buckets. Buckets can be implemented as lists for chaining or as empty slots for open addressing, depending on the collision handling strategy chosen.\n• : Adding Items to the Hash Table. This method handles the insertion of new key-value pairs into the hash table. It computes the hash index for the key using a hash function and places the value in the appropriate bucket, handling collisions as necessary.\n• : Retrieving Items by Key. The get method searches for a key in the hash table and returns the corresponding value. It handles potential collisions and returns None if the key is not found, ensuring robust data retrieval.\n• : Removing Items by Key. This method removes a key-value pair from the hash table using the key. It locates the correct bucket and removes the item, managing collisions that might affect the subsequent elements in the bucket.\n• : Internal Method to Apply the Hash Function. This is a helper method that simplifies the process of applying the hash function to a key to ensure that the key is converted to a valid index within the bounds of the bucket list.\n\n4.3 Explanation of Handling Collisions Using Chaining (Linked Lists) or Open Addressing\n• Chaining (Linked Lists): Chaining is a collision resolution technique where each bucket at a specific index in the array can start a linked list. All key-value pairs that hash to the same index are stored in this list, allowing multiple entries at the same index but potentially increasing the time complexity for searching elements in the worst-case scenario.\n• Open Addressing: Open addressing stores all elements directly in the array and resolves collisions by finding another empty slot within the array. The common strategies for open addressing include linear probing, quadratic probing, and double hashing, each with distinct approaches to resolving collisions efficiently.\n\n5.1 Detailed Explanation of Chaining and Its Implementation in Python\n\nChaining is a common method to handle collisions in hash tables, where each bucket at a specific index can hold more than one element. This approach uses a secondary data structure, such as a linked list, to store multiple entries that hash to the same index. By allowing each bucket to store a list of entries, chaining handles collisions gracefully and maintains performance even under high load factors.\n\nImplementation in Python: Here’s a detailed example of implementing chaining in a hash table using linked lists:\n\nIn this implementation, each bucket can potentially store multiple entries linked together in a linked list. When a new key-value pair is inserted, the hash table checks if there’s already an entry at the computed index. If the bucket is empty, it simply inserts a new node. If not, it checks for an existing key to update or appends a new node at the end of the chain if the key does not exist. This method is efficient in terms of memory usage and simplifies the handling of collisions.\n• It handles high collision scenarios gracefully as the hash table can store more entries than it has buckets.\n• It does not require resizing the entire table, as only affected chains need to be adjusted.\n• Chaining uses more memory because of the overhead associated with linked list pointers.\n• Operations like searching for an element can become slower in worst-case scenarios when many elements hash to the same index, as it requires traversing a linked list.\n\nThis method of collision handling is suitable for applications where the load factor is high and the hash table needs to handle a large number of collisions efficiently.\n\nOpen addressing resolves collisions by probing different bucket positions within the array itself.\n\nLinear Probing: is a straightforward open addressing strategy used to resolve collisions in hash tables. Unlike chaining, where collisions are handled by linking entries at the same index, linear probing searches for the next available slot within the array to store the new entry. This method ensures all entries are stored directly within the array, which can lead to better cache performance and more efficient use of space.\n\nIn this implementation, when a collision occurs (i.e., the hashed index is already occupied), the hash table searches linearly for the next available slot. This method is simple and effective but can lead to clustering, where consecutive slots get filled, increasing the average search time as the table fills up.\n• Space-efficient as it doesn’t require additional memory outside the main table.\n• Clustering can significantly affect performance, especially in high-load scenarios.\n• The entire table must be resized when the load factor threshold is reached, which can be computationally expensive.\n\nLinear probing works well for tables with low to moderate load factors and is particularly effective when the frequency of deletions is low, as deletions can create gaps that complicate the probing sequence.\n\nQuadratic Probing: is an advanced open addressing technique used to resolve collisions in hash tables, offering a significant improvement over linear probing by addressing the issue of clustering. Unlike linear probing, which searches for the next available slot in a linear sequence, quadratic probing uses a quadratic polynomial to calculate the interval between probes, reducing the likelihood of creating clusters.\n\nIn this implementation, quadratic probing calculates the next index using a quadratic function of the number of probes, significantly reducing the problem of primary clustering encountered in linear probing. The probing function uses the formula , where and are constants, and is incremented on each probe until an empty slot is found or a loop is detected.\n• Utilizes the hash table space more efficiently by spreading out the clustered items.\n• The calculation for the next index is still relatively simple and quick.\n• Secondary clustering can still occur, though less severe than with linear probing.\n• The choice of c1 and c2 values can significantly affect performance and needs careful tuning.\n• Like other open addressing methods, it requires resizing when the table becomes too full.\n\nQuadratic probing is an effective collision resolution technique for moderate load factors and is particularly beneficial in scenarios where the hash table size can be kept large enough to avoid frequent resizing.\n\nDouble Hashing: is a refined method of open addressing that uses two hash functions to resolve collisions, significantly reducing the clustering problems associated with simpler forms of probing. Unlike linear or quadratic probing, double hashing uses a second hash function to calculate the step size after a collision occurs, ensuring that each probe follows a unique sequence based on the key. This method is known for its efficiency and effectiveness in distributing entries evenly across the hash table.\n\nIn this implementation, the first hash function determines the initial slot, and if a collision occurs, the second hash function provides an offset for the next probe, differing from linear and quadratic probing where the step is fixed or increases in a predictable pattern. Double hashing thus minimizes both primary and secondary clustering, leading to more uniform data distribution.\n• Minimizes clustering more effectively than other probing techniques.\n• Provides high performance in hash tables with high load factors.\n• Each key gets a unique probing sequence, which improves the performance over other probing methods.\n• More complex to implement than linear or quadratic probing.\n• Requires additional computation to calculate the second hash function.\n• Performance depends heavily on the quality of both hash functions.\n\nDouble hashing is particularly useful in applications where the hash table experiences high traffic or when the key distribution might lead to frequent collisions. Its ability to efficiently distribute entries makes it an excellent choice for large datasets.\n\n5.3 Pros and Cons of Each Collision Resolution Technique\n\nChaining: Pros: Simple to implement; handles high collision scenarios gracefully; does not require rehashing entire table on resizing. Cons: Uses more memory; linked list operations could be slower due to non-contiguous memory allocations.\n\nOpen Addressing: Pros: More space efficient as it stores all elements in the hash table array itself; better cache performance due to contiguous memory usage. Cons: Clustering can occur, reducing efficiency; table can become full, necessitating resizing; complex to implement resizing.\n\nEach collision handling method has trade-offs and is best suited for specific types of applications. Chaining can be preferable for hash tables with unknown or highly variable loads, while open addressing may be better for applications with stable data sets and good hash functions to minimize collisions.\n\nEffective testing of a hash table is crucial to ensure its reliability and efficiency in handling data under various conditions. Here’s how you can write comprehensive test cases for your hash table implementation using different probing techniques:\n• Start by writing simple tests to verify that basic operations, such as insert, get, and remove, work as expected. This involves adding elements to the hash table, retrieving them using their keys, and removing some to see if the table updates correctly.\n• Ensure that the hash table performs efficiently even as the load factor increases. This can be done by inserting a large number of elements and measuring the time taken for various operations. Test how the hash table behaves when nearing its capacity and how it handles resizing.\n• Create a scenario where many keys hash to the same value to test how the hash table handles collisions. This involves testing both the efficiency of collision handling mechanisms (chaining, linear probing, quadratic probing, double hashing) and the structural integrity under stress.\n• Chaining: Use pytest fixtures to create scenarios where multiple entries hash to the same bucket and ensure that they are retrievable and deletable. Test how the chaining handles extensive lists within buckets.\n\n2. Linear Probing: Focus on testing how the hash table resolves collisions by finding the next available slot. Verify that entries are correctly overwritten or updated and how the table expands when needed.\n\n3. Quadratic Probing: Similar to linear probing, but test for different initial collisions and ensure that the quadratic step calculation correctly places entries in non-clashing slots.\n\n4. Double Hashing: Test the effectiveness of using a secondary hash function in reducing collisions. Verify that the double hash method distributes entries more evenly across the table than other methods.\n\nDebugging a hash table often involves identifying issues related to collision handling, hash function distribution, and dynamic resizing. Here are some strategies:\n• Collision Handling: If elements are frequently lost or overwritten, review your collision resolution logic. Ensure that insertions, deletions, and lookups handle colliding keys correctly.\n• Hash Function Quality: Poor distribution by the hash function can lead to performance bottlenecks. If certain buckets are overused, consider revising your hash function. Testing the distribution of keys across buckets can highlight this issue.\n• Memory Leaks and Overflows: Especially relevant in languages with manual memory management, but in Python, check for unintended references that prevent garbage collection. Also, ensure that your resizing logic adequately manages memory without causing overflows or excessive reallocations.\n• Concurrency Issues: If the hash table is used in a multi-threaded application, race conditions can corrupt the data structure. Implement locks or use concurrent data structures to ensure thread safety.\n\nDynamic resizing is a critical feature for maintaining efficient performance in a hash table as the number of elements grows. Without resizing, the load factor (the ratio of the number of elements to the number of buckets) increases, leading to more collisions and consequently longer search times. To enhance performance, hash tables can be dynamically resized by:\n• Doubling the Size: When the load factor exceeds a certain threshold (commonly set at 0.7 or 0.75), the size of the hash table is doubled. This process involves creating a new hash table with twice the number of buckets and rehashing all existing elements into the new table.\n• Halving the Size: Similarly, if the load factor falls below a lower threshold (like 0.1), the size of the hash table is halved to save space and maintain efficiency in scenarios with decreasing data size.\n\nImplementing dynamic resizing involves careful handling to ensure data integrity and minimal performance impact during the resizing operation.\n\nHash tables are ubiquitous in software engineering, used in a variety of applications where quick data retrieval is crucial:\n• Databases: Hash tables power many database index mechanisms, enabling quick lookup, insertion, and deletion of records.\n• Caching Systems: Many web and application servers use hash tables for caching, reducing the number of requests to the database by storing frequently accessed data in a rapidly retrievable format.\n• Unique Item Tracking: Hash tables are ideal for tasks that require tracking unique items or checking for item existence due to their constant time performance characteristics.\n\nOptimizations can include choosing the right hash function to minimize collisions, using techniques like universal hashing, and optimizing memory allocation strategies to enhance cache performance.\n\nPython’s built-in is essentially a highly optimized hash table that is integrated into the Python language:\n• Performance: Python’s is implemented in C, giving it a significant speed advantage over a custom hash table written in pure Python due to lower-level optimizations and absence of the overhead introduced by Python's dynamic typing.\n• Features: Python’s comes with additional features like being order-preserving (as of Python 3.7), which are not typically available in basic hash table implementations.\n• Ease of Use: Being a built-in type, is directly supported by Python’s syntax and standard library, making it more convenient to use than implementing a custom hash table.\n\nWhile custom hash tables can be tailored for specific needs, for most applications, Python’s offers sufficient performance and features, making it unnecessary to implement a custom hash table unless you require specific behaviors that does not support, such as different collision handling or real-time resizing thresholds.\n\nThroughout this article, we’ve provided an in-depth look at implementing hash tables in Python, exploring various methods for resolving collisions, including chaining, linear probing, quadratic probing, and double hashing. We have detailed each method’s approach to handling collisions and optimizing the performance of hash tables, offering a comprehensive guide on their effective application. We also covered detailed testing approaches to ensure the reliability and efficiency of hash tables under different scenarios. This discussion not only enhances your understanding of data structures but also equips you with the practical skills needed for complex data management challenges in Python projects. By incorporating thorough testing practices, we showed how to improve the integrity and performance of your implementations.\n\nThank you for being a part of the In Plain English community! Before you go:\n• Be sure to clap and follow the writer ️👏️️\n• Tired of blogging platforms that force you to deal with algorithmic content? Try Differ"
    },
    {
        "link": "https://stackoverflow.com/questions/15992620/reasons-as-to-use-quadratic-probing-for-hash-table-implementation",
        "document": "Assuming we need some collision resolution algorithm,\n\nQuadratic probing can be a more efficient algorithm in a closed hash table, since it better avoids the clustering problem that can occur with linear probing, although it is not immune.\n\nQuadratic probing isn't perfect, but it does offer some advantages over alternatives:\n\nDoes he know that the hash table will always be less than half full?\n\nNot necessarily; it depends on the resizing strategy used, if any.\n\nConsider your learning on QP to be primarily educational. Practical hash table implementations don't often use open addressing, in my experience."
    },
    {
        "link": "https://runestone.academy/ns/books/published/pythonds/SortSearch/Hashing.html",
        "document": "In previous sections we were able to make improvements in our search algorithms by taking advantage of information about where items are stored in the collection with respect to one another. For example, by knowing that a list was ordered, we could search in logarithmic time using a binary search. In this section we will attempt to go one step further by building a data structure that can be searched in \\(O(1)\\) time. This concept is referred to as hashing.\n\nIn order to do this, we will need to know even more about where the items might be when we go to look for them in the collection. If every item is where it should be, then the search can use a single comparison to discover the presence of an item. We will see, however, that this is typically not the case.\n\nA hash table is a collection of items which are stored in such a way as to make it easy to find them later. Each position of the hash table, often called a slot, can hold an item and is named by an integer value starting at 0. For example, we will have a slot named 0, a slot named 1, a slot named 2, and so on. Initially, the hash table contains no items so every slot is empty. We can implement a hash table by using a list with each element initialized to the special Python value . Figure 4 shows a hash table of size \\(m=11\\). In other words, there are m slots in the table, named 0 through 10.\n\nThe mapping between an item and the slot where that item belongs in the hash table is called the hash function. The hash function will take any item in the collection and return an integer in the range of slot names, between 0 and m-1. Assume that we have the set of integer items 54, 26, 93, 17, 77, and 31. Our first hash function, sometimes referred to as the “remainder method,” simply takes an item and divides it by the table size, returning the remainder as its hash value (\\(h(item)=item \\% 11\\)). Table 4 gives all of the hash values for our example items. Note that this remainder method (modulo arithmetic) will typically be present in some form in all hash functions, since the result must be in the range of slot names.\n\nOnce the hash values have been computed, we can insert each item into the hash table at the designated position as shown in Figure 5. Note that 6 of the 11 slots are now occupied. This is referred to as the load factor, and is commonly denoted by \\(\\lambda = \\frac {numberofitems}{tablesize}\\). For this example, \\(\\lambda = \\frac {6}{11}\\).\n\nNow when we want to search for an item, we simply use the hash function to compute the slot name for the item and then check the hash table to see if it is present. This searching operation is \\(O(1)\\), since a constant amount of time is required to compute the hash value and then index the hash table at that location. If everything is where it should be, we have found a constant time search algorithm.\n\nYou can probably already see that this technique is going to work only if each item maps to a unique location in the hash table. For example, if the item 44 had been the next item in our collection, it would have a hash value of 0 (\\(44 \\% 11 == 0\\)). Since 77 also had a hash value of 0, we would have a problem. According to the hash function, two or more items would need to be in the same slot. This is referred to as a collision (it may also be called a “clash”). Clearly, collisions create a problem for the hashing technique. We will discuss them in detail later.\n\nGiven a collection of items, a hash function that maps each item into a unique slot is referred to as a perfect hash function. If we know the items and the collection will never change, then it is possible to construct a perfect hash function (refer to the exercises for more about perfect hash functions). Unfortunately, given an arbitrary collection of items, there is no systematic way to construct a perfect hash function. Luckily, we do not need the hash function to be perfect to still gain performance efficiency. One way to always have a perfect hash function is to increase the size of the hash table so that each possible value in the item range can be accommodated. This guarantees that each item will have a unique slot. Although this is practical for small numbers of items, it is not feasible when the number of possible items is large. For example, if the items were nine-digit Social Security numbers, this method would require almost one billion slots. If we only want to store data for a class of 25 students, we will be wasting an enormous amount of memory. Our goal is to create a hash function that minimizes the number of collisions, is easy to compute, and evenly distributes the items in the hash table. There are a number of common ways to extend the simple remainder method. We will consider a few of them here. The folding method for constructing hash functions begins by dividing the item into equal-size pieces (the last piece may not be of equal size). These pieces are then added together to give the resulting hash value. For example, if our item was the phone number 436-555-4601, we would take the digits and divide them into groups of 2 (43,65,55,46,01). After the addition, \\(43+65+55+46+01\\), we get 210. If we assume our hash table has 11 slots, then we need to perform the extra step of dividing by 11 and keeping the remainder. In this case \\(210\\ \\%\\ 11\\) is 1, so the phone number 436-555-4601 hashes to slot 1. Some folding methods go one step further and reverse every other piece before the addition. For the above example, we get \\(43+56+55+64+01 = 219\\) which gives \\(219\\ \\%\\ 11 = 10\\). Another numerical technique for constructing a hash function is called the mid-square method. We first square the item, and then extract some portion of the resulting digits. For example, if the item were 44, we would first compute \\(44 ^{2} = 1,936\\). By extracting the middle two digits, 93, and performing the remainder step, we get 5 (\\(93\\ \\%\\ 11\\)). Table 5 shows items under both the remainder method and the mid-square method. You should verify that you understand how these values were computed. We can also create hash functions for character-based items such as strings. The word “cat” can be thought of as a sequence of ordinal values. We can then take these three ordinal values, add them up, and use the remainder method to get a hash value (see Figure 6). Listing 1 shows a function called that takes a string and a table size and returns the hash value in the range from 0 to -1. It is interesting to note that when using this hash function, anagrams will always be given the same hash value. To remedy this, we could use the position of the character as a weight. Figure 7 shows one possible way to use the positional value as a weighting factor. The modification to the function is left as an exercise. You may be able to think of a number of additional ways to compute hash values for items in a collection. The important thing to remember is that the hash function has to be efficient so that it does not become the dominant part of the storage and search process. If the hash function is too complex, then it becomes more work to compute the slot name than it would be to simply do a basic sequential or binary search as described earlier. This would quickly defeat the purpose of hashing.\n\nWe now return to the problem of collisions. When two items hash to the same slot, we must have a systematic method for placing the second item in the hash table. This process is called collision resolution. As we stated earlier, if the hash function is perfect, collisions will never occur. However, since this is often not possible, collision resolution becomes a very important part of hashing. One method for resolving collisions looks into the hash table and tries to find another open slot to hold the item that caused the collision. A simple way to do this is to start at the original hash value position and then move in a sequential manner through the slots until we encounter the first slot that is empty. Note that we may need to go back to the first slot (circularly) to cover the entire hash table. This collision resolution process is referred to as open addressing in that it tries to find the next open slot or address in the hash table. By systematically visiting each slot one at a time, we are performing an open addressing technique called linear probing. Figure 8 shows an extended set of integer items under the simple remainder method hash function (54,26,93,17,77,31,44,55,20). Table 4 above shows the hash values for the original items. Figure 5 shows the original contents. When we attempt to place 44 into slot 0, a collision occurs. Under linear probing, we look sequentially, slot by slot, until we find an open position. In this case, we find slot 1. Again, 55 should go in slot 0 but must be placed in slot 2 since it is the next open position. The final value of 20 hashes to slot 9. Since slot 9 is full, we begin to do linear probing. We visit slots 10, 0, 1, and 2, and finally find an empty slot at position 3. Once we have built a hash table using open addressing and linear probing, it is essential that we utilize the same methods to search for items. Assume we want to look up the item 93. When we compute the hash value, we get 5. Looking in slot 5 reveals 93, and we can return . What if we are looking for 20? Now the hash value is 9, and slot 9 is currently holding 31. We cannot simply return since we know that there could have been collisions. We are now forced to do a sequential search, starting at position 10, looking until either we find the item 20 or we find an empty slot. A disadvantage to linear probing is the tendency for clustering; items become clustered in the table. This means that if many collisions occur at the same hash value, a number of surrounding slots will be filled by the linear probing resolution. This will have an impact on other items that are being inserted, as we saw when we tried to add the item 20 above. A cluster of values hashing to 0 had to be skipped to finally find an open position. This cluster is shown in Figure 9. One way to deal with clustering is to extend the linear probing technique so that instead of looking sequentially for the next open slot, we skip slots, thereby more evenly distributing the items that have caused collisions. This will potentially reduce the clustering that occurs. Figure 10 shows the items when collision resolution is done with a “plus 3” probe. This means that once a collision occurs, we will look at every third slot until we find one that is empty. The general name for this process of looking for another slot after a collision is rehashing. With simple linear probing, the rehash function is \\(newhashvalue = rehash(oldhashvalue)\\) where \\(rehash(pos) = (pos + 1) \\% sizeoftable\\). The “plus 3” rehash can be defined as \\(rehash(pos) = (pos+3) \\% sizeoftable\\). In general, \\(rehash(pos) = (pos + skip) \\% sizeoftable\\). It is important to note that the size of the “skip” must be such that all the slots in the table will eventually be visited. Otherwise, part of the table will be unused. To ensure this, it is often suggested that the table size be a prime number. This is the reason we have been using 11 in our examples. A variation of the linear probing idea is called quadratic probing. Instead of using a constant “skip” value, we use a rehash function that increments the hash value by 1, 3, 5, 7, 9, and so on. This means that if the first hash value is h, the successive values are \\(h+1\\), \\(h+4\\), \\(h+9\\), \\(h+16\\), and so on. In general, the i will be i^2 \\(rehash(pos) = (h + i^2)\\). In other words, quadratic probing uses a skip consisting of successive perfect squares. Figure 11 shows our example values after they are placed using this technique. An alternative method for handling the collision problem is to allow each slot to hold a reference to a collection (or chain) of items. Chaining allows many items to exist at the same location in the hash table. When collisions happen, the item is still placed in the proper slot of the hash table. As more and more items hash to the same location, the difficulty of searching for the item in the collection increases. Figure 12 shows the items as they are added to a hash table that uses chaining to resolve collisions. When we want to search for an item, we use the hash function to generate the slot where it should reside. Since each slot holds a collection, we use a searching technique to decide whether the item is present. The advantage is that on the average there are likely to be many fewer items in each slot, so the search is perhaps more efficient. We will look at the analysis for hashing at the end of this section. Q-1: In a hash table of size 13 which index positions would the following two keys map to? 27, 130\n• Be careful to use modulo not integer division\n• Don't divide by two, use the modulo operator. Q-2: Suppose you are given the following set of keys to insert into a hash table that holds exactly 11 values: 113 , 117 , 97 , 100 , 114 , 108 , 116 , 105 , 99 Which of the following best demonstrates the contents of the hash table after all the keys have been inserted using linear probing?\n• It looks like you may have been doing modulo 2 arithmentic. You need to use the hash table size as the modulo value.\n• Using modulo 11 arithmetic and linear probing gives these values\n• It looks like you are using modulo 10 arithmetic, use the table size.\n• Be careful to use modulo not integer division.\n\nOne of the most useful Python collections is the dictionary. Recall that a dictionary is an associative data type where you can store key–data pairs. The key is used to look up the associated data value. We often refer to this idea as a map. The map abstract data type is defined as follows. The structure is an unordered collection of associations between a key and a data value. The keys in a map are all unique so that there is a one-to-one relationship between a key and a value. The operations are given below.\n• None Create a new, empty map. It returns an empty map collection.\n• None Add a new key-value pair to the map. If the key is already in the map then replace the old value with the new value.\n• None Given a key, return the value stored in the map or otherwise.\n• None Delete the key-value pair from the map using a statement of the form .\n• None Return the number of key-value pairs stored in the map.\n• None Return for a statement of the form , if the given key is in the map, otherwise. One of the great benefits of a dictionary is the fact that given a key, we can look up the associated data value very quickly. In order to provide this fast look up capability, we need an implementation that supports an efficient search. We could use a list with sequential or binary search but it would be even better to use a hash table as described above since looking up an item in a hash table can approach \\(O(1)\\) performance. In Listing 2 we use two lists to create a class that implements the Map abstract data type. One list, called , will hold the key items and a parallel list, called , will hold the data values. When we look up a key, the corresponding position in the data list will hold the associated data value. We will treat the key list as a hash table using the ideas presented earlier. Note that the initial size for the hash table has been chosen to be 11. Although this is arbitrary, it is important that the size be a prime number so that the collision resolution algorithm can be as efficient as possible. implements the simple remainder method. The collision resolution technique is linear probing with a “plus 1” rehash function. The function (see Listing 3) assumes that there will eventually be an empty slot unless the key is already present in the . It computes the original hash value and if that slot is not empty, iterates the function until an empty slot occurs. If a nonempty slot already contains the key, the old data value is replaced with the new data value. Dealing with the situation where there are no empty slots left is an exercise. Likewise, the function (see Listing 4) begins by computing the initial hash value. If the value is not in the initial slot, is used to locate the next possible position. Notice that line 15 guarantees that the search will terminate by checking to make sure that we have not returned to the initial slot. If that happens, we have exhausted all possible slots and the item must not be present. The final methods of the class provide additional dictionary functionality. We overload the __getitem__ and __setitem__ methods to allow access using``[]``. This means that once a has been created, the familiar index operator will be available. We leave the remaining methods as exercises. The following session shows the class in action. First we will create a hash table and store some items with integer keys and string data values. Next we will access and modify some items in the hash table. Note that the value for the key 20 is being replaced. The complete hash table example can be found in ActiveCode 1. class HashTable: def __init__(self): self.size = 11 self.slots = [None] * self.size self.data = [None] * self.size def put(self,key,data): hashvalue = self.hashfunction(key,len(self.slots)) if self.slots[hashvalue] == None: self.slots[hashvalue] = key self.data[hashvalue] = data else: if self.slots[hashvalue] == key: self.data[hashvalue] = data #replace else: nextslot = self.rehash(hashvalue,len(self.slots)) while self.slots[nextslot] != None and \\ self.slots[nextslot] != key: nextslot = self.rehash(nextslot,len(self.slots)) if self.slots[nextslot] == None: self.slots[nextslot]=key self.data[nextslot]=data else: self.data[nextslot] = data #replace def hashfunction(self,key,size): return key%size def rehash(self,oldhash,size): return (oldhash+1)%size def get(self,key): startslot = self.hashfunction(key,len(self.slots)) data = None stop = False found = False position = startslot while self.slots[position] != None and \\ not found and not stop: if self.slots[position] == key: found = True data = self.data[position] else: position=self.rehash(position,len(self.slots)) if position == startslot: stop = True return data def __getitem__(self,key): return self.get(key) def __setitem__(self,key,data): self.put(key,data) H=HashTable() H[54]=\"cat\" H[26]=\"dog\" H[93]=\"lion\" H[17]=\"tiger\" H[77]=\"bird\" H[31]=\"cow\" H[44]=\"goat\" H[55]=\"pig\" H[20]=\"chicken\" print(H.slots) print(H.data) print(H[20]) print(H[17]) H[20]='duck' print(H[20]) print(H[99])"
    }
]