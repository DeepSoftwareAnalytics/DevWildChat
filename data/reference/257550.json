[
    {
        "link": "https://docs.opencv.org/4.x/d2/de8/group__core__array.html",
        "document": ""
    },
    {
        "link": "https://docs.opencv.org/3.4/d2/de8/group__core__array.html",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/66385471/cant-set-a-b-in-cv2-convertscaleabs-for-a-linear-tranform-of-the-input",
        "document": "I'm learning OpenCV, and looking at convertScaleAbs to transform the original values to the range [0,255], quite similar to what normalize do in the mode .\n\nAs far as I understand, values are transformed according to y = a*x + b, then the resulting values are clipped and converted to uint8. If this is correct, then selecting a and b this way:\n\nshould linearly transform the original values to [0,255] and the final step, the clipping, should not change the values (only the type). However I cannot obtain this correct result with the a and b values above. I create random original values, then show the result of (the ones expected), then the results of (wrong, everything converted to 255):\n\nHere is my code:"
    },
    {
        "link": "https://docs.opencv.org/4.x/d3/dc1/tutorial_basic_linear_transform.html",
        "document": "Prev Tutorial: Adding (blending) two images using OpenCV \n\n Next Tutorial: Discrete Fourier Transform \n\n\n\nIn this tutorial you will learn how to:\n• Learn what cv::saturate_cast does and why it is useful\n• Get some cool info about pixel transformations\n• Improve the brightness of an image on a practical example\n• A general image processing operator is a function that takes one or more input images and produces an output image.\n• Image transforms can be seen as:\n• In this kind of image processing transform, each output pixel's value depends on only the corresponding input pixel value (plus, potentially, some globally collected information or parameters).\n• Examples of such operators include brightness and contrast adjustments as well as color correction and transformations.\n• None Two commonly used point processes are multiplication and addition with a constant:\n• The parameters \\(\\alpha > 0\\) and \\(\\beta\\) are often called the gain and bias parameters; sometimes these parameters are said to control contrast and brightness respectively.\n• None You can think of \\(f(x)\\) as the source image pixels and \\(g(x)\\) as the output image pixels. Then, more conveniently we can write the expression as: where \\(i\\) and \\(j\\) indicates that the pixel is located in the i-th row and j-th column.\n• We load an image using cv::imread and save it in a Mat object:\n• Now, since we will make some transformations to this image, we need a new Mat object to store it. Also, we want this to have the following features:\n• Same size and type as the original image\n\nWe observe that cv::Mat::zeros returns a Matlab-style zero initializer based on image.size() and image.type()\n• We ask now the values of \\(\\alpha\\) and \\(\\beta\\) to be entered by the user:\n• Now, to perform the operation \\(g(i,j) = \\alpha \\cdot f(i,j) + \\beta\\) we will access to each pixel in image. Since we are operating with BGR images, we will have three values per pixel (B, G and R), so we will also access them separately. Here is the piece of code:\n\nNotice the following (C++ code only):\n• To access each pixel in the images we are using this syntax: image.at<Vec3b>(y,x)[c] where y is the row, x is the column and c is B, G or R (0, 1 or 2).\n• Since the operation \\(\\alpha \\cdot p(i,j) + \\beta\\) can give values out of range or not integers (if \\(\\alpha\\) is float), we use cv::saturate_cast to make sure the values are valid.\n• Finally, we create windows and show the images, the usual way.\n\nwhere cv::Mat::convertTo would effectively perform *new_image = a*image + beta*. However, we wanted to show you how to access each pixel. In any case, both methods give the same result but convertTo is more optimized and works a lot faster.\n• Running our code and using \\(\\alpha = 2.2\\) and \\(\\beta = 50\\)\n• We get this:\n\nIn this paragraph, we will put into practice what we have learned to correct an underexposed image by adjusting the brightness and the contrast of the image. We will also see another technique to correct the brightness of an image called gamma correction.\n\nIncreasing (/ decreasing) the \\(\\beta\\) value will add (/ subtract) a constant value to every pixel. Pixel values outside of the [0 ; 255] range will be saturated (i.e. a pixel value higher (/ lesser) than 255 (/ 0) will be clamped to 255 (/ 0)).\n\nThe histogram represents for each color level the number of pixels with that color level. A dark image will have many pixels with low color value and thus the histogram will present a peak in its left part. When adding a constant bias, the histogram is shifted to the right as we have added a constant bias to all the pixels.\n\nThe \\(\\alpha\\) parameter will modify how the levels spread. If \\( \\alpha < 1 \\), the color levels will be compressed and the result will be an image with less contrast.\n\nNote that these histograms have been obtained using the Brightness-Contrast tool in the Gimp software. The brightness tool should be identical to the \\(\\beta\\) bias parameters but the contrast tool seems to differ to the \\(\\alpha\\) gain where the output range seems to be centered with Gimp (as you can notice in the previous histogram).\n\nIt can occur that playing with the \\(\\beta\\) bias will improve the brightness but in the same time the image will appear with a slight veil as the contrast will be reduced. The \\(\\alpha\\) gain can be used to diminue this effect but due to the saturation, we will lose some details in the original bright regions.\n\nGamma correction can be used to correct the brightness of an image by using a non linear transformation between the input values and the mapped output values:\n\nAs this relation is non linear, the effect will not be the same for all the pixels and will depend to their original value.\n\nWhen \\( \\gamma < 1 \\), the original dark regions will be brighter and the histogram will be shifted to the right whereas it will be the opposite with \\( \\gamma > 1 \\).\n\nThe following image has been corrected with: \\( \\alpha = 1.3 \\) and \\( \\beta = 40 \\).\n\nThe overall brightness has been improved but you can notice that the clouds are now greatly saturated due to the numerical saturation of the implementation used (highlight clipping in photography).\n\nThe following image has been corrected with: \\( \\gamma = 0.4 \\).\n\nThe gamma correction should tend to add less saturation effect as the mapping is non linear and there is no numerical saturation possible as in the previous method.\n\nThe previous figure compares the histograms for the three images (the y-ranges are not the same between the three histograms). You can notice that most of the pixel values are in the lower part of the histogram for the original image. After \\( \\alpha \\), \\( \\beta \\) correction, we can observe a big peak at 255 due to the saturation as well as a shift in the right. After gamma correction, the histogram is shifted to the right but the pixels in the dark regions are more shifted (see the gamma curves figure) than those in the bright regions.\n\nIn this tutorial, you have seen two simple methods to adjust the contrast and the brightness of an image. They are basic techniques and are not intended to be used as a replacement of a raster graphics editor!\n\nA look-up table is used to improve the performance of the computation as only 256 values needs to be calculated once."
    },
    {
        "link": "https://geeksforgeeks.org/image-enhancement-techniques-using-opencv-python",
        "document": "Image enhancement is the process of improving the quality and appearance of an image. It can be used to correct flaws or defects in an image, or to simply make an image more visually appealing. Image enhancement techniques can be applied to a wide range of images, including photographs, scans, and digital images. Some common goals of image enhancement include increasing contrast, sharpness, and colorfulness; reducing noise and blur; and correcting distortion and other defects. Image enhancement techniques can be applied manually using image editing software, or automatically using algorithms and computer programs such as OpenCV. \n\nIn this article, we will explore a variety of image enhancement techniques that can be performed using OpenCV and Python. OpenCV is a powerful, open-source computer vision library that provides a wide range of image processing and computer vision algorithms. By combining the capabilities of OpenCV with the versatility of Python, we can easily implement a variety of image enhancement techniques to improve the quality and appearance of our images. \n\nIn the following sections, we will discuss the different image enhancement techniques and how to implement them using OpenCV and Python. There are several image enhancement techniques that you can use with OpenCV and Python to improve the quality and clarity of images. Here are a few examples:\n\nAdjusting the brightness and contrast of an image can significantly affect its visual appeal and effectiveness. It can also help to correct defects or flaws in the image and make it easier to see details. Finding the right balance of brightness and contrast is important for creating an attractive and effective image.\n\nThere are several ways to adjust the brightness and contrast of an image using OpenCV and Python. One common method is to use the cv2.addWeighted() function, which allows you to adjust the brightness by adding a scalar value to each pixel in the image, and the contrast by scaling the pixel values.\n\nHere is an example of how to adjust the brightness and contrast of an image using the cv2.addWeighted() function:\n\nIn this example, the brightness of the image is adjusted by adding 10 to each pixel value, and the contrast is adjusted by scaling the pixel values by 2.3. You can adjust the values of brightness and contrast to achieve the desired level of brightness and contrast.\n\nAnother method for adjusting the brightness and contrast of an image is to use the cv2.convertScaleAbs() function, which allows you to adjust the brightness and contrast using a combination of scaling and shifting the pixel values.\n\nIn this example, the brightness and contrast are adjusted using a combination of scaling and shifting the pixel values. You can adjust the values of alpha and beta to achieve the desired level of brightness and contrast.\n\nSharpening is the process of enhancing the edges and fine details in an image to make it appear sharper and more defined. It is important because it can help to bring out the details and features in an image, making it more visually appealing and easier to understand. Sharpening can be used to correct blur or softness in an image and can be applied using a variety of techniques.\n\nOne common method for sharpening images using OpenCV and Python is to use the cv2.filter2D() function, which convolves the image with a kernel. The kernel can be designed to enhance the edges in the image, resulting in a sharper image.\n\nHere is an example of how to sharpen an image using the cv2.filter2D() function:\n\nIn this example, a 3×3 sharpening kernel is used to enhance the edges in the image. You can experiment with different kernels to achieve the desired level of sharpening. Numpy is used to create the sharpening kernel is created as a NumPy array using the np.array() function. This array is then passed as an argument to the cv2.filter2D() function, which convolves the image with the kernel to sharpen it.\n\nAnother method for sharpening images is to use the cv2.Laplacian() function, which calculates the Laplacian of an image and returns the result as a sharpened image.\n\nIn this example, the Laplacian operator calculates the sharpened image. You can adjust the depth of the output image using the cv2.CV_64F parameter.\n\nNoise reduction is the process of removing or reducing unwanted noise or artifacts from an image. It is important because it can improve the visual quality and clarity of the image and make it easier to analyze or process using computer algorithms. Noise can be introduced into an image due to a variety of factors and can degrade its quality. There are several techniques for reducing noise, including using filters such as the median filter or the Gaussian filter. It is important to apply noise reduction judiciously to avoid blur or loss of detail in the image.\n\nOne common method for removing noise from images using OpenCV and Python is to use a median filter. The median filter works by replacing each pixel in the image with the median value of a set of neighboring pixels. This can help to smooth out noise and reduce artifacts in the image.\n\nHere is an example of how to remove noise from an image using the cv2.medianBlur() function in OpenCV:\n\nIn this example, the cv2.medianBlur() function is used to apply a median filter to the image. The 5 parameter specifies the size of the kernel to use for the filter. You can adjust the kernel size to achieve the desired level of noise reduction.\n\nAnother method for removing noise from images is to use a Gaussian filter, which uses a weighted average of neighboring pixels to smooth out noise and reduce artifacts. You can use the cv2.GaussianBlur() function to apply a Gaussian filter to an image in OpenCV.\n\nIn this example, the cv2.GaussianBlur() function is used to apply a Gaussian filter to the image. The (5, 5) parameter specifies the size of the kernel to use for the filter, and the 0 parameter specifies the standard deviation of the Gaussian function. You can adjust these parameters to achieve the desired level of noise reduction.\n\nColor enhancement is adjusting the colors in an image to make them more vibrant, balanced, or natural. It can be used to correct color defects or problems in an image or to simply make an image more appealing and aesthetically pleasing. Color enhancement is important because it can significantly affect the visual impact and effectiveness of an image. It can also be useful for correcting color errors or problems in an image and can make it easier to see details and features in the image. There are several techniques for enhancing the colors in an image, including adjusting the color balance, adjusting the saturation, and adjusting the hue.\n\nThere are several ways to enhance the colors in an image using OpenCV and Python. One common method is to use the cv2.cvtColor() function, which allows you to convert the image from one color space to another. This can be useful for adjusting the color balance or saturation of the image.\n\nHere is an example of how to enhance the colors in an image using the cv2.cvtColor() function:\n\nThis code first converts the image from the BGR color space to the HSV color space using the cv2.cvtColor() function. It then adjusts the hue, saturation, and value (brightness) of the image by multiplying the corresponding channels by a scalar value. Finally, it converts the image back to the BGR color space and saves the modified image. You can adjust the scalar values to achieve the desired level of color enhancement.\n\nImage resizing and scaling involve adjusting the dimensions and size of an image. Both are important for meeting specific requirements or context, such as fitting a specific size or aspect ratio or reducing the file size. There are several techniques, including interpolation methods like the nearest neighbor, bilinear, and bicubic interpolation. It is important to choose a method that preserves image quality and clarity.\n\nYou can use the cv2.resize() function in OpenCV to resize and scale images. The cv2.resize() function takes the following arguments:\n• dsize: The size of the output image, specified as a tuple (width, height).\n• fx: The scaling factor along the x-axis.\n• fy: The scaling factor along the y-axis.\n• interpolation: The interpolation method to use.\n\nHere is an example of how to use the cv2.resize() function to resize an image:\n\nIn this example, the image is resized to a width of 400 pixels and a height of 300 pixels.\n\nYou can also use the fx and fy parameters to specify the scaling factors along the x-axis and y-axis, respectively. For example:\n\nIn this example, the image is scaled by a factor of 2 along both axes, resulting in an image that is twice the size of the original. The interpolation parameter allows you to specify the interpolation method to use when resizing or scaling the image. The available options include cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, and others.\n\nThis is just a basic example of how to resize and scale images using OpenCV and Python. You can adjust the size and scaling factors to achieve the desired results, and you can also specify the interpolation method to use when resizing or scaling the image.\n\nWe can also inverse the color by simply subtracting each value from 255\n\nHistogram equalization is a technique used to adjust the contrast of an image by spreading out the intensity values of the pixels in the image. It is important because it can improve the contrast and clarity of an image, making it easier to see details and features that might be difficult to see in an image with low contrast. There are several different methods for performing histogram equalization, including global histogram equalization and local histogram equalization. Global histogram equalization adjusts the contrast of the entire image, while local histogram equalization adjusts the contrast in small, localized areas of the image.\n\nYou can use the cv2.equalizeHist() function in OpenCV to equalize the histogram of an image. This function takes the image data as an argument and returns the equalized image data. Here is an example of how to use the cv2.equalizeHist() function to equalize the histogram of an image:\n\nIn this example, the image is first loaded from a file using the cv2.imread() function. It is then converted to grayscale using the cv2.cvtColor() function. The cv2.equalizeHist() function is then called and passed the grayscale image data as an argument. The equalized image data is stored in the equalized_image variable. Finally, the modified image is saved to a file using the cv2.imwrite() function.\n\nNote that the cv2.equalizeHist() function only works on grayscale images. If you want to equalize the histogram of a color image, you will need to convert the image to a color space that separates the intensity values (such as the YCrCb color space) and apply histogram equalization to the intensity channel. You can then convert the image back to the original color space if desired.\n\nImage enhancement is a wide field that involves adjusting images to improve their visual quality or to make them more suitable for further analysis. There are many techniques for enhancing images, such as:\n• Morphological transformations: These are operations based on the image shape. They are typically applied to binary images, but can also be used with grayscale images. Examples include dilation, erosion, opening, closing, etc. Operations can be used to enhance or modify the shape or structure of objects in an image. To apply morphological operations with OpenCV and Python, you can use functions such as erode, dilate, and morphologyEx.\n• Edge detection: OpenCV provides several functions for performing edge detection, such as Canny(), Sobel(), and Laplacian(). These functions can be used to identify edges in an image by looking for sharp changes in pixel intensity.\n• Color correction: OpenCV provides several functions for adjusting the colors in an image, such as cvtColor() and inRange(). These functions can be used to perform tasks such as color balance, color grading, and white balancing.\n• Image gradients: OpenCV provides several functions for computing image gradients, such as Scharr(), Sobel(), and Laplacian(). These functions can be used to highlight changes in pixel intensity in an image and can be useful for tasks such as edge detection and image segmentation.\n• Image cropping: Cropping techniques can be used to remove unwanted areas from an image. To crop an image, you can use the copyMakeBorder function to create a new image with the desired dimensions, and then copy the relevant portion of the original image into the new image.\n• Image rotation: Rotation techniques can be used to change the orientation of an image. To rotate an image with OpenCV, you can use the warpAffine function with a rotation matrix.\n• Image blending: Blending techniques can be used to combine two or more images together. To blend images with OpenCV and Python, you can use the addWeighted function to combine the images using a weighted average.\n• Image thresholding: Thresholding techniques can be used to convert an image to black and white by setting a threshold value for pixel intensity. To apply thresholding, you can use the threshold function.\n• Image deblurring: Deblurring techniques can be used to remove blur from an image caused by camera shake, out-of-focus subjects, or other factors. To deblur an image, you can use the wiener function, which applies a Wiener filter to the image.\n\n\n\nOpenCV is a powerful library for image processing and computer vision tasks and it provides many advanced image enhancement techniques that can be used for a variety of applications. Some of these techniques are:\n• Super-resolution: OpenCV provides the pyrUp() and pyrDown() functions for upsampling and downsampling images, respectively. These functions can be used as part of a super-resolution algorithm to increase the resolution of an image.\n• Image restoration: OpenCV provides several functions for image restoration, such as fastNlMeansDenoising() and fastNlMeansDenoisingColored(). These functions can be used to remove noise and improve the visual quality of an image.\n• Image fusion: OpenCV provides the addWeighted() function for combining two images using a weighted sum. This function can be used to fuse multiple images of the same scene to create a single image that contains more information or is of higher quality.\n• Image segmentation: OpenCV provides several functions for image segmentation, including threshold(), adaptiveThreshold(), and findContours(). These functions can be used to partition an image into regions or segments that correspond to different objects or regions of interest.\n• Image recognition: OpenCV provides several functions for image recognition, including HOGDescriptor() and SIFT(). These functions can be used to extract features from an image and train a machine-learning model to recognize objects or scenes.\n• Object detection: OpenCV provides several functions for object detection, including HOGDescriptor() and SIFT(). These functions can be used to detect and locate objects in an image or video in real time.\n• Image registration: OpenCV provides the registerTranslation() function for aligning or registering two or more images of the same scene. This function can be used to align images from different sensors or from different viewpoints.\n• Image stitching: Image stitching techniques can be used to combine multiple images into a single panoramic or mosaic image. To apply image stitching with OpenCV and Python, you can use techniques such as feature matching, which matches the features in the source images to create a common reference frame, and image warping, which warps the images to align them with the reference frame.\n\n\n\nTo effectively use image enhancement techniques using OpenCV and Python, it is important to choose the right technique for your images and desired outcomes, experiment with different parameters, use caution when increasing image contrast, use color appropriately, and consider using other image processing techniques as needed. For further reading and resources on image enhancement using OpenCV and Python, consider exploring the OpenCV documentation and tutorials, as well as online resources such as the PyImageSearch blog and tutorials, and the Python Machine Learning with OpenCV course. \n\nIn conclusion, image enhancement is a crucial technique used to improve the visual quality of images in various fields such as medicine, robotics, security, photography, remote sensing, and manufacturing. OpenCV is a powerful open-source library that provides a wide range of image enhancement techniques for use in Python. By using OpenCV, developers can easily implement image enhancement techniques in their applications to improve the visual quality of images and extract valuable information from them. Whether you are a researcher, a developer, or a professional working in a field that relies on images, OpenCV and Python offers a flexible and powerful toolkit for enhancing the quality of your images."
    },
    {
        "link": "https://numpy.org/doc/2.1/reference/generated/numpy.ndarray.html",
        "document": "An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)\n\nArrays should be constructed using , or (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(…)) for instantiating an array.\n\nFor more information, refer to the module and examine the methods and attributes of an array.\n\nThere are two modes of creating an array using :\n• None If buffer is None, then only , , and order are used.\n• None If buffer is an object exposing the buffer interface, then all keywords are interpreted.\n\nNo method is needed because the array is fully initialized after the method.\n\nThese examples illustrate the low-level constructor. Refer to the See Also section above for easier ways of constructing an ndarray.\n\nFirst mode, buffer is None:\n\nReturns True if all elements evaluate to True. Returns True if any of the elements of a evaluate to True. Return indices of the maximum values along the given axis. Return indices of the minimum values along the given axis. Returns the indices that would partition this array. Returns the indices that would sort this array. Copy of the array, cast to a specified type. Swap the bytes of the array elements Use an index array to construct a new array from a set of choices. Return an array whose values are limited to . Return selected slices of this array along given axis. Return the cumulative product of the elements along the given axis. Return the cumulative sum of the elements along the given axis. Dump a pickle of the array to the specified file. Returns the pickle of the array as a string. Fill the array with a scalar value. Return a copy of the array collapsed into one dimension. Returns a field of the given array as a certain type. Copy an element of an array to a standard Python scalar and return it. Return the maximum along a given axis. Returns the average of the array elements along given axis. Return the minimum along a given axis. Return the indices of the elements that are non-zero. Partially sorts the elements in the array in such a way that the value of the element in k-th position is in the position it would be in a sorted array. Return the product of the array elements over the given axis Set for all n in indices. Returns an array containing the same data with a new shape. Return a with each element rounded to the given number of decimals. Find indices where elements of v should be inserted in a to maintain order. Put a value into a specified place in a field defined by a data-type. Remove axes of length one from a. Returns the standard deviation of the array elements along given axis. Return the sum of the array elements over the given axis. Return a view of the array with axis1 and axis2 interchanged. Return an array formed from the elements of a at the given indices. Construct Python bytes containing the raw data bytes in the array. Write array to a file as text or binary (default). Return the array as an -levels deep nested list of Python scalars. A compatibility alias for , with exactly the same behavior. Return the sum along diagonals of the array. Returns a view of the array with axes transposed. Returns the variance of the array elements, along given axis. New view of array with the same data."
    },
    {
        "link": "https://scikit-image.org/docs/stable/user_guide/numpy_images.html",
        "document": "A crash course on NumPy for images#\n\nImages in are represented by NumPy ndarrays. Hence, many common operations can be achieved using standard NumPy methods for manipulating arrays:\n\nRetrieving the geometry of the image and the number of pixels:\n\nNumPy arrays representing images can be of different integer or float numerical types. See Image data types and what they mean for more information about these types and how treats them.\n\nNumPy indexing can be used both for looking at the pixel values and to modify them: # Get the value of the pixel at the 10th row and 20th column # Set to black the pixel at the 3rd row and 10th column Be careful! In NumPy indexing, the first dimension ( ) corresponds to rows, while the second ( ) corresponds to columns, with the origin ( ) at the top-left corner. This matches matrix/linear algebra notation, but is in contrast to Cartesian (x, y) coordinates. See Coordinate conventions below for more details. Beyond individual pixels, it is possible to access/modify values of whole sets of pixels using the different indexing capabilities of NumPy. # Set the first ten lines to \"black\" (0) # Set to \"white\" (255) the pixels where mask is True Masks are very useful when you need to select a set of pixels on which to perform the manipulations. The mask can be any boolean array of the same shape as the image (or a shape broadcastable to the image shape). This can be used to define a region of interest, for example, a disk: Boolean operations from NumPy can be used to define even more complex masks:\n\nAll of the above remains true for color images. A color image is a NumPy array with an additional trailing dimension for the channels: This shows that is a 300-by-451 pixel image with three channels (red, green, and blue). As before, we can get and set the pixel values: # Set the pixel at (50th row, 60th column) to \"black\" # set the pixel at (50th row, 61st column) to \"green\" We can also use 2D boolean masks for 2D multichannel images, as we did with the grayscale image above: The example color images included in have channels stored along the last axis, although other software may follow different conventions. The scikit-image library functions supporting color images have a argument that can be used to specify which axis of an array corresponds to channels.\n\nBecause represents images using NumPy arrays, the coordinate conventions must match. Two-dimensional (2D) grayscale images (such as above) are indexed by rows and columns (abbreviated to either or ), with the lowest element at the top-left corner. In various parts of the library, you will also see and refer to lists of row and column coordinates. We distinguish this convention from , which commonly denote standard Cartesian coordinates, where is the horizontal coordinate, - the vertical one, and the origin is at the bottom left (Matplotlib axes, for example, use this convention). In the case of multichannel images, any dimension (array axis) can be used for color channels, and is denoted by or . Prior to scikit-image 0.19, this channel dimension was always last, but in the current release the channel dimension can be specified by a argument. Functions that require multichannel data default to . Otherwise, functions default to , indicating that no axis is assumed to correspond to channels. Finally, for volumetric (3D) images, such as videos, magnetic resonance imaging (MRI) scans, confocal microscopy, etc., we refer to the leading dimension as , abbreviated as or . These conventions are summarized below: Dimension name and order conventions in scikit-image # Note that the position of is controlled by the argument. Many functions in can operate on 3D images directly: In many cases, however, the third spatial dimension has lower resolution than the other two. Some functions provide a keyword argument to help handle this kind of data: Other times, the processing must be done plane-wise. When planes are stacked along the leading dimension (in agreement with our convention), the following syntax can be used:\n\nNotes on the order of array dimensions# Although the labeling of the axes might seem arbitrary, it can have a significant effect on the speed of operations. This is because modern processors never retrieve just one item from memory, but rather a whole chunk of adjacent items (an operation called prefetching). Therefore, processing of elements that are next to each other in memory is faster than processing them when they are scattered, even if the number of operations is the same: When the last/rightmost dimension becomes even larger the speedup is even more dramatic. It is worth thinking about data locality when developing algorithms. In particular, uses C-contiguous arrays by default. When using nested loops, the last/rightmost dimension of the array should be in the innermost loop of the computation. In the example above, the numpy operator iterates over all remaining dimensions."
    },
    {
        "link": "https://scipy-lectures.org/advanced/image_processing",
        "document": "2.6. Image manipulation and processing using Numpy and Scipy¶\n\nThis section addresses basic image manipulation and processing using the core scientific modules NumPy and SciPy. Some of the operations covered by this tutorial may be useful for other kinds of multidimensional array processing than image processing. In particular, the submodule provides functions operating on n-dimensional NumPy arrays.\n\nTools used in this tutorial:\n• None : submodule dedicated to image processing (n-dimensional images). See the documentation:\n• Image segmentation: labeling pixels corresponding to different objects\n\n# First we need to create the PNG file Need to know the shape and dtype of the image (how to separate data bytes). For large data, use for memory mapping:\n\nLocal filters: replace the value of pixels by a function of the values of neighboring pixels. increase the weight of edges by adding an approximation of the Laplacian: A Gaussian filter smoothes the noise out… and the edges as well:\n• Create a binary image (of 0s and 1s) with several objects (circles, ellipses, squares, or random shapes).\n• Try two different denoising methods for denoising the image: gaussian filtering and median filtering.\n• Compare the histograms of the two different denoised images. Which one is the closest to the histogram of the original (noise-free) image? More denoising filters are available in , see the Scikit-image: image processing tutorial. See wikipedia for a definition of mathematical morphology. Probe an image with a simple shape (a structuring element), and modify this image according to how the shape locally fits or misses the image. Erosion = minimum filter. Replace the value of a pixel by the minimal value covered by the structuring element.: Many other mathematical morphology operations: hit and miss transform, tophat, etc.\n\nUse mathematical morphology to clean up the result: Check that reconstruction operations (erosion + propagation) produce a better result than opening/closing: Check how a first denoising step (e.g. with a median filter) modifies the histogram, and check that the resulting histogram-based segmentation is more accurate. More advanced segmentation algorithms are found in the : see Scikit-image: image processing. Other Scientific Packages provide algorithms that can be useful for image processing. In this example, we use the spectral clustering function of the in order to segment glued objects. # Convert the image into a graph with the value of the gradient on # Take a decreasing function of the gradient: we take it weakly # dependant from the gradient the segmentation is close to a voronoi\n\nCompute size, mean_value, etc. of each region: Can be used outside the limited scope of segmentation applications. # note that we use broadcasting When regions are regular blocks, it is more efficient to use stride tricks (Example: fake dimensions with strides). One example with mathematical morphology: granulometry"
    },
    {
        "link": "https://numpy.org/doc/2.2/reference/arrays.ndarray.html",
        "document": "An is a (usually fixed-size) multidimensional container of items of the same type and size. The number of dimensions and items in an array is defined by its , which is a of N non-negative integers that specify the sizes of each dimension. The type of items in the array is specified by a separate data-type object (dtype), one of which is associated with each ndarray.\n\nAs with other container objects in Python, the contents of an can be accessed and modified by indexing or slicing the array (using, for example, N integers), and via the methods and attributes of the .\n\nDifferent can share the same data, so that changes made in one may be visible in another. That is, an ndarray can be a “view” to another ndarray, and the data it is referring to is taken care of by the “base” ndarray. ndarrays can also be views to memory owned by Python or objects implementing the or array interfaces.\n\nAn instance of class consists of a contiguous one-dimensional segment of computer memory (owned by the array, or by some other object), combined with an indexing scheme that maps N integers into the location of an item in the block. The ranges in which the indices can vary is specified by the of the array. How many bytes each item takes and how the bytes are interpreted is defined by the data-type object associated with the array. A segment of memory is inherently 1-dimensional, and there are many different schemes for arranging the items of an N-dimensional array in a 1-dimensional block. NumPy is flexible, and objects can accommodate any strided indexing scheme. In a strided scheme, the N-dimensional index \\((n_0, n_1, ..., n_{N-1})\\) corresponds to the offset (in bytes): from the beginning of the memory block associated with the array. Here, \\(s_k\\) are integers which specify the of the array. The column-major order (used, for example, in the Fortran language and in Matlab) and row-major order (used in C) schemes are just specific kinds of strided scheme, and correspond to memory that can be addressed by the strides: Both the C and Fortran orders are contiguous, i.e., single-segment, memory layouts, in which every part of the memory block can be accessed by some combination of the indices. Contiguous arrays and single-segment arrays are synonymous and are used interchangeably throughout the documentation. While a C-style and Fortran-style contiguous array, which has the corresponding flags set, can be addressed with the above strides, the actual strides may be different. This can happen in two cases:\n• None If then for any legal index . This means that in the formula for the offset \\(n_k = 0\\) and thus \\(s_k n_k = 0\\) and the value of \\(s_k\\) = self.strides[k] is arbitrary.\n• None If an array has no elements ( ) there is no legal index and the strides are never used. Any array with no elements may be considered C-style and Fortran-style contiguous. Point 1. means that and always have the same contiguity and flags value. This also means that even a high dimensional array could be C-style and Fortran-style contiguous at the same time. An array is considered aligned if the memory offsets for all elements and the base offset itself is a multiple of . Understanding memory-alignment leads to better performance on most hardware. It does not generally hold that for C-style contiguous arrays or for Fortran-style contiguous arrays is true. Data in new is in the row-major (C) order, unless otherwise specified, but, for example, basic array slicing often produces views in a different scheme. Several algorithms in NumPy work on arbitrarily strided arrays. However, some algorithms require single-segment arrays. When an irregularly strided array is passed in to such algorithms, a copy is automatically made.\n\nArray attributes reflect information that is intrinsic to the array itself. Generally, accessing an array through its attributes allows you to get and sometimes set intrinsic properties of the array without creating a new array. The exposed attributes are the core parts of an array and only some of them can be reset meaningfully without creating a new array. Information on each attribute is given below. The following attributes contain information about the memory layout of the array: Information about the memory layout of the array. Tuple of bytes to step in each dimension when traversing an array. Python buffer object pointing to the start of the array's data. Number of elements in the array. Length of one array element in bytes. Total bytes consumed by the elements of the array. Base object if memory is from some other object. The data type object associated with the array can be found in the attribute: The real part of the array. The imaginary part of the array. An object to simplify the interaction of the array with the ctypes module.\n\nAn object has many methods which operate on or with the array in some fashion, typically returning an array result. These methods are briefly explained below. (Each method’s docstring has a more complete description.) For the following methods there are also corresponding functions in : , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , . Copy an element of an array to a standard Python scalar and return it. Return the array as an -levels deep nested list of Python scalars. A compatibility alias for , with exactly the same behavior. Construct Python bytes containing the raw data bytes in the array. Write array to a file as text or binary (default). Dump a pickle of the array to the specified file. Returns the pickle of the array as a string. Copy of the array, cast to a specified type. Swap the bytes of the array elements New view of array with the same data. Returns a field of the given array as a certain type. Fill the array with a scalar value. For reshape, resize, and transpose, the single tuple argument may be replaced with integers which will be interpreted as an n-tuple. Returns an array containing the same data with a new shape. Returns a view of the array with axes transposed. Return a view of the array with axis1 and axis2 interchanged. Return a copy of the array collapsed into one dimension. Remove axes of length one from a. For array methods that take an axis keyword, it defaults to None. If axis is None, then the array is treated as a 1-D array. Any other value for axis represents the dimension along which the operation should proceed. Return an array formed from the elements of a at the given indices. Set for all n in indices. Use an index array to construct a new array from a set of choices. Returns the indices that would sort this array. Partially sorts the elements in the array in such a way that the value of the element in k-th position is in the position it would be in a sorted array. Returns the indices that would partition this array. Find indices where elements of v should be inserted in a to maintain order. Return the indices of the elements that are non-zero. Return selected slices of this array along given axis. Many of these methods take an argument named axis. In such cases,\n• None If axis is None (the default), the array is treated as a 1-D array and the operation is performed over the entire array. This behavior is also the default if self is a 0-dimensional array or array scalar. (An array scalar is an instance of the types/classes float32, float64, etc., whereas a 0-dimensional array is an ndarray instance containing precisely one array scalar.)\n• None If axis is an integer, then the operation is done over the given axis (for each 1-D subarray that can be created along the given axis). Example of the axis argument A 3-dimensional array of size 3 x 3 x 3, summed over each of its three axes: # for sum, axis is the first keyword, so we may omit it, # specifying only its value The parameter dtype specifies the data type over which a reduction operation (like summing) should take place. The default reduce data type is the same as the data type of self. To avoid overflow, it can be useful to perform the reduction using a larger data type. For several methods, an optional out argument can also be provided and the result will be placed into the output array given. The out argument must be an and have the same number of elements. It can have a different data type in which case casting will be performed. Return the maximum along a given axis. Return indices of the maximum values along the given axis. Return the minimum along a given axis. Return indices of the minimum values along the given axis. Return an array whose values are limited to . Return a with each element rounded to the given number of decimals. Return the sum along diagonals of the array. Return the sum of the array elements over the given axis. Return the cumulative sum of the elements along the given axis. Returns the average of the array elements along given axis. Returns the variance of the array elements, along given axis. Returns the standard deviation of the array elements along given axis. Return the product of the array elements over the given axis Return the cumulative product of the elements along the given axis. Returns True if all elements evaluate to True. Returns True if any of the elements of a evaluate to True.\n\nArithmetic and comparison operations on are defined as element-wise operations, and generally yield objects as results. Each of the arithmetic operations ( , , , , , , , or , , , , , , ) and the comparisons ( , , , , , ) is equivalent to the corresponding universal function (or ufunc for short) in NumPy. For more information, see the section on Universal Functions. Truth value of an array ( ): Truth-value testing of an array invokes , which raises an error if the number of elements in the array is not 1, because the truth value of such arrays is ambiguous. Use and instead to be clear about what is meant in such cases. (If you wish to check for whether an array is empty, use for example .)\n• None Any third argument to is silently ignored, as the underlying takes only two arguments.\n• None Because is a built-in type (written in C), the special methods are not directly defined.\n• None The functions called to implement many arithmetic special methods for arrays can be modified using . In place operations will perform the calculation using the precision decided by the data type of the two operands, but will silently downcast the result (if necessary) so it can fit back into the array. Therefore, for mixed precision calculations, can be different than . For example, suppose . Then, is different than : while they both perform the same computation, casts the result to fit back in , whereas re-binds the name to the result. Matrix operators and were introduced in Python 3.5 following PEP 465, and the operator has been introduced in NumPy 1.10.0. Further information can be found in the documentation."
    },
    {
        "link": "http://py5coding.org/integrations/numpy.html",
        "document": "Numpy is used extensively in the Python world. It is the foundation of virtually all of the numerical computing projects that are implemented in the Python language. It has a long history within the Python community, and like Charts, Plots, and Matplotlib, numpy’s contributions to the scientific community cannot be overstated. Py5 is well positioned to be used for numerical computing because of its close ties with numpy.\n\nNumpy is one of py5’s dependencies, so it will always be installed alongside py5.\n\nIn Processing and p5, direct pixel manipulation is done with , a one dimensional array of colors (integers). Using this array can be a bit tedious because we usually think about the Sketch window in two dimensions, not one. For example, to change a pixel at a specific location in the Sketch window, we must do a few calculations to find that pixel’s location in the one dimensional array. You can accomplish many things working with a one dimensional array of pixels. However, working with pixels in this way is also very different from how direct pixel manipulation is typically done by virtually every Python program that has access to numpy. In the Python world, direct pixel manipulation is typically done with a 3 dimensional array. The 3 dimensions represent the vertical and horizontal positioning of the pixel and the pixel’s color, split into several color channels. To provide analogous functionality, py5 offers the 3 dimensional array . This is actualized in np_pixels[], Py5Graphics.np_pixels[], and Py5Image.np_pixels[]. Let’s explore these two pixel data structures with a simple example. We will start with our imports. Our example will first load an image and draw it to the screen. Then we will swap the color channels for all of the pixels contained within an inner region of the Sketch window. For each modified pixel, we will rotate the color channels so that Blue becomes Red, Red becomes Green, and Green becomes Blue. To do this we must loop through all of the pixels in pixels[] and extract each pixel’s Red, Green, and Blue values. Then we will re-order the Red, Green, and Blue values to create a new color. This new color will be assigned back to pixels[]. In the Sketch code, observe the calls to load_pixels() and update_pixels(). These are necessary to prepare pixels[] for accessing color information and to write changes to pixels[] back to the Sketch window. Also observe the code used to find each pixel in the pixels[] array. If that calculation was incorrect, the Sketch would behave erratically or crash. If you are running this code yourself, you will also notice the Sketch is very slow. It takes at least a few seconds for it to complete the pixel alterations. It does finish eventually, and has a nice result: Now let’s do this again, but with np_pixels[]. In the following code, the loop is gone, and load_pixels() and update_pixels() have been replaced with load_np_pixels() and update_np_pixels(). Observe how our code is extracting each of the individual color channels. We are indexing into np_pixels[] to extract entire blocks of data. The last index value in each line of code, 0, 1, 2, and 3, correspond to the Alpha, Red, Green, and Blue channels, in that order. Those blocks of numbers are then assigned back to np_pixels[] to swap the color channels. This approach yields the same result, but is considerably faster. Before moving on to the next section, take a moment to ponder the effects of swapping color channels. The image retains the same basic structure, but the colors look odd. From looking at an image, do you think you’d be able to tell that the color channels have been moved around?\n\nThe array’s third dimension can sometimes be tricky to work with because you must be cognizant of where each color channel is located in the Color Channel Ordering. Sometimes the term “bands” will be used. The core issue you need to pay attention to is the fact that there is no standard or “correct” way to order the Color Channel. One library might order the Color Channel starting with Alpha, then Red, Green, and finally Blue (like py5 does), but another might use the order Red, Green, Blue, and then Alpha. Most libraries will offer a way to re-order the Color Channels, of course, but to do that successfully, you need to know what order you are starting with and what order you should re-order it to. Bottom line, if your code expects the Color Channel Order to be one way but the image data is actually ordered a different way, the image’s colors will be incorrect. It is best to illustrate what this means for you, the py5 coder, with a brief example. We will explore this by loading an image of the py5 logo and using it to understand Color Channel Ordering. To begin, we will load the image with the PIL, the Python Imaging Library. Nice looking logo. :) To explore Color Channels, we will convert the image to a numpy array. The last value is 4, indicating this array has 4 Color Channels. Next we will load the same image using OpenCV. OpenCV loads images as numpy arrays. We must use the flag to instruct it to retain the image’s alpha channel. Now we will use both arrays in a Sketch. We will use the create_image_from_numpy() method to create a Py5Image object from each array. The results are completely wrong. None of the colors look right. The two images don’t look the same as each other, and neither looks like the actual py5 logo: The answer has to do with Color Channel Ordering. We know from the previous section that in py5, the Color Channel Order of np_pixels[] starts with Alpha, followed by Red, Green, and finally, Blue (also abbreviated as ARGB). And here’s the problem: our PIL and OpenCV image arrays use different Color Channel Orderings. What Color Channel Order do they use? Well, for the PIL Image object, we can use the property to find out: It is ‘RGBA’, not ‘ARGB’. The Alpha channel is last, not first. It’s different from what py5 and the create_image_from_numpy() method expect. Before we continue, let’s think about why this matters. A numpy array is essentially a multi-dimensional grid of numbers. Each dimension of the grid of numbers means something different. We know that for image data, the first dimension corresponds to height, the second to width, and third to the color channel. When py5 receives a numpy array and converts it into an image, it must make a decision and interpret the array’s last dimension as a specific sequence of colors. If it uses the wrong sequence colors, it will make mistakes, interpreting the colors incorrectly. In this case, the create_image_from_numpy() method was interpreting the Color Channel Order as ‘ARGB’, which it will do by default, but the Color Channel Order in the array was actually ‘RGBA’. That means that the image’s first color channel was actually Red, but py5 interpreted it as the Alpha channel. The image’s second color channel, Green, was interpreted by py5 as the Red channel. The image’s third color channel, Blue, was interpreted by py5 as the Green channel. And finally, the image’s last color channel, Alpha, was interpreted by py5 as the Blue channel. Every Color Channel was interpreted incorrectly. How do we fix it? Well, one way is to manipulate the PIL Image object to re-order the color channels so that when it is converted to a numpy array, the color channel is ordered as py5 expects. That would actually work, but we want a solution that involves py5. In this case, we will pass an extra parameter to the create_image_from_numpy() method to instruct it to interpret the Color Channel Ordering with a different order (‘RGBA’). It looks better. The logo image on the left is correct. The black color in the logo image on the right is correct, but the rest are incorrect. We must still be using the wrong Color Channel Order for the image loaded by OpenCV. If we search around online we will discover that OpenCV reverses the Color Channel Order, ordering it ‘BGR’, or ‘BGRA’ for images with alpha channels. Let us try our Sketch again, this time using the Color Channel Order ‘BGRA’ for the OpenCV array. Now both logo images are drawn correctly. For the first two examples exploring Color Channel Ordering, py5 was interpreting the array’s Color Channel Ordering incorrectly. There were no exceptions or obvious errors other than the fact that the colors looked wrong to us. Keep in mind that if we were unfamiliar with what the py5 logo actually looks like, we might not have spotted the mistake at all. This suggests a useful best practice for writing code with image arrays: always test out your code with a familiar image. It will help you spot errors that might otherwise be overlooked.\n\nIf you are using py5 and need to create a shape with thousands of vertices, one approach for implementing this would involve a loop that creates each vertex, one at a time. Unfortunately, that py5 code would execute slowly. It would execute slowly because the method call for each individual vertex would involve a separate call to py5’s underlying Processing code. A better approach would involve omitting the loop and creating all of the vertices with a single command. This approach lets py5 create the vertices in the most efficient way possible. The performance difference between the two approaches can be significant. Let’s explore this difference with a simple example that draws a spiral using a loop. In this example, we create a loop that draws one vertex at a time. For each step, it calculates the angle in radians and the radius of the vertex from the spiral center. Then it uses trigonometry to calculate the x and y coordinates of the vertex. The problem here is that the vertex() method is called times. That’s a lot, and can cause performance problems for py5. It’s not that big of a deal here but this will cause problems if the number of vertices increases even more. Let’s look at a different approach that avoids the loop. # angles of all of the vertices in radians # radii of all of the vertices from the center # assemble x and y coordinates into one array This code is very similar to before, except that we are using numpy arrays instead of doing calculations on one number a time. We start by creating numpy arrays and that contain all of the vertex angles and all of the vertex radii. We do that with the numpy methods arange() and linspace(). Then we proceed much like the first example, calculating the x and y coordinates. The code looks the same, but because and are arrays, we are calculating the coordinates for all of the vertices at the same time. The last step is to assemble and into one numpy array using numpy’s stack() method. The result is the same but the code is faster and more efficient. Py5 provides many methods for bulk operations. Here is the complete list:"
    }
]