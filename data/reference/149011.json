[
    {
        "link": "https://hardysimpson.github.io/zlog/UsersGuide-EN.html",
        "document": "This parameter causes the zlog library to reload the configuration file automatically after a period, which is measured by number of log times per process. When the number reaches the value, it calls zlog_reload() internally. The number is reset to zero at the last zlog_reload() or zlog_init(). As zlog_reload() is atomic, if zlog_reload() fails, zlog still runs with the current configuration. So reloading automatically the configuration is safe. The default is 0, which means never reload automatically.\n\nThis specifies a lock file for rotating a log safely in multi-process situations. zlog will open the file at zlog_init() with the permission of read-write. The pseudo-code for rotating a log file is:\n\nmutex_lock is for multi-thread and fcntl_lock is for multi-process. fcntl_lock is the POSIX advisory record locking. See man 3 fcntl for details. The lock is system-wide, and when a process dies unexpectedly, the operating system releases all locks owned by the process. That’s why I chose fcntl lock for rotating log safely. The process needs read-write permisson for lock_file to lock it.\n\nIf you choose another path as lock file, for example, /tmp/zlog.lock, zlog will create it at zlog_init(). Make sure your program has permission to create and read-write the file. If processes run by different operating system users need to write and rotate the same log file, make sure that each program has permission to create and read-write the same lock file."
    },
    {
        "link": "https://github.com/HardySimpson/zlog/blob/master/doc/UsersGuide-EN.lyx",
        "document": ""
    },
    {
        "link": "https://github.com/HardySimpson/zlog",
        "document": "Actually, in the C world there was NO good logging library for applications like logback in java or log4cxx in c++. Using printf can work, but can not be redirected or reformatted easily. syslog is slow and is designed for system use. So I wrote zlog. It is faster, safer and more powerful than log4c. So it can be widely used.\n\nPREFIX indicates the installation destination for zlog. After installation, refresh your dynamic linker to make sure your program can find zlog library.\n\nBefore running a real program, make sure libzlog.so is in the directory where the system's dynamic lib loader can find it. The command metioned above are for linux. Other systems will need a similar set of actions.\n\nThere are 3 important concepts in zlog: categories, formats and rules.\n\nCategories specify different kinds of log entries. In the zlog source code, category is a variable. In your program, different categories for the log entries will distinguish them from each other.\n\nFormats describe log patterns, such as: with or without time stamp, source file, source line.\n\nRules consist of category, level, output file (or other channel) and format. In brief, if the category string in a rule in the configuration file equals the name of a category variable in the source, then they match. Still there is complex match range of category. Rule decouples variable conditions. For example, log4j must specify a level for each logger(or inherit from father logger). That's not convenient when each grade of logger has its own level for output(child logger output at the level of debug, when father logger output at the level of error)\n\nNow create a configuration file. The function zlog_init takes the files path as its only argument.\n\nIn the configuration file log messages in the category and a level of DEBUG or higher are output to standard output, with the format of simple . If you want to direct out to a file and limit the files maximum size, use this configuration\n• high-performance, 250'000 logs/second on my laptop, about 1000 times faster than syslog(3) with rsyslogd\n• self debuggable, can output zlog's self debug&error log at runtime\n• No external dependencies, just based on a POSIX system and a C99 compliant vsnprintf."
    },
    {
        "link": "https://betterstack.com/community/guides/logging/how-to-manage-log-files-with-logrotate-on-ubuntu-20-04",
        "document": "Controlling the sizes of log files on a Linux server is crucial due to their continuous growth. As log files accumulate, they can consume valuable storage space, strain server resources, and cause performance and memory issues.\n\nTo address this problem, log rotation is commonly employed. It involves renaming or compressing log files before they become too large, while also removing or archiving old logs to free up storage space.\n\nOn most Linux distributions, the preferred tool for log rotation is the logrotate program, which we will be focusing on in this tutorial.\n\nBefore proceeding with the rest of this tutorial, please ensure that you have:\n• A basic knowledge of working with the Linux command line.\n• A Linux server that includes the non-root user with access. We'll be using Ubuntu 22.04 throughout in this guide but everything should work even if you're on some other distribution.\n• Prior knowledge of how to work with system log files on Linux and Journalctl.\n\nSending your application logs to a file is the first step towards persisting them and making them available for historical analysis, auditing, and troubleshooting, although you'll likely want to aggregate them in the cloud to unlock the full potential of your log data.\n\nEven when you've adopted a log management service like Logtail, we generally don't recommend sending the logs to the service directly from the application code for a few reasons:\n• None If a network connection or logging endpoint becomes temporarily unavailable, the application has to attempt resource-intensive retry logic to resume log streaming, which could impact overall performance.\n• None If there's a persistent outage, the logs could be dropped and lost forever which could impact the troubleshooting process, ability to comply with regulations, or even conduct security investigations.\n\nTherefore, we recommend persisting your logs to a local file to provide some redundancy, then use a log forwarder like Vector to transmit them to their final destination. This approach has a few notable advantages:\n• None It decouples the log generation process from the log transmission process. This separation of concerns allows the application to focus on its core functionality without being concerned about the intricacies of log transmission. It also simplifies application development and maintenance, as you can rely on the log forwarder to handle the complexities of log delivery.\n• None Log forwarders can typically aggregate logs from multiple sources, such as different applications or servers, into a centralized location and this flexibility allows you to adapt your log management infrastructure as your needs evolve, without requiring changes to individual applications.\n• None Log forwarders can handle network disruptions, retries, and buffering of log data so that the log data is delivered reliably even in the event of an extended outage.\n• None They can support different log formats and protocols, making it easier to send logs to multiple destinations or perform transformations on the log data.\n\nOnce you've started persisting your logs to local files, you'll need to implement a process for keeping individual files from becoming too large, and also a way to remove or archive older logs that are no longer needed to free up disk space.\n\nWhen log files get too large, they become tedious to work with and searching for the records relevant to your current tasks can take a long time due to the large volume of records.\n\nTherefore, implementing log rotation to spread the log data over several files and to remove older items is a must. It involves renaming log files on a predefined schedule or when the file reaches a predefined size. Once the specified condition is met, the log file is renamed to preserve its contents and make way for a new file.\n\nTypically an auto incrementing number or timestamp is appended to the filename to indicate its time of rotation which is often helpful in narrowing down your search when investigating an issue that occurred on a specific date.\n\nAfter the file is renamed, a new log file with the same name is created to capture the latest entries from the application or service. A cleanup process is also initiated to prevent an accumulation of rotated log files as older logs beyond a specified retention period are removed. This process repeats indefinitely as long as the log rotation mechanism is working.\n\nThe daemon is pre-installed and active by default in Ubuntu and most mainstream Linux distributions. If Logrotate is not installed on your machine, ensure to install it first through your distribution's package manager.\n\nThe Logrotate daemon uses configuration files to specify all the log rotation details for an application. The default setup consists of the following aspects:\n• : this is the main configuration file for the Logrotate utility. It defines the global settings and defaults for log rotation that are applied to all log files unless overridden by individual Logrotate configuration files in the directory.\n• : this directory includes files that configure log rotation policies specific to the log files produced by a individual applications or services.\n\nWe will examine both configuration possibilities below.\n\nFirst off, let's view the main Logrotate configuration file at . Go ahead and print its contents with the utility:\n\nThe command above prints the entire contents of this file:\n\nHere's a description of what each of the above configuration directives mean (lines that begin with indicate a comment):\n• : represents the frequency of log rotation. Alternatively, you can specify another time interval ( , , , or ). Since the utility is typical run once per day, you may need to change this configuration if a if a shorter rotation frequency than is desired (see below).\n• : log rotation is performed with the root user and admin group. By using this directive, you can ensure that the rotated log files are owned by a specific user and group, which can be useful for access control and permissions management. This is particularly relevant when the log files need to be accessed or managed by a specific user or group with appropriate privileges.\n• : log files are rotated four times before old ones are removed. If is set to zero, then the old versions are removed immediately and not rotated. If it is set to , the older logs will not be remove at all except if affected by .\n• : immediately after rotation, create a new log file with the same name as the one just rotated.\n• : if this option is enabled, rotated log files will be renamed by appending a date to their filenames, allowing for better organization and differentiation of log files based on the date of rotation (especially when the frequency of rotation is or greater). The default scheme for rotated files is , , and so on, but enabling this option changes it to . You can change the date format through the and directives.\n• : this rule determines whether old log files should be compressed (using gzip by default) or not. Log compression is turned off by default but you can enable it to save on disk space.\n• : this directive is used to include additional configuration files or snippets. It allows you to modularize and organize your Logrotate configuration by splitting it into multiple files. In this case, the files in the directory have been included in the configuration.\n\nAs noted earlier, the file serves as a global configuration file for Logrotate, providing default settings and options for log rotation across the system. It sets the stage for log rotation but can be extended or overridden by the configuration files in the directory which typically configure the rotation policy for specific application logs.\n\nNext, let's view the contents of the directory. It typically contains additional Logrotate configuration files for various applications or services installed on your machine:\n\nYou will observe that quite a few programs have their log rotation configuration in this directory. Each configuration file within focuses on a particular application or log file set, specifying the log file path, rotation frequency, compression settings, and any additional directives necessary for managing the logs of that specific application or service.\n\nHaving separate configuration files in this directory allows for easy customization and maintenance of log rotation settings for individual applications or services without affecting other log files. For example, let's take a look at the config file for the Rsyslog utility through the command:\n\nYou'll see the program's output appear on the screen:\n\nThe above configuration specifies the rotation rules for several log files located in the directory. It also includes the following directives in addition to the ones we examined in the previous section:\n• : continue log rotation without reporting any error if any of the specified log files are missing.\n• : ensures that log files are not rotated if they are empty. If a log file is empty, it won't trigger rotation.\n• : delays compression of the rotated log files until the next rotation cycle. This allows for the previous log file to be available for analysis before compression.\n• : ensures that the commands or scripts specified in the or directive are executed only once, regardless of the number of log files being rotated. By default, executes the commands/scripts separately for each log file being rotated.\n• and : encloses the commands or scripts to be executed after log rotation. In this case, the script is executed after a successful rotation. It sends the signal to the Rsyslog service so that it can close and reopen the log file for writing.\n\nOverall, this configuration ensures that the specified log files are rotated weekly, compressed, and limited to a maximum of rotated log files. It also includes additional directives for handling missing or empty log files and executes a post-rotation script specific to Rsyslog.\n\nOther useful directives to note include:\n• : specifies the maximum size in bytes, kilobytes, megabytes, or gigabytes that a log file can reach before rotation is initiated. This causes the default schedule to be ignored if as long as is specified after the time directive ( , , etc).\n\nIn this example, Logrotate will trigger rotation when myapp.log reaches 10 megabytes in size. Once the size threshold is crossed, rotation will be initiated regardless of the time schedule ( in this case).\n• : the log files are rotated according to the specified time schedule, but not before the specified size is reached. Therefore, when is used, both file size and timestamp are considered to determine if the file should be rotated.\n\nWhen using , rotation will not occur until the file reaches a minimum of 10 megabytes even if the daily schedule is met.\n• : specifies that the log files are rotated once they exceed the stated file size, even when the time interval has not yet been reached.\n\nIn this snippet, rotation will occur when a size of 10 megabytes is reached. Otherwise, it will rotate weekly.\n\nLogrotate offers two directives that specify how the log rotation should be handled: and . The former is the default, and its works by renaming a log file (say ) to , before creating a new file will be created to continue logging.\n\nIn mode, the file is copied to a new file, then the original file is emptied (truncated), allowing the application to continue writing to it as if it were a new file. This mode is useful if your application or process does not handle log file rotation gracefully by automatically switching to the new log file after rotation.\n\nIt's worth noting that while avoids interrupting the logging process, it may cause a brief period of log loss during the rotation process since the original file is truncated. However, this is usually acceptable for applications that don't rely on continuous log analysis and can tolerate occasional gaps in the logs.\n\nSo far, we've seen how Logrotate can be used to manage the log files for system services and pre-installed utilities on your Linux server. Now, let's look at how to do the same thing for custom applications or services that you've deployed to the server.\n\nTo simulate an application that writes logs continuously to a file, create the following bash script somewhere on your filesystem. It writes fictional but realistic-looking log records to a file every second:\n\nSave the file, then make it executable:\n\nAfterward, create the directory using elevated privileges, then change the ownership of the directory to your user so that the script can write files to the directory:\n\nYou can now execute the script and it should begin to write the logs to the file every second:\n\nAt this stage, you must set up a log rotation policy to prevent the file from growing too large and taking up valuable disk space on the server. There are two main options for doing this:\n• None Create a new Logrotate configuration file and place it in the directory to perform log rotation according to the system's default schedule (it runs once per day by default but you can change it.\n• None Create the configuration file that is independent of the system's Logrotate schedule and execute Logrotate at your preferred pace using through a cronjob.\n\nIn this section, you will create a standard configuration file for your application logs and place it in the directory. Go ahead and create a new file in the directory with your text editor:\n\nAdd the following text to the file:\n\nThe configuration above applies to all the files ending with in the directory. We've already discussed what each directive here does earlier, so we won't go over that again here.\n\nSave the file and test the new configuration by executing the command below. The option instructs to operate in test mode where only debug messages are printed.\n\nYou should spot an entry for the configuration that looks similar to what is displayed below:\n\nThe above output indicates that the configuration file at has been found by the program. Therefore, the log files specified therein will now be rotated according to the defined policy along with the other system and application logs.\n\nIf you want to test that the log rotation works without without waiting for the specified schedule, you can use the option like this:\n\nYou will observe that the old log file was renamed and compressed and a new one was created:\n\nAnother way to verify if a particular log file is rotating or not, and to check the last date and time of its rotation, examine the file (or on Red Hat systems) like this:\n\nAs mentioned earlier, a system-independent Logrotate configuration is one that is not run on the default system schedule. Such a configuration will not be included in the directory. Instead, you place the file in some other directory and create a cron job that will execute Logrotate with the configuration file at custom time interval.\n\nChange into your home directory, and create a directory therein:\n\nNext, edit your script and change the variable to the following:\n\nAfterward, rerun the script so that the logs are now written to the directory:\n\nTo create a system-independent Logrotate configuration for these logs, you must create your configuration file outside of . Therefore, go ahead and create a file within the directory:\n\nPopulate the file with the following contents:\n\nThis configuration is the same as in the previous section, except that has been changed to so that the log files are rotated every hour instead of once per day.\n\nYou also need to create a Logrotate state file which stores information such as the last rotation date and time, the number of rotations performed, and other relevant details. This allows Logrotate to accurately perform rotations and prevent unnecessary rotations when they are not required.\n\nIn the default Logrotate setup, the state file is located in the directory. However, we will create a custom one through the command below:\n\nThe option tells to use an alternative state file located at . The command will create this file if it doesn't already exist, and you can view its content with :\n\nYou'll see the program's output appear on the screen:\n\nThe output indicates that Logrotate identified the relevant log file and when it last considered them for rotation. The next step here is to set up a cron job to execute the file at your desired frequency (hourly in this case).\n\nGo ahead and open the cron jobs configuration file by executing in your terminal:\n\nThe option is used to edit the current user's cron jobs using the editor specified by the or environmental variables. The above command should open a configuration file in your preferred text editor specified by one of these variables.\n\nAt the bottom of the file, add the following line:\n\nThis new line specifies that the cron job will be executed every hour (at minute 0), and the command will run with your custom configuration and state file. The full path of the binary is used here just to be safe.\n\nSave and close the modified file. You will observe the following output:\n\nNow that your log rotation policy is all set up, you can view the directory after an hour to confirm that the log file therein are rotated according to the defined policy. For more details about cron jobs see the following tutorial or type in your terminal.\n\nAs mentioned earlier, when using the default system configuration, Logrotate only runs once per day which means using the option in a configuration will be ineffective. However, you can modify this behaviour by changing the location of the script that runs Logrotate. On Ubuntu, its located at which indicates that the script is run once per day by the system's daily cronjob. If you want to change the schedule to , move the script to the directory using the command below:c\n\nAfterward, the script should be executed by the system's hourly cronjob so that the option works normally henceforth.\n\nRunning commands or scripts before or after log rotation\n\nLogrotate provides the ability to run arbitrary commands or scripts before and after log rotation through the and directives. As their names implies, the former executes commands or scripts before log rotation while the latter does the same thing after log rotation. Both directives are closed using the directive.\n\nYou can use to perform any necessary preparations or actions required prior to the rotation, while should be used to perform tasks such as restarting services, notifying stakeholders, or further processing of the rotated log files.\n\nFor example, you can monitor your log rotation configuration by pinging a monitoring service like Better Uptime so that if the rotation does not execute as scheduled, you'll get an alert to investigate the problem further.\n\nIn this example, is used to report that the log rotation was successful according to the defined schedule. If this report is not received within the expected period, an incident will be created and you will receive notifications at the configured channels (such as Email, Slack, SMS, etc). Its always a good idea to set up such monitoring so that if there's an issue with the rotation, you catch and fix it quickly before it causes more severe problems.\n\nNote that commands or scripts are only executed when at least one file that matches the specified pattern was rotated. The directive above is used to specify that the commands in and blocks should be run only once no matter how many log files were rotated. Normally, the commands are run once per rotated log file which is not ideal in this scenario.\n\nIf or commands or scripts are not executing as expected, ensure that they have the correct permissions and are executable. You can use the command to make the scripts or binaries executable where applicable. Additionally, double-check that the paths to the scripts are accurate and that any dependencies required by the scripts are installed.\n\nAs seen earlier in , Logrotate performs its duties with the privileges of the user and the group. This allows the tool to perform the log rotation operation with elevated permissions, typically required to access and manage system logs.\n\nThis also means that newly created log files by the tool will be owned by the user and group, but this may sometimes prevent the application or service producing the logs from being able to access the file. In such situations, you need to modify your settings to ensure that the right access permissions are set on the file.\n\nHence, the directive provides a few additional options:\n\nIn this example, when Logrotate creates a new log file ( ) after rotation, it will set the file permissions to 644 (read-write for the owner, and read-only for the group and others). The file will be owned by and assigned to .\n\nYou need to ensure that the Logrotate utility is running correctly at all times so that your scheduled log rotation tasks are executed as expected. If log files are not rotating as expected, it could be due to incorrect configuration or permissions issues.\n\nTo fix such problems, first check the Logrotate status file at to ensure that the log file is indeed included in the rotation schedule and to confirm when it was last rotated.\n\nIf a pattern that matches the log file is not included here, you may need to verify if a corresponding configuration file for the application or service is present in the directory.\n\nThe command also provides a helpful option to test and debug configuration issues by simulating log rotation without actually rotating the logs. For example, if you notice that the rotated logs are not being compressed and you run in debug mode, you may observe the following output indicating that the directive was misspelled:\n\nAnother useful option is which provides detailed output and information about the log rotation process. When enabled, Logrotate displays additional messages, including the files being rotated, the actions taken, and any errors or warnings encountered during the rotation.\n\nIf you're running through a cronjob, you can specify the option and redirect its standard output and standard error to a file using the syntax below:\n\nFor the system cronjob, you must edit the script that's located in by default. Note that when enabling verbose mode here, it'll include information about all logs being rotated on the system which can be pretty huge and mostly irrelevant. We recommend using the cron method shown above if you only care about the logs for a specific service or application.\n\nThe next time Logrotate executes, the file will be created in the specified directory and you'll find all the details of the log rotation. Here's some example output from a successful rotation attempt:\n\nOnce you're collecting Logrotate logs as above, you can forward them to Logtail so that you can easily search for key events and receive alerts when an error is encountered.\n\nIn this tutorial, we explored log rotation in Linux and its implementation using the Logrotate program. We began by examining the configuration files and discussing key directives commonly encountered. We then created a standard Logrotate configuration for a custom application and then transitioned to a system-independent configuration, before discussing some common problems with Logrotate and how to troubleshoot them effectively.\n\nTo further expand your knowledge of Logrotate and explore its full capabilities, I encourage you to consult its manual page. Simply run in your terminal to access the comprehensive documentation.\n\nThanks for reading, and happy logging!"
    },
    {
        "link": "https://technolinchpin.wordpress.com/2016/01/13/choosing-data-loggers",
        "document": "The loggers play an important role in system development helping out the developers to collect the traces to analyzes program flow and data elements. Though sometimes its still fine to rely on printf & family of APIs for trivial logging solution , it needs a lot of thought and effort to create a logging service infrastructure performance optimized , support configuration to allow flexibility in formatting and provisions to manage the logs in an efficient way. Pumping information out to scattered printfs serve as logger services and spitting them in a haywire manner can be of real performance impacts. There is where a well structured logger service implementation stands out!\n\nFor the system level logger , the journal services with systemd and syslog/sylog-ng loggers can be seen as best available options . However the way these daemons manage the centralized logging services may not be always appropriate in cases where the application needs support of creating out of the box logs customized with a context info and want to save the logs associated with these contexts to analyze data /event flow pattern , sequence of state /actions or consolidate error statistics etc in their preferred format.\n\nMy search was not really promising initially and I could find anything beyond syslog and systemd-journal . But had to explore until I located log4c and zlog. The log4c really stand out in its capabilities though , I personally felt it lacks enough flexibility while managing the log storage mechanism when compared to zlog which is a must to feature I was looking for , as the environment I want to use zlog is contained in storage space and hence I really need an implementation properly take care of identifying ways to manage the storage , like with the support to configure the permitted number of log file entries saved , size limt of individual log files and with provisions for managing log rotation and flexibility in configuring the log buffer size limits and their sync options (when and how often to sync the log data with the saved content in disc) , this configuration can have a real performance impact!.\n\nIn that case it looks to me zlog is a reliable, high efficient, thread safe, flexible, clear model, pure c logging library with easy to understand configuration rules.\n\nCategory specifies different kinds of log entries. In the zlog source code, category is a (zlog_cateogory_t *) variable. In your program, different categories for the log entries will distinguish them from each other.\n\nzlog support multiple formats that describes detail log patterns, such as: with or without time stamp, source file, source line.\n\nRule consists of category, level, output file (or other channel) and format. In brief, if the category string in a rule in the configuration file equals the name of a category variable in the source, then they match.\n• multiple output, include static file path, dynamic file path, stdout, stderr, syslog, user-defined output . And I was looking for storing them in files.\n• user-defined log level, at the cost of O(1)\n• safely rotate log file on multiple-process or multiple-threads condition very important to have and zlog is thread safe.\n• dzlog, a default category log API for easy use , application need not define any category leave to zlog to use the default category makes the logging format really simple!.\n• self debuggable, can output zlog’s self debug&error log at runtime\n• Not depend on any other 3rd party library, just base on POSIX system(and a C99 compliant vsnprintf).\n\n[1] : zlog Cross compile and dynamically link to application (shared under LGPL v2 license clause) :"
    },
    {
        "link": "https://hardysimpson.github.io/zlog/UsersGuide-EN.html",
        "document": "This parameter causes the zlog library to reload the configuration file automatically after a period, which is measured by number of log times per process. When the number reaches the value, it calls zlog_reload() internally. The number is reset to zero at the last zlog_reload() or zlog_init(). As zlog_reload() is atomic, if zlog_reload() fails, zlog still runs with the current configuration. So reloading automatically the configuration is safe. The default is 0, which means never reload automatically.\n\nThis specifies a lock file for rotating a log safely in multi-process situations. zlog will open the file at zlog_init() with the permission of read-write. The pseudo-code for rotating a log file is:\n\nmutex_lock is for multi-thread and fcntl_lock is for multi-process. fcntl_lock is the POSIX advisory record locking. See man 3 fcntl for details. The lock is system-wide, and when a process dies unexpectedly, the operating system releases all locks owned by the process. That’s why I chose fcntl lock for rotating log safely. The process needs read-write permisson for lock_file to lock it.\n\nIf you choose another path as lock file, for example, /tmp/zlog.lock, zlog will create it at zlog_init(). Make sure your program has permission to create and read-write the file. If processes run by different operating system users need to write and rotate the same log file, make sure that each program has permission to create and read-write the same lock file."
    },
    {
        "link": "https://stackoverflow.com/questions/10973362/python-logging-function-name-file-name-line-number-using-a-single-file",
        "document": "You have a few marginally related questions here.\n\nI'll start with the easiest: (3). Using you can aggregate all calls to a single log file or other output target: they will be in the order they occurred in the process.\n\nNext up: (2). provides a dict of the current scope. Thus, in a method that has no other arguments, you have in scope, which contains a reference to the current instance. The trick being used that is stumping you is the string formatting using a dict as the RHS of the operator. will be replaced by whatever the value of is.\n\nFinally, you can use some introspection tricks, similar to those used by that can log more info:\n\nThis will log the message passed in, plus the (original) function name, the filename in which the definition appears, and the line in that file. Have a look at inspect - Inspect live objects for more details.\n\nAs I mentioned in my comment earlier, you can also drop into a interactive debugging prompt at any time by inserting the line in, and re-running your program. This enables you to step through the code, inspecting data as you choose."
    },
    {
        "link": "https://github.com/Cysharp/ZLogger/blob/master/README.md",
        "document": "Zero Allocation Text/Structured Logger for .NET and Unity, with StringInterpolation and Source Generator, built on top of a .\n\nThe usual destinations for log output are , , , all in UTF8 format. However, since typical logging architectures are based on Strings (UTF16), this requires additional encoding costs. In ZLogger, we utilize the String Interpolation Improvement of C# 10 and by leveraging .NET 8's IUtf8SpanFormattable, we have managed to avoid the boxing of values and maintain high performance by consistently outputting directly in UTF8 from input to output.\n\nZLogger is built directly on top of . is an official log abstraction used in many frameworks, such as ASP.NET Core and Generic Host. However, since regular loggers have their own systems, a bridge is required to connect these systems, and this is where a lot of overhead can be observed. ZLogger eliminates the need for this bridge, thereby completely avoiding overhead.\n\nThis benchmark is for writing to a file, but the default settings of typical loggers are very slow. This is because they flush after every write. In the benchmark, to ensure fairness, careful attention was paid to set the options in each logger for maximum speed. ZLogger is designed to be the fastest by default, so there is no need to worry about any settings.\n\nThe slowness of this default setting is due to I/O, so it can be mitigated by using a faster drive. When taking benchmarks, please note that the results can vary greatly not only on your local (which is probably fast!) but also on drives attached to the cloud and in environments like Docker. One of the good points about the async-buffered setting is that it can reduce the impact of such I/O issues.\n\nZLogger focuses on the new syntax of C#, and fully adopts Interpolated Strings.\n\nThis allows for providing parameters to logs in the most convenient form. Also, by closely integrating with System.Text.Json's Utf8JsonWriter, it not only enables high-performance output of text logs but also makes it possible to efficiently output structured logs.\n\nZLogger also emphasizes console output, which is crucial in cloud-native applications. By default, it outputs with performance that can withstand destinations in cloud log management. Of course, it supports both text logs and structured logs.\n\nZLogger delivers its best performance with .NET 8 and above, but it is designed to maintain consistent performance with .NET Standard 2.0 and .NET 6 through a fallback to its own IUtf8SpanFormattable.\n\nAs for standard logger features, it supports loading LogLevel from json, filtering by category, and scopes, as found in Microsoft.Extensions.Logging. In terms of output destinations, it is equipped with sufficient capabilities for , , , , , and an for sending logs over HTTP and similar protocols.\n\nThis library is distributed via NuGet, supporting , , and or above. For Unity, the requirements and installation process are completely different. See the Unity section for details.\n\nHere is the most simple sample on ASP.NET Core.\n\nYou can get logger from dependency injection.\n\nThis simple logger setup is possible because it is integrated with by default. For reference, here's how you would set it up using LoggerFactory:\n\nNormally, you don't create LoggerFactory yourself. Instead, you set up a Generic Host and receive ILogger through dependency injection (DI). You can setup logger by .NET Generic Host(for ASP.NET Core) and if you want to use this in ConsoleApplication, we provides ConsoleAppFramework to use hosting abstraction.\n\nHere is the showcase of providers.\n\nLook at the use of loggers and the syntax of ZLog.\n\nAll standard methods are processed as strings by ZLogger's Provider. However, by using our unique methods, you can process them at high performance while remaining in UTF8. Additionally, these methods support both text logs and structured logs using String Interpolation syntax.\n\nAll logging methods are completely similar as Microsoft.Extensions.Logging.LoggerExtensions, but it has Z prefix overload.\n\nThe ZLog* method uses InterpolatedStringHandler in .NET and prepare the template at compile time.\n\nSome special custom formats are also supported. The can be used when you want to explicitly give the structured log a name other than the name of the variable to capture. can be used to log the result of JsonSerializing an object.\n\nThe parameter name specification and format string can be used together.\n\nBy adding Providers, you can configure where the logs are output. ZLogger has the following providers.\n\nAll Providers can take an Action that sets as the last argument. As follows.\n\nIf you are using , you can set the log level through configuration. In this case, alias of Provider can be used. for example:\n\nEach Provider's behavior can be modified using the common . For details, please refer to the ZLoggerOptions section. Additionally, you can customize structured logging (JSON Logging) using the method within these options. For more information on this, check the Formatter Configurations section.\n\nConsole writes to the standard output. Console output is not only for development purposes, but also serves as a standard log input port in containerized and cloud environments, making performance critically important. ZLogger has been optimized to maximize console output performance.\n\nIf you are using , the following additional options are available:\n\nFile outputs text logs to a file. This is a Provider that writes to a single file in append mode at high speed.\n\nIf you are using , the following additional options are available:\n\nRollingFile is a Provider that dynamically changes the output file based on certain conditions.\n\nIf you are using , the following additional options are available:\n\nStream can output logs to any arbitrary Stream. For example, if you output to a MemoryStream, you can retrieve the rendered results in memory. If you pass a NetworkStream such as TCP, it will write logs over the network.\n\nInMemory allows you to retrieve rendered strings as they are generated. It can be conveniently used for purposes such as accumulating logs in a or for display on screen.\n\nIf you are using , the following additional options are available:\n\nLogProcessor is the most primitive Provider that allows you to customize output on a per-log basis ( ) by implementing a custom .\n\nFor example, a LogProcessor that propagates logs as string events can be written as follows:\n\nNote that is pooled, so you must always call .\n\nHere's a more complex example. can batch logs together, which is useful for scenarios like sending multiple log lines via HTTP in a single request.\n\nIn this case, the LogEntry is NonReturnable, so there's no need to call Return().\n\nBoth PlainText and JSON can be customized in addition to the standard log formats.\n\nYou can set Prefix and Suffix individually for text output. For performance reasons, the first argument is a special String Interpolation Template, which is formatted by the lambda expression in the second argument. For properties that can actually be retrieved with , refer to LogInfo. It is also possible to retrieve the log file path and line number from LogInfo.\n\nOnly LogLevel supports a special format specification. By passing , you can get a 3-character log level notation such as , , , , , , (the length of the beginning matches, making it easier to read when opened in an editor). For Timestamp, there are (local, local-longdate, longdate are same, there are alias), , , , , , , . Default is .\n\nSetExceptionFormatter allows you to customize the display when outputting exceptions. This can be easily converted to a string using .\n\nYou can flexibly change the JSON output format by modifying the JsonFormatter options. For example, if you set to only , you will get only the payload JSON. By default, the payload part is output directly without nesting, but if you set , you can output the payload JSON to a nested location. It is also possible to add values for arbitrary JSON Objects using .\n\nThe following is an example of customization to conform to the Google Cloud Logging format. We have also changed standard key names such as Timestamp.\n\nThe list of properties is as follows.\n\nBy default, JSON key names are output as is, so in the following character output, \"user.Name\" becomes the JSON key name.\n\nIf you set this to , it becomes . If you set this to , the first character is replaced with lower-case, resulting in .\n\nThe following is a list of KeyNameMutators provided as standard:\n\nWe also support structured logging output in binary format using MessagePack instead of JSON. requires a reference to the additional package .\n\nThe list of properties is as follows.\n\nIf you want to create a formatter other than the default PlainText, Json, and MessagePack, you can implement to create any custom output.\n\nAdditional information about when each log was written can be obtained from this LogInfo struct.\n\nThe following are common option items for all providers.\n\nBy default, is set. Also, only one formatter can be set for one provider. If you want to use multiple formatters, you need to add multiple providers.\n\nA log method generator similar to .NET 6's Compile-time logging source generation is bundled as standard.\n\nThis can achieve the highest performance. It's also possible to use special format specifiers like .\n\nMicrosoft.CodeAnalysis.BannedApiAnalyzers is an interesting analyzer, you can prohibit the normal Log method and induce the user to call ZLogger's ZLog method.\n\nAll you have to do is prepare the following configuration.\n\nLike the traditional log manager, how to get and store logger per type without DI(such as ). You can get from before Run and set to the global static loggerfactory store.\n\nYou can use this logger manager like following.\n\nZLogger uses some of the compile time features of C# 10, and ZLogger.Generator uses some of the features of C# 11.\n\nTo use them in Unity, needs to check the Unity version and set up the compiler.\n• Unity 2022.2 or newer\n• Unity internally embeds the .NET SDK 6. So C# 10 is available via compiler arguments.\n• Unity 2022.3.12f1 or newer\n• Unity internaly update .NET SDK 6. So C# 11 features are in preview.\n• Install NuGetForUnity\n• Required to install the dlls of ZLogger and its dependencies.\n• Install CsprojModifier\n• Required to develop in the IDE with a new language version.\n• Setup the C# compiler for unity.\n• Add a text file named with the following contents under your Assets/.\n• Note:\n• If you are using assembly definition, put it in the same folder as the asmdef that references ZLogger.\n• If you are using Unity 2022.3.12f1 or newer, you can use allows parts of C# 11 features.\n• Setup the C# compiler for your IDE.\n• Add a text file named LangVersion.props with the following contents\n• Open Project Settings and [C# Project Modifier] section under the [Editor].\n• Add the .props file you just created, to the list of [Additional project imports].\n• Note:\n• If you are using assembly definition, add your additional csproj in the list of [The project to be addef for import].\n• If you want to use Source Generator, require Unity 2022.3.12f1 and change to\n\nThe basic functions of ZLogger are also available in Unity as follows. Use LoggerFactory directly to create loggers.\n\nThis library is licensed under the MIT License."
    },
    {
        "link": "https://github.com/HardySimpson/zlog/blob/master/README.md",
        "document": "Actually, in the C world there was NO good logging library for applications like logback in java or log4cxx in c++. Using printf can work, but can not be redirected or reformatted easily. syslog is slow and is designed for system use. So I wrote zlog. It is faster, safer and more powerful than log4c. So it can be widely used.\n\nPREFIX indicates the installation destination for zlog. After installation, refresh your dynamic linker to make sure your program can find zlog library.\n\nBefore running a real program, make sure libzlog.so is in the directory where the system's dynamic lib loader can find it. The command metioned above are for linux. Other systems will need a similar set of actions.\n\nThere are 3 important concepts in zlog: categories, formats and rules.\n\nCategories specify different kinds of log entries. In the zlog source code, category is a variable. In your program, different categories for the log entries will distinguish them from each other.\n\nFormats describe log patterns, such as: with or without time stamp, source file, source line.\n\nRules consist of category, level, output file (or other channel) and format. In brief, if the category string in a rule in the configuration file equals the name of a category variable in the source, then they match. Still there is complex match range of category. Rule decouples variable conditions. For example, log4j must specify a level for each logger(or inherit from father logger). That's not convenient when each grade of logger has its own level for output(child logger output at the level of debug, when father logger output at the level of error)\n\nNow create a configuration file. The function zlog_init takes the files path as its only argument.\n\nIn the configuration file log messages in the category and a level of DEBUG or higher are output to standard output, with the format of simple . If you want to direct out to a file and limit the files maximum size, use this configuration\n• high-performance, 250'000 logs/second on my laptop, about 1000 times faster than syslog(3) with rsyslogd\n• self debuggable, can output zlog's self debug&error log at runtime\n• No external dependencies, just based on a POSIX system and a C99 compliant vsnprintf."
    },
    {
        "link": "https://stackoverflow.com/questions/38665440/custom-logger-with-time-stamp-in-python",
        "document": "I have lots of code on a project with print statements and wanted to make a quick a dirty logger of these print statements and decided to go the custom route. I managed to put together a logger that prints both to the terminal and to a file (with the help of this site), but now I want to add a simple time stamp to each statement and I am running into a weird issue.\n\nHere is my logging class.\n\nNotice the stamp method that I then attempt to use in the write method.\n\nWhen running the following two lines I get an unexpected output:\n\nThis what the output also looks in the log file, however, I see no reason why the string that I am adding appends to the end. Can someone help me here?\n\nUPDATE See answer below. However, for quicker reference the issue is using \"print()\" in general; replace it with sys.stdout.write after assigning the variable.\n\nAlso use \"logging\" for long-term/larger projects right off the bat."
    }
]