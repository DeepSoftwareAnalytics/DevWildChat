[
    {
        "link": "https://ieeexplore.ieee.org/document/6306148",
        "document": ""
    },
    {
        "link": "http://azadproject.ir/wp-content/uploads/2013/07/Fuzzy-adaptive-PID-control.pdf",
        "document": ""
    },
    {
        "link": "https://ojs.imeti.org/index.php/AITI/article/view/6047",
        "document": "T. Hagiwara, I. Murakami, T. Sakanushi, K. Yamada, and Y. Ando, A Design Method of Robust Stabilizing Modified PID Conterollers for Multiple-Input∕Multiple-Output Plants, vol. 5. Three Park Avenue, New York, NY 10016-5990: ASME, 2009.\n\nK. J. Åström and R. M. Murray, Feedback Systems: An Introduction for Scientists and Engineers, Princeton University Press, 2008.\n\nK. H Ang, G. Chong, and L. Yun, “PID Control System Analysis, Design, and Technology,” IEEE Transactions on Control Systems Technology, vol. 13, no. 4, pp. 559-576, July 2005.\n\nK. J. Åström and T. Hägglund, “The Future of PID Control,” Control Engineering Practice, vol. 9, no. 11, pp. 1163-1175, November 2001.\n\nW. J. Rugh and J. S. Shamma, “Research on Gain Scheduling,” Automatica, vol. 36, no. 10, pp. 1401-1425, October 2000.\n\nJ. S. Shamma, “Analysis and Design of Gain Scheduled Control Systems,” Department of Mechanical Engineering, Massachusetts Institute of Technology, Technical Report LIDS-TH-1770, May 1988.\n\nJ. S. Shamma and M. Athans, “Guaranteed Properties of Gain Scheduled Control for Linear Parameter-Varying Plants,” Automatica, vol. 27, no. 3, pp. 559-564, May 1991.\n\nZ. Yu, J. Z. Cao, H. T. Yang, H. N. Guo, B. Gao, and L. Yang, “Advanced Fuzzy PID Composite Control for Stabilized Platform System,” 2012 IEEE International Conference on Mechatronics and Automation, August 2012.\n\nA. A. Roshdy, Y. Z. Lin, C. Su, H. F. Mokbel, and T. Wang, “Design and Performance of Nonlinear Fuzzy Logic PI Controller for Line of Sight Stabilized Platform,” International Conference on Optoelectronics and Microelectronics, August 2012.\n\nM. Yang, X. H. Gao, X. J. Zhang, and C. Wang, “Simulation Analysis of Multi-Axle Vehicle's Turning Braking Stability Based on Fuzzy Control Theory,” The 2nd International Conference on Industrial Mechatronics and Automation, May 2010.\n\nH. Zhou, W. Chen, and K. Fang, “A Fuzzy Control Algorithm for Collecting Main Pressure Controlling Using Expert Control,” International Conference on Intelligent Computation Technology and Automation, May 2010.\n\nG. Chen, “Conventional and Fuzzy PID Controllers: An Overview,” International Journal of Intelligent and Control Systems, vol. 1, no. 2, pp. 235-246, 1996.\n\nL. Luoh, “Control Design of T-S Fuzzy Large-Scale Systems,” International Journal of Innovative Computing, Information and Control, vol. 5, pp. 2869-2880, September 2009.\n\nM. Wang, B. Chen, and S. Tong, “Adaptive Fuzzy Tracking Control for Strict-Feedback Nonlinear Systems With Unknown Time Delays,” International Journal of Innovative Computing, Information and Control, vol. 4, pp. 829-837, April 2008.\n\nM. Short and F. Abugchem, “On the Jitter Sensitivity of an Adaptive Digital Controller: A Computational Simulation Study,” International Journal of Engineering and Technology Innovation, vol. 9, no. 4, pp. 241-256, September 2019.\n\nJ. L. Peczkowski and M. K. Sain, “Design of Nonlinear Multivariable Feedback Controls by Total Synthesis,” American Control Conference, IEEE, July 1984, pp. 688-697.\n\nV. Aparna, M. Hussain K, D. N. Jamal, and M. S. M. Shajahan, “Implementation of Gain Scheduling Multiloop PI Controller Using Optimization Algorithms for a Dual Interacting Conical Tank Process,” 2nd International Conference on Trends in Electronics and Informatics, May 2018.\n\nX. Zhou, L. Li, Y. Jia, and T. Cai, “Adaptive Fuzzy/Proportion Integration Differentiation (PID) Compound Control for Unbalance Torque Disturbance Rejection of Aerial Inertially Stabilized Platform,” International Journal of Advanced Robotic Systems, vol. 13, no. 5, pp. 1-11, October 2016.\n\nC. Fuyan, Z. Guomin, L. Youshan, and X. Zhengming, “Fuzzy Control of a Double-Inverted Pendulum,” Fuzzy Sets and Systems, vol. 79, no. 3, pp. 315-321, May 1996.\n\nS. R. Mahapatro, B. Subudhi, and S. Ghosh, “Adaptive Fuzzy PI Controller Design for Coupled Tank System: An Experimental Validation,” IFAC Proceedings Volumes, vol. 47, no. 1, pp. 878-881, 2014.\n\nS. R. Mahapatro, B. Subudhi, and S. Ghosh, “Design and Real‐Time Implementation of an Adaptive Fuzzy Sliding Mode Controller for a Coupled Tank System,” International Journal of Numerical Modelling: Electronic Networks, Devices and Fields, vol. 32, no. 1, p. e2485, September 2018.\n\nG. M. Tamilselvan and P. Aarthy, “Online Tuning of Fuzzy Logic Controller Using Kalman Algorithm for Conical Tank System,” Journal of Applied Research and Technology, vol. 15, no. 5, pp. 492-503, October 2017.\n\nG. Sakthivel, T. S. Anandhi, and S. P. Natarajan, “Design of Fuzzy Logic Controller for a Spherical Tank System and Its Real Time Implementation,” International Journal of Engineering Research and Applications, vol. 1, no. 3, 2011.\n\nG. Stephanopoulos, “Chemical Process Control: An Introduction to Theory and Practice,” Automatica, vol.21, no. 4, pp. 502-504, July 1985.\n\nJ. M. Mendel, “Fuzzy Logic Systems for Engineering: A Tutorial,” Proceedings of the IEEE, vol. 83, no. 3, pp. 345-377, March 1995."
    },
    {
        "link": "https://researchgate.net/publication/349955648_Design_and_Implementation_of_Adaptive_PID_and_Adaptive_Fuzzy_Controllers_for_a_Level_Process_Station",
        "document": "Design and Implementation of Adaptive PID and Adaptive Fuzzy This proposed work prop oses the design and real-time i mplementation of an adapti ve fuzzy logic controller (FLC) and a proportional-integral-derivative (PID) controller for adaptive gain scheduling that can be configured for any comple x industria l nonlinea r app lication. I nitially, the open-loop test of the single-input single-output (SISO) system, with n onlinearities and disturbances, is conducted to represent the m athematical model of the process around a set of equilibrium points. The adaptive controllers are then developed and d eployed by using the national parameters are ad apted in real-time correspo nding to the changes in the process variable. The resulting servo and regulatory per formance o f the controlle rs are compared in MATLAB® software. The adaptive fuzzy controller is deduced to be the better controller as it can generate the desired output with quicker settling times, fewer oscillations, The fluid level control is a severe problem in industries as an ineffective control action will cause critical complexities in process operation, which may disagree with the equilibrium of the process reaction. Real-time level control is also a challenging problem because of inherent nonlinear characte ristics, such as interaction effects, para metric uncertainties, and input dead time and measurement dela ys; these nonlinear features significantly affect t he accuracy of industrial processes. Most industrial processes deploy a conventional proportional-integral-derivative (PID) controller for its simple setup. The structure of the PID controller is widespread, and several thumb rules are available to tune its parameters [1-5]. However, in a nonlinear system, a controller built for a specific equilibrium p oint will not work satisfactorily at other equilibrium po ints because of system uncertainties resulting from the scaling of the process co mponents. Therefore, a precise co ntrol action cannot be achieved with such open-loop PID tuning schemes. In order to overcome the limitation of the PID controller in a nonlinear plant, gain scheduling is proposed. The scheduling of PID gains is a significant enhancement in the PID structure [1, 6] and is usually carried out to reduce the slow convergence of traditional PID controllers. The ty pical implementation of a PID controller with gain scheduling requires the development of mathematical process m odels around several equilibrium p oints and the estimation of corresponding PID gains. T hen, the controllers of each operating region would be combined into a single controller for the entire operation region of the plant [7].\n\nHowever, no algor ithms can ascer tain the different thumb rules, such as t he optimal numb er of op erating points and the appropriate process variable dynamic s, to design a gain sched uling PID controller. There is also no practical technique to test the reliability and efficacy of a gain sch eduled control system [8-11]. Furthermore, one significant limitation of gain scheduling is its i nability to achieve a high pre cision of stability b y minimizing the effects of the external disturbances in the system [12]. Gain scheduled PID is also a model-based design method, and any uncertainty in the process m odel might have a significant impact on the controller's performance. Hence, it is necessary to use an algorithm with a simple structure that would not require knowledge about the process 's mathematical model. Therefore, this contin ues to be a research interest for various researchers to determine the most accurate and stable control method, which can also reject multiple disturbances. Even though the gain scheduling PID control produces highly stab le and precise control action, its dynamic response is usually inadequate, which leads to overshoot and substantial settling time. Relatively, the fuzzy control logic is ca pable of excellent dynamic response in nonlinear and time-varying s ystems by generating outputs with lesser overshoot and response times. Fuzz y logic control (FLC) has rapidly evolved into one of the most successful theories for complex co ntrol s ystems. From control systems to artificial intellige nce, fuzzy logic has been applied in many fields. Fuzzy logic is a suitable alternative to PID structures for nonlinear and time-varying sy stems [13]. It reduces the oscillations of the manipulated varia ble around the setpoint, which leads to a quick convergence of the p rocess variable. The most crucial advantage of fuzzy logic is that it does not require knowledge of th e mathematical model. Thus, it can effectively handle the parametric uncertainty, which makes it more robust than PID controllers [1 3-16]. However, for lar ge time-delay systems, a fuzzy logic control scheme might not be considered suitable. It should also b e noted that the complex structure of a fuzzy lo gic controller must be simplified befor e real-time implementation by combining the proportional error component and its derivative linearly [17-19]. In order to ensure a robust and stable co ntrol action, the fuzzy rule base should possess a linear dep endence on the summed fuzzy inputs [20] . Since ever y real -time i ndustrial process is laden with intrinsic nonlinearities, disturbances, process variable saturation, hysteresis, and nonlinear flow dynamics, i t is necessary to implement a n adaptive controller with high dis turbance rejec tion capability to achieve adequate control against these nonlinearities and time-var ying disturbances [21]. In this proposed work, the advantages of both ada ptive control and fuzzy logic control w ere incorporated by implementing an adaptive fuzzy controller, and its efficiency was co mpared to that of a gain scheduling PID c ontroller. The various topics of this paper are arranged in the following manner: Section 2 presents a brief description of the various adaptive PID and ad aptive fuzzy co ntrol techniques employed in complex nonlinear indus trial systems. Section 3 details the hardware of the level process station. The m athematical modeling of the process is described in sec tion 4. The controller design steps and the simula tion results are explai ned in parts 5 and 6. T he real-time implementation of the controllers and their corresponding results are presented in section 7. Section 8 discusses the adaptive controllers' performance, and the concluding explanations about the suitable controller for the proposed system are provided in sectio n 9. Several tec hniques are proposed in the literature to impro ve upon the limitations of traditional P ID contro l scheme, namely, adaptive gain scheduling tec hnique, adaptive PID design using the Asynchronous Advantage Actor -Critic (A3C) assignment (CRA) technique, and PID controller with decoupler and inverting decoupler. The gain scheduling strate gy proposed in this w ork has been successf ully implemented in several systems; Sai n and Peczkowski [22-24] have implemented gain scheduling PID controllers for engine speed control for several nonlinear turbojet\n\nfor a nonli near interacting multiple-input multiple-o utput (MIMO) conical tank system. Åstrom [1] has i mplemented gain scheduling PID control in several s ystems such as car air-fu el ratio systems, ship steering systems, and effluent systems. Many articles also elaborate on the implementation of fuzzy logic in industrial applications. Zhou et al. [26] implemented an adaptive fuzzy-based PID control scheme to improve the controller's disturbance rejection capability in an inertially stabilized platform (ISP). Cheng et al. [27] developed an FLC scheme to stabilize a double inverted pend ulum system. Mahapatro, Subudhi, and Ghosh proposed the design and implementation of an adaptive fuzzy PI controller [28] and adaptive fuzzy Sliding Mod e Controller (SMC) [29] to reduce the chatterin g and to improve r obustness in the liquid level control of a coupled tank system. Tamilselvan and Aarthy [30] proposed an FLC using the Kalman algorithm to co ntrol the fluid level in a conical tank system. Sakthivel, Anandhi, and Natarajan [31] d esigned and implemented a fuzzy logic liquid level controller for a nonlinear spherical tank s ystem with three operating regions. In the level process station, the process tank level is controlled by regulating the fluid's inlet flow rate. A wheel flow meter and an orifice plate with a differential pressure transmitter ( DPT) are used to measure the liquid's flow rate in the pipeline. A radio frequency (RF) capacitance level transmit ter is used to measure the process tank le vel. Initially, a pump with a discharge capacity of 1200 lph is used to fetch the water from the reservoir tank and discharge it to an equal percentage control valve. During th e fluid flow through the orifice plate, a differential pressure is developed across it. The DPT, whose corresponding output range is between 4 -20 mA, senses the differenti al pressure. This a nalog output is converted and scaled to 0-5 V and fed to the 12-bit analog-to-digital converter (ADC) in national ins truments reconfigurable (a) The level process station (b) The schematic diagram of the system Fig. 1 The level process station and its sche matic representation The adaptive PID or adaptive fuzzy co ntrol algorithm infers the correspo nding digital output, which is later co nverted to an analog value of 0-5 V by a 12-bit d igital-to-analog converter (DAC). The DAC outp ut is fed to an electro-pneumatic (E/P) converter to generate a 3-15 psi output. The E/P converter's output manipulates the control valve's stem position to regulate the inlet flow rate, which eventua lly maintains the fluid level at the desired value. Fig. 1 shows the level process station and its schematic diagram on which the adaptive P ID and adaptive fuzzy controllers are i mplemented.\n\nFig. 16 illustrates the servo and regulatory responses of the adaptive PI D controller to varying setpoints and input disturbance in the four operating regions. Fig. 17 presents the servo and regulatory responses of the adaptive fuzzy controller in operating regions 3 and 4, and the response within the encircled area in Figs From the responses depicted in Figs. 16, Figs. 17, and the time-domain specifications o f th e servo and regulatory responses stated in Table 7; it can be inferred that the controllers are capable of effecti ve disturbance reje ction and setpoint tracking. Although the adaptive P ID controller exhibits quicker control action, it generates significant overshoot, o scillations, and offset in the respo nse. In contrast, the adaptive FLC displays better regulatory capabilities than its counterpart by generating steady-state output quickly with no overshoot but with a significant of fset. Table 7 Time-domain specifications of the ser vo and regulatory responses of the co ntrollers in real-time plant The setpoint tracking and the disturbance rejection capability of the simulated controllers and the real-time controllers are compared and depicted in Figs. 18 to 21. The servo response of the controllers in the simulation in the operating regions 3 and 4 is compared in Fig. 18 . Similarl y, the servo and regulatory respon ses o f the controller s in the si mulation in the operating regions 3 and 4 are compared and depicted in Fig. 19, where in the respo nse within the encircled area illustrates the ad aptive (a) The simulated servo responses in region 3 (b) The simulated servo responses in region 4 Fig. 18 The comparison of servo resp onse of the controllers in the operating regio ns 3 & 4 in simulation Although the adaptive PID controller responds quickly to variations in the process variable, the response is rather abrupt, leading to overshoots and sh arp oscillations. Meanwhile, the response of the adaptive fuzzy controller is s mooth without demonstrates the servo behavior of the proposed controllers in the operating regions 3 & 4 in the process plant. Fig. 21 shows the servo and regulatory behavior of the adaptive controllers in real-time in the operating regions 3 & 4, wherein the response within the encircled area depicts the controllers' regulator y action.\n\n[19] M. Wang, B. Chen, and S. Tong, “Adaptive Fuzzy Tracking Control for Strict-Feed back Nonlinear Systems With [21] M . Sh ort a nd F . Abu gchem , “O n the Jit ter S ensit ivit y of an A dapti ve D igita l Con trol ler: A Com puta tiona l Sim ulat ion S tudy ,” Inte rnati onal Jour nal of Eng inee ring and Techn ology Inn ovati on, vol. 9, n o. 4, pp. 241- 256, Septe mber 2019 . [22] J. L. Peczkowski and M. K. Sain, “Design of Nonlinear Multivariable Feedbac k Controls by Total Synthesis,” America n Controller Using Optimization Algorithms for a Dual Interacting Conical Tank Process,” 2nd International Conference on Trends in Electronics and Infor matics, May 2018. [27] C . Fuyan, Z. Guomin, L. Youshan, and X. Zhengming, “Fuzzy Control of a Do uble-Inverted Pendulum,” Fuzzy Sets and [28] S . R. Mahapatro, B. Subudhi, and S. Ghosh, “Adaptive Fuzzy PI Controller Design for Coupled Tank System: An [29] S . R. Mahapatro, B. Subudhi, and S. Ghosh, “Design and Real Time Implementation of an Adaptive Fuzzy Sliding Mode Controller for a Coupled Tank System,” International Journal of Numerical Modelling: Electronic Networks, Devices and [30] G. M. Tamilselvan and P. Aarthy, “Online Tuning of Fuzzy Logic Controller Using Kal man Algorithm for Conical Tank System,” Journal of Applied Research and Technology, vol. 15, no. 5, pp. 492-503, October 2017. [31] G. Sakthivel, T. S. Anandhi, and S. P. Natarajan, “Design of Fuzzy Logic Contro ller for a Spherical Tank System and I ts Real Time Implementation,” International Jo urnal of Engineering Research and Applicatio ns, vol. 1, no. 3, 2011. [32] G. Stephanopoulos, “Chemical Process Control: An Introduction to Theory and Pr actice,” Automatica, vol.21, no. 4, pp. [33] J. M. Mendel, “Fuzzy Logic Systems for Engineering: A Tutorial,” Proceedings of the I EEE, vol. 83, no. 3, pp. 345-377 , Copyright© by the authors. Licensee TAET I, Taiwan. This article is an open access a rticle distributed under the terms and conditions of the Creative Commons Attribution (CC BY-NC) license"
    },
    {
        "link": "https://researchgate.net/publication/4009350_Design_and_implementation_of_fuzzy-based_PID_controller",
        "document": ""
    },
    {
        "link": "https://automaticaddison.com/how-to-simulate-a-robotic-arm-in-gazebo-ros-2",
        "document": "In this tutorial, I will guide you through the process of simulating and performing basic control of a robotic arm in Gazebo. By the end of this tutorial, you will be able to build this:\n\nGazebo is a robotics simulator that enables the testing and development of robots in a virtual environment. It supports a wide range of robots and integrates seamlessly with ROS 2, facilitating the transition from simulation to real-world application. This makes Gazebo an essential tool for roboticists aiming to prototype and refine algorithms efficiently.\n\nBefore we begin, I should advise you that Gazebo has been going through a lot of changes over the last several years. These changes are discussed here. They have changed the name several times and have not fully ported all the ROS plugins from the old classic Gazebo to the new Gazebo simulation engine (some folks still call the new Gazebo, “Ignition”, although it is no longer called Ignition).\n\nFor this reason, I have split this tutorial into two sections: Gazebo (new version) and Gazebo Classic (old version). I will show you how to launch and perform basic control of a robotic arm using both Gazebo versions.\n• You understand what a joint and link are.\n• You know what a ROS 2 package is.\n• You have created a URDF file for a simulated robotic arm that you want to use in Gazebo. I will be using the myCobot 280 for Arduino by Elephant Robotics.\n• If you are using a virtual machine on Ubuntu, I recommend you use Oracle VirtualBox instead of VMWare. I had a lot of issues on VMWare that prevented me from running Gazebo.\n• I am using ROS 2 Iron, the most recent version at the time of writing this tutorial, but you are welcome to use other versions of ROS 2 if you prefer.\n\nAll my code for this project is located here on GitHub.\n\nThe first thing we need to do is to install the package that handles the integration between ROS 2 and Gazebo. The name of this package is ros_gz. Here is the GitHub repository, and here are the official installation instructions. Let’s walk through the steps together.\n\nI am using ROS 2 Iron, but regardless of the ROS 2 version you are using, you will need to open a terminal window, and type this command to install the package:\n\nType Y and press Enter to install the package.\n\nThe command above installs ros_gz and the correct version of Gazebo for your ROS 2 version.\n\nTo test your installation, we will run the example from this part of the repo.\n\nHere is what you should see:\n\nFeel free to run the other demos which you can find here.\n\nWhen you run each demo, you can explore the topics by typing:\n\nThe first thing we need to do is to create an environment for your simulated robot.\n\nWe are going to create a new package named mycobot_gazebo.\n\nAlso, create other folders we will need later:\n\nNow let’s create our world. This first world we will create is an empty world.\n\nSave the file, and close it.\n\nThis file describes a simulated world using the Simulation Description Format (SDF) version 1.6. It defines a world named “default”.\n• The physics plugin simulates how objects interact and move in the world.\n• The user commands plugin allows people to control things in the simulation.\n• The scene broadcaster plugin sends out updates about what’s happening in the world.\n\nTwo models are brought into the world from external sources:\n• Ground Plane, which gives a flat surface for objects to rest on.\n\nThe scene settings specify that shadows should not be rendered in this world.\n\nThis file serves as a basic template for creating a simulated environment, providing the fundamental elements needed for a functional simulation.\n\nSave the file, and close it.\n\nThis code describes a simulated world using the Simulation Description Format (SDF) version 1.6. It defines a detailed indoor residential environment. The world is named ‘default’ and includes plugins for physics simulation, user commands, and scene broadcasting. Gravity is set to Earth-like conditions.\n\nThe scene properties define ambient and background lighting, with shadows, grid, and origin visual turned off. The world is populated with numerous models representing typical household items and furniture. These include structural elements like walls, windows, doors, and flooring, as well as furniture such as beds, chairs, tables, and sofas.\n\nThe environment also includes kitchen appliances, electronic devices, decorative items, and various small objects to create a realistic home setting. Each model is positioned precisely within the world using coordinate systems. Many models are marked as static, meaning they won’t move during the simulation.\n\nLighting is carefully set up with multiple light sources, including a sun for overall illumination and various point and spot lights to simulate indoor lighting. These lights are placed strategically to create a realistic lighting environment, with some casting shadows and others providing ambient illumination.\n\nCreate a models folder. These models are physical objects that will exist inside your house world.\n\nMake sure you put these models inside your models folder.\n\nLet’s walk through one of those models. We will take a look at the refrigerator model.\n\nThe refrigerator model is defined using the Simulation Description Format (SDF) version 1.6. Here’s a breakdown of its structure:\n\nThe model is named “aws_robomaker_residential_Refrigerator_01“. It contains a single link, which represents the main body of the refrigerator.\n\nThe link has three main components:\n• Inertial properties: A mass of 1 unit is specified, which affects how the object behaves in physical simulations.\n• Collision geometry: This defines the shape used for collision detection. It uses a mesh file named “aws_Refrigerator_01_collision.DAE” located in the model’s meshes directory. This mesh is a simplified version of the refrigerator’s shape for efficient collision calculations.\n• Visual geometry: This defines how the refrigerator looks in the simulation. It uses a separate mesh file named “aws_Refrigerator_01_visual.DAE“. This mesh is more detailed than the collision mesh to provide a realistic appearance.\n\nBoth the collision and visual meshes are scaled to 1:1:1, meaning they use their original size.\n\nThe visual component also includes a metadata tag specifying it belongs to layer 1, which is used for rendering or interaction purposes in the simulation environment.\n\nFinally, the model is set to static, meaning it won’t move during the simulation. This is appropriate for a large appliance like a refrigerator that typically stays in one place.\n\nThis structure allows the simulation to efficiently handle both the physical interactions and visual representation of the refrigerator in the Gazebo virtual environment.\n\nOh and one other thing I should mention. DAE (Digital Asset Exchange) files, also known as COLLADA (COLLAborative Design Activity) files, can be created by various 3D modeling and animation software.\n\nThe DAE format is widely used in game development, virtual reality, augmented reality, and simulation environments because it’s an open standard that can store 3D assets along with their associated metadata.\n\nFor the specific refrigerator model mentioned in the SDF file, it was likely created using a 3D modeling software as part of the AWS RoboMaker residential models set. The modelers would have created both a detailed visual mesh and a simplified collision mesh, exporting each as separate DAE files for use in the simulation environment.\n\nThe model.sdf and model.config files are typically created manually using a basic text editor.\n\nNow let’s create our URDF file. A URDF (Unified Robot Description Format) file is an XML format file used to describe the physical configuration and properties of a robot in a structured and standard way.\n\nAdd this code. Then save and close.\n\nThe file begins with an XML declaration. Then, it defines the robot using the tag:\n\nThis line specifies that we’re describing a robot named “mycobot_280” and it uses the xacro namespace, which allows for more dynamic URDF creation\n\nThis defines a “world” link, which represents the fixed world frame.\n\nIf this robotic arm was attached to a humanoid robot, the “world” link would likely be replaced by a link representing the part of the humanoid robot to which the arm is attached. This could be:\n• The torso or chest of the humanoid robot\n\nFor example, instead of:\n\nYou might see something like:\n\nIn this case, “torso_link” would be the parent link to which the arm is attached. The xyz values in the joint would define where on the torso the arm is mounted, and the rpy (roll, pitch, yaw) values would define its orientation relative to the torso.\n\nThis change would effectively integrate the robotic arm into the larger structure of the humanoid robot, allowing for coordinated movement between the arm and the rest of the robot’s body.\n\nThese lines define properties that can be reused throughout the file. “effort” is set to 5.0 and “velocity” to 2.792527.\n\nThe units for effort and velocity in URDF files are typically as follows:\n• Effort: The unit for effort is Newton-meters (N·m). This represents the maximum torque that can be applied by the joint actuator.\n• Velocity: The unit for velocity is radians per second (rad/s). This represents the maximum rotational speed of the joint.\n\nThe file then defines several links. Let’s look at the “base_link” as an example:\n\nEach link has three main sections:\n• <inertial>: Defines the mass and inertia of the link. This is crucial for dynamic simulations.\n• <visual>: Describes how the link looks, usually referencing a 3D model file.\n• <collision>: Defines the shape used for collision detection, often identical to the visual shape.\n\nThe file continues to define more links in a similar manner for each part of the robot (link1, link2, etc., up to the gripper components).\n\nJoints connect links and define how they can move relative to each other. Here’s an example of a revolute joint from the file:\n• name: Unique identifier for the joint\n• type: “revolute” allows rotation around an axis\n• parent and child: Specify which links this joint connects\n• origin: Position and orientation of the joint relative to the parent link\n\nThe file includes several revolute joints for the main arm segments and the gripper.\n\nMimic joints: Some gripper joints use the “mimic” property:\n\nThis means the joint mimics another joint’s movement, useful for synchronized gripper finger movement.\n\nYou will notice I added plugins at the end of the URDF File. Here is a list of plugins that can be added to our URDF file to create extra functionality.\n\nThe two plugins I added are the JointStatePublisher and the JointPositionController.\n\nThe JointStatePublisher publishes the state of all joints in a robot to the “/joint_states” topic, including positions (radians or meters), velocities (radians per second or meters per second), and efforts (Nm or N) as a sensor_msgs/JointState message.\n\nThe JointPositionController subscribes to target joint angles (i.e. positions) as a std_msgs/Float64 message (i.e. a floating-point number like 0.36).\n\nWhat Would Happen on a Real Robot?\n\nIn a real robotic arm like the myCobot 280 or Kinova Gen3 lite, you wouldn’t need to add Gazebo plugins to your URDF if you’re not using Gazebo for simulation. Instead, you would interface directly with the real hardware. Here’s how the system would typically work:\n\nRobotic Arm Physical Components (myCobot 280 robotic arm with Arduino is the example here)\n• The robotic arm has servo motors for each joint (i.e. the movable pieces of the robotic arm). For example, the myCobot 280 uses custom servos, while an arm like Kinova Gen3 lite uses proprietary actuators.\n• Each servo motor has a built-in encoder that measures the joint’s position (e.g. ticks, degree, or radians).\n• The servo positions are controlled by firmware running on a microcontroller (e.g. Arduino).\n• The robotic arm also has a physical motor controller (i.e. an electronic circuit) that controls the voltage and current supplied to move the motor to reach a target angular position based on the commands it receives from Arduino.\n• A USB or Ethernet connection links the microcontroller (e.g. Arduino) to your computer.\n• None Receives commands from the computer and sends back to the computer the “state” or position (typically in radians) of each joint.\n• ROS 2 Control is a software component that bridges between ROS 2 and your myCobot 280’s hardware (via the Arduino).\n• You would create a custom hardware interface package for your specific arm.\n• For example, you would write a C++ class that inherits from hardware_interface::SystemInterface. This class defines how to communicate with your Arduino board.\n• This ROS 2 Control hardware interface would communicate with the Arduino over USB, sending commands to the Arduino and receiving joint states from the Arduino:\n• Defines command interfaces for each joint (for sending new position commands).\n• Defines state interfaces for each joint (for reading positions).\n\nThis hardware interface acts an important link between the physical robot (controlled by the Arduino) and the ROS 2 control system. It abstracts away the hardware-specific details, allowing the rest of the ROS 2 system to work with standardized interfaces.\n• Moving further away from the hardware and up the software chain, we have the “Controller Manager”.\n• Load and manage the controllers specified in your configuration (e.g., a joint trajectory controller for coordinated joint motion).\n• Handle the lifecycle of these controllers (configure, activate, deactivate).\n• Manages the flow of data between the hardware interface and the controllers.\n• Not actually a controller, but a specialized component that reads joint states from the hardware interface.\n• The Joint State Broadcaster reads joint states from your custom hardware interface and reports them on the /joint_states topic for components like MoveIt 2 to understand the current state of the robotic arm.\n• JointTrajectoryController is one of many available ROS 2 controllers. This software controller receives desired trajectories for the arm joints.\n• It generates appropriate commands and sends them to the physical motor controllers (i.e. Arduino) through the ROS 2 control hardware interface.\n• Receiving desired joint trajectories (e.g., from MoveIt 2 or a custom node) on a topic like /joint_trajectory_controller/joint_trajectory as a trajectory_msgs/msg/JointTrajectory message. Each message contains a series of waypoints, each specifying joint positions (and optionally velocities and accelerations) at specific time points.\n• Sending position commands to the hardware interface at a high frequency (typically 100-1000 Hz).\n• These commands are then sent to the Arduino, which controls the actual servo motors.\n• Uses the published joint states to maintain an internal model of the robot’s current state.\n• For the myCobot280, it would:\n• Receive a goal pose for the gripper (e.g., from a user interface or task planner).\n• Use its knowledge of the robot’s kinematics to plan a collision-free path to the goal pose\n• Send this trajectory to the Joint Trajectory Controller for execution.\n\nSummary of the Workflow for an Example Pick and Place System Using ROS 2 Control and MoveIt 2\n\nHere is how it would all work for a real robotic arm with perhaps a depth camera (like the Intel RealSense) involved to see objects in the world.\n• When new goal set (e.g., “move gripper to object detected by camera”):\n\nThis setup integrates the depth camera data into the control loop, allowing for:\n\nThe system maintains the abstraction of the myCobot280’s Arduino control, while adding the capability to react to its environment using the RealSense camera. This makes it suitable for tasks like pick-and-place operations, where the robot can locate objects with the camera and plan motions to grasp them with the gripper.\n\nTo enable Gazebo to communicate with to ROS 2 topics and vice versa, you need to set up a parameter file that bridges the topics between the two platforms. You can see tutorials of how this works here on GitHub (for ROS 2 Iron and older)\n\nAdd this code, and then save the file.\n\nThe first part of the file deals with a topic called joint_states, which is published by the JointState plugin. This means that data about the joint states of the robot is being sent from Gazebo to ROS.\n\nThe rest of the file lists topics that the JointPositionController plugin subscribes to. This means that these topics are used to send commands to specific joints of the robot from ROS to Gazebo.\n\nLet’s create a launch file to spawn both our world and our robotic arm.\n\nSave the file, and close it.\n\nNow let’s modify our package.xml file to update the package dependencies.\n\nAdd this code. Save the file, and then close it.\n\nGo to the CMakeLists.txt, and make sure it looks like this:\n\nYou can comment out or remove any folders or scripts in CMakeLists.txt that we have not yet created in this tutorial (e.g. test_set_joint_position_publisher.py). Be sure to do this before running “colcon build” below.\n\nSave the file, and close it.\n\nMake sure all dependencies are installed.\n\nLaunch the World and URDF Files Together\n\nRun the launch file using the following command:\n\nHere is the output using the empty.world file:\n\nHere is the output using the house.world file. You might see a pop up asking if you would like to Force Quit Gazebo…just wait and be patient for everything to load (house.world is full of SDF models):\n\nTo see a list of active topics, type:\n\nTo see the options, you can type:\n\nGazebo often doesn’t close cleanly, so if you have trouble loading Gazebo, reboot your computer.\n\nI could not get the JointTrajectoryController to work in this new Gazebo. As I mentioned earlier, things are moving fast over at Gazebo, and not all the plugins have been transitioned over. However, I was able to get the JointPositionController to work and make the arm move.\n\nTo send joint angles to the robot using ROS 2, type this in a terminal window:\n\nLet’s create a ROS 2 node that can loop through a list of joint positions to simulate the robotic arm moving from the home position to a goal location and then back to home, repeatedly.\n\nAdd this code, and then close it.\n\nUpdate your CMakeLists.txt to include this new script.\n\nNow launch the robot. Wait for everything to come up, including RViz.\n\nRun the basic joint position publisher to simulate movement for your robotic arm:\n\nYour arm will move from a home position to a goal position over and over again. It won’t be the prettiest movement, but we will fix that in a future tutorial.\n\nNow let’s take a look at how to simulate and move a robotic arm using the classic version of Gazebo (which will reach end of life in 2025).\n\nOpen a new terminal window, and install the packages that will enable you to use ROS 2 to interface with Gazebo Classic.\n\nAdd this code. Then save and close.\n\nLet’s create a launch file to spawn both our world and our robotic arm.\n\nSave the file, and close it.\n\nCreate an environment for your simulated robot.\n\nIf you want gravity turned ON, the gravity line should look like this:\n\nIf you want gravity turned OFF, the gravity line should look like this:\n\nSave the file, and close it.\n\nSave the file, and close it.\n\nLaunch the World and URDF Files Together\n\nRun the launch file using the following command:\n\nHere is the output using the empty_classic.world file:\n\nHere is the output using the house_classic.world file:\n\nTo make the robot move, you use this command:\n\nYou can change the values in the positions array to move the robotic arm to different poses.\n\nIf you want to close the gripper, the gripper_controller position value needs to be -0.7.\n\nThe two Gazebo plugins defined in the xacro file that make all this work are libgazebo_ros_joint_state_publisher.so (publishes the joint states from Gazebo to ROS 2) and libgazebo_ros_joint_pose_trajectory.so (subscribes to the desired goal poses for the arm that are sent from ROS 2 to Gazebo). That is all you need for basic arm control.\n\nTo close Gazebo, press CTRL + C on your keyboard.\n\nLet’s create a ROS 2 node that can loop through a list of trajectories to simulate the arm moving from the home position to a goal location and then back to home.\n\nMake sure to update your CMakeLists.txt file.\n\nNow launch the robot. Wait for everything to come up, including RViz.\n\nRun the basic joint trajectory publisher to simulate movement for your robotic arm:\n\nThe output should look like the animated image at the beginning of this blog post.\n\nNote that even though we have the “mimic” tag in the URDF file for the gripper, we still have to explicitly define all non-fixed joints in the URDF Gazebo plugins section in order for everything to work properly in Gazebo."
    },
    {
        "link": "https://github.com/modulabs/arm-control",
        "document": "arm_controllers is general robot arm controller. Elfin is 6-dof manipulator. elfin_description, elfin_gazebo is forked from [3], elfin_launch are added. elfin_control has controller and gain in yaml file.\n\nImplemented various control algorithm on 6-dof Elfin manipulator simulation using ros-control frameworks.\n\nDepending on the controller you want to run, use suitable launch file. If you want to use motion controller in joint space, then you may choose this controllers as follows:\n\nIf you want to use motion controller in task space, then you may choose this controllers as follows:\n\nIf you want to use motion and force controller in task space, then you may choose this controllers as follows:\n\nIf you want to plot data in rqt graph, use rqt_plot.launch file. Customize perspective files to plot data you need.\n• Slotine, On the Adaptive Control of Robot Manipulators"
    },
    {
        "link": "https://eprints.whiterose.ac.uk/170059/7/peerj_cs_383.pdf",
        "document": ""
    },
    {
        "link": "https://ijisae.org/index.php/IJISAE/article/view/6456",
        "document": "Rodriguez, F., Munoz, F., & Garcia, A. (2021). ROS-based simulation and control of a multi-DOF robotic arm for industrial applications. In 2021 IEEE International Conference on Industrial Technology (ICIT) (pp. 953-958). IEEE.\n\nSantos, P., Costa, P., & Moreira, A. P. (2021). Implementation of a ROS-based robotic system for manipulation tasks. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 4523-4529). IEEE.\n\nWang, J., Zhou, Z., & Chen, Y. (2022). An open-source ROS-based simulation framework for robotic manipulators. Robotics and Autonomous Systems, 147, 103857.\n\nLee, S., Kim, H., & Park, J. (2020). Development and validation of a ROS-Gazebo based simulation platform for robotic applications. International Journal of Advanced Robotic Systems, 17(3), 1729881420926184.\n\nKim, Y., & Kim, J. (2022). A ROS-based real-time control system for a multi-DOF robotic arm with Gazebo simulation. In 2022 IEEE International Conference on Robotics and Automation (ICRA) (pp. 2534-2540). IEEE.\n\nSharma, R., & Kumar, V. (2020). Real-time simulation and control of a robotic arm using ROS and Gazebo. In 2020 3rd International Conference on Robotics and Automation Engineering (ICRAE) (pp. 56-60). IEEE.\n\nZhang, L., Liu, Y., & Chen, H. (2021). Dynamic simulation and control of a robotic manipulator using ROS and Gazebo. Applied Sciences, 11(14), 6432.\n\nLee, D., Park, M., & Lee, B. (2021). Design and simulation of an autonomous robotic arm with multi-DOF using ROS. Journal of Automation and Control Engineering, 9(4), 300-306.\n\nHuang, J., & Zhang, Z. (2021). A simulation study on ROS-based control of a robotic arm in a Gazebo environment. In 2021 IEEE International Conference on Mechatronics and Automation (ICMA) (pp. 1024-1030). IEEE.\n\nPatel, D., & Shah, V. (2022). Implementation of inverse kinematics for a robotic arm using ROS and Gazebo. In 2022 IEEE International Conference on Advanced Robotics and Mechatronics (ICARM) (pp. 345-350). IEEE.\n\nAhn, J., & Lee, H. (2020). Development of a teleoperation system for a robotic arm using ROS and Gazebo. Sensors, 20(22), 6597.\n\nNair, S., & Menon, P. (2021). Real-time control of a 6-DOF robotic arm using ROS and Gazebo. In 2021 IEEE International Conference on Robotics and Biomimetics (ROBIO) (pp. 678-684). IEEE.\n\nWang, T., & Li, X. (2022). ROS-based simulation and path planning for an industrial robotic arm. Journal of Robotics, 2022, 9328437.\n\nXu, Y., & Zhao, Y. (2020). A simulation framework for multi-DOF robotic arms using ROS and Gazebo. International Journal of Advanced Manufacturing Technology, 108(9), 2915-2925.\n\nPark, J., & Kim, S. (2021). ROS and Gazebo-based simulation of a multi-DOF robotic arm for manufacturing tasks. Procedia Manufacturing, 54, 345-351.\n\nZhao, R., & Liu, J. (2022). Design and control of a ROS-based robotic arm with multi-DOF. Robotics and Computer-Integrated Manufacturing, 73, 102249.\n\nWei, Z., & Zhou, Y. (2021). A ROS-Gazebo based framework for robotic arm control and simulation. In 2021 IEEE International Conference on Robotics and Automation (ICRA) (pp. 1123-1129). IEEE.\n\nKarthik, V., & Ramakrishnan, S. (2020). Simulation and control of a robotic arm with ROS and Gazebo for industrial automation. In 2020 IEEE International Conference on Emerging Technologies and Factory Automation (ETFA) (pp. 1119-1125). IEEE.\n\nCho, K., & Kim, J. (2021). Development of a multi-DOF robotic arm simulation environment using ROS. Sensors and Actuators A: Physical, 317, 112477.\n\nZeng, Y., & Sun, J. (2022). Path planning and control of a robotic arm using ROS and Gazebo. International Journal of Robotics Research, 41(5), 507-519.\n\nWu, L., & Liu, X. (2021). ROS-based design and control of a robotic arm for medical applications. Journal of Medical Robotics Research, 6(3), 2150007.\n\nShen, J., & Wu, H. (2020). Simulation of a multi-DOF robotic arm in ROS and Gazebo. In 2020 IEEE International Conference on Robotics and Automation (ICRA) (pp. 4230-4235). IEEE.\n\nZhou, M., & Yang, F. (2021). ROS and Gazebo-based inverse kinematics for robotic arm control. In 2021 International Conference on Robotics and Automation Engineering (ICRAE) (pp. 204-210). IEEE.\n\nLi, B., & Xu, D. (2022). Real-time control of robotic arms using ROS: A review. Robotics and Autonomous Systems, 150, 103917.\n\nNguyen, T., & Le, H. (2021). Educational use of ROS and Gazebo in robotic arm simulation. IEEE Access, 9, 98273-98283.\n\nWang, Y., & Chen, H. (2022). Development of an AI-driven robotic arm using ROS and Gazebo. Journal of Artificial Intelligence Research, 74, 343-358.\n\nHe, L., & Wu, M. (2020). ROS-Gazebo based control system for a multi-DOF robotic arm. In 2020 IEEE International Conference on Control and Automation (ICCA) (pp. 1150-1155). IEEE.\n\nKappler, D., et al. (2015). \"Real-time perception meets reactive motion generation.\" In 2015 IEEE International Conference on Robotics and Automation (ICRA) (pp. 5927-5932). IEEE.\n\nLevine, S., Pastor, P., Krizhevsky, A., & Quillen, D. (2018). \"Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection.\" The International Journal of Robotics Research, 37(4-5), 421-436.\n\nTsardoulias, E., Mitkas, P. A., & Gasteratos, A. (2014). \"Real-time adaptive kinematics for robotic manipulators using a hybrid approach of neuro-fuzzy methods.\" Robotics and Autonomous Systems, 62(9), 1244-1256.\n\nArivalagan, M., M. Lavanya, A. Manonmani, S. Sivasubramanian, and P. Hosanna Princye. \"Agricultural robot for automized fertilizing and vigilance for crops.\" In 2020 IEEE International Conference on Advances and Developments in Electrical and Electronics Engineering (ICADEE), pp. 1-3. IEEE, 2020.\n\nAnitha, K., T. Aravind, and S. Praveen Kumar. \"Spying robot for the war field with a wireless night vision camera.\" In AIP Conference Proceedings, vol. 2935, no. 1. AIP Publishing, 2024.\n\nSheebajoice, C., Reddy, R. S. K., Kumar, T. R. V., & Tharun, V. (2023, March). Obstacle Detection and Safe Navigation in Unexpected Situations of Intelligent Vehicle. In 2023 International Conference on Innovative Data Communication Technologies and Application (ICIDCA) (pp. 620-625). IEEE.\n\nDubey, A. P., Pattnaik, S. M., Banerjee, A., Sarkar, R., & Kumar, S. (2016). Autonomous control and implementation of coconut tree climbing and harvesting robot. Procedia computer science, 85, 755-766.\n\nSivakumar, P., Ramesh, K., & Manimaran, S. (2024, March). Modelling of tethered UAV system for coconut/ice apple harvesting–A review. In AIP Conference Proceedings (Vol. 3035, No. 1). AIP Publishing.\n\nPrados, C., Hernando, M., Gambao, E., & Brunete, A. (2024). A Review and Evaluation of Control Architectures for Modular Legged and Climbing Robots. Biomimetics, 9(6), 319.\n\nMendoza, N., & Haghshenas-Jaryani, M. (2024). Combined Soft Grasping and Crawling Locomotor Robot for Exterior Navigation of Tubular Structures. Machines, 12(3), 157.\n\nMegalingam, R. K., Senthil, A. P., Raghavan, D., & Manoharan, S. K. (2024). Modeling of novel circular gait motion through daisy sequence fitting algorithm in a vertical climbing snake robot. Journal of Field Robotics, 41(2), 211-226.\n\nFang, G., & Cheng, J. (2023). Advances in climbing robots for vertical structures in the past decade: a review. Biomimetics, 8(1), 47.\n\nRafiee, M., Razeghi, M., Choobineh, A., Jahangiri, M., & Seif, M. (2023). Development of an Ergonomic, Portable, Climber-Propelled Date Tree Climbing Device. Journal of Agromedicine, 28(3), 497-510.\n\nSubramanian, P., & Sankar, T. S. (2023). Development of a novel coconut-tree-climbing machine for harvesting. Mechanics Based Design of Structures and Machines, 51(5), 2757-2775."
    },
    {
        "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8022577",
        "document": "ROS and Gazebo are the main tools of this experiment. Gazebo is a 3D dynamic simulator that can accurately and efficiently simulate robots in complex indoor and outdoor environments. Gazebo uses a distributed architecture with independent libraries for physical simulation, user interfaces, communication and sensor generation. The three base controllers used for benchmarking come from ROS: JointPositionController, JointVelocityController and JointEffortController. These controllers are included in the ROS control package:\n• None JointPositionController: The input data is position (radians (or) meters). The data calculation of the controller is to convert the difference between the goal position and the current position into force data through the PID controller.\n• None JointVelocityController: The input data is the velocity (radians/sec (or) meters/sec). The data calculation of the controller is to convert the difference between the goal velocity and the current velocity into force data through the PID controller.\n• None JointEffortController: The input data is the effort (force (or) torque). The controller does not need the data conversion of the PID controller, because its input data and output data are all efforts. It is worth noting that the first two controllers will be affected by the PID controller, so in the subsequent experiments will involve the adjustment of the PID parameters, so that the controller can achieve better results. The two advanced controllers selected for this article are AIC and MRAC.\n• None AIC: the control scheme is based on Karl Friston’s free energy principle (FEP) (Friston, Mattout & Kilner, 2011). The active inference framework uses FEP to explain perceptions and actions, and the subject has a world model that is optimized using sensory input by predicting and interpreting its feelings. The main idea of active inference is that the brain can manipulate sensory input and brain state to minimize prediction errors (16). In this way, by changing the sensory input of behavior or modifying its beliefs about the state of the world, behavior and perception can be used to minimize free energy. AIC can be used as an adaptive control scheme for robotic arms, which is easily scalable to high DOF, and it maintains high performance even in the presence of large unmodeled dynamics.\n• None MRAC: model reference adaptive control is a direct adaptive strategy with some adjustable controller parameters and adjustment mechanisms for adjusting them (Jain & Nigam, 2013). Different from commonly used PID controllers, adaptive controllers are good at dealing with uncertain factors in unstructured environments. Usually, the adaptive controller consists of two loops for state feedback and parameter adjustment. The similarity between AIC and MRAC is that they both belong to the adaptive controller type. This experiment simulates that the robotic arm will grab an object from one bookshelf and place it on another bookshelf. It mainly simulates actions instead of actually grabbing objects. The purpose of this is to observe as much as possible the performance of the controller without interference from other external objects. The last forward lean motion is to test whether the joints are in good working condition when the robot arm moves forward. In addition, the purpose of the robot arm moving to the left and right is to obtain the state of different joints when they move in the opposite direction, such as whether the position data is completely opposite, etc. The diversity of data provides strong evidence for controller performance analysis. During the experiment, all actions are repeated multiple times. This is because during the exercise test, sometimes the robot arm will have abnormal movements that are not easy to attract attention. To make the experimental data more convincing, every time during the test, motion will be executed multiple times, and then the observation data will be used for subsequent data analysis. These tasks are to obtain the position data and force data output by the robot arm during operation. The position data is used to analyze the accuracy and control efficiency of the controller to control the robot arm, for example, by analyzing time from one position to another. It can be seen that the controller controls the operating efficiency of the robotic arm. Effort data is mainly used to analyze the jitter of the robotic arm. It is noted that the consideration behind is that the essence of the robot arm is to change the position of an object, including its own joint position, therefore, such a simple test environment was designed, and this test result can be used in some similar environments, however, the environment changes greatly, for instance, in some very cluttered scenarios, more tests are needed to illustrate the results. The robustness test of the controller is by applying an instantaneous external force to a specific joint of the robot arm, because the robot arm will have a position shift or jitter after receiving external force. Then, by collecting joint position data, the time required for the robotic arm to return to a normal state or whether the controller can still control the robotic arm to work normally after receiving external force can be obtained. The joints that accept the external force are selected based on the changes in the position of the joints in space. The more obvious the changes in the position of the joints in space, the easier it is to see the changes in the data after being subjected to the external force. On the contrary, if you choose joint 1 or 2 which has no spatial position change, it is not easy to see the running status of the robot arm after force in the simulation. Finally, in the robustness test, there is another point that needs to be explained. The direction of the robot arm’s instantaneous force is opposite to the direction of the robot arm’s movement. Figures 1 and 2 are all the tasks of this experiment. The three action tasks in Fig. 1 are mainly to complete the general test of the basic controller and advanced controller. The purpose of the test in Fig. 2 is to complete the robustness of the controller. From Fig. 2, it can be seen that the robot arm is running in the right direction, but the external disturbance is left. The purpose of this design is to clearly see the robot arm’s response to external disturbance. In addition, if the direction of force is the same as the direction of movement of the robot arm, the adjustment of the robot arm’s controller to external disturbance may not be clearly observed unless a very large force is applied. Figure 1. Simulation Scene: (A) Initial pose; (B) picking from the right shelf; (C) picking from the left shelf; (D) placing the object. Figure 2. The direction and position of the external force on the robot arm. PID controller is composed of a proportional unit, integral unit and derivative unit. Adjust the performance of the PID controller by adjusting the three parameters. There are many ways to adjust PID, which can be adjusted manually or dynamically adjusted by machine learning. In this article, manual tuning parameters is used. The adjustment method is to adjust the proportional unit P first, then adjust the integral unit I, and finally adjust the differential unit D if necessary. In general, only adjust the parameters of P and I. In addition, the PID controller is just a simple controller with limited performance, so it cannot solve all control problems. In this article, there are two basic controllers that will be affected by PID parameters: joint Velocity Controller and Joint Position Controller. Taking the Joint Velocity Controller as an example, the comparison chart before and after PID parameter adjustment is shown in Fig. 3. Before the PID controller is adjusted, the robot arm cannot reach the set pose and there will be many poses that are not set. This means that the robot arm cannot be effectively controlled by the controller. In Fig. 3, before the PID controller is adjusted, the rotation angle of joint 2 often exceeds the given range, and its maximum value even exceeds that of joint 1. After adjusting the PID controller, the performance of the robot arm is obviously better. If the PID controller is not adjusted, the performance of the two basic controllers cannot meet the needs of the experiment, because the robot arm cannot reach the set posture during the simulation. Figure 3. Comparison of PID controller before and after adjustment: (A) Joint velocity controller before PID adjustment; (B) Joint velocity controller after PID adjustment.\n\nFigure 4 shows the position data of the three basic controllers. It is clear to see from the figure that the data of the joints with the most obvious data changes are basically the same between different controllers. In the simulation process, these three controllers can also control the robot arm to reach the accurate position, and there is no too abnormal data, such as unable to reach the accurate position. But this result is after proper adjustment of PID parameters. It can also be seen from Fig. 4 that the robot arm takes the same time to complete the task designed in Fig. 1. In addition, the data output by the robot arm is the same whether it is at the inflexion point or at the maximum value. However, there are also joints with the significantly different performance data output, such as joint 2 and joint 4. Joint Position Controller has very obvious data anomalies between 0 and 3 s. Although Joint Velocity Controller also has this performance, it is far less obvious than Joint Position Controller. Furthermore, there are obvious data anomalies between 7–13 and 20–23 s, which caused the joints to shake at the target position during the simulation. However, the Joint Effort Controller does not have such abnormal data. Figure 5 shows the effort data of the three basic controllers. It can be clearly seen in Fig. 5 that the Joint Effort Controller has no jitter data, and all the data is normal, but the other two controllers have obvious jitter data. During the simulation, the jitter of the robot arm controlled by the other two controllers can be seen from the simulation animation, but because the amplitude is not very large, it does not look obvious. However, the effort data clearly shows the jitter. Although both the Joint Position Controller and the Joint Velocity Controller are jittering, the Joint Velocity Controller does not make some joints shake significantly at certain times. For example, there is no serious jitter problem for joint 2 between 2 and 7 s. The jitter problem of Joint Position Controller basically lasts for the entire task process, and all joints have jitter. Since displaying the status of 7 joints at the same time is too confusing for the data of the Joint Velocity Controller and Joint Position Controller, then joint 4 is selected for specific data display. Since the position data shows that the degree of change of joint 4 is not very large, the jitter value will be relatively small. If a joint with a very large value is selected, it will be difficult to read the jitter data. The last test is a robustness test. Figure 6 shows the result. The purpose of this test is to observe the time it takes for different controllers to respond to external interference before recovering the interference, and whether the robot arm can return to a normal state. In order to be more intuitive, Table 1 records the response time of the three basic controllers and the time of accept external force. “Accept external force” means the time when the robot arm is subjected to an external force in the experiment. In the experiment, the robot arm is accepted to two times external forces. “Response time” means how long it takes for the controller to restore the robot arm to its previous state when the position of the joint changes. According to the final result, the Joint Position Controller takes the shortest time, and the Joint Effort Controller takes the longest time. Of course, this does not mean that the Joint Position Controller is better than the Joint Effort Controller. The specific reasons will be explained in the data analysis section. In addition, even if the time is different, the difference is only 0.1 s, so the robustness test of the three controllers is equivalent to obtaining relatively good results. First, the three controllers restore the robot arm to the normal state and the time it takes is all short, secondly, these three basic controllers have the ability to restore the robot arm to normal state. The accuracy of the robotic arm can be determined by observing whether each joint of the robotic arm reaches the correct position. By observing the force of each joint, it can be seen whether the robot arm is running stably. Figure 7 shows the position data of the experimental results of the advanced controller benchmarking of this article. Through this data, the accuracy and control efficiency of the advanced controller AIC and MRAC to control the robot arm is obtained. Figure 7 also shows that it takes less time for the MRAC to control the robot arm to reach the designated position than the AIC to control the robot arm to reach the designated position. The AIC controller takes 30 s but the MRAC only takes 20 s. Furthermore, the turning point of each line in the figure is the position data of the joint when the robot arm reaches the specified position, so whether it is in the simulation animation or the joint output data can prove that the two advanced controllers can control the robot arm to the specified position because the value of the turning point is the same. However, the joint 2 data in Fig. 7 has obvious differences. The fluctuation of joint 2 of the robot arm controlled by MRAC is more obvious than that of AIC. For the convenience of observation, the data of joint 2 is taken out separately. Since the data of joint 4 in Fig. 7 is not obvious, the data of joint 4 is also taken out. Figure 8 is the Effort data figure of AIC and MRAC. It can be seen from the figure that the jitter of the robotic arm controlled by the MRAC controller is much more serious than the AIC, which is not obvious in the position data. Furthermore, when the AIC controlled robotic arm has data fluctuations, the MRAC controlled robotic arm also has data fluctuations, such as at 6 and 12 s. The difference is that MRAC is sometimes earlier than AIC. This may be because the robot arm controlled by MRAC runs faster. For MRAC, the problem of joint 2 is more serious, and the jitter of joint 2 in the simulation animation is indeed more obvious. Although other joints also jitter, the gap is still relatively large compared with joint 2. Compared with MRAC, there is almost no jitter in the robot arm controlled by AIC, and the data change law of AIC is smoother, and the data change of MRAC is more chaotic. Figure 9 shows the robustness test results of the advanced controller AIC and MRAC. The direction and position of the robot arm bearing the external force have been shown in Fig. 2. Furthermore, the movement direction of the robot arm is to the right, and the robot arm will receive an external force to the left. Furthermore, it is not difficult to see from Fig. 9 that AIC reacts more smoothly to external interference, while MRAC is more chaotic. This phenomenon should be because the jitter of MRAC is more obvious than that of AIC, which leads to more chaotic MRAC data output. Another phenomenon is worth noting. The time interval between two data fluctuations of the robot arm controlled by AIC is about 5 s, for example, between 1 and 6 s. The time interval of MRAC is 2 s, for example, 5–7 s. This phenomenon should be caused because the robot arm controlled by AIC runs slower than the robot arm controlled by MRAC. In order to better display the experimental results, relevant experimental data are recorded in Table 2. The final result shows that the robot arm controlled by the AIC controller needs 0.3 s to recover to the previous state after receiving the external force, while the MRAC only needs 0.1 s. In addition, the output data value of the MRAC after the external force is smaller than the value of AIC a lot of. However, this does not mean that the robustness of MRAC is better than that of AIC controller. This experimental result can only prove that the two advanced controllers can adjust the robot arm to the state before the external force is applied. The data size and response time output by the robotic arm may also be affected by other factors, such as jitter. More analysis will be explained in the data analysis section.\n\nAccording to the experimental results of the basic controller, several conclusions can be drawn:\n• None The basic controller can control the robot arm to the designated position, and the time spent is the same;\n• None The basic controller has jitter, but the jitter of the Joint Effort Controller is the least obvious, and the other two basic controllers have obvious jitter;\n• None All three basic controllers can restore the robot arm interfered by an external force to the state before the interference, but the response time of the controller is different. Based on the previous introduction to the basic controller, it is not difficult to see that the two controllers that perform poorly (with obvious jitter) are affected by the PID parameters, so the adjustment of the PID parameters will directly affect the performance of the basic controller. It is challenging to obtain very good results based on the adjustment of PID parameters because there are seven joints that need to adjust PID parameters in this robot arm, and the joints will affect each other during the adjustment process. For example, when the PID parameters of joint 1 are adjusted to a satisfactory condition, the PID parameters of joint 2 are adjusted, but during the adjustment process, it is found that the data output of joint 1 has changed again. Therefore, in this experiment, adjust the PID parameters as much as possible. The result is that each joint can work, and every PID parameter adjustment is not forced to obtain very good results. So PID parameters affect the performance of the two basic controllers. In addition, the poor performance of the controller may also due to the controller itself. The Joint Velocity Controller and Joint Position Controller accept speed and position values as input data, and then output as Effort commands through PID mapping. First, that also proved the conclusion of the experiment, that is, PID parameters affect the performance of the basic controller, and besides, because the data needs to be mapped and converted, this may also cause problems with the controller, as the data conversion may cause errors. The better-performing Joint Effort Controller does not need to consider these issues. Because the input and output of the Joint Effort Controller are both Efforts data, it does not need to map input and output data, nor is it affected by PID parameters, so this may be a reason of the better performance of the Joint Effort Controller. Although the performance of the two basic controllers is not very good, it is only reflected in the occurrence of jitter. More importantly, the three basic controllers can spend the same time controlling the robot arm to the designated position. This test also did a robustness test on the controller, the purpose is to observe whether the controller is capable of restoring the controller interfered by external forces to the state before the interference. From the results, the three controllers all meet the requirements. The difference is that the response time and output value of the controller are different. This should be caused by the jitter problem of the robot arm itself. It can be seen from the Effort data output by the robotic arm that there is jitter between the joints. When the robot arm is subjected to external force, the force generated by the jitter and the jitter direction will affect the magnitude of the external force. As a result, the controller’s response and time to external force interference in different results. Although the result data is different, this robustness test proves that the basic controller has the ability to deal with simple external disturbances and restore the robot arm to the state before the disturbance. Active inference controller and MRAC are the advanced controllers selected for this controller benchmarking. From the results, both controllers can control the robot arm to reach the preset posture, but they take different time. MRAC is obviously faster to complete the task than AIC. According to the characteristics of the AIC controller, some parameters need to be adjusted adaptively within the AIC. Most of the parameters of MRAC are preset, so this may be one of the reasons that cause the MRAC controller to execute faster than AIC. In addition, the jitter problem of MRAC controller is obviously more obvious than that of the AIC controller. According to the understanding of MRAC, this may also be due to the parameter setting of MRAC itself. In Fig. 1, it can be seen that the data output by the plant will be input to the controller, which is MRAC, which compares the data with the reference model internally. This process will produce errors, which will be passed to the adaptive algorithm module, and then the recalculated result is still with error, and finally, the result of the controller output is with error, this result is passed to Actuator and Plant. Since the model parameters are set in advance, any inappropriate situation occurs, the internal parameters of the controller will not be changed. Therefore, if the MRAC jitter problem is solved, the MRAC parameter setting and the reference model setting should be very accurate. But in general, the results of this benchmark test can show that these two controllers can meet the basic requirements of controlling the operation of the robotic arm. Although the time is different, the two advanced controllers can control the robotic arm to reach the specified pose. This benchmarking also designed a simple robustness test for the advanced controller. According to the results, it can be seen that the AIC response to the external force is smoother, and it took more time to restore the robot arm to the state before the external force interference. Like the basic controller, although MRAC can control the robotic arm to return to the state before the interference in a short time, due to the jitter problem of MRAC itself, it does not mean that MRAC is more robust than AIC. If the further testing is needed, the jitter problem should be solved first, whether it is a basic controller or an advanced controller, this is necessary, the purpose of this is to minimize the impact of other uncertainties on the experiment."
    }
]