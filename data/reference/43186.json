[
    {
        "link": "https://geeksforgeeks.org/decision-tree-implementation-python",
        "document": "Decision Tree is one of the most powerful and popular algorithms. Python Decision-tree algorithm falls under the category of supervised learning algorithms. It works for both continuous as well as categorical output variables. In this article, We are going to implement a Decision tree in Python algorithm on the Balance Scale Weight & Distance Database presented on the UCI.\n\nA Decision tree is a tree-like structure that represents a set of decisions and their possible consequences. Each node in the tree represents a decision, and each branch represents an outcome of that decision. The leaves of the tree represent the final decisions or predictions.\n\nDecision trees are created by recursively partitioning the data into smaller and smaller subsets. At each partition, the data is split based on a specific feature, and the split is made in a way that maximizes the information gain.\n\nIn the above figure, decision tree is a flowchart-like tree structure that is used to make decisions. It consists of Root Node(WINDY), Internal nodes(OUTLOOK, TEMPERATURE), which represent tests on attributes, and leaf nodes, which represent the final decisions. The branches of the tree represent the possible outcomes of the tests.\n• Root Node: The decision tree’s starting node, which stands for the complete dataset.\n• Branch Nodes: Internal nodes that represent decision points, where the data is split based on a specific attribute.\n• Decision Rules: Rules that govern the splitting of data at each branch node.\n• Attribute Selection: The process of choosing the most informative attribute for each split.\n• Splitting Criteria: Metrics like information gain, entropy, or the Gini Index are used to calculate the optimal split.\n\nAssumptions we make while using Decision tree\n• None At the beginning, we consider the whole training set as the root.\n• None Attributes are assumed to be categorical for information gain and for gini index, attributes are assumed to be continuous.\n• None On the basis of attribute values records are distributed recursively.\n• None We use statistical methods for ordering attributes as root or internal node.\n\nGini index and information gain both of these methods are used to select from the n attributes of the dataset which attribute would be placed at the root node or the internal node.\n• None Gini Index is a metric to measure how often a randomly chosen element would be incorrectly identified.\n• None It means an attribute with lower gini index should be preferred.\n• None Sklearn supports “gini” criteria for Gini Index and by default, it takes “gini” value.\n\nIf a random variable x can take N different value, the i’value [Tex]x_{i} [/Tex] with probability [Tex]p_{ii} [/Tex] we can associate the following entropy with x :\n• None Entropy is the measure of uncertainty of a random variable, it characterizes the impurity of an arbitrary collection of examples. The higher the entropy the more the information content.\n\nDefinition: Suppose S is a set of instances, A is an attribute, [Tex]S_{v} [/Tex] is the subset of s with A = v and Values(A) is the set of all possible of A, then\n• None The entropy typically changes when we use a node in a Python decision tree to partition the training instances into smaller subsets. Information gain is a measure of this change in entropy.\n• None Sklearn supports “entropy” criteria for Information Gain and if we want to use Information Gain method in sklearn then we have to mention it explicitly.\n\nTitle : Balance Scale Weight & Distance Database Number of Instances : 625 (49 balanced, 288 left, 288 right) Number of Attributes : 4 (numeric) + class name = 5 Attribute Information: 1. Class Name (Target variable): 3 L [balance scale tip to the left] B [balance scale be balanced] R [balance scale tip to the right] 2. Left-Weight: 5 (1, 2, 3, 4, 5) 3. Left-Distance: 5 (1, 2, 3, 4, 5) 4. Right-Weight: 5 (1, 2, 3, 4, 5) 5. Right-Distance: 5 (1, 2, 3, 4, 5) Missing Attribute Values: None Class Distribution: 1. 46.08 percent are L 2. 07.84 percent are B 3. 46.08 percent are R\n\nYou can find more details of the dataset.\n• sklearn :\n• None In python, sklearn is a machine learning package which include a lot of ML algorithms.\n• None Here, we are using some of its modules like train_test_split, DecisionTreeClassifier and accuracy_score.\n• NumPy :\n• None It is a numeric python module which provides fast maths functions for calculations.\n• None It is used to read data in numpy arrays and for manipulation purpose.\n• Pandas :\n• None Used to read and write different files.\n• None Data manipulation can be done easily with dataframes.\n\nIn Python, sklearn is the package which contains all the required packages to implement Machine learning algorithm. You can install the sklearn package by following the commands given below.\n\nBefore using the above command make sure you have scipy and numpy packages installed. If you don’t have pip. You can install it using\n\nWhile implementing the decision tree in Python we will go through the following two phases:\n• None\n• None Split the dataset from train and test using Python sklearn package.\n• None To import and manipulate the data we are using the pandas\n• None Here, we are using a URL which is directly fetching the dataset from the UCI site no need to download the dataset. When you try to run this code on your system make sure the system should have an active Internet connection.\n• None As the dataset is separated by “,” so we have to pass the sep parameter’s value as “,”.\n• None Another thing is notice is that the dataset doesn’t contain the header so we will pass the Header parameter’s value as none. If we will not pass the header parameter then it will consider the first line of the dataset as the header.\n• None Before training the model we have to split the dataset into the training and testing dataset.\n• None To split the dataset for training and testing we are using the sklearn module train_test_split\n• None First of all we have to separate the target variable from the attributes in the dataset.\n• None Above are the lines from the code which separate the dataset. The variable X contains the attributes while the variable Y contains the target variable of the dataset.\n• None Next step is to split the dataset for training and testing purpose.\n• None Above line split the dataset for training and testing. As we are splitting the dataset in a ratio of 70:30 between training and testing so we are pass test_size\n• random_state variable is a pseudo-random number generator state used for random sampling.\n\nBelow is the code for the sklearn decision tree in Python.\n\nImporting the necessary libraries required for the implementation of decision tree in Python.\n• : function, which is responsible for splitting the dataset into training and testing sets. It separates the target variable (class labels) from the features and splits the data using the function from scikit-learn. It sets the test size to 30% and uses a random state of 100 for reproducibility.\n• : function, which is responsible for training a decision tree classifier using the Gini index as the splitting criterion. It creates a classifier object with the specified parameters (criterion, random state, max depth, min samples leaf) and trains it on the training data.\n• : function, which is responsible for training a decision tree classifier using entropy as the splitting criterion. It creates a classifier object with the specified parameters (criterion, random state, max depth, min samples leaf) and trains it on the training data.\n• : function, which is responsible for making predictions on the test data using the trained classifier object. It passes the test data to the classifier’s\n• : function, which is responsible for calculating the accuracy of the predictions. It calculates and prints the confusion matrix, accuracy score, and classification report, providing detailed performance evaluation.\n\nBy using function from the submodule to plot the decision tree. The function takes the following arguments:\n• None : This argument fills the nodes of the tree with different colors based on the predicted class majority.\n• None : This argument provides the names of the features used in the decision tree.\n• None : This argument provides the names of the different classes.\n• None : This argument rounds the corners of the nodes for a more aesthetically pleasing appearance.\n\nThis defines two decision tree classifiers, training and visualization of decision trees based on different splitting criteria, one using the Gini index and the other using entropy,\n\nIt performs the operational phase of the decision tree model, which involves:\n• None Imports and splits data for training and testing.\n• None Uses Gini and entropy criteria to train two decision trees.\n• None Generates class labels for test data using each model.\n• None Calculates and compares accuracy of both models.\n\nEvaluates the performance of the trained decision trees on the unseen test data and provides insights into their effectiveness for the specific classification task and evaluates their performance on a dataset using the confusion matrix, accuracy score, and classification report.\n\nPython Decision trees are versatile tools with a wide range of applications in machine learning:\n• None Classification: Making predictions about categorical results, like if an email is spam or not.\n• None Regression: The estimation of continuous values; for example, feature-based home price prediction.\n• None Feature Selection: Feature selection lowers dimensionality and boosts model performance by determining which features are most pertinent to a certain job.\n\nPython decision trees provide a strong and comprehensible method for handling machine learning tasks. They are an invaluable tool for a variety of applications because of their ease of use, efficiency, and capacity to handle both numerical and categorical data. Decision trees are a useful tool for making precise forecasts and insightful analysis when used carefully.\n\n3. How do you implement a decision tree in Python?\n\n4. How do you evaluate the performance of a decision tree?\n\n5. What are some of the challenges of using decision trees?"
    },
    {
        "link": "https://datacamp.com/tutorial/decision-tree-classification-python",
        "document": "Master the basics of data analysis with Python in just four hours. This online course will introduce the Python interface and explore popular packages."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/tree.html",
        "document": "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n\nFor instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.\n\nSome advantages of decision trees are:\n• None Simple to understand and to interpret. Trees can be visualized.\n• None Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Some tree and algorithm combinations support missing values.\n• None The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n• None Able to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialized in analyzing datasets that have only one type of variable. See algorithms for more information.\n• None Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n• None Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n• None Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n• None Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n• None Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n• None Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure. Therefore, they are not good at extrapolation.\n• None The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n• None There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n• None Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n\nis a class capable of performing multi-class classification on a dataset. As with other classifiers, takes as input two arrays: an array X, sparse or dense, of shape holding the training samples, and an array Y of integer values, shape , holding the class labels for the training samples: After being fitted, the model can then be used to predict the class of samples: In case that there are multiple classes with the same and highest probability, the classifier will predict the class with the lowest index amongst those classes. As an alternative to outputting a specific class, the probability of each class can be predicted, which is the fraction of training samples of the class in a leaf: is capable of both binary (where the labels are [-1, 1]) classification and multiclass (where the labels are [0, …, K-1]) classification. Using the Iris dataset, we can construct a tree as follows: Once trained, you can plot the tree with the function: We can also export the tree in Graphviz format using the exporter. If you use the conda package manager, the graphviz binaries and the python package can be installed with . Alternatively binaries for graphviz can be downloaded from the graphviz project homepage, and the Python wrapper installed from pypi with . Below is an example graphviz export of the above tree trained on the entire iris dataset; the results are saved in an output file : The exporter also supports a variety of aesthetic options, including coloring nodes by their class (or value for regression) and using explicit variable and class names if desired. Jupyter notebooks also render these plots inline automatically: Alternatively, the tree can also be exported in textual format with the function . This method doesn’t require the installation of external libraries and is more compact:\n• None Plot the decision surface of decision trees trained on the iris dataset\n\nA multi-output problem is a supervised learning problem with several outputs to predict, that is when Y is a 2d array of shape . When there is no correlation between the outputs, a very simple way to solve this kind of problem is to build n independent models, i.e. one for each output, and then to use those models to independently predict each one of the n outputs. However, because it is likely that the output values related to the same input are themselves correlated, an often better way is to build a single model capable of predicting simultaneously all n outputs. First, it requires lower training time since only a single estimator is built. Second, the generalization accuracy of the resulting estimator may often be increased. With regard to decision trees, this strategy can readily be used to support multi-output problems. This requires the following changes:\n• None Store n output values in leaves, instead of 1;\n• None Use splitting criteria that compute the average reduction across all n outputs. This module offers support for multi-output problems by implementing this strategy in both and . If a decision tree is fit on an output array Y of shape then the resulting estimator will:\n• None Output a list of n_output arrays of class probabilities upon . The use of multi-output trees for regression is demonstrated in Decision Tree Regression. In this example, the input X is a single real value and the outputs Y are the sine and cosine of X. The use of multi-output trees for classification is demonstrated in Face completion with a multi-output estimators. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.\n• None M. Dumont et al, Fast multi-class image annotation with random subwindows and multiple output randomized trees, International Conference on Computer Vision Theory and Applications 2009\n• None Decision trees tend to overfit on data with a large number of features. Getting the right ratio of samples to number of features is important, since a tree with few samples in high dimensional space is very likely to overfit.\n• None Consider performing dimensionality reduction (PCA, ICA, or Feature selection) beforehand to give your tree a better chance of finding features that are discriminative.\n• None Understanding the decision tree structure will help in gaining more insights about how the decision tree makes predictions, which is important for understanding the important features in the data.\n• None Visualize your tree as you are training by using the function. Use as an initial tree depth to get a feel for how the tree is fitting to your data, and then increase the depth.\n• None Remember that the number of samples required to populate the tree doubles for each additional level the tree grows to. Use to control the size of the tree to prevent overfitting.\n• None Use or to ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. While can create arbitrarily small leaves, guarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems. For classification with few classes, is often the best choice. Note that considers samples directly and independent of , if provided (e.g. a node with m weighted samples is still treated as having exactly m samples). Consider or if accounting for sample weights is required at splits.\n• None Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights ( ) for each class to the same value. Also note that weight-based pre-pruning criteria, such as , will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like .\n• None If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as , which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights.\n• None All decision trees use arrays internally. If training data is not in this format, a copy of the dataset will be made.\n• None If the input matrix X is very sparse, it is recommended to convert to sparse before calling fit and sparse before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples.\n\nWhat are all the various decision tree algorithms and how do they differ from each other? Which one is implemented in scikit-learn? ID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalize to unseen data. C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. The accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it. C5.0 is Quinlan’s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate. CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node. scikit-learn uses an optimized version of the CART algorithm; however, the scikit-learn implementation does not support categorical variables for now.\n\nGiven training vectors \\(x_i \\in R^n\\), i=1,…, l and a label vector \\(y \\in R^l\\), a decision tree recursively partitions the feature space such that the samples with the same labels or similar target values are grouped together. Let the data at node \\(m\\) be represented by \\(Q_m\\) with \\(n_m\\) samples. For each candidate split \\(\\theta = (j, t_m)\\) consisting of a feature \\(j\\) and threshold \\(t_m\\), partition the data into \\(Q_m^{left}(\\theta)\\) and \\(Q_m^{right}(\\theta)\\) subsets The quality of a candidate split of node \\(m\\) is then computed using an impurity function or loss function \\(H()\\), the choice of which depends on the task being solved (classification or regression) Select the parameters that minimises the impurity Recurse for subsets \\(Q_m^{left}(\\theta^*)\\) and \\(Q_m^{right}(\\theta^*)\\) until the maximum allowable depth is reached, \\(n_m < \\min_{samples}\\) or \\(n_m = 1\\). If a target is a classification outcome taking on values 0,1,…,K-1, for node \\(m\\), let be the proportion of class k observations in node \\(m\\). If \\(m\\) is a terminal node, for this region is set to \\(p_{mk}\\). Common measures of impurity are the following. The entropy criterion computes the Shannon entropy of the possible classes. It takes the class frequencies of the training data points that reached a given leaf \\(m\\) as their probability. Using the Shannon entropy as tree node splitting criterion is equivalent to minimizing the log loss (also known as cross-entropy and multinomial deviance) between the true labels \\(y_i\\) and the probabilistic predictions \\(T_k(x_i)\\) of the tree model \\(T\\) for class \\(k\\). To see this, first recall that the log loss of a tree model \\(T\\) computed on a dataset \\(D\\) is defined as follows: where \\(D\\) is a training dataset of \\(n\\) pairs \\((x_i, y_i)\\). In a classification tree, the predicted class probabilities within leaf nodes are constant, that is: for all \\((x_i, y_i) \\in Q_m\\), one has: \\(T_k(x_i) = p_{mk}\\) for each class \\(k\\). This property makes it possible to rewrite \\(\\mathrm{LL}(D, T)\\) as the sum of the Shannon entropies computed for each leaf of \\(T\\) weighted by the number of training data points that reached each leaf: If the target is a continuous value, then for node \\(m\\), common criteria to minimize as for determining locations for future splits are Mean Squared Error (MSE or L2 error), Poisson deviance as well as Mean Absolute Error (MAE or L1 error). MSE and Poisson deviance both set the predicted value of terminal nodes to the learned mean value \\(\\bar{y}_m\\) of the node whereas the MAE sets the predicted value of terminal nodes to the median \\(median(y)_m\\). Setting might be a good choice if your target is a count or a frequency (count per some unit). In any case, \\(y >= 0\\) is a necessary condition to use this criterion. Note that it fits much slower than the MSE criterion. For performance reasons the actual implementation minimizes the half mean poisson deviance, i.e. the mean poisson deviance divided by 2. Note that it fits much slower than the MSE criterion.\n\n, have built-in support for missing values using , where the splits are determined in a greedy fashion. , and have built-in support for missing values for , where the splits are determined randomly. For more details on how the splitter differs on non-missing values, see the Forest section. The criterion supported when there are missing-values are , ’, or , for classification or , , or for regression. First we will describe how , handle missing-values in the data. For each potential threshold on the non-missing data, the splitter will evaluate the split with all the missing values going to the left node or the right node. Decisions are made as follows:\n• None By default when predicting, the samples with missing values are classified with the class used in the split found during training:\n• None If the criterion evaluation is the same for both nodes, then the tie for missing value at predict time is broken by going to the right node. The splitter also checks the split where all the missing values go to one child and non-missing values go to the other:\n• None If no missing values are seen during training for a given feature, then during prediction missing values are mapped to the child with the most samples: , and handle missing values in a slightly different way. When splitting a node, a random threshold will be chosen to split the non-missing values on. Then the non-missing values will be sent to the left and right child based on the randomly selected threshold, while the missing values will also be randomly sent to the left or right child. This is repeated for every feature considered at each split. The best split among these is chosen. During prediction, the treatment of missing-values is the same as that of the decision tree:\n• None By default when predicting, the samples with missing values are classified with the class used in the split found during training.\n• None If no missing values are seen during training for a given feature, then during prediction missing values are mapped to the child with the most samples.\n\nMinimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting, described in Chapter 3 of [BRE]. This algorithm is parameterized by \\(\\alpha\\ge0\\) known as the complexity parameter. The complexity parameter is used to define the cost-complexity measure, \\(R_\\alpha(T)\\) of a given tree \\(T\\): where \\(|\\widetilde{T}|\\) is the number of terminal nodes in \\(T\\) and \\(R(T)\\) is traditionally defined as the total misclassification rate of the terminal nodes. Alternatively, scikit-learn uses the total sample weighted impurity of the terminal nodes for \\(R(T)\\). As shown above, the impurity of a node depends on the criterion. Minimal cost-complexity pruning finds the subtree of \\(T\\) that minimizes \\(R_\\alpha(T)\\). The cost complexity measure of a single node is \\(R_\\alpha(t)=R(t)+\\alpha\\). The branch, \\(T_t\\), is defined to be a tree where node \\(t\\) is its root. In general, the impurity of a node is greater than the sum of impurities of its terminal nodes, \\(R(T_t)<R(t)\\). However, the cost complexity measure of a node, \\(t\\), and its branch, \\(T_t\\), can be equal depending on \\(\\alpha\\). We define the effective \\(\\alpha\\) of a node to be the value where they are equal, \\(R_\\alpha(T_t)=R_\\alpha(t)\\) or \\(\\alpha_{eff}(t)=\\frac{R(t)-R(T_t)}{|T|-1}\\). A non-terminal node with the smallest value of \\(\\alpha_{eff}\\) is the weakest link and will be pruned. This process stops when the pruned tree’s minimal \\(\\alpha_{eff}\\) is greater than the parameter. L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984."
    },
    {
        "link": "https://usdsi.org/data-science-insights/a-comprehensive-guide-on-implementing-decision-trees-in-python",
        "document": "Mastering a programming language is an easier way to earn the most popular role in data sciences. It is a clever way to give computers instructions on what to do and Python is ranked highly among programming languages.\n\nPython is unique because it is effortless to learn and use, especially compared to other programming languages. That is why many people like to use it, especially those just starting with programming.\n\nPython is a universal language. One of the key benefits of Python is that it can be used for a wide range of tasks. You can build websites, analyze data, create artificial intelligence models, and more.\n\nPython is perfect for doing things in machine learning, which is all about teaching computers to make decisions based on data.\n\nAnd one of the ways that computers can make decisions is through decision trees. It is like a flowchart of yes/no questions that help the laptop reach a final decision. You can build and use decision trees with the help of Python.\n\nPython provides the tools and functionality for implementing decision trees straightforwardly and efficiently. You can use Python to construct decision trees and make predictions regardless of your skill level.\n\nThese are excellent tools that data scientists use to make predictions and classify data. They create a flowchart-like structure that asks a series of yes or no questions to reach a final decision. And the great thing about decision trees is that they are intuitive and easy to understand, you can see how different variables are connected and how they impact the outcome.\n\nDecision trees are so important in data science that they are often included in certification programs. So, it is no surprise that people use Python to implement decision trees.\n\nAnd since Python is such a widely used language in the data science community, you will have plenty of resources to help you. So, what are you waiting for? Start learning about decision trees and Python today!\n\nAre you interested in learning about Decision Trees and Python? You may get closer to your career goal by:\n• Online Courses: You can sign up for courses on Coursera, Udemy, and edX. These courses usually cover topics like data science, machine learning, and Python programming, and they offer the flexibility to learn at a pace that suits you and on a schedule that works for you.\n• Textbooks: If you prefer learning from books, there are plenty of great options. You can check out your local library or bookstore or buy one online. Look for books on data science, machine learning, or Python programming that cover decision trees.\n• Practice: The best way to learn is by doing, so try to practice what you have learned by building your Decision Trees in Python. You can start with simple projects and work up to more complex ones.\n• Join a community: Participating in online communities or forums centered around data science, machine learning, or Python programming can be an effective way to seek guidance, ask questions, and learn from others with similar interests.\n\nBeing one of the most widely used programming languages, Python provides ample resources for data science professionals to implement decision trees.\n\nDecision trees are algorithms that use a tree-like model to make decisions based on data. It is composed of branches and leaves, and each unit represents a decision that needs to be made.\n\nThe leaves represent the outcome of the decisions made along the way. Decision Trees classify or predict outcomes based on various input variables. Before implementing decision trees in Python, it is essential to understand what they are and how they work.\n\nData science professionals use decision trees for various tasks, including regression, classification, and association rule mining. They are instrumental in machine learning algorithms because they can easily interpret and handle categorical and numerical data.\n\nAdditionally, decision trees are a non-parametric algorithm, meaning they do not require assumptions about the data distribution.\n\nThere are several libraries available when it comes to implementing decision trees in Python. Python's most popular libraries for decision tree implementation include sci-kit-learn, xgboost, and light bum.\n\nEvery library has strengths and limitations, and data science professionals must choose the library that best fits their needs.\n\nIn sci-kit-learn, decision trees are implemented using the Decision Tree Classifier or Decision Tree Regressor classes.\n\nThese classes provide a convenient and straightforward way for Data Science Professionals to implement decision trees in Python.\n\nSci-kit-learn offers various tools for evaluating the performance of decision trees, including cross-validation, confusion matrices, and ROC curves.\n\nXgboost and light bum are libraries designed explicitly for gradient boosting, a popular machine learning algorithm that uses decision trees as its base learner.\n\nThese libraries are known for their speed and accuracy and are popular among data science professionals for their ability to handle large datasets.\n\nIn addition to the libraries mentioned above, many tutorials and courses are available for data science professionals looking to implement decision trees in Python.\n\nMany Data Science Certification Programs cover decision trees and offer hands-on exercises and projects to help data science professionals build their skills.\n\nDecision Trees are a fundamental aspect of data science and machine learning, and Python provides many resources for Data Science Professionals to implement decision trees.\n\nWhether using sci-kit-learn, xgboost, light bum, or another library, data science professionals have many options for implementing decision trees in Python. By combining their knowledge of decision trees with the power of Python, expert professionals can make informed decisions and extract valuable insights from data."
    },
    {
        "link": "https://medium.com/@enozeren/building-a-decision-tree-from-scratch-324b9a5ed836",
        "document": "The history of a decision tree goes back to 1950s. William Welson has introduced some kind of a tree structure in 1959 in his “Matching and prediction on the principle of biological classification” paper. After than structured algorithms were proposed by Ross Quinlan (ID3, C4.5) and Leo Breiman (CART) in 1980s. After 90s, some scientist proposed more advanced algorithms which are built upon simple decision trees. In this article we’ll be using the CART algorithm.\n\nA tree is basicly structured as the Image 1. In this implementation we will build a decision tree classifier. Therefore, the output of the tree will be a categorical variable.\n\nNOTE: To see the full code, visit the github code by clicking here. In this article we won’t go over all the code.\n\nTo create our tree from scratch first we create a class called DecisionTree in python. To train our tree we will develop a “train” function and after training to predict an output we will develop a “predict” function.\n\nIn the Image 2 above, we see the functions we need for our class. We will go through the important ones together.\n\nThe main idea for training a decision tree is about how you will split your data. The best split is basically a split where after spliting each split has data points where every data point has the same class. We should define a numerical metric that gives this property. Even though the “Gini Impurity” is the default metric used for splitting in the CART algorithm, in this implementation we will use entropy metric. Entropy is calculated with the function below (Grus, 2019)[1]:\n\nThis metric will give us a low entropy if the splitted data group have a dominant class, otherwise it will give us a high entropy (see the Image 3 below).\n\nWe will be searching for splits which have the lowest entropy. Basically “find_best_split” function is built upon the entropy metric.\n\nNow when we see 2 split we can tell which one is better by looking at the entropy score (lower is the better). For example when you have 2 features with numerical values, and you can compare splitting with feature 1 median value and feature 2 median value and check the entropies of resulting groups. If splitting with feature 1 yields lower entropy then you go with feature 1. After this split in each level of the tree you use this algorithm to decide with which feature to split. This approach is a greedy approach since we choose the best split in each level of the tree. Greedy approach has some disadvantages and some solutions for those disadvantages are proposed but in this implementation we stick with the greedy approach for the sake of simplicity.\n\nHere is our function for finding the best split.\n\nTo create a tree we need a basic node structure where it has left and right nodes. Therefore, we create the “TreeNode” class below where we will store the split information and the relation to the left and right nodes.\n\nNow we have everything we need for creating a tree. We can write a recursive function which creates our tree and eventually stops if stopping criterions satisfied.\n\nAs we have seen in the “create_tree” function, the splitting is performed recursively, therefore we need to tell the function when to stop splitting. The criterions for that is called stopping criterions. In this implementations we have used 3 stopping criteria.\n• The depth of the tree (max_depth)\n• The minimum acceptable number of samples in the leaf after split (min_samples_leaf)\n\nThese stopping criterions are defined when we create the DecisionTree object. See the __init__ function for that.\n\nSince we have all the functions we need for training we can take a look at the train function. It only takes the training sets, starts the recursive creating tree process and stores the first node of the tree (a.k.a. root).\n\nWhen making the prediction, we will calculate the probability of the sample belonging to any class. Each leaf has constant probabilities for each label and those probabilities are learned in the training phase. To make a new prediction we will take the unlabelled data and start instructions from the first node (root) and follow the path which the unlabelled data satisfies. We can do it by using a while loop which stops when there is no other node to go (or in another words reaching the leaf node).\n\nAfter predicting the probabilities, the “predict” functions returns the most probable class as the prediction."
    },
    {
        "link": "https://geeksforgeeks.org/decision-tree-implementation-python",
        "document": "Decision Tree is one of the most powerful and popular algorithms. Python Decision-tree algorithm falls under the category of supervised learning algorithms. It works for both continuous as well as categorical output variables. In this article, We are going to implement a Decision tree in Python algorithm on the Balance Scale Weight & Distance Database presented on the UCI.\n\nA Decision tree is a tree-like structure that represents a set of decisions and their possible consequences. Each node in the tree represents a decision, and each branch represents an outcome of that decision. The leaves of the tree represent the final decisions or predictions.\n\nDecision trees are created by recursively partitioning the data into smaller and smaller subsets. At each partition, the data is split based on a specific feature, and the split is made in a way that maximizes the information gain.\n\nIn the above figure, decision tree is a flowchart-like tree structure that is used to make decisions. It consists of Root Node(WINDY), Internal nodes(OUTLOOK, TEMPERATURE), which represent tests on attributes, and leaf nodes, which represent the final decisions. The branches of the tree represent the possible outcomes of the tests.\n• Root Node: The decision tree’s starting node, which stands for the complete dataset.\n• Branch Nodes: Internal nodes that represent decision points, where the data is split based on a specific attribute.\n• Decision Rules: Rules that govern the splitting of data at each branch node.\n• Attribute Selection: The process of choosing the most informative attribute for each split.\n• Splitting Criteria: Metrics like information gain, entropy, or the Gini Index are used to calculate the optimal split.\n\nAssumptions we make while using Decision tree\n• None At the beginning, we consider the whole training set as the root.\n• None Attributes are assumed to be categorical for information gain and for gini index, attributes are assumed to be continuous.\n• None On the basis of attribute values records are distributed recursively.\n• None We use statistical methods for ordering attributes as root or internal node.\n\nGini index and information gain both of these methods are used to select from the n attributes of the dataset which attribute would be placed at the root node or the internal node.\n• None Gini Index is a metric to measure how often a randomly chosen element would be incorrectly identified.\n• None It means an attribute with lower gini index should be preferred.\n• None Sklearn supports “gini” criteria for Gini Index and by default, it takes “gini” value.\n\nIf a random variable x can take N different value, the i’value [Tex]x_{i} [/Tex] with probability [Tex]p_{ii} [/Tex] we can associate the following entropy with x :\n• None Entropy is the measure of uncertainty of a random variable, it characterizes the impurity of an arbitrary collection of examples. The higher the entropy the more the information content.\n\nDefinition: Suppose S is a set of instances, A is an attribute, [Tex]S_{v} [/Tex] is the subset of s with A = v and Values(A) is the set of all possible of A, then\n• None The entropy typically changes when we use a node in a Python decision tree to partition the training instances into smaller subsets. Information gain is a measure of this change in entropy.\n• None Sklearn supports “entropy” criteria for Information Gain and if we want to use Information Gain method in sklearn then we have to mention it explicitly.\n\nTitle : Balance Scale Weight & Distance Database Number of Instances : 625 (49 balanced, 288 left, 288 right) Number of Attributes : 4 (numeric) + class name = 5 Attribute Information: 1. Class Name (Target variable): 3 L [balance scale tip to the left] B [balance scale be balanced] R [balance scale tip to the right] 2. Left-Weight: 5 (1, 2, 3, 4, 5) 3. Left-Distance: 5 (1, 2, 3, 4, 5) 4. Right-Weight: 5 (1, 2, 3, 4, 5) 5. Right-Distance: 5 (1, 2, 3, 4, 5) Missing Attribute Values: None Class Distribution: 1. 46.08 percent are L 2. 07.84 percent are B 3. 46.08 percent are R\n\nYou can find more details of the dataset.\n• sklearn :\n• None In python, sklearn is a machine learning package which include a lot of ML algorithms.\n• None Here, we are using some of its modules like train_test_split, DecisionTreeClassifier and accuracy_score.\n• NumPy :\n• None It is a numeric python module which provides fast maths functions for calculations.\n• None It is used to read data in numpy arrays and for manipulation purpose.\n• Pandas :\n• None Used to read and write different files.\n• None Data manipulation can be done easily with dataframes.\n\nIn Python, sklearn is the package which contains all the required packages to implement Machine learning algorithm. You can install the sklearn package by following the commands given below.\n\nBefore using the above command make sure you have scipy and numpy packages installed. If you don’t have pip. You can install it using\n\nWhile implementing the decision tree in Python we will go through the following two phases:\n• None\n• None Split the dataset from train and test using Python sklearn package.\n• None To import and manipulate the data we are using the pandas\n• None Here, we are using a URL which is directly fetching the dataset from the UCI site no need to download the dataset. When you try to run this code on your system make sure the system should have an active Internet connection.\n• None As the dataset is separated by “,” so we have to pass the sep parameter’s value as “,”.\n• None Another thing is notice is that the dataset doesn’t contain the header so we will pass the Header parameter’s value as none. If we will not pass the header parameter then it will consider the first line of the dataset as the header.\n• None Before training the model we have to split the dataset into the training and testing dataset.\n• None To split the dataset for training and testing we are using the sklearn module train_test_split\n• None First of all we have to separate the target variable from the attributes in the dataset.\n• None Above are the lines from the code which separate the dataset. The variable X contains the attributes while the variable Y contains the target variable of the dataset.\n• None Next step is to split the dataset for training and testing purpose.\n• None Above line split the dataset for training and testing. As we are splitting the dataset in a ratio of 70:30 between training and testing so we are pass test_size\n• random_state variable is a pseudo-random number generator state used for random sampling.\n\nBelow is the code for the sklearn decision tree in Python.\n\nImporting the necessary libraries required for the implementation of decision tree in Python.\n• : function, which is responsible for splitting the dataset into training and testing sets. It separates the target variable (class labels) from the features and splits the data using the function from scikit-learn. It sets the test size to 30% and uses a random state of 100 for reproducibility.\n• : function, which is responsible for training a decision tree classifier using the Gini index as the splitting criterion. It creates a classifier object with the specified parameters (criterion, random state, max depth, min samples leaf) and trains it on the training data.\n• : function, which is responsible for training a decision tree classifier using entropy as the splitting criterion. It creates a classifier object with the specified parameters (criterion, random state, max depth, min samples leaf) and trains it on the training data.\n• : function, which is responsible for making predictions on the test data using the trained classifier object. It passes the test data to the classifier’s\n• : function, which is responsible for calculating the accuracy of the predictions. It calculates and prints the confusion matrix, accuracy score, and classification report, providing detailed performance evaluation.\n\nBy using function from the submodule to plot the decision tree. The function takes the following arguments:\n• None : This argument fills the nodes of the tree with different colors based on the predicted class majority.\n• None : This argument provides the names of the features used in the decision tree.\n• None : This argument provides the names of the different classes.\n• None : This argument rounds the corners of the nodes for a more aesthetically pleasing appearance.\n\nThis defines two decision tree classifiers, training and visualization of decision trees based on different splitting criteria, one using the Gini index and the other using entropy,\n\nIt performs the operational phase of the decision tree model, which involves:\n• None Imports and splits data for training and testing.\n• None Uses Gini and entropy criteria to train two decision trees.\n• None Generates class labels for test data using each model.\n• None Calculates and compares accuracy of both models.\n\nEvaluates the performance of the trained decision trees on the unseen test data and provides insights into their effectiveness for the specific classification task and evaluates their performance on a dataset using the confusion matrix, accuracy score, and classification report.\n\nPython Decision trees are versatile tools with a wide range of applications in machine learning:\n• None Classification: Making predictions about categorical results, like if an email is spam or not.\n• None Regression: The estimation of continuous values; for example, feature-based home price prediction.\n• None Feature Selection: Feature selection lowers dimensionality and boosts model performance by determining which features are most pertinent to a certain job.\n\nPython decision trees provide a strong and comprehensible method for handling machine learning tasks. They are an invaluable tool for a variety of applications because of their ease of use, efficiency, and capacity to handle both numerical and categorical data. Decision trees are a useful tool for making precise forecasts and insightful analysis when used carefully.\n\n3. How do you implement a decision tree in Python?\n\n4. How do you evaluate the performance of a decision tree?\n\n5. What are some of the challenges of using decision trees?"
    },
    {
        "link": "https://datacamp.com/tutorial/decision-tree-classification-python",
        "document": "Master the basics of data analysis with Python in just four hours. This online course will introduce the Python interface and explore popular packages."
    },
    {
        "link": "https://medium.com/@bagusmurdyantoro1997/building-decision-tree-algorithm-from-scratch-in-python-4adc26ba1b57",
        "document": "A decision tree is a model that divides the data into subsets based on the value of input features. This division results in a tree structure where each internal node represents a decision based on one of the input features, each branch represents an outcome of that decision, and each leaf node represents a final output or decision (a class label for classification or a continuous value for regression). Let’s understand the decision tree structure with a simple example, which decides whether to play tennis based on weather conditions. The structure of this decision tree can be visualized as follows: 1. Root Node (Root Node — “Weather Conditions”) : This is the top most node of the tree where The decision-making process begins here, focusing on overall weather conditions. 2. Decision Nodes: These are nodes where the futher decisions are made based on the different attributes.\n• Decision Node 1 — “Is it raining?”: This node checks if it’s raining to decide the next steps. This decision node come after the root node of decision, Determines the next steps based on rainfall.\n• Decision Node 2 — “Is it windy?”: Checks wind conditions as the next factor. 3. Leaf Nodes — Outcomes Based on the Decision Tree: These are the final nodes of the tree that provide the decision or outcome that do not splits further.\n• Branch No (Not Windy): Leaf Node: “Play Tennis” — Favorable conditions for tennis, neither rainy nor windy. 4. Branches: These are the links between nodes, representing the outcome of a decision. Branches represent the flow of decisions from each node. Example: From “Is it raining?”, branches lead directly to a decision of not playing tennis if it’s raining. In this decision tree, each decision or node effectively narrows down the conditions, ultimately leading to a clear decision regarding playing tennis based on the weather.\n\nDecision trees use various metrics to decide how to split the data. These metrics are crucial in determining the best way to divide the dataset at each step of the tree-building process. To find the best split at each node, we iterate through each feature and its possible thresholds, calculating the impurity reduction. The best split is the one that results in the highest decrease in impurity. The algorithm selects the best feature and threshold for splitting based on impurity measures. The objective function is to maximize the impurity decrease (for classification) or minimize the error (for regression). The cost objective/function in a decision tree is to create splits that result in the most homogenous sub-nodes. For classification, this could be minimizing Gini impurity or entropy, and for regression, minimizing the mean squared error. Used in classification, Gini Impurity measures how often a randomly chosen element would be incorrectly classified. It is calculated using the formula: where ( p_i ) is the proportion of samples that belong to class ( i ). Entropy is another measure for classification. It indicates the disorder or uncertainty in the data. The formula for entropy is: where ( p_i ) is again the proportion of samples that belong to class ( i ). Used in regression, Mean Squared Error (MSE) indicates the average of the squares of the differences between the actual values and the predicted values. It is given by: where ( y_i ) is the actual value and ( \\hat{y_i} ) is the predicted value, and ( n ) is the number of samples. These metrics are fundamental in guiding the decision-making process within a decision tree, helping to determine the most informative features and splits.\n\nWe’re going to explore a decision tree through pseudocode. Pseudocode isn’t actual code but more like a blueprint. It’s an easy way to understand the steps in building a decision tree without getting into complex code details. Imagine it as a straightforward map guiding us through the process. Pseudocode for Decision Tree Algorithm:\n\n\n\n1. Initialize the Decision Tree\n\n Define a DecisionTree class with parameters:\n\n - max_depth: Maximum depth of the tree\n\n - min_samples_split: Minimum samples required to split a node\n\n - min_samples_leaf: Minimum samples required at a leaf node\n\n - impurity: Type of impurity measure (e.g., Gini, Entropy)\n\n Initialize root node to None\n\n\n\n2. Define Node Structure\n\n Define a Node class with attributes:\n\n - feature_index: Index of the feature used for splitting\n\n - threshold: Value to split the feature on\n\n - left: Left child node\n\n - right: Right child node\n\n - value: Prediction value at the node\n\n - is_leaf: Boolean indicating if the node is a leaf\n\n\n\n3. Calculate Impurity\n\n Define functions to calculate Gini, Entropy, or MSE based on the impurity parameter.\n\n\n\n4. Find the Best Split\n\n Define a function to find the best split:\n\n - For each feature in the dataset:\n\n - For each unique value of the feature as a threshold:\n\n - Split the dataset into two subsets\n\n - Calculate the impurity of each subset\n\n - Calculate the information gain\n\n - Choose the feature and threshold with the highest information gain\n\n\n\n5. Build the Tree\n\n Define a function to build the tree:\n\n - If stopping criteria are met (max depth, min samples), create a leaf node\n\n - Otherwise, find the best split and recursively build left and right subtrees\n\n\n\n6. Fit the Model\n\n Define a fit function:\n\n - Call the build tree function starting from the root with the entire dataset\n\n\n\n7. Predict\n\n Define a predict function:\n\n - For each instance in the input:\n\n - Traverse the tree based on feature values\n\n - Return the prediction at the leaf node\n\n\n\n8. Traverse the Tree for Prediction\n\n Define a function to traverse the tree for a single instance prediction.\n\n\n\n9. Prune the Tree\n\n Define a pruning function:\n\n - For each node in the tree, starting from the leaves:\n\n - If pruning the node (turning it into a leaf) reduces the complexity of the tree without significantly increasing error, then prune the node\n\n - The pruning decision is based on a complexity parameter (alpha) and validation data, if available.\n\n\n\nEnd of Decision Tree Algorithm\n\nImplementing a decision tree in Python involves understanding several key concepts and translating them into code. Let’s break down the process: A decision tree is composed of nodes and branches, where each node represents a decision point based on a feature, and branches represent the outcomes of these decisions. In Python, each node can be represented as an instance of a class. This class contains attributes like (the feature on which to split), (the value to split the feature on), and (child nodes), and (the prediction at this node, used in leaf nodes). The class is responsible for the logic of building the tree and making predictions. Key Attributes :\n• : The maximum depth of the tree.\n• : Minimum number of samples required to consider a split.\n• : Minimum number of samples that a leaf node must have.\n• : The measure used to calculate the quality of a split (e.g., Gini, Entropy). One of the crucial steps in building the tree is deciding how to split the data. First, calculate the impurity of the current node. This is done using methods like Gini or Entropy for classification, and Mean Squared Error for regression. def _calculate_gini(self, y):\n\n _, counts = np.unique(y, return_counts=True)\n\n probabilities = counts / counts.sum()\n\n return 1 - np.sum(probabilities**2)\n\n\n\ndef _calculate_entropy(self, y):\n\n _, counts = np.unique(y, return_counts=True)\n\n probabilities = np.clip(counts / counts.sum(), 1e-10, 1)\n\n return -np.sum(probabilities * np.log2(probabilities))\n\n\n\ndef _calculate_mse(self, y):\n\n return np.mean((y - np.mean(y)) ** 2)\n\n\n\ndef _calculate_impurity(self, y):\n\n if self.task == 'classification':\n\n if self.impurity == 'gini':\n\n return self._calculate_gini(y)\n\n elif self.impurity == 'entropy':\n\n return self._calculate_entropy(y)\n\n else:\n\n raise ValueError(f\"Unknown impurity criterion: {self.impurity}\")\n\n elif self.task == 'regression':\n\n if self.impurity == 'mse':\n\n return self._calculate_mse(y)\n\n else:\n\n raise ValueError(f\"Unknown task: {self.task}\") Iterate over each feature and its potential thresholds to find the split that results in the highest reduction in impurity. def _best_split(self, X, y):\n\n n_samples = len(y)\n\n best_feature, best_threshold, best_gain = None, None, float('-inf')\n\n current_impurity = self._calculate_impurity(y)\n\n for feature_idx in range(self.n_features):\n\n thresholds = np.unique(X[:, feature_idx])\n\n for threshold in thresholds:\n\n X_left, y_left, X_right, y_right = self._split_data(X, y, feature_idx, threshold)\n\n if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n\n continue\n\n left_impurity = self._calculate_impurity(y_left)\n\n right_impurity = self._calculate_impurity(y_right)\n\n weighted_impurity = (len(y_left) * left_impurity + len(y_right) * right_impurity) / n_samples\n\n impurity_gain = current_impurity - weighted_impurity\n\n if impurity_gain > best_gain and impurity_gain >= self.min_impurity_decrease:\n\n best_feature = feature_idx\n\n best_threshold = threshold\n\n best_gain = impurity_gain\n\n return best_feature, best_threshold Recursively split the data, creating child nodes until the stopping criteria are met (max depth, min samples, etc). def _build_tree(self, X, y, depth=0):\n\n n_samples = len(y)\n\n node_impurity = self._calculate_impurity(y)\n\n node_value = self._calculate_leaf_value(y)\n\n node = Node(value=node_value, impurity=node_impurity, is_leaf=True, n_samples=n_samples)\n\n\n\n if (self.max_depth is None or depth < self.max_depth) and len(y) >= self.min_samples_split and len(np.unique(y)) > 1:\n\n best_feature, best_threshold = self._best_split(X, y)\n\n if best_feature is not None:\n\n X_left, y_left, X_right, y_right = self._split_data(X, y, best_feature, best_threshold)\n\n left_node = self._build_tree(X_left, y_left, depth + 1)\n\n right_node = self._build_tree(X_right, y_right, depth + 1)\n\n\n\n node.feature = best_feature\n\n node.threshold = best_threshold\n\n node.left = left_node\n\n node.right = right_node\n\n node.is_leaf = False\n\n\n\n return node To make predictions, traverse the tree from the root to a leaf, following the decision path determined by the splits. def _predict_one(self, x):\n\n node = self.root\n\n while node is not None and not node.is_leaf:\n\n if x[node.feature] <= node.threshold:\n\n node = node.left\n\n else:\n\n node = node.right\n\n return node.value if node is not None else None\n\n\n\ndef predict(self, X):\n\n X = np.array(X)\n\n return np.array([self._predict_one(sample) for sample in X])"
    },
    {
        "link": "https://stackoverflow.com/questions/2358045/how-can-i-implement-a-tree-in-python",
        "document": "How can I implement a general tree in Python? Is there a built-in data structure for this?\n\nPython doesn't have the quite the extensive range of \"built-in\" data structures as Java does. However, because Python is dynamic, a general tree is easy to create. For example, a binary tree might be: class Tree: def __init__(self): self.left = None self.right = None self.data = None You can use it like this: If you need an arbitrary number of children per node, then use a list of children:\n\nI've implemented trees using nested dicts. It is quite easy to do, and it has worked for me with pretty large data sets. I've posted a sample below, and you can see more at Google code def addBallotToTree(self, tree, ballotIndex, ballot=\"\"): \"\"\"Add one ballot to the tree. The root of the tree is a dictionary that has as keys the indicies of all continuing and winning candidates. For each candidate, the value is also a dictionary, and the keys of that dictionary include \"n\" and \"bi\". tree[c][\"n\"] is the number of ballots that rank candidate c first. tree[c][\"bi\"] is a list of ballot indices where the ballots rank c first. If candidate c is a winning candidate, then that portion of the tree is expanded to indicate the breakdown of the subsequently ranked candidates. In this situation, additional keys are added to the tree[c] dictionary corresponding to subsequently ranked candidates. tree[c][\"n\"] is the number of ballots that rank candidate c first. tree[c][\"bi\"] is a list of ballot indices where the ballots rank c first. tree[c][d][\"n\"] is the number of ballots that rank c first and d second. tree[c][d][\"bi\"] is a list of the corresponding ballot indices. Where the second ranked candidates is also a winner, then the tree is expanded to the next level. Losing candidates are ignored and treated as if they do not appear on the ballots. For example, tree[c][d][\"n\"] is the total number of ballots where candidate c is the first non-losing candidate, c is a winner, and d is the next non-losing candidate. This will include the following ballots, where x represents a losing candidate: [c d] [x c d] [c x d] [x c x x d] During the count, the tree is dynamically updated as candidates change their status. The parameter \"tree\" to this method may be the root of the tree or may be a sub-tree. \"\"\" if ballot == \"\": # Add the complete ballot to the tree weight, ballot = self.b.getWeightedBallot(ballotIndex) else: # When ballot is not \"\", we are adding a truncated ballot to the tree, # because a higher-ranked candidate is a winner. weight = self.b.getWeight(ballotIndex) # Get the top choice among candidates still in the running # Note that we can't use Ballots.getTopChoiceFromWeightedBallot since # we are looking for the top choice over a truncated ballot. for c in ballot: if c in self.continuing | self.winners: break # c is the top choice so stop else: c = None # no candidates left on this ballot if c is None: # This will happen if the ballot contains only winning and losing # candidates. The ballot index will not need to be transferred # again so it can be thrown away. return # Create space if necessary. if not tree.has_key(c): tree[c] = {} tree[c][\"n\"] = 0 tree[c][\"bi\"] = [] tree[c][\"n\"] += weight if c in self.winners: # Because candidate is a winner, a portion of the ballot goes to # the next candidate. Pass on a truncated ballot so that the same # candidate doesn't get counted twice. i = ballot.index(c) ballot2 = ballot[i+1:] self.addBallotToTree(tree[c], ballotIndex, ballot2) else: # Candidate is in continuing so we stop here. tree[c][\"bi\"].append(ballotIndex)\n\nIf you are already using the networkx library, then you can implement a tree using that. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. As 'tree' is another term for a (normally rooted) connected acyclic graph, and these are called 'arborescences' in NetworkX. You may want to implement a plane tree (aka ordered tree) where each sibling has a unique rank and this is normally done via labelling the nodes. However, graph language looks different from tree language, and the means of 'rooting' an arborescence is normally done by using a directed graph so, while there are some really cool functions and corresponding visualisations available, it would probably not be an ideal choice if you are not already using networkx. An example of building a tree: The library enables each node to be any hashable object, and there is no constraint on the number of children each node has.\n\nThere are two parts to this question.\n• How to implement a generic tree in Python\n• Is there an inbuilt DS for it? The second question seems to be answered multiple times, so I'll take the first one. I've tried to implement an N-ary Tree structure and some basic operations like, from collections import deque from typing import List, Optional class GenericTree: def __init__(self, data: str): self.data = data self.children = [] self.parent = None def add_child(self, child: 'GenericTree') -> None: \"\"\" Add a child to the current node \"\"\" child.parent = self self.children.append(child) def preorder(self, node: 'GenericTree') -> List[str]: \"\"\" Preorder traversal of the Tree \"\"\" return [] if not node else [node.data] + [j for i in node.children for j in self.preorder(i)] def postorder(self, node: 'GenericTree') -> List[str]: \"\"\" Postorder traversal of the Tree \"\"\" return [] if not node else [j for i in node.children for j in self.postorder(i)] + [node.data] def get_level(self) -> int: \"\"\" Get the depth of the current node from the root \"\"\" level = 0 parent = self.parent while parent: level += 1 parent = parent.parent return level def level_order(self) -> List[List[str]]: \"\"\" Gets nodes per level/ depth from the root \"\"\" q = deque([self]) levels = [] while q: level, level_size = [], len(q) for _ in range(level_size): current = q.popleft() level.append(current.data) q.extend(current.children) if level: levels.append(level) return levels def print_tree(self) -> None: \"\"\" Prints the tree in tree format \"\"\" prefix = ' ' * self.get_level() + \"|__ \" if self.parent else \"\" print(prefix + self.data) for child in self.children: child.print_tree() def dfs_search(self, key: str) -> bool: \"\"\" Search for a given Key in the Tree using DFS \"\"\" if self.data == key: return True for child in self.children: if child.dfs_search(key): return True return False def bfs_search(self, key: str) -> bool: \"\"\" Search for a node with the specified key using BFS. \"\"\" q = deque([self]) while q: current = q.popleft() if current.data == key: return True q.extend(current.children) return False def build_tree() -> GenericTree: \"\"\" Build a generic tree from the given data for demo \"\"\" tree = GenericTree('Electronics') laptop = GenericTree('Laptops') for item in ['iMac', 'Dell', 'PC']: laptop.add_child(GenericTree(item)) cellphone = GenericTree('CellPhones') for item in ['pixel', 'iphone', 'Blackberry']: cellphone.add_child(GenericTree(item)) tv = GenericTree('TV') for item in ['Sony', 'Samsung', 'LG']: tv.add_child(GenericTree(item)) for item in [laptop, cellphone, tv]: tree.add_child(item) return tree if __name__ == '__main__': tree = build_tree() print(\"Preorder: \", tree.preorder(tree)) print(\"--\" * 70) print(\"Postorder: \", tree.postorder(tree)) print(\"--\" * 70) print(\"Level order: \") for level in tree.level_order(): print(level) print(\"--\" * 70) print(\"Print Tree:\") tree.print_tree() print(\"--\" * 70) for search_key in [\"iphone\", \"tesla\"]: print(f\"DFS for '{search_key}': {'FOUND' if tree.dfs_search(search_key) else 'NOT Found'}\") print(\"--\" * 70) for search_key in [\"iphone\", \"tesla\"]: print(f\"BFS for '{search_key}': {'FOUND' if tree.bfs_search(search_key) else 'NOT Found'}\") And the output will look like this Preorder: ['Electronics', 'Laptops', 'iMac', 'Dell', 'PC', 'CellPhones', 'pixel', 'iphone', 'Blackberry', 'TV', 'Sony', 'Samsung', 'LG'] -------------------------------------------------------------------------------------------------------------------------------------------- Postorder: ['iMac', 'Dell', 'PC', 'Laptops', 'pixel', 'iphone', 'Blackberry', 'CellPhones', 'Sony', 'Samsung', 'LG', 'TV', 'Electronics'] -------------------------------------------------------------------------------------------------------------------------------------------- Level order: ['Electronics'] ['Laptops', 'CellPhones', 'TV'] ['iMac', 'Dell', 'PC', 'pixel', 'iphone', 'Blackberry', 'Sony', 'Samsung', 'LG'] -------------------------------------------------------------------------------------------------------------------------------------------- Print Tree: Electronics |__ Laptops |__ iMac |__ Dell |__ PC |__ CellPhones |__ pixel |__ iphone |__ Blackberry |__ TV |__ Sony |__ Samsung |__ LG -------------------------------------------------------------------------------------------------------------------------------------------- DFS for 'iphone': FOUND DFS for 'tesla': NOT Found -------------------------------------------------------------------------------------------------------------------------------------------- BFS for 'iphone': FOUND BFS for 'tesla': NOT Found Process finished with exit code 0\n\nIf you want to create a tree data structure then first you have to create the treeElement object. If you create the treeElement object, then you can decide how your tree behaves. To do this following is the TreeElement class: class TreeElement (object): def __init__(self): self.elementName = None self.element = [] self.previous = None self.elementScore = None self.elementParent = None self.elementPath = [] self.treeLevel = 0 def goto(self, data): for child in range(0, len(self.element)): if (self.element[child].elementName == data): return self.element[child] def add(self): single_element = TreeElement() single_element.elementName = self.elementName single_element.previous = self.elementParent single_element.elementScore = self.elementScore single_element.elementPath = self.elementPath single_element.treeLevel = self.treeLevel self.element.append(single_element) return single_element Now, we have to use this element to create the tree, I am using A* tree in this example. class AStarAgent(Agent): # Initialization Function: Called one time when the game starts def registerInitialState(self, state): return; # GetAction Function: Called with every frame def getAction(self, state): # Sorting function for the queue def sortByHeuristic(each_element): if each_element.elementScore: individual_score = each_element.elementScore[0][0] + each_element.treeLevel else: individual_score = admissibleHeuristic(each_element) return individual_score # check the game is over or not if state.isWin(): print('Job is done') return Directions.STOP elif state.isLose(): print('you lost') return Directions.STOP # Create empty list for the next states astar_queue = [] astar_leaf_queue = [] astar_tree_level = 0 parent_tree_level = 0 # Create Tree from the give node element astar_tree = TreeElement() astar_tree.elementName = state astar_tree.treeLevel = astar_tree_level astar_tree = astar_tree.add() # Add first element into the queue astar_queue.append(astar_tree) # Traverse all the elements of the queue while astar_queue: # Sort the element from the queue if len(astar_queue) > 1: astar_queue.sort(key=lambda x: sortByHeuristic(x)) # Get the first node from the queue astar_child_object = astar_queue.pop(0) astar_child_state = astar_child_object.elementName # get all legal actions for the current node current_actions = astar_child_state.getLegalPacmanActions() if current_actions: # get all the successor state for these actions for action in current_actions: # Get the successor of the current node next_state = astar_child_state.generatePacmanSuccessor(action) if next_state: # evaluate the successor states using scoreEvaluation heuristic element_scored = [(admissibleHeuristic(next_state), action)] # Increase the level for the child parent_tree_level = astar_tree.goto(astar_child_state) if parent_tree_level: astar_tree_level = parent_tree_level.treeLevel + 1 else: astar_tree_level += 1 # create tree for the finding the data astar_tree.elementName = next_state astar_tree.elementParent = astar_child_state astar_tree.elementScore = element_scored astar_tree.elementPath.append(astar_child_state) astar_tree.treeLevel = astar_tree_level astar_object = astar_tree.add() # If the state exists then add that to the queue astar_queue.append(astar_object) else: # Update the value leaf into the queue astar_leaf_state = astar_tree.goto(astar_child_state) astar_leaf_queue.append(astar_leaf_state) You can add/remove any elements from the object, but make the structure intect."
    },
    {
        "link": "https://w3schools.com/python/python_ml_decision_tree.asp",
        "document": "In this chapter we will show you how to make a \"Decision Tree\". A Decision Tree is a Flow Chart, and can help you make decisions based on previous experience.\n\nIn the example, a person will try to decide if he/she should go to a comedy show or not.\n\nLuckily our example person has registered every time there was a comedy show in town, and registered some information about the comedian, and also registered if he/she went or not.\n\nNow, based on this data set, Python can create a decision tree that can be used to decide if any new shows are worth attending to.\n\nHow Does it Work?\n\nFirst, read the dataset with pandas:\n\nTo make a decision tree, all data has to be numerical.\n\nWe have to convert the non numerical columns 'Nationality' and 'Go' into numerical values.\n\nPandas has a method that takes a dictionary with information on how to convert the values.\n\nMeans convert the values 'UK' to 0, 'USA' to 1, and 'N' to 2.\n\nThen we have to separate the feature columns from the target column.\n\nThe feature columns are the columns that we try to predict from, and the target column is the column with the values we try to predict.\n\nNow we can create the actual decision tree, fit it with our details. Start by importing the modules we need:\n\nThe decision tree uses your earlier decisions to calculate the odds for you to wanting to go see a comedian or not.\n\nLet us read the different aspects of the decision tree:\n\nmeans that every comedian with a rank of 6.5 or lower will follow the arrow (to the left), and the rest will follow the arrow (to the right).\n\nrefers to the quality of the split, and is always a number between 0.0 and 0.5, where 0.0 would mean all of the samples got the same result, and 0.5 would mean that the split is done exactly in the middle.\n\nmeans that there are 13 comedians left at this point in the decision, which is all of them since this is the first step.\n\nmeans that of these 13 comedians, 6 will get a \"NO\", and 7 will get a \"GO\".\n\nThe next step contains two boxes, one box for the comedians with a 'Rank' of 6.5 or lower, and one box with the rest.\n\nmeans all of the samples got the same result.\n\nmeans that there are 5 comedians left in this branch (5 comedian with a Rank of 6.5 or lower).\n\nmeans that 5 will get a \"NO\" and 0 will get a \"GO\".\n\nmeans that the comedians with a nationality value of less than 0.5 will follow the arrow to the left (which means everyone from the UK, ), and the rest will follow the arrow to the right.\n\nmeans that about 22% of the samples would go in one direction.\n\nmeans that there are 8 comedians left in this branch (8 comedian with a Rank higher than 6.5).\n\nmeans that of these 8 comedians, 1 will get a \"NO\" and 7 will get a \"GO\".\n\nmeans that comedians at the age of 35.5 or younger will follow the arrow to the left, and the rest will follow the arrow to the right.\n\nmeans that about 37,5% of the samples would go in one direction.\n\nmeans that there are 4 comedians left in this branch (4 comedians from the UK).\n\nmeans that of these 4 comedians, 1 will get a \"NO\" and 3 will get a \"GO\".\n\nmeans all of the samples got the same result.\n\nmeans that there are 4 comedians left in this branch (4 comedians not from the UK).\n\nmeans that of these 4 comedians, 0 will get a \"NO\" and 4 will get a \"GO\".\n\nmeans all of the samples got the same result.\n\nmeans that there are 2 comedians left in this branch (2 comedians at the age 35.5 or younger).\n\nmeans that of these 2 comedians, 0 will get a \"NO\" and 2 will get a \"GO\".\n\nmeans that comedians with 9.5 years of experience, or less, will follow the arrow to the left, and the rest will follow the arrow to the right.\n\nmeans that 50% of the samples would go in one direction.\n\nmeans that there are 2 comedians left in this branch (2 comedians older than 35.5).\n\nmeans that of these 2 comedians, 1 will get a \"NO\" and 1 will get a \"GO\".\n\nmeans all of the samples got the same result.\n\nmeans that there is 1 comedian left in this branch (1 comedian with 9.5 years of experience or less).\n\nmeans that 0 will get a \"NO\" and 1 will get a \"GO\".\n\nmeans all of the samples got the same result.\n\nmeans that there is 1 comedians left in this branch (1 comedian with more than 9.5 years of experience).\n\nmeans that 1 will get a \"NO\" and 0 will get a \"GO\".\n\nWe can use the Decision Tree to predict new values.\n\nExample: Should I go see a show starring a 40 years old American comedian, with 10 years of experience, and a comedy ranking of 7?"
    }
]