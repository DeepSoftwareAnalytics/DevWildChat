[
    {
        "link": "https://numpy.org/doc/2.1/reference/generated/numpy.dot.html",
        "document": "\n• None If both a and b are 1-D arrays, it is inner product of vectors (without complex conjugation).\n• None If both a and b are 2-D arrays, it is matrix multiplication, but using or is preferred.\n• None If either a or b is 0-D (scalar), it is equivalent to and using or is preferred.\n• None If a is an N-D array and b is a 1-D array, it is a sum product over the last axis of a and b.\n• None If a is an N-D array and b is an M-D array (where ), it is a sum product over the last axis of a and the second-to-last axis of b:\n\nIt uses an optimized BLAS library when possible (see ).\n\nOutput argument. This must have the exact kind that would be returned if it was not used. In particular, it must have the right type, must be C-contiguous, and its dtype must be the dtype that would be returned for dot(a,b). This is a performance feature. Therefore, if these conditions are not met, an exception is raised, instead of attempting to be flexible. Returns the dot product of a and b. If a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. If out is given, then it is returned. If the last dimension of a is not the same size as the second-to-last dimension of b.\n\nFor 2-D arrays it is the matrix product:"
    },
    {
        "link": "https://numpy.org/doc/1.23/numpy-user.pdf",
        "document": ""
    },
    {
        "link": "https://codecademy.com/resources/docs/numpy/built-in-functions/dot",
        "document": "In NumPy, the method computes the dot product of two arrays. For 1-D arrays, it provides a scalar value. When both arrays are 2-D, it performs matrix multiplication, resulting in a new 2-D array. Further details about various scenarios, including arrays with more than two dimensions, can be found in the NumPy documentation. This method is widely utilized in linear algebra and numerical computations.\n• : The first array, which could be 1-D, 2-D, or higher dimensional.\n• : The second array, having compatible dimensions with , and also could be 1-D, 2-D, or higher dimensional.\n• : Optional parameter that specifies the output array where the result is stored. If not provided, a new array is created.\n\nThe usage of the method is demonstrated in the following example:\n\nThe above code will produce the following output:"
    },
    {
        "link": "https://numpy.org/doc",
        "document": ""
    },
    {
        "link": "https://numpy.org/devdocs/user/quickstart.html",
        "document": "You’ll need to know a bit of Python. For a refresher, see the Python tutorial. To work the examples, you’ll need installed in addition to NumPy. This is a quick overview of arrays in NumPy. It demonstrates how n-dimensional (\\(n>=2\\)) arrays are represented and can be manipulated. In particular, if you don’t know how to apply common functions to n-dimensional arrays (without using for-loops), or if you want to understand axis and shape properties for n-dimensional arrays, this article might be of help. After reading, you should be able to:\n• None Understand the difference between one-, two- and n-dimensional arrays in NumPy;\n• None Understand how to apply some linear algebra operations to n-dimensional arrays without using for-loops;\n\nNumPy’s main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of non-negative integers. In NumPy dimensions are called axes. For example, the array for the coordinates of a point in 3D space, , has one axis. That axis has 3 elements in it, so we say it has a length of 3. In the example pictured below, the array has 2 axes. The first axis has a length of 2, the second axis has a length of 3. NumPy’s array class is called . It is also known by the alias . Note that is not the same as the Standard Python Library class , which only handles one-dimensional arrays and offers less functionality. The more important attributes of an object are: the number of axes (dimensions) of the array. the dimensions of the array. This is a tuple of integers indicating the size of the array in each dimension. For a matrix with n rows and m columns, will be . The length of the tuple is therefore the number of axes, . the total number of elements of the array. This is equal to the product of the elements of . an object describing the type of the elements in the array. One can create or specify dtype’s using standard Python types. Additionally NumPy provides types of its own. numpy.int32, numpy.int16, and numpy.float64 are some examples. the size in bytes of each element of the array. For example, an array of elements of type has 8 (=64/8), while one of type has 4 (=32/8). It is equivalent to . the buffer containing the actual elements of the array. Normally, we won’t need to use this attribute because we will access the elements in an array using indexing facilities. There are several ways to create arrays. For example, you can create an array from a regular Python list or tuple using the function. The type of the resulting array is deduced from the type of the elements in the sequences. A frequent error consists in calling with multiple arguments, rather than providing a single sequence as an argument. : array() takes from 1 to 2 positional arguments but 4 were given transforms sequences of sequences into two-dimensional arrays, sequences of sequences of sequences into three-dimensional arrays, and so on. The type of the array can also be explicitly specified at creation time: Often, the elements of an array are originally unknown, but its size is known. Hence, NumPy offers several functions to create arrays with initial placeholder content. These minimize the necessity of growing arrays, an expensive operation. The function creates an array full of zeros, the function creates an array full of ones, and the function creates an array whose initial content is random and depends on the state of the memory. By default, the dtype of the created array is , but it can be specified via the key word argument . To create sequences of numbers, NumPy provides the function which is analogous to the Python built-in , but returns an array. When is used with floating point arguments, it is generally not possible to predict the number of elements obtained, due to the finite floating point precision. For this reason, it is usually better to use the function that receives as an argument the number of elements that we want, instead of the step: # useful to evaluate function at lots of points When you print an array, NumPy displays it in a similar way to nested lists, but with the following layout:\n• None the last axis is printed from left to right,\n• None the second-to-last is printed from top to bottom,\n• None the rest are also printed from top to bottom, with each slice separated from the next by an empty line. One-dimensional arrays are then printed as rows, bidimensionals as matrices and tridimensionals as lists of matrices. See below to get more details on . If an array is too large to be printed, NumPy automatically skips the central part of the array and only prints the corners: To disable this behaviour and force NumPy to print the entire array, you can change the printing options using . Arithmetic operators on arrays apply elementwise. A new array is created and filled with the result. Unlike in many matrix languages, the product operator operates elementwise in NumPy arrays. The matrix product can be performed using the operator (in python >=3.5) or the function or method: Some operations, such as and , act in place to modify an existing array rather than create a new one. # b is not automatically converted to integer type : Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind' When operating with arrays of different types, the type of the resulting array corresponds to the more general or precise one (a behavior known as upcasting). Many unary operations, such as computing the sum of all the elements in the array, are implemented as methods of the class. By default, these operations apply to the array as though it were a list of numbers, regardless of its shape. However, by specifying the parameter you can apply an operation along the specified axis of an array: NumPy provides familiar mathematical functions such as sin, cos, and exp. In NumPy, these are called “universal functions” ( ). Within NumPy, these functions operate elementwise on an array, producing an array as output. One-dimensional arrays can be indexed, sliced and iterated over, much like lists and other Python sequences. # from start to position 6, exclusive, set every 2nd element to 1000 Multidimensional arrays can have one index per axis. These indices are given in a tuple separated by commas: # each row in the second column of b # equivalent to the previous example # each column in the second and third row of b When fewer indices are provided than the number of axes, the missing indices are considered complete slices # the last row. Equivalent to b[-1, :] The expression within brackets in is treated as an followed by as many instances of as needed to represent the remaining axes. NumPy also allows you to write this using dots as . The dots ( ) represent as many colons as needed to produce a complete indexing tuple. For example, if is an array with 5 axes, then # same as c[1, :, :] or c[1] Iterating over multidimensional arrays is done with respect to the first axis: However, if one wants to perform an operation on each element in the array, one can use the attribute which is an iterator over all the elements of the array:\n\nChanging the shape of an array# An array has a shape given by the number of elements along each axis: The shape of an array can be changed with various commands. Note that the following three commands all return a modified array, but do not change the original array: The order of the elements in the array resulting from is normally “C-style”, that is, the rightmost index “changes the fastest”, so the element after is . If the array is reshaped to some other shape, again the array is treated as “C-style”. NumPy normally creates arrays stored in this order, so will usually not need to copy its argument, but if the array was made by taking slices of another array or created with unusual options, it may need to be copied. The functions and can also be instructed, using an optional argument, to use FORTRAN-style arrays, in which the leftmost index changes the fastest. The function returns its argument with a modified shape, whereas the method modifies the array itself: If a dimension is given as in a reshaping operation, the other dimensions are automatically calculated: Several arrays can be stacked together along different axes: The function stacks 1D arrays as columns into a 2D array. It is equivalent to only for 2D arrays: # the result is different In general, for arrays with more than two dimensions, stacks along their second axes, stacks along their first axes, and allows for an optional arguments giving the number of the axis along which the concatenation should happen. In complex cases, and are useful for creating arrays by stacking numbers along one axis. They allow the use of range literals . When used with arrays as arguments, and are similar to and in their default behavior, but allow for an optional argument giving the number of the axis along which to concatenate. Splitting one array into several smaller ones# Using , you can split an array along its horizontal axis, either by specifying the number of equally shaped arrays to return, or by specifying the columns after which the division should occur: # Split `a` after the third and the fourth column splits along the vertical axis, and allows one to specify along which axis to split.\n\nWhen operating and manipulating arrays, their data is sometimes copied into a new array and sometimes not. This is often a source of confusion for beginners. There are three cases: No copy at all# Simple assignments make no copy of objects or their data. # no new object is created # a and b are two names for the same ndarray object Python passes mutable objects as references, so function calls make no copy. # id is a unique identifier of an object Different array objects can share the same data. The method creates a new array object that looks at the same data. # c is a view of the data owned by a # a's shape doesn't change, reassigned c is still a view of a Slicing an array returns a view of it: # s[:] is a view of s. Note the difference between s = 10 and s[:] = 10 The method makes a complete copy of the array and its data. # a new array object with new data is created Sometimes should be called after slicing if the original array is not required anymore. For example, suppose is a huge intermediate result and the final result only contains a small fraction of , a deep copy should be made when constructing with slicing: # the memory of ``a`` can be released. If is used instead, is referenced by and will persist in memory even if is executed. See also Copies and views. Here is a list of some useful NumPy functions and methods names ordered in categories. See Routines and objects by topic for the full list.\n\nNumPy offers more indexing facilities than regular Python sequences. In addition to indexing by integers and slices, as we saw before, arrays can be indexed by arrays of integers and arrays of booleans. # the elements of `a` at the positions `i` When the indexed array is multidimensional, a single array of indices refers to the first dimension of . The following example shows this behavior by converting an image of labels into a color image using a palette. # each value corresponds to a color in the palette We can also give indexes for more than one dimension. The arrays of indices for each dimension must have the same shape. # indices for the first dim of `a` # indices for the second dim # i and j must have equal shape In Python, is exactly the same as —so we can put and in a and then do the indexing with that. However, we can not do this by putting and into an array, because this array will be interpreted as indexing the first dimension of . File , line , in : index 3 is out of bounds for axis 0 with size 3 Another common use of indexing with arrays is the search of the maximum value of time-dependent series: # index of the maxima for each series # times corresponding to the maxima You can also use indexing with arrays as a target to assign to: However, when the list of indices contains repetitions, the assignment is done several times, leaving behind the last value: This is reasonable enough, but watch out if you want to use Python’s construct, as it may not do what you expect: Even though 0 occurs twice in the list of indices, the 0th element is only incremented once. This is because Python requires to be equivalent to . When we index arrays with arrays of (integer) indices we are providing the list of indices to pick. With boolean indices the approach is different; we explicitly choose which items in the array we want and which ones we don’t. The most natural way one can think of for boolean indexing is to use boolean arrays that have the same shape as the original array: # `b` is a boolean with `a`'s shape This property can be very useful in assignments: # All elements of `a` higher than 4 become 0 You can look at the following example to see how to use boolean indexing to generate an image of the Mandelbrot set: \"\"\"Returns an image of the Mandelbrot fractal of size (h,w).\"\"\" The second way of indexing with booleans is more similar to integer indexing; for each dimension of the array we give a 1D boolean array selecting the slices we want: Note that the length of the 1D boolean array must coincide with the length of the dimension (or axis) you want to slice. In the previous example, has length 3 (the number of rows in ), and (of length 4) is suitable to index the 2nd axis (columns) of . The function can be used to combine different vectors so as to obtain the result for each n-uplet. For example, if you want to compute all the a+b*c for all the triplets taken from each of the vectors a, b and c: You could also implement the reduce as follows: The advantage of this version of reduce compared to the normal ufunc.reduce is that it makes use of the broadcasting rules in order to avoid creating an argument array the size of the output times the number of vectors."
    },
    {
        "link": "https://docs.scipy.org/doc/scipy-1.7.1/reference/reference/spatial.distance.html",
        "document": "Distance matrix computation from a collection of raw observation vectors stored in a rectangular array.\n\nPredicates for checking the validity of distance matrices, both condensed and redundant. Also contained in this module are functions for computing the number of observations in a distance matrix.\n\nDistance functions between two numeric vectors and . Computing distances over a large collection of vectors is inefficient for these functions. Use for this purpose.\n\nCompute the Bray-Curtis distance between two 1-D arrays. Compute the Canberra distance between two 1-D arrays. Compute the correlation distance between two 1-D arrays. Computes the Euclidean distance between two 1-D arrays. Compute the Jensen-Shannon distance (metric) between two probability arrays. Compute the Mahalanobis distance between two 1-D arrays. Compute the Minkowski distance between two 1-D arrays. Return the standardized Euclidean distance between two 1-D arrays. Compute the squared Euclidean distance between two 1-D arrays. Compute the weighted Minkowski distance between two 1-D arrays.\n\nDistance functions between two boolean vectors (representing sets) and . As in the case of numerical vectors, is more efficient for computing the distances between all pairs."
    },
    {
        "link": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html",
        "document": "Computes the Euclidean distance between two 1-D arrays.\n\nThe Euclidean distance between 1-D arrays u and v, is defined as\n\nThe weights for each value in u and v. Default is None, which gives each value a weight of 1.0 The Euclidean distance between vectors u and v."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html",
        "document": "Compute the distance matrix between each pair from a vector array X and Y.\n\nFor efficiency reasons, the euclidean distance between a pair of row vector x and y is computed as:\n\nThis formulation has two advantages over other ways of computing distances. First, it is computationally efficient when dealing with sparse data. Second, if one argument varies but the other remains unchanged, then and/or can be pre-computed.\n\nHowever, this is not the most precise way of doing this computation, because this equation potentially suffers from “catastrophic cancellation”. Also, the distance matrix returned by this function may not be exactly symmetric as required by, e.g., functions.\n\nRead more in the User Guide.\n\nAn array where each row is a sample and each column is a feature. An array where each row is a sample and each column is a feature. If , method uses . Y_norm_squared array-like of shape (n_samples_Y,) or (n_samples_Y, 1) or (1, n_samples_Y), default=None Pre-computed dot-products of vectors in Y (e.g., ) May be ignored in some cases, see the note below. X_norm_squared array-like of shape (n_samples_X,) or (n_samples_X, 1) or (1, n_samples_X), default=None Pre-computed dot-products of vectors in X (e.g., ) May be ignored in some cases, see the note below. Returns the distances between the row vectors of and the row vectors of .\n\nTo achieve a better accuracy, and may be unused if they are passed as ."
    },
    {
        "link": "https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy",
        "document": "I want to expound on the simple answer with various performance notes. np.linalg.norm will do perhaps more than you need:\n\nFirstly - this function is designed to work over a list and return all of the values, e.g. to compare the distance from to the set of points :\n\nisn't as innocent as it looks.\n\nFirstly - every time we call it, we have to do a global lookup for \"np\", a scoped lookup for \"linalg\" and a scoped lookup for \"norm\", and the overhead of merely calling the function can equate to dozens of python instructions.\n\nLastly, we wasted two operations on to store the result and reload it for return...\n\nFirst pass at improvement: make the lookup faster, skip the store\n\nWe get the far more streamlined:\n\nThe function call overhead still amounts to some work, though. And you'll want to do benchmarks to determine whether you might be better doing the math yourself:\n\nOn some platforms, is faster than . Your mileage may vary.\n\nWhy are you calculating distance? If the sole purpose is to display it,\n\nmove along. But if you're comparing distances, doing range checks, etc., I'd like to add some useful performance observations.\n\nLet’s take two cases: sorting by distance or culling a list to items that meet a range constraint.\n\nThe first thing we need to remember is that we are using Pythagoras to calculate the distance ( ) so we're making a lot of calls. Math 101:\n\nIn short: until we actually require the distance in a unit of X rather than X^2, we can eliminate the hardest part of the calculations.\n\nGreat, both functions no-longer do any expensive square roots. That'll be much faster, but before you go further, check yourself: why did sort_things_by_distance need a \"naive\" disclaimer both times above? Answer at the very bottom (*a1).\n\nWe can improve in_range by converting it to a generator:\n\nThis especially has benefits if you are doing something like:\n\nBut if the very next thing you are going to do requires a distance,\n\nThis can be especially useful if you might chain range checks ('find things that are near X and within Nm of Y', since you don't have to calculate the distance again).\n\nBut what about if we're searching a really large list of and we anticipate a lot of them not being worth consideration?\n\nThere is actually a very simple optimization:\n\nWhether this is useful will depend on the size of 'things'.\n\nAnd again, consider yielding the dist_sq. Our hotdog example then becomes:"
    },
    {
        "link": "https://medium.com/towards-data-science/euclidean-distance-numpy-1b2784e966fc",
        "document": "How To Compute Euclidean Distance in NumPy\n\nEuclidean distance between two points corresponds to the length of a line segment between the two points. Assuming that we have two points A (x₁, y₁) and B (x₂, y₂), the Euclidean distance between the points is illustrated in the diagram below.\n\nThe mathematical formula used to compute the euclidean distance between two points, is given below.\n\nIn today’s short tutorial we will explore a few different ways in which you can compute the Euclidean Distance when working with NumPy arrays. More specifically, we will showcase how to do so using\n• and a combination of and methods\n\nFirst, let’s create an example NumPy array that we will be referencing in the following sections in order to demonstrate a few different ways for computing Euclidean Distance."
    },
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html",
        "document": ""
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/indexing.html",
        "document": "The axis labeling information in pandas objects serves many purposes:\n• None Identifies data (i.e. provides metadata) using known indicators, important for analysis, visualization, and interactive console display.\n• None Allows intuitive getting and setting of subsets of the data set.\n\nIn this section, we will focus on the final point: namely, how to slice, dice, and generally get and set subsets of pandas objects. The primary focus will be on Series and DataFrame as they have received more development attention in this area.\n\nSee the MultiIndex / Advanced Indexing for and more advanced indexing documentation.\n\nSee the cookbook for some advanced strategies.\n\nWhether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called and should be avoided. See Returning a View versus Copy. is strict when you present slicers that are not compatible (or convertible) with the index type. For example using integers in a . These will raise a . Traceback (most recent call last) in in in in # GH#33146 if start and end are combinations of str and None and Index is not # monotonic, we can not use Index.slice_indexer because it does not honor the # actual elements, is only searching for start and end in Compute the slice indexer for input labels and step. in in # For datetime indices label may be a string that has to be converted # to datetime boundary according to its resolution. # we need to look up the label in # Pandas supports slicing with dates, treated as datetimes at midnight. in in : cannot do slice indexing on DatetimeIndex with these indexers [2] of type int String likes in slicing can be convertible to the type of the index and lead to natural slicing. pandas provides a suite of methods in order to have purely label based indexing. This is a strict inclusion based protocol. Every label asked for must be in the index, or a will be raised. When slicing, both the start bound AND the stop bound are included, if present in the index. Integers are valid labels, but they refer to the label and not the position. The attribute is the primary access method. The following are valid inputs:\n• None A single label, e.g. or (Note that is interpreted as a label of the index. This use is not an integer position along the index.).\n• None A slice object with labels (Note that contrary to usual Python slices, both the start and the stop are included, when present in the index! See Slicing with labels. Note that setting works as well: For getting a cross section using a label (equivalent to ): For getting values with a boolean array: For getting a value explicitly: # this is also equivalent to ``df1.at['a','A']`` When using with slices, if both the start and the stop labels are present in the index, then elements located between the two (including them) are returned: If at least one of the two is absent, but the index is sorted, and can be compared against start and stop labels, then slicing will still work as expected, by selecting labels which rank between the two: However, if at least one of the two is absent and the index is not sorted, an error will be raised (since doing otherwise would be computationally expensive, as well as potentially ambiguous for mixed type indexes). For instance, in the above example, would raise . For the rationale behind this behavior, see Endpoints are inclusive. Also, if the index has duplicate labels and either the start or the stop label is duplicated, an error will be raised. For instance, in the above example, would raise a . For more information about duplicate labels, see Duplicate Labels.\n\nA random selection of rows or columns from a Series or DataFrame with the method. The method will sample rows by default, and accepts a specific number of rows/columns to return, or a fraction of rows. # When no arguments are passed, returns 1 row. # One may specify either a number of rows: # Or a fraction of the rows: By default, will return each row at most once, but one can also sample with replacement using the option: By default, each row has an equal probability of being selected, but if you want rows to have different probabilities, you can pass the function sampling weights as . These weights can be a list, a NumPy array, or a Series, but they must be of the same length as the object you are sampling. Missing values will be treated as a weight of zero, and inf values are not allowed. If weights do not sum to 1, they will be re-normalized by dividing all weights by the sum of the weights. For example: When applied to a DataFrame, you can use a column of the DataFrame as sampling weights (provided you are sampling rows and not columns) by simply passing the name of the column as a string. also allows users to sample columns instead of rows using the argument. Finally, one can also set a seed for ’s random number generator using the argument, which will accept either an integer (as a seed) or a NumPy RandomState object. # With a given seed, the sample will always draw the same rows.\n\nSelecting values from a Series with a boolean vector generally returns a subset of the data. To guarantee that selection output has the same shape as the original data, you can use the method in and . To return only the selected rows: To return a Series of the same shape as the original: Selecting values from a DataFrame with a boolean criterion now also preserves input data shape. is used under the hood as the implementation. The code below is equivalent to . In addition, takes an optional argument for replacement of values where the condition is False, in the returned copy. You may wish to set values based on some boolean criteria. This can be done intuitively like so: The signature for differs from . Roughly is equivalent to . Furthermore, aligns the input boolean condition (ndarray or DataFrame), such that partial selection with setting is possible. This is analogous to partial setting via (but on the contents rather than the axis labels). Where can also accept and parameters to align the input when performing the . This is equivalent to (but faster than) the following. can accept a callable as condition and arguments. The function must be with one argument (the calling Series or DataFrame) and that returns valid output as condition and argument. is the inverse boolean operation of .\n\nobjects have a method that allows selection using an expression. You can get the value of the frame where column has values between the values of columns and . For example: Do the same thing but fall back on a named index if there is no column with the name . If instead you don’t want to or cannot name your index, you can use the name in your query expression: If the name of your index overlaps with a column name, the column name is given precedence. For example, # uses the column 'a', not the index You can still use the index in a query expression by using the special identifier ‘index’: If for some reason you have a column named , then you can refer to the index as as well, but at this point you should consider renaming your columns to something less ambiguous. You can also use the levels of a with a as if they were columns in the frame: If the levels of the are unnamed, you can refer to them using special names: The convention is , which means “index level 0” for the 0th level of the . A use case for is when you have a collection of objects that have a subset of column names (or index levels/names) in common. You can pass the same query to both frames without having to specify which frame you’re interested in querying Slightly nicer by removing the parentheses (comparison operators bind tighter than and ): Use English instead of symbols: Pretty close to how you might write it on paper: also supports special use of Python’s and comparison operators, providing a succinct syntax for calling the method of a or . # get all rows where columns \"a\" and \"b\" have overlapping values # How you'd do it in pure Python You can combine this with other expressions for very succinct queries: # rows where cols a and b have overlapping values # and col c's values are less than col d's Note that and are evaluated in Python, since has no equivalent of this operation. However, only the / expression itself is evaluated in vanilla Python. For example, in the expression is evaluated by and then the operation is evaluated in plain Python. In general, any operations that can be evaluated using will be. Special use of the operator with objects# Comparing a of values to a column using / works similarly to / . You can negate boolean expressions with the word or the operator. Of course, expressions can be arbitrarily complex too: 'a < b < c and (not bools) or bools > 2' using is slightly faster than Python for large frames. You will only see the performance benefits of using the engine with if your frame has more than approximately 100,000 rows. This plot was created using a with 3 columns each containing floating point values generated using .\n\nThe pandas class and its subclasses can be viewed as implementing an ordered multiset. Duplicates are allowed. also provides the infrastructure necessary for lookups, data alignment, and reindexing. The easiest way to create an directly is to pass a or other sequence to : If no dtype is given, tries to infer the dtype from the data. It is also possible to give an explicit dtype when instantiating an : You can also pass a to be stored in the index: The name, if set, will be shown in the console display: Indexes are “mostly immutable”, but it is possible to set and change their attribute. You can use the , to set these attributes directly, and they default to returning a copy. See Advanced Indexing for usage of MultiIndexes. , , and also take an optional argument The two main operations are and . Difference is provided via the method. Also available is the operation, which returns elements that appear in either or , but not in both. This is equivalent to the Index created by , with duplicates dropped. The resulting index from a set operation will be sorted in ascending order. When performing between indexes with different dtypes, the indexes must be cast to a common dtype. Typically, though not always, this is object dtype. The exception is when performing a union between integer and float data. In this case, the integer values are converted to float Even though can hold missing values ( ), it should be avoided if you do not want any unexpected results. For example, some operations exclude missing values implicitly. fills missing values with specified scalar value.\n\nCopy-on-Write will become the new default in pandas 3.0. This means that chained indexing will never work. As a consequence, the won’t be necessary anymore. See this section for more context. We recommend turning Copy-on-Write on to leverage the improvements with even before pandas 3.0 is available. When setting values in a pandas object, care must be taken to avoid what is called . Here is an example. first second first second Name: (one, second), dtype: object These both yield the same results, so which should you use? It is instructive to understand the order of operations on these and why method 2 ( ) is much preferred over method 1 (chained ). selects the first level of the columns and returns a DataFrame that is singly-indexed. Then another Python operation selects the series indexed by . This is indicated by the variable because pandas sees these operations as separate events. e.g. separate calls to , so it has to treat them as linear operations, they happen one after another. Contrast this to which passes a nested tuple of to a single call to . This allows pandas to deal with this as a single entity. Furthermore this order of operations can be significantly faster, and allows one to index both axes if so desired. Why does assignment fail when using chained indexing?# Copy-on-Write will become the new default in pandas 3.0. This means than chained indexing will never work. As a consequence, the won’t be necessary anymore. See this section for more context. We recommend turning Copy-on-Write on to leverage the improvements with even before pandas 3.0 is available. The problem in the previous section is just a performance issue. What’s up with the warning? We don’t usually throw warnings around when you do something that might cost a few extra milliseconds! But it turns out that assigning to the product of chained indexing has inherently unpredictable results. To see this, think about how the Python interpreter executes this code: But this code is handled differently: See that in there? Outside of simple cases, it’s very hard to predict whether it will return a view or a copy (it depends on the memory layout of the array, about which pandas makes no guarantees), and therefore whether the will modify or a temporary object that gets thrown out immediately afterward. That’s what is warning you about! You may be wondering whether we should be concerned about the property in the first example. But is guaranteed to be itself with modified indexing behavior, so / operate on directly. Of course, may be a view or a copy of . Sometimes a warning will arise at times when there’s no obvious chained indexing going on. These are the bugs that is designed to catch! pandas is probably trying to warn you that you’ve done this: # Is foo a view? A copy? Nobody knows! # We don't know whether this will modify df or not! Copy-on-Write will become the new default in pandas 3.0. This means than chained indexing will never work. As a consequence, the won’t be necessary anymore. See this section for more context. We recommend turning Copy-on-Write on to leverage the improvements with even before pandas 3.0 is available. When you use chained indexing, the order and type of the indexing operation partially determine whether the result is a slice into the original object, or a copy of the slice. pandas has the because assigning to a copy of a slice is frequently not intentional, but a mistake caused by chained indexing returning a copy where a slice was expected. If you would like pandas to be more or less trusting about assignment to a chained indexing expression, you can set the option to one of these values:\n• None means pandas will raise a you have to deal with.\n• None will suppress the warnings entirely. # This will show the SettingWithCopyWarning # but the frame values will be set This however is operating on a copy and will not work. A chained assignment can also crop up in setting in a mixed dtype frame. These setting rules apply to all of . The following is the recommended access method using for multiple items (using ) and a single item using a fixed index: The following can work at times, but it is not guaranteed to, and therefore should be avoided: Last, the subsequent example will not work at all, and so should be avoided: Traceback (most recent call last) in in in in in : A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy The chained assignment warnings / exceptions are aiming to inform the user of a possibly invalid assignment. There may be false positives; situations where a chained assignment is inadvertently reported."
    },
    {
        "link": "https://pandas.pydata.org/pandas-docs/version/1.3/user_guide/indexing.html",
        "document": "The axis labeling information in pandas objects serves many purposes:\n• None Identifies data (i.e. provides metadata) using known indicators, important for analysis, visualization, and interactive console display.\n• None Allows intuitive getting and setting of subsets of the data set.\n\nIn this section, we will focus on the final point: namely, how to slice, dice, and generally get and set subsets of pandas objects. The primary focus will be on Series and DataFrame as they have received more development attention in this area.\n\nSee the MultiIndex / Advanced Indexing for and more advanced indexing documentation.\n\nSee the cookbook for some advanced strategies.\n\nWhether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called and should be avoided. See Returning a View versus Copy. is strict when you present slicers that are not compatible (or convertible) with the index type. For example using integers in a . These will raise a . TypeError: cannot do slice indexing on <class 'pandas.tseries.index.DatetimeIndex'> with these indexers [2] of <type 'int'> String likes in slicing can be convertible to the type of the index and lead to natural slicing. pandas will raise a if indexing with a list with missing labels. See list-like Using loc with missing keys in a list is Deprecated. pandas provides a suite of methods in order to have purely label based indexing. This is a strict inclusion based protocol. Every label asked for must be in the index, or a will be raised. When slicing, both the start bound AND the stop bound are included, if present in the index. Integers are valid labels, but they refer to the label and not the position. The attribute is the primary access method. The following are valid inputs:\n• None A single label, e.g. or (Note that is interpreted as a label of the index. This use is not an integer position along the index.).\n• None A slice object with labels (Note that contrary to usual Python slices, both the start and the stop are included, when present in the index! See Slicing with labels. Note that setting works as well: For getting a cross section using a label (equivalent to ): For getting values with a boolean array: For getting a value explicitly: # this is also equivalent to ``df1.at['a','A']`` When using with slices, if both the start and the stop labels are present in the index, then elements located between the two (including them) are returned: If at least one of the two is absent, but the index is sorted, and can be compared against start and stop labels, then slicing will still work as expected, by selecting labels which rank between the two: However, if at least one of the two is absent and the index is not sorted, an error will be raised (since doing otherwise would be computationally expensive, as well as potentially ambiguous for mixed type indexes). For instance, in the above example, would raise . For the rationale behind this behavior, see Endpoints are inclusive. Also, if the index has duplicate labels and either the start or the stop label is duplicated, an error will be raised. For instance, in the above example, would raise a . For more information about duplicate labels, see Duplicate Labels.\n\nA random selection of rows or columns from a Series or DataFrame with the method. The method will sample rows by default, and accepts a specific number of rows/columns to return, or a fraction of rows. # When no arguments are passed, returns 1 row. # One may specify either a number of rows: # Or a fraction of the rows: By default, will return each row at most once, but one can also sample with replacement using the option: By default, each row has an equal probability of being selected, but if you want rows to have different probabilities, you can pass the function sampling weights as . These weights can be a list, a NumPy array, or a Series, but they must be of the same length as the object you are sampling. Missing values will be treated as a weight of zero, and inf values are not allowed. If weights do not sum to 1, they will be re-normalized by dividing all weights by the sum of the weights. For example: When applied to a DataFrame, you can use a column of the DataFrame as sampling weights (provided you are sampling rows and not columns) by simply passing the name of the column as a string. also allows users to sample columns instead of rows using the argument. Finally, one can also set a seed for ’s random number generator using the argument, which will accept either an integer (as a seed) or a NumPy RandomState object. # With a given seed, the sample will always draw the same rows.\n\nSelecting values from a Series with a boolean vector generally returns a subset of the data. To guarantee that selection output has the same shape as the original data, you can use the method in and . To return only the selected rows: To return a Series of the same shape as the original: Selecting values from a DataFrame with a boolean criterion now also preserves input data shape. is used under the hood as the implementation. The code below is equivalent to . In addition, takes an optional argument for replacement of values where the condition is False, in the returned copy. You may wish to set values based on some boolean criteria. This can be done intuitively like so: By default, returns a modified copy of the data. There is an optional parameter so that the original data can be modified without creating a copy: The signature for differs from . Roughly is equivalent to . Furthermore, aligns the input boolean condition (ndarray or DataFrame), such that partial selection with setting is possible. This is analogous to partial setting via (but on the contents rather than the axis labels). Where can also accept and parameters to align the input when performing the . This is equivalent to (but faster than) the following. can accept a callable as condition and arguments. The function must be with one argument (the calling Series or DataFrame) and that returns valid output as condition and argument. is the inverse boolean operation of .\n\nobjects have a method that allows selection using an expression. You can get the value of the frame where column has values between the values of columns and . For example: Do the same thing but fall back on a named index if there is no column with the name . If instead you don’t want to or cannot name your index, you can use the name in your query expression: If the name of your index overlaps with a column name, the column name is given precedence. For example, # uses the column 'a', not the index You can still use the index in a query expression by using the special identifier ‘index’: If for some reason you have a column named , then you can refer to the index as as well, but at this point you should consider renaming your columns to something less ambiguous. You can also use the levels of a with a as if they were columns in the frame: If the levels of the are unnamed, you can refer to them using special names: The convention is , which means “index level 0” for the 0th level of the . A use case for is when you have a collection of objects that have a subset of column names (or index levels/names) in common. You can pass the same query to both frames without having to specify which frame you’re interested in querying Slightly nicer by removing the parentheses (comparison operators bind tighter than and ): Use English instead of symbols: Pretty close to how you might write it on paper: also supports special use of Python’s and comparison operators, providing a succinct syntax for calling the method of a or . # get all rows where columns \"a\" and \"b\" have overlapping values # How you'd do it in pure Python You can combine this with other expressions for very succinct queries: # rows where cols a and b have overlapping values # and col c's values are less than col d's Note that and are evaluated in Python, since has no equivalent of this operation. However, only the / expression itself is evaluated in vanilla Python. For example, in the expression is evaluated by and then the operation is evaluated in plain Python. In general, any operations that can be evaluated using will be. Special use of the operator with objects¶ Comparing a of values to a column using / works similarly to / . You can negate boolean expressions with the word or the operator. Of course, expressions can be arbitrarily complex too: 'a < b < c and (not bools) or bools > 2' using is slightly faster than Python for large frames. You will only see the performance benefits of using the engine with if your frame has more than approximately 200,000 rows. This plot was created using a with 3 columns each containing floating point values generated using .\n\nThe pandas class and its subclasses can be viewed as implementing an ordered multiset. Duplicates are allowed. However, if you try to convert an object with duplicate entries into a , an exception will be raised. also provides the infrastructure necessary for lookups, data alignment, and reindexing. The easiest way to create an directly is to pass a or other sequence to : You can also pass a to be stored in the index: The name, if set, will be shown in the console display: Indexes are “mostly immutable”, but it is possible to set and change their attribute. You can use the , to set these attributes directly, and they default to returning a copy. See Advanced Indexing for usage of MultiIndexes. , , and also take an optional argument The two main operations are and . Difference is provided via the method. Also available is the operation, which returns elements that appear in either or , but not in both. This is equivalent to the Index created by , with duplicates dropped. The resulting index from a set operation will be sorted in ascending order. When performing between indexes with different dtypes, the indexes must be cast to a common dtype. Typically, though not always, this is object dtype. The exception is when performing a union between integer and float data. In this case, the integer values are converted to float Even though can hold missing values ( ), it should be avoided if you do not want any unexpected results. For example, some operations exclude missing values implicitly. fills missing values with specified scalar value.\n\nWhen setting values in a pandas object, care must be taken to avoid what is called . Here is an example. first second first second Name: (one, second), dtype: object These both yield the same results, so which should you use? It is instructive to understand the order of operations on these and why method 2 ( ) is much preferred over method 1 (chained ). selects the first level of the columns and returns a DataFrame that is singly-indexed. Then another Python operation selects the series indexed by . This is indicated by the variable because pandas sees these operations as separate events. e.g. separate calls to , so it has to treat them as linear operations, they happen one after another. Contrast this to which passes a nested tuple of to a single call to . This allows pandas to deal with this as a single entity. Furthermore this order of operations can be significantly faster, and allows one to index both axes if so desired. Why does assignment fail when using chained indexing?¶ The problem in the previous section is just a performance issue. What’s up with the warning? We don’t usually throw warnings around when you do something that might cost a few extra milliseconds! But it turns out that assigning to the product of chained indexing has inherently unpredictable results. To see this, think about how the Python interpreter executes this code: But this code is handled differently: See that in there? Outside of simple cases, it’s very hard to predict whether it will return a view or a copy (it depends on the memory layout of the array, about which pandas makes no guarantees), and therefore whether the will modify or a temporary object that gets thrown out immediately afterward. That’s what is warning you about! You may be wondering whether we should be concerned about the property in the first example. But is guaranteed to be itself with modified indexing behavior, so / operate on directly. Of course, may be a view or a copy of . Sometimes a warning will arise at times when there’s no obvious chained indexing going on. These are the bugs that is designed to catch! pandas is probably trying to warn you that you’ve done this: # Is foo a view? A copy? Nobody knows! # We don't know whether this will modify df or not! When you use chained indexing, the order and type of the indexing operation partially determine whether the result is a slice into the original object, or a copy of the slice. pandas has the because assigning to a copy of a slice is frequently not intentional, but a mistake caused by chained indexing returning a copy where a slice was expected. If you would like pandas to be more or less trusting about assignment to a chained indexing expression, you can set the option to one of these values:\n• None means pandas will raise a you have to deal with.\n• None will suppress the warnings entirely. # This will show the SettingWithCopyWarning # but the frame values will be set This however is operating on a copy and will not work. A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_index,col_indexer] = value instead A chained assignment can also crop up in setting in a mixed dtype frame. These setting rules apply to all of . The following is the recommended access method using for multiple items (using ) and a single item using a fixed index: The following can work at times, but it is not guaranteed to, and therefore should be avoided: Last, the subsequent example will not work at all, and so should be avoided: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_index,col_indexer] = value instead The chained assignment warnings / exceptions are aiming to inform the user of a possibly invalid assignment. There may be false positives; situations where a chained assignment is inadvertently reported."
    },
    {
        "link": "https://pandas.pydata.org/pandas-docs/version/1.3/ecosystem.html",
        "document": "Increasingly, packages are being built on top of pandas to address specific needs in data preparation, analysis and visualization. This is encouraging because it means pandas is not only helping users to handle their data tasks but also that it provides a better starting point for developers to build powerful and more focused data tools. The creation of libraries that complement pandas’ functionality also allows pandas development to remain focused around it’s original requirements.\n\nThis is an inexhaustive list of projects that build on pandas in order to provide tools in the PyData space. For a list of projects that depend on pandas, see the libraries.io usage page for pandas or search pypi for pandas.\n\nWe’d like to make it easier for users to find these projects, if you know of other substantial projects that you feel should be on this list, please let us know.\n\nIPython is an interactive command shell and distributed computing environment. IPython tab completion works with pandas methods and also attributes like DataFrame columns. Jupyter Notebook is a web application for creating Jupyter notebooks. A Jupyter notebook is a JSON document containing an ordered list of input/output cells which can contain code, text, mathematics, plots and rich media. Jupyter notebooks can be converted to a number of open standard output formats (HTML, HTML presentation slides, LaTeX, PDF, ReStructuredText, Markdown, Python) through ‘Download As’ in the web interface and in a shell. pandas DataFrames implement and methods which are utilized by Jupyter Notebook for displaying (abbreviated) HTML or LaTeX tables. LaTeX output is properly escaped. (Note: HTML tables may or may not be compatible with non-HTML Jupyter output formats.) See Options and Settings and Available Options for pandas settings. qgrid is “an interactive grid for sorting and filtering DataFrames in IPython Notebook” built with SlickGrid. Spyder is a cross-platform PyQt-based IDE combining the editing, analysis, debugging and profiling functionality of a software development tool with the data exploration, interactive execution, deep inspection and rich visualization capabilities of a scientific environment like MATLAB or Rstudio. Its Variable Explorer allows users to view, manipulate and edit pandas , , and objects like a “spreadsheet”, including copying and modifying values, sorting, displaying a “heatmap”, converting data types and more. pandas objects can also be renamed, duplicated, new columns added, copied/pasted to/from the clipboard (as TSV), and saved/loaded to/from a file. Spyder can also import data from a variety of plain text and binary files or the clipboard into a new pandas DataFrame via a sophisticated import wizard. Most pandas classes, methods and data attributes can be autocompleted in Spyder’s Editor and IPython Console, and Spyder’s Help pane can retrieve and render Numpydoc documentation on pandas objects in rich text with Sphinx both automatically and on-demand.\n\nis a remote data access library for pandas (PyPI: ). It is based on functionality that was located in and but was split off in v0.19. See more in the pandas-datareader docs: The following data feeds are available: Quandl API for Python wraps the Quandl REST API to return pandas DataFrames with timeseries indexes. PyDatastream is a Python interface to the Refinitiv Datastream (DWS) REST API to return indexed pandas DataFrames with financial data. This package requires valid credentials for this API (non free). pandaSDMX is a library to retrieve and acquire statistical data and metadata disseminated in SDMX 2.1, an ISO-standard widely used by institutions such as statistics offices, central banks, and international organisations. pandaSDMX can expose datasets and related structural metadata including data flows, code-lists, and data structure definitions as pandas Series or MultiIndexed DataFrames. fredapi is a Python interface to the Federal Reserve Economic Data (FRED) provided by the Federal Reserve Bank of St. Louis. It works with both the FRED database and ALFRED database that contains point-in-time data (i.e. historic data revisions). fredapi provides a wrapper in Python to the FRED HTTP API, and also provides several convenient methods for parsing and analyzing point-in-time data from ALFRED. fredapi makes use of pandas and returns data in a Series or DataFrame. This module requires a FRED API key that you can obtain for free on the FRED website. is a Python package that translates SQL syntax directly into operations on pandas DataFrames. This is useful when migrating from a database to using pandas or for users more comfortable with SQL looking for a way to interface with pandas.\n\nBlaze provides a standard API for doing computations with various in-memory and on-disk backends: NumPy, pandas, SQLAlchemy, MongoDB, PyTables, PySpark. Cylon is a fast, scalable, distributed memory parallel runtime with a pandas like Python DataFrame API. ”Core Cylon” is implemented with C++ using Apache Arrow format to represent the data in-memory. Cylon DataFrame API implements most of the core operators of pandas such as merge, filter, join, concat, group-by, drop_duplicates, etc. These operators are designed to work across thousands of cores to scale applications. It can interoperate with pandas DataFrame by reading data from pandas or converting data to pandas so users can selectively scale parts of their pandas DataFrame applications. # Using 1000s of cores across the cluster to compute the join Dask is a flexible parallel computing library for analytics. Dask provides a familiar interface for out-of-core, parallel and distributed computing. Dask-ML enables parallel and distributed machine learning using Dask alongside existing machine learning libraries like Scikit-Learn, XGBoost, and TensorFlow. Ibis offers a standard way to write analytics code, that can be run in multiple engines. It helps in bridging the gap between local Python environments (like pandas) and remote storage and execution systems like Hadoop components (like HDFS, Impala, Hive, Spark) and SQL databases (Postgres, etc.). Koalas provides a familiar pandas DataFrame interface on top of Apache Spark. It enables users to leverage multi-cores on one machine or a cluster of machines to speed up or scale their DataFrame code. The DataFrame is a parallel and distributed drop-in replacement for pandas. This means that you can use Modin with existing pandas code or write new code with the existing pandas API. Modin can leverage your entire machine or cluster to speed up and scale your pandas workloads, including traditionally time-consuming tasks like ingesting data ( , , , etc.). Odo provides a uniform API for moving data between different formats. It uses pandas own for CSV IO and leverages many existing packages such as PyTables, h5py, and pymongo to move data between non pandas formats. Its graph based approach is also extensible by end users for custom formats that may be too specific for the core of odo. Pandarallel provides a simple way to parallelize your pandas operations on all your CPUs by changing only one line of code. If also displays progress bars. Increasingly, packages are being built on top of pandas to address specific needs in data preparation, analysis and visualization. Vaex is a Python library for Out-of-Core DataFrames (similar to pandas), to visualize and explore big tabular datasets. It can calculate statistics such as mean, sum, count, standard deviation etc, on an N-dimensional grid up to a billion (109) objects/rows per second. Visualization is done using histograms, density plots and 3d volume rendering, allowing interactive exploration of big data. Vaex uses memory mapping, zero memory copy policy and lazy computations for best performance (no memory wasted)."
    },
    {
        "link": "https://quora.com/I-love-the-flexibility-of-pandas-dataframes-but-I-feel-like-they-can-make-code-harder-to-read-and-maintain-What-are-some-pandas-best-practices-that-address-this-issue",
        "document": "Something went wrong. Wait a moment and try again."
    }
]