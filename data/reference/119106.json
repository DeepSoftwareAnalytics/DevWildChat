[
    {
        "link": "https://cran.r-project.org/web/packages/vars/vars.pdf",
        "document": ""
    },
    {
        "link": "https://researchgate.net/profile/David-Booth-7/post/How_to_select_optimal_lag_between_dependent_variable_and_independent_variables/attachment/59d649eb79197b80779a450a/AS%3A473055043559424%401489796519099/download/VARS_how_to_use.pdf",
        "document": ""
    },
    {
        "link": "https://cran.r-project.org/web/packages/BigVAR/vignettes/BigVAR.html",
        "document": "\\({\\tt BigVAR}\\) is the companion R package to the papers “VARX-L: Structured Regularization for Large Vector Autoregression with Exogenous Variables” (Joint with David Matteson and Jacob Bien) and “High Dimensional Forecasting via Interpretable Vector Autoregression (HLag)” (Joint with Ines Wilms, David Matteson, and Jacob Bien). \\({\\tt BigVAR}\\) allows for the simultaneous estimation and forecasting of high-dimensional time series by applying structured penalties to the standard vector autoregression (VAR) and vector autoregression with exogenous variables (VARX) frameworks. This is useful in many applications which make use of time-dependent data, such as macroeconomics, finance, and internet traffic, as the conventional VAR and VARX are heavily overparameterized. In addition, as stated in Ghysels and Marcellino (2018), VARs with “large enough” lag order can adequately approximate VARMA models. Our package adapts solution methods from the regularization literature to a multivariate time series setting, allowing for the computationally efficient estimation of high-dimensional VAR and VARX models. We also allow for least squares refitting based on the nonzero support selected by our procedures as well as the ability to incorporate mild non-stationarity by shrinking toward a vector random walk. For more information on these extensions, we refer you to our papers Nicholson, Matteson, and Bien (2017b) and Nicholson et al. (2020). This vignette presents a brief formal overview of our notation, the models contained in \\({\\tt BigVAR}\\), and the functionality of the package. For an interactive tutorial see the shiny app. Any questions or feature requests with regard to \\({\\tt BigVAR}\\) can be addressed to wbn8@cornell.edu. If you have basic questions about VAR or multivariate time series in general, we recommend consulting Lütkepohl (2005). Notation and Methodology provides an overview of the VARX model as well as the \\({\\tt BigVAR}\\) framework. Our penalty structures are described in VARX-L and HLAG. Empirical penalty parameter selection procedures are discussed in Penalty Parameter Selection and N-fold cross validation. Package specific syntax is detailed in BigVAR Details. Finally, example applications and extensions of \\({\\tt BigVAR}\\) are provided in Selecting a Structure and Impulse Response Functions. The stable version of \\({\\tt BigVAR}\\) is available on cran. The developmental release can be installed from github using the following command: In this section, we provide a basic overview of the capabilities of \\({\\tt BigVAR}\\). Further sections will provide elaboration as to the full functionality of \\({\\tt BigVAR}\\). \\(\\mathbf{Y}\\), a simulated multivariate time series of dimension \\(100\\times 3\\) is included with \\({\\tt BigVAR}\\) and is used throughout this vignette (details as to its construction are provided in Example Data). It can be accessed by calling: In order to forecast \\(\\hat{y}_{t+1}\\) using a vector autoregression with a lasso penalty \\(\\lambda=1\\) and maximum lag order of 2, one can simply run # 3 x 7 coefficient matrix B = BigVAR.fit(Y,struct='Basic',p=2,lambda=1)[,,1] # construct 7 x 99 lag matrix of Y Z = VARXLagCons(Y,p=2,oos=TRUE)$Z # obtain out of sample forecasts yhat = B%*%Z[,ncol(Z),drop=F] Some potential use cases of \\({\\tt BigVAR.fit}\\) are elaborated upon in Extensions. More sophisticated analysis requires the construction of an object of class \\({\\tt BigVAR}\\) as described in constructModel. Let \\(\\{ \\mathbf{y_t}\\}_{t = 1}^T\\) denote a \\(k\\) dimensional vector time series and \\(\\{\\mathbf{x}_t\\}_{t=1}^{T}\\) denote an \\(m\\)-dimensional unmodeled series. A vector autoregression with exogenous variables of order (p,s) , VARX\\(_{k,m}\\)(\\(p,s\\)), can be expressed as\n\n \\[ \\begin{align} \\label{VAR1} \\mathbf{y}_t=\\mathbf{\n\nu}+\\sum_{\\ell=1}^p\\mathbf{\\Phi}^{(\\ell)}\\mathbf{y}_{t-\\ell}+\\sum_{j=1}^s \\mathbf{\\beta}^{(j)}\\mathbf{x}_{t-j}+\\mathbf{u}_t \\; \\text{ for } \\;t=1,\\ldots,T, \\end{align} \\] in which \\(\\mathbf{\n\nu}\\) denotes a \\(k\\times 1\\) intercept vector, each \\(\\mathbf{\\Phi}^{(\\ell)}\\) represents a \\(k\\times k\\) endogenous (modeled) coefficient matrix, each \\(\\mathbf{\\beta}^{(j)}\\) represents a \\(k\\times m\\) exogenous (unmodeled) coefficient matrix, and \\(\\mathbf{u}_t\\stackrel{\\text{wn}}{\\sim}(\\mathbf{0},\\mathbf{\\Sigma}_u)\\). Note that the VAR is a special case of Equation \\(\\ref{VAR1}\\) in which the second summation (\\(\\sum_{j=1}^s \\mathbf{\\beta}^{(j)}\\mathbf{x}_{t-j}\\)) is not included.\n• The Basic VARX-L will zero individual elements in\n• The Lag VARX-L assigns a group lasso penalty by lag coefficient matrix . Each column in the exogenous coefficient matrices is assigned to its own group.\n• The Own/Other VARX-L assigns separate group lasso penalties to own lags ( and other lags . It applies the same penalty as the Lag VARX-L to the exogenous structure.\n• The Sparse Lag and Sparse Own/Other VARX-L extend the aforementioned structures to allow for within-group sparsity. We recently added an elastic net penalty (BasicEN) which combines the Basic VARX-L with a ridge penalty. \\(\\tt BigVAR\\) can be used to apply the following penalties to the VARX (Equation \\(\\ref{VAR1}\\)): \\(\\lambda>0\\) is a penalty parameter that can be selected via a validation procedure or chosen by the user; larger values of \\(\\lambda\\) encourage a greater degree of sparsity. \\(0\\leq \\alpha\\leq 1\\) is an additional penalty parameter set by default to \\(\\frac{1}{k+1}\\) to control within-group sparsity in the sparse group setting or the trade-off between the ridge and lasso penalty in the elastic net setting. We allow for \\(\\alpha\\) to be estimated empirically with the option \\({\\tt dual=TRUE}\\) in the function \\({\\tt constructModel}\\). \\(\\rho_1=\\sqrt{k}\\) and \\(\\gamma_1=\\sqrt{k(k-1)}\\) are fixed weights accounting for the cardinality of each group. We additionally incorporate several VAR-specific penalties that directly address lag order selection. In addition to returning sparse solutions, our \\(\\text{HLAG}_k(p)\\) procedures induce regularization toward models with low maximum lag order. To allow for greater flexibility, instead of imposing a single, universal lag order (as information criterion minimization based approaches tend to do), we allow it to vary across marginal models (i.e. the rows of the coefficient matrix \\(\\mathbf{\\Phi}=[\\mathbf{\\Phi}^{(1)},\\dots,\\mathbf{\\Phi}^{(p)}]\\)). \\(\\tt{BigVAR}\\) includes three HLAG models as well as the Lag-weighted Lasso, which incorporates a lasso penalty that increases geometrically as the lag order increases. This penalty does not directly address lag order but it encourages a greater degree of sparsity at more distant lags (as controlled by the additional penalty parameter \\(\\gamma \\in (0,1)\\). The componentwise HLAG embeds a conventional lag order selection penalty into the hierarchical group lasso; the maximum lag order can vary across series, but within a series all components have the same maximum lag. The Own/Other HLAG adds another layer of lag order: within a lag a series own lags will be prioritized over other lags. Finally, the Elementwise HLAG allows for the most flexibility, allowing each series in each marginal model to have its own maximum lag order resulting in \\(k^2\\) possible lag orders. In addition to the HLAG and VARX-L frameworks we also include two non-convex penalties: Smoothly Clipped Absolute Deviation (SCAD) and Minimax Concave Penalty (MCP). These penalties serve to obviate the bias of lasso-type penalties, which tend to “over-regularize”, by decreasing the amount of penalization as the magnitude of the coefficient decreases. Though these penalties are not convex, the coordinate descent algorithm developed by Breheny and Huang (2011) fits seamlessly into the \\({\\tt BigVAR}\\) framework. Along with SCAD and MCP, we also incorporate the Bayesian VAR developed by Bańbura, Giannone, and Reichlin (2010) into the BigVAR framework under the name \\({\\tt BGR}\\).\n\nOur package allows for the straightforward estimation of the aforementioned VARX-L and HLAG procedures. The end-user completely specifies their model by constructing an object of class \\({\\tt BigVAR}\\). To construct an object of class \\({\\tt BigVAR}\\), simply run the command \\({\\tt constructModel}\\): data(Y) # Create a Basic VAR-L (Lasso Penalty) with maximum lag order p=4, 10 grid points with lambda optimized according to rolling validation of 1-step ahead MSFE mod1<-constructModel(Y,p=4,\"Basic\",gran=c(150,10),h=1,cv=\"Rolling\",verbose=FALSE,IC=TRUE,model.controls=list(intercept=TRUE)) The first four arguments below are required; the rest only need to be specified if expanded or non-standard functionality is requested. The first 7 can be applied to VAR and VARX models, EFX can only be applied toward VARX models, the remaining 5 are only applicable to VAR models.\n• None \\({\\tt gran}\\): two options for the grid of penalty parameters \\(\\lambda\\). The first option controls the depth of the lambda grid (a good default option is 50). The second option controls the number of grid values (a good default is 10). If your grid does not go deep enough, your forecasts results may be suboptimal, but if it is too deep, the routine may take a substantial amount of time to run. The index of the optimal penalty parameter is monitored by \\({\\tt cv.BigVAR}\\). If it is on the border of the grid, it is recommended to re-run the function with a larger granularity parameter. If you set the option \\({\\tt ownlambdas}\\) to \\({\\tt TRUE}\\), \\({\\tt gran}\\) is used to supply a user defined grid of lambdas. For more details on the granularity parameter, see Diagnostics.\n• None \\({\\tt h}\\): Forecast horizon in which to optimize (default 1).\n• None \\({\\tt verbose}\\): Logical, if \\({\\tt TRUE}\\), will display a progress bar for the validation and evaluation procedures.\n• None \\({\\tt IC}:\\) Logical, if \\({\\tt TRUE}\\), will return AIC and BIC VAR(X) benchmarks.\n• None \\({\\tt VARX}:\\) VARX characteristics in a list form. The list must contain two arguments:\n• None \\({\\tt window.size}\\): Size of window for rolling cv. (Default is 0; resulting in an expanding window).\n• None \\({\\tt cv}:\\) Type of validation (as described in Penalty Parameter Selection) used to select the penalty parameter (options are “rolling” (default) and “LOO,” a pseudo “leave one out” cv procedure that respects time dependence).\n• None \\({\\tt ownlambdas}:\\) Logical, Indicator as to whether the user supplied a penalty grid in slot \\({\\tt gran}\\) (default \\({\\tt FALSE}\\)).\n• None \\({\\tt recursive}\\) Whether recursive (iterated) or direct forecasts are used for multi-step VAR predictions (default FALSE, indicating direct forecasts are used). Note that only direct forecasts are available for VARX models. For more information on the distinction consult Marcellino, Stock, and Watson (2006).\n• None \\({\\tt \\verb|separate_lambdas|}\\): Logical, indicator to use separate penalty parameters for each series. This option is only valid for the structures \\({\\tt Basic, BasicEN,HLAG,HLAGOO,HLAGELEM, SCAD, MCP}\\).\n• None \\({\\tt \\verb|model.controls|}\\) As the capabilities of BigVAR have expanded, we have decided to consolidate parameters into the list model.controls. These parameters include:\n• : Logical, indicator as to whether an intercept should be fit (default ). (Note that the intercept is fit separately and not subject to regularization).\n• : Logical, option to select a pseudo “Minnesota Prior” (shrinks series toward a known constant matrix ; useful for mildly non-stationary data)\n• : vector of coefficients to shrink toward (only used if is , default is , corresponding to a vector random walk).\n• : Logical, option to use our relaxed VAR(X) procedure to re-fit the nonzero support selected by a model via least squares (default ).\n• : Numeric or vector; user defined denoting the trade-off between and penalties (in the Sparse Lag and Sparse Own/Other structures) or the trade-off between the lasso and ridge penalties in the elastic net structure. Defaults to if not specified.\n• None \\({\\tt \\verb|rolling_oos|}\\) Logical, option to re-determine the optimal penalty parameter following each iteration of rolling cross validation, (see Rolling Extension), default \\({\\tt FALSE}\\). One we construct the model object, we can run \\({\\tt \\verb|cv.BigVAR(mod1)|}\\), which selects the optimal penalty parameter via a validation procedure, evaluates its forecast accuracy, and compares it against conditional mean, random walk, AIC, and BIC VAR(X) benchmarks over an out-of-sample period \\({\\tt T_2}\\) through \\({\\tt T}\\). results=cv.BigVAR(mod1) results #> *** BIGVAR MODEL Results *** #> Structure #> [1] \"Basic\" #> Loss #> [1] \"L2\" #> Forecast Horizon #> [1] 1 #> Minnesota VAR #> [1] FALSE #> Maximum Lag Order #> [1] 4 #> Optimal Lambda #> [1] 0.02764 #> Grid Depth #> [1] 150 #> Index of Optimal Lambda #> [1] 10 #> Fraction of active coefficients #> [1] 0.8489 #> In-Sample Loss #> [1] 0.045 #> BigVAR Out of Sample Loss #> [1] 0.0373 #> *** Benchmark Results *** #> Conditional Mean Out of Sample Loss #> [1] 0.244 #> AIC Out of Sample Loss #> [1] 0.0396 #> BIC Out of Sample Loss #> [1] 0.0396 #> RW Out of Sample Loss #> [1] 0.19 In order to account for time-dependence, penalty parameter selection is conducted in a rolling manner. Define time indices \\(T_1=\\left \\lfloor \\frac{T}{3} \\right\\rfloor\\), and \\(T_2=\\left\\lfloor \\frac{2T}{3} \\right\\rfloor\\) The training period \\(T_1+1\\) through \\(T_2\\) is used to select \\(\\lambda\\), \\(T_2+1\\) through \\(T\\) is for evaluation of forecast accuracy in a rolling manner. The process is visualized in the following figure Define \\(\\hat{\\mathbf{y}}_{t+1}^{\\lambda}\\) as the one-step ahead forecast based on \\(\\mathbf{y}_1,\\dots\\mathbf{y}_t\\). We choose \\(\\lambda\\) based on minimizing one-step ahead mean square forecast error (MSFE) over the training period: MSFE\\((\\lambda)=\\frac{1}{(T_2-T_1-1)}\\sum_{t=T_1}^{T_2-1} \\|\\hat{\\mathbf{y}}_{t+1}^{\\lambda}-\\mathbf{y}_{t+1}\\|_F^2.\\) Though we select \\(\\lambda\\) based on minimizing one-step ahead mean squared forecast error (MSFE) by default, this can be easily generalized to longer forecast horizons (by adjusting \\({\\tt h}\\) in \\({\\tt constructModel}\\)) or alternative loss functions (by adjusting \\({\\tt loss}\\) in the \\({\\tt model.controls}\\) list within \\({\\tt constructModel}\\)). By default, the selected penalty parameter is fixed throughout the forecast evaluation period. In certain applications, it may be more appropriate to allow for a greater degree of flexibility. Consequently, by setting \\({\\tt \\verb|rolling_oos|}\\) to \\({\\tt TRUE}\\) in \\({\\tt constructModel}\\) we allow for the penalty parameter to be re-evaluated using a rolling window following each iteration in the forecast evaluation period, as depicted in the following figure As an alternative to rolling validation, we also offer a psuedo “leave-one-out” selection approach that respects the intrinsic time ordering of the VARX model. This procedure iterates through the data in the same manner as rolling validation. However, at each iteration \\(t\\), the row \\(\\mathbf{y}_t\\) is removed from consideration when constructing the VARX lag matrix and instead used as a test set. Every other row up to \\(T_2\\) is used for training, as visualized in the following figure This procedure is particularly amenable relative to rolling validation in scenarios with limited data. Generally, the only potential post-hoc diagnostic procedures are adjusting the depth/size of the penalty grid as well as the maximum lag order. We suggest setting the maximum lag order based on the frequency of the data (e.g. 4 for quarterly, 12 for monthly, etc). The method \\({\\tt \\verb|cv.BigVAR|}\\) method returns an object of class \\({\\tt \\verb|BigVAR.results|}\\). This object inherits the properties of class \\({\\tt BigVAR}\\) and contains both in and out-of-sample diagnostic information. For information on all fields, consult the package manual. \\({\\tt BigVAR.results}\\) also has a plot method, to show a comparison of in-sample MSFE over the grid of \\(\\lambda\\) values. Generally, you want this graph to have a parabolic shape with the optimal value in one of the middle indices. In this scenario, since the slope of the line is very flat, it is likely that increasing the depth of the grid (i.e. the first parameter of \\({\\tt gran}\\) in \\(\\tt{constructModel}\\)) would not substantially improve forecasts. It is not recommended to make the depth too large as it substantially increases computation time. However, since the slope of the line in this case is quite steep, it is likely that forecasts will be improved by increasing the depth. As evidenced above, this plot does not always take on a parabolic shape. On occasion, when the grid is very deep, it will start to level off. In this scenario, it is best to decrease the depth of the grid. We can also view the sparsity pattern of the final estimated coefficient matrix with \\({\\tt \\verb|SparsityPlot.BigVAR.results|}\\) Finally, out of sample predictions can be computed with \\({\\tt predict}\\) 95 percent confidence intervals can be returned with the option \\({\\tt confint=TRUE}\\) A formatted dataframe of the coefficient matrix of the final iteration of forecast evaluation can be obtained via the \\({\\tt coef}\\) method \\({\\tt Y}\\), the sparse multivariate time series included with \\({\\tt BigVAR}\\) was generated using matrix \\(\\mathbf{A}\\), included with \\({\\tt BigVAR}\\) as \\({\\tt Generator}\\). The sparsity structure of \\(\\mathbf{A}\\) is visualized in the following plot data(Y) # simulated multivariate time series # coefficient matrix used to generate Y data(Generator) # note that coefficients with a darker shade are larger in magnitude SparsityPlot(A[1:3,],p=4,3,s=0,m=0,title=\"Sparsity Structure of Generator Matrix\") In order to generate multivariate VARs, we transform \\(k\\times kp\\) coefficient matrix to its multiple companion form (i.e. converting to a \\(kp\\times kp\\) matrix representing a VAR of lag order 1). For details, consult page 15 of Lütkepohl (2005).\n\nIn certain scenarios, it may be overly cumbersome to construct a \\(\\tt{BigVAR}\\) object and perform rolling validation or lambda grid construction (for example, out-of-sample testing once an “optimal” penalty parameter has been selected). As an alternative, we include the function \\(\\tt{\\verb|BigVAR.fit|}\\) which will fit a \\({\\tt BigVAR}\\) model with a fixed penalty parameter without requiring the construction of a \\({\\tt BigVAR}\\) object. If, instead of rolling or “leave one out” validation, you wish to use a custom procedure to set the penalty parameters, you can do so using repeated calls to \\({\\tt \\verb|BigVAR.fit|}\\). As an example, we provide a N-fold cross validation function (which does not respect time dependence). # N-fold cross validation for VAR # Y: data # nfolds: number of cross validation folds # struct: penalty structure # p: lag order # nlambdas: number of lambdas: # gran1: depth of lambda grid # seed: set to make it reproducible NFoldcv <- function(Y,nfolds,struct,p,nlambdas,gran1,seed) { MSFE <- matrix(0,nrow=nrow(Y),ncol=10) A <- constructModel(Y,p,struct=struct,gran=c(gran1,nlambdas),verbose=F) # construct lag matrix Z1 <- VARXLagCons(Y,X=NULL,s=0,p=p,0,0) trainZ <- Z1$Z[2:nrow(Z1$Z),] trainY <- matrix(Y[(p+1):nrow(Y),],ncol=ncol(Y)) set.seed(seed) inds <- sample(nrow(trainY)) B <- BigVAR.est(A) lambda.grid <- B$lambda folds <- cut(inds,breaks=nfolds,labels=FALSE) MSFE <- matrix(0,nrow=nfolds,ncol=nlambdas) for(i in 1:nfolds){ test <- trainY[which(folds==i),] train <- trainY[which(folds!=i),] testZ <-t(t(trainZ)[which(folds!=i),]) B=BigVAR.fit(train,p=p,lambda=lambda.grid,struct='Basic') #iterate over lambdas for(j in 1:nlambdas){ MSFETemp <- c() for(k in 1:nrow(test)) { tempZ <- testZ[,k,drop=FALSE] bhat <- matrix(B[,2:dim(B)[2],j],nrow=ncol(Y),ncol=(p*ncol(Y))) preds <- B[,1,j]+bhat%*%tempZ MSFETemp <- c(MSFETemp,sum(abs(test[k,]-preds))^2) } MSFE[i,j] <- mean(MSFETemp) } } return(list(MSFE=MSFE,lambdas=lambda.grid)) } # 10 fold cv MSFEs<-NFoldcv(Y,nfolds=10,\"Basic\",p=5,nlambdas=10,gran1=50,seed=2000) # choose smaller lambda in case of ties (prevents extremely sparse solutions) opt=MSFEs$lambda[max(which(colMeans(MSFEs$MSFE)==min(colMeans(MSFEs$MSFE))))] opt #> [1] 5.60583 We have noticed a relative dearth of packages that allow for the estimation and forecasting of VAR and VARX models via least squares with lag order selected according to an information criterion. By default, \\(\\tt{cv.BigVAR}\\) returns least squares AIC and BIC benchmarks as forecast comparison. \\(\\tt{VARXFit}\\) will fit a VAR or VARX model with pre-specified maximum lag orders and the function \\(\\tt{VARXForecastEval}\\) will evaluate the forecasting performance of VAR and VARX models with information criterion selected by AIC or BIC over a user-specified time horizon. The arguments to \\(\\tt{VARXForecastEval}\\) are detailed below:\n• : Logical, indicator to use “iterated” (default indicating that direct forecasts are used). Note that one may encounter scenarios in which the number of least squares VARX parameters \\((k^2\\times p+m*k*s) > (k+m)T\\). Our algorithm will terminate lag order selection as soon as the problem becomes ill-posed. In the event that the problem is ill-conditioned at \\(p=1\\), the algorithm will always return a lag order of zero. An example usage of \\(\\tt{VARXForecastEval}\\) is below. In addition, one-step ahead predictions from VARX models can be computed using the function \\(\\tt{PredictVARX}\\). The choice of structured penalty is not always clear at the outset of the forecasting problem. Since our methods are all computationally manageable across most dimensions, one approach that we recommend is to use a subset of the data to fit models with all applicable structured penalties and find the set of “superior models” using the \\(\\tt{MCSProcedure}\\) function from the package \\(\\tt{MCS}\\). For more information about the package and procedure consult Bernardi and Catania (2018) and the original paper Hansen, Lunde, and Nason (2011). We will start by simulating a \\(\\text{VAR}_3(6)\\) The first lag matrix and the own lags in the fifth coefficient matrix are the only nonzero entries. This suggests that the structures incorporating an Own/Other or Lag type penalty will achieve the best forecast performance. There is no within-group sparsity so we should not expect the Sparse counterparts to be in the set of superior models. library(MCS) # train on first 250 observations YTrain=Y[1:250,] Loss <- c() T1=1*floor(nrow(YTrain)/3);T2=2*floor(nrow(YTrain)/3) p=8 structures<-c(\"Basic\",\"BasicEN\",\"Lag\",\"SparseLag\",\"OwnOther\",\"HLAGC\",\"HLAGOO\",\"HLAGELEM\",\"MCP\",\"SCAD\") for(i in structures){ # construct BigVAR object; we will perform a dual grid search for the sparse lag and sparse own/other models if(i%in%c(\"SparseLag\",\"SparseOO\")){ alpha=seq(0,1,length=10) }else{ alpha=0 } A<- constructModel(YTrain,p=p,struct=i,gran=c(100,10),T1=T1,T2=T2,verbose=FALSE,model.controls=list(intercept=FALSE,alpha=alpha)) # perform rolling cv res<- cv.BigVAR(A) # save out of sample loss for each structure Loss <- cbind(Loss,res@OOSMSFE) } # construct AIC and BIC benchmarks BIC <- VARXForecastEval(YTrain,matrix(0,nrow=nrow(YTrain)),p,0,T2,nrow(YTrain),\"BIC\",1)$MSFE AIC <- VARXForecastEval(YTrain,matrix(0,nrow=nrow(YTrain)),p,0,T2,nrow(YTrain),\"AIC\",1)$MSFE Loss <- as.data.frame(Loss) names(Loss) <- structures Loss <- cbind(Loss,BIC,AIC) names(Loss)[(ncol(Loss)-1):ncol(Loss)] <- c(\"BIC\",\"AIC\") names(Loss) <- paste0(names(Loss),\"L\") mcs.test <- MCSprocedure(as.matrix(Loss),verbose=FALSE) mcs.test #> #> ------------------------------------------ #> - Superior Set of Models - #> ------------------------------------------ #> Rank_M v_M MCS_M Rank_R v_R MCS_R Loss #> LagL 2 1.511579 0.1336 2 1.511579 0.1336 0.01397682 #> OwnOtherL 1 -1.511579 1.0000 1 -1.511579 1.0000 0.01376844 #> #> Details #> ------------------------------------------ #> #> Number of eliminated models : 10 #> Statistic : Tmax #> Elapsed Time : Time difference of 17.68301 secs As expected, we find that the set of superior models contains only the Own/Other VAR-L and Lag VAR-L. Though \\({\\tt BigVAR}\\) is primarily designed to forecast high-dimensional time series, it can also be of use in analyzing the joint dynamics of a group of interrelated time series. In order to conduct policy analysis, many macroeconomists make use of VARs to examine the impact of shocks to certain variables on the entire system (holding all other variables fixed). This is know as impulse response analysis. For example, a macroeconomist may wish to analyze the impact of a 100 basis point increase in the Federal Funds Rate on all included series over the next 8 quarters. To do so, we can utilize the function \\({\\tt generateIRF}\\), which converts the last estimated \\({\\tt BigVAR}\\) coefficient matrix to fundamental form. We use the following function to generate an impulse response function: suppressMessages(library(expm)) # Phi k x kp coefficient matrix # sigma kxk residual covariance matrix # n number of time steps to run IRF # p lag order # k number of series # Y0: k dimensional vector reflecting initialization of the IRF generateIRF <- function(Phi,Sigma,n,k,p,Y0) { if(p>1){ A <-VarptoVar1MC(Phi,p,k) }else{ A <- Phi } J <- matrix(0,nrow=k,ncol=k*p) diag(J) <- 1 P <- t(chol(Sigma)) IRF <- matrix(0,nrow=k,ncol=n+1) for(i in 0:n) { phi1 <- J%*%(A%^%i)%*%t(J) theta20 <- phi1%*%P IRF[,i+1] <- (theta20%*%Y0) } return(IRF) } require(quantmod) #> Loading required package: quantmod #> Loading required package: xts #> Loading required package: zoo #> #> Attaching package: 'zoo' #> The following objects are masked from 'package:base': #> #> as.Date, as.Date.numeric #> Loading required package: TTR #> Registered S3 method overwritten by 'quantmod': #> method from #> as.zoo.data.frame zoo require(zoo) # get GDP, Federal Funds Rate, CPI from FRED #Gross Domestic Product (Relative to 2000) getSymbols('GDP',src='FRED',type='xts') #> 'getSymbols' currently uses auto.assign=TRUE by default, but will #> use auto.assign=FALSE in 0.5-0. You will still be able to use #> 'loadSymbols' to automatically load data. getOption(\"getSymbols.env\") #> and getOption(\"getSymbols.auto.assign\") will still be checked for #> alternate defaults. #> #> This message is shown once per session and may be disabled by setting #> options(\"getSymbols.warning4.0\"=FALSE). See ?getSymbols for details. #> [1] \"GDP\" GDP<- aggregate(GDP,as.yearqtr,mean) GDP <- GDP/mean(GDP[\"2000\"])*100 # Transformation Code: First Difference of Logged Variables GDP <- diff(log(GDP)) index(GDP) <- as.yearqtr(index(GDP)) # Federal Funds Rate getSymbols('FEDFUNDS',src='FRED',type='xts') #> [1] \"FEDFUNDS\" FFR <- aggregate(FEDFUNDS,as.yearqtr,mean) # Transformation Code: First Difference FFR <- diff(FFR) # CPI ALL URBAN CONSUMERS, relative to 1983 getSymbols('CPIAUCSL',src='FRED',type='xts') #> [1] \"CPIAUCSL\" CPI <- aggregate(CPIAUCSL,as.yearqtr,mean) CPI <- CPI/mean(CPI['1983'])*100 # Transformation code: difference of logged variables CPI <- diff(log(CPI)) # Seasonally Adjusted M1 getSymbols('M1SL',src='FRED',type='xts') #> [1] \"M1SL\" M1<- aggregate(M1SL,as.yearqtr,mean) # Transformation code, difference of logged variables M1 <- diff(log(M1)) # combine series Y <- cbind(CPI,FFR,GDP,M1) names(Y) <- c(\"CPI\",\"FFR\",\"GDP\",\"M1\") Y <- na.omit(Y) k=ncol(Y) T <- nrow(Y) # start/end of rolling validation T1 <- which(index(Y)==\"1985 Q1\") T2 <- which(index(Y)==\"2005 Q1\") #Demean Y <- Y - (c(rep(1, nrow(Y))))%*%t(c(apply(Y[1:T1,], 2, mean))) #Standarize Variance for (i in 1:k) { Y[, i] <- Y[, i]/apply(Y[1:T1,], 2, sd)[i] } library(expm) # Fit an Elementwise HLAG model Model1=constructModel(as.matrix(Y),p=4,struct=\"HLAGELEM\",gran=c(25,10),verbose=FALSE,VARX=list(),T1=T1,T2=T2) Model1Results=cv.BigVAR(Model1) # generate IRF for 10 quarters following a 1 percent increase in the federal funds rate IRFS <- generateIRF(Phi=Model1Results@betaPred[,2:ncol(Model1Results@betaPred)],Sigma=cov(Model1Results@resids),n=10,k=ncol(Y),p=4,Y0=c(0,.01,0,0)) IRFS <- generateIRF(Model1Results@betaPred[,2:ncol(Model1Results@betaPred)],cov(Model1Results@resids),10,4,4,c(0,.01,0,0)) The impulse responses generated from this “shock” are depicted below."
    },
    {
        "link": "https://econometrics-with-r.org/16.1-vector-autoregressions.html",
        "document": ""
    },
    {
        "link": "https://numberanalytics.com/blog/practical-implementation-vector-autoregression",
        "document": "Vector Autoregression (VAR) has emerged as a robust and versatile technique in the field of time series analysis. Its ability to capture dynamic relationships among multiple variables makes it a go-to method for both researchers and practitioners dealing with complex economic, financial, or environmental data. In this post, we explore the practical implementation of VAR—from understanding its foundational aspects to the detailed nuances of model building and tuning, culminating in real-world applications. We help you navigate the full spectrum of VAR analysis in modern time series studies.\n\nVector Autoregression models provide an empirical framework for capturing the linear interdependencies among multiple time series. Unlike univariate autoregressive models that consider a single variable, VAR allows simultaneous tracking of multiple variables, making it particularly valuable in understanding economic and financial systems.\n• is a vector containing the time series variables.\n• is a vector of error terms (often assumed to be white noise).\n• denotes the number of lags included in the model.\n\nThis mathematical framework underpins how current endogenous variables are influenced by their own lagged values along with those of other variables. The dynamic structure and interconnectedness are why VAR is widely celebrated in econometrics and other specialized areas.\n\nThe journey toward a successful VAR analysis starts with the right computational environment. Various software packages, including R, Python (statsmodels, pmdarima), and MATLAB, offer dedicated functionalities for time series analysis.\n• Python: With libraries such as Statsmodels and its VAR class, Python is a powerful tool for econometric modeling.\n• R: The “vars” package is particularly popular for VAR implementations, offering extensive diagnostic tools.\n• MATLAB: Known for its robustness in numerical computing, MATLAB provides additional toolboxes that facilitate VAR modeling.\n• Install Required Libraries:\n\n For Python users, this might involve installing packages using pip:\n• Data Acquisition and Cleaning:\n\n Data preparation is crucial. Ensure your data is clean (no missing values) and has consistent sampling intervals.\n• Time Series Decomposition:\n\n Visualize and decompose the time series to understand trends, seasonality, and cyclic behaviors. This step helps in making informed decisions regarding stationarity—an essential property for VAR.\n\nData quality directly impacts the performance and credibility of any statistical model. For VAR, consider the following procedures:\n\nMany time series models, including VAR, require stationarity. A stationary series has constant mean and variance over time. One effective method of achieving stationarity is differencing the series:\n\nThe Augmented Dickey-Fuller (ADF) test is widely used to test for stationarity. If the p-value is below a certain threshold (commonly 0.05), the null hypothesis of a unit root is rejected.\n\nTransformations (like logarithms) may be applied to stabilize the variance. Standardization (subtracting the mean and dividing by the standard deviation) also helps improve empirical stability across variables.\n\nChoosing the right lag order is critical. Information criteria such as the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) provide guidance:\n\nwhere σ^p2​ is the estimated error variance for lag p, k is the number of estimated parameters, and T is the sample size.\n\nWith a solid understanding of VAR and a well-prepared dataset, you are now set to dive into the more technical aspects of model building.\n\nHaving set the stage, we now delve into the process of constructing a VAR model, starting with variable selection and moving onto estimation. This step-by-step guide is designed to bring clarity to the process, ensuring robust results throughout.\n\nBefore running the model, it is essential to identify the endogenous variables that capture the essential dynamics of the system. Select those based on theoretical relationships and empirical evidence.\n\nThe lag order p determines how many past periods influence the current state. Use lag selection criteria (AIC, BIC, or even the Hannan-Quinn criterion) to choose a lag length that minimizes the estimated error variance without overfitting. For instance, if you have determined that p=2 is optimal, your VAR model would look like:\n\nOnce variables and lags are selected, parameter estimation begins. Estimation of VAR models typically employs Ordinary Least Squares (OLS) for each equation in the system. The reasoning is that, under certain conditions, OLS provides consistent estimates even in multivariate settings.\n\nFor each equation, the OLS estimator is given by:\n• contains the current values for the variable,\n\nThis systematic approach across each variable in the vector streamlines the estimation process.\n\nA simple implementation using Python’s statsmodels might look as follows:\n\nThis snippet demonstrates the iterative process of fitting a VAR model. In R, similar steps are followed using the “vars” package, ensuring that the software environment supports these fundamental operations.\n\nAfter estimating parameters, it is imperative to assess the VAR model’s adequacy. Diagnostic tests include:\n• Granger Causality Tests: To check if one time series can predict another.\n• Impulse Response Functions (IRF): Evaluate the effect of shocks on the system.\n• Variance Decomposition: Determine the influence share of each shock over time.\n\nFor example, to calculate the impulse response:\n\nThis impulse response function shows how a shock at time t affects the system up to a horizon h.\n\nWhen all diagnostic results align with theoretical expectations, the VAR model can be considered a reliable tool for further applications or forecasting.\n\nAchieving a high-performing VAR model requires continuous refinement. Model tuning and statistical validation are crucial to ensure the robustness and reliability of forecasts.\n\nEven after initial estimation, the selected variables and lag parameters might need further scrutiny. Parameter refinement can involve re-evaluating the lag structure or even reconsidering the variable set based on out-of-sample performance. Automated methods (like grid search) can help explore the parameter space more efficiently.\n\nUnlike typical cross-validation methods for independent observations, time series cross-validation accounts for the sequential nature of data. A common technique is the rolling window approach. Suppose a forecast origin moves forward one time step at a time while re-estimating the model:\n\nThis method helps detect any overfitting issues and ensures that the model maintains its predictive power over time.\n\nValidation is an essential step in VAR modeling:\n\nCheck the residuals to ensure they approximate white noise. If significant autocorrelation is present in the residuals, it might indicate that additional lags or variables are needed. Plotting the autocorrelation function (ACF) or conducting the Ljung-Box test can help:\n\nwhere ρ^​k​ is the autocorrelation of the residuals at lag k.\n\nForecast accuracy should be evaluated both in-sample and out-of-sample. Metrics like Mean Squared Error (MSE) or Mean Absolute Percentage Error (MAPE) quantify accuracy:\n\nComparing MSE across various models ensures that the selected VAR model truly captures the underlying dynamics without overfitting.\n\nThrough continuous refinement and diagnostic checks, the model’s ability to forecast effectively improves. Often, a slight recalibration of lag parameters or inclusion/exclusion of variables—based on validation feedback—can significantly enhance model performance. Combining multiple diagnostics provides a holistic view of model efficiency, ensuring insights are robust and actionable.\n\nVAR models extend beyond academic exercises. Their practical applications range from economic policymaking to risk management in finance. This section highlights how VAR can be applied in real-world scenarios and the tools that facilitate its implementation.\n\nModern VAR analysis tools are designed with user-friendliness and robustness in mind. Below is an overview of popular platforms:\n• R – vars Package:\n\n Provides comprehensive functions for model estimation, diagnostic testing, and plotting impulse responses.\n\n Example:\n• Python – Statsmodels:\n\n The module in Python is widely used due to its flexibility and efficiency.\n• MATLAB – Econometrics Toolbox:\n\n Contains built-in functions for VAR, forecasting, and further econometric analysis.\n• Preprocessing is Key:\n\n Always dedicate time to data cleaning and transformation before modeling.\n• Iterative Improvements:\n\n Continuously incorporate economic theory and domain insights when selecting variables and tuning parameters.\n• Parallelization:\n\n For large datasets, consider parallel computing to speed up model estimation processes.\n\nConsider a scenario where VAR is applied to the analysis of macroeconomic factors such as inflation, interest rates, and gross domestic product (GDP). These variables often interact in complex ways:\n• Theoretical Foundation:\n\n Economic theory suggests that inflation might influence GDP, while GDP dynamics affect interest rate settings by central banks.\n• \n• Data Collection: Monthly or quarterly datasets for these economic indicators.\n• Stationarity Tests: Diff-specify or transform the series to achieve stationarity.\n• Lag Selection: Often using AIC/BIC suggests a VAR(2) or VAR(3) model.\n• Diagnostics: Utilize impulse response functions to observe how inflation shocks propagate over time.\n• \n• Policy Implications:\n• Identification of leading indicators among the variables enables better forecasting and policy interventions.\n• Risk Management:\n• In finance, VAR models help assess the impact of economic shocks on portfolios, guiding hedging strategies or risk assessments.\n\nThis case study exemplifies how VAR can guide informed decision-making by integrating theoretical expectations with empirical validations.\n\nTo further explore VAR and its multifaceted applications, consider the following resources:\n• Books:\n• “Time Series Analysis” by James D. Hamilton offers deep insights into VAR and multivariate time series modeling.\n• Online Courses:\n• Platforms like Coursera and edX provide courses that cover time series econometrics extensively.\n• Research Papers:\n• Numerous journals and working papers detail advanced methodologies and case studies, providing a wealth of insights for practitioners.\n\nAdditionally, participating in communities such as Stack Overflow or specialized forums can help you stay updated on best practices and emerging trends in VAR analysis.\n\nVector Autoregression is an indispensable tool in modern time series analysis. From its foundational theory to practical implementation, VAR’s versatility allows it to model complex systems with interdependent variables, providing invaluable insights for forecasting and policy evaluation.\n\nIn this article, we started by establishing a solid understanding of VAR concepts and prepared the data meticulously—emphasizing the importance of stationarity and proper lag selection. We then walked through a detailed process of model building, including parameter estimation and rigorous diagnostic testing. Finally, we explored how tuning, validation, and real-world applications of VAR play a crucial role in extracting meaningful insights from data.\n\nWhether you are working in economic policy, financial risk management, or environmental studies, the VAR framework equips you with the analytical processes needed to capture dynamic interrelations in your data. By leveraging modern software tools and staying updated on methodological advancements, you can ensure that your time series analysis remains relevant and robust in today’s complex data environments.\n\nAs you continue to refine your approach, remember that the power of VAR lies not just in its mathematical elegance but in its practical utility—transforming raw data into actionable insights, one lag at a time.\n\nNote: Mathematical expressions in this article are rendered using LaTeX syntax. If you’re using a platform that supports math rendering in markdown, these expressions should display correctly.\n• SPSS and SAS: For professionals and academics, IBM SPSS and SAS provide powerful statistical analysis tools that simplify complex data processing with advanced diagnostic capabilities. Both software packages feature a traditional menu-driven user interface (UI/UX), making them accessible for users who prefer a point-and-click approach over coding-based workflows.\n• Number Analytics: Number Analytics is an AI-powered statistical software that automates statistical model selection, result interpretation, and report documentation. Designed for business professionals with limited statistical background, it simplifies complex analyses with an intuitive, user-friendly approach. Try Number Analytics. (Number Analytics)"
    },
    {
        "link": "http://library.virginia.edu/data/articles/understanding-robust-standard-errors",
        "document": "What are robust standard errors? How do we calculate them? Why use them? Why not use them all the time if they’re so robust? Those are the kinds of questions this post intends to address.\n\nTo begin, let’s start with the relatively easy part: getting robust standard errors for basic linear models in Stata and R. In Stata, simply appending to the end of regression syntax returns robust standard errors. “vce” is short for “variance-covariance matrix of the estimators.” “robust” indicates which type of variance-covariance matrix to calculate. Here’s a quick example using the auto data set that comes with Stata 16:\n\nNotice the third column indicates “robust” standard errors.\n\nTo replicate the result in R takes a bit more work. First we load the haven package to use the function that allows us to import Stata data sets. Then we load two more packages: lmtest and sandwich. The lmtest package provides the function that allows us to re-calculate a coefficient table using a different variance-covariance matrix. The sandwich package provides the function that allows us to calculate robust standard errors. The argument allows us to specify what kind of robust standard errors to calculate. “HC1” is one of several types available in the sandwich package and happens to be the default type in Stata 16. (We talk more about the different types and why it’s called the “sandwich” package below.)\n\nIf you look carefully, you’ll notice the standard errors in the R output match those in the Stata output.\n\nIf we want 95% confidence intervals like those produced in Stata, we need to use the function:\n\nWhile not really the point of this post, we should note the results say that larger turn circles and bigger trunks are associated with lower gas mileage.\n\nNow that we know the basics of getting robust standard errors out of Stata and R, let’s talk a little about why they’re robust by exploring how they’re calculated.\n\nThe usual method for estimating coefficient standard errors of a linear model can be expressed with this somewhat intimidating formula:\n\nwhere \\(X\\) is the model matrix (i.e., the matrix of the predictor values) and \\(\\Omega = \\sigma^2 I_n\\), which is shorthand for a matrix with nothing but \\(\\sigma^2\\) on the diagonal and 0’s everywhere else. Two main things to notice about this equation:\n• The \\(X^T \\Omega X\\) in the middle\n• The \\((X^TX)^{-1}\\) on each side\n\nSome statisticians and econometricians refer to this formula as a \"sandwich\" because it's like an equation sandwich: We have \"meat\" in the middle, \\(X^T \\Omega X\\), and \"bread\" on the outside, \\((X^TX)^{-1}\\). Calculating robust standard errors means substituting a new kind of \"meat.\"\n\nBefore we do that, let's use this formula by hand to see how it works when we calculate the usual standard errors. This will give us some insight to the meat of the sandwich. To make this easier to demonstrate, we'll use a small toy data set.\n\nNotice the way we generated . It is simply the number 5 with some random noise from a \\(N(0,1.2)\\) distribution plus the number 35. There is no relationship between and . However, when we regress on using we get a slope coefficient of about 5.2 that appears to be “significant”.\n\nClearly the 5th data point is highly influential and driving the “statistical significance,” which might lead us to think we have specified a “correct” model. Of course, we know that we specified a “wrong” model because we generated the data. does not have a relationship with ! It would be nice if we could guard against this sort of thing from happening: specifying a wrong model but getting a statistically significant result. One way we could do that is modifying how the coefficient standard errors are calculated.\n\nLet’s see how they were calculated in this case using the formula we specified above. Below is \\(\\sigma^2\\), is \\(I_n\\), and is the model matrix. Notice we can use the base R function to get the model matrix from a fitted model. We save the formula result into , which is the variance-covariance matrix. Finally we take square root of the diagonal elements to get the standard errors output in the model summary.\n\nNow let’s take a closer look at the “meat” in this sandwich formula:\n\nThat is a matrix of constant variance. This is one of the assumptions of classic linear modeling: The errors (or residuals) are drawn from a single normal distribution with mean 0 and a fixed variance. The object above is the estimated variance of that normal distribution. But what if we modified this matrix so that the variance was different for some observations? For example, it might make sense to assume the error of the 5th data point was drawn from a normal distribution with a larger variance. This would result in a larger standard error for the slope coefficient, indicating greater uncertainty in our coefficient estimate.\n\nThis is the idea of “robust” standard errors: modifying the “meat” in the sandwich formula to allow for things like non-constant variance (and/or autocorrelation, a phenomenon we don’t address in this post).\n\nSo how do we automatically determine non-constant variance estimates? It might not surprise you that there are several ways. The sandwich package provides seven different types at the time of this writing (version 2.5-1). The default version in Stata is identified in the sandwich package as “HC1”. The HC stands for heteroskedasticity-consistent. Heteroskedasticity is another word for non-constant. The formula for “HC1” is as follows:\n\nwhere \\(\\hat{\\mu}_i^2\\) refers to squared residuals, \\(n\\) is the number of observations, and \\(k\\) is the number of coefficients. In our simple model above, \\(k = 2\\), since we have an intercept and a slope. Let’s modify our formula above to substitute HC1 “meat” in our sandwich:\n\nNotice we no longer have constant variance for each observation. The estimated variance is instead the residual squared multiplied by (5/3). When we use this to estimate “robust” standard errors for our coefficients, we get slightly different estimates.\n\nNotice the slope standard error actually got smaller. It looks like the HC1 estimator may not be the best choice for such a small sample. The default estimator for the sandwich package is known as “HC3”:\n\nwhere \\(h_i\\) are the hat values from the hat matrix. A Google search or any textbook on linear modeling can tell you more about hat values and how they’re calculated. For our purposes, it suffices to know that they range from 0 to 1, and that larger values are indicative of influential observations. We see then that HC3 is a ratio that will be larger for values with high residuals and relatively high hat values. We can manually calculate the HC3 estimator using the base R and functions as follows:\n\nNotice that the 5th observation has a huge estimated variance of about 721. When we calculate the robust standard errors for the model coefficients we get a much bigger standard error for the slope.\n\nOf course we wouldn’t typically calculate robust standard errors by hand like this. We would use the function in the sandwich package as we demonstrated at the beginning of this post along with the function from the lmtest package. (Or use in Stata.)\n\nNow the slope coefficient estimate is no longer “significant” since the standard error is larger. This standard error estimate is robust to the influence of the outlying 5th observation.\n\nSo when should we use robust standard errors? One flag is seeing large residuals and high leverage (i.e., hat values). For instance, the following base R diagnostic plot graphs residuals versus hat values. A point in the upper or lower right corners is an observation exhibiting influence on the model. Our 5th observation has a corner all to itself.\n\nBut it’s important to remember large residuals (or evidence of non-constant variance) could be due to a misspecified model. We may be missing key predictors, interactions, or non-linear effects. Because of this, it might be a good idea to think carefully about your model before reflexively deploying robust standard errors. Zeileis (2006), the author of the sandwich package, also gives two reasons for not using robust standard errors “for every model in every analysis”:\n\nFirst, the use of sandwich estimators when the model is correctly specified leads to a loss of power. Second, if the model is not correctly specified, the sandwich estimators are only useful if the parameters estimates are still consistent, i.e., if the misspecification does not result in bias.\n\nWe can demonstrate each of these points via simulation.\n\nIn the first simulation, we generate data with an interaction, fit the correct model, and then calculate both the usual and robust standard errors. We then check how often we correctly reject the null hypothesis of no interaction between and . This is an estimation of power for this particular hypothesis test.\n\nThe proportion of times we reject the null of no interaction using robust standard errors is lower than simply using the usual standard errors, which means we have a loss of power. (Though admittedly, the loss of power in this simulation is rather small.)\n\nThe second simulation is much like the first, except now we fit the wrong model and get biased estimates.\n\nWe see the simulated data from the wrong model is severely biased and is consistently over- or under-estimating the response. In this case robust standard errors would not be useful because our model is wrong.\n\nRelated to this last point, Freedman (2006) expresses skepticism about even using robust standard errors:\n\nIf the model is nearly correct, so are the usual standard errors, and robustification is unlikely to help much. On the other hand, if the model is seriously in error, the sandwich may help on the variance side, but the parameters being estimated…are likely to be meaningless---except perhaps as descriptive statistics.\n\nThere is much to think about before using robust standard errors. But hopefully you now have a better understanding of what they are and how they’re calculated. Zeileis (2004) provides a deeper and accessible introduction to the sandwich package, including how to use robust standard errors for addressing suspected autocorrelation.\n• Freedman, D. A. (2006). On the so-called \"Huber sandwich estimator\" and \"robust standard errors.\" The American Statistician, 60(4). 299--302.\n• R Core Team. (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing. URL https://www.R-project.org/\n• Zeileis, A. (2004). Econometric computing with HC and HAC covariance matrix estimators. Journal of Statistical Software, 11(10), 1--17. https://doi.org/10.18637/jss.v011.i10"
    },
    {
        "link": "https://cran.r-project.org/web/packages/sandwich/sandwich.pdf",
        "document": ""
    },
    {
        "link": "https://sandwich.r-forge.r-project.org/articles/sandwich.html",
        "document": "The sandwich package is designed for obtaining covariance matrix estimators of parameter estimates in statistical models where certain model assumptions have been violated. More specifically, the estimators are useful in a situation where the model’s score function was correctly specified (e.g., the mean function in a linear regression model) but that the remaining likelihood was potentially misspecified (e.g., due to heteroscedasticity or a lack of independence). In this case, the usual parameter estimates are still consistent but the associated covariance matrix estimate is not, thus potentially biasing the inference such as parameter tests or confidence intervals. Luckily, there are covariance matrix estimators that are consistent under these misspecifications and that can simply be plugged in to the usual inference based on the central limit theorem. Because the covariance matrix estimators are a product of two outer “bread” matrices (based on the Hessian of the log-likelihood) and an innter “meat” matrix (based on cross-products of the corresponding score function), they are also known as “sandwich” covariances. The sandwich package provides a wide range of such sandwich covariances in an object-oriented framework. This means that it takes the models fitted by another package without any modification or adjustment, extracts certain quantities from them, and computes the different covariance matrices based on these. For sandwich to work the model class has to provide at least an method for extracting the score matrix (containing the gradient contributions per observation) and a method for extracting the inverse of some estimate of the Fisher information (that most of the “usual” covariances are based on). Some sandwich covariances also need further methods, e.g., or etc. The covariances provided are:\n• for heteroscedastiticy- and autocorrelation-consistent (HAC) covariances in time series data with convenience interfaces (Andrews’ kernel HAC estimator), , and (weighted empirical adaptive variance estimation by Lumley and Heagerty).\n• and for panel and panel-corrected covariances.\n• for bootstrap covariances (with default method and dedicated fast and more flexible methods for and objects). These can be applied to many different model classes, including , , , , , , , , , , and many more. (There are some exceptions, though, not all covariances are applicable to all models). The resulting covariances can be subsequently plugged in to various functions for Wald-type inference, including , , and from the lmtest package or , , , , and from car package. Several R packages can be used to create model summary tables or visualizations based on the covariance and inference functions above. In particular broom provides a unified interface for collecting information about models, including a method for objects. Based on broom several packages provide model summary tables with the modelsummary package being particularly versatile, including a graphical to go along with the tabular .\n\nThe stable release version of sandwich is hosted on the Comprehensive R Archive Network (CRAN) at https://CRAN.R-project.org/package=sandwich and can be installed via The development version of sandwich is hosted on R-Forge at https://R-Forge.R-project.org/projects/sandwich/ in a Subversion (SVN) repository. It can be installed via\n\nTo illustrate the functionality of the package we employ a well-known data set that was created by Petersen for benchmarking clustered standard errors. However, because the data is so simple, we will use it here to illustrate all sorts of covariances (not only clustered ones). The data come from a simple linear regression of a response on an explanatory variable , but the data are clustered by (500 firms) and (10 years). The data can be loaded and the model fitted by ordinary least squares (OLS): The regression summary for a linear model uses the “usual” OLS standard errors, assuming that the data are uncorrelated and homoscedastic. The summary provides partial Wald tests for the regression coefficients and also an over F test assessing all the regressors, i.e., in this case equivalent to a t test of . To obtain the analogous tests and corresponding confidence intervals using the basic “robust” sandwich covariance for cross-section data we can combine sandwich with the lmtest package: Other covariances can be plugged in analogously, potentially passing along further options for the covariance. For example, the clustered covariance can be used with the clustering variable as: In the same way we can compute and compare many of the other estimators in the package. A selection, mostly just with the default settings, is provided here. First, the covarainces are computed and then the corresponding standard errors extracted (square root of the diagonal): This shows that due to the cluster-correlation in the data, the usual standard errors and cross-section covariances are much too small. In contrast, the different types of clustered standard errors are much larger and more appropriate here.\n\nFor creating publication-quality tables of the model summaries using sandwich-based standard errors, we use the function from the modelsummary package. Similar to this takes the unmodified object along with a specification (starting from version 0.6.5 of the package). The latter might be specified via a covariance extractor function, certain strings (like or ), a clustering formula (like ), or simply a covariance matrix. Given that we already computed a list of covariance matrices above, we simply use the list element with the clustered covariance: ## `modelsummary` 2.0.0 now uses `tinytable` as its default table-drawing ## Revert to `kableExtra` for one session: The result is a nicely formatted table with content similar to the verbatim output from the corresponding call above. (Note that in previous versions of the argument was called which is still supported for backward compatibility.) Moreover, we can also easily produce such a model summary directly from the output including the modified standard errors and test statistics. Even more nicely, this can also be done for a list of objects. Therefore, we use below to apply always to the same model but with the different matrices from . The resulting list of objects can then be displayed directly with . Note that this includes a reduced list of summary statistics, e.g., no R-squared. This could be changed by adding the argument in the call above. This would save the full model object in addition to the output. Finally, the different coefficients with corresponding confidence intervals can be displayed using . In the figure shown below we add some ggplot2-based customizations to the plot: omit the display for the intercepts (i.e., only show the coefficients), flip the axes, and use a custom color palette that highlights the confidence intervals based on the disfferent clustered standard errors which are more appropriate for this data."
    },
    {
        "link": "https://preview.library.virginia.edu/data/articles/understanding-robust-standard-errors",
        "document": "What are robust standard errors? How do we calculate them? Why use them? Why not use them all the time if they’re so robust? Those are the kinds of questions this post intends to address.\n\nTo begin, let’s start with the relatively easy part: getting robust standard errors for basic linear models in Stata and R. In Stata, simply appending to the end of regression syntax returns robust standard errors. “vce” is short for “variance-covariance matrix of the estimators.” “robust” indicates which type of variance-covariance matrix to calculate. Here’s a quick example using the auto data set that comes with Stata 16:\n\nNotice the third column indicates “robust” standard errors.\n\nTo replicate the result in R takes a bit more work. First we load the haven package to use the function that allows us to import Stata data sets. Then we load two more packages: lmtest and sandwich. The lmtest package provides the function that allows us to re-calculate a coefficient table using a different variance-covariance matrix. The sandwich package provides the function that allows us to calculate robust standard errors. The argument allows us to specify what kind of robust standard errors to calculate. “HC1” is one of several types available in the sandwich package and happens to be the default type in Stata 16. (We talk more about the different types and why it’s called the “sandwich” package below.)\n\nIf you look carefully, you’ll notice the standard errors in the R output match those in the Stata output.\n\nIf we want 95% confidence intervals like those produced in Stata, we need to use the function:\n\nWhile not really the point of this post, we should note the results say that larger turn circles and bigger trunks are associated with lower gas mileage.\n\nNow that we know the basics of getting robust standard errors out of Stata and R, let’s talk a little about why they’re robust by exploring how they’re calculated.\n\nThe usual method for estimating coefficient standard errors of a linear model can be expressed with this somewhat intimidating formula:\n\nwhere \\(X\\) is the model matrix (i.e., the matrix of the predictor values) and \\(\\Omega = \\sigma^2 I_n\\), which is shorthand for a matrix with nothing but \\(\\sigma^2\\) on the diagonal and 0’s everywhere else. Two main things to notice about this equation:\n• The \\(X^T \\Omega X\\) in the middle\n• The \\((X^TX)^{-1}\\) on each side\n\nSome statisticians and econometricians refer to this formula as a \"sandwich\" because it's like an equation sandwich: We have \"meat\" in the middle, \\(X^T \\Omega X\\), and \"bread\" on the outside, \\((X^TX)^{-1}\\). Calculating robust standard errors means substituting a new kind of \"meat.\"\n\nBefore we do that, let's use this formula by hand to see how it works when we calculate the usual standard errors. This will give us some insight to the meat of the sandwich. To make this easier to demonstrate, we'll use a small toy data set.\n\nNotice the way we generated . It is simply the number 5 with some random noise from a \\(N(0,1.2)\\) distribution plus the number 35. There is no relationship between and . However, when we regress on using we get a slope coefficient of about 5.2 that appears to be “significant”.\n\nClearly the 5th data point is highly influential and driving the “statistical significance,” which might lead us to think we have specified a “correct” model. Of course, we know that we specified a “wrong” model because we generated the data. does not have a relationship with ! It would be nice if we could guard against this sort of thing from happening: specifying a wrong model but getting a statistically significant result. One way we could do that is modifying how the coefficient standard errors are calculated.\n\nLet’s see how they were calculated in this case using the formula we specified above. Below is \\(\\sigma^2\\), is \\(I_n\\), and is the model matrix. Notice we can use the base R function to get the model matrix from a fitted model. We save the formula result into , which is the variance-covariance matrix. Finally we take square root of the diagonal elements to get the standard errors output in the model summary.\n\nNow let’s take a closer look at the “meat” in this sandwich formula:\n\nThat is a matrix of constant variance. This is one of the assumptions of classic linear modeling: The errors (or residuals) are drawn from a single normal distribution with mean 0 and a fixed variance. The object above is the estimated variance of that normal distribution. But what if we modified this matrix so that the variance was different for some observations? For example, it might make sense to assume the error of the 5th data point was drawn from a normal distribution with a larger variance. This would result in a larger standard error for the slope coefficient, indicating greater uncertainty in our coefficient estimate.\n\nThis is the idea of “robust” standard errors: modifying the “meat” in the sandwich formula to allow for things like non-constant variance (and/or autocorrelation, a phenomenon we don’t address in this post).\n\nSo how do we automatically determine non-constant variance estimates? It might not surprise you that there are several ways. The sandwich package provides seven different types at the time of this writing (version 2.5-1). The default version in Stata is identified in the sandwich package as “HC1”. The HC stands for heteroskedasticity-consistent. Heteroskedasticity is another word for non-constant. The formula for “HC1” is as follows:\n\nwhere \\(\\hat{\\mu}_i^2\\) refers to squared residuals, \\(n\\) is the number of observations, and \\(k\\) is the number of coefficients. In our simple model above, \\(k = 2\\), since we have an intercept and a slope. Let’s modify our formula above to substitute HC1 “meat” in our sandwich:\n\nNotice we no longer have constant variance for each observation. The estimated variance is instead the residual squared multiplied by (5/3). When we use this to estimate “robust” standard errors for our coefficients, we get slightly different estimates.\n\nNotice the slope standard error actually got smaller. It looks like the HC1 estimator may not be the best choice for such a small sample. The default estimator for the sandwich package is known as “HC3”:\n\nwhere \\(h_i\\) are the hat values from the hat matrix. A Google search or any textbook on linear modeling can tell you more about hat values and how they’re calculated. For our purposes, it suffices to know that they range from 0 to 1, and that larger values are indicative of influential observations. We see then that HC3 is a ratio that will be larger for values with high residuals and relatively high hat values. We can manually calculate the HC3 estimator using the base R and functions as follows:\n\nNotice that the 5th observation has a huge estimated variance of about 721. When we calculate the robust standard errors for the model coefficients we get a much bigger standard error for the slope.\n\nOf course we wouldn’t typically calculate robust standard errors by hand like this. We would use the function in the sandwich package as we demonstrated at the beginning of this post along with the function from the lmtest package. (Or use in Stata.)\n\nNow the slope coefficient estimate is no longer “significant” since the standard error is larger. This standard error estimate is robust to the influence of the outlying 5th observation.\n\nSo when should we use robust standard errors? One flag is seeing large residuals and high leverage (i.e., hat values). For instance, the following base R diagnostic plot graphs residuals versus hat values. A point in the upper or lower right corners is an observation exhibiting influence on the model. Our 5th observation has a corner all to itself.\n\nBut it’s important to remember large residuals (or evidence of non-constant variance) could be due to a misspecified model. We may be missing key predictors, interactions, or non-linear effects. Because of this, it might be a good idea to think carefully about your model before reflexively deploying robust standard errors. Zeileis (2006), the author of the sandwich package, also gives two reasons for not using robust standard errors “for every model in every analysis”:\n\nFirst, the use of sandwich estimators when the model is correctly specified leads to a loss of power. Second, if the model is not correctly specified, the sandwich estimators are only useful if the parameters estimates are still consistent, i.e., if the misspecification does not result in bias.\n\nWe can demonstrate each of these points via simulation.\n\nIn the first simulation, we generate data with an interaction, fit the correct model, and then calculate both the usual and robust standard errors. We then check how often we correctly reject the null hypothesis of no interaction between and . This is an estimation of power for this particular hypothesis test.\n\nThe proportion of times we reject the null of no interaction using robust standard errors is lower than simply using the usual standard errors, which means we have a loss of power. (Though admittedly, the loss of power in this simulation is rather small.)\n\nThe second simulation is much like the first, except now we fit the wrong model and get biased estimates.\n\nWe see the simulated data from the wrong model is severely biased and is consistently over- or under-estimating the response. In this case robust standard errors would not be useful because our model is wrong.\n\nRelated to this last point, Freedman (2006) expresses skepticism about even using robust standard errors:\n\nIf the model is nearly correct, so are the usual standard errors, and robustification is unlikely to help much. On the other hand, if the model is seriously in error, the sandwich may help on the variance side, but the parameters being estimated…are likely to be meaningless---except perhaps as descriptive statistics.\n\nThere is much to think about before using robust standard errors. But hopefully you now have a better understanding of what they are and how they’re calculated. Zeileis (2004) provides a deeper and accessible introduction to the sandwich package, including how to use robust standard errors for addressing suspected autocorrelation.\n• Freedman, D. A. (2006). On the so-called \"Huber sandwich estimator\" and \"robust standard errors.\" The American Statistician, 60(4). 299--302.\n• R Core Team. (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing. URL https://www.R-project.org/\n• Zeileis, A. (2004). Econometric computing with HC and HAC covariance matrix estimators. Journal of Statistical Software, 11(10), 1--17. https://doi.org/10.18637/jss.v011.i10"
    },
    {
        "link": "https://r-econometrics.com/methods/hcrobusterrors",
        "document": "Although heteroskedasticity does not produce biased OLS estimates, it leads to a bias in the variance-covariance matrix. This means that standard model testing methods such as t tests or F tests cannot be relied on any longer. This post provides an intuitive illustration of heteroskedasticity and covers the calculation of standard errors that are robust to it.\n\nA popular illustration of heteroskedasticity is the relationship between saving and income, which is shown in the following graph. The dataset is contained the wooldridge package. # Load packages library(dplyr) library(ggplot2) library(wooldridge) # Load the sample data(\"saving\") # Only use positive values of saving, which are smaller than income saving <- saving %>% filter(sav > 0, inc < 20000, sav < inc) # Plot ggplot(saving, aes(x = inc, y = sav)) + geom_point() + geom_smooth(method = \"lm\", se = FALSE) + labs(x = \"Annual income\", y = \"Annual savings\") The regression line in the graph shows a clear positive relationship between saving and income. However, as income increases, the differences between the observations and the regression line become larger. This means that there is higher uncertainty about the estimated relationship between the two variables at higher income levels. This is an example of heteroskedasticity. Since standard model testing methods rely on the assumption that there is no correlation between the independent variables and the variance of the dependent variable, the usual standard errors are not very reliable in the presence of heteroskedasticity. Fortunately, the calculation of robust standard errors can help to mitigate this problem.\n\nThe regression line above was derived from the model \\[sav_i = \\beta_0 + \\beta_1 inc_i + \\epsilon_i,\\] for which the following code produces the standard R output: # Estimate the model model <- lm(sav ~ inc, data = saving) # Print estimates and standard test statistics summary(model) ## ## Call: ## lm(formula = sav ~ inc, data = saving) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2667.8 -874.5 -302.7 431.1 4606.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 316.19835 462.06882 0.684 0.49595 ## inc 0.14052 0.04672 3.007 0.00361 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 1413 on 73 degrees of freedom ## Multiple R-squared: 0.1102, Adjusted R-squared: 0.09805 ## F-statistic: 9.044 on 1 and 73 DF, p-value: 0.003613 Since we already know that the model above suffers from heteroskedasticity, we want to obtain heteroskedasticity robust standard errors and their corresponding t values. In R the function from the lmtest package can be used in combination with the function from the sandwich package to do this. The first argument of the function contains the output of the function and calculates the t test based on the variance-covariance matrix provided in the argument. The function produces that matrix and allows to obtain several types of heteroskedasticity robust versions of it. In our case we obtain a simple White standard error, which is indicated by . Other, more sophisticated methods are described in the documentation of the function, . ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 316.198354 414.728032 0.7624 0.448264 ## inc 0.140515 0.048805 2.8791 0.005229 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 In the post on hypothesis testing the F test is presented as a method to test the joint significance of multiple regressors. The following example adds two new regressors on education and age to the above model and calculates the corresponding (non-robust) F test using the function. ## Analysis of Variance Table ## ## Model 1: sav ~ inc ## Model 2: sav ~ inc + size + educ + age ## Res.Df RSS Df Sum of Sq F Pr(>F) ## 1 73 145846877 ## 2 70 144286605 3 1560272 0.2523 0.8594 For a heteroskedasticity robust F test we perform a Wald test using the function, which is also contained in the lmtest package. It can be used in a similar way as the function, i.e., it uses the output of the restricted and unrestricted model and the robust variance-covariance matrix as argument . Based on the variance-covariance matrix of the unrestriced model we, again, calculate White standard errors."
    }
]