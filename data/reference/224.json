[
    {
        "link": "https://aliaksandrsiarohin.github.io/first-order-model-website",
        "document": "How does it work?\n\nFor training, we employ a large collection of video sequences containing objects of the same object category. Our model is trained to reconstruct the training videos by combining a single frame and a learned latent representation of the motion in the video. Observing frame pairs (source and driving), each extracted from the same video, it learns to encode motion as a combination of motion-specific keypoint displacements and local affine transformations. At test time we apply our model to pairs composed of the source image and of each frame of the driving video and perform image animation of the source object.\n\nAn overview of our approach is presented in Figure above. Our framework is composed of two main modules: the motion estimation module and the image generation module. The purpose of the motion estimation module is to predict a dense motion field. We assume there exists an abstract reference frame. And we independently estimate two transformations: from reference to source and from reference to driving. This choice allows us to independently process source and driving frames. This is desired since, at test time the model receives pairs of the source image and driving frames sampled from a different video, which can be very different visually.\n\nIn the first step, we approximate both transformations from sets of sparse trajectories, obtained by using keypoints learned in a self-supervised way. We model motion in the neighbourhood of each keypoint using local affine transformations. Compared to using keypoint displacements only, the local affine transformations allow us to model a larger family of transformations. During the second step, a dense motion network combines the local approximations to obtain the resulting dense motion field. Furthermore, in addition to the dense motion field, this network outputs an occlusion mask that indicates which image parts of driving can be reconstructed by warping of the source image and which parts should be inpainted (inferred from the context). Finally, the generation module renders an image of the source object moving as provided in the driving video. Here, we use a generator network that warps the source image according to dense motion and inpaints the image parts that are occluded in the source image."
    },
    {
        "link": "https://github.com/AliaksandrSiarohin/first-order-model",
        "document": "!!! Check out our new paper and framework improved for articulated objects\n\nThis repository contains the source code for the paper First Order Motion Model for Image Animation by Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci and Nicu Sebe.\n\nThe videos on the left show the driving videos. The first row on the right for each dataset shows the source videos. The bottom row contains the animated sequences with motion transferred from the driving video and object taken from the source image. We trained a separate network for each task.\n\nWe support . To install the dependencies run:\n\nThere are several configuration ( ) files one for each . See to get description of each parameter.\n\nCheckpoints can be found under following link: google-drive or yandex-disk.\n\nTo run a demo, download checkpoint and run the following command:\n\nThe result will be stored in .\n\nThe driving videos and source images should be cropped before it can be used in our method. To obtain some semi-automatic crop suggestions you can use . It will generate commands for crops using ffmpeg. In order to use the script, face-alligment library is needed:\n\nIf you are having trouble getting the demo to work because of library compatibility issues, and you're running Linux, you might try running it inside a Docker container, which would give you better control over the execution environment.\n\nRequirements: Docker 19.03+ and nvidia-docker installed and able to successfully run the usage tests.\n\nWe'll first build the container.\n\nAnd now that we have the container available locally, we can use it to run the demo.\n\n@graphemecluster prepared a GUI demo for the Google Colab. It also works in Kaggle. For the source code, see .\n\nFor the old demo, see .\n\nIt is possible to modify the method to perform face-swap using supervised segmentation masks. For both unsupervised and supervised video editing, such as face-swap, please refer to Motion Co-Segmentation.\n\nThe code will create a folder in the log directory (each run will create a time-stamped new directory). Checkpoints will be saved to this folder. To check the loss values during training see . You can also check training data reconstructions in the subfolder. By default the batch size is tuned to run on 2 or 4 Titan-X gpu (apart from speed it does not make much difference). You can change the batch size in the train_params in corresponding file.\n\nYou will need to specify the path to the checkpoint, the subfolder will be created in the checkpoint folder. The generated video will be stored to this folder, also generated videos will be stored in subfolder in loss-less '.png' format for evaluation. Instructions for computing metrics from the paper can be found: https://github.com/AliaksandrSiarohin/pose-evaluation.\n\nYou will need to specify the path to the checkpoint, the subfolder will be created in the same folder as the checkpoint. You can find the generated video there and its loss-less version in the subfolder. By default video from test set will be randomly paired, but you can specify the \"source,driving\" pairs in the corresponding files. The path to this file should be specified in corresponding file in pairs_list setting.\n\nThere are 2 different ways of performing animation: by using absolute keypoint locations or by using relative keypoint locations.\n• Animation using absolute coordinates: the animation is performed using the absolute positions of the driving video and appearance of the source image. In this way there are no specific requirements for the driving video and source appearance that is used. However this usually leads to poor performance since irrelevant details such as shape is transferred. Check animate parameters in to enable this mode.\n• Animation using relative coordinates: from the driving video we first estimate the relative movement of each keypoint, then we add this movement to the absolute position of keypoints in the source image. This keypoint along with source image is used for animation. This usually leads to better performance, however this requires that the object in the first frame of the video and in the source image have the same pose\n• Bair. This dataset can be directly downloaded.\n• Mgif. This dataset can be directly downloaded.\n• Fashion. Follow the instruction on dataset downloading from.\n• Taichi. Follow the instructions in data/taichi-loading or instructions from https://github.com/AliaksandrSiarohin/video-preprocessing.\n• Nemo. Please follow the instructions on how to download the dataset. Then the dataset should be preprocessed using scripts from https://github.com/AliaksandrSiarohin/video-preprocessing.\n• VoxCeleb. Please follow the instruction from https://github.com/AliaksandrSiarohin/video-preprocessing.\n• Resize all the videos to the same size e.g 256x256, the videos can be in '.gif', '.mp4' or folder with images. We recommend the later, for each video make a separate folder with all the frames in '.png' format. This format is loss-less, and it has better i/o performance.\n• Create a folder with 2 subfolders and , put training videos in the and testing in the .\n• Create a config , in dataset_params specify the root dir the . Also adjust the number of epoch in train_params."
    },
    {
        "link": "https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/en_US/tutorials/motion_driving.md",
        "document": "First order motion model is to complete the Image animation task, which consists of generating a video sequence so that an object in a source image is animated according to the motion of the driving video. The image below gives examples of source images with objects and driving videos containing a series of motions.\n\nThe following gif clearly expounds the principle:\n\nThe proposed method is not exclusively for facial expression transfer, it also supports other object with training on similar datasets. For example, you can transfer the motion of playing Tai Chi by training on Tai Chi video datasets and achieve facial expression transfer by using dataset voxceleb for training. After that, you could realize real-time image animation with the corresponding pre-training model.\n• \n• \n• We adopt PaddleGAN's face detection model S3FD to detect all the faces in an image and transfer those expressions for multi-faces swapping. a. Use the S3FD model to detect all the faces in an image b. Use the First Order Motion model to do the facial expression transfer of each face c. Crop those \"new\" generated faces and put them back to the original photo At the same time, PaddleGAN also provides a \"faceutils\" tool for face-related work, including face detection, face segmentation, keypoints detection, etc.\n• \n• This effect significantly improves the definition of the driven video.\n• \n• 💙**Special For Love Confession on May 20th (pronounced as I love you)**💙：https://aistudio.baidu.com/aistudio/projectdetail/1956943\n\nUsers can upload a source image with single or multiple faces and driving video, then substitute the paths of source image and driving video for the and parameters respectively and run the following command. It will generate a video file named in the folder, which is the animated video file.\n\nNote: For photos with multiple faces, the longer the distances between faces, the better the result quality you can get, or you could optimize the effect by adjusting ratio.\n\nThe original image and driving video here are provided for demonstration purposes, the running command is as follows:\n• VoxCeleb See here. Here you can process the data sizes according to your requirements. We deal with two resolution sizes: 256 and 512, the results can be seen below：\n• dataset_name.yaml: Configure your own yaml document and parameters\n• Training using multiple GPUs: change the nn.BatchNorm in /ppgan/modules/first_order.py to nn.SyncBatchNorm\n\nCurrently, we use mobilenet combined with pruning to compress models, see the comparison below:\n\nTraining: First, set mode in configs/firstorder_vox_mobile_256.yaml as kp_detector, train the compressed kp_detector model, and immobilize the original generator model. Then set mode in configs/firstorder_vox_mobile_256.yaml as generator，train the compressed generator model, and immobilize the original kp_detector model. Finally, set mode as both and modify kp_weight_path and gen_weight_path in the config to the path of trained model to train together。\n\nUse the script to export the configuration file used when the model has been deployed with the config name of . The export script of the model is as follows.\n\nThe prediction models will be exported to the directory of as , , 。\n• FOM-Lite-Demo。For more details, please refer to Paddle-Lite . Current problems： (a).Paddle Lite performs slightly worse than Paddle Inference，under optimization (b).Run Generator in a single thread, if the number of frames is too large, it will run at the small core rather than the large core."
    },
    {
        "link": "https://papers.nips.cc/paper/8935-first-order-motion-model-for-image-animation",
        "document": "Requests for name changes in the electronic proceedings will be accepted with no questions asked. However name changes may cause bibliographic tracking issues. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.\n\nUse the \"Report an Issue\" link to request a name change."
    },
    {
        "link": "https://arxiv.org/abs/2003.00196",
        "document": ""
    },
    {
        "link": "https://pytorch.org/vision/main/transforms.html",
        "document": "Torchvision supports common computer vision transformations in the and modules. Transforms can be used to transform or augment data for training or inference of different tasks (image classification, detection, segmentation, video classification).\n\nTransforms are typically passed as the or argument to the Datasets.\n\nMost transformations accept both PIL images and tensor inputs. Both CPU and CUDA tensors are supported. The result of both backends (PIL or Tensors) should be very close. In general, we recommend relying on the tensor backend for performance. The conversion transforms may be used to convert to and from PIL images, or for converting dtypes and ranges. Tensor image are expected to be of shape , where is the number of channels, and and refer to height and width. Most transforms support batched tensor input. A batch of Tensor images is a tensor of shape , where is a number of images in the batch. The v2 transforms generally accept an arbitrary number of leading dimensions and can handle batched images or batched videos. The expected range of the values of a tensor image is implicitly defined by the tensor dtype. Tensor images with a float dtype are expected to have values in . Tensor images with an integer dtype are expected to have values in where is the largest value that can be represented in that dtype. Typically, images of dtype are expected to have values in . Use to convert both the dtype and range of the inputs.\n\nV1 or V2? Which one should I use?¶ TL;DR We recommending using the transforms instead of those in . They’re faster and they can do more things. Just change the import and you should be good to go. Moving forward, new features and improvements will only be considered for the v2 transforms. In Torchvision 0.15 (March 2023), we released a new set of transforms available in the namespace. These transforms have a lot of advantages compared to the v1 ones (in ):\n• None They can transform images but also bounding boxes, masks, or videos. This provides support for tasks beyond image classification: detection, segmentation, video classification, etc. See Getting started with transforms v2 and Transforms v2: End-to-end object detection/segmentation example.\n• None They support more transforms like and . See How to use CutMix and MixUp.\n• None Future improvements and features will be added to the v2 transforms only. These transforms are fully backward compatible with the v1 ones, so if you’re already using tranforms from , all you need to do to is to update the import to . In terms of output, there might be negligible differences due to implementation differences.\n\nWe recommend the following guidelines to get the best performance out of the transforms:\n• None Rely on the v2 transforms from\n• None Use tensors instead of PIL images\n• None Use dtype, especially for resizing This is what a typical transform pipeline could look like: # Convert to tensor, only needed if you had a PIL image # optional, most input are already uint8 at this point The above should give you the best performance in a typical training environment that relies on the with . Transforms tend to be sensitive to the input strides / memory format. Some transforms will be faster with channels-first images while others prefer channels-last. Like operators, most transforms will preserve the memory format of the input, but this may not always be respected due to implementation details. You may want to experiment a bit if you’re chasing the very best performance. Using on individual transforms may also help factoring out the memory format variable (e.g. on ). Note that we’re talking about memory format, not tensor shape. Note that resize transforms like and typically prefer channels-last input and tend not to benefit from at this time.\n\nTransforms are available as classes like , but also as functionals like in the namespace. This is very much like the package which defines both classes and functional equivalents in . The functionals support PIL images, pure tensors, or TVTensors, e.g. both and are valid. Random transforms like will randomly sample some parameter each time they’re called. Their functional counterpart ( ) does not do any kind of random sampling and thus have a slighlty different parametrization. The class method of the transforms class can be used to perform parameter sampling when using the functional APIs. The namespace also contains what we call the “kernels”. These are the low-level functions that implement the core functionalities for specific types, e.g. or . They are public, although not documented. Check the code to see which ones are available (note that those starting with a leading underscore are not public!). Kernels are only really useful if you want torchscript support for types like bounding boxes or masks.\n\nMost transform classes and functionals support torchscript. For composing transforms, use instead of : v2 transforms support torchscript, but if you call on a v2 class transform, you’ll actually end up with its (scripted) v1 equivalent. This may lead to slightly different results between the scripted and eager executions due to implementation differences between v1 and v2. If you really need torchscript support for the v2 transforms, we recommend scripting the functionals from the namespace to avoid surprises. Also note that the functionals only support torchscript for pure tensors, which are always treated as images. If you need torchscript support for other types like bounding boxes or masks, you can rely on the low-level kernels. For any custom transformations to be used with , they should be derived from ."
    },
    {
        "link": "https://pytorch.org/vision/0.9/transforms.html",
        "document": "Transforms are common image transformations. They can be chained together using . Additionally, there is the module. Functional transforms give fine-grained control over the transformations. This is useful if you have to build a more complex transformation pipeline (e.g. in the case of segmentation tasks).\n\nAll transformations accept PIL Image, Tensor Image or batch of Tensor Images as input. Tensor Image is a tensor with shape, where is a number of channels, and are image height and width. Batch of Tensor Images is a tensor of shape, where is a number of images in the batch. Deterministic or random transformations applied on the batch of Tensor Images identically transform all the images of the batch.\n\nCrops the given image at the center. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions. If image size is smaller than output size along any edge, image is padded with 0 and then center cropped. size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). img (PIL Image or Tensor) – Image to be cropped. Randomly change the brightness, contrast, saturation and hue of an image. If the image is torch Tensor, it is expected to have […, 3, H, W] shape, where … means an arbitrary number of leading dimensions. If img is PIL Image, mode “1”, “L”, “I”, “F” and modes with transparency (alpha channel) are not supported.\n• brightness (float or tuple of python:float (min, max)) – How much to jitter brightness. brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers.\n• contrast (float or tuple of python:float (min, max)) – How much to jitter contrast. contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast] or the given [min, max]. Should be non negative numbers.\n• saturation (float or tuple of python:float (min, max)) – How much to jitter saturation. saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation] or the given [min, max]. Should be non negative numbers.\n• hue (float or tuple of python:float (min, max)) – How much to jitter hue. hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5. Get the parameters for the randomized transform to be applied on image.\n• brightness (tuple of python:float (min, max), optional) – The range from which the brightness_factor is chosen uniformly. Pass None to turn off the transformation.\n• contrast (tuple of python:float (min, max), optional) – The range from which the contrast_factor is chosen uniformly. Pass None to turn off the transformation.\n• saturation (tuple of python:float (min, max), optional) – The range from which the saturation_factor is chosen uniformly. Pass None to turn off the transformation.\n• hue (tuple of python:float (min, max), optional) – The range from which the hue_factor is chosen uniformly. Pass None to turn off the transformation. The parameters used to apply the randomized transform along with their random order. Crop the given image into four corners and the central crop. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your Dataset returns. See below for an example of how to deal with this. size (sequence or int) – Desired output size of the crop. If size is an instead of sequence like (h, w), a square crop of size (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). # this is a list of PIL Images #In your test loop you can do the following: img (PIL Image or Tensor) – Image to be cropped. tuple of 5 images. Image can be PIL Image or Tensor Convert image to grayscale. If the image is torch Tensor, it is expected to have […, 3, H, W] shape, where … means an arbitrary number of leading dimensions num_output_channels (int) – (1 or 3) number of channels desired for output image\n• If : returned image is 3 channel with r == g == b img (PIL Image or Tensor) – Image to be converted to grayscale. Pad the given image on all sides with the given “pad” value. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means at most 2 leading dimensions for mode reflect and symmetric, at most 3 leading dimensions for mode edge, and an arbitrary number of leading dimensions for mode constant\n• padding (int or sequence) – Padding on each border. If a single int is provided this is used to pad all borders. If sequence of length 2 is provided this is the padding on left/right and top/bottom respectively. If a sequence of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. In torchscript mode padding as single int is not supported, use a sequence of length 1: .\n• fill (number or str or tuple) – Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant. Only number is supported for torch Tensor. Only int or str or tuple value is supported for PIL Image.\n• padding_mode (str) – Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.\n• constant: pads with a constant value, this value is specified with fill\n• None edge: pads with the last value at the edge of the image, if input a 5D torch Tensor, the last 3 dimensions will be padded instead of the last 2\n• reflect: pads with reflection of image without repeating the last value on the edge For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]\n• symmetric: pads with reflection of image repeating the last value on the edge For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] img (PIL Image or Tensor) – Image to be padded. Random affine transformation of the image keeping center invariant. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n• degrees (sequence or number) – Range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees). Set to 0 to deactivate rotations.\n• translate (tuple, optional) – tuple of maximum absolute fraction for horizontal and vertical translations. For example translate=(a, b), then horizontal shift is randomly sampled in the range -img_width * a < dx < img_width * a and vertical shift is randomly sampled in the range -img_height * b < dy < img_height * b. Will not translate by default.\n• scale (tuple, optional) – scaling factor interval, e.g (a, b), then scale is randomly sampled from the range a <= scale <= b. Will keep original scale by default.\n• shear (sequence or number, optional) – Range of degrees to select from. If shear is a number, a shear parallel to the x axis in the range (-shear, +shear) will be applied. Else if shear is a sequence of 2 values a shear parallel to the x axis in the range (shear[0], shear[1]) will be applied. Else if shear is a sequence of 4 values, a x-axis shear in (shear[0], shear[1]) and y-axis shear in (shear[2], shear[3]) will be applied. Will not apply shear by default.\n• interpolation (InterpolationMode) – Desired interpolation enum defined by . Default is . If input is Tensor, only , are supported. For backward compatibility integer values (e.g. ) are still acceptable.\n• fill (sequence or number) – Pixel fill value for the area outside the transformed image. Default is . If given a number, the value is used for all bands respectively. If input is PIL Image, the options is only available for .\n• fillcolor (sequence or number, optional) – deprecated argument and will be removed since v0.10.0. Please use the parameter instead.\n• resample (int, optional) – deprecated argument and will be removed since v0.10.0. Please use the parameter instead. img (PIL Image or Tensor): Image to be transformed. params to be passed to the affine transformation Apply randomly a list of transformations with a given probability. In order to script the transformation, please use as input instead of list/tuple of transforms as shown below: Make sure to use only scriptable transformations, i.e. that work with , does not require functions or . Crop the given image at a random location. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions, but if non-constant padding is used, the input is expected to have at most 2 leading dimensions\n• size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n• padding (int or sequence, optional) – Optional padding on each border of the image. Default is None. If a single int is provided this is used to pad all borders. If sequence of length 2 is provided this is the padding on left/right and top/bottom respectively. If a sequence of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. In torchscript mode padding as single int is not supported, use a sequence of length 1: .\n• pad_if_needed (boolean) – It will pad the image if smaller than the desired size to avoid raising an exception. Since cropping is done after padding, the padding seems to be done at a random offset.\n• fill (number or str or tuple) – Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant. Only number is supported for torch Tensor. Only int or str or tuple value is supported for PIL Image.\n• padding_mode (str) – Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.\n• constant: pads with a constant value, this value is specified with fill\n• edge: pads with the last value on the edge of the image\n• reflect: pads with reflection of image (without repeating the last value on the edge) padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]\n• symmetric: pads with reflection of image (repeating the last value on the edge) padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] img (PIL Image or Tensor) – Image to be cropped. Get parameters for for a random crop.\n• img (PIL Image or Tensor) – Image to be cropped. params (i, j, h, w) to be passed to for random crop. Randomly convert image to grayscale with a probability of p (default 0.1). If the image is torch Tensor, it is expected to have […, 3, H, W] shape, where … means an arbitrary number of leading dimensions p (float) – probability that image should be converted to grayscale. Grayscale version of the input image with probability p and unchanged with probability (1-p). - If input image is 1 channel: grayscale version is 1 channel - If input image is 3 channel: grayscale version is 3 channel with r == g == b img (PIL Image or Tensor) – Image to be converted to grayscale. Horizontally flip the given image randomly with a given probability. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions p (float) – probability of the image being flipped. Default value is 0.5 img (PIL Image or Tensor) – Image to be flipped. Performs a random perspective transformation of the given image with a given probability. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n• distortion_scale (float) – argument to control the degree of distortion and ranges from 0 to 1. Default is 0.5.\n• p (float) – probability of the image being transformed. Default is 0.5.\n• interpolation (InterpolationMode) – Desired interpolation enum defined by . Default is . If input is Tensor, only , are supported. For backward compatibility integer values (e.g. ) are still acceptable.\n• fill (sequence or number) – Pixel fill value for the area outside the transformed image. Default is . If given a number, the value is used for all bands respectively. If input is PIL Image, the options is only available for . img (PIL Image or Tensor) – Image to be Perspectively transformed. Get parameters for for a random perspective transform.\n• distortion_scale (float) – argument to control the degree of distortion and ranges from 0 to 1. List containing [top-left, top-right, bottom-right, bottom-left] of the original image, List containing [top-left, top-right, bottom-right, bottom-left] of the transformed image. Crop the given image to random size and aspect ratio. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions A crop of random size (default: of 0.08 to 1.0) of the original size and a random aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop is finally resized to given size. This is popularly used to train the Inception networks.\n• size (int or sequence) – expected output size of each edge. If size is an int instead of sequence like (h, w), a square output size is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). In torchscript mode size as single int is not supported, use a sequence of length 1: .\n• scale (tuple of python:float) – scale range of the cropped image before resizing, relatively to the origin image.\n• ratio (tuple of python:float) – aspect ratio range of the cropped image before resizing.\n• interpolation (InterpolationMode) – Desired interpolation enum defined by . Default is . If input is Tensor, only , and are supported. For backward compatibility integer values (e.g. ) are still acceptable. img (PIL Image or Tensor) – Image to be cropped and resized. Get parameters for for a random sized crop.\n• scale (list) – range of scale of the origin size cropped\n• ratio (list) – range of aspect ratio of the origin aspect ratio cropped params (i, j, h, w) to be passed to for a random Rotate the image by angle. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n• degrees (sequence or number) – Range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees).\n• interpolation (InterpolationMode) – Desired interpolation enum defined by . Default is . If input is Tensor, only , are supported. For backward compatibility integer values (e.g. ) are still acceptable.\n• expand (bool, optional) – Optional expansion flag. If true, expands the output to make it large enough to hold the entire rotated image. If false or omitted, make the output image the same size as the input image. Note that the expand flag assumes rotation around the center and no translation.\n• center (sequence, optional) – Optional center of rotation, (x, y). Origin is the upper left corner. Default is the center of the image.\n• fill (sequence or number) – Pixel fill value for the area outside the rotated image. Default is . If given a number, the value is used for all bands respectively. If input is PIL Image, the options is only available for .\n• resample (int, optional) – deprecated argument and will be removed since v0.10.0. Please use the parameter instead. img (PIL Image or Tensor) – Image to be rotated. Get parameters for for a random rotation. angle parameter to be passed to for random rotation. Note: This transform is deprecated in favor of RandomResizedCrop. Vertically flip the given image randomly with a given probability. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions p (float) – probability of the image being flipped. Default value is 0.5 img (PIL Image or Tensor) – Image to be flipped. Resize the input image to the given size. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n• size (sequence or int) – Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number. i.e, if height > width, then image will be rescaled to (size * height / width, size). In torchscript mode size as single int is not supported, use a sequence of length 1: .\n• interpolation (InterpolationMode) – Desired interpolation enum defined by . Default is . If input is Tensor, only , and are supported. For backward compatibility integer values (e.g. ) are still acceptable. img (PIL Image or Tensor) – Image to be scaled. Note: This transform is deprecated in favor of Resize. Crop the given image into four corners and the central crop plus the flipped version of these (horizontal flipping is used by default). If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your Dataset returns. See below for an example of how to deal with this.\n• size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n• vertical_flip (bool) – Use vertical flipping instead of horizontal # this is a list of PIL Images #In your test loop you can do the following: img (PIL Image or Tensor) – Image to be cropped. tuple of 10 images. Image can be PIL Image or Tensor Blurs image with randomly chosen Gaussian blur. If the image is torch Tensor, it is expected to have […, C, H, W] shape, where … means an arbitrary number of leading dimensions.\n• kernel_size (int or sequence) – Size of the Gaussian kernel.\n• sigma (float or tuple of python:float (min, max)) – Standard deviation to be used for creating kernel to perform blurring. If float, sigma is fixed. If it is tuple of float (min, max), sigma is chosen uniformly at random to lie in the given range. img (PIL Image or Tensor) – image to be blurred.\n• sigma_min (float) – Minimum standard deviation that can be chosen for blurring kernel.\n• sigma_max (float) – Maximum standard deviation that can be chosen for blurring kernel. Standard deviation to be passed to calculate kernel for gaussian blurring.\n\nFunctional transforms give you fine-grained control of the transformation pipeline. As opposed to the transformations above, functional transforms don’t contain a random number generator for their parameters. That means you have to specify/generate all parameters, but you can reuse the functional transform. Example: you can apply a functional transform with the same parameters to multiple images like this: Example: you can use a functional transform to build transform classes with custom behavior: \"\"\"Rotate by one of the given angles.\"\"\"\n• img (PIL Image or Tensor) – Image to be adjusted.\n• img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, (If) –\n• ... means it can have an arbitrary number of leading dimensions. (where) –\n• brightness_factor (float) – How much to adjust the brightness. Can be any non negative number. 0 gives a black image, 1 gives the original image while 2 increases the brightness by a factor of 2.\n• img (PIL Image or Tensor) – Image to be adjusted.\n• img is torch Tensor, it is expected to be in [..., 3, H, W] format, (If) –\n• ... means it can have an arbitrary number of leading dimensions. (where) –\n• contrast_factor (float) – How much to adjust the contrast. Can be any non negative number. 0 gives a solid gray image, 1 gives the original image while 2 increases the contrast by a factor of 2. Also known as Power Law Transform. Intensities in RGB mode are adjusted based on the following equation: See Gamma Correction for more details.\n• img (PIL Image or Tensor) – PIL Image to be adjusted.\n• img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, (If) –\n• ... means it can have an arbitrary number of leading dimensions. (where) –\n• img is PIL Image, modes with transparency (If) –\n• gamma (float) – Non negative real number, same as in the equation. gamma larger than 1 make the shadows darker, while gamma smaller than 1 make dark regions lighter. The image hue is adjusted by converting the image to HSV and cyclically shifting the intensities in the hue channel (H). The image is then converted back to original image mode. is the amount of shift in H channel and must be in the interval . See Hue for more details.\n• img (PIL Image or Tensor) – Image to be adjusted.\n• img is torch Tensor, it is expected to be in [..., 3, H, W] format, (If) –\n• ... means it can have an arbitrary number of leading dimensions. (where) –\n• img is PIL Image mode \"1\", \"L\", \"I\", \"F\" and modes with transparency (If) –\n• hue_factor (float) – How much to shift the hue channel. Should be in [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in HSV space in positive and negative direction respectively. 0 means no shift. Therefore, both -0.5 and 0.5 will give an image with complementary colors while 0 gives the original image.\n• img (PIL Image or Tensor) – Image to be adjusted.\n• img is torch Tensor, it is expected to be in [..., 3, H, W] format, (If) –\n• ... means it can have an arbitrary number of leading dimensions. (where) –\n• saturation_factor (float) – How much to adjust the saturation. 0 will give a black and white image, 1 will give the original image while 2 will enhance the saturation by a factor of 2. Adjust the sharpness of an image.\n• img (PIL Image or Tensor) – Image to be adjusted.\n• img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, (If) –\n• ... means it can have an arbitrary number of leading dimensions. (where) –\n• sharpness_factor (float) – How much to adjust the sharpness. Can be any non negative number. 0 gives a blurred image, 1 gives the original image while 2 increases the sharpness by a factor of 2. img: torch.Tensor, angle: float, translate: List[int], scale: float, shear: List[float], interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.NEAREST: 'nearest'>, fill: Union[List[float], NoneType] = None, resample: Union[int, NoneType] = None, fillcolor: Union[List[float], NoneType] = None → torch.Tensor¶ Apply affine transformation on the image keeping image center invariant. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n• angle (number) – rotation angle in degrees between -180 and 180, clockwise direction.\n• shear (float or sequence) – shear angle value in degrees between -180 to 180, clockwise direction. If a sequence is specified, the first value corresponds to a shear parallel to the x axis, while the second value corresponds to a shear parallel to the y axis.\n• interpolation (InterpolationMode) – Desired interpolation enum defined by . Default is . If input is Tensor, only , are supported. For backward compatibility integer values (e.g. ) are still acceptable.\n• fill (sequence or number, optional) – Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively. In torchscript mode single int/float value is not supported, please use a sequence of length 1: . If input is PIL Image, the options is only available for .\n• fillcolor (sequence, int, float) – deprecated argument and will be removed since v0.10.0. Please use the parameter instead.\n• resample (int, optional) – deprecated argument and will be removed since v0.10.0. Please use the parameter instead. Maximize contrast of an image by remapping its pixels per channel so that the lowest becomes black and the lightest becomes white. img (PIL Image or Tensor) – Image on which autocontrast is applied. If img is torch Tensor, it is expected to be in […, 1 or 3, H, W] format, where … means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode “L” or “RGB”. An image that was autocontrasted. Crops the given image at the center. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions. If image size is smaller than output size along any edge, image is padded with 0 and then center cropped.\n• img (PIL Image or Tensor) – Image to be cropped.\n• output_size (sequence or int) – (height, width) of the crop box. If int or sequence with single int, it is used for both directions. Convert a tensor image to the given and scale the values accordingly This function does not support PIL Image. When converting from a smaller to a larger integer the maximum values are not mapped exactly. If converted back and forth, this mismatch has no effect. – When trying to cast to or as well as for trying to cast to . These conversions might lead to overflow errors since the floating point cannot store consecutive integers over the whole range of the integer . Crop the given image at specified location and output size. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n• img (PIL Image or Tensor) – Image to be cropped. (0,0) denotes the top left corner of the image.\n• top (int) – Vertical component of the top left corner of the crop box.\n• left (int) – Horizontal component of the top left corner of the crop box. Equalize the histogram of an image by applying a non-linear mapping to the input in order to create a uniform distribution of grayscale values in the output. img (PIL Image or Tensor) – Image on which equalize is applied. If img is torch Tensor, it is expected to be in […, 1 or 3, H, W] format, where … means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode “P”, “L” or “RGB”. An image that was equalized. Erase the input Tensor Image with given value. This transform does not support PIL Image.\n• img (Tensor Image) – Tensor image of size (C, H, W) to be erased\n• i (int) – i in (i,j) i.e coordinates of the upper left corner.\n• j (int) – j in (i,j) i.e coordinates of the upper left corner.\n• inplace (bool, optional) – For in-place operations. By default is set False. Crop the given image into four corners and the central crop. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your returns.\n• img (PIL Image or Tensor) – Image to be cropped.\n• size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). Corresponding top left, top right, bottom left, bottom right and center crop. Performs Gaussian blurring on the image by given kernel. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n• img (PIL Image or Tensor) – Image to be blurred\n• kernel_size (sequence of python:ints or int) – Gaussian kernel size. Can be a sequence of integers like or a single integer for square kernels. In torchscript mode kernel_size as single int is not supported, use a sequence of length 1: .\n• sigma (sequence of python:floats or float, optional) – Gaussian kernel standard deviation. Can be a sequence of floats like or a single float to define the same sigma in both X/Y directions. If None, then it is computed using as . Default, None. In torchscript mode sigma as single float is not supported, use a sequence of length 1: . img (PIL Image or Tensor) – Image to be flipped. If img is a Tensor, it is expected to be in […, H, W] format, where … means it can have an arbitrary number of leading dimensions. Invert the colors of an RGB/grayscale image. img (PIL Image or Tensor) – Image to have its colors inverted. If img is torch Tensor, it is expected to be in […, 1 or 3, H, W] format, where … means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode “L” or “RGB”. Normalize a tensor image with mean and standard deviation. This transform does not support PIL Image. This transform acts out of place by default, i.e., it does not mutates the input tensor. See for more details.\n• tensor (Tensor) – Tensor image of size (C, H, W) or (B, C, H, W) to be normalized.\n• mean (sequence) – Sequence of means for each channel.\n• std (sequence) – Sequence of standard deviations for each channel. Pad the given image on all sides with the given “pad” value. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means at most 2 leading dimensions for mode reflect and symmetric, at most 3 leading dimensions for mode edge, and an arbitrary number of leading dimensions for mode constant\n• img (PIL Image or Tensor) – Image to be padded.\n• padding (int or sequence) – Padding on each border. If a single int is provided this is used to pad all borders. If sequence of length 2 is provided this is the padding on left/right and top/bottom respectively. If a sequence of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. In torchscript mode padding as single int is not supported, use a sequence of length 1: .\n• fill (number or str or tuple) – Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant. Only number is supported for torch Tensor. Only int or str or tuple value is supported for PIL Image.\n• padding_mode – Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.\n• constant: pads with a constant value, this value is specified with fill\n• None edge: pads with the last value on the edge of the image, if input a 5D torch Tensor, the last 3 dimensions will be padded instead of the last 2\n• reflect: pads with reflection of image (without repeating the last value on the edge) padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]\n• symmetric: pads with reflection of image (repeating the last value on the edge) padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] Perform perspective transform of the given image. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n• img (PIL Image or Tensor) – Image to be transformed.\n• startpoints (list of list of python:ints) – List containing four lists of two integers corresponding to four corners of the original image.\n• endpoints (list of list of python:ints) – List containing four lists of two integers corresponding to four corners of the transformed image.\n• interpolation (InterpolationMode) – Desired interpolation enum defined by . Default is . If input is Tensor, only , are supported. For backward compatibility integer values (e.g. ) are still acceptable.\n• fill (sequence or number, optional) – Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively. In torchscript mode single int/float value is not supported, please use a sequence of length 1: . If input is PIL Image, the options is only available for . Convert a to a tensor of the same type. This function does not support torchscript. See for more details. pic (PIL Image) – Image to be converted to tensor. Posterize an image by reducing the number of bits for each color channel.\n• img (PIL Image or Tensor) – Image to have its colors posterized. If img is torch Tensor, it should be of type torch.uint8 and it is expected to be in […, 1 or 3, H, W] format, where … means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode “L” or “RGB”.\n• bits (int) – The number of bits to keep for each channel (0-8). Resize the input image to the given size. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n• img (PIL Image or Tensor) – Image to be resized.\n• size (sequence or int) – Desired output size. If size is a sequence like (h, w), the output size will be matched to this. If size is an int, the smaller edge of the image will be matched to this number maintaining the aspect ratio. i.e, if height > width, then image will be rescaled to . In torchscript mode size as single int is not supported, use a sequence of length 1: .\n• interpolation (InterpolationMode) – Desired interpolation enum defined by . Default is . If input is Tensor, only , and are supported. For backward compatibility integer values (e.g. ) are still acceptable. Crop the given image and resize it to desired size. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n• img (PIL Image or Tensor) – Image to be cropped. (0,0) denotes the top left corner of the image.\n• top (int) – Vertical component of the top left corner of the crop box.\n• left (int) – Horizontal component of the top left corner of the crop box.\n• size (sequence or int) – Desired output size. Same semantics as .\n• interpolation (InterpolationMode) – Desired interpolation enum defined by . Default is . If input is Tensor, only , and are supported. For backward compatibility integer values (e.g. ) are still acceptable. Convert RGB image to grayscale version of image. If the image is torch Tensor, it is expected to have […, 3, H, W] shape, where … means an arbitrary number of leading dimensions Please, note that this method supports only RGB images as input. For inputs in other color spaces, please, consider using meth: with PIL Image.\n• img (PIL Image or Tensor) – RGB Image to be converted to grayscale.\n• num_output_channels (int) – number of channels of the output image. Value can be 1 or 3. Default, 1. if num_output_channels = 3 : returned image is 3 channel with r = g = b img: torch.Tensor, angle: float, interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.NEAREST: 'nearest'>, expand: bool = False, center: Union[List[int], NoneType] = None, fill: Union[List[float], NoneType] = None, resample: Union[int, NoneType] = None → torch.Tensor¶ Rotate the image by angle. If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n• img (PIL Image or Tensor) – image to be rotated.\n• interpolation (InterpolationMode) – Desired interpolation enum defined by . Default is . If input is Tensor, only , are supported. For backward compatibility integer values (e.g. ) are still acceptable.\n• expand (bool, optional) – Optional expansion flag. If true, expands the output image to make it large enough to hold the entire rotated image. If false or omitted, make the output image the same size as the input image. Note that the expand flag assumes rotation around the center and no translation.\n• center (sequence, optional) – Optional center of rotation. Origin is the upper left corner. Default is the center of the image.\n• fill (sequence or number, optional) – Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively. In torchscript mode single int/float value is not supported, please use a sequence of length 1: . If input is PIL Image, the options is only available for . Solarize an RGB/grayscale image by inverting all pixel values above a threshold.\n• img (PIL Image or Tensor) – Image to have its colors inverted. If img is torch Tensor, it is expected to be in […, 1 or 3, H, W] format, where … means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode “L” or “RGB”.\n• threshold (float) – All pixels equal or above this value are inverted. Generate ten cropped images from the given image. Crop the given image into four corners and the central crop plus the flipped version of these (horizontal flipping is used by default). If the image is torch Tensor, it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your returns.\n• img (PIL Image or Tensor) – Image to be cropped.\n• size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n• vertical_flip (bool) – Use vertical flipping instead of horizontal Corresponding top left, top right, bottom left, bottom right and center crop and same for the flipped image. Convert PIL image of any mode (RGB, HSV, LAB, etc) to grayscale version of image. This transform does not support torch Tensor.\n• img (PIL Image) – PIL Image to be converted to grayscale.\n• num_output_channels (int) – number of channels of the output image. Value can be 1 or 3. Default is 1. if num_output_channels = 3 : returned image is 3 channel with r = g = b Convert a tensor or an ndarray to PIL Image. This function does not support torchscript. See for more details.\n• pic (Tensor or numpy.ndarray) – Image to be converted to PIL Image. Convert a or to tensor. This function does not support torchscript. See for more details. pic (PIL Image or numpy.ndarray) – Image to be converted to tensor. img (PIL Image or Tensor) – Image to be flipped. If img is a Tensor, it is expected to be in […, H, W] format, where … means it can have an arbitrary number of leading dimensions."
    },
    {
        "link": "https://pytorch.org/vision/0.20/transforms.html",
        "document": "Torchvision supports common computer vision transformations in the and modules. Transforms can be used to transform or augment data for training or inference of different tasks (image classification, detection, segmentation, video classification).\n\nTransforms are typically passed as the or argument to the Datasets.\n\nMost transformations accept both PIL images and tensor inputs. Both CPU and CUDA tensors are supported. The result of both backends (PIL or Tensors) should be very close. In general, we recommend relying on the tensor backend for performance. The conversion transforms may be used to convert to and from PIL images, or for converting dtypes and ranges. Tensor image are expected to be of shape , where is the number of channels, and and refer to height and width. Most transforms support batched tensor input. A batch of Tensor images is a tensor of shape , where is a number of images in the batch. The v2 transforms generally accept an arbitrary number of leading dimensions and can handle batched images or batched videos. The expected range of the values of a tensor image is implicitly defined by the tensor dtype. Tensor images with a float dtype are expected to have values in . Tensor images with an integer dtype are expected to have values in where is the largest value that can be represented in that dtype. Typically, images of dtype are expected to have values in . Use to convert both the dtype and range of the inputs.\n\nV1 or V2? Which one should I use?¶ TL;DR We recommending using the transforms instead of those in . They’re faster and they can do more things. Just change the import and you should be good to go. Moving forward, new features and improvements will only be considered for the v2 transforms. In Torchvision 0.15 (March 2023), we released a new set of transforms available in the namespace. These transforms have a lot of advantages compared to the v1 ones (in ):\n• None They can transform images but also bounding boxes, masks, or videos. This provides support for tasks beyond image classification: detection, segmentation, video classification, etc. See Getting started with transforms v2 and Transforms v2: End-to-end object detection/segmentation example.\n• None They support more transforms like and . See How to use CutMix and MixUp.\n• None Future improvements and features will be added to the v2 transforms only. These transforms are fully backward compatible with the v1 ones, so if you’re already using tranforms from , all you need to do to is to update the import to . In terms of output, there might be negligible differences due to implementation differences.\n\nWe recommend the following guidelines to get the best performance out of the transforms:\n• None Rely on the v2 transforms from\n• None Use tensors instead of PIL images\n• None Use dtype, especially for resizing This is what a typical transform pipeline could look like: # Convert to tensor, only needed if you had a PIL image # optional, most input are already uint8 at this point The above should give you the best performance in a typical training environment that relies on the with . Transforms tend to be sensitive to the input strides / memory format. Some transforms will be faster with channels-first images while others prefer channels-last. Like operators, most transforms will preserve the memory format of the input, but this may not always be respected due to implementation details. You may want to experiment a bit if you’re chasing the very best performance. Using on individual transforms may also help factoring out the memory format variable (e.g. on ). Note that we’re talking about memory format, not tensor shape. Note that resize transforms like and typically prefer channels-last input and tend not to benefit from at this time.\n\nTransforms are available as classes like , but also as functionals like in the namespace. This is very much like the package which defines both classes and functional equivalents in . The functionals support PIL images, pure tensors, or TVTensors, e.g. both and are valid. Random transforms like will randomly sample some parameter each time they’re called. Their functional counterpart ( ) does not do any kind of random sampling and thus have a slighlty different parametrization. The class method of the transforms class can be used to perform parameter sampling when using the functional APIs. The namespace also contains what we call the “kernels”. These are the low-level functions that implement the core functionalities for specific types, e.g. or . They are public, although not documented. Check the code to see which ones are available (note that those starting with a leading underscore are not public!). Kernels are only really useful if you want torchscript support for types like bounding boxes or masks.\n\nMost transform classes and functionals support torchscript. For composing transforms, use instead of : v2 transforms support torchscript, but if you call on a v2 class transform, you’ll actually end up with its (scripted) v1 equivalent. This may lead to slightly different results between the scripted and eager executions due to implementation differences between v1 and v2. If you really need torchscript support for the v2 transforms, we recommend scripting the functionals from the namespace to avoid surprises. Also note that the functionals only support torchscript for pure tensors, which are always treated as images. If you need torchscript support for other types like bounding boxes or masks, you can rely on the low-level kernels. For any custom transformations to be used with , they should be derived from ."
    },
    {
        "link": "https://pytorch.org/vision/stable/models.html",
        "document": "The subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection, video classification, and optical flow.\n\nTorchVision offers pre-trained weights for every provided architecture, using the PyTorch . Instancing a pre-trained model will download its weights to a cache directory. This directory can be set using the environment variable. See for details. The pre-trained models provided in this library may have their own licenses or terms and conditions derived from the dataset used for training. It is your responsibility to determine whether you have permission to use the models for your use case. Backward compatibility is guaranteed for loading a serialized to the model created using old PyTorch version. On the contrary, loading entire saved models or serialized (serialized using older versions of PyTorch) may not preserve the historic behaviour. Refer to the following documentation As of v0.13, TorchVision offers a new Multi-weight support API for loading different weights to the existing model builder methods: # Best available weights (currently alias for IMAGENET1K_V2) # Note that these weights may change across versions Migrating to the new API is very straightforward. The following method calls between the 2 APIs are all equivalent: Note that the parameter is now deprecated, using it will emit warnings and will be removed on v0.15. Before using the pre-trained models, one must preprocess the image (resize with right resolution/interpolation, apply inference transforms, rescale the values etc). There is no standard way to do this as it depends on how a given model was trained. It can vary across model families, variants or even weight versions. Using the correct preprocessing method is critical and failing to do so may lead to decreased accuracy or incorrect outputs. All the necessary information for the inference transforms of each pre-trained model is provided on its weights documentation. To simplify inference, TorchVision bundles the necessary preprocessing transforms into each model weight. These are accessible via the attribute: # Apply it to the input image Some models use modules which have different training and evaluation behavior, such as batch normalization. To switch between these modes, use or as appropriate. See or for details. As of v0.14, TorchVision offers a new mechanism which allows listing and retrieving models and weights by their names. Here are a few examples on how to use them: Here are the available public functions to retrieve models and their corresponding weights: Gets the model name and configuration and returns an instantiated model. Returns the weights enum class associated to the given model. Gets the weights enum value by its full name. Returns a list with the names of registered models. Most pre-trained models can be accessed directly via PyTorch Hub without having TorchVision installed: You can also retrieve all the available weights of a specific model via PyTorch Hub by doing: The only exception to the above are the detection models included on . These models require TorchVision to be installed because they depend on custom C++ operators.\n\nThe segmentation module is in Beta stage, and backward compatibility is not guaranteed. The following semantic segmentation models are available, with or without pre-trained weights: Here is an example of how to use the pre-trained semantic segmentation models: # Step 1: Initialize model with the best available weights # Step 4: Use the model and visualize the prediction The classes of the pre-trained model outputs can be found at . The output format of the models is illustrated in Semantic segmentation models. Table of all available semantic segmentation weights¶ All models are evaluated a subset of COCO val2017, on the 20 categories that are present in the Pascal VOC dataset:\n\nThe pre-trained models for detection, instance segmentation and keypoint detection are initialized with the classification models in torchvision. The models expect a list of . Check the constructor of the models for more information. The detection module is in Beta stage, and backward compatibility is not guaranteed. The following object detection models are available, with or without pre-trained weights: Here is an example of how to use the pre-trained object detection models: # Step 1: Initialize model with the best available weights # Step 4: Use the model and visualize the prediction The classes of the pre-trained model outputs can be found at . For details on how to plot the bounding boxes of the models, you may refer to Instance segmentation models. Table of all available Object detection weights¶ The following instance segmentation models are available, with or without pre-trained weights: For details on how to plot the masks of the models, you may refer to Instance segmentation models. Table of all available Instance segmentation weights¶ Box and Mask MAPs are reported on COCO val2017: The following person keypoint detection models are available, with or without pre-trained weights: The classes of the pre-trained model outputs can be found at . For details on how to plot the bounding boxes of the models, you may refer to Visualizing keypoints. Table of all available Keypoint detection weights¶ Box and Keypoint MAPs are reported on COCO val2017:\n\nThe video module is in Beta stage, and backward compatibility is not guaranteed. The following video classification models are available, with or without pre-trained weights: Here is an example of how to use the pre-trained video classification models: # Step 1: Initialize model with the best available weights # Step 4: Use the model and print the predicted category The classes of the pre-trained model outputs can be found at . Table of all available video classification weights¶ Accuracies are reported on Kinetics-400 using single crops for clip length 16:"
    },
    {
        "link": "https://pytorch.org/vision/0.8/transforms.html",
        "document": "Transforms are common image transformations. They can be chained together using . Additionally, there is the module. Functional transforms give fine-grained control over the transformations. This is useful if you have to build a more complex transformation pipeline (e.g. in the case of segmentation tasks).\n\nAll transformations accept PIL Image, Tensor Image or batch of Tensor Images as input. Tensor Image is a tensor with shape, where is a number of channels, and are image height and width. Batch of Tensor Images is a tensor of shape, where is a number of images in the batch. Deterministic or random transformations applied on the batch of Tensor Images identically transform all the images of the batch.\n\nCrops the given image at the center. The image can be a PIL Image or a torch Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a tuple or list of length 1, it will be interpreted as (size[0], size[0]). img (PIL Image or Tensor) – Image to be cropped. Randomly change the brightness, contrast and saturation of an image.\n• brightness (float or tuple of python:float (min, max)) – How much to jitter brightness. brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers.\n• contrast (float or tuple of python:float (min, max)) – How much to jitter contrast. contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast] or the given [min, max]. Should be non negative numbers.\n• saturation (float or tuple of python:float (min, max)) – How much to jitter saturation. saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation] or the given [min, max]. Should be non negative numbers.\n• hue (float or tuple of python:float (min, max)) – How much to jitter hue. hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5. Get a randomized transform to be applied on image. Arguments are same as that of __init__. Transform which randomly adjusts brightness, contrast and saturation in a random order. Crop the given image into four corners and the central crop. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your Dataset returns. See below for an example of how to deal with this. size (sequence or int) – Desired output size of the crop. If size is an instead of sequence like (h, w), a square crop of size (size, size) is made. If provided a tuple or list of length 1, it will be interpreted as (size[0], size[0]). # this is a list of PIL Images #In your test loop you can do the following: img (PIL Image or Tensor) – Image to be cropped. tuple of 5 images. Image can be PIL Image or Tensor Convert image to grayscale. The image can be a PIL Image or a Tensor, in which case it is expected to have […, 3, H, W] shape, where … means an arbitrary number of leading dimensions num_output_channels (int) – (1 or 3) number of channels desired for output image\n• If : returned image is 3 channel with r == g == b img (PIL Image or Tensor) – Image to be converted to grayscale. Pad the given image on all sides with the given “pad” value. The image can be a PIL Image or a torch Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n• padding (int or tuple or list) – Padding on each border. If a single int is provided this is used to pad all borders. If tuple of length 2 is provided this is the padding on left/right and top/bottom respectively. If a tuple of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. In torchscript mode padding as single int is not supported, use a tuple or list of length 1: .\n• fill (int or tuple) – Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant\n• padding_mode (str) – Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant. Mode symmetric is not yet supported for Tensor inputs.\n• constant: pads with a constant value, this value is specified with fill\n• edge: pads with the last value at the edge of the image\n• reflect: pads with reflection of image without repeating the last value on the edge For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]\n• symmetric: pads with reflection of image repeating the last value on the edge For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] img (PIL Image or Tensor) – Image to be padded. Random affine transformation of the image keeping center invariant. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n• degrees (sequence or float or int) – Range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees). Set to 0 to deactivate rotations.\n• translate (tuple, optional) – tuple of maximum absolute fraction for horizontal and vertical translations. For example translate=(a, b), then horizontal shift is randomly sampled in the range -img_width * a < dx < img_width * a and vertical shift is randomly sampled in the range -img_height * b < dy < img_height * b. Will not translate by default.\n• scale (tuple, optional) – scaling factor interval, e.g (a, b), then scale is randomly sampled from the range a <= scale <= b. Will keep original scale by default.\n• shear (sequence or float or int, optional) – Range of degrees to select from. If shear is a number, a shear parallel to the x axis in the range (-shear, +shear) will be applied. Else if shear is a tuple or list of 2 values a shear parallel to the x axis in the range (shear[0], shear[1]) will be applied. Else if shear is a tuple or list of 4 values, a x-axis shear in (shear[0], shear[1]) and y-axis shear in (shear[2], shear[3]) will be applied. Will not apply shear by default.\n• resample (int, optional) – An optional resampling filter. See filters for more information. If omitted, or if the image has mode “1” or “P”, it is set to . If input is Tensor, only and are supported.\n• fillcolor (tuple or int) – Optional fill color (Tuple for RGB Image and int for grayscale) for the area outside the transform in the output image (Pillow>=5.0.0). This option is not supported for Tensor input. Fill value for the area outside the transform in the output image is always 0. img (PIL Image or Tensor): Image to be transformed. params to be passed to the affine transformation Apply randomly a list of transformations with a given probability. In order to script the transformation, please use as input instead of list/tuple of transforms as shown below: Make sure to use only scriptable transformations, i.e. that work with , does not require functions or .\n• transforms (list or tuple or torch.nn.Module) – list of transformations Crop the given image at a random location. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n• size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a tuple or list of length 1, it will be interpreted as (size[0], size[0]).\n• padding (int or sequence, optional) – Optional padding on each border of the image. Default is None. If a single int is provided this is used to pad all borders. If tuple of length 2 is provided this is the padding on left/right and top/bottom respectively. If a tuple of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. In torchscript mode padding as single int is not supported, use a tuple or list of length 1: .\n• pad_if_needed (boolean) – It will pad the image if smaller than the desired size to avoid raising an exception. Since cropping is done after padding, the padding seems to be done at a random offset.\n• fill (int or tuple) – Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant\n• padding_mode (str) – Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant. Mode symmetric is not yet supported for Tensor inputs.\n• constant: pads with a constant value, this value is specified with fill\n• edge: pads with the last value on the edge of the image\n• reflect: pads with reflection of image (without repeating the last value on the edge) padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]\n• symmetric: pads with reflection of image (repeating the last value on the edge) padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] img (PIL Image or Tensor) – Image to be cropped. Get parameters for for a random crop.\n• img (PIL Image or Tensor) – Image to be cropped. params (i, j, h, w) to be passed to for random crop. Randomly convert image to grayscale with a probability of p (default 0.1). The image can be a PIL Image or a Tensor, in which case it is expected to have […, 3, H, W] shape, where … means an arbitrary number of leading dimensions p (float) – probability that image should be converted to grayscale. Grayscale version of the input image with probability p and unchanged with probability (1-p). - If input image is 1 channel: grayscale version is 1 channel - If input image is 3 channel: grayscale version is 3 channel with r == g == b img (PIL Image or Tensor) – Image to be converted to grayscale. Horizontally flip the given image randomly with a given probability. The image can be a PIL Image or a torch Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions p (float) – probability of the image being flipped. Default value is 0.5 img (PIL Image or Tensor) – Image to be flipped. Performs a random perspective transformation of the given image with a given probability. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n• distortion_scale (float) – argument to control the degree of distortion and ranges from 0 to 1. Default is 0.5.\n• p (float) – probability of the image being transformed. Default is 0.5.\n• interpolation (int) – Interpolation type. If input is Tensor, only and are supported. Default, for PIL images and Tensors.\n• fill (n-tuple or int or float) – Pixel fill value for area outside the rotated image. If int or float, the value is used for all bands respectively. Default is 0. This option is only available for . This option is not supported for Tensor input. Fill value for the area outside the transform in the output image is always 0. img (PIL Image or Tensor) – Image to be Perspectively transformed. Get parameters for for a random perspective transform.\n• distortion_scale (float) – argument to control the degree of distortion and ranges from 0 to 1. List containing [top-left, top-right, bottom-right, bottom-left] of the original image, List containing [top-left, top-right, bottom-right, bottom-left] of the transformed image. Crop the given image to random size and aspect ratio. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions A crop of random size (default: of 0.08 to 1.0) of the original size and a random aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop is finally resized to given size. This is popularly used to train the Inception networks.\n• size (int or sequence) – expected output size of each edge. If size is an int instead of sequence like (h, w), a square output size is made. If provided a tuple or list of length 1, it will be interpreted as (size[0], size[0]).\n• scale (tuple of python:float) – range of size of the origin size cropped\n• ratio (tuple of python:float) – range of aspect ratio of the origin aspect ratio cropped.\n• interpolation (int) – Desired interpolation enum defined by filters. Default is . If input is Tensor, only , and are supported. img (PIL Image or Tensor) – Image to be cropped and resized. Get parameters for for a random sized crop.\n• scale (list) – range of scale of the origin size cropped\n• ratio (list) – range of aspect ratio of the origin aspect ratio cropped params (i, j, h, w) to be passed to for a random Rotate the image by angle. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n• degrees (sequence or float or int) – Range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees).\n• resample (int, optional) – An optional resampling filter. See filters for more information. If omitted, or if the image has mode “1” or “P”, it is set to PIL.Image.NEAREST. If input is Tensor, only and are supported.\n• expand (bool, optional) – Optional expansion flag. If true, expands the output to make it large enough to hold the entire rotated image. If false or omitted, make the output image the same size as the input image. Note that the expand flag assumes rotation around the center and no translation.\n• center (list or tuple, optional) – Optional center of rotation, (x, y). Origin is the upper left corner. Default is the center of the image.\n• fill (n-tuple or int or float) – Pixel fill value for area outside the rotated image. If int or float, the value is used for all bands respectively. Defaults to 0 for all bands. This option is only available for Pillow>=5.2.0. This option is not supported for Tensor input. Fill value for the area outside the transform in the output image is always 0. img (PIL Image or Tensor) – Image to be rotated. Get parameters for for a random rotation. angle parameter to be passed to for random rotation. Note: This transform is deprecated in favor of RandomResizedCrop. Vertically flip the given image randomly with a given probability. The image can be a PIL Image or a torch Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions p (float) – probability of the image being flipped. Default value is 0.5 img (PIL Image or Tensor) – Image to be flipped. Resize the input image to the given size. The image can be a PIL Image or a torch Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n• size (sequence or int) – Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number. i.e, if height > width, then image will be rescaled to (size * height / width, size). In torchscript mode padding as single int is not supported, use a tuple or list of length 1: .\n• interpolation (int, optional) – Desired interpolation enum defined by filters. Default is . If input is Tensor, only , and are supported. img (PIL Image or Tensor) – Image to be scaled. Note: This transform is deprecated in favor of Resize. Crop the given image into four corners and the central crop plus the flipped version of these (horizontal flipping is used by default). The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your Dataset returns. See below for an example of how to deal with this.\n• size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a tuple or list of length 1, it will be interpreted as (size[0], size[0]).\n• vertical_flip (bool) – Use vertical flipping instead of horizontal # this is a list of PIL Images #In your test loop you can do the following: img (PIL Image or Tensor) – Image to be cropped. tuple of 10 images. Image can be PIL Image or Tensor Blurs image with randomly chosen Gaussian blur. The image can be a PIL Image or a Tensor, in which case it is expected to have […, C, H, W] shape, where … means an arbitrary number of leading dimensions\n• kernel_size (int or sequence) – Size of the Gaussian kernel.\n• sigma (float or tuple of python:float (min, max)) – Standard deviation to be used for creating kernel to perform blurring. If float, sigma is fixed. If it is tuple of float (min, max), sigma is chosen uniformly at random to lie in the given range. img (PIL Image or Tensor) – image to be blurred.\n• sigma_min (float) – Minimum standard deviation that can be chosen for blurring kernel.\n• sigma_max (float) – Maximum standard deviation that can be chosen for blurring kernel. Standard deviation to be passed to calculate kernel for gaussian blurring.\n\nFunctional transforms give you fine-grained control of the transformation pipeline. As opposed to the transformations above, functional transforms don’t contain a random number generator for their parameters. That means you have to specify/generate all parameters, but you can reuse the functional transform. Example: you can apply a functional transform with the same parameters to multiple images like this: Example: you can use a functional transform to build transform classes with custom behavior: \"\"\"Rotate by one of the given angles.\"\"\"\n• img (PIL Image or Tensor) – Image to be adjusted.\n• brightness_factor (float) – How much to adjust the brightness. Can be any non negative number. 0 gives a black image, 1 gives the original image while 2 increases the brightness by a factor of 2.\n• img (PIL Image or Tensor) – Image to be adjusted.\n• contrast_factor (float) – How much to adjust the contrast. Can be any non negative number. 0 gives a solid gray image, 1 gives the original image while 2 increases the contrast by a factor of 2. Also known as Power Law Transform. Intensities in RGB mode are adjusted based on the following equation: See Gamma Correction for more details.\n• img (PIL Image or Tensor) – PIL Image to be adjusted.\n• gamma (float) – Non negative real number, same as in the equation. gamma larger than 1 make the shadows darker, while gamma smaller than 1 make dark regions lighter. The image hue is adjusted by converting the image to HSV and cyclically shifting the intensities in the hue channel (H). The image is then converted back to original image mode. is the amount of shift in H channel and must be in the interval . See Hue for more details.\n• img (PIL Image or Tensor) – Image to be adjusted.\n• hue_factor (float) – How much to shift the hue channel. Should be in [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in HSV space in positive and negative direction respectively. 0 means no shift. Therefore, both -0.5 and 0.5 will give an image with complementary colors while 0 gives the original image.\n• img (PIL Image or Tensor) – Image to be adjusted.\n• saturation_factor (float) – How much to adjust the saturation. 0 will give a black and white image, 1 will give the original image while 2 will enhance the saturation by a factor of 2. Apply affine transformation on the image keeping image center invariant. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n• angle (float or int) – rotation angle in degrees between -180 and 180, clockwise direction.\n• translate (list or tuple of python:integers) – horizontal and vertical translations (post-rotation translation)\n• shear (float or tuple or list) – shear angle value in degrees between -180 to 180, clockwise direction. If a tuple of list is specified, the first value corresponds to a shear parallel to the x axis, while the second value corresponds to a shear parallel to the y axis.\n• resample ( or or , optional) – An optional resampling filter. See filters for more information. If omitted, or if the image is PIL Image and has mode “1” or “P”, it is set to . If input is Tensor, only and are supported.\n• fillcolor (int) – Optional fill color for the area outside the transform in the output image (Pillow>=5.0.0). This option is not supported for Tensor input. Fill value for the area outside the transform in the output image is always 0. Crops the given image at the center. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n• img (PIL Image or Tensor) – Image to be cropped.\n• output_size (sequence or int) – (height, width) of the crop box. If int or sequence with single int it is used for both directions. Convert a tensor image to the given and scale the values accordingly When converting from a smaller to a larger integer the maximum values are not mapped exactly. If converted back and forth, this mismatch has no effect. – When trying to cast to or as well as for trying to cast to . These conversions might lead to overflow errors since the floating point cannot store consecutive integers over the whole range of the integer . Crop the given image at specified location and output size. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n• img (PIL Image or Tensor) – Image to be cropped. (0,0) denotes the top left corner of the image.\n• top (int) – Vertical component of the top left corner of the crop box.\n• left (int) – Horizontal component of the top left corner of the crop box. Erase the input Tensor Image with given value.\n• img (Tensor Image) – Tensor image of size (C, H, W) to be erased\n• i (int) – i in (i,j) i.e coordinates of the upper left corner.\n• j (int) – j in (i,j) i.e coordinates of the upper left corner.\n• inplace (bool, optional) – For in-place operations. By default is set False. Crop the given image into four corners and the central crop. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your returns.\n• img (PIL Image or Tensor) – Image to be cropped.\n• size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a tuple or list of length 1, it will be interpreted as (size[0], size[0]). Corresponding top left, top right, bottom left, bottom right and center crop. Performs Gaussian blurring on the img by given kernel. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n• img (PIL Image or Tensor) – Image to be blurred\n• kernel_size (sequence of python:ints or int) – Gaussian kernel size. Can be a sequence of integers like or a single integer for square kernels. In torchscript mode kernel_size as single int is not supported, use a tuple or list of length 1: .\n• sigma (sequence of python:floats or float, optional) – Gaussian kernel standard deviation. Can be a sequence of floats like or a single float to define the same sigma in both X/Y directions. If None, then it is computed using as . Default, None. In torchscript mode sigma as single float is not supported, use a tuple or list of length 1: . Horizontally flip the given PIL Image or Tensor. img (PIL Image or Tensor) – Image to be flipped. If img is a Tensor, it is expected to be in […, H, W] format, where … means it can have an arbitrary number of trailing dimensions. Normalize a tensor image with mean and standard deviation. This transform acts out of place by default, i.e., it does not mutates the input tensor. See for more details.\n• tensor (Tensor) – Tensor image of size (C, H, W) or (B, C, H, W) to be normalized.\n• mean (sequence) – Sequence of means for each channel.\n• std (sequence) – Sequence of standard deviations for each channel. Pad the given image on all sides with the given “pad” value. The image can be a PIL Image or a torch Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n• img (PIL Image or Tensor) – Image to be padded.\n• padding (int or tuple or list) – Padding on each border. If a single int is provided this is used to pad all borders. If tuple of length 2 is provided this is the padding on left/right and top/bottom respectively. If a tuple of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. In torchscript mode padding as single int is not supported, use a tuple or list of length 1: .\n• fill (int or str or tuple) – Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant. Only int value is supported for Tensors.\n• padding_mode – Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant. Mode symmetric is not yet supported for Tensor inputs.\n• constant: pads with a constant value, this value is specified with fill\n• edge: pads with the last value on the edge of the image\n• reflect: pads with reflection of image (without repeating the last value on the edge) padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]\n• symmetric: pads with reflection of image (repeating the last value on the edge) padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] Perform perspective transform of the given image. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n• img (PIL Image or Tensor) – Image to be transformed.\n• startpoints (list of list of python:ints) – List containing four lists of two integers corresponding to four corners of the original image.\n• endpoints (list of list of python:ints) – List containing four lists of two integers corresponding to four corners of the transformed image.\n• interpolation (int) – Interpolation type. If input is Tensor, only and are supported. Default, for PIL images and Tensors.\n• fill (n-tuple or int or float) – Pixel fill value for area outside the rotated image. If int or float, the value is used for all bands respectively. This option is only available for . This option is not supported for Tensor input. Fill value for the area outside the transform in the output image is always 0. Convert a to a tensor of the same type. See for more details. pic (PIL Image) – Image to be converted to tensor. Resize the input image to the given size. The image can be a PIL Image or a torch Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n• img (PIL Image or Tensor) – Image to be resized.\n• size (sequence or int) – Desired output size. If size is a sequence like (h, w), the output size will be matched to this. If size is an int, the smaller edge of the image will be matched to this number maintaining the aspect ratio. i.e, if height > width, then image will be rescaled to . In torchscript mode size as single int is not supported, use a tuple or list of length 1: .\n• interpolation (int, optional) – Desired interpolation enum defined by filters. Default is . If input is Tensor, only , and are supported. Crop the given image and resize it to desired size. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n• img (PIL Image or Tensor) – Image to be cropped. (0,0) denotes the top left corner of the image.\n• top (int) – Vertical component of the top left corner of the crop box.\n• left (int) – Horizontal component of the top left corner of the crop box.\n• size (sequence or int) – Desired output size. Same semantics as .\n• interpolation (int, optional) – Desired interpolation enum defined by filters. Default is . If input is Tensor, only , and are supported. Convert RGB image to grayscale version of image. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions Please, note that this method supports only RGB images as input. For inputs in other color spaces, please, consider using meth: with PIL Image.\n• img (PIL Image or Tensor) – RGB Image to be converted to grayscale.\n• num_output_channels (int) – number of channels of the output image. Value can be 1 or 3. Default, 1. if num_output_channels = 3 : returned image is 3 channel with r = g = b Rotate the image by angle. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n• img (PIL Image or Tensor) – image to be rotated.\n• angle (float or int) – rotation angle value in degrees, counter-clockwise.\n• resample ( or or , optional) – An optional resampling filter. See filters for more information. If omitted, or if the image has mode “1” or “P”, it is set to .\n• expand (bool, optional) – Optional expansion flag. If true, expands the output image to make it large enough to hold the entire rotated image. If false or omitted, make the output image the same size as the input image. Note that the expand flag assumes rotation around the center and no translation.\n• center (list or tuple, optional) – Optional center of rotation. Origin is the upper left corner. Default is the center of the image.\n• fill (n-tuple or int or float) – Pixel fill value for area outside the rotated image. If int or float, the value is used for all bands respectively. Defaults to 0 for all bands. This option is only available for . This option is not supported for Tensor input. Fill value for the area outside the transform in the output image is always 0. Generate ten cropped images from the given image. Crop the given image into four corners and the central crop plus the flipped version of these (horizontal flipping is used by default). The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your returns.\n• img (PIL Image or Tensor) – Image to be cropped.\n• size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a tuple or list of length 1, it will be interpreted as (size[0], size[0]).\n• vertical_flip (bool) – Use vertical flipping instead of horizontal Corresponding top left, top right, bottom left, bottom right and center crop and same for the flipped image. Convert PIL image of any mode (RGB, HSV, LAB, etc) to grayscale version of image.\n• img (PIL Image) – PIL Image to be converted to grayscale.\n• num_output_channels (int) – number of channels of the output image. Value can be 1 or 3. Default, 1. if num_output_channels = 3 : returned image is 3 channel with r = g = b Convert a tensor or an ndarray to PIL Image. See for more details.\n• pic (Tensor or numpy.ndarray) – Image to be converted to PIL Image. See for more details. pic (PIL Image or numpy.ndarray) – Image to be converted to tensor. Vertically flip the given PIL Image or torch Tensor. img (PIL Image or Tensor) – Image to be flipped. If img is a Tensor, it is expected to be in […, H, W] format, where … means it can have an arbitrary number of trailing dimensions."
    },
    {
        "link": "https://huggingface.co/docs/transformers/en/model_doc/gpt2",
        "document": "and get access to the augmented documentation experience\n\nOpenAI GPT-2 model was proposed in Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever from OpenAI. It’s a causal (unidirectional) transformer pretrained using language modeling on a very large corpus of ~40 GB of text data.\n\nThe abstract from the paper is the following:\n\nGPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n\nWrite With Transformer is a webapp created and hosted by Hugging Face showcasing the generative capabilities of several models. GPT-2 is one of them and is available in five different sizes: small, medium, large, xl and a distilled version of the small checkpoint: distilgpt-2.\n\nThis model was contributed by thomwolf. The original code can be found here.\n• GPT-2 is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n• GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n• The model can take the past_key_values (for PyTorch) or past (for TF) as input, which is the previously computed key/value attention pairs. Using this (past_key_values or past) value prevents the model from re-computing pre-computed values in the context of text generation. For PyTorch, see past_key_values argument of the GPT2Model.forward() method, or for TF the past argument of the TFGPT2Model.call() method for more information on its usage.\n• Enabling the scale_attn_by_inverse_layer_idx and reorder_and_upcast_attn flags will apply the training stability improvements from Mistral (for PyTorch only).\n\nThe method can be used to generate text using GPT2 model.\n\nFlash Attention 2 is a faster, optimized version of the attention scores computation which relies on kernels.\n\nFirst, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the official documentation. If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered above.\n\nNext, install the latest version of Flash Attention 2:\n\nTo load a model using Flash Attention 2, we can pass the argument to . We’ll also load the model in half-precision (e.g. ), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation in transformers using checkpoint and the Flash Attention 2 version of the model using a sequence length of 512.\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of . This function encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the official documentation or the GPU Inference page for more information.\n\nSDPA is used by default for when an implementation is available, but you may also set in to explicitly request SDPA to be used.\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. or ).\n\nOn a local benchmark (rtx3080ti-16GB, PyTorch 2.2.1, OS Ubuntu 22.04) using with gpt2-large, we saw the following speedups during training and inference.\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with GPT2. If you’re interested in submitting a resource to be included here, please feel free to open a Pull Request and we’ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n• A blog on how to Finetune a non-English GPT-2 Model with Hugging Face.\n• A blog on How to generate text: using different decoding methods for language generation with Transformers with GPT-2.\n• A blog on Faster Text Generation with TensorFlow and XLA with GPT-2.\n• A blog on How to train a Language Model with Megatron-LM with a GPT-2 model.\n• A notebook on how to finetune GPT2 to generate lyrics in the style of your favorite artist. 🌎\n• A notebook on how to finetune GPT2 to generate tweets in the style of your favorite Twitter user. 🌎\n• Causal language modeling chapter of the 🤗 Hugging Face Course.\n• GPT2LMHeadModel is supported by this causal language modeling example script, text generation example script, and notebook.\n• TFGPT2LMHeadModel is supported by this causal language modeling example script and notebook.\n• FlaxGPT2LMHeadModel is supported by this causal language modeling example script and notebook."
    },
    {
        "link": "https://huggingface.co/docs/transformers/main/en/model_doc/gpt2",
        "document": "and get access to the augmented documentation experience\n\nOpenAI GPT-2 model was proposed in Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever from OpenAI. It’s a causal (unidirectional) transformer pretrained using language modeling on a very large corpus of ~40 GB of text data.\n\nThe abstract from the paper is the following:\n\nGPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n\nWrite With Transformer is a webapp created and hosted by Hugging Face showcasing the generative capabilities of several models. GPT-2 is one of them and is available in five different sizes: small, medium, large, xl and a distilled version of the small checkpoint: distilgpt-2.\n\nThis model was contributed by thomwolf. The original code can be found here.\n• GPT-2 is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n• GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n• The model can take the past_key_values (for PyTorch) or past (for TF) as input, which is the previously computed key/value attention pairs. Using this (past_key_values or past) value prevents the model from re-computing pre-computed values in the context of text generation. For PyTorch, see past_key_values argument of the GPT2Model.forward() method, or for TF the past argument of the TFGPT2Model.call() method for more information on its usage.\n• Enabling the scale_attn_by_inverse_layer_idx and reorder_and_upcast_attn flags will apply the training stability improvements from Mistral (for PyTorch only).\n\nThe method can be used to generate text using GPT2 model.\n\nFlash Attention 2 is a faster, optimized version of the attention scores computation which relies on kernels.\n\nFirst, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the official documentation. If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered above.\n\nNext, install the latest version of Flash Attention 2:\n\nTo load a model using Flash Attention 2, we can pass the argument to . We’ll also load the model in half-precision (e.g. ), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation in transformers using checkpoint and the Flash Attention 2 version of the model using a sequence length of 512.\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of . This function encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the official documentation or the GPU Inference page for more information.\n\nSDPA is used by default for when an implementation is available, but you may also set in to explicitly request SDPA to be used.\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. or ).\n\nOn a local benchmark (rtx3080ti-16GB, PyTorch 2.2.1, OS Ubuntu 22.04) using with gpt2-large, we saw the following speedups during training and inference.\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with GPT2. If you’re interested in submitting a resource to be included here, please feel free to open a Pull Request and we’ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n• A blog on how to Finetune a non-English GPT-2 Model with Hugging Face.\n• A blog on How to generate text: using different decoding methods for language generation with Transformers with GPT-2.\n• A blog on Faster Text Generation with TensorFlow and XLA with GPT-2.\n• A blog on How to train a Language Model with Megatron-LM with a GPT-2 model.\n• A notebook on how to finetune GPT2 to generate lyrics in the style of your favorite artist. 🌎\n• A notebook on how to finetune GPT2 to generate tweets in the style of your favorite Twitter user. 🌎\n• Causal language modeling chapter of the 🤗 Hugging Face Course.\n• GPT2LMHeadModel is supported by this causal language modeling example script, text generation example script, and notebook.\n• TFGPT2LMHeadModel is supported by this causal language modeling example script and notebook.\n• FlaxGPT2LMHeadModel is supported by this causal language modeling example script and notebook."
    },
    {
        "link": "https://huggingface.co/openai-community/gpt2",
        "document": "Test the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\n\nPretrained model on English language using a causal language modeling (CLM) objective. It was introduced in this paper and first released at this page.\n\nDisclaimer: The team releasing GPT-2 also wrote a model card for their model. Content from this model card has been written by the Hugging Face team to complete the information they provided and give specific examples of bias.\n\nGPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token only uses the inputs from to but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.\n\nThis is the smallest version of GPT-2, with 124M parameters.\n\nYou can use the raw model for text generation or fine-tune it to a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n\nHow to use\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\nThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their model card:\n\nBecause large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes.\n\nHere's an example of how the model can have biased predictions:\n\nThis bias will also affect all fine-tuned versions of this model.\n\nThe OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web pages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from this dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText here.\n\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.\n\nThe larger model was trained on 256 cloud TPU v3 cores. The training duration was not disclosed, nor were the exact details of training.\n\nThe model achieves the following results without any fine-tuning (zero-shot):"
    },
    {
        "link": "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py",
        "document": ""
    },
    {
        "link": "https://huggingface.co/learn/nlp-course/en/chapter7/6",
        "document": "Up until now, we’ve mostly been using pretrained models and fine-tuning them for new use cases by reusing the weights from pretraining. As we saw in Chapter 1, this is commonly referred to as transfer learning, and it’s a very successful strategy for applying Transformer models to most real-world use cases where labeled data is sparse. In this chapter, we’ll take a different approach and train a completely new model from scratch. This is a good approach to take if you have a lot of data and it is very different from the pretraining data used for the available models. However, it also requires considerably more compute resources to pretrain a language model than just to fine-tune an existing one. Examples where it can make sense to train a new model include for datasets consisting of musical notes, molecular sequences such as DNA, or programming languages. The latter have recently gained traction thanks to tools such as TabNine and GitHub’s Copilot, powered by OpenAI’s Codex model, that can generate long sequences of code. This task of text generation is best addressed with auto-regressive or causal language models such as GPT-2.\n\nIn this section we will build a scaled-down version of a code generation model: we’ll focus on one-line completions instead of full functions or classes, using a subset of Python code. When working with data in Python you are in frequent contact with the Python data science stack, consisting of the , , , and libraries. When using those frameworks it’s common to need to look up specific commands, so it would be nice if we could use a model to complete these calls for us.\n\nIn Chapter 6 we created an efficient tokenizer to process Python source code, but what we still need is a large-scale dataset to pretrain a model on. Here, we’ll apply our tokenizer to a corpus of Python code derived from GitHub repositories. We will then use the API and 🤗 Accelerate to train the model. Let’s get to it!\n\nThis is actually showcasing the model that was trained and uploaded to the Hub using the code shown in this section. You can find it here. Note that since there is some randomization happening in the text generation, you will probably get a slightly different result.\n\nPython code is abundantly available from code repositories such as GitHub, which we can use to create a dataset by scraping for every Python repository. This was the approach taken in the Transformers textbook to pretrain a large GPT-2 model. Using a GitHub dump of about 180 GB containing roughly 20 million Python files called , the authors built a dataset that they then shared on the Hugging Face Hub.\n\nHowever, training on the full corpus is time- and compute-consuming, and we only need the subset of the dataset concerned with the Python data science stack. So, let’s start by filtering the dataset for all files that include any of the libraries in this stack. Because of the dataset’s size, we want to avoid downloading it; instead, we’ll use the streaming feature to filter it on the fly. To help us filter the code samples using the libraries we mentioned earlier, we’ll use the following function:\n\nLet’s test it on two examples:\n\nWe can use this to create a function that will stream the dataset and filter the elements we want:\n\nThen we can simply apply this function to the streaming dataset:\n\nThis leaves us with about 3% of the original dataset, which is still quite sizable — the resulting dataset is 6 GB and consists of 600,000 Python scripts!\n\nFiltering the full dataset can take 2-3h depending on your machine and bandwidth. If you don’t want to go through this lengthy process yourself, we provide the filtered dataset on the Hub for you to download:\n\nLet’s look at an example from the dataset. We’ll just show the first 200 characters of each field:\n\nWe can see that the field contains the code that we want our model to train on. Now that we have a dataset, we need to prepare the texts so they’re in a format suitable for pretraining.\n\nThe first step will be to tokenize the data, so we can use it for training. Since our goal is to mainly autocomplete short function calls, we can keep the context size relatively small. This has the benefit that we can train the model much faster and it requires significantly less memory. If it is important for your application to have more context (for example, if you want the model to write unit tests based on a file with the function definition), make sure you increase that number, but also keep in mind that this comes with a greater GPU memory footprint. For now, let’s fix the context size at 128 tokens, as opposed to the 1,024 or 2,048 used in GPT-2 or GPT-3, respectively.\n\nMost documents contain many more than 128 tokens, so simply truncating the inputs to the maximum length would eliminate a large fraction of our dataset. Instead, we’ll use the option to tokenize the whole input and split it into several chunks, as we did in Chapter 6. We’ll also use the option to return the length of each created chunk automatically. Often the last chunk will be smaller than the context size, and we’ll get rid of these pieces to avoid padding issues; we don’t really need them as we have plenty of data anyway.\n\nLet’s see exactly how this works by looking at the first two examples:\n\nWe can see that we get 34 segments in total from those two examples. Looking at the chunk lengths, we can see that the chunks at the ends of both documents have less than 128 tokens (117 and 41, respectively). These represent just a small fraction of the total chunks that we have, so we can safely throw them away. With the field, we can also reconstruct which chunks belonged to which input samples.\n\nWith this operation we’re using a handy feature of the function in 🤗 Datasets, which is that it does not require one-to-one maps; as we saw in section 3, we can create batches with more or fewer elements than the input batch. This is useful when doing operations like data augmentation or data filtering that change the number of elements. In our case, when tokenizing each element into chunks of the specified context size, we create many samples from each document. We just need to make sure to delete the existing columns, since they have a conflicting size. If we wanted to keep them, we could repeat them appropriately and return them within the call:\n\nWe now have 16.7 million examples with 128 tokens each, which corresponds to about 2.1 billion tokens in total. For reference, OpenAI’s GPT-3 and Codex models are trained on 300 and 100 billion tokens, respectively, where the Codex models are initialized from the GPT-3 checkpoints. Our goal in this section is not to compete with these models, which can generate long, coherent texts, but to create a scaled-down version providing a quick autocomplete function for data scientists.\n\nNow that we have the dataset ready, let’s set up the model!\n\nOur first step is to freshly initialize a GPT-2 model. We’ll use the same configuration for our model as for the small GPT-2 model, so we load the pretrained configuration, make sure that the tokenizer size matches the model vocabulary size and pass the and (beginning and end of sequence) token IDs:\n\nWith that configuration, we can load a new model. Note that this is the first time we don’t use the function, since we’re actually initializing a model ourself:\n\nOur model has 124M parameters that we’ll have to tune. Before we can start training, we need to set up a data collator that will take care of creating the batches. We can use the collator, which is designed specifically for language modeling (as the name subtly suggests). Besides stacking and padding batches, it also takes care of creating the language model labels — in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training so we don’t need to duplicate the .\n\nNote that supports both masked language modeling (MLM) and causal language modeling (CLM). By default it prepares data for MLM, but we can switch to CLM by setting the argument :\n\nLet’s have a look at an example:\n\nWe can see that the examples have been stacked and all the tensors have the same shape.\n\nNow we have everything in place to actually train our model — that wasn’t so much work after all! Before we start training we should log in to Hugging Face. If you’re working in a notebook, you can do so with the following utility function:\n\nThis will display a widget where you can enter your Hugging Face login credentials.\n\nIf you aren’t working in a notebook, just type the following line in your terminal:\n\nAll that’s left to do is configure the training arguments and fire up the . We’ll use a cosine learning rate schedule with some warmup and an effective batch size of 256 ( * ). Gradient accumulation is used when a single batch does not fit into memory, and incrementally builds up the gradient through several forward/backward passes. We’ll see this in action when we create the training loop with 🤗 Accelerate.\n\nNow we can just start the and wait for training to finish. Depending on whether you run it on the full or a subset of the training set this will take 20 or 2 hours, respectively, so grab a few coffees and a good book to read!\n\nAfter training completes, we can push the model and tokenizer to the Hub:\n\nNow is the moment of truth: let’s see how well the trained model actually works! We can see in the logs that the loss went down steadily, but to put the model to the test let’s take a look at how well it works on some prompts. To do that we’ll wrap the model in a text generation , and we’ll put it on the GPU for fast generations if there is one available:\n\nLet’s start with the simple task of creating a scatter plot:\n\nThe result looks correct. Does it also work for a operation? Let’s see if we can create a from two arrays:\n\nNice, that’s the correct answer — although it then inserts the column again. Since the number of generated tokens is limited, the following loop is cut off. Let’s see if we can do something a bit more complex and have the model help us use the operation:\n\nNot bad; that’s the right way to do it. Finally, let’s see if we can also use it for and set up a Random Forest model:\n\nLooking at these few examples, it seems that the model has learned some of the syntax of the Python data science stack (of course, we would need to evaluate it more thoroughly before deploying the model in the real world). Sometimes it requires more customization of the model training to achieve the necessary performance for a given use case, however. For example, what if we would like to dynamically update the batch size or have a conditional training loop that skips bad examples on the fly? One option would be to subclass the and add the necessary changes, but sometimes it’s simpler to write the training loop from scratch. That’s where 🤗 Accelerate comes in.\n\nWe’ve seen how to train a model with the , which can allow for some customization. However, sometimes we want full control over the training loop, or we want to make some exotic changes. In this case 🤗 Accelerate is a great choice, and in this section we’ll go through the steps to use it to train our model. To make things more interesting, we’ll also add a twist to the training loop.\n\nSince we are mainly interested in sensible autocompletion for the the data science libraries, it makes sense to give more weight to training samples that make more use of these libraries. We can easily identify these examples through the use of keywords such as , , , , and , which are the most frequent import names for , , and as well as the fit/predict pattern of the latter. If these are each represented as a single token, we can easily check if they occur in the input sequence. Tokens can have a whitespace prefix, so we’ll also check for those versions in the tokenizer vocabulary. To verify that it works, we’ll add one test token which should be split into multiple tokens:\n\nGreat, that seems to work nicely! We can now write a custom loss function that takes the input sequence, the logits, and the key tokens we just selected as inputs. First we need to align the logits and inputs: the input sequence shifted by one to the right forms the labels, since the next token is the label for the current token. We can achieve this by starting the labels from the second token of the input sequence, since the model does not make a prediction for the first token anyway. Then we cut off the last logit, as we don’t have a label for the token that follows the full input sequence. With that we can compute the loss per sample and count the occurrences of all keywords in each sample. Finally, we calculate the weighted average over all samples using the occurrences as weights. Since we don’t want to throw away all the samples that have no keywords, we add 1 to the weights:\n\nBefore we can start training with this awesome new loss function, we need to prepare a few things:\n• We need dataloaders to load the data in batches.\n• We need to set up weight decay parameters.\n• From time to time we want to evaluate, so it makes sense to wrap the evaluation code in a function.\n\nLet’s start with the dataloaders. We only need to set the dataset’s format to , and then we can pass it to a PyTorch with the appropriate batch size:\n\nNext, we group the parameters so that the optimizer knows which ones will get an additional weight decay. Usually, all bias and LayerNorm weights terms are exempt from this; here’s how we can do this:\n\nSince we want to evaluate the model regularly on the validation set during training, let’s write a function for that as well. It just runs through the evaluation dataloader and gathers all the losses across processes:\n\nWith the function we can report loss and perplexity at regular intervals. Next, we redefine our model to make sure we train from scratch again:\n\nWe can then define our optimizer, using the function from before to split the parameters for weight decay:\n\nNow let’s prepare the model, optimizer, and dataloaders so we can start training:\n\nNow that we have sent our to , we can use its length to compute the number of training steps. Remember that we should always do this after preparing the dataloader, as that method will change its length. We use a classic linear schedule from the learning rate to 0:\n\nLastly, to push our model to the Hub, we will need to create a object in a working folder. First log in to the Hugging Face Hub, if you aren’t logged in already. We’ll determine the repository name from the model ID we want to give our model (feel free to replace the with your own choice; it just needs to contain your username, which is what the function does):\n\nThen we can clone that repository in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with:\n\nWe can now upload anything we save in by calling the method. This will help us upload the intermediate models at the end of each epoch.\n\nBefore we train, let’s run a quick test to see if the evaluation function works properly:\n\nThose are very high values for loss and perplexity, but that’s not surprising as we haven’t trained the model yet. With that, we have everything prepared to write the core part of the training script: the training loop. In the training loop we iterate over the dataloader and pass the batches to the model. With the logits, we can then evaluate our custom loss function. We scale the loss by the number of gradient accumulation steps so as not to create larger losses when aggregating more steps. Before we optimize, we also clip the gradients for better convergence. Finally, every few steps we evaluate the model on the evaluation set with our new function:\n\nAnd that’s it — you now have your own custom training loop for causal language models such as GPT-2 that you can further customize to your needs."
    }
]