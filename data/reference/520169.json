[
    {
        "link": "https://numpy.org/doc/2.1/reference/generated/numpy.frombuffer.html",
        "document": "An object that exposes the buffer interface. Number of items to read. means all data in the buffer. Start reading the buffer from this offset (in bytes); default: 0. Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as supports the protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.\n\nIf the buffer has data that is not in machine byte-order, this should be specified as part of the data-type, e.g.:\n\nThe data of the resulting array will not be byteswapped, but will be interpreted correctly.\n\nThis function creates a view into the original object. This should be safe in general, but it may make sense to copy the result when the original object is mutable or untrusted."
    },
    {
        "link": "https://stackoverflow.com/questions/52165652/reading-a-structured-binary-file-with-numpy-fromfile-vs-read-frombuffer",
        "document": "I’m reading a binary file using and wondering whether I should use repeated calls to or reading from the file manually and calling :\n\nIs there a difference (performance or otherwise) between these two methods?"
    },
    {
        "link": "https://docs.vultr.com/python/third-party/numpy/frombuffer",
        "document": "The function in NumPy is a powerful tool for converting data that resides in a buffer, like bytes or a byte-like object, into a NumPy array. This is especially useful when dealing with binary data stored in files or received from network connections where performance is critical. This function allows for a direct read of the buffer’s contents into a NumPy array structure, avoiding the overhead of copying data and thereby enhancing the efficiency of memory usage.\n\nIn this article, you will learn how to utilize the function to convert various types of buffers into NumPy arrays. Explore how this function works with different data types and how to specify data types to correctly interpret the buffer's contents.\n• None Use the function to convert this buffer to a NumPy array. This code converts a buffer of bytes to a NumPy array of unsigned 8-bit integers. The parameter tells how to interpret each byte in the buffer.\n• None Specify the byte order in the data type during conversion. The provided buffer is interpreted as a 32-bit unsigned integer array assuming the system's default byte order. If necessary, you can explicitly set the byte order (little-endian or big-endian) in the parameter.\n\nFrom Strings and Other Buffers\n• Encoding the string as UTF-8 converts it into a byte buffer, which then turns into an array of 8-bit unsigned integers.\n• A memoryview is an intermediate step that allows you to handle the buffer without copying it. can take this memoryview directly and create a NumPy array from it.\n\nUsing the function from NumPy efficiently converts data stored in buffers into NumPy arrays, facilitating rapid data processing and manipulation. Whether working with raw binary files, processing strings for machine learning inputs, or interfacing with low-level network protocols, ensures you have the tools to efficiently move data into the powerful computational environment provided by NumPy. Understand and exploit these techniques to enhance the performance and efficiency of your data-driven Python applications."
    },
    {
        "link": "https://stackoverflow.com/questions/77848941/how-to-use-numpy-frombuffer-to-read-a-file-sent-using-fastapi",
        "document": "is to read raw, \"binary\" data. So if you are trying to read float64, for examples, it just read packets of 64 bits (as the internal representation of float64) and fills a numpy array of float64 with it.\n\nEtc. I could continue with examples of float32 etc. But that would be longer to detail, and useless, since understanding frombuffer is not really what you want. Point is, it is not what you think it is. In practice, is for reading numpy array from memory that was produced by a previously ( ). Or by some equivalent code for other libraries or language (for example if a C code the content of a array, then you could get the back into a numpy array with .\n\nBut of course, you can use only if you have a consistent number of bytes. So, for a , the number of bits have to be a multiple of 16, so the number of bytes has to be a multiple of 2. And, in your case, for a , the number of bits has to be a multiple of 64, so number of bytes a multiple of 8. Which is not the case. Which is lucky for you. Because if your data happened to contain a multiple of 8 bytes (it has a 12.5% probability to happen), it would have worked without error, and you would have some hard time understanding why, with no error message at all, you end up with a numpy array containing numbers that are not the good ones. (Just had 3 spaces at the end of your file...)\n\nWhat to do then\n\nThe bytes you are trying to parse are obviously in ascii format, containing decimal representation of real numbers, separated by tabulations ( ) and line feed ( ). Sometimes called format.\n\nSo what you need is a function that read and parse tsv format. Those are not \"ready to use\" bytes representing numbers (it is a human readable format).\n\ndoes just that.\n\n Its normal usage is to open files. But it can also parse directly data, as long as you feed it with array (or generators) of lines. So, your is a bytestring containing lines (each of them containing numbers separated by tabs) separated by line feed. Just split it with separator, to get an array of lines, and give that array to\n\nis what you want"
    },
    {
        "link": "https://towardsdatascience.com/loading-binary-data-to-numpy-pandas-9caa03eb0672",
        "document": "In the real world, Data doesn’t always come packaged in tidy, easy-to-load files. Sometimes your data is going to live in obscure binary or irregularly structured text formats and will arrive at your doorstep without any efficient Python-based loaders. For modest amounts of data, it’s usually easy to put together a custom loader using simple native Python. But for larger data, pure Python solutions can become unacceptably slow, and at that point, it’s time to invest in building something faster.\n\nIn this article, I’ll show you how to use a combination of built-in functions, the C-API, and Cython to quickly and easily put together your own super-fast custom data loader for NumPy/Pandas. First, we’ll review a common structure that’s often used for storing binary data, and then write code to load some sample data. Along the way, we’ll take brief detours into the C-API and the Python buffer protocol so that you understand how all the pieces work. There‘s a lot here, but don’t worry – it’s all very straightforward and we’ll make sure that the most important parts of the code are generic and reusable. You can also follow along with a working notebook here. When we’re done, you’ll be able to easily adapt the code to your specific data format and get back to analysis!\n\nFor our purposes, a binary data file is nothing more than a large array of bytes that encodes a series of data elements such as integers, floats, or character arrays. While there are many formats for the binary encoding, one common format consists of a series of individual ‘records’ stored back-to-back one after another. Within each record, the first bytes typically encode a header which specifies the length (in bytes) of the record, as well as other identifying information that allows the user to decode the data.\n\nGenerally, there will be multiple record types in the file, all of which share a common header format. For example, binary data from a car’s computer might have one record type for driver controls such as the brake pedal and steering wheel positions, and another type to record engine statistics such as fuel consumption and temperature.\n\nIn order to load binary data, you need to refer to documentation for your binary format to know exactly how the bytes encode data. For the purposes of demonstration, we’ll work with sample data laid out like this:\n\nIn the next section, we’ll see how to deal with the simple case where the data contains only a single record type\n\nSay we have some data with the record layout given above where all records have an identical 9-byte message body:\n\nWe’ll first load our data to a Numpy array and with that done, it’s just a one liner to create a Pandas DataFrame.\n\nThe only tricky part here is that NumPy arrays can only hold data of a single type, while our data has both integers and character arrays. Fortunately, numpy lets us define structured types with multiple subcomponents. So what we do is construct a NumPy dtype which has the same structure as our binary records. If you want to read the NumPy dtype docs you can do that here, but specifying the dtype is really pretty simple. Here’s a dtype which matches the format for our sample binary data:\n\nWith our dtype defined, we can go ahead and load the data with just a few lines:\n\nAnd that’s it! Couldn’t be easier, right? One little thing to take care of is that the name column in our data is holding objects of type . We’d probably rather have strings, so let’s use the Series.str.decode() method to do the conversion from to objects:\n\nIn the snippets above, we first loaded our binary file to a bytes array and then created a NumPy array with the function . Alternatively you can combine these two steps by using the function , but it’s sometimes useful to manually dig into your binary data and poke around. If you need a quick introduction or refresher on how to manipulate and view byte data in Python, have a look at this notebook which I set up as a quick tutorial reference for this article.\n\nWhat about binary data with multiple record types?\n\nLoading data as we did above was super easy, but unfortunately binary data is usually not structured so nicely. Typically there are many different record types all mixed together in a single file, and we need a way to load these into one or more DataFrames.\n\nThe challenge here is that NumPy only knows how to load binary data that is stored in a ‘simple’ format, where the data exists in a contiguous block of memory consisting of identical records stacked back-to-back. In the example above, our data had only a single fixed-length record type, and that made it very easy to load.\n\nIn general, in order to load binary data to NumPy we’ll need to split it into one or more homogeneous arrays as shown below:\n\nOne way to do the split above is to write some pre-processing code (pick any language you want) to split the binary data into one or more files. If you go that route then you can simply do your pre-processing and then load the individual files like we did above. The downside to this approach is that the pre-processing will create multiple copies of your data on disk, which isn’t very elegant and could potentially be a hassle.\n\nSo instead of writing out separate files, we’ll show how to set up memory arrays in Cython, one for each record type that we’re interested in, and efficiently fill them with our binary records. We’ll then expose these arrays to NumPy by using the buffer protocol from the Python C-API. We could do all of this in native Python, but we’ll use Cython because we want our solution to be fast (binary files are sometimes quite large).\n\nThe Python C-API and the Buffer Protocol\n\nThe Python C-API is the doorway into a lower-level implementation of Python. It allows programmers to extend Python with code written in C/C++, and also lets you embed Python into other programming languages. We won’t need to know much about the C-API, though. All we need is a high level understanding of the buffer protocol.\n\nThe buffer protocol operates at the C-API level and defines a way that Python objects can access and share each others memory. When we call on an object that implements the buffer protocol, NumPy goes down into the C-API and asks the object for a view of its internal memory. If successful, NumPy goes on to set up an array using the shared data. Note that there is no copying going on here! After the call to , both the original buffer object and the NumPy array are sharing the same underlying memory. A simplified version of the process looks something like this:\n\nRather than use the C-API directly, we’re going to interact with the C-API via Cython because it’s a lot easier than writing code directly in C/C++. As you’ll see, it’s also very easy to implement the buffer protocol from Cython.\n\nCython is an extension to Python which is a combination of Python and C/C++. Code compiled from Cython often runs much faster than native Python and gives you the ability to use functions and classes from C/C++ libraries. We won’t give an introduction to Cython in this article, but t[here](https://towardsdatascience.com/speed-up-your-python-code-with-cython-8879105f2b6f) are a number of introductory tutorials – for example here and here.\n\nImplementing the buffer protocol from Cython is fortunately very easy. All we need to do is implement the two methods getbuffer and releasebuffer. Behind the scenes, Cython has some special handling of these so that they get correctly tied to our object in the C-API, but we don’t need to worry about that. All we need to do is implement the two methods and they’re both pretty simple in our case. Here’s what they do:\n\n*getbuffer(self, Py_buffer , int)** This method will be called by any consumer object that wants a view of our memory. It has two arguments: an integer of bit flags, and a pointer to an object of type Py_buffer, which is a simple C struct containing fields which we need to fill in. The flags indicate details about the data format that the consumer is expecting. In our case, we’ll support just the simplest type, which is one-dimensional data stored in a contiguous block of memory. So all we have to do in getbuffer is check that the flags indicate a simple buffer, and then fill in a few self-explanatory fields in the Py_buffer struct (see code below).\n\n*releasebuffer(self, Py_buffer )** The purpose of releasebuffer is to allow reference counting so that our code knows when it can release and/or reallocate memory in the Py_buffer structure. However, NumPy doesn’t respect this, and expects that buffers maintain their data even after calls to releasebuffer. So in our case, we don’t actually have to do anything here.\n\nThe easiest way to use Cython from a Jupyter notebook is to first load Cython as shown below. You may need to pip install Cython first. As always, consider using a virtual environment.\n\nNext, you enter Cython code in a separate cell starting with the IPython magic Here we’re defining a class , which implements the buffer protocol and can also be used from Python. This class is a generic reusable container that simply holds binary data and allows access via the buffer protocol so that NumPy can share the data.\n\nWith our shiny new class we can redo our previous example like this:\n\nIf you’ve made it this far, congratulations! All the hard work is done. We’ve learned how to load structured binary data to NumPy and also used Cython to create a container for data that can be efficiently accessed via .\n\nAs our final task, we’ll use Cython to build a fast data-parsing function which is specialized to our binary data format. The function takes our input binary data as a byte array and two additional objects. It uses some simple C pointer arithmetic to step through our binary file and fans out the records to one or the other of the objects depending on the value of msg_type. If there are any records in the file with msg_type not equal to 1 or 2, these will be skipped. Note that we’ve also repeated the SimplestBuffer definition in this cell so that Cython can find it.\n\nFor this next example, I’ve set up some sample binary data that contains the same records we loaded before, plus some new records which use the same header but have a message body consisting of four 32-bit integers. To see this new code in action you can do something like this:\n\nSo that’s progress!!! At this point, we’ve successfully loaded a binary file containing mixed record types into two DataFrames, one for each record type.\n\nThe code above is a complete working example, but there are a few improvements that are probably a good idea.\n\nFirst, we should improve the memory safety of SimplestBuffer so that the underlying memory can’t get reallocated while NumPy or Pandas is sharing the memory.\n\nSecondly, we should allow for preallocation of memory on the buffer and for the ability to read bytes directly from a file. Note however that the C++ vector is already reasonably efficient about reallocating memory, and with regard to reading from files, it’s often faster to read all the binary data into an intermediate buffer before processing rather than making many small reads on the file system. Nevertheless, both of these features are easy to implement and can lead to speedups.\n\nAnd finally, it’s often useful to generate loadable modules from Cython rather than putting all of the Cython into Jupyter notebooks.\n\nIn order to save space we won’t show code for these improvements here, but have a look at the notebook referenced earlier, which has complete code for all of the examples and extensions above.\n\nI hope you’ve found this notebook useful and that it helps you to load your binary data and get back to analysis!!! But before wrapping up, I want to add just a few more remarks.\n\nEvaluation Speed: We didn’t do any benchmarking here, but in my tests, I’ve found that loading binary data using the above methods is about as fast as loading equivalent DataFrames from pickled binaries, and sometimes it’s even faster! One area that is not fast, however, is the conversion of byte arrays to strings using . In my experience, this conversion is often the slowest part of loading binary data. So you might want to consider just leaving some or all of your character data as byte arrays rather than converting to native string objects.\n\nVariable Record Lengths: In the examples here, our record types all had fixed lengths. But in the wild, binary records often have variable lengths, due either to the presence of variable-length character arrays, or repeating groups within the record. In order to handle records of this type, you’ll have to truncate the character arrays to some fixed length and find a way to deal with any repeating groups. The general tools above are all you really need, so just be aware that this is something you may have to deal with and you’ll have no problems coming up with a solution that works for you in your situation.\n\nIrregularly Structured text data: In this article, we focused on binary data, but I just want to note that if you have large quantities of irregularly structured text data, you can use the same techniques demonstrated here to efficiently process and load your data. Again, just figure out a final structure that works as a DataFrame, and then write some Cython to parse your text file into one or more buffers as we did above.\n\nThanks for reading and please let me know if you have any comments or suggestions."
    },
    {
        "link": "https://docs.python.org/3/library/struct.html",
        "document": "This module converts between Python values and C structs represented as Python objects. Compact format strings describe the intended conversions to/from Python values. The module’s functions and objects can be used for two largely distinct applications, data exchange with external sources (files or network connections), or data transfer between the Python application and the C layer.\n\nSeveral functions (and methods of ) take a buffer argument. This refers to objects that implement the Buffer Protocol and provide either a readable or read-writable buffer. The most common types used for that purpose are and , but many other types that can be viewed as an array of bytes implement the buffer protocol, so that they can be read/filled without additional copying from a object.\n\nThe module defines the following exception and functions: Exception raised on various occasions; argument is a string describing what is wrong. Return a bytes object containing the values v1, v2, … packed according to the format string format. The arguments must match the values required by the format exactly. Pack the values v1, v2, … according to the format string format and write the packed bytes into the writable buffer buffer starting at position offset. Note that offset is a required argument. Unpack from the buffer buffer (presumably packed by ) according to the format string format. The result is a tuple even if it contains exactly one item. The buffer’s size in bytes must match the size required by the format, as reflected by . Unpack from buffer starting at position offset, according to the format string format. The result is a tuple even if it contains exactly one item. The buffer’s size in bytes, starting at position offset, must be at least the size required by the format, as reflected by . Iteratively unpack from the buffer buffer according to the format string format. This function returns an iterator which will read equally sized chunks from the buffer until all its contents have been consumed. The buffer’s size in bytes must be a multiple of the size required by the format, as reflected by . Each iteration yields a tuple as specified by the format string. Return the size of the struct (and hence of the bytes object produced by ) corresponding to the format string format.\n\nFormat strings describe the data layout when packing and unpacking data. They are built up from format characters, which specify the type of data being packed/unpacked. In addition, special characters control the byte order, size and alignment. Each format string consists of an optional prefix character which describes the overall properties of the data and one or more format characters which describe the actual data values and padding. By default, C types are represented in the machine’s native format and byte order, and properly aligned by skipping pad bytes if necessary (according to the rules used by the C compiler). This behavior is chosen so that the bytes of a packed struct correspond exactly to the memory layout of the corresponding C struct. Whether to use native byte ordering and padding or standard formats depends on the application. Alternatively, the first character of the format string can be used to indicate the byte order, size and alignment of the packed data, according to the following table: If the first character is not one of these, is assumed. The number 1023 ( in hexadecimal) has the following byte representations: Native byte order is big-endian or little-endian, depending on the host system. For example, Intel x86, AMD64 (x86-64), and Apple M1 are little-endian; IBM z and many legacy architectures are big-endian. Use to check the endianness of your system. Native size and alignment are determined using the C compiler’s expression. This is always combined with native byte order. Standard size depends only on the format character; see the table in the Format Characters section. Note the difference between and : both use native byte order, but the size and alignment of the latter is standardized. The form represents the network byte order which is always big-endian as defined in IETF RFC 1700. There is no way to indicate non-native byte order (force byte-swapping); use the appropriate choice of or .\n• None Padding is only automatically added between successive structure members. No padding is added at the beginning or the end of the encoded struct.\n• None No padding is added when using non-native size and alignment, e.g. with ‘<’, ‘>’, ‘=’, and ‘!’.\n• None To align the end of a structure to the alignment requirement of a particular type, end the format with the code for that type with a repeat count of zero. See Examples. Format characters have the following meaning; the conversion between C and Python values should be obvious given their types. The ‘Standard size’ column refers to the size of the packed value in bytes when using standard size; that is, when the format string starts with one of , , or . When using native size, the size of the packed value is platform-dependent. Changed in version 3.3: Added support for the and formats. Changed in version 3.6: Added support for the format.\n• None The conversion code corresponds to the _Bool type defined by C standards since C99. In standard mode, it is represented by one byte.\n• None When attempting to pack a non-integer using any of the integer conversion codes, if the non-integer has a method then that method is called to convert the argument to an integer before packing. Changed in version 3.2: Added use of the method for non-integers.\n• None The and conversion codes are only available for the native size (selected as the default or with the byte order character). For the standard size, you can use whichever of the other integer formats fits your application.\n• None For the , and conversion codes, the packed representation uses the IEEE 754 binary32, binary64 or binary16 format (for , or respectively), regardless of the floating-point format used by the platform.\n• None The format character is only available for the native byte ordering (selected as the default or with the byte order character). The byte order character chooses to use little- or big-endian ordering based on the host system. The struct module does not interpret this as native ordering, so the format is not available.\n• None The IEEE 754 binary16 “half precision” type was introduced in the 2008 revision of the IEEE 754 standard. It has a sign bit, a 5-bit exponent and 11-bit precision (with 10 bits explicitly stored), and can represent numbers between approximately and at full precision. This type is not widely supported by C compilers: on a typical machine, an unsigned short can be used for storage, but not for math operations. See the Wikipedia page on the half-precision floating-point format for more information.\n• None The format character encodes a “Pascal string”, meaning a short variable-length string stored in a fixed number of bytes, given by the count. The first byte stored is the length of the string, or 255, whichever is smaller. The bytes of the string follow. If the string passed in to is too long (longer than the count minus 1), only the leading bytes of the string are stored. If the string is shorter than , it is padded with null bytes so that exactly count bytes in all are used. Note that for , the format character consumes bytes, but that the string returned can never contain more than 255 bytes.\n• None For the format character, the count is interpreted as the length of the bytes, not a repeat count like for the other format characters; for example, means a single 10-byte string mapping to or from a single Python byte string, while means 10 separate one byte character elements (e.g., ) mapping to or from ten different Python byte objects. (See Examples for a concrete demonstration of the difference.) If a count is not given, it defaults to 1. For packing, the string is truncated or padded with null bytes as appropriate to make it fit. For unpacking, the resulting bytes object always has exactly the specified number of bytes. As a special case, means a single, empty string (while means 0 characters). A format character may be preceded by an integral repeat count. For example, the format string means exactly the same as . Whitespace characters between formats are ignored; a count and its format must not contain whitespace though. When packing a value using one of the integer formats ( , , , , , , , , , ), if is outside the valid range for that format then is raised. Changed in version 3.1: Previously, some of the integer formats wrapped out-of-range values and raised instead of . For the format character, the return value is either or . When packing, the truth value of the argument object is used. Either 0 or 1 in the native or standard bool representation will be packed, and any non-zero value will be when unpacking. Native byte order examples (designated by the format prefix or lack of any prefix character) may not match what the reader’s machine produces as that depends on the platform and compiler. Pack and unpack integers of three different sizes, using big endian ordering: Attempt to pack an integer which is too large for the defined field: Demonstrate the difference between and format characters: Unpacked fields can be named by assigning them to variables or by wrapping the result in a named tuple: The ordering of format characters may have an impact on size in native mode since padding is implicit. In standard mode, the user is responsible for inserting any desired padding. Note in the first call below that three NUL bytes were added after the packed to align the following integer on a four-byte boundary. In this example, the output was produced on a little endian machine: The following format results in two pad bytes being added at the end, assuming the platform’s longs are aligned on 4-byte boundaries:\n\nTwo main applications for the module exist, data interchange between Python and C code within an application or another application compiled using the same compiler (native formats), and data interchange between applications using agreed upon data layout (standard formats). Generally speaking, the format strings constructed for these two domains are distinct. When constructing format strings which mimic native layouts, the compiler and machine architecture determine byte ordering and padding. In such cases, the format character should be used to specify native byte ordering and data sizes. Internal pad bytes are normally inserted automatically. It is possible that a zero-repeat format code will be needed at the end of a format string to round up to the correct byte boundary for proper alignment of consecutive chunks of data. Consider these two simple examples (on a 64-bit, little-endian machine): Data is not padded to an 8-byte boundary at the end of the second format string without the use of extra padding. A zero-repeat format code solves that problem: The format code can be used to specify the repeat, but for native formats it is better to use a zero-repeat format like . By default, native byte ordering and alignment is used, but it is better to be explicit and use the prefix character. When exchanging data beyond your process such as networking or storage, be precise. Specify the exact byte order, size, and alignment. Do not assume they match the native order of a particular machine. For example, network byte order is big-endian, while many popular CPUs are little-endian. By defining this explicitly, the user need not care about the specifics of the platform their code is running on. The first character should typically be or (or ). Padding is the responsibility of the programmer. The zero-repeat format character won’t work. Instead, the user must explicitly add pad bytes where needed. Revisiting the examples from the previous section, we have: The above results (executed on a 64-bit machine) aren’t guaranteed to match when executed on different machines. For example, the examples below were executed on a 32-bit machine:"
    },
    {
        "link": "https://digitalocean.com/community/tutorials/python-struct-pack-unpack",
        "document": "Python struct module is capable of performing the conversions between the Python values and C structs, which are represented as Python Strings.\n• Python struct module can be used in handling binary data stored in files, database or from network connections etc.\n• It uses format Strings as compact descriptions of the layout of the C structs and the intended conversion to/from Python values.\n\nThere are five important functions in struct module - , , , and . In all these functions, we have to provide the format of the data to be converted into binary. Some of the popular format characters are:\n\nYou can get the complete list of format characters here. Let’s start looking into struct module functions one by one.\n\nThis function packs a list of values into a String representation of the specified type. The arguments must match the values required by the format exactly. Let’s quickly look at struct pack() example:\n\nWhen we run this script, we get the following representation: Note that ‘b’ in the Output stands for binary.\n\nThis function unpacks the packed value into its original representation with the specified format. This function always returns a tuple, even if there is only one element. Let’s quickly look at struct unpack() function example:\n\nWhen we run this script, we get back our original representation: Clearly, we must tell the Python interpreter the format we need to unpack the values into.\n\nThis function calculates and returns the size of the String representation of struct with a given format. Size is calculated in terms of bytes. Let’s quickly look at an example code snippet:\n\nWhen we run this script, we get the following representation:\n\nThese functions allow us to pack the values into string buffer and unpack from a string buffer. These functions are introduced in version 2.5.\n\nWhen we run this script, we get the following representation: That’s all for a short introduction of python module."
    },
    {
        "link": "https://stackoverflow.com/questions/16410841/converting-data-to-binary-using-struct-in-python",
        "document": "I have the following dict which I want to write to a file in binary:\n\nI went ahead to use the struct module in this way:\n\nThe output of this is packed = ['\\xfe\\xca\\x07\\x00\\xbe\\x00\\x00', '\\x00\\x00\\x00\\x00', '\\x01\\x00e\\x00', '\\x02\\x00\\x07\\x00', '\\x03\\x00\\x00\\x00', '\\x04\\x00\\x00\\x00', '\\xfe\\xca\\x07\\x00\\xbd\\x00\\x00', '\\x00\\x00\n\n\\x00', '\\x01\\x00\\x84\\x00', '\\x02\\x00\\x11\\x00', '\\x03\\x00\\x14\\x00', '\\x04\\x00(\\x00']\n\nAfter which I write this as a binary file :\n\nHere is what the binary file looks like: '\\xfe\\xca\\x07\\x00\\xbe\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00e\\x00\\x02\\x00\\x07\\x00\\x03\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xfe\\xca\\x07\\x00\\xbd\\x00\\x00\\x00\\x00\n\n\\x00\\x01\\x00\\x84\\x00\\x02\\x00\\x11\\x00\\x03\\x00\\x14\\x00\\x04\\x00(\\x00'\n\nIt's all OK until I tried to unpack the data. Now I want to read the contents of the binary file and arrange it back to how my dict looked liked in the first place. This is how I tried to unpack it but I was always getting an error:\n\nI realize the method I tried to unpack will always run into problems, but I can't seem to find a good way in unpacking the contents of the binary file. Any help is much appreciated.."
    },
    {
        "link": "https://stackoverflow.com/questions/11421735/int-to-binary-python",
        "document": "This question is probably very easy for most of you, but i cant find the answer so far.\n\nI'm building a network packet generator that goes like this:\n\nLater, I call my class and send the packet like this:\n\nUnfortunatly, on the wire i have (257 in ) instead of having this : .\n\nWhat i would like to do, is being able to:\n\nand have it converted to int correctly on the wire"
    },
    {
        "link": "https://docs.python.org/es/3.7/library/struct.html",
        "document": "To align the end of a structure to the alignment requirement of a particular type, end the format with the code for that type with a repeat count of zero. See Examples .\n\nThere is no way to indicate non-native byte order (force byte-swapping); use the appropriate choice of '<' or '>' .\n\nNote the difference between '@' and '=' : both use native byte order, but the size and alignment of the latter is standardized.\n\nFormat characters have the following meaning; the conversion between C and Python values should be obvious given their types. The “Standard size” column refers to the size of the packed value in bytes when using standard size; that is, when the format string starts with one of , , or . When using native size, the size of the packed value is platform-dependent.\n• None When attempting to pack a non-integer using any of the integer conversion codes, if the non-integer has a method then that method is called to convert the argument to an integer before packing.\n• None The IEEE 754 binary16 «half precision» type was introduced in the 2008 revision of the IEEE 754 standard. It has a sign bit, a 5-bit exponent and 11-bit precision (with 10 bits explicitly stored), and can represent numbers between approximately and at full precision. This type is not widely supported by C compilers: on a typical machine, an unsigned short can be used for storage, but not for math operations. See the Wikipedia page on the half-precision floating-point format for more information.\n\nFor the format character, the count is interpreted as the length of the bytes, not a repeat count like for the other format characters; for example, means a single 10-byte string, while means 10 characters. If a count is not given, it defaults to 1. For packing, the string is truncated or padded with null bytes as appropriate to make it fit. For unpacking, the resulting bytes object always has exactly the specified number of bytes. As a special case, means a single, empty string (while means 0 characters).\n\nWhen packing a value using one of the integer formats ( , , , , , , , , , ), if is outside the valid range for that format then is raised.\n\nThe format character encodes a «Pascal string», meaning a short variable-length string stored in a fixed number of bytes, given by the count. The first byte stored is the length of the string, or 255, whichever is smaller. The bytes of the string follow. If the string passed in to is too long (longer than the count minus 1), only the leading bytes of the string are stored. If the string is shorter than , it is padded with null bytes so that exactly count bytes in all are used. Note that for , the format character consumes bytes, but that the string returned can never contain more than 255 bytes."
    }
]