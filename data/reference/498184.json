[
    {
        "link": "https://requests.readthedocs.io",
        "document": "Requests is an elegant and simple HTTP library for Python, built for human beings.\n\nRequests allows you to send HTTP/1.1 requests extremely easily. There’s no need to manually add query strings to your URLs, or to form-encode your POST data. Keep-alive and HTTP connection pooling are 100% automatic, thanks to urllib3."
    },
    {
        "link": "https://stackoverflow.com/questions/75435585/using-python-3-9-1-and-requests-2-25-1-a-local-connection-to-a-mongoose-http-ser",
        "document": "I am writing a little REST API client using Python, Java and NodeJS. The server is written using the Mongoose HTTP server.\n\nWith Java and NodeJS every request takes only milliseconds but with Python every request takes 2 seonds.\n\nI confirmed that this is not a requests problem by using urllib directly. This also takes 2 seconds per request.\n\nI also tried \"Connection\" \"Close\", no change...\n\nAny ideas why the request takes 2 seconds with Python but not with Java and NodeJS ?"
    },
    {
        "link": "https://requests.readthedocs.io/en/master/user/quickstart",
        "document": "Eager to get started? This page gives a good introduction in how to get started with Requests.\n\nFirst, make sure that:\n\nLet’s get started with some simple examples.\n\nMaking a request with Requests is very simple. Now, let’s try to get a webpage. For this example, let’s get GitHub’s public timeline: Now, we have a object called . We can get all the information we need from this object. Requests’ simple API means that all forms of HTTP request are as obvious. For example, this is how you make an HTTP POST request: Nice, right? What about the other HTTP request types: PUT, DELETE, HEAD and OPTIONS? These are all just as simple: That’s all well and good, but it’s also only the start of what Requests can do.\n\nYou often want to send some sort of data in the URL’s query string. If you were constructing the URL by hand, this data would be given as key/value pairs in the URL after a question mark, e.g. . Requests allows you to provide these arguments as a dictionary of strings, using the keyword argument. As an example, if you wanted to pass and to , you would use the following code: You can see that the URL has been correctly encoded by printing the URL: Note that any dictionary key whose value is will not be added to the URL’s query string. You can also pass a list of items as a value:\n\nWe can read the content of the server’s response. Consider the GitHub timeline again: Requests will automatically decode content from the server. Most unicode charsets are seamlessly decoded. When you make a request, Requests makes educated guesses about the encoding of the response based on the HTTP headers. The text encoding guessed by Requests is used when you access . You can find out what encoding Requests is using, and change it, using the property: If you change the encoding, Requests will use the new value of whenever you call . You might want to do this in any situation where you can apply special logic to work out what the encoding of the content will be. For example, HTML and XML have the ability to specify their encoding in their body. In situations like this, you should use to find the encoding, and then set . This will let you use with the correct encoding. Requests will also use custom encodings in the event that you need them. If you have created your own encoding and registered it with the module, you can simply use the codec name as the value of and Requests will handle the decoding for you.\n\nThere’s also a builtin JSON decoder, in case you’re dealing with JSON data: In case the JSON decoding fails, raises an exception. For example, if the response gets a 204 (No Content), or if the response contains invalid JSON, attempting raises . This wrapper exception provides interoperability for multiple exceptions that may be thrown by different python versions and json serialization libraries. It should be noted that the success of the call to does not indicate the success of the response. Some servers may return a JSON object in a failed response (e.g. error details with HTTP 500). Such JSON will be decoded and returned. To check that a request is successful, use or check is what you expect.\n\nIn the rare case that you’d like to get the raw socket response from the server, you can access . If you want to do this, make sure you set in your initial request. Once you do, you can do this: In general, however, you should use a pattern like this to save what is being streamed to a file: Using will handle a lot of what you would otherwise have to handle when using directly. When streaming a download, the above is the preferred and recommended way to retrieve the content. Note that can be freely adjusted to a number that may better fit your use cases. An important note about using versus . will automatically decode the and transfer-encodings. is a raw stream of bytes – it does not transform the response content. If you really need access to the bytes as they were returned, use .\n\nIf you’d like to add HTTP headers to a request, simply pass in a to the parameter. For example, we didn’t specify our user-agent in the previous example: Note: Custom headers are given less precedence than more specific sources of information. For instance:\n• None Authorization headers set with will be overridden if credentials are specified in , which in turn will be overridden by the parameter. Requests will search for the netrc file at , , or at the path specified by the environment variable.\n• None Authorization headers will be removed if you get redirected off-host.\n• None Proxy-Authorization headers will be overridden by proxy credentials provided in the URL.\n• None Content-Length headers will be overridden when we can determine the length of the content. Furthermore, Requests does not change its behavior at all based on which custom headers are specified. The headers are simply passed on into the final request. Note: All header values must be a , bytestring, or unicode. While permitted, it’s advised to avoid passing unicode header values.\n\nTypically, you want to send some form-encoded data — much like an HTML form. To do this, simply pass a dictionary to the argument. Your dictionary of data will automatically be form-encoded when the request is made: The argument can also have multiple values for each key. This can be done by making either a list of tuples or a dictionary with lists as values. This is particularly useful when the form has multiple elements that use the same key: There are times that you may want to send data that is not form-encoded. If you pass in a instead of a , that data will be posted directly. For example, the GitHub API v3 accepts JSON-Encoded POST/PATCH data: Please note that the above code will NOT add the header (so in particular it will NOT set it to ). If you need that header set and you don’t want to encode the yourself, you can also pass it directly using the parameter (added in version 2.4.2) and it will be encoded automatically: Note, the parameter is ignored if either or is passed.\n\nYou can set the filename, content_type and headers explicitly: If you want, you can send strings to be received as files: In the event you are posting a very large file as a request, you may want to stream the request. By default, does not support this, but there is a separate package which does - . You should read the toolbelt’s documentation for more details about how to use it. For sending multiple files in one request refer to the advanced section. It is strongly recommended that you open files in binary mode. This is because Requests may attempt to provide the header for you, and if it does this value will be set to the number of bytes in the file. Errors may occur if you open the file in text mode.\n\nWe can view the server’s response headers using a Python dictionary: The dictionary is special, though: it’s made just for HTTP headers. According to RFC 7230, HTTP Header names are case-insensitive. So, we can access the headers using any capitalization we want: It is also special in that the server could have sent the same header multiple times with different values, but requests combines them so they can be represented in the dictionary within a single mapping, as per RFC 7230: A recipient MAY combine multiple header fields with the same field name into one “field-name: field-value” pair, without changing the semantics of the message, by appending each subsequent field value to the combined field value in order, separated by a comma.\n\nYou can tell Requests to stop waiting for a response after a given number of seconds with the parameter. Nearly all production code should use this parameter in nearly all requests. Failure to do so can cause your program to hang indefinitely: is not a time limit on the entire response download; rather, an exception is raised if the server has not issued a response for seconds (more precisely, if no bytes have been received on the underlying socket for seconds). If no timeout is specified explicitly, requests do not time out.\n\nIn the event of a network problem (e.g. DNS failure, refused connection, etc), Requests will raise a exception. will raise an if the HTTP request returned an unsuccessful status code. If a request times out, a exception is raised. If a request exceeds the configured number of maximum redirections, a exception is raised. All exceptions that Requests explicitly raises inherit from . Ready for more? Check out the advanced section."
    },
    {
        "link": "https://pypi.org/project/requests/2.5.1",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://pypi.org/project/requests",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://networktocode.com/blog/how-to-write-better-python-tests-for-network-programming",
        "document": "In this blog post, I will share with you a simple technique that helped me a lot in writing better, testable code: writing tests in parallel with developing the code.\n\nWhy Is It Worth Having Tests?\n\nHave you heard any of these?\n• Writing tests slows down development. I will write tests when the code is ready.\n• It may still change; if I write tests now I will have to rewrite them, so I will write tests when the code is ready.\n\nI have heard it countless times, and also said it myself. Today I think it is one of the most common mistakes to leave tests for later. It usually means that tests are not as good as they could be or there are no tests at all due to other priorities. Furthermore, if you expect your code to change that is actually a good argument to have tests. When you expect changes, you know that you will eventually have to retest. Perhaps you will have to amend your tests, but when some of your tests fail after the change, you get the extra verification that it’s only related to the change. Lack of decent tests results in technical debt. And like any debt, sooner or later you will have to pay it off. It usually happens when you go back to your code after a while to change/fix something, and all that time you could spend writing tests you will probably spend on manually retesting your code after changing or fixing something. If you still remember how you tested it before, this may be manageable; if not, you will spend even more time on it. You can even skip testing and rely on the grace of the gods that it will work well. But you may avoid all of this if you change just one thing!\n\nHow Do You Run Your Code?\n\nRight? OK, time for the pro tip!\n\nWhat if you avoid running code directly and run it with tests instead?\n\nWhen developing code, we write functions, classes, methods. And we run them to test whether they give us what we expect. Running your code for the first time is the right time to develop tests! All you need to do is just run your code with instead of running it directly; capture outputs which you normally check with ; and gradually build your tests as you develop your code.\n\nLet’s get our hands dirty by creating some practical examples. This is our project structure:\n\nCreate our first function in , something simple.\n\nNow we should test our function to check whether we get what we expect. But instead of running , we create a test in and we run . Remember the option, as it gives all outputs on-screen. We use in the test, but you can use it anywhere in your code. Now we just want to capture our print the same way we would by running and calling our function there.\n\nI usually use option to point to a specific test. This is convenient when you already have many tests, and you want to work on one. Let’s run tests again, limiting them to only the test we work on.\n\nOur output is , and it is indeed the sum of all the arguments we passed to our function. Now we can just replace with and we now have a test that compares the function call result with our previously captured expected result. We have our first test completed, which will remain and will be executed automatically whenever we run our tests in the future.\n\nNote option, which gives more verbose output. Let’s make one more function and test.\n\nAgain we modify to , and we add the expected result and run the test again.\n\nAs you see, the effort is comparable to typical testing with , but with a little more effort, we have unit tests that will remain after we remove print statements. This is a huge benefit for the future and for anyone else who will work with our code.\n\nLet’s develop something more practical from the networking world. We will use to get software version from a device, and we develop that through tests.\n\nLet’s run to see what we get from the device.\n\nWe need index and the key. We modify the return in our function in and run the test again.\n\nNow we can modify our test: remove and add and enter the returned value as the expected value, then we run the test again.\n\nOur test works fine, but it takes 8 sec to complete because we still connect to the real device. We need to mock up netmiko output. Under , we create class, where we overwrite netmiko method, which we use to get structured output of , and we return the same output that we collected from the device with print. Because we call ConnectHandler with context manager, we also need to implement and methods. Next we create fixture where we use pytest to patch in our module. This fixture we use as an argument in our test function. You can read more on how to mock/monkeypatch in pytest documentation.\n\nWe run the test again.\n\nThis time it took only 0.02 sec to execute the test because we used mock and did not connect to the device anymore.\n\nCheck out Netmiko Sandbox, where you can get more practice with structured command output from multiple vendor devices—all available as code, so you don’t even have to run any device! You can also easily collect command outputs for your mocks.\n\nAlso check out Adam’s awesome series of blog posts on pytest in the networking world, where Adam shares practical fundamentals of testing. Part 1Part 2Part 3 Pay attention to test parametrization and consider how we could extend our first two tests with more parameters."
    },
    {
        "link": "https://realpython.com/pytest-python-testing",
        "document": "is a popular testing framework for Python that simplifies the process of writing and executing tests. To start using , install it with in a virtual environment. offers several advantages over that ships with Python, such as less boilerplate code, more readable output, and a rich plugin ecosystem.\n\nBy the end of this tutorial, you’ll understand that:\n• Using requires installing it with in a virtual environment to set up the command.\n• allows for less code, easier readability, and more features compared to .\n• Managing test dependencies and state with is made efficient through the use of fixtures, which provide explicit dependency declarations.\n• Parametrization in helps avoid redundant test code by allowing multiple test scenarios from a single test function.\n• Assertion introspection in provides detailed information about failures in the test report.\n\nWhat Makes So Useful? If you’ve written unit tests for your Python code before, then you may have used Python’s built-in module. provides a solid base on which to build your test suite, but it has a few shortcomings. A number of third-party testing frameworks attempt to address some of the issues with , and has proven to be one of the most popular. is a feature-rich, plugin-based ecosystem for testing your Python code. If you haven’t had the pleasure of using yet, then you’re in for a treat! Its philosophy and features will make your testing experience more productive and enjoyable. With , common tasks require less code and advanced tasks can be achieved through a variety of time-saving commands and plugins. It’ll even run your existing tests out of the box, including those written with . As with most frameworks, some development patterns that make sense when you first start using can start causing pains as your test suite grows. This tutorial will help you understand some of the tools provides to keep your testing efficient and effective even as it scales.\n• Arrange, or set up, the conditions for the test\n• Act by calling some function or method\n• Assert that some end condition is true Testing frameworks typically hook into your test’s assertions so that they can provide information when an assertion fails. , for example, provides a number of helpful assertion utilities out of the box. However, even a small set of tests requires a fair amount of boilerplate code. Imagine you’d like to write a test suite just to make sure that is working properly in your project. You might want to write one test that always passes and one that always fails: You can then run those tests from the command line using the option of : As expected, one test passed and one failed. You’ve proven that is working, but look at what you had to do:\n• Write a method in for each test\n• Use one of the methods from to make assertions That’s a significant amount of code to write, and because it’s the minimum you need for any test, you’d end up writing the same code over and over. simplifies this workflow by allowing you to use normal functions and Python’s keyword directly: That’s it. You don’t have to deal with any imports or classes. All you need to do is include a function with the prefix. Because you can use the keyword, you don’t need to learn or remember all the different methods in , either. If you can write an expression that you expect to evaluate to , and then will test it for you. Not only does eliminate a lot of boilerplate, but it also provides you with a much more detailed and easy-to-read output. You can run your test suite using the command from the top-level folder of your project: presents the test results differently than , and the file was also automatically included. The report shows:\n• The system state, including which versions of Python, , and any plugins you have installed\n• The , or the directory to search under for configuration and tests\n• The number of tests the runner discovered These items are presented in the first section of the output: The output then indicates the status of each test using a syntax similar to :\n• An means that the test has failed.\n• An means that the test raised an unexpected exception. The special characters are shown next to the name with the overall progress of the test suite shown on the right: For tests that fail, the report gives a detailed breakdown of the failure. In the example, the tests failed because always fails: This extra output can come in extremely handy when debugging. Finally, the report gives an overall status report of the test suite: When compared to unittest, the output is much more informative and readable. In the next section, you’ll take a closer look at how takes advantage of the existing keyword. Being able to use the keyword is also powerful. If you’ve used it before, then there’s nothing new to learn. Here are a few assertion examples so you can get an idea of the types of test you can make: They look very much like normal Python functions. All of this makes the learning curve for shallower than it is for because you don’t need to learn new constructs to get started. Note that each test is quite small and self-contained. This is common—you’ll see long function names and not a lot going on within a function. This serves mainly to keep your tests isolated from each other, so if something breaks, you know exactly where the problem is. A nice side effect is that the labeling is much better in the output. To see an example of a project that creates a test suite along with the main project, check out the Build a Hash Table in Python With TDD tutorial. Additionally, you can work on Python practice problems to try test-driven development yourself while you get ready for your next interview or parse CSV files. In the next section, you’re going to be examining fixtures, a great pytest feature to help you manage test input values. Your tests will often depend on types of data or test doubles that mock objects your code is likely to encounter, such as dictionaries or JSON files. With , you might extract these dependencies into and methods so that each test in the class can make use of them. Using these special methods is fine, but as your test classes get larger, you may inadvertently make the test’s dependence entirely implicit. In other words, by looking at one of the many tests in isolation, you may not immediately see that it depends on something else. Over time, implicit dependencies can lead to a complex tangle of code that you have to unwind to make sense of your tests. Tests should help to make your code more understandable. If the tests themselves are difficult to understand, then you may be in trouble! takes a different approach. It leads you toward explicit dependency declarations that are still reusable thanks to the availability of fixtures. fixtures are functions that can create data, test doubles, or initialize system state for the test suite. Any test that wants to use a fixture must explicitly use this fixture function as an argument to the test function, so dependencies are always stated up front: Looking at the test function, you can immediately tell that it depends on a fixture, without needing to check the whole file for fixture definitions. Note: You usually want to put your tests into their own folder called at the root level of your project. For more information about structuring a Python application, check out the video course on that very topic. Fixtures can also make use of other fixtures, again by declaring them explicitly as dependencies. That means that, over time, your fixtures can become bulky and modular. Although the ability to insert fixtures into other fixtures provides enormous flexibility, it can also make managing dependencies more challenging as your test suite grows. Later in this tutorial, you’ll learn more about fixtures and try a few techniques for handling these challenges. As your test suite grows, you may find that you want to run just a few tests on a feature and save the full suite for later. provides a few ways of doing this:\n• Name-based filtering: You can limit to running only those tests whose fully qualified names match a particular expression. You can do this with the parameter.\n• Directory scoping: By default, will run only those tests that are in or under the current directory.\n• Test categorization: can include or exclude tests from particular categories that you define. You can do this with the parameter. Test categorization in particular is a subtly powerful tool. enables you to create marks, or custom labels, for any test you like. A test may have multiple labels, and you can use them for granular control over which tests to run. Later in this tutorial, you’ll see an example of how marks work and learn how to make use of them in a large test suite. When you’re testing functions that process data or perform generic transformations, you’ll find yourself writing many similar tests. They may differ only in the input or output of the code being tested. This requires duplicating test code, and doing so can sometimes obscure the behavior that you’re trying to test. offers a way of collecting several tests into one, but they don’t show up as individual tests in result reports. If one test fails and the rest pass, then the entire group will still return a single failing result. offers its own solution in which each test can pass or fail independently. You’ll see how to parametrize tests with later in this tutorial. One of the most beautiful features of is its openness to customization and new features. Almost every piece of the program can be cracked open and changed. As a result, users have developed a rich ecosystem of helpful plugins. Although some plugins focus on specific frameworks like Django, others are applicable to most test suites. You’ll see details on some specific plugins later in this tutorial.\n\nfixtures are a way of providing data, test doubles, or state setup to your tests. Fixtures are functions that can return a wide range of values. Each test that depends on a fixture must explicitly accept that fixture as an argument. Imagine you’re writing a function, , to process the data returned by an API endpoint. The data represents a list of people, each with a given name, family name, and job title. The function should output a list of strings that include each person’s full name (their followed by their ), a colon, and their : In good TDD fashion, you’ll want to first write a test for it. You might write the following code for that: While writing this test, it occurs to you that you may need to write another function to transform the data into comma-separated values for use in Excel: Your to-do list grows! That’s good! One of the advantages of TDD is that it helps you plan out the work ahead. The test for the function would look awfully similar to the function: Notably, both the tests have to repeat the definition of the variable, which is quite a few lines of code. If you find yourself writing several tests that all make use of the same underlying test data, then a fixture may be in your future. You can pull the repeated data into a single function decorated with to indicate that the function is a fixture: You can use the fixture by adding the function reference as an argument to your tests. Note that you don’t call the fixture function. takes care of that. You’ll be able to use the return value of the fixture function as the name of the fixture function: Each test is now notably shorter but still has a clear path back to the data it depends on. Be sure to name your fixture something specific. That way, you can quickly determine if you want to use it when writing new tests in the future! When you first discover the power of fixtures, it can be tempting to use them all the time, but as with all things, there’s a balance to be maintained. Fixtures are great for extracting data or objects that you use across multiple tests. However, they aren’t always as good for tests that require slight variations in the data. Littering your test suite with fixtures is no better than littering it with plain data or objects. It might even be worse because of the added layer of indirection. As with most abstractions, it takes some practice and thought to find the right level of fixture use. Nevertheless, fixtures will likely be an integral part of your test suite. As your project grows in scope, the challenge of scale starts to come into the picture. One of the challenges facing any kind of tool is how it handles being used at scale, and luckily, has a bunch of useful features that can help you manage the complexity that comes with growth. How to Use Fixtures at Scale As you extract more fixtures from your tests, you might see that some fixtures could benefit from further abstraction. In , fixtures are modular. Being modular means that fixtures can be imported, can import other modules, and they can depend on and import other fixtures. All this allows you to compose a suitable fixture abstraction for your use case. For example, you may find that fixtures in two separate files, or modules, share a common dependency. In this case, you can move fixtures from test modules into more general fixture-related modules. That way, you can import them back into any test modules that need them. This is a good approach when you find yourself using a fixture repeatedly throughout your project. If you want to make a fixture available for your whole project without having to import it, a special configuration module called will allow you to do that. looks for a module in each directory. If you add your general-purpose fixtures to the module, then you’ll be able to use that fixture throughout the module’s parent directory and in any subdirectories without having to import it. This is a great place to put your most widely used fixtures. Another interesting use case for fixtures and is in guarding access to resources. Imagine that you’ve written a test suite for code that deals with API calls. You want to ensure that the test suite doesn’t make any real network calls even if someone accidentally writes a test that does so. provides a fixture to replace values and behaviors, which you can use to great effect: By placing in and adding the option, you ensure that network calls will be disabled in every test across the suite. Any test that executes code calling will raise a indicating that an unexpected network call would have occurred. Your test suite is growing in numbers, which gives you a great feeling of confidence to make changes and not break things unexpectedly. That said, as your test suite grows, it might start taking a long time. Even if it doesn’t take that long, perhaps you’re focusing on some core behavior that trickles down and breaks most tests. In these cases, you might want to limit the test runner to only a certain category of tests.\n\nIn any large test suite, it would be nice to avoid running all the tests when you’re trying to iterate quickly on a new feature. Apart from the default behavior of to run all tests in the current working directory, or the filtering functionality, you can take advantage of markers. enables you to define categories for your tests and provides options for including or excluding categories when you run your suite. You can mark a test with any number of categories. Marking tests is useful for categorizing tests by subsystem or dependencies. If some of your tests require access to a database, for example, then you could create a mark for them. Pro tip: Because you can give your marks any name you want, it can be easy to mistype or misremember the name of a mark. will warn you about marks that it doesn’t recognize in the test output. You can use the flag to the command to ensure that all marks in your tests are registered in your configuration file, . It’ll prevent you from running your tests until you register any unknown marks. For more information on registering marks, check out the documentation. When the time comes to run your tests, you can still run them all by default with the command. If you’d like to run only those tests that require database access, then you can use . To run all tests except those that require database access, you can use . You can even use an fixture to limit database access to those tests marked with . Some plugins expand on the functionality of marks by adding their own guards. The plugin, for instance, provides a mark. Any tests without this mark that try to access the database will fail. The first test that tries to access the database will trigger the creation of Django’s test database. The requirement that you add the mark nudges you toward stating your dependencies explicitly. That’s the philosophy, after all! It also means that you can much more quickly run tests that don’t rely on the database, because will prevent the test from triggering database creation. The time savings really add up, especially if you’re diligent about running your tests frequently. provides a few marks out of the box:\n• skips a test if the expression passed to it evaluates to .\n• indicates that a test is expected to fail, so if the test does fail, the overall suite can still result in a passing status.\n• creates multiple variants of a test with different values as arguments. You’ll learn more about this mark shortly. You can see a list of all the marks that knows about by running . On the topic of parametrization, that’s coming up next.\n\nYou saw earlier in this tutorial how fixtures can be used to reduce code duplication by extracting common dependencies. Fixtures aren’t quite as useful when you have several tests with slightly different inputs and expected outputs. In these cases, you can parametrize a single test definition, and will create variants of the test for you with the parameters you specify. Imagine you’ve written a function to tell if a string is a palindrome. An initial set of tests could look like this: All of these tests except the last two have the same shape: This is starting to smell a lot like boilerplate. so far has helped you get rid of boilerplate, and it’s not about to let you down now. You can use to fill in this shape with different values, reducing your test code significantly: The first argument to is a comma-delimited string of parameter names. You don’t have to provide more than one name, as you can see in this example. The second argument is a list of either tuples or single values that represent the parameter value(s). You could take your parametrization a step further to combine all your tests into one: Even though this shortened your code, it’s important to note that in this case you actually lost some of the more descriptive nature of the original functions. Make sure you’re not parametrizing your test suite into incomprehensibility. You can use parametrization to separate the test data from the test behavior so that it’s clear what the test is testing, and also to make the different test cases easier to read and maintain.\n\nEach time you switch contexts from implementation code to test code, you incur some overhead. If your tests are slow to begin with, then overhead can cause friction and frustration. You read earlier about using marks to filter out slow tests when you run your suite, but at some point you’re going to need to run them. If you want to improve the speed of your tests, then it’s useful to know which tests might offer the biggest improvements. can automatically record test durations for you and report the top offenders. Use the option to the command to include a duration report in your test results. expects an integer value and will report the slowest number of tests. A new section will be included in your test report: Each test that shows up in the durations report is a good candidate to speed up because it takes an above-average amount of the total testing time. Note that short durations are hidden by default. As spelled out in the report, you can increase the report verbosity and show these by passing together with . Be aware that some tests may have an invisible setup overhead. You read earlier about how the first test marked with will trigger the creation of the Django test database. The report reflects the time it takes to set up the database in the test that triggered the database creation, which can be misleading. You’re well on your way to full test coverage. Next, you’ll be taking a look at some of the plugins that are part of the rich plugin ecosystem.\n\nYou learned about a few valuable plugins earlier in this tutorial. In this section, you’ll be exploring those and a few others in more depth—everything from utility plugins like to library-specific ones, like those for Django. Often the order of your tests is unimportant, but as your codebase grows, you may inadvertently introduce some side effects that could cause some tests to fail if they were run out of order. forces your tests to run in a random order. always collects all the tests it can find before running them. just shuffles that list of tests before execution. This is a great way to uncover tests that depend on running in a specific order, which means they have a stateful dependency on some other test. If you built your test suite from scratch in , then this isn’t very likely. It’s more likely to happen in test suites that you migrate to . The plugin will print a seed value in the configuration description. You can use that value to run the tests in the same order as you try to fix the issue. If you want to measure how well your tests cover your implementation code, then you can use the coverage package. integrates coverage, so you can run to see the test coverage report and boast about it on your project front page. provides a handful of useful fixtures and marks for dealing with Django tests. You saw the mark earlier in this tutorial. The fixture provides direct access to an instance of Django’s . The fixture provides a quick way to set or override Django settings. These plugins are a great boost to your Django testing productivity! If you’re interested in learning more about using with Django, then check out How to Provide Test Fixtures for Django Models in Pytest. can be used to run tests that fall outside the traditional scope of unit testing. Behavior-driven development (BDD) encourages writing plain-language descriptions of likely user actions and expectations, which you can then use to determine whether to implement a given feature. pytest-bdd helps you use Gherkin to write feature tests for your code. You can see which other plugins are available for with this extensive list of third-party plugins."
    },
    {
        "link": "https://pytest-with-eric.com/introduction/python-unit-testing-best-practices",
        "document": "In software development, the importance of unit testing cannot be overstated.\n\nA poorly constructed test suite can lead to a fragile codebase, hidden bugs, and hours wasted in debugging.\n\nNot to mention hard to maintain, extend, redundant, non-deterministic and no proper coverage. So how do you deal with the complexities of crafting effective unit tests?\n\nHow do you ensure your code works as expected? What’s the difference between good and great software?\n\nAfter conversations with Senior Python developers, reading countless books on the subject and my broad experience as a Python developer for 8+ years, I collected the below Python unit testing best practices.\n\nThis article will guide you through the labyrinth of best practices and strategies to ensure your tests are reliable, maintainable and efficient.\n\nYou’ll learn how to keep your tests fast, simple, and focused on singular functions or components.\n\nWe’ll delve into the art of making tests readable and deterministic, ensuring they are an integrated part of your build process.\n\nThe nuances of naming conventions, isolation of components, and the avoidance of implementation coupling will be dissected.\n\nWe’ll also touch upon advanced concepts like Dependency Injection, Test Coverage Analysis, and Test-Driven Development (TDD).\n\nLet’s embark on this journey to ensure your unit tests are as robust and effective as your code.\n\nWhy Care About Unit Testing Best Practices?\n\nWithout proper practices, tests can become a source of frustration and inefficiency bottlenecks.\n\nIf you don’t take the time to learn and put in place best practices from the get-go, refactoring and maintenance become difficult.\n\nFrom minor issues like changing your tests often to larger issues like broken CI Pipelines blocking releases.\n• None Inefficient Testing: Tests that are slow or complex can delay development cycles, making the process cumbersome and less productive.\n• None Lack of Clarity: Badly written tests with unclear intentions make it difficult for developers to understand what is being tested or why a test fails.\n• None Fragile Tests: Tests that are tightly coupled with implementation details can break easily with minor code changes, leading to increased maintenance.\n• None Test Redundancy: Writing tests that duplicate implementation logic can lead to bloated test suites.\n• None Non-Deterministic Tests: Flaky tests that produce inconsistent results undermine trust in the testing suite and can mask real issues.\n• None Inadequate Coverage: Missing key scenarios, especially edge cases, can leave significant gaps in the test suite, allowing bugs to go undetected.\n\nIn this article, we aim to resolve these challenges by providing guidelines and techniques to write tests that are fast, simple, readable, deterministic and well-integrated into the development process.\n\nThe goal is to enhance the effectiveness of the testing process, leading to more reliable and maintainable Python applications.\n\nFast tests foster regular testing, which in turn speeds up bug discovery. Long-running tests can slow down development and discourage regular testing.\n\nCertain tests, due to their reliance on complex data structures, may take longer. It’s wise to divide these into a separate suite, run via scheduled tasks or use mocking to reduce external resource dependency.\n\nFast tests can be run regularly, a key aspect in continuous integration environments where tests may accompany every commit.\n\nTo maintain a strong development workflow, prioritize testing concise code segments and minimize dependencies and large setups.\n\nThe faster your tests, the more dynamic and efficient your development cycle will be.\n\nEach test should be able to run individually and as part of the test suite, irrespective of the sequence in which it’s executed.\n\nAdhering to this rule necessitates that each test starts with a fresh dataset and often requires some form of post-test cleanup.\n\nPytest manages this process via Fixture Setup/Teardown, ensuring that each test is self-contained and its outcome unaffected by preceding or later tests.\n\nEach Test Should Test One Thing\n\nA test unit should concentrate on one small functionality and verify its correctness.\n\nAvoid combining many functionalities in a single test, as this can obscure the source of errors and be difficult to refactor.\n\nInstead, write separate tests for each functionality, which aids in pinpointing specific issues and ensures clarity.\n\nWhat Not To Do:\n\nWhat To Do:\n\nTests Should Be Readable (Use Sound Naming Conventions)\n\nReadable tests act as documentation, conveying the intended behaviour of the code.\n\nWell-structured and comprehensible tests help with easier maintenance and quicker debugging.\n\nThe use of descriptive names and straightforward logic ensures that anyone reviewing the code can understand the test’s purpose and function.\n\nWhat Not To Do:\n\nWhat To Do:\n\nDeterministic tests give consistent results under the same conditions, ensuring test predictability and trustworthiness.\n\nThis test is deterministic because it produces the same outcome.\n\nIt avoids relying on external factors like network dependencies or random data, which can lead to erratic test results.\n\nBy ensuring tests are deterministic, you create a robust foundation for your test suite.\n\nThis allows for repeatable, reliable testing across different environments and over time.\n\nThis test is non-deterministic because it relies on a random number generator.\n\nThe outcome can vary between runs, sometimes passing and sometimes failing, making it unreliable.\n\nTo manage non-deterministic tests, it’s crucial to drop randomness or external dependencies where possible.\n\nIf randomness is essential, consider using fixed seeds for random number generators.\n\nFor external dependencies like databases or APIs, use mock objects or fixtures to simulate predictable responses.\n\nLibraries like Faker and Hypothesis allow you to test your code against a variety of sample input data.\n\nThey should focus on what the code does, not how it does it.\n\nThis distinction is crucial for creating robust, maintainable tests that remain valid even when the underlying implementation changes.\n\nWhat Not To Do:\n\nWhat To Do:\n\nIn the second example, the test checks if correctly computes the sum, independent of how the function achieves this.\n\nThis approach allows the function to evolve (e.g., optimization, refactoring) without requiring changes to the test as long as it fulfils its intended behaviour.\n\nTests Should Be Part Of The Commit and Build Process\n\nIntegrating tests into the commit and build process is essential for maintaining code quality.\n\nThis integration ensures that tests are automatically run during the build, catching issues early and preventing bugs from reaching production.\n\nIt enforces a standard of code health and functionality, serving as a crucial checkpoint before deployment.\n\nAutomated testing within the build process streamlines development, enhances reliability and fosters a culture of continuous integration and delivery.\n\nYou can use pre-commit hooks to run tests before each commit, ensuring that only passing code is committed.\n\nYou can set up Pytest to run with CI tooling like GitHub Actions, Bamboo, CircleCI, Jenkins and so on that runs your test after deployment thus helping your release with confidence.\n\nEach test should aim to verify a single aspect of the code, making it easier to identify the cause of any failures.\n\nThis approach simplifies tests and makes them more maintainable.\n\nIn the recommended approach, each test focuses on a single assertion, providing a clear and specific test for each aspect of the user profile.\n\nThis makes it easier to diagnose issues when a test fails, as each test is focused on one functionality.\n\nUsing fake data or databases in testing allows you to simulate various scenarios without relying on actual databases, which can be unpredictable and slow down tests.\n\nFake data ensures consistency in test environments and prevents tests from failing due to external data changes.\n\nIt also helps keep tests faster and prevents accidental updates to the source data.\n\nIn the first example, is used to test the calculation function without needing real data.\n\nIn the second, a mock database ( ) is used to simulate database interactions.\n\nThis technique isolates the tests from external data sources and makes them more predictable and faster.\n\nYou can also spin up a local SQLite database for your tests.\n\nSQLite databases are lightweight, don’t need a separate server, and can be integrated into a test setup.\n\nIn Python unit testing, Dependency Injection (DI) is a best practice that enhances code testability and modularity.\n\nDI involves providing objects with their dependencies from outside, rather than having them create dependencies internally.\n\nThis method is particularly valuable in testing, as it allows for easy substituting of real dependencies with mocks or stubs.\n\nFor instance, a class that requires a database connection can be injected with a mock database during testing, isolating the class’s functionality from external systems.\n\nThis separation fosters cleaner, more maintainable code and simplifies the creation of focused reliable unit tests.\n\nIn this scenario, testing ‘s method is challenging because it’s tightly coupled with .\n\nIn the DI example, receives its dependency from the outside.\n\nThis allows us to inject a during testing, focusing the test on ‘s behaviour without relying on the actual database connection logic.\n\nThis makes the test more reliable, faster, and independent of external factors like a real database.\n\nUse Setup and Teardown To Isolate Test Dependencies\n\nIn Python unit testing, isolating test dependencies is key to reliable outcomes.\n\nSetup and teardown methods are instrumental and enable reusable and isolated test environments\n\nThe setup method prepares the test environment before each test, ensuring a clean state.\n\nConversely, the teardown method cleans up after tests, removing any data or state alterations.\n\nThis process prevents interference between tests, maintaining their independence.\n\nFixtures, in Pytest, provide a flexible way to set up and tear down code for many tests.\n\nThis approach ensures that the resource is set up before each test that uses the fixture and properly cleaned up afterwards, maintaining test isolation and reliability.\n\nIf you need a refresher on Pytest fixture setup and teardown, this article is a good starting point.\n\nYou can easily control fixture scopes using the parameter as detailed in this guide.\n\nOrganizing related tests into modules or classes is a cornerstone of good unit testing in Python.\n\nBy grouping, you logically categorize tests based on functionality, such as testing a specific module or feature.\n\nThis not only makes it easier to locate and run related tests but also improves the readability of your test suite.\n\nFor instance, all tests related to a class might reside in a class:\n\nThis structure ensures an organized approach, where related tests are grouped, making it easier to navigate and maintain the test suite.\n\nMock Where Necessary (with Caution)\n\nIn Python unit testing, the judicious use of mocking is essential.\n\nMocking replaces parts of your system under test with mock objects, isolating tests from external dependencies like databases or APIs.\n\nThis accelerates testing and focuses on the unit’s functionality.\n\nHowever, over-mocking can lead to tests that pass despite bugs in production code, as they might not accurately represent real-world interactions.\n\nThese articles provide a practical guide on how to use Mock and Monkeypatch in Pytest.\n\nUse Test Framework Features To Your Advantage\n\nLeveraging the full spectrum of features offered by Python test frameworks like Pytest can significantly enhance your testing practice. Key among these in Pytest are , fixtures, and parameterization.\n\nserves as a central place for fixture definitions, making them accessible across multiple test files, thereby promoting reuse and reducing code duplication.\n\nPytest Fixtures offer a powerful way to set up and tear down resources needed for tests, ensuring isolation and reliability.\n\nPytest Parameterization allows for the easy creation of multiple test cases from a single test function, enabling efficient testing of various input scenarios.\n\nHarnessing these features streamlines the testing process, ensuring more organized, maintainable, and comprehensive test suites.\n\nTDD is a foundational practice involving writing tests before writing the actual code.\n\nStart by creating a test for a new function or feature, which initially fails (as the feature isn’t implemented yet).\n\nThen, write the minimal amount of code required to pass the test.\n\nThis cycle of “Red-Green-Refactor” — writing a failing test, making it pass, and then refactoring — guides development, ensuring that your codebase is thoroughly tested and designed with testing in mind.\n\nAs you iterate, the tests evolve along with the code, facilitating a robust and maintainable codebase.\n\nConsistent review and refactoring of tests are crucial.\n\nThis involves examining tests to ensure they remain relevant, efficient, and readable.\n\nRefactoring might include removing redundancy, updating tests to reflect code changes, or improving test organization and naming.\n\nThis proactive approach keeps the test suite streamlined, improves its reliability, and ensures it continues to support ongoing development and codebase evolution.\n\nCoverage analysis involves using tools like coverage.py to measure the extent to which your test suite exercises your codebase.\n\nWhile this is helpful, it doesn’t mean your code is free from bugs. It means you’ve tested the implemented logic.\n\nTesting edge cases and against a variety of test data to uncover potential bugs should be your top priority.\n\nYou can read more about how to generate Pytest coverage reports here.\n\nTest for Security Issues as Part of Your Unit Tests\n\nSecurity testing involves crafting tests to uncover vulnerabilities like injection flaws, broken authentication, and data exposure.\n\nUsing libraries like , you can automate the detection of common security issues.\n\nFor instance, if your application involves user input processing, your tests should include cases that check for common security issues like SQL injection or cross-site scripting (XSS).\n\nThis function is vulnerable to SQL injection. To test for this security issue, you can write a unit test that attempts to exploit this vulnerability:\n\nIn this test, simulates an SQL injection attack.\n\nThe test checks if the function properly handles such input, ideally by raising a or similar.\n\nUsing tools like , you can automatically scan your code for such vulnerabilities and known security issues.\n\nI would also recommend against using raw SQL in your code unless absolutely necessary and instead use an ORM (Object Relational Mapping) tool like SQLAlchemy or SQL Model to perform database operations.\n\nIn this comprehensive guide, we’ve gone through the essential Python unit testing best practices.\n\nFrom the importance of fast, independent, and focused tests, to the clarity that comes from readable and deterministic tests, you now have the tools to enhance your testing strategy.\n\nWe’ve covered advanced techniques like Dependency Injection, Test Coverage Analysis, Test-Driven Development (TDD), and the critical role of security testing in unit tests.\n\nAs you apply these best practices, continue to review and refine your tests.\n\nEmbrace the mindset of continuous improvement and let these principles guide you towards creating exemplary Python applications.\n\nYour journey in mastering Python unit testing doesn’t end here; it evolves with every line of code you write and the tests you create.\n\nSo, go ahead, apply these Python unit testing best practices, and witness the transformation in your development workflow. Happy testing! 🚀🐍🧪\n\nIf you have ideas for improvement or like me to cover anything specific, please send me a message via Twitter, GitHub or Email.\n\nWhat is Setup and Teardown in Pytest? (Importance of a Clean Test Environment)\n\nHow To Generate Beautiful & Comprehensive Pytest Code Coverage Reports (With Example)\n\nIntroduction to Pytest Mocking - What It Is and Why You Need It\n\nThe Ultimate Guide To Using Pytest Monkeypatch with 2 Code Examples\n\nA Step-by-Step Guide To Using Pytest Fixtures With Arguments\n\nHow to Use Hypothesis and Pytest for Robust Property-Based Testing in Python\n\nAutomated Python Unit Testing Made Easy with Pytest and GitHub Actions\n\nWhat Are Pytest Fixture Scopes? (How To Choose The Best Scope For Your Test)\n\nTesting Your Code - The Hitchhiker’s Guide to Python\n\nPython Unit Testing: Best Practices to Follow\n\nUnit Testing Best Practices: 9 to Ensure You Do It Right\n\nPython’s top testing best practices"
    },
    {
        "link": "https://medium.com/@ydmarinb/detailed-guide-to-writing-efficient-unit-tests-in-python-with-pytest-2e56b33cb7dc",
        "document": "In the software development world, unit testing is indispensable for ensuring code quality and reliability. Pytest is an exceptional tool in Python for this purpose, offering simplicity and power in test writing. In this article, we’ll take you on a detailed journey to master the art of writing efficient unit tests in Python using Pytest.\n\nBefore we start, you need to install Pytest. It’s easy and quick with pip:\n\nTo understand better, we’ll create a basic calculator with various functions. We start with the function:\n\nLet’s start by writing tests for our function.\n\nRunning these tests is as simple as running in your terminal.\n\nPytest allows testing multiple scenarios with ease. Let’s expand our test for with different test cases:\n\nWe add functionality to to handle errors when inputs are not numbers:\n\nAnd here’s the test for this new functionality:\n\nLet’s not forget about . Here's a simple test for that function:\n• Clear and Descriptive Names: Choose names that clearly reflect what the test is checking.\n• Small and Focused Tests: Each test should test a single behavior or functionality.\n• Use of : Pytest makes writing legible and concise asserts easy.\n• Independence of Tests: Ensure that each test functions in isolation.\n• Avoid Complex Logic in Tests: Tests should be simple and straightforward.\n\nUnit tests are a pillar in the development of reliable and quality software. With Pytest, you have a powerful and flexible tool at your disposal. This guide provides a solid foundation for integrating unit tests into your Python projects, significantly improving your code quality."
    },
    {
        "link": "https://stackoverflow.com/questions/3942820/how-to-do-unit-testing-of-functions-writing-files-using-pythons-unittest",
        "document": "I have a Python function that writes an output file to disk. I want to write a unit test for it using Python's module. How should I assert equality of files? I would like to get an error if the file content differs from the expected one + list of differences. As in the output of the Unix diff command. Is there an official or recommended way of doing that?"
    }
]