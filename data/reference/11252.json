[
    {
        "link": "https://albumentations.ai/docs",
        "document": "Albumentations is a fast and flexible image augmentation library. The library is widely used in industry, deep learning research, machine learning competitions, and open source projects. Albumentations is written in Python, and it is licensed under the MIT license. The source code is available at https://github.com/albumentations-team/albumentations.\n\nIf you are new to image augmentation, start with our \"Learning Path\" for beginners. It describes what image augmentation is, how it can boost deep neural networks' performance, and why you should use Albumentations.\n\nFor hands-on experience, check out our \"Quick Start Guide\" and \"Examples\" sections. They show how you can use the library for different computer vision tasks: image classification, semantic segmentation, instance segmentation, object detection, and keypoint detection. Each example includes a link to Google Colab, where you can run the code by yourself.\n\nYou can also visit explore.albumentations.ai to visually explore and experiment with different augmentations in your browser. This interactive tool helps you better understand how each transform affects images before implementing it in your code.\n\n\"API Reference\" contains the description of Albumentations' methods and classes.\n• Transform Library Comparison - Find equivalent transforms between Albumentations and other libraries (torchvision, Kornia)\n• Using Albumentations to augment bounding boxes for object detection tasks\n• How to use Albumentations for detection tasks if you need to keep all bounding boxes\n• Applying the same augmentation with the same parameters to multiple images, masks, bounding boxes, or keypoints\n• How to save and load parameters of an augmentation pipeline\n• Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.\n• How to save and load transforms to HuggingFace Hub.\n\nExamples of how to use Albumentations with different deep learning frameworks¶"
    },
    {
        "link": "https://albumentations.ai/docs/getting_started/image_augmentation",
        "document": "We can divide the process of image augmentation into four steps:\n• Import albumentations and a library to read images from the disk (e.g., OpenCV).\n• Pass images to the augmentation pipeline and receive augmented images.\n• Import a library to read images from the disk. In this example, we will use OpenCV. It is an open-source computer vision library that supports many image formats. Albumentations has OpenCV as a dependency, so you already have OpenCV installed.\n\nTo define an augmentation pipeline, you need to create an instance of the class. As an argument to the class, you need to pass a list of augmentations you want to apply. A call to will return a transform function that will perform image augmentation.\n\nLet's look at an example:\n\nIn the example, receives a list with three augmentations: , , and . You can find the full list of all available augmentations in the GitHub repository and in the API Docs. A demo playground that demonstrates how augmentations will transform the input image is available at https://explore.albumentations.ai.\n\nTo create an augmentation, you create an instance of the required augmentation class and pass augmentation parameters to it. receives two parameters, and . means that will take an input image, extract a random patch with size 256 by 256 pixels from it and then pass the result to the next augmentation in the pipeline (in this case to ).\n\nin this example has one parameter named . is a special parameter that is supported by almost all augmentations. It controls the probability of applying the augmentation. means that with a probability of 50%, the transform will flip the image horizontally, and with a probability of 50%, the transform won't modify the input image.\n\nin the example also has one parameter, . With a probability of 20%, this augmentation will change the brightness and contrast of the image received from . And with a probability of 80%, it will keep the received image unchanged.\n\nA visualized version of the augmentation pipeline. You pass an image to it, the image goes through all transformations, and then you receive an augmented image from the pipeline.\n\nTo pass an image to the augmentation pipeline, you need to read it from the disk. The pipeline expects to receive an image in the form of a NumPy array. If it is a color image, it should have three channels in the following order: Red, Green, Blue (so a regular RGB image).\n\nTo read images from the disk, you can use OpenCV - a popular library for image processing. It supports a lot of input formats and is installed along with Albumentations since Albumentations utilizes that library under the hood for a lot of augmentations.\n\nTo read an image with OpenCV\n\nStep 4. Pass images to the augmentation pipeline and receive augmented images.¶\n\nTo pass an image to the augmentation pipeline you need to call the function created by a call to at Step 2. In the argument to that function, you need to pass an image that you want to augment.\n\nwill return a dictionary with a single key . Value at that key will contain an augmented image.\n\nTo augment the next image, you need to call again and pass a new image as the argument:\n\nEach augmentation will change the input image with the probability set by the parameter . Also, many augmentations have parameters that control the magnitude of changes that will be applied to an image. For example, has two parameters: that controls the magnitude of adjusting brightness and that controls the magnitude of adjusting contrast. The bigger the value, the more the augmentation will change an image. During augmentation, a magnitude of the transformation is sampled from a uniform distribution limited by and . That means that if you make multiple calls to with the same input image, you will get a different output image each time.\n• Showcase. Cool augmentation examples on diverse set of images from various real-world tasks."
    },
    {
        "link": "https://github.com/albumentations-team/albumentations_examples",
        "document": "If you found errors in the notebooks, please create:\n• Pull request with the fix or\n• Submit an issue to the main repository of Albumentations\n• example_bboxes.ipynb. Using Albumentations to augment bounding boxes for object detection tasks.\n• example_bboxes2.ipynb. How to use Albumentations for detection tasks if you need to keep all bounding boxes.\n• example_multi_target.ipynb. Applying the same augmentation with the same parameters to multiple images, masks, bounding boxes, or keypoints.\n• serialization.ipynb. How to save and load parameters of an augmentation pipeline.\n• showcase.ipynb. Showcase. Cool augmentation examples on diverse set of images from various real-world tasks.\n• example_documents.ipynb. How to apply Morphological, both erosion and dilation.\n• example_hfhub.ipynb. How to load and save augmentation pipeline to HuggungFace Hub.\n• example_OverlayElements.ipynb. How to apply OverlayElements to update text on the image\n• example_textimage.ipynb. How to add text to the image"
    },
    {
        "link": "https://albumentations.ai/docs/examples",
        "document": ""
    },
    {
        "link": "https://docs.ultralytics.com/integrations/albumentations",
        "document": "Enhance Your Dataset to Train YOLO11 Using Albumentations\n\nWhen you are building computer vision models, the quality and variety of your training data can play a big role in how well your model performs. Albumentations offers a fast, flexible, and efficient way to apply a wide range of image transformations that can improve your model's ability to adapt to real-world scenarios. It easily integrates with Ultralytics YOLO11 and can help you create robust datasets for object detection, segmentation, and classification tasks.\n\nBy using Albumentations, you can boost your YOLO11 training data with techniques like geometric transformations and color adjustments. In this article, we'll see how Albumentations can improve your data augmentation process and make your YOLO11 projects even more impactful. Let's get started!\n\nAlbumentations is an open-source image augmentation library created in June 2018. It is designed to simplify and accelerate the image augmentation process in computer vision. Created with performance and flexibility in mind, it supports many diverse augmentation techniques, ranging from simple transformations like rotations and flips to more complex adjustments like brightness and contrast changes. Albumentations helps developers generate rich, varied datasets for tasks like image classification, object detection, and segmentation.\n\nYou can use Albumentations to easily apply augmentations to images, segmentation masks, bounding boxes, and key points, and make sure that all elements of your dataset are transformed together. It works seamlessly with popular deep learning frameworks like PyTorch and TensorFlow, making it accessible for a wide range of projects.\n\nAlso, Albumentations is a great option for augmentation whether you're handling small datasets or large-scale computer vision tasks. It ensures fast and efficient processing, cutting down the time spent on data preparation. At the same time, it helps improve model performance, making your models more effective in real-world applications.\n\nAlbumentations offers many useful features that simplify complex image augmentations for a wide range of computer vision applications. Here are some of the key features:\n• Wide Range of Transformations: Albumentations offers over 70 different transformations, including geometric changes (e.g., rotation, flipping), color adjustments (e.g., brightness, contrast), and noise addition (e.g., Gaussian noise). Having multiple options enables the creation of highly diverse and robust training datasets.\n• High Performance Optimization: Built on OpenCV and NumPy, Albumentations uses advanced optimization techniques like SIMD (Single Instruction, Multiple Data), which processes multiple data points simultaneously to speed up processing. It handles large datasets quickly, making it one of the fastest options available for image augmentation.\n• Three Levels of Augmentation: Albumentations supports three levels of augmentation: pixel-level transformations, spatial-level transformations, and mixing-level transformation. Pixel-level transformations only affect the input images without altering masks, bounding boxes, or key points. Meanwhile, both the image and its elements, like masks and bounding boxes, are transformed using spatial-level transformations. Furthermore, mixing-level transformations are a unique way to augment data as it combines multiple images into one.\n• Benchmarking Results: When it comes to benchmarking, Albumentations consistently outperforms other libraries, especially with large datasets.\n\nWhy Should You Use Albumentations for Your Vision AI Projects?\n\nWith respect to image augmentation, Albumentations stands out as a reliable tool for computer vision tasks. Here are a few key reasons why you should consider using it for your Vision AI projects:\n• Easy-to-Use API: Albumentations provides a single, straightforward API for applying a wide range of augmentations to images, masks, bounding boxes, and keypoints. It's designed to adapt easily to different datasets, making data preparation simpler and more efficient.\n• Rigorous Bug Testing: Bugs in the augmentation pipeline can silently corrupt input data, often going unnoticed but ultimately degrading model performance. Albumentations addresses this with a thorough test suite that helps catch bugs early in development.\n• Extensibility: Albumentations can be used to easily add new augmentations and use them in computer vision pipelines through a single interface along with built-in transformations.\n\nHow to Use Albumentations to Augment Data for YOLO11 Training\n\nNow that we've covered what Albumentations is and what it can do, let's look at how to use it to augment your data for YOLO11 model training. It's easy to set up because it integrates directly into Ultralytics' training mode and applies automatically if you have the Albumentations package installed.\n\nTo use Albumentations with YOLO11, start by making sure you have the necessary packages installed. If Albumentations isn't installed, the augmentations won't be applied during training. Once set up, you'll be ready to create an augmented dataset for training, with Albumentations integrated to enhance your model automatically.\n\nFor detailed instructions and best practices related to the installation process, check our Ultralytics Installation guide. While installing the required packages for YOLO11, if you encounter any difficulties, consult our Common Issues guide for solutions and tips.\n\nAfter installing the necessary packages, you're ready to start using Albumentations with YOLO11. When you train YOLO11, a set of augmentations is automatically applied through its integration with Albumentations, making it easy to enhance your model's performance.\n\nNext, let's take look a closer look at the specific augmentations that are applied during training.\n\nThe Blur transformation in Albumentations applies a simple blur effect to the image by averaging pixel values within a small square area, or kernel. This is done using OpenCV function, which helps reduce noise in the image, though it also slightly reduces image details.\n\nHere are the parameters and values used in this integration:\n• blur_limit: This controls the size range of the blur effect. The default range is (3, 7), meaning the kernel size for the blur can vary between 3 and 7 pixels, with only odd numbers allowed to keep the blur centered.\n• p: The probability of applying the blur. In the integration, p=0.01, so there's a 1% chance that this blur will be applied to each image. The low probability allows for occasional blur effects, introducing a bit of variation to help the model generalize without over-blurring the images.\n\nThe MedianBlur transformation in Albumentations applies a median blur effect to the image, which is particularly useful for reducing noise while preserving edges. Unlike typical blurring methods, MedianBlur uses a median filter, which is especially effective at removing salt-and-pepper noise while maintaining sharpness around the edges.\n\nHere are the parameters and values used in this integration:\n• blur_limit: This parameter controls the maximum size of the blurring kernel. In this integration, it defaults to a range of (3, 7), meaning the kernel size for the blur is randomly chosen between 3 and 7 pixels, with only odd values allowed to ensure proper alignment.\n• p: Sets the probability of applying the median blur. Here, p=0.01, so the transformation has a 1% chance of being applied to each image. This low probability ensures that the median blur is used sparingly, helping the model generalize by occasionally seeing images with reduced noise and preserved edges.\n\nThe image below shows an example of this augmentation applied to an image.\n\nThe ToGray transformation in Albumentations converts an image to grayscale, reducing it to a single-channel format and optionally replicating this channel to match a specified number of output channels. Different methods can be used to adjust how grayscale brightness is calculated, ranging from simple averaging to more advanced techniques for realistic perception of contrast and brightness.\n\nHere are the parameters and values used in this integration:\n• num_output_channels: Sets the number of channels in the output image. If this value is more than 1, the single grayscale channel will be replicated to create a multichannel grayscale image. By default, it's set to 3, giving a grayscale image with three identical channels.\n• method: Defines the grayscale conversion method. The default method, \"weighted_average\", applies a formula (0.299R + 0.587G + 0.114B) that closely aligns with human perception, providing a natural-looking grayscale effect. Other options, like \"from_lab\", \"desaturation\", \"average\", \"max\", and \"pca\", offer alternative ways to create grayscale images based on various needs for speed, brightness emphasis, or detail preservation.\n• p: Controls how often the grayscale transformation is applied. With p=0.01, there is a 1% chance of converting each image to grayscale, making it possible for a mix of color and grayscale images to help the model generalize better.\n\nThe image below shows an example of this grayscale transformation applied.\n\nThe CLAHE transformation in Albumentations applies Contrast Limited Adaptive Histogram Equalization (CLAHE), a technique that enhances image contrast by equalizing the histogram in localized regions (tiles) instead of across the whole image. CLAHE produces a balanced enhancement effect, avoiding the overly amplified contrast that can result from standard histogram equalization, especially in areas with initially low contrast.\n\nHere are the parameters and values used in this integration:\n• clip_limit: Controls the contrast enhancement range. Set to a default range of (1, 4), it determines the maximum contrast allowed in each tile. Higher values are used for more contrast but may also introduce noise.\n• tile_grid_size: Defines the size of the grid of tiles, typically as (rows, columns). The default value is (8, 8), meaning the image is divided into a 8x8 grid. Smaller tile sizes provide more localized adjustments, while larger ones create effects closer to global equalization.\n• p: The probability of applying CLAHE. Here, p=0.01 introduces the enhancement effect only 1% of the time, ensuring that contrast adjustments are applied sparingly for occasional variation in training images.\n\nThe image below shows an example of the CLAHE transformation applied.\n\nIf you are interested in learning more about Albumentations, check out the following resources for more in-depth instructions and examples:\n• Albumentations Documentation: The official documentation provides a full range of supported transformations and advanced usage techniques.\n• Ultralytics Albumentations Guide: Get a closer look at the details of the function that facilitate this integration.\n• Albumentations GitHub Repository: The repository includes examples, benchmarks, and discussions to help you get started with customizing augmentations.\n\nIn this guide, we explored the key aspects of Albumentations, a great Python library for image augmentation. We discussed its wide range of transformations, optimized performance, and how you can use it in your next YOLO11 project.\n\nAlso, if you'd like to know more about other Ultralytics YOLO11 integrations, visit our integration guide page. You'll find valuable resources and insights there.\n\nHow can I integrate Albumentations with YOLO11 for improved data augmentation?\n\nAlbumentations integrates seamlessly with YOLO11 and applies automatically during training if you have the package installed. Here's how to get started:\n\nThe integration includes optimized augmentations like blur, median blur, grayscale conversion, and CLAHE with carefully tuned probabilities to enhance model performance.\n\nWhat are the key benefits of using Albumentations over other augmentation libraries?\n\nAlbumentations stands out for several reasons:\n• Performance: Built on OpenCV and NumPy with SIMD optimization for superior speed\n• Compatibility: Works seamlessly with popular frameworks like PyTorch and TensorFlow\n• Ease of use: Single unified API for all augmentation types\n\nWhat types of computer vision tasks can benefit from Albumentations augmentation?\n• Pose Estimation: Helps models adapt to different viewpoints and lighting conditions\n\nThe library's diverse augmentation options make it valuable for any vision task requiring robust model performance."
    },
    {
        "link": "https://albumentations.ai/docs/examples/example",
        "document": "This example shows how you can use Albumentations to define a simple augmentation pipeline.\n\nRead the image from the disk and convert it from the BGR color space to the RGB color space¶\n\nFor historical reasons, OpenCV reads an image in BGR format (so color channels of the image have the following order: Blue, Green, Red). Albumentations uses the most common and popular RGB image format. So when using OpenCV, we need to convert the image format to RGB explicitly.\n\nDefine a single augmentation, pass the image to it and receive the augmented image¶\n\nWe fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn't fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time.\n\nDefine an augmentation pipeline using , pass the image to it and receive the augmented image¶"
    },
    {
        "link": "https://stackoverflow.com/questions/75903878/use-custom-transformer-for-albumentations",
        "document": "I want to use the following custom transformer\n\nI use the following code to put it in compose :\n\nbut I get this error:"
    },
    {
        "link": "https://neptune.ai/blog/data-augmentation-in-python",
        "document": "In machine learning (ML), the situation when the model does not generalize well from the training data to unseen data is called overfitting. As you might know, it is one of the trickiest obstacles in applied machine learning.\n\nThe first step in tackling this problem is to actually know that your model is overfitting. That is where proper cross-validation comes in.\n\nAfter identifying the problem you can prevent it from happening by applying regularization or training with more data. Still, sometimes you might not have additional data to add to your initial dataset. Acquiring and labeling additional data points may also be the wrong path. Of course, in many cases, it will deliver better results, but in terms of work, it is often time-consuming and expensive.\n\nThat is where data augmentation (DA) comes in.\n\nData augmentation is a technique that can be used to artificially expand the size of a training set by creating modified data from the existing one. It is a good practice to use DA if you want to prevent overfitting, or the initial dataset is too small to train on, or even if you want to squeeze better performance from your model.\n\nLet’s make this clear, data augmentation is not only used to prevent overfitting. In general, having a large dataset is crucial for the performance of both ML and Deep Learning (DL) models. However, we can improve the performance of the model by augmenting the data we already have. It means that data augmentation is also good for enhancing the model’s performance.\n\n\n\nIn general, DA is frequently used when building a DL model. That is why throughout this article, we will mostly talk about performing data augmentation with various DL frameworks. Still, you should keep in mind that you can augment the data for the ML problems as well.\n• Any other types of data\n\nWe will focus on image augmentations as those are the most popular ones. Nevertheless, augmenting other types of data is as efficient and easy. That is why it’s good to remember some common techniques which can be performed to augment the data.\n\nWe can apply various changes to the initial data. For example, for images, we can use:\n• Geometric transformations – you can randomly flip, crop, rotate or translate images, and that is just the tip of the iceberg\n• Mixing images – basically, mix images with one another. Might be counterintuitive, but it works\n\nFor text there are:\n• Syntax-tree manipulation – paraphrase the sentence to be grammatically correct using the same words\n• Other described in the article about data augmentation in NLP\n\nFor audio augmentation, you can use:\n• Changing the speed of the tape\n• And many more\n\nMoreover, the greatest advantage of the augmentation techniques is that you may use all of them at once. Thus, you may get plenty of unique samples of data from the initial one.\n\nAs mentioned above, in Deep Learning, data augmentation is a common practice. Therefore, every DL framework has its own augmentation methods or even a whole library. For example, let’s see how to apply image augmentations using built-in methods in TensorFlow (TF) and Keras, PyTorch, and MxNet.\n\n\n\nTo augment images when using TensorFlow or Keras as our DL framework, we can:\n• Write our own augmentation pipelines or layers using tf.image.\n\nLet’s take a closer look on the first technique and define a function that will visualize an image and then apply the flip to that image using tf.image. You may see the code and the result below.\n\nFor finer control, you can write your own augmentation pipeline. In most cases, it is useful to apply augmentations on a whole dataset, not a single image. You can implement it as follows.\n\nOf course, that is just the tip of the iceberg. TensorFlow API has plenty of augmentation techniques. If you want to read more on the topic please check the official documentation or other articles.\n\nAs mentioned above, Keras has a variety of preprocessing layers that may be used for data augmentation. You can apply them as follows.\n\nAlso, you may use ImageDataGenerator (tf.keras.preprocessing.image.ImageDataGenerator) that generates batches of tensor images with real-time DA.\n\nTransforms library is the augmentation part of the torchvision package that consists of popular datasets, model architectures, and common image transformations for Computer Vision tasks.\n\nTo install Transforms you simply need to install torchvision:\n\nTransforms library contains different image transformations that can be chained together using the Compose method. Functionally, Transforms has a variety of augmentation techniques implemented. You can combine them by using Compose method. Just check the official documentation and you will certainly find the augmentation for your task.\n\nAdditionally, there is the torchvision.transforms.functional module. It has various functional transforms that give fine-grained control over the transformations. It might be really useful if you are building a more complex augmentation pipeline, for example, in the case of segmentation tasks.\n\nBesides that, Transforms doesn’t have a unique feature. It’s used mostly with PyTorch as it’s considered a built-in augmentation library.\n\nLet’s see how to apply augmentations using Transforms. You should keep in mind that Transforms works only with PIL images. That is why you should either read an image in PIL format or add the necessary transformation to your augmentation pipeline.\n\nSometimes you might want to write a custom Dataloader for the training. Let’s see how to apply augmentations via Transforms if you are doing so.\n\nMxnet also has a built-in augmentation library called Transforms (mxnet.gluon.data.vision.transforms). It is pretty similar to PyTorch Transforms library. There is pretty much nothing to add. Check the Transforms section above if you want to find more on this topic. General usage is as follows.\n\nThose are nice examples, but from my experience, the real power of data augmentation comes out when you are using custom libraries:\n• They have a wider set of transformation methods\n• They allow you to create custom augmentation\n• You can stack one transformation with another.\n\nThat is why using custom DA libraries might be more effective than using built-in ones.\n\nIn this section, we will talk about the following libraries :\n\nWe will look at the installation, augmentation functions, augmenting process parallelization, custom augmentations, and provide a simple example. Remember that we will focus on image augmentation as it is most commonly used.\n\nBefore we start I have a few general notes, about using custom augmentation libraries with different DL frameworks.\n\nIn general, all libraries can be used with all frameworks if you perform augmentation before training the model.\n\n\n\nThe point is that some libraries have pre-existing synergy with the specific framework, for example, Albumentations and Pytorch. It’s more convenient to use such pairs. Still, if you need specific functional or you like one library more than another you should either perform DA before starting to train a model or write a custom Dataloader and training process instead.\n\nThe second major topic is using custom augmentations with different augmentation libraries. For example, you want to use your own CV2 image transformation with a specific augmentation from Albumentations library.\n\nLet’s make this clear, you can do that with any library, but it might be more complicated than you think. Some libraries have a guide in their official documentation of how to do it, but others do not.\n\nIf there is no guide, you basically have two ways:\n• Apply augmentations separately, for example, use your transformation operation and then the pipeline.\n• Check Github repositories in case someone has already figured out how to integrate a custom augmentation to the pipeline correctly.\n\nOk, with that out of the way, let’s dive in.\n\nMoving on to the libraries, Augmentor is a Python package that aims to be both a data augmentation tool and a library of basic image pre-processing functions.\n\nIt is pretty easy to install Augmentor via pip:\n\nIf you want to build the package from the source, please, check the official documentation.\n\nIn general, Augmentor consists of a number of classes for standard image transformation functions, such as Crop, Rotate, Flip, and many more.\n\nAugmentor allows the user to pick a probability parameter for every transformation operation. This parameter controls how often the operation is applied. Thus, Augmentor allows forming an augmenting pipeline that chains together a number of operations that are applied stochastically.\n\nThis means that each time an image is passed through the pipeline, a completely different image is returned. Depending on the number of operations in the pipeline and the probability parameter, a very large amount of new image data can be created. Basically, that is data augmentation at its best.\n\nWhat can we do with images using Augmentor? Augmentor is more focused on geometric transformation though it has other augmentations too. The main features of Augmentor package are:\n• Perspective skewing – look at an image from a different angle\n• Shearing – tilt an image along with one of its sides\n\nAugmentor is a well-knit library. You can use it with various DL frameworks (TF, Keras, PyTorch, MxNet) because augmentations may be applied even before you set up a model.\n\nMoreover, Augmentor allows you to add custom augmentations. It might be a little tricky as it requires writing a new operation class, but you can do that.\n\nUnfortunately, Augmentor is neither extremely fast nor flexible functional wise. There are libraries that have more transformation functions available and can perform DA way faster and more effectively. That is why Augmentor is probably the least popular DA library.\n• We need to import it.\n• Add some operations in there\n• Use sample method to get the augmented images.\n\nPlease pay attention when using sample you need to specify the number of augmented images you want to get.\n\nAlbumentations is a computer vision tool designed to perform fast and flexible image augmentations. It appears to have the largest set of transformation functions of all image augmentation libraries.\n\n\n\nLet’s install Albumentations via pip. If you want to do it somehow else, check the official documentation.\n\nAlbumentations provides a single and simple interface to work with different computer vision tasks such as classification, segmentation, object detection, pose estimation, and many more. The library is optimized for maximum speed and performance and has plenty of different image transformation operations.\n\nIf we are talking about data augmentations, there is nothing Albumentations can not do. To tell the truth, Albumentations is the most stacked library as it does not focus on one specific area of image transformations. You can simply check the official documentation and you will find an operation that you need.\n\nMoreover, Albumentations has seamless integration with deep learning frameworks such as PyTorch and Keras. The library is a part of the PyTorch ecosystem but you can use it with TensorFlow as well. Thus, Albumentations is the most commonly used image augmentation library.\n\nOn the other hand, Albumentations is not integrated with MxNet, which means if you are using MxNet as a DL framework you should write a custom Dataloader or use another augmentation library.\n\n\n\nIt’s worth mentioning that Albumentations is an open-source library. You can easily check the original code if you want to.\n\nLet’s see how to augment an image using Albumentations. You need to define the pipeline using the Compose method (or you can use a single augmentation), pass an image to it, and get the augmented one.\n\nNow, after reading about Augmentor and Albumentations you might think all image augmentation libraries are pretty similar to one another.\n\nThat is right. In many cases, the functionality of each library is interchangeable. Nevertheless, each one has its own key features.\n\nImgAug is also a library for image augmentations. It is pretty similar to Augmentor and Albumentations functional wise, but the main feature stated in the official ImgAug documentation is the ability to execute augmentations on multiple CPU cores. If you want to do that you might want to check the following guide.\n\nAs you may see, this’s pretty different from the Augmentors focus on geometric transformations or Albumentations attempting to cover all augmentations possible.\n\nNevertheless, ImgAug’s key feature seems a bit weird as both Augmentor and Albumentations can be executed on multiple CPU cores as well. Anyway ImgAug supports a wide range of augmentation techniques just like Albumentations and implements sophisticated augmentation with fine-grained control.\n\n\n\nImgAug can be easily installed via pip or conda.\n\nLike other image augmentation libraries, ImgAug is easy to use. To define an augmenting pipeline use the Sequential method and then simply stack different transformation operations like in other libraries.\n\nOn the other hand, Autoaugment is something more interesting. As you might know, using Machine Learning (ML) to improve ML design choices has already reached the space of DA.\n\nIn 2018 Google has presented Autoaugment algorithm which is designed to search for the best augmentation policies. Autoaugment helped to improve state-of-the-art model performance on such datasets as CIFAR-10, CIFAR-100, ImageNet, and others.\n\nStill, AutoAugment is tricky to use, as it does not provide the controller module, which prevents users from running it for their own datasets. That is why using AutoAugment might be relevant only if it already has the augmentation strategies for the dataset we plan to train on and the task we are up to.\n\nThereby let us take a closer look at DeepAugment that is a bit faster and more flexible alternative to AutoAugment. DeepAugment has no strong connection to AutoAugment besides the general idea and was developed by a group of enthusiasts. You can install it via pip:\n\nIt’s important for us to know how to use DeepAugment to get the best augmentation strategies for our images. You may do it as follows or check out the official Github repository.\n\nPlease, keep in mind that when you use optimize method you should specify the number of samples that will be used to find the best augmentation strategies.\n\nOverall, both AutoAugment and DeepAugment are not commonly used. Still, it might be quite useful to run them if you have no idea of what augmentation techniques will be the best for your data. You should only keep in mind that it will take plenty of time because multiple models will be trained.\n\nIt’s worth mentioning that we have not covered all custom image augmentation libraries, but we have covered the major ones. Now you know what libraries are the most popular, what advantages and disadvantages they have, and how to use them. This knowledge will help you to find any additional information if you need so.\n\nAs you may have already figured out, the augmentation process is quite expensive time- and computation-wise.\n\nThe time needed to perform DA depends on the number of data points we need to transform, on the overall augmenting pipeline difficulty, and even on the hardware that you use to augment your data.\n\n\n\nLet’s run some experiments to find out the fastest augmentation library. We will perform these experiments for Augmentor, Albumentations, ImgAug, and Transforms. We will use an image dataset from Kaggle that is made for flower recognition and contains over four thousand images.\n\nFor our first experiment, we will create an augmenting pipeline that consists only of two operations. These will be Horizontal Flip with 0.4 probability and Vertical Flip with 0.8 probability. Let’s apply the pipeline to every image in the dataset and measure the time.\n\nAs we have anticipated, Augmentor performs way slower than other libraries. Still, both Albumentations and Transforms show a good result as they are optimized to perform fast augmentations.\n\n\n\nFor our second experiment, we will create a more complex pipeline with various transformations to see if Transforms and Albumentations stay at the top. We will stack more geometric transformations as a pipeline. Thus, we will be able to use all libraries as Augmentor, for example, doesn’t have much kernel filter operations.\n\nYou may find the full pipeline in the notebook that I’ve prepared for you. Please, feel free to experiment and play with it.\n\nOnce more Transforms and Albumentations are at the top.\n\nMoreover, I used neptune.ai to compare the CPU usage. if we check the CPU-usage graphs in the Neptune app, we will find out that both Albumentations and Transforms use less than 60% of CPU resources.\n\nOn the other hand, Augmentor and ImgAug use more than 80%.\n\nAs you may have noticed, both Albumentations and Transforms are really fast. That is why they are commonly used in real life.\n\nIt’s worth mentioning that despite DA being a powerful tool you should use it carefully. There are some general rules that you might want to follow when applying augmentations:\n• Choose proper augmentations for your task. Let’s imagine that you are trying to detect a face on an image. You choose Random Erasing as an augmentation technique and suddenly your model does not perform well even on training. That is because there is no face on an image as it was randomly erased by the augmentation technique. The same thing is with voice detection and applying noise injection to the tape as an augmentation. Keep these cases in mind and be logical when choosing DA techniques.\n• Do not use too many augmentations in one sequence. You may simply create a totally new observation that has nothing in common with your original training (or testing data)\n• Display augmented data (images and text) in the notebook and listen to the converted audio sample before starting training on them. It’s quite easy to make a mistake when forming an augmenting pipeline. That is why it’s always better to double-check the result.\n• Time the augmenting process and check the number of computational resources involved. As you may have seen above, neptune.ai can help you do that. Do not forget about the time library either.\n\nAlso, it’s a great practice to check Kaggle notebooks before creating your own augmenting pipeline. There are plenty of ideas you may find there. Try to find a notebook for a similar task and check if the author applied the same augmentations as you’ve planned.\n\nIn this article, we have figured out what data augmentation is, what DA techniques are there, and what libraries you can use to apply them.\n\nTo my knowledge, the best publically available library is Albumentations. That is why if you are working with images and do not use MxNet or TensorFlow as your DL framework, you should probably use Albumentations for DA.\n\nHopefully, with this information, you will have no problems setting up the DA for your next machine learning project."
    },
    {
        "link": "https://stackoverflow.com/questions/73684921/how-to-use-custom-or-albumentation-augmentations-on-detectron-2",
        "document": "So after running through the code flow and documentation, I found out that Each class is dependent on the class which is inherited from the fvcore library\n\nAlso the same dependency is also defined vaguely in this highlighted documentation block\n\nThen looking at all the aspects above and then going through this github PR merge fight thread, I sense that I'm on the right track so following augmentation, I have made this code which works perfectly for custom augmentation\n\nI looked into the implementation of and built my own code to generate transformations randomly. Setting mimics like functionality."
    },
    {
        "link": "https://edlitera.com/blog/posts/albumentations-pytorch-image-augmentation",
        "document": "In my previous articles in this series, I covered how to apply different types of transformations to images using the Albumentations library. Transforming images using various pixel-level and spatial-level transformations allows you to artificially increase the size of your dataset, to the point where you can use relatively small datasets to train a computer vision model. In this article, I'm going to demonstrate how to create an image augmentation pipeline with Albumentations and PyTorch.\n• Intro to Image Augmentation: How to Use Spatial-Level Image Transformations\n• Intro to Image Augmentation: What Are Pixel-Based Transformations?\n\nClassification is a relatively simple problem. Your goal is to train a model so that, when you feed an image to it, it will predict the label that is associated with that image. There are different types of classification problems (binary, multiclass, etc.), but the type of classification problem you're working on doesn't actually matter that much when you're looking at how to construct an image augmentation pipeline.\n\nA typical pipeline would therefore include just picking the transformations you plan on using and applying those transformations to the images in our training set of images. The label connected to the artificially created image will be the same as the label connected to the original image. Basically, I can say that the label of the image persists through the transformations.\n\nI'm going to demonstrate how to use Albumentations together with PyTorch to build an image augmentation pipeline.\n\nBefore all else let's go ahead and import everything you need to create it:\n\nAfter importing everything we need we can move on to creating a pipeline. In practice, you rarely apply just one transformation to an image. In most cases you apply multiple transformations to an image when you pass it through an image augmentation pipeline, so to start let's create a simple function that applies multiple transformations to an image. It will simulate the results of running an image through a pipeline. I can later demonstrate how to create a real pipeline with PyTorch.\n\nHow to Apply Multiple Transformations to Images\n\nTo apply multiple transformations to an image you use the Compose class. By using the Compose class you get a transform function. That transform function is what will be used for image augmentation.\n• Intro to Programming: What Are Functions and Methods in Python?\n• Intro to Programming: What Are Different Data Types in Programming?\n\nA typical example would be something like this:\n\nThe pipeline above consists of three transformations: the HorizontalFlip transformation, the VerticalFlip transformation, and the RandomRain transformation. The way it works is pretty straightforward: when an image gets to it, it has a 50% chance of being horizontally flipped (because p=0.5 for HorizontalFlip), 30% chance to be vertically flipped (because p=0.3 for VerticalFlip) and a random rain effect will always be applied to it (because p=1.0 for RandomRain). Using such a pipeline is very easy.\n\nLet's create a function that will run an image through your pipeline:\n\nTo demonstrate the results of applying multiple transformations to an image I'll use the image of the golf ball I used in the previous articles of this series.\n\nThe original golf ball image looks like this:\n\nNow let's demonstrate what is the result of applying multiple transformations on that image, using the function I defined earlier.\n\nTo do that, I'll run the following code:\n\nThe result of applying these transformations to my image will look like this:\n\nTake into consideration that, because I didn't define that all transformations have a 100% chance of being applied you might end up with an image that looks a bit different. If you want to make sure that you get the same result I got here just run the code a few times, or change the augmentation pipeline (assign the value 1.0 to all p values in the augmentation pipeline).\n\nAfter demonstrating how to apply multiple transformations to an image I can move on to explaining how to create a pipeline using PyTorch.\n\nWhy You Should Use PyTorch to Create Image Augmentation Pipelines\n\nThe PyTorch library already has a built-in package dedicated to performing image augmentation. However, that built-in package is a lot slower than the Albumentations library when it comes to performing image augmentation. Also, Albumentations is much more powerful in terms of the sheer number of different transformations that it allows the user to apply to an image.\n\nBefore diving deep into how to create an image augmentation pipeline by combining PyTorch with Albumentations, I'll first go over how you feed data to PyTorch models. This is important because it is prerequisite knowledge for building an image augmentation pipeline. For those that don't know how you feed data to PyTorch models, this will be extremely useful, and for those that do already know how it's done, it can serve as a refresher.\n\nWhat Are the Basics of Creating Datasets for PyTorch Models\n\nIn practice, it is a much better idea to separate the code used for creating a dataset that a neural network will use from the code that creates the neural network itself. PyTorch offers you two ways of preparing data for a model. You can do it either using the Dataset class or the DataLoader class, both of which can be imported from the torch.utils.data package of PyTorch. The Dataset class allows you to store your training samples with their corresponding labels, while the DataLoader class wraps an iterable around the Dataset class to enable easy access to the training samples you plan on using to train your model.\n\nEven though the process of using these classes to prepare data for a model might at first seem daunting, it is relatively simple if you understand how these classes work. In most cases, you use the Dataset class to prepare your data. I'll demonstrate how you can use the Dataset class to create your custom class that will allow you to not only load data but also apply transformations from Albumentations on the images it processes.\n\nThere are three functions a custom Dataset needs to implement to work properly:\n\nLet's break down what each of these does, and at the same time build your custom Dataset class.\n\nThis function runs once when you instantiate the Dataset object. This function will initialize the directory that contains images, the file that contains our labels, and the transformations you will apply to your images.\n\nThis function returns the number of samples in your dataset.\n\nThis function returns a sample from the dataset, identified with a given index. Using that index this function will go to the location where an image is stored in memory and it will load it in while also retrieving its corresponding label. The function will also apply transformations on the image if we define that you want to use some. Finally, the function will return a tuple that consists of two members: the image itself and its corresponding label.\n\nAfter defining how the aforementioned functions work, it is time to start building your custom class.\n\nLet's start building a custom Dataset class. I'll call it CustomDataset. To create it, I'm going to inherit from the Dataset class available in PyTorch. The __init__ and __len__ functions will be pretty much unchanged from how they are implemented in the Dataset class offered by PyTorch.\n\nThe only difference between this custom class and the original one lies in how is the __getitem__ function implemented. The first part of that function is pretty much the same as in the standard class, however, the second part is different because I don't want to use the built-in PyTorch transformations but the ones offered in Albumentations.\n\nNow that my three functions are ready, I can combine them to create our CustomDataset class.\n\nNow that the class is ready, I can use it to augment images before passing them into my model. To demonstrate, I’m going to run only one image through this pipeline, the image of the golf ball, but the same concept applies to running multiple images. I’ll also assign a label to the golf ball image for the purposes of this demonstration: let's say that the label associated with the golf ball is 0.\n\nTo run my images and apply the augmentation pipeline consisting of the three transformations I defined earlier in the article, I can use the following code:\n\nThe code above will create an object of the CustomDataset class. If you check the first member it produces, it is going to end up being a tuple that consists of the transformed image of our golf ball together with its label.\n\nSo if I run the following code:\n\nyou will end up with the following tuple:\n\nTo get the image, I’ll just extract the first member of that tuple:\n• Intro to Programming: What Are Tuples in Python?\n\nThe result I will get by running the code above is an image that looks like this:\n\nAgain, there might be some slight differences in the final image because this image augmentation pipeline created with the Compose class doesn't apply all transformations to the image 100% of the time. If you want to try and get a different final result, just rerun the code that creates the custom dataset object and take a look at the resultant image:\n\nWith this article, I’ll finish this series of articles on using Albumentations for image augmentation. In it, I described how to combine multiple transformations into a pipeline, and I later covered how to integrate that pipeline with PyTorch, one of the two most popular frameworks for creating and training Deep Learning models in Python.\n\nI explained how data is usually loaded in PyTorch models, and how you can modify the classes used for loading data so that you can use transformations from Albumentations instead of those built-in into PyTorch. The task I tackled in the article was image classification. Pipelines for object detection and more advanced tasks are a bit more complicated, but generally follow the same principles that were mentioned in this article."
    }
]