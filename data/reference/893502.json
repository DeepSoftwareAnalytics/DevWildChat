[
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.normal.html",
        "document": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.\n\nThe is a tensor with the mean of each output element’s normal distribution\n\nThe is a tensor with the standard deviation of each output element’s normal distribution\n\nThe shapes of and don’t need to match, but the total number of elements in each tensor need to be the same.\n\nSimilar to the function above, but the means are shared among all drawn elements.\n\nSimilar to the function above, but the standard deviations are shared among all drawn elements.\n\nSimilar to the function above, but the means and standard deviations are shared among all drawn elements. The resulting tensor has size given by ."
    },
    {
        "link": "https://pytorch.org/docs/stable/distributions.html",
        "document": "The package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. This package generally follows the design of the TensorFlow Distributions package.\n\nIt is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through. These are the score function estimator/likelihood ratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning, and the pathwise derivative estimator is commonly seen in the reparameterization trick in variational autoencoders. Whilst the score function only requires the value of samples f(x), the pathwise derivative requires the derivative f′(x). The next sections discuss these two in a reinforcement learning example. For more details see Gradient Estimation Using Stochastic Computation Graphs .\n\nWhen the probability density function is differentiable with respect to its parameters, we only need and to implement REINFORCE: where θ are the parameters, α is the learning rate, r is the reward and p(a∣πθ(s)) is the probability of taking action a in state s given policy πθ. In practice we would sample an action from the output of a network, apply this action in an environment, and then use to construct an equivalent loss function. Note that we use a negative because optimizers use gradient descent, whilst the rule above assumes gradient ascent. With a categorical policy, the code for implementing REINFORCE would be as follows: # Note that this is equivalent to what used to be called multinomial\n• None event_dim (int) – Optional size of . This should be zero for univariate random variables, 1 for distributions over vectors, 2 for distributions over matrices, etc. Transform functor that applies a sequence of transforms component-wise to each submatrix at , of length , in a way compatible with . Composes multiple transforms in a chain. The transforms being composed are responsible for caching.\n• None parts (list of ) – A list of transforms to compose.\n• None cache_size (int) – Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported. Transforms an uncontrained real vector x with length D∗(D−1)/2 into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:\n• None First we convert x into a lower triangular matrix in row order.\n• None For each row Xi​ of the lower triangular part, we apply a signed version of class to transform Xi​ into a unit Euclidean length vector using the following steps: - Scales into the interval (−1,1) domain: ri​=tanh(Xi​). - Transforms into an unsigned domain: zi​=ri2​. - Applies si​=StickBreakingTransform(zi​). - Transforms back into signed domain: yi​=sign(ri​)∗si​ ​. Transform via the cumulative distribution function of a probability distribution. distribution (Distribution) – Distribution whose cumulative distribution function to use for the transformation. Wrapper around another transform to treat -many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out -many of the rightmost dimensions in .\n• None reinterpreted_batch_ndims (int) – The number of extra rightmost dimensions to treat as dependent. Transform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries. This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization. Unit Jacobian transform to reshape the rightmost part of a tensor. Note that and must have the same number of elements, just as for . Transform via the mapping y=1+exp(−x)1​ and x=logit(y). Transform via the mapping Softplus(x)=log(1+exp(x)). The implementation reverts to the linear function when x>20. It is equivalent to However this might not be numerically stable, thus it is recommended to use instead. Note that one should use when it comes to values. Transform from unconstrained space to the simplex via y=exp(x) then normalizing. This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms. Transform functor that applies a sequence of transforms component-wise to each submatrix at in a way compatible with . Transform from unconstrained space to the simplex of one additional dimension via a stick-breaking process. This transform arises as an iterated sigmoid transform in a stick-breaking construction of the distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses. This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization. Abstract class for invertable transformations with computable log det jacobians. They are primarily used in . Caching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching: However the following will error when caching due to dependency reversal: Derived classes should implement one or both of or . Derived classes that set should also implement . cache_size (int) – Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.\n• None domain ( ) – The constraint representing valid inputs to this transform.\n• None codomain ( ) – The constraint representing valid outputs to this transform which are inputs to the inverse transform.\n• None bijective (bool) – Whether this transform is bijective. A transform is bijective iff and for every in the domain and in the codomain. Transforms that are not bijective should at least maintain the weaker pseudoinverse properties and .\n• None sign (int or Tensor) – For bijective univariate transforms, this should be +1 or -1 depending on whether transform is monotone increasing or decreasing. Returns the inverse of this transform. This should satisfy . Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms. Computes the log det jacobian given input and output. Infers the shape of the forward computation, given the input shape. Defaults to preserving shape. Infers the shapes of the inverse computation, given the output shape. Defaults to preserving shape.\n\nPyTorch provides two global objects that link objects to objects. These objects both input constraints and return transforms, but they have different guarantees on bijectivity.\n• None looks up a bijective from to the given . The returned transform is guaranteed to have and should implement .\n• None looks up a not-necessarily bijective from to the given . The returned transform is not guaranteed to implement . The registry is useful for performing unconstrained optimization on constrained parameters of probability distributions, which are indicated by each distribution’s dict. These transforms often overparameterize a space in order to avoid rotation; they are thus more suitable for coordinate-wise optimization algorithms like Adam: The registry is useful for Hamiltonian Monte Carlo, where samples from a probability distribution with constrained are propagated in an unconstrained space, and algorithms are typically rotation invariant.: An example where and differ is : returns a that simply exponentiates and normalizes its inputs; this is a cheap and mostly coordinate-wise operation appropriate for algorithms like SVI. In contrast, returns a that bijects its input down to a one-fewer-dimensional space; this a more expensive less numerically stable transform but is needed for algorithms like HMC. The and objects can be extended by user-defined constraints and transforms using their method either as a function on singleton constraints: or as a decorator on parameterized constraints: You can create your own registry by creating a new object.\n• None constraint (subclass of ) – A subclass of , or a singleton object of the desired class."
    },
    {
        "link": "https://stackoverflow.com/questions/51136581/how-do-i-create-a-normal-distribution-in-pytorch",
        "document": "It depends on what you want to generate.\n\nfor all all distribution (say normal, poisson or uniform etc) use or . A detail of all these methods can be seen here - https://pytorch.org/docs/stable/distributions.html#normal\n\nOnce you define these methods you can use .sample method to generate the number of instances. It also allows you to generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched."
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.randn.html",
        "document": "Returns a tensor filled with random numbers from a normal distribution with mean and variance (also called the standard normal distribution).\n\nFor complex dtypes, the tensor is i.i.d. sampled from a complex normal distribution with zero mean and unit variance as\n\nThis is equivalent to separately sampling the real (Re) and imaginary (Im) part of outi​ as\n\nThe shape of the tensor is defined by the variable argument .\n\nsize (int...) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ).\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: .\n• None pin_memory (bool, optional) – If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: ."
    },
    {
        "link": "https://stackoverflow.com/questions/51136581/how-do-i-create-a-normal-distribution-in-pytorch/55202492",
        "document": "It depends on what you want to generate.\n\nfor all all distribution (say normal, poisson or uniform etc) use or . A detail of all these methods can be seen here - https://pytorch.org/docs/stable/distributions.html#normal\n\nOnce you define these methods you can use .sample method to generate the number of instances. It also allows you to generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched."
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.bmm.html",
        "document": "Performs a batch matrix-matrix product of matrices stored in and .\n\nand must be 3-D tensors each containing the same number of matrices.\n\nIf is a (b×n×m) tensor, is a (b×m×p) tensor, will be a (b×n×p) tensor.\n\nOn certain ROCm devices, when using float16 inputs this module will use different precision for backward."
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.matmul.html",
        "document": "The behavior depends on the dimensionality of the tensors as follows:\n• None If both tensors are 1-dimensional, the dot product (scalar) is returned.\n• None If both arguments are 2-dimensional, the matrix-matrix product is returned.\n• None If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.\n• None If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.\n• None If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a batched matrix multiply is returned. If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). For example, if is a (j×1×n×n) tensor and is a (k×n×n) tensor, will be a (j×k×n×n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions. For example, if is a (j×1×n×m) tensor and is a (k×m×p) tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the matrix dimensions) are different. will be a (j×k×n×p) tensor.\n\nThis operation has support for arguments with sparse layouts. In particular the matrix-matrix (both arguments 2-dimensional) supports sparse arguments with the same restrictions as\n\nSparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, or may not have autograd support. If you notice missing functionality please open a feature request.\n\nOn certain ROCm devices, when using float16 inputs this module will use different precision for backward."
    },
    {
        "link": "https://restack.io/p/pytorch-knowledge-bmm-vs-matmul",
        "document": "Explore the differences between PyTorch bmm and matmul operations for efficient tensor computations.\n\nBatched computations in PyTorch allow for efficient processing of multiple inputs simultaneously, leveraging the capabilities of underlying mathematical libraries. This approach is particularly beneficial when using operations like and , which are designed to handle batch operations directly. However, it is crucial to understand the nuances of these operations to avoid unexpected results. When performing batched matrix multiplication, the results may not always align with expectations. For instance, consider two 3D tensors, and , suitable for batched matrix multiplication. The expression does not guarantee bitwise equality with . This discrepancy arises from the optimizations and internal implementations used in PyTorch, which can yield slightly different results compared to non-batched computations. Similarly, operations on tensor slices can lead to unexpected outcomes. For example, if is a 2D tensor, the operation may not be bitwise equal to . This highlights the importance of understanding how batched operations interact with tensor slicing. When training models with FP16 precision, it is essential to be aware of potential convergence issues, particularly with FP16 denorms being flushed to zero. Denormal values often occur during the backward pass, especially during gradient calculations. By default, PyTorch utilizes alternate implementations from rocBLAS and MIOpen during this phase, which can be adjusted using specific environment variables: The following operations may leverage rocBLAS: MIOpen may be used in the following operations: Understanding these performance considerations is vital for optimizing batched computations in PyTorch, especially when dealing with precision and the intricacies of tensor operations.\n• None Learn about broadcasting in Pytorch, a powerful feature that simplifies tensor operations and enhances computational efficiency.\n• None Explore the fundamentals of batch normalization in Pytorch, enhancing model training and performance.\n\nIn PyTorch, understanding the differences between (batch matrix multiplication) and (matrix multiplication) is crucial for optimizing performance in deep learning applications. Both functions serve to multiply matrices, but they are designed for different scenarios, particularly when dealing with batches of data. is specifically tailored for batch operations. It takes in two 3D tensors, where the first dimension represents the batch size. For instance, if you have two tensors and , both of shape and respectively, the result of will yield a tensor of shape . This is particularly useful when you need to perform the same matrix multiplication across multiple sets of matrices in a single operation, thus enhancing computational efficiency. import torch # Create random tensors for batch matrix multiplication A = torch.randn(10, 3, 4) # Batch of 10 matrices of size 3x4 B = torch.randn(10, 4, 5) # Batch of 10 matrices of size 4x5 # Perform batch matrix multiplication C = torch.bmm(A, B) # Resulting tensor will be of size (10, 3, 5) On the other hand, is used for standard matrix multiplication between two 2D tensors. It does not support batch operations directly. If you need to multiply matrices that are part of a batch, you would typically loop through each matrix in the batch and apply individually, which can be less efficient than using . import torch # Create two 2D tensors for standard matrix multiplication A = torch.randn(3, 4) # Matrix of size 3x4 B = torch.randn(4, 5) # Matrix of size 4x5 # Perform standard matrix multiplication C = torch.mm(A, B) # Resulting tensor will be of size (3, 5) When deciding between and , consider the following:\n• Batch Size: If you are working with batches of matrices, is the preferred choice due to its optimized performance for batch operations.\n• Dimensionality: Ensure that the dimensions of your tensors align correctly for the operation you intend to perform. Mismatched dimensions will lead to runtime errors.\n• Memory Efficiency: Using can reduce memory overhead by minimizing the number of intermediate tensors created during batch operations. In summary, while both and are essential for matrix operations in PyTorch, choosing the right function based on your data structure and computational needs can significantly impact performance. For batch operations, always opt for to leverage its efficiency and speed.\n• None Learn about broadcasting in Pytorch, a powerful feature that simplifies tensor operations and enhances computational efficiency.\n• None Explore the fundamentals of batch normalization in Pytorch, enhancing model training and performance. Build reliable and accurate AI agents in code, capable of running and persisting month-lasting processes in the background."
    },
    {
        "link": "https://stackoverflow.com/questions/56543924/batch-matrix-multiplication-in-pytorch-confused-with-the-handling-of-the-outpu",
        "document": "Array contains a batch of RGB images, with shape:\n\nwhereas Array contains coefficients needed for a \"transformation-like\" operation on images, with shape:\n\nTo put it simply, the operation for a single image is a multiplication that outputs an environment map ( ).\n\nThe output I want should hold shape:\n\nI tried using but failed. Is this possible somehow?"
    },
    {
        "link": "https://github.com/pytorch/pytorch/issues/103674",
        "document": "It seems that the on gpu is worse than per batch multiplication by 10x-100x.\n\nIf I change batch_size to 100, it outputs:\n\nAs a contrast, on CPU:\n\n For , output:\n\nAnything wrong with the bmm cuda implementation?"
    },
    {
        "link": "https://medium.com/biased-algorithms/mastering-pytorch-to-device-an-advanced-guide-for-efficient-device-management-0290b086f17e",
        "document": "“Success lies in the details,” they say. And when it comes to deep learning, that detail is often device management. Think of it like this: you have an engine capable of incredible power, but without fuel efficiency, all that potential fizzles. Device management in PyTorch is that fuel efficiency — moving data and models to the GPU (or CPU, when needed) to maximize performance while keeping memory overhead low. Here’s the deal: this guide isn’t about the “what” of —it’s all about the “how” and “why” for pros like you. We’ll skip the obvious and focus on best practices, common pitfalls, and advanced techniques. For you to walk away with practical code examples that fine-tune your PyTorch projects to fully utilize your hardware, so your models run smoother and faster.\n\nLet’s get straight to the point. The method is your go-to function for transferring data and models to the GPU (or back to the CPU) with PyTorch. At its core, it's simple. But we’re setting up here for efficiency later on, so here’s a quick refresher. Example: Moving Tensors and Models to GPU import torch\n\n\n\n# Define your device - GPU if available, else CPU\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n# Moving a tensor to the selected device\n\ntensor = torch.randn(10) # a random tensor of shape (10,)\n\ntensor = tensor.to(device) # move tensor to GPU (if available)\n\n\n\n# Moving a model to the selected device\n\nmodel = torch.nn.Linear(10, 5) # an example linear model\n\nmodel = model.to(device) # transfer model to GPU Here’s what’s happening: when you call , you’re telling PyTorch to move data or models to the device you’ve specified, which is GPU if it’s available. This simple call is foundational, but as we go on, you’ll see just how much more can be done when you leverage it strategically.\n\nNow, let’s get serious about efficiency. Not every piece of data needs to hog GPU memory. By carefully deciding what goes to the GPU and what stays on the CPU, you can dramatically reduce memory load and accelerate your computations. Example 1: Moving Only Relevant Parts to GPU Sometimes, you may only need specific layers or particular data chunks on the GPU, not the entire model or dataset. Here’s how to do it: # Consider a model with multiple layers\n\nclass MyModel(torch.nn.Module):\n\n def __init__(self):\n\n super(MyModel, self).__init__()\n\n self.layer1 = torch.nn.Linear(10, 50)\n\n self.layer2 = torch.nn.Linear(50, 5)\n\n\n\n def forward(self, x):\n\n x = self.layer1(x)\n\n return self.layer2(x)\n\n\n\nmodel = MyModel()\n\n\n\n# Move only specific layers to the GPU\n\nmodel.layer1.to(device) # Move only layer1 to the GPU\n\nmodel.layer2.to(\"cpu\") # Keep layer2 on the CPU Here’s why this matters: targeted device placement optimizes memory usage by ensuring only what’s necessary is on the GPU, allowing you to handle larger models or datasets within limited GPU memory. For flexible code that runs across any setup (single-GPU, multi-GPU, or CPU-only), it’s best to dynamically assign the device. This small step makes your code more portable. # Dynamically set device based on system configuration\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"Using device: {device}\")\n\n\n\n# Moving a model to the dynamically selected device\n\nmodel = MyModel().to(device) Here’s the trick: by dynamically choosing the device, you make sure your code can scale up or down without any manual adjustment, ready for multi-GPU setups or CPU-only environments alike. In these snippets, you’re already making strides in reducing your memory footprint and boosting performance. From here, we’ll dig deeper into more complex scenarios, like optimizing data transfer and managing devices efficiently across multi-GPU settings.\n\nYou’d think moving data to the right device is straightforward, right? But here’s the catch: even seasoned data scientists encounter device mismatches that lead to silent performance issues or outright runtime errors. Here’s how you can sidestep these pitfalls like a pro. One common trap is inadvertently moving your model and data to different devices, which can cause cryptic errors or silent slowdowns. Here’s a look at this pitfall and how to handle it. # Device selection\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n# Suppose you move the model to GPU\n\nmodel = torch.nn.Linear(10, 5).to(device)\n\n\n\n# But forget to move the input data to GPU\n\ndata = torch.randn(10) # this tensor is still on the CPU\n\n\n\n# This will raise a runtime error\n\noutput = model(data) Always double-check the device of both your model and data before passing data through your model. Use assertions for explicit checks if you’re debugging: # Ensuring data and model are on the same device\n\nassert data.device == model.weight.device, \"Data and model are on different devices!\" This simple assert can save hours of debugging by immediately flagging when there’s a mismatch, instead of waiting for PyTorch’s error or a significant slowdown. A common scenario is forgetting to transfer data from a DataLoader to the same device as the model, leading to significant performance bottlenecks. Here’s a way to ensure consistent device transfer: # Define your DataLoader and move batches to the correct device within the training loop\n\nfor batch in dataloader:\n\n batch = batch.to(device) # Ensure each batch is moved to the model's device\n\n output = model(batch) Consistency is king when it comes to devices. Always make sure that both your model and data batches are on the same device to avoid slowdowns and runtime errors. You’ll save memory and time by staying consistent, especially when training across multiple epochs.\n\nYou might be wondering if there’s a way to speed up data transfer. Here’s the answer: non-blocking transfers can make a difference in performance, especially when loading large batches of data from CPU to GPU. Example: Using for Asynchronous Data Transfer The argument allows you to load data asynchronously, minimizing data-loading bottlenecks. Here’s how you’d use it: By adding , you enable asynchronous operations, reducing the time your code spends waiting for data transfers. This approach works particularly well when used alongside in DataLoaders. Real-World Example: Using in DataLoader Pinning memory can speed up data transfer from CPU to GPU, as it allows the data to be directly accessed by the GPU. Here’s how to set it up: # Define DataLoader with pin_memory\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=64, pin_memory=True)\n\n\n\n# Within your training loop\n\nfor data in dataloader:\n\n data = data.to(device, non_blocking=True)\n\n # Continue with training steps When to Use: You’ll see gains from this method when training on large datasets where CPU-GPU transfer time is a noticeable part of the runtime.\n\nMulti-GPU setups bring extra power but also extra complexity. Let’s break down how to use PyTorch’s built-in tools like and to handle multi-GPU training smoothly. When you need to utilize multiple GPUs, makes it easy to distribute your model across devices without changing much code. # Wrapping a model for DataParallel usage\n\nmodel = torch.nn.Linear(10, 5)\n\nmodel = torch.nn.DataParallel(model) # Distribute model across available GPUs\n\n\n\n# Move model to GPU\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = model.to(device) Here’s the advantage: handles splitting data across GPUs automatically, so you can keep working with your model as if it’s running on a single device. Example 2: Using for Advanced Multi-GPU Support For setups that require scalability and flexibility, the module is ideal, though it requires more setup. Here’s a quick example using to initialize and handle multi-GPU training: import torch.distributed as dist\n\n\n\n# Initialize the process group\n\ndist.init_process_group(\"nccl\", init_method=\"env://\")\n\n\n\n# Define your device\n\nlocal_rank = torch.distributed.get_rank()\n\ntorch.cuda.set_device(local_rank)\n\ndevice = torch.device(\"cuda\", local_rank)\n\n\n\n# Example model and data transfer\n\nmodel = MyModel().to(device)\n\ndata = data.to(device) By leveraging , you can coordinate training across multiple GPUs and even multiple nodes, offering flexibility for larger-scale projects. Here’s a trick for debugging: check device placement for all model parameters. Sometimes, issues arise because not every part of the model is on the correct device. You can verify this by inspecting each parameter:\n• For multi-GPU setups, is simpler, while offers scalability.\n• Regularly check device placement for all parameters, especially in custom models where layers may be inadvertently left on the CPU.\n\nWhen it comes to squeezing out every ounce of performance, benchmarking is invaluable. Device transfer times might seem negligible, but with large datasets or complex models, they can add up fast. Here’s how you can measure and optimize these transfers using a mix of and . Here’s a straightforward way to measure how long it takes to move data from the CPU to the GPU. gives you a fine-grained view into GPU-related timings, while works for CPU operations. Combining the two lets you see the complete picture. import torch\n\nimport time\n\n\n\n# Initialize a large tensor\n\ndata = torch.randn(10000, 10000)\n\n\n\n# Set up CUDA events for GPU timing\n\nstart_event = torch.cuda.Event(enable_timing=True)\n\nend_event = torch.cuda.Event(enable_timing=True)\n\n\n\n# Move data to the GPU and time it\n\nstart_event.record()\n\ndata = data.to(\"cuda\")\n\nend_event.record()\n\n\n\n# Wait for the events to complete\n\ntorch.cuda.synchronize()\n\n\n\n# Calculate elapsed time\n\nprint(f\"Time to move tensor to GPU: {start_event.elapsed_time(end_event)} milliseconds\") Explanation: By timing the operation, you can identify potential bottlenecks. If your model spends too much time on data transfers, it may be worth investigating other strategies, like minimizing transfers or using pinned memory to accelerate the process. To really see the impact of transfer times, try measuring the transfer cost with a batch of model parameters. # Example: Timing large model parameter transfer\n\nmodel = torch.nn.Linear(5000, 5000)\n\nstart_time = time.time()\n\nmodel = model.to(\"cuda\")\n\ntorch.cuda.synchronize() # Ensures all operations are complete before timing\n\n\n\nprint(f\"Model transfer time: {time.time() - start_time} seconds\") The goal here is to make transfer costs explicit, so you know exactly what each operation is costing. By benchmarking, you can make informed choices, like restructuring code to reduce the number of transfers if they’re a significant bottleneck.\n\nHere’s the deal: device-agnostic code makes your work not just functional but adaptable. This is especially useful when code might run on systems without a GPU or when you want to toggle seamlessly between CPU and GPU. Here are some key practices to follow. Creating a consistent device setup ensures that every tensor and model layer knows where to go without needing manual adjustment later on. Here’s a simple, device-agnostic setup: # Define your device\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n# Ensure tensor is always transferred to the correct device\n\ntensor = torch.randn(10).to(device) With this setup, you can handle both CPU-only and GPU-capable setups without needing to modify your code. When to Use Sometimes, you need to retrieve data from the GPU for logging, monitoring, or evaluation without affecting the original computation graph. That’s where comes in handy. # Move tensor to CPU for inspection without affecting gradients\n\ntensor = torch.randn(10, requires_grad=True).to(device)\n\ncpu_tensor = tensor.detach().cpu()\n\nprint(cpu_tensor) Using lets you safely move data back to the CPU without altering the model’s gradients or creating unnecessary device conflicts. This approach is ideal for device-agnostic code since it keeps data accessible on the CPU, making it easier to work with in mixed device environments. Device-agnostic code isn’t just about convenience — it’s about portability and readability. With a few standardized calls, your code remains versatile, reducing the need for modifications when switching devices or deploying on various hardware setups."
    },
    {
        "link": "https://discuss.pytorch.org/t/how-to-optimize-memory-allocation-with-intermediate-tensor-operations/149053",
        "document": "I’m creating a custom and I’ve been running to GPU OOM problems because at times the tensors within the module can get quite large. From a memory management perspective I know that using tensor methods like , etc are cheap as they don’t reassign the underlying data structure. To better optimise my code I have been using to inspect the memory footprint.\n\nI have noticed that the following operations do not assign the intermediate tensor in memory, i.e. the pointwise multiplication result is not stored in memory, and is instead immediately passed to the function.\n\nHowever, an operation like seemingly will assign a new tensor in memory as output. Here the expanded tensor will be created in memory after the operation.\n\nMy question is: in order to better optimise my code, how can I tell which operations/functions are likely to defer memory assignment? Is there any way that I can apply for example without the output being explicitly assigned in memory, like with ?"
    },
    {
        "link": "https://digitalocean.com/community/tutorials/pytorch-memory-multi-gpu-debugging",
        "document": "When working with deep learning models that use PyTorch, efficiently managing GPUs can make a huge difference in performance. Whether you’re training large models or running complex computations, using multiple GPUs can significantly speed up the process. However, handling multiple GPUs properly requires understanding different parallelism techniques, automating GPU selection, and troubleshooting memory issues.\n• How do you use multiple GPUs for your network, whether through data parallelism (splitting data across GPUs) or model parallelism (distributing model layers across GPUs)?\n• How to automate GPU selection so PyTorch assigns available GPUs to new objects.\n• How to diagnose and fix memory issues, ensuring smooth training and inference without running into out-of-memory errors.\n\nBy the end of this guide, you will understand how to optimize GPU usage in PyTorch.\n\nBefore diving into PyTorch 101: Memory Management and Using Multiple GPUs, ensure you have the following:\n• PyTorch is installed on your system.\n• Access to a CUDA-enabled GPU or multiple GPUs for testing (optional but recommended).\n\nEvery Tensor in PyTorch has a member function. It’s job is to put the tensor on which it’s called to a certain device whether it be the CPU or a certain GPU. Input to the function is a object which can initialised with either of the following inputs.\n• for putting it on GPU number 0. Similarly, if you want to put the tensors on\n\nGenerally, whenever you initialise a Tensor, it’s put on the CPU. You can move it to the GPU then. You can check whether a GPU is available or not by invoking the function.\n\nYou can also move a tensor to a certain GPU by giving it’s index as the argument to function.\n\nImportantly, the above piece of code is device agnostic, that is, you don’t have to separately change it for it to work on both GPU and the CPU.\n\nOne way to transfer tensors to GPUs is by using the function, where specifies the index of the GPU. If you call without arguments, the tensor will be placed on GPU 0 by default.\n\nAdditionally, the class provides and methods that can move the entire neural network to a specific device. Unlike tensors, when you use the method on an object, it’s sufficient to call the function directly; you do not need to assign the returned value.\n\nIt’s beneficial to explicitly choose which GPU a tensor is assigned to; however, we typically create many tensors during operations. We want these tensors to be automatically created on a specific device to minimize cross-device transfers that can slow down our code. PyTorch offers functionality to help us achieve this.\n\nOne useful function is . This function is only supported for GPU tensors and returns the index of the GPU on which a tensor is located. Using this function, we can determine the tensor device and automatically move any newly created tensor to that device.\n\nWe can also call while creating new Tensors. By default all tensors created by call are put on GPU 0, but this can be changed by the following statement.\n\nIf a tensor results from an operation between two operands on the same device, the resultant tensor will also be on that device. If the operands are on different devices, an error will occur.\n\nOne can also use the new functions that made their way to PyTorch in version 1.0. When a function like new_ones is called on a Tensor, it returns a new tensor of the same data type and on the same device as the tensor on which the new_ones function was invoked.\n\nA detailed list of functions can be found in PyTorch docs.\n\nThere are two ways we could make use of multiple GPUs.\n• Data Parallelism, where we divide batches into smaller batches and process these smaller batches in parallel on multiple GPUs.\n• Model Parallelism involves breaking the neural network into smaller subnetworks and then executing these subnetworks on different GPUs.\n\nData Parallelism in PyTorch is achieved through the class. You initialize a nn.DataParallel object with a object representing your network and a list of GPU IDs, across which the batches must be parallelized.\n\nNow, you can simply execute the object just like a .\n\nHowever, there are a few things I want to clarify. Although our data has to be parallelized over multiple GPUs, we have to initially store it on a single GPU.\n\nWe also need to make sure the DataParallel object is on that particular GPU. The syntax remains similar to what we did earlier with nn.Module.\n\nIn effect, the following diagram describes how works.\n\ntakes the input, splits it into smaller batches, replicates the neural network across all the devices, executes the pass and then collects the output back on the original GPU.\n\nOne issue with can be that it can put asymmetrical load on one GPU (the main node). There are generally two ways to circumvent these problem.\n• First, is to compute the loss during the forward pass. This makes sure at least the loss calculation phase is parallelised.\n• Another way is to implement a parallel loss function layer. This is beyond the scope of this article. However, for those interested I have given a link to a medium article detailing implementation of such a layer at the end of this article.\n\nModel parallelism means breaking your network into smaller subnetworks that you put on different GPUs. The main motivation is that your network might be too large to fit inside a single GPU.\n\nNote that model parallelism is often slower than data parallelism. Splitting a single network into multiple GPUs introduces dependencies between GPUs, which prevents them from running in a truly parallel way. The advantage one derives from model parallelism is not about speed but about the ability to run networks whose size is too large to fit on a single GPU.\n\nAs Figure B shows, Subnet 2 waits for Subnet 1 during the forward pass, while Subnet 1 waits for Subnet 2 during the backward pass.\n\nModel Parallelism with Dependencies Implementing Model parallelism in PyTorch is pretty easy as long as you remember two things.\n• The input and the network should always be on the same device.\n• and functions have autograd support, so your gradients can be copied from one GPU to another during backward pass. We will use the following piece of code to understand this better.\n\nIn the function we have put the sub-networks on GPUs 0 and 1 respectively.\n\nNotice in the function, we transfer the intermediate output from to GPU 1 before feeding it to . Since has autograd support, the loss backpropagated from will be copied to buffers of for further backpropagation.\n\nThis section will discuss how to diagnose memory issues and explore potential solutions if your network is utilizing more memory than necessary.\n\nWhile going out of memory may necessitate reducing batch size, one can do certain checks to ensure that memory usage is optimal.\n\nOne way to track GPU usage is by monitoring memory usage in a console with the nvidia-smi command. The problem with this approach is that peak GPU usage and out-of-memory happen so fast that you can’t quite pinpoint which part of your code is causing the memory overflow.\n\nFor this, we will use an extension called GPUtil, which you can install with pip by running the following command.\n\nJust put the second line wherever you want to see the GPU Utilization. By placing this statement in different places in the code, you can figure out what part is causing the network to go OOM.\n\nPyTorch has a pretty aggressive garbage collector. The garbage collection will free it as soon as a variable goes out of scope.\n\nIt’s important to note that Python does not enforce scoping rules as strictly as languages like C or C++. A variable remains in memory as long as references (or pointers) exist. This behavior is influenced by the fact that variables in Python do not need to be explicitly declared.\n\nAs a result, memory occupied by tensors holding your , tensors can still not be freed even once you are out of the training loop. Consider the following chunk of code.\n\nWhen executing the code snippet above, you’ll notice that the value of ‘i’ persists even after we exit the loop where it was initialized. Similarly, the tensors that store loss and output can remain in memory beyond the training loop. To properly release the memory occupied by these tensors, we should use the ‘del’ keyword.\n\nIn fact, as a general rule of thumb, if you are done with a tensor, you should del as it won’t be garbage collected unless there is no reference to it left.\n\nUsing Python Data Types Instead of 1-D Tensors\n\nIn our training loop, we frequently aggregate values to calculate metrics, with the most common example being the update of the running loss during each iteration. However, if this is not done carefully in PyTorch, it can result in excessive memory usage beyond what is necessary.\n\nConsider the following snippet of code.\n\nWe expect that in subsequent iterations, the reference to will be reassigned to the new and the object representing from the earlier representation will be freed. However, this doesn’t happen.\n\nWhy is that?\n\nSince is differentiable, the line creates a computation graph that includes an function node. During subsequent iterations, additional nodes are added to this graph, and no objects holding the values of are released. Typically, the memory allocated for a computation graph is freed when is called on it; however, in this case, there is no opportunity to call\n\nThe computation graph is formed when you keep adding the loss tensor to the variable To prevent the creation of any computation graph, the solution is to add a Python data type instead of a tensor to We merely replace the line total_loss += iter_loss with total_loss += iter_loss.item(). item returns the Python data type from a tensor containing single values.\n\nWhile PyTorch efficiently manages memory usage, it may not return memory to the operating system (OS) even after you delete your tensors. Instead, this memory is cached to facilitate the quick allocation of new tensors without requesting additional memory from the OS.\n\nThis behavior can pose a problem when using more than two processes in your workflow. If the first process completes its tasks but retains the GPU memory, it can lead to out-of-memory (OOM) errors when the second process is initiated.\n\nTo address this issue, you can include a specific command at the end of your code to ensure that memory is released properly.\n\nThis will make sure that the space held by the process is released.\n\nThe following output is produced when this code is executed on a Tesla K80\n\nBy default, PyTorch creates a computational graph during the forward pass. This process involves allocating buffers to store gradients and intermediate values necessary for computing the gradient during the backward pass.\n\nDuring the backward pass, most of these buffers are freed, except for those allocated for leaf variables. However, during inference, since there is no backward pass, these buffers remain allocated, which can lead to excessive memory usage over time.\n\nTo prevent this memory buildup when executing code that does not require backpropagation, always place it within a context manager.\n\nYou can utilize the benchmark instead of the standard benchmark. CuDNN offers significant optimizations that can reduce your memory usage, especially when the input size to your neural network is fixed. To enable the cuDNN benchmark, add the following lines at the beginning of your code.\n\nThe new RTX and Volta cards by nVidia support both 16-bit training and inference.\n\nHowever, the 16-bit training options must be taken with a pinch of salt.\n\nWhile 16-bit tensors can cut your GPU usage by almost half, they have a few issues.\n\nIn PyTorch, batch normalization layers can have convergence issues when using half-precision floats. If you encounter this problem, ensure that the batch normalization layers are set to\n\nAlso, you need to make sure when the output is passed through different layers in the function, the input to the batch norm layer is converted from to and then the output needs to be converted back to\n\nOne can find a good discussion of 16-bit training in PyTorch here.\n\nAdditionally, it is important to ensure that when the output is processed through various layers in the forward function, the input to the batch normalization layer is converted from float16 to float32. After processing, the output should be converted back to float16.\n\nBe cautious of overflow issues when using 16-bit floats. I once encountered an overflow while trying to store the union area of two bounding boxes (for calculating Intersection over Union, or IoU) in a It’s important to ensure that the value you intend to save in a float16 is realistic. Nvidia has recently released a PyTorch extension called Apex that facilitates numerically safe mixed precision training in PyTorch. I have provided the link to that at the end of the article.\n\nManaging memory efficiently in PyTorch, especially with multiple GPUs, is crucial for getting the best performance out of deep learning models. Understanding memory allocation, caching, and gradient accumulation can prevent out-of-memory errors and make the most of your hardware.\n\nWhen scaling up to multiple GPUs, using or helps distribute workloads effectively. However, each approach has trade-offs, so choosing the right one depends on your use case. Keeping an eye on memory usage with tools like and clearing unused tensors with can go a long way in avoiding performance bottlenecks. Ultimately, optimizing memory in PyTorch isn’t just about freeing up space—it’s about ensuring smooth, efficient training so your models run faster and more reliably. With the right practices, you can push the limits of your hardware while keeping things stable and efficient."
    },
    {
        "link": "https://discuss.pytorch.org/t/best-practices-for-gpu-memory-management-explicit-outputs-for-example/16250",
        "document": "Some operations in pyTorch can be done “inplace”, some you can specify an explicit output= variable for, and some you simply have to eat the Tensor/Variable it returns, creating a potentially large number of intermediary arrays that have to come and go every iteration.\n\nIn my lab code I encounter all of the above, but the impact difference is not trivial to estimate or measure if you don’t have alternatives.\n\nFor example, if I have an operation that outputs 1 GB of data, should I just let Torch allocate a new 1 GB GPU RAM chunk every time, and ‘del’ the previous data from the last iteration, or does that cause slowdowns? If a method with an explicit output is available, would that be better? Why do some methods have these output choices and some don’t?\n\nI’ve read some discussions here about this from a total memory point of view, where you simply need to ‘del’ some Tensors all time time. Does this have a performance impact for example does it create a GPU barrier?"
    },
    {
        "link": "https://docs.ncsa.illinois.edu/systems/hal/en/latest/user-guide/prog-env/gpu-memory.html",
        "document": "Manage GPU Memory When Using TensorFlow and PyTorch\n\nModern machine learning frameworks take advantage of GPUs to accelerate training/evaluation. Typically, the major platforms use NVIDIA CUDA to map deep learning graphs to operations that are then run on the GPU. CUDA requires the program to explicitly manage memory on the GPU and there are multiple strategies to do this. Unfortunately, TensorFlow does not release memory until the end of the program, and while PyTorch can release memory, it is difficult to ensure that it can and does. This can be side-stepped by using process isolation, which is applicable for both frameworks.\n\nBy default, TensorFlow tries to allocate as much memory as it can on the GPU. The theory is if the memory is allocated in one large block, subsequent creation of variables will be closer in memory and improve performance. This behavior can be tuned in TensorFlow using the API. We’ll point out a couple of functions here:\n• None List the GPUs currently usable by the python process:\n• None Set the specific devices TensorFlow will use:\n• None Set the object to use memory growth mode. In this mode, TensorFlow will only allocate the memory it needs, and grow it over time. TensorFlow set_memory_growth documentation.\n• None In a python context block, will restrict all tensors to being allocated only on the specified device:\n• None Get the memory usage of the device:\n• None An object that allows the user to set special requirements on a particular device: This can be used to restrict the amount of memory Tensorflow will use. TensorFlow LogicalDeviceConfiguration documentation. If TensorFlow can use multiple GPUs, you can restrict which one it uses in the following way: # Get a list of GPU devices gpus = tf.config.list_physical_devices('GPU') # Restrict Tensorflow to only use the first. tf.config.set_visible_devices(gpus[:1], device_type='GPU') Restrict How Much Memory TensorFlow Can Allocate on a GPU You can create a logical device with the maximum amount of memory you want TensorFlow to allocate using the following: # First, Get a list of GPU devices gpus = tf.config.list_physical_devices('GPU') # Restrict to only the first GPU. tf.config.set_visible_devices(gpus[:1], device_type='GPU') # Create a LogicalDevice with the appropriate memory limit log_dev_conf = tf.config.LogicalDeviceConfiguration( memory_limit=2*1024 # 2 GB ) # Apply the logical device configuration to the first GPU tf.config.set_logical_device_configuration( gpus[0], [log_dev_conf])\n\nCurrently, PyTorch has no mechanism to limit direct memory consumption, however PyTorch does have some mechanisms for monitoring memory consumption and clearing the GPU memory cache. If you are careful in deleting all python variables referencing CUDA memory, PyTorch will eventually garbage collect the memory. We review these methods here.\n• None Specify the amount of CUDA memory currently allocated on a given device:\n• None Release all unoccupied cached memory currently held by the caching allocator: See more at the “How can I release the unused gpu memory?” PyTorch discussion.\n\nTo ensure you can clean up any GPU memory when you’re finished, you can also try process isolation. This requires you to define a pickle-able python method which you can then send to a separate python process with multiprocessing. Upon completion, the other process will terminate and clean up its memory ensuring you don’t leave any unneeded variables behind. This is the strategy employed by DRYML, which provides a function decorator to manage the process creation and retrieval of results. We’ll present a simple example here showing how you might do it. # The following will not work in an interactive python shell. It must be run as a standalone script. # Define the function in which we want to allocate memory. def memory_consumer(): import tensorflow as tf test_array = tf.random.uniform((5,5)) av = tf.reduce_mean(test_array) print(av) if __name__ == \"__main__\": import multiprocessing as mp # Set the multiprocessing start method as 'spawn' since we're interested in only consuming memory for the problem at hand. 'fork' copies all current variables (which may be a lot) mp.set_start_method('spawn') # Call that function using a separate process p = mp.Process(target=memory_consumer) # Start the process p.start() # Wait for the process to complete by joining. p.join() There are other methods such as multiprocessing pools which handle variable creation and return value management for you. Learn more about how DRYML does process isolation in the DRYML GitHub repository."
    }
]