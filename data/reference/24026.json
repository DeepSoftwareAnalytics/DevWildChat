[
    {
        "link": "https://threejs.org/docs#api/materials/ShaderMaterial.uniforms",
        "document": ""
    },
    {
        "link": "https://discourse.threejs.org/t/updating-shadermaterial-uniform-not-rendering/22324",
        "document": "Hi everyone, this is one of my first times using Three.js so any help would be appericated!\n\nIssue:\n\n I am using Robin Delas hover effect: hover-effect/1.gif at b7779968babc482ce4b6ddc7b29273a4a220b104 · robin-dela/hover-effect · GitHub\n\n I changed the code to use this effect on click instead of hover for a carousel. I was able to create the effect for the swapping of two images (3rd is a displacement for effect) already rendered.\n\nI would like to update the image src to the image not shown, so I can create a roating effect based on the current slide my carousel is on. If you watch the video, once I initally use the effect I would be showing img2, and when I click again I would go back to first but I would change its source to display the 3rd background image to match its position in the carousel.\n\nI was able to change the source with:\n\n mat.uniforms.texture1.value = ‘images/test-services-3.jpg’;\n\nBut it displays a black screen… (Logically I think its something to do with rendering the texture again, but I cannot figure it out)\n\nChanges would take place inside\n\n function transitionOut() {\n\n }"
    },
    {
        "link": "https://threejs.org/docs/api/en/materials/ShaderMaterial.html",
        "document": "A material rendered with custom shaders. A shader is a small program written in [link:https://www.khronos.org/files/opengles_shading_language.pdf GLSL] that runs on the GPU. You may want to use a custom shader if you need to:\n• implement an effect not included with any of the built-in [page:Material materials]\n• combine many objects into a single [page:BufferGeometry] in order to improve performance\n• A `ShaderMaterial` will only be rendered properly by [page:WebGLRenderer], since the GLSL code in the [link:https://en.wikipedia.org/wiki/Shader#Vertex_shaders vertexShader] and [link:https://en.wikipedia.org/wiki/Shader#Pixel_shaders fragmentShader] properties must be compiled and run on the GPU using WebGL.\n• As of THREE r72, directly assigning attributes in a ShaderMaterial is no longer supported. A [page:BufferGeometry] instance must be used instead, using [page:BufferAttribute] instances to define custom attributes.\n• As of THREE r77, [page:WebGLRenderTarget] or [page:WebGLCubeRenderTarget] instances are no longer supposed to be used as uniforms. Their [page:Texture texture] property must be used instead.\n• Built in attributes and uniforms are passed to the shaders along with your code. If you don't want the [page:WebGLProgram] to add anything to your shader code, you can use [page:RawShaderMaterial] instead of this class.\n• You can use the directive #pragma unroll_loop_start and #pragma unroll_loop_end in order to unroll a `for` loop in GLSL by the shader preprocessor. The directive has to be placed right above the loop. The loop formatting has to correspond to a defined standard.\n• The loop has to be [link:https://en.wikipedia.org/wiki/Normalized_loop normalized].\n• The loop variable has to be *i*.\n• The value `UNROLLED_LOOP_INDEX` will be replaced with the explicitly value of *i* for the given iteration and can be used in preprocessor statements.\n\nYou can specify two different types of shaders for each material:\n• The vertex shader runs first; it receives `attributes`, calculates / manipulates the position of each individual vertex, and passes additional data (`varying`s) to the fragment shader.\n• The fragment ( or pixel ) shader runs second; it sets the color of each individual \"fragment\" (pixel) rendered to the screen. There are three types of variables in shaders: uniforms, attributes, and varyings:\n• `Uniforms` are variables that have the same value for all vertices - lighting, fog, and shadow maps are examples of data that would be stored in uniforms. Uniforms can be accessed by both the vertex shader and the fragment shader.\n• `Attributes` are variables associated with each vertex---for instance, the vertex position, face normal, and vertex color are all examples of data that would be stored in attributes. Attributes can `only` be accessed within the vertex shader.\n• `Varyings` are variables that are passed from the vertex shader to the fragment shader. For each fragment, the value of each varying will be smoothly interpolated from the values of adjacent vertices. Note that `within` the shader itself, uniforms and attributes act like constants; you can only modify their values by passing different values to the buffers from your JavaScript code.\n\nThe [page:WebGLRenderer] provides many attributes and uniforms to shaders by default; definitions of these variables are prepended to your `fragmentShader` and `vertexShader` code by the [page:WebGLProgram] when the shader is compiled; you don't need to declare them yourself. See [page:WebGLProgram] for details of these variables. Some of these uniforms or attributes (e.g. those pertaining lighting, fog, etc.) require properties to be set on the material in order for [page:WebGLRenderer] to copy the appropriate values to the GPU - make sure to set these flags if you want to use these features in your own shader. If you don't want [page:WebGLProgram] to add anything to your shader code, you can use [page:RawShaderMaterial] instead of this class.\n\nBoth custom attributes and uniforms must be declared in your GLSL shader code (within `vertexShader` and/or `fragmentShader`). Custom uniforms must be defined in `both` the `uniforms` property of your `ShaderMaterial`, whereas any custom attributes must be defined via [page:BufferAttribute] instances. Note that `varying`s only need to be declared within the shader code (not within the material). To declare a custom attribute, please reference the [page:BufferGeometry] page for an overview, and the [page:BufferAttribute] page for a detailed look at the `BufferAttribute` API. When creating your attributes, each typed array that you create to hold your attribute's data must be a multiple of your data type's size. For example, if your attribute is a [page:Vector3 THREE.Vector3] type, and you have 3000 vertices in your [page:BufferGeometry], your typed array value must be created with a length of 3000 * 3, or 9000 (one value per-component). A table of each data type's size is shown below for reference: Note that attribute buffers are `not` refreshed automatically when their values change. To update custom attributes, set the `needsUpdate` flag to true on the [page:BufferAttribute] of the geometry (see [page:BufferGeometry] for further details). To declare a custom [page:Uniform], use the `uniforms` property: uniforms: { time: { value: 1.0 }, resolution: { value: new THREE.Vector2() } } You're recommended to update custom [page:Uniform] values depending on [page:Object3D object] and [page:Camera camera] in [page:Object3D.onBeforeRender] because [page:Material] can be shared among [page:Mesh meshes], [page:Matrix4 matrixWorld] of [page:Scene] and [page:Camera] are updated in [page:WebGLRenderer.render], and some effects render a [page:Scene scene] with their own private [page:Camera cameras].\n\n[page:Object parameters] - (optional) an object with one or more properties defining the material's appearance. Any property of the material (including any property inherited from [page:Material]) can be passed in here.\n\nSee the base [page:Material] class for common properties.\n\nDefines whether this material supports clipping; true to let the renderer pass the clippingPlanes uniform. Default is false.\n\nWhen the rendered geometry doesn't include these attributes but the material does, these default values will be passed to the shaders. This avoids errors when buffer data is missing.\n\nDefines custom constants using `#define` directives within the GLSL code for both the vertex shader and the fragment shader; each key/value pair yields another directive: yields the lines in the GLSL code.\n\nAn object with the following properties: this.extensions = { clipCullDistance: false, // set to use vertex shader clipping multiDraw: false // set to use vertex shader multi_draw / enable gl_DrawID };\n\nDefine whether the material color is affected by global fog settings; true to pass fog uniforms to the shader. Default is false.\n\nFragment shader GLSL code. This is the actual code for the shader. In the example above, the `vertexShader` and `fragmentShader` code is extracted from the DOM; it could be passed as a string directly or loaded via AJAX instead.\n\nDefines the GLSL version of custom shader code. Valid values are `THREE.GLSL1` or `THREE.GLSL3`. Default is `null`.\n\nIf set, this calls [link:https://developer.mozilla.org/en-US/docs/Web/API/WebGLRenderingContext/bindAttribLocation gl.bindAttribLocation] to bind a generic vertex index to an attribute variable. Default is undefined.\n\nRead-only flag to check if a given object is of type [name].\n\nDefines whether this material uses lighting; true to pass uniform data related to lighting to this shader. Default is false.\n\nControls wireframe thickness. Default is `1`.\n\n\n\n Due to limitations of the [link:https://www.khronos.org/registry/OpenGL/specs/gl/glspec46.core.pdf OpenGL Core Profile] with the [page:WebGLRenderer WebGL] renderer on most platforms linewidth will always be `1` regardless of the set value.\n\nDefine whether the material is rendered with flat shading. Default is false.\n\nAn object of the form: specifying the uniforms to be passed to the shader code; keys are uniform names, values are definitions of the form where `value` is the value of the uniform. Names must match the name of the uniform, as defined in the GLSL code. Note that uniforms are refreshed on every frame, so updating the value of the uniform will immediately update the value available to the GLSL code.\n\nCan be used to force a uniform update while changing uniforms in [page:Object3D.onBeforeRender](). Default is `false`.\n\nDefines whether vertex coloring is used. Default is `false`.\n\nVertex shader GLSL code. This is the actual code for the shader. In the example above, the `vertexShader` and `fragmentShader` code is extracted from the DOM; it could be passed as a string directly or loaded via AJAX instead.\n\nRender geometry as wireframe (using GL_LINES instead of GL_TRIANGLES). Default is false (i.e. render as flat polygons).\n\nControls wireframe thickness. Default is `1`.\n\n\n\n Due to limitations of the [link:https://www.khronos.org/registry/OpenGL/specs/gl/glspec46.core.pdf OpenGL Core Profile] with the [page:WebGLRenderer WebGL] renderer on most platforms linewidth will always be `1` regardless of the set value.\n\nSee the base [page:Material] class for common methods.\n\nGenerates a shallow copy of this material. Note that the vertexShader and fragmentShader are copied `by reference`, as are the definitions of the `attributes`; this means that clones of the material will share the same compiled [page:WebGLProgram]. However, the `uniforms` are copied `by value`, which allows you to have different sets of uniforms for different copies of the material."
    },
    {
        "link": "https://stackoverflow.com/questions/29928973/how-do-you-update-a-uniform-in-three-js",
        "document": "For those not using THREE.ShaderMaterial\n\nIf for example you are modifying a material beyond just THREE.ShaderMaterial, say THREE.MeshStandardMaterial, via edits to its fragment shader, you will need to do things somewhat differently.\n\nIn order to modify this shader's uniforms, WITHOUT requiring recompilation, you will need to store a reference to that shader within the onBeforeCompile callback, then access those uniforms via that stored reference's uniforms.\n\nFirst plug into THREE.Material's onBeforeCompile hook transferring the values stored somewhere (in my case a uniforms dictionary created in the constructor) into the shader's uniforms. This is important because shader compilation takes place prior to first usage. Store a reference to the shader in your class for access. Do any of your other work to the fragment shader using these uniforms as you see fit.\n\nGet or set the property from the stored uniforms if not compiled yet, otherwise use the shader's own uniform value, which only exists after compilation."
    },
    {
        "link": "https://github.com/drcmda/react-three-fiber/issues/53",
        "document": ""
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Web/API/Touch_events/Using_Touch_Events",
        "document": ""
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Web/API/Touch_events",
        "document": ""
    },
    {
        "link": "https://medium.com/@mysteryweevil/mastering-touch-events-and-mobile-event-handling-in-javascript-4bca8d448abf",
        "document": "As the mobile landscape continues to evolve, understanding how to effectively handle touch events and mobile-specific interactions has become increasingly crucial for web developers.\n\nIn this article, we’ll explore the essential concepts and techniques you need to know to create seamless and responsive mobile experiences using JavaScript.\n\nIn the world of mobile, traditional mouse events like , , and are often not enough to capture the nuances of touch-based interactions. Instead, we have a set of touch-specific events that allow us to track and respond to user gestures on mobile devices. Here are some of the most commonly used touch events:\n• : Fired when a finger is dragged across the screen.\n• : Fired when a finger is lifted off the screen.\n• : Fired when a touch event is interrupted, such as when the user's finger leaves the screen."
    },
    {
        "link": "https://codeguage.com/courses/js/touch-events-basics",
        "document": "Before we can start to create touch-powered JavaScript applications, we need to understand all the different events fired throughout the lifecycle of a touch point in contact with a touch surface. In particular, there are four standard events to consider as follows:\n• — fired when a touch point comes into contact with the touch surface.\n• — fired constantly as the touch point moves across the touch surface.\n• — an implementation-dependent event which typically in a case where the touch couldn't be correctly detected. The and events could be thought of as and . That is, when the touch point goes down onto the screen, fires, and when it goes up away from the screen, fires. and , please refer to To read more aboutand, please refer to JavaScript Mouse Events Each of these events can be handled on any given element or on the whole document. They can be handled directly via the category of properties or by using the method. Even the corresponding HTML attributes are provided, however we won't use them since they're not a good development practice at all. Hence, to handle the event on the whole element, we could use any of the following statements: We'll go with the latter approach, i.e. using , since the former is not good practice once again, especially when we want to assign multiple event handlers to an event on a single target. Using also has the benefit of setting up passive listeners which is not available in the mere -like properties. With this done, let's now create a very very elementary program that notifies the user when a touch point comes into contact with the screen and when it leaves it. First we have the following HTML and CSS to create a considerably large touch area with a light grey background color, and an element to showcase the output made by the script: And then we have the following script to enable touch interaction on it: The workflow of this program is extremely basic. When a touch point touches the screen in the area defined by , a event is emitted and likewise the handler above outputs 'Touch begins'. Similarly, when the touch point leaves the surface, a event fires and likewise the handler above outputs 'Touch ends'. It's extremely important to take note of the way works. That is, it only fires when the touch point leaves the screen (either by dragging it outside the physical bounds of the screen or by lifting the touch point), NOT when it leaves the element. It doesn't fire when the touch point moves out of the element on which the event is handled. For instance, in the example above, try initiating a touch inside , then taking your finger all the way out of the element, and then finally leaving the screen. You'll still notice the event fire and that's because it fires only when the physical touch action comes to an end, regardless of whether it happens inside or outside the respective element. The events and could've been named and respectively, but does that really make sense?\n\n\n\nFor a mouse, yes the names and make sense, owing to the nature of the buttons on the mouse that literally go down and up during a mouse interaction. Likewise, we have the events and .\n\n\n\nBut and don't make much sense, if at all, when thought from the perspective of a touch point, be that a finger or a stylus. Let's consider another example, this time a bit more complex, utilizing the event: The idea is to create a circle for each of the events , and , and display them one after another inside a element. The circle for should be green while the circle for should be red. All the circles in between should be yellow. Now we suggest you to try developing this program on your own first. It's superbly simple to code. Alright, assuming that you've given it a try, let's code the program together. First we have the following HTML and CSS. The HTML is the exact same as before while the CSS has one addition i.e. styles for elements which will eventually be added inside : Now, let's talk about the JavaScript code. One action that's common to each event's handler is creating a circle. Moreover, there seems to be a desire to give a different color to each circle based on the event fired. This hints us at defining a function to create circles that takes in one argument which is the background color to apply to the circle. Following is the definition of the function which we call : The code is pretty much self-explanatory — first a element node is created, then the class given to it, followed by the desired background color, and finally placed right inside the element. With this function at hand, now we are only left to call it inside the three event handlers. This is accomplished below: Now we could go on and create many such programs but they will all be boring until and unless we use the information captured by each of the fired events. That's where the interface comes into the game. It's time to explore it...\n\nWhenever a touch event is fired, the event object passed in to the respective handler function is a instance. As with all event objects, it contains a lot of useful information about the fired event. inherits from the interface as it represents an event that takes place in the user interface, which in turn (as we know) inherits from the interface. Shown below are some of the common properties: You might be wondering why there aren't any / , or / , or / properties in here like in a object. After all, how could we track the touch point(s) moving across the touch surface without these properties? Here's the simple answer: a object works differently than in that it provides information for the mouse pointer which can only be one. Likewise, it directly holds the respective properties / , / , and / . In contrast, provides information for an entirely different medium i.e. a touch point such as a finger or stylus over a touch surface. It's quite common for touch surfaces to have multi-touch support where we could have more than one touch point working on the surface simultaneously. Hence, for a object to provide information for all the touch points, it can't obviously just have one pair of properties / , or / , or / . Rather, it ought to provide a list containing elements, each of which represents a given touch point and thereby carries these properties on it. This is where the two interfaces and step into the equation. is merely a list containing instances, each of which represents a given touch point on the touch surface. The following three properties of a object are instances: , and . We'll cover and all three of these instances in next chapter but for now, it's worthwhile to know a little bit about them.\n• is a list of objects representing all the touch points currently in contact with the touch surface.\n• is a list of objects representing the touch points that fired the current event.\n• is a list of objects representing the touch points that have the same target element as the target of the touch point which fired the current event. For now, we'll focus on the first one, i.e. . It's the simplest of all.\n\nAs stated before, is a property of the object passed into a touch event handler function. is a instance representing all of the touch points that are currently in contact with the touch surface. Each element in the list is a instance with numerous properties on it. The most useful for us are There are a couple more useful properties to consider but we'll leave them for the next-to-next chapter on JavaScript Touch Events — The interface. Anyways, it's time to consider an example using the object. In the code below, we listen to the two events, and , and display the co-ordinates of the touch point out on the document as they occur. Note that we're assuming that the and elements have already been set up in the HTML, as we did in the previous code snippets. As per the reason for not listening to the event, it's because on the list is empty, given that the whole touch interaction involved just one touch point. There is a different way to tract the co-ordinates of the touch point upon which we'll see later on below. As you can see, working with touch events isn't as difficult as it might seem. In the example below, we demonstrate yet another touch application. This time, we showcase the total number of contact points on the touch surface using the property of the object. The idea is really simple: when a new contact is made i.e. when gets fired and when an existing contact is removed from the surface i.e. when gets fired, we output . For either event, holds as many elements as there are touch points currently on the surface. Likewise, its property would provide us with this exact number. Shown below is the complete code: Guess what? Our inventory of examples hasn't ended yet. It's time for another example, this time to display a circle right at the point of contact on the touch surface. What we mean is demonstrated below: Whenever we put our finger in contact with touch surface on the document's viewport, a circle is shown right at that point. When the finger leaves contact, the circle remains there. Note that we are assuming that only one touch point is interacting in the example above. With multi-touch interaction, the program won't work because it isn't made to handle that. To handle multi-touch interaction, we ought to use the object instead of . We'll cover it in the next chapter. How to accomplish this task? What we need is once again a function to create a circle but now instead of providing it the desired background color (as we did before), we'll need to provide the x and y co-ordinates of the given touch point so that the circle (which would obviously be fixed positioned) could correctly be positioned. Here we do require a little bit of logic and math to create the program. But don't worry; it won't be a second course on calculus, but just some very elementary arithmetic. One more thing that's quite clear is that we'll need to set up event handlers on the whole document instead of on just one single element therein. For this, we'll use the object. Instead of , we could also use , or . They are all the same as far as the handling of each event goes. So to begin with, here's the CSS code for the class which will ultimately be given to each element created at the location of the touch point upon : The JavaScript is also very basic — just a function, that creates a circle, handles the event. Perhaps the most important statements to look over here are on lines 4 and 5. They serve to correctly position the circle right at the place where the touch point initiated the event. The reason for subtracting from both and is so that the mid-point of the circle created can coincide with the mid-point of the touch point. And that's it. Simple, as always.\n\nIn this section, we see how to use the events and and the object to detect whether the user performed a swipe. A swipe gesture is typically taken as a rapid movement of a touch point in roughly one direction on the touch surface before leaving contact with it. There are generally four kinds of swipe gestures following from the four typical directions used in computing: swipe-up, swipe-down, swipe-left and swipe-right. For instance, swipe-up means that we start off from the bottom of the touch surface and then move the touch point upwards swiftly before leaving contact with the surface. Swiping gestures are performed all day long on touch devices — they are the most basic and most intuitive of all kinds of complex touch gestures performed. Now the question is, how to detect a swipe action in JavaScript? Or let's refine the question further to tackle a more specific problem — how to detect a swipe-left action in JavaScript? We need essentially two things to detect a swipe-left gesture:\n• Check if the direction of the gesture is towards the left.\n• Check if the gesture is quick enough. How to accomplish the first of these? Well, let's see it with the help of some examples. Suppose that we start off a touch gesture at the filled grey mark below and then end it right at the hollow mark. The direction is shown with a line connecting the marks. Does this gesture seem to go leftwards? Well, it surely doesn't. Instead, it's going to the top-right. Now consider the following: Does this seem to go leftwards? Once again, no. It's going to the top-left, whereas we want something close to going strictly to the left. Time for another example: Does this seem to go leftwards? Well, yes. The gesture seems to be in a perfect straight line towards the left. And here's the final example: Does this seem to go leftwards? Well, yes. Although the gesture doesn't draw a perfectly horizontal 180° line, it still constitutes a swipe-left. That's because we allow for a certain level of deflection in the gesture either above the starting point or below it. After all, no one performs swipe-left gestures in perfect 180° angles! Now based on all these examples, it's clear that we need to make two computations in order to detect whether a touch gesture is really going to the left:\n• The change in the x co-ordinate at the end of the gesture.\n• The change in the y co-ordinate at the end of the gesture. Both the readings for these co-ordinates are first obtained on the event and then on the event. It's during that the change is calculated by subtracting the previous values from the latest values. If the change in the x co-ordinate is greater than a given threshold value and the change in the y co-ordinate is less than a maximum threshold value, then the touch gesture is considered to have met the first condition of a swipe-left gesture. But how do we figure out the co-ordinates of the touch point on ? As we know, represents a list of all the touch points currently in contact with the touch surface, however in this case our touch point isn't on the surface. How could we obtain information for something that just isn't there? Well, surely we could, all thanks to the object. is another instance that exists on a object, however it's a little bit different than . The exact detail and examples of the difference between these are both left for the next chapter, but let's try to summarize it for the sake of this example. is a instance representing all the touch points that triggered the current event. In the case of , contains all those touch points that left the touch surface to ultimately cause the event to be fired. Perfect. Our problem has been effectively solved. Now it's time to look into the second condition, which is regarding timing. Here's the same example as before: It's possible to perform this gesture in two ways: one which spans more than 1 second and one which spans less than 300ms. Only the latter here constitutes a swipe; not the former. That's because a swipe is taken to be a touch gesture that happens quickly, not something that happens in units of seconds. So how to determine the time taken by the gesture in JavaScript? Well, the answer lies in working with dates and times in JavaScript which happens via the interface. It's covered in the last unit of this course where we showcase all the miscellaneous concepts of JavaScript. The chapter doesn't require any of the following units, hence you could go there right now and learn about the interface if you haven't already, before continuing on reading below. Coming back to the discussion, we can use the constructor to time the touch gesture and then check whether the time taken is less than the given amount or not. As with computing the change in the co-ordinates of the touch point, first a reading for the time is taken on and then on . The difference between these represents the time span of the gesture, in milliseconds. If the time span is less than 300ms, the second condition for a swipe-left is met. Using 300 here isn't a fixed value proposed by some kind of a standard. It's just a good approximation of how fast should a gesture be in order to constitute it as a swipe. If you want to, you could scale this value up to allow for slow gestures to be treated as swipes, or maybe even scale it down to get the action to be performed even faster. With the logic for both the conditions of a swipe understood, it's time to start coding. One thing to note before we start coding is that both and can hold multiple elements. However, we want to focus on only one single touch point in our simple detector. Henceforth, we'll directly access the first element of both these lists and use that element in our computations. Alright, everything's set by now and so it's time to write the program. For the HTML and CSS, we're using the same setup as before, i.e. with the and elements. Shown below is the JavaScript: var touchRegionElement = document.getElementById('touch-region'); var outputElement = document.getElementById('output'); var initialX, initialY, initialTime; touchRegionElement.addEventListener('touchstart', function(e) { initialX = e.touches[0].clientX; initialY = e.touches[0].clientY; initialTime = new Date(); }); touchRegionElement.addEventListener('touchend', function(e) { var deltaX = e.changedTouches[0].clientX - initialX; var deltaY = Math.abs(e.changedTouches[0].clientY - initialY); var deltaTime = new Date() - initialTime; if (deltaX <= -30 && deltaY <= 100 && deltaTime <= 300) { outputElement.innerText = 'Swipe-left detected'; outputElement.style.color = 'green'; } else { outputElement.innerText = 'Not a swipe-left'; outputElement.style.color = 'red'; } }); The idea is that when a swipe-left gesture is detected, the message 'Swipe left detected' is output on the document with a green color to signal success. However, if this is not the case, then the message 'Not a swipe-left' is output with a red color."
    },
    {
        "link": "https://sencha.com/blog/event-handling-in-javascript-a-practical-guide-with-examples",
        "document": "Back in the day, websites used to be static, meaning users could only view the content but not interact with it. However, we can now create highly interactive user interfaces thanks to JavaScript and JS frameworks. Specifically, event handlers in JavaScript are what allow us to build dynamic web pages and deliver interactive experiences. In the modern web development landscape, events are essentially user actions that occur as a result of user interaction with the web page, such as submitting a form, clicking a button, playing a video on the web page, minimizing the browser window, etc. Event handling allows developers to verify and handle these actions to deliver a more responsive and engaging user experience.\n\nHence, understanding how events work and how to handle them efficiently is essential for every developer looking to create modern web applications. This article will discuss all the ins and outs of event handling in JavaScript. We’ll also briefly discuss how a good JavaScript framework like Ext JS handles events.\n\nEvents are essentially the actions that occur on a web app due to user interaction, such as clicking a button. In JavaScript, when an event occurs, the app fires the event, which is kind of a signal that an event has occurred. The app then automatically responds to the user in the form of output, thanks to event handlers in JavaScript. An event handler is essentially a function with a block of code that is executed or triggered when a specific event fires.\n\nSometimes, when an event occurs, it triggers multiple events. This is because web elements in an app are often nested. This is where event propagation comes in. Event propagation involves capturing and bubbling phases as the event travels across the DOM hierarchy. We’ll discuss these phases later in the article.\n\nThere are common types of events:\n• Keyboard/touch events: Occur when a user presses or releases a key on the keyboard or performs an action with a touch-enabled smartphone, laptop or tablet.\n• Click events: Fires when a user clicks on a button or other such web element.\n• Mouse hover events: These events are fired when a user performs an action with the mouse, such as scrolling a page or moving the cursor.\n• Form/submit events: Triggered when a user submits a form, modifies it, or resets it.\n• Drag and drop events: Occurs when a user drags and drops an element on the web page, such as dragging and dropping an image on a file uploader.\n\nAn event listener is essentially a JavaScript function that waits for a specific event to occur and then executes a callback function to respond to that event. Event listeners and event handlers are often considered the same thing. However, in essence, they work together to respond to an event. As the name suggests, the listener listens for the event, and the handler is the code that is executed in response to that event.\n\nThere are two common built-in event listener methods in JavaScript: addEventListener and removeEventListener. The addEventListener() method enables us to attach an event handler to an element. We can also add multiple event handlers to an element. removeEventListener() allows us to remove an event listener/handler from a specific element.\n\nWhen an event occurs, it belongs to a specific event object. The event object is essentially the argument passed into the callback/event handler function. It provides information about the event, such as the target element, the type of event, etc. It also contains additional properties for the specific event type.\n• target: Represents the element that fired the event.\n• type: Tells about the specific type of the event, such as click or submit\n• keyCode: Used for keyboard events. It contains the Unicode value of the key pressed by the user\n\nHere is an example code demonstrating the use of the event object (Click event):\n\nBased on the concepts we discussed in the previous sections, here is an example for creating a simple button-click event:\n\nHere is a basic example demonstrating how to handle form submissions:\n\nWeb browsers often have a default behavior for certain events. When such an event occurs, the browser’s default behavior is triggered in response to that event. preventDefault() provides us with a way to stop or prevent this default behavior.\n\nFor instance, when a user submits a form, the browser automatically initializes a request to the server. This results in page reload or navigation to a new page, affecting the user experience. Developers can use preventDefault() to stop this default behavior and handle form submission asynchronously without causing a page to reload. For example, in the above code, we’ve used preventDefault() to stop or prevent the default form submission behavior.\n\nEvent delegation in JavaScript is an advanced technique for handling events more efficiently. In event delegation, we add or attach an event listener/listeners to a common parent element. This way, we don’t have to attach the event listener to each element separately. Events are processed and monitored as they traverse the DOM hierarchy. Event delegation is common in popular javascript frameworks\n\nHere is an example of event delegation:\n\nHandling keyboard events, such as key down and key up, allows us to:\n• Respond to user interactions/inputs with the keyboard\n\nKey Down and Key Up are two main types of mouse events. A key-down event occurs when a user presses a key on the keyboard. A key-up event is triggered when a user releases the key after it is pressed down.\n\nHere is an example code for handling a key-down event:\n\nHere is an example code for handling a key-up event:\n\nHandling touch and mobile events to create a responsive and touch-friendly design, providing an intuitive way to interact with the web app.\n\nHere is an example code for Touchstart, Touchmove, and Touchend:\n• Gesture events, such as gesturestart, gesturechange, and gestureend. These events are used for gestures like pinch-zoom.\n• orientationchange event used for detecting changes in device orientation.\n\nAlso Read: Angular vs Ext JS: Which JavaScript Framework Should You Use?\n\nAs aforementioned, event bubbling and capturing are a part of the event propagation process. In event bubbling, the event starts from the same target element that fired the event. It then bubbles up or propagates through its parent and ancestor elements in the DOM till it reaches the root element. This allows you to handle the event in a parent element instead of the target element. Event bubbling is the default event behaviour on elements.\n\nIn event capturing, the event traverses from the outermost parent or ancestor element to the target element. It is also called event trickling.\n\nJavaScript also allows you to create and dispatch custom events designed to meet your specific application needs. For instance, you can create custom events for cross-component state management.\n\nHere is how to create a custom event:\n\nHere is how to dispatch the event:\n• Combine multiple events that trigger similar actions into one event listener.\n• Use event capturing only when needed. Otherwise, use bubbling.\n\nExt JS is a leading Javascript framework for creating high-performance web and mobile applications. It offers over 140+ pre-built components and supports MVVM architecture and two-way data binding. Events are a core concept in the Ext JS framework that enables your code to react to changes in your app. Here is an example code for button-click event in Ext JS:\n\nYou can learn more about handling events in Ext JS here.\n\nTransform your digital landscape with Sencha: Master JavaScript frameworks for unrivalled web development excellence\n\nIn the web development process, events refer to user actions, such as such as clicking a button, minimizing the browser window, or submitting a form. Event handling in JavaScript and JavaScript frameworks allows us to respond to user actions and interactions and create dynamic and interactive websites. This article explores various concerts related to event handling in JavaScript with examples.\n\nWhat is event handling in JavaScript?\n\nEvent handling in JS refers to using event listeners to wait for an event to occur on an element and responding to that event using event handlers or callback functions.\n\nHow do I attach an event listener to an element?\n\nYou can use JavaScript’s built-in addEventListener() method to attach an event to an element.\n\nWhat is the event object in JavaScript?\n\nThe event object in JS is essentially the argument passed into the callback/event handler function. It provides valuable information about the event, such as the target element, the type of event, etc.\n\nWhat are the most popular JavaScript frameworks?\n\nBest JavaScript frameworks and JavaScript libraries include Ext JS, React and Angular. Ext JS offers 140+ high-performance pre-built components for developing web applications quickly. React is another popular JavaScript framework known for creating customized and reusable elements and virtual DOM. Angular is another open-source JavaScript framework that utilizes component-based architecture and allows developers to build high-performance single-page applications."
    }
]