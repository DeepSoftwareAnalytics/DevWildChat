[
    {
        "link": "https://postgresql.org/docs/current/functions-json.html",
        "document": "To provide native support for JSON data types within the SQL environment, PostgreSQL implements the SQL/JSON data model. This model comprises sequences of items. Each item can hold SQL scalar values, with an additional SQL/JSON null value, and composite data structures that use JSON arrays and objects. The model is a formalization of the implied data model in the JSON specification RFC 7159.\n\nSQL/JSON allows you to handle JSON data alongside regular SQL data, with transaction support, including:\n\nTo learn more about the SQL/JSON standard, see [sqltr-19075-6]. For details on JSON types supported in PostgreSQL, see Section 8.14.\n\nTable 9.45 shows the operators that are available for use with JSON data types (see Section 8.14). In addition, the usual comparison operators shown in Table 9.1 are available for , though not for . The comparison operators follow the ordering rules for B-tree operations outlined in Section 8.14.4. See also Section 9.21 for the aggregate function which aggregates record values as JSON, the aggregate function which aggregates pairs of values into a JSON object, and their equivalents, and . Extracts 'th element of JSON array (array elements are indexed from zero, but negative integers count from the end). Extracts JSON object field with the given key. Extracts JSON object field with the given key, as . Extracts JSON sub-object at the specified path, where path elements can be either field keys or array indexes. Extracts JSON sub-object at the specified path as . \n\n The field/element/path extraction operators return NULL, rather than failing, if the JSON input does not have the right structure to match the request; for example if no such key or array element exists. Some further operators exist only for , as shown in Table 9.46. Section 8.14.4 describes how these operators can be used to effectively search indexed data. Does the first JSON value contain the second? (See Section 8.14.3 for details about containment.) Is the first JSON value contained in the second? Does the text string exist as a top-level key or array element within the JSON value? Do any of the strings in the text array exist as top-level keys or array elements? Do all of the strings in the text array exist as top-level keys or array elements? Concatenates two values. Concatenating two arrays generates an array containing all the elements of each input. Concatenating two objects generates an object containing the union of their keys, taking the second object's value when there are duplicate keys. All other cases are treated by converting a non-array input into a single-element array, and then proceeding as for two arrays. Does not operate recursively: only the top-level array or object structure is merged. To append an array to another array as a single entry, wrap it in an additional layer of array, for example: Deletes a key (and its value) from a JSON object, or matching string value(s) from a JSON array. Deletes all matching keys or array elements from the left operand. Deletes the array element with specified index (negative integers count from the end). Throws an error if JSON value is not an array. Deletes the field or array element at the specified path, where path elements can be either field keys or array indexes. Does JSON path return any item for the specified JSON value? (This is useful only with SQL-standard JSON path expressions, not predicate check expressions, since those always return a value.) Returns the result of a JSON path predicate check for the specified JSON value. (This is useful only with predicate check expressions, not SQL-standard JSON path expressions, since it will return if the path result is not a single boolean value.) \n\n The operators and suppress the following errors: missing object field or array element, unexpected JSON item type, datetime and numeric errors. The -related functions described below can also be told to suppress these types of errors. This behavior might be helpful when searching JSON document collections of varying structure. Table 9.47 shows the functions that are available for constructing and values. Some functions in this table have a clause, which specifies the data type returned. It must be one of , , , a character string type ( , , or ), or a type that can be cast to . By default, the type is returned. Converts any SQL value to or . Arrays and composites are converted recursively to arrays and objects (multidimensional arrays become arrays of arrays in JSON). Otherwise, if there is a cast from the SQL data type to , the cast function will be used to perform the conversion; otherwise, a scalar JSON value is produced. For any scalar other than a number, a Boolean, or a null value, the text representation will be used, with escaping as necessary to make it a valid JSON string value. Converts an SQL array to a JSON array. The behavior is the same as except that line feeds will be added between top-level array elements if the optional boolean parameter is true. Constructs a JSON array from either a series of parameters or from the results of , which must be a SELECT query returning a single column. If is specified, NULL values are ignored. This is always the case if a is used. Converts an SQL composite value to a JSON object. The behavior is the same as except that line feeds will be added between top-level elements if the optional boolean parameter is true. Builds a possibly-heterogeneously-typed JSON array out of a variadic argument list. Each argument is converted as per or . Builds a JSON object out of a variadic argument list. By convention, the argument list consists of alternating keys and values. Key arguments are coerced to text; value arguments are converted as per or . Constructs a JSON object of all the key/value pairs given, or an empty object if none are given. is a scalar expression defining the key, which is converted to the type. It cannot be nor can it belong to a type that has a cast to the type. If is specified, there must not be any duplicate . Any pair for which the evaluates to is omitted from the output if is specified; if is specified or the clause omitted, the key is included with value . Builds a JSON object out of a text array. The array must have either exactly one dimension with an even number of members, in which case they are taken as alternating key/value pairs, or two dimensions such that each inner array has exactly two elements, which are taken as a key/value pair. All values are converted to JSON strings. This form of takes keys and values pairwise from separate text arrays. Otherwise it is identical to the one-argument form. Converts a given expression specified as or string (in UTF8 encoding) into a JSON value. If is NULL, an null value is returned. If is specified, the must not contain any duplicate object keys. Converts a given SQL scalar value into a JSON scalar value. If the input is NULL, an null is returned. If the input is number or a boolean value, a corresponding JSON number or boolean value is returned. For any other value, a JSON string is returned. Converts an SQL/JSON expression into a character or binary string. The can be of any JSON type, any character string type, or in UTF8 encoding. The returned type used in can be any character string type or . The default is . \n\n This predicate tests whether can be parsed as JSON, possibly of a specified type. If or or is specified, the test is whether or not the JSON is of that particular type. If is specified, then any object in the is also tested to see if it has duplicate keys. SELECT js, js IS JSON \"json?\", js IS JSON SCALAR \"scalar?\", js IS JSON OBJECT \"object?\", js IS JSON ARRAY \"array?\" FROM (VALUES ('123'), ('\"abc\"'), ('{\"a\": \"b\"}'), ('[1,2]'),('abc')) foo(js); js | json? | scalar? | object? | array? ------------+-------+---------+---------+-------- 123 | t | t | f | f \"abc\" | t | t | f | f {\"a\": \"b\"} | t | f | t | f [1,2] | t | f | f | t abc | f | f | f | f SELECT js, js IS JSON OBJECT \"object?\", js IS JSON ARRAY \"array?\", js IS JSON ARRAY WITH UNIQUE KEYS \"array w. UK?\", js IS JSON ARRAY WITHOUT UNIQUE KEYS \"array w/o UK?\" FROM (VALUES ('[{\"a\":\"1\"}, {\"b\":\"2\",\"b\":\"3\"}]')) foo(js); -[ RECORD 1 ]-+-------------------- js | [{\"a\":\"1\"}, + | {\"b\":\"2\",\"b\":\"3\"}] object? | f array? | t array w. UK? | f array w/o UK? | t \n\n Table 9.49 shows the functions that are available for processing and values. Expands the top-level JSON array into a set of JSON values. Expands the top-level JSON array into a set of values. Returns the number of elements in the top-level JSON array. Expands the top-level JSON object into a set of key/value pairs. Expands the top-level JSON object into a set of key/value pairs. The returned s will be of type . Extracts JSON sub-object at the specified path. (This is functionally equivalent to the operator, but writing the path out as a variadic list can be more convenient in some cases.) Extracts JSON sub-object at the specified path as . (This is functionally equivalent to the operator.) Returns the set of keys in the top-level JSON object. Expands the top-level JSON object to a row having the composite type of the argument. The JSON object is scanned for fields whose names match column names of the output row type, and their values are inserted into those columns of the output. (Fields that do not correspond to any output column name are ignored.) In typical use, the value of is just , which means that any output columns that do not match any object field will be filled with nulls. However, if isn't then the values it contains will be used for unmatched columns. To convert a JSON value to the SQL type of an output column, the following rules are applied in sequence:\n• A JSON null value is converted to an SQL null in all cases.\n• If the output column is of type or , the JSON value is just reproduced exactly.\n• If the output column is a composite (row) type, and the JSON value is a JSON object, the fields of the object are converted to columns of the output row type by recursive application of these rules.\n• Likewise, if the output column is an array type and the JSON value is a JSON array, the elements of the JSON array are converted to elements of the output array by recursive application of these rules.\n• Otherwise, if the JSON value is a string, the contents of the string are fed to the input conversion function for the column's data type.\n• Otherwise, the ordinary text representation of the JSON value is fed to the input conversion function for the column's data type. While the example below uses a constant JSON value, typical use would be to reference a or column laterally from another table in the query's clause. Writing in the clause is good practice, since all of the extracted columns are available for use without duplicate function calls. select * from json_populate_record(null::myrowtype, '{\"a\": 1, \"b\": [\"2\", \"a b\"], \"c\": {\"d\": 4, \"e\": \"a b c\"}, \"x\": \"foo\"}') → Function for testing . Returns if the input would finish without an error for the given input JSON object; that is, it's valid input, otherwise. ERROR: value too long for type character(2) Expands the top-level JSON array of objects to a set of rows having the composite type of the argument. Each element of the JSON array is processed as described above for . Expands the top-level JSON object to a row having the composite type defined by an clause. (As with all functions returning , the calling query must explicitly define the structure of the record with an clause.) The output record is filled from fields of the JSON object, in the same way as described above for . Since there is no input record value, unmatched columns are always filled with nulls. select * from json_to_record('{\"a\":1,\"b\":[1,2,3],\"c\":[1,2,3],\"e\":\"bar\",\"r\": {\"a\": 123, \"b\": \"a b c\"}}') as x(a int, b text, c int[], d text, r myrowtype) → Expands the top-level JSON array of objects to a set of rows having the composite type defined by an clause. (As with all functions returning , the calling query must explicitly define the structure of the record with an clause.) Each element of the JSON array is processed as described above for . Returns with the item designated by replaced by , or with added if is true (which is the default) and the item designated by does not exist. All earlier steps in the path must exist, or the is returned unchanged. As with the path oriented operators, negative integers that appear in the count from the end of JSON arrays. If the last path step is an array index that is out of range, and is true, the new value is added at the beginning of the array if the index is negative, or at the end of the array if it is positive. If is not , behaves identically to . Otherwise behaves according to the value of which must be one of , , , or . The default is . Returns with inserted. If the item designated by the is an array element, will be inserted before that item if is false (which is the default), or after it if is true. If the item designated by the is an object field, will be inserted only if the object does not already contain that key. All earlier steps in the path must exist, or the is returned unchanged. As with the path oriented operators, negative integers that appear in the count from the end of JSON arrays. If the last path step is an array index that is out of range, the new value is added at the beginning of the array if the index is negative, or at the end of the array if it is positive. Deletes all object fields that have null values from the given JSON value, recursively. Null values that are not object fields are untouched. Checks whether the JSON path returns any item for the specified JSON value. (This is useful only with SQL-standard JSON path expressions, not predicate check expressions, since those always return a value.) If the argument is specified, it must be a JSON object, and its fields provide named values to be substituted into the expression. If the argument is specified and is , the function suppresses the same errors as the and operators do. Returns the SQL boolean result of a JSON path predicate check for the specified JSON value. (This is useful only with predicate check expressions, not SQL-standard JSON path expressions, since it will either fail or return if the path result is not a single boolean value.) The optional and arguments act the same as for . Returns all JSON items returned by the JSON path for the specified JSON value. For SQL-standard JSON path expressions it returns the JSON values selected from . For predicate check expressions it returns the result of the predicate check: , , or . The optional and arguments act the same as for . Returns all JSON items returned by the JSON path for the specified JSON value, as a JSON array. The parameters are the same as for . Returns the first JSON item returned by the JSON path for the specified JSON value, or if there are no results. The parameters are the same as for . These functions act like their counterparts described above without the suffix, except that these functions support comparisons of date/time values that require timezone-aware conversions. The example below requires interpretation of the date-only value as a timestamp with time zone, so the result depends on the current TimeZone setting. Due to this dependency, these functions are marked as stable, which means these functions cannot be used in indexes. Their counterparts are immutable, and so can be used in indexes; but they will throw errors if asked to make such comparisons. Converts the given JSON value to pretty-printed, indented text. Returns the type of the top-level JSON value as a text string. Possible types are , , , , , and . (The result should not be confused with an SQL NULL; see the examples.) \n\n\n\nSQL/JSON path expressions specify item(s) to be retrieved from a JSON value, similarly to XPath expressions used for access to XML content. In PostgreSQL, path expressions are implemented as the data type and can use any elements described in Section 8.14.7. JSON query functions and operators pass the provided path expression to the path engine for evaluation. If the expression matches the queried JSON data, the corresponding JSON item, or set of items, is returned. If there is no match, the result will be , , or an error, depending on the function. Path expressions are written in the SQL/JSON path language and can include arithmetic expressions and functions. A path expression consists of a sequence of elements allowed by the data type. The path expression is normally evaluated from left to right, but you can use parentheses to change the order of operations. If the evaluation is successful, a sequence of JSON items is produced, and the evaluation result is returned to the JSON query function that completes the specified computation. To refer to the JSON value being queried (the context item), use the variable in the path expression. The first element of a path must always be . It can be followed by one or more accessor operators, which go down the JSON structure level by level to retrieve sub-items of the context item. Each accessor operator acts on the result(s) of the previous evaluation step, producing zero, one, or more output items from each input item. For example, suppose you have some JSON data from a GPS tracker that you would like to parse, such as: To retrieve the available track segments, you need to use the accessor operator to descend through surrounding JSON objects, for example: To retrieve the contents of an array, you typically use the operator. The following example will return the location coordinates for all the available track segments: Here we started with the whole JSON input value ( ), then the accessor selected the JSON object associated with the object key, then the accessor selected the JSON array associated with the key within that object, then the accessor selected each element of that array (producing a series of items), then the accessor selected the JSON array associated with the key within each of those objects. In this example, each of those objects had a key; but if any of them did not, the accessor would have simply produced no output for that input item. To return the coordinates of the first segment only, you can specify the corresponding subscript in the accessor operator. Recall that JSON array indexes are 0-relative: The result of each path evaluation step can be processed by one or more of the operators and methods listed in Section 9.16.2.3. Each method name must be preceded by a dot. For example, you can get the size of an array: More examples of using operators and methods within path expressions appear below in Section 9.16.2.3. A path can also contain filter expressions that work similarly to the clause in SQL. A filter expression begins with a question mark and provides a condition in parentheses: Filter expressions must be written just after the path evaluation step to which they should apply. The result of that step is filtered to include only those items that satisfy the provided condition. SQL/JSON defines three-valued logic, so the condition can produce , , or . The value plays the same role as SQL and can be tested for with the predicate. Further path evaluation steps use only those items for which the filter expression returned . The functions and operators that can be used in filter expressions are listed in Table 9.51. Within a filter expression, the variable denotes the value being considered (i.e., one result of the preceding path step). You can write accessor operators after to retrieve component items. For example, suppose you would like to retrieve all heart rate values higher than 130. You can achieve this as follows: To get the start times of segments with such values, you have to filter out irrelevant segments before selecting the start times, so the filter expression is applied to the previous step, and the path used in the condition is different: You can use several filter expressions in sequence, if required. The following example selects start times of all segments that contain locations with relevant coordinates and high heart rate values: Using filter expressions at different nesting levels is also allowed. The following example first filters all segments by location, and then returns high heart rate values for these segments, if available: You can also nest filter expressions within each other. This example returns the size of the track if it contains any segments with high heart rate values, or an empty sequence otherwise: PostgreSQL's implementation of the SQL/JSON path language has the following deviations from the SQL/JSON standard. As an extension to the SQL standard, a PostgreSQL path expression can be a Boolean predicate, whereas the SQL standard allows predicates only within filters. While SQL-standard path expressions return the relevant element(s) of the queried JSON value, predicate check expressions return the single three-valued result of the predicate: , , or . For example, we could write this SQL-standard filter expression: Predicate check expressions are required in the operator (and the function), and should not be used with the operator (or the function). There are minor differences in the interpretation of regular expression patterns used in filters, as described in Section 9.16.2.4. When you query JSON data, the path expression may not match the actual JSON data structure. An attempt to access a non-existent member of an object or element of an array is defined as a structural error. SQL/JSON path expressions have two modes of handling structural errors:\n• lax (default) — the path engine implicitly adapts the queried data to the specified path. Any structural errors that cannot be fixed as described below are suppressed, producing no match.\n• strict — if a structural error occurs, an error is raised. Lax mode facilitates matching of a JSON document and path expression when the JSON data does not conform to the expected schema. If an operand does not match the requirements of a particular operation, it can be automatically wrapped as an SQL/JSON array, or unwrapped by converting its elements into an SQL/JSON sequence before performing the operation. Also, comparison operators automatically unwrap their operands in lax mode, so you can compare SQL/JSON arrays out-of-the-box. An array of size 1 is considered equal to its sole element. Automatic unwrapping is not performed when:\n• The path expression contains or methods that return the type and the number of elements in the array, respectively.\n• The queried JSON data contain nested arrays. In this case, only the outermost array is unwrapped, while all the inner arrays remain unchanged. Thus, implicit unwrapping can only go one level down within each path evaluation step. For example, when querying the GPS data listed above, you can abstract from the fact that it stores an array of segments when using lax mode: In strict mode, the specified path must exactly match the structure of the queried JSON document, so using this path expression will cause an error: ERROR: jsonpath member accessor can only be applied to an object To get the same result as in lax mode, you have to explicitly unwrap the array: The unwrapping behavior of lax mode can lead to surprising results. For instance, the following query using the accessor selects every value twice: This happens because the accessor selects both the array and each of its elements, while the accessor automatically unwraps arrays when using lax mode. To avoid surprising results, we recommend using the accessor only in strict mode. The following query selects each value just once: The unwrapping of arrays can also lead to unexpected results. Consider this example, which selects all the arrays: As expected it returns the full arrays. But applying a filter expression causes the arrays to be unwrapped to evaluate each item, returning only the items that match the expression: This despite the fact that the full arrays are selected by the path expression. Use strict mode to restore selecting the arrays: Table 9.50 shows the operators and methods available in . Note that while the unary operators and methods can be applied to multiple values resulting from a preceding path step, the binary operators (addition etc.) can only be applied to single values. In lax mode, methods applied to an array will be executed for each value in the array. The exceptions are and , which apply to the array itself. Unary plus (no operation); unlike addition, this can iterate over multiple values Negation; unlike subtraction, this can iterate over multiple values Type of the JSON item (see ) Size of the JSON item (number of array elements, or 1 if not an array) Boolean value converted from a JSON boolean, number, or string String value converted from a JSON boolean, number, string, or datetime Nearest integer greater than or equal to the given number Nearest integer less than or equal to the given number Absolute value of the given number Big integer value converted from a JSON number or string Rounded decimal value converted from a JSON number or string ( and must be integer values) Integer value converted from a JSON number or string Numeric value converted from a JSON number or string Date/time value converted from a string using the specified template Time without time zone value converted from a string Time without time zone value converted from a string, with fractional seconds adjusted to the given precision Time with time zone value converted from a string Time with time zone value converted from a string, with fractional seconds adjusted to the given precision Timestamp without time zone value converted from a string Timestamp without time zone value converted from a string, with fractional seconds adjusted to the given precision Timestamp with time zone value converted from a string Timestamp with time zone value converted from a string, with fractional seconds adjusted to the given precision The object's key-value pairs, represented as an array of objects containing three fields: , , and ; is a unique identifier of the object the key-value pair belongs to \n\n The result type of the and methods can be , , , , or . Both methods determine their result type dynamically. The method sequentially tries to match its input string to the ISO formats for , , , , and . It stops on the first matching format and emits the corresponding data type. The method determines the result type according to the fields used in the provided template string. The and methods use the same parsing rules as the SQL function does (see Section 9.8), with three exceptions. First, these methods don't allow unmatched template patterns. Second, only the following separators are allowed in the template string: minus sign, period, solidus (slash), comma, apostrophe, semicolon, colon and space. Third, separators in the template string must exactly match the input string. If different date/time types need to be compared, an implicit cast is applied. A value can be cast to or , can be cast to , and to . However, all but the first of these conversions depend on the current TimeZone setting, and thus can only be performed within timezone-aware functions. Similarly, other date/time-related methods that convert strings to date/time types also do this casting, which may involve the current TimeZone setting. Therefore, these conversions can also only be performed within timezone-aware functions. Equality comparison (this, and the other comparison operators, work on all JSON scalar values) JSON constant (note that, unlike in SQL, comparison to works normally) Tests whether the first operand matches the regular expression given by the second operand, optionally with modifications described by a string of characters (see Section 9.16.2.4). Tests whether the second operand is an initial substring of the first operand. Tests whether a path expression matches at least one SQL/JSON item. Returns if the path expression would result in an error; the second example uses this to avoid a no-such-key error in strict mode. \n\n SQL/JSON path expressions allow matching text to a regular expression with the filter. For example, the following SQL/JSON path query would case-insensitively match all strings in an array that start with an English vowel: The optional string may include one or more of the characters for case-insensitive match, to allow and to match at newlines, to allow to match a newline, and to quote the whole pattern (reducing the behavior to a simple substring match). The SQL/JSON standard borrows its definition for regular expressions from the operator, which in turn uses the XQuery standard. PostgreSQL does not currently support the operator. Therefore, the filter is implemented using the POSIX regular expression engine described in Section 9.7.3. This leads to various minor discrepancies from standard SQL/JSON behavior, which are cataloged in Section 9.7.3.8. Note, however, that the flag-letter incompatibilities described there do not apply to SQL/JSON, as it translates the XQuery flag letters to match what the POSIX engine expects. Keep in mind that the pattern argument of is a JSON path string literal, written according to the rules given in Section 8.14.7. This means in particular that any backslashes you want to use in the regular expression must be doubled. For example, to match string values of the root document that contain only digits:\n\nSQL/JSON functions , , and described in Table 9.52 can be used to query JSON documents. Each of these functions apply a (an SQL/JSON path query) to a (the document). See Section 9.16.2 for more details on what the can contain. The can also reference variables, whose values are specified with their respective names in the clause that is supported by each function. can be a value or a character string that can be successfully cast to .\n• Returns true if the SQL/JSON applied to the yields any items, false otherwise.\n• The clause specifies the behavior if an error occurs during evaluation. Specifying will cause an error to be thrown with the appropriate message. Other options include returning values or or the value which is actually an SQL NULL. The default when no clause is specified is to return the value . ERROR: jsonpath array subscript is out of bounds\n• Returns the result of applying the SQL/JSON to the .\n• By default, the result is returned as a value of type , though the clause can be used to return as some other type to which it can be successfully coerced.\n• If the path expression may return multiple values, it might be necessary to wrap those values using the clause to make it a valid JSON string, because the default behavior is to not wrap them, as if were specified. The clause is by default taken to mean , which means that even a single result value will be wrapped. To apply the wrapper only when multiple values are present, specify . Getting multiple values in result will be treated as an error if is specified.\n• If the result is a scalar string, by default, the returned value will be surrounded by quotes, making it a valid JSON value. It can be made explicit by specifying . Conversely, quotes can be omitted by specifying . To ensure that the result is a valid JSON value, cannot be specified when is also specified.\n• The clause specifies the behavior if evaluating yields an empty set. The clause specifies the behavior if an error occurs when evaluating , when coercing the result value to the type, or when evaluating the expression if the evaluation returns an empty set.\n• For both and , specifying will cause an error to be thrown with the appropriate message. Other options include returning an SQL NULL, an empty array ( ), an empty object ( ), or a user-specified expression ( ) that can be coerced to jsonb or the type specified in . The default when or is not specified is to return an SQL NULL value. JSON_QUERY(jsonb '[1,[2,3],null]', 'lax $[*][$off]' PASSING 1 AS off WITH CONDITIONAL WRAPPER) →\n• Returns the result of applying the SQL/JSON to the .\n• Only use if the extracted value is expected to be a single scalar item; getting multiple values will be treated as an error. If you expect that extracted value might be an object or an array, use the function instead.\n• By default, the result, which must be a single scalar value, is returned as a value of type , though the clause can be used to return as some other type to which it can be successfully coerced.\n• The and clauses have similar semantics as mentioned in the description of , except the set of values returned in lieu of throwing an error is different.\n• Note that scalar strings returned by always have their quotes removed, equivalent to specifying in . JSON_VALUE(jsonb '[1,2]', 'strict $[$off]' PASSING 1 as off) → \n\n The expression is converted to by an implicit cast if the expression is not already of type . Note, however, that any parsing errors that occur during that conversion are thrown unconditionally, that is, are not handled according to the (specified or implicit) clause. returns an SQL NULL if returns a JSON , whereas returns the JSON as is.\n\nis an SQL/JSON function which queries data and presents the results as a relational view, which can be accessed as a regular SQL table. You can use inside the clause of a , , or and as data source in a statement. Taking JSON data as input, uses a JSON path expression to extract a part of the provided data to use as a row pattern for the constructed view. Each SQL/JSON value given by the row pattern serves as source for a separate row in the constructed view. To split the row pattern into columns, provides the clause that defines the schema of the created view. For each column, a separate JSON path expression can be specified to be evaluated against the row pattern to get an SQL/JSON value that will become the value for the specified column in a given output row. JSON data stored at a nested level of the row pattern can be extracted using the clause. Each clause can be used to generate one or more columns using the data from a nested level of the row pattern. Those columns can be specified using a clause that looks similar to the top-level COLUMNS clause. Rows constructed from NESTED COLUMNS are called child rows and are joined against the row constructed from the columns specified in the parent clause to get the row in the final view. Child columns themselves may contain a specification thus allowing to extract data located at arbitrary nesting levels. Columns produced by multiple s at the same level are considered to be siblings of each other and their rows after joining with the parent row are combined using UNION. The rows produced by are laterally joined to the row that generated them, so you do not have to explicitly join the constructed view with the original table holding data. Each syntax element is described below in more detail. The specifies the input document to query, the is an SQL/JSON path expression defining the query, and is an optional name for the . The optional clause provides data values for the variables mentioned in the . The result of the input data evaluation using the aforementioned elements is called the row pattern, which is used as the source for row values in the constructed view. The clause defining the schema of the constructed view. In this clause, you can specify each column to be filled with an SQL/JSON value obtained by applying a JSON path expression against the row pattern. has the following variants: Adds an ordinality column that provides sequential row numbering starting from 1. Each (see below) gets its own counter for any nested ordinality columns. Inserts an SQL/JSON value obtained by applying against the row pattern into the view's output row after coercing it to specified . Specifying makes it explicit that you expect the value to be a valid object. It only makes sense to specify if is one of , , , , , , , or a domain over these types. Optionally, you can specify and clauses to format the output. Note that specifying overrides if also specified, because unquoted literals do not constitute valid values. Optionally, you can use and clauses to specify whether to throw the error or return the specified value when the result of JSON path evaluation is empty and when an error occurs during JSON path evaluation or when coercing the SQL/JSON value to the specified type, respectively. The default for both is to return a value. This clause is internally turned into and has the same semantics as or . The latter if the specified type is not a scalar type or if either of , , or clause is present. Inserts a boolean value obtained by applying against the row pattern into the view's output row after coercing it to specified . The value corresponds to whether applying the expression to the row pattern yields any values. The specified should have a cast from the type. Optionally, you can use to specify whether to throw the error or return the specified value when an error occurs during JSON path evaluation or when coercing SQL/JSON value to the specified type. The default is to return a boolean value . This clause is internally turned into and has the same semantics as . Extracts SQL/JSON values from nested levels of the row pattern, generates one or more columns as defined by the subclause, and inserts the extracted SQL/JSON values into those columns. The expression in the subclause uses the same syntax as in the parent clause. The syntax is recursive, so you can go down multiple nested levels by specifying several subclauses within each other. It allows to unnest the hierarchy of JSON objects and arrays in a single function invocation rather than chaining several expressions in an SQL statement. In each variant of described above, if the clause is omitted, path expression is used, where is the provided column name. The optional serves as an identifier of the provided . The name must be unique and distinct from the column names. The optional can be used to specify how to handle errors when evaluating the top-level . Use if you want the errors to be thrown and to return an empty table, that is, a table containing 0 rows. Note that this clause does not affect the errors that occur when evaluating columns, for which the behavior depends on whether the clause is specified against a given column. In the examples that follow, the following table containing JSON data will be used: The following query shows how to use to turn the JSON objects in the table to a view containing columns for the keys , , and contained in the original JSON along with an ordinality column: SELECT jt.* FROM my_films, JSON_TABLE (js, '$.favorites[*]' COLUMNS ( id FOR ORDINALITY, kind text PATH '$.kind', title text PATH '$.films[*].title' WITH WRAPPER, director text PATH '$.films[*].director' WITH WRAPPER)) AS jt; The following is a modified version of the above query to show the usage of arguments in the filter specified in the top-level JSON path expression and the various options for the individual columns: SELECT jt.* FROM my_films, JSON_TABLE (js, '$.favorites[*] ? (@.films[*].director == $filter)' PASSING 'Alfred Hitchcock' AS filter, 'Vertigo' AS filter2 COLUMNS ( id FOR ORDINALITY, kind text PATH '$.kind', title text FORMAT JSON PATH '$.films[*].title' OMIT QUOTES, director text PATH '$.films[*].director' KEEP QUOTES)) AS jt; The following is a modified version of the above query to show the usage of for populating title and director columns, illustrating how they are joined to the parent columns id and kind: SELECT jt.* FROM my_films, JSON_TABLE ( js, '$.favorites[*] ? (@.films[*].director == $filter)' PASSING 'Alfred Hitchcock' AS filter COLUMNS ( id FOR ORDINALITY, kind text PATH '$.kind', NESTED PATH '$.films[*]' COLUMNS ( title text FORMAT JSON PATH '$.title' OMIT QUOTES, director text PATH '$.director' KEEP QUOTES))) AS jt; The following is the same query but without the filter in the root path: SELECT jt.* FROM my_films, JSON_TABLE ( js, '$.favorites[*]' COLUMNS ( id FOR ORDINALITY, kind text PATH '$.kind', NESTED PATH '$.films[*]' COLUMNS ( title text FORMAT JSON PATH '$.title' OMIT QUOTES, director text PATH '$.director' KEEP QUOTES))) AS jt; The following shows another query using a different object as input. It shows the UNION \"sibling join\" between paths and and also the usage of column at levels (columns , , and ): SELECT * FROM JSON_TABLE ( '{\"favorites\": {\"movies\": [{\"name\": \"One\", \"director\": \"John Doe\"}, {\"name\": \"Two\", \"director\": \"Don Joe\"}], \"books\": [{\"name\": \"Mystery\", \"authors\": [{\"name\": \"Brown Dan\"}]}, {\"name\": \"Wonder\", \"authors\": [{\"name\": \"Jun Murakami\"}, {\"name\":\"Craig Doe\"}]}] }}'::json, '$.favorites[*]' COLUMNS ( user_id FOR ORDINALITY, NESTED '$.movies[*]' COLUMNS ( movie_id FOR ORDINALITY, mname text PATH '$.name', director text), NESTED '$.books[*]' COLUMNS ( book_id FOR ORDINALITY, bname text PATH '$.name', NESTED '$.authors[*]' COLUMNS ( author_id FOR ORDINALITY, author_name text PATH '$.name')))); user_id | movie_id | mname | director | book_id | bname | author_id | author_name ---------+----------+-------+----------+---------+---------+-----------+-------------- 1 | 1 | One | John Doe | | | | 1 | 2 | Two | Don Joe | | | | 1 | | | | 1 | Mystery | 1 | Brown Dan 1 | | | | 2 | Wonder | 1 | Jun Murakami 1 | | | | 2 | Wonder | 2 | Craig Doe (5 rows)"
    },
    {
        "link": "https://postgresql.org/docs/12/functions-json.html",
        "document": "To learn more about the SQL/JSON standard, see [sqltr-19075-6]. For details on JSON types supported in PostgreSQL, see Section 8.14.\n\nTable 9.44 shows the operators that are available for use with JSON data types (see Section 8.14). Get JSON array element (indexed from zero, negative integers count from the end) Get JSON object at the specified path Get JSON object at the specified path as \n\n There are parallel variants of these operators for both the and types. The field/element/path extraction operators return the same type as their left-hand input (either or ), except for those specified as returning , which coerce the value to text. The field/element/path extraction operators return NULL, rather than failing, if the JSON input does not have the right structure to match the request; for example if no such element exists. The field/element/path extraction operators that accept integer JSON array subscripts all support negative subscripting from the end of arrays. The standard comparison operators shown in Table 9.1 are available for , but not for . They follow the ordering rules for B-tree operations outlined at Section 8.14.4. See also Section 9.20 for the aggregate function which aggregates record values as JSON, the aggregate function which aggregates pairs of values into a JSON object, and their equivalents, and . Some further operators also exist only for , as shown in Table 9.45. Many of these operators can be indexed by operator classes. For a full description of containment and existence semantics, see Section 8.14.3. Section 8.14.4 describes how these operators can be used to effectively index data. Does the left JSON value contain the right JSON path/value entries at the top level? Are the left JSON path/value entries contained at the top level within the right JSON value? Does the exist as a top-level key within the JSON value? Do any of these array exist as top-level keys? Do all of these array exist as top-level keys? Concatenate two values into a new value Delete key/value pair or element from left operand. Key/value pairs are matched based on their key value. Delete multiple key/value pairs or elements from left operand. Key/value pairs are matched based on their key value. Delete the array element with specified index (Negative integers count from the end). Throws an error if top level container is not an array. Delete the field or element with specified path (for JSON arrays, negative integers count from the end) Does JSON path return any item for the specified JSON value? Returns the result of JSON path predicate check for the specified JSON value. Only the first item of the result is taken into account. If the result is not Boolean, then is returned. \n\n The operator concatenates two JSON objects by generating an object containing the union of their keys, taking the second object's value when there are duplicate keys. All other cases produce a JSON array: first, any non-array input is converted into a single-element array, and then the two arrays are concatenated. It does not operate recursively; only the top-level array or object structure is merged. The and operators suppress the following errors: lacking object field or array element, unexpected JSON item type, and numeric errors. This behavior might be helpful while searching over JSON document collections of varying structure. Table 9.46 shows the functions that are available for creating and values. (There are no equivalent functions for , of the and functions. However, the function supplies much the same functionality as these functions would.) Returns the value as or . Arrays and composites are converted (recursively) to arrays and objects; otherwise, if there is a cast from the type to , the cast function will be used to perform the conversion; otherwise, a scalar value is produced. For any scalar type other than a number, a Boolean, or a null value, the text representation will be used, in such a fashion that it is a valid or value. Returns the array as a JSON array. A PostgreSQL multidimensional array becomes a JSON array of arrays. Line feeds will be added between dimension-1 elements if is true. Returns the row as a JSON object. Line feeds will be added between level-1 elements if is true. Builds a JSON object out of a variadic argument list. By convention, the argument list consists of alternating keys and values. Builds a JSON object out of a text array. The array must have either exactly one dimension with an even number of members, in which case they are taken as alternating key/value pairs, or two dimensions such that each inner array has exactly two elements, which are taken as a key/value pair. This form of takes keys and values pairwise from two separate arrays. In all other respects it is identical to the one-argument form. \n\n and have the same behavior as except for offering a pretty-printing option. The behavior described for likewise applies to each individual value converted by the other JSON creation functions. The hstore extension has a cast from to , so that values converted via the JSON creation functions will be represented as JSON objects, not as primitive string values. Table 9.47 shows the functions that are available for processing and values. Returns the number of elements in the outermost JSON array. Expands the outermost JSON object into a set of key/value pairs. Expands the outermost JSON object into a set of key/value pairs. The returned values will be of type . Returns JSON value pointed to by (equivalent to operator). Returns JSON value pointed to by as (equivalent to operator). Returns set of keys in the outermost JSON object. Expands the object in to a row whose columns match the record type defined by (see note below). select * from json_populate_record(null::myrowtype, '{\"a\": 1, \"b\": [\"2\", \"a b\"], \"c\": {\"d\": 4, \"e\": \"a b c\"}}') Expands the outermost array of objects in to a set of rows whose columns match the record type defined by (see note below). Returns the type of the outermost JSON value as a text string. Possible types are , , , , , and . Builds an arbitrary record from a JSON object (see note below). As with all functions returning , the caller must explicitly define the structure of the record with an clause. select * from json_to_record('{\"a\":1,\"b\":[1,2,3],\"c\":[1,2,3],\"e\":\"bar\",\"r\": {\"a\": 123, \"b\": \"a b c\"}}') as x(a int, b text, c int[], d text, r myrowtype) Builds an arbitrary set of records from a JSON array of objects (see note below). As with all functions returning , the caller must explicitly define the structure of the record with an clause. Returns with all object fields that have null values omitted. Other null values are untouched. Returns with the section designated by replaced by , or with added if is true (default is ) and the item designated by does not exist. As with the path oriented operators, negative integers that appear in count from the end of JSON arrays. Returns with inserted. If section designated by is in a JSONB array, will be inserted before target or after if is true (default is ). If section designated by is in JSONB object, will be inserted only if does not exist. As with the path oriented operators, negative integers that appear in count from the end of JSON arrays. Checks whether JSON path returns any item for the specified JSON value. Returns the result of JSON path predicate check for the specified JSON value. Only the first item of the result is taken into account. If the result is not Boolean, then is returned. Gets all JSON items returned by JSON path for the specified JSON value. Gets all JSON items returned by JSON path for the specified JSON value and wraps result into an array. Gets the first JSON item returned by JSON path for the specified JSON value. Returns on no results. \n\n Many of these functions and operators will convert Unicode escapes in JSON strings to the appropriate single character. This is a non-issue if the input is type , because the conversion was already done; but for input, this may result in throwing an error, as noted in Section 8.14. The functions , , and operate on a JSON object, or array of objects, and extract the values associated with keys whose names match column names of the output row type. Object fields that do not correspond to any output column name are ignored, and output columns that do not match any object field will be filled with nulls. To convert a JSON value to the SQL type of an output column, the following rules are applied in sequence:\n• A JSON null value is converted to a SQL null in all cases.\n• If the output column is of type or , the JSON value is just reproduced exactly.\n• If the output column is a composite (row) type, and the JSON value is a JSON object, the fields of the object are converted to columns of the output row type by recursive application of these rules.\n• Likewise, if the output column is an array type and the JSON value is a JSON array, the elements of the JSON array are converted to elements of the output array by recursive application of these rules.\n• Otherwise, if the JSON value is a string literal, the contents of the string are fed to the input conversion function for the column's data type.\n• Otherwise, the ordinary text representation of the JSON value is fed to the input conversion function for the column's data type. While the examples for these functions use constants, the typical use would be to reference a table in the clause and use one of its or columns as an argument to the function. Extracted key values can then be referenced in other parts of the query, like clauses and target lists. Extracting multiple values in this way can improve performance over extracting them separately with per-key operators. All the items of the parameter of as well as except the last item must be present in the . If is false, all items of the parameter of must be present. If these conditions are not met the is returned unchanged. If the last path item is an object key, it will be created if it is absent and given the new value. If the last path item is an array index, if it is positive the item to set is found by counting from the left, and if negative by counting from the right - designates the rightmost element, and so on. If the item is out of the range -array_length .. array_length -1, and create_missing is true, the new value is added at the beginning of the array if the item is negative, and at the end of the array if it is positive. The function's return value should not be confused with a SQL NULL. While calling will return , calling will return a SQL NULL. If the argument to contains duplicate field names in any object, the result could be semantically somewhat different, depending on the order in which they occur. This is not an issue for since values never have duplicate object field names. The , , , , and functions have optional and arguments. If the argument is specified, it provides an object containing named variables to be substituted into a expression. If the argument is specified and has the value, these functions suppress the same errors as the and operators.\n\nSQL/JSON path expressions specify the items to be retrieved from the JSON data, similar to XPath expressions used for SQL access to XML. In PostgreSQL, path expressions are implemented as the data type and can use any elements described in Section 8.14.6. JSON query functions and operators pass the provided path expression to the path engine for evaluation. If the expression matches the queried JSON data, the corresponding SQL/JSON item is returned. Path expressions are written in the SQL/JSON path language and can also include arithmetic expressions and functions. Query functions treat the provided expression as a text string, so it must be enclosed in single quotes. A path expression consists of a sequence of elements allowed by the data type. The path expression is evaluated from left to right, but you can use parentheses to change the order of operations. If the evaluation is successful, a sequence of SQL/JSON items (SQL/JSON sequence) is produced, and the evaluation result is returned to the JSON query function that completes the specified computation. To refer to the JSON data to be queried (the context item), use the sign in the path expression. It can be followed by one or more accessor operators, which go down the JSON structure level by level to retrieve the content of context item. Each operator that follows deals with the result of the previous evaluation step. For example, suppose you have some JSON data from a GPS tracker that you would like to parse, such as: To retrieve the available track segments, you need to use the accessor operator for all the preceding JSON objects: If the item to retrieve is an element of an array, you have to unnest this array using the operator. For example, the following path will return location coordinates for all the available track segments: To return the coordinates of the first segment only, you can specify the corresponding subscript in the accessor operator. Note that the SQL/JSON arrays are 0-relative: The result of each path evaluation step can be processed by one or more operators and methods listed in Section 9.15.2.3. Each method name must be preceded by a dot. For example, you can get an array size: For more examples of using operators and methods within path expressions, see Section 9.15.2.3. When defining the path, you can also use one or more filter expressions that work similar to the clause in SQL. A filter expression begins with a question mark and provides a condition in parentheses: Filter expressions must be specified right after the path evaluation step to which they are applied. The result of this step is filtered to include only those items that satisfy the provided condition. SQL/JSON defines three-valued logic, so the condition can be , , or . The value plays the same role as SQL and can be tested for with the predicate. Further path evaluation steps use only those items for which filter expressions return . Functions and operators that can be used in filter expressions are listed in Table 9.49. The path evaluation result to be filtered is denoted by the variable. To refer to a JSON element stored at a lower nesting level, add one or more accessor operators after . Suppose you would like to retrieve all heart rate values higher than 130. You can achieve this using the following expression: To get the start time of segments with such values instead, you have to filter out irrelevant segments before returning the start time, so the filter expression is applied to the previous step, and the path used in the condition is different: You can use several filter expressions on the same nesting level, if required. For example, the following expression selects all segments that contain locations with relevant coordinates and high heart rate values: Using filter expressions at different nesting levels is also allowed. The following example first filters all segments by location, and then returns high heart rate values for these segments, if available: You can also nest filter expressions within each other: This expression returns the size of the track if it contains any segments with high heart rate values, or an empty sequence otherwise. PostgreSQL's implementation of SQL/JSON path language has the following deviations from the SQL/JSON standard:\n• item method is not implemented yet mainly because immutable functions and operators cannot reference session timezone, which is used in some datetime operations. Datetime support will be added to in future versions of PostgreSQL.\n• A path expression can be a Boolean predicate, although the SQL/JSON standard allows predicates only in filters. This is necessary for implementation of the operator. For example, the following expression is valid in PostgreSQL:\n• There are minor differences in the interpretation of regular expression patterns used in filters, as described in Section 9.15.2.2. When you query JSON data, the path expression may not match the actual JSON data structure. An attempt to access a non-existent member of an object or element of an array results in a structural error. SQL/JSON path expressions have two modes of handling structural errors:\n• lax (default) — the path engine implicitly adapts the queried data to the specified path. Any remaining structural errors are suppressed and converted to empty SQL/JSON sequences.\n• strict — if a structural error occurs, an error is raised. The lax mode facilitates matching of a JSON document structure and path expression if the JSON data does not conform to the expected schema. If an operand does not match the requirements of a particular operation, it can be automatically wrapped as an SQL/JSON array or unwrapped by converting its elements into an SQL/JSON sequence before performing this operation. Besides, comparison operators automatically unwrap their operands in the lax mode, so you can compare SQL/JSON arrays out-of-the-box. An array of size 1 is considered equal to its sole element. Automatic unwrapping is not performed only when:\n• The path expression contains or methods that return the type and the number of elements in the array, respectively.\n• The queried JSON data contain nested arrays. In this case, only the outermost array is unwrapped, while all the inner arrays remain unchanged. Thus, implicit unwrapping can only go one level down within each path evaluation step. For example, when querying the GPS data listed above, you can abstract from the fact that it stores an array of segments when using the lax mode: In the strict mode, the specified path must exactly match the structure of the queried JSON document to return an SQL/JSON item, so using this path expression will cause an error. To get the same result as in the lax mode, you have to explicitly unwrap the array: The accessor can lead to surprising results when using the lax mode. For instance, the following query selects every value twice: This happens because the accessor selects both the array and each of its elements, while the accessor automatically unwraps arrays when using the lax mode. To avoid surprising results, we recommend using the accessor only in the strict mode. The following query selects each value just once: SQL/JSON path expressions allow matching text to a regular expression with the filter. For example, the following SQL/JSON path query would case-insensitively match all strings in an array that start with an English vowel: The optional string may include one or more of the characters for case-insensitive match, to allow and to match at newlines, to allow to match a newline, and to quote the whole pattern (reducing the behavior to a simple substring match). The SQL/JSON standard borrows its definition for regular expressions from the operator, which in turn uses the XQuery standard. PostgreSQL does not currently support the operator. Therefore, the filter is implemented using the POSIX regular expression engine described in Section 9.7.3. This leads to various minor discrepancies from standard SQL/JSON behavior, which are cataloged in Section 9.7.3.8. Note, however, that the flag-letter incompatibilities described there do not apply to SQL/JSON, as it translates the XQuery flag letters to match what the POSIX engine expects. Keep in mind that the pattern argument of is a JSON path string literal, written according to the rules given in Section 8.14.6. This means in particular that any backslashes you want to use in the regular expression must be doubled. For example, to match string values of the root document that contain only digits: Table 9.48 shows the operators and methods available in . Table 9.49 shows the available filter expression elements. Plus operator that iterates over the SQL/JSON sequence Minus operator that iterates over the SQL/JSON sequence Approximate floating-point number converted from an SQL/JSON number or a string Nearest integer greater than or equal to the SQL/JSON number Nearest integer less than or equal to the SQL/JSON number Absolute value of the SQL/JSON number Sequence of object's key-value pairs represented as array of items containing three fields ( , , and ). is a unique identifier of the object key-value pair belongs to. \n\n Value used to perform comparison with JSON literal Value used to perform comparison with JSON literal Value used to perform comparison with JSON value Tests whether the first operand matches the regular expression given by the second operand, optionally with modifications described by a string of characters (see Section 9.15.2.2) Tests whether the second operand is an initial substring of the first operand Tests whether a path expression matches at least one SQL/JSON item"
    },
    {
        "link": "https://medium.com/@atarax/finally-json-table-is-here-postgres-17-a9b5245649bd",
        "document": "In today’s data-driven world, JSON has become the backbone of modern application development (Thanks to REST). Its flexible and lightweight structure allows developers to easily transmit and store data, making it an indispensable format for APIs, configurations, and document stores.\n\nPostgreSQL has long been a pioneer in JSON support, being the first relational database to add native JSON handling back in 2012. Over the years, PostgreSQL has continuously enhanced its JSON capabilities. With the release of PostgreSQL 17, they’ve introduced even more powerful features for working with JSON, such as , SQL/JSON constructors (like , , ), and query functions ( , , ). These new features provide developers with more sophisticated ways to interact with and extract value from their JSON data. Additionally, this release expands JSONPath expressions, emphasizing converting JSON data to native PostgreSQL data types, including numeric, boolean, string, and date/time types.\n\nI have been using Postgres for quite some time now and I am a big-time advocate of Postgres as a go-to relational database. As someone who has been using PostgreSQL extensively throughout my development career, I’ve worked heavily with JSON and JSONB type columns in Postgres. As quoted by Uncle Ben in Spiderman — With great power comes great responsibilities. A developer choosing complex JSON will pay the price for it on code complexity. Dealing with JSON columns can push you to write SQL queries that can soon start to feel complex or unreadable. But with time Postgres has greatly improved the JSON support for developers.\n\nIn PostgreSQL 17, several new features provide much-needed support for working with JSONB data types. One of the standout features is the long-awaited JSON_TABLE, which we'll be exploring in depth.\n\nJSON_TABLE was first discussed in this commit of Andrew Dunstan and it almost made it to the release two years ago but was dropped off at the last minute. MySQL added JSON_TABLE() several years ago, and it is popular. Oracle and SQL Server also have it. Now, here is Postgres with this in its arsenal.\n\nis an SQL/JSON function which queries JSON data and presents the results as a relational view, which can be accessed as a regular SQL table. You can use inside the clause of a , , or and as data source in a statement. Taking JSON data as input, uses a JSON path expression to extract a part of the provided data to use as a row pattern for the constructed view. Each SQL/JSON value given by the row pattern serves as source for a separate row in the constructed view. To split the row pattern into columns, provides the clause that defines the schema of the created view. For each column, a separate JSON path expression can be specified to be evaluated against the row pattern to get an SQL/JSON value that will become the value for the specified column in a given output row.\n\nA good use case for JSON_TABLE is when you need to work with structured data stored in a JSON document but want to interact with it using traditional SQL operations. By mapping parts of the JSON document into rows and columns, JSON_TABLE creates a virtual table that allows you to treat the JSON data like any other relational table. This can be particularly useful when you need to insert extracted data into an existing database table, or when you want to query the JSON data within a SQL join, filter, or aggregation.\n\nIts time to get our hands dirty by writing some SQL queries.\n\nWe will be using docker to spin up a postgres17 instance with a dummy table/data and run a few SQL queries to see how JSON_TABLE can be very handy while dealing with JSONB columns.\n• DBeaver or any other tool to connect to your Postgres DB and run queries\n\nNote — Ensure that Docker is installed on your machine. If it’s not installed, download and install Docker from the official website: Docker Installation.\n• Create a directory postgres-init in your project folder. This directory will hold the initialization scripts.\n• Inside postgres-init, create the following two files:\n\n2. 02-create_table_schema_and_insert.sh: Script to create a schema or any other necessary database setup.\n\nNote — You can refactor the above SQL query in 2 different bash scripts to create a table and insert values into it.\n\nStep 4: Verify if the container is up and running\n\nOutput will be something like this\n\nI will be using DBeaver for demonstration though the steps will remain the same.\n\nSet up the values as above to connect to your Postgres Db.\n\nTo do the same on the terminal, without installing any additional tool follow the steps below:\n• Find the name or ID of the container running the PostgreSQL database. You can use the docker ps command to list all running containers and their IDs and names.\n• Use the docker exec command to start a shell inside the container. Replace “container_name_or_id” with the name or ID of the container.\n• Once you are inside the container’s shell, run the psql command to connect to the PostgreSQL database. Replace “username” and “database_name” with your database credentials.\n\nWe are all set to see JSON_TABLE in action now.\n\nLet's write a SQL query to extract information from the JSON structure in the data column of the products table:\n• This specifies that we are querying the entire data JSON object.\n• brand text PATH ‘$.brand’: Extracts the brand of the product.\n• model text PATH ‘$.model’: Extracts the model of the product.\n• release_year integer PATH ‘$.release_year’: Extracts the release_year as an integer.\n\nAS jt: This names the resulting virtual table jt, which you can then query like any other table.\n\nThere are other intricacies like using PASSING parameters to filter the JSON_TABLE at the time of creation and get relevant rows only, NESTED PATH for populating certain rows and normalizing the table.\n\nYou can read more about it in the example provided in Postgres Documentation here.\n\nHopefully, this blog gave you an understanding of how you can effectively query a JSONB column in your Postgres table now with JSON_TABLE. If you have any questions or comments, please feel free to leave them below. Your feedback is always appreciated."
    },
    {
        "link": "https://neon.tech/docs/functions/json_table",
        "document": "The function transforms JSON data into relational views, allowing you to query JSON data using standard SQL operations. Added in PostgreSQL 17, this feature helps you work with complex JSON data by presenting it as a virtual table which you can access with regular SQL queries.\n\nUse when you need to:\n• Apply SQL operations like filtering and aggregation to JSON data\n\nuses the following syntax:\n• : Defines the schema of the virtual table\n• : Specifies how to extract values for each column\n• : Name for the resulting virtual table\n\nLet's explore using a library management system example. We'll store book information including reviews, borrowing history, and metadata in JSON format.\n\n\"comment\": \"Good examples, could use more exercises\",\n\nThis query extracts core book details from the JSON structure into a relational format.\n\nThis query flattens the reviews array into rows, making it easy to analyze reader feedback.\n\nUse this query to calculate review statistics for each book.\n\nThis query extracts array fields and metadata into queryable columns.\n\nreturns NULL for missing values by default. You can modify this behavior with error handling clauses:\n\nThis example shows how to handle errors when extracting JSON data. There is an error here because the field is not of type .\n• \n• Use JSON operators ( , , ) when possible"
    },
    {
        "link": "https://docs.oracle.com/en/database/oracle/oracle-database/19/adjsn/function-JSON_TABLE.html",
        "document": "SQL/JSON function projects specific JSON data to columns of various SQL data types. You use it to map parts of a JSON document into the rows and columns of a new, virtual table, which you can also think of as an inline view. You can then insert this virtual table into a pre-existing database table, or you can query it using SQL — in a join expression, for example. A common use of is to create a view of JSON data. You can use such a view just as you would use any table or view. This lets applications, tools, and programmers operate on JSON data without consideration of the syntax of JSON or JSON path expressions. Defining a view over JSON data in effect maps a kind of schema onto that data. This mapping is after the fact: the underlying JSON data can be defined and created without any regard to a schema or any particular pattern of use. Data first, schema later. Such a schema (mapping) imposes no restriction on the kind of JSON documents that can be stored in the database (other than being well-formed JSON data). The view exposes only data that conforms to the mapping (schema) that defines the view. To change the schema, just redefine the view — no need to reorganize the underlying JSON data. You use in a SQL clause. It is a row source: it generates a row of virtual-table data for each JSON value selected by a row path expression (row pattern). The columns of each generated row are defined by the column path expressions of the clause. Typically a invocation is laterally joined, implicitly, with a source table in the list, whose rows each contain a JSON document that is used as input to the function. generates zero or more new rows, as determined by evaluating the row path expression against the input document. The first argument to is a SQL expression. It can be a table or view column value, a PL/SQL variable, or a bind variable with proper casting. The result of evaluating the expression is used as the context item for evaluating the row path expression. The second argument to is the SQL/JSON row path expression followed by an optional error clause for handling the row and the (required) clause, which defines the columns of the virtual table to be created. There is no clause. There are two levels of error handling for , corresponding to the two levels of path expressions: row and column. When present, a column error handler overrides row-level error handling. The default error handler for both levels is . As an alternative to passing the context-item argument and the row path expression, you can use simple dot-notation syntax. (You can still use an error clause, and the clause is still required.) Dot notation specifies a table or view column together with a simple path to the targeted JSON data. For example, these two queries are equivalent: And in cases where the row path expression is only , which targets the entire document, you can omit the path part. These queries are equivalent: Example 20-1 illustrates the difference between using the simple dot notation and using the fuller, more explicit notation. This example uses for two equivalent queries. The first query uses the simple, dot-notation syntax for the expressions that target the row and column data. The second uses the full syntax. Except for column , whose SQL identifier is quoted, the SQL column names are, in effect, uppercase. (Identifier contains a space character.) In the first query the column names are written exactly the same as the names of the targeted object fields, including with respect to letter case. Regardless of whether they are quoted, they are interpreted case-sensitively for purposes of establishing the default path (the path used when there is no explicit clause).\n• Explicit clauses with SQL/JSON column path expressions, to target the object fields that are projected\n\nIn a clause you can often use a clause instead of SQL/JSON function . This can mean a simpler query expression. It also has the advantage of including rows with non- relational columns when the JSON column is . The clause is a shortcut for using with an ANSI left outer join. That is, these two queries are equivalent: Using a left outer join with , or using the clause, allows the selection result to include rows with relational columns where there is no corresponding JSON-column data, that is, where the JSON column is . The only semantic difference between the two is that if you use a clause then the JSON column itself is not included in the result. The clause provides the same clause as , including the possibility of nested columns. Advantages of using are that you need not provide an check constraint (needed for with the simple dot notation) or a table alias, and you need not specify . The clause syntax is simpler, it allows all of the flexibility of the clause, and it performs an implicit left outer join. Example 20-2 Equivalent: SQL NESTED and JSON_TABLE with LEFT OUTER JOIN These two queries are equivalent. One uses SQL/JSON function with an explicit . The other uses a SQL clause. SELECT id, requestor, type, \"number\" FROM j_purchaseorder (po_document COLUMNS (Requestor, NESTED ShippingInstructions.Phone[*] COLUMNS (type, \"number\"))) ON 1=1); SELECT id, requestor, type, \"number\" FROM j_purchaseorder po_document COLUMNS (Requestor, NESTED ShippingInstructions.Phone[*] COLUMNS (type, \"number\"); The output is the same in both cases: If table had a row with non- values for columns and , but a value for column then that row would appear in both cases. But it would not appear in the case if were absent.\n\nThe mandatory clause for SQL/JSON function defines the columns of the virtual table that the function creates. It consists of the keyword followed by the following entries, enclosed in parentheses. Other than the optional entry, each entry in the clause is either a regular column specification or a nested columns specification.\n• At most one entry in the clause can be a column name followed by the keywords , which specifies a column of generated row numbers (SQL data type ). These numbers start with one. For example:\n• A regular column specification consists of a column name followed by an optional scalar data type for the column, which can be SQL data type , , , , , or (the same as for the clause of ), followed by an optional value clause and an optional clause. The default data type is . Data type is used for Oracle Spatial and Graph data. In particular, this means that you can use with GeoJSON data, which is a format for encoding geographic data in JSON. Oracle extends the SQL/JSON standard in the case when the returning data type for a column is , by allowing optional keyword immediately after the data type. When is present and the value to return is wider than , the value is truncated — only the first characters are returned. If is absent then this case is treated as an error, handled as usual by an error clause or the default error-handling behavior.\n• A nested columns specification consists of the keyword followed by an optional keyword, a SQL/JSON row path expression, and then a clause. This clause specifies columns that represent nested data. The row path expression used here provides a refined context for the specified nested columns: each nested column path expression is relative to the row path expression. You can nest columns clauses to project values that are present in arrays at different levels to columns of the same row. A clause at any level (nested or not) has the same characteristics. In other words, the clause is defined recursively. For each level of nesting (that is, for each use of keyword ), the nested clause is said to be the child of the clause within which it is nested, which is its parent. Two or more clauses that have the same parent clause are siblings. The virtual tables defined by parent and child clauses are joined using an outer join, with the parent being the outer table. The virtual columns defined by sibling clauses are joined using a join. Example 20-1 and Example 20-8 illustrate the use of a nested columns clause. The only thing required in a regular column specification is the column name. Defining the column projection in more detail, by specifying a scalar data type, value handling, or a target path, is optional.\n• The optional value clause specifies how to handle the data projected to the column: whether to handle it as would , , or . This value handling includes the return data type, return format (pretty or ASCII), wrapper, and error treatment. By default, the projected data is handled as if by . If you use keyword then it is handled as if by . If you use keywords then it is handled as if by . For you can override the default wrapping behavior by adding an explicit wrapper clause. You can override the default error handling for the given handler ( , , or ) by adding an explicit error clause appropriate for it.\n• The optional clause specifies the portion of the row that is to be used as the column content. The column path expression following keyword is matched against the context item provided by the virtual row. The column path expression must represent a relative path; it is relative to the path specified by the row path expression. If the clause is not present then the behavior is the same as if it were present with a path of , where is the column name. That is, the name of the object field that is targeted is taken implicitly as the column name. For purposes of specifying the targeted field only, the SQL identifier used for is interpreted case-sensitively, even if it is not quoted. The SQL name of the column follows the usual rule: if it is enclosed in double quotation marks ( ) then the letter case used is significant; otherwise, it is not (it is treated as if uppercase). For example, these two clauses are equivalent. For SQL, case is significant only for column . Example 20-1 presents equivalent queries that illustrate this. If you use in a given column specification then semantics are used when projecting the JSON data to the column. The data type specified for the column can be any of the SQL data types that can return: , , or .\n• The projected JSON data is always well-formed. This includes ensuring that non-ASCII characters in string values are escaped as needed. For example, a TAB character (CHARACTER TABULATION, U+0009) is escaped as .\n• You can use a wrapper clause, to project multiple JSON values as elements in an array. If you do not use in a given column specification then semantics are used when projecting the JSON data. The data type specified for the column can be any of the SQL data types that can return: , , , , , , or . error handling applies (and you cannot use a wrapper clause). For example, here the value of column is projected directly using semantics, and the value of column is projected as a JSON string using semantics: You typically use when the projected data is a JSON object or array. You typically do not use when the projected data is a JSON scalar.\n• for information about using Oracle Spatial and Graph data\n\nSQL/JSON function generalizes SQL/JSON condition and SQL/JSON functions and . Everything that you can do using these functions you can do using . For the jobs they accomplish, the syntax of these functions is simpler to use than is the syntax of . If you use any of , , or more than once, or in combination, to access the same data then a single invocation of presents the advantage that the data is parsed only once. Because of this, the optimizer often automatically rewrites multiple invocations of , and (any combination) to fewer invocations of instead, so the data is parsed only once. Example 20-3 and Example 20-4 illustrate this. They each select the requestor and the set of phones used by each object in column . But Example 20-4 parses that column only once, not four times. Note the following in connection with Example 20-4:\n• A JSON value of is a value as far as SQL is concerned; it is not , which in SQL represents the absence of a value (missing, unknown, or inapplicable data). In Example 20-4, if the JSON value of object attribute is then the SQL string is returned.\n• is a SQL condition; you can use it in a SQL clause, a statement, or a check constraint. In Example 20-3 it is used in a clause. Function employs the semantics of implicitly when you specify keyword . It must return a SQL value in the virtual column. Since Oracle SQL has no Boolean data type, a SQL string or is used to represent the Boolean value. This is the case in Example 20-4: the value is stored in column , and it is then tested explicitly for equality against the literal SQL string .\n• JSON field has a JSON Boolean value. When is applied to that value it is returned as a string. In Example 20-4, data type is used as the column data type. Function implicitly uses for this column, returning the value as a value, which is then tested for equality against the literal SQL string . SELECT json_ (po_document, '$.Requestor' RETURNING VARCHAR2(32)), json_ (po_document, '$.ShippingInstructions.Phone' RETURNING VARCHAR2(100)) FROM j_purchaseorder WHERE json_ (po_document, '$.ShippingInstructions.Address.zipCode') AND json_ (po_document, '$.AllowPartialShipment' RETURNING VARCHAR2(5 CHAR)) = ; Example 20-4 Using JSON_TABLE to Extract Data Without Multiple Parses SELECT jt.requestor, jt.phones FROM j_purchaseorder, json_table(po_document, '$' COLUMNS (requestor VARCHAR2(32 CHAR) PATH '$.Requestor', phones VARCHAR2(100 CHAR) FORMAT JSON PATH '$.ShippingInstructions.Phone', (5 CHAR) PATH '$.AllowPartialShipment', (5 CHAR) PATH '$.ShippingInstructions.Address.zipCode')) jt WHERE AND ;\n• Using SQL/JSON Function JSON_VALUE With a Boolean JSON Value\n\nA JSON value can be an array or can include one or more arrays, nested to any number of levels inside other JSON arrays or objects. You can use a path clause to project specific elements of an array. Example 20-5 projects the requestor and associated phone numbers from the JSON data in column . The entire JSON array is projected as a column of JSON data, . To format this JSON data as a column, the keywords are needed. What if you wanted to project the individual elements of JSON array and not the array as a whole? Example 20-6 shows one way to do this, which you can use if the array elements are the only data you need to project. If you want to project both the requestor and the corresponding phone data then the row path expression of Example 20-6 ( ) is not appropriate: it targets only the (phone object) elements of array . Example 20-7 shows one way to target both: use a row path expression that targets both the name and the entire phones array, and use column path expressions that target fields and of individual phone objects. In Example 20-7 as in Example 20-5, keywords are needed because the resulting columns contain JSON data, namely arrays of phone types or phone numbers, with one array element for each phone. In addition, unlike the case for Example 20-5, a wrapper clause is needed for column and column , because array contains multiple objects with fields and . Sometimes you might not want the effect of Example 20-7. For example, you might want a column that contains a single phone number (one row per number), rather than one that contains a JSON array of phone numbers (one row for all numbers for a given purchase order). To obtain that result, you need to tell to project the array elements, by using a path clause for the array. A path clause acts, in effect, as an additional row source (row pattern). Example 20-8 illustrates this. You can use any number of keywords in a given invocation. In Example 20-8 the outer clause is the parent of the nested (inner) clause. The virtual tables defined are joined using an outer join, with the table defined by the parent clause being the outer table in the join. Example 20-5 Projecting an Entire JSON Array as JSON Data Example 20-7 Projecting Elements of a JSON Array Plus Other Data This example shows two equivalent queries that project array elements. The first query uses the simple, dot-notation syntax for the expressions that target the row and column data. The second uses the full syntax. Except for column , whose SQL identifier is quoted ( ), the SQL column names are, in effect, uppercase. (Column is lowercase.) In the first query the column names are written exactly the same as the field names that are targeted, including with respect to letter case. Regardless of whether they are quoted, they are interpreted case-sensitively for purposes of establishing the proper path.\n• Explicit clauses with SQL/JSON column path expressions, to target the object fields that are projected\n\nTo improve query performance you can create a view over JSON data that you project to columns using SQL/JSON function . To further improve query performance you can create a materialized view and place the JSON data in memory. Example 20-9 defines a view over JSON data. It uses a path clause to project the elements of array . Example 20-10 defines a materialized view that has the same data and structure as Example 20-9. In general, you cannot update a view directly (whether materialized or not). But if a materialized view is created using keywords and , as in Example 20-10, then the view is updated automatically whenever you update the base table. You can use to project any fields as view columns, and the view creation (materialized or not) can involve joining any tables and any number of invocations of . The only differences between Example 20-9 and Example 20-10 are these:\n• The use of keyword .\n• The use of .\n• The use of . The use of means that the materialized view will be refreshed incrementally. For this to occur, you must use either or (if there is no primary key). Oracle recommends that you specify a primary key for a table that has a JSON column and that you use when creating a materialized view based on it. You could use in place of for the view creation. The former synchronizes the view with the base table only when your table-updating transaction is committed. Until then the table changes are not reflected in the view. If you use then the view is immediately synchronized after each DML statement. This also means that a view created using reflects any rollbacks that you might perform. (A subsequent statement ends the transaction, preventing a rollback.)"
    },
    {
        "link": "https://hashrocket.com/blog/posts/dealing-with-nested-json-objects-in-postgresql",
        "document": "Over the past few weeks I've been having to deal with nested JSON columns in Postgres. It was turning into me just throwing queries at it until something stuck. Well, as it would turn out, it's actually very simple! I just needed to take the time to sit down and learn about a few operators and what they're actually doing.\n\nWith that being said, here is a condensed version of some things I learned that helped me navigate through JSON columns in Postgres.\n\nFirst, let's create a very basic table containing a json column\n\nNow, let's seed this table with some data about a few different vehicles\n\nNow that we have our data, let's go over some of the basics.\n\nUsing the (Stabby) operator will return JSON data. So here we can see a few examples of how we would get the json values out of the column:\n\nAs you can see, 'misc' is a nested JSON object. If you want to go another layer deeper into a nested JSON object, we can simply add another\n\nNow, as I said before, this isn't giving us text values. These quotations are an indicator that is returning JSON. In order to grab the text value we need to use the (Double Stabby) operator. This is equivalent to adding afterwards, typecasting it to a .\n\nA simpler way to do nested JSON attributes is to use the (Waffle Cone) operator with an array of the path you want to follow.\n\nJust like before, a single returns JSON, so if we wanted to grab the actual text value, we'll need to use the (Double Waffle Cone) operator.\n\nOkay, but what if you want the entire row that matches a certain criteria? We'll move this into a where clause, and then use the operator.\n\nA good way to think of this operator is to replace it with the word \"contains\".\n\nThis query will return to us every row where there is a 4 door car\n\nThis query will return to us every row containing a Toyota or a Honda\n\nThis query will return to us every row containing both color and doors keys inside of misc.\n\nIf you are looking for a specific JSON object inside of another, you want to use the (Penguin) operator.\n\nFor example, if were looking specifically for :\n\nAs you might have guessed, we can look for nested JSON objects\n\nThis will return to us every row containing red, 2-door vehicles\"\n\nThere is also the (Ice Cream Cone or Reverse Penguin) operator that behaves the same, only in reverse.\n\nI hope this helps clear up any confusion you might have had about querying nested JSON objects in Postgres.\n\nIf you want to learn more about these operators or if you want to see what else you can do, here is a link to the docs for JSON functions in Postgres."
    },
    {
        "link": "https://stackoverflow.com/questions/44075557/how-to-query-nested-arrays-in-a-postgres-json-column",
        "document": "I have some json similar to the json below stored in a postgres json column. I'm trying query it to identify some incorrectly entered data. I'm basically looking for addresses where the house description is the same as the house number. I can't quite work out how to do it.\n\nI have written this sql which will find where the data matches:\n\nThis obviously only works on the first address on the first school. Is there a way of querying all of the addresses of every school?"
    },
    {
        "link": "https://stackoverflow.com/questions/43114765/querying-nested-json-arrays-in-postgresql",
        "document": "I am currently experimenting with the JSON functionality PostgreSQL. While queries of nested key-value pair objects are very easy to query, I am having trouble querying JSON arrays. To be more specific, I have a table with two columns: An integer primary key and a JSONB column . The JSONB data have the following structure:\n\nI want to select all table rows that have at least one element whose is and has a value between and . A pseudo-SQL query would be:"
    },
    {
        "link": "https://postgresql.org/docs/current/functions-json.html",
        "document": "To provide native support for JSON data types within the SQL environment, PostgreSQL implements the SQL/JSON data model. This model comprises sequences of items. Each item can hold SQL scalar values, with an additional SQL/JSON null value, and composite data structures that use JSON arrays and objects. The model is a formalization of the implied data model in the JSON specification RFC 7159.\n\nSQL/JSON allows you to handle JSON data alongside regular SQL data, with transaction support, including:\n\nTo learn more about the SQL/JSON standard, see [sqltr-19075-6]. For details on JSON types supported in PostgreSQL, see Section 8.14.\n\nTable 9.45 shows the operators that are available for use with JSON data types (see Section 8.14). In addition, the usual comparison operators shown in Table 9.1 are available for , though not for . The comparison operators follow the ordering rules for B-tree operations outlined in Section 8.14.4. See also Section 9.21 for the aggregate function which aggregates record values as JSON, the aggregate function which aggregates pairs of values into a JSON object, and their equivalents, and . Extracts 'th element of JSON array (array elements are indexed from zero, but negative integers count from the end). Extracts JSON object field with the given key. Extracts JSON object field with the given key, as . Extracts JSON sub-object at the specified path, where path elements can be either field keys or array indexes. Extracts JSON sub-object at the specified path as . \n\n The field/element/path extraction operators return NULL, rather than failing, if the JSON input does not have the right structure to match the request; for example if no such key or array element exists. Some further operators exist only for , as shown in Table 9.46. Section 8.14.4 describes how these operators can be used to effectively search indexed data. Does the first JSON value contain the second? (See Section 8.14.3 for details about containment.) Is the first JSON value contained in the second? Does the text string exist as a top-level key or array element within the JSON value? Do any of the strings in the text array exist as top-level keys or array elements? Do all of the strings in the text array exist as top-level keys or array elements? Concatenates two values. Concatenating two arrays generates an array containing all the elements of each input. Concatenating two objects generates an object containing the union of their keys, taking the second object's value when there are duplicate keys. All other cases are treated by converting a non-array input into a single-element array, and then proceeding as for two arrays. Does not operate recursively: only the top-level array or object structure is merged. To append an array to another array as a single entry, wrap it in an additional layer of array, for example: Deletes a key (and its value) from a JSON object, or matching string value(s) from a JSON array. Deletes all matching keys or array elements from the left operand. Deletes the array element with specified index (negative integers count from the end). Throws an error if JSON value is not an array. Deletes the field or array element at the specified path, where path elements can be either field keys or array indexes. Does JSON path return any item for the specified JSON value? (This is useful only with SQL-standard JSON path expressions, not predicate check expressions, since those always return a value.) Returns the result of a JSON path predicate check for the specified JSON value. (This is useful only with predicate check expressions, not SQL-standard JSON path expressions, since it will return if the path result is not a single boolean value.) \n\n The operators and suppress the following errors: missing object field or array element, unexpected JSON item type, datetime and numeric errors. The -related functions described below can also be told to suppress these types of errors. This behavior might be helpful when searching JSON document collections of varying structure. Table 9.47 shows the functions that are available for constructing and values. Some functions in this table have a clause, which specifies the data type returned. It must be one of , , , a character string type ( , , or ), or a type that can be cast to . By default, the type is returned. Converts any SQL value to or . Arrays and composites are converted recursively to arrays and objects (multidimensional arrays become arrays of arrays in JSON). Otherwise, if there is a cast from the SQL data type to , the cast function will be used to perform the conversion; otherwise, a scalar JSON value is produced. For any scalar other than a number, a Boolean, or a null value, the text representation will be used, with escaping as necessary to make it a valid JSON string value. Converts an SQL array to a JSON array. The behavior is the same as except that line feeds will be added between top-level array elements if the optional boolean parameter is true. Constructs a JSON array from either a series of parameters or from the results of , which must be a SELECT query returning a single column. If is specified, NULL values are ignored. This is always the case if a is used. Converts an SQL composite value to a JSON object. The behavior is the same as except that line feeds will be added between top-level elements if the optional boolean parameter is true. Builds a possibly-heterogeneously-typed JSON array out of a variadic argument list. Each argument is converted as per or . Builds a JSON object out of a variadic argument list. By convention, the argument list consists of alternating keys and values. Key arguments are coerced to text; value arguments are converted as per or . Constructs a JSON object of all the key/value pairs given, or an empty object if none are given. is a scalar expression defining the key, which is converted to the type. It cannot be nor can it belong to a type that has a cast to the type. If is specified, there must not be any duplicate . Any pair for which the evaluates to is omitted from the output if is specified; if is specified or the clause omitted, the key is included with value . Builds a JSON object out of a text array. The array must have either exactly one dimension with an even number of members, in which case they are taken as alternating key/value pairs, or two dimensions such that each inner array has exactly two elements, which are taken as a key/value pair. All values are converted to JSON strings. This form of takes keys and values pairwise from separate text arrays. Otherwise it is identical to the one-argument form. Converts a given expression specified as or string (in UTF8 encoding) into a JSON value. If is NULL, an null value is returned. If is specified, the must not contain any duplicate object keys. Converts a given SQL scalar value into a JSON scalar value. If the input is NULL, an null is returned. If the input is number or a boolean value, a corresponding JSON number or boolean value is returned. For any other value, a JSON string is returned. Converts an SQL/JSON expression into a character or binary string. The can be of any JSON type, any character string type, or in UTF8 encoding. The returned type used in can be any character string type or . The default is . \n\n This predicate tests whether can be parsed as JSON, possibly of a specified type. If or or is specified, the test is whether or not the JSON is of that particular type. If is specified, then any object in the is also tested to see if it has duplicate keys. SELECT js, js IS JSON \"json?\", js IS JSON SCALAR \"scalar?\", js IS JSON OBJECT \"object?\", js IS JSON ARRAY \"array?\" FROM (VALUES ('123'), ('\"abc\"'), ('{\"a\": \"b\"}'), ('[1,2]'),('abc')) foo(js); js | json? | scalar? | object? | array? ------------+-------+---------+---------+-------- 123 | t | t | f | f \"abc\" | t | t | f | f {\"a\": \"b\"} | t | f | t | f [1,2] | t | f | f | t abc | f | f | f | f SELECT js, js IS JSON OBJECT \"object?\", js IS JSON ARRAY \"array?\", js IS JSON ARRAY WITH UNIQUE KEYS \"array w. UK?\", js IS JSON ARRAY WITHOUT UNIQUE KEYS \"array w/o UK?\" FROM (VALUES ('[{\"a\":\"1\"}, {\"b\":\"2\",\"b\":\"3\"}]')) foo(js); -[ RECORD 1 ]-+-------------------- js | [{\"a\":\"1\"}, + | {\"b\":\"2\",\"b\":\"3\"}] object? | f array? | t array w. UK? | f array w/o UK? | t \n\n Table 9.49 shows the functions that are available for processing and values. Expands the top-level JSON array into a set of JSON values. Expands the top-level JSON array into a set of values. Returns the number of elements in the top-level JSON array. Expands the top-level JSON object into a set of key/value pairs. Expands the top-level JSON object into a set of key/value pairs. The returned s will be of type . Extracts JSON sub-object at the specified path. (This is functionally equivalent to the operator, but writing the path out as a variadic list can be more convenient in some cases.) Extracts JSON sub-object at the specified path as . (This is functionally equivalent to the operator.) Returns the set of keys in the top-level JSON object. Expands the top-level JSON object to a row having the composite type of the argument. The JSON object is scanned for fields whose names match column names of the output row type, and their values are inserted into those columns of the output. (Fields that do not correspond to any output column name are ignored.) In typical use, the value of is just , which means that any output columns that do not match any object field will be filled with nulls. However, if isn't then the values it contains will be used for unmatched columns. To convert a JSON value to the SQL type of an output column, the following rules are applied in sequence:\n• A JSON null value is converted to an SQL null in all cases.\n• If the output column is of type or , the JSON value is just reproduced exactly.\n• If the output column is a composite (row) type, and the JSON value is a JSON object, the fields of the object are converted to columns of the output row type by recursive application of these rules.\n• Likewise, if the output column is an array type and the JSON value is a JSON array, the elements of the JSON array are converted to elements of the output array by recursive application of these rules.\n• Otherwise, if the JSON value is a string, the contents of the string are fed to the input conversion function for the column's data type.\n• Otherwise, the ordinary text representation of the JSON value is fed to the input conversion function for the column's data type. While the example below uses a constant JSON value, typical use would be to reference a or column laterally from another table in the query's clause. Writing in the clause is good practice, since all of the extracted columns are available for use without duplicate function calls. select * from json_populate_record(null::myrowtype, '{\"a\": 1, \"b\": [\"2\", \"a b\"], \"c\": {\"d\": 4, \"e\": \"a b c\"}, \"x\": \"foo\"}') → Function for testing . Returns if the input would finish without an error for the given input JSON object; that is, it's valid input, otherwise. ERROR: value too long for type character(2) Expands the top-level JSON array of objects to a set of rows having the composite type of the argument. Each element of the JSON array is processed as described above for . Expands the top-level JSON object to a row having the composite type defined by an clause. (As with all functions returning , the calling query must explicitly define the structure of the record with an clause.) The output record is filled from fields of the JSON object, in the same way as described above for . Since there is no input record value, unmatched columns are always filled with nulls. select * from json_to_record('{\"a\":1,\"b\":[1,2,3],\"c\":[1,2,3],\"e\":\"bar\",\"r\": {\"a\": 123, \"b\": \"a b c\"}}') as x(a int, b text, c int[], d text, r myrowtype) → Expands the top-level JSON array of objects to a set of rows having the composite type defined by an clause. (As with all functions returning , the calling query must explicitly define the structure of the record with an clause.) Each element of the JSON array is processed as described above for . Returns with the item designated by replaced by , or with added if is true (which is the default) and the item designated by does not exist. All earlier steps in the path must exist, or the is returned unchanged. As with the path oriented operators, negative integers that appear in the count from the end of JSON arrays. If the last path step is an array index that is out of range, and is true, the new value is added at the beginning of the array if the index is negative, or at the end of the array if it is positive. If is not , behaves identically to . Otherwise behaves according to the value of which must be one of , , , or . The default is . Returns with inserted. If the item designated by the is an array element, will be inserted before that item if is false (which is the default), or after it if is true. If the item designated by the is an object field, will be inserted only if the object does not already contain that key. All earlier steps in the path must exist, or the is returned unchanged. As with the path oriented operators, negative integers that appear in the count from the end of JSON arrays. If the last path step is an array index that is out of range, the new value is added at the beginning of the array if the index is negative, or at the end of the array if it is positive. Deletes all object fields that have null values from the given JSON value, recursively. Null values that are not object fields are untouched. Checks whether the JSON path returns any item for the specified JSON value. (This is useful only with SQL-standard JSON path expressions, not predicate check expressions, since those always return a value.) If the argument is specified, it must be a JSON object, and its fields provide named values to be substituted into the expression. If the argument is specified and is , the function suppresses the same errors as the and operators do. Returns the SQL boolean result of a JSON path predicate check for the specified JSON value. (This is useful only with predicate check expressions, not SQL-standard JSON path expressions, since it will either fail or return if the path result is not a single boolean value.) The optional and arguments act the same as for . Returns all JSON items returned by the JSON path for the specified JSON value. For SQL-standard JSON path expressions it returns the JSON values selected from . For predicate check expressions it returns the result of the predicate check: , , or . The optional and arguments act the same as for . Returns all JSON items returned by the JSON path for the specified JSON value, as a JSON array. The parameters are the same as for . Returns the first JSON item returned by the JSON path for the specified JSON value, or if there are no results. The parameters are the same as for . These functions act like their counterparts described above without the suffix, except that these functions support comparisons of date/time values that require timezone-aware conversions. The example below requires interpretation of the date-only value as a timestamp with time zone, so the result depends on the current TimeZone setting. Due to this dependency, these functions are marked as stable, which means these functions cannot be used in indexes. Their counterparts are immutable, and so can be used in indexes; but they will throw errors if asked to make such comparisons. Converts the given JSON value to pretty-printed, indented text. Returns the type of the top-level JSON value as a text string. Possible types are , , , , , and . (The result should not be confused with an SQL NULL; see the examples.) \n\n\n\nSQL/JSON path expressions specify item(s) to be retrieved from a JSON value, similarly to XPath expressions used for access to XML content. In PostgreSQL, path expressions are implemented as the data type and can use any elements described in Section 8.14.7. JSON query functions and operators pass the provided path expression to the path engine for evaluation. If the expression matches the queried JSON data, the corresponding JSON item, or set of items, is returned. If there is no match, the result will be , , or an error, depending on the function. Path expressions are written in the SQL/JSON path language and can include arithmetic expressions and functions. A path expression consists of a sequence of elements allowed by the data type. The path expression is normally evaluated from left to right, but you can use parentheses to change the order of operations. If the evaluation is successful, a sequence of JSON items is produced, and the evaluation result is returned to the JSON query function that completes the specified computation. To refer to the JSON value being queried (the context item), use the variable in the path expression. The first element of a path must always be . It can be followed by one or more accessor operators, which go down the JSON structure level by level to retrieve sub-items of the context item. Each accessor operator acts on the result(s) of the previous evaluation step, producing zero, one, or more output items from each input item. For example, suppose you have some JSON data from a GPS tracker that you would like to parse, such as: To retrieve the available track segments, you need to use the accessor operator to descend through surrounding JSON objects, for example: To retrieve the contents of an array, you typically use the operator. The following example will return the location coordinates for all the available track segments: Here we started with the whole JSON input value ( ), then the accessor selected the JSON object associated with the object key, then the accessor selected the JSON array associated with the key within that object, then the accessor selected each element of that array (producing a series of items), then the accessor selected the JSON array associated with the key within each of those objects. In this example, each of those objects had a key; but if any of them did not, the accessor would have simply produced no output for that input item. To return the coordinates of the first segment only, you can specify the corresponding subscript in the accessor operator. Recall that JSON array indexes are 0-relative: The result of each path evaluation step can be processed by one or more of the operators and methods listed in Section 9.16.2.3. Each method name must be preceded by a dot. For example, you can get the size of an array: More examples of using operators and methods within path expressions appear below in Section 9.16.2.3. A path can also contain filter expressions that work similarly to the clause in SQL. A filter expression begins with a question mark and provides a condition in parentheses: Filter expressions must be written just after the path evaluation step to which they should apply. The result of that step is filtered to include only those items that satisfy the provided condition. SQL/JSON defines three-valued logic, so the condition can produce , , or . The value plays the same role as SQL and can be tested for with the predicate. Further path evaluation steps use only those items for which the filter expression returned . The functions and operators that can be used in filter expressions are listed in Table 9.51. Within a filter expression, the variable denotes the value being considered (i.e., one result of the preceding path step). You can write accessor operators after to retrieve component items. For example, suppose you would like to retrieve all heart rate values higher than 130. You can achieve this as follows: To get the start times of segments with such values, you have to filter out irrelevant segments before selecting the start times, so the filter expression is applied to the previous step, and the path used in the condition is different: You can use several filter expressions in sequence, if required. The following example selects start times of all segments that contain locations with relevant coordinates and high heart rate values: Using filter expressions at different nesting levels is also allowed. The following example first filters all segments by location, and then returns high heart rate values for these segments, if available: You can also nest filter expressions within each other. This example returns the size of the track if it contains any segments with high heart rate values, or an empty sequence otherwise: PostgreSQL's implementation of the SQL/JSON path language has the following deviations from the SQL/JSON standard. As an extension to the SQL standard, a PostgreSQL path expression can be a Boolean predicate, whereas the SQL standard allows predicates only within filters. While SQL-standard path expressions return the relevant element(s) of the queried JSON value, predicate check expressions return the single three-valued result of the predicate: , , or . For example, we could write this SQL-standard filter expression: Predicate check expressions are required in the operator (and the function), and should not be used with the operator (or the function). There are minor differences in the interpretation of regular expression patterns used in filters, as described in Section 9.16.2.4. When you query JSON data, the path expression may not match the actual JSON data structure. An attempt to access a non-existent member of an object or element of an array is defined as a structural error. SQL/JSON path expressions have two modes of handling structural errors:\n• lax (default) — the path engine implicitly adapts the queried data to the specified path. Any structural errors that cannot be fixed as described below are suppressed, producing no match.\n• strict — if a structural error occurs, an error is raised. Lax mode facilitates matching of a JSON document and path expression when the JSON data does not conform to the expected schema. If an operand does not match the requirements of a particular operation, it can be automatically wrapped as an SQL/JSON array, or unwrapped by converting its elements into an SQL/JSON sequence before performing the operation. Also, comparison operators automatically unwrap their operands in lax mode, so you can compare SQL/JSON arrays out-of-the-box. An array of size 1 is considered equal to its sole element. Automatic unwrapping is not performed when:\n• The path expression contains or methods that return the type and the number of elements in the array, respectively.\n• The queried JSON data contain nested arrays. In this case, only the outermost array is unwrapped, while all the inner arrays remain unchanged. Thus, implicit unwrapping can only go one level down within each path evaluation step. For example, when querying the GPS data listed above, you can abstract from the fact that it stores an array of segments when using lax mode: In strict mode, the specified path must exactly match the structure of the queried JSON document, so using this path expression will cause an error: ERROR: jsonpath member accessor can only be applied to an object To get the same result as in lax mode, you have to explicitly unwrap the array: The unwrapping behavior of lax mode can lead to surprising results. For instance, the following query using the accessor selects every value twice: This happens because the accessor selects both the array and each of its elements, while the accessor automatically unwraps arrays when using lax mode. To avoid surprising results, we recommend using the accessor only in strict mode. The following query selects each value just once: The unwrapping of arrays can also lead to unexpected results. Consider this example, which selects all the arrays: As expected it returns the full arrays. But applying a filter expression causes the arrays to be unwrapped to evaluate each item, returning only the items that match the expression: This despite the fact that the full arrays are selected by the path expression. Use strict mode to restore selecting the arrays: Table 9.50 shows the operators and methods available in . Note that while the unary operators and methods can be applied to multiple values resulting from a preceding path step, the binary operators (addition etc.) can only be applied to single values. In lax mode, methods applied to an array will be executed for each value in the array. The exceptions are and , which apply to the array itself. Unary plus (no operation); unlike addition, this can iterate over multiple values Negation; unlike subtraction, this can iterate over multiple values Type of the JSON item (see ) Size of the JSON item (number of array elements, or 1 if not an array) Boolean value converted from a JSON boolean, number, or string String value converted from a JSON boolean, number, string, or datetime Nearest integer greater than or equal to the given number Nearest integer less than or equal to the given number Absolute value of the given number Big integer value converted from a JSON number or string Rounded decimal value converted from a JSON number or string ( and must be integer values) Integer value converted from a JSON number or string Numeric value converted from a JSON number or string Date/time value converted from a string using the specified template Time without time zone value converted from a string Time without time zone value converted from a string, with fractional seconds adjusted to the given precision Time with time zone value converted from a string Time with time zone value converted from a string, with fractional seconds adjusted to the given precision Timestamp without time zone value converted from a string Timestamp without time zone value converted from a string, with fractional seconds adjusted to the given precision Timestamp with time zone value converted from a string Timestamp with time zone value converted from a string, with fractional seconds adjusted to the given precision The object's key-value pairs, represented as an array of objects containing three fields: , , and ; is a unique identifier of the object the key-value pair belongs to \n\n The result type of the and methods can be , , , , or . Both methods determine their result type dynamically. The method sequentially tries to match its input string to the ISO formats for , , , , and . It stops on the first matching format and emits the corresponding data type. The method determines the result type according to the fields used in the provided template string. The and methods use the same parsing rules as the SQL function does (see Section 9.8), with three exceptions. First, these methods don't allow unmatched template patterns. Second, only the following separators are allowed in the template string: minus sign, period, solidus (slash), comma, apostrophe, semicolon, colon and space. Third, separators in the template string must exactly match the input string. If different date/time types need to be compared, an implicit cast is applied. A value can be cast to or , can be cast to , and to . However, all but the first of these conversions depend on the current TimeZone setting, and thus can only be performed within timezone-aware functions. Similarly, other date/time-related methods that convert strings to date/time types also do this casting, which may involve the current TimeZone setting. Therefore, these conversions can also only be performed within timezone-aware functions. Equality comparison (this, and the other comparison operators, work on all JSON scalar values) JSON constant (note that, unlike in SQL, comparison to works normally) Tests whether the first operand matches the regular expression given by the second operand, optionally with modifications described by a string of characters (see Section 9.16.2.4). Tests whether the second operand is an initial substring of the first operand. Tests whether a path expression matches at least one SQL/JSON item. Returns if the path expression would result in an error; the second example uses this to avoid a no-such-key error in strict mode. \n\n SQL/JSON path expressions allow matching text to a regular expression with the filter. For example, the following SQL/JSON path query would case-insensitively match all strings in an array that start with an English vowel: The optional string may include one or more of the characters for case-insensitive match, to allow and to match at newlines, to allow to match a newline, and to quote the whole pattern (reducing the behavior to a simple substring match). The SQL/JSON standard borrows its definition for regular expressions from the operator, which in turn uses the XQuery standard. PostgreSQL does not currently support the operator. Therefore, the filter is implemented using the POSIX regular expression engine described in Section 9.7.3. This leads to various minor discrepancies from standard SQL/JSON behavior, which are cataloged in Section 9.7.3.8. Note, however, that the flag-letter incompatibilities described there do not apply to SQL/JSON, as it translates the XQuery flag letters to match what the POSIX engine expects. Keep in mind that the pattern argument of is a JSON path string literal, written according to the rules given in Section 8.14.7. This means in particular that any backslashes you want to use in the regular expression must be doubled. For example, to match string values of the root document that contain only digits:\n\nSQL/JSON functions , , and described in Table 9.52 can be used to query JSON documents. Each of these functions apply a (an SQL/JSON path query) to a (the document). See Section 9.16.2 for more details on what the can contain. The can also reference variables, whose values are specified with their respective names in the clause that is supported by each function. can be a value or a character string that can be successfully cast to .\n• Returns true if the SQL/JSON applied to the yields any items, false otherwise.\n• The clause specifies the behavior if an error occurs during evaluation. Specifying will cause an error to be thrown with the appropriate message. Other options include returning values or or the value which is actually an SQL NULL. The default when no clause is specified is to return the value . ERROR: jsonpath array subscript is out of bounds\n• Returns the result of applying the SQL/JSON to the .\n• By default, the result is returned as a value of type , though the clause can be used to return as some other type to which it can be successfully coerced.\n• If the path expression may return multiple values, it might be necessary to wrap those values using the clause to make it a valid JSON string, because the default behavior is to not wrap them, as if were specified. The clause is by default taken to mean , which means that even a single result value will be wrapped. To apply the wrapper only when multiple values are present, specify . Getting multiple values in result will be treated as an error if is specified.\n• If the result is a scalar string, by default, the returned value will be surrounded by quotes, making it a valid JSON value. It can be made explicit by specifying . Conversely, quotes can be omitted by specifying . To ensure that the result is a valid JSON value, cannot be specified when is also specified.\n• The clause specifies the behavior if evaluating yields an empty set. The clause specifies the behavior if an error occurs when evaluating , when coercing the result value to the type, or when evaluating the expression if the evaluation returns an empty set.\n• For both and , specifying will cause an error to be thrown with the appropriate message. Other options include returning an SQL NULL, an empty array ( ), an empty object ( ), or a user-specified expression ( ) that can be coerced to jsonb or the type specified in . The default when or is not specified is to return an SQL NULL value. JSON_QUERY(jsonb '[1,[2,3],null]', 'lax $[*][$off]' PASSING 1 AS off WITH CONDITIONAL WRAPPER) →\n• Returns the result of applying the SQL/JSON to the .\n• Only use if the extracted value is expected to be a single scalar item; getting multiple values will be treated as an error. If you expect that extracted value might be an object or an array, use the function instead.\n• By default, the result, which must be a single scalar value, is returned as a value of type , though the clause can be used to return as some other type to which it can be successfully coerced.\n• The and clauses have similar semantics as mentioned in the description of , except the set of values returned in lieu of throwing an error is different.\n• Note that scalar strings returned by always have their quotes removed, equivalent to specifying in . JSON_VALUE(jsonb '[1,2]', 'strict $[$off]' PASSING 1 as off) → \n\n The expression is converted to by an implicit cast if the expression is not already of type . Note, however, that any parsing errors that occur during that conversion are thrown unconditionally, that is, are not handled according to the (specified or implicit) clause. returns an SQL NULL if returns a JSON , whereas returns the JSON as is.\n\nis an SQL/JSON function which queries data and presents the results as a relational view, which can be accessed as a regular SQL table. You can use inside the clause of a , , or and as data source in a statement. Taking JSON data as input, uses a JSON path expression to extract a part of the provided data to use as a row pattern for the constructed view. Each SQL/JSON value given by the row pattern serves as source for a separate row in the constructed view. To split the row pattern into columns, provides the clause that defines the schema of the created view. For each column, a separate JSON path expression can be specified to be evaluated against the row pattern to get an SQL/JSON value that will become the value for the specified column in a given output row. JSON data stored at a nested level of the row pattern can be extracted using the clause. Each clause can be used to generate one or more columns using the data from a nested level of the row pattern. Those columns can be specified using a clause that looks similar to the top-level COLUMNS clause. Rows constructed from NESTED COLUMNS are called child rows and are joined against the row constructed from the columns specified in the parent clause to get the row in the final view. Child columns themselves may contain a specification thus allowing to extract data located at arbitrary nesting levels. Columns produced by multiple s at the same level are considered to be siblings of each other and their rows after joining with the parent row are combined using UNION. The rows produced by are laterally joined to the row that generated them, so you do not have to explicitly join the constructed view with the original table holding data. Each syntax element is described below in more detail. The specifies the input document to query, the is an SQL/JSON path expression defining the query, and is an optional name for the . The optional clause provides data values for the variables mentioned in the . The result of the input data evaluation using the aforementioned elements is called the row pattern, which is used as the source for row values in the constructed view. The clause defining the schema of the constructed view. In this clause, you can specify each column to be filled with an SQL/JSON value obtained by applying a JSON path expression against the row pattern. has the following variants: Adds an ordinality column that provides sequential row numbering starting from 1. Each (see below) gets its own counter for any nested ordinality columns. Inserts an SQL/JSON value obtained by applying against the row pattern into the view's output row after coercing it to specified . Specifying makes it explicit that you expect the value to be a valid object. It only makes sense to specify if is one of , , , , , , , or a domain over these types. Optionally, you can specify and clauses to format the output. Note that specifying overrides if also specified, because unquoted literals do not constitute valid values. Optionally, you can use and clauses to specify whether to throw the error or return the specified value when the result of JSON path evaluation is empty and when an error occurs during JSON path evaluation or when coercing the SQL/JSON value to the specified type, respectively. The default for both is to return a value. This clause is internally turned into and has the same semantics as or . The latter if the specified type is not a scalar type or if either of , , or clause is present. Inserts a boolean value obtained by applying against the row pattern into the view's output row after coercing it to specified . The value corresponds to whether applying the expression to the row pattern yields any values. The specified should have a cast from the type. Optionally, you can use to specify whether to throw the error or return the specified value when an error occurs during JSON path evaluation or when coercing SQL/JSON value to the specified type. The default is to return a boolean value . This clause is internally turned into and has the same semantics as . Extracts SQL/JSON values from nested levels of the row pattern, generates one or more columns as defined by the subclause, and inserts the extracted SQL/JSON values into those columns. The expression in the subclause uses the same syntax as in the parent clause. The syntax is recursive, so you can go down multiple nested levels by specifying several subclauses within each other. It allows to unnest the hierarchy of JSON objects and arrays in a single function invocation rather than chaining several expressions in an SQL statement. In each variant of described above, if the clause is omitted, path expression is used, where is the provided column name. The optional serves as an identifier of the provided . The name must be unique and distinct from the column names. The optional can be used to specify how to handle errors when evaluating the top-level . Use if you want the errors to be thrown and to return an empty table, that is, a table containing 0 rows. Note that this clause does not affect the errors that occur when evaluating columns, for which the behavior depends on whether the clause is specified against a given column. In the examples that follow, the following table containing JSON data will be used: The following query shows how to use to turn the JSON objects in the table to a view containing columns for the keys , , and contained in the original JSON along with an ordinality column: SELECT jt.* FROM my_films, JSON_TABLE (js, '$.favorites[*]' COLUMNS ( id FOR ORDINALITY, kind text PATH '$.kind', title text PATH '$.films[*].title' WITH WRAPPER, director text PATH '$.films[*].director' WITH WRAPPER)) AS jt; The following is a modified version of the above query to show the usage of arguments in the filter specified in the top-level JSON path expression and the various options for the individual columns: SELECT jt.* FROM my_films, JSON_TABLE (js, '$.favorites[*] ? (@.films[*].director == $filter)' PASSING 'Alfred Hitchcock' AS filter, 'Vertigo' AS filter2 COLUMNS ( id FOR ORDINALITY, kind text PATH '$.kind', title text FORMAT JSON PATH '$.films[*].title' OMIT QUOTES, director text PATH '$.films[*].director' KEEP QUOTES)) AS jt; The following is a modified version of the above query to show the usage of for populating title and director columns, illustrating how they are joined to the parent columns id and kind: SELECT jt.* FROM my_films, JSON_TABLE ( js, '$.favorites[*] ? (@.films[*].director == $filter)' PASSING 'Alfred Hitchcock' AS filter COLUMNS ( id FOR ORDINALITY, kind text PATH '$.kind', NESTED PATH '$.films[*]' COLUMNS ( title text FORMAT JSON PATH '$.title' OMIT QUOTES, director text PATH '$.director' KEEP QUOTES))) AS jt; The following is the same query but without the filter in the root path: SELECT jt.* FROM my_films, JSON_TABLE ( js, '$.favorites[*]' COLUMNS ( id FOR ORDINALITY, kind text PATH '$.kind', NESTED PATH '$.films[*]' COLUMNS ( title text FORMAT JSON PATH '$.title' OMIT QUOTES, director text PATH '$.director' KEEP QUOTES))) AS jt; The following shows another query using a different object as input. It shows the UNION \"sibling join\" between paths and and also the usage of column at levels (columns , , and ): SELECT * FROM JSON_TABLE ( '{\"favorites\": {\"movies\": [{\"name\": \"One\", \"director\": \"John Doe\"}, {\"name\": \"Two\", \"director\": \"Don Joe\"}], \"books\": [{\"name\": \"Mystery\", \"authors\": [{\"name\": \"Brown Dan\"}]}, {\"name\": \"Wonder\", \"authors\": [{\"name\": \"Jun Murakami\"}, {\"name\":\"Craig Doe\"}]}] }}'::json, '$.favorites[*]' COLUMNS ( user_id FOR ORDINALITY, NESTED '$.movies[*]' COLUMNS ( movie_id FOR ORDINALITY, mname text PATH '$.name', director text), NESTED '$.books[*]' COLUMNS ( book_id FOR ORDINALITY, bname text PATH '$.name', NESTED '$.authors[*]' COLUMNS ( author_id FOR ORDINALITY, author_name text PATH '$.name')))); user_id | movie_id | mname | director | book_id | bname | author_id | author_name ---------+----------+-------+----------+---------+---------+-----------+-------------- 1 | 1 | One | John Doe | | | | 1 | 2 | Two | Don Joe | | | | 1 | | | | 1 | Mystery | 1 | Brown Dan 1 | | | | 2 | Wonder | 1 | Jun Murakami 1 | | | | 2 | Wonder | 2 | Craig Doe (5 rows)"
    },
    {
        "link": "https://dba.stackexchange.com/questions/322634/how-to-filter-rows-based-on-a-nested-json-array-field",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    }
]