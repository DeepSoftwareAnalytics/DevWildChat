[
    {
        "link": "https://docs.scala-lang.org/overviews/collections-2.13/iterators.html",
        "document": "An iterator is not a collection, but rather a way to access the elements of a collection one by one. The two basic operations on an iterator are and . A call to will return the next element of the iterator and advance the state of the iterator. Calling again on the same iterator will then yield the element one beyond the one returned previously. If there are no more elements to return, a call to will throw a . You can find out whether there are more elements to return using Iterator’s method.\n\nThe most straightforward way to “step through” all the elements returned by an iterator uses a while-loop:\n\nIterators in Scala also provide analogues of most of the methods that you find in the and classes. For instance, they provide a method which executes a given procedure on each element returned by an iterator. Using , the loop above could be abbreviated to:\n\nAs always, for-expressions can be used as an alternate syntax for expressions involving , , , and , so yet another way to print all elements returned by an iterator would be:\n\nThere’s an important difference between the foreach method on iterators and the same method on iterable collections: When called on an iterator, will leave the iterator at its end when it is done. So calling again on the same iterator will fail with a . By contrast, when called on a collection, leaves the number of elements in the collection unchanged (unless the passed function adds or removes elements, but this is discouraged, because it may lead to surprising results).\n\nThe other operations that has in common with have the same property. For instance, iterators provide a method, which returns a new iterator:\n\nAs you can see, after the call to , the iterator hasn’t advanced to its end, but traversing the iterator resulting from the call to also traverses and advances it to its end.\n\nAnother example is the method, which can be used to find the first elements of an iterator that has a certain property. For instance, to find the first word in the iterator above that has at least two characters you could write:\n\nNote again that was changed by the call to : it now points to the second word “number” in the list. In fact, and the result returned by will return exactly the same sequence of elements.\n\nOne way to circumvent this behavior is to the underlying iterator instead of calling methods on it directly. The two iterators that result will each return exactly the same elements as the underlying iterator :\n\nThe two iterators work independently: advancing one does not affect the other, so that each can be destructively modified by invoking arbitrary methods. This creates the illusion of iterating over the elements twice, but the effect is achieved through internal buffering. As usual, the underlying iterator cannot be used directly and must be discarded.\n\nIn summary, iterators behave like collections if one never accesses an iterator again after invoking a method on it. The Scala collection libraries make this explicit with an abstraction IterableOnce, which is a common superclass of Iterable and Iterator. only has two methods: and . If an object is in fact an , its operation always returns itself, in its current state, but if it is an , its operation always return a new . A common use case of is as an argument type for methods that can take either an iterator or a collection as argument. An example is the appending method in class . It takes an parameter, so you can append elements coming from either an iterator or a collection.\n\nAll operations on iterators are summarized below.\n\nUnlike operations directly on a concrete collection like , operations on are lazy.\n\nA lazy operation does not immediately compute all of its results. Instead, it computes the results as they are individually requested.\n\nSo the expression would not print anything to the screen. The method in this case doesn’t apply its argument function to the values in the range, it returns a new that will do this as each one is requested. Adding to the end of that expression will actually print the elements.\n\nA consequence of this is that a method like or won’t necessarily apply its argument function to all the input elements. The expression would only print the values to , for instance, since those are only ones that will be requested from the returned by .\n\nThis is one of the reasons why it’s important to only use pure functions as arguments to , , and similar methods. Remember, a pure function has no side-effects, so one would not normally use in a . is used to demonstrate laziness as it’s not normally visible with pure functions.\n\nLaziness is still valuable, despite often not being visible, as it can prevent unneeded computations from happening, and can allow for working with infinite sequences, like so:\n\nSometimes you want an iterator that can “look ahead”, so that you can inspect the next element to be returned without advancing past that element. Consider for instance, the task to skip leading empty strings from an iterator that returns a sequence of strings. You might be tempted to write the following\n\nBut looking at this code more closely, it’s clear that this is wrong: The code will indeed skip leading empty strings, but it will also advance past the first non-empty string!\n\nThe solution to this problem is to use a buffered iterator. Class BufferedIterator is a subclass of Iterator, which provides one extra method, . Calling on a buffered iterator will return its first element but will not advance the iterator. Using a buffered iterator, skipping empty words can be written as follows.\n\nEvery iterator can be converted to a buffered iterator by calling its method. Here’s an example:\n\nNote that calling on the buffered iterator does not advance it. Therefore, the subsequent call returns the same value as .\n\nAs usual, the underlying iterator must not be used directly and must be discarded.\n\nThe buffered iterator only buffers the next element when is invoked. Other derived iterators, such as those produced by and , may buffer arbitrary subsequences of the underlying iterator. But iterators can be efficiently joined by adding them together with :\n\nIn the second version of , the unconsumed zeros are buffered internally. In the first version, any leading zeros are dropped and the desired result constructed as a concatenated iterator, which simply calls its two constituent iterators in turn."
    },
    {
        "link": "https://docs.scala-lang.org/overviews/collections/iterators.html",
        "document": "An iterator is not a collection, but rather a way to access the elements of a collection one by one. The two basic operations on an iterator are and . A call to will return the next element of the iterator and advance the state of the iterator. Calling again on the same iterator will then yield the element one beyond the one returned previously. If there are no more elements to return, a call to will throw a . You can find out whether there are more elements to return using Iterator’s method.\n\nThe most straightforward way to “step through” all the elements returned by an iterator uses a while-loop:\n\nIterators in Scala also provide analogues of most of the methods that you find in the , and classes. For instance, they provide a method which executes a given procedure on each element returned by an iterator. Using , the loop above could be abbreviated to:\n\nAs always, for-expressions can be used as an alternate syntax for expressions involving , , , and , so yet another way to print all elements returned by an iterator would be:\n\nThere’s an important difference between the foreach method on iterators and the same method on traversable collections: When called on an iterator, will leave the iterator at its end when it is done. So calling again on the same iterator will fail with a . By contrast, when called on a collection, leaves the number of elements in the collection unchanged (unless the passed function adds or removes elements, but this is discouraged, because it may lead to surprising results).\n\nThe other operations that Iterator has in common with have the same property. For instance, iterators provide a method, which returns a new iterator:\n\nAs you can see, after the call to , the iterator has advanced to its end.\n\nAnother example is the method, which can be used to find the first elements of an iterator that has a certain property. For instance, to find the first word in the iterator above that has at least two characters you could write:\n\nNote again that was changed by the call to : it now points to the second word “number” in the list. In fact, and the result returned by will return exactly the same sequence of elements.\n\nOne way to circumvent this behavior is to the underlying iterator instead of calling methods on it directly. The two iterators that result will each return exactly the same elements as the underlying iterator :\n\nThe two iterators work independently: advancing one does not affect the other, so that each can be destructively modified by invoking arbitrary methods. This creates the illusion of iterating over the elements twice, but the effect is achieved through internal buffering. As usual, the underlying iterator cannot be used directly and must be discarded.\n\nIn summary, iterators behave like collections if one never accesses an iterator again after invoking a method on it. The Scala collection libraries make this explicit with an abstraction TraversableOnce, which is a common superclass of Traversable and Iterator. As the name implies, objects can be traversed using but the state of that object after the traversal is not specified. If the object is in fact an , it will be at its end after the traversal, but if it is a , it will still exist as before. A common use case of is as an argument type for methods that can take either an iterator or a traversable as argument. An example is the appending method in class . It takes a parameter, so you can append elements coming from either an iterator or a traversable collection.\n\nAll operations on iterators are summarized below.\n\nUnlike operations directly on a concrete collection like , operations on are lazy.\n\nA lazy operation does not immediately compute all of its results. Instead, it computes the results as they are individually requested.\n\nSo the expression would not print anything to the screen. The method in this case doesn’t apply its argument function to the values in the range, it returns a new that will do this as each one is requested. Adding to the end of that expression will actually print the elements.\n\nA consequence of this is that a method like or won’t necessarily apply its argument function to all the input elements. The expression would only print the values to , for instance, since those are only ones that will be requested from the returned by .\n\nThis is one of the reasons why it’s important to only use pure functions as arguments to , , and similar methods. Remember, a pure function has no side-effects, so one would not normally use in a . is used to demonstrate laziness as it’s not normally visible with pure functions.\n\nLaziness is still valuable, despite often not being visible, as it can prevent unneeded computations from happening, and can allow for working with infinite sequences, like so:\n\nSometimes you want an iterator that can “look ahead”, so that you can inspect the next element to be returned without advancing past that element. Consider for instance, the task to skip leading empty strings from an iterator that returns a sequence of strings. You might be tempted to write the following\n\nBut looking at this code more closely, it’s clear that this is wrong: The code will indeed skip leading empty strings, but it will also advance past the first non-empty string!\n\nThe solution to this problem is to use a buffered iterator. Class BufferedIterator is a subclass of Iterator, which provides one extra method, . Calling on a buffered iterator will return its first element but will not advance the iterator. Using a buffered iterator, skipping empty words can be written as follows.\n\nEvery iterator can be converted to a buffered iterator by calling its method. Here’s an example:\n\nNote that calling on the buffered iterator does not advance it. Therefore, the subsequent call returns the same value as .\n\nAs usual, the underlying iterator must not be used directly and must be discarded.\n\nThe buffered iterator only buffers the next element when is invoked. Other derived iterators, such as those produced by and , may buffer arbitrary subsequences of the underlying iterator. But iterators can be efficiently joined by adding them together with :\n\nIn the second version of , the unconsumed zeros are buffered internally. In the first version, any leading zeros are dropped and the desired result constructed as a concatenated iterator, which simply calls its two constituent iterators in turn."
    },
    {
        "link": "https://geeksforgeeks.org/iterators-in-scala",
        "document": "An iterator is a way to access elements of a collection one-by-one. It resembles to a collection in terms of syntax but works differently in terms of functionality. An iterator defined for any collection does not load the entire collection into the memory but loads elements one after the other. Therefore, iterators are useful when the data is too large for the memory. To access elements we can make use of hasNext() to check if there are elements available and next() to print the next element.\n\n Syntax:\n\nWe can define an iterator for any collection(Arrays, Lists, etc) and can step through the elements of that particular collection.\n\n Example:\n• Using while loop: Simplest way to access elements is to use while loop along with hasNext and next methods.\n• Using foreach action: We can make use of foreach to print elements by passing println function as parameter. Note that foreach is a higher order function that takes another function as parameter. In other words, println function is applied on every element.\n• Using for loop: Another straightforward way is to use for loop. It works in very similar way as accessing elements of any collection using for loop.\n• Using built-in functions min and max Iterators can be traversed only once. Therefore, we should redefine the iterator after finding the maximum value.\n\nNOTE: Both functions large() and small() are written as two separate codes to reduce burden on online compiler."
    },
    {
        "link": "https://scala-lang.org/api/2.13.5/scala/collection/AbstractIterator.html",
        "document": ""
    },
    {
        "link": "https://data-flair.training/blogs/scala-iterator",
        "document": "We believe you’ve come here after all other collections. Before proceeding, you should read up on Scala Tuples, another kind of collection. In this tutorial on Scala Iterator, we will discuss iterators, how to create iterator in Scala and process them, and what methods to call on them.\n\nAn Introduction to Iterators in Scala\n\nAfter collections like Lists, Sets, and Maps, we will discuss a way to iterate on them(how we can access their elements one by one).\n\nTwo important methods in Scala Iterator are next() and hasNext. hasNext will tell if there is another element to return, while next() will return that element.\n\nBasically, to declare an iterator in Scala over a collection, we pass values to Iterator().\n\nWe take this iterator:\n\nHence, let’s take a simple while loop to iterate:\n\n2\n\n 4\n\n 3\n\n 7\n\n 9\n\n If we hadn’t provided the println() to it.next(), it would print nothing. Also, once it reaches the end of the iterator, the iterator is now empty.\n\nWe can use it.min and it.max to determine the largest and smallest elements in a Scala iterator.\n\nThis will return the smallest value in the Scala iterator.\n\nThis Scala Iterator will return the largest value in the iterator.\n\nWow, okay, we must declare the iterator again since it’s empty now.\n\nSo, let’s check if the Scala iterator has exhausted now:\n\nYes. Yes, it has.\n\nFurther, we can use either of two methods- size and length.\n\nMethods to Call on an Iterator in Scala\n\nNext, here are some common methods we can call on an iterator:\n\nIf the Scala iterator has another element to return, this returns true; otherwise, false.\n\nNow, this returns the next element from the iterator.\n\nThis appends the elements of the Scala iterator to a String Builder and returns it.\n\nThis lets us include a separator for the above functionality.\n\nThis returns a buffered iterator from the iterator.\n\nIf the Scala iterator contains element elem, this returns true; otherwise, false.\n\nThis copies the elements of the array, beginning at index start, and for length len, into an Array.\n\nThis counts the number of elements that satisfy a predicate and returns this count.\n\nThis moves the iterator n places ahead. If n is greater than the iterator’s length, this simply exhausts it.\n\nThis keeps advancing the iterator as long as the predicate is satisfied.\n\nThis creates a duplicate of the iterator that will iterate over the same sequence of values.\n\nIf the predicate holds true for some values in the iterator, this returns true; otherwise, false.\n\nThis filters out such elements (see above)\n\nParallelly, this will create a new iterator with only those elements that do not satisfy the predicate.\n\nThis returns the first value, if any, that satisfies the predicate.\n\nIf the predicate holds true for all values in the Scala iterator, this returns true; otherwise, false.\n\nIf the iterator is empty, this returns true; otherwise, false.\n\nThis returns the index of the first occurrence of the element elem in the Scala iterator.\n\nThis returns the index of the first value that satisfies the predicate. If there’s none, it returns -1.\n\nIf hasNext returns false, then the iterator is empty.\n\nIf we can repeatedly traverse the iterator, this returns true; otherwise, false.\n\nHere, this returns the number of elements in the iterator. Once we’ve called this method, the iterator exhausts.\n\nThis applies the function to every value in the iterator and then returns a new iterator from this.\n\nThis returns the largest element from the Scala iterator.\n\nThis returns the smallest element from the iterator.\n\nThis represents all the elements of the iterator as a String.\n\nThis allows us to declare a separator for the same (see above)\n\nIf the iterator in Scala is empty, this returns false; otherwise, true.\n\nThis multiplies all elements from the iterator and returns the result.\n\nIf both iterators produce the same elements in the same order, this returns true; otherwise, false.\n\nThis returns a sequential view of the iterator in Scala.\n\nThis returns the size of the iterator.\n\nThis creates a new iterator with elements from from until until.\n\nThis returns the sum of all elements in the Scala iterator.\n\nThis returns the first n values from the iterator, or the entire iterator, whichever is shorter.\n\nThis returns an Array from the elements of the iterator.\n\nThis returns a Buffer from the iterator’s elements.\n\nThis returns a Scala iterable holding all elements of the iterator in Scala. This doesn’t terminate for infinite iterators. Let’s take a Scala Iterable Example.\n\nBasically, this returns a new iterator from the iterator.\n\nThis Scala iterator to list returns a List from the iterator.\n\nThis returns a Sequence from the iterator.\n\nThis represents the iterator as a String.\n\nThis returns a new Scala iterator holding pairs of corresponding elements in the iterator. How many elements does this return? Whatever’s minimum of the sizes of both iterators.\n\nSo, this was all about Scala Iterators. Hope you like our explanation.\n\nFinally, we have seen Scala iterators. Moreover, we also discuss the ways to access collection elements one by one. Furthermore, if you have any query, feel free to ask in the comment section."
    },
    {
        "link": "https://jayconrod.com/posts/27/processing-large-files-in-scala",
        "document": "Part of my research lately has been analyzing the heap behavior of several network applications. This involves logging every , , load, and store that occurs while a program is executing, then running a number of analysis tools on the log later. I decided to write my log processing tools in Scala since the combination of pattern matching and good data structures allows me to write new analysis tools very quickly.\n\nWhile writing these tools, I experienced a lot of performance problems. Some of the logs can get quite large (on the order of gigabytes). Here are some of the techniques I used to make things run at an acceptable speed.\n\nBy default, Scala sets the maximum heap size to 256 MB. This is not nearly enough if you have any reasonably large data set that you want to hold in memory. If you exceed this limit, you will get an , and your program will crash. There will also be substantial garbage collection overhead as you approach the limit. With Java, you can increase the maximum heap size by passing the command line option followed by the maximum size of the heap. Although the Scala code runner doesn't accept Java arguments, it does let you pass them directly to Java through the environment variable. So to start a Scala program with a 4GB heap, you would run it like this:\n\nIf you are reading individual bytes from a binary file with a , you are going to have terrible performance. Every time you read from the stream, Java will make a system call, which is expensive. This can be mitigated by wrapping the file with a , but you still need to make a number of system calls proportional to the size of the file.\n\nJava lets you map the contents of a file directly into memory using and from the java.nio package. This works like the system call in C. The file's contents will appear in your address space, but since it won't be on the heap, it will not cause any additional garbage collection overhead. Once a file is mapped, you can read data from it using the methods in without making any additional system calls. When you read from a part of the file that hasn't been read before, the kernel will load that section of the file automatically.\n\nYou can map a file like this:\n\nThere are some important caveats for memory mapped I/O. First, each mapping can only cover 2GB of a file. This is apparently because ByteBuffer uses signed 32-bit integers for offsets and positions. Use multiple mappings if you need to map a larger file, one for each 2GB chunk. Second, there is no way to manually unmap a file. Unmapping occurs automatically when the object gets garbage collected, but there is no way to control when that occurs. Because of both of these caveats, I would highly recommend running a 64-bit JVM to avoid exhausting your virtual address space.\n\n3. Make Java do endian conversion for you\n\nMost computers are based on the x86 architecture, which means that binary values in your data are probably in little-endian format (least significant byte first). By default, Java expects file data to be in big-endian format, so you would normally have to run a bit of code to swap bytes every time you read an integer. If you're using memory mapped I/O (or if you're using a at all), you can set the byte order to little-endian to eliminate this overhead:\n\nChances are, the JVM will still do some byte order conversions internally, but these should be on highly optimized code paths, i.e., not random JITed byte code).\n\n4. Use streams instead of lists or arrays\n\nWhen I first wrote my data processing tools, I followed a very simple pattern:\n• make one or more passes over the ArrayBuffer\n\nFollowing this pattern, my programs would read the log quickly at first but would get slower and slower as more data was put on the heap, eventually grinding to a halt. The data processing passes wouldn't even get to run because my programs would run out of memory. Since many of my tools only need to make one pass over the log, it made more sense to present the sequence of events as a stream.\n\nStreams in Scala are like lists, but they don't evaluate their contents until requested. This means a program can read and process data at the same time. Memory usage is kept at a fixed level, since new events aren't read until they are needed, and old events can be garbage collected. If you need to make multiple passes over a large data set, streams are still useful. Even though old events may have been garbage collected, reading them a second time will be fast since the file will probably still be in the kernel's buffer cache.\n\nThe event and the position of the next event in the buffer are computed lazily. The event is returned by . The rest of the stream is generated in by creating a new at the position of the next event.\n\nThe stream can be initialized and used like this:\n\nWhen I write code, I usually strive for simplicity and readability over performance. This is not always a best practice, especially when you are running a very simple, readable O(n2) algorithm on a 30 million element data set. If you can switch from a O(n) container to one that is O(lg n) or O(1), it will probably boost your performance significantly. The extra complication may be worth it."
    },
    {
        "link": "https://blog.lunatech.com/posts/2023-07-28-streams-in-scala--an-introductory-guide",
        "document": "There are many reasons for using a stream-processing approach when writing software. In this blog post I’m going to focus on just one of those reasons: Memory Management. By processing elements one at a time streams enables you to avoid loading the entire dataset into memory and reducing the risk of encountering the dreaded 'Out of memory' error. Streams provide a lazy evaluation mechanism where elements are computed on-demand rather than being eagerly evaluated and stored in memory. This lazy evaluation, retaining only the necessary elements in memory, allows for efficient memory utilization especially when dealing with large datasets or potentially infinite sequences of data. With streams, you can confidently tackle memory-intensive tasks, knowing that the memory footprint is optimized, leading to more stable and scalable applications. If you’ve ever run a program that has thrown the infamous Out Of Memory Error (OOM) then you’re going to appreciate the power of streams in Scala. Let’s take a look at an example. What would be the result of running this code: The above code would result in When the above snippet is ran, the jvm tries to allocate memory for the elements of the range and the first transformer ( map operation ) is called immediately, which also tries to allocate memory for each element in the map. In total, we end up having 100,000,000 elements which the jvm needs to allocate memory for in the heap space which exceeds the maximum heap size specified through the -Xmx flag on the jvm. The \"GC overhead limit exceeded\" error is an error message commonly encountered in Java Virtual Machine (JVM) based applications. This error occurs when the garbage collector is spending an excessive amount of time collecting garbage with little memory reclaimed. This error message is essentially an indication that the JVM is struggling to free up enough memory, and the application’s performance is significantly impacted. It typically occurs when the garbage collector is running continuously, consuming an excessive amount of CPU time. To prevent the garbage collector from consuming an excessive amount of CPU time with little benefit, the JVM defines a threshold known as the \"GC overhead limit.\" Once this is reached, An error is thrown. How would we avoid running into this kind of error? Whilst we can consider increasing the heap size allocated to the JVM using the -Xmx flag, I would strongly discourage this as it doesn’t fix the problem when we have more data or infinite data. This is where streaming comes in! If your applications continuously require you to scale vertically ( increase the memory allocated to the application ) then you may want to consider building such application the streaming way. This will help you save cost and also make your application more stable. The above code would be rewritten to the following: The above fixes the issue. It’s the same task but with the use of toStream. What changed? Chaining made ( 1 to 10,000 ) not to be computed ( 1,2,3…​10000) when the code is run except when it is required, So what will see when you output ? you will see something like . So the next question would be, when is an element of a stream required? An element of a stream is required when you call terminating operations like . These operations require one element after the other from the stream when it is called. One other thing you would notice if you ran that piece of code is that your IDE says that toStream is deprecated. We will also talk about this in the upcoming section.\n\nStream in Scala is part of the collection hierarchy which extends LinearSeq. They’re like views, only the elements that are accessed are computed. In views, elements are recomputed each time they’re accessed. In a stream elements are retained as they’re evaluated. Other than this behaviour, a Stream behaves similarly to a List. The elements of stream are lazily computed. In the Scala Stream, only the first element is pre-computed. As of Scala 2.13 Stream was replaced with LazyList where no element is computed unless requested. LazyList is designed to address the issues with Stream and provides a more predictable evaluation model. // Scala prior to 2.13 (1 to 10000).toStream // output: Stream(1, <not computed>) // Scala >= 2.13 (1 to 10000).to(LazyList) // output: LazyList(<not computed>) Transformer methods are collection methods, they’re part of the collections API. Transformer methods convert a given input collection to a new output collection, based on a function you provide which maps input to output. Examples of Transformer methods includes , and . Terminator methods are collection API methods which perform a final computation on a collection and return a non-collection result, such as an integer or a boolean value, for example. Terminator methods effectively terminate the computation and produce a final output. Examples of terminator methods include , , and . Be careful with Terminator methods. Calls to these methods are evaluated immediately and can easily cause java.lang.OutOfMemoryError errors:\n\nWhile you can begin using LazyList collections with the information provided so far, I think it would also be good to have a basic understanding of the LazyList Implementation. Call-by-name (also known as pass-by-name) is a parameter evaluation strategy in programming languages where the argument expression is not evaluated before it is passed to a function or method. Instead, the expression is evaluated each time it is referenced within the function or method body. Just for note, the other parameter evaluation strategy is called, Call-by-value ( CBV ) When you create a LazyList, these are generalised summaries of the sequence of events. // 1. The apply method is called from LazyList(1,2,3,4,5,7) which then calls the `from` implementation from LazyList companion object def apply[A](elems: A*): CC[A] = from(elems) // 2. Here, The 3rd case create an instance using the newLL method def from[A](coll: collection.IterableOnce[A]): LazyList[A] = coll match { case lazyList: LazyList[A] => lazyList case _ if coll.knownSize == 0 => empty[A] case _ => newLL(stateFromIterator(coll.iterator)) } // And here is the type of parameter the newLL receives. It receives a call-by-name parameter! /** Creates a new LazyList. */ @inline private def newLL[A](state: => State[A]): LazyList[A] = new LazyList[A](() => state) This portion is called call-by-name. The state parameter has a return type of ⇒ State[A]. This parameter is not evaluated when passed, it’s only evaluated when a terminating method is called. So all transforming method operate on the state without it being called. The same CBN is used as in the case below: The parameters below are called call-by name /** An alternative way of building and matching lazy lists using LazyList.cons(hd, tl). */ object cons { /** A lazy list consisting of a given first element and remaining elements * @param hd The first element of the result lazy list * @param tl The remaining elements of the result lazy list */ def apply[A](hd: => A, tl: => LazyList[A]): LazyList[A] = newLL(sCons(hd, newLL(tl.state))) /** Maps a lazy list to its head and tail */ def unapply[A](xs: LazyList[A]): Option[(A, LazyList[A])] = #::.unapply(xs) }\n\nLet’s consider a real-life scenario: Assume we are tasked with finding specific terms (e.g., success, failure, etc.) within large log files from various services running on our server. Our objective is to retrieve all occurrences of these terms and have the option to select the first few results. As you may know, Scala provides a Source API for reading files. In this task, we would compare two approaches and see why one is better than the other. In the above snippet, we chained getLines and toList which ends up loading the content of the file into memory before filter is called. When we call toList, it evaluates immediately, and only after having read all lines from the file the filtering is applied. using a strict data structure like List would be a bad idea because of memory usage because the file could be large. In the above snippet we chained getLines with to(LazyList). With this, the content of the files is not loaded into memory. We then apply the filter and take functions which still don’t load the content. The content of the file is only loaded when we call a terminating method like foreach. The benefit of this approach is that it only compute the first ten lines that match the filter predicate so that we don’t end up loading everything from file.\n\nSome Scala libraries offer enhanced stream processing capabilities compared to the LazyList API. These libraries are implemented following the Reactive stream standard. Reactive Streams is an initiative to provide a standard for asynchronous stream processing with non-blocking back pressure. The Reactive Streams standard establishes two communication channels: an upstream demand channel and a downstream data channel. Publishers follow a request-based approach and only send data when a demand for a certain number of elements arrives through the demand channel. They can then push up to that requested number of elements downstream, either in batches or individually. As long as there is outstanding demand, the publisher can continue pushing data to the subscriber as it becomes available. However, when the demand is exhausted, the publisher cannot send data unless prompted by a demand signal from downstream. This mechanism, known as backpressure, ensures controlled flow and prevents overwhelming the subscriber. In response to backpressure, the source can choose to allocate more resources, slow down its production, or even discard data. To summarise, handling an un-bounded volume of data in an asynchronous system requires some form of control between the producer and the consumer otherwise we would have overwhelming data sent to the consumer from multiple threads. The Reactive stream standard introduces a concept of back-pressure which is a means of communication between the producers and the consumer. The reactive stream defines an interface which must be implemented. The low-level interface of the Reactive streams: This interface is just a representation of the core components of reactive streams and the actual implementation is way harder and beyond the scope of this post. It’s recommended you make use of the high-level stream API The below libraries take into account this reactive stream interface and implement high-level stream API Akka Streams is a powerful and scalable stream processing library built on top of the Akka toolkit. It provides a high-level DSL for composing and executing stream-based computations. Akka Streams offers backpressure support, fault-tolerance, and integration with other Akka components. It’s widely used in building reactive and distributed systems. fs2 (Functional Streams for Scala) formerly called Scalaz-Stream is a functional stream processing library that provides a purely functional, composable, and resource-safe approach to handling streams. It leverages functional programming concepts such as cats-effect and functional abstractions to build complex stream processing pipelines. fs2 focuses on efficiency, type safety, and composability. Beyond stream processing, fs2 can be used for everything from task execution to control flow. ZIO Streams is part of the ZIO ecosystem, which is a powerful and purely functional library for building concurrent and resilient applications. ZIO Streams offers composable, resource-safe, and type-safe stream processing capabilities. It integrates well with other ZIO components, allowing you to build complex and concurrent stream-based workflows. These libraries provide advanced features, performance optimizations, concurrent handling of data, proper error handling and additional abstractions for handling streams in Scala. Depending on your specific requirements and use case, you can choose the library that best aligns with your needs."
    },
    {
        "link": "https://subpath.medium.com/managing-huge-datasets-with-scala-spark-9840ad760424",
        "document": "Spark is awesome! It’s scalable and fast, especially when you writing in a “native Spark” and avoiding custom Udfs. But when you working with large data frames there are some tips that you can use to avoid OOM errors and speed up the whole computation.\n\nHere is a shortlist of the things that I learned from my personal experience.\n\nUsing configuration suited for the task\n\nIt’s always a good idea to start with proper configuration.\n\nIn my opinion, Spark has awesome documentation, highly recommend starting with it. \n\nDepending on how you use your Spark: inside cluster or in a stand-alone mode your configuration will be different. I’m using spark mostly in standalone mode, so here are my examples:\n\nWhen you are working with a large dataset you need to increase default memory allocation and maxResultSize value.\n\nWhen you trying to save a large data frame to the database or some bucket I noticed that sometimes tasks might fail just because default timeouts thresholds are too small.\n\nYou can use the garbage collector provided by JVM\n\nUsing caching in the right place:\n\nCaching is an essential thing in spark and by using it in the correct places you can seriously reduce execution time. Sometime you might consider .persist instead of caching. You can read more about it here.\n\nJoins are a pretty expensive operation to run, there are several tricks you can use, but the one that I found the most useful is re-partitioning before joining. It helps because awesome by the time you get to join you can already perform some operations on your datasets and partitions might be skewed. And skewed partitions will seriously affect join execution time.\n\nThe same goes for groupBy, It usually helps a lot\n\nIn order to get the best performance from Spark, you need to pay attention to partitions skew. There are many great articles about it (1, 2), so I would not repeat them here. But just keep in mind that sometimes repartitions trick my new work if the column that you chose for partitioning is skewed. If you are not sure which column to choose you can always use the salting trick.\n\nI should also mention elephant in the room — User Defined Functions. Because they give you so much freedom it’s sometimes tempting to use them more often than you actually need them.\n\nEvery time when you wanna implement something custom I recommend you to double-check collections with default spark functions from the org.apache.spark.sql.functions. Maybe you can solve your problem using expr :).\n\nI also recommend you to check out spark-daria. It’s a collection with some useful methods that expand Spark capabilities.\n\nDeleting cached datasets after you are done with them\n\nIf you cached some data frames using you can call to delete it from memory.\n\nOr you can flush memory completely using"
    },
    {
        "link": "https://linkedin.com/advice/0/what-best-practices-handling-data-too-large-fit-memory",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/1875795/best-practices-for-storing-and-using-data-frames-too-large-for-memory",
        "document": "I'm working with a large data frame, and have run up against RAM limits. At this point, I probably need to work with a serialized version on the disk. There are a few packages to support out-of-memory operations, but I'm not sure which one will suit my needs. I'd prefer to keep everything in data frames, so the package looks encouraging, but there are still compatibility problems that I can't work around.\n\nWhat's the first tool to reach for when you realize that your data has reached out-of-memory scale?"
    }
]