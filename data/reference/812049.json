[
    {
        "link": "https://realpython.com/python-requests",
        "document": "The Requests library is the de facto standard for making HTTP requests in Python. It abstracts the complexities of making requests behind a beautiful, simple API so that you can focus on interacting with services and consuming data in your application.\n\nThroughout this tutorial, you’ll see some of the most useful features that Requests has to offer as well as ways to customize and optimize those features for different situations that you may come across. You’ll also learn how to use Requests in an efficient way as well as how to prevent requests to external services from slowing down your application.\n\nIn this tutorial, you’ll learn how to:\n• Make requests using the most common HTTP methods\n• Customize your requests’ headers and data using the query string and message body\n• Inspect data from your requests and responses\n• Configure your requests to help prevent your application from backing up or slowing down\n\nFor the best experience working through this tutorial, you should have basic general knowledge of HTTP. That said, you still may be able to follow along fine without it.\n\nIn the upcoming sections, you’ll see how you can install and use in your application. If you want to play with the code examples that you’ll see in this tutorial, as well as some additional ones, then you can download the code examples and work with them locally:\n\nEven though the Requests library is a common staple for many Python developers, it’s not included in Python’s standard library. There are good reasons for that decision, primarily that the library can continue to evolve more freely as a self-standing project. Note: Requests doesn’t support asynchronous HTTP requests directly. If you need async support in your program, you should try out AIOHTTP or HTTPX. The latter library is broadly compatible with Requests’ syntax. Because Requests is a third-party library, you need to install it before you can use it in your code. As a good practice, you should install external packages into a virtual environment, but you may choose to install into your global environment if you’re planning to use it across multiple projects. Whether you’re working in a virtual environment or not, you’ll need to install : Once has finished installing , you can use it in your application. Importing looks like this: Now that you’re all set up, it’s time to begin your journey through Requests. Your first goal will be learning how to make a request.\n\nA is a powerful object for inspecting the results of the request. Make that same request again, but this time store the return value in a variable so that you can get a closer look at its attributes and behaviors: In this example, you’ve captured the return value of , which is an instance of , and stored it in a variable called . You can now use to see a lot of information about the results of your request. The first bit of information that you can gather from is the status code. A status code informs you of the status of the request. For example, a status means that your request was successful, whereas a status means that the resource you were looking for wasn’t found. There are many other possible status codes as well to give you specific insights into what happened with your request. By accessing , you can see the status code that the server returned: returned , which means that your request was successful and the server responded with the data that you were requesting. Sometimes, you might want to use this information to make decisions in your code: With this logic, if the server returns a status code, then your program will print . If the result is a , then your program will print . Requests goes one step further in simplifying this process for you. If you use a instance in a conditional expression, then it’ll evaluate to if the status code was smaller than , and otherwise. Therefore, you can simplify the last example by rewriting the statement: In the code snippet above, you implicitly check whether the of is between and . If it’s not, then you raise an exception that includes the non-success status code in an f-string. Note: This truth value test is possible because is an overloaded method on . This means that the adapted default behavior of takes the status code into account when determining the truth value of the object. Keep in mind that this method is not verifying that the status code is equal to . The reason for this is that other status codes within the to range, such as and , are also considered successful in the sense that they provide some workable response. For example, the status code tells you that the response was successful, but there’s no content to return in the message body. So, make sure you use this convenient shorthand only if you want to know if the request was generally successful. Then, if necessary, you’ll need to handle the response appropriately based on the status code. Let’s say you don’t want to check the response’s status code in an statement. Instead, you want to use Request’s built-in capacities to raise an exception if the request was unsuccessful. You can do this using : If you invoke , then Requests will raise an for status codes between and . If the status code indicates a successful request, then the program will proceed without raising that exception. Now, you know a lot about how to deal with the status code of the response that you got back from the server. However, when you make a request, you rarely only care about the status code of the response. Usually, you want to see more. Next, you’ll see how to view the actual data that the server sent back in the body of the response. The response of a request often has some valuable information, known as a payload, in the message body. Using the attributes and methods of , you can view the payload in a variety of different formats. To see the response’s content in , you use : While gives you access to the raw bytes of the response payload, you’ll often want to convert them into a string using a character encoding such as UTF-8. will do that for you when you access : Because the decoding of to a requires an encoding scheme, Requests will try to guess the encoding based on the response’s headers if you don’t specify one. You can provide an explicit encoding by setting before accessing : If you take a look at the response, then you’ll see that it’s actually serialized JSON content. To get a dictionary, you could take the that you retrieved from and deserialize it using . However, a simpler way to accomplish this task is to use : The of the return value of is a dictionary, so you can access values in the object by key: You can do a lot with status codes and message bodies. But, if you need more information, like metadata about the response itself, then you’ll need to look at the response’s headers. The response headers can give you useful information, such as the content type of the response payload and a time limit on how long to cache the response. To view these headers, access : returns a dictionary-like object, allowing you to access header values by key. For example, to see the content type of the response payload, you can access : There’s something special about this dictionary-like headers object, though. The HTTP specification defines headers as case-insensitive, which means that you’re able to access these headers without worrying about their capitalization: Whether you use the key or , you’ll get the same value. Now that you’ve seen the most useful attributes and methods of in action, you already have a good overview of Requests’ basic usage. You can get content from the Internet and work with the response that you receive. But there’s more to the Internet than plain and straightforward URLs. In the next section, you’ll take a step back and see how your responses change when you customize your requests to account for query string parameters.\n\nAccording to the HTTP specification, , , and the less common requests pass their data through the message body rather than through parameters in the query string. Using Requests, you’ll pass the payload to the corresponding function’s parameter. takes a dictionary, a list of tuples, bytes, or a file-like object. You’ll want to adapt the data that send in the body of your request to the specific needs of the service that you’re interacting with. For example, if your request’s content type is , then you can send the form data as a dictionary: You can also send that same data as a list of tuples: If, however, you need to send JSON data, then you can use the parameter. When you pass JSON data via , Requests will serialize your data and add the correct header for you. Like you learned earlier, the httpbin service accepts test requests and responds with data about the requests. For instance, you can use it to inspect a basic request: You can see from the response that the server received your request data and headers as you sent them. Requests also provides this information to you in the form of a that you’ll inspect in more detail in the next section.\n\nAuthentication helps a service understand who you are. Typically, you provide your credentials to a server by passing data through the header or a custom header defined by the service. All the functions of Requests that you’ve seen to this point provide a parameter called , which allows you to pass your credentials: The request succeeds if the credentials that you pass in the tuple to are valid. When you pass your credentials in a tuple to the parameter, Requests applies the credentials using HTTP’s Basic access authentication scheme under the hood. You may wonder where the string that Requests set as the value for your header comes from. In short, it’s a Base64-encoded string of the username and password with the prefix :\n• First, Requests combines the username and password that you provided, putting a colon in between them. So for the username and password , this becomes .\n• Then, Requests encodes this string in Base64 using . The encoding converts the string to .\n• Finally, Requests adds in front of this Base64 string. This is how the final value for the header becomes in the example shown above. HTTP Basic authentication isn’t very secure, because you can decode the username and password from the Base64 string. That’s why it’s important to always send these requests over HTTPS, which provides an additional layer of security by encrypting the entire HTTP request. You could make the same request by passing explicit Basic authentication credentials using : Though you don’t need to be explicit for Basic authentication, you may want to authenticate using another method. Requests provides other methods of authentication out of the box, such as and . A real-world example of an API that requires authentication is GitHub’s authenticated user API. This endpoint provides information about the authenticated user’s profile. If you try to make a request without credentials, then you’ll see that the status code is : If you don’t provide authentication credentials when accessing a service that requires them, then you’ll get an HTTP error code as a response. To make a request to GitHub’s authenticated user API, you first need to generate a personal access token with the read:user scope. Then you can pass this token as the second element in a tuple to : Like you learned previously, this approach passes the credentials to , which expects a username and a password and sends the credentials as a Base64-encoded string with the prefix : This works, but it’s not the right way to authenticate with a Bearer token—and using an empty string input for the superfluous username is awkward. With Requests, you can supply your own authentication mechanism to fix that. To try this out, create a subclass of and implement : \"\"\"Attach an API token to the Authorization header.\"\"\" Here, your custom mechanism receives a token, then includes that token in the header of your request, also setting the recommended prefix to the string. You can now use this custom token authentication to make your call to GitHub’s authenticated user API: Your custom created a well-formatted string for the header. You can now use this more intuitive way of interacting with a token-based authentication scheme such as the one that parts of GitHub’s API require. Note: While you could construct the authentication string outside of a custom authentication class and pass it directly with , this appoach is discouraged because it can lead to unexpected behavior. When you attempt to set your authentication credentials directly using , then Requests may internally overwrite your input. This can happen, for example, if you have a file that provides authentication credentials. Requests will attempt to get the credentials from the file if you don’t provide an authentication method using . Bad authentication mechanisms can lead to security vulnerabilities. Unless a service requires a custom authentication mechanism for some reason, you’ll always want to use a tried-and-true auth scheme like the built-in Basic authentication or OAuth, for example through Requests-OAuthlib. While you’re thinking about security, consider dealing with SSL certificates using Requests.\n\nAnytime the data that you’re trying to send or receive is sensitive, security is important. The way that you communicate with secure sites over HTTP is by establishing an encrypted connection using SSL, which means that verifying the target server’s SSL certificate is critical. The good news is that Requests does this for you by default. However, there are some cases where you might want to change this behavior. If you want to disable SSL certificate verification, then you pass to the parameter of the request function: InsecureRequestWarning: Unverified HTTPS request is being made to host Requests even warns you when you’re making an insecure request to help you keep your data safe! Note: Requests uses a package called to provide certificate authorities. This lets Requests know which authorities it can trust. Therefore, you should update frequently to keep your connections as secure as possible. Now that you know how to make all sorts of HTTP requests using Requests, authenticated or not, you may wonder about how you can make sure that your program works as quickly as possible. In the next section, you’ll learn about a few ways that you can improve performance with the help of Requests.\n\nWhen using Requests, especially in a production application environment, it’s important to consider performance implications. Features like timeout control, sessions, and retry limits can help you keep your application running smoothly. When you make an inline request to an external service, your system will need to wait for the response before moving on. If your application waits too long for that response, requests to your service could back up, your user experience could suffer, or your background jobs could hang. By default, Requests will wait indefinitely on the response, so you should almost always specify a timeout duration to prevent these issues from happening. To set the request’s timeout, use the parameter. can be an integer or float representing the number of seconds to wait on a response before timing out: In the first request, the request will time out after 1 second. In the second request, the request will time out after 3.05 seconds. You can also pass a tuple to with the following two elements:\n• Connect timeout: The time it allows for the client to establish a connection to the server\n• Read timeout: The time it’ll wait on a response once your client has established a connection Both of these elements should be numbers, and can be of type or : If the request establishes a connection within 3.05 seconds and receives data within 5 seconds of the connection being established, then the response will be returned as it was before. If the request times out, then the function will raise a exception: \"The request did not time out\" Your program can catch the exception and respond accordingly. Until now, you’ve been dealing with high-level APIs such as and . These functions are abstractions of what’s going on when you make your requests. They hide implementation details, such as how connections are managed, so that you don’t have to worry about them. Underneath those abstractions is a class called . If you need to fine-tune your control over how requests are being made or improve the performance of your requests, you may need to use a instance directly. Sessions are used to persist parameters across requests. For example, if you want to use the same authentication across multiple requests, then you can use a session: In this code example, you use a context manager to ensure that the session releases the resources when it doesn’t need them anymore. In line 7, you log in using your custom . You only need to log in once per session, and then you can make multiple authenticated requests. Requests will persist the credentials while the session exists. You then make two requests to the authenticated user API in lines 9 and 10 using instead of . The primary performance optimization of sessions comes in the form of persistent connections. When your app makes a connection to a server using a , it keeps that connection around in a connection pool. When your app wants to connect to the same server again, it’ll reuse a connection from the pool rather than establishing a new one. When a request fails, you may want your application to retry the same request. However, Requests won’t do this for you by default. To apply this functionality, you need to implement a custom transport adapter. Transport adapters let you define a set of configurations for each service that you’re interacting with. For example, say you want all requests to to retry two times before finally raising a . You’d build a transport adapter, set its parameter, and mount it to an existing : In this example, you’ve set up your session so that it’ll retry a maximum of two times when your request to GitHub’s API doesn’t work as expected. When you mount the —in this case, —to , then will adhere to its configuration for each request to . Note: While the implementation shown above works, you won’t see any effect of the retry behavior unless there’s something wrong with your network connection or GitHub’s servers. If you want to play around with code that builds on top of this example, and you’d like to inspect when the retries happen, then you’re in luck. You can download the materials of this tutorial and take a look at : Get Your Code: Click here to download the free sample code that shows you how to use Python’s Requests library. The code in this file improves on the example shown above by using the underlying to further customize the retry functionality. It also adds logging to display debugging output, which gives you a chance to monitor when Python attempted the retries. Requests comes packaged with intuitive implementations for timeouts, transport adapters, and sessions that can help you keep your code efficient and your application resilient."
    },
    {
        "link": "https://requests.readthedocs.io",
        "document": "Requests is an elegant and simple HTTP library for Python, built for human beings.\n\nRequests allows you to send HTTP/1.1 requests extremely easily. There’s no need to manually add query strings to your URLs, or to form-encode your POST data. Keep-alive and HTTP connection pooling are 100% automatic, thanks to urllib3."
    },
    {
        "link": "https://docs.python.org/3/library/http.html",
        "document": "is a package that collects several modules for working with the HyperText Transfer Protocol:\n• None is a low-level HTTP protocol client; for high-level URL opening use\n• None has utilities for implementing state management with cookies\n\nThe module also defines the following enums that help you work with http related code:\n\nThe enum values have several properties to indicate the HTTP status category: A subclass of that defines a set of HTTP methods and descriptions written in English."
    },
    {
        "link": "https://stackoverflow.com/questions/48326026/how-to-handle-http-status-codes-across-a-python-application",
        "document": "I'm trying to standardize my handling of HTTP status codes returned from various APIs in an effort to reduce the amount of code I copy across files.\n\nFor my current application, I have a set of files that each contain one class that all derive inheritance from one master class. I'm using the Python requests module to consume the APIs.\n\nSo far, I've written custom status code handling in each function to continue with a 200, log the request I sent with a 400, log the url for a 404, retry for 5xx, but it's a hassle to keep copying this code across functions and classes.\n\nI'm thinking of the following (note that I've simplified my code here to just use GETs, but in reality, I'm mostly POSTing and receiving back a json response):\n\nHowever, this still seems iffy. I don't know if it's reasonable to try to account for every status code returned by an API. And this would also require me to copy this code to any new project I start working on.\n\nIs there a better way to do this?"
    },
    {
        "link": "https://proxiesapi.com/articles/handling-http-status-codes-with-python-requests",
        "document": "When making HTTP requests in Python, you'll often want to check the status code of the response to determine if the request succeeded or not. The requests library makes this easy.\n\nThis will print the status code integer for the response. Some common codes you may see:\n\nYou can check for specific codes like this:\n\nOr simply check if it's a successful code:\n\nIf the status is 400 or 500, you may want to handle it differently or log the error.\n\nOne tip is to import Python's module which contains dictionaries for status code meanings:\n\nThis prints a text description of each code.\n\nHandling status codes correctly ensures your Python scripts detect issues, retry requests, and even notify you if things fail. Sites often use codes like 429 for rate limiting, so you'll want to catch that."
    },
    {
        "link": "https://crummy.com/software/BeautifulSoup/bs4/doc",
        "document": "Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. But you'll only ever have to deal with about four kinds of objects: , , , and . These objects represent the HTML elements that comprise the page. A object corresponds to an XML or HTML tag in the original document. Tags have a lot of attributes and methods, and I'll cover most of them in Navigating the tree and Searching the tree. For now, the most important methods of a tag are for accessing its name and attributes. If you change a tag's name, the change will be reflected in any markup generated by Beautiful Soup down the line: An HTML or XML tag may have any number of attributes. The tag has an attribute \"id\" whose value is \"boldest\". You can access a tag's attributes by treating the tag like a dictionary: You can access the dictionary of attributes directly as : You can add, remove, and modify a tag's attributes. Again, this is done by treating the tag as a dictionary: HTML 4 defines a few attributes that can have multiple values. HTML 5 removes a couple of them, but defines a few more. The most common multi-valued attribute is (that is, a tag can have more than one CSS class). Others include , , , , and . By default, Beautiful Soup stores the value(s) of a multi-valued attribute as a list: When you turn a tag back into a string, the values of any multi-valued attributes are consolidated: If an attribute looks like it has more than one value, but it's not a multi-valued attribute as defined by any version of the HTML standard, Beautiful Soup stores it as a simple string: You can force all attributes to be stored as strings by passing as a keyword argument into the constructor: You can use to always return the value in a list container, whether it's a string or multi-valued attribute value: If you parse a document as XML, there are no multi-valued attributes: Again, you can configure this using the argument: You probably won't need to do this, but if you do, use the defaults as a guide. They implement the rules described in the HTML specification: A tag can contain strings as pieces of text. Beautiful Soup uses the class to contain these pieces of text: A is just like a Python Unicode string, except that it also supports some of the features described in Navigating the tree and Searching the tree. You can convert a to a Unicode string with : You can't edit a string in place, but you can replace one string with another, using replace_with(): supports most of the features described in Navigating the tree and Searching the tree, but not all of them. In particular, since a string can't contain anything (the way a tag may contain a string or another tag), strings don't support the or attributes, or the method. If you want to use a outside of Beautiful Soup, you should call on it to turn it into a normal Python Unicode string. If you don't, your string will carry around a reference to the entire Beautiful Soup parse tree, even when you're done using Beautiful Soup. This is a big waste of memory. The object represents the parsed document as a whole. For most purposes, you can treat it as a object. This means it supports most of the methods described in Navigating the tree and Searching the tree. You can also pass a object into one of the methods defined in Modifying the tree, just as you would a . This lets you do things like combine two parsed documents: Since the object doesn't correspond to an actual HTML or XML tag, it has no name and no attributes. But sometimes it's useful to reference its (such as when writing code that works with both and objects), so it's been given the special \"[document]\": , , and cover almost everything you'll see in an HTML or XML file, but there are a few leftover bits. The main one you'll probably encounter is the . \"<b><!--Hey, buddy. Want to buy a used parser?--></b>\" The object is just a special type of : # 'Hey, buddy. Want to buy a used parser' But when it appears as part of an HTML document, a is displayed with special formatting: # <!--Hey, buddy. Want to buy a used parser?--> Beautiful Soup defines a few subclasses to contain strings found inside specific HTML tags. This makes it easier to pick out the main body of the page, by ignoring strings that probably represent programming directives found within the page. (These classes are new in Beautiful Soup 4.9.0, and the html5lib parser doesn't use them.) A subclass that represents embedded CSS stylesheets; that is, any strings found inside a tag during document parsing. A subclass that represents embedded Javascript; that is, any strings found inside a tag during document parsing. A subclass that represents embedded HTML templates; that is, any strings found inside a tag during document parsing. Beautiful Soup defines some classes for holding special types of strings that can be found in XML documents. Like , these classes are subclasses of that add something extra to the string on output. A subclass representing the declaration at the beginning of an XML document. A subclass representing the document type declaration which may be found near the beginning of an XML document. A subclass that represents the contents of an XML processing instruction.\n\nHere's the \"Three sisters\" HTML document again: <p class=\"story\">Once upon a time there were three little sisters; and their names were and they lived at the bottom of a well.</p> I'll use this as an example to show you how to move from one part of a document to another. Tags may contain strings and more tags. These elements are the tag's children. Beautiful Soup provides a lot of different attributes for navigating and iterating over a tag's children. Note that Beautiful Soup strings don't support any of these attributes, because a string can't have children. The simplest way to navigate the parse tree is to find a tag by name. To do this, you can use the method: For convenience, just saying the name of the tag you want is equivalent to (if no built-in attribute has that name). If you want the <head> tag, just say : You can use this trick again and again to zoom in on a certain part of the parse tree. This code gets the first <b> tag beneath the <body> tag: (and its convenience equivalent) gives you only the first tag by that name: If you need to get all the <a> tags, you can use : For more complicated tasks, such as pattern-matching and filtering, you can use the methods described in Searching the tree. A tag's children are available in a list called : The object itself has children. In this case, the <html> tag is the child of the object.: A string does not have , because it can't contain anything: Instead of getting them as a list, you can iterate over a tag's children using the generator: If you want to modify a tag's children, use the methods described in Modifying the tree. Don't modify the the list directly: that can lead to problems that are subtle and difficult to spot. The and attributes consider only a tag's direct children. For instance, the <head> tag has a single direct child—the <title> tag: But the <title> tag itself has a child: the string \"The Dormouse's story\". There's a sense in which that string is also a child of the <head> tag. The attribute lets you iterate over all of a tag's children, recursively: its direct children, the children of its direct children, and so on: The <head> tag has only one child, but it has two descendants: the <title> tag and the <title> tag's child. The object only has one direct child (the <html> tag), but it has a whole lot of descendants: If a tag has only one child, and that child is a , the child is made available as : If a tag's only child is another tag, and that tag has a , then the parent tag is considered to have the same as its child: If a tag contains more than one thing, then it's not clear what should refer to, so is defined to be : If there's more than one thing inside a tag, you can still look at just the strings. Use the generator to see all descendant strings: # 'Once upon a time there were three little sisters; and their names were\n\n' # ';\n\nand they lived at the bottom of a well.' Newlines and spaces that separate tags are also strings. You can remove extra whitespace by using the generator instead: # 'Once upon a time there were three little sisters; and their names were' # ';\n\n and they lived at the bottom of a well.' Here, strings consisting entirely of whitespace are ignored, and whitespace at the beginning and end of strings is removed. Continuing the \"family tree\" analogy, every tag and every string has a parent: the tag that contains it. You can access an element's parent with the attribute. In the example \"three sisters\" document, the <head> tag is the parent of the <title> tag: The title string itself has a parent: the <title> tag that contains it: The parent of a top-level tag like <html> is the object itself: And the of a object is defined as None: You can iterate over all of an element's parents with . This example uses to travel from an <a> tag buried deep within the document, to the very top of the document: The generator is a variant of which gives you the entire ancestry of an element, including the element itself: Consider a simple document like this: The <b> tag and the <c> tag are at the same level: they're both direct children of the same tag. We call them siblings. When a document is pretty-printed, siblings show up at the same indentation level. You can also use this relationship in the code you write. You can use and to navigate between page elements that are on the same level of the parse tree: The <b> tag has a , but no , because there's nothing before the <b> tag on the same level of the tree. For the same reason, the <c> tag has a but no : The strings \"text1\" and \"text2\" are not siblings, because they don't have the same parent: In real documents, the or of a tag will usually be a string containing whitespace. Going back to the \"three sisters\" document: You might think that the of the first <a> tag would be the second <a> tag. But actually, it's a string: the comma and newline that separate the first <a> tag from the second: The second <a> tag is then the of the comma string: You can iterate over a tag's siblings with or : # '; and they lived at the bottom of a well.' # 'Once upon a time there were three little sisters; and their names were\n\n' Take a look at the beginning of the \"three sisters\" document: An HTML parser takes this string of characters and turns it into a series of events: \"open an <html> tag\", \"open a <head> tag\", \"open a <title> tag\", \"add a string\", \"close the <title> tag\", \"open a <p> tag\", and so on. The order in which the opening tags and strings are encountered is called document order. Beautiful Soup offers tools for searching a document's elements in document order. The attribute of a string or tag points to whatever was parsed immediately after the opening of the current tag or after the current string. It might be the same as , but it's usually drastically different. Here's the final <a> tag in the \"three sisters\" document. Its is a string: the conclusion of the sentence that was interrupted by the start of the <a> tag: # ';\n\nand they lived at the bottom of a well.' But the of that <a> tag, the thing that was parsed immediately after the <a> tag, is not the rest of that sentence: it's the string \"Tillie\" inside it: That's because in the original markup, the word \"Tillie\" appeared before that semicolon. The parser encountered an <a> tag, then the word \"Tillie\", then the closing </a> tag, then the semicolon and rest of the sentence. The semicolon is on the same level as the <a> tag, but the word \"Tillie\" was encountered first. The attribute is the exact opposite of . It points to the opening tag or string that was parsed immediately before this one: You should get the idea by now. You can use these iterators to move forward or backward in the document as it was parsed: # ';\n\nand they lived at the bottom of a well.'\n\nBeautiful Soup's main strength is in searching the parse tree, but you can also modify the tree and write your changes as a new HTML or XML document. I covered this earlier, in , but it bears repeating. You can rename a tag, change the values of its attributes, add new attributes, and delete attributes: If you set a tag's attribute to a new string, the tag's contents are replaced with that string: Be careful: if the tag contained other tags, they and all their contents will be destroyed. You can add to a tag's contents with . It works just like calling on a Python list: Starting in Beautiful Soup 4.7.0, also supports a method called , which adds every element of a list to a , in order: If you need to add a string to a document, no problem—you can pass a Python string in to , or you can call the constructor: If you want to create a comment or some other subclass of , just call the constructor: # ['Hello', ' there', 'Nice to see you.'] What if you need to create a whole new tag? The best solution is to call the factory method : Only the first argument, the tag name, is required. Because insertion methods return the newly inserted element, you can create, insert, and obtain an element in one step: is just like , except the new element doesn't necessarily go at the end of its parent's . It will be inserted at whatever numeric position you say, similar to on a Python list: # <a href=\"http://example.com/\">I linked to but did not endorse <i>example.com</i></a> # ['I linked to ', 'but did not endorse ', <i>example.com</i>] You can pass more than one element into . All the elements will be inserted, starting at the numeric position you provide. The method inserts tags or strings immediately before something else in the parse tree: The method inserts tags or strings immediately after something else in the parse tree: Both methods return the list of newly inserted elements. removes a tag or string from the tree. It returns the tag or string that was extracted: At this point you effectively have two parse trees: one rooted at the object you used to parse the document, and one rooted at the tag that was extracted. You can go on to call on a child of the element you extracted: removes a tag from the tree, then completely destroys it and its contents: The behavior of a decomposed or is not defined and you should not use it for anything. If you're not sure whether something has been decomposed, you can check its property (new in Beautiful Soup 4.9.0): extracts a tag or string from the tree, then replaces it with one or more tags or strings of your choice: returns the tag or string that got replaced, so that you can examine it or add it back to another part of the tree. The ability to pass multiple arguments into replace_with() is new in Beautiful Soup 4.10.0. wraps an element in the object you specify. It returns the new wrapper: This method is new in Beautiful Soup 4.0.5. is the opposite of . It replaces a tag with whatever's inside that tag. It's good for stripping out markup: Like , returns the tag that was replaced. After calling a bunch of methods that modify the parse tree, you may end up with two or more objects next to each other. Beautiful Soup doesn't have any problems with this, but since it can't happen in a freshly parsed document, you might not expect behavior like the following: You can call to clean up the parse tree by consolidating adjacent strings: This method is new in Beautiful Soup 4.8.0.\n\nAny HTML or XML document is written in a specific encoding like ASCII or UTF-8. But when you load that document into Beautiful Soup, you'll discover it's been converted to Unicode: It's not magic. (That sure would be nice.) Beautiful Soup uses a sub-library called Unicode, Dammit to detect a document's encoding and convert it to Unicode. The autodetected encoding is available as the attribute of the object: If is , that means the document was already Unicode when it was passed into Beautiful Soup: Unicode, Dammit guesses correctly most of the time, but sometimes it makes mistakes. Sometimes it guesses correctly, but only after a byte-by-byte search of the document that takes a very long time. If you happen to know a document's encoding ahead of time, you can avoid mistakes and delays by passing it to the constructor as . Here's a document written in ISO-8859-8. The document is so short that Unicode, Dammit can't get a lock on it, and misidentifies it as ISO-8859-7: We can fix this by passing in the correct : If you don't know what the correct encoding is, but you know that Unicode, Dammit is guessing wrong, you can pass the wrong guesses in as : Windows-1255 isn't 100% correct, but that encoding is a compatible superset of ISO-8859-8, so it's close enough. ( is a new feature in Beautiful Soup 4.4.0.) In rare cases (usually when a UTF-8 document contains text written in a completely different encoding), the only way to get Unicode may be to replace some characters with the special Unicode character \"REPLACEMENT CHARACTER\" (U+FFFD, �). If Unicode, Dammit needs to do this, it will set the attribute to on the or object. This lets you know that the Unicode representation is not an exact representation of the original—some data was lost. If a document contains �, but is , you'll know that the � was there originally (as it is in this paragraph) and doesn't stand in for missing data. When you write out an output document from Beautiful Soup, you get a UTF-8 document, even if the input document wasn't in UTF-8 to begin with. Here's a document written in the Latin-1 encoding: Note that the <meta> tag has been rewritten to reflect the fact that the document is now in UTF-8. If you don't want UTF-8, you can pass an encoding into : You can also call encode() on the object, or any element in the soup, just as if it were a Python string: Any characters that can't be represented in your chosen encoding will be converted into numeric XML entity references. Here's a document that includes the Unicode character SNOWMAN: The SNOWMAN character can be part of a UTF-8 document (it looks like ☃), but there's no representation for that character in ISO-Latin-1 or ASCII, so it's converted into \"☃\" for those encodings: You can use Unicode, Dammit without using Beautiful Soup. It's useful whenever you have data in an unknown encoding and you just want it to become Unicode: Unicode, Dammit's guesses will get a lot more accurate if you install one of these Python libraries: , , or . The more data you give Unicode, Dammit, the more accurately it will guess. If you have your own suspicions as to what the encoding might be, you can pass them in as a list: Unicode, Dammit has two special features that Beautiful Soup doesn't use. You can use Unicode, Dammit to convert Microsoft smart quotes to HTML or XML entities: You can also convert Microsoft smart quotes to ASCII quotes: Hopefully you'll find this feature useful, but Beautiful Soup doesn't use it. Beautiful Soup prefers the default behavior, which is to convert Microsoft smart quotes to Unicode characters along with everything else: Sometimes a document is mostly in UTF-8, but contains Windows-1252 characters such as (again) Microsoft smart quotes. This can happen when a website includes data from multiple sources. You can use to turn such a document into pure UTF-8. Here's a simple example: This document is a mess. The snowmen are in UTF-8 and the quotes are in Windows-1252. You can display the snowmen or the quotes, but not both: Decoding the document as UTF-8 raises a , and decoding it as Windows-1252 gives you gibberish. Fortunately, will convert the string to pure UTF-8, allowing you to decode it to Unicode and display the snowmen and quote marks simultaneously: only knows how to handle Windows-1252 embedded in UTF-8 (or vice versa, I suppose), but this is the most common case. Note that you must know to call on your data before passing it into or the constructor. Beautiful Soup assumes that a document has a single encoding, whatever it might be. If you pass it a document that contains both UTF-8 and Windows-1252, it's likely to think the whole document is Windows-1252, and the document will come out looking like . is new in Beautiful Soup 4.1.0.\n\nIf you're having trouble understanding what Beautiful Soup does to a document, pass the document into the function. (This function is new in Beautiful Soup 4.2.0.) Beautiful Soup will print out a report showing you how different parsers handle the document, and tell you if you're missing a parser that Beautiful Soup could be using: # I noticed that html5lib is not installed. Installing it may help. # Trying to parse your data with html.parser # Here's what html.parser did with the document: Just looking at the output of diagnose() might show you how to solve the problem. Even if not, you can paste the output of when asking for help. There are two different kinds of parse errors. There are crashes, where you feed a document to Beautiful Soup and it raises an exception (usually an ). And there is unexpected behavior, where a Beautiful Soup parse tree looks a lot different than the document used to create it. These problems are almost never problems with Beautiful Soup itself. This is not because Beautiful Soup is an amazingly well-written piece of software. It's because Beautiful Soup doesn't include any parsing code. Instead, it relies on external parsers. If one parser isn't working on a certain document, the best solution is to try a different parser. See Installing a parser for details and a parser comparison. If this doesn't help, you might need to inspect the document tree found inside the object, to see where the markup you're looking for actually ended up.\n• None (on the line ): Caused by running an old Python 2 version of Beautiful Soup under Python 3, without converting the code.\n• None - Caused by running an old Python 2 version of Beautiful Soup under Python 3.\n• None - Caused by running the Python 3 version of Beautiful Soup under Python 2.\n• None - Caused by running Beautiful Soup 3 code in an environment that doesn't have BS3 installed. Or, by writing Beautiful Soup 4 code without knowing that the package name has changed to .\n• None - Caused by running Beautiful Soup 4 code in an environment that doesn't have BS4 installed. By default, Beautiful Soup parses documents as HTML. To parse a document as XML, pass in \"xml\" as the second argument to the constructor: You'll need to have lxml installed.\n• None If your script works on one computer but not another, or in one virtual environment but not another, or outside the virtual environment but not inside, it's probably because the two environments have different parser libraries available. For example, you may have developed the script on a computer that has lxml installed, and then tried to run it on a computer that only has html5lib installed. See Differences between parsers for why this matters, and fix the problem by mentioning a specific parser library in the constructor.\n• None Because HTML tags and attributes are case-insensitive, all three HTML parsers convert tag and attribute names to lowercase. That is, the markup <TAG></TAG> is converted to <tag></tag>. If you want to preserve mixed-case or uppercase tags and attributes, you'll need to parse the document as XML.\n• None (or just about any other ) - This problem shows up in two main situations. First, when you try to print a Unicode character that your console doesn't know how to display. (See this page on the Python wiki for help.) Second, when you're writing to a file and you pass in a Unicode character that's not supported by your default encoding. In this case, the simplest solution is to explicitly encode the Unicode string into UTF-8 with .\n• None - Caused by accessing when the tag in question doesn't define the attribute. The most common errors are and . Use if you're not sure is defined, just as you would with a Python dictionary.\n• None - This usually happens because you expected to return a single tag or string. But returns a list of tags and strings—a object. You need to iterate over the list and look at the of each one. Or, if you really only want one result, you need to use instead of .\n• None - This usually happens because you called and then tried to access the attribute of the result. But in your case, didn't find anything, so it returned , instead of returning a tag or a string. You need to figure out why your call isn't returning anything.\n• None - This usually happens because you're treating a string as though it were a tag. You may be iterating over a list, expecting that it contains nothing but tags, when it actually contains both tags and strings. Beautiful Soup will never be as fast as the parsers it sits on top of. If response time is critical, if you're paying for computer time by the hour, or if there's any other reason why computer time is more valuable than programmer time, you should forget about Beautiful Soup and work directly atop lxml. That said, there are things you can do to speed up Beautiful Soup. If you're not using lxml as the underlying parser, my advice is to start. Beautiful Soup parses documents significantly faster using lxml than using html.parser or html5lib. You can speed up encoding detection significantly by installing the cchardet library. Parsing only part of a document won't save you much time parsing the document, but it can save a lot of memory, and it'll make searching the document much faster.\n\nBeautiful Soup 3 is the previous release series, and is no longer supported. Development of Beautiful Soup 3 stopped in 2012, and the package was completely discontinued in 2021. There's no reason to install it unless you're trying to get very old software to work, but it's published through PyPi as : You can also download a tarball of the final release, 3.2.2. If you ran or , but your code doesn't work, you installed Beautiful Soup 3 by mistake. You need to run . The documentation for Beautiful Soup 3 is archived online. Most code written against Beautiful Soup 3 will work against Beautiful Soup 4 with one simple change. All you should have to do is change the package name from to . So this:\n• None If you get the \"No module named BeautifulSoup\", your problem is that you're trying to run Beautiful Soup 3 code, but you only have Beautiful Soup 4 installed.\n• None If you get the \"No module named bs4\", your problem is that you're trying to run Beautiful Soup 4 code, but you only have Beautiful Soup 3 installed. Although BS4 is mostly backward-compatible with BS3, most of its methods have been deprecated and given new names for PEP 8 compliance. There are numerous other renames and changes, and a few of them break backward compatibility. Here's what you'll need to know to convert your BS3 code and habits to BS4: Beautiful Soup 3 used Python's , a module that was deprecated and removed in Python 3.0. Beautiful Soup 4 uses by default, but you can plug in lxml or html5lib and use that instead. See Installing a parser for a comparison. Since is not the same parser as , you may find that Beautiful Soup 4 gives you a different parse tree than Beautiful Soup 3 for the same markup. If you swap out for lxml or html5lib, you may find that the parse tree changes yet again. If this happens, you'll need to update your scraping code to process the new tree. I renamed three attributes to avoid using words that have special meaning to Python. Unlike my changes to method names (which you'll see in the form of deprecation warnings), these changes did not preserve backwards compatibility. If you used these attributes in BS3, your code will break in BS4 until you change them. Some of the generators used to yield after they were done, and then stop. That was a bug. Now the generators just stop. There is no longer a class for parsing XML. To parse XML you pass in \"xml\" as the second argument to the constructor. For the same reason, the constructor no longer recognizes the argument. Beautiful Soup's handling of empty-element XML tags has been improved. Previously when you parsed XML you had to explicitly say which tags were considered empty-element tags. The argument to the constructor is no longer recognized. Instead, Beautiful Soup considers any empty tag to be an empty-element tag. If you add a child to an empty-element tag, it stops being an empty-element tag. An incoming HTML or XML entity is always converted into the corresponding Unicode character. Beautiful Soup 3 had a number of overlapping ways of dealing with entities, which have been removed. The constructor no longer recognizes the or arguments. (Unicode, Dammit still has , but its default is now to turn smart quotes into Unicode.) The constants , , and have been removed, since they configure a feature (transforming some but not all entities into Unicode characters) that no longer exists. If you want to turn Unicode characters back into HTML entities on output, rather than turning them into UTF-8 characters, you need to use an output formatter. Tag.string now operates recursively. If tag A contains a single tag B and nothing else, then A.string is the same as B.string. (Previously, it was None.) Multi-valued attributes like have lists of strings as their values, not simple strings. This may affect the way you search by CSS class. objects now implement the method, such that two objects are considered equal if they generate the same markup. This may change your script's behavior if you put objects into a dictionary or set. If you pass one of the methods both string and a tag-specific argument like name, Beautiful Soup will search for tags that match your tag-specific criteria and whose Tag.string matches your string value. It will not find the strings themselves. Previously, Beautiful Soup ignored the tag-specific arguments and looked for strings. The constructor no longer recognizes the argument. It's now the parser's responsibility to handle markup correctly. The rarely-used alternate parser classes like and have been removed. It's now the parser's decision how to handle ambiguous markup. The method now returns a Unicode string, not a bytestring."
    },
    {
        "link": "https://beautiful-soup-4.readthedocs.io/en/latest",
        "document": "If you’re using a recent version of Debian or Ubuntu Linux, you can install Beautiful Soup with the system package manager: Beautiful Soup 4 is published through PyPi, so if you can’t install it with the system packager, you can install it with or . The package name is , and the same package works on Python 2 and Python 3. Make sure you use the right version of or for your Python version (these may be named and respectively if you’re using Python 3). If you don’t have or installed, you can download the Beautiful Soup 4 source tarball and install it with . If all else fails, the license for Beautiful Soup allows you to package the entire library with your application. You can download the tarball, copy its directory into your application’s codebase, and use Beautiful Soup without installing it at all. I use Python 2.7 and Python 3.2 to develop Beautiful Soup, but it should work with other recent versions. Beautiful Soup is packaged as Python 2 code. When you install it for use with Python 3, it’s automatically converted to Python 3 code. If you don’t install the package, the code won’t be converted. There have also been reports on Windows machines of the wrong version being installed. If you get the “No module named HTMLParser”, your problem is that you’re running the Python 2 version of the code under Python 3. If you get the “No module named html.parser”, your problem is that you’re running the Python 3 version of the code under Python 2. In both cases, your best bet is to completely remove the Beautiful Soup installation from your system (including any directory created when you unzipped the tarball) and try the installation again. If you get the “Invalid syntax” on the line , you need to convert the Python 2 code to Python 3. You can do this either by installing the package: or by manually running Python’s conversion script on the directory: Beautiful Soup supports the HTML parser included in Python’s standard library, but it also supports a number of third-party Python parsers. One is the lxml parser. Depending on your setup, you might install lxml with one of these commands: Another alternative is the pure-Python html5lib parser, which parses HTML the way a web browser does. Depending on your setup, you might install html5lib with one of these commands: This table summarizes the advantages and disadvantages of each parser library:\n• Lenient (As of Python 2.7.3 and 3.2.)\n• Not as fast as lxml, less lenient than html5lib.\n• The only currently supported XML parser\n• Parses pages the same way a web browser does If you can, I recommend you install and use lxml for speed. If you’re using a version of Python 2 earlier than 2.7.3, or a version of Python 3 earlier than 3.2.2, it’s that you install lxml or html5lib–Python’s built-in HTML parser is just not very good in older versions. Note that if a document is invalid, different parsers will generate different Beautiful Soup trees for it. See Differences between parsers for details.\n\n<p class=\"story\">Once upon a time there were three little sisters; and their names were and they lived at the bottom of a well.</p> I’ll use this as an example to show you how to move from one part of a document to another. Tags may contain strings and other tags. These elements are the tag’s . Beautiful Soup provides a lot of different attributes for navigating and iterating over a tag’s children. Note that Beautiful Soup strings don’t support any of these attributes, because a string can’t have children. The simplest way to navigate the parse tree is to say the name of the tag you want. If you want the <head> tag, just say : You can do use this trick again and again to zoom in on a certain part of the parse tree. This code gets the first <b> tag beneath the <body> tag: Using a tag name as an attribute will give you only the tag by that name: If you need to get the <a> tags, or anything more complicated than the first tag with a certain name, you’ll need to use one of the methods described in Searching the tree, such as : A tag’s children are available in a list called : The object itself has children. In this case, the <html> tag is the child of the object.: A string does not have , because it can’t contain anything: Instead of getting them as a list, you can iterate over a tag’s children using the generator: The and attributes only consider a tag’s children. For instance, the <head> tag has a single direct child–the <title> tag: But the <title> tag itself has a child: the string “The Dormouse’s story”. There’s a sense in which that string is also a child of the <head> tag. The attribute lets you iterate over of a tag’s children, recursively: its direct children, the children of its direct children, and so on: The <head> tag has only one child, but it has two descendants: the <title> tag and the <title> tag’s child. The object only has one direct child (the <html> tag), but it has a whole lot of descendants: If a tag has only one child, and that child is a , the child is made available as : If a tag’s only child is another tag, and tag has a , then the parent tag is considered to have the same as its child: If a tag contains more than one thing, then it’s not clear what should refer to, so is defined to be : If there’s more than one thing inside a tag, you can still look at just the strings. Use the generator: # u'Once upon a time there were three little sisters; and their names were\n\n' # u';\n\nand they lived at the bottom of a well.' These strings tend to have a lot of extra whitespace, which you can remove by using the generator instead: # u'Once upon a time there were three little sisters; and their names were' # u';\n\nand they lived at the bottom of a well.' Here, strings consisting entirely of whitespace are ignored, and whitespace at the beginning and end of strings is removed. Continuing the “family tree” analogy, every tag and every string has a : the tag that contains it. You can access an element’s parent with the attribute. In the example “three sisters” document, the <head> tag is the parent of the <title> tag: The title string itself has a parent: the <title> tag that contains it: The parent of a top-level tag like <html> is the object itself: And the of a object is defined as None: You can iterate over all of an element’s parents with . This example uses to travel from an <a> tag buried deep within the document, to the very top of the document: Consider a simple document like this: The <b> tag and the <c> tag are at the same level: they’re both direct children of the same tag. We call them . When a document is pretty-printed, siblings show up at the same indentation level. You can also use this relationship in the code you write. You can use and to navigate between page elements that are on the same level of the parse tree: The <b> tag has a , but no , because there’s nothing before the <b> tag on the same level of the tree . For the same reason, the <c> tag has a but no : The strings “text1” and “text2” are siblings, because they don’t have the same parent: In real documents, the or of a tag will usually be a string containing whitespace. Going back to the “three sisters” document: You might think that the of the first <a> tag would be the second <a> tag. But actually, it’s a string: the comma and newline that separate the first <a> tag from the second: The second <a> tag is actually the of the comma: You can iterate over a tag’s siblings with or : # u'; and they lived at the bottom of a well.' # u'Once upon a time there were three little sisters; and their names were\n\n' Take a look at the beginning of the “three sisters” document: An HTML parser takes this string of characters and turns it into a series of events: “open an <html> tag”, “open a <head> tag”, “open a <title> tag”, “add a string”, “close the <title> tag”, “open a <p> tag”, and so on. Beautiful Soup offers tools for reconstructing the initial parse of the document. The attribute of a string or tag points to whatever was parsed immediately afterwards. It might be the same as , but it’s usually drastically different. Here’s the final <a> tag in the “three sisters” document. Its is a string: the conclusion of the sentence that was interrupted by the start of the <a> tag.: # '; and they lived at the bottom of a well.' But the of that <a> tag, the thing that was parsed immediately after the <a> tag, is the rest of that sentence: it’s the word “Tillie”: That’s because in the original markup, the word “Tillie” appeared before that semicolon. The parser encountered an <a> tag, then the word “Tillie”, then the closing </a> tag, then the semicolon and rest of the sentence. The semicolon is on the same level as the <a> tag, but the word “Tillie” was encountered first. The attribute is the exact opposite of . It points to whatever element was parsed immediately before this one: You should get the idea by now. You can use these iterators to move forward or backward in the document as it was parsed: # u';\n\nand they lived at the bottom of a well.'\n\nBeautiful Soup defines a lot of methods for searching the parse tree, but they’re all very similar. I’m going to spend a lot of time explaining the two most popular methods: and . The other methods take almost exactly the same arguments, so I’ll just cover them briefly. Once again, I’ll be using the “three sisters” document as an example: <p class=\"story\">Once upon a time there were three little sisters; and their names were and they lived at the bottom of a well.</p> By passing in a filter to an argument like , you can zoom in on the parts of the document you’re interested in. Before talking in detail about and similar methods, I want to show examples of different filters you can pass into these methods. These filters show up again and again, throughout the search API. You can use them to filter based on a tag’s name, on its attributes, on the text of a string, or on some combination of these. The simplest filter is a string. Pass a string to a search method and Beautiful Soup will perform a match against that exact string. This code finds all the <b> tags in the document: If you pass in a byte string, Beautiful Soup will assume the string is encoded as UTF-8. You can avoid this by passing in a Unicode string instead. If you pass in a regular expression object, Beautiful Soup will filter against that regular expression using its method. This code finds all the tags whose names start with the letter “b”; in this case, the <body> tag and the <b> tag: This code finds all the tags whose names contain the letter ‘t’: If you pass in a list, Beautiful Soup will allow a string match against item in that list. This code finds all the <a> tags all the <b> tags: The value matches everything it can. This code finds the tags in the document, but none of the text strings: If none of the other matches work for you, define a function that takes an element as its only argument. The function should return if the argument matches, and otherwise. Here’s a function that returns if a tag defines the “class” attribute but doesn’t define the “id” attribute: Pass this function into and you’ll pick up all the <p> tags: This function only picks up the <p> tags. It doesn’t pick up the <a> tags, because those tags define both “class” and “id”. It doesn’t pick up tags like <html> and <title>, because those tags don’t define “class”. If you pass in a function to filter on a specific attribute like , the argument passed into the function will be the attribute value, not the whole tag. Here’s a function that finds all tags whose attribute does not match a regular expression: The function can be as complicated as you need it to be. Here’s a function that returns if a tag is surrounded by string objects: Now we’re ready to look at the search methods in detail. The method looks through a tag’s descendants and retrieves descendants that match your filters. I gave several examples in Kinds of filters, but here are a few more: # u'Once upon a time there were three little sisters; and their names were\n\n' Some of these should look familiar, but others are new. What does it mean to pass in a value for , or ? Why does find a <p> tag with the CSS class “title”? Let’s look at the arguments to . Pass in a value for and you’ll tell Beautiful Soup to only consider tags with certain names. Text strings will be ignored, as will tags whose names that don’t match. This is the simplest usage: Recall from Kinds of filters that the value to can be a string, a regular expression, a list, a function, or the value True. Any argument that’s not recognized will be turned into a filter on one of a tag’s attributes. If you pass in a value for an argument called , Beautiful Soup will filter against each tag’s ‘id’ attribute: If you pass in a value for , Beautiful Soup will filter against each tag’s ‘href’ attribute: You can filter an attribute based on a string, a regular expression, a list, a function, or the value True. This code finds all tags whose attribute has a value, regardless of what the value is: You can filter multiple attributes at once by passing in more than one keyword argument: Some attributes, like the data-* attributes in HTML 5, have names that can’t be used as the names of keyword arguments: # SyntaxError: keyword can't be an expression You can use these attributes in searches by putting them into a dictionary and passing the dictionary into as the argument: You can’t use a keyword argument to search for HTML’s ‘name’ element, because Beautiful Soup uses the argument to contain the name of the tag itself. Instead, you can give a value to ‘name’ in the argument: It’s very useful to search for a tag that has a certain CSS class, but the name of the CSS attribute, “class”, is a reserved word in Python. Using as a keyword argument will give you a syntax error. As of Beautiful Soup 4.1.2, you can search by CSS class using the keyword argument : As with any keyword argument, you can pass a string, a regular expression, a function, or : Remember that a single tag can have multiple values for its “class” attribute. When you search for a tag that matches a certain CSS class, you’re matching against of its CSS classes: You can also search for the exact string value of the attribute: But searching for variants of the string value won’t work: If you want to search for tags that match two or more CSS classes, you should use a CSS selector: In older versions of Beautiful Soup, which don’t have the shortcut, you can use the trick mentioned above. Create a dictionary whose value for “class” is the string (or regular expression, or whatever) you want to search for: With you can search for strings instead of tags. As with and the keyword arguments, you can pass in a string, a regular expression, a list, a function, or the value True. Here are some examples: \"\"\"Return True if this string is the only child of its parent tag.\"\"\" Although is for finding strings, you can combine it with arguments that find tags: Beautiful Soup will find all tags whose matches your value for . This code finds the <a> tags whose is “Elsie”: The argument is new in Beautiful Soup 4.4.0. In earlier versions it was called : returns all the tags and strings that match your filters. This can take a while if the document is large. If you don’t need the results, you can pass in a number for . This works just like the LIMIT keyword in SQL. It tells Beautiful Soup to stop gathering results after it’s found a certain number. There are three links in the “three sisters” document, but this code only finds the first two: If you call , Beautiful Soup will examine all the descendants of : its children, its children’s children, and so on. If you only want Beautiful Soup to consider direct children, you can pass in . See the difference here: Here’s that part of the document: The <title> tag is beneath the <html> tag, but it’s not beneath the <html> tag: the <head> tag is in the way. Beautiful Soup finds the <title> tag when it’s allowed to look at all descendants of the <html> tag, but when restricts it to the <html> tag’s immediate children, it finds nothing. Beautiful Soup offers a lot of tree-searching methods (covered below), and they mostly take the same arguments as : , , , , and the keyword arguments. But the argument is different: and are the only methods that support it. Passing into a method like wouldn’t be very useful. Because is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the object or a object as though it were a function, then it’s the same as calling on that object. These two lines of code are equivalent: These two lines are also equivalent: The method scans the entire document looking for results, but sometimes you only want to find one result. If you know a document only has one <body> tag, it’s a waste of time to scan the entire document looking for more. Rather than passing in every time you call , you can use the method. These two lines of code are equivalent: The only difference is that returns a list containing the single result, and just returns the result. If can’t find anything, it returns an empty list. If can’t find anything, it returns : Remember the trick from Navigating using tag names? That trick works by repeatedly calling : I spent a lot of time above covering and . The Beautiful Soup API defines ten other methods for searching the tree, but don’t be afraid. Five of these methods are basically the same as , and the other five are basically the same as . The only differences are in what parts of the tree they search. First let’s consider and . Remember that and work their way down the tree, looking at tag’s descendants. These methods do the opposite: they work their way the tree, looking at a tag’s (or a string’s) parents. Let’s try them out, starting from a string buried deep in the “three daughters” document: # <p class=\"story\">Once upon a time there were three little sisters; and their names were # and they lived at the bottom of a well.</p> One of the three <a> tags is the direct parent of the string in question, so our search finds it. One of the three <p> tags is an indirect parent of the string, and our search finds that as well. There’s a <p> tag with the CSS class “title” in the document, but it’s not one of this string’s parents, so we can’t find it with . You may have made the connection between and , and the .parent and .parents attributes mentioned earlier. The connection is very strong. These search methods actually use to iterate over all the parents, and check each one against the provided filter to see if it matches. These methods use .next_siblings to iterate over the rest of an element’s siblings in the tree. The method returns all the siblings that match, and only returns the first one: These methods use .previous_siblings to iterate over an element’s siblings that precede it in the tree. The method returns all the siblings that match, and only returns the first one: These methods use .next_elements to iterate over whatever tags and strings that come after it in the document. The method returns all matches, and only returns the first match: # u';\n\nand they lived at the bottom of a well.', u'\n\n\n\n', u'...', u'\n\n'] In the first example, the string “Elsie” showed up, even though it was contained within the <a> tag we started from. In the second example, the last <p> tag in the document showed up, even though it’s not in the same part of the tree as the <a> tag we started from. For these methods, all that matters is that an element match the filter, and show up later in the document than the starting element. These methods use .previous_elements to iterate over the tags and strings that came before it in the document. The method returns all matches, and only returns the first match: # [<p class=\"story\">Once upon a time there were three little sisters; ...</p>, The call to found the first paragraph in the document (the one with class=”title”), but it also finds the second paragraph, the <p> tag that contains the <a> tag we started with. This shouldn’t be too surprising: we’re looking at all the tags that show up earlier in the document than the one we started with. A <p> tag that contains an <a> tag must have shown up before the <a> tag it contains. As of version 4.7.0, Beautiful Soup supports most CSS4 selectors via the SoupSieve project. If you installed Beautiful Soup through , SoupSieve was installed at the same time, so you don’t have to do anything extra. has a method which uses SoupSieve to run a CSS selector against a parsed document and return all the matching elements. has a similar method which runs a CSS selector against the contents of a single tag. The SoupSieve documentation lists all the currently supported CSS selectors, but here are some of the basics: Find tags that match any selector from a list of selectors: Test for the existence of an attribute: There’s also a method called , which finds only the first tag that matches a selector: If you’ve parsed XML that defines namespaces, you can use them in CSS selectors.: When handling a CSS selector that uses namespaces, Beautiful Soup uses the namespace abbreviations it found when parsing the document. You can override this by passing in your own dictionary of abbreviations: All this CSS selector stuff is a convenience for people who already know the CSS selector syntax. You can do all of this with the Beautiful Soup API. And if CSS selectors are all you need, you should parse the document with lxml: it’s a lot faster. But this lets you CSS selectors with the Beautiful Soup API.\n\nBeautiful Soup’s main strength is in searching the parse tree, but you can also modify the tree and write your changes as a new HTML or XML document. I covered this earlier, in Attributes, but it bears repeating. You can rename a tag, change the values of its attributes, add new attributes, and delete attributes: If you set a tag’s attribute to a new string, the tag’s contents are replaced with that string: Be careful: if the tag contained other tags, they and all their contents will be destroyed. You can add to a tag’s contents with . It works just like calling on a Python list: Starting in Beautiful Soup 4.7.0, also supports a method called , which works just like calling on a Python list: If you need to add a string to a document, no problem–you can pass a Python string in to , or you can call the constructor: If you want to create a comment or some other subclass of , just call the constructor: # [u'Hello', u' there', u'Nice to see you.'] What if you need to create a whole new tag? The best solution is to call the factory method : Only the first argument, the tag name, is required. is just like , except the new element doesn’t necessarily go at the end of its parent’s . It’ll be inserted at whatever numeric position you say. It works just like on a Python list: # <a href=\"http://example.com/\">I linked to but did not endorse <i>example.com</i></a> # [u'I linked to ', u'but did not endorse', <i>example.com</i>] The method inserts tags or strings immediately before something else in the parse tree: The method inserts tags or strings immediately following something else in the parse tree: removes a tag or string from the tree. It returns the tag or string that was extracted: At this point you effectively have two parse trees: one rooted at the object you used to parse the document, and one rooted at the tag that was extracted. You can go on to call on a child of the element you extracted: removes a tag from the tree, then completely destroys it and its contents : removes a tag or string from the tree, and replaces it with the tag or string of your choice: returns the tag or string that was replaced, so that you can examine it or add it back to another part of the tree. wraps an element in the tag you specify. It returns the new wrapper: This method is new in Beautiful Soup 4.0.5. is the opposite of . It replaces a tag with whatever’s inside that tag. It’s good for stripping out markup: Like , returns the tag that was replaced. After calling a bunch of methods that modify the parse tree, you may end up with two or more objects next to each other. Beautiful Soup doesn’t have any problems with this, but since it can’t happen in a freshly parsed document, you might not expect behavior like the following: You can call to clean up the parse tree by consolidating adjacent strings: The method is new in Beautiful Soup 4.8.0.\n\nSpecifying the parser to use¶ If you just need to parse some HTML, you can dump the markup into the constructor, and it’ll probably be fine. Beautiful Soup will pick a parser for you and parse the data. But there are a few additional arguments you can pass in to the constructor to change which parser is used. The first argument to the constructor is a string or an open filehandle–the markup you want parsed. The second argument is you’d like the markup parsed. If you don’t specify anything, you’ll get the best HTML parser that’s installed. Beautiful Soup ranks lxml’s parser as being the best, then html5lib’s, then Python’s built-in parser. You can override this by specifying one of the following:\n• What type of markup you want to parse. Currently supported are “html”, “xml”, and “html5”.\n• The name of the parser library you want to use. Currently supported options are “lxml”, “html5lib”, and “html.parser” (Python’s built-in HTML parser). If you don’t have an appropriate parser installed, Beautiful Soup will ignore your request and pick a different parser. Right now, the only supported XML parser is lxml. If you don’t have lxml installed, asking for an XML parser won’t give you one, and asking for “lxml” won’t work either. Beautiful Soup presents the same interface to a number of different parsers, but each parser is different. Different parsers will create different parse trees from the same document. The biggest differences are between the HTML parsers and the XML parsers. Here’s a short document, parsed as HTML: Since an empty <b /> tag is not valid HTML, the parser turns it into a <b></b> tag pair. Here’s the same document parsed as XML (running this requires that you have lxml installed). Note that the empty <b /> tag is left alone, and that the document is given an XML declaration instead of being put into an <html> tag.: There are also differences between HTML parsers. If you give Beautiful Soup a perfectly-formed HTML document, these differences won’t matter. One parser will be faster than another, but they’ll all give you a data structure that looks exactly like the original HTML document. But if the document is not perfectly-formed, different parsers will give different results. Here’s a short, invalid document parsed using lxml’s HTML parser. Note that the dangling </p> tag is simply ignored: Here’s the same document parsed using html5lib: Instead of ignoring the dangling </p> tag, html5lib pairs it with an opening <p> tag. This parser also adds an empty <head> tag to the document. Here’s the same document parsed with Python’s built-in HTML parser: Like html5lib, this parser ignores the closing </p> tag. Unlike html5lib, this parser makes no attempt to create a well-formed HTML document by adding a <body> tag. Unlike lxml, it doesn’t even bother to add an <html> tag. Since the document “<a></p>” is invalid, none of these techniques is the “correct” way to handle it. The html5lib parser uses techniques that are part of the HTML5 standard, so it has the best claim on being the “correct” way, but all three techniques are legitimate. Differences between parsers can affect your script. If you’re planning on distributing your script to other people, or running it on multiple machines, you should specify a parser in the constructor. That will reduce the chances that your users parse a document differently from the way you parse it.\n\nAny HTML or XML document is written in a specific encoding like ASCII or UTF-8. But when you load that document into Beautiful Soup, you’ll discover it’s been converted to Unicode: It’s not magic. (That sure would be nice.) Beautiful Soup uses a sub-library called Unicode, Dammit to detect a document’s encoding and convert it to Unicode. The autodetected encoding is available as the attribute of the object: Unicode, Dammit guesses correctly most of the time, but sometimes it makes mistakes. Sometimes it guesses correctly, but only after a byte-by-byte search of the document that takes a very long time. If you happen to know a document’s encoding ahead of time, you can avoid mistakes and delays by passing it to the constructor as . Here’s a document written in ISO-8859-8. The document is so short that Unicode, Dammit can’t get a lock on it, and misidentifies it as ISO-8859-7: We can fix this by passing in the correct : If you don’t know what the correct encoding is, but you know that Unicode, Dammit is guessing wrong, you can pass the wrong guesses in as : Windows-1255 isn’t 100% correct, but that encoding is a compatible superset of ISO-8859-8, so it’s close enough. ( is a new feature in Beautiful Soup 4.4.0.) In rare cases (usually when a UTF-8 document contains text written in a completely different encoding), the only way to get Unicode may be to replace some characters with the special Unicode character “REPLACEMENT CHARACTER” (U+FFFD, �). If Unicode, Dammit needs to do this, it will set the attribute to on the or object. This lets you know that the Unicode representation is not an exact representation of the original–some data was lost. If a document contains �, but is , you’ll know that the � was there originally (as it is in this paragraph) and doesn’t stand in for missing data. When you write out a document from Beautiful Soup, you get a UTF-8 document, even if the document wasn’t in UTF-8 to begin with. Here’s a document written in the Latin-1 encoding: Note that the <meta> tag has been rewritten to reflect the fact that the document is now in UTF-8. If you don’t want UTF-8, you can pass an encoding into : You can also call encode() on the object, or any element in the soup, just as if it were a Python string: Any characters that can’t be represented in your chosen encoding will be converted into numeric XML entity references. Here’s a document that includes the Unicode character SNOWMAN: The SNOWMAN character can be part of a UTF-8 document (it looks like ☃), but there’s no representation for that character in ISO-Latin-1 or ASCII, so it’s converted into “☃” for those encodings: You can use Unicode, Dammit without using Beautiful Soup. It’s useful whenever you have data in an unknown encoding and you just want it to become Unicode: Unicode, Dammit’s guesses will get a lot more accurate if you install the or Python libraries. The more data you give Unicode, Dammit, the more accurately it will guess. If you have your own suspicions as to what the encoding might be, you can pass them in as a list: Unicode, Dammit has two special features that Beautiful Soup doesn’t use. You can use Unicode, Dammit to convert Microsoft smart quotes to HTML or XML entities: You can also convert Microsoft smart quotes to ASCII quotes: Hopefully you’ll find this feature useful, but Beautiful Soup doesn’t use it. Beautiful Soup prefers the default behavior, which is to convert Microsoft smart quotes to Unicode characters along with everything else: Sometimes a document is mostly in UTF-8, but contains Windows-1252 characters such as (again) Microsoft smart quotes. This can happen when a website includes data from multiple sources. You can use to turn such a document into pure UTF-8. Here’s a simple example: This document is a mess. The snowmen are in UTF-8 and the quotes are in Windows-1252. You can display the snowmen or the quotes, but not both: Decoding the document as UTF-8 raises a , and decoding it as Windows-1252 gives you gibberish. Fortunately, will convert the string to pure UTF-8, allowing you to decode it to Unicode and display the snowmen and quote marks simultaneously: only knows how to handle Windows-1252 embedded in UTF-8 (or vice versa, I suppose), but this is the most common case. Note that you must know to call on your data before passing it into or the constructor. Beautiful Soup assumes that a document has a single encoding, whatever it might be. If you pass it a document that contains both UTF-8 and Windows-1252, it’s likely to think the whole document is Windows-1252, and the document will come out looking like . is new in Beautiful Soup 4.1.0.\n\nBeautiful Soup 3 is the previous release series, and is no longer being actively developed. It’s currently packaged with all major Linux distributions: It’s also published through PyPi as .: You can also download a tarball of Beautiful Soup 3.2.0. If you ran or , but your code doesn’t work, you installed Beautiful Soup 3 by mistake. You need to run . The documentation for Beautiful Soup 3 is archived online. Most code written against Beautiful Soup 3 will work against Beautiful Soup 4 with one simple change. All you should have to do is change the package name from to . So this:\n• If you get the “No module named BeautifulSoup”, your problem is that you’re trying to run Beautiful Soup 3 code, but you only have Beautiful Soup 4 installed.\n• If you get the “No module named bs4”, your problem is that you’re trying to run Beautiful Soup 4 code, but you only have Beautiful Soup 3 installed. Although BS4 is mostly backwards-compatible with BS3, most of its methods have been deprecated and given new names for PEP 8 compliance. There are numerous other renames and changes, and a few of them break backwards compatibility. Here’s what you’ll need to know to convert your BS3 code and habits to BS4: Beautiful Soup 3 used Python’s , a module that was deprecated and removed in Python 3.0. Beautiful Soup 4 uses by default, but you can plug in lxml or html5lib and use that instead. See Installing a parser for a comparison. Since is not the same parser as , you may find that Beautiful Soup 4 gives you a different parse tree than Beautiful Soup 3 for the same markup. If you swap out for lxml or html5lib, you may find that the parse tree changes yet again. If this happens, you’ll need to update your scraping code to deal with the new tree. Some arguments to the Beautiful Soup constructor were renamed for the same reasons: I renamed one method for compatibility with Python 3: I renamed one attribute to use more accurate terminology: I renamed three attributes to avoid using words that have special meaning to Python. Unlike the others, these changes are not backwards compatible. If you used these attributes in BS3, your code will break on BS4 until you change them. I gave the generators PEP 8-compliant names, and transformed them into properties: Some of the generators used to yield after they were done, and then stop. That was a bug. Now the generators just stop. There are two new generators, .strings and .stripped_strings. yields NavigableString objects, and yields Python strings that have had whitespace stripped. There is no longer a class for parsing XML. To parse XML you pass in “xml” as the second argument to the constructor. For the same reason, the constructor no longer recognizes the argument. Beautiful Soup’s handling of empty-element XML tags has been improved. Previously when you parsed XML you had to explicitly say which tags were considered empty-element tags. The argument to the constructor is no longer recognized. Instead, Beautiful Soup considers any empty tag to be an empty-element tag. If you add a child to an empty-element tag, it stops being an empty-element tag. An incoming HTML or XML entity is always converted into the corresponding Unicode character. Beautiful Soup 3 had a number of overlapping ways of dealing with entities, which have been removed. The constructor no longer recognizes the or arguments. (Unicode, Dammit still has , but its default is now to turn smart quotes into Unicode.) The constants , , and have been removed, since they configure a feature (transforming some but not all entities into Unicode characters) that no longer exists. If you want to turn Unicode characters back into HTML entities on output, rather than turning them into UTF-8 characters, you need to use an output formatter. Tag.string now operates recursively. If tag A contains a single tag B and nothing else, then A.string is the same as B.string. (Previously, it was None.) Multi-valued attributes like have lists of strings as their values, not strings. This may affect the way you search by CSS class. If you pass one of the methods both string a tag-specific argument like name, Beautiful Soup will search for tags that match your tag-specific criteria and whose Tag.string matches your value for string. It will find the strings themselves. Previously, Beautiful Soup ignored the tag-specific arguments and looked for strings. The constructor no longer recognizes the argument. It’s now the parser’s responsibility to handle markup correctly. The rarely-used alternate parser classes like and have been removed. It’s now the parser’s decision how to handle ambiguous markup. The method now returns a Unicode string, not a bytestring."
    },
    {
        "link": "https://stackoverflow.com/questions/73867545/validate-html-with-beautifulsoup",
        "document": "I use BeautifulSoup 3.2.1 to parse a lot of HTML files translated with eTranslation.\n\nI found sometimes cuts a section of my HTML file. And it is related to invalid tags or problems found in the HTML.\n\nAlso I found works better in these cases of bad written HTML.\n\nIs there a way to detect which HTML file is invalid using BeautifulSoup?\n\nI image something like this:"
    },
    {
        "link": "https://crummy.com/software/BeautifulSoup/bs4/css-selector-update-doc",
        "document": "<p class=\"story\">Once upon a time there were three little sisters; and their names were and they lived at the bottom of a well.</p> I’ll use this as an example to show you how to move from one part of a document to another. Tags may contain strings and other tags. These elements are the tag’s . Beautiful Soup provides a lot of different attributes for navigating and iterating over a tag’s children. Note that Beautiful Soup strings don’t support any of these attributes, because a string can’t have children. The simplest way to navigate the parse tree is to say the name of the tag you want. If you want the <head> tag, just say : You can do use this trick again and again to zoom in on a certain part of the parse tree. This code gets the first <b> tag beneath the <body> tag: Using a tag name as an attribute will give you only the tag by that name: If you need to get the <a> tags, or anything more complicated than the first tag with a certain name, you’ll need to use one of the methods described in Searching the tree, such as : A tag’s children are available in a list called : The object itself has children. In this case, the <html> tag is the child of the object.: A string does not have , because it can’t contain anything: Instead of getting them as a list, you can iterate over a tag’s children using the generator: If you want to modify a tag’s children, use the methods described in Modifying the tree. Don’t modify the the list directly: that can lead to problems that are subtle and difficult to spot. The and attributes only consider a tag’s children. For instance, the <head> tag has a single direct child–the <title> tag: But the <title> tag itself has a child: the string “The Dormouse’s story”. There’s a sense in which that string is also a child of the <head> tag. The attribute lets you iterate over of a tag’s children, recursively: its direct children, the children of its direct children, and so on: The <head> tag has only one child, but it has two descendants: the <title> tag and the <title> tag’s child. The object only has one direct child (the <html> tag), but it has a whole lot of descendants: If a tag has only one child, and that child is a , the child is made available as : If a tag’s only child is another tag, and tag has a , then the parent tag is considered to have the same as its child: If a tag contains more than one thing, then it’s not clear what should refer to, so is defined to be : If there’s more than one thing inside a tag, you can still look at just the strings. Use the generator: # 'Once upon a time there were three little sisters; and their names were\n\n' # ';\n\nand they lived at the bottom of a well.' These strings tend to have a lot of extra whitespace, which you can remove by using the generator instead: # 'Once upon a time there were three little sisters; and their names were' # ';\n\n and they lived at the bottom of a well.' Here, strings consisting entirely of whitespace are ignored, and whitespace at the beginning and end of strings is removed. Continuing the “family tree” analogy, every tag and every string has a : the tag that contains it. You can access an element’s parent with the attribute. In the example “three sisters” document, the <head> tag is the parent of the <title> tag: The title string itself has a parent: the <title> tag that contains it: The parent of a top-level tag like <html> is the object itself: And the of a object is defined as None: You can iterate over all of an element’s parents with . This example uses to travel from an <a> tag buried deep within the document, to the very top of the document: Consider a simple document like this: The <b> tag and the <c> tag are at the same level: they’re both direct children of the same tag. We call them . When a document is pretty-printed, siblings show up at the same indentation level. You can also use this relationship in the code you write. You can use and to navigate between page elements that are on the same level of the parse tree: The <b> tag has a , but no , because there’s nothing before the <b> tag on the same level of the tree . For the same reason, the <c> tag has a but no : The strings “text1” and “text2” are siblings, because they don’t have the same parent: In real documents, the or of a tag will usually be a string containing whitespace. Going back to the “three sisters” document: You might think that the of the first <a> tag would be the second <a> tag. But actually, it’s a string: the comma and newline that separate the first <a> tag from the second: The second <a> tag is actually the of the comma: You can iterate over a tag’s siblings with or : # '; and they lived at the bottom of a well.' # 'Once upon a time there were three little sisters; and their names were\n\n' Take a look at the beginning of the “three sisters” document: An HTML parser takes this string of characters and turns it into a series of events: “open an <html> tag”, “open a <head> tag”, “open a <title> tag”, “add a string”, “close the <title> tag”, “open a <p> tag”, and so on. Beautiful Soup offers tools for reconstructing the initial parse of the document. The attribute of a string or tag points to whatever was parsed immediately afterwards. It might be the same as , but it’s usually drastically different. Here’s the final <a> tag in the “three sisters” document. Its is a string: the conclusion of the sentence that was interrupted by the start of the <a> tag.: # ';\n\nand they lived at the bottom of a well.' But the of that <a> tag, the thing that was parsed immediately after the <a> tag, is the rest of that sentence: it’s the word “Tillie”: That’s because in the original markup, the word “Tillie” appeared before that semicolon. The parser encountered an <a> tag, then the word “Tillie”, then the closing </a> tag, then the semicolon and rest of the sentence. The semicolon is on the same level as the <a> tag, but the word “Tillie” was encountered first. The attribute is the exact opposite of . It points to whatever element was parsed immediately before this one: You should get the idea by now. You can use these iterators to move forward or backward in the document as it was parsed: # ';\n\nand they lived at the bottom of a well.'\n\nBeautiful Soup defines a lot of methods for searching the parse tree, but they’re all very similar. I’m going to spend a lot of time explaining the two most popular methods: and . The other methods take almost exactly the same arguments, so I’ll just cover them briefly. Once again, I’ll be using the “three sisters” document as an example: <p class=\"story\">Once upon a time there were three little sisters; and their names were and they lived at the bottom of a well.</p> By passing in a filter to an argument like , you can zoom in on the parts of the document you’re interested in. Before talking in detail about and similar methods, I want to show examples of different filters you can pass into these methods. These filters show up again and again, throughout the search API. You can use them to filter based on a tag’s name, on its attributes, on the text of a string, or on some combination of these. The simplest filter is a string. Pass a string to a search method and Beautiful Soup will perform a match against that exact string. This code finds all the <b> tags in the document: If you pass in a byte string, Beautiful Soup will assume the string is encoded as UTF-8. You can avoid this by passing in a Unicode string instead. If you pass in a regular expression object, Beautiful Soup will filter against that regular expression using its method. This code finds all the tags whose names start with the letter “b”; in this case, the <body> tag and the <b> tag: This code finds all the tags whose names contain the letter ‘t’: If you pass in a list, Beautiful Soup will allow a string match against item in that list. This code finds all the <a> tags all the <b> tags: The value matches everything it can. This code finds the tags in the document, but none of the text strings: If none of the other matches work for you, define a function that takes an element as its only argument. The function should return if the argument matches, and otherwise. Here’s a function that returns if a tag defines the “class” attribute but doesn’t define the “id” attribute: Pass this function into and you’ll pick up all the <p> tags: # <p class=\"story\">Once upon a time there were…bottom of a well.</p>, This function only picks up the <p> tags. It doesn’t pick up the <a> tags, because those tags define both “class” and “id”. It doesn’t pick up tags like <html> and <title>, because those tags don’t define “class”. If you pass in a function to filter on a specific attribute like , the argument passed into the function will be the attribute value, not the whole tag. Here’s a function that finds all tags whose attribute does not match a regular expression: The function can be as complicated as you need it to be. Here’s a function that returns if a tag is surrounded by string objects: Now we’re ready to look at the search methods in detail. The method looks through a tag’s descendants and retrieves descendants that match your filters. I gave several examples in Kinds of filters, but here are a few more: # 'Once upon a time there were three little sisters; and their names were\n\n' Some of these should look familiar, but others are new. What does it mean to pass in a value for , or ? Why does find a <p> tag with the CSS class “title”? Let’s look at the arguments to . Pass in a value for and you’ll tell Beautiful Soup to only consider tags with certain names. Text strings will be ignored, as will tags whose names that don’t match. This is the simplest usage: Recall from Kinds of filters that the value to can be a string, a regular expression, a list, a function, or the value True. Any argument that’s not recognized will be turned into a filter on one of a tag’s attributes. If you pass in a value for an argument called , Beautiful Soup will filter against each tag’s ‘id’ attribute: If you pass in a value for , Beautiful Soup will filter against each tag’s ‘href’ attribute: You can filter an attribute based on a string, a regular expression, a list, a function, or the value True. This code finds all tags whose attribute has a value, regardless of what the value is: You can filter multiple attributes at once by passing in more than one keyword argument: Some attributes, like the data-* attributes in HTML 5, have names that can’t be used as the names of keyword arguments: # SyntaxError: keyword can't be an expression You can use these attributes in searches by putting them into a dictionary and passing the dictionary into as the argument: You can’t use a keyword argument to search for HTML’s ‘name’ element, because Beautiful Soup uses the argument to contain the name of the tag itself. Instead, you can give a value to ‘name’ in the argument: It’s very useful to search for a tag that has a certain CSS class, but the name of the CSS attribute, “class”, is a reserved word in Python. Using as a keyword argument will give you a syntax error. As of Beautiful Soup 4.1.2, you can search by CSS class using the keyword argument : As with any keyword argument, you can pass a string, a regular expression, a function, or : Remember that a single tag can have multiple values for its “class” attribute. When you search for a tag that matches a certain CSS class, you’re matching against of its CSS classes: You can also search for the exact string value of the attribute: But searching for variants of the string value won’t work: If you want to search for tags that match two or more CSS classes, you should use a CSS selector: In older versions of Beautiful Soup, which don’t have the shortcut, you can use the trick mentioned above. Create a dictionary whose value for “class” is the string (or regular expression, or whatever) you want to search for: With you can search for strings instead of tags. As with and the keyword arguments, you can pass in a string, a regular expression, a list, a function, or the value True. Here are some examples: \"\"\"Return True if this string is the only child of its parent tag.\"\"\" Although is for finding strings, you can combine it with arguments that find tags: Beautiful Soup will find all tags whose matches your value for . This code finds the <a> tags whose is “Elsie”: The argument is new in Beautiful Soup 4.4.0. In earlier versions it was called : returns all the tags and strings that match your filters. This can take a while if the document is large. If you don’t need the results, you can pass in a number for . This works just like the LIMIT keyword in SQL. It tells Beautiful Soup to stop gathering results after it’s found a certain number. There are three links in the “three sisters” document, but this code only finds the first two: If you call , Beautiful Soup will examine all the descendants of : its children, its children’s children, and so on. If you only want Beautiful Soup to consider direct children, you can pass in . See the difference here: Here’s that part of the document: The <title> tag is beneath the <html> tag, but it’s not beneath the <html> tag: the <head> tag is in the way. Beautiful Soup finds the <title> tag when it’s allowed to look at all descendants of the <html> tag, but when restricts it to the <html> tag’s immediate children, it finds nothing. Beautiful Soup offers a lot of tree-searching methods (covered below), and they mostly take the same arguments as : , , , , and the keyword arguments. But the argument is different: and are the only methods that support it. Passing into a method like wouldn’t be very useful. Because is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the object or a object as though it were a function, then it’s the same as calling on that object. These two lines of code are equivalent: These two lines are also equivalent: The method scans the entire document looking for results, but sometimes you only want to find one result. If you know a document only has one <body> tag, it’s a waste of time to scan the entire document looking for more. Rather than passing in every time you call , you can use the method. These two lines of code are equivalent: The only difference is that returns a list containing the single result, and just returns the result. If can’t find anything, it returns an empty list. If can’t find anything, it returns : Remember the trick from Navigating using tag names? That trick works by repeatedly calling : I spent a lot of time above covering and . The Beautiful Soup API defines ten other methods for searching the tree, but don’t be afraid. Five of these methods are basically the same as , and the other five are basically the same as . The only differences are in what parts of the tree they search. First let’s consider and . Remember that and work their way down the tree, looking at tag’s descendants. These methods do the opposite: they work their way the tree, looking at a tag’s (or a string’s) parents. Let’s try them out, starting from a string buried deep in the “three daughters” document: # <p class=\"story\">Once upon a time there were three little sisters; and their names were # and they lived at the bottom of a well.</p> One of the three <a> tags is the direct parent of the string in question, so our search finds it. One of the three <p> tags is an indirect parent of the string, and our search finds that as well. There’s a <p> tag with the CSS class “title” in the document, but it’s not one of this string’s parents, so we can’t find it with . You may have made the connection between and , and the .parent and .parents attributes mentioned earlier. The connection is very strong. These search methods actually use to iterate over all the parents, and check each one against the provided filter to see if it matches. These methods use .next_siblings to iterate over the rest of an element’s siblings in the tree. The method returns all the siblings that match, and only returns the first one: These methods use .previous_siblings to iterate over an element’s siblings that precede it in the tree. The method returns all the siblings that match, and only returns the first one: These methods use .next_elements to iterate over whatever tags and strings that come after it in the document. The method returns all matches, and only returns the first match: # ';\n\nand they lived at the bottom of a well.', '\n\n', '...', '\n\n'] In the first example, the string “Elsie” showed up, even though it was contained within the <a> tag we started from. In the second example, the last <p> tag in the document showed up, even though it’s not in the same part of the tree as the <a> tag we started from. For these methods, all that matters is that an element match the filter, and show up later in the document than the starting element. These methods use .previous_elements to iterate over the tags and strings that came before it in the document. The method returns all matches, and only returns the first match: # [<p class=\"story\">Once upon a time there were three little sisters; ...</p>, The call to found the first paragraph in the document (the one with class=”title”), but it also finds the second paragraph, the <p> tag that contains the <a> tag we started with. This shouldn’t be too surprising: we’re looking at all the tags that show up earlier in the document than the one we started with. A <p> tag that contains an <a> tag must have shown up before the <a> tag it contains. and objects support CSS selectors through their property. The actual selector implementation is handled by the Soup Sieve package, available on PyPI as . If you installed Beautiful Soup through , Soup Sieve was installed at the same time, so you don’t have to do anything extra. The Soup Sieve documentation lists all the currently supported CSS selectors, but here are some of the basics. You can find tags: Find tags that match any selector from a list of selectors: Test for the existence of an attribute: There’s also a method called , which finds only the first tag that matches a selector: As a convenience, you can call and can directly on the or object, omitting the property: CSS selector support is a convenience for people who already know the CSS selector syntax. You can do all of this with the Beautiful Soup API. If CSS selectors are all you need, you should skip Beautiful Soup altogether and parse the document with : it’s a lot faster. But Soup Sieve lets you CSS selectors with the Beautiful Soup API. Soup Sieve offers a substantial API beyond the and methods, and you can access most of that API through the attribute of or . What follows is just a list of the supported methods; see the Soup Sieve documentation for full documentation. The method works the same as , but it returns a generator instead of a list: The method returns the nearest parent of a given that matches a CSS selector, similar to Beautiful Soup’s method: # <p class=\"story\">Once upon a time there were three little sisters; and their names were # and they lived at the bottom of a well.</p> The method returns a boolean depending on whether or not a specific matches a selector: The method returns the subset of a tag’s direct children that match a selector: The method escapes CSS identifiers that would otherwise be invalid: If you’ve parsed XML that defines namespaces, you can use them in CSS selectors.: Beautiful Soup tries to use namespace prefixes that make sense based on what it saw while parsing the document, but you can always provide your own dictionary of abbreviations: The property was added in Beautiful Soup 4.12.0. Prior to this, only the and convenience methods were supported. The Soup Sieve integration was added in Beautiful Soup 4.7.0. Earlier versions had the method, but only the most commonly-used CSS selectors were supported.\n\nBeautiful Soup’s main strength is in searching the parse tree, but you can also modify the tree and write your changes as a new HTML or XML document. I covered this earlier, in Attributes, but it bears repeating. You can rename a tag, change the values of its attributes, add new attributes, and delete attributes: If you set a tag’s attribute to a new string, the tag’s contents are replaced with that string: Be careful: if the tag contained other tags, they and all their contents will be destroyed. You can add to a tag’s contents with . It works just like calling on a Python list: Starting in Beautiful Soup 4.7.0, also supports a method called , which adds every element of a list to a , in order: If you need to add a string to a document, no problem–you can pass a Python string in to , or you can call the constructor: If you want to create a comment or some other subclass of , just call the constructor: # ['Hello', ' there', 'Nice to see you.'] What if you need to create a whole new tag? The best solution is to call the factory method : Only the first argument, the tag name, is required. is just like , except the new element doesn’t necessarily go at the end of its parent’s . It’ll be inserted at whatever numeric position you say. It works just like on a Python list: # <a href=\"http://example.com/\">I linked to but did not endorse <i>example.com</i></a> # ['I linked to ', 'but did not endorse', <i>example.com</i>] The method inserts tags or strings immediately before something else in the parse tree: The method inserts tags or strings immediately following something else in the parse tree: removes a tag or string from the tree. It returns the tag or string that was extracted: At this point you effectively have two parse trees: one rooted at the object you used to parse the document, and one rooted at the tag that was extracted. You can go on to call on a child of the element you extracted: removes a tag from the tree, then completely destroys it and its contents : The behavior of a decomposed or is not defined and you should not use it for anything. If you’re not sure whether something has been decomposed, you can check its property : removes a tag or string from the tree, and replaces it with one or more tags or strings of your choice: returns the tag or string that got replaced, so that you can examine it or add it back to another part of the tree. The ability to pass multiple arguments into replace_with() is new in Beautiful Soup 4.10.0. wraps an element in the tag you specify. It returns the new wrapper: This method is new in Beautiful Soup 4.0.5. is the opposite of . It replaces a tag with whatever’s inside that tag. It’s good for stripping out markup: Like , returns the tag that was replaced. After calling a bunch of methods that modify the parse tree, you may end up with two or more objects next to each other. Beautiful Soup doesn’t have any problems with this, but since it can’t happen in a freshly parsed document, you might not expect behavior like the following: You can call to clean up the parse tree by consolidating adjacent strings: This method is new in Beautiful Soup 4.8.0.\n\nThe method will turn a Beautiful Soup parse tree into a nicely formatted Unicode string, with a separate line for each tag and each string: You can call on the top-level object, or on any of its objects: Since it adds whitespace (in the form of newlines), changes the meaning of an HTML document and should not be used to reformat one. The goal of is to help you visually understand the structure of the documents you work with. If you just want a string, with no fancy formatting, you can call on a object, or on a within it: The function returns a string encoded in UTF-8. See Encodings for other options. You can also call to get a bytestring, and to get Unicode. If you give Beautiful Soup a document that contains HTML entities like “&lquot;”, they’ll be converted to Unicode characters: If you then convert the document to a bytestring, the Unicode characters will be encoded as UTF-8. You won’t get the HTML entities back: By default, the only characters that are escaped upon output are bare ampersands and angle brackets. These get turned into “&”, “<”, and “>”, so that Beautiful Soup doesn’t inadvertently generate invalid HTML or XML: You can change this behavior by providing a value for the argument to , , or . Beautiful Soup recognizes five possible values for . The default is . Strings will only be processed enough to ensure that Beautiful Soup generates valid HTML/XML: If you pass in , Beautiful Soup will convert Unicode characters to HTML entities whenever possible: If you pass in , it’s similar to , but Beautiful Soup will omit the closing slash in HTML void tags like “br”: In addition, any attributes whose values are the empty string will become HTML-style boolean attributes: If you pass in , Beautiful Soup will not modify strings at all on output. This is the fastest option, but it may lead to Beautiful Soup generating invalid HTML/XML, as in these examples: If you need more sophisticated control over your output, you can use Beautiful Soup’s class. Here’s a formatter that converts strings to uppercase, whether they occur in a text node or in an attribute value: Here’s a formatter that increases the indentation when pretty-printing: Subclassing or will give you even more control over the output. For example, Beautiful Soup sorts the attributes in every tag by default: To turn this off, you can subclass the method, which controls which attributes are output and in what order. This implementation also filters out the attribute called “m” whenever it appears: One last caveat: if you create a object, the text inside that object is always presented exactly as it appears, with no formatting . Beautiful Soup will call your entity substitution function, just in case you’ve written a custom function that counts all the strings in the document or something, but it will ignore the return value: If you only want the human-readable text inside a document or tag, you can use the method. It returns all the text in a document or beneath a tag, as a single Unicode string: You can specify a string to be used to join the bits of text together: You can tell Beautiful Soup to strip whitespace from the beginning and end of each bit of text: But at that point you might want to use the .stripped_strings generator instead, and process the text yourself: As of Beautiful Soup version 4.9.0, when lxml or html.parser are in use, the contents of <script>, <style>, and <template> tags are generally not considered to be ‘text’, since those tags are not part of the human-visible content of the page. As of Beautiful Soup version 4.10.0, you can call get_text(), .strings, or .stripped_strings on a NavigableString object. It will either return the object itself, or nothing, so the only reason to do this is when you’re iterating over a mixed list.\n\nSpecifying the parser to use¶ If you just need to parse some HTML, you can dump the markup into the constructor, and it’ll probably be fine. Beautiful Soup will pick a parser for you and parse the data. But there are a few additional arguments you can pass in to the constructor to change which parser is used. The first argument to the constructor is a string or an open filehandle–the markup you want parsed. The second argument is you’d like the markup parsed. If you don’t specify anything, you’ll get the best HTML parser that’s installed. Beautiful Soup ranks lxml’s parser as being the best, then html5lib’s, then Python’s built-in parser. You can override this by specifying one of the following:\n• None What type of markup you want to parse. Currently supported are “html”, “xml”, and “html5”.\n• None The name of the parser library you want to use. Currently supported options are “lxml”, “html5lib”, and “html.parser” (Python’s built-in HTML parser). If you don’t have an appropriate parser installed, Beautiful Soup will ignore your request and pick a different parser. Right now, the only supported XML parser is lxml. If you don’t have lxml installed, asking for an XML parser won’t give you one, and asking for “lxml” won’t work either. Beautiful Soup presents the same interface to a number of different parsers, but each parser is different. Different parsers will create different parse trees from the same document. The biggest differences are between the HTML parsers and the XML parsers. Here’s a short document, parsed as HTML using the parser that comes with Python: Since a standalone <b/> tag is not valid HTML, html.parser turns it into a <b></b> tag pair. Here’s the same document parsed as XML (running this requires that you have lxml installed). Note that the standalone <b/> tag is left alone, and that the document is given an XML declaration instead of being put into an <html> tag.: There are also differences between HTML parsers. If you give Beautiful Soup a perfectly-formed HTML document, these differences won’t matter. One parser will be faster than another, but they’ll all give you a data structure that looks exactly like the original HTML document. But if the document is not perfectly-formed, different parsers will give different results. Here’s a short, invalid document parsed using lxml’s HTML parser. Note that the <a> tag gets wrapped in <body> and <html> tags, and the dangling </p> tag is simply ignored: Here’s the same document parsed using html5lib: Instead of ignoring the dangling </p> tag, html5lib pairs it with an opening <p> tag. html5lib also adds an empty <head> tag; lxml didn’t bother. Here’s the same document parsed with Python’s built-in HTML parser: Like lxml, this parser ignores the closing </p> tag. Unlike html5lib or lxml, this parser makes no attempt to create a well-formed HTML document by adding <html> or <body> tags. Since the document “<a></p>” is invalid, none of these techniques is the ‘correct’ way to handle it. The html5lib parser uses techniques that are part of the HTML5 standard, so it has the best claim on being the ‘correct’ way, but all three techniques are legitimate. Differences between parsers can affect your script. If you’re planning on distributing your script to other people, or running it on multiple machines, you should specify a parser in the constructor. That will reduce the chances that your users parse a document differently from the way you parse it.\n\nAny HTML or XML document is written in a specific encoding like ASCII or UTF-8. But when you load that document into Beautiful Soup, you’ll discover it’s been converted to Unicode: It’s not magic. (That sure would be nice.) Beautiful Soup uses a sub-library called Unicode, Dammit to detect a document’s encoding and convert it to Unicode. The autodetected encoding is available as the attribute of the object: Unicode, Dammit guesses correctly most of the time, but sometimes it makes mistakes. Sometimes it guesses correctly, but only after a byte-by-byte search of the document that takes a very long time. If you happen to know a document’s encoding ahead of time, you can avoid mistakes and delays by passing it to the constructor as . Here’s a document written in ISO-8859-8. The document is so short that Unicode, Dammit can’t get a lock on it, and misidentifies it as ISO-8859-7: We can fix this by passing in the correct : If you don’t know what the correct encoding is, but you know that Unicode, Dammit is guessing wrong, you can pass the wrong guesses in as : Windows-1255 isn’t 100% correct, but that encoding is a compatible superset of ISO-8859-8, so it’s close enough. ( is a new feature in Beautiful Soup 4.4.0.) In rare cases (usually when a UTF-8 document contains text written in a completely different encoding), the only way to get Unicode may be to replace some characters with the special Unicode character “REPLACEMENT CHARACTER” (U+FFFD, �). If Unicode, Dammit needs to do this, it will set the attribute to on the or object. This lets you know that the Unicode representation is not an exact representation of the original–some data was lost. If a document contains �, but is , you’ll know that the � was there originally (as it is in this paragraph) and doesn’t stand in for missing data. When you write out a document from Beautiful Soup, you get a UTF-8 document, even if the document wasn’t in UTF-8 to begin with. Here’s a document written in the Latin-1 encoding: Note that the <meta> tag has been rewritten to reflect the fact that the document is now in UTF-8. If you don’t want UTF-8, you can pass an encoding into : You can also call encode() on the object, or any element in the soup, just as if it were a Python string: Any characters that can’t be represented in your chosen encoding will be converted into numeric XML entity references. Here’s a document that includes the Unicode character SNOWMAN: The SNOWMAN character can be part of a UTF-8 document (it looks like ☃), but there’s no representation for that character in ISO-Latin-1 or ASCII, so it’s converted into “☃” for those encodings: You can use Unicode, Dammit without using Beautiful Soup. It’s useful whenever you have data in an unknown encoding and you just want it to become Unicode: Unicode, Dammit’s guesses will get a lot more accurate if you install one of these Python libraries: , , or . The more data you give Unicode, Dammit, the more accurately it will guess. If you have your own suspicions as to what the encoding might be, you can pass them in as a list: Unicode, Dammit has two special features that Beautiful Soup doesn’t use. You can use Unicode, Dammit to convert Microsoft smart quotes to HTML or XML entities: You can also convert Microsoft smart quotes to ASCII quotes: Hopefully you’ll find this feature useful, but Beautiful Soup doesn’t use it. Beautiful Soup prefers the default behavior, which is to convert Microsoft smart quotes to Unicode characters along with everything else: Sometimes a document is mostly in UTF-8, but contains Windows-1252 characters such as (again) Microsoft smart quotes. This can happen when a website includes data from multiple sources. You can use to turn such a document into pure UTF-8. Here’s a simple example: This document is a mess. The snowmen are in UTF-8 and the quotes are in Windows-1252. You can display the snowmen or the quotes, but not both: Decoding the document as UTF-8 raises a , and decoding it as Windows-1252 gives you gibberish. Fortunately, will convert the string to pure UTF-8, allowing you to decode it to Unicode and display the snowmen and quote marks simultaneously: only knows how to handle Windows-1252 embedded in UTF-8 (or vice versa, I suppose), but this is the most common case. Note that you must know to call on your data before passing it into or the constructor. Beautiful Soup assumes that a document has a single encoding, whatever it might be. If you pass it a document that contains both UTF-8 and Windows-1252, it’s likely to think the whole document is Windows-1252, and the document will come out looking like . is new in Beautiful Soup 4.1.0.\n\nIf you’re having trouble understanding what Beautiful Soup does to a document, pass the document into the function. (New in Beautiful Soup 4.2.0.) Beautiful Soup will print out a report showing you how different parsers handle the document, and tell you if you’re missing a parser that Beautiful Soup could be using: # I noticed that html5lib is not installed. Installing it may help. # Trying to parse your data with html.parser # Here's what html.parser did with the document: Just looking at the output of diagnose() may show you how to solve the problem. Even if not, you can paste the output of when asking for help. There are two different kinds of parse errors. There are crashes, where you feed a document to Beautiful Soup and it raises an exception, usually an . And there is unexpected behavior, where a Beautiful Soup parse tree looks a lot different than the document used to create it. Almost none of these problems turn out to be problems with Beautiful Soup. This is not because Beautiful Soup is an amazingly well-written piece of software. It’s because Beautiful Soup doesn’t include any parsing code. Instead, it relies on external parsers. If one parser isn’t working on a certain document, the best solution is to try a different parser. See Installing a parser for details and a parser comparison. The most common parse errors are and . These are both generated by Python’s built-in HTML parser library, and the solution is to install lxml or html5lib. The most common type of unexpected behavior is that you can’t find a tag that you know is in the document. You saw it going in, but returns or returns . This is another common problem with Python’s built-in HTML parser, which sometimes skips tags it doesn’t understand. Again, the best solution is to install lxml or html5lib.\n• None (on the line ): Caused by running an old Python 2 version of Beautiful Soup under Python 3, without converting the code.\n• None - Caused by running an old Python 2 version of Beautiful Soup under Python 3.\n• None - Caused by running the Python 3 version of Beautiful Soup under Python 2.\n• None - Caused by running Beautiful Soup 3 code on a system that doesn’t have BS3 installed. Or, by writing Beautiful Soup 4 code without knowing that the package name has changed to .\n• None - Caused by running Beautiful Soup 4 code on a system that doesn’t have BS4 installed. By default, Beautiful Soup parses documents as HTML. To parse a document as XML, pass in “xml” as the second argument to the constructor: You’ll need to have lxml installed.\n• None If your script works on one computer but not another, or in one virtual environment but not another, or outside the virtual environment but not inside, it’s probably because the two environments have different parser libraries available. For example, you may have developed the script on a computer that has lxml installed, and then tried to run it on a computer that only has html5lib installed. See Differences between parsers for why this matters, and fix the problem by mentioning a specific parser library in the constructor.\n• None Because HTML tags and attributes are case-insensitive, all three HTML parsers convert tag and attribute names to lowercase. That is, the markup <TAG></TAG> is converted to <tag></tag>. If you want to preserve mixed-case or uppercase tags and attributes, you’ll need to parse the document as XML.\n• None (or just about any other ) - This problem shows up in two main situations. First, when you try to print a Unicode character that your console doesn’t know how to display. (See this page on the Python wiki for help.) Second, when you’re writing to a file and you pass in a Unicode character that’s not supported by your default encoding. In this case, the simplest solution is to explicitly encode the Unicode string into UTF-8 with .\n• None - Caused by accessing when the tag in question doesn’t define the attribute. The most common errors are and . Use if you’re not sure is defined, just as you would with a Python dictionary.\n• None - This usually happens because you expected to return a single tag or string. But returns a _list_ of tags and strings–a object. You need to iterate over the list and look at the of each one. Or, if you really only want one result, you need to use instead of .\n• None - This usually happens because you called and then tried to access the attribute of the result. But in your case, didn’t find anything, so it returned , instead of returning a tag or a string. You need to figure out why your call isn’t returning anything.\n• None - This usually happens because you’re treating a string as though it were a tag. You may be iterating over a list, expecting that it contains nothing but tags, when it actually contains both tags and strings. Beautiful Soup will never be as fast as the parsers it sits on top of. If response time is critical, if you’re paying for computer time by the hour, or if there’s any other reason why computer time is more valuable than programmer time, you should forget about Beautiful Soup and work directly atop lxml. That said, there are things you can do to speed up Beautiful Soup. If you’re not using lxml as the underlying parser, my advice is to start. Beautiful Soup parses documents significantly faster using lxml than using html.parser or html5lib. You can speed up encoding detection significantly by installing the cchardet library. Parsing only part of a document won’t save you much time parsing the document, but it can save a lot of memory, and it’ll make the document much faster.\n\nBeautiful Soup 3 is the previous release series, and is no longer being actively developed. It’s currently packaged with all major Linux distributions: It’s also published through PyPi as .: You can also download a tarball of Beautiful Soup 3.2.0. If you ran or , but your code doesn’t work, you installed Beautiful Soup 3 by mistake. You need to run . The documentation for Beautiful Soup 3 is archived online. Most code written against Beautiful Soup 3 will work against Beautiful Soup 4 with one simple change. All you should have to do is change the package name from to . So this:\n• None If you get the “No module named BeautifulSoup”, your problem is that you’re trying to run Beautiful Soup 3 code, but you only have Beautiful Soup 4 installed.\n• None If you get the “No module named bs4”, your problem is that you’re trying to run Beautiful Soup 4 code, but you only have Beautiful Soup 3 installed. Although BS4 is mostly backwards-compatible with BS3, most of its methods have been deprecated and given new names for PEP 8 compliance. There are numerous other renames and changes, and a few of them break backwards compatibility. Here’s what you’ll need to know to convert your BS3 code and habits to BS4: Beautiful Soup 3 used Python’s , a module that was deprecated and removed in Python 3.0. Beautiful Soup 4 uses by default, but you can plug in lxml or html5lib and use that instead. See Installing a parser for a comparison. Since is not the same parser as , you may find that Beautiful Soup 4 gives you a different parse tree than Beautiful Soup 3 for the same markup. If you swap out for lxml or html5lib, you may find that the parse tree changes yet again. If this happens, you’ll need to update your scraping code to deal with the new tree. Some arguments to the Beautiful Soup constructor were renamed for the same reasons: I renamed one method for compatibility with Python 3: I renamed one attribute to use more accurate terminology: I renamed three attributes to avoid using words that have special meaning to Python. Unlike the others, these changes are not backwards compatible. If you used these attributes in BS3, your code will break on BS4 until you change them. These methods are left over from the Beautiful Soup 2 API. They’ve been deprecated since 2006, and should not be used at all: I gave the generators PEP 8-compliant names, and transformed them into properties: Some of the generators used to yield after they were done, and then stop. That was a bug. Now the generators just stop. There are two new generators, .strings and .stripped_strings. yields NavigableString objects, and yields Python strings that have had whitespace stripped. There is no longer a class for parsing XML. To parse XML you pass in “xml” as the second argument to the constructor. For the same reason, the constructor no longer recognizes the argument. Beautiful Soup’s handling of empty-element XML tags has been improved. Previously when you parsed XML you had to explicitly say which tags were considered empty-element tags. The argument to the constructor is no longer recognized. Instead, Beautiful Soup considers any empty tag to be an empty-element tag. If you add a child to an empty-element tag, it stops being an empty-element tag. An incoming HTML or XML entity is always converted into the corresponding Unicode character. Beautiful Soup 3 had a number of overlapping ways of dealing with entities, which have been removed. The constructor no longer recognizes the or arguments. (Unicode, Dammit still has , but its default is now to turn smart quotes into Unicode.) The constants , , and have been removed, since they configure a feature (transforming some but not all entities into Unicode characters) that no longer exists. If you want to turn Unicode characters back into HTML entities on output, rather than turning them into UTF-8 characters, you need to use an output formatter. Tag.string now operates recursively. If tag A contains a single tag B and nothing else, then A.string is the same as B.string. (Previously, it was None.) Multi-valued attributes like have lists of strings as their values, not strings. This may affect the way you search by CSS class. objects now implement the method, such that two objects are considered equal if they generate the same markup. This may change your script’s behavior if you put objects into a dictionary or set. If you pass one of the methods both string a tag-specific argument like name, Beautiful Soup will search for tags that match your tag-specific criteria and whose Tag.string matches your value for string. It will find the strings themselves. Previously, Beautiful Soup ignored the tag-specific arguments and looked for strings. The constructor no longer recognizes the argument. It’s now the parser’s responsibility to handle markup correctly. The rarely-used alternate parser classes like and have been removed. It’s now the parser’s decision how to handle ambiguous markup. The method now returns a Unicode string, not a bytestring."
    },
    {
        "link": "https://stackoverflow.com/questions/65158345/beautiful-soup-not-returning-anything-i-expected",
        "document": "First of all, yes, the code from that Udemy course could be outdated. Especially if it shows what selectors/HTML elements they used to extract certain data.\n\nSome of the selectors are changed a few times on Bing.\n\nAs baduker said, it's because Bing detects that it's a script that sends a request. It simply detects because the default user-agent is so when you make a request, Bing sees that the user-agent from that request is thus unusual.\n\nTo override the default user-agent we can pass a new user-agent to custom-made request headers. Check what's your user-agent.\n\nAdditionally, you can rotate user-agents which will reduce the chance of being blocked a little more. You can also play around by passing different device user-agent such as desktop, tablet, or mobile.\n\nOutputs (only links from organic results, not from the knowledge graph, related searches, etc):\n\nIf you don't want to deal with bypassing blocks or maintaining your code, have a look at Bing Search API from SerpApi."
    },
    {
        "link": "https://pytba.readthedocs.io",
        "document": "TeleBot is synchronous and asynchronous implementation of Telegram Bot API."
    },
    {
        "link": "https://pytba.readthedocs.io/en/latest/types.html",
        "document": ""
    },
    {
        "link": "https://core.telegram.org/bots/api",
        "document": "All queries to the Telegram Bot API must be served over HTTPS and need to be presented in this form: . Like this for example:\n\nThe boost was obtained by the creation of a Telegram Premium or a Telegram Star giveaway. This boosts the chat 4 times for the duration of the corresponding Telegram Premium subscription for Telegram Premium giveaways and prize_star_count / 500 times for one year for Telegram Star giveaways.\n• Provide Telegram with an HTTP URL for the file to be sent. Telegram will download and send the file. 5 MB max size for photos and 20 MB max for other types of content.\n\nUse this method to get basic information about a file and prepare it for downloading. For the moment, bots can download files of up to 20MB in size. On success, a File object is returned. The file can then be downloaded via the link , where is taken from the response. It is guaranteed that the link will be valid for at least 1 hour. When the link expires, a new one can be requested by calling getFile again.\n\nUse this method to set a new profile photo for the chat. Photos can't be changed for private chats. The bot must be an administrator in the chat for this to work and must have the appropriate administrator rights. Returns True on success."
    },
    {
        "link": "https://stackoverflow.com/questions/77554321/python-telegram-bot-problem-sending-messages-through-the-bot-to-a-chat-as-if-t",
        "document": "Im creating a telegram bot and im stuck with one problem.\n\nAfter launching the bot, it sends me two buttons, \"yes\" and \"no.\" I press one of them, and I want my response to remain in the chat under my name.\n\nSo as a beginner i dont have as much information as i need.\n\nIn my code, errors occurred, particularly issues with data types and function arguments. When I used in the callback handling function, errors arose because this function doesn't directly accept the argument. The maximum I could achieve was having the bot send a response in its name, not mine. Therefore, I needed to use another method to pass user data so that the bot could send a message as if it were from me. But I don't know which one.\n\nThank you in advance for any response. |"
    },
    {
        "link": "https://pypi.org/project/pyTelegramBotAPI/3.6.3",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    }
]