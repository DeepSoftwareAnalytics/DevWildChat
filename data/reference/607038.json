[
    {
        "link": "https://docs.ultralytics.com/usage/python",
        "document": "Welcome to the Ultralytics YOLO Python Usage documentation! This guide is designed to help you seamlessly integrate Ultralytics YOLO into your Python projects for object detection, segmentation, and classification. Here, you'll learn how to load and use pretrained models, train new models, and perform predictions on images. The easy-to-use Python interface is a valuable resource for anyone looking to incorporate YOLO into their Python projects, allowing you to quickly implement advanced object detection capabilities. Let's get started!\n\nFor example, users can load a model, train it, evaluate its performance on a validation set, and even export it to ONNX format with just a few lines of code.\n\nTrain mode is used for training a YOLO model on a custom dataset. In this mode, the model is trained using the specified dataset and hyperparameters. The training process involves optimizing the model's parameters so that it can accurately predict the classes and locations of objects in an image.\n\nVal mode is used for validating a YOLO model after it has been trained. In this mode, the model is evaluated on a validation set to measure its accuracy and generalization performance. This mode can be used to tune the hyperparameters of the model to improve its performance.\n\nPredict mode is used for making predictions using a trained YOLO model on new images or videos. In this mode, the model is loaded from a checkpoint file, and the user can provide images or videos to perform inference. The model predicts the classes and locations of objects in the input images or videos.\n\nExport mode is used for exporting a YOLO model to a format that can be used for deployment. In this mode, the model is converted to a format that can be used by other software applications or hardware devices. This mode is useful when deploying the model to production environments.\n\nTrack mode is used for tracking objects in real-time using a YOLO model. In this mode, the model is loaded from a checkpoint file, and the user can provide a live video stream to perform real-time object tracking. This mode is useful for applications such as surveillance systems or self-driving cars.\n\nBenchmark mode is used to profile the speed and accuracy of various export formats for YOLO. The benchmarks provide information on the size of the exported format, its metrics (for object detection and segmentation) or metrics (for classification), and the inference time in milliseconds per image across various export formats like ONNX, OpenVINO, TensorRT and others. This information can help users choose the optimal export format for their specific use case based on their requirements for speed and accuracy.\n\nThe model class serves as a high-level wrapper for the Trainer classes. Each YOLO task has its own trainer, which inherits from . This architecture allows for greater flexibility and customization in your machine learning workflows.\n\nYou can easily customize Trainers to support custom tasks or explore research and development ideas. The modular design of Ultralytics YOLO allows you to adapt the framework to your specific needs, whether you're working on a novel computer vision task or fine-tuning existing models for better performance.\n\nHow can I integrate YOLO into my Python project for object detection?\n\nIntegrating Ultralytics YOLO into your Python projects is simple. You can load a pretrained model or train a new model from scratch. Here's how to get started:\n\nSee more detailed examples in our Predict Mode section.\n\nWhat are the different modes available in YOLO?\n\nUltralytics YOLO provides various modes to cater to different machine learning workflows. These include:\n‚Ä¢ Predict: Make predictions on new images or video streams.\n‚Ä¢ Export: Export models to various formats like ONNX and TensorRT.\n\nEach mode is designed to provide comprehensive functionalities for different stages of model development and deployment.\n\nHow do I train a custom YOLO model using my dataset?\n\nTo train a custom YOLO model, you need to specify your dataset and other hyperparameters. Here's a quick example:\n\nFor more details on training and hyperlinks to example usage, visit our Train Mode page.\n\nHow do I export YOLO models for deployment?\n\nExporting YOLO models in a format suitable for deployment is straightforward with the function. For example, you can export a model to ONNX format:\n\nFor various export options, refer to the Export Mode documentation.\n\nYes, validating YOLO models on different datasets is possible. After training, you can use the validation mode to evaluate the performance:\n\nCheck the Val Mode page for detailed examples and usage."
    },
    {
        "link": "https://docs.ultralytics.com",
        "document": "Introducing Ultralytics YOLO11, the latest version of the acclaimed real-time object detection and image segmentation model. YOLO11 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.\n\nExplore the Ultralytics Docs, a comprehensive resource designed to help you understand and utilize its features and capabilities. Whether you are a seasoned machine learning practitioner or new to the field, this hub aims to maximize YOLO's potential in your projects.\n\n\n\n \n\n Watch: How to Train a YOLO11 model on Your Custom Dataset in Google Colab.\n\nYOLO (You Only Look Once), a popular object detection and image segmentation model, was developed by Joseph Redmon and Ali Farhadi at the University of Washington. Launched in 2015, YOLO gained popularity for its high speed and accuracy.\n‚Ä¢ YOLOv2, released in 2016, improved the original model by incorporating batch normalization, anchor boxes, and dimension clusters.\n‚Ä¢ YOLOv3, launched in 2018, further enhanced the model's performance using a more efficient backbone network, multiple anchors, and spatial pyramid pooling.\n‚Ä¢ YOLOv4 was released in 2020, introducing innovations like Mosaic data augmentation, a new anchor-free detection head, and a new loss function.\n‚Ä¢ YOLOv5 further improved the model's performance and added new features such as hyperparameter optimization, integrated experiment tracking, and automatic export to popular export formats.\n‚Ä¢ YOLOv6 was open-sourced by Meituan in 2022 and is used in many of the company's autonomous delivery robots.\n‚Ä¢ YOLOv7 added additional tasks such as pose estimation on the COCO keypoints dataset.\n‚Ä¢ YOLOv8 released in 2023 by Ultralytics, introduced new features and improvements for enhanced performance, flexibility, and efficiency, supporting a full range of vision AI tasks.\n‚Ä¢ YOLOv9 introduces innovative methods like Programmable Gradient Information (PGI) and the Generalized Efficient Layer Aggregation Network (GELAN).\n‚Ä¢ YOLOv10 created by researchers from Tsinghua University using the Ultralytics Python package, provides real-time object detection advancements by introducing an End-to-End head that eliminates Non-Maximum Suppression (NMS) requirements.\n‚Ä¢ YOLO11 üöÄ NEW: Ultralytics' latest YOLO models, deliver state-of-the-art (SOTA) performance across multiple tasks, including object detection, segmentation, pose estimation, tracking, and classification, leveraging capabilities across diverse AI applications and domains.\n\nUltralytics offers two licensing options to accommodate diverse use cases:\n‚Ä¢ AGPL-3.0 License: This OSI-approved open-source license is ideal for students and enthusiasts, promoting open collaboration and knowledge sharing. See the LICENSE file for more details.\n‚Ä¢ Enterprise License: Designed for commercial use, this license permits seamless integration of Ultralytics software and AI models into commercial goods and services, bypassing the open-source requirements of AGPL-3.0. If your scenario involves embedding our solutions into a commercial offering, reach out through Ultralytics Licensing.\n\nOur licensing strategy is designed to ensure that any improvements to our open-source projects are returned to the community. We hold the principles of open source close to our hearts ‚ù§Ô∏è, and our mission is to guarantee that our contributions can be utilized and expanded upon in ways that are beneficial to all.\n\nObject detection has evolved significantly over the years, from traditional computer vision techniques to advanced deep learning models. The YOLO family of models has been at the forefront of this evolution, consistently pushing the boundaries of what's possible in real-time object detection.\n\nYOLO's unique approach treats object detection as a single regression problem, predicting bounding boxes and class probabilities directly from full images in one evaluation. This revolutionary method has made YOLO models significantly faster than previous two-stage detectors while maintaining high accuracy.\n\nWith each new version, YOLO has introduced architectural improvements and innovative techniques that have enhanced performance across various metrics. YOLO11 continues this tradition by incorporating the latest advancements in computer vision research, offering even better speed-accuracy trade-offs for real-world applications.\n\nWhat is Ultralytics YOLO and how does it improve object detection?\n\nUltralytics YOLO is the latest advancement in the acclaimed YOLO (You Only Look Once) series for real-time object detection and image segmentation. It builds on previous versions by introducing new features and improvements for enhanced performance, flexibility, and efficiency. YOLO supports various vision AI tasks such as detection, segmentation, pose estimation, tracking, and classification. Its state-of-the-art architecture ensures superior speed and accuracy, making it suitable for diverse applications, including edge devices and cloud APIs.\n\nHow can I get started with YOLO installation and setup?\n\nGetting started with YOLO is quick and straightforward. You can install the Ultralytics package using pip and get up and running in minutes. Here's a basic installation command:\n\nFor a comprehensive step-by-step guide, visit our Quickstart page. This resource will help you with installation instructions, initial setup, and running your first model.\n\nHow can I train a custom YOLO model on my dataset?\n\nTraining a custom YOLO model on your dataset involves a few detailed steps:\n‚Ä¢ Use the command to start training. (Each has its own argument)\n\nHere's example code for the Object Detection Task:\n\nFor a detailed walkthrough, check out our Train a Model guide, which includes examples and tips for optimizing your training process.\n\nWhat are the licensing options available for Ultralytics YOLO?\n‚Ä¢ AGPL-3.0 License: This open-source license is ideal for educational and non-commercial use, promoting open collaboration.\n‚Ä¢ Enterprise License: This is designed for commercial applications, allowing seamless integration of Ultralytics software into commercial products without the restrictions of the AGPL-3.0 license.\n\nFor more details, visit our Licensing page.\n\nHow can Ultralytics YOLO be used for real-time object tracking?\n\nUltralytics YOLO supports efficient and customizable multi-object tracking. To utilize tracking capabilities, you can use the command, as shown below:\n\nFor a detailed guide on setting up and running object tracking, check our Track Mode documentation, which explains the configuration and practical applications in real-time scenarios."
    },
    {
        "link": "https://github.com/ultralytics/ultralytics",
        "document": "Ultralytics YOLO11 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. YOLO11 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks. We hope that the resources here will help you get the most out of YOLO. Please browse the Ultralytics Docs for details, raise an issue on GitHub for support, questions, or discussions, become a member of the Ultralytics Discord, Reddit and Forums! To request an Enterprise License please complete the form at Ultralytics Licensing.\n\nSee below for a quickstart install and usage examples, and see our Docs for full documentation on training, validation, prediction and deployment.\n\nYOLO may be used directly in the Command Line Interface (CLI) with a command: can be used for a variety of tasks and modes and accepts additional arguments, e.g. . See the YOLO CLI Docs for examples. YOLO may also be used directly in a Python environment, and accepts the same arguments as in the CLI example above: See YOLO Python Docs for more examples.\n\nYOLO11 Detect, Segment and Pose models pretrained on the COCO dataset are available here, as well as YOLO11 Classify models pretrained on the ImageNet dataset. Track mode is available for all Detect, Segment and Pose models. All Models download automatically from the latest Ultralytics release on first use.\n\nOur key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with W&B, Comet, Roboflow and OpenVINO, can optimize your AI workflow.\n\nExperience seamless AI with Ultralytics HUB ‚≠ê, the all-in-one solution for data visualization, YOLO11 üöÄ model training and deployment, without any coding. Transform images into actionable insights and bring your AI visions to life with ease using our cutting-edge platform and user-friendly Ultralytics App. Start your journey for Free now!\n\nWe love your input! Ultralytics YOLO would not be possible without help from our community. Please see our Contributing Guide to get started, and fill out our Survey to send us feedback on your experience. Thank you üôè to all our contributors!\n\nUltralytics offers two licensing options to accommodate diverse use cases:\n‚Ä¢ AGPL-3.0 License: This OSI-approved open-source license is ideal for students and enthusiasts, promoting open collaboration and knowledge sharing. See the LICENSE file for more details.\n‚Ä¢ Enterprise License: Designed for commercial use, this license permits seamless integration of Ultralytics software and AI models into commercial goods and services, bypassing the open-source requirements of AGPL-3.0. If your scenario involves embedding our solutions into a commercial offering, reach out through Ultralytics Licensing.\n\nFor Ultralytics bug reports and feature requests please visit GitHub Issues. Become a member of the Ultralytics Discord, Reddit, or Forums for asking questions, sharing projects, learning discussions, or for help with all things Ultralytics!"
    },
    {
        "link": "https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov8.md",
        "document": "YOLOv8 was released by Ultralytics on January 10th, 2023, offering cutting-edge performance in terms of accuracy and speed. Building upon the advancements of previous YOLO versions, YOLOv8 introduced new features and optimizations that make it an ideal choice for various object detection tasks in a wide range of applications.\n‚Ä¢ Advanced Backbone and Neck Architectures: YOLOv8 employs state-of-the-art backbone and neck architectures, resulting in improved feature extraction and object detection performance.\n‚Ä¢ Anchor-free Split Ultralytics Head: YOLOv8 adopts an anchor-free split Ultralytics head, which contributes to better accuracy and a more efficient detection process compared to anchor-based approaches.\n‚Ä¢ Optimized Accuracy-Speed Tradeoff: With a focus on maintaining an optimal balance between accuracy and speed, YOLOv8 is suitable for real-time object detection tasks in diverse application areas.\n‚Ä¢ Variety of Pre-trained Models: YOLOv8 offers a range of pre-trained models to cater to various tasks and performance requirements, making it easier to find the right model for your specific use case.\n\nThe YOLOv8 series offers a diverse range of models, each specialized for specific tasks in computer vision. These models are designed to cater to various requirements, from object detection to more complex tasks like instance segmentation, pose/keypoints detection, oriented object detection, and classification.\n\nEach variant of the YOLOv8 series is optimized for its respective task, ensuring high performance and accuracy. Additionally, these models are compatible with various operational modes including Inference, Validation, Training, and Export, facilitating their use in different stages of deployment and development.\n\nThis table provides an overview of the YOLOv8 model variants, highlighting their applicability in specific tasks and their compatibility with various operational modes such as Inference, Validation, Training, and Export. It showcases the versatility and robustness of the YOLOv8 series, making them suitable for a variety of applications in computer vision.\n\nThis example provides simple YOLOv8 training and inference examples. For full documentation on these and other modes see the Predict, Train, Val and Export docs pages.\n\nNote the below example is for YOLOv8 Detect models for object detection. For additional supported tasks see the Segment, Classify, OBB docs and Pose docs.\n\nIf you use the YOLOv8 model or any other software from this repository in your work, please cite it using the following format:\n\nPlease note that the DOI is pending and will be added to the citation once it is available. YOLOv8 models are provided under AGPL-3.0 and Enterprise licenses.\n\nYOLOv8 is designed to improve real-time object detection performance with advanced features. Unlike earlier versions, YOLOv8 incorporates an anchor-free split Ultralytics head, state-of-the-art backbone and neck architectures, and offers optimized accuracy-speed tradeoff, making it ideal for diverse applications. For more details, check the Overview and Key Features sections.\n\nYOLOv8 supports a wide range of computer vision tasks, including object detection, instance segmentation, pose/keypoints detection, oriented object detection, and classification. Each model variant is optimized for its specific task and compatible with various operational modes like Inference, Validation, Training, and Export. Refer to the Supported Tasks and Modes section for more information.\n\nYOLOv8 models achieve state-of-the-art performance across various benchmarking datasets. For instance, the YOLOv8n model achieves a mAP (mean Average Precision) of 37.3 on the COCO dataset and a speed of 0.99 ms on A100 TensorRT. Detailed performance metrics for each model variant across different tasks and datasets can be found in the Performance Metrics section.\n\nTraining a YOLOv8 model can be done using either Python or CLI. Below are examples for training a model using a COCO-pretrained YOLOv8 model on the COCO8 dataset for 100 epochs:\n\nFor further details, visit the Training documentation.\n\nYes, YOLOv8 models can be benchmarked for performance in terms of speed and accuracy across various export formats. You can use PyTorch, ONNX, TensorRT, and more for benchmarking. Below are example commands for benchmarking using Python and CLI:"
    },
    {
        "link": "https://huggingface.co/Ultralytics/YOLOv8",
        "document": "Ultralytics YOLOv8 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks. We hope that the resources here will help you get the most out of YOLOv8. Please browse the YOLOv8 Docs for details, raise an issue on GitHub for support, questions, or discussions, become a member of the Ultralytics Discord, Reddit and Forums! To request an Enterprise License please complete the form at Ultralytics Licensing.\n\nSee below for a quickstart installation and usage example, and see the YOLOv8 Docs for full documentation on training, validation, prediction and deployment.\n\nYOLOv8 may be used directly in the Command Line Interface (CLI) with a command: can be used for a variety of tasks and modes and accepts additional arguments, i.e. . See the YOLOv8 CLI Docs for examples. YOLOv8 may also be used directly in a Python environment, and accepts the same arguments as in the CLI example above: See YOLOv8 Python Docs for more examples.\n\nYOLOv8 Detect, Segment and Pose models pretrained on the COCO dataset are available here, as well as YOLOv8 Classify models pretrained on the ImageNet dataset. Track mode is available for all Detect, Segment and Pose models.\n\nAll Models download automatically from the latest Ultralytics release on first use.\n\nOur key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with Roboflow, ClearML, Comet, Neural Magic and OpenVINO, can optimize your AI workflow.\n\nExperience seamless AI with Ultralytics HUB ‚≠ê, the all-in-one solution for data visualization, YOLOv5 and YOLOv8 üöÄ model training and deployment, without any coding. Transform images into actionable insights and bring your AI visions to life with ease using our cutting-edge platform and user-friendly Ultralytics App. Start your journey for Free now!\n\nWe love your input! YOLOv5 and YOLOv8 would not be possible without help from our community. Please see our Contributing Guide to get started, and fill out our Survey to send us feedback on your experience. Thank you üôè to all our contributors!\n\nUltralytics offers two licensing options to accommodate diverse use cases:\n‚Ä¢ AGPL-3.0 License: This OSI-approved open-source license is ideal for students and enthusiasts, promoting open collaboration and knowledge sharing. See the LICENSE file for more details.\n‚Ä¢ Enterprise License: Designed for commercial use, this license permits seamless integration of Ultralytics software and AI models into commercial goods and services, bypassing the open-source requirements of AGPL-3.0. If your scenario involves embedding our solutions into a commercial offering, reach out through Ultralytics Licensing.\n\nFor Ultralytics bug reports and feature requests please visit GitHub Issues. Become a member of the Ultralytics Discord, Reddit, or Forums for asking questions, sharing projects, learning discussions, or for help with all things Ultralytics!"
    },
    {
        "link": "https://docs.opencv.org/4.x/d6/d6e/group__imgproc__draw.html",
        "document": "Drawing functions work with matrices/images of arbitrary depth. The boundaries of the shapes can be rendered with antialiasing (implemented only for 8-bit images for now). All the functions include the parameter color that uses an RGB value (that may be constructed with the Scalar constructor ) for color images and brightness for grayscale images. For color images, the channel ordering is normally Blue, Green, Red. This is what imshow, imread, and imwrite expect. So, if you form a color using the Scalar constructor, it should look like:\n\nIf you are using your own image rendering and I/O functions, you can use any channel ordering. The drawing functions process each channel independently and do not depend on the channel order or even on the used color space. The whole image can be converted from BGR to RGB or to a different color space using cvtColor .\n\nIf a drawn figure is partially or completely outside the image, the drawing functions clip it. Also, many drawing functions can handle pixel coordinates specified with sub-pixel accuracy. This means that the coordinates can be passed as fixed-point numbers encoded as integers. The number of fractional bits is specified by the shift parameter and the real point coordinates are calculated as \\(\\texttt{Point}(x,y)\\rightarrow\\texttt{Point2f}(x*2^{-shift},y*2^{-shift})\\) . This feature is especially effective when rendering antialiased shapes.\n\nDraws a simple or thick elliptic arc or fills an ellipse sector. The function cv::ellipse with more parameters draws an ellipse outline, a filled ellipse, an elliptic arc, or a filled ellipse sector. The drawing code uses general parametric form. A piecewise-linear curve is used to approximate the elliptic arc boundary. If you need more control of the ellipse rendering, you can retrieve the curve using ellipse2Poly and then render it with polylines or fill it with fillPoly. If you use the first variant of the function and want to draw the whole ellipse, not an arc, pass and . If is greater than , they are swapped. The figure below explains the meaning of the parameters to draw the blue arc. Half of the size of the ellipse main axes. Starting angle of the elliptic arc in degrees. Ending angle of the elliptic arc in degrees. Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that a filled ellipse sector is to be drawn. Type of the ellipse boundary. See LineTypes Number of fractional bits in the coordinates of the center and values of axes."
    },
    {
        "link": "https://geeksforgeeks.org/python-opencv-cv2-puttext-method",
        "document": "is a library of Python bindings designed to solve computer vision problems.\n\nmethod is used to draw a text string on any image.\n\nSyntax: Parameters:image: It is the image on which text is to be drawn. text:org: It is the coordinates of the bottom-left corner of the text string in the image. The coordinates are represented as tuples of two values i.e. ( XYfont: It denotes the font type. Some of font types are FONT_HERSHEY_SIMPLEX, FONT_HERSHEY_PLAIN, fontScale: Font scale factor that is multiplied by the font-specific base size. color: It is the color of text string to be drawn. For BGR thickness: It is the thickness of the line in pxlineType: This is an optional parameter.It gives the type of the line to be used. bottomLeftOrigin: This is an optional parameter. When it is true, the image data origin is at the bottom-left corner. Otherwise, it is at the top-left corner. Return Value:\n\nImage used for all the below examples:\n\nHow Does OpenCV Work in Python?\n\nWhat is an Innovative Use of cv2.circle() in OpenCV?\n\nHow to Show Text in OpenCV?\n\nWhat is OpenCV in Object Detection?\n\nWhat is the Method Used to Read Image in OpenCV Library?\n\nThe method used to read an image in OpenCV is . This function loads an image from the specified file and returns it as a NumPy array. The image is read in the color format specified by the flag provided to . Here‚Äôs how to use it: The function is crucial for bringing image data into an environment where it can be processed and analyzed using various OpenCV functions."
    },
    {
        "link": "https://geeksforgeeks.org/python-opencv-cv2-rectangle-method",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/56108183/python-opencv-cv2-drawing-rectangle-with-text",
        "document": "I draw a rectangle on my image using\n\nI would like to draw rectangles with text information on them. How do I do it? Are there any ready to use implementations available? Or should I match the top left coordinate of the rectangle and try to display a different cv2 text element with the cv2 rect element?\n\nCan you direct me to any code implementation/workaround?\n\nP.S: I don't want to use the object_detection. visualisation utils available with tf."
    },
    {
        "link": "https://docs.opencv.org/4.x/dc/da5/tutorial_py_drawing_functions.html",
        "document": "\n‚Ä¢ Learn to draw different geometric shapes with OpenCV\n‚Ä¢ You will learn these functions : cv.line(), cv.circle() , cv.rectangle(), cv.ellipse(), cv.putText() etc.\n\nIn all the above functions, you will see some common arguments as given below:\n‚Ä¢ img : The image where you want to draw the shapes\n‚Ä¢ color : Color of the shape. for BGR, pass it as a tuple, eg: (255,0,0) for blue. For grayscale, just pass the scalar value.\n‚Ä¢ thickness : Thickness of the line or circle etc. If -1 is passed for closed figures like circles, it will fill the shape. default thickness = 1\n‚Ä¢ lineType : Type of line, whether 8-connected, anti-aliased line etc. By default, it is 8-connected. cv.LINE_AA gives anti-aliased line which looks great for curves.\n\nTo draw a line, you need to pass starting and ending coordinates of line. We will create a black image and draw a blue line on it from top-left to bottom-right corners.\n\nTo draw a rectangle, you need top-left corner and bottom-right corner of rectangle. This time we will draw a green rectangle at the top-right corner of image.\n\nTo draw a circle, you need its center coordinates and radius. We will draw a circle inside the rectangle drawn above.\n\nTo draw the ellipse, we need to pass several arguments. One argument is the center location (x,y). Next argument is axes lengths (major axis length, minor axis length). angle is the angle of rotation of ellipse in anti-clockwise direction. startAngle and endAngle denotes the starting and ending of ellipse arc measured in clockwise direction from major axis. i.e. giving values 0 and 360 gives the full ellipse. For more details, check the documentation of cv.ellipse(). Below example draws a half ellipse at the center of the image.\n\nTo draw a polygon, first you need coordinates of vertices. Make those points into an array of shape ROWSx1x2 where ROWS are number of vertices and it should be of type int32. Here we draw a small polygon of with four vertices in yellow color.\n\nTo put texts in images, you need specify following things.\n‚Ä¢ Text data that you want to write\n‚Ä¢ Position coordinates of where you want put it (i.e. bottom-left corner where data starts).\n‚Ä¢ regular things like color, thickness, lineType etc. For better look, lineType = cv.LINE_AA is recommended.\n\nWe will write OpenCV on our image in white color.\n\nSo it is time to see the final result of our drawing. As you studied in previous articles, display the image to see it.\n‚Ä¢ The angles used in ellipse function is not our circular angles. For more details, visit this discussion.\n‚Ä¢ Try to create the logo of OpenCV using drawing functions available in OpenCV."
    }
]