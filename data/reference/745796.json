[
    {
        "link": "https://stackoverflow.com/questions/53821528/correct-way-of-performing-multiple-sequential-actions-over-a-python-list",
        "document": "I have to perform multiple sequential operations over a Python list that stores lines of a text in order to clean that data. Currently (as you can see bellow) I am creating a new variable every time each operation is performed. My question is: is there a better (more pythonistic) way of doing all these actions without creating a new variable every time I want to change the data?\n\nThis is just an example of what I am currently doing:"
    },
    {
        "link": "https://labex.io/tutorials/python-how-to-generate-sequential-lists-in-python-431128",
        "document": "In Python, lists are versatile and fundamental data structures that allow you to store and manipulate collections of items. Understanding how to generate sequential lists is crucial for efficient programming.\n\nThe simplest way to create a list is through direct initialization:\n\nYou can use the constructor to create lists from other iterable objects:\n\nThe function is powerful for generating sequential lists:\n• Choose the most readable and efficient method for your specific use case\n\nAt LabEx, we recommend mastering these list generation techniques to write more pythonic and efficient code. Practice and experimentation are key to becoming proficient in Python list manipulation."
    },
    {
        "link": "https://stackoverflow.com/questions/64940055/is-there-a-way-of-toggling-elements-in-a-python-list",
        "document": "I'm new in programming, and am finding a way (if any) to toggle the elements in a list.\n\nI have a list where the elements can be either 1 or 0. I want to toggle them went I give a particular input.\n\n I tried the following way:\n\nBut printing this is just print a copy of the original list.\n\nHow do I toggle them? Is there a function for that?\n\n I am on Python 3.8."
    },
    {
        "link": "https://pythonhosted.org/SimPy/Tutorials/TheBank.html",
        "document": "SimPy is used to develop a simple simulation of a bank with a number of tellers. This Python package provides Processes to model active components such as messages, customers, trucks, and planes. It has three classes to model facilities where congestion might occur: Resources for ordinary queues, Levels for the supply of quantities of material, and Stores for collections of individual items. Only examples of Resources are described here. It also provides Monitors and Tallys to record data like queue lengths and delay times and to calculate simple averages. It uses the standard Python random package to generate random numbers. Starting with SimPy 2.0 an object-oriented programmer’s interface was added to the package. It is quite compatible with the current procedural approach which is used in the models described here. SimPy can be obtained from: http://sourceforge.net/projects/simpy. The examples run with SimPy version 1.5 and later. This tutorial is best read with the SimPy Manual or Cheatsheet at your side for reference. Before attempting to use SimPy you should be familiar with the Python language. In particular you should be able to use classes. Python is free and available for most machine types. You can find out more about it at the Python web site. SimPy is compatible with Python version 2.3 and later.\n\nIn this tutorial we model a simple bank with customers arriving at random. We develop the model step-by-step, starting out simply, and producing a running program at each stage. The programs we develop are available without line numbers and ready to go, in the directory. Please copy them, run them and improve them - and in the tradition of open-source software suggest your modifications to the SimPy users list. Object-orented versions of all the models are included in the same directory. A simulation should always be developed to answer a specific question; in these models we investigate how changing the number of bank servers or tellers might affect the waiting time for customers. We first model a single customer who arrives at the bank for a visit, looks around at the decor for a time and then leaves. There is no queueing. First we will assume his arrival time and the time he spends in the bank are fixed. We define a class derived from the SimPy class. We create a object, who arrives at the bank at simulation time and leaves after a fixed time of minutes. Examine the following listing which is a complete runnable Python script, except for the line numbers. We use comments to divide the script up into sections. This makes for clarity later when the programs get more complicated. Line 1 is a normal Python documentation string; line 2 imports the SimPy simulation code. The class definition, lines 6-12, defines our customer class and has the required generator method (called ) (line 9) having a statement (line 11). Such a method is called a Process Execution Method (PEM) in SimPy. The customer’s PEM, lines 9-12, models his activities. When he arrives (it will turn out to be a ‘he’ in this model), he will print out the simulation time, , and his name (line 10). The function can be used at any time in the simulation to find the current simulation time though it cannot be changed by the programmer. The customer’s name will be set when the customer is created later in the script (line 22). He then stays in the bank for a fixed simulation time (line 11). This is achieved by the statement. This is the first of the special simulation commands that offers. After a simulation time of , the program’s execution returns to the line after the statement, line 12. The customer then prints out the current simulation time and his name. This completes the declaration of the class. Line 21 calls which sets up the simulation system ready to receive calls. In line 22, we create a customer, , with name . All SimPy Processes have a attribute. We in line 23 specifying the object ( ) to be activated, the call of the action routine ( ) and that it is to be activated at time 5 ( ). This will activate exactly minutes after the current time, in this case after the start of the simulation at . The call of an action routine such as can specify the values of arguments, here the . Finally the call of in line 24 will start the simulation. This will run until the simulation time is unless stopped beforehand either by the command or by running out of events to execute (as will happen here). was set to in line 16. \"\"\" Customer arrives, looks around and leaves \"\"\" The short trace printed out by the statements shows the result. The program finishes at simulation time because there are no further events to be executed. At the end of the routine, the customer has no more actions and no other objects or customers are active. 5.0 Klaus Here I am 15.0 Klaus I must leave Now we extend the model to allow our customer to arrive at a random simulated time though we will keep the time in the bank at 10.0, as before. The change occurs in line 3 of the program and in lines 22, 25, and 26. In line 3 we import from the standard Python module to give us to generate the random time of arrival. We also import the function to initialize the random number stream to allow control of the random numbers. In line 22 we provide an initial seed of . An exponential random variate, , is generated in line 25. Note that the Python Random module’s function uses the rate (that is, ) as the argument. The generated random variate, , is used in Line 26 as the argument to the call. looks around and then leaves \"\"\" The result is shown below. The customer now arrives at time 10.5809. Changing the seed value would change that time. 10.5809228279 Klaus Here I am 20.5809228279 Klaus I must leave The display looks pretty untidy. In the next example I will try and make it tidier.\n\nOur simulation does little so far. To consider a simulation with several customers we return to the simple deterministic model and add more . The program is almost as easy as the first example (A Customer arriving at a fixed time). The main change is in lines 22-27 where we create, name, and activate three customers. We also increase the maximum simulation time to (line 16 and referred to in line 29). Observe that we need only one definition of the class and create several objects of that class. These will act quite independently in this model. Each customer stays for a different so, instead of setting a common value for this we set it for each customer. The customers are started at different times (using ). activation time occurs before , so will arrive first even though his activation statement appears later in the script. As promised, the print statements have been changed to use Python string formatting (lines 10 and 12). The statements look complicated but the output is much nicer. \"\"\" Customer arrives, looks around and leaves \"\"\" The trace produced by the program is shown below. Again the simulation finishes before the specified in the call. 2.0000 Tony: Here I am 5.0000 Klaus: Here I am 9.0000 Tony: I must leave 12.0000 Evelyn: Here I am 15.0000 Klaus: I must leave 32.0000 Evelyn: I must leave Another change will allow us to have more customers. As it is tedious to give a specially chosen name to each one, we will call them and use a separate class to create and activate them. To make things clearer we do not use the random numbers in this model. The following listing shows the new program. Lines 6-13 define a class. Its PEM, here called , is defined in lines 9-13. This PEM has a couple of arguments: the of customers to be generated and the Time Between Arrivals, . It consists of a loop that creates a sequence of numbered from to , inclusive. We create a customer and give it a name in line 11. It is then activated at the current simulation time (the final argument of the statement is missing so that the default value of is used as the time). We also specify how long the customer is to stay in the bank. To keep it simple, all customers stay exactly minutes. After each new customer is activated, the holds for a fixed time ( ) before creating the next one (line 13). A , , is created in line 32 and activated at line 33 where the number of customers to be generated is set to and the interval between customers to . Once started at time it creates customers at intervals and each customer then operates independently of the others: \"\"\" Customer arrives, looks around and leaves \"\"\" 0.0000 Customer00: Here I am 10.0000 Customer01: Here I am 12.0000 Customer00: I must leave 20.0000 Customer02: Here I am 22.0000 Customer01: I must leave 30.0000 Customer03: Here I am 32.0000 Customer02: I must leave 40.0000 Customer04: Here I am 42.0000 Customer03: I must leave 52.0000 Customer04: I must leave We now extend this model to allow arrivals at random. In simulation this is usually interpreted as meaning that the times between customer arrivals are distributed as exponential random variates. There is little change in our program, we use a object, as before. The exponential random variate is generated in line 14 with as the mean Time Between Arrivals and used in line 15. Note that this parameter is not exactly intuitive. As already mentioned, the Python method uses the rate of arrivals as the parameter not the average interval between them. The exponential delay between two arrivals gives pseudo-random arrivals. In this model the first customer arrives at time . The method is called to initialize the random number stream in the routine (line 33). It is possible to leave this call out but if we wish to do serious comparisons of systems, we must have control over the random variates and therefore control over the seeds. Then we can run identical models with different seeds or different models with identical seeds. We provide the seeds as control parameters of the run. Here a seed is assigned in line 33 but it is clear it could have been read in or manually entered on an input form. \"\"\" Customer arrives, looks around and leaves \"\"\" with the following output: 0.0000 Customer00: Here I am 12.0000 Customer00: I must leave 21.1618 Customer01: Here I am 32.8968 Customer02: Here I am 33.1618 Customer01: I must leave 33.3790 Customer03: Here I am 36.3979 Customer04: Here I am 44.8968 Customer02: I must leave 45.3790 Customer03: I must leave 48.3979 Customer04: I must leave\n\nSo far, the model has been more like an art gallery, the customers entering, looking around, and leaving. Now they are going to require service from the bank clerk. We extend the model to include a service counter which will be modelled as an object of SimPy’s class with a single resource unit. The actions of a are simple: a customer a unit of the resource (a clerk). If one is free he gets service (and removes the unit). If there is no free clerk the customer joins the queue (managed by the resource object) until it is their turn to be served. As each customer completes service and the unit, the clerk can start serving the next in line. The service counter is created as a ( ) in line 38. This is provided as an argument to the (line 45) which, in turn, provides it to each customer it creates and activates (line 14). The actions involving the service counter, , in the customer’s PEM are:\n• the statement in line 25. If the server is free then the customer can start service immediately and the code moves on to line 26. If the server is busy, the customer is automatically queued by the Resource. When it eventually comes available the PEM moves on to line 26.\n• the statement in line 28 where the operation of the service counter is modelled. Here the service time is a fixed . During this period the customer is being served.\n• the statement in line 29. The current customer completes service and the service counter becomes available for any remaining customers in the queue. Observe that the service counter is used with the pattern ( ; ; ). To show the effect of the service counter on the activities of the customers, I have added line 22 to record when the customer arrived and line 26 to record the time between arrival in the bank and starting service. Line 26 is after the command and will be reached only when the request is satisfied. It is before the that corresponds to the start of service. The variable will record how long the customer waited and will be 0 if he received service at once. This technique of saving the arrival time in a variable is common. So the statement also prints out how long the customer waited in the bank before starting service. Examining the trace we see that the first two customers get instant service but the others have to wait. We still only have five customers (line 35) so we cannot draw general conclusions. 0.000 Customer00: Here I am 0.000 Customer00: Waited 0.000 12.000 Customer00: Finished 21.162 Customer01: Here I am 21.162 Customer01: Waited 0.000 32.897 Customer02: Here I am 33.162 Customer01: Finished 33.162 Customer02: Waited 0.265 33.379 Customer03: Here I am 36.398 Customer04: Here I am 45.162 Customer02: Finished 45.162 Customer03: Waited 11.783 57.162 Customer03: Finished 57.162 Customer04: Waited 20.764 69.162 Customer04: Finished This is a simple change to the model in that we retain the single service counter but make the customer service time a random variable. As is traditional in the study of simple queues we first assume an exponential service time and set the mean to . The service time random variable, , is generated in line 26 and used in line 27. The argument to be used in the call of is not the mean of the distribution, , but is the rate . We have also collected together a number of constants by defining a number of appropriate variables and giving them values. These are in lines 31 to 42. 0.0000 Customer00: Here I am 0.0000 Customer00: Waited 0.000 8.7558 Customer01: Here I am 10.6770 Customer02: Here I am 22.7622 Customer03: Here I am 32.7477 Customer04: Here I am 55.0607 Customer00: Finished 55.0607 Customer01: Waited 46.305 61.8905 Customer01: Finished 61.8905 Customer02: Waited 51.213 83.7556 Customer02: Finished 83.7556 Customer03: Waited 60.993 108.7794 Customer03: Finished 108.7794 Customer04: Waited 76.032 118.8254 Customer04: Finished This model with random arrivals and exponential service times is an example of an M/M/1 queue and could rather easily be solved analytically to calculate the steady-state mean waiting time and other operating characteristics. (But not so easily solved for its transient behavior.)\n\nWhen we introduce several counters we must decide on a queue discipline. Are customers going to make one queue or are they going to form separate queues in front of each counter? Then there are complications - will they be allowed to switch lines (jockey)? We first consider a single queue with several counters and later consider separate isolated queues. We will not look at jockeying. Here we model a bank whose customers arrive randomly and are to be served at a group of counters, taking a random time for service, where we assume that waiting customers form a single first-in first-out queue. The only difference between this model and the single-server model is in line 42. We have provided two counters by increasing the capacity of the resource to 2. These units of the resource correspond to the two counters. Because both clerks cannot be called , we have used a general name of . The waiting times in this model are much shorter than those for the single service counter. For example, the waiting time for has been reduced from to minutes. Again we have too few customers processed to draw general conclusions. 0.0000 Customer00: Here I am 0.0000 Customer00: Waited 0.000 8.7558 Customer01: Here I am 8.7558 Customer01: Waited 0.000 10.6770 Customer02: Here I am 20.6626 Customer03: Here I am 23.2580 Customer01: Finished 23.2580 Customer02: Waited 12.581 30.0878 Customer02: Finished 30.0878 Customer03: Waited 9.425 37.0790 Customer04: Here I am 51.9528 Customer03: Finished 51.9528 Customer04: Waited 14.874 55.0607 Customer00: Finished 61.9988 Customer04: Finished Each counter is now assumed to have its own queue. The programming is more complicated because the customer has to decide which queue to join. The obvious technique is to make each counter a separate resource and it is useful to make a list of resource objects (line 56). In practice, a customer will join the shortest queue. So we define the Python function, (lines 17-19) which returns the sum of the number waiting and the number being served for a particular counter, . This function is used in line 28 to list the numbers at each counter. It is then easy to find which counter the arriving customer should join. We have also modified the trace printout, line 29 to display the state of the system when the customer arrives. We choose the shortest queue in lines 30-32 (the variable ). The rest of the program is the same as before. \"\"\" Total number of customers in the resource R\"\"\" The results show how the customers choose the counter with the smallest number. Unlucky who joins the wrong queue has to wait until finishes at time . There are, however, too few arrivals in these runs, limited as they are to five customers, to draw any general conclusions about the relative efficiencies of the two systems. 0.0000 Customer00: Here I am. [0, 0] 0.0000 Customer00: Waited 0.000 8.7558 Customer01: Here I am. [1, 0] 8.7558 Customer01: Waited 0.000 10.6770 Customer02: Here I am. [1, 1] 20.6626 Customer03: Here I am. [2, 1] 23.2580 Customer01: Finished 23.2580 Customer03: Waited 2.595 30.0878 Customer03: Finished 37.0790 Customer04: Here I am. [2, 0] 37.0790 Customer04: Waited 0.000 55.0607 Customer00: Finished 55.0607 Customer02: Waited 44.384 62.1029 Customer04: Finished 65.1067 Customer02: Finished\n\nThe traces of output that have been displayed so far are valuable for checking that the simulation is operating correctly but would become too much if we simulate a whole day. We do need to get results from our simulation to answer the original questions. What, then, is the best way to summarize the results? One way is to analyze the traces elsewhere, piping the trace output, or a modified version of it, into a real statistical program such as R for statistical analysis, or into a file for later examination by a spreadsheet. We do not have space to examine this thoroughly here. Another way of presenting the results is to provide graphical output. SimPy offers an easy way to gather a few simple statistics such as averages: the and classes. The records the values of chosen variables as time series (but see the comments in Final Remarks). We now demonstrate a that records the average waiting times for our customers. We return to the system with random arrivals, random service times and a single queue and remove the old trace statements. In practice, we would make the printouts controlled by a variable, say, which is set in the experimental data (or read in as a program option - but that is a different story). This would aid in debugging and would not complicate the data analysis. We will run the simulations for many more arrivals. A Monitor, , is created in line 42. It the waiting time mentioned in line 24. We run customers (in the call of in line 45) and have increased to minutes. The average waiting time for 50 customers in this 2-counter system is more reliable (i.e., less subject to random simulation effects) than the times we measured before but it is still not sufficiently reliable for real-world decisions. We should also replicate the runs using different random number seeds. The result of this run is: To get a number of independent measurements we must replicate the runs using different random number seeds. Each replication must be independent of previous ones so the Monitor and Resources must be redefined for each run. We can no longer allow them to be global objects as we have before. We will define a function, with a parameter so that the random number seed can be different for different runs (lines 40-50). The contents of the function are the same as the section in the previous program except for one vital change. This is required since the Monitor, , is defined inside the function (line 43). A customer can no longer refer to it. In the spirit of quality computer programming we will pass as a function argument. Unfortunately we have to do this in two steps, first to the (line 48) and then from the to the (line 13). is run for four different random-number seeds to get a set of replications (lines 54-57). \"\"\" bank12: Multiple runs of the bank with a Monitor\"\"\" The results show some variation. Remember, though, that the system is still only operating for 50 customers so the system may not be in steady-state. Average wait for 50 completions was 2.75 minutes. Average wait for 50 completions was 6.01 minutes. Average wait for 50 completions was 5.53 minutes. Average wait for 50 completions was 3.76 minutes."
    },
    {
        "link": "https://github.com/TL-System/ns.py",
        "document": "This discrete-event network simulator is based on , which is a general-purpose discrete event simulation framework for Python. is designed to be flexible and reusable, and can be used to connect multiple networking components together easily, including packet generators, network links, switch elements, schedulers, traffic shapers, traffic monitors, and demultiplexing elements.\n\nFirst, launch the terminal and create a new environment (say, called ):\n\nThat's it! You can now try to run some examples in the directory. To upgrade Python packages in the current environment, run the command:\n\nThe network components that have already been implemented include:\n• : a simple representation of a network packet, carrying its creation time, size, packet id, flow id, source and destination.\n• : generates packets according to provided distributions of inter-arrival times and packet sizes.\n• : generates packets according to a trace file, with each row in the trace file representing a packet.\n• : generates packets using TCP as the transport protocol.\n• : redirects real-world packets (with fixed packet sizes) into the simulation environment.\n• : an output port on a switch with a given rate and buffer size (in either bytes or the number of packets), using the simple tail-drop mechanism to drop packets.\n• : an output port on a switch with a given rate and buffer size (in either bytes or the number of packets), using the Early Random Detection (RED) mechanism to drop packets.\n• : a network wire (cable) with its propagation delay following a given distribution. There is no need to model the bandwidth of the wire, as that can be modeled by its upstream or scheduling server.\n• : a splitter that simply sends the original packet out of port 1 and sends a copy of the packet out of port 2.\n• : an n-way splitter that sends copies of the packet to n downstream elements.\n• : a two rate three color marker that marks packets as green, yellow, or red (refer to RFC 2698 for more details).\n• : a demultiplexing element that chooses the output port at random.\n• : a demultiplexing element that uses a Flow Information Base (FIB) to make packet forwarding decisions based on flow IDs.\n• : a two-rate three-color token bucket shaper with both committed and peak rates/burst sizes.\n• : a packet switch with a FIFO bounded buffer on each of the outgoing ports.\n• : a fair packet switch with a choice of a WFQ, DRR, Static Priority or Virtual Clock scheduler, as well as bounded buffers, on each of the outgoing ports. It also shows an example how a simple hash function can be used to map tuples of (flow_id, node_id, and port_id) to class IDs, and then use the parameter to activate class-based scheduling rather than flow_based scheduling.\n• : records the number of packets in a . The monitoring interval follows a given distribution.\n• : records performance statistics in a scheduling server, such as , , , or .\n• : a sorted based on tags, useful in the implementation of WFQ and Virtual Clock.\n• : a global singleton instance that reads parameter settings from a configuration file. Use to access the instance globally.\n\nSome of these examples requires installing . It has not been included in the list of dependencies in , and needs to be installed separately in the current Python environment.\n• : A basic example that connects two packet generators to a network wire with a propagation delay distribution, and then to a packet sink. It showcases , , and .\n• : an example that contains a packet generator connected to a downstream switch port, which is then connected to a packet sink. It showcases , , and .\n• : this example shows how to simulate a port with exponential packet inter-arrival times and exponentially distributed packet sizes. It showcases , , , and .\n• : this example shows how a two-hop simple network from a sender to a receiver, via a simple packet forwarding switch, can be configured, and how acknowledgment packets can be sent from the receiver back to the sender via the same switch. The sender uses a TCP as its transport protocol, and the congestion control algorithm is configurable (such as TCP Reno or TCP CUBIC). It showcases , , , , and .\n• : this example creates a traffic shaper whose bucket size is the same as the packet size, and whose bucket rate is one half the input packet rate. It showcases , , and .\n• : this example creates a two-rate three-color traffic shaper. It showcases , , and .\n• : this example shows how to use two Static Priority (SP) schedulers to construct a more complex two-layer scheduler, turning on for the upstream scheduler and for the downstream one. It showcases , , and .\n• : this example shows how to use the Weighted Fair Queueing (WFQ) scheduler, and how to use a server monitor to record performance statistics with a finer granularity using a sampling distribution. It showcases , , , , and .\n• : this example shows how to use the Virtual Clock scheduler, and how to use a server monitor to record performance statistics with a finer granularity using a sampling distribution. It showcases , , , , and .\n• : this example shows how to use the Deficit Round Robin (DRR) scheduler. It showcases , , and .\n• , , : these examples have shown how to construct a two-level topology consisting of Deficit Round Robin (DRR), Weighted Fair Queueing (WFQ) and Static Priority (SP) servers. They also show how to use strings for flow IDs and to use dictionaries to provide per-flow weights to the DRR, WFQ, or SP servers, so that group IDs and per-group flow IDs can be easily used to construct globally unique flow IDs.\n• : this example shows how to combine a Random Early Detection (RED) buffer (or a tail-drop buffer) and a WFQ server. The RED or tail-drop buffer serves as an upstream input buffer, configured to recognize that its downstream element has a zero-buffer configuration. The WFQ server is initialized with zero buffering as the downstream element after the RED or tail-drop buffer. Packets will be dropped when the downstream WFQ server is the bottleneck. It showcases , , , , , and , as well as how and can be used to construct more complex network elements using elementary elements.\n• : an example that shows how to construct and use a FatTree topology for network flow simulation. It showcases , , , and . If per-flow fairness is desired, would be used, along with Weighted Fair Queueing, Deficit Round Robin, or Virtual Clock as the scheduling discipline at each outgoing port of the switch.\n\nSimilar to the emulation mode in the ns-3 simulator, supports an emulation mode that serves as a proxy between a real-world client (such as a modern web browser) and a real-world server (such as a node.js webserver). All incoming traffic from a real-world client are handled by the , sent via a simulated network topology, and forwarded by the to a real-world server. Here is a high-level overview of the design of 's emulation mode:\n\nhas been provided as an example that shows how a real-world client and server can communicate using a simulated network environment as the proxy, and how and are to be used to achieve this objective.\n\nA simple echo client and echo server have been provided for an example demonstration how the proxy works. To run this example with the provided echo client and echo server, start the server first:\n\nThe TCP echo server will listen on port 10000 on .\n\nNow run the provided simple example for a TCP proxy:\n\nThis TCP proxy will now listen on port 5000, and redirects all traffic to , which is where the TCP echo server is.\n\nIt will send one simple message to port 5000, where the TCP proxy is.\n\nTo use an UDP proxy instead, first run the UDP echo server, which listens on port 10000 on :\n\nThen run the UDP proxy on port 10000, asking it to redirect all traffic to , where the UDP echo server is.\n\nA simple HTTPS server has been provided in . To use it to test the emulation mode, you will need to generate a self-signed server certificate first:\n\nNow you can run the proxy with the HTTPS server as its destination:\n\nFinally, run a HTTPS client to connect to the HTTP server:\n\nTo design and implement new network components in this framework, you will first need to read the 10-minute SimPy tutorial. It literally takes 10 minutes to read, but if that is still a bit too long, you can safely skip the section on Process Interaction, as this feature will rarely be used in this network simulation framework.\n\nIn the Basic Concepts section of this tutorial, pay attention to three simple calls: , , and . These are heavily used in this network simulation framework.\n\nThe first is used in our component's constructor to add this component's method to the environment. For example, in :\n\nKeep in mind that not all network components need to be run as a SimPy process (more discussions on processes later). While traffic shapers, packet generators, ports (buffers), port monitors, and packet schedulers definitely should be implemented as processes, a flow demultiplexer, a packet sink, a traffic marker, or a traffic splitter do not need to be modeled as processes. They just represent additional processing on packets inside a network.\n\nThe second call, , is used by our examples to run the environment after connecting all the network components together. For example, in :\n\nThis call simply runs the environment for 100 seconds.\n\nThe third call, , schedules an event to be fired sometime in the future. SimPy uses an ancient feature in Python that's not well known, generator functions, to implement what it called processes. The term process is a bit confusing, as it has nothing to do with processes in operating systems. In SimPy, each process is simply a sequence of timed events, and multiple processes occur concurrently in real-time. For example, a scheduler is a process in a network, and so is a traffic shaper. The traffic shaper runs concurrently with the scheduler, and both of these components run concurrently with other traffic shapers and schedulers in other switches throughout the network.\n\nIn order to implement these processes in a network simulation, we almost always use the call. Here, uses the feature of generator functions to return an iterator, rather than a value. This is just a fancier way of saying that it yields the process in SimPy, allowing other processes to run for a short while, and it will be resumed at a later time specified by the timeout value. For example, for a Deficit Round Robin (DRR) scheduler to send a packet (in ), it simply calls:\n\nwhich implies that the scheduler process will resume its execution after the transmission time of the packet elapses. A side note: in our network components implemented so far, we assume that the (or bandwidth) of a link is measured in bits per second, while everything else is measured in bytes. As a result, we will need a little bit of a unit conversion here.\n\nWhat a coincidence: the keyword in Python in generator functions is the same as the system call in an operating system kernel! This makes the code much more readable: whenever a process in SimPy needs to wait for a shared resource or a timeout, simply call , just like calling a system call in an operating system.\n\nWatch out for a potential pitfall: Make sure that you call at least once in every path of program execution. This is more important in an infinite loop in , which is very typical in our network components since the environment can be run for a finite amount of simulation time. For example, at the end of each iteration of the infinite loop in , we call :\n\nThis works just like a call on a binary semaphore in operating systems, and will make sure that other processes have a chance to run when there are no packets in the scheduler. This is, on the other hand, not a problem in our Weighted Fair Queueing (WFQ) scheduler ( ), since we call to retrieve the next packet for processing, and is implemented as a sorted queue ( ). This process will not be resumed after if there are no packets in the scheduler.\n\nThe Shared Resources section of the 10-minute SimPy tutorial discussed a mechanism to request and release (either automatically or manually) a shared resource by using the and calls. In this network simulation framework, we will simplify this by directly calling:\n\nHere, is an instance of , which is a simple first-in-first-out buffer containing shared resources in SimPy. We initialize one such buffer for each flow in :\n\nHow do we send a packet to a downstream component in the network? All we need to do is to call the component's function. For example, in , we run:\n\nafter a timeout expires. Here, is initialized to , and it is up to the program to set up. In , we set the downstream component of our DRR scheduler to a packet sink:\n\nBy connecting multiple components this way, a network can be established with packets flowing from packet generators to packet sinks, going through a variety of schedulers, traffic shapers, traffic splitters, and flow demultiplexers.\n\nFlow IDs are assigned to packets when they are generated by a packet generator, which is (optionally) initialized with a specific flow ID. We use flow IDs extensively as indices of data structures, such as lists and dictionaries, throughout our framework. For example, in , we use flow IDs as indices to look up our lists (or dictionaries, if strings are used for flow IDs) of deficit counters and quantum values:\n\nMost often, the mapping between flow IDs and per-flow parameters, such as weights in a Weighted Fair Queueing scheduler or priorities in a Static Priority scheduler, need to be stored in a dictionary, and then used to initialized these schedulers. An optional (but not recommended) style is to assign consecutive integers as flow IDs to the flows throughout the entire network, and then use simple lists of per-flow parameters to initialize the schedulers. In this case, flow IDs will be directly used as indices to look up these lists to find the parameter values."
    },
    {
        "link": "https://stackoverflow.com/questions/15236840/for-a-simulation-system-which-data-structure-is-most-suitable",
        "document": "I am in the planning phase of building a simulation and need ideas on how to represent data, based on memory and speed considerations.\n\nAt each time-step, the simulation process creates 10^3 to 10^4 new data records, and looks at each new or existing records (there are 10^6 to 10^8 of them) then either deletes it or modifies it.\n\nEach record has 3-10 simple fields, each either an integer or a string of several ASCII characters. In addition, each record has 1-5 other fields, each a variable-length list containing integers. A typical record weighs 100-500 bytes.\n\nThe modify-or-delete process works like this: For this record, compute a function whose arguments are the values of some of this record's fields, and the values of these fields of another record. Depending on the results, the process prepares to delete or modify its fields in some way.\n\nThen repeat for each other record. Then move to the next record and repeat. When all records have been processed, the simulation is ready to move to the next time-step.\n\nJust before moving to the next time-step, apply all the deletions and modifications as prepared.\n\nThe more records allowed, the better the simulation. If all records are in RAM, downside is simulation size and presumably upside is speed. The simulation doesn't need to be realtime, but obviously I don't want it too slow.\n\nTo represent each record in memory, I know of these options: a list or dict (with some lists nested in it), or a class instance. To store away all the records and continue the simulation another day, the options in order of decreasing familiarity to me are: a csv file where each line is a record, or just put all records in RAM then put them into a file (perhaps using pickle), or use some sort of database.\n\nI've learned Python basics plus some concepts like generators but haven't learned database, haven't tried pickling, and obviously need to learn more. If possible, I'd avoid multiple computers because I have only 1, and concurrency because it looks too scary.\n\nWhat would you advise about how to represent records in memory, and about how to store away the simulated system?"
    },
    {
        "link": "https://stackoverflow.com/questions/65449773/is-there-a-way-to-dynamically-change-variables-depending-on-length-of-list",
        "document": "May seem like a silly question is there any way to create variables depending on the number of objects in a list. The context to this is I am trying to create an n body simulation. One in which the user can manually chose the planets they want in the solar system and then run the script with only those planets. So prior to them running the chosing the planets and running the script I will not know how many variables to create. This problem of how many variables to create is show by: The mass is created by a class objects: class Objects():\n\nThe function is solved by using scipy.integrate.solve_ivp by:\n\nWhere the function is:\n\nMy main question centres around this function. When the user defines what planets they want to use I have no idea what the number of planets will be. I know the range which is might be but that’s all. Is there a feasible way to do this or is this a lost cause?\n\nI hope this clarifys the question with some additional code. Sorry about the previous lack of code its, I didn’t realise it was valuable at first and didn’t want to burden with too much code."
    },
    {
        "link": "https://adimahamuni.medium.com/python-best-practices-good-coding-practices-and-style-guidelines-for-python-c38fe08c33e0",
        "document": "It is known for its simplicity, readability, and ease of use. However, to write high-quality and efficient Python code, it is essential to follow some best practices. In this article, we will discuss some of the best practices for writing Python code.\n\nGuide PEP 8 is the official style guide for Python. It provides guidelines for writing Python code that is consistent, readable, and easy to understand. Some of the key guidelines include:\n• Limit the length of a line to 79 characters.\n• Use lowercase letters for variable names and underscores to separate words.\n• Use meaningful names for variables and functions.\n• Use spaces around operators and after commas.\n\nBy following these guidelines, you can make your code more readable and easier to maintain.\n\nComments are an essential part of any codebase. They help to explain what the code does, why it does it, and how it works. When writing comments, make sure to be clear and concise. Avoid comments that simply repeat what the code…"
    },
    {
        "link": "https://datacamp.com/tutorial/data-structures-python",
        "document": "Master the basics of data analysis with Python in just four hours. This online course will introduce the Python interface and explore popular packages."
    },
    {
        "link": "https://realpython.com/python-data-structures",
        "document": "Data structures are the fundamental constructs around which you build your programs. Each data structure provides a particular way of organizing data so it can be accessed efficiently, depending on your use case. Python ships with an extensive set of data structures in its standard library.\n\nHowever, Python’s naming convention doesn’t provide the same level of clarity that you’ll find in other languages. In Java, a list isn’t just a —it’s either a or an . Not so in Python. Even experienced Python developers sometimes wonder whether the built-in type is implemented as a linked list or a dynamic array.\n• Which common abstract data types are built into the Python standard library\n• How the most common abstract data types map to Python’s naming scheme\n• How to put abstract data types to practical use in various algorithms\n\nIn Python, dictionaries (or dicts for short) are a central data structure. Dicts store an arbitrary number of objects, each identified by a unique dictionary key. Dictionaries are also often called maps, hashmaps, lookup tables, or associative arrays. They allow for the efficient lookup, insertion, and deletion of any object associated with a given key. Phone books make a decent real-world analog for dictionary objects. They allow you to quickly retrieve the information (phone number) associated with a given key (a person’s name). Instead of having to read a phone book front to back to find someone’s number, you can jump more or less directly to a name and look up the associated information. This analogy breaks down somewhat when it comes to how the information is organized to allow for fast lookups. But the fundamental performance characteristics hold. Dictionaries allow you to quickly find the information associated with a given key. Dictionaries are one of the most important and frequently used data structures in computer science. So, how does Python handle dictionaries? Let’s take a tour of the dictionary implementations available in core Python and the Python standard library. Because dictionaries are so important, Python features a robust dictionary implementation that’s built directly into the core language: the data type. Python also provides some useful syntactic sugar for working with dictionaries in your programs. For example, the curly-brace ({ }) dictionary expression syntax and dictionary comprehensions allow you to conveniently define new dictionary objects: There are some restrictions on which objects can be used as valid keys. Python’s dictionaries are indexed by keys that can be of any hashable type. A hashable object has a hash value that never changes during its lifetime (see ), and it can be compared to other objects (see ). Hashable objects that compare as equal must have the same hash value. Immutable types like strings and numbers are hashable and work well as dictionary keys. You can also use objects as dictionary keys as long as they contain only hashable types themselves. For most use cases, Python’s built-in dictionary implementation will do everything you need. Dictionaries are highly optimized and underlie many parts of the language. For example, class attributes and variables in a stack frame are both stored internally in dictionaries. Python dictionaries are based on a well-tested and finely tuned hash table implementation that provides the performance characteristics you’d expect: O(1) time complexity for lookup, insert, update, and delete operations in the average case. There’s little reason not to use the standard implementation included with Python. However, specialized third-party dictionary implementations exist, such as skip lists or B-tree–based dictionaries. Besides plain objects, Python’s standard library also includes a number of specialized dictionary implementations. These specialized dictionaries are all based on the built-in dictionary class (and share its performance characteristics) but also include some additional convenience features. Let’s take a look at them. Python includes a specialized subclass that remembers the insertion order of keys added to it: . Note: is not a built-in part of the core language and must be imported from the module in the standard library. While standard instances preserve the insertion order of keys in CPython 3.6 and above, this was simply a side effect of the CPython implementation and was not defined in the language spec until Python 3.7. So, if key order is important for your algorithm to work, then it’s best to communicate this clearly by explicitly using the class: odict_keys(['one', 'two', 'three', 'four']) Until Python 3.8, you couldn’t iterate over dictionary items in reverse order using . Only instances offered that functionality. Even in Python 3.8, and objects aren’t exactly the same. instances have a method that is unavailable on plain instance, as well as a more customizable method than the one plain instances. The class is another dictionary subclass that accepts a callable in its constructor whose return value will be used if a requested key cannot be found. This can save you some typing and make your intentions clearer as compared to using or catching a exception in regular dictionaries: # initializes it using the default factory, # i.e. list() in this example: The data structure groups multiple dictionaries into a single mapping. Lookups search the underlying mappings one by one until a key is found. Insertions, updates, and deletions only affect the first mapping added to the chain: ChainMap({'one': 1, 'two': 2}, {'three': 3, 'four': 4}) # ChainMap searches each collection in the chain # from left to right until it finds the key (or fails): File , line , in : is a wrapper around a standard dictionary that provides a read-only view into the wrapped dictionary’s data. This class was added in Python 3.3 and can be used to create immutable proxy versions of dictionaries. can be helpful if, for example, you’d like to return a dictionary carrying internal state from a class or module while discouraging write access to this object. Using allows you to put these restrictions in place without first having to create a full copy of the dictionary: File , line , in : # Updates to the original are reflected in the proxy: All the Python dictionary implementations listed in this tutorial are valid implementations that are built into the Python standard library. If you’re looking for a general recommendation on which mapping type to use in your programs, I’d point you to the built-in data type. It’s a versatile and optimized hash table implementation that’s built directly into the core language. I would recommend that you use one of the other data types listed here only if you have special requirements that go beyond what’s provided by . All the implementations are valid options, but your code will be clearer and easier to maintain if it relies on standard Python dictionaries most of the time.\n\nAn array is a fundamental data structure available in most programming languages, and it has a wide range of uses across different algorithms. In this section, you’ll take a look at array implementations in Python that use only core language features or functionality that’s included in the Python standard library. You’ll see the strengths and weaknesses of each approach so you can decide which implementation is right for your use case. But before we jump in, let’s cover some of the basics first. How do arrays work, and what are they used for? Arrays consist of fixed-size data records that allow each element to be efficiently located based on its index: Because arrays store information in adjoining blocks of memory, they’re considered contiguous data structures (as opposed to linked data structures like linked lists, for example). A real-world analogy for an array data structure is a parking lot. You can look at the parking lot as a whole and treat it as a single object, but inside the lot there are parking spots indexed by a unique number. Parking spots are containers for vehicles—each parking spot can either be empty or have a car, a motorbike, or some other vehicle parked on it. But not all parking lots are the same. Some parking lots may be restricted to only one type of vehicle. For example, a motor home parking lot wouldn’t allow bikes to be parked on it. A restricted parking lot corresponds to a typed array data structure that allows only elements that have the same data type stored in them. Performance-wise, it’s very fast to look up an element contained in an array given the element’s index. A proper array implementation guarantees a constant O(1) access time for this case. Python includes several array-like data structures in its standard library that each have slightly different characteristics. Let’s take a look. Lists are a part of the core Python language. Despite their name, Python’s lists are implemented as dynamic arrays behind the scenes. This means a list allows elements to be added or removed, and the list will automatically adjust the backing store that holds these elements by allocating or releasing memory. Python lists can hold arbitrary elements—everything is an object in Python, including functions. Therefore, you can mix and match different kinds of data types and store them all in a single list. This can be a powerful feature, but the downside is that supporting multiple data types at the same time means that data is generally less tightly packed. As a result, the whole structure takes up more space: ['one', 'hello', 'three'] Just like lists, tuples are part of the Python core language. Unlike lists, however, Python’s objects are immutable. This means elements can’t be added or removed dynamically—all elements in a tuple must be defined at creation time. Tuples are another data structure that can hold elements of arbitrary data types. Having this flexibility is powerful, but again, it also means that data is less tightly packed than it would be in a typed array: Python’s module provides space-efficient storage of basic C-style data types like bytes, 32-bit integers, floating-point numbers, and so on. Arrays created with the class are mutable and behave similarly to lists except for one important difference: they’re typed arrays constrained to a single data type. Because of this constraint, objects with many elements are more space efficient than lists and tuples. The elements stored in them are tightly packed, and this can be useful if you need to store many elements of the same type. Also, arrays support many of the same methods as regular lists, and you might be able to use them as a drop-in replacement without requiring other changes to your application code: File , line , in : must be real number, not str Python 3.x uses objects to store textual data as immutable sequences of Unicode characters. Practically speaking, that means a is an immutable array of characters. Oddly enough, it’s also a recursive data structure—each character in a string is itself a object of length 1. String objects are space efficient because they’re tightly packed and they specialize in a single data type. If you’re storing Unicode text, then you should use a string. Because strings are immutable in Python, modifying a string requires creating a modified copy. The closest equivalent to a mutable string is storing individual characters inside a list: File , line , in : File , line , in : # Strings can be unpacked into a list to objects are immutable sequences of single bytes, or integers in the range 0 ≤ x ≤ 255. Conceptually, objects are similar to objects, and you can also think of them as immutable arrays of bytes. Like strings, have their own literal syntax for creating objects and are space efficient. objects are immutable, but unlike strings, there’s a dedicated mutable byte array data type called that they can be unpacked into: # Bytes literals have their own syntax: File , line , in : bytes must be in range(0, 256) File , line , in : File , line , in : The type is a mutable sequence of integers in the range 0 ≤ x ≤ 255. The object is closely related to the object, with the main difference being that a can be modified freely—you can overwrite elements, remove existing elements, or add new ones. The object will grow and shrink accordingly. A can be converted back into immutable objects, but this involves copying the stored data in full—a slow operation taking O(n) time: # Bytearrays can grow and shrink in size: File , line , in : 'str' object cannot be interpreted as an integer File , line , in : byte must be in range(0, 256) # Bytearrays can be converted back into bytes objects: # (This will copy the data) There are a number of built-in data structures you can choose from when it comes to implementing arrays in Python. In this section, you’ve focused on core language features and data structures included in the standard library. If you’re willing to go beyond the Python standard library, then third-party packages like NumPy and pandas offer a wide range of fast array implementations for scientific computing and data science. If you want to restrict yourself to the array data structures included with Python, then here are a few guidelines:\n• If you need to store arbitrary objects, potentially with mixed data types, then use a or a , depending on whether or not you want an immutable data structure.\n• If you have numeric (integer or floating-point) data and tight packing and performance is important, then try out .\n• If you have textual data represented as Unicode characters, then use Python’s built-in . If you need a mutable string-like data structure, then use a of characters.\n• If you want to store a contiguous block of bytes, then use the immutable type or a if you need a mutable data structure. In most cases, I like to start out with a simple . I’ll only specialize later on if performance or storage space becomes an issue. Most of the time, using a general-purpose array data structure like gives you the fastest development speed and the most programming convenience. I’ve found that this is usually much more important in the beginning than trying to squeeze out every last drop of performance right from the start.\n\nCompared to arrays, record data structures provide a fixed number of fields. Each field can have a name and may also have a different type. In this section, you’ll see how to implement records, structs, and plain old data objects in Python using only built-in data types and classes from the standard library. Note: I’m using the definition of a record loosely here. For example, I’m also going to discuss types like Python’s built-in that may or may not be considered records in a strict sense because they don’t provide named fields. Python offers several data types that you can use to implement records, structs, and data transfer objects. In this section, you’ll get a quick look at each implementation and its unique characteristics. At the end, you’ll find a summary and a decision-making guide that will help you make your own picks. Note: This tutorial is adapted from the chapter “Common Data Structures in Python” in Python Tricks: The Book. If you enjoy what you’re reading, then be sure to check out the rest of the book. As mentioned previously, Python dictionaries store an arbitrary number of objects, each identified by a unique key. Dictionaries are also often called maps or associative arrays and allow for efficient lookup, insertion, and deletion of any object associated with a given key. Using dictionaries as a record data type or data object in Python is possible. Dictionaries are easy to create in Python as they have their own syntactic sugar built into the language in the form of dictionary literals. The dictionary syntax is concise and quite convenient to type. Data objects created using dictionaries are mutable, and there’s little protection against misspelled field names as fields can be added and removed freely at any time. Both of these properties can introduce surprising bugs, and there’s always a trade-off to be made between convenience and error resilience: Python’s tuples are a straightforward data structure for grouping arbitrary objects. Tuples are immutable—they can’t be modified once they’ve been created. Performance-wise, tuples take up slightly less memory than lists in CPython, and they’re also faster to construct. As you can see in the bytecode disassembly below, constructing a tuple constant takes a single opcode, while constructing a list object with the same contents requires several more operations: However, you shouldn’t place too much emphasis on these differences. In practice, the performance difference will often be negligible, and trying to squeeze extra performance out of a program by switching from lists to tuples will likely be the wrong approach. A potential downside of plain tuples is that the data you store in them can only be pulled out by accessing it through integer indexes. You can’t give names to individual properties stored in a tuple. This can impact code readability. Also, a tuple is always an ad-hoc structure: it’s difficult to ensure that two tuples have the same number of fields and the same properties stored in them. This makes it easy to introduce slip-of-the-mind bugs, such as mixing up the field order. Therefore, I would recommend that you keep the number of fields stored in a tuple as low as possible: File , line , in : # No protection against missing or extra fields Classes allow you to define reusable blueprints for data objects to ensure each object provides the same set of fields. Using regular Python classes as record data types is feasible, but it also takes manual work to get the convenience features of other implementations. For example, adding new fields to the constructor is verbose and takes time. Also, the default string representation for objects instantiated from custom classes isn’t very helpful. To fix that, you may have to add your own method, which again is usually quite verbose and must be updated each time you add a new field. Fields stored on classes are mutable, and new fields can be added freely, which you may or may not like. It’s possible to provide more access control and to create read-only fields using the decorator, but once again, this requires writing more glue code. Writing a custom class is a great option whenever you’d like to add business logic and behavior to your record objects using methods. However, this means that these objects are technically no longer plain data objects: # String representation is not very useful Data classes are available in Python 3.7 and above. They provide an excellent alternative to defining your own data storage classes from scratch. By writing a data class instead of a plain Python class, your object instances get a few useful features out of the box that will save you some typing and manual implementation work:\n• The syntax for defining instance variables is shorter, since you don’t need to implement the method.\n• Instances of your data class automatically get nice-looking string representation via an auto-generated method.\n• Instance variables accept type annotations, making your data class self-documenting to a degree. Keep in mind that type annotations are just hints that are not enforced without a separate type-checking tool. Data classes are typically created using the decorator, as you’ll see in the code example below: # Type annotations are not enforced without To learn more about Python data classes, check out the The Ultimate Guide to Data Classes in Python 3.7. The class available in Python 2.6+ provides an extension of the built-in data type. Similar to defining a custom class, using allows you to define reusable blueprints for your records that ensure the correct field names are used. objects are immutable, just like regular tuples. This means you can’t add new fields or modify existing fields after the instance is created. Besides that, objects are, well . . . named tuples. Each object stored in them can be accessed through a unique identifier. This frees you from having to remember integer indexes or resort to workarounds like defining integer constants as mnemonics for your indexes. objects are implemented as regular Python classes internally. When it comes to memory usage, they’re also better than regular classes and just as memory efficient as regular tuples: objects can be an easy way to clean up your code and make it more readable by enforcing a better structure for your data. I find that going from ad-hoc data types like dictionaries with a fixed format to objects helps me to express the intent of my code more clearly. Often when I apply this refactoring, I magically come up with a better solution for the problem I’m facing. Using objects over regular (unstructured) tuples and dicts can also make your coworkers’ lives easier by making the data that’s being passed around self-documenting, at least to a degree: Added in Python 3.6, is the younger sibling of the class in the module. It’s very similar to , with the main difference being an updated syntax for defining new record types and added support for type hints. Please note that type annotations are not enforced without a separate type-checking tool like mypy. But even without tool support, they can provide useful hints for other programmers (or be terribly confusing if the type hints become out of date): File , line , in : File , line , in : # Type annotations are not enforced without The class converts between Python values and C structs serialized into Python objects. For example, it can be used to handle binary data stored in files or coming in from network connections. Structs are defined using a mini language based on format strings that allows you to define the arrangement of various C data types like , , and as well as their variants. Serialized structs are seldom used to represent data objects meant to be handled purely inside Python code. They’re intended primarily as a data exchange format rather than as a way of holding data in memory that’s only used by Python code. In some cases, packing primitive data into structs may use less memory than keeping it in other data types. However, in most cases that would be quite an advanced (and probably unnecessary) optimization: # All you get is a blob of data: # Data blobs can be unpacked again: Here’s one more slightly obscure choice for implementing data objects in Python: . This class was added in Python 3.3 and provides attribute access to its namespace. This means instances expose all of their keys as class attributes. You can use dotted attribute access instead of the square-bracket indexing syntax that’s used by regular dicts. All instances also include a meaningful by default. As its name proclaims, is simple! It’s basically a dictionary that allows attribute access and prints nicely. Attributes can be added, modified, and deleted freely: As you’ve seen, there’s quite a number of different options for implementing records or data objects. Which type should you use for data objects in Python? Generally your decision will depend on your use case:\n• If you have only a few fields, then using a plain tuple object may be okay if the field order is easy to remember or field names are superfluous. For example, think of an point in three-dimensional space.\n• If you need immutable fields, then plain tuples, , and are all good options.\n• If you need to lock down field names to avoid typos, then and are your friends.\n• If you want to keep things simple, then a plain dictionary object might be a good choice due to the convenient syntax that closely resembles JSON.\n• If you need full control over your data structure, then it’s time to write a custom class with setters and getters.\n• If you need to add behavior (methods) to the object, then you should write a custom class, either from scratch, or using the decorator, or by extending or .\n• If you need to pack data tightly to serialize it to disk or to send it over the network, then it’s time to read up on because this is a great use case for it! If you’re looking for a safe default choice, then my general recommendation for implementing a plain record, struct, or data object in Python would be to use in Python 2.x and its younger sibling, in Python 3.\n\nIn this section, you’ll see how to implement mutable and immutable set and multiset (bag) data structures in Python using built-in data types and classes from the standard library. A set is an unordered collection of objects that doesn’t allow duplicate elements. Typically, sets are used to quickly test a value for membership in the set, to insert or delete new values from a set, and to compute the union or intersection of two sets. In a proper set implementation, membership tests are expected to run in fast O(1) time. Union, intersection, difference, and subset operations should take O(n) time on average. The set implementations included in Python’s standard library follow these performance characteristics. Just like dictionaries, sets get special treatment in Python and have some syntactic sugar that makes them easy to create. For example, the curly-brace set expression syntax and set comprehensions allow you to conveniently define new set instances: But be careful: To create an empty set you’ll need to call the constructor. Using empty curly-braces ( ) is ambiguous and will create an empty dictionary instead. Python and its standard library provide several set implementations. Let’s have a look at them. The type is the built-in set implementation in Python. It’s mutable and allows for the dynamic insertion and deletion of elements. Python’s sets are backed by the data type and share the same performance characteristics. Any hashable object can be stored in a set: The class implements an immutable version of that can’t be changed after it’s been constructed. objects are static and allow only query operations on their elements, not inserts or deletions. Because objects are static and hashable, they can be used as dictionary keys or as elements of another set, something that isn’t possible with regular (mutable) objects: File , line , in : # Frozensets are hashable and can # be used as dictionary keys: The class in the Python standard library implements a multiset, or bag, type that allows elements in the set to have more than one occurrence. This is useful if you need to keep track of not only if an element is part of a set, but also how many times it’s included in the set: One caveat for the class is that you’ll want to be careful when counting the number of elements in a object. Calling returns the number of unique elements in the multiset, whereas the total number of elements can be retrieved using : Sets are another useful and commonly used data structure included with Python and its standard library. Here are a few guidelines for deciding which one to use:\n• If you need a mutable set, then use the built-in type.\n• If you need hashable objects that can be used as dictionary or set keys, then use a .\n• If you need a multiset, or bag, data structure, then use .\n\nA stack is a collection of objects that supports fast Last-In/First-Out (LIFO) semantics for inserts and deletes. Unlike lists or arrays, stacks typically don’t allow for random access to the objects they contain. The insert and delete operations are also often called push and pop. A useful real-world analogy for a stack data structure is a stack of plates. New plates are added to the top of the stack, and because the plates are precious and heavy, only the topmost plate can be moved. In other words, the last plate on the stack must be the first one removed (LIFO). To reach the plates that are lower down in the stack, the topmost plates must be removed one by one. Performance-wise, a proper stack implementation is expected to take O(1) time for insert and delete operations. Stacks have a wide range of uses in algorithms. For example, they’re used in language parsing as well as runtime memory management, which relies on a call stack. A short and beautiful algorithm using a stack is depth-first search (DFS) on a tree or graph data structure. Python ships with several stack implementations that each have slightly different characteristics. Let’s take a look at them and compare their characteristics. Python’s built-in type makes a decent stack data structure as it supports push and pop operations in amortized O(1) time. Python’s lists are implemented as dynamic arrays internally, which means they occasionally need to resize the storage space for elements stored in them when elements are added or removed. The list over-allocates its backing storage so that not every push or pop requires resizing. As a result, you get an amortized O(1) time complexity for these operations. The downside is that this makes their performance less consistent than the stable O(1) inserts and deletes provided by a linked list–based implementation (as you’ll see below with ). On the other hand, lists do provide fast O(1) time random access to elements on the stack, and this can be an added benefit. There’s an important performance caveat that you should be aware of when using lists as stacks: To get the amortized O(1) performance for inserts and deletes, new items must be added to the end of the list with the method and removed again from the end using . For optimum performance, stacks based on Python lists should grow towards higher indexes and shrink towards lower ones. Adding and removing from the front is much slower and takes O(n) time, as the existing elements must be shifted around to make room for the new element. This is a performance antipattern that you should avoid as much as possible: The class implements a double-ended queue that supports adding and removing elements from either end in O(1) time (non-amortized). Because deques support adding and removing elements from either end equally well, they can serve both as queues and as stacks. Python’s objects are implemented as doubly-linked lists, which gives them excellent and consistent performance for inserting and deleting elements but poor O(n) performance for randomly accessing elements in the middle of a stack. Overall, is a great choice if you’re looking for a stack data structure in Python’s standard library that has the performance characteristics of a linked-list implementation: The stack implementation in the Python standard library is synchronized and provides locking semantics to support multiple concurrent producers and consumers. Besides , the module contains several other classes that implement multi-producer, multi-consumer queues that are useful for parallel computing. Depending on your use case, the locking semantics might be helpful, or they might just incur unneeded overhead. In this case, you’d be better off using a or a as a general-purpose stack: As you’ve seen, Python ships with several implementations for a stack data structure. All of them have slightly different characteristics as well as performance and usage trade-offs. If you’re not looking for parallel processing support (or if you don’t want to handle locking and unlocking manually), then your choice comes down to the built-in type or . The difference lies in the data structure used behind the scenes and overall ease of use. is backed by a dynamic array, which makes it great for fast random access but requires occasional resizing when elements are added or removed. The list over-allocates its backing storage so that not every push or pop requires resizing, and you get an amortized O(1) time complexity for these operations. But you do need to be careful to only insert and remove items using and . Otherwise, performance slows down to O(n). is backed by a doubly-linked list, which optimizes appends and deletes at both ends and provides consistent O(1) performance for these operations. Not only is its performance more stable, the class is also easier to use because you don’t have to worry about adding or removing items from the wrong end. In summary, is an excellent choice for implementing a stack (LIFO queue) in Python.\n\nIn this section, you’ll see how to implement a First-In/First-Out (FIFO) queue data structure using only built-in data types and classes from the Python standard library. A queue is a collection of objects that supports fast FIFO semantics for inserts and deletes. The insert and delete operations are sometimes called enqueue and dequeue. Unlike lists or arrays, queues typically don’t allow for random access to the objects they contain. Imagine a line of Pythonistas waiting to pick up their conference badges on day one of PyCon registration. As new people enter the conference venue and queue up to receive their badges, they join the line (enqueue) at the back of the queue. Developers receive their badges and conference swag bags and then exit the line (dequeue) at the front of the queue. Another way to memorize the characteristics of a queue data structure is to think of it as a pipe. You add ping-pong balls to one end, and they travel to the other end, where you remove them. While the balls are in the queue (a solid metal pipe) you can’t get at them. The only way to interact with the balls in the queue is to add new ones at the back of the pipe (enqueue) or to remove them at the front (dequeue). Queues are similar to stacks. The difference between them lies in how items are removed. With a queue, you remove the item least recently added (FIFO) but with a stack, you remove the item most recently added (LIFO). Performance-wise, a proper queue implementation is expected to take O(1) time for insert and delete operations. These are the two main operations performed on a queue, and in a correct implementation, they should be fast. Queues have a wide range of applications in algorithms and often help solve scheduling and parallel programming problems. A short and beautiful algorithm using a queue is breadth-first search (BFS) on a tree or graph data structure. Scheduling algorithms often use priority queues internally. These are specialized queues. Instead of retrieving the next element by insertion time, a priority queue retrieves the highest-priority element. The priority of individual elements is decided by the queue based on the ordering applied to their keys. A regular queue, however, won’t reorder the items it carries. Just like in the pipe example, you get out what you put in, and in exactly that order. Python ships with several queue implementations that each have slightly different characteristics. Let’s review them. It’s possible to use a regular as a queue, but this is not ideal from a performance perspective. Lists are quite slow for this purpose because inserting or deleting an element at the beginning requires shifting all the other elements by one, requiring O(n) time. Therefore, I would not recommend using a as a makeshift queue in Python unless you’re dealing with only a small number of elements: The class implements a double-ended queue that supports adding and removing elements from either end in O(1) time (non-amortized). Because deques support adding and removing elements from either end equally well, they can serve both as queues and as stacks. Python’s objects are implemented as doubly-linked lists. This gives them excellent and consistent performance for inserting and deleting elements, but poor O(n) performance for randomly accessing elements in the middle of the stack. As a result, is a great default choice if you’re looking for a queue data structure in Python’s standard library: The implementation in the Python standard library is synchronized and provides locking semantics to support multiple concurrent producers and consumers. The module contains several other classes implementing multi-producer, multi-consumer queues that are useful for parallel computing. Depending on your use case, the locking semantics might be helpful or just incur unneeded overhead. In this case, you’d be better off using as a general-purpose queue: is a shared job queue implementation that allows queued items to be processed in parallel by multiple concurrent workers. Process-based parallelization is popular in CPython due to the global interpreter lock (GIL) that prevents some forms of parallel execution on a single interpreter process. As a specialized queue implementation meant for sharing data between processes, makes it easy to distribute work across multiple processes in order to work around the GIL limitations. This type of queue can store and transfer any pickleable object across process boundaries: Python includes several queue implementations as part of the core language and its standard library. objects can be used as queues, but this is generally not recommended due to slow performance. If you’re not looking for parallel processing support, then the implementation offered by is an excellent default choice for implementing a FIFO queue data structure in Python. It provides the performance characteristics you’d expect from a good queue implementation and can also be used as a stack (LIFO queue).\n\nA priority queue is a container data structure that manages a set of records with totally-ordered keys to provide quick access to the record with the smallest or largest key in the set. You can think of a priority queue as a modified queue. Instead of retrieving the next element by insertion time, it retrieves the highest-priority element. The priority of individual elements is decided by the order applied to their keys. Priority queues are commonly used for dealing with scheduling problems. For example, you might use them to give precedence to tasks with higher urgency. Think about the job of an operating system task scheduler: Ideally, higher-priority tasks on the system (such as playing a real-time game) should take precedence over lower-priority tasks (such as downloading updates in the background). By organizing pending tasks in a priority queue that uses task urgency as the key, the task scheduler can quickly select the highest-priority tasks and allow them to run first. In this section, you’ll see a few options for how you can implement priority queues in Python using built-in data structures or data structures included in Python’s standard library. Each implementation will have its own upsides and downsides, but in my mind there’s a clear winner for most common scenarios. Let’s find out which one it is. You can use a sorted to quickly identify and delete the smallest or largest element. The downside is that inserting new elements into a list is a slow O(n) operation. While the insertion point can be found in O(log n) time using in the standard library, this is always dominated by the slow insertion step. Maintaining the order by appending to the list and re-sorting also takes at least O(n log n) time. Another downside is that you must manually take care of re-sorting the list when new elements are inserted. It’s easy to introduce bugs by missing this step, and the burden is always on you, the developer. This means sorted lists are only suitable as priority queues when there will be few insertions: # Remember to re-sort every time a new element is inserted, is a binary heap implementation usually backed by a plain , and it supports insertion and extraction of the smallest element in O(log n) time. This module is a good choice for implementing priority queues in Python. Since technically provides only a min-heap implementation, extra steps must be taken to ensure sort stability and other features typically expected from a practical priority queue: uses internally and shares the same time and space complexities. The difference is that is synchronized and provides locking semantics to support multiple concurrent producers and consumers. Depending on your use case, this might be helpful, or it might just slow your program down slightly. In any case, you might prefer the class-based interface provided by over the function-based interface provided by : Python includes several priority queue implementations ready for you to use. stands out from the pack with a nice object-oriented interface and a name that clearly states its intent. It should be your preferred choice. If you’d like to avoid the locking overhead of , then using the module directly is also a good option."
    }
]