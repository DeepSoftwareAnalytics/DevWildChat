[
    {
        "link": "https://docs.opencv.org/4.x/d2/d58/tutorial_table_of_content_dnn.html",
        "document": "\n• How to enable Halide backend for improve efficiency\n• How to schedule your network for Halide backend\n• How to run deep networks in browser\n\nIn this section you will find the guides, which describe how to run classification, segmentation and detection PyTorch DNN models with OpenCV.\n• Conversion of PyTorch Classification Models and Launch with OpenCV Python\n• Conversion of PyTorch Classification Models and Launch with OpenCV C++\n• Conversion of PyTorch Segmentation Models and Launch with OpenCV\n\nIn this section you will find the guides, which describe how to run classification, segmentation and detection TensorFlow DNN models with OpenCV.\n• Conversion of TensorFlow Classification Models and Launch with OpenCV Python\n• Conversion of TensorFlow Detection Models and Launch with OpenCV Python\n• Conversion of TensorFlow Segmentation Models and Launch with OpenCV"
    },
    {
        "link": "https://docs.opencv.org/4.x/d6/d0f/group__dnn.html",
        "document": "\n• API for new layers creation, layers are building bricks of neural networks;\n• API to construct and modify comprehensive neural networks from layers;\n• functionality for loading serialized networks models from different frameworks.\n\nFunctionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.\n\nCreates 4-dimensional blob from image. Optionally resizes and crops from center, subtract values, scales values by , swap Blue and Red channels. scalar with mean values which are subtracted from channels. Values are intended to be in (mean-R, mean-G, mean-B) order if has BGR ordering and is true. flag which indicates that swap first and last channels in 3-channel image is necessary. flag which indicates whether image will be cropped after resize or not if is true, input image is resized so one side after resize is equal to corresponding dimension in and another one is equal or larger. Then, crop from the center is performed. If is false, direct resize without cropping and preserving aspect ratio is performed. The order and usage of and are (input - mean) * scalefactor.\n\nCreates 4-dimensional blob from series of images. Optionally resizes and crops from center, subtract values, scales values by , swap Blue and Red channels. input images (all with 1-, 3- or 4-channels). scalar with mean values which are subtracted from channels. Values are intended to be in (mean-R, mean-G, mean-B) order if has BGR ordering and is true. flag which indicates that swap first and last channels in 3-channel image is necessary. flag which indicates whether image will be cropped after resize or not if is true, input image is resized so one side after resize is equal to corresponding dimension in and another one is equal or larger. Then, crop from the center is performed. If is false, direct resize without cropping and preserving aspect ratio is performed. The order and usage of and are (input - mean) * scalefactor.\n\npath to the file, dumped from Torch by using torch.save() function. specifies whether the network was serialized in ascii mode or binary. specifies testing phase of network. If true, it's similar to evaluate() method in Torch. Ascii mode of Torch serializer is more preferable, because binary mode extensively use type of C language, which has various bit-length on different systems. The loading file must contain serialized nn.Module object with importing network. Try to eliminate a custom objects from serialazing data to avoid importing errors. Also some equivalents of these classes from cunn, cudnn, and fbcunn may be successfully imported."
    },
    {
        "link": "https://learnopencv.com/deep-learning-with-opencvs-dnn-module-a-definitive-guide",
        "document": "The field of computer vision has existed since the late 1960s. Image classification and object detection are some of the oldest problems in computer vision that researchers have tried to solve for many decades. Using neural networks and deep learning, we have reached a stage where computers can start to understand and recognize an object with high accuracy, even surpassing humans in many cases. And to learn about neural networks and deep learning with computer vision, the OpenCV DNN module is a great place to start. With its highly optimized CPU performance, beginners can get started easily even if they do not have a very powerful GPU enabled system.\n\nIn this regard, this blog post will serve as the best starting point.\n\nNot only the theory part but also we cover hands-on experience with OpenCV DNN. We will discuss classification and object detection in images and real-time videos in detail.\n• What is an OpenCV DNN Module?\n• What different models does it support?\n• What are the frameworks that it supports?\n• A complete step by step guide to image classification using OpenCV DNN.\n\nSo, let’s jump into the blog post now and get started with deep learning in computer vision with the OpenCV DNN module.\n\nWe all know OpenCV as one of the best computer vision libraries. Additionally, it also has functionalities for running deep learning inference as well. The best part is supporting the loading of different models from different frameworks, using which we can carry out several deep learning functionalities. The feature of supporting models from different frameworks has been a part of OpenCV since version 3.3. Still, many newcomers to the field are unaware of this great feature of OpenCV. Therefore, they tend to miss many fun and good learning opportunities.\n\nThe OpenCV DNN module only supports deep learning inference on images and videos. It does not support fine-tuning and training. Still, the OpenCV DNN module can be a perfect starting point for any beginner to get into deep learning based computer vision and play around.\n\nOne of the OpenCV DNN module’s best things is that it is highly optimized for Intel processors. We can get good FPS when running inference on real-time videos for object detection and image segmentation applications. We often get higher FPS with the DNN module when using a model pre-trained using a specific framework. For example, let us look at image classification inference speed for different frameworks.\n\nThe above results are inference timing for the DenseNet121 model. Surprisingly, OpenCV is much faster than TensorFlow’s original implementations while falling behind PyTorch by a small margin. In fact, TensorFlow’s inference time is close to 1 second, whereas OpenCV takes less than 200 milliseconds.\n\nThe above benchmarks are done using the latest versions at the time of this writing. They are PyTorch 1.8.0, OpenCV 4.5.1, and TensorFlow 2.4. All tests are done on Google Colab which has Intel Xeon processors 2.3Ghz processors.\n\nThe same is true even in the case of object detection.\n\nThe above plot shows the results for FPS on video with Tiny YOLOv4 on the original Darknet framework and OpenCV. The benchmark was done on an Intel i7 8th Gen laptop CPU with 2.6 GHz clock speed. We can see that in the same video, the OpenCV’s DNN module is running at 35 FPS whereas Darknet compiled with OpenMP and AVX is running at 15 FPS. And Darknet (without OpenMP or AVX) Tiny YOLOv4 is the slowest, running at only 3 FPS. This is a huge difference considering we are using the original Darknet Tiny YOLOv4 models in both cases.\n\nThe above graphs show the actual usefulness and power of the OpenCV DNN module when working with CPUs. Because of its fast inference time, even on CPUs, it can act as an excellent deployment tool on edge devices where computation power is limited. The edge devices based on ARM processors are some of the best examples. The following graph is good proof of that.\n\nThe above plots show the FPS for different frameworks and models running on the Raspberry Pi 3B. The results are very impressive. For the SqueezeNet and MobileNet models, OpenCV surpasses all the other frameworks in terms of FPS. For GoogLeNet, OpenCV comes second, with TensorFlow being the fastest. For Network in Network, OpenCV Raspberry FPS is the slowest.\n\nThe above few graphs show optimized OpenCV, and how fast it is for neural network inference. The data serves as a perfect reason to choose to learn about the OpenCV DNN module in detail.\n\nWe have established that by using the OpenCV DNN module, we can carry out deep learning based computer vision inference on images and videos. Let us take a look at all the functionalities it supports. Interestingly, most of the deep learning and computer vision tasks that we can think of are supported. The following list will give us a pretty good idea of the features.\n\nThe list is extensive and provides a lot of practical deep learning use cases. Find out more about all these in detail by visiting the OpenCV repository’s Deep Learning in OpenCV Wiki page.\n\nThe impressive fact is that there are many models to choose from depending on the system’s hardware and computing capability (we will see them later). We can find a model for every use case, from really compute-intensive models for state-of-the-art results to models that can run on low-powered edge devices.\n\nObserve that it is impossible to go through all the above use cases in a single blog post. Hence, we will discuss Object Detection and human pose estimation in detail to give an idea of the working of select different models using OpenCV DNN.\n\nTo support all the applications we discussed above, we need a lot of pre-trained models. Moreover, there are many state-of-the-art models to choose from. The following table lists some models according to the different deep learning applications.\n\nThe models mentioned above are not exhaustive. There exist many more models. As noted earlier, completing listing or discussing each in detail in a single blog is almost impossible. The above list gives us a pretty good idea of how practical the DNN module can be in exploring deep learning in computer vision.\n\nLooking at all the above models, a question that comes to mind is, “are all these models supported by a single framework”? Actually, no.\n\n\n\nOpenCV DNN module supports many popular deep learning frameworks. The following are the deep learning frameworks that the OpenCV DNN module supports.\n\nWe need two things to use a pre-trained Caffe model with OpenCV DNN. One is the model.caffemodel file that contains the pre-trained weights. The other one is the model architecture file which has a .prototxt extension. It is like a plain text file with a JSON like structure containing all the neural network layers’ definitions. To get a clear idea of how this file looks, please visit this link.\n\nFor loading pre-trained TensorFlow models, we also need two files. The model weights file and a protobuf text file contain the model configuration. The weight file has a .pb extension which is a protobuf file containing all the pre-trained weights. If you have worked with TensorFlow before, you would know that the .pb file is the model checkpoint we get after saving the model and freezing the weights. The model configuration is held in the protobuf text file, which has a .pbtxt file extension.\n\n\n\nNote: In newer versions of TensorFlow the model weight file might not be in .pb format. This is also true if you are trying to use one of your own saved models which may be in .ckpt or .h5 format. In that case, there are some intermediate steps to be performed before the models can be used with the OpenCV DNN module. In such cases, converting the models to ONNX format and then to .pb format is the best possible way to ensure that everything will work as expected.\n\nFor loading Torch model files, we need the file containing the pre-trained weights. Generally, this file has a .t7 or .net extension. But with the latest PyTorch models having a .pth extension, first converting to ONNX is the best way to proceed. After converting to ONNX, you can load them directly as OpenCV DNN supports ONNX models.\n\nThe OpenCV DNN module supports the famous Darknet framework as well. One may recognize this if they have used official YOLO models with the Darknet framework.\n\nGenerally, to load the Darknet models, we need one model weights file having the .weights extension. The network configuration file will always be a .cfg file for a Darknet model.\n\nUsing Models that have been converted to ONNX format from different frameworks like Keras and PyTorch\n\nOften, models trained in frameworks like PyTorch or TensorFlow might not be ready for use directly with the OpenCV DNN module. In those cases, generally, we convert the models to the ONNX format (Open Neural Network Exchange), which can then be used as it is or even converted to formats supported by other frameworks like TensorFlow or PyTorch.\n\nTo load an ONNX model, we need the .onnx weight file for the OpenCV DNN module.\n\nPlease visit the official OpenCV documentation to learn about the different frameworks, their weight files, and the configuration files.\n\nMost probably, the above list covers all the famous deep learning frameworks. To get a complete idea of all the frameworks and models that the OpenCV DNN module supports, please visit the official Wiki page.\n\nAll the models we see here are tested to work perfectly with the OpenCV DNN module. In theory, any model from the above frameworks should work with the DNN module. We only need to find the correct weight file and the corresponding neural network architecture file. Things will clarify more when we start this tutorial’s coding part.\n\nWe have covered enough theory. Let us dive into the coding part of this tutorial. First, we will have a complete walkthrough of image classification using the OpenCV DNN module. Then we will carry out object detection using the DNN module.\n\nThis section will classify an image using the OpenCV DNN module. We will cover each step in detail to clear everything by the end of this section.\n\nWe will use a neural network model trained on the very famous ImageNet dataset using the Caffe framework. Specifically, we will use the DensNet121 deep neural network model for the classification task. The advantage being it is pre-trained on 1000 classes from the ImageNet dataset. We can expect that the model will already see whatever image we want to classify. This allows us to choose from an extensive range of images.\n\nWe will use the following image of a tiger for the image classification task.\n\nIn very brief, the following are the steps that we will follow while classifying an image.\n• Load the class names text file from the disk and extract the required labels.\n• Load the image from the disk and prepare the image to be in the correct input format for the deep learning model.\n• Forward propagate the input image through the model and obtain the outputs.\n\nNow let us see each step in detail, along with the code.\n\nImporting the Modules and Loading the Class Text Files\n\nWe will need to import the OpenCV and Numpy modules for the Python code. For C++, we need to include the OpenCV and OpenCV’s DNN library.\n\nRemember that we discussed that the DenseNet121 model we will use had been trained on the 1000 ImageNet classes. We will need some way to load these 1000 classes into memory and have easy access to them. Such classes are typically available in text files. One such file is called the classification_classes_ILSVRC2012.txt file containing all the class names in the following format.\n\nEach new line contains all the labels or class names specific to a single image. For example, the first line contains . These are two names that belong to the same kind of fish. Similarly, the second line has two names belonging to the goldfish. Typically, the first name is the most common name that almost everyone recognizes.\n\nLet us see how we can load such a text file and extract the first name from each line to use them as labels while classifying images.\n\nFirst, we open the text file containing all the class names in reading mode and split them using each new line. Now, we would have all the classes stored in the list in the following format.\n\nHowever, we need the first name from each line only. That is what the second line of code does. For each element in the image_net_names list, we split the elements using comma (,) as the delimiter and only keep the first of those elements. These names are saved in the list. Now, the list will look like the following.\n\nAs discussed earlier, we will use a pre-trained DenseNet121 model that has been trained using the Caffe deep learning framework.\n\n\n\nWe will need the model weight files ( ) and the model configuration file ( ).\n\nLet us see the code and then get to the explanation part of the loading of the model.\n\nYou can see, we are using a function called from the OpenCV DNN module, which accepts three input arguments.\n• : This is the path to the pre-trained weights file. In our case, it is the pre-trained Caffe model.\n• : This is the path to the model configuration file, and it is the Caffe model’s .prototxt file in this case.\n• : Finally, we need to provide the framework name that we are loading the models. For us, it is the Caffe framework.\n\nAlong with the function, the DNN module also provides functions to load models from specific frameworks, where we do not have to provide the argument. The following are those functions.\n• : This is used to load pre-trained Caffe models and accepts two arguments. They are the path to the prototxt file and the path to the Caffe model file.\n• (): We can use this function to directly load the TensorFlow pre-trained models. This also accepts two arguments. One is the path to the frozen model graph and the other is the path to the model architecture protobuf text file.\n• : We can use this to load Torch and PyTorch models which have been saved using the () function. We need to provide the model path as the argument.\n• (): This is used to load the models trained using the DarkNet framework. We need to provide two arguments here as well. One of the path to the model weights and the other is the path to the model configuration file.\n• (): We can use this to load ONNX models and we only need to provide the path to the ONNX model file.\n\nThis blog post will stick with the () function to load the pre-trained models. We will use the same function in the object detection section as well.\n\nRead the Image and Prepare it for Model Input\n\nWe will read the image from the disk, as usual, using OpenCV’s () function. Note there are a few other details that we need to take care of. The pre-trained models we load using the DNN module do not directly take the read image as input. We need to do some preprocessing before that.\n\nLet us first write the code, and then it will be much easier to get into the technical details.\n\nWhile reading the image, we assume that it is two directories previous to the current directory and inside the folder. The next few steps are essential. We have a () function which prepares the image in the correct format to be fed into the model. Let us go over all the arguments and learn about them in detail.\n• : This is the input image that we just read above using the imread() function.\n• : This value scales the image by the provided value. It has a default value of 1 which means that no scaling is performed.\n• : This is the size that the image will be resized to. We have provided the size as 224×224 as most classification models trained on the ImageNet dataset expect this size only.\n• : The mean argument is pretty important. These are actually the mean values that are subtracted from the image’s RGB color channels. This normalizes the input and makes the final input invariance to different illumination scales.\n\nThere is one other thing to note here. All the deep learning models expect input in batches. However, we only have one image here. Nevertheless, the blob output that we get here actually has a shape of . Observe that one extra batch dimension has been added by the () function. This would be the final and correct input format for the neural network model.\n\nForward Propagate the Input Through the Model\n\nNow, as our input is ready, we can make the predictions.\n\nThere are two steps for making predictions.\n• First, we have to set the input blob to our neural network model that we have loaded from the disk.\n• The second step is to use the forward() function for forward propagating the blob through the model, which gives us all the outputs.\n\nWe are carrying out both the steps in the above code block.\n\nThe , which is an array, holds all the predictions. But before we can see the outputs and class labels correctly, there are a few preprocessing steps that we need to complete.\n\nCurrently, has a shape of ( ) and it is difficult to extract the class labels as it is. So, the following block of code reshapes the , after which we can easily get the correct class labels and map the label ID to the class names.\n\nAfter we shape the , it has a shape of indicating that it has 1000 rows for all the 1000 labels. Each row holds the score corresponding to the class label, which looks like the following.\n\nFrom these, we are extracting the highest label index and storing it in . However, these scores are not actually probability scores. We need to get the softmax probabilities to know with what probability the model predicts the highest-scoring label.\n\nIn the Python code above, we are converting the scores to softmax probabilities using . Then we are multiplying the highest probability score with 100 to get the predicted score percentage.\n\nThe final steps would be annotating the class name and percentage on top of the image. Then we are visualizing the image and saving the result to the disk.\n\nAfter executing the code, we will get the following output.\n\nThe DenseNet121 model correctly predicts the image as that of a tiger, and that too with 91% confidence. The result is quite good.\n\nIn the above sections, we saw how to use the OpenCV DNN module for image classification using the DenseNet121 neural network model. We also went through each of the steps in detail to better understand the working of the DNN module.\n\nWe will use OpenCV DNN and object detection in images and videos in the following sections.\n\nUsing the OpenCV DNN module, we can easily get started with Object Detection in deep learning and computer vision. Like classification, we will load the images, the appropriate models and forward propagate the input through the model. The preprocessing steps for proper visualization in object detection is going to be a bit different. We will get to all of that as we progress through the rest of the blog post.\n\nLet us start with object detection in images.\n\nJust like classification, here also, we will leverage the pre-trained models. These models have been trained on the MS COCO dataset, the current benchmark dataset for deep learning based object detection models.\n\nMS COCO has almost 80 classes of objects, starting from a person to a car, to a toothbrush. The dataset contains 80 classes of everyday objects. We will also use a text file to load all the labels in the MS COCO dataset for object detection.\n\nFor object detection, we will use the following image.\n\nWe will use MobileNet SSD (Single Shot Detector), which has been trained on the MS COCO dataset using the TensorFlow deep learning framework. SSD models are generally faster when compared to other object detection models. Moreover, the MobileNet backbone also makes them less compute-intensive. So, it is a good model to start learning about object detection with OpenCV DNN.\n\nLet us start with the coding part.\n• In the Python code, first we are importing the cv2 and numpy modules.\n• For C++, we need to include the OpenCV and OpenCV DNN libraries.\n\nNext we read the file, which contains all the class names separated by a new line. We are storing each class name in list.\n\nThe class_names list will be similar to the following.\n\nAlong with that, we also have a COLORS array that holds tuples of three integer values. We can apply these random colors while drawing the bounding box for each class. The best part is that we will have a different colored bounding box for each class, and it will be easy for us to differentiate between the classes in the final result.\n\nLoad the MobileNet SSD Model and Prepare the Input\n\nWe will load the MobileNet SSD model using the readNet() function, which we used earlier also.\n\nIn the above code block:\n• The model argument accepts the frozen inference graph path as the input, a pre-trained model that contains the weights.\n• The argument accepts the path to the model configuration file that is a protobuf text file.\n• Finally, we specify the , which is TensorFlow in this case.\n\nNext, we will read the image from the disk and prepare the input blob file.\n\nFor object detection, we are using a bit different argument values in the () function.\n• We specify the to be 300×300 is the input size that SSD models generally expect in almost all frameworks. It is the same for TensorFlow as well.\n• We are also using the the argument this time. Generally, OpenCV reads the image in BGR format, and for object detection, the models expect the input to be in RGB format. So, the argument will swap the R and B channels of the image, making it RGB format.\n\nWe then set the blob to the MobileNet SSD model and forward propagating it using the () function.\n\nThe we have is structured as follows:\n• Here, index position 1 contains the class label, which can be from 1 to 80.\n• Index position 2 contains the confidence score. This is not a probability score but rather the model’s confidence for the object belonging to the class that it has detected.\n• Of the final four values, the first two are x, y bounding box coordinates, and the last is the bounding box’s width and height.\n\nLooping Over the Detections and Drawing the Bounding Boxes\n\nWe are all set to loop over the detections in , and draw the bounding boxes around each detected object. The following is the code for looping over the detections.\n• Inside the loop, first, we extract the confidence score of the current detected object. As discussed, we can get it from the index position 2.\n• Then we have the block to check whether the detected object’s confidence is above a certain threshold or not. We are only moving forward to draw the bounding boxes of the confidence is above 0.4.\n• We get the class ID and map it to the MS COCO class names. Then we get a single color for the current class to draw the bounding boxes and put the class label text on top of the bounding box.\n• We are then extracting the bounding box x and y coordinates and the bounding box’s width and height. Multiplying them with the image width and height, respectively, provides us with the correct values to draw the rectangles.\n• In the final few steps, we are drawing the bounding box rectangles, writing the class text on top and visualizing the resulting image.\n\nThis is all the code that we need for object detection in images using OpenCV DNN. Executing the code gives us the following result.\n\nIn the above image, we can see that the results seem good. The model is detecting almost all the objects that are visible. However, there are a few incorrect predictions too. For example, the MobileNet SSD model is detecting the bicycle as a motorcycle on the right side. MobileNet SSDs tend to make such mistakes as they are made for real-time applications and trade accuracy for speed.\n\nThis marks object detection in images using OpenCV DNN. We will do one final thing to improve the learning process in this blog post. That is object detection in videos.\n\nThe code for object detection in videos will be very similar to that of images. There will be a few changes as we make predictions on video frames instead of images.\n\nThe few lines of code are identical to object detection in images. Let us complete that part first.\n\nWe can see that most of the code is the same. We are loading the same MS COCO class file and MobileNet SSD model.\n\nHere, instead of an image, we are capturing a video using the () object. We are also creating a VideoWriter() object to properly save the resulting video frames.\n\nLooping Over the Video Frames and Detecting Objects in Each Frame\n\nAs of now, we have our video and the MobileNet SSD model ready. The next step is to loop over each video frame and carry out object detection in each frame. In this way, we will treat each frame just as an image.\n\nIn the above code block, the model detects objects in each frame until there are no frames to be looped over in the video. Some important things to note:\n• We are storing the start time before the detections in the variable and the end time after the detection ends.\n• The above time variables help us to calculate the FPS (Frames Per Seconds). We are calculating the FPS and storing it in .\n• In the final part of the code, we are also writing the calculated FPS on top of the current frame to get an idea of what kind of speed we can expect while running MobileNet SSD models using the OpenCV DNN module.\n• Finally, we are visualizing each frame on the screen and saving those to disk as well.\n\nExecuting the above code will give the following output.\n\nWe are getting around more than 40 FPS on most frames on an i7 12th Gen laptop CPU. That is not bad at all, considering the number of detections. The model can detect almost all persons, vehicles, and traffic lights. Still, it is suffering a bit when trying to detect small objects such as handbags and backpacks. The 40 FPS on the CPU is what we get in return for the trade-off in accuracy and fewer detections of smaller objects.\n\nWe can also run all the classification and detection inference on GPU. We will need to compile the OpenCV DNN module from the source with GPU.\n• If on Ubuntu, visit this post of LearnOpenCV.com to compile OpenCV with GPU.\n• If on Windows, visit this link to compile OpenCV with GPU.\n\nTo run the inference on GPU, we need to simply change the C++ and Python code.\n\nAfter loading the neural network model from the disk, we should add the above two lines of code. The first line of code ensures that the neural network will use the CUDA backend if the DNN module supports the CUDA GPU model.\n\nThe second line of code tells that all the neural network computations will happen on the GPU instead of the CPU. Using the CUDA enabled GPU, we should get higher FPS on the object detection video inference compared to the CPU. Even with images, the inference time should be much lower than in the case of CPU.\n\nWe introduced OpenCV’s DNN Module and discussed why we chose the DNN module. We have seen bar graphs comparing the performance. We also looked at the different Deep Learning functionalities, models and frameworks that OpenCV DNN Supports.\n\nWe discussed the Image Classification and Object Detection tasks using OpenCV’s DNN module for a hands-on experience. We also saw Object Detection in Videos using OpenCV DNN.\n• Neural Networks and Deep Learning have reached a stage where computers can accurately understand and recognize objects. On occasion, they even surpass humans in certain use cases.\n• OpenCV DNN module:\n• Is the preferred choice for model inference, especially on Intel CPUs.\n• It comes with off-shelf, ready-to-use models and algorithms that fit most use cases.\n• Although the DNN module does not have training capabilities but still has excellent deployment support for edge devices.\n\nI hope you enjoyed the blog and learned the basics of OpenCV’s DNN Module. Do share your experience in the comments section."
    },
    {
        "link": "https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV",
        "document": "Deep Learning is the most popular and the fastest growing area in Computer Vision nowadays. Since OpenCV 3.1 there is DNN module in the library that implements forward pass (inferencing) with deep networks, pre-trained using some popular deep learning frameworks, such as Caffe. In OpenCV 3.3 the module has been promoted from opencv_contrib repository to the main repository (https://github.com/opencv/opencv/tree/master/modules/dnn) and has been accelerated significantly.\n\nThe module has no any extra dependencies, except for libprotobuf, and libprotobuf is now included into OpenCV.\n• Models in ONNX format (as the main method to import models from PyTorch and Keras for some cases)\n\nYou also can write your own Custom layer.\n\nThe module includes some SSE, AVX, AVX2 and NEON acceleration of the performance-critical layers as well as support of CUDA for the most of the layers. There is also constantly-improved Halide backend. OpenCL (libdnn-based) backend is being developed and should be integrated after OpenCV 3.3 release. Here you may find the up-to-date benchmarking results: DNN Efficiency\n\nThe provided API (for C++ and Python) is very easy to use, just load the network and run it. Multiple inputs/outputs are supported. Here are the examples: https://github.com/opencv/opencv/tree/master/samples/dnn.\n\nThere is Habrahabr article describing the module: https://habrahabr.ru/company/intel/blog/333612/ (in Russian).\n\nThe following networks have been tested and known to work:"
    },
    {
        "link": "https://pyimagesearch.com/2017/08/21/deep-learning-with-opencv",
        "document": "Two weeks ago OpenCV 3.3 was officially released, bringing with it a highly improved deep learning ( ) module. This module now supports a number of deep learning frameworks, including Caffe, TensorFlow, and Torch/PyTorch.\n\nFurthermore, this API for using pre-trained deep learning models is compatible with both the C++ API and the Python bindings, making it dead simple to:\n• Pass the image through the network and obtain the output classifications.\n\nWhile we cannot train deep learning models using OpenCV (nor should we), this does allow us to take our models trained using dedicated deep learning libraries/tools and then efficiently use them directly inside our OpenCV scripts.\n\nIn the remainder of this blog post I’ll demonstrate the fundamentals of how to take a pre-trained deep learning network on the ImageNet dataset and apply it to input images.\n\nTo learn more about deep learning with OpenCV, just keep reading.\n\nIn the first part of this post, we’ll discuss the OpenCV 3.3 release and the overhauled module.\n\nWe’ll then write a Python script that will use OpenCV and GoogleLeNet (pre-trained on ImageNet) to classify images.\n\nFinally, we’ll explore the results of our classifications.\n\nThe dnn module of OpenCV has been part of the repository since version v3.1. Now in OpenCV 3.3 it is included in the main repository.\n\nWhy should you care?\n\nDeep Learning is a fast growing domain of Machine Learning and if you’re working in the field of computer vision/image processing already (or getting up to speed), it’s a crucial area to explore.\n\nWith OpenCV 3.3, we can utilize pre-trained networks with popular deep learning frameworks. The fact that they are pre-trained implies that we don’t need to spend many hours training the network — rather we can complete a forward pass and utilize the output to make a decision within our application.\n\nOpenCV does not (and does not intend to be) to be a tool for training networks — there are already great frameworks available for that purpose. Since a network (such as a CNN) can be used as a classifier, it makes logical sense that OpenCV has a Deep Learning module that we can leverage easily within the OpenCV ecosystem.\n• GoogleLeNet (used in this blog post)\n\nThe release notes for this module are available on the OpenCV repository page.\n\nAleksandr Rybnikov, the main contributor for this module, has ambitious plans for this module so be sure to stay on the lookout and read his release notes (in Russian, so make sure you have Google Translation enabled in your browser if Russian is not your native language).\n\nIt’s my opinion that the module will have a big impact on the OpenCV community, so let’s get the word out.\n\nInstalling OpenCV 3.3 is on par with installing other versions. The same install tutorials can be utilized — just make sure you download and use the correct release.\n\nSimply follow these instructions for MacOS or Ubuntu while making sure to use the opencv and opencv_contrib releases for OpenCV 3.3. If you opt for the MacOS + homebrew install instructions, be sure to use the switch (among the others mentioned) to get the bleeding edge version of OpenCV.\n\nIf you’re using virtual environments (highly recommended), you can easily install OpenCV 3.3 alongside a previous version. Just create a brand new virtual environment (and name it appropriately) as you follow the tutorial corresponding to your system.\n\nKeras is currently not supported (since Keras is actually a wrapper around backends such as TensorFlow and Theano), although I imagine it’s only a matter of time until Keras is directly supported given the popularity of the deep learning library.\n\nUsing OpenCV 3.3 we can load images from disk using the following functions inside :\n\nWe can directly import models from various frameworks via the “create” methods:\n\nAlthough I think it’s easier to simply use the “read” methods and load a serialized model from disk directly:\n\nOnce we have loaded a model from disk, the `.forward` method is used to forward-propagate our image and obtain the actual classification.\n\nTo learn how all these OpenCV deep learning pieces fit together, let’s move on to the next section.\n\nIn this section, we’ll be creating a Python script that can be used to classify input images using OpenCV and GoogLeNet (pre-trained on ImageNet) using the Caffe framework.\n\nThe GoogLeNet architecture (now known as “Inception” after the novel micro-architecture) was introduced by Szegedy et al. in their 2014 paper, Going deeper with convolutions.\n\nOther architectures are also supported with OpenCV 3.3 including AlexNet, ResNet, and SqueezeNet — we’ll be examining these architectures for deep learning with OpenCV in a future blog post.\n\nIn the meantime, let’s learn how we can load a pre-trained Caffe model and use it to classify an image using OpenCV.\n\nTo begin, open up a new file, name it , and insert the following code:\n\nOn Lines 2-5 we import our necessary packages.\n\nOn Line 8 we create an argument parser followed by establishing four required command line arguments (Lines 9-16):\n• : The path to the input image.\n• : The path to the Caffe “deploy” prototxt file.\n• : The pre-trained Caffe model (i.e,. the network weights themselves).\n\nNow that we’ve established our arguments, we parse them and store them in a variable, , for easy access later.\n\nOn Line 20, we load the from disk via .\n\nLet’s take a closer look at the class label data which we load on Lines 23 and 24:\n\nAs you can see, we have a unique identifier followed by a space, some class labels, and a new-line. Parsing this file line-by-line is straightforward and efficient using Python.\n\nFirst, we load the class label from disk into a list. To do this we strip whitespace from the beginning and end of each line while using the new-line (‘ ‘) as the row delimiter (Line 23). The result is a list of IDs and labels:\n\nSecond, we use list comprehension to extract the relevant class labels from by looking for the space (‘ ‘) after the ID, followed by delimiting class labels with a comma (‘ ‘). The result is simply a list of class labels:\n\nNow that we’ve taken care of the labels, let’s dig into the module of OpenCV 3.3:\n\nTaking note of the comment in the block above, we use to perform mean subtraction to normalize the input image which results in a known blob shape (Line 31).\n\nWe then load our model from disk:\n\nSince we’ve opted to use Caffe, we utilize to load our Caffe model definition and pre-trained from disk (Line 35).\n\nIf you are familiar with Caffe, you’ll recognize the file as a plain text configuration which follows a JSON-like structure — I recommend that you open from the “Downloads” section in a text editor to inspect it.\n\nNote: If you are unfamiliar with configuring Caffe CNNs, then this is a great time to consider the PyImageSearch Gurus course — inside the course you’ll get an in depth look at using deep nets for computer vision and image classification.\n\nNow let’s complete a forward pass through the network with as the input:\n\nIt is important to note at this step that we aren’t training a CNN — rather, we are making use of a pre-trained network. Therefore we are just passing the blob through the network (i.e., forward propagation) to obtain the result (no back-propagation).\n\nFirst, we specify as our input (Line 39). Second, we make a timestamp (Line 40), followed by passing our input image through the network and storing the predictions. Finally, we set an timestamp (Line 42) so we can calculate the difference and print the elapsed time (Line 43).\n\nLet’s finish up by determining the top five predictions for our input image:\n\nUsing NumPy, we can easily sort and extract the top five predictions on Line 47.\n\nNext, we will display the top five class predictions:\n\nThe idea for this loop is to (1) draw the top prediction label on the image itself and (2) print the associated class label probabilities to the terminal.\n\nLastly, we display the image to the screen (Line 64) and wait for the user to press a key before exiting (Line 65).\n\nNow that we have implemented our Python script to utilize deep learning with OpenCV, let’s go ahead and apply it to a few example images.\n\nMake sure you use the “Downloads” section of this blog post to download the source code + pre-trained GoogLeNet architecture + example images.\n\nFrom there, open up a terminal and execute the following command:\n\nIn the above example, we have Jemma, the family beagle. Using OpenCV and GoogLeNet we have correctly classified this image as “beagle”.\n\nFurthermore, inspecting the top-5 results we can see that the other top predictions are also relevant, all of them of which are dogs that have similar physical appearances as beagles.\n\nTaking a look at the timing we also see that the forward pass took < 1 second, even though we are using our CPU.\n\nKeep in mind that the forward pass is substantially faster than the backward pass as we do not need to compute the gradient and backpropagate through the network.\n\nLet’s classify another image using OpenCV and deep learning:\n\nOpenCV and GoogLeNet correctly label this image as “traffic light” with 100% certainty.\n\nIn this example we have a “bald eagle”:\n\nWe are once again able to correctly classify the input image.\n\nOur final example is a “vending machine”:\n\nOpenCV + deep learning once again correctly classifes the image.\n\nIn today’s blog post we learned how to use OpenCV for deep learning.\n\nWith the release of OpenCV 3.3 the deep neural network ( ) library has been substantially overhauled, allowing us to load pre-trained networks via the Caffe, TensorFlow, and Torch/PyTorch frameworks and then use them to classify input images.\n\nI imagine Keras support will also be coming soon, given how popular the framework is. This will likely take be a non-trivial implementation as Keras itself can support multiple numeric computation backends.\n\nOver the next few weeks we’ll:\n• Take a deeper dive into the module and how it can be used inside our Python + OpenCV scripts.\n• Learn how to modify Caffe files to be compatible with OpenCV.\n• Discover how we can apply deep learning using OpenCV to the Raspberry Pi.\n\nThis is a can’t-miss series of blog posts, so be before you go, make sure you enter your email address in the form below to be notified when these posts go live!"
    },
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/models/load_model",
        "document": "Used in the notebooks\n\nNote that the model variables may have different name values ( property, e.g. ) after being reloaded. It is recommended that you use layer attributes to access specific variables, e.g. ."
    },
    {
        "link": "https://tensorflow.org/tutorials/keras/save_and_load",
        "document": "Save and categorize content based on your preferences.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nModel progress can be saved during and after training. This means a model can resume where it left off and avoid long training times. Saving also means you can share your model and others can recreate your work. When publishing research models and techniques, most machine learning practitioners share:\n• code to create the model, and\n• the trained weights, or parameters, for the model\n\nSharing this data helps others understand how the model works and try it themselves with new data.\n\nThere are different ways to save TensorFlow models depending on the API you're using. This guide uses tf.keras—a high-level API to build and train models in TensorFlow. The new, high-level format used in this tutorial is recommended for saving Keras objects, as it provides robust, efficient name-based saving that is often easier to debug than low-level or legacy formats. For more advanced saving or serialization workflows, especially those involving custom objects, please refer to the Save and load Keras models guide. For other approaches, refer to the Using the SavedModel format guide.\n\nGet an example dataset\n\nTo demonstrate how to save and load weights, you'll use the MNIST dataset. To speed up these runs, use the first 1000 examples:\n\nYou can use a trained model without having to retrain it, or pick-up training where you left off in case the training process was interrupted. The callback allows you to continually save the model both during and at the end of training.\n\nCreate a callback that saves weights only during training:\n\nThis creates a single collection of TensorFlow checkpoint files that are updated at the end of each epoch:\n\nAs long as two models share the same architecture you can share weights between them. So, when restoring a model from weights-only, create a model with the same architecture as the original model and then set its weights.\n\nNow rebuild a fresh, untrained model and evaluate it on the test set. An untrained model will perform at chance levels (~10% accuracy):\n\nThen load the weights from the checkpoint and re-evaluate:\n\nThe callback provides several options to provide unique names for checkpoints and adjust the checkpointing frequency.\n\nTrain a new model, and save uniquely named checkpoints once every five epochs:\n\nNow, review the resulting checkpoints and choose the latest one:\n\nTo test, reset the model, and load the latest checkpoint:\n\nWhat are these files?\n\nThe above code stores the weights to a collection of checkpoint-formatted files that contain only the trained weights in a binary format. Checkpoints contain:\n• One or more shards that contain your model's weights.\n• An index file that indicates which weights are stored in which shard.\n\nIf you are training a model on a single machine, you'll have one shard with the suffix:\n\nTo save weights manually, use . By default, —and the method in particular—uses the TensorFlow Checkpoint format with a extension. To save in the HDF5 format with a extension, refer to the Save and load models guide.\n\nCall to save a model's architecture, weights, and training configuration in a single zip archive.\n\nAn entire model can be saved in three different file formats (the new format and two legacy formats: , and ). Saving a model as automatically saves in the latest format.\n\nYou can switch to the SavedModel format by:\n\nYou can switch to the H5 format by:\n\nSaving a fully-functional model is very useful—you can load them in TensorFlow.js (Saved Model, HDF5) and then train and run them in web browsers, or convert them to run on mobile devices using TensorFlow Lite (Saved Model, HDF5)\n\n*Custom objects (for example, subclassed models or layers) require special attention when saving and loading. Refer to the Saving custom objects section below.\n\nThe new Keras v3 saving format, marked by the extension, is a more simple, efficient format that implements name-based saving, ensuring what you load is exactly what you saved, from Python's perspective. This makes debugging much easier, and it is the recommended format for Keras.\n\nThe section below illustrates how to save and restore the model in the format.\n\nTry running evaluate and predict with the loaded model:\n\nThe SavedModel format is another way to serialize models. Models saved in this format can be restored using and are compatible with TensorFlow Serving. The SavedModel guide goes into detail about how to the SavedModel. The section below illustrates the steps to save and restore the model.\n\nThe SavedModel format is a directory containing a protobuf binary and a TensorFlow checkpoint. Inspect the saved model directory:\n\nThe restored model is compiled with the same arguments as the original model. Try running evaluate and predict with the loaded model:\n\nKeras provides a basic legacy high-level save format using the HDF5 standard.\n\nNow, recreate the model from that file:\n\nKeras saves models by inspecting their architectures. This technique saves everything:\n• The model's training configuration (what you pass to the method)\n• The optimizer and its state, if any (this enables you to restart training where you left off)\n\nKeras is not able to save the optimizers (from ) since they aren't compatible with checkpoints. For v1.x optimizers, you need to re-compile the model after loading—losing the state of the optimizer.\n\nIf you are using the SavedModel format, you can skip this section. The key difference between high-level /HDF5 formats and the low-level SavedModel format is that the /HDF5 formats uses object configs to save the model architecture, while SavedModel saves the execution graph. Thus, SavedModels are able to save custom objects like subclassed models and custom layers without requiring the original code. However, debugging low-level SavedModels can be more difficult as a result, and we recommend using the high-level format instead due to its name-based, Keras-native nature.\n\nTo save custom objects to and HDF5, you must do the following:\n• Define a method in your object, and optionally a classmethod.\n• returns a JSON-serializable dictionary of parameters needed to recreate the object.\n• uses the returned config from to create a new object. By default, this function will use the config as initialization kwargs ( ).\n• Pass the custom objects to the model in one of three ways:\n• Register the custom object with the decorator. (recommended)\n• Directly pass the object to the argument when loading the model. The argument must be a dictionary mapping the string class name to the Python class. E.g.,\n• Use a with the object included in the dictionary argument, and place a call within the scope.\n\nRefer to the Writing layers and models from scratch tutorial for examples of custom objects and .\n\n# Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the \"Software\"), # to deal in the Software without restriction, including without limitation # the rights to use, copy, modify, merge, publish, distribute, sublicense, # and/or sell copies of the Software, and to permit persons to whom the # Software is furnished to do so, subject to the following conditions: # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL # THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER"
    },
    {
        "link": "https://geeksforgeeks.org/tf-keras-models-load_model-in-tensorflow",
        "document": "TensorFlow is an open-source machine-learning library developed by Google. In this article, we are going to explore the how can we load a model in TensorFlow.\n\ntf.keras.models.load_model function is used to load saved models from storage for further use. It allows users to easily retrieve trained models from disk or other storage mediums.\n\nThe syntax of the tf.keras.models.load_model function is as follows:\n• file path: This argument specifies the path to the saved model file or an h5py.File object from which to load the model.\n• custom_objects: An optional dictionary that maps names (strings) to custom classes or functions to be considered during deserialization. This parameter proves invaluable when dealing with models containing custom layers or loss functions.\n• compile : A boolean flag indicating whether to compile the loaded model after loading. When set to True, the model will be compiled, leveraging the optimizer and loss function specified during training. Conversely, setting it to False allows for skipping compilation, useful when solely interested in model inference.\n\nThis code defines a simple CNN model with three convolutional layers followed by max pooling, flattening, and two dense layers for classification. The model takes input images of size 128x128 pixels with 3 channels (RGB) and outputs a probability distribution over 10 classes using SoftMax activation.\n\nWe load a saved model from the file 'model.h5' using TensorFlow's function and then prints a summary of the loaded model, showing the model architecture, layer names, output shapes, and number of parameters.\n\nIn conclusion, the tf.keras.models.load_model function is a powerful tool for loading saved Keras models in TensorFlow. By understanding its usage and arguments, developers can seamlessly integrate saved models into their applications, enabling efficient model deployment and inference."
    },
    {
        "link": "https://keras.io/api/models/model_saving_apis/model_saving_and_loading",
        "document": "\n• filepath: or object. The path where to save the model. Must end in (unless saving the model as an unzipped directory via ).\n• overwrite: Whether we should overwrite any existing model at the target location, or instead ask the user via an interactive prompt.\n• zipped: Whether to save the model as a zipped archive (default when saving locally), or as an unzipped directory (default when saving on the Hugging Face Hub).\n\nNote that is an alias for .\n• The model's optimizer's state (if any)\n\nThus models can be reinstantiated in the exact same state.\n• filepath: or object. Path where to save the model.\n• overwrite: Whether we should overwrite any existing model at the target location, or instead ask the user via an interactive prompt.\n• zipped: Whether to save the model as a zipped archive (default when saving locally), or as an unzipped directory (default when saving on the Hugging Face Hub).\n\nNote that is an alias for .\n\nThe saved file is a archive that contains:\n• The model's optimizer's state (if any)\n\nThus models can be reinstantiated in the exact same state.\n• filepath: or object, path to the saved model file.\n• custom_objects: Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization.\n• compile: Boolean, whether to compile the model after loading.\n• safe_mode: Boolean, whether to disallow unsafe deserialization. When , loading an object has the potential to trigger arbitrary code execution. This argument is only applicable to the Keras v3 model format. Defaults to .\n\nA Keras model instance. If the original model was compiled, and the argument is set, then the returned model will be compiled. Otherwise, the model will be left uncompiled.\n\nNote that the model variables may have different name values ( property, e.g. ) after being reloaded. It is recommended that you use layer attributes to access specific variables, e.g. ."
    },
    {
        "link": "https://tensorflow.org/tutorials/distribute/save_and_load",
        "document": "This tutorial demonstrates how you can save and load models in a SavedModel format with during or after training. There are two kinds of APIs for saving and loading a Keras model: high-level ( and ) and low-level ( and ).\n\nTo learn about SavedModel and serialization in general, please read the saved model guide, and the Keras model serialization guide. Let's start with a simple example.\n\nLoad and prepare the data with TensorFlow Datasets and , and create the model using :\n\nNow that you have a simple model to work with, let's explore the saving/loading APIs. There are two kinds of APIs available:\n\nHere is an example of saving and loading a model with the Keras API:\n\nAfter restoring the model, you can continue training on it, even without needing to call again, since it was already compiled before saving. The model is saved a Keras zip archive format, marked by the extension. For more information, please refer to the guide on Keras saving.\n\nNow, restore the model and train it using a :\n\nAs the output shows, loading works as expected with . The strategy used here does not have to be the same strategy used before saving.\n\nSaving the model with lower-level API is similar to the Keras API:\n\nLoading can be done with . However, since it is a lower-level API (and hence has a wider range of use cases), it does not return a Keras model. Instead, it returns an object that contain functions that can be used to do inference. For example:\n\nThe loaded object may contain multiple functions, each associated with a key. The key is the default key for the inference function with a saved Keras model. To do inference with this function:\n\nYou can also load and do inference in a distributed manner:\n\nCalling the restored function is just a forward pass on the saved model ( ). What if you want to continue training the loaded function? Or what if you need to embed the loaded function into a bigger model? A common practice is to wrap this loaded object into a Keras layer to achieve this. Luckily, TF Hub has for this purpose, shown here:\n\nIn the above example, Tensorflow Hub's wraps the result loaded back from into a Keras layer that is used to build another model. This is very useful for transfer learning.\n\nWhich API should I use?\n\nFor saving, if you are working with a Keras model, use the Keras API unless you need the additional control allowed by the low-level API. If what you are saving is not a Keras model, then the lower-level API, , is your only choice.\n\nFor loading, your API choice depends on what you want to get from the model loading API. If you cannot (or do not want to) get a Keras model, then use . Otherwise, use . Note that you can get a Keras model back only if you saved a Keras model.\n\nIt is possible to mix and match the APIs. You can save a Keras model with , and load a non-Keras model with the low-level API, .\n\nWhen saving and loading from a local I/O device while training on remote devices—for example, when using a Cloud TPU—you must use the option in and to set the I/O device to . For example:\n\nOne special case is when you create Keras models in certain ways, and then save them before training. For example:\n\nA SavedModel saves the objects generated when you trace a (check When is a Function tracing? in the Introduction to graphs and tf.function guide to learn more). If you get a like this it's because was not able to find or create a traced .\n\nUsually the model's forward pass—the method—will be traced automatically when the model is called for the first time, often via the Keras method. A can also be generated by the Keras Sequential and Functional APIs, if you set the input shape, for example, by making the first layer either a or another layer type, and passing it the keyword argument.\n\nTo verify if your model has any traced s, check if is :\n\nLet's use to train the model, and notice that the gets defined and model saving will work:"
    },
    {
        "link": "https://stackoverflow.com/questions/70805012/cv2-videocapture-html-live-stream",
        "document": "So the URL I was trying to connect to had a little display of the livestream from the raspberry pi and the settings under it. What I had to do along with the code from this answer was right click on the live stream display, open it up in another tab and use the URL from that tab. It's about 2-3 seconds off though."
    },
    {
        "link": "https://stackoverflow.com/questions/58293187/opencv-real-time-streaming-video-capture-is-slow-how-to-drop-frames-or-get-sync",
        "document": "My hypothesis is that the jitter is most likely due to network limitations and occurs when a frame packet is dropped. When a frame is dropped, this causes the program to display the last \"good\" frame which results in the display freezing. This is probably a hardware or bandwidth issue but we can alleviate some of this with software. Here are some possible changes:\n\nWe set the object to have a limited buffer size with the parameter. The idea is that by limiting the buffer, we will always have the latest frame. This can also help to alleviate the problem of frames randomly jumping ahead.\n\nCurrently, I believe the is reading too fast even though it is in its own dedicated thread. This may be one reason why all the frames appear to pool up and suddenly burst in the next frame. For instance, say in a one second time interval, it may produce 15 new frames but in the next one second interval, only 3 frames are returned. This may be due to the network packet frame loss so to ensure that we obtain constant frame rates, we simply add a delay in the frame retrieval thread. A delay to obtain roughly FPS does a good job to \"normalize\" the frame rate and smooth the transition between frames incase there is packet loss.\n\nNote: We should try to match the frame rate of the stream but I'm not sure what the FPS of the webcam is so I just guessed FPS. Also, there is usually a \"direct\" stream link instead of going through a intermediate webserver which can greatly improve performance.\n\nIf you try using a saved video file, you will notice that there is no jitter. This confirms my suspicion that the problem is most likely due to network latency.\n• None Video Streaming from IP Camera in Python Using OpenCV cv2.VideoCapture\n• None How to capture multiple camera streams with OpenCV?\n• None OpenCV real time streaming video capture is slow. How to drop frames or get synced with real time?"
    },
    {
        "link": "https://medium.com/@alionurulker/live-stream-on-any-camera-using-opencv-and-python-e18d4de6fad7",
        "document": "A simple guide to using the OpenCV with Python to live stream from any on IP camera.\n\nI’m a big fan of OpenCV and have been using it to develop projects for a while, however, I was struggling to find a simple way to stream on my android phone. Real-time live streaming has a nice simple way. But currently, no equivalent app for IOS of the app I was using. But I believe that there are a few IP camera apps on the app store.\n\nBefore we start, I’m assuming you have a project already setup in Python(I’m using version 2.7 can be differences in other versions), OpenCV, and are doing something with the Computer Vision. If not you can find any IP camera app your phone store, you can search using these keywords such as IPCAM, IPWEBCAM. Let’s go to how we do this.\n\n0 into the VideoCapture(0) means default camera. If you change it to 1 or other numbers, it views connected other cameras.\n\nFirst of all, we should import “requests”, “cv2”, and “numpy as np” libraries for our project.\n\nAnd then, create a variable as named URL for views of the stream, we can see what’s going on writing /shot.jpg on our browser."
    },
    {
        "link": "https://cloudinary.com/guides/video-effects/cv2-video-capture-python",
        "document": "Getting Started with cv2 Video Capture in Python\n\n\n\n From self-driving cars making split-second decisions, security cameras, or content creators fine-tuning their visuals, video is everywhere. But how do we, as developers, tap into the power of video streams with Python? Thankfully, there’s an easy solution: using CV2 for video capture in Python.\n\nOpenCV allows Python developers to process frames and video streams in real-time and automate tasks like background removal and object tracking. It’s a powerful, mature library for computer vision, suitable for many different applications.\n\nIn this guide, we’ll discuss the basics of using cv2 for video capture with Python for reading video feeds from files and webcam. Mastering cv2 video capture in Python will help you streamline all your video projects and make the most of OpenCV’s capabilities.\n• What is cv2 (OpenCV) in Python?\n• Best Practices for Using cv2 Video Capture in Python\n• Using Cloudinary with cv2 for Better Video Processing\n\nWhat is cv2 (OpenCV) in Python?\n\nOpenCV is an open-source computer vision library designed for processing images and videos. It offers a vast collection of algorithms and tools that enable developers to efficiently manipulate, analyze, and enhance visual data. Whether you’re working on augmented reality, object tracking, or real-time video processing, OpenCV simplifies complex tasks, making it an essential tool for anyone handling video and image data.\n\nIt also supports real-time applications, allowing you to use motion detection and video processing. It’s an industry standard that helps diversify its projects associated with media automation, robotics, and security.\n\nAn important feature of OpenCV is its video capture functionality, which allows it to read, open, and process video streams from various sources, including video files, webcams, and network streams. With cv2, you can analyze motion, apply filters, extract frames, and even perform real-time video processing. This makes it one of the most essential tools for handling video content efficiently in Python.\n\nBefore starting to capture videos, you need to set up OpenCV in Python and the installation process is rather simple.\n\nTo install OpenCV, open your command prompt or terminal and run the following code in a virtual environment.\n\nOnce completed, double check to ensure it’s installed properly by running the following code in Python script:\n\nIf the installation is successful, this will print the installed OpenCV version (which, as of publication, is ). With OpenCV set up, you can immediately start processing video streams.\n\nCapturing video with OpenCV’s function is simple and powerful. This step-by-step guide covers the fundamentals of video streaming, from setup to real-time frame processing.\n\nStep 1: First things first, import the OpenCV library:\n\nStep 2: Start video capture. Use to open a video source, whether it’s a webcam, a video file, or even a network stream.\n• For a webcam, pass 0 as the argument (or 1, 2, etc., for multiple cameras).\n\nFor a video file, provide the file path as a string.\n\nStep 3: Double check if the video source has opened successfully:\n\nStep 5: When finished, release the video source and close any OpenCV windows:\n\nBest Practices for Using cv2 Video Capture in Python\n\nWhen working with video capture in OpenCV, you can ensure smooth performance by following some best practices, such as:\n• Managing Resources Efficiently: Always close any OpenCV windows and release the video source when done to prevent memory leaks. If the application runs for a longer time or is handling high-resolution video, keep an eye on it and manage its memory usage to avoid crashes.\n• Error Handling in Video Capture: Check if the video source is open before reading a frame. If the file or camera is inaccessible, use blocks to prevent crashes. Additionally, ensure that each frame is successfully captured. In case of a missing frame, implement fallback mechanisms such as retrying or switching to an alternate video source.\n• Optimizing Frame Processing: Convert frames in grayscale if you don’t need color, as it reduces the computation load for face recognition. By resizing the frames, the processing speed and real-time applications become more efficient. Apply preprocessing techniques like Gaussian blur or edge detection to improve feature extraction in real-time applications, such as object detection or motion tracking.\n\nUsing Cloudinary with cv2 for Enhanced Video Processing\n\nCloudinary is a powerful media management platform that can significantly enhance OpenCV’s video capture capabilities. By integrating Cloudinary with OpenCV, you can optimize video storage, transformation, and delivery for a seamless media processing workflow.\n\nCV2 video capture in Python is great when you need to capture and process video, but when you have to work with optimization and large-scale video storage, Cloudinary can help. It’s a powerful Digital Asset Management Platform that offers a cloud-based media management solution enhancing the capabilities of OpenCV with its efficient storage, optimized delivery, and real-time transformation.\n\nVideo file management can sometimes be overwhelming, especially when the data assets are large; Cloudinary simplifies this process with flexible cloud storage and automated workflows to manage video files. Your captured video files can be easily retrieved, uploaded, and transformed, making it excellent for video management.\n\nCloudinary’s cloud storage is secure and ensures that you can access video files anywhere. When working remotely in teams, video can be processed in one location and can be accessed in another, simultaneously. It offers powerful real-time transformation tools so that developers can adjust video resolution and convert formats of files.\n\nBy integrating Cloudinary with OpenCV’s cv2.VideoCapture, developers can build scalable, high-performance video processing applications that leverage cloud-based storage, transformation, and streaming for optimal efficiency.\n\n\n\nWorking with cv2 Video Capture in Python is more than just capturing video—it’s about doing it smartly. Managing resources, handling errors smartly, and optimizing performance can make all the difference, especially in real-time applications like object detection and motion tracking. Simple tweaks, like resizing frames or converting them to grayscale, can save you a ton of time and energy.\n\nWant to take it further? Cloudinary’s secure cloud storage, dynamic transformations, and adaptive bitrate streaming make video management effortless, ensuring smooth playback across devices.\n\nExperiment, optimize, and explore Cloudinary’s tools to supercharge your video processing workflow. Streamline your video processing with Cloudinary—get started for free!"
    },
    {
        "link": "https://geeksforgeeks.org/python-opencv-capture-video-from-camera",
        "document": "Python provides various libraries for image and video processing. One of them is OpenCV. OpenCV is a vast library that helps in providing various functions for image and video operations. With OpenCV, we can capture a video from the camera. It lets you create a video capture object which is helpful to capture videos through webcam and then you may perform desired operations on that video.\n• None ) to create a video capture object for the camera.\n• None Create a VideoWriter object to save captured frames as a video in the computer.\n• None Set up an infinite while loop and use the method to read the frames using the above created object.\n• None method to show the frames in the video.\n• None Breaks the loop when the user clicks a specific key.\n\nBelow is the implementation.\n\nFirst, we import the OpenCV library for python i.e. cv2. We use the VideoCapture() class of OpenCV to create the camera object which is used to capture photos or videos. The constructor of this class takes an integer argument which denotes the hardware camera index. Suppose the devices has two cameras, then we can capture photos from first camera by using VideoCapture(0) and if we want to use second camera, we should use VideoCapture(1).\n\nThe dimensions of the frame and the recorded video must be specified. For this purpose, we use the camera object method which is cam.get() to get cv2.CAP_PROP_FRAME_WIDTH and cv2.CAP_PROP_FRAME_HEIGHT which are the fields of cv2 package. The default values of these fields depend upon the actual hardware camera and drivers which are used.\n\nWe are creating a VideoWriter object to write the captured frames to a video file which is stored in the local machine. The constructor of this class takes the following parameters:\n• Filepath/Filename: The file would be saved by specified name and at the specified path. If we mention only the file name, then the video file would be saved at the same folder where the python script for capturing video is running.\n• Four Character Code: A four-letter word to denote the specific encoding and decoding technique or format which must be used in order to create the video. Some popular codecs include ‘MJPG’ = Motion JPEG codec, ‘X264’ = H.264 codec, ‘MP4V’ =\n• Frame rate: Frame rate determines the number of frames or individual images which are shown in a second. Higher frame rates produce smoother video, but it will also take more storage space as compared to lower frame rates.\n• Video Dimension: The height and width of the video which are passed as a tuple.\n\nInside an infinite while loop, we use the read() method of the camera object which returns two values, the first value is boolean and describes whether the frame was successfully captured or not. The second value is a 3D numpy array which represents the actual photo capture. In order to represent an image, we need two-dimensional data structure and for representing color information, we must add another dimension to it making it three dimensional.\n\nThen, we are writing the individual frames to the video file using the VideoWriter object which was previously created using the write() method which takes the captured frame as the argument. To display the captured frame, we are using cv2.imshow() function, which takes window name and the captured frame as arguments. This process is repeated infinitely to produce continuous video stream which consists of these frames.\n\nIn order to exit the application, we have break out of this infinite while loop. Hence we are assigning a key which can be used to quit the application. Here the cv2.waitKey() function takes and integer argument which denotes the number of milliseconds the program waits for to check whether the user has pressed any button. We can use this function to adjust the frame rate of the captured video by providing certain delay. We have assigned “q” as the quit button and we are checking every millisecond whether it is pressed in order to break out of the while loop. The ord() function returns the ASCII value of the character which is passed to it as an argument."
    }
]