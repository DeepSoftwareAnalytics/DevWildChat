[
    {
        "link": "https://geeksforgeeks.org/time-complexity-and-space-complexity",
        "document": "Many times there are more than one ways to solve a problem with different algorithms and we need a way to compare multiple ways. Also, there are situations where we would like to know how much time and resources an algorithm might take when implemented. To measure performance of algorithms, we typically use time and space complexity analysis. The idea is to measure order of growths in terms of input size.\n• None Independent of the machine and its configuration, on which the algorithm is running on.\n• None Shows a direct correlation with the number of inputs.\n• None Can distinguish two algorithms clearly without ambiguity.\n\nTime Complexity: The time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the input. Note that the time to run is a function of the length of the input and not the actual execution time of the machine on which the algorithm is running on.\n\nThe valid algorithm takes a finite amount of time for execution. The time required by the algorithm to solve given problem is called time complexity of the algorithm. Time complexity is very useful measure in algorithm analysis.\n\nIt is the time needed for the completion of an algorithm. To estimate the time complexity, we need to consider the cost of each fundamental instruction and the number of times the instruction is executed.\n\nExample 1: Addition of two scalar variables.\n\nThe addition of two scalar numbers requires one addition operation. the time complexity of this algorithm is constant, so T(n) = O(1) .\n\nIn order to calculate time complexity on an algorithm, it is assumed that a constant time c is taken to execute one operation, and then the total operations for an input length on N are calculated. Consider an example to understand the process of calculation: Suppose a problem is to find whether a pair (X, Y) exists in an array, A of N elements whose sum is Z. The simplest idea is to consider every pair and check if it satisfies the given condition or not.\n\nThe pseudo-code is as follows:\n\nBelow is the implementation of the above approach:\n\nAssuming that each of the operations in the computer takes approximately constant time, let it be c. The number of lines of code executed actually depends on the value of Z. During analyses of the algorithm, mostly the worst-case scenario is considered, i.e., when there is no pair of elements with sum equals Z. In the worst case,\n\nSo total execution time is N*c + N*N*c + c. Now ignore the lower order terms since the lower order terms are relatively insignificant for large input, therefore only the highest order term is taken (without constant) which is N*N in this case. Different notations are used to describe the limiting behavior of a function, but since the worst case is taken so big-O notation will be used to represent the time complexity.\n\nHence, the time complexity is O(N2) for the above algorithm. Note that the time complexity is solely based on the number of elements in array A i.e the input length, so if the length of the array will increase the time of execution will also increase.\n\nOrder of growth is how the time of execution depends on the length of the input. In the above example, it is clearly evident that the time of execution quadratically depends on the length of the array. Order of growth will help to compute the running time with ease.\n\nAnother Example: Let’s calculate the time complexity of the below algorithm:\n\nThis is a tricky case. In the first look, it seems like the complexity is O(N * log N). N for the j′s loop and log(N) for i′s loop. But it’s wrong. Let’s see why.\n\nThink about how many times count++ will run.\n• None And so on.\n\nThe total number of times count++ will run is N + N/2 + N/4+…+1= 2 * N. So the time complexity will be O(N).\n\nSome general time complexities are listed below with the input range for which they are accepted in competitive programming:\n\nProblem-solving using computer requires memory to hold temporary data or final result while the program is in execution. The amount of memory required by the algorithm to solve given problem is called space complexity of the algorithm.\n\nThe space complexity of an algorithm quantifies the amount of space taken by an algorithm to run as a function of the length of the input. Consider an example: Suppose a problem to find the frequency of array elements.\n\nIt is the amount of memory needed for the completion of an algorithm.\n\nTo estimate the memory requirement we need to focus on two parts:\n\n(1) A fixed part: It is independent of the input size. It includes memory for instructions (code), constants, variables, etc.\n\n(2) A variable part: It is dependent on the input size. It includes memory for recursion stack, referenced variables, etc.\n\nExample : Addition of two scalar variables\n\nThe addition of two scalar numbers requires one extra memory location to hold the result. Thus the space complexity of this algorithm is constant, hence S(n) = O(1).\n\nThe pseudo-code is as follows:\n\nBelow is the implementation of the above approach:\n\nHere two arrays of length N, and variable i are used in the algorithm so, the total space used is N * c + N * c + 1 * c = 2N * c + c, where c is a unit space taken. For many inputs, constant c is insignificant, and it can be said that the space complexity is O(N).\n\nThere is also auxiliary space, which is different from space complexity. The main difference is where space complexity quantifies the total space used by the algorithm, auxiliary space quantifies the extra space that is used in the algorithm apart from the given input. In the above example, the auxiliary space is the space used by the freq[] array because that is not part of the given input. So total auxiliary space is N * c + c which is O(N) only."
    },
    {
        "link": "https://geeksforgeeks.org/complete-guide-on-complexity-analysis",
        "document": "What is the need for Complexity Analysis?\n• None Complexity Analysis determines the amount of time and space resources required to execute it.\n• None It is used for comparing different algorithms on different input sizes.\n• None Complexity helps to determine the difficulty of a problem.\n• None often measured by how much time and space (memory) it takes to solve a particular problem\n\nBig-O notation represents the upper bound of the running time of an algorithm. Therefore, it gives the worst-case complexity of an algorithm. By using big O- notation, we can asymptotically limit the expansion of a running time to a range of constant factors above and below. It is a model for quantifying algorithm performance. \n\n\n\nOmega notation represents the lower bound of the running time of an algorithm. Thus, it provides the best-case complexity of an algorithm.\n\nThe execution time serves as a lower bound on the algorithm’s time complexity. It is defined as the condition that allows an algorithm to complete statement execution in the shortest amount of time.\n\nTheta notation encloses the function from above and below. Since it represents the upper and the lower bound of the running time of an algorithm, it is used for analyzing the average-case complexity of an algorithm. The execution time serves as both a lower and upper bound on the algorithm’s time complexity. It exists as both, the most, and least boundaries for a given input value.\n\nBig-Ο is used as a tight upper bound on the growth of an algorithm’s effort (this effort is described by the function f(n)), even though, as written, it can also be a loose upper bound. “Little-ο” (ο()) notation is used to describe an upper bound that cannot be tight. \n\n\n\nLet f(n) and g(n) be functions that map positive integers to positive real numbers. We say that f(n) is ω(g(n)) (or f(n) ∈ ω(g(n))) if for any real constant c > 0, there exists an integer constant n0 ≥ 1 such that f(n) > c * g(n) ≥ 0 for every integer n ≥ n0.\n\nThe complexity of an algorithm can be measured in three ways:\n\nThe time complexity of an algorithm is defined as the amount of time taken by an algorithm to run as a function of the length of the input. Note that the time to run is a function of the length of the input and not the actual execution time of the machine on which the algorithm is running on\n\nTo estimate the time complexity, we need to consider the cost of each fundamental instruction and the number of times the instruction is executed.\n• None If we have statements with basic operations like comparisons, return statements, assignments, and reading a variable. We can assume they take constant time each O(1).\n\nThis is the result of calculating the overall time complexity.\n\nAssuming that n is the size of the input, let’s use T(n) to represent the overall time and t to represent the amount of time that a statement or collection of statements takes to execute.\n• None For any loop, we find out the runtime of the block inside them and multiply it by the number of times the program will repeat the loop.\n\nFor the above example, the loop will execute n times, and it will print “GeeksForGeeks” N number of times. so the time taken to run this program is:\n• None For 2D arrays, we would have nested loop concepts, which means a loop inside a loop.\n\nFor the above example, the cout statement will execute n*m times, and it will print “GeeksForGeeks” N*M number of times. so the time taken to run this program is:\n\nThe amount of memory required by the algorithm to solve a given problem is called the space complexity of the algorithm. Problem-solving using a computer requires memory to hold temporary data or final result while the program is in execution.\n\nThe space Complexity of an algorithm is the total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. \n\nSpace complexity is a parallel concept to time complexity. If we need to create an array of size n, this will require O(n) space. If we create a two-dimensional array of size n*n, this will require O(n2) space.\n\nHowever, just because you have n calls total doesn’t mean it takes O(n) space.\n\nLook at the below function :\n\nThe temporary space needed for the use of an algorithm is referred to as auxiliary space. Like temporary arrays, pointers, etc. \n\nIt is preferable to make use of Auxiliary Space when comparing things like sorting algorithms. \n\nfor example, sorting algorithms take O(n) space, as there is an input array to sort. but auxiliary space is O(1) in that case.\n\nHow does Complexity affect any algorithm?\n\nHow to optimize the time and space complexity of an Algorithm?\n\nOptimization means modifying the brute-force approach to a problem. It is done to derive the best possible solution to solve the problem so that it will take less time and space complexity. We can optimize a program by either limiting the search space at each step or occupying less search space from the start.\n\nWe can optimize a solution using both time and space optimization. To optimize a program,\n• reduce the time taken to run the program and increase the space occupied;\n• None we can reduce the memory usage of the program and increase its total run time, or\n• None we can reduce both time and space\n\nDifferent types of Complexity exist in the program:\n\nIf the function or method of the program takes negligible execution time. Then that will be considered as constant complexity.\n\nExample: The below program takes a constant amount of time.\n\nIt imposes a complexity of O(log(N)). It undergoes the execution of the order of log(N) steps. To perform operations on N elements, it often takes the logarithmic base as 2.\n\nExample: The below program takes logarithmic complexity.\n\n// location of x in given array arr[l..r] is present, // If the element is present at the middle // If element is smaller than mid, then // it can only be present in left subarray // Else the element can only be present // We reach here when element is not \"Element is not present in array\" // the index of x in the given array arr[l..r] if present, // If the element is present at the middle itself // If element is smaller than mid, then // it can only be present in the left subarray // Else the element can only be present // We reach here when the element is not present in the array \"Element is not present in the array\" // This code is contributed by Utkarsh Kumar # Python program for the above approach # location of x in the given array arr[l..r] if present, # If the element is present at the middle itself # If the element is smaller than mid, then # it can only be present in the left subarray # Else the element can only be present # We reach here when the element is not \"Element is not present in array\" # This code is contributed by Susobhan Akhuli // C# program for the above approach // location of x in given array arr[l..r] if present, // If the element is present at the middle // If the element is smaller than mid, then // it can only be present in the left subarray // Else the element can only be present // We reach here when the element is not \"Element is not present in array\" // This code is contributed by Susobhan Akhuli // the index of x in the given array arr[l..r] if present, // If the element is present at the middle itself // If element is smaller than mid, then // it can only be present in the left subarray // Else the element can only be present // We reach here when the element is not present in the array \"Element is not present in the array\"\n\nIt imposes a complexity of O(N). It encompasses the same number of steps as that of the total number of elements to implement an operation on N elements.\n\nExample: The below program takes Linear complexity.\n\nIt imposes a complexity of O(n2). For N input data size, it undergoes the order of N2 count of operations on N number of elements for solving a given problem.\n\nExample: The below program takes quadratic complexity.\n\nIt imposes a complexity of O(n!). For N input data size, it executes the order of N! steps on N elements to solve a given problem.\n\nExample: The below program takes factorial complexity.\n\nIt imposes a complexity of O(2N), O(N!), O(nk), …. For N elements, it will execute the order of the count of operations that is exponentially dependable on the input data size.\n\nExample: The below program takes exponential complexity.\n\nWorst Case time complexity of different data structures for different operations\n• None Top MCQs on Complexity Analysis of Algorithms with Answers\n• None Top MCQs on Complexity Analysis using Recurrence Relations with Answers\n\nComplexity analysis is a very important technique to analyze any problem. The interviewer often checks your ideas and coding skills by asking you to write a code giving restrictions on its time or space complexities. By solving more and more problems anyone can improve their logical thinking day by day. Even in coding contests optimized solutions are accepted. The naive approach can give TLE(Time limit exceed)."
    },
    {
        "link": "https://labuladong.online/algo/en/essential-technique/complexity-analysis",
        "document": "My previous articles primarily focused on explaining algorithm principles and problem-solving thinking, often glossing over the analysis of time and space complexity for the following reasons:\n• None For beginner readers, I want you to concentrate on understanding the principles of algorithms. Adding too much mathematical content can easily discourage readers.\n• None A correct understanding of the underlying principles of common algorithms is a prerequisite for analyzing complexity. Especially for recursion-related algorithms, only by thinking and analyzing from a tree perspective can you correctly analyze their complexity.\n\nSince historical articles now cover the core principles of all common algorithms, I have specifically written a guide on time and space complexity analysis. Instead of giving you a fish, it is better to teach you how to fish by providing a general method to analyze the time and space complexity of any algorithm.\n\nThis article will be lengthy and cover the following points:\n• None Using time complexity to deduce problem-solving approaches, reducing trial and error.\n• None Where does the time go? Common coding mistakes that lead to algorithm timeouts.\n• None Analysis methods for time/space complexity of recursive algorithms, with a focus on this area, using dynamic programming and backtracking algorithms as examples.\n\nBefore explaining the concept of complexity and specific calculation methods, I will discuss some common techniques and pitfalls encountered in practice.\n\nComplexity analysis is not intended to make things difficult for you; rather, it serves as a guide to help you solve problems. You should pay attention to the data scale provided in the problem before you start coding, as complexity analysis can help you avoid wasting time on incorrect approaches. Sometimes, it even directly suggests which algorithm to use. Why is this important? Typically, problems indicate the scale of the test cases. We can use this information to deduce the allowable time complexity for the problem, which further helps us determine the appropriate algorithm to apply.\n\nFor example, if a problem provides an input array with a length up to , we can infer that the time complexity should be less than . It needs to be optimized to or . An algorithm with complexity would reach about , which most judging systems cannot handle efficiently.\n\nTo keep the complexity within or , our options become limited. Suitable approaches might include sorting the array, prefix sums, two-pointer techniques, or one-dimensional dynamic programming. Starting with these strategies is more reliable. Nested for loops, two-dimensional dynamic programming, and backtracking can typically be ruled out immediately.\n\nAnother clear example is when the data scale is very small, such as an array length not exceeding . In this case, we can be fairly certain that a brute-force exhaustive search is likely required.\n\nJudging platforms tend to challenge you with larger data scales, so if they provide small data scales, it's likely because the optimal solution involves exponential or factorial complexity. You can confidently use backtracking algorithms without considering other approaches.\n\nTherefore, it's important to consider all the information given in the problem, particularly the data scale, before starting to code. Ignoring this and diving directly into coding can lead to unnecessary detours.\n\nThis section summarizes common coding errors, especially among beginners, that can lead to unexpected time consumption, slow down your algorithm, or even cause timeouts.\n\nWhile solving algorithm problems, we might use functions to print some states for debugging purposes.\n\nHowever, remember to comment out these output statements before submitting your code, as standard output involves I/O operations that can significantly slow down your algorithm.\n\nIncorrectly Passing by Value Instead of Reference\n\nFor example, in C++, function parameters are passed by value by default. If you pass a as a parameter, the data structure is fully copied, leading to additional time and space consumption. This is particularly problematic for recursive functions, where passing by value almost guarantees timeout or memory overflow.\n\nThe correct approach is to pass by reference, such as , to avoid unnecessary copying. Readers using other languages should verify if similar issues exist and understand their programming language's parameter passing mechanism.\n\nThis is a rather tricky problem, often occurring in object-oriented languages like Java, though it is rare. Nonetheless, it merits attention.\n\nIn Java, is an interface with many implementing classes, such as and . If you have learned from the basic knowledge sections Implementing Dynamic Arrays Step by Step and Implementing Doubly Linked Lists Step by Step, you know that methods of and differ in complexity, such as the method in being and in being .\n\nSo, if a function parameter is passed to you as a type, would you dare to perform for random access? Probably not.\n\nIn such cases, it is generally safer to create an yourself, copy all elements from the passed , and then access them via an index.\n\nThese are the main non-algorithmic issues to watch out for. Pay attention to them, and you should be fine. Let's move on to the formal explanation of algorithm complexity analysis.\n\nThe symbol (uppercase letter 'O', not the number 0) is commonly used to represent a set of functions. For example, represents a set of functions derived from . When we say an algorithm has a time complexity of , it means the complexity of the algorithm is described by a function within this set.\n\nIn theory, understanding this abstract mathematical definition can answer all your questions about Big O notation.\n\nHowever, since many people feel dizzy when they see mathematical definitions, I will list two properties used in complexity analysis. Remembering these two is sufficient.\n\n1. Retain only the term with the highest growth rate; other terms can be omitted.\n\nFirstly, constant factors in multiplication and addition can be ignored, as shown in the following example:\n\nOf course, don't eliminate constants blindly. Some constants cannot be removed, as they might involve exponential rules we learned in high school:\n\nBesides constant factors, terms with slower growth rates can be ignored in the presence of terms with faster growth rates:\n\nAll the examples listed above are the simplest and most common ones, which can be correctly explained by the definition of Big O notation. If you encounter more complex complexity scenarios, you can also judge whether your complexity expression is correct based on the definition.\n\nIn other words, as long as you provide an upper bound, using Big O notation to represent it is correct.\n\nFor example, consider the following code:\n\nIf we consider this as an algorithm, then clearly its time complexity is . However, if you insist on saying its time complexity is , theoretically, that's acceptable, because the notation represents an upper bound. Indeed, the time complexity of this algorithm will not exceed the upper bound of . Although this bound is not \"tight,\" it still fits the definition, so theoretically, it's not incorrect.\n\nThe above example is too simple, and unnecessarily expanding its time complexity upper bound seems meaningless. However, some algorithms have complexities that depend on the input data, making it difficult to provide a very precise time complexity in advance. In such cases, using Big O notation to expand the upper bound of time complexity becomes meaningful.\n\nFor instance, in the previous article The Core Framework of Dynamic Programming, we discussed the brute-force recursive solution to the coin change problem. The core code framework is as follows:\n\nWhen , the recursive tree of the algorithm looks like this:\n\nThe method for calculating the time complexity of recursive algorithms will be discussed later. For now, let's determine the number of nodes in this recursive tree.\n\nAssume the value of is , and the number of elements in the list is . This recursive tree is a -ary tree. However, the growth of this tree is directly related to the denominations of coins in the list, resulting in an irregular structure that makes it difficult to precisely calculate the total number of nodes.\n\nIn such cases, a simpler approach is to approximate based on the worst-case scenario:\n\nWhat is the height of this tree? Unknown, so we handle it based on the worst-case scenario, assuming all coins have a denomination of 1, making the tree height .\n\nWhat is the structure of this tree? Unknown, so we handle it based on the worst-case scenario, assuming it is a full -ary tree.\n\nHow many nodes are on this tree? Handling it based on the worst-case scenario, a full -ary tree of height has a total number of nodes calculated using the geometric series formula , denoted in Big O notation as .\n\nOf course, we know the actual number of nodes is less than this, but using as an approximate upper bound is acceptable and clearly shows that this is an exponential algorithm that requires optimization for efficiency.\n\nTherefore, sometimes your estimated time complexity may differ from someone else's, which doesn't necessarily mean one of you is wrong; it could simply be a difference in estimation precision.\n\nTheoretically, we aim to achieve an accurate and \"tight\" upper bound, but obtaining an accurate upper bound requires strong mathematical skills. When preparing for interviews, it's acceptable to settle for less as long as the order of magnitude (linear/exponential/logarithmic/quadratic, etc.) matches.\n\nIn the field of algorithms, besides using Big O notation for asymptotic upper bounds, there are also asymptotic lower bounds and asymptotic tight bounds, which interested readers can explore on their own. However, from a practical perspective, the above explanation of Big O notation is sufficient.\n\nThe space complexity of non-recursive algorithms is usually easy to calculate; just check if it requires arrays or similar storage space. Therefore, I will mainly discuss the analysis of time complexity."
    },
    {
        "link": "https://datacamp.com/tutorial/big-o-notation-time-complexity",
        "document": "Explore data structures such as linked lists, stacks, queues, hash tables, and graphs; and search and sort algorithms!"
    },
    {
        "link": "https://intersog.com/blog/strategy/algorithm-complexity-estimation-a-bit-of-theory-and-why-it-is-necessary-to-know",
        "document": "Computational complexity or simply complexity of an algorithm is a measure of the amount of time and/or space required by an algorithm for an input of a given size.\n\nThe analysis of algorithms is the process of finding the computational complexity of algorithms.\n\nUsually, this involves determining a function that relates the length of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). An algorithm is said to be efficient when this function's values are small, or grow slowly compared to a growth in the size of the input. We are going to find out how to calculate complexity of algorithm and explain how to find time complexity of an algorithm using examples.\n\nAsymptotic notations are the mathematical notations used to describe the running time of an algorithm when the input tends towards a particular value or a limiting value. There are mainly three asymptotic notations: Theta notation, Omega notation and Big-O notation. These are used to determine the time complexity of algorithm.\n\nTheta notation encloses the function from above and below. Since it represents the upper and the lower bound of the running time of an algorithm, it is used for analyzing the average case complexity of an algorithm.\n\nOmega notation represents the lower bound of the running time of an algorithm. Thus, it provides the best case complexity of an algorithm. For any value of n, the minimum time required by the algorithm is given by Omega Ω(f(n)).\n\nBig-O notation represents the upper bound of the running time of an algorithm. Thus, it gives the worst case complexity of an algorithm. It is widely used to analyze an algorithm as we are always interested in the worst case scenario. For any value of n, the running time of an algorithm does not cross time provided by O(f(n)).\n\nDevelopers typically solve for the worst case scenario, Big-O, because you’re not expecting your algorithm to run in the best or even average cases when calculating time complexity of an algorithm. It allows you to make analytical statements such as, “well, in the worst case scenario, my algorithm will scale this quickly”.\n\nNext chart shows the dependence of the execution time on the amount of incoming data for different types of running times:\n\nBelow are short descriptions of some of the most common running times with examples of algorithms which are marked on the right side of the chart and sorted in order from most efficient to least efficient.\n\nA constant-time algorithm is one that takes the same amount of time, regardless of its input. Examples:\n• Given two numbers, report the sum.\n• Given key-value hash table, return value for key.\n• Given a list of numbers, report the result of adding the first element to itself 1,000,000 times.\n\nA logarithmic-time algorithm is one that requires a number of steps proportional to the log(n). In most cases, we use 2 as the base of the log, but it doesn't matter which base because we ignore constants. Because we use the base 2, we can rephrase this in the following way: every time the size of the input doubles, our algorithm performs one more step for computational complexity analysis.\n\nAn algorithm is said to take linear time, or O(n) time, if it's time complexity is O(n). Informally, this means that the running time increases at most linearly with the size of the input. More precisely, this means that there is a constant c such that the running time is at most cn for every input of size n. Example,\n• A procedure that adds up all elements of a list requires time proportional to the length of the list, if the adding time is constant, or, at least, bounded by a constant.\n\nLinearithmic algorithms are capable of good performance with very large data sets. Some examples of linearithmic algorithms are:\n\nQuadratic Time Complexity represents an algorithm whose performance is directly proportional to the squared size of the input data set (think of Linear, but squared). Within our programs, this time complexity will occur whenever we nest over multiple iterations within the data sets.\n\nAn algorithm is said to be of polynomial time if its running time is upper bounded by a polynomial expression in the size of the input for the algorithm, i.e., f(n) = O(n^C) for some positive constant C.\n\nProblems for which a deterministic polynomial time algorithm exists belong to the complexity class P, which is central in the field of computational complexity theory. Cobham's thesis states that polynomial time is a synonym for “tractable”, “feasible”, “efficient”, or “fast”. Examples:\n• The selection sort sorting algorithm on n integers performs An^2 operations for some constant A. Thus it runs in time O(n^2) and is a polynomial time algorithm.\n• All the basic arithmetic operations (addition, subtraction, multiplication, division, and comparison) can be done in polynomial time.\n• Maximum matchings in graphs can be found in polynomial time.\n\nExponential (base 2) running time means that the calculations performed by an algorithm double every time as the input grows. Examples:\n• Power Set: finding all the subsets on a set.\n\nAn example of an algorithm that runs in factorial time is bogosort, a notoriously inefficient sorting algorithm based on trial and error.\n\nBogosort sorts a list of n items by repeatedly shuffling the list until it is found to be sorted. In the average case, each pass through the bogosort algorithm will examine one of the n! orderings of the n items. If the items are distinct, only one such ordering is sorted. Bogosort shares patrimony with the infinite monkey theorem.\n\nOperations with data structures, for example search, insert, delete, sort, have certain estimates in terms of complexity both in terms of execution time and memory.\n\nIt is very important to consider this when choosing a data structure for solving a particular problem. It is also important to keep in mind that the implementation of various object structures and data types and collections may differ for different programming languages.\n\nIt can be changed with the release of new versions of the language of programming and frameworks. It may also depend on the characteristics of the computer on which the execution is taking place (processor specifications, etc.) Below is a table on the execution time of basic operations with data structures.\n\nEstimating the algorithm complexity is an important part of algorithm design as it provides useful information about expected performance.\n\nIt is a common misconception that estimating the complexity of algorithms will become less important as a result of Moore's Law, which assumes an exponential increase in the power of modern computers. This is wrong, because this increase in power allows you to work with big data, which in turn increases the need for complexity measures in algorithms.\n\nTo give you an idea of how to efficiently calculate time complexity algorithm with examples, if someone wants to alphabetically sort a list of several hundred entries, such as a book bibliography, any algorithm should work in less than a second.\n\nOn one hand, for a list of a million entries (like phone numbers in a big city), elementary algorithms requiring O (n^2) comparisons would have to do a trillion comparisons, which would take about three hours at a rate of 10 million comparisons per second.\n\nOn the other hand, quicksort and merge sort only require O(nlogn) comparisons (as average complexity for the former, as worst case for the latter). For n = 1.000.000, this gives approximately 30.000.000 comparisons, which takes only 3 seconds with 10 million comparisons per second, which is a more efficient way to compute complexity of algorithm.\n\nThus, the complexity estimate can allow many inefficient algorithms to be eliminated before any implementation. It can also be used to tune complex algorithms without testing all variations. By identifying the most costly stages of a complex algorithm, the study of complexity also allows you to focus on these stages' efforts to improve implementation efficiency. I hope this time complexity of algorithms tutorial clears some things out and gives you an idea on how to calculate time complexity for a given algorithm."
    },
    {
        "link": "https://geeksforgeeks.org/lower-and-upper-bound-theory",
        "document": "Lower and upper bound theory is a mathematical concept that involves finding the smallest and largest possible values for a quantity, given certain constraints or conditions. It is often used in optimization problems, where the goal is to find the maximum or minimum value of a function subject to certain constraints.\n• None In mathematical terms, the lower bound of a set of numbers is the smallest number in the set, while the upper bound is the largest number. If a set has a lower and upper bound, it is said to be bounded.\n• None Lower and upper bound theory can also be used to determine the range of possible values for a variable. For example, if we know that a certain variable must be between 0 and 1, we can say that its lower bound is 0 and its upper bound is 1.\n• None The concept of lower and upper bounds is closely related to the concepts of infimum and supremum. The infimum of a set is the greatest lower bound of the set, while the supremum is the least upper bound of the set.\n• None Overall, lower and upper bound theory is an important tool in mathematical analysis, optimization, and decision-making, as it allows us to determine the range of possible values for a quantity and identify the optimal value within that range.\n\nThe Lower and Upper Bound Theory provides a way to find the lowest complexity algorithm to solve a problem. Before understanding the theory, first, let’s have a brief look at what Lower and Upper bounds are.\n• Lower Bound – \n\n Let L(n) be the running time of an algorithm A(say), then g(n) is the Lower Bound of A if there exist two constants C and N such that L(n) >= C*g(n) for n > N. Lower bound of an algorithm is shown by the asymptotic notation called\n• Upper Bound – \n\n Let U(n) be the running time of an algorithm A(say), then g(n) is the Upper Bound of A if there exist two constants C and N such that U(n) <= C*g(n) for n > N. Upper bound of an algorithm is shown by the asymptotic notation called (or just Oh).\n\n1. Lower Bound Theory: \n\nAccording to the lower bound theory, for a lower bound L(n) of an algorithm, it is not possible to have any other algorithm (for a common problem) whose time complexity is less than L(n) for random input. Also, every algorithm must take at least L(n) time in the worst case. Note that L(n) here is the minimum of all the possible algorithms, of maximum complexity. \n\nThe Lower Bound is very important for any algorithm. Once we calculated it, then we can compare it with the actual complexity of the algorithm and if their order is the same then we can declare our algorithm as optimal. So in this section, we will be discussing techniques for finding the lower bound of an algorithm.\n\nNote that our main motive is to get an optimal algorithm, which is the one having its Upper Bound the Same as its Lower Bound (U(n)=L(n)). Merge Sort is a common example of an optimal algorithm.\n\nTrivial Lower Bound – \n\nIt is the easiest method to find the lower bound. The Lower bounds which can be easily observed based on the number of input taken and the number of output produced are called Trivial Lower Bound.\n\nExample: Multiplication of n x n matrix, where,\n\nIn the above example, it’s easily predictable that the lower bound is O(n2).\n\nComputational Model – \n\nThe method is for all those algorithms that are comparison-based. For example, in sorting, we have to compare the elements of the list among themselves and then sort them accordingly. Similar is the case with searching and thus we can implement the same in this case. Now we will look at some examples to understand its usage.\n\nOrdered Searching – \n\nIt is a type of searching in which the list is already sorted.\n\nExample-1: Linear search \n\nExplanation – \n\nIn linear search, we compare the key with the first element if it does not match we compare it with the second element, and so on till we check against the nth element. Else we will end up with a failure.\n\nExample-2: Binary search \n\nExplanation – \n\nIn binary search, we check the middle element against the key, if it is greater we search the first half else we check the second half and repeat the same process. \n\nThe diagram below there is an illustration of binary search in an array consisting of 4 elements \n\n\n\n\n\nCalculating the lower bound: The max no of comparisons is n. Let there be k levels in the tree.\n• None No. of nodes will be 2\n• None The upper bound of no of nodes in any comparison-based search of an element in the list of size n will be n as there are a maximum of n comparisons in worst case scenario 2\n• None Each level will take 1 comparison thus no. of comparisons k≥|log\n\nThus the lower bound of any comparison-based search from a list of n elements cannot be less than log(n). Therefore we can say that Binary Search is optimal as its complexity is Θ(log n).\n\nSorting – \n\nThe diagram below is an example of a tree formed in sorting combinations with 3 elements.\n\nExplanation – \n\nFor n elements, we have a total of n! combinations (leaf nodes). (Refer to the diagram the total combinations are 3! or 6) also, it is clear that the tree formed is a binary tree. Each level in the diagram indicates a comparison. Let there be k levels => 2k is the total number of leaf nodes in a full binary tree thus in this case we have n!≤2k.\n\nAs the k in the above example is the no of comparisons thus by computational model lower bound = k.\n\nUsing Lower bond theory to solve the algebraic problem:\n• Straight Line Program – \n\n The type of program built without any loops or control structures is called the Straight Line Program. For example,\n• Algebraic Problem – \n\n Problems related to algebra like solving equations inequalities etc. come under algebraic problems. For example, solving equation ax\n\nThe complexity for solving here is 4 (excluding the returning). \n\nThe above example shows us a simple way to solve an equation for a 2-degree polynomial i.e., 4 thus for nth degree polynomial we will have a complexity of O(n2).\n\nLet us demonstrate via an algorithm.\n\nExample: x+a is a polynomial of degree n.\n\n\n\nLoop within a loop => complexity = O(n2);\n\nNow to find an optimal algorithm we need to find the lower bound here (as per lower bound theory). As per Lower Bound Theory, The optimal algorithm to solve the above problem is the one having complexity O(n). Let’s prove this theorem using lower bounds.\n\nTheorem: To prove that the optimal algo of solving a n degree polynomial is O(n) \n\nProof: The best solution for reducing the algo is to make this problem less complex by dividing the polynomial into several straight-line problems.\n\nThe complexity of this code is O(n). This way of solving such equations is called Horner’s method. Here is where lower bound theory works and gives the optimum algorithm’s complexity as O(n). \n\n2. Upper Bound Theory: \n\nAccording to the upper bound theory, for an upper bound U(n) of an algorithm, we can always solve the problem at most U(n) time. Time taken by a known algorithm to solve a problem with worse case input gives us the upper bound.\n\nIt’s difficult to provide a comprehensive list of advantages and disadvantages of lower and upper bound theory, as it depends on the specific context in which it is being used. However, here are some general advantages and disadvantages:\n• None Provides a clear understanding of the range of possible values for a quantity, which can be useful in decision-making.\n• None Helps to identify the optimal value within the range of possible values, which can lead to more efficient and effective solutions to problems.\n• None Can be used to prove the existence of solutions to optimization problems.\n• None Provides a theoretical framework for analyzing and solving a wide range of mathematical problems.\n• None May not always provide a precise solution to optimization problems, as the optimal value may not be within the range of possible values determined by the lower and upper bounds.\n• None Can be computationally intensive, especially for complex optimization problems with many constraints.\n• None May be limited by the accuracy of the data used to determine the lower and upper bounds."
    },
    {
        "link": "https://quora.com/Can-you-explain-the-difference-between-upper-bound-and-lower-bound-in-relation-to-an-algorithms-running-time",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://cstheory.stackexchange.com/questions/5553/what-is-the-right-definition-of-upper-and-lower-bounds",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://quora.com/In-algorithms-what-is-the-upper-and-lower-bound",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://stackoverflow.com/questions/44178964/upper-bounds-and-lower-bounds-in-algorithms",
        "document": "One almost never discusses the best case. It is simply not that interesting. An algorithm can always be modified to have the smallest theoretically possible best case, which is O(max(size-of-input, size-of-output)), simply by recognising one particular input and producing output precomputed for that input. In the benchmarking business this is known as cheating.\n\nThe term bound here has the same meaning as in the rest of mathematics, namely, an arbitrary value that is no larger (no smaller) than any element of a given set.\n\nFor example, when discussing the set of sorting algorithms, we can prove that no comparison-based sorting algorithm has better asymptotic efficiency than O(n log n) in the worst case (and also in the average case). Thus O(n log n) is a lower bound on the efficiency of all possible comparison-based sorting algorithms in the worst case (and also in the average case). O(n) is another lower bound. O(n log n) is a better lower bound than O(n). And it just happens that O(n log n) is the tight lower bound, because there are in fact sorting algorithms with this complexity.\n\nThere is no finite upper bound on the complexity of the set of sorting algorithms because an arbitrarily bad sorting algorithm can be created.\n\nOn the other hand, we can discuss a particular sorting algorithm, and prove that it never exceeds a certain number of operations, which would be the upper bound on its complexity. For example, the quicksort algorithm has an upper bound of O(n2). It also has an upper bound of O(n3). It does not have an upper bound of O(n log n) because there are inputs that make it exceed this number of operations. The bound of O(n2) is tight, because it is attained for some inputs.\n\nOne theoretically could discuss the lower bound in the same sense as above, but this is almost never being done (this would amount to discussing the complexity of the best case, which we are not normally interested in).\n\nWe can also discuss the difficulty of a particular problem, and place both upper and lower bounds on it. How efficient is the most efficient algorithm (in the worst or average case) that solves it? (We don't discuss the best case because the answer is not interesting, see above). For the comparison-based sorting problem, we know that both the tight upper bound and the tight lower bound are O(n log n) because there are in fact O(n log n) sorting algorithms and it can be shown that no better algorithm exists. This is not a very interesting case because we can find the most efficient algorithm possible. For e.g. the knapsack problem the situation is more interesting. We only know that an upper bound of O(2n) exists because an algorithm with this complexity trivially exists (the brute force one). We suspect but cannot prove this bound is tight. We also cannot provide any good lower bound (we suspect that there are no algorithms that solve it with polynomial complexity but cannot prove it)."
    }
]