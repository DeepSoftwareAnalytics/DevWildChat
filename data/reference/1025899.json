[
    {
        "link": "https://medium.com/researchify/fine-tuning-bert-for-text-classification-a-step-by-step-guide-with-code-examples-0dea8513bcf2",
        "document": "In our last blog, we explored how to choose the right transformer model, highlighting BERT’s strengths in classification tasks. Now, we dive deeper into fine-tuning BERT with real-world implementations and hands-on code.\n\nText classification is a cornerstone of natural language processing (NLP), enabling tasks such as sentiment analysis, spam detection, and topic categorization. At the forefront of NLP advancements is BERT (Bidirectional Encoder Representations from Transformers), a pre-trained transformer model renowned for its ability to understand context in text.\n\nFine-tuning BERT for classification tasks not only leverages its contextual understanding but also allows for exceptional performance, even with smaller datasets, as long as they are clean and well-prepared. This blog will guide you through the process of fine-tuning BERT step-by-step, demonstrating its real-world applications with hands-on code and practical insights.\n\nBefore fine-tuning BERT, it’s essential to prepare clean, balanced, and well-structured data to ensure the model learns meaningful patterns and generalizes effectively. In this tutorial, we’ll use a real-world example of resume text chunks, each representing different sections such as Contact Information, Education, Work Experience, and Skills.\n\nTo achieve this, ensure the data is free from irrelevant text or missing rows, balanced across classes, and diverse enough to improve model generalization. This will optimize the model’s performance during training and inference.\n\nThis sets up our environment for the next steps.\n\nData preparation is critical for training an effective classification model. Here’s what we focus on:\n• Cleaning: Removing irrelevant or incomplete data ensures we work with a high-quality dataset.\n• Label Encoding: Converting categorical labels (like “Education” or “Skills”) into numeric labels makes the data compatible with the model.\n• Train-Test Split: Separating the data into training and validation sets ensures the model can generalize well.\n\nFor example, with a dataset of resume chunks labeled by section:\n\nThis ensures the data is structured, labeled, and ready for tokenization.\n\nAfter data preparation, we need to tokenize the text and format it for the BERT model:\n\nThe BertTokenizer splits text into smaller subwords and tokens that BERT can process. It also adds special tokens like:\n• [CLS]: Indicates the start of the input.\n\nThe custom ClassificationDataset class takes raw text, tokenizes it, and prepares it for training. It also creates:\n\nThis class ensures the data is in the correct format for BERT.\n\nBERT is a pre-trained model that can be fine-tuned for specific tasks like classification:\n\n1. Load Pre-trained BERT: We use bert-base-uncased, which is a lowercase English BERT model.\n\n2. Specify the Number of Labels: In this case, the number of unique sections in the resume data.\n\n3. Device Setup: Leverages GPU if available for faster training.\n\nThe model is now ready for training.\n\nThe training loop is where the model learns patterns in the data:\n\n1. Forward Pass: The input data is fed through the model to calculate predictions.\n\n2. Loss Calculation: The loss measures how far the predictions are from actual labels.\n\n3. Backward Pass: The optimizer updates the model weights to minimize the loss.\n\nThis trains the model to classify resume sections effectively.\n\nAfter training, the model is evaluated to measure its performance on the validation set. Metrics like accuracy, precision, recall, and F1 score are calculated.\n\nThe fine-tuned BERT model, trained on 1,839 labeled data points, achieved the following validation metrics: Accuracy: 0.8553, F1 Score: 0.8572, Precision: 0.8617, Recall: 0.8553\n\nThese results demonstrate the model’s strong performance and generalization capability, even with a modest dataset size.\n\nSaving the trained model, tokenizer, and label encoder allows us to reuse them for predictions or further training.\n\nFinally, we use the saved model to classify new text data.\n\nThis demonstrates how to classify new resume chunks with the fine-tuned model.\n\nTo make this tutorial more accessible, I’ve provided a Colab notebook that includes all the code and explanations discussed in this blog. You can run the notebook in your browser, explore the implementation hands-on, and adapt it to your datasets with ease.\n\nIn this blog, we explored how to fine-tune BERT for classification tasks, using real-world data and hands-on implementation. From data preparation to evaluation, each step was tailored to help you apply BERT effectively in your own projects.\n\nNext, we’ll dive into advanced techniques like deploying models into production environments, scaling them for real-world applications, and understanding their capabilities and limitations. Stay tuned!"
    },
    {
        "link": "https://analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification",
        "document": "With the advancement in deep learning, neural network architectures like recurrent neural networks (RNN and LSTM) and convolutional neural networks (CNN) have shown a decent improvement in performance in solving several Natural Language Processing (NLP) tasks like text classification, language modeling, machine translation, etc.\n\nHowever, this performance of deep learning models in NLP pales in comparison to the performance of deep learning in Computer Vision.\n\nOne of the main reasons for this slow progress could be the lack of large labeled text datasets. Most of the labeled text datasets are not big enough to train deep neural networks because these networks have a huge number of parameters and training such networks on small datasets will cause overfitting.\n\nAnother quite important reason for NLP lagging behind computer vision was the lack of transfer learning in NLP. Transfer learning has been instrumental in the success of deep learning in computer vision. This happened due to the availability of huge labeled datasets like Imagenet on which deep CNN based models were trained and later they were used as pre-trained models for a wide range of computer vision tasks.\n\nThat was not the case with NLP until 2018 when the transformer model was introduced by Google. Ever since the transfer learning in NLP is helping in solving many tasks with state of the art performance.\n\nIn this article, I explain how do we fine-tune BERT for text classification.\n\nIf you want to learn NLP from scratch, check out our course – Natural Language Processing (NLP) Using Python\n\nTransfer learning is a technique where a deep learning model trained on a large dataset is used to perform similar tasks on another dataset. We call such a deep learning model a pre-trained model. The most renowned examples of pre-trained models are the computer vision deep learning models trained on the ImageNet dataset. So, it is better to use a pre-trained model as a starting point to solve a problem rather than building a model from scratch.\n\nThis breakthrough of transfer learning in computer vision occurred in the year 2012-13. However, with recent advances in NLP, transfer learning has become a viable option in this NLP as well.\n\nMost of the tasks in NLP such as text classification, language modeling, machine translation, etc. are sequence modeling tasks. The traditional machine learning models and neural networks cannot capture the sequential information present in the text. Therefore, people started using recurrent neural networks (RNN and LSTM) because these architectures can model sequential information present in the text.\n\nHowever, these recurrent neural networks have their own set of problems. One major issue is that RNNs can not be parallelized because they take one input at a time. In the case of a text sequence, an RNN or LSTM would take one token at a time as input. So, it will pass through the sequence token by token. Hence, training such a model on a big dataset will take a lot of time.\n\nSo, the need for transfer learning in NLP was at an all-time high. In 2018, the transformer was introduced by Google in the paper “Attention is All You Need” which turned out to be a groundbreaking milestone in NLP.\n\nSoon a wide range of transformer-based models started coming up for different NLP tasks. There are multiple advantages of using transformer-based models, but the most important ones are:\n• These models do not process an input sequence token by token rather they take the entire sequence as input in one go which is a big improvement over RNN based models because now the model can be accelerated by the GPUs.\n• We don’t need labeled data to pre-train these models. It means that we have to just provide a huge amount of unlabeled text data to train a transformer-based model. We can use this trained model for other NLP tasks like text classification, named entity recognition, text generation, etc. This is how transfer learning works in NLP. \n\n\n\nBERT and GPT-2 are the most popular transformer-based models and in this article, we will focus on BERT and learn how we can use a pre-trained BERT model to perform text classification.\n\nBERT (Bidirectional Encoder Representations from Transformers) is a big neural network architecture, with a huge number of parameters, that can range from 100 million to over 300 million. So, training a BERT model from scratch on a small dataset would result in overfitting.\n\nSo, it is better to use a pre-trained BERT model that was trained on a huge dataset, as a starting point. We can then further train the model on our relatively smaller dataset and this process is known as model fine-tuning.\n• Train the entire architecture – We can further train the entire pre-trained model on our dataset and feed the output to a softmax layer. In this case, the error is back-propagated through the entire architecture and the pre-trained weights of the model are updated based on the new dataset.\n• Train some layers while freezing others – Another way to use a pre-trained model is to train it partially. What we can do is keep the weights of initial layers of the model frozen while we retrain only the higher layers. We can try and test as to how many layers to be frozen and how many to be trained.\n• Freeze the entire architecture – We can even freeze all the layers of the model and attach a few neural network layers of our own and train this new model. Note that the weights of only the attached layers will be updated during model training.\n\nIn this tutorial, we will use the third approach. We will freeze all the layers of BERT during fine-tuning and append a dense layer and a softmax layer to the architecture.\n\nYou’ve heard about BERT, you’ve read about how incredible it is, and how it’s potentially changing the NLP landscape. But what is BERT in the first place?\n\nHere’s how the research team behind BERT describes the NLP framework:\n\nThat sounds way too complex as a starting point. But it does summarize what BERT does pretty well so let’s break it down.\n\nFirstly, BERT stands for Bidirectional Encoder Representations from Transformers. Each word here has a meaning to it and we will encounter that one by one in this article. For now, the key takeaway from this line is – BERT is based on the Transformer architecture. Secondly, BERT is pre-trained on a large corpus of unlabelled text including the entire Wikipedia (that’s 2,500 million words!) and Book Corpus (800 million words).\n\nThird, BERT is a “deep bidirectional” model. Bidirectional means that BERT learns information from both the left and the right side of a token’s context during the training phase.\n\nTo learn more about the BERT architecture and its pre-training tasks, then you may like to read the below article:\n\nNow we will fine-tune a BERT model to perform text classification with the help of the Transformers library. You should have a basic understanding of defining, training, and evaluating neural network models in PyTorch. If you want a quick refresher on PyTorch then you can go through the article below:\n• A Beginner-Friendly Guide to PyTorch and How it Works from Scratch\n\nWe have a collection of SMS messages. Some of these messages are spam and the rest are genuine. Our task is to build a system that would automatically detect whether a message is spam or not.\n\nThe dataset that we will be using for this use case can be downloaded from here (right-click and click on “Save link as…”).\n\nI suggest you use Google Colab to perform this task so that you can use the GPU. Firstly, activate the GPU runtime on Colab by clicking on Runtime -> Change runtime type -> Select GPU.\n\nWe will then install Huggingface’s transformers library. This library lets you import a wide range of transformer-based pre-trained models. Just execute the code below to install the library.\n\nYou would have to upload the downloaded spam dataset to your Colab runtime. Then read it into a pandas dataframe.\n\nThe dataset consists of two columns – “label” and “text”. The column “text” contains the message body and the “label” is a binary variable where 1 means spam and 0 means the message is not a spam.\n\nNow we will split this dataset into three sets – train, validation, and test.\n\nWe will fine-tune the model using the train set and the validation set, and make predictions for the test set.\n\nWe will import the BERT-base model that has 110 million parameters. There is an even bigger BERT model called BERT-large that has 345 million parameters.\n\nLet’s see how this BERT tokenizer works. We will try to encode a couple of sentences using the tokenizer.\n\nAs you can see the output is a dictionary of two items.\n• ‘input_ids’ contains the integer sequences of the input sentences. The integers 101 and 102 are special tokens. We add them to both the sequences, and 0 represents the padding token.\n• ‘attention_mask’ contains 1’s and 0’s. It tells the model to pay attention to the tokens corresponding to the mask value of 1 and ignore the rest.\n\nSince the messages (text) in the dataset are of varying length, therefore we will use padding to make all the messages have the same length. We can use the maximum sequence length to pad the messages. However, we can also have a look at the distribution of the sequence lengths in the train set to find the right padding length.\n\nWe can clearly see that most of the messages have a length of 25 words or less. Whereas the maximum length is 175. So, if we select 175 as the padding length then all the input sequences will have length 175 and most of the tokens in those sequences will be padding tokens which are not going to help the model learn anything useful and on top of that, it will make the training slower.\n\nTherefore, we will set 25 as the padding length.\n\nSo, we have now converted the messages in train, validation, and test set to integer sequences of length 25 tokens each.\n\nNext, we will convert the integer sequences to tensors.\n\nNow we will create dataloaders for both train and validation set. These dataloaders will pass batches of train data and validation data as input to the model during the training phase.\n\nIf you can recall, earlier I mentioned in this article that I would freeze all the layers of the model before fine-tuning it. So, let’s do it first.\n\nThis will prevent updating of model weights during fine-tuning. If you wish to fine-tune even the pre-trained weights of the BERT model then you should not execute the code above.\n\nMoving on we will now let’s define our model architecture.\n\nWe will use AdamW as our optimizer. It is an improved version of the Adam optimizer. To learn more about it do check out this paper.\n\nThere is a class imbalance in our dataset. The majority of the observations are not spam. So, we will first compute class weights for the labels in the train set and then pass these weights to the loss function so that it takes care of the class imbalance.\n\nSo, till now we have defined the model architecture, we have specified the optimizer and the loss function, and our dataloaders are also ready. Now we have to define a couple of functions to train (fine-tune) and evaluate the model, respectively.\n\nWe will use the following function to evaluate the model. It will use the validation set data.\n\nNow we will finally start fine-tuning of the model.\n\nYou can see that the validation loss is still decreasing at the end of the 10th epoch. So, you may try a higher number of epochs. Now let’s see how well it performs on the test dataset.\n\nTo make predictions, we will first of all load the best model weights which were saved during the training process.\n\nOnce the weights are loaded, we can use the fine-tuned model to make predictions on the test set.\n\nBoth recall and precision for class 1 are quite high which means that the model predicts this class pretty well. However, our objective was to detect spam messages, so misclassifying class 1 (spam) samples is a bigger concern than misclassifying class 0 samples. If you look at the recall for class 1, it is 0.90 which means that the model was able to correctly classify 90% of the spam messages. However, precision is a bit on the lower side for class 1. It means that the model misclassifies some of the class 0 messages (not spam) as spam.\n\nTo summarize, in this article, we fine-tuned a pre-trained BERT model to perform text classification on a very small dataset. I urge you to fine-tune BERT on a different dataset and see how it performs. You can even perform multiclass or multi-label classification with the help of BERT. In addition to that, you can even train the entire BERT architecture as well if you have a bigger dataset.\n\nIn case you are looking for a roadmap to becoming an expert in NLP read the following article-\n• A Comprehensive Learning Path to Understand and Master NLP in 2020\n\nYou may use the comment section in case you have any thoughts to share or have any doubts."
    },
    {
        "link": "https://reddit.com/r/LanguageTechnology/comments/pwj8bi/bert_finetuning_techniques",
        "document": "I am currently in the process of fine-tuning BERT for a classification problem using a small dataset.\n\nI came across this article stepping through a tutorial on how to do so. https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n\nOne area I was curious about in the article was the brief discussion in techniques. They discussed training the entire architecture, freeze some layers or freeze the entire architecture.\n\nCan anyone here help point me in a direction to learn more about each technique?\n\nMore specifically, what the pros and cons? When to apply them in practice? And are these the only ones?"
    },
    {
        "link": "https://medium.com/@hassaanidrees7/fine-tuning-transformers-techniques-for-improving-model-performance-4b4353e8ba93",
        "document": "Transformers have become the backbone of many state-of-the-art models in natural language processing (NLP) and beyond. However, to maximize their performance on specific tasks, fine-tuning is essential. For students and practitioners in AI and machine learning, understanding fine-tuning techniques is crucial for leveraging the full potential of Transformer models. This blog post explores various fine-tuning techniques and strategies to enhance Transformer model performance.\n\nFine-tuning is the process of taking a pre-trained Transformer model and adapting it to a specific task or dataset. This process involves further training the model on a smaller, task-specific dataset, allowing it to learn task-specific features and improve its performance.\n• Task-Specific Data: A smaller dataset tailored to the specific application or task at hand.\n• Adaptation: Modifying the pre-trained model to optimize performance for the specific task.\n• Choose a Pre-Trained Model: Select a pre-trained Transformer model that aligns with your task requirements. Popular choices include BERT, GPT-3, and T5.\n\n2 .Prepare the Dataset: Organize and preprocess your task-specific dataset. Ensure the data is tokenized and formatted correctly for the chosen model.\n\n3. Set Up Training Arguments: Configure the training parameters such as learning rate, batch size, and number of epochs.\n\n4. Define the Trainer: Use the Hugging Face class to handle the training loop and evaluation.\n\n5. Train the Model: Fine-tune the model on your specific dataset.\n• Discriminative Fine-Tuning: Apply different learning rates to different layers of the model. This technique helps in adjusting the learning rates according to the specificity of each layer.\n\n2. Gradual Unfreezing: Gradually unfreeze the layers of the Transformer model during training. This approach helps in retaining the learned features in the early layers while fine-tuning the higher layers.\n\n3. Layer-Wise Learning Rate Decay (LLRD): Assign higher learning rates to the top layers and lower learning rates to the bottom layers. This technique ensures that the top layers adapt faster to the specific task.\n\n4. Data Augmentation: Enhance your dataset by creating variations of the training data. This technique helps in making the model robust and improving generalization.\n\n5. Regularization Techniques: Use dropout and weight decay to prevent overfitting and improve model generalization.\n\n6. Warm-Up Steps: Implement warm-up steps to gradually increase the learning rate at the beginning of training. This technique helps in stabilizing training.\n\nAfter fine-tuning, it’s essential to evaluate the model to ensure it meets the performance requirements.\n\nFine-tuned Transformer models can be applied to a wide range of tasks:\n• Machine Translation: Translate text from one language to another.\n• Question Answering: Provide accurate answers to questions based on given context.\n\nFine-tuning Transformer models is a powerful technique to tailor pre-trained models for specific tasks, enhancing their performance and utility. By applying the techniques discussed in this post, students and practitioners can optimize Transformer models to achieve state-of-the-art results in various applications. Experiment with these techniques, fine-tune your models, and unlock the full potential of Transformers in your AI projects. Share your thoughts and questions in the comments below, and stay tuned for more insights into the world of machine learning and NLP."
    },
    {
        "link": "https://addepto.com/blog/what-is-fine-tuning-in-nlp",
        "document": "Fine-tuning allows NLP models to adapt pre-trained language models for specific tasks, saving time and resources while improving performance. This guide explores fine-tuning techniques, challenges, and practical applications.\n• Pre-trained models like BERT, GPT-3, and XLNet can be fine-tuned for applications such as text classification, sentiment analysis, and machine translation.\n\nWhat is Fine-Tuning in NLP?\n\nFine-tuning, a form of transfer learning, optimizes large pre-trained language models for domain-specific tasks. Instead of training from scratch, developers adjust model parameters using smaller, targeted datasets to improve performance.\n\nFor example, a generic English language model can be fine-tuned for legal or medical applications by training on domain-specific texts.\n• Task-Specific Architecture Modification – Adjusting a model’s structure to better suit a new task, such as modifying a text classifier for language modeling.\n• Domain Adaptation – Training a pre-trained model on a smaller, domain-specific dataset to enhance its effectiveness in a particular field.\n• Knowledge Distillation – Transferring knowledge from a large model to a smaller one by mimicking its probability distributions, improving efficienc\n• BERT – Excels at understanding word context in sentences, suitable for sentiment analysis, named entity recognition, and question-answering.\n• GPT-3 – A powerful model with 175 billion parameters, capable of text generation, translation, and summarization.\n• XLNet – Uses a permutation-based training method, improving language comprehension and supporting applications like classification and translation.\n\nRead more about The best NLP model GPT alternatives\n• Text Classification – Categorizing text for applications like spam detection and sentiment analysis.\n• Named Entity Recognition (NER) – Identifying entities such as names, locations, and organizations.\n• Overfitting & Underfitting – Overfitting occurs with excessive domain-specific data, while underfitting results from insufficient training. Techniques like regularization and early stopping mitigate these risks.\n\nFine-tuning is a powerful way to adapt pre-trained NLP models for specialized tasks, improving efficiency and accuracy. By selecting the right techniques and addressing challenges, businesses can leverage NLP for better insights and automation."
    },
    {
        "link": "https://platform.openai.com/docs/guides/prompt-engineering",
        "document": ""
    },
    {
        "link": "https://andrewmayne.com/2021/04/16/advanced-prompt-design-for-gpt-3-how-to-make-a-prompt-20x-more-efficient",
        "document": "TL;DR: GPT-3 is much more capable than people realize when you utilize advanced prompt design that shows it what you want performed in a task then show it how to perform this task with a list of inputs.\n\nOne of my favorite prompts in our OpenAI documentation is an example showing how to get GPT-3 to perform a task on a list of items. I want to expand on that because I don’t think enough developers are really paying attention to the implications of it.\n\nGPT-3 can perform a wide variety of tasks like classification, summarization and chat from a simple prompt (the text we send to GPT-3.) However, simple prompts only hint at what GPT-3 is actually capable of doing.\n• Knowing how to do things\n\nMost prompts only focus on one of those capabilities. If you combine one task with another GPT-3 can be used to do more things and do them more efficiently.\n\nIf I wanted to have GPT-3 classify text sentiment with an emoji, a simple prompt would look like this: I’d give the model a few examples of the text to classify and how to classify it and then provide it with an incomplete example for GPT-3 to finish by following the previous pattern.\n\nGPT-3 is able to read the text and complete it with an appropriate emoji:\n\nIn a real-time application a developer might use a prompt like this to perform the task because they only need to classify one sentence at a time, but in other applications where you’re trying process large amounts of text or have the ability to group them in batches, there’s a much more efficient way to perform this task.\n\nAs I mentioned before, one of GPT-3’s capabilities is its ability to follow instructions (OpenAI even has a version of GPT-3 that is even more focused on this.) If you tell it to make a list, it can make a list. If you tell it to read instructions, it will read instructions. As long as GPT-3 clearly understands what’s a task and what’s content, it can switch back and forth between these capabilities.\n\nKeeping this in mind, we can make a more complex prompt that can process multiple user inputs by clearly teaching GPT-3 how to perform a task, the content to perform it on and what to do.\n\nWe start by first showing GPT-3 what we want done. In this case it’s converting text to emoji. We can use our previous example for this. I’ll also throw in a title to help prime it for the task:\n\nNext we’ll add a separator, write the word “Text” and give GPT-3 a list of numbered sentences followed by the word “Emoji” and the number 1 to start a list.\n\nWhen we ask for a completion, GPT-3 is able to understand the task from the previous example and then apply it to our list of sentences:\n\nAs you can see, GPT-3 was able to apply what it learned from the first example to a list of ten sentences without the need to provide extra descriptive text or make multiple API calls. In this example we only had it create ten responses, but it’s capable of doing many more.\n\nLet’s break this down: We taught it how to perform a task using a descriptive method of defining what was input (the text) and what was the goal (an emoji) and then showed it how to apply it to a list so we didn’t have to keep sending the prompt to API for every item.\n\nWhen you need to process large batches of data, this approach can help do that more efficiently. I’ve tested it with key points extractions, word tagging and summarization, among other tasks."
    },
    {
        "link": "https://promptingguide.ai/introduction/tips",
        "document": "Here are some tips to keep in mind while you are designing your prompts:\n\nAs you get started with designing prompts, you should keep in mind that it is really an iterative process that requires a lot of experimentation to get optimal results. Using a simple playground from OpenAI or Cohere is a good starting point.\n\nYou can start with simple prompts and keep adding more elements and context as you aim for better results. Iterating your prompt along the way is vital for this reason. As you read the guide, you will see many examples where specificity, simplicity, and conciseness will often give you better results.\n\nWhen you have a big task that involves many different subtasks, you can try to break down the task into simpler subtasks and keep building up as you get better results. This avoids adding too much complexity to the prompt design process at the beginning.\n\nYou can design effective prompts for various simple tasks by using commands to instruct the model what you want to achieve, such as \"Write\", \"Classify\", \"Summarize\", \"Translate\", \"Order\", etc.\n\nKeep in mind that you also need to experiment a lot to see what works best. Try different instructions with different keywords, contexts, and data and see what works best for your particular use case and task. Usually, the more specific and relevant the context is to the task you are trying to perform, the better. We will touch on the importance of sampling and adding more context in the upcoming guides.\n\nOthers recommend that you place instructions at the beginning of the prompt. Another recommendation is to use some clear separator like \"###\" to separate the instruction and context.\n\nBe very specific about the instruction and task you want the model to perform. The more descriptive and detailed the prompt is, the better the results. This is particularly important when you have a desired outcome or style of generation you are seeking. There aren't specific tokens or keywords that lead to better results. It's more important to have a good format and descriptive prompt. In fact, providing examples in the prompt is very effective to get desired output in specific formats.\n\nWhen designing prompts, you should also keep in mind the length of the prompt as there are limitations regarding how long the prompt can be. Thinking about how specific and detailed you should be. Including too many unnecessary details is not necessarily a good approach. The details should be relevant and contribute to the task at hand. This is something you will need to experiment with a lot. We encourage a lot of experimentation and iteration to optimize prompts for your applications.\n\nAs an example, let's try a simple prompt to extract specific information from a piece of text.\n\nInput text is obtained from this Nature article (opens in a new tab).\n\nGiven the tips above about being detailed and improving format, it's easy to fall into the trap of wanting to be too clever about prompts and potentially creating imprecise descriptions. It's often better to be specific and direct. The analogy here is very similar to effective communication -- the more direct, the more effective the message gets across.\n\nFor example, you might be interested in learning the concept of prompt engineering. You might try something like:\n\nIt's not clear from the prompt above how many sentences to use and what style. You might still somewhat get good responses with the above prompts but the better prompt would be one that is very specific, concise, and to the point. Something like:\n\nTo do or not to do?\n\nAnother common tip when designing prompts is to avoid saying what not to do but say what to do instead. This encourages more specificity and focuses on the details that lead to good responses from the model.\n\nHere is an example of a movie recommendation chatbot failing at exactly what I don't want it to do because of how I wrote the instruction -- focusing on what not to do.\n\nHere is a better prompt:\n\nThe following is an agent that recommends movies to a customer. The agent is responsible to recommend a movie from the top global trending movies. It should refrain from asking users for their preferences and avoid asking for personal information. If the agent doesn't have a movie to recommend, it should respond \"Sorry, couldn't find a movie to recommend today.\". Customer: Please recommend a movie based on my interests.\n\nSome of the examples above were adopted from the \"Best practices for prompt engineering with OpenAI API\" article. (opens in a new tab)"
    },
    {
        "link": "https://medium.com/edge-analytics/getting-the-most-out-of-gpt-3-based-text-classifiers-part-one-797460a5556e",
        "document": "Getting the Most Out of GPT-3-based Text Classifiers: Part One\n\nThis is part one of a series on how to get the most out of GPT-3 for text classification tasks (Part 2, Part 3). In this post, we’ll talk about a common issue, “out-of-bounds” predictions, and how to reduce or even completely eliminate it.\n\nAt Edge Analytics, we’re using GPT-3 and other cutting-edge NLP technologies to create end-to-end solutions. Check out our recent work with inVibe using GPT-3 to improve the market research process!\n\nGPT-3 stands for “Generative Pre-trained Transformer 3”. It was created by OpenAI and at the time of writing is the largest model of its kind, consisting of over 175 billion parameters. It was also pre-trained using one of the largest text corpuses ever, consisting of about 499 billion tokens (approximately 2 trillion characters) which includes a significant chunk of all text available on the internet.\n\nGPT-3 uses a text-based interface. It accepts a sequence of text (i.e., the “prompt”) as an input and outputs a sequence of text that it predicts should come next (i.e., the “prediction” or “completion”). Through this surprisingly simple interface, GPT-3 is able to produce impressive results. The trick is designing the right prompt to extract the right knowledge encoded within GPT-3.\n\nAt the time of writing, GPT-3 is in a private beta. You have to apply for access on the OpenAI website. We recommend watching this YouTube video for a good overview of the process and some tips for getting access.\n\nWhat is an out-of-bounds prediction?\n\nThis post describes some techniques we use when leveraging GPT-3 for text classification tasks. One common issue is that GPT-3 can produce outputs that are not one of the intended classes. For example, when we designed a GPT-3-based sentiment classifier to label text as either “positive”, “mixed”, or “negative”, it would it would sometimes predict “unknown”. This label makes sense semantically but is not one of the three labels we expected. We call this kind of output from GPT-3 an “out-of-bounds” prediction.\n\nThe reason “out-of-bounds” predictions can occur has to do with how GPT-3 was trained and what it is designed to do. GPT-3 is not just a text classifier; it doesn’t even have any built-in rules for it! GPT-3 was really only designed to do one thing: predict the sequence of text that is most likely to follow the prompt. It has to learn the rules of the classifier on the fly based on the prompt and any similarities or patterns from its massive corpus. Given this context, it’s understandable that it might get things wrong sometimes.\n\nLuckily, the GPT-3 API gives us some knobs to tweak to almost entirely eliminate out-of-bounds predictions. In this blog post, we’ll cover two of them:\n\nThe parameter is pretty straightforward. The GPT-3 documentation explains how works: \"Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.\" For text classification tasks, you usually want to set this to 0.\n\nWhile setting to 0 goes a long way to reduce out-of-bounds predictions, it doesn't completely eliminate them. To take things one step further, you can use the parameter (more on that in a bit).\n\nIn order to understand how can be used to create a whitelist of labels, we first need to take a step back and explain how GPT-3 interprets text. GPT-3 doesn't see text as a string of characters or words, but as a list of \"token ids\". Each token id is a unique number which represents a short sequence of characters (on average each token corresponds to about 4 characters of English text). The process for converting text to tokens is based on the frequency of certain combinations of characters. Common words like \"cat\" and \"ball\" are likely to be assigned their own unique token, whereas less common words like \"establishment\" are likely to be sliced up into more than one token (e.g., \"estab\", \"lish\", \"ment\").\n\nAs to why GPT-3 uses this tokenization approach? Well, it has a few benefits. It provides a fairly dense encoding for text that computers can easily understand. Since every word can be constructed by a combination of tokens, no words are considered completely out-of-vocabulary (a problem that can sometimes plague other encoding techniques like word embeddings). GPT-3 can accept as input words that it has never seen before, and given enough context, can potentially understand them.\n\nThe parameter is a bit more complicated than . Effectively, it can be used to tweak the probability of certain tokens being included in GPT-3’s prediction. As the documentation elaborates, “values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token”. For most text classification tasks, we want to restrict GPT-3’s output to a whitelist of labels that we define ahead of time. In other words, “exclusive selection” is exactly what we want.\n\nSo in order to use to create a whitelist of labels, we need to first convert our labels to tokens. For this, we’ll need to import the transformers library.\n\nBut there’s a catch! Because of how token-encoding works, (with a preceding space) is represented by a different token than (without the space). To make matters more complicated, it’s not always easy to tell which representation GPT-3 will choose, since it can depend on the preceding text. We’ve found the most reliable way to account for this difference is to add both and to our whitelist. We do this for each label.\n\nThe final step is to convert this list of tokens into the dictionary format expected for the parameter, where each token has a weight (or bias) of 100. Effectively we are telling GPT-3 to exclusively construct its prediction from this set of tokens.\n\nThe reason this technique does not completely eliminate out-of-bounds responses is that, hypothetically, there is nothing stopping GPT-3 from combining the tokens in our whitelist in unexpected ways. For example, it might combine some of the tokens for “negative” and “mixed” to output “negamixed”. In practice it is unlikely that GPT-3 will predict a high probability for made up words, and these kinds of out-of-bounds predictions are exceedingly rare (though it does depend on the exact labels used).\n\nIn this post, we covered two ways to reduce out-of-bounds predictions when using GPT-3 as a text classifier. First, set the parameter to 0. Second — and this is a bit more complicated — use the parameter to force GPT-3 to exclusively build its prediction from a predefined set of tokens.\n\nTo put everything together, let’s look at a specific demonstration of how these techniques can be used to improve the performance of a GPT-3-based text classifier. In this simplified example, we want GPT-3 to classify foods as either a “fruit” or a “vegetable”.\n\nEdge Analytics has helped multiple companies build solutions that leverage GPT-3. More broadly, we specialize in data science, machine learning, and algorithm development both on the edge and in the cloud. We provide end-to-end support throughout a product’s lifecycle, from quick exploratory prototypes to production-level AI/ML algorithms. We partner with our clients, who range from Fortune 500 companies to innovative startups, to turn their ideas into reality. Have a hard problem in mind? Get in touch at info@edgeanalytics.io.\n\nGetting the Most Out of GPT-3-based Text Classifiers: Part 1, Part 2, Part 3"
    },
    {
        "link": "https://analyticsvidhya.com/blog/2022/05/prompt-engineering-in-gpt-3",
        "document": "This article was published as a part of the Data Science Blogathon.\n\nMost of you definitely faced this question in your data science journey. Large Language Models are often tens of terabytes in size and are trained on massive volumes of text data, occasionally reaching petabytes. They’re also among the models with the most parameters, where a “parameter” is a value that the model may alter independently as it learns.\n\nLLM is the most searched topic by data scientists and data science enthusiasts for the last few years. Different language models can be used to do different NLP and ML tasks such as classification, summarization, translation, question answering, etc. A probability distribution over word sequences is a statistical language model(Language Model). It assigns a probability P to the entire series given a sequence of length m. The language model gives context for distinguishing between phonetically identical words and phrases. Each year a number of new language models are evolving by creating a new benchmark. The number of parameters is also increasing in each model.\n\nFor the past few years, the size of large language models has increased by a factor of ten per year. It’s beginning to resemble another Moore’s Law.\n\nEven though we have enough language models now itself, all the data science enthusiasts are eagerly waiting for the arrival of the new superhero- who is none other than GPT-4.\n\nThe day for the release of GPT-4 is getting closer. According to current projections, the model will be released in 2022, most likely in July or August. So before that royal entry, we should make our hands dirty by playing with the current GPT model which is GPT-3. In this article, I am trying to explain to you a very fundamental concept that we need to understand before moving to play with GPT models, which is – Prompt engineering. \n\n\n\nDue to security reasons, GPT-3 is not open source and will be available only through an API. OpenAI’s OpenAI API service, a cloud-based application programming interface with usage-based billing, makes GPT-3 available to developers. Natural language processing includes natural language generation as one of its primary components, which focuses on creating human language natural text. The main purpose of GPT-3 is natural language generation. Along with natural language generation, it supports lots of other tasks. Some of those are listed below.\n\nGPT-3 adds 175 billion parameters to the GPT-2 design, as well as altered initialization, pre-normalization, and configurable tokenization. It displays strong performance on a variety of NLP tasks and benchmarks in three different scenarios: zero-shot, one-shot, and few-shot. Among that one-shot learning and few-shot learning, the user needs to provide some expected input and output of the specific use-case to the API. After that, the user needs to provide a sample trigger to generate the required output. This trigger is called the prompt in GPT-3. In GPT-3’s API, a ‘prompt‘ is a parameter that is provided to the API so that it is able to identify the context of the problem to be solved. Depending on how the prompt is written, the returned text will attempt to match the pattern accordingly. The below graph shows the accuracy of GPT-3 with prompt and without prompt in the models with different numbers of parameters. From this chart, we can clearly identify the importance prompt based modeling in GPT-3.\n\nNext, we will discuss how to create the prompts for different tasks in GPT-3, which is called prompt engineering.\n\nFrom creating original stories to doing intricate text analysis, GPT-3 can do it all. Because they can accomplish so many things, you must be specific in your instructions. A excellent prompt generally relies on showing rather than telling. Prompt creation follows three main guidelines:\n\nShow and tell: Make your intentions obvious by using instructions, examples, or a combination of the two.\n\nProvide quality data: Make sure there are enough samples if you’re trying to develop a classifier or get the model to follow a pattern.\n\nCheck your settings: The top_p and temperature settings determine how predictable the model is in providing a response.\n\n\n\nAs I mentioned at the beginning, due to the security purpose GPT-3 is not open source yet. It is available only through the API now. For getting the API you should signup into the openAI platform. In the early times, there was a waiting list for getting the API key. Thank God..!! but now the wishlist has been removed by the openAI team and you will get the key at the signup time itself. For understanding more about the portal, check the official webpage here.\n\nYou can use the openAI python wrapper library(check here for more details)to work with GPT-3. This can be used in other methods like curl, JSON, and node.js also. But here I am going to show you the examples in the GPT-3 basic playground terminal which is provided on the openAI website.\n\nNext, we are going to check the different types of prompt design for various tasks in Natural Language Processing using GPT-3.\n\nHere we are showing some celebrities’ names, who are popular in their domains, and asking the GPT-3 engine to classify them according to their domain. This is the perfect example of zero-shot classification. That is system is classifying the names based on its own trained world knowledge. We are not providing any sample examples to train the system.\n\nHere is the perfect example of few-shot learning. We need to extract some entities from a passage. So we are feeding some examples to the system and training the system to learn from those few examples. The ### token is used to differentiate between different training examples. Finally, the testing sentence will be given and the system will be able to extract the entities from the text. We can modify/alter the outputs by changing the configurations like temperature, Top p..etc showing at the left panel of the window.\n\nThis is also an example of few-shot learning. The task we are going to handle is unstructured question answering. By providing a context with a question, we are training the model to predict the answer to the question from the context. Some examples are passed to the model for training purposes. Every example is distinguished with the token – ###. If you are not satisfied with the GPT-3 provided answer, you can change the engine(current engine is – text-davinci-002) and other hyperparameters (temperature and top_p, etc) for generating other answers.\n\nThe process performed here is automatic text summarization, one of the popular activity in natural language processing. GPT-3 handles the task as a zero-shot learning strategy. Here in the prompt, we are just telling that, summarize the following document and provide a sample paragraph as input. No sample training examples are given since it is zero-shot learning, not few-shot learning. After triggering the API, we will get the summarized format of the input paragraph. If you are not satisfied with this summarization result you can improve the accuracy by either changing the model parameters as discussed above section or providing some examples as few-shot learning.\n\nWhile all prompts result in completions, in situations where you want the API to start up where you left off, it’s useful to conceive of text completion as its own task. For example, if this query is given, the API will continue thinking about long lining. You can lower the temperature setting to keep the API focused on the prompt’s intent, or raise it to allow it to wander.\n\n\n\nFrom creating original stories to doing intricate text analysis, openAI GPT-3 can do it all. Because they can accomplish so many things, you must be specific in your instructions. Various NLP tasks such as text classification, text summarization, sentence completion, etc can be done using GPT-3 by prompting. An excellent prompt generally relies on showing rather than telling. \n\n\n\nShow and tell, Provide Quality data, and Change settings.\n\nAlong with that, we can get the outputs in three ways\n\nZero-shot learning: Where no examples are given for training.\n\nOne-shot learning: Here only one example is provided for the training pur\n\nFew-shot learning: Where we can provide a few examples to train the model along with the prompt.\n\nGPT-3 is not open source yet and is only available via the openAI API. Here I have showcased to you the examples in the GPT-3 basic playground terminal which is provided on the openAI website, rather than any programming language wrapper of openAI.\n\nThe simplest kind of Artificial General Intelligence is GPT-3. Because a single model can perform multiple types of Natural Language Processing jobs. It operates on the basis of prompts, and a strong prompt can help the model provide the best results. I tried to explain some approaches for creating good prompts in GPT-3 playground using examples in this article. I hope this article has helped you learn more about GPT-3 prompts and how to create your own. Please share any problems you experience while creating the prompts, as well as any thoughts, suggestions, or comments, in the section below.\n\nThe media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion."
    }
]