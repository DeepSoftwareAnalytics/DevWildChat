[
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-release-1.17/api/python/reference/pyflink.table/api/pyflink.table.table_environment.StreamTableEnvironment.sql_query.html",
        "document": ""
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-release-1.17/api/python/reference/pyflink.table/api/pyflink.table.table_environment.StreamTableEnvironment.register_table.html",
        "document": ""
    },
    {
        "link": "https://bookstack.cn/read/flink-1.18-en/a19d0b52dc5219d4.md",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/75976664/reading-from-kafka-with-pyflink-not-working",
        "document": "This is example with . Link\n\nI want to read records from the topic and print them with . When I want to produce to topic it works, but I can't read from that topic.\n\nI checked out that with and ."
    },
    {
        "link": "https://pyflink.readthedocs.io/en/main/getting_started/quickstart/table_api.html",
        "document": "This document is a short introduction to the PyFlink Table API, which is used to help novice users quickly understand the basic usage of PyFlink Table API.\n\nYou can run the latest version of these examples by yourself in ‘Live Notebook: Table’ at the quickstart page.\n\nFor advanced usage, you can refer to the latest version of PyFlink Table API doc\n\nis the entry point and central context for creating Table and SQL API programs. Flink is an unified streaming and batch computing engine, which provides unified streaming and batch API to create a . is responsible for:\n• None management: Creation, listing s, Conversion between and , etc. For more details of how to create a , you can refer to the latest version Create a TableEnvironment\n\nis a core component of the Python Table API. A object describes a pipeline of data transformations. It does not contain the data itself in any way. Instead, it describes how to read data from a table source, how to add some compute on data and how to eventually write data to a table sink. The declared pipeline can be printed, optimized, and eventually executed in a cluster. The pipeline can work with bounded or unbounded streams which enables both streaming and batch scenarios. A is always bound to a specific . It is not possible to combine tables from different s in same query, e.g., to join or union them. Firstly, you can create a from a Python Object /Users/duanchen/sourcecode/flink/flink-python/dev/.conda/lib/python3.7/site-packages/pyflink/table/utils.py:55: FutureWarning: Schema passed to names= option, please pass schema= explicitly. Will raise exception in future return pa.RecordBatch.from_arrays(arrays, schema) # create a StreamExecutionEnvironment which is the entry point of `DataStream` program.\n\nYou can get the schema of as follows: collects the contents of the current to local client. +----+----------------------+--------------------------------+ | op | id | data | +----+----------------------+--------------------------------+ | +I | 1 | Hi | | +I | 2 | Hello | +----+----------------------+--------------------------------+ 2 rows in set PyFlink Table also provides the conversion back to a pandas DataFrame to leverage pandas API.\n\nPyFlink Table is lazily evaluated and simply selecting a column does not trigger the computation but it returns a Column instance. These Column s can be used to select the columns from a . For example, takes the column instances that returns another . To select a subset of rows, use .\n\nPyFlink supports various UDFs and APIs to allow users to execute Python native functions. See also the latest User-defined Functions and Row-based Operations. The first example is UDFs used in & /Users/duanchen/sourcecode/flink/flink-python/dev/.conda/lib/python3.7/site-packages/pyflink/table/utils.py:55: FutureWarning: Schema passed to names= option, please pass schema= explicitly. Will raise exception in future return pa.RecordBatch.from_arrays(arrays, schema) # use the Python function in SQL API Another example is UDFs used in"
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/group-agg",
        "document": ""
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-release-1.16/api/python//reference/pyflink.table/api/pyflink.table.WindowGroupedTable.aggregate.html",
        "document": ""
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-release-1.16/api/python//reference/pyflink.table/api/pyflink.table.GroupedTable.select.html",
        "document": ""
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-master/api/python/reference/pyflink.table/api/pyflink.table.GroupedTable.select.html",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/68614440/cannot-use-some-aggregate-functions-sum-or-count-on-python-flink-1-11-x",
        "document": "Just wanted to check if my code is wrong or this is a pyflink 1.11.X\n\nWhen I try to count the number of elements in a group by query (either 'GroupBy Aggregation' or 'GroupBy Window Aggregation') pyFlink throws the following error:\n\nIf I upgrade to pyflink 1.12.X I have a workaround by casting the type to double. Surprisinglym it works with 1.12 but not with 1.11.\n\nCan someone shad some light on this? Please find below an example to reproduce the error."
    }
]