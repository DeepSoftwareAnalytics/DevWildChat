[
    {
        "link": "https://kalmanfilter.net",
        "document": "The Kalman Filter algorithm is a powerful tool for estimating and predicting system states in the presence of uncertainty and is widely used as a fundamental component in applications such as target tracking, navigation, and control.\n\nAlthough the Kalman Filter is a straightforward concept, many resources on the subject require extensive mathematical background and fail to provide practical examples and illustrations, making it more complicated than necessary.\n\nBack in 2017, I created an online tutorial based on numerical examples and intuitive explanations to make the topic more accessible and understandable. The online tutorial provides introductory material covering the univariate (one-dimensional) and multivariate (multidimensional) Kalman Filters.\n\nOver time, I have received many requests to include more advanced topics, such as non-linear Kalman Filters (Extended Kalman Filter and Unscented Kalman Filter), sensors fusion, and practical implementation guidelines.\n\nBased on the material covered in the online tutorial, I authored a book.\n\nThe original online tutorial is available for free access. The e-book and the source code (Python and MATLAB) for the numerical examples are available for purchase.\n\nThe book takes the reader from the basics to the advanced topics, covering both theoretical concepts and practical applications. The writing style is intuitive, prioritizing clarity of ideas over mathematical rigor, and it approaches the topic from a philosophical perspective before delving into quantification.\n\nThe book contains many illustrative examples, including 14 fully solved numerical examples with performance plots and tables. Examples progress in a paced, logical manner and build upon each other.\n\nThe book also includes the necessary mathematical background, providing a solid foundation to expand your knowledge and help to overcome your math fears.\n\nUpon finishing this book, you will be able to design, simulate, and evaluate the performance of the Kalman Filter.\n• Part 1 serves as an introduction to the Kalman Filter, using eight numerical examples, and doesn't require any prior mathematical knowledge. You can call it \"The Kalman Filter for Dummies,\" as it aims to provide an intuitive understanding and develop \"Kalman Filter intuition.\" Upon completing Part 1, readers will thoroughly understand the Kalman Filter's concept and be able to design a univariate (one-dimensional) Kalman Filter. Most of this part is available for free access!\n• Part 2 presents the Kalman Filter in matrix notation, covering the multivariate (multidimensional) Kalman Filter. It includes a mathematical derivation of Kalman Filter equations, dynamic systems modeling, and two numerical examples. This section is more advanced and requires basic knowledge of Linear Algebra (only matrix operations). Upon completion, readers will understand the math behind the Kalman Filter and be able to design a multivariate Kalman Filter. Most of this part is available for free access!\n• Part 3 is dedicated to the non-linear Kalman Filter, which is essential for mastering the Kalman Filter since most real-life systems are non-linear. This part begins with a problem statement and describes the differences between linear and non-linear systems. It includes derivation and examples of the most common non-linear filters: the Extended Kalman Filter and the Unscented Kalman Filter.\n• Part 4 contains practical guidelines for Kalman Filter implementation, including sensor fusion, variable measurement uncertainty, treatment of missing measurements, treatment of outliers, and the Kalman Filter design process."
    },
    {
        "link": "https://mathworks.com/matlabcentral/fileexchange/172219-sensor-fusion-extended-kalman-filter-and-motion-estimation",
        "document": ""
    },
    {
        "link": "https://ahrs.readthedocs.io/en/latest/filters/ekf.html",
        "document": "The Extended Kalman Filter is one of the most used algorithms in the world, and this module will use it to compute the attitude as a quaternion with the observations of tri-axial gyroscopes, accelerometers and magnetometers.\n\nThe state is the physical state, which can be described by dynamic variables. The noise in the measurements means that there is a certain degree of uncertainty in them [HSS11].\n\nA dynamical system is a system whose state evolves over time, so differential equations are normally used to model them [LJ15]. There is also noise in the dynamics of the system, process noise, which means we cannot be entirely deterministic, but we can get indirect noisy measurements.\n\nHere, the term filtering refers to the process of filtering out the noise in the measurements to provide an optimal estimate for the state, given the observed measurements.\n\nThe instantaneous state of the system is represented with a vector updated through discrete time increments to generate the next state. The simplest of the state space models are linear models [HSS11], which can be expressed with equations of the following form:\n• None \\(\\mathbf{x}_t\\in\\mathbb{R}^n\\) is the state of the system describing the condition of \\(n\\) elements at time \\(t\\).\n• None \\(\\mathbf{z}_t\\in\\mathbb{R}^m\\) are the measurements at time \\(t\\).\n• None \\(\\mathbf{w}_x\\sim\\mathcal{N}(\\mathbf{0}, \\mathbf{Q}_t)\\) is the process noise at time \\(t\\).\n• None \\(\\mathbf{w}_z\\sim\\mathcal{N}(\\mathbf{0}, \\mathbf{R}_t)\\) is the measurement noise at time \\(t\\).\n• None \\(\\mathbf{F}\\in\\mathbb{R}^{n\\times n}\\) is called either the State Transition Matrix or the Fundamental Matrix, and sometimes is represented with \\(\\mathbf{\\Phi}\\). It depends on the literature.\n\nMany linear models are also described with continuous-time state equations of the form:\n\nwhere \\(\\mathbf{A}\\) and \\(\\mathbf{L}\\) are constant matrices characterizing the behaviour of the model, and \\(\\mathbf{w}_t\\) is a white noise with a power spectral density \\(\\sigma_\\mathbf{w}^2\\).\n\nThe main difference is that \\(\\mathbf{A}\\) models a set of linear differential equations, and is continuous. \\(\\mathbf{F}\\) is discrete, and represents a set of linear equations (not differential equations) which transitions \\(\\mathbf{x}_{t-1}\\) to \\(\\mathbf{x}_t\\) over a discrete time step \\(\\Delta t\\).\n\nA common way to obtain \\(\\mathbf{F}\\) uses the matrix exponential, which can be expanded with a Taylor series [Sol17] :\n\nThe main goal is to find an equation that recursively finds the value of \\(\\mathbf{x}_t\\) in terms of \\(\\mathbf{x}_{t-1}\\).\n\nThe solution proposed by [Kal60] models a system with a set of \\(n^{th}\\)-order differential equations, converts them into an equivalent set of first-order differential equations, and puts them into the matrix form \\(\\dot{\\mathbf{x}}=\\mathbf{Ax}\\). Once in this form several techniques are used to convert these linear differential equations into the recursive equation \\(\\mathbf{x}_t = \\mathbf{Fx}_{t-1}\\). The Kalman filter has two steps: 1. The prediction step estimates the next state, and its covariance, at time \\(t\\) of the system given the previous state at time \\(t-1\\). 2. The correction step rectifies the estimation with a set of measurements \\(\\mathbf{z}\\) at time \\(t\\).\n• None \\(\\hat{\\mathbf{P}}_t\\in\\mathbb{R}^{n\\times n}\\) is the Predicted Covariance of the state before seeing the measurements \\(\\mathbf{z}_t\\).\n• None \\(\\mathbf{P}_t\\in\\mathbb{R}^{n\\times n}\\) is the Estimated Covariance of the state after seeing the measurements \\(\\mathbf{z}_t\\).\n• None \\(\\mathbf{u}_t\\in\\mathbb{R}^k\\) is a Control input vector defining the expected behaviour of the system.\n• None \\(\\mathbf{H}\\in\\mathbb{R}^{m\\times n}\\) is the Observation model linking the predicted state and the measurements.\n• None \\(\\mathbf{v}_t\\in\\mathbb{R}^m\\) is the Innovation or Measurement residual.\n• None \\(\\mathbf{K}_t\\in\\mathbb{R}^{n\\times m}\\) is the filter gain, a.k.a. the Kalman Gain, telling how much the predictions should be corrected. The predicted state \\(\\hat{\\mathbf{x}}_t\\) is estimated in the first step based on the previously computed state \\(\\mathbf{x}_{t-1}\\), and later is “corrected” during the second step to obtain the final estimation \\(\\mathbf{x}_t\\). A similar computation happens with its covariance \\(\\mathbf{P}\\). The hat notation is (mostly) used to indicate an estimated value. It marks here the calculation of the state in the prediction step at time \\(t\\). The loop starts with prior mean \\(\\mathbf{x}_0\\) and prior covariance \\(\\mathbf{P}_0\\), which are defined by the system model.\n\nThe functions have been Gaussian and linear so far and, thus, the output was always another Gaussian, but Gaussians are not closed under nonlinear functions. The EKF handles nonlinearity by forming a Gaussian approximation to the joint distribution of state \\(\\mathbf{x}\\) and measurements \\(\\mathbf{z}\\) using Taylor series based transformations [HSS11]. Likewise, the EKF is split into two steps: where \\(\\mathbf{f}\\) is the nonlinear dynamic model function, and \\(\\mathbf{h}\\) is the nonlinear measurement model function. The matrices \\(\\mathbf{F}\\) and \\(\\mathbf{H}\\) are the Jacobians of \\(\\mathbf{f}\\) and \\(\\mathbf{h}\\), respectively: Notice that the matrices \\(\\mathbf{F}_{t-1}\\) and \\(\\mathbf{H}_t\\) of the normal KF are replaced with Jacobian matrices \\(\\mathbf{F}(\\mathbf{x}_{t-1}, \\mathbf{u}_t)\\) and \\(\\mathbf{H}(\\hat{\\mathbf{x}}_t)\\) in the EKF, respectively. The predicted state, \\(\\hat{\\mathbf{x}}_t\\), and the residual of the prediction, \\(\\mathbf{v}_t\\), are also calculated differently. The state transition and the observation models must be differentiable functions. In this case, we will use the EKF to estimate an orientation represented as a quaternion \\(\\mathbf{q}\\). First, we predict the new state (newest orientation) using the immediate measurements of the gyroscopes, then we correct this state using the measurements of the accelerometers and magnetometers. All sensors are assumed to have a fixed sampling rate (\\(f=\\frac{1}{\\Delta t}\\)), even though the time step can be computed at any sample \\(n\\) as \\(\\Delta t = t_n - t_{n-1}\\). Numerical integration will give a discrete set of \\(n\\) values, that are approximations at discrete times \\(t_n = t_0 + n\\Delta t\\). Gyroscope data are treated as external inputs to the filter rather than as measurements, and their measurement noises enter the filter as process noise rather than as measurement noise [Sab11]. For this model, the quaternion \\(\\mathbf{q}\\) will be the state vector, and the angular velocity \\(\\boldsymbol\\omega\\), in rad/s, will be the control vector: Therefore, transition models are described as: with \\(\\mathbf{w}_\\mathbf{q}\\) being the process noise.\n\nIn the first step, the quaternion at time \\(t\\) is predicted by integrating the angular rate \\(\\boldsymbol\\omega\\), and adding it to the formerly computed quaternion \\(\\mathbf{q}_{t-1}\\): This constant augmentation is sometimes termed the Attitude Propagation. Because exact closed-form solutions of the integration are not available, an approximation method is required. Using the Euler-Rodrigues rotation formula to redefine the quaternion [Sab11] we find: where \\(\\mathbf{I}_4\\) is a \\(4\\times 4\\) Identity matrix, and: The expression \\(\\lfloor\\mathbf{x}\\rfloor_\\times\\) expands a vector \\(\\mathbf{x}\\in\\mathbb{R}^3\\) into a \\(3\\times 3\\) skew-symmetric matrix. The large term inside the brackets, multiplying \\(\\mathbf{q}_{t-1}\\), is an orthogonal rotation retaining the normalization of the propagated attitude quaternions, and we might be tempted to consider it equal to \\(\\mathbf{F}\\), but it is not yet linear, although it is already discrete. It must be linearized to be used in the EKF. \\(n^{th}\\)-order polynomial linearization methods can be built from Taylor series of \\(\\mathbf{q}(t_n+\\Delta t)\\) around the time \\(t=t_n\\): where \\(\\dot{\\mathbf{q}}=\\frac{d\\mathbf{q}}{dt}\\) is the derivative of the quaternion : The angular rates \\(\\boldsymbol\\omega\\) are measured by the gyroscopes in the local sensor frame. Hence, this term describes the evolution of the orientation with respect to the local frame [Sol17]. Using the definition of \\(\\dot{\\mathbf{q}}\\), the predicted state, \\(\\hat{\\mathbf{q}}_t\\) is written as [SJM78]: Assuming the angular rate is constant over the period \\([t-1, t]\\), we have \\(\\dot{\\boldsymbol\\omega}=0\\), and we can prescind from the derivatives of \\(\\boldsymbol\\Omega\\) reducing the series to: Notice the series has the known form of the matrix exponential: The error of the approximation vanishes rapidly at higher orders, or when the time step \\(\\Delta t \\to 0\\). The more terms we have, the better our approximation becomes, with the downside of a big computational demand. For simple architectures (like embedded systems) we can reduce this burden by truncating the series to its second term making it a First Order EKF , and achieving fairly good results. Thus, our process model shortens to: And now, we simply compute \\(\\mathbf{F}\\) as : Per definition, the predicted error state covariance is calculated as: We already know how to compute \\(\\mathbf{F}(\\mathbf{q}_{t-1}, \\boldsymbol\\omega_t)\\), but we still need to compute the Process Noise Covariance Matrix \\(\\mathbf{Q}_t\\). The noise at the prediction step lies mainy in the control input. Consequently, \\(\\mathbf{Q}_t\\) is derived mainly from the gyroscope. We define \\(\\boldsymbol\\sigma_\\boldsymbol\\omega^2= \\begin{bmatrix}\\sigma_{\\omega x}^2 & \\sigma_{\\omega y}^2 & \\sigma_{\\omega z}^2\\end{bmatrix}^T\\) as the spectral density of the gyroscopic noises on each axis, whose standard deviation, \\(\\boldsymbol\\sigma_\\boldsymbol\\omega\\), is specified as a scalar in rad/s. Without boring too much into DSP analysis, we can frankly say that the smaller the spectral density of a sensor is, the less noisy its signals are, and we would tend to trust its readings a bit more. Normally, these noises can be found already in the datasheets provided by the sensor manufacturers. Taking for granted that the gyroscope is the same kind on its axes with the manufacturer guaranteeing perfect orthogonality and uncorrelation between them, we build the spectral noise covariance matrix as: The easiest solution is to assume that this noise is consistent, and our process noise covariance would simply be \\(\\mathbf{Q}_t = \\boldsymbol\\Sigma_\\boldsymbol\\omega\\), but in a real system this covariance changes depending on the angular velocity, so it will need to be recomputed for every prediction. We need to know how much noise is added to the system over a discrete interval \\(\\Delta t\\) and, therefore, we integrate the process noise over the interval \\([0, \\Delta t]\\) As we have seen, the matrix exponentials tend to be computationally demanding, so, we opt to use the Jacobian of the prediction, but with respect to the angular rate: Notice this Jacobian is very similar to \\(\\mathbf{F}\\), but this one has partial derivatives with respect to \\(\\boldsymbol\\omega\\). Having the noise values and the Jacobian \\(\\mathbf{W}_t\\), the Process Noise Covariance is computed at each time \\(t\\) with: For convenience, it is assumed that the noises are equal on each axis, and don’t influence each other, yielding a white, uncorrelated and isotropic noise : further simplifying the computation to: The assumption that the noise variances of the gyroscope axes are all equal (\\(\\sigma_{wx}=\\sigma_{wy}=\\sigma_{wz}\\)) is almost never true in reality. It is possible to infer the individual variances through a careful modeling and calibration process [LSWA03]. If these three different values are at hand, it is then recommended to compute the Process Noise Covariance with \\(\\mathbf{W}_t\\boldsymbol\\Sigma_\\boldsymbol\\omega\\mathbf{W}_t^T\\). Finally, the prediction step of this model would propagate the covariance matrix like:\n\nThe gyroscope measurements have been used, so far, to predict the new state of the quaternion, but there are also accelerometer and magnetometer readings available, that can be used to correct the estimation. We know, from the equations above, that the corrected state can be computed as: The Kalman Gain can be thought of as a blending value within the range \\([0.0, 1.0]\\), that decides how much of the innovation \\(\\mathbf{v}_t = \\mathbf{z}_t - \\mathbf{h}(\\mathbf{q}_t)\\) will be considered. You can see, for example, that:\n• None If \\(\\mathbf{K}=0\\), there is no correction: \\(\\mathbf{q}_t=\\hat{\\mathbf{q}}_t\\).\n• None If \\(\\mathbf{K}=1\\), the state will be corrected with all the innovation: \\(\\mathbf{q}_t = \\hat{\\mathbf{q}}_t + \\mathbf{v}_t\\). We start by defining the measurement vector as: These are the values obtained from the sensors, where \\(\\mathbf{a}\\in\\mathbb{R}^3\\) is a tri-axial accelerometer sample, in \\(\\frac{m}{s^2}\\), and \\(\\mathbf{m}\\in\\mathbb{R}^3\\) is a tri-axial magnetometer sample, in \\(\\mu T\\). Their noises \\(\\mathbf{w}_\\mathbf{a}\\) and \\(\\mathbf{w}_\\mathbf{m}\\) are assumed independent white Gaussian with zero mean of covariances \\(\\boldsymbol\\Sigma_\\mathbf{a}=\\boldsymbol\\sigma_\\mathbf{a}^2\\mathbf{I}_3\\) and \\(\\boldsymbol\\Sigma_\\mathbf{m}=\\boldsymbol\\sigma_\\mathbf{m}^2\\mathbf{I}_3\\). However, while the gyroscopes give measurements in the sensor frame, the accelerometers and magnetometers deliver measurements of the global frame. We must represent the accelerometers and magnetometer readings, if we want to correct the estimated orientation in the sensor frame. The predicted quaternion \\(\\hat{\\mathbf{q}}_t\\) describes the orientation of the sensor frame with respect to the global frame. For that reason, it will be used to rotate the sensor measurements. To rotate any vector \\(\\mathbf{x}\\in\\mathbb{R}^3\\) through a quaternion \\(\\mathbf{q}\\in\\mathbb{H}^4\\), we can use its representation as a rotation matrix \\(\\mathbf{C}(\\mathbf{q})\\in\\mathbb{R}^{3\\times 3}\\). For the estimated quaternion \\(\\hat{\\mathbf{q}}_t\\) this becomes: The rotation matrix \\(\\mathbf{C}(\\hat{\\mathbf{q}})\\) in this estimator is computed with the function , using the version 2. By default (version 1), the function returns a rotation matrix with a different definition of diagonal values. Please see the documentation of the function for more details. There are two main global reference frames based on the local tangent plane:\n• None NED defines the X-, Y-, and Z-axis colinear to the geographical North, East, and Down directions, respectively.\n• None ENU defines the X-, Y-, and Z-axis colinear to the geographical East, North, and Up directions, respectively. The gravitational acceleration vector in a global NED frame is normally defined as \\(\\mathbf{g}_\\mathrm{NED}=\\begin{bmatrix}0 & 0 & -9.81\\end{bmatrix}^T\\), where the normal acceleration, \\(g_0\\approx 9.81 \\frac{m}{s^2}\\), acts solely on the Z-axis . For the ENU frame, it simply flips the sign along the Z-axis, \\(\\mathbf{g}_\\mathrm{ENU}=\\begin{bmatrix}0 & 0 & 9.81\\end{bmatrix}^T\\). Earth’s magnetic field is also represented with a 3D vector, \\(\\mathbf{r} =\\begin{bmatrix}r_x & r_y & r_z\\end{bmatrix}^T\\), whose values indicate the direction of the magnetic flow. In an ideal case only the North component holds a value, yielding the options \\(\\mathbf{r}_\\mathrm{NED}=\\begin{bmatrix}r_x & 0 & 0\\end{bmatrix}^T\\) or \\(\\mathbf{r}_\\mathrm{ENU}=\\begin{bmatrix}0 & r_y & 0\\end{bmatrix}^T\\) The geomagnetic field is, however, not regular on the planet, and it even changes over time (see WMM for more details.) Usually, certain authors prefer to define this referencial vector discarding the eastwardly magnetic information, sometimes projecting its magnitude against the XY plane with the help of the magnetic dip angle, \\(\\theta\\), resulting in \\(\\mathbf{r}_\\mathrm{NED} = \\begin{bmatrix}\\cos\\theta & 0 & \\sin\\theta\\end{bmatrix}^T\\) and \\(\\mathbf{r}_\\mathrm{ENU} = \\begin{bmatrix}0 & \\cos\\theta & -\\sin\\theta\\end{bmatrix}^T\\). From the acceleration and magnetic field merely their directions are required, not really their magnitudes. We can, therefore, use their normalized values, simplifying their definition to: To compare these vectors against their corresponding observations, we must also normalize the sensors’ measurements: The expected gravitational acceleration in the sensor frame, \\(\\hat{\\mathbf{a}}\\), can be estimated from the estimated orientation: Similarly, the expected magnetic field in sensor frame, \\(\\hat{\\mathbf{m}}\\) is: The measurement model, \\(\\mathbf{h}(\\hat{\\mathbf{q}}_t)\\), and its Jacobian, \\(\\mathbf{H}(\\hat{\\mathbf{q}}_t)\\), can be used to correct the predicted model. The Measurement model is directly defined with these transformations as: The measurement equations are nonlinear, which forces to, accordingly, compute their Jacobian matrix as: This Jacobian can be refactored as: The measurement noise covariance matrix, \\(\\mathbf{R}\\in\\mathbb{R}^{6\\times 6}\\), is expressed directly in terms of the statistics of the measurement noise affecting each sensor [Sab11]. The sensor noises are considered as uncorrelated and isotropic, which creates a diagonal matrix: This definition allows us to obtain simple expressions for \\(\\mathbf{P}_t\\). The rest of the EKF elements in the correction step are obtained as defined originally: Lastly, the corrected state (quaternion) and its covariance at time \\(t\\) are computed with: The EKF is not an optimal estimator and might diverge quickly with an incorrect model. The origin of the problem lies in the lack of independence of the four components of the quaternion, since they are related by the unit-norm constraint. The easiest solution is to normalize the corrected state. Even though it is neither elegant nor optimal, this “brute-force” approach to compute the final quaternion is proven to work generally well [Sab11].\n\nThe estimation is simplified by giving the sensor values at the construction of the EKF object. This will perform all steps above and store the estimated orientations, as quaternions, in the attribute . In this case, the measurement vector, set in the attribute , is equal to the measurements of the accelerometer. If extra information from a magnetometer is available, it will also be considered to estimate the attitude. For this case, the measurement vector contains the accelerometer and magnetometer measurements together: at each time \\(t\\). The most common sampling frequency is 100 Hz, which is used in the filter. If that is different in the given sensor data, it can be changed too. Normally, when using the magnetic data, a referencial magnetic field must be given. This filter computes the local magnetic field in Munich, Germany, but it can also be set to a different reference with the parameter . If the full referencial vector is not available, the magnetic dip angle, in degrees, can be also used. The initial quaternion is estimated with the first observations of the tri-axial accelerometers and magnetometers, but it can also be given directly in the parameter . Measurement noise variances must be set from each sensor, so that the Process and Measurement Covariance Matrix can be built. They are set in an array equal to for the gyroscope, accelerometer and magnetometer, respectively. If a different set of noise variances is used, they can be set with the parameter : or the individual variances can be set separately too: This class can also differentiate between NED and ENU frames. By default it estimates the orientations using the NED frame, but ENU is used if set in its parameter:\n• None gyr (numpy.ndarray, default: None) – N-by-3 array with measurements of angular velocity in rad/s\n• None acc (numpy.ndarray, default: None) – N-by-3 array with measurements of acceleration in in m/s^2\n• None mag (numpy.ndarray, default: None) – N-by-3 array with measurements of magnetic field in nT\n• None frame (str, default: ‘NED’) – Local tangent plane coordinate frame. Valid options are right-handed for North-East-Down and for East-North-Up.\n• None noises (numpy.ndarray) – List of noise variances for each type of sensor. Default values: .\n• None Dt (float, default: 0.01) – Sampling step in seconds. Inverse of sampling frequency. NOT required if value is given. Given a vector \\(\\mathbf{x}\\in\\mathbb{R}^3\\), return a \\(4\\times 4\\) matrix of the form: This operator is constantly used at different steps of the EKF. If only the gravitational acceleration is used to correct the estimation, a \\(3\\times 4\\) matrix: If the gravitational acceleration and the geomagnetic field are used, then a \\(6\\times 4\\) matrix is used: If is equal to , the computation is carried out as: The refactored mode might lead to slightly different results as it employs more and different operations than the normal mode, originated by the nummerical capabilities of the host system.\n• None mode (str, default: ) – Computation mode for Observation matrix. Options are: , or . If only the gravitational acceleration is used to correct the estimation, a vector with 3 elements is used: If the gravitational acceleration and the geomagnetic field are used, then a vector with 6 elements is used: Perform an update of the state.\n• None dt (float, default: None) – Time step, in seconds, between consecutive Quaternions."
    },
    {
        "link": "https://medium.com/@satya15july_11937/sensor-fusion-with-kalman-filter-c648d6ec2ec2",
        "document": "Autonomous cars are equipped with multiple sensors like Camera, Radar, Lidar etc.Here are some data on their performance for different task and weather condition:\n\nAs you can see in the above figure,all sensors has some advantages and disadvantages.But, If you fuse the output of different sensors,then they will never fail under any weather condition.\n\nLet’s say, we want to measure the distance(depth) of different obstacles from a given car.\n• Distance can be measured using Cameras with Stereo setup.(Please check my article more info https://medium.com/@satya15july_11937/depth-estimation-from-stereo-images-using-deep-learning-314952b8eaf9)\n• Distance can also be measured using Lidar.\n\nAs mentioned in Fig.1,if we can fuse the output from Stereo Cameras and Lidar, then our system can predict distance/depth robustly in any condition. Let’s say this is our problem and we want to fuse the output of both the sensors and predict the distance or depth.\n\nThere are many ways to fuse the output of different sensors, but here we will discuss about how to fuse them with Kalman Filter.\n\nThe basic idea of the Kalman filter is to use a model of the system being measured, and to update the model as new measurements become available. The filter works by making a prediction of the current state of the system based on the previous state estimate and the system model, and then combining this prediction with a new measurement to obtain an updated state estimate.\n• When are you predicting some state that means you are referring to probability distribution.\n• Probability distribution has a Mean and a Variance or Co-Variance.\n• It’s an iterative process and tries to predict the state at each step and update/rectify the state based on the measurement data from the sensors.\n• With the below formula, Kalman filter tries to predict and update the state (X`/X) and uncertainty(P`/P).\n\nKalman Filter is used in many areas like\n\nKalman filters are of following types:\n\nKalman filter predicts the uncertainty(which is variance ) and that’s why it’s expressed in terms of probability distribution, which in this case is Gaussian.\n\nKalman filter uses the below formula for prediction\n\nThe above formula expect the Posterior to be Gaussian,this is possible only when the following properties for multiplication and addition are satisfied.\n\nPosterior can be Non-gaussian, if either prior or likelihood(measurement from sensor) method used are Non-Linear.In that case, you can use\n• Unscented Kalman Filter(UKF)\n\n- Linearize the problem by using Sigma Points and transformed method\n• Extended Kalman Filter(EKF)\n\n- Linearize the problem by using Jacobians and Taylor expansion.\n\nHere are some non-linear examples :\n\nLinear Kalman filter is the foundation before jumping to UKF and EKF.Let’s focus on Linear Kalman Filter in this article.\n\nLinear Filter can be designed with the following steps:\n\nIn Kalman filter, the control function is used to incorporate the effect of a control input into the state prediction equation. The control input is a known signal that is used to influence the evolution of the system, and it can be used to improve the accuracy of the state estimate.\n\nThe control function is typically represented by a matrix B, which maps the control input to the change in the state vector and u maps to control input\n\nFor example, robot sends steering and velocity signals based on its current position vs desired position.So in that case, control input u and Control function B need to be defined. If there is no information on input control,then set B and u to 0.\n\nThe control function is particularly useful in situations where the system can be influenced by a known control input, such as in robotics or aircraft control. By incorporating the control function into the state prediction equation, the Kalman filter can take advantage of the known influence of the control input to improve the accuracy of the state estimate.\n\nIn Kalman filter, the prediction step is the first step of the recursive process, which uses the previous state estimate and its covariance to predict the current state estimate and its covariance. The prediction step consists of two sub-steps:\n\nAfter the prediction step, the Kalman filter uses the predicted state estimate and its covariance to perform the measurement update step, which incorporates the new measurement into the state estimate and updates its covariance and state.\n\nKalman filter is used for object tracking in many computer vision application.I used it in Driver Monitoring system(DMS) in order to improve the FPS on low powered device.\n\nObject detection(i.e. ROI prediction) is a linear problem and Let’s discuss how to implement it.\n\nB and u is set to 0 as this is invalid for this given scenario.\n\nPlease check the full code which I shared @ https://github.com/satya15july/linear_kalman_filter.\n\nIn the above video, you can see that Kalman filter prediction matches with the object detector.But in the below video, Kalman filter prediction does not match with object detector initially as the shape of car changes from frame to frame.But prediction stabilizes when the shape stop fluctuating.\n\nDoes this fluctuation in prediction mean that we can not use Kalman filter for object tracking/Object detection?\n\nLet’s take the use case of Face detection/tracking in Driver Monitoring System(DMS),\n• Normally, driver focuses on the road and it does not move frequently.\n• So in that case, we can use Kalman filter for Face detection with every alternate frame and we can uses some intelligence so that we use it more for Face Tracking/Face Detection.\n• If you can do this, then your Driver Monitoring system performance can be improved a lot.Isn’t it?\n\nHere is statistics (which i have collected from my implementation)\n\nI think if you closely follow this article, you can understand how to Fuse different sensors using Kalman filter.I will share the code with explanation in my next article.\n\nHere I gave an overview of what is Kalman filter and how to use linear kalman filter for object tracking.But the world is full of non-linear problems.Problem such as Robot Localization(SLAM), tracking plane/missile are non-linear problems and they can not be solved through linear Kalman filter. It can be solved using UKF or EKF."
    },
    {
        "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7570822",
        "document": "The use of Unmanned Aerial Vehicles (UAVs) has spread to numerous disciplines and will continue to expand in the foreseeable future, as more affordable sensors, actuators, and processing units become available, together with the advances in their legislation around the globe. Nowadays, UAVs are used in several applications in the scientific, civil, and commercial fields; for instance, in precision agriculture, weather monitoring, search and rescue, mining, remote sensing, and delivery of goods [1,2]. In this research, we focus on the use of UAVs for the delivery of payloads. A large number of applications make use of UAVs to deliver food, merchandise, medical supplies, or rescue equipment, among others. When natural disasters occur, such as earthquakes, landslides, blizzards, wildfires, or floods, affecting the terrestrial infrastructure, UAVs can promptly deliver emergency supplies. Additionally, UAVs provide a solution in situations where the rugged topography of a region inhibits fast transportation. For example, the company Zipline is delivering daily blood and blood products to health centers in the regions around the countries of Rwanda and Ghana, in Africa [3]. Given the coronavirus disease 2019 (COVID-19) pandemic, Zipline is also distributing medical supplies and personal protective equipment, and recently pursuing the possibility to provide COVID-19 supplies in the USA [4]. Several applications of UAVs rely on vehicles instrumented with expensive and delicate sensors. For instance, in remote sensing scenarios, it is common the use of Light Detection and Ranging (LiDAR); optical, multispectral, hyperspectral, and thermal cameras; radar; global navigation satellite system (GNSS) receivers; and processing units [5]. In the case of a vehicle malfunction, these instruments could be damaged given a collision with the ground, or lost in the event of loss of flight control or communication, causing landing in an unknown or unreachable location. In these situations, a safe-landing feature is most needed to recover the vehicle and its equipment. Precision Aerial Delivery Systems (PADSs) make use of guidance, navigation, and control, to maneuver parafoil-payload systems towards safe-landing zones, increasing the chances of successfully delivering payloads on target, and diminishing the possible damage while landing. A typical descent flight of a PADS is illustrated in Figure 1. The trajectory is composed of three segments: an initial flight towards the intended landing point (homing); a lemniscate (figure-eight) or circular pattern while losing altitude (energy management); and finally, a narrow turn to reach the target facing into the wind (landing). In this illustration three vehicles are depicted to represent different application scenarios, for example, a cargo aircraft for the delivery of goods, or as a recovery system for a weather balloon or a multicopter. Design, modeling, and characterization of PADSs have been taking place since the beginning of the 1970s. Static and dynamic longitudinal stability was first studied by Goodrick, together with the early development of the equations of motion governing parafoil-payload systems, considering 3- and 6-degrees of freedom (DOF) [6,7], and the characterization of a 150 kg test payload. In the following years, the Small Autonomous Parafoil Landing Experiment (ALEX I and II), described in detail by Jann [8,9], set a milestone on modeling, validation, and verification of PADSs using 3- and 4-DOF, employing a ram-air parachute with a payload of 100 kg. Different models have been developed for a varying range of DOF, either considering the parafoil-payload system as a rigid body or allowing relative motion between their components [10,11,12,13]. Comparisons between models happen to be a complex task, since different considerations, payload masses, parafoil aerodynamic properties, and general assumptions are used by different authors. To be able to compare the effectiveness between models ranging from 6- to 9-DOF, Gorman and Slegers [14,15] developed a parafoil model using the same aerodynamic properties for each scenario, realizing different DOF by modifying the kinematics constraints between the parafoil and the payload, under the same control inputs and initial conditions. The 6-DOF model represents the three inertial position components of the joint connecting the parafoil and the payload, as well as three Euler orientation angles. The 7-, 8-, and 9-DOF models incorporate extra Euler orientation angles for the payload, depending on the connection constraints. The 7-DOF model allows for yaw relative motion, the 8-DOF model allows yaw and pitch, whereas the 9-DOF model allows yaw, pitch, and roll relative motion between the parafoil and the payload. By contrasting experimental data with the models, Gorman and Slegers were able to conclude that the 6-DOF trajectory differed from the 7-, 8-, and 9-DOF trajectories mainly because of the great yaw relative motion (parafoil-payload twist). Since the relative roll and pitch motion is negligible in comparison to the relative yaw motion, the 7-DOF represents the model with the minimum DOF that captures the most significant flight dynamics, while the 6-DOF successfully characterizes the parafoil-payload system as a rigid body, presenting lower complexity than the rest of the proposed models. Since the PADS analyzed in this investigation is considered a rigid body, a 6-DOF model is adopted. Due to the versatility of these delivery systems, and their possible applications in military scenarios, different sectors of the US Army teamed to form the Joint Precision Airdrop System (JPADS), categorizing their systems by weight as micro-light (4.5 kg to 68 kg), ultra-light (113 kg to 318 kg), extra-light (318 kg to 998 kg), light (2268 kg to 4536 kg) and medium (6804 kg to 19,051 kg) [16,17]. Initially, one of the JPADS requirements was to achieve $13.22 USD per kilogram of payload, a goal only met for the heavy categories, but far from being met for the light categories (micro-, ultra-, and extra-light) [18]. Making more affordable systems has been a priority, even at the expense of reducing their delivery precision. The Affordable Guided Airdrop System (AGAS), formed in 1999 [19], has the goal to minimize as much as possible the cost of delivery systems, obtaining the best achievable accuracy within a threshold of 100 m circular error probability (CEP). The AGAS initiative has been focusing on the extra-light and lightweight ranges, where mainly 900 kg payload implementations have been tested using circular parachutes, achieving 211 m CEP when 12 hour-old wind forecast information was used, and 38 m CEP when near real-time wind profiles were used [20]. Lighter systems have been able to achieve up to 10 m in specific cases [18]. Reduction in the actual landing distance to the intended point of impact, which translates to small CEP, has been achieved by the development of optimal control and guidance [21,22], strongly dependent on high-quality information about the wind profiles during the entire flight, from the beginning of the descent up to touchdown, and the capability to account for variable winds [23]. Nevertheless, achieving such high precision comes with a price, requiring computational power and elevated costs on sensors, actuators, and parachutes. Although the 5 to 19,000 kg weight-range has been widely analyzed and tested, delivery systems for lighter applications have not been thoroughly explored, especially for low-cost solutions. Lightweight applications of PADSs face the challenge of relying on light hardware to perform guidance, navigation, and control. Current parafoil-payload systems make use of high-end processing units, capable of handling sophisticated real-time implementations of highly nonlinear dynamical models to estimate their attitude and position to operate their guidance system. If low-cost miniaturized versions of these delivery systems are to be explored, simplified models that capture their flight dynamics need to be developed. Additionally, estimation schemes for their position and attitude need to be taken into account, especially for further guidance and control implementations on vehicles with limited sensors, actuators, and processing capabilities. The state estimation process based on the fusion of dynamic models and measurements obtained from different sensors is commonly and efficiently performed through a model-based Kalman filter. Whether a total state-space formulation of a Kalman filter is used; an error state-space, either with a feedforward or feedback implementation; an extended Kalman filter; an unscented Kalman filter; or any of modern developed variants, a Kalman filter is one of the most used algorithms for sensor fusion, particularly in navigation applications [24,25,26,27]. The type of Kalman filter implementation strongly depends on the intended application, the dynamics of the process to estimate, the required accuracy, and the computational capabilities at disposal. According to a study performed by Zhang et al. [28], in which the performance of a Kalman filter, an extended Kalman filter, an unscented Kalman filter, and variations of these types of filters were compared for inertial navigation systems, the best accuracy is obtained by the unscented Kalman filter for their experiments. However, the unscented Kalman filter is the algorithm that demands the most computational effort among their comparison. On the other hand, the computational time required for the Kalman filter (not extended nor unscented) showed to be the lowest, 3 to 10 times smaller than the extended or unscented Kalman filter implementation, at the cost of having six to eight times smaller accuracy in the estimates. Following the current development needs of PADSs, particularly for the micro-lightweight category, as well as the goals established by the AGAS program, in which the improvements of low-cost systems are prioritized even at the expense of decreasing the landing accuracy, a total state-space formulation of a Kalman filter is adopted for the estimation scheme proposed in this investigation. This type of implementation enables the use of low-cost, small, and lightweight sensors and processing units, requiring a linear model that represents the flight dynamics of the system. This research presents the development, implementation, and comparison of a sensor fusion algorithm and estimation scheme for the position and attitude of PADSs, employing a Kalman filter based on two proposed 6-DOF dynamic models, suitable for lightweight low-cost applications. Section 2 details the proposed estimation scheme. The dynamic models that characterize the flight of the parafoil-payload system are presented in Section 3, firstly introducing a nonlinear 6-DOF dynamic model, and then the development of two linear alternatives: a linearized version of the nonlinear 6-DOF model, and a double integrator model. The required sensors and their characteristics suitable to perform the position and attitude estimation are presented in Section 4, whereas Section 5 presents the Kalman filter algorithm that incorporates the dynamic models and measurements from different sensors. The performance of the proposed estimation scheme is evaluated through simulations in Section 6, followed by the discussion of the results in Section 7. Finally, the conclusions of the proposed estimation scheme are outlined and the considered future work is depicted.\n\nTo validate the accuracy of the models proposed in this research, and for comparison purposes, the 6-DOF model developed and tested by Ward et al. [29] is adopted as reference (henceforth denoted as the reference model). This model is based on the system identification of a series of flight tests, using a micro-lightweight PADS with a total mass of kg and a canopy with a wingspan of m, fitting the objective of this investigation. It captures the nonlinearities of the flight dynamics of the parafoil-payload system, therefore, it will serve as a reference to evaluate the performance of the linear models developed in this study. The equations of motion of the reference model are derived from Newtonian mechanics, considering the parafoil-payload system as a rigid body, i.e., without relative motion between the parafoil and the payload. These equations are described in the body reference frame with a North-East-Down (NED) coordinate system, accounting for 6-DOF corresponding to the linear ( ) and angular ( ) velocity vectors, with components and , respectively. The dynamic equations are obtained by relating the time derivative of the linear and angular momentum to the sum of forces and moments, about the center of gravity in the body reference frame. The forces under consideration include the aerodynamic forces that act on each element of the canopy . The canopy is discretized in seven elements, allowing for brake deflections only in the outermost elements. The deflection of the left and right brakes provide steerability to the system, by changing the lift and drag coefficients of the corresponding element of the canopy. These deflections can take any value from to 1 (dimensionless). Additionally, the aerodynamic forces acting on the payload , and the weight of the parafoil-payload system are included. Finally, apparent mass forces and moments, caused by the acceleration of the fluid through which the vehicle moves, are considered according to [30]. Since the dynamic equations are obtained in a rotating reference frame (non-inertial), fictitious forces ( ) and moments ( ) emerge in the equations for the change of linear and angular momentum. For a detailed explanation of the computation of each term, as well as the system identification for the parafoil-payload system, refer to the full articles [29,31]. The resulting dynamic equations of the reference model [29] (p. 591, Equations (16)–(18)), are presented in compact form in Equation (1), representing the change of the state vector as the change in linear and angular velocity in the body reference frame , where the matrix incorporates the geometry, mass, and inertial properties of the parafoil-payload system of the reference model. While the forces and moments acting on the system are functions of the state variables in , the aerodynamic forces and moments of the parafoil and payload, , , , and , are also functions of an external wind vector . For the purpose of the development and simulation of the estimation scheme proposed in this research, the three components of the external wind vector are assumed to be constant and equal to zero throughout the descent trajectory. By assigning initial conditions and control inputs Equation (1) can be solved, and the resulting components can be transformed to the inertial reference frame by rotating according to the Euler orientation angles. Instead of adopting the start of the descent trajectory as the origin of the inertial z component, this is translated to the intended point of landing to better represent visually the followed trajectory of the PADS. Figure 3 presents the trajectory under study, constructed by implementing the reference model executing the three maneuvers described in Table 1 as constant control inputs. The control inputs remain constant during the flight until the next maneuver is executed. This trajectory will serve as a reference for the analysis of the linear models presented in the following sections of this work. A video showing the reference trajectory flight is available as Supplementary Materials. Descent trajectory under study, followed by a Precision Aerial Delivery System (PADS) navigating towards a landing target (parafoil-payload system not to scale). The trajectory corresponds to the implementation of the 6-degrees of freedom (DOF) nonlinear reference model. The linear approximation of the equations of motion is computed as the first-order Taylor series, evaluated around stable points of the states and inputs ( ). For the multivariable case under study , each of the forces and moments that the system experiences can be expressed as a power series representation in the form: where represents higher-order terms of the Taylor series, neglected for the linearization. Since no wind, nor other perturbations are considered, fixing the control inputs in the reference model to a given deflection brake value ( ), result in stable states after a stabilization period of approximately 10 s, depending on the magnitude of the maneuver. Thus, for each of the three segments of constant inputs on the flight trajectory, a set of stable points is determined from the reference model as the resulting state vector. The sable points obtained for each maneuver segment, applying the deflection brakes reported in Table 1 as stable control inputs and , are listed in Table 2. Applying this linearization method to Equation (1) and grouping common terms, results in the equation of motion of the linear 6-DOF model: where and denote the forces and moments evaluated at the stable points. In addition, and represent the corresponding Jacobian matrices. For convenience, the linearized equation of motion is presented in a state-space representation: Notice that matrices A and B are constant as long as the control inputs remain fixed, considerably simplifying the number of computations to be performed during flight. Similarly, all components of the vector are constant under the same condition, except for the components of the force produced by the weight of the vehicle ( ), which depend on its attitude. Matrices A and B, as well as vector , evaluated at the stable points for each maneuver segment, are given in Appendix A. The complete description of the position and attitude of the PADS in the inertial reference frame, as well as the linear and angular velocity in the body reference frame, can be expressed in a concise form by combining the kinematic and dynamic equations of motion in a single state-space representation: where matrices A and B, as well as the vector are distributed in components , , and respectively, to accommodate an adequate state-space representation. Additionally, and are the rotation matrices that transform the linear and angular velocity components from the body to the inertial reference frame: where the shorthand notation , , and denotes , , and , respectively. Rewriting Equation (6) leads to a compact equation, where the superscript is used to indicate that an element corresponds to the linearized model: While Equation (9) represents a time-continuous model, information from the onboard sensors arrive at discrete times. Solving the differential equation for the state vector using Euler’s method, with time step , a time-discrete model is obtained as where I represents the identity matrix. It is relevant to observe that Equation (10) describes 12 components in total, three for each vector: the position vector and the angular position vector , expressed in the inertial reference frame, with the latter corresponding to the Euler orientation angles (roll, pitch, yaw); and the linear velocity vector and angular velocity vector , expressed in the body reference frame. The development of an alternative linear model is achieved by exploiting the properties of the flight dynamics of PADSs. Since these are vehicles navigating in an underactuated controlled descent flight, typically without propulsion, and subject only to variations in the wind profiles, the flight dynamics are normally smooth, i.e., without sudden changes in the state variables of the parafoil-payload system, except for the voluntary control inputs exerted on the vehicle. Taking this into consideration, a double integrator model is proposed, based on the assumption that between consecutive measurements of the sensors acquiring information regarding the position and attitude of the vehicle, the linear and angular accelerations remain constant for seconds. Integrating twice these accelerations with respect to time leads to the following equations of motion for the position and attitude, in the inertial reference frame: which can be expressed in compact form as where the superscript D is used to indicate that an element corresponds to the double integrator model. Notice that Equation (14) also describes 12 components in total, corresponding to the position, linear velocity, angular position, and angular velocity vectors, in the inertial reference frame. The position and angular position vectors can be directly compared with the analogous components of the nonlinear and linearized models. Contrarily, the linear and angular velocity vectors obtained from the double integrator model cannot be directly compared with the similar components from the nonlinear or linearized models, since they are expressed in different reference frames. To be able to compare them, the linear and angular velocity vectors obtained from the double integrator model are transformed to the body reference frame with the rotation matrices and , respectively.\n\nThe hardware under consideration must comply with the requirements of a lightweight mission, where the allocated volume and weight for the low-cost sensors are very limited. Since this estimation scheme is intended to be used in low-cost applications. To evaluate the performance of the estimation scheme, simulated measurements are generated at 5 Hz sample rate, based on the state variables of the reference model and the statistical characteristics of the sensors acquiring this information in practice. This characterization is obtained through experimentation with real flight data from similar vehicles. Specifically, measurements from a GNSS receiver, a magnetometer, a gyroscope, and an accelerometer, are simulated by corrupting the position, angular position, angular velocity, and linear acceleration, of the reference model with white Gaussian noise, according to the statistical parameters reported in Table 3. The obtained deviation values from experimentation are similar in comparison to the noise and bias values reported in similar experiments. For example, Slegers and Yakimenko [32] present a bias in global positioning system (GPS) measurements of 2 m, plus noise of m; 2 deg bias and 1 deg noise for the angular position; and finally, 1 deg/s bias and 1 deg/s noise for the angular velocity. In their study, they also draw upon a linearization process of the flight dynamics with a sample rate of 2 Hz, assuming constant aerodynamic velocity. Cacan et al. [23] report 4 Hz sample rate for their guidance, navigation, and control algorithm, of a micro-lightweight PADS. Ward et al. [31] report noise with a standard deviation of 2 m in positioning measurements, and a standard deviation of 10 deg and 2 deg for the heading measurement bias and noise respectively, using 4 Hz sampling rate. The sampling rate of 5 Hz for our investigation was chosen according to the processing capabilities of typical low-end microprocessors and sensors. For example, low-cost GNSS receivers as the u-blox NEO-6M or NEO-M8N can provide navigation information up to 1 Hz to 5 Hz, limiting the measurement availability [33,34]. Typical performance of processing units for this lightweight and low-cost application range from a microprocessor with 8-bit, 20 MHz capabilities, as the ATmega328 chip [35]; 32-bit, 216 MHz as the STM32F765 chip [36]; up to 64-bit, 1.5 GHz as the BCM2711 chip [37].\n\nTypically, PADSs are equipped with a GNSS receiver and an IMU, that provide information at discrete times about the position and attitude of the vehicle during its flight. The objective of any guidance, navigation, and control scheme is to process the incoming information from the sensors, in order to estimate the state variables of the system, to plan the most suitable trajectory towards the landing target while managing the energy budget. Given a linear discrete-time model for the state vector and a measurement vector : with process noise and measurement noise , both assumed to be white, uncorrelated, zero-mean, and normally-distributed, the Kalman filter provides the best estimate of the states . In the following, the implementation of a model-based Kalman filter is described using a total state-space formulation [38]. Initially, an a priori state estimate and error covariance estimate are obtained by propagating the a posteriori estimate from epoch to k: where Q represent the process noise covariance matrix, estimated using the autocorrelation function , with and for each model, determined by tuning the filter during its implementation. Note that Equation (17) has the same form as the state vector obtained in Equation (10) for the linearized model, as well as in Equation (14) for the double integrator model, developed in the previous sections. Next, the a posteriori state estimate is computed as a linear combination of the a priori estimate , and the difference between the measurement vector and the prediction , weighted by the Kalman gain , designed to minimize the a posteriori error covariance : where H denotes the observation matrix, R represents the measurement noise covariance matrix, and I corresponds to the identity matrix. The simulated measurements are incorporated into the Kalman filter as the measurement vector , while the squared of the standard deviations of the characterized sensors are used as the elements of the diagonal measurement noise covariance matrix R. The observation matrix H communicates the availability of the measurements, relating them with the state vector. The implementation of the filter using the linearized model is realized by executing the constant brake deflections described in Table 1 as control inputs, fusing the simulated measurements obtained by the sensors. On the other hand, the double integrator model requires linear and angular accelerations as control inputs, which typically can be deduced from measurements employing onboard sensors. These accelerations are also simulated based on the reference model, by transforming the linear and angular acceleration from the body reference frame to the inertial reference frame, and adding white Gaussian noise with standard deviations of / and 2 deg/ respectively, according to experimental data.\n\nTwo separate implementations of the described discrete Kalman filter scheme were performed: one using the linearized model and brake deflections as control inputs, and another using the double integrator model and the simulated accelerations as control inputs. Both implementations were performed using the same initial conditions and simulated measurements. The Kalman filter estimates based on the proposed linearized model ( ) and the double integrator model are presented in Figure 4, Figure 5, Figure 6 and Figure 7, together with the reference model for comparison purposes. The different maneuver segments are presented in all figures with vertical dashed lines at the time of execution, with the maneuver identifier displayed at the bottom to facilitate the interpretation. Kalman filter inertial position estimates based on the proposed linearized model and the double integrator model , together with the components of the inertial position vector from the reference model . Kalman filter angular position estimates based on the proposed linearized model and the double integrator model , together with the components of the angular position vector from the reference model . These components are expressed in the inertial reference frame, corresponding to the Euler orientation angles roll, pitch, and yaw, respectively. Kalman filter linear velocity estimates based on the proposed linearized model and the double integrator model , together with the components of the linear velocity vector from the reference model . These components are expressed in the body reference frame. Kalman filter angular velocity estimates based on the proposed linearized model and the double integrator model , together with the components of the angular velocity vector from the reference model . These components are expressed in the body reference frame. From Figure 4, it can be observed that the Kalman filtering scheme based on each of the proposed models was capable of reproducing the components of the inertial position of the vehicle, for any of the flight scenarios corresponding to the maneuver segments. Due to the smooth evolution of each of the components of the inertial position, both the linearized model and the double integrator model were suitable for emulating the behavior of the position of the nonlinear reference model. This mild progression of the inertial position of the PADS closely represented a real flight scenario. From the beginning of the descent trajectory, after the full inflation of the canopy and stabilization of the gliding flight, the only external perturbation experienced by the parafoil-payload system originated from the wind. While strong wind profiles could substantially affect the flight dynamics of small vehicles, a fuselage designed to reduce drag could be employed to mitigate this effect. Additionally, the heavier the payload, the less prone it was to suffer abrupt changes in position due to its inertia. The components of the linear velocity, presented in Figure 6, demonstrate that the lateral component of the velocity (v) was virtually zero along the complete descent trajectory. This reflects the fact that there were no perturbations that modify the lateral movement of the vehicle aside from the control inputs, and that the flight was dominated by its longitudinal dynamics. In the three linear velocity components, it was distinctly recognizable that the estimations based on the double integrator model strongly varied around the reference model, in comparison with the estimates from the linearized model. While the bounded variation confirmed that the Kalman filter estimation scheme converged towards the reference model, it was an indication that the Kalman gain was favoring the measurements instead of the model. Note that along the descent trajectory, the deflection brakes applied on each maneuver were held constant until the next maneuver was reached, or the vehicle landed. Furthermore, the maneuvers were applied instantaneously, as step functions. This means that the delayed response of the actuators, and the elasticity of the lines used to apply the deflection brakes were not taken into account, which would modify the transient response of the state variables. Despite the condition analyzed in this investigation where the brake deflections were applied infinitely fast, the estimation process based on the proposed linearized model was able to follow the dynamics of the reference model, suggesting that for smoother executions of the maneuvers, the performance only could improve. In real applications, the maneuvers cannot be applied instantaneously, but gradually, leading to a smoother response on the state variables. The consequence of this abrupt change in the maneuver segments can be observed, for example, in the angular velocity components in Figure 7, especially when the last maneuver is applied. While Figure 4, Figure 5, Figure 6 and Figure 7 provide valuable information about the 12 components of the state vector estimated by virtue of the proposed Kalman filter scheme, it is difficult to appreciate the shape and magnitude of the error with respect to the reference model. The error was computed as the difference between the reference model and the Kalman filter state estimates based on the linearized model and the double integrator model , respectively. This error is denoted as and for each implementation. To obtain a better understanding of how closely the estimation scheme followed the reference model for the inertial position, the error in the magnitude of the position vector obtained with the reference model and the magnitude of the Kalman filter position vector estimates are presented in Figure 8, denoted as . It can be appreciated that both models were capable of representing the tridimensional position of the vehicle during the different flight segments since in general, the error oscillated around zero. In contrast, the filter estimates based on the linearized model presented a smoother variation, with an error magnitude approximately two times smaller. Error in the magnitude of the inertial position vector between the reference model and the Kalman filter estimates from the linearized model and the double integrator model . To evaluate the performance in estimating the attitude of the PADS, Figure 9 presents the error between the Euler angles of the reference model and the Kalman filter estimates using each of the proposed models. Notice that during the first and second maneuver segments ( and ), corresponding to a straight flight and a wide turn, the two models captured the same behavior as the reference model. Nevertheless, the double integrator model presented a larger and consistent error for roll and pitch angles, and a smaller but still noticeable constant error for the yaw angle , after the third maneuver was applied ( ), where the flight trajectory with a narrower turn was followed. This is a consequence of the higher rate of change in the angular position that the vehicle experienced during the last flight segment, where a −0.4 right brake deflection was being applied. The double integrator model relied on the assumption that the parafoil-payload system experienced epoch-wise constant inertial acceleration during the integration period. Consequently, the higher the body-fixed angular velocity, the less this assumption was fulfilled, as can be verified from the error in the angular velocity components presented in Figure 10. Error in the inertial angular position components between the reference model and the Kalman filter estimates from the linearized model ( ) and the double integrator model ( ). Error in the angular velocity components, expressed in the body reference frame, between the reference model and the Kalman filter estimates from the linearized model ( ) and the double integrator model ( ). The achievable turn rate for any PADS has a practical limit, since a steep turn rate could induce spiral divergence. For a vehicle with similar characteristics as the simulated in this study, Ward et al. [39,40] report maximum turn rates from 15 deg/s up to 25 deg/s, depending on the flight mode and control scheme. For the case of larger parafoil-payload vehicles, Lingard [11] reports a maximum constant turn rate of deg/s for a canopy with 30 m of wingspan.\n\nIt is relevant to highlight some properties and prerequisites for the operation and deduction of the linearized model and the double integrator model. The linearized model requires the calculation of the Jacobian matrices, increasing the complexity of its deduction depending on the forces and moments representing the interaction between the vehicle and its surroundings. This requires the characterization of the parafoil-payload system, limiting the flexibility of this model to be applied in any other vehicle with different properties. Moreover, the stable points need to be computed prior to the descent trajectory. While this demands additional effort, it is important to note that this task is performed on the ground, where typically more computational resources are available, without increasing the real-time computational burden during the flight. Since the flight conditions variate from flight to flight, it is required to calculate a set of stable points assuming the planned maneuvers to reach the target. If the computed stable points differ greatly from the actual conditions during flight, the performance would deteriorate. In contrast, the double integrator model does not depend on the physical properties of the parafoil-payload system, simplifying the implementation of this model on vehicles with different characteristics. As a consequence, this model does not directly relate the brake deflections to the state variables, making it not suitable for a straightforward application of a control scheme for guidance. While the implementation of this model is much simpler than the linearized model, without requiring the computation of the stable points or other parameters before the flight, it heavily depends on the sample rate and quality of the measurements provided by the sensors. The selected 5 Hz sample rate for the measurements is a conservative choice that complies with the capacities of any relatively modern low-cost processing unit and sensors for real-time applications. The higher the capabilities of the sensors and the processing unit, the larger the achievable sample rate during flight. This might be desirable when fast maneuvers are required or strong wind profiles modify the trajectory of the vehicle, although this implies higher costs associated with instrumentation, energy storage, and a heavier payload, sometimes not feasible for micro-lightweight PADSs. In the case that exogenous forces are present during the flight as a consequence of strong winds, which are not incorporated into the flight dynamic models as the external wind vector , it is expected that the predictions provided by the models would deteriorate. Another source of perturbations could be the effect of mismodeled interactions in the parafoil-payload system, due to the relative motion of its components. Nevertheless, the overall performance of the proposed estimation scheme would not be necessarily deteriorated proportionally to the magnitude of the perturbations, since the Kalman gain would favor the measurements from the sensors or the predictions of the dynamic models, attempting to compensate for these effects by minimizing the a posteriori error covariance."
    },
    {
        "link": "https://sciencedirect.com/science/article/pii/S0924271623002289",
        "document": ""
    },
    {
        "link": "http://arxiv.org/pdf/1708.05375",
        "document": ""
    },
    {
        "link": "https://openaccess.thecvf.com/content_ICCV_2017/papers/Ji_SurfaceNet_An_End-To-End_ICCV_2017_paper.pdf",
        "document": ""
    },
    {
        "link": "https://researchgate.net/publication/382658823_Real_time_3D_object_reconstruction_using_Multi-View_Stereo_MVS_Networks",
        "document": "Let I_i be the input image from the i-th viewpoint, where i = 1, 2, ..., N.\n\nEach image I_i is a 2D array of pixel values I_i(x, y), where (x, y) represents a pixel\n\nThe goal is t o estimate t he depth or disparity map D_ i(x, y) for each image, which represents\n\nthe distance or disparity of each pixel from the camera.\n\ntakes the input image I_i and produces the estimated depth map D_i .\n\nnovel views of the object by combining the information from multiple viewpoints.\n\nGiven a t arget viewpoint j, t he t ask is to synthesize an image I_j' as if it was captured from\n\nTo achieve this, the depth maps D_i are used to determine the visibility of e ach pixel in the\n\nFor each p ixel (x, y) in t he target view, the algorithm finds t he corresponding so urce view(s)\n\n(i.e., the viewpoint(s) where this pixel is visible) based on the depth maps.\n\nThe color value of the synthesized image I _j'(x, y) is t hen obta ined by warp ing and blending\n\nthe corresponding pixels from the source views.\n\nThe m athematical f ormulations f or MVS Networks inv olve the specific architectures an d\n\ntechniques employed i n t he dept h est imation and view s ynthesis st ages[19][20][21]. These\n\narchitectures typically utilize co nvolutional layers, pooling layers, and other components o f\n\nmathematical details depend on the specific network design and optimization a lgorithms used\n\nDuring training, MVS Networks are t ypically optimized using loss functions that measure the\n\ndiscrepancy between the est imated dept h maps and ground t ruth depth maps, as well as t he\n\nquality of the synthesized views co mpared to t he actual views. The networ k parameters are\n\nadjusted iterat ively through backpropagation and gradient-based o ptimization t echniques to\n\nminimize the loss and improve the accuracy of depth estimation and view syn thesis[22][23].\n\nIt's important to note that t he specific mathematical for mulations a nd networ k architectures\n\nfor MVS Networks can vary depending o n the res earch and implementation[24][25].\n\nResearchers cont inue t o explore and develop new techniques to en hance the accuracy and\n\nAfter obtaining the depth maps D_i for each image, the view synthesis process generates"
    },
    {
        "link": "https://nature.com/articles/s41598-024-55612-6",
        "document": "The basic principle of multi-view stereo (MVS) is to perform 3D reconstruction by extracting depth information from multiple views. Most current SOTA MVS networks are based on Vision Transformer, which usually means expensive computational complexity. To reduce computational complexity and improve depth map accuracy, we propose a MVS network with Bidirectional Semantic Information (BSI-MVS). Firstly, we design a Multi-Level Spatial Pyramid module to generate multiple layers of feature map for extracting multi-scale information. Then we propose a 2D Bidirectional-LSTM module to capture bidirectional semantic information at different time steps in the horizontal and vertical directions, which contains abundant depth information. Finally, cost volumes are built based on various levels of feature maps to optimize the final depth map. We experiment on the DTU and BlendedMVS datasets. The result shows that our network, in terms of overall metrics, surpasses TransMVSNet, CasMVSNet, CVP-MVSNet, and AACVP-MVSNet respectively by 17.84%, 36.42%, 14.96%, and 4.86%, which also shows a noticeable performance enhancement in objective metrics and visualizations.\n\nMVS technology facilitates a profound interaction between the digital and real worlds through 3D reconstruction. Traditional methods for 3D reconstruction1,2 based on geometric shapes can be categorized into various approaches depending on the inclusion of prior conditions. These approaches include contour-based methods, focused area methods, motion-based methods, and others. Traditional 3D reconstruction methods based on visual geometry primarily utilize the 3D geometric information present in 2D images as prior knowledge. This technique significantly restores the 3D scene without additional conditions. By leveraging the inherent geometric cues captured in the 2D images, these methods can deduce or derive the intrinsic 3D structure of the scene. However, traditional MVS methods, based on an ideal Lambertian scene and strict geometric relationships, may encounter challenges in reconstructing complex geometries or texture-free areas, leading to holes and texture blending issues. With increasing computing power and the continuous advancement of deep learning, there has been a surge in research utilizing deep learning techniques for MVS tasks. MVS essentially uses prior geometric knowledge to recover spatial 3D shapes. Deep learning based methods do not discard this principle; instead, they employ neural networks to facilitate the process of geometric reconstruction. These methods learn the matching relationships between images from different viewpoints, enabling more robust 3D reconstruction. MVSNet3, as a learning-based method, introduces a breakthrough approach by transforming the 3D reconstruction issue into a deep map inference issue. This innovative methodology can be roughly divided into four key steps: image feature extraction, cost volume construction, cost volume regularization, and depth estimation4. By adopting this framework, MVSNet pioneers the use of deep learning techniques to achieve multi-view 3D reconstruction, paving the way for significant advancements in the field. For the past few years, a new approach to 3D reconstruction incorporating the Transformer5 model, originally used for natural language processing, has emerged on the basis of deep learning. This approach employs the Transformer model for the feature extraction phase of 3D reconstruction, introducing a new perspective and potential improvements to the field. Thanks to the attention mechanism and the contextual aggregation of location encoding, the Transformer model has the ability to capture global information and semantic details of relevant locations. However, integrating the Transformer model into the 3D reconstruction field presents a significant challenge for the underlying hardware infrastructure. The Vision Transformer6 divides the input image into a series of patches and performs operations on them, resulting in a substantial increase in computational complexity. This can make it difficult to handle high-resolution images and slow down the convergence of the entire network. Additionally, there is a limitation on the sequence length that the Vision Transformer can effectively handle. Longer sequences may result in information loss during the processing stage. This constraint needs to be considered when applying the Vision Transformer to 3D reconstruction tasks to ensure that important details and contextual information are adequately preserved in the reconstructed output. The global attention mechanism in the Vision Transformer model can make the network more sensitive to noise in the input sequence during the 3D reconstruction process. This sensitivity to noise can potentially impact the quality and accuracy of the reconstructed output. Recently, there has been a resurgence in convolutional neural networks (CNNs) for sequence modeling. A notable approach involves utilizing BiLSTM7, like the Sequencer8 model, to address the task. Unlike the attention mechanism employed in Transformers, Sequencer utilizes BiLSTMs to process time sequences in width and height directions. Additionally, feature fusion is performed using convolutions, enabling better attention to temporal information. This alternative approach offers an effective solution for modeling sequences, considering both the spatial and temporal aspects, and has shown promising results in various applications. We propose a MVS network with Bidirectional Semantic Information for 3D reconstruction called BSI-MVS. Our network utilizes a BiLSTM approach to spatially and temporally combine information for improved model generalization and enhanced 3D reconstruction accuracy. In BSI-MVS, we adopt a spatiotemporally combined approach incorporating a Multi-Level Spatial Pyramid module. This module enables the construction of a spatial pyramid at multiple scales, enabling the entire network to capture and encompass a diverse set of spatial semantic information. BSI-MVS can capture details and contextual information across different spatial resolutions by integrating information from multi-scales. This spatial pyramid construction enhances the network's ability to handle variations in spatial structures and improves the overall performance and accuracy of the MVS network. The coarse-resolution feature maps obtained from the Multi-Level Spatial Pyramid are individually processed in the horizontal and vertical directions using BiLSTM to address the problem of long-term dependency. The following are the essential contributions of this paper:\n• We introduce an MVS network with Bidirectional Semantic Information for reconstructing low-resolution images in the MVS task to solve the calculation accuracy problem.\n• We propose a Multi-Level Spatial Pyramid module (MLSP) and a Bidirectional-LSTM module (BiLSTM) for the feature extraction stage. For low-resolution images, The MLSP can enhance the robustness of BSI-MVS by constructing multi-scale information. Then, using the BiLSTM module at each level of the feature pyramid allows for enhancing the understanding of contextual semantic information.\n• We have benchmarked our network against pre-existing methods on a DTU dataset9 widely used by the MVS task. In the end, our proposed network achieved superior results. Furthermore, we evaluated the network's visualization capabilities on the BlendedMVS dataset10, which showed improved visualization results compared to other approaches.\n\nInspired by human visual perception, Vision Transformer mechanisms enable efficient image scanning to extract relevant information about the target of interest. As the Vision Transformer becomes more widely used in computer vision, the Vision Transformer is also used in 3D reconstruction for better feature extraction. TransMVSNet11 is the first network to use the Vision Transformer for the MVS task, which uses the Vision Transformer for global contextual perception within and between images. AACVP-MVSNet12 introduces an attention layer to improve feature extraction and uses a similarity metric to aggregate cost volume. Liao, Jinli, et al.13 proposed to use an improved window attention mechanism for the global feature aggregation and local feature matching phases of 3D reconstruction with the aim of reducing redundancy and increasing smoothness. SENet (Squeeze-and-Excitation Network) is a novel approach that leverages convolutional neural networks (CNNs) and attention mechanisms to model channel relationships. The attention mechanism in SENet is designed to learn channel dependencies in order to highlight valuable information within each channel while suppressing irrelevant or redundant features. By effectively capturing and recalibrating channel-wise feature responses, SENet enhances the network's ability to focus on informative features, leading to improved performance and better utilization of channel information within CNNs. While the Vision Transformer-based MVSNet has shown improvements in the quality of 3D reconstruction, it is crucial to consider the following limitations. The global self-attentiveness mechanism employed in the Vision Transformer introduces challenges such as increased network parameters and computational complexity. This network can result in higher resource requirements and longer inference times. Moreover, the inherently global nature of self-attention may make the network more sensitive to environmental disturbances and variations, potentially impacting the overall stability of the reconstruction process. These trade-offs between improved reconstruction quality and increased computational burden need to be carefully considered when applying Vision Transformer-based approaches in the context of MVS. MVSNet, as a CNN-based method14, introduced the concept of transforming the 3D reconstruction problem into a depth map inference problem. This novel approach paved the way for leveraging deep learning techniques in multi-view 3D reconstruction. Cas-MVSNet15 utilizes a cascading architecture with multiple sub-networks for 3D reconstruction. CVP-MVSNet16 is a system that utilizes a compact and lightweight network to construct a pyramid of cost volumes. This approach allows for achieving enhanced resolution in 3D reconstruction. By leveraging this technique, CVP-MVSNet can generate more detailed and accurate reconstructions. GeoMVSNet17 uses the geometric prior to guide the fusion process for better feature fusion. In the MVS task, it is common to apply cost volume regularization18 to smooth the features. However, this regularization technique alone cannot completely address the issue of ambiguous feature matching caused by reflections or texture-free regions with unreliable 2D image features. These challenges can still cause imprecision in the reconstructed 3D models. Hence, it is crucial to focus on learning influential and representative characteristics during the feature extraction stage to enhance the generalizability of MVS systems. By obtaining high-quality features, the MVS algorithm can better handle challenging scenarios, such as reflections19 and texture-free regions, leading to more reliable and accurate 3D reconstructions. Experimental results show that a well-designed CNN can achieve results beyond the Vision Transformer. ConvNext20 is a novel architecture that builds upon the SwimTransformer21 model. It incorporates convolutional layers to achieve an attention-like mechanism, surpassing the performance of the original SwimTransformer model. By leveraging convolutional operations, ConvNext enhances the model's ability to capture relevant features and improve its overall performance in various tasks, while FLOPS are significantly lower than SwimTransformer. InceptionNext22 achieves superior performance using separable convolution compared to SwimTransformer with significant reductions in both the number of parameters and FLOPS. Sequencer utilizes LSTM instead of an attention mechanism for natural language processing. The network leverages bidirectional long-short-distance memory to perform classification tasks by serializing feature maps. And BH-RMVSNet23 uses bidirectional hybrid LSTM for the cost volume regularisation in MVSNet to improve memory efficiency. Hence, replacing the Vision Transformer for feature extraction in 3D reconstruction with convolutional neural networks can improve network performance while reducing the number of parameters.\n\nTo optimize the utilization of visual data and enhance the fidelity of the reconstructed 3D models, we have proposed an innovative method called BSI-MVS. Figure 1 depicts the detailed process of our proposed method, which is divided into three main stages: spatiotemporal feature extraction, cost volume regularization, and depth estimation. The input to our network is a reference image \\(I_{0} \\in {\\mathbb{R}}^{H \\times W}\\), where H and W are the height and width of the image, N source images \\(\\left\\{ {I_{{{\\text{i}} = 1}}^{N} } \\right\\}\\), and the camera's internal and external reference matrices for the corresponding viewpoint \\(\\left\\{ {K_{i} ,R_{i} ,{\\text{t}}_{i} } \\right\\}_{i = 0}^{N}\\). In our proposed network, the initial stage involves passing all input images through a Multi-level Pyramid module with weight sharing. In the second step of our proposed network, the low-resolution feature maps obtained from the previous step are inputted into the BiLSTM module. By leveraging the capabilities of the BiLSTM module, the network can better understand and encode relevant contextual information in the feature maps. Finally, similar to standard MVS networks, our approach performs cost volume construction, regularization, and in-depth reasoning. Our proposed MLSP module facilitates the interaction of data across diverse spatial levels, enabling the fusion of information at various resolutions. It achieves this by downsampling the input image and fusing the resulting feature maps from various layers. It allows the merging of different details from different levels of spatial detail, enhancing the network's ability to capture comprehensive and contextually rich representations of the input data. By enabling information exchange at multiple spatial scales, the MLSP module contributes to the network's capacity for more robust and effective feature extraction. The MLSP module utilizes distinct branches to aggregate features from various layers, capturing different perceptual fields and information at multiple scales (Fig. 2). In the first step of our proposed MLSP module, we generate two images of different resolutions by employing bilinear interpolation on the input image. These images are then processed by a feature fusion layer with shared weights, extracting features and generating a feature map with a channel dimension of 16 at various scales. This process allows for the construction of a spatial pyramid, incorporating information from different resolutions into the subsequent stages of the network. In the second step, feature maps from various scales in the spatial pyramid are concatenated to achieve multi-scale information fusion. Finally, the feature maps in the constructed multi-level pyramid are enhanced and aggregated using the Mixer Layer, which optimally utilizes them within the BiLSTM module. Integrating the Transformer into 3D reconstruction has led to the inclusion of a global attention mechanism in the feature extraction module of many MVS networks. This mechanism enables improved consideration of global information. However, it also results in increased computational complexity and heightened sensitivity to noise, which can adversely affect the quality of 3D reconstruction. As a solution, we suggest the utilization of BiLSTM in the feature fusion module instead of the Transformer. This approach enables us to prioritize long-range semantic features, increase resistance to interference, improve the capacity for generalization, and ultimately elevate the quality of 3D reconstruction. In the BiLSTM module, we implemented long-range dependencies using LSTM in both vertical and horizontal directions, similar to the Vision Transformer, while also reducing the number of parameters. We propose to apply BiLSTM to the low-resolution feature map fusion module. The BiLSTM module is mainly composed of the Layer-norm24 block, BiLSTM block, Depth Fusion block, and Residual connection, as shown in Fig. 3. For the sake of future reference, we define the input low-resolution feature map as \\(p_{i} \\in {\\mathbb{R}}^{C \\times H \\times W}\\). To align with the Layer-norm block used in the Transformer, the input to the BiLSTM block requires restructuring, denoted as \\(\\left\\{ {H \\times W \\times C} \\right\\}\\). Initially, the permute function is employed to modify the channel structure of the input feature map, resulting in the restructured representation denoted as \\(P \\in {\\mathbb{R}}^{H \\times W \\times C}\\). Subsequently, the modified feature maps were passed into the BiLSTM block, where two residual joins were executed. Finally, the output is processed by the Layer-norm block, and then the permute function is used to restore it to its original channel structure, which is denoted as \\(\\left\\{ {C \\times H \\times W} \\right\\}\\). The BiLSTM block consists of two separate BiLSTM layers that process feature sequences in the row and column directions, as illustrated in Fig. 4 below. The combination of the two individual LSTMs25 forms a BiLSTM layer. LSTMs belong to a distinct category of recurrent neural networks (RNNs) that exhibit proficiency in capturing long-range relationships and mitigating the challenge of vanishing or exploding gradients related to distant connections. Taking the BiLSTM layer in the vertical direction as an example, if the input sequence of the BiLSTM is defined as \\(\\overrightarrow {H}\\), then \\(\\overleftarrow {H}\\) is an inverted rearrangement sequence of \\(\\overrightarrow {H}\\). The \\(\\overrightarrow {H}\\) sequence is input into one of the ordinary LSTMs, and the corresponding LSTM is named Forward LSTM. The sequence, \\(\\overleftarrow {H}\\), is input into another standard LSTM, which is commonly known as the Backward LSTM. The Backward LSTM handles the input sequence in reverse order. The resulting output, represented as \\(\\overrightarrow {{Y_{for} }}\\), from the Forward LSTM, and the output, denoted as \\(\\overleftarrow {{Y_{back} }}\\), from the Backward LSTM, are subsequently concatenated together. Both \\(\\overrightarrow {{Y_{for} }}\\) and \\(\\overleftarrow {{Y_{back} }}\\) have a channel dimension of D. The final concatenated output, \\(Y\\), has a channel dimension of 2D. This splicing operation combines the information from both the forward and backward directions, enabling the model to capture bidirectional dependencies and leverage them for improved performance. The resulting spliced output, \\(Y\\), with an increased channel dimension, provides richer and more comprehensive information for subsequent stages of the model. In the preliminary step, the input characteristic map is specified as \\(P \\in {\\mathbb{R}}^{H \\times W \\times C}\\). In utilizing the BiLSTM network, the characteristic map must be serialized. Serialization involves converting the multidimensional characteristic map into a sequential representation. As shown in Fig. 4 below for the Vertical BiLSTM module and the Horizontal BiLSTM module, the number of tokens in the height and width directions are W and H, respectively. During the subsequent step, the feature map sequence is separately fed into the BiLSTM block along the width and height directions. Where \\(\\left\\{ {P_{:,w,:} \\in {\\mathbf{\\mathbb{R}}}^{H \\times C} } \\right\\}_{w = 1}^{W}\\) represents a group of sequences entered horizontally, W represents the aggregate count of sequences entered horizontally, and C represents the number of channels. All input sequences \\(P_{:,w,:}\\) are passed through the weight-sharing Vertical BiLSTM block: Similarly, \\(\\left\\{ {P_{h,:,:} \\in {\\mathbf{\\mathbb{R}}}^{W \\times C} } \\right\\}_{h = 1}^{H}\\) represents the set of vertically-oriented input sequences, H represents the total count of vertically-oriented input sequences, and C represents the number of channels. These inputs are all fed into the weight-sharing Horizontal BiLSTM block: In the third step, \\(Y^{ver} \\in {\\mathbf{\\mathbb{R}}}^{H \\times W \\times 4D}\\) and \\(Y^{hor} \\in {\\mathbf{\\mathbb{R}}}^{H \\times W \\times 4D}\\) are concatenated in the channel dimension, where D represents the hidden dimension of the BiLSTM block. This concatenated feature map is then propagated through a fully connected layer feedforward network. That is, \\(FC( \\bullet )\\) in the following equation accomplishes channel fusion, yielding the ultimate output feature map \\(Y \\in {\\mathbf{\\mathbb{R}}}^{H \\times W \\times C}\\). In the BiLSTM block, the input feature map needs to be serialized in horizontal and vertical directions. The resulting sequences are then separately fed into the corresponding BiLSTM layer in each direction. Finally, the outputs from both directions are stitched together. The described process may limit the network's ability to effectively combine local and global features, leading to a potential lack of generalization. To enhance the feature representation further, we propose the utilization of a deep fusion module that combines convolutional operations with a Bottleneck-like approach to fuse the resulting feature maps. This fusion module aims to capture and integrate multi-scale information effectively, leading to improved feature representation capabilities. The Depth Fusion Block is visually represented in Fig. 5 below. In the initial phase, a 1 × 1 convolutional kernel is employed to expand the channel dimension of the feature map, doubling its original size while preserving the feature map scale. Subsequently, a convolutional kernel of size three performs the same mapping while preserving the channel dimension and the feature map scale unchanged. A convolution kernel of size one is employed to restore the original channel dimension. The GELU activation function is applied throughout the module. Sampling occurs at the lowest resolution level, perpendicular to the direction normal to the reference viewpoint, assuming widely spaced depths, as set up in CVPMVSNet. To acquire the ultimate depth value of a point, the output probability volume is aggregated along the depth axis through averaging. The probabilities associated with pixel points corresponding to different depths undergo a weighting process based on their respective depth values, followed by aggregation. To improve the accuracy of the depth map, our network leverages the lowest resolution depth map as prior information and applies Bicubic interpolation to upsample the initial rough depth map. Building upon this approach, the depth hypothesis is progressively refined to construct a cost volume pyramid. This pyramid enables continuous optimization of the depth map while adding finer details. In line with the coarse-to-fine multi-stage MVS method, we employ the L1 loss as a supervised signal. This involves sampling the true depth map into the layer pyramid depth map and calculating the absolute distance between the true and predicted depths. The loss function is calculated as follows: In this context, the notation Ω represents the collection of valid pixel points. At the same time, i signifies the ith level of the pyramid, \\({\\mathbf{D}}_{GT}^{l} \\left( p \\right)\\) is the actual depth value of pixel x at level i, and \\({\\mathbf{D}}^{i} (x)\\) is the predicted depth value of pixel x at level i.\n\nOur network is trained and tested using the DTU dataset, which is a publicly available dataset. This dataset utilizes an industrial robot arm with adjustable luminance lights to capture photographs of objects from various viewpoints. Each viewpoint in the DTU dataset is precisely controlled, ensuring that a 3D point cloud is acquired using a structured light sensor. This allows for offline evaluation of the point cloud and facilitates easy monitoring of experimental results. The DTU dataset comprises 124 distinct scenes, each captured from either 49 or 64 viewpoints. These viewpoints cover a range of seven different lighting conditions, encompassing various geometries and texture structures in the scenes. As an illustration, Fig. 6 displays the scan96 scenes within the dataset. The images are arranged from left to right, showcasing the images captured at seven distinct luminance levels. Moreover, the images are vertically arranged in sequential order, signifying that they correspond to images taken from unique vantage points. As depicted in Fig. 7, presented underneath, the BlendedMVS dataset encompasses a diverse collection of 113 scenes with varying scales, and the number of views ranges from 20 to 1000. In contrast to the DTU dataset, which utilizes a fixed number of views, the BlendedMVS dataset employs multiple cameras to capture random shots. Additionally, depth sensors are used to accurately measure depth information. By incorporating randomness in viewpoint selection and incorporating accurate depth measurements, the BlendedMVS dataset aims to simulate real-world scenarios more effectively, enabling better performance and generalization of algorithms trained on it. However, this dataset does not provide a true value point cloud and does not allow for point cloud evaluation. Therefore, we solely utilize the BlendedMVS dataset for qualitative evaluation and visualization result presentation. For evaluating the point cloud model generated by our network, three selected metrics are Accuracy (Acc), Completeness (Comp), and Overall score (Overall). These metrics are employed to evaluate the precision, comprehensiveness, and overall fidelity of the reconstructed point cloud in comparison to the ground truth reference. Each metric is measured in millimeters, and decreased values for these metrics correspond to improved algorithm effectiveness. This means that a smaller value for Accuracy (Acc), Completeness (Comp), and Overall score (Overall) indicates a better reconstruction quality of the point cloud generated by the algorithm. The objective is to minimize these metrics, indicating a stronger alignment of the reconstructed point cloud with the ground truth data. In this context, P represents the points in the predicted point cloud model, while G represents all the points in the real point cloud. The experimental environment consisted of an AMD Ryzen 7 5800X 8-Core Processor as the CPU, 32 G.B. of memory, and an NVIDIA GeForce TITAN RTX GPU with 24 G.B. of video memory. The deep learning framework used was PyTorch, version 1.7.1, with CUDA version 10.1 for GPU acceleration. For training and testing, the image width is 160, height is 128 and the number of input views is 3. For model optimization, the Adam optimizer was employed with hyperparameters set to β1 = 0.9 and β2 = 0.999. Table 1 presents a comprehensive analysis of the algorithm discussed in our model and other learning-based MVS methods using the DTU dataset. The comparison is based on objective metrics, and it provides insights into the performance of our algorithm concerning other existing methods. In terms of runtime and parameter count, our model has achieved satisfactory results, demonstrating relatively low runtime and reasonable parameter count. This balance enhances the practical feasibility and efficiency of our method in real-world applications. Colmap is a classical MVS algorithm that reconstructs 3D models by iteratively establishing correspondences between image pairs. This step-by-step approach allows Colmap to gradually build the 3D model by leveraging the detected correspondences. In the case of low-resolution images, the Colmap method may encounter challenges in the feature point matching phase, leading to sparse correspondences and, subsequently, poor reconstruction results. R-MVSNet27 Replaces 3DCNN with GRU for Recurrent Regularisation. TransMVS is a method that adopts a Transformer-based approach for MVS tasks. Cas-MVSNet, on the other hand, utilizes a cascaded multi-scale cost volume strategy combined with adaptive depth sampling. CVP-MVSNet, in contrast, employs a compact and lightweight network architecture to construct pyramids of cost volumes, enabling higher-resolution 3D reconstruction. AACVP-MVSNet, in contrast, incorporates an attention layer into the network architecture to enhance feature extraction. Due to the extensive computational resources required by GeoMVSNet17, we did not retrain the model but conducted direct testing. Regarding accuracy and overall performance, as shown in Fig. 8, the algorithm proposed in this paper showcases superior results compared to other methods. To comprehensively evaluate the generalization capability of our proposed model, we reconstruct the scene from the BlendedMVS dataset without conducting any fine-tuning on the network explicitly trained on the DTU dataset. The results are shown in Fig. 9 below, where the significantly overall reconstructed point cloud of this network shows greater density in the visualization output when compared to the other networks. Our proposed network incorporates the MLSP module, which facilitates the construction of a multi-level spatial pyramid. This design enables the network to interact with image information at different scales, capturing holistic and detailed information. By leveraging the multi-level spatial pyramid, our algorithm can effectively incorporate contextual information and capture the hierarchical structure of the input data. We integrate the BiLSTM module into our model to enable semantic information filtering. This block allows the model to selectively focus on the most relevant and helpful information for the task of 3D reconstruction. By incorporating the BiLSTM module, our model becomes more adept at capturing and utilizing meaningful semantic information, which enhances the quality and accuracy of the generated 3D reconstructions. The effectiveness of all our proposed modules is validated through ablation experiments, and the corresponding results are presented in Table 2. The comprehensive model that incorporates all the proposed modules demonstrates superior performance across all metrics, attaining optimal results. To account for the varying requirements of the BiLSTM module, we investigated to assess the impact of selecting different numbers of layers on the results. Based on the observations outlined in Table 3, a discernible pattern emerges whereby the metrics exhibit a decline to a certain degree as the number of layers in the BiLSTM module increases. However, beyond a specific range of layer configurations, the metrics start to increase. Consequently, the best results were achieved when utilizing four layers in the BiLSTM module. This indicates that four layers strike an optimal balance between model complexity and performance, resulting in the most favorable outcomes for the task at hand. Table 3 Ablation study on the number of Different BiLSTM modules. As the BiLSTM Module utilizes BiLSTM to process feature sequences in the row and column directions, it is necessary to serialize the input feature. Our proposed BiLSTM Module integrates an additional feature fusion block. This block is positioned before the output and aims to fuse and enhance semantic information across different temporal and spatial dimensions. As shown in the data in Table 4, Depth-CNN indicates the use of depth-separable convolution for feature fusion, while Linear indicates the use of linear layers for feature fusion. Based on the experimental results, it is evident that the proposed Depth Fusion block, which incorporates a BottleNeck-like structure, outperforms other methods in all metrics. The block indicates that the chosen architecture and design of the Depth Fusion block are highly effective in achieving superior performance across various evaluation metrics. The BottleNeck-like structure likely contributes to the optimal fusion of depth information, resulting in enhanced results for the task under consideration. In our proposed network, we incorporate the BiLSTM module for feature aggregation following the MLSP module. This design allows for effectively integrating and aggregating features from multiple spatial scales. The BiLSTM module plays a crucial role in leveraging contextual information and capturing long-range dependencies among the features, leading to improved performance in tasks that require a comprehensive understanding of the input data. By combining the MLSP and BiLSTM modules, our network benefits from multi-scale feature representation and contextual modeling, enhancing its overall capability for the given task. Table 5 primarily investigates the substitution of the BiLSTM module in our proposed network with either an attention mechanism or a convolutional neural network. In Table 5, Vanilla-SA, LSDA, and SSA represent initial self-attention, long-short distance attention28, and scaling attention29. In Table 5, SSA-IWS13 refers to alternate scaling and interactive window attention, while Linear-SA-soft represents linear attention30. Control-SA is to perform feature extraction using a network that freezes pre-trained weights along with self-attention and then fuses the extracted feature maps. ConvNext-Depth applies ConvNext architecture with four layers featuring hidden dimensions that progressively grow. On the other hand, ConvNext-Block denotes the usage of ConvNext with four layers of the exact dimensions. ConvNext is a convolutional implementation of the SwimTransformer and ResNet-like structure. It has performed superior to SwimTransformer and ResNet in various classification and image detection tasks. This indicates that ConvNext improves accuracy and effectiveness in capturing and representng features in the context of these tasks. Incep-Block denotes a structure that employs InceptionNext as the primary architecture. ConvNext-Incep indicates a structure that alternates between using ConvNext and InceptionNext within the network. The metric results (Fig. 10) indicate that our proposed structure, the BiLSTM module, achieves optimal performance in completeness and objectivity metrics. This suggests that the BiLSTM module effectively captures and incorporates relevant information, resulting in comprehensive and accurate results for the given task. Results on different Modules. (a) ours; (b) vanilla-SA; (c) LSDA; (d) SSA; (e) SSA-IWSA; (f) SSA-IWSA-soft; (g) Control-SA; (h) ConvNext-depth; (i) ConvNext-Block; (j) Incep-Block; (k) ConvNext-Incep. We compared these networks' cost volume regularization modules to facilitate a more comprehensive comparison between convolutional neural networks and Vision Transformer-based neural networks. In Table 6, 3Dvit refers to using 3DTransformer, while 3DVit-BN indicates using 3D Transformer with BatchNorm regularization. The experimental metrics demonstrate that convolutional networks (3DCNN) for cost volume regularization achieve optimal performance across all metrics. In Table 6, 3Dvit refers to using 3DTransformer, while 3DVit-BN indicates using 3D Transformer with BatchNorm regularization. The experimental metrics demonstrate that convolutional networks (3DCNN) for cost volume regularization achieve optimal performance across all metrics."
    },
    {
        "link": "http://graphics.cs.uh.edu/yding/papers/2016-TVC-YuDing.pdf",
        "document": ""
    },
    {
        "link": "https://myweb.uiowa.edu/wanxwang",
        "document": "· HIK: A hybrid inverse-kinematics solution to generate avatars in real time for virtual reality applications.\n\n· https://homepage.divms.uiowa.edu/~kearney/ , the homepage of the research advisor, project supervisor, Dr. Joseph K. Kearney.\n\n· A demo of HIK working on different models, in the sequence of their presence they are: 163 joints model from Makehuman.org, 63 joints model from mixamo.com, a model of Metahuman, 67 joints model from mixamo.com .\n\n· A demo of the full body movement comparison between the virtual person and the real person, HIK (the hybrid inverse kinematics method), works on a general (fully rigged) humanoid model, regardless of skeletal structure or bone convention, allows small adjustment for body height and arm stretch length to adapt for different human subjects.\n\n· A demo of the full body movement comparison between the virtual person and the real person. The movement is driven by HIK.\n\n· A demo after implementing \"parallel restart\": numerical IK algorithms get stuck at local minima more often with the defined joint limits, robotic IK has numerous solutions to restart IK search, inspired from robotic IK, HIK uses the posture graph search to find local minima to initiate numerical IK computation in PARALLEL, that reduces chances of a failed numerical computation, in practice, this solution has fail rate lower than 1%. Numerical IK computation restart could cause jittering and jerky movement, choosing from several possible IK solutions for the smoothest posture transition, could reduce the side effect.\n\n· A demo of the GSIK (generic sequential IK framework) and SDLS IK chain solver with a posture graph, without joint limit specification, generates natural postures and smooth transitions.\n\n· A demo of the GSIK (generic sequential IK framework) and SDLS IK chain solver without a posture graph and joint limit specification can generate an unnatural posture.\n\n· https://youtu.be/1HVC7dCYBzU, the comparison between HTR and BVH.\n\n· https://youtu.be/gXLk47gTAGY, a prototype of IK usage in VR environment with Blender SDLS IK solver.\n\n· https://youtu.be/C-mCLFqCMOA, a prototype of IK with DLS (damped least square) method with MATLAB.\n\n· https://github.com/wwxhero/lib_hIK.git , a library exposing the forward kinematics and inverse kinematics functions, the library interface is in C language, the implementation is with C++ language.\n\n· https://github.com/wwxhero/hIK_unreal.git, an animation node plugin that utilizes the lib_hIK for forward kinematics or FK, and inverse kinematics or IK, for FK, it supports playing back BVH file, a motion capture data format, within the Unreal animation system, for IK, it takes configurable number of end effectors as input, and generate the local space bone configurations. It interfaces with lib_hIK through a simple set of C functions.\n\n· https://github.com/wwxhero/hIK.git, a set of unreal engine game projects for testing and using the IK (inverse kinematics) in Unreal and VR environment.\n\n· https://github.com/wwxhero/Posture_Graph, a set of tools that work on BVH files and HTR files (not standard HTR files, the header part of file is in BVH, the animation stack stores HTR animation, since there is no free tool that supports HTR, thus for convenience this semi-HTR is chosen to be used.), to build the posture graph, more information about posture graph in the project description section.\n\n· The classes design overview in UML(hit for a large view):\n\nThis is a Ph.D. dissertation project, the goal of this project is to create a full-body inverse kinematics (IK) solution that generates postures in real time in a VR environment powered with the Unreal Engine 4. The input includes 6 tracking locations: head, pelvis, wrists, and ankles. The key points of the goal include:\n\n1. The IK solution generates postures in real time and the quality meets standards for both computation speed and fidelity. For the computational speed standard, it should not drop frames per-second (FPS) lower than 50 for a normal scene setting; for the fidelity standard, the posture it generates should look natural and approximate the real posture, and the posture transitions or animation should be smooth.\n\n2. The IK solution generates postures for any humanoid model rigged with a general skeletal structure, such as models from http://www.makehumancommunity.org/content/downloads.html or from the Metahuman creator for Unreal Engine 4.\n\nTo achieve the first goal, the numerical IK methods are favored. Damped least square or DLS and selective damped least square or SDLS are used for animation making that generates smooth transitions. The smooth and continuous input of the VR tracking data is also a positive factor for the DLS and SDLS solvers to work fast to meet the speed requirement standard. To meet the fidelity standard, we use a posture graph. A posture graph is constructed from a set of postures where, , collected from BVH files converted from the CMU web site, http://mocap.cs.cmu.edu/tools.php and https://sites.google.com/a/cgspeed.com/cgspeed/motion-capture/cmu-bvh-conversion. The files store millions of postures, and are the (natural) posture transitions. The body movement is best described with a graph, where each posture is a snapshot of the body movement and transitions are for the posture change. Searching along the natural transitions for a posture that best satisfies the end effectors orientation and position would result a smooth and natural animation. This argument will be shown true through this project; a conventional method with the joint limit will work as a supplement to the posture graph to guarantee posture naturalness. L-BFGS, a gradient based optimization method, will be added for a better performance with joint limits, if necessary, at the end. Thus, it requires to have different numerical IK methods working under a generic sequential IK framework or GSIK that breaks the humanoid skeletal into kinematic groups, where each group includes at least one kinematic chain and each numerical IK algorithm works on a chain. For dealing with the performance uncertainty, we provide the configurations for defining the IK chains and individual joints. The parameters to tune the IK algorithm performance is configurable through an XML file. An example of a configuration XML file is shown below,\n\nUsing XML could avoid the impact from Unreal Engine upgrades. The configuration works as a supplement to the source code, but without the need to compile and build. We optimized the forward kinematics (FK) that is fundamental for IK: rotation is represented with quaternions giving the advantage of no scaling in the skeletal structure of BVH and HTR. FK computations can run as fast as a million times per second. FK results are cached until the posture changes. FK is frequently used for IK computation, thus IK is optimized as well. At the integration layer for Unreal Engine 4, we optimized the animation node output for the Unreal Engine 4 animation system. Since HIK is defined as a full body IK solution, the animation node of the Unreal Engine 4 powered by HIK outputs a local space “Transform”, unlike the Unreal Engine 4 built-in IK that generates the component space “Transform” that fits for a small number of bones (1-2 bones) adjustment. HIK generates postures of an avatar with many more bones. It is not uncommon to see an avatar with more than 100 bones. Working in this way directs the output posture into the Unreal Engine animation system without any unnecessary intermediate animation processing.\n\nTo achieve the second goal, we use animation retargeting. A posture graph is built on the skeletal structure of 31 bones using the HTR convention: the Y axis is aligned with the bone axis. To support an avatar with any number of bones possibly without this convention, we retarget the IK result from the 31-HTR-bone structure to the unspecified humanoid model with animation retargeting. The animation retargeting is defined with pairs of bones in XML file. This mechanism enables a wide support of humanoid models, with only a little of configuration work. A snippet of code for animation retargeting definition in an XML file is shown below.\n\nThe IK solution shares some properties in common with the formerly available commercial full-body IK product, Ikinema. Both can specify the number of iterations and the weight of position and orientation tasks. Since only the numerical IKs use the weight for position and orientation tasks, we assume that Ikinema is also based on numerical methods. Both methods generate a local space ‘Transform’ for the Unreal Engine animation system. But HIK differs from Ikinema in the following respects:\n\n1. the HIK source code is open under the GNU General Public License.\n\n2. HIK supports a general unreal engine skeletal mesh through animation retargeting. Ikinema uses Unreal Engine 4 UI operations to configure for each model.\n\nHIK ports the DLS and SLDS numerical solvers from Blender code with the following additions and extensions:\n\n1. Blender IK computation uses double precision data types. The goal of IK is to generate smooth transitions but not for real-time computation. HIK is more flexible on the data type, which could either be defined as float or double for a real number data type.\n\n2. HIK numerical computation is done with a kinematic-group coordinate space as the world space. This results in the translation part of transformation being compacted around zero or the origin which is important for the real number to be defined as a single precision floating point number with a limited precision. Group-space coordinates best exploit the floating-point number precision limit.\n\nThe progress of the project is steady, here is the list of work steps to achieve our goal:\n• Articulated Body tree structure is established and being used for FBX, BVH, HTR data.\n• Articulated body partition for joint groups, with each group including at least 1 joint chain.\n• Construct posture-graph from HTR data for each joint group.\n• (Optional) Mining BVH data to study the joint stiffness and joint rotational limit parameters.\n• Evaluate IK solution with different parameter settings in a mock VR environment (version 2.1).\n• Make available for more model choices, this solution is designed to support a generic articulated model (FBX) that can posed in 'T' posture.\n• To fix the minor issue: shoes penetrating deep into ground.\n\nAt present, the work has been completed through item 14.\n\nPosture Graph fundamentals, the epsilon postures (left) and the corresponding posture graph visualizations:\n\nJoint limit specification is through XML configuration file, the numerical algorithm clamps the joint rotation within the limited range, to avoid the extream cases that violate joint bio-mechanical possibilities, the following snipped of XML code is an example of joint limit specification:\n\nJoint rotation is first decomposed as swing-twist components: twist component is named as \"R_tau\", swing component is named as (R_theta, R_phi) respectively, they are all in degress, with different ways for clamping, intepreting (R_theta, R_phi) can be different, C_Sphercial clamping interprets (R_theta, R_phi) as sopherical coordinate (ro, theta, phi) with ro = 1, direct clamping interprets (R_theta, R_phi) as the two components for quaternions (_, sin(alpha/2)*u_x, _, sin(alpha/2)*u_z) where u = (u_x, 0, u_z) is the unit rotational vector, and alpha is the rotational angle. for the joint (knee) with one DOF rotation, specifying rotation with direct clamp is more intuitive, for the joint with three DOFs rotation, sopherical coordinate will perform better along the clampping edges, it can be mathematically proven. Twist rotation is intuitive to understand, thus only the swing rotation is visualized. Comparing the two figures below, we can see the spherical map has a steighter edges, thus more suitable to clamp with in-equations.\n\n· Integrting posture graph runtime search with the nuermical IK solutions, with support for joint limit, joint dexterity, parallel IK search with posture graph, and nuermical computation restart. (Dec, 2021 - Jan, 2022)\n\n· Implementing GSIK (general sequential IK) framework, porting Jacobian nuermical solver from Blender, and re-structuring, integrating into GSIK framework. (Sep, 2021)\n\n· Re-orient bone axis with HTR convension for the BVH data sources. (July, 2021)\n\n· Setup FK support with Eigen library by implementing BVH animation and retargeting to Unreal sleltel mesh on Unreal Engine 4. (June, 2021)\n\n· Prototype IK usage in VR environment with Blender. (May, 2021)"
    },
    {
        "link": "http://andreasaristidou.com/publications/papers/IK_survey.pdf",
        "document": ""
    },
    {
        "link": "https://whizzystudios.com/post/forward-kinematics-vs-inverse-kinematics-in-3d-character-rigging",
        "document": ""
    },
    {
        "link": "https://courses.cs.washington.edu/courses/cse490v/20wi/public/report_13.pdf",
        "document": ""
    }
]