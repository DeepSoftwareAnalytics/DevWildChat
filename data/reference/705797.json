[
    {
        "link": "https://dip-mazumder.medium.com/java-memory-model-a-comprehensive-guide-ba9643b839e",
        "document": "The Java Memory Model (JMM) governs how threads in Java interact with memory. It guarantees that changes made by one thread become visible to others, providing a framework for safe multi-threading. JMM ensures proper synchronization through constructs like `synchronized` blocks, `volatile` variables, and memory barriers. It’s crucial for preventing data races and ensuring consistent behavior in multi-threaded Java programs. Understanding JMM is fundamental for writing reliable and efficient concurrent code\n\nEach thread running within the Java virtual machine has its own thread stack. Local variables for primitive types are fully stored on the thread stack and are not visible to other threads. Even if two threads are executing the same code, they will create their own separate copies of local variables for that code in their respective thread stacks\n• When the thread is started, the Java runtime environment creates a new thread stack for the thread. The thread stack is initially empty.\n• When the thread calls , the runtime environment pushes a new frame onto the thread stack. The frame contains the call stack and local variables for ."
    },
    {
        "link": "https://jenkov.com/tutorials/java-concurrency/java-memory-model.html",
        "document": "The Java memory model specifies how the Java virtual machine works with the computer's memory (RAM). The Java virtual machine is a model of a whole computer so this model naturally includes a memory model - AKA the Java memory model.\n\nIt is very important to understand the Java memory model if you want to design correctly behaving concurrent programs. The Java memory model specifies how and when different threads can see values written to shared variables by other threads, and how to synchronize access to shared variables when necessary.\n\nThe original Java memory model was insufficient, so the Java memory model was revised in Java 1.5. This version of the Java memory model is still in use in Java today (Java 14+).\n\nIn case you prefer video, I have a video version of this tutorial available here: \n\n Java Memory Model Tutorial Video\n\nThe Java memory model used internally in the JVM divides memory between thread stacks and the heap. This diagram illustrates the Java memory model from a logic perspective:\n\nEach thread running in the Java virtual machine has its own thread stack. The thread stack contains information about what methods the thread has called to reach the current point of execution. I will refer to this as the \"call stack\". As the thread executes its code, the call stack changes.\n\nThe thread stack also contains all local variables for each method being executed (all methods on the call stack). A thread can only access it's own thread stack. Local variables created by a thread are invisible to all other threads than the thread who created it. Even if two threads are executing the exact same code, the two threads will still create the local variables of that code in each their own thread stack. Thus, each thread has its own version of each local variable.\n\nAll local variables of primitive types ( , , , , , , , ) are fully stored on the thread stack and are thus not visible to other threads. One thread may pass a copy of a pritimive variable to another thread, but it cannot share the primitive local variable itself.\n\nThe heap contains all objects created in your Java application, regardless of what thread created the object. This includes the object versions of the primitive types (e.g. , , etc.). It does not matter if an object was created and assigned to a local variable, or created as a member variable of another object, the object is still stored on the heap.\n\nHere is a diagram illustrating the call stack and local variables stored on the thread stacks, and objects stored on the heap:\n\nA local variable may be of a primitive type, in which case it is totally kept on the thread stack.\n\nA local variable may also be a reference to an object. In that case the reference (the local variable) is stored on the thread stack, but the object itself if stored on the heap.\n\nAn object may contain methods and these methods may contain local variables. These local variables are also stored on the thread stack, even if the object the method belongs to is stored on the heap.\n\nAn object's member variables are stored on the heap along with the object itself. That is true both when the member variable is of a primitive type, and if it is a reference to an object.\n\nStatic class variables are also stored on the heap along with the class definition.\n\nObjects on the heap can be accessed by all threads that have a reference to the object. When a thread has access to an object, it can also get access to that object's member variables. If two threads call a method on the same object at the same time, they will both have access to the object's member variables, but each thread will have its own copy of the local variables.\n\nHere is a diagram illustrating the points above:\n\nTwo threads have a set of local variables. One of the local variables ( ) point to a shared object on the heap (Object 3). The two threads each have a different reference to the same object. Their references are local variables and are thus stored in each thread's thread stack (on each). The two different references point to the same object on the heap, though.\n\nNotice how the shared object (Object 3) has a reference to Object 2 and Object 4 as member variables (illustrated by the arrows from Object 3 to Object 2 and Object 4). Via these member variable references in Object 3 the two threads can access Object 2 and Object 4.\n\nThe diagram also shows a local variable which point to two different objects on the heap. In this case the references point to two different objects (Object 1 and Object 5), not the same object. In theory both threads could access both Object 1 and Object 5, if both threads had references to both objects. But in the diagram above each thread only has a reference to one of the two objects.\n\nSo, what kind of Java code could lead to the above memory graph? Well, code as simple as the code below:\n\nIf two threads were executing the method then the diagram shown earlier would be the outcome. The method calls and calls .\n\ndeclares a primitive local variable ( of type ) and an local variable which is an object reference ( ).\n\nEach thread executing will create its own copy of and on their respective thread stacks. The variables will be completely separated from each other, only living on each thread's thread stack. One thread cannot see what changes another thread makes to its copy of .\n\nEach thread executing will also create their own copy of . However, the two different copies of both end up pointing to the same object on the heap. The code sets to point to an object referenced by a static variable. There is only one copy of a static variable and this copy is stored on the heap. Thus, both of the two copies of end up pointing to the same instance of which the static variable points to. The instance is also stored on the heap. It corresponds to Object 3 in the diagram above.\n\nNotice how the class contains two member variables too. The member variables themselves are stored on the heap along with the object. The two member variables point to two other objects. These objects correspond to Object 2 and Object 4 in the diagram above.\n\nNotice also how creates a local variable named . This local variable is an object reference to an object. The method sets the reference to point to a new instance. The reference will be stored in one copy per thread executing . The two objects instantiated will be stored on the heap, but since the method creates a new object every time the method is executed, two threads executing this method will create separate instances. The objects created inside correspond to Object 1 and Object 5 in the diagram above.\n\nNotice also the two member variables in the class of type which is a primitive type. Since these variables are member variables, they are still stored on the heap along with the object. Only local variables are stored on the thread stack.\n\nModern hardware memory architecture is somewhat different from the internal Java memory model. It is important to understand the hardware memory architecture too, to understand how the Java memory model works with it. This section describes the common hardware memory architecture, and a later section will describe how the Java memory model works with it.\n\nHere is a simplified diagram of modern computer hardware architecture:\n\nA modern computer often has 2 or more CPUs in it. Some of these CPUs may have multiple cores too. The point is, that on a modern computer with 2 or more CPUs it is possible to have more than one thread running simultaneously. Each CPU is capable of running one thread at any given time. That means that if your Java application is multithreaded, one thread per CPU may be running simultaneously (concurrently) inside your Java application.\n\nEach CPU contains a set of registers which are essentially in-CPU memory. The CPU can perform operations much faster on these registers than it can perform on variables in main memory. That is because the CPU can access these registers much faster than it can access main memory.\n\nEach CPU may also have a CPU cache memory layer. In fact, most modern CPUs have a cache memory layer of some size. The CPU can access its cache memory much faster than main memory, but typically not as fast as it can access its internal registers. So, the CPU cache memory is somewhere in between the speed of the internal registers and main memory. Some CPUs may have multiple cache layers (Level 1 and Level 2), but this is not so important to know to understand how the Java memory model interacts with memory. What matters is to know that CPUs can have a cache memory layer of some sort.\n\nA computer also contains a main memory area (RAM). All CPUs can access the main memory. The main memory area is typically much bigger than the cache memories of the CPUs.\n\nTypically, when a CPU needs to access main memory it will read part of main memory into its CPU cache. It may even read part of the cache into its internal registers and then perform operations on it. When the CPU needs to write the result back to main memory it will flush the value from its internal register to the cache memory, and at some point flush the value back to main memory.\n\nThe values stored in the cache memory is typically flushed back to main memory when the CPU needs to store something else in the cache memory. The CPU cache can have data written to part of its memory at a time, and flush part of its memory at a time. It does not have to read / write the full cache each time it is updated. Typically the cache is updated in smaller memory blocks called \"cache lines\". One or more cache lines may be read into the cache memory, and one or mor cache lines may be flushed back to main memory again.\n\nBridging The Gap Between The Java Memory Model And The Hardware Memory Architecture\n\nAs already mentioned, the Java memory model and the hardware memory architecture are different. The hardware memory architecture does not distinguish between thread stacks and heap. On the hardware, both the thread stack and the heap are located in main memory. Parts of the thread stacks and heap may sometimes be present in CPU caches and in internal CPU registers. This is illustrated in this diagram:\n\nWhen objects and variables can be stored in various different memory areas in the computer, certain problems may occur. The two main problems are:\n\nBoth of these problems will be explained in the following sections.\n\nIf two or more threads are sharing an object, without the proper use of either declarations or synchronization, updates to the shared object made by one thread may not be visible to other threads.\n\nImagine that the shared object is initially stored in main memory. A thread running on CPU one then reads the shared object into its CPU cache. There it makes a change to the shared object. As long as the CPU cache has not been flushed back to main memory, the changed version of the shared object is not visible to threads running on other CPUs. This way each thread may end up with its own copy of the shared object, each copy sitting in a different CPU cache.\n\nThe following diagram illustrates the sketched situation. One thread running on the left CPU copies the shared object into its CPU cache, and changes its variable to 2. This change is not visible to other threads running on the right CPU, because the update to has not been flushed back to main memory yet.\n\nTo solve this problem you can use Java's volatile keyword. The keyword can make sure that a given variable is read directly from main memory, and always written back to main memory when updated.\n\nIf two or more threads share an object, and more than one thread updates variables in that shared object, race conditions may occur.\n\nImagine if thread A reads the variable of a shared object into its CPU cache. Imagine too, that thread B does the same, but into a different CPU cache. Now thread A adds one to , and thread B does the same. Now has been incremented two times, once in each CPU cache.\n\nIf these increments had been carried out sequentially, the variable would be been incremented twice and had the original value + 2 written back to main memory.\n\nHowever, the two increments have been carried out concurrently without proper synchronization. Regardless of which of thread A and B that writes its updated version of back to main memory, the updated value will only be 1 higher than the original value, despite the two increments.\n\nThis diagram illustrates an occurrence of the problem with race conditions as described above:\n\nTo solve this problem you can use a Java synchronized block. A synchronized block guarantees that only one thread can enter a given critical section of the code at any given time. Synchronized blocks also guarantee that all variables accessed inside the synchronized block will be read in from main memory, and when the thread exits the synchronized block, all updated variables will be flushed back to main memory again, regardless of whether the variable is declared volatile or not."
    },
    {
        "link": "https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html",
        "document": "While most of the discussion in the preceding chapters is concerned only with the behavior of code as executed a single statement or expression at a time, that is, by a single thread, the Java Virtual Machine can support many threads of execution at once. These threads independently execute code that operates on values and objects residing in a shared main memory. Threads may be supported by having many hardware processors, by time-slicing a single hardware processor, or by time-slicing many hardware processors.\n\nThreads are represented by the class. The only way for a user to create a thread is to create an object of this class; each thread is associated with such an object. A thread will start when the method is invoked on the corresponding object.\n\nThe behavior of threads, particularly when not correctly synchronized, can be confusing and counterintuitive. This chapter describes the semantics of multithreaded programs; it includes rules for which values may be seen by a read of shared memory that is updated by multiple threads. As the specification is similar to the memory models for different hardware architectures, these semantics are known as the Java programming language memory model. When no confusion can arise, we will simply refer to these rules as \"the memory model\".\n\nThese semantics do not prescribe how a multithreaded program should be executed. Rather, they describe the behaviors that multithreaded programs are allowed to exhibit. Any execution strategy that generates only allowed behaviors is an acceptable execution strategy.\n\nThe Java programming language provides multiple mechanisms for communicating between threads. The most basic of these methods is synchronization, which is implemented using monitors. Each object in Java is associated with a monitor, which a thread can lock or unlock. Only one thread at a time may hold a lock on a monitor. Any other threads attempting to lock that monitor are blocked until they can obtain a lock on that monitor. A thread t may lock a particular monitor multiple times; each unlock reverses the effect of one lock operation. The statement (§14.19) computes a reference to an object; it then attempts to perform a lock action on that object's monitor and does not proceed further until the lock action has successfully completed. After the lock action has been performed, the body of the statement is executed. If execution of the body is ever completed, either normally or abruptly, an unlock action is automatically performed on that same monitor. A method (§8.4.3.6) automatically performs a lock action when it is invoked; its body is not executed until the lock action has successfully completed. If the method is an instance method, it locks the monitor associated with the instance for which it was invoked (that is, the object that will be known as during execution of the body of the method). If the method is , it locks the monitor associated with the object that represents the class in which the method is defined. If execution of the method's body is ever completed, either normally or abruptly, an unlock action is automatically performed on that same monitor. The Java programming language neither prevents nor requires detection of deadlock conditions. Programs where threads hold (directly or indirectly) locks on multiple objects should use conventional techniques for deadlock avoidance, creating higher-level locking primitives that do not deadlock, if necessary. Other mechanisms, such as reads and writes of variables and the use of classes in the package, provide alternative ways of synchronization.\n\nEvery object, in addition to having an associated monitor, has an associated wait set. A wait set is a set of threads. When an object is first created, its wait set is empty. Elementary actions that add threads to and remove threads from wait sets are atomic. Wait sets are manipulated solely through the methods , , and . Wait set manipulations can also be affected by the interruption status of a thread, and by the class's methods dealing with interruption. Additionally, the class's methods for sleeping and joining other threads have properties derived from those of wait and notification actions. Wait actions occur upon invocation of , or the timed forms and . A call of with a parameter of zero, or a call of with two zero parameters, is equivalent to an invocation of . A thread returns normally from a wait if it returns without throwing an . Let thread t be the thread executing the method on object m, and let n be the number of lock actions by t on m that have not been matched by unlock actions. One of the following actions occurs:\n• If n is zero (i.e., thread t does not already possess the lock for target m), then an is thrown.\n• If this is a timed wait and the argument is not in the range of or the argument is negative, then an is thrown.\n• If thread t is interrupted, then an is thrown and t's interruption status is set to false.\n• Otherwise, the following sequence occurs:\n• Thread t is added to the wait set of object m, and performs n unlock actions on m.\n• Thread t does not execute any further instructions until it has been removed from m's wait set. The thread may be removed from the wait set due to any one of the following actions, and will resume sometime afterward:\n• A action being performed on m in which t is selected for removal from the wait set.\n• An action being performed on t.\n• If this is a timed wait, an internal action removing t from m's wait set that occurs after at least milliseconds plus nanoseconds elapse since the beginning of this wait action.\n• An internal action by the implementation. Implementations are permitted, although not encouraged, to perform \"spurious wake-ups\", that is, to remove threads from wait sets and thus enable resumption without explicit instructions to do so. Notice that this provision necessitates the Java coding practice of using only within loops that terminate only when some logical condition that the thread is waiting for holds. Each thread must determine an order over the events that could cause it to be removed from a wait set. That order does not have to be consistent with other orderings, but the thread must behave as though those events occurred in that order. For example, if a thread t is in the wait set for m, and then both an interrupt of t and a notification of m occur, there must be an order over these events. If the interrupt is deemed to have occurred first, then t will eventually return from by throwing , and some other thread in the wait set for m (if any exist at the time of the notification) must receive the notification. If the notification is deemed to have occurred first, then t will eventually return normally from with an interrupt still pending.\n• If thread t was removed from m's wait set in step 2 due to an interrupt, then t's interruption status is set to false and the method throws . Notification actions occur upon invocation of methods and . Let thread t be the thread executing either of these methods on object m, and let n be the number of lock actions by t on m that have not been matched by unlock actions. One of the following actions occurs:\n• If n is zero, then an is thrown. This is the case where thread t does not already possess the lock for target m.\n• If n is greater than zero and this is a action, then if m's wait set is not empty, a thread u that is a member of m's current wait set is selected and removed from the wait set. There is no guarantee about which thread in the wait set is selected. This removal from the wait set enables u's resumption in a wait action. Notice, however, that u's lock actions upon resumption cannot succeed until some time after t fully unlocks the monitor for m.\n• If n is greater than zero and this is a action, then all threads are removed from m's wait set, and thus resume. Notice, however, that only one of them at a time will lock the monitor required during the resumption of wait. Interruption actions occur upon invocation of , as well as methods defined to invoke it in turn, such as . Let t be the thread invoking u , for some thread u, where t and u may be the same. This action causes u's interruption status to be set to true. Additionally, if there exists some object m whose wait set contains u, then u is removed from m's wait set. This enables u to resume in a wait action, in which case this wait will, after re-locking m's monitor, throw . Invocations of can determine a thread's interruption status. The method may be invoked by a thread to observe and clear its own interruption status. The above specifications allow us to determine several properties having to do with the interaction of waits, notification, and interruption. If a thread is both notified and interrupted while waiting, it may either:\n• return normally from , while still having a pending interrupt (in other words, a call to would return true)\n• return from by throwing an The thread may not reset its interrupt status and return normally from the call to . Similarly, notifications cannot be lost due to interrupts. Assume that a set s of threads is in the wait set of an object m, and another thread performs a on m. Then either:\n• at least one thread in s must return normally from , or\n• all of the threads in s must exit by throwing Note that if a thread is both interrupted and woken via , and that thread returns from by throwing an , then some other thread in the wait set must be notified.\n\nA memory model describes, given a program and an execution trace of that program, whether the execution trace is a legal execution of the program. The Java programming language memory model works by examining each read in an execution trace and checking that the write observed by that read is valid according to certain rules. The memory model describes possible behaviors of a program. An implementation is free to produce any code it likes, as long as all resulting executions of a program produce a result that can be predicted by the memory model. This provides a great deal of freedom for the implementor to perform a myriad of code transformations, including the reordering of actions and removal of unnecessary synchronization. The semantics of the Java programming language allow compilers and microprocessors to perform optimizations that can interact with incorrectly synchronized code in ways that can produce behaviors that seem paradoxical. Here are some examples of how incorrectly synchronized programs may exhibit surprising behaviors. Consider, for example, the example program traces shown in Table 17.4-A. This program uses local variables and and shared variables and . Initially, . \n\n It may appear that the result and is impossible. Intuitively, either instruction 1 or instruction 3 should come first in an execution. If instruction 1 comes first, it should not be able to see the write at instruction 4. If instruction 3 comes first, it should not be able to see the write at instruction 2. If some execution exhibited this behavior, then we would know that instruction 4 came before instruction 1, which came before instruction 2, which came before instruction 3, which came before instruction 4. This is, on the face of it, absurd. However, compilers are allowed to reorder the instructions in either thread, when this does not affect the execution of that thread in isolation. If instruction 1 is reordered with instruction 2, as shown in the trace in Table 17.4-B, then it is easy to see how the result and might occur. \n\n To some programmers, this behavior may seem \"broken\". However, it should be noted that this code is improperly synchronized:\n• there is a write in one thread,\n• a read of the same variable by another thread,\n• and the write and read are not ordered by synchronization. This situation is an example of a data race (§17.4.5). When code contains a data race, counterintuitive results are often possible. Several mechanisms can produce the reordering in Table 17.4-B. A Just-In-Time compiler in a Java Virtual Machine implementation may rearrange code, or the processor. In addition, the memory hierarchy of the architecture on which a Java Virtual Machine implementation is run may make it appear as if code is being reordered. In this chapter, we shall refer to anything that can reorder code as a compiler. Another example of surprising results can be seen in Table 17.4-C. Initially, and . This program is also incorrectly synchronized; it writes to shared memory without enforcing any ordering between those writes. \n\n One common compiler optimization involves having the value read for reused for : they are both reads of with no intervening write. This situation is shown in Table 17.4-D. \n\n Now consider the case where the assignment to in Thread 2 happens between the first read of and the read of in Thread 1. If the compiler decides to reuse the value of for the , then and will have the value , and will have the value . From the perspective of the programmer, the value stored at has changed from to and then changed back. \n\n The memory model determines what values can be read at every point in the program. The actions of each thread in isolation must behave as governed by the semantics of that thread, with the exception that the values seen by each read are determined by the memory model. When we refer to this, we say that the program obeys intra-thread semantics. Intra-thread semantics are the semantics for single-threaded programs, and allow the complete prediction of the behavior of a thread based on the values seen by read actions within the thread. To determine if the actions of thread t in an execution are legal, we simply evaluate the implementation of thread t as it would be performed in a single-threaded context, as defined in the rest of this specification. Each time the evaluation of thread t generates an inter-thread action, it must match the inter-thread action a of t that comes next in program order. If a is a read, then further evaluation of t uses the value seen by a as determined by the memory model. This section provides the specification of the Java programming language memory model except for issues dealing with fields, which are described in §17.5. The memory model specified herein is not fundamentally based in the object-oriented nature of the Java programming language. For conciseness and simplicity in our examples, we often exhibit code fragments without class or method definitions, or explicit dereferencing. Most examples consist of two or more threads containing statements with access to local variables, shared global variables, or instance fields of an object. We typically use variables names such as or to indicate variables local to a method or thread. Such variables are not accessible by other threads. Memory that can be shared between threads is called shared memory or heap memory. All instance fields, fields, and array elements are stored in heap memory. In this chapter, we use the term variable to refer to both fields and array elements. Local variables (§14.4), formal method parameters (§8.4.1), and exception handler parameters (§14.20) are never shared between threads and are unaffected by the memory model. Two accesses to (reads of or writes to) the same variable are said to be conflicting if at least one of the accesses is a write. An inter-thread action is an action performed by one thread that can be detected or directly influenced by another thread. There are several kinds of inter-thread action that a program may perform:\n• \n• The (synthetic) first and last action of a thread.\n• Actions that start a thread or detect that a thread has terminated (§17.4.4).\n• External Actions. An external action is an action that may be observable outside of an execution, and has a result based on an environment external to the execution.\n• Thread divergence actions (§17.4.9). A thread divergence action is only performed by a thread that is in an infinite loop in which no memory, synchronization, or external actions are performed. If a thread performs a thread divergence action, it will be followed by an infinite number of thread divergence actions. Thread divergence actions are introduced to model how a thread may cause all other threads to stall and fail to make progress. This specification is only concerned with inter-thread actions. We do not need to concern ourselves with intra-thread actions (e.g., adding two local variables and storing the result in a third local variable). As previously mentioned, all threads need to obey the correct intra-thread semantics for Java programs. We will usually refer to inter-thread actions more succinctly as simply actions. An action a is described by a tuple < t, k, v, u >, comprising:\n• v - the variable or monitor involved in the action. For lock actions, v is the monitor being locked; for unlock actions, v is the monitor being unlocked. If the action is a (volatile or non-volatile) read, v is the variable being read. If the action is a (volatile or non-volatile) write, v is the variable being written.\n• u - an arbitrary unique identifier for the action An external action tuple contains an additional component, which contains the results of the external action as perceived by the thread performing the action. This may be information as to the success or failure of the action, and any values read by the action. Parameters to the external action (e.g., which bytes are written to which socket) are not part of the external action tuple. These parameters are set up by other actions within the thread and can be determined by examining the intra-thread semantics. They are not explicitly discussed in the memory model. In non-terminating executions, not all external actions are observable. Non-terminating executions and observable actions are discussed in §17.4.9. Among all the inter-thread actions performed by each thread t, the program order of t is a total order that reflects the order in which these actions would be performed according to the intra-thread semantics of t. A set of actions is sequentially consistent if all actions occur in a total order (the execution order) that is consistent with program order, and furthermore, each read r of a variable v sees the value written by the write w to v such that:\n• w comes before r in the execution order, and\n• there is no other write w' such that w comes before w' and w' comes before r in the execution order. Sequential consistency is a very strong guarantee that is made about visibility and ordering in an execution of a program. Within a sequentially consistent execution, there is a total order over all individual actions (such as reads and writes) which is consistent with the order of the program, and each individual action is atomic and is immediately visible to every thread. If a program has no data races, then all executions of the program will appear to be sequentially consistent. Sequential consistency and/or freedom from data races still allows errors arising from groups of operations that need to be perceived atomically and are not. If we were to use sequential consistency as our memory model, many of the compiler and processor optimizations that we have discussed would be illegal. For example, in the trace in Table 17.4-C, as soon as the write of to occurred, subsequent reads of that location would be required to see that value. Every execution has a synchronization order. A synchronization order is a total order over all of the synchronization actions of an execution. For each thread t, the synchronization order of the synchronization actions (§17.4.2) in t is consistent with the program order (§17.4.3) of t. Synchronization actions induce the synchronized-with relation on actions, defined as follows:\n• An unlock action on monitor m synchronizes-with all subsequent lock actions on m (where \"subsequent\" is defined according to the synchronization order).\n• A write to a volatile variable v (§8.3.1.4) synchronizes-with all subsequent reads of v by any thread (where \"subsequent\" is defined according to the synchronization order).\n• An action that starts a thread synchronizes-with the first action in the thread it starts.\n• The write of the default value (zero, , or ) to each variable synchronizes-with the first action in every thread. Although it may seem a little strange to write a default value to a variable before the object containing the variable is allocated, conceptually every object is created at the start of the program with its default initialized values.\n• The final action in a thread synchronizes-with any action in another thread that detects that has terminated. may accomplish this by calling or .\n• If thread interrupts thread , the interrupt by synchronizes-with any point where any other thread (including ) determines that has been interrupted (by having an thrown or by invoking or ). The source of a synchronizes-with edge is called a release, and the destination is called an acquire. Two actions can be ordered by a happens-before relationship. If one action happens-before another, then the first is visible to and ordered before the second. If we have two actions x and y, we write hb(x, y) to indicate that x happens-before y.\n• If x and y are actions of the same thread and x comes before y in program order, then hb(x, y).\n• There is a happens-before edge from the end of a constructor of an object to the start of a finalizer (§12.6) for that object.\n• If an action x synchronizes-with a following action y, then we also have hb(x, y).\n• If hb(x, y) and hb(y, z), then hb(x, z). The methods of class (§17.2.1) have lock and unlock actions associated with them; their happens-before relationships are defined by these associated actions. It should be noted that the presence of a happens-before relationship between two actions does not necessarily imply that they have to take place in that order in an implementation. If the reordering produces results consistent with a legal execution, it is not illegal. For example, the write of a default value to every field of an object constructed by a thread need not happen before the beginning of that thread, as long as no read ever observes that fact. More specifically, if two actions share a happens-before relationship, they do not necessarily have to appear to have happened in that order to any code with which they do not share a happens-before relationship. Writes in one thread that are in a data race with reads in another thread may, for example, appear to occur out of order to those reads. The happens-before relation defines when data races take place. A set of synchronization edges, S, is sufficient if it is the minimal set such that the transitive closure of S with the program order determines all of the happens-before edges in the execution. This set is unique. It follows from the above definitions that:\n• An unlock on a monitor happens-before every subsequent lock on that monitor.\n• A write to a field (§8.3.1.4) happens-before every subsequent read of that field.\n• A call to on a thread happens-before any actions in the started thread.\n• All actions in a thread happen-before any other thread successfully returns from a on that thread.\n• The default initialization of any object happens-before any other actions (other than default-writes) of a program. When a program contains two conflicting accesses (§17.4.1) that are not ordered by a happens-before relationship, it is said to contain a data race. The semantics of operations other than inter-thread actions, such as reads of array lengths (§10.7), executions of checked casts (§5.5, §15.16), and invocations of virtual methods (§15.12), are not directly affected by data races. Therefore, a data race cannot cause incorrect behavior such as returning the wrong length for an array. A program is correctly synchronized if and only if all sequentially consistent executions are free of data races. If a program is correctly synchronized, then all executions of the program will appear to be sequentially consistent (§17.4.3). This is an extremely strong guarantee for programmers. Programmers do not need to reason about reorderings to determine that their code contains data races. Therefore they do not need to reason about reorderings when determining whether their code is correctly synchronized. Once the determination that the code is correctly synchronized is made, the programmer does not need to worry that reorderings will affect his or her code. A program must be correctly synchronized to avoid the kinds of counterintuitive behaviors that can be observed when code is reordered. The use of correct synchronization does not ensure that the overall behavior of a program is correct. However, its use does allow a programmer to reason about the possible behaviors of a program in a simple way; the behavior of a correctly synchronized program is much less dependent on possible reorderings. Without correct synchronization, very strange, confusing and counterintuitive behaviors are possible. We say that a read r of a variable v is allowed to observe a write w to v if, in the happens-before partial order of the execution trace:\n• r is not ordered before w (i.e., it is not the case that hb(r, w)), and\n• there is no intervening write w' to v (i.e. no write w' to v such that hb(w, w') and hb(w', r)). Informally, a read r is allowed to see the result of a write w if there is no happens-before ordering to prevent that read. A set of actions A is happens-before consistent if for all reads r in A, where W(r) is the write action seen by r, it is not the case that either hb(r, W(r)) or that there exists a write w in A such that w.v = r.v and hb(W(r), w) and hb(w, r). In a happens-before consistent set of actions, each read sees a write that it is allowed to see by the happens-before ordering. For the trace in Table 17.4.5-A, initially . The trace can observe and and still be happens-before consistent, since there are execution orders that allow each read to see the appropriate write. Table 17.4.5-A. Behavior allowed by happens-before consistency, but not sequential consistency. \n\n Since there is no synchronization, each read can see either the write of the initial value or the write by the other thread. An execution order that displays this behavior is: Another execution order that is happens-before consistent is: In this execution, the reads see writes that occur later in the execution order. This may seem counterintuitive, but is allowed by happens-before consistency. Allowing reads to see later writes can sometimes produce unacceptable behaviors. \n\n An execution E is described by a tuple < P, A, po, so, W, V, sw, hb >, comprising:\n• po - program order, which for each thread t, is a total order over all actions performed by t in A\n• so - synchronization order, which is a total order over all synchronization actions in A\n• W - a write-seen function, which for each read r in A, gives W(r), the write action seen by r in E.\n• V - a value-written function, which for each write w in A, gives V(w), the value written by w in E. Note that the synchronizes-with and happens-before elements are uniquely determined by the other components of an execution and the rules for well-formed executions (§17.4.7). An execution is happens-before consistent if its set of actions is happens-before consistent (§17.4.5). We only consider well-formed executions. An execution E = < P, A, po, so, W, V, sw, hb > is well formed if the following are true:\n• Each read sees a write to the same variable in the execution. All reads and writes of volatile variables are volatile actions. For all reads r in A, we have W(r) in A and W(r).v = r.v. The variable r.v is volatile if and only if r is a volatile read, and the variable w.v is volatile if and only if w is a volatile write.\n• The happens-before order is given by the transitive closure of synchronizes-with edges and program order. It must be a valid partial order: reflexive, transitive and antisymmetric.\n• For each thread t, the actions performed by t in A are the same as would be generated by that thread in program-order in isolation, with each write w writing the value V(w), given that each read r sees the value V(W(r)). Values seen by each read are determined by the memory model. The program order given must reflect the program order in which the actions would be performed according to the intra-thread semantics of P.\n• For all volatile reads r in A, it is not the case that either so(r, W(r)) or that there exists a write w in A such that w.v = r.v and so(W(r), w) and so(w, r). We use f| to denote the function given by restricting the domain of f to d. For all x in d, f| (x) = f(x), and for all x not in d, f| (x) is undefined. We use p| to represent the restriction of the partial order p to the elements in d. For all x,y in d, p(x,y) if and only if p| (x,y). If either x or y are not in d, then it is not the case that p| (x,y). A well-formed execution E = < P, A, po, so, W, V, sw, hb > is validated by committing actions from A. If all of the actions in A can be committed, then the execution satisfies the causality requirements of the Java programming language memory model. Starting with the empty set as C , we perform a sequence of steps where we take actions from the set of actions A and add them to a set of committed actions C to get a new set of committed actions C . To demonstrate that this is reasonable, for each C we need to demonstrate an execution E containing C that meets certain conditions. Formally, an execution E satisfies the causality requirements of the Java programming language memory model if and only if there exist:\n• Sets of actions C , C , ... such that:\n• C is a proper subset of C If A is finite, then the sequence C , C , ... will be finite, ending in a set C = A. If A is infinite, then the sequence C , C , ... may be infinite, and it must be the case that the union of all elements of this infinite sequence is equal to A. Given these sets of actions C , ... and executions E , ... , every action in C must be one of the actions in E . All actions in C must share the same relative happens-before order and synchronization order in both E and E. Formally: The values written by the writes in C must be the same in both E and E. Only the reads in C need to see the same writes in E as in E. Formally: All reads in E that are not in C must see writes that happen-before them. Each read r in C - C must see writes in C in both E and E, but may see a different write in E from the one it sees in E. Formally:\n• For any read r in A - C , we have hb (W (r), r)\n• For any read r in (C - C ), we have W (r) in C and W(r) in C Given a set of sufficient synchronizes-with edges for E , if there is a release-acquire pair that happens-before (§17.4.5) an action you are committing, then that pair must be present in all E , where j ≥ i. Formally:\n• Let ssw be the sw edges that are also in the transitive reduction of hb but not in po. We call ssw the sufficient synchronizes-with edges for E . If ssw (x, y) and hb (y, z) and z in C , then sw (x, y) for all j ≥ i. If an action y is committed, all external actions that happen-before y are also committed.\n• If y is in C , x is an external action and hb (x, y), then x in C . Example 17.4.8-1. Happens-before Consistency Is Not Sufficient Happens-before consistency is a necessary, but not sufficient, set of constraints. Merely enforcing happens-before consistency would allow for unacceptable behaviors - those that violate the requirements we have established for programs. For example, happens-before consistency allows values to appear \"out of thin air\". This can be seen by a detailed examination of the trace in Table 17.4.8-A. \n\n The code shown in Table 17.4.8-A is correctly synchronized. This may seem surprising, since it does not perform any synchronization actions. Remember, however, that a program is correctly synchronized if, when it is executed in a sequentially consistent manner, there are no data races. If this code is executed in a sequentially consistent way, each action will occur in program order, and neither of the writes will occur. Since no writes occur, there can be no data races: the program is correctly synchronized. Since this program is correctly synchronized, the only behaviors we can allow are sequentially consistent behaviors. However, there is an execution of this program that is happens-before consistent, but not sequentially consistent: This result is happens-before consistent: there is no happens-before relationship that prevents it from occurring. However, it is clearly not acceptable: there is no sequentially consistent execution that would result in this behavior. The fact that we allow a read to see a write that comes later in the execution order can sometimes thus result in unacceptable behaviors. Although allowing reads to see writes that come later in the execution order is sometimes undesirable, it is also sometimes necessary. As we saw above, the trace in Table 17.4.5-A requires some reads to see writes that occur later in the execution order. Since the reads come first in each thread, the very first action in the execution order must be a read. If that read cannot see a write that occurs later, then it cannot see any value other than the initial value for the variable it reads. This is clearly not reflective of all behaviors. We refer to the issue of when reads can see future writes as causality, because of issues that arise in cases like the one found in Table 17.4.8-A. In that case, the reads cause the writes to occur, and the writes cause the reads to occur. There is no \"first cause\" for the actions. Our memory model therefore needs a consistent way of determining which reads can see writes early. Examples such as the one found in Table 17.4.8-A demonstrate that the specification must be careful when stating whether a read can see a write that occurs later in the execution (bearing in mind that if a read sees a write that occurs later in the execution, it represents the fact that the write is actually performed early). The memory model takes as input a given execution, and a program, and determines whether that execution is a legal execution of the program. It does this by gradually building a set of \"committed\" actions that reflect which actions were executed by the program. Usually, the next action to be committed will reflect the next action that can be performed by a sequentially consistent execution. However, to reflect reads that need to see later writes, we allow some actions to be committed earlier than other actions that happen-before them. Obviously, some actions may be committed early and some may not. If, for example, one of the writes in Table 17.4.8-A were committed before the read of that variable, the read could see the write, and the \"out-of-thin-air\" result could occur. Informally, we allow an action to be committed early if we know that the action can occur without assuming some data race occurs. In Table 17.4.8-A, we cannot perform either write early, because the writes cannot occur unless the reads see the result of a data race. \n\n For programs that always terminate in some bounded finite period of time, their behavior can be understood (informally) simply in terms of their allowable executions. For programs that can fail to terminate in a bounded amount of time, more subtle issues arise. The observable behavior of a program is defined by the finite sets of external actions that the program may perform. A program that, for example, simply prints \"Hello\" forever is described by a set of behaviors that for any non-negative integer i, includes the behavior of printing \"Hello\" i times. Termination is not explicitly modeled as a behavior, but a program can easily be extended to generate an additional external action executionTermination that occurs when all threads have terminated. We also define a special hang action. If behavior is described by a set of external actions including a hang action, it indicates a behavior where after the external actions are observed, the program can run for an unbounded amount of time without performing any additional external actions or terminating. Programs can hang if all threads are blocked or if the program can perform an unbounded number of actions without performing any external actions. A thread can be blocked in a variety of circumstances, such as when it is attempting to acquire a lock or perform an external action (such as a read) that depends on external data. An execution may result in a thread being blocked indefinitely and the execution's not terminating. In such cases, the actions generated by the blocked thread must consist of all actions generated by that thread up to and including the action that caused the thread to be blocked, and no actions that would be generated by the thread after that action. To reason about observable behaviors, we need to talk about sets of observable actions. If O is a set of observable actions for an execution E, then set O must be a subset of E's actions, A, and must contain only a finite number of actions, even if A contains an infinite number of actions. Furthermore, if an action y is in O, and either hb(x, y) or so(x, y), then x is in O. Note that a set of observable actions are not restricted to external actions. Rather, only external actions that are in a set of observable actions are deemed to be observable external actions. A behavior B is an allowable behavior of a program P if and only if B is a finite set of external actions and either:\n• There exists an execution E of P, and a set O of observable actions for E, and B is the set of external actions in O (If any threads in E end in a blocked state and O contains all actions in E, then B may also contain a hang action); or\n• There exists a set O of actions such that B consists of a hang action plus all the external actions in O and for all k ≥ | O |, there exists an execution E of P with actions A, and there exists a set of actions O' such that:\n• Both O and O' are subsets of A that fulfill the requirements for sets of observable actions. Note that a behavior B does not describe the order in which the external actions in B are observed, but other (internal) constraints on how the external actions are generated and performed may impose such constraints.\n\nFields declared final are initialized once, but never changed under normal circumstances. The detailed semantics of fields are somewhat different from those of normal fields. In particular, compilers have a great deal of freedom to move reads of fields across synchronization barriers and calls to arbitrary or unknown methods. Correspondingly, compilers are allowed to keep the value of a field cached in a register and not reload it from memory in situations where a non- field would have to be reloaded. fields also allow programmers to implement thread-safe immutable objects without synchronization. A thread-safe immutable object is seen as immutable by all threads, even if a data race is used to pass references to the immutable object between threads. This can provide safety guarantees against misuse of an immutable class by incorrect or malicious code. fields must be used correctly to provide a guarantee of immutability. An object is considered to be completely initialized when its constructor finishes. A thread that can only see a reference to an object after that object has been completely initialized is guaranteed to see the correctly initialized values for that object's fields. The usage model for fields is a simple one: Set the fields for an object in that object's constructor; and do not write a reference to the object being constructed in a place where another thread can see it before the object's constructor is finished. If this is followed, then when the object is seen by another thread, that thread will always see the correctly constructed version of that object's fields. It will also see versions of any object or array referenced by those fields that are at least as up-to-date as the fields are. Example 17.5-1. Fields In The Java Memory Model The program below illustrates how fields compare to normal fields. class FinalFieldExample { final int x; int y; static FinalFieldExample f; public FinalFieldExample() { x = 3; y = 4; } static void writer() { f = new FinalFieldExample(); } static void reader() { if (f != null) { int i = f.x; // guaranteed to see 3 int j = f.y; // could see 0 } } } The class has a field and a non- field . One thread might execute the method and another might execute the method . Because the method writes after the object's constructor finishes, the method will be guaranteed to see the properly initialized value for : it will read the value . However, is not ; the method is therefore not guaranteed to see the value for it. \n\n fields are designed to allow for necessary security guarantees. Consider the following program. One thread (which we shall refer to as thread 1) executes: objects are intended to be immutable and string operations do not perform synchronization. While the implementation does not have any data races, other code could have data races involving the use of objects, and the memory model makes weak guarantees for programs that have data races. In particular, if the fields of the class were not , then it would be possible (although unlikely) that thread 2 could initially see the default value of for the offset of the string object, allowing it to compare as equal to \" \". A later operation on the object might see the correct offset of , so that the object is perceived as being \" \". Many security features of the Java programming language depend upon objects being perceived as truly immutable, even if malicious code is using data races to pass references between threads. \n\n Let o be an object, and c be a constructor for o in which a field f is written. A freeze action on field f of o takes place when c exits, either normally or abruptly. Note that if one constructor invokes another constructor, and the invoked constructor sets a field, the freeze for the field takes place at the end of the invoked constructor. For each execution, the behavior of reads is influenced by two additional partial orders, the dereference chain dereferences() and the memory chain mc(), which are considered to be part of the execution (and thus, fixed for any particular execution). These partial orders must satisfy the following constraints (which need not have a unique solution):\n• Dereference Chain: If an action a is a read or write of a field or element of an object o by a thread t that did not initialize o, then there must exist some read r by thread t that sees the address of o such that r dereferences(r, a).\n• Memory Chain: There are several constraints on the memory chain ordering:\n• If r is a read that sees a write w, then it must be the case that mc(w, r).\n• If r and a are actions such that dereferences(r, a), then it must be the case that mc(r, a).\n• If w is a write of the address of an object o by a thread t that did not initialize o, then there must exist some read r by thread t that sees the address of o such that mc(r, w). Given a write w, a freeze f, an action a (that is not a read of a field), a read r of the field frozen by f, and a read r such that hb(w, f), hb(f, a), mc(a, r ), and dereferences(r , r ), then when determining which values can be seen by r , we consider hb(w, r ). (This happens-before ordering does not transitively close with other happens-before orderings.) Note that the dereferences order is reflexive, and r can be the same as r . For reads of fields, the only writes that are deemed to come before the read of the field are the ones derived through the field semantics. A read of a field of an object within the thread that constructs that object is ordered with respect to the initialization of that field within the constructor by the usual happens-before rules. If the read occurs after the field is set in the constructor, it sees the value the field is assigned, otherwise it sees the default value. In some cases, such as deserialization, the system will need to change the fields of an object after construction. fields can be changed via reflection and other implementation-dependent means. The only pattern in which this has reasonable semantics is one in which an object is constructed and then the fields of the object are updated. The object should not be made visible to other threads, nor should the fields be read, until all updates to the fields of the object are complete. Freezes of a field occur both at the end of the constructor in which the field is set, and immediately after each modification of a field via reflection or other special mechanism. Even then, there are a number of complications. If a field is initialized to a constant expression (§15.28) in the field declaration, changes to the field may not be observed, since uses of that field are replaced at compile time with the value of the constant expression. Another problem is that the specification allows aggressive optimization of fields. Within a thread, it is permissible to reorder reads of a field with those modifications of a field that do not take place in the constructor. class A { final int x; A() { x = 1; } int f() { return d(this,this); } int d(A a1, A a2) { int i = a1.x; g(a1); int j = a2.x; return j - i; } static void g(A a) { // uses reflection to change a.x to 2 } } In the method, the compiler is allowed to reorder the reads of and the call to freely. Thus, could return , , or . \n\n An implementation may provide a way to execute a block of code in a -field-safe context. If an object is constructed within a -field-safe context, the reads of a field of that object will not be reordered with modifications of that field that occur within that -field-safe context. A -field-safe context has additional protections. If a thread has seen an incorrectly published reference to an object that allows the thread to see the default value of a field, and then, within a -field-safe context, reads a properly published reference to the object, it will be guaranteed to see the correct value of the field. In the formalism, code executed within a -field-safe context is treated as a separate thread (for the purposes of field semantics only). In an implementation, a compiler should not move an access to a field into or out of a -field-safe context (although it can be moved around the execution of such a context, so long as the object is not constructed within that context). One place where use of a -field-safe context would be appropriate is in an executor or thread pool. By executing each in a separate -field-safe context, the executor could guarantee that incorrect access by one to a object o will not remove field guarantees for other s handled by the same executor. Normally, a field that is and may not be modified. However, , , and are fields that, for legacy reasons, must be allowed to be changed by the methods , , and . We refer to these fields as being write-protected to distinguish them from ordinary fields. The compiler needs to treat these fields differently from other fields. For example, a read of an ordinary field is \"immune\" to synchronization: the barrier involved in a lock or volatile read does not have to affect what value is read from a field. Since the value of write-protected fields may be seen to change, synchronization events should have an effect on them. Therefore, the semantics dictate that these fields be treated as normal fields that cannot be changed by user code, unless that user code is in the class."
    },
    {
        "link": "https://stackoverflow.com/questions/65896858/how-to-understand-jdk9-memory-model",
        "document": "There is no such thing like “immediately” for updates. Even electricity moves with a finite speed. Generally, asking for a perceivable effect within a particular time span is like asking for a particular execution time for an operation. Neither can be guaranteed, as they are properties of the underlying architecture which the JVM can’t change.\n\nPractically, of course, JVM developers try to make the operations as fast as possible and all that matters to you, as a programmer, is that there is no faster alternative to opaque writes regarding inter-thread visibility of updates. The stronger access modes do not change how fast an update will become visible, they add additional constraints to reordering of reads and writes.\n\nSo in your example, the update will become visible as fast as the architecture and system load will allow1, but don’t ask for actual numbers. No-one can say how long it will take. If you need guarantees in terms of time quantities, you need a special (“real-time”) implementation that can give you additional guarantees beyond the Java Memory Model.\n\n1 To name a practical scenario: thread 1 and 2 may compete for the same CPU. Thread 1 writes the value and continues to run for the operating system specific time before the task is switched (and it’s not even guaranteed that thread 2 is the next one). This implies that a rather long time may elapse, in both terms, wall clock time and thread 1’s progress after the write. Of course, other threads may also make a lot of progress on the other CPU cores in the meanwhile. But it’s also possible that thread 2’s polling before thread 1 commits the write is the cause of thread 1 not getting a chance to write the new value. That’s why you should mark such polling loops with or , to give the execution environment a chance to prevent such scenarios. See this Q&A for a discussion about the difference between the two."
    },
    {
        "link": "https://medium.com/@MrAndroid/java-multithreading-part-1-java-memory-model-fb8e0cfab9d3",
        "document": "The memory model exists both at the language level and at the processor level, but they are not directly related. The language model can provide both weaker and stronger guarantees than the processor model.\n\nThe Java Memory Model (JMM) describes the behavior of a program in a multithreaded environment.\n\nThe JVM defines how the Java virtual machine works with computer memory (RAM) and explains the possible behavior of threads and what a programmer should rely on when developing an application.\n\nCurrent hardware memory architecture is different than java memory model. It’s important to understand how JVM work with hardware memory.\n\nModern computers have several powerful processors. Each of the processors interacts with computer memory. The processor loads data from memory, processes it, and then writes that data to memory.\n\nBut RAM has a slower memory access speed than the processor can handle.\n\nThere is a situation in which the speed of the processor and the speed of memory access has a significant difference.\n\nTo solve the problem of the fact that the processor is in standby mode while it is accessing data in memory, the processors have added their own cache.\n\nThe cache consists of allocated memory for each CPU and register.\n\nA cache is faster than accessing the main memory but slower than accessing its internal registers.\n\nSome CPUs have multiple levels of cache (Level 1, Level 2, etc.).\n\nHow does the CPU read and write data to main memory?\n\n1) CPU reads data from main memory into it’s cache;\n\n2) CPU may read data into it’s internal register from it’s cache;\n\n3) CPU performs operations on the data;\n\n4) CPU writes data back to the cache if necessary;\n\n5) CPU writes data from the cache to main memory as needed.\n\nWhen does the CPU cache clear?\n\nThis happens when the cache is full and the data stored in it is about to be overwritten by new data being loaded into the cache.\n\nThe JVM divides memory between thread stack and the heap.\n\nEach thread has its own stack, a memory are a that stores local variables, object references, and information about which methods the thread has called.\n\nLocal primitive variables on the stack are only visible to the thread that owns it. Even if two threads are executing the same code, they will still create local variables for that code on their own stacks. Thus, each thread has its own version of each local variable.\n\nLocal reference variables, referencing objects on the heap, also only visible to the thread that owns it.\n\nWhat is the heap?\n\nThe heap contains all objects created by the application. Objects reside here regardless of which thread created them.\n\nObjects on the heap are visible to all threads.\n\nWhat are the differences between stack and heap in terms of multithreading?\n\nStack is part of memory witch visibility only for thread. Every thread has your own stack, stack keep locale variables, variables of methods and call stack. Variables in stack not visible for other threads.\n\nHeap is common part of memory. All objects is creating in heap.\n\nTo improve performance, thread can caches values from the heap onto its stack.\n\nThe Java Memory Model and how it maps to Computer Hardware Memory\n\nVariables referenced by the stack of thread and heap can be cached in all levels of the computer’s memory.\n\nThis behaiver of keep data in memroy can create problems:\n\nFirst, threads can lose visibility updates, what was made be other threads to shared objects.\n\nSecond, it’s race conditions. Threads can hold on to stale shared variable state and threads will be see unsynchronized data.\n\nThread can only access its own stack. Local variables created by a thread are not visible for other threads than the thread owner that created it.\n\nThe heap contains all the objects created in your Java application.\n\nLocal variable can be of a primitive type and in this case it is stored on the thread stack or local variable can also be a reference to an object. In this case, the reference (local variable) is stored on the thread stack, but the object itself is stored on the heap.\n\nObject can have methods and this methods can have local variables. This local variables will stored in stack of thread, even if the object is stored on the heap.\n\nLocal varibles of object will stored in heap. It doesn’t matter it’s primitive variable or referensh on objects.\n\nStatic class variables are also stored on the heap.\n\nIf two threads share an object at the same time without synchronization or a proper object update message, then updates to the shared object made by one thread may not be visible to other threads.\n\nLet’s imagine that ThreadA running on CPU1 reads data and caches it in its register while making some changes to this data.\n\nAt the same time, ThreadB that is running on CPU2 is also reading this object’s data.\n\nA situation arises in which each of the threads has its own copy of the shared object, each copy is in the CPU cache.\n\nThis situation will continue until each of the CPUs has flushed its cache to the main memory (RAM).\n\nA program must be correctly synchronized to avoid the kinds of counterintuitive behaviors. Java includes certain language constructs that help solve concurrency problems.\n\nThese are fundamental to correct concurrent programming. As a rule, concurrent errors associated with these categories.\n\nVisibility — determines when activities in one thread are made visible from another thread.\n\nJava Memory Model makes promises objects that are created or altered in one thread may be visible in another thread. And doesn’t matter how many CPU’s or cores we have.\n\nNow imagine that two threads(ThreadA, ThreadB) are created, and ThreadA call’s method work(), and at some point, ThreadB calls stopWork().\n\nBecause there is no happens-before relationship between the two threads, the thread in the loop may never see the update to done performed by the other thread.\n\nhappens-before means that all changes that were made in ThreadB that this operation entailed are visible to ThreadA at the time of its execution.\n\nIn practice, this may happen if the compiler detects that no writes are performed to done in the first thread; \n\nThe compiler may hoist the read of done out of the loop, transforming it into an infinite loop.\n\nAtomicity —atomicity of operations. An atomic operation looks like a single and indivisible processor instruction, which may be either already executed or not yet executed.\n\nAtomicity can be applied to a sequence of actions.\n\nConsider the following example.\n\nNow assume that threadA calls deposit(5), while another ThreadB calls withdraw(5). There is an initial balance of 10. Ideally, at the end of these two calls, there would still be a balance of 10.\n\nThe following situation may occur:\n\nThreadA gets a balance value of 10. At the same time, ThreadB calls the “withdraw” method and writes a balance of 5.\n\nBut ThreadA saved the balance in variable b as 10 and after adding 5 will write 15.\n\nAs a result of this lack of atomicity, the balance is 15 instead of 10.\n\nFor this example, making the deposit() and withdraw() methods synchronized will ensure that the actions of those methods take place atomically.\n\nJava allows 64-bit long and double values to be treated as two 32-bit values. A long write is a 64-bit write operation that is executed as two separate 32-bit operations.\n\nImagine that we have two streams ThreadA and ThreadB. ThreadA do work for set new value for balance and ThreadB do work for get balance and procces it value.\n\nIf you do not synchronize the code written above. This can lead to a situation where ThreadB sees the first 32 bits of a 64-bit value from one entry and the second 32 bits from another entry.\n\nWe don’t have guarantee which jobs are executed first or last.\n\nCompilers can reorder processor instructions to improve performance and execute them in random order until no difference is visible to the thread inside.\n\nConsider what happens if threadOne() gets executed in one thread and threadTwo() gets executed in another.\n\nWould it be possible for threadTwo() to return the value true?\n\nIf ordering is not guaranteed, then the assignments to a and b in threadOne() can be performed out of order.\n\nCompilers have substantial freedom to reorder code in the absence of synchronization. This might result in threadTwo() being executed after the assignment to b, but before the assignment to a.\n\nThis happens because in the threadOne method, the write operation data in the variables a and b are in no way dependent on each other and for one thread, it does not matter in what order the values ​​of these variables will be assigned.\n\nHow do you fix it?\n\nTo do this, we should understand how work threads and synchronize our code.\n\nThread-Safety is the process to make our program safe to use in multithreaded environment.\n\nFor example, we have some user balance to which we access from different threads for reading and writing data. Regardless of how many threads work with it, we guarantee that all threads will receive its up-to-date data.\n\nThere are different ways through which we can make our program thread safe. It’s will be in next topics.\n\nWhat is difference between concurrency and parallelism?\n\nConcurrency is when some tasks can start, run, and complete in some unspecified order. It can be be interleaving instructions via time slicing.\n\nParallelism happening at literally the same time. This requires hardware support (coprocessors, multi-core processors).\n\nAll parallelism is concurrent, but not all concurrency is parallel.\n\nWe have studied how works Java Memory Model and how it maps to Computer Hardware Memory and with problems we can have if your code not synchronized"
    },
    {
        "link": "https://medium.com/@basecs101/how-to-properly-use-synchronized-and-volatile-in-your-java-application-updated-latest-60b882fba4a1",
        "document": "How to Properly Use Synchronized and Volatile in Your Java Application\n\nI have seen many developers a bit confused when it comes to multi-threading. This is primarily because a lot of things happen in the background and many concepts are related to it.\n\nYou should know the basics like Race Condition, Critical section, and Context Switching between threads. I have already covered these concepts and their examples in detail.\n\nIn this article, I’ll be covering two important keywords synchronized and volatile, and their proper usage. This is one of the most asked interview questions from the multi-threading area."
    },
    {
        "link": "https://medium.com/google-developer-experts/on-properly-using-volatile-and-synchronized-702fc05faac2",
        "document": "In the last weeks I have been writing about the transient modifier and the different types of references available in Java. I want to hold the topic of underused/misused topics in Java and bring you this week the volatile and synchronized modifiers .\n\nMultithreading is an entire discipline that takes years to master and properly understand. We will keep a short introduction in this article.\n\nIn computing, a resource can be accessed from different threads concurrently. This can lead to inconsistency and corrupt data. A thread ThreadA accesses a resource and modifies it. In the meantime, the thread ThreadB starts accessing the same resource. Data may get corrupted since it is concurrently being modified. Let´s analyze an example without any kind of protection:\n\nIf you execute this command, the result is eclectic and non-deterministic. Each time you will end up with a different random output. That is because each thread execute in different instants.\n\nBoth modifiers deal with multithreading and protection of code sections from threads accesses. My gut feeling is that synchronized is more widely used and understood than volatile, so I will start my article explaining how it works. We will need it as well later to understand the differences with volatile.\n\nThe synchronized modifier can be applied to an statement block or a method. synchronized provides protection by ensuring that a crucial section of the code is never executed concurrently by two different threads, ensuring data consistency. Let´s apply the modifier synchronized in the previous example to see how it would be protected:\n\nNote that in this example we have added synchronized to the section where the function printCount() runs. If you execute now this function, the result will always be the same:\n\nNow that synchronized has been explained, let´s take a look on volatile.\n\nWe have mentioned before that synchronized modifier could apply to blocks and methods. The first difference between them is that volatile is a modifier that can be applied to fields.\n\nA small note on how Java memory and multithreading work. When we are working with multithreading environments, each thread creates its own copy on a local cache of the variable they are dealing with. When this value is being updated, the update happens first in the local cache copy, and not in the real variable. Therefore, other threads are agnostic about the values that other threads are changing.\n\nHere volatile changes the paradigm. When a variable is declared as volatile, it will not be stored in the local cache of a thread. Instead, each thread will access the variable in the main memory and other threads will be able to access the updated value. Let´s compare all the methods for a proper understanding:\n\nThe first method is non-protected. A thread T1 will access the method, create its own local copy of firstVariable and play with it. In the meantime, T2 and T3 can also access firstVariable and modify its value. T1, T2 and T3 will have their own values of firstVariable, which might not be the same and that have not been copied to the Main memory of java, where the real results are hold.\n\ngetSecondVariable(), on the other hand, accesses a variable that has been declared as volatile. That means, each thread is still able to access the method or block since it has not been protected with synchronized, but they will all access the same variable from the main memory, and will not create their own local copy. Each thread will be accessing the same value.\n\nAnd as we can imagine from the previous examples, getThirdVariable() will only be accessed from one thread at a time. That ensures that the variable will remained synchronized throughout all the threads executions.\n\nAfter reading this short article, a question will likely pop-up in your mind. I understand the theoretical implications, but what are the practical ones? synchronized will probably be easier to understand at this point, but when it is useful to apply the volatile modifier? I like to showcase always an example to provide a better understanding.\n\nThink of a Date variable. The Date variable always need to be the same, seems it updates on a regular pace. Each Thread accessing a Date variable that has been declared as volatile will always show the same value.\n\nThere are two main aspects regarding thread safety. One is execution control, and the other one is memory visibility. Whereas volatile provides memory visibility (all threads will access the same value from the main memory) there is no guarantee of execution control, and this latest can only be achieved via synchronized.\n\nAlso, in Java all read and write operations will be atomic for a volatile variable (including long and double variables). Many platforms perform the operations in long and double in two steps, writing/reading 32 bytes at a time, and allowing two threads to see two different values. This can be avoided by using volatile.\n\nThanks Erik Hellman for your code and article review, you rock.\n\nI write my thoughts about Software Engineering and life in general in my Twitter account. If you have liked this article or it did help you, feel free to share and/or leave a comment. This is the currency that fuels amateur writers."
    },
    {
        "link": "https://stackoverflow.com/questions/9851133/when-to-use-volatile-and-synchronized",
        "document": "indicates that a variable will be shared among several threads. It's used to ensure consistency by \"locking\" access to the variable, so that one thread can't modify it while another is using it.\n\nClassic Example: updating a global variable that indicates the current time\n\n The function must be able to complete uninterrupted because, as it runs, it creates temporary inconsistencies in the value of the global variable . Without synchronization, another function might see a of \"12:60:00\" or, at the comment marked with , it would see \"11:00:00\" when the time is really \"12:00:00\" because the hours haven't incremented yet.\n\nsimply tells the compiler not to make assumptions about the constant-ness of a variable, because it may change when the compiler wouldn't normally expect it. For example, the software in a digital thermostat might have a variable that indicates the temperature, and whose value is updated directly by the hardware. It may change in places that a normal variable wouldn't.\n\nIf is not declared to be , the compiler is free to optimize this:\n\nBy declaring to be , you're telling the compiler that it has to check its value each time it runs through the loop.\n\nIn short, lets you control access to a variable, so you can guarantee that updates are atomic (that is, a set of changes will be applied as a unit; no other thread can access the variable when it's half-updated). You can use it to ensure consistency of your data. On the other hand, is an admission that the contents of a variable are beyond your control, so the code must assume it can change at any time."
    },
    {
        "link": "https://stackoverflow.com/questions/17748078/simplest-and-understandable-example-of-volatile-keyword-in-java",
        "document": "Ideally, if keepRunning wasn't volatile, thread should keep on running indefinitely. But, it does stop after few seconds.\n\nBelow code snippet doesn't work as expected (taken from here ):\n\nBut, what I'm searching for is, a good case example, which shows what would happen if variable wasn't volatile and if it were.\n\nI'm reading about volatile keyword in Java and completely understand the theory part of it.\n\nSynchronization (Locking) --> Guarantees visibility and atomicity (if done properly) Volatile is not a substitute for synchronization Use volatile only when you are updating the reference and not performing some other operations on it. will not be thread safe without use of synchronization or AtomicInteger as incrementing is an compound operation. Why program does not run indefinitely? Well that depends on various circumstances. In most cases JVM is smart enough to flush the contents. Correct use of volatile discusses various possible uses of volatile. Using volatile correctly is tricky, I would say \"When in doubt, Leave it out\", use synchronized block instead. synchronized block can be used in place of volatile but the inverse is not true.\n\nFor your particular example: if not declared volatile the server JVM could hoist the variable out of the loop because it is not modified in the loop (turning it into an infinite loop), but the client JVM would not. That is why you see different results. When a field is declared , the compiler and runtime are put on notice that this variable is shared and that operations on it should not be reordered with other memory operations. Volatile variables are not cached in registers or in caches where they are hidden from other processors, so a read of a volatile variable always returns the most recent write by any thread. The visibility effects of volatile variables extend beyond the value of the volatile variable itself. When thread A writes to a volatile variable and subsequently thread B reads that same variable, the values of all variables that were visible to A prior to writing to the volatile variable become visible to B after reading the volatile variable. The most common use for volatile variables is as a completion, interruption, or status flag: volatile boolean flag; while (!flag) { // do something untill flag is true } Volatile variables can be used for other kinds of state information, but more care is required when attempting this. For example, the semantics of volatile are not strong enough to make the increment operation ( ) atomic, unless you can guarantee that the variable is written only from a single thread. Locking can guarantee both visibility and atomicity; volatile variables can only guarantee visibility. You can use volatile variables only when all the following criteria are met:\n• Writes to the variable do not depend on its current value, or you can ensure that only a single thread ever updates the value;\n• The variable does not participate in invariants with other state variables; and\n• Locking is not required for any other reason while the variable is being accessed. Debugging tip: be sure to always specify the JVM command line switch when invoking the JVM, even for development and testing. The server JVM performs more optimization than the client JVM, such as hoisting variables out of a loop that are not modified in the loop; code that might appear to work in the development environment (client JVM) can break in the deployment environment (server JVM). This is an excerpt from \"Java Concurrency in Practice\", the best book you can find on this subject.\n\nWhat is the keyword? The keyword prevents caching of variables. Consider this code, first without the keyword: class MyThread extends Thread { private boolean running = true; //non-volatile keyword public void run() { while (running) { System.out.println(\"hello\"); } } public void shutdown() { running = false; } } public class Main { public static void main(String[] args) { MyThread obj = new MyThread(); obj.start(); Scanner input = new Scanner(System.in); input.nextLine(); obj.shutdown(); } } Ideally, this program should print until the key is pressed. But on some machines it may happen that the variable is cached and you cannot change its value from the method which results in infinite printing of the text. Thus, by using the keyword, it is guaranteed that your variable will not be cached and the code will run fine on all machines. Using the keyword is a good and safer programming practice.\n\n: Volatile Keyword is applicable to variables. volatile keyword in Java guarantees that value of the volatile variable will always be read from main memory and not from Thread's local cache. Volatile Field: An indication to the VM that multiple threads may try to access/update the field's value at the same time. To a special kind of instance variables which has to shared among all the threads with Modified value. Similar to Static(Class) variable, Only one copy of volatile value is cached in main memory, So that before doing any ALU Operations each thread has to read the updated value from Main memory after ALU operation it has to write to main memory direclty. (A write to a volatile variable v synchronizes-with all subsequent reads of v by any thread) This means that changes to a volatile variable are always visible to other threads. Here to a if Thread t1 changes the value in t1's cache, Thread t2 can't access the changed value untill t1 writes, t2 read from main memory for the most recent modified value, which may lead to . +--------------+--------+-------------------------------------+ | Flag Name | Value | Interpretation | +--------------+--------+-------------------------------------+ | ACC_VOLATILE | 0x0040 | Declared volatile; cannot be cached.| +--------------+--------+-------------------------------------+ |ACC_TRANSIENT | 0x0080 | Declared transient; not written or | | | | read by a persistent object manager.| +--------------+--------+-------------------------------------+ : Memory that can be shared between threads is called shared memory or heap memory. All instance fields, static fields, and array elements are stored in heap memory. Synchronization: synchronized is applicable to methods, blocks. allows to execute only 1-thread at a time on object. If t1 takes control, then remaining threads has to wait untill it release the control. public class VolatileTest implements Runnable { private static final int MegaBytes = 10241024; private static final Object counterLock = new Object(); private static int counter = 0; private static volatile int counter1 = 0; private volatile int counter2 = 0; private int counter3 = 0; @Override public void run() { for (int i = 0; i < 5; i++) { concurrentMethodWrong(); } } void addInstanceVolatile() { synchronized (counterLock) { counter2 = counter2 + 1; System.out.println( Thread.currentThread().getName() +\"\\t\\t « InstanceVolatile :: \"+ counter2); } } public void concurrentMethodWrong() { counter = counter + 1; System.out.println( Thread.currentThread().getName() +\" « Static :: \"+ counter); sleepThread( 1/4 ); counter1 = counter1 + 1; System.out.println( Thread.currentThread().getName() +\"\\t « StaticVolatile :: \"+ counter1); sleepThread( 1/4 ); addInstanceVolatile(); sleepThread( 1/4 ); counter3 = counter3 + 1; sleepThread( 1/4 ); System.out.println( Thread.currentThread().getName() +\"\\t\\t\\t\\t\\t « Instance :: \"+ counter3); } public static void main(String[] args) throws InterruptedException { Runtime runtime = Runtime.getRuntime(); int availableProcessors = runtime.availableProcessors(); System.out.println(\"availableProcessors :: \"+availableProcessors); System.out.println(\"MAX JVM will attempt to use : \"+ runtime.maxMemory() / MegaBytes ); System.out.println(\"JVM totalMemory also equals to initial heap size of JVM : \"+ runtime.totalMemory() / MegaBytes ); System.out.println(\"Returns the amount of free memory in the JVM : \"+ untime.freeMemory() / MegaBytes ); System.out.println(\" ===== ----- ===== \"); VolatileTest volatileTest = new VolatileTest(); Thread t1 = new Thread( volatileTest ); t1.start(); Thread t2 = new Thread( volatileTest ); t2.start(); Thread t3 = new Thread( volatileTest ); t3.start(); Thread t4 = new Thread( volatileTest ); t4.start(); Thread.sleep( 10 );; Thread optimizeation = new Thread() { @Override public void run() { System.out.println(\"Thread Start.\"); Integer appendingVal = volatileTest.counter2 + volatileTest.counter2 + volatileTest.counter2; System.out.println(\"End of Thread.\" + appendingVal); } }; optimizeation.start(); } public void sleepThread( long sec ) { try { Thread.sleep( sec * 1000 ); } catch (InterruptedException e) { e.printStackTrace(); } } } Static[ ] vs Volatile[ ] - Both are not cached by threads\n• None Static fields are common to all threads and get stored in Method Area. Static with volatile no use. Static field cant be serialized.\n• None Volatile mainly used with instance variable which get stored in heap area. The main use of volatile is to maintain updated value over all the Threads. instance volatile field can be Serialized.\n\nIdeally, if keepRunning wasn't volatile, thread should keep on running indefinitely. But, it does stop after few seconds. If you are running in a single-processor or if your system is very busy, the OS may be swapping out the threads which causes some levels of cache invalidation. Not having a doesn't mean that memory will not be shared, but the JVM is trying to not synchronize memory if it can for performance reasons so the memory may not be updated. Another thing to note is that is synchronized because the underlying does synchronization to stop overlapping output. So you are getting memory synchronization \"for free\" in the main-thread. This still doesn't explain why the reading loop sees the updates at all however. Whether the lines are in or out, your program spins for me under Java6 on a MacBook Pro with an Intel i7. Can anyone explain volatile with example ? Not with theory from JLS. I think your example is good. Not sure why it isn't working with all statements removed. It works for me. Is volatile substitute for synchronization ? Does it achieve atomicity ? In terms of memory synchronization, throws up the same memory barriers as a block except that the barrier is uni-directional versus bi-directional. reads throw up a load-barrier while writes throw up a store-barrier. A block is a bi-directional barrier with the addition of mutex locking. In terms of , however, the answer is \"it depends\". If you are reading or writing a value from a field then provides proper atomicity. However, incrementing a field suffers from the limitation that is actually 3 operations: read, increment, write. In that case or more complex mutex cases, a full block may be necessary. solves the issue with a complicated test-and-set spin-loop.\n\nis not going to necessarily create giant changes, depending on the JVM and compiler. However, for many (edge) cases, it can be the difference between optimization causing a variable's changes to fail to be noticed as opposed to them being correctly written. Basically, an optimizer may choose to put non-volatile variables on registers or on the stack. If another thread changes them in the heap or the classes' primitives, the other thread will keep looking for it on the stack, and it'll be stale. ensures such optimizations don't happen and all reads and writes are directly to the heap or another place where all threads will see it."
    },
    {
        "link": "https://bito.ai/resources/java-volatile-vs-synchronized-java-explained",
        "document": "Understanding Java volatile and synchronized is essential for Java developers, as it helps them maintain safe, concurrent access of data in a multi-threaded environment. With this guide, we’ll explore what each of these concepts means, their similarities and differences, and useful examples. After reading this article, you should have a better grasp on when and why to use volatile and synchronized in your code.\n\nWhat is Volatile in Java?\n\nVolatile is a keyword in Java language that marks an attribute or variable as “volatile”—meaning it can be accessed and changed concurrently by multiple threads. This keyword is used to alert the JVM (Java Virtual Machine) that the variable could be changed unexpectedly. The volatile keyword guarantees visibility of changes across threads – meaning regardless of age of the cached copies, all threads will see the same value. It also ensures that each read is taken directly from main memory and not thread-local caches.\n\nSince a volatile variable is not cached, it can be slightly slower to use in comparison to other immutable variables. Nevertheless, the advantages of having reliable and read-write permission granted across threads make it a great practice. It ensures safe concurrent access to data and can be used to check for certain conditions in a loop.\n\nWhat is Synchronized in Java?\n\nSynchronized is another keyword used to mark a method or block of code as “synchronized” and guarantee mutual exclusion, so that only one thread can execute the code at a time. This keyword has been part of the Java language since version 1.0 and serves an important role in enabling thread-safe execution. The synchronized keyword is used around a method or a block of code to add lock to the code segment.\n\nThe advantage here is that the lock is acquired on class level (static synchronized) or on object level lock (non-static synchronized), which makes it suitable for cases when synchronization needs to be done between threads. Furthermore, synchronized blocks are implemented using monitors in Java and the waiting threads will get notified when the objects lock can be acquired.\n\nBoth volatile and synchronized keywords have a role in supporting thread safety. However, they accomplish this in different ways.\n• Visibility: Volatile makes values written by one thread visible to other threads immediately. It achieves this by telling the JVM not to retrieve any cached copies of the field in question, only main memory values.\n• Threads: Synchronized restricts access around a particular piece of code to one thread at a time. No other thread can enter the same piece of code until the original thread exits it. It does this by using locking mechanisms.\n• Use Cases: As accessibility is the primary focus for volatile variables, they are used for synchronization issues related to visibility. Synchronization is used for control access problem where multiple threads compete for shared resources.\n\nUses of Volatile and Synchronized\n\nVolatile is best used to guarantee visibility across threads. When you need a variable to serve as a “checkpoint” so that multiple threads modify or refer to data coherently, using volatile becomes important. For example, if you have multiple threads working together to create an object, using volatile could stop those threads from working on incomplete or inconsistent objects.\n\nSimilarly, synchronized keyword is useful when it’s necessary to ensure only one thread at a time can execute a critical section of code. It prevents race conditions from happening where multiple threads attempt to modify a single resource at the same time. Critical sections of code that must be accessed sequentially – such as methods that rely on in-memory caches – should use synchronized for thread safety.\n\nPros and Cons of Volatile and Synchronized\n\nVolatile and synchronized can both help you write thread-safe code, but their ways of implementation can lead to pros and cons.\n• Cons: Does not provide absolute protection across threads.\n• Pros: Guarantees mutual exclusion; visibility of changes across threads.\n• Cons: Costly process as threads must acquire a lock before accessing code; holding locks on shared resources can also lead to deadlocks.\n\nPerformance Impact of Using Volatile and Synchronized\n\nUsing any kind of synchronization comes with a performance cost – as mentioned earlier, being able to access objects across multiple threads requires each thread to acquire a lock on the resource before accessing it. This can cause an overhead on the performance of an application, leading to slower and more resource consuming operations.\n\nSimilarly, when it comes to volatile variables, because the changes are taken from main memory instead of the thread-local cache, this comes with an performance impact due to frequent read and write operations on memory.\n\nSuppose you have a resource (an object) that multiple threads are trying to use simultaneously. To prevent race conditions where two or more threads overwrite data while modifying the object, you can use synchronized keyword around key parts of the code that are updating the data.\n\n~~~ public class MyResource { public synchronized void updateResource() { // update logic goes here } public void useResource() { // read logic goes here } } ~~~\n\nIn this example, any attempt by multiple threads to access useResource method will be allowed, while updateResource method will force any other thread to wait until the first one finishes.\n\nSimilarly, using volatile variable also helps in thread safety when you have multiple threads reading and writing common variable(s).\n\n~~~ public class MyApp { public static volatile int counter = 0; public static void main(String args[]) throws InterruptedException { //we have created two threads t1 and t2 accessing increment() and decrement() method respectively Runnable r1 = () -> increment(); Runnable r2 = () -> decrement(); Thread t1 = new Thread(r1); Thread t2 = new Thread(r2); t1.start(); t2.start(); t1.join(); t2.join(); //will print 0 System.out.println(“Counter value:” + counter); } public static void increment() { counter++; // can induce visibility problems in absence of volatile } public static void decrement() { counter–; // can induce visibility problems in absence of volatile } } ~~~\n\nThe counter variable increments and decrements in different threads so it must be declared volatile to ensure all threads will always see the same value.\n\nTroubleshooting Tips for Using Java Volatile and Synchronized\n\nSynchronizing your code is generally easier than volatile variables because all you need to do is add synchronization around certain sections of code. Volatile requires more changes such as changing access permissions of the fields being used by several threads.\n\nIn addition to this, opting for immutability (where variables cannot be changed) helps a great deal when writing concurrent applications as they are inherently thread-safe. A good example of an immutability design pattern is using immutable objects as map keys etc.\n\nVolatile and synchronized are integral elements of Java language when it comes to developing multi-threaded applications. As we saw in this article, both keywords work on different levels – while volatile helps make values written by one thread visible to other threads, synchronized increases concurrency control around data sections that can potentially introduce race conditions."
    }
]