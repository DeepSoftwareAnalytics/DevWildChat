[
    {
        "link": "https://ninjaone.com/blog/what-is-server-management-tools-and-best-practices",
        "document": "In the digital age, organizations depend more on IT than ever before. The foundation of many IT functions — including data storage, website hosting, emails, and software — is server management. Without reliable, functioning servers, most IT functionality would collapse.\n\nMany businesses have migrated internal IT to cloud services using servers located in remote data centers, but a significant number still have in-house servers or use a hybrid environment of in-house and cloud services. Regardless of location, managing a server entails monitoring and maintaining hardware, software, security, and backups.\n\nIn this guide, NinjaOne explores server management from the perspective of today’s server technology, security concerns, and helpful software tools.\n\nWhat this article will cover:\n• Server management is defined for IT professionals\n• What are server management best practices?\n\nThe key objectives of server management are:\n• Scale servers and related operations to meet the needs of the organization over time\n\nThe overall impact of server management on IT is quite comprehensive, making its scope an umbrella that covers nearly everything the department handles. Let’s take a closer look at some of the specifics within this broad-reaching concept of managing a server infrastructure:\n\nStarting with the foundation of effective server management, we have the hardware. Everything depends on functioning hardware. Within this wider subject, there are a few key hardware elements that should be monitored and managed closely as part of any server management strategy:\n\nEssentially the brain of a server, the CPU performs all the calculations to make programs run. Because they’re not only essential but heavily used, CPUs need to be constantly monitored to avoid overtaxing them — a problem that can result in everything from slowed operations to complete system crashes. There are several ways to address an overtaxed CPU. Upgrading is the most obvious option, but you can also add more CPU resources from another asset, halt resourcing-hogging processes, or fine-tune system-wide performance to take the load off of the CPU.\n\nDoing all of that work makes CPUs run hot. Servers, in general, produce ample amounts of heat, which is why server farms are at times built in cold locations (even underground or underwater). If CPUs run too hot, they can fail with disastrous results.\n\nServers are built with cooling systems and thermometers that allow for easier server management, even by remote. If a server’s temperature gets too high, an IT technician can shut down the hardware and assess the situation before the heat goes critical. Overheating issues are often caused by excess strain on the system or failed cooling devices.\n\nRAM is a server’s working memory, the temporary storage used for fast operations and caching. RAM has a direct correlation to a system’s performance, especially in the cases of certain high-demand software. Running out of RAM during normal use can impede performance and may prevent certain applications from running at all.\n\nThe hard drive or hard disks provide persistent storage for the server. Important data is stored here, and because hard disks are also used for caching in many cases, they can create performance issues when they’re near capacity.\n\nHard drive usage needs to be monitored so that the required amount of storage capacity is always available when needed. Additionally, hard drive health should be watched closely to prevent costly failures resulting in lost data.\n\nThe server should be kept in a location with optimal environmental conditions. Humidity should be kept in check, and the server room should allow for optimal cooling. For security reasons, servers should only be accessible to authorized individuals.\n\nYour IT infrastructure also depends on software to function. As such, software should be subject to constant monitoring and scheduled maintenance, just like hardware. Understanding the software within your IT environment makes it easier to identify performance issues and perform troubleshooting.\n\nSecurity is a key concern in all aspects of IT, and server management should involve keeping a secure network from the inside out. While security policies differ depending on the organization, there are several standard considerations for most use cases:\n• Staying on top of all software and firmware updates (using a when possible)\n• Install and configure firewalls to keep out unauthorized network traffic\n• Incorporate tools and procedures demanded by security best practices and any relevant compliance standards\n\nA critical concern for security and business continuity is regular backups and backup testing. Data loss from a disaster or a ransomware attack can cripple most organizations — full server backups and a robust backup solution can be a lifesaver in these situations. Options for backup include local, cloud, and server backup software to support both physical and virtual servers.\n\nManagement of backups is an important consideration here. Not only do backups need to be properly configured for the use case, but they should also be regularly tested to ensure functionality before they’re needed. An IT professional that needs to manage backups for many different clients and workstations across multiple networks — such as a managed service provider — will need a multi-tenant solution with a single portal for easy management.\n\nThe server’s power supply should also have a fallback to ensure data isn’t lost during a power outage. Many options are available for this function, including uninterruptible power supplies (USPs) with built-in surge protection, power conditioning, and emergency power that can keep the server running for a short time during an outage.\n\nVirtualization is commonplace in modern IT and brings its own considerations. A physical server usually runs one instance on a single piece of hardware, but a virtual server can allow multiple servers to be hosted on one machine.\n\nVirtual servers, or virtual machines (VM), allow you to do more with less hardware. While convenience is embraced by the IT community, virtual environments can be slightly more complex to manage than physical servers. That said, the same management principles that apply to traditional server management also apply to virtual servers.\n\nOrganizations have a few choices to make when it comes to server management. Not every organization needs (or can budget for) an in-house team to manage their servers and IT environment.\n\nIf personnel or costs aren’t an issue, internal management provides the advantage of having total control of your server environments. If your own IT team will be handling server management, it’s important that they have the right tools for the task. Many tools are available for system administrators, giving them features like automation, notifications, and reporting that make the job easier to manage.\n\nRemote monitoring and management tools can offer many of these essential functions while also giving your team the ability to interface with the server and make adjustments or repairs remotely. For large enterprises, this capability is nearly essential.\n\nOrganizations that don’t want to take on the task of server management internally have the option to go with external server management. By working with a managed service provider or other outside IT company, they can put the responsibility of server monitoring and maintenance on outsourced professionals.\n\nThe NinjaOne endpoint management platform provides a suite of remote tools operating entirely in the background providing IT organizations with an array of server management options and capabilities. These include secure remote access to servers, the ability to remediate issues through a technician’s actions or through automation, patch operating systems, and third-party applications, and update configurations in real-time.\n\nModern organizations rely on their IT to function. The foundation of a reliable, working IT environment is servers that are tracked and maintained by professionals. There are several best practices for managing the hardware and software involved in server operation, and when followed, these guidelines can help ensure working, efficient technology with minimal downtime.\n\nSeveral tools are available to aid in managing servers, but for organizations without an in-house IT team, the best solution may be to outsource this task to outside professionals."
    },
    {
        "link": "https://curatepartners.com/blogs/skills-tools-platforms/optimizing-it-infrastructure-the-importance-and-best-practices-of-server-management",
        "document": "In today’s digital age, where businesses rely heavily on technology for their operations, effective server management is more critical than ever. Servers are the backbone of IT infrastructure, supporting a myriad of services, applications, and data storage needs. This article delves into the intricacies of server management, its key aspects, and how Curate Consulting Services can assist businesses in finding specialized talent to manage and optimize their server environments.\n\nServer management refers to the process of overseeing and maintaining the operations, configurations, and security of servers within a computing environment. It encompasses a range of tasks related to the setup, monitoring, optimization, and troubleshooting of server hardware and software to ensure reliable and efficient performance. Effective server management is crucial for supporting various IT services, applications, and data storage.\n\nThe initial setup and deployment of servers involve configuring hardware, installing operating systems, and preparing the server environment for specific tasks. This foundational step in server management ensures that the server is ready to support the desired applications and services.\n\nManaging server configurations involves defining and maintaining the settings, parameters, and software installations on servers. Configuration management tools, such as Ansible, Puppet, and Chef, help ensure consistency across multiple servers and facilitate quick updates or changes. This consistency is vital for maintaining stability and reducing the risk of configuration-related issues.\n\nRegular monitoring of server performance is essential for maintaining optimal operation. This involves tracking key metrics such as CPU usage, memory utilization, disk I/O, and network activity. Monitoring tools like Nagios and Microsoft System Center provide insights into resource usage, enabling administrators to identify and address performance bottlenecks promptly.\n\nServer security is a critical aspect of server management. Implementing robust security measures, such as access controls, firewalls, intrusion detection systems, and regular security updates, is essential to protect servers from unauthorized access, data breaches, and malicious activities. Security management aims to safeguard sensitive data and maintain the integrity of the server environment.\n\nKeeping servers up-to-date with the latest software patches and updates is crucial for addressing security vulnerabilities and improving overall stability. Patch management involves scheduling and applying updates without disrupting server operations, ensuring that servers remain secure and perform optimally.\n\nImplementing robust backup and recovery strategies is vital for safeguarding data against accidental deletion, hardware failures, or other disasters. Server management includes defining backup schedules, testing recovery processes, and ensuring data integrity. Effective backup and recovery practices minimize downtime and data loss, ensuring business continuity.\n\nDepending on the workload, server resources may need to be scaled up or down. Server management involves assessing resource demands, optimizing configurations, and scaling infrastructure to meet performance requirements. This flexibility ensures that servers can handle varying workloads efficiently.\n\nIn environments using virtualization technologies, administrators manage virtual machines (VMs) and hypervisors. This includes creating, configuring, and monitoring VMs, as well as optimizing resource allocation. Virtualization management tools, such as VMware and Hyper-V, help administrators maintain a flexible and efficient server environment.\n\nServer management includes the implementation of logging and auditing mechanisms. Logging captures events and activities on servers, providing valuable data for troubleshooting and security analysis. Auditing ensures compliance with security policies and regulations, helping organizations maintain a secure and compliant server environment.\n\nAdministrators manage user accounts, permissions, and access controls on servers. This involves creating, modifying, or deactivating user accounts, as well as defining roles and permissions. Effective user and access management practices ensure that only authorized personnel have access to sensitive data and systems.\n\nServer administrators are responsible for responding to incidents, diagnosing problems, and troubleshooting issues that may arise. This includes analyzing logs, identifying root causes, and implementing solutions to restore normal server operations. Prompt and effective incident response minimizes downtime and maintains the reliability of IT services.\n\nSeveral tools are available to assist in server management. These tools automate routine tasks, provide monitoring capabilities, and offer centralized control over server configurations. Examples include:\n• Ansible: An automation tool for configuration management, application deployment, and task automation.\n• Puppet: A configuration management tool that automates the management of server infrastructure.\n• Chef: An automation platform that transforms infrastructure into code, enabling configuration management and continuous delivery.\n• Nagios: A monitoring tool that provides comprehensive monitoring of IT infrastructure, including servers.\n• Microsoft System Center: A suite of tools for managing and monitoring IT environments, including servers, applications, and networks.\n\nEffective server management is vital for ensuring the availability, performance, and security of IT services. Properly managed servers contribute to the reliability of applications, data storage, and network connectivity, supporting the overall functionality of an organization’s IT infrastructure. Key benefits of effective server management include:\n• Enhanced Performance: Regular monitoring and optimization ensure that servers operate at peak performance, minimizing latency and maximizing efficiency.\n• Increased Security: Robust security measures protect servers from threats, reducing the risk of data breaches and unauthorized access.\n• Improved Reliability: Consistent configuration management and timely updates minimize the risk of system failures and downtime.\n• Scalability: Effective resource scaling ensures that servers can handle varying workloads, supporting business growth and flexibility.\n• Business Continuity: Robust backup and recovery strategies safeguard data and ensure continuity in the event of a disaster.\n\nIn the competitive world of technology, having the right talent is crucial for leveraging advanced server management practices. Curate Consulting Services specializes in identifying and placing top-tier talent to meet the unique needs of businesses. Here’s how we can help:\n\nOur team has a deep understanding of the technology landscape, including the specific skills required for effective server management. We can help you find candidates who not only have technical expertise but also a passion for innovation and problem-solving.\n\nEvery business is unique, and so are its staffing needs. We offer tailored solutions that align with your company’s goals and culture. Whether you need permanent hires, contract staff, or specialized project teams, we can provide the right talent for your requirements.\n\nWith years of experience in the industry, we have built an extensive network of professionals across various domains. This network allows us to quickly identify and connect you with candidates who are the perfect fit for your needs.\n\nBy providing you with the right talent, we enable your business to innovate and grow. Our candidates bring fresh perspectives and advanced skills, helping you stay ahead in the competitive landscape.\n\nFrom identifying potential candidates to onboarding them, we handle the entire recruitment process. Our comprehensive approach ensures a seamless experience for both employers and candidates, saving you time and resources.\n\nServer management is a critical component of IT infrastructure, ensuring the smooth operation, security, and performance of servers. With the right practices and tools, businesses can optimize their server environments to support their broader objectives. However, leveraging advanced server management practices requires specialized talent.\n\nCurate Consulting Services is dedicated to helping businesses find the right professionals to manage and optimize their servers. By connecting you with skilled candidates, we ensure that your IT infrastructure is managed efficiently and effectively, supporting your business’s success and growth.\n\nIn the rapidly evolving world of technology, staying ahead means embracing best practices in server management and having the right team to implement them. With Curate Consulting Services, you can achieve new heights of performance and reliability in your IT infrastructure, delivering unparalleled services to your clients and stakeholders."
    },
    {
        "link": "https://quora.com/What-are-the-best-practices-for-setting-up-a-new-server-configuration",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://cloud.google.com/architecture/connected-devices/best-practices-provisioning-configuring-bare-metal",
        "document": "This document suggests best practices for designing and implementing reliable and automated provisioning and configuration processes for devices running at the edges of your environment such as the following:\n• Edge devices, such as Internet of Things (IoT) devices, microcomputers and microcontrollers\n\nRead this document if you design provisioning and configuration processes for edge devices and bare metal systems and servers, or if you want to learn more about best practices for provisioning and configuring these device types.\n\nThis document doesn't list all of the possible best practices for provisioning and configuring edge and bare metal systems and servers, and it doesn't give you guarantees of success. Instead, it helps you to stimulate discussions about potential changes and improvements to your provisioning and configuration processes.\n\nThis document is part of a series of documents that provide information about IoT architectures on Google Cloud. The other documents in this series include the following:\n• Best practices for running an IoT backend on Google Cloud\n• Best practices for automatically provisioning and configuring edge and bare metal systems and servers (this document)\n\nManually provisioning and configuring a large fleet of devices is prone to human error and doesn't scale as your fleet grows. For example, you might forget to run a critical provisioning or configuration task, or you might be relying on partially or fully undocumented processes. Fully automated and reliable provisioning and configuration processes help solve these issues. They also help you manage the lifecycle of each device from manufacture to decommissioning to disposal.\n\nThe following terms are important for understanding how to implement and build automated provisioning and configuration processes for your devices:\n• Edge device: A device that you deploy at the edges of your environment that is proximate to the data you want to process.\n• Provisioning process: The set of tasks that you must complete to prepare a device for configuration.\n• Configuration process: The set of tasks you must complete to make a device ready to operate in a specific environment.\n• Configuration management: The set of tasks that you continuously perform to manage the configuration of your environment and devices.\n• Base image: A minimal working operating system (OS) or firmware image produced by your company or produced by a device or OS manufacturer.\n• Golden image: An immutable OS or firmware image that you create for your devices or prepare from a base image. Golden images include all data and configuration information that your devices need to accomplish their assigned tasks. You can prepare various golden images to accomplish different tasks. Synonyms for golden image types include flavors, spins, and archetypes.\n• Silver image: An OS or firmware image that you prepare for your devices by applying minimal changes to a golden image or a base image. Devices running a silver image complete their provisioning and configuration upon the first boot, according to the needs of the use cases that those devices must support.\n• Seed device: A device that bootstraps your environment without external dependencies.\n• Network booting: The set of technologies that lets a device obtain software and any related configuration information from the network, instead of from a storage system that's attached to the device.\n\nTo help you set goals and avoid common issues, apply the best practices for provisioning and configuration that are described in the following sections.\n\nDuring their first boot, or anytime it's necessary, your devices should be able to provision and configure themselves using only the software image installed inside them.\n\nTo avoid implementing the logic you need during the provisioning and configuration processes, you can use tools that give you the primitives needed to orchestrate and implement those processes. For example, you can use cloud-init and its NoCloud data source, together with scripting or a configuration management tool, such as Ansible, Puppet, or Chef, running against the local host.\n\nTo design reliable provisioning and configuration processes, ensure that all the steps and tasks performed during those processes are valid, possibly in an automated manner. For example, you can use an automated compliance testing framework, such as InSpec, to verify that your provisioning and configuration processes are operating as expected.\n\nThis best practice helps you avoid single points of failure and the need for manual intervention when you need to complete device provisioning and configuration.\n\nWhen designing your edge devices, minimize their variance in terms of purpose and specialty. This recommendation doesn't mean that all your edge devices must be equal to each other or share the same purpose, but they should be as homogeneous as possible. For example, you might define device archetypes by the workload types they need to support. Then you can deploy and manage your devices according to the properties of those archetypes.\n\nTo ensure that you're following this best practice, verify that you can pick a device at random from the ones of a given archetype and then do the following:\n• Treat the device like you would other devices of the same archetype. Doing so shows that you have operational efficiency.\n• Replace the device with devices of the same archetype without additional customizations. Doing so shows that you have correctly implemented those archetypes.\n\nThis best practice ensures that you reduce the variance in your fleet of devices, leading to less fragmentation in your environment and in the provisioning and configuration processes.\n\nUse seed devices to bootstrap your environment\n\nWhen provisioning and configuring your devices, you might come across a circular dependency problem: your devices need supporting infrastructure to provision and configure themselves, but that infrastructure isn't in place because you still have to provision and configure it.\n\nYou can solve this problem with seed devices. Seed devices have a temporary special purpose. After completing the tasks for which the special purpose was designed, the device conforms its behavior and status to the relevant archetype.\n\nFor example, if you're using cloud-init to automatically initialize your devices, you might need to configure a cloud-init NoCloud data source in the following ways:\n• Provide the NoCloud data source data to the seed device through a file system.\n• None Wait for the seed device to complete its own provisioning and configuration with its special purpose, which includes serving the NoCloud data source data to other devices over the network. The provisioning and configuration processes on the seed device then wait until the conditions to drop the seed device's temporary special purpose are met. Some examples of these conditions are:\n• Are there other devices in the environment that serve the NoCloud data source data over the network?\n• Are there enough nodes in the cluster?\n• Did the first backup complete?\n• None Provision and configure other devices that download the NoCloud data source data over the network from the seed device. Some devices must be able to serve the NoCloud data source data over the network.\n• None The provisioning and configuration processes on the seed device resume because the conditions to drop the special purpose of the seed device are met: there are other devices in the fleet that serve the NoCloud data source data over the network.\n• None The provisioning and configuration processes on the seed device drop the special purpose, making the seed device indistinguishable from other devices of the same archetype.\n\nThis best practice ensures that you can bootstrap your environment even without supporting infrastructure and without contravening the Avoid special-purpose devices best practice.\n\nMinimize the statefulness of your devices\n\nWhen designing your edge devices, keep the need to store stateful information at minimum. Edge devices might have limited hardware resources, or be deployed in harsh environments. Minimizing the stateful information that they need to function simplifies the provisioning, configuration, backup, and recovery processes because you can treat such devices homogeneously. If a stateless edge device starts to malfunction and it's not recoverable, for example, you can swap it with another device of the same archetype with minimal disruptions or data loss.\n\nThis best practice helps you avoid unanticipated issues due to data loss, or due to your processes being too complex. Most complexity comes from the need to support a fleet of heterogeneous devices.\n\nTo avoid expensive provisioning and configuration tasks when your devices first boot, and to spare device resources, customize the OS and firmware images before making them available. You could, for example, install dependencies directly in the image instead of installing them when each device boots for the first time.\n\nWhen preparing the OS and firmware images for your devices, you start from a base image. When you customize the base image, you can do the following:\n• Produce golden images. Golden images contain all dependencies in the image so that your devices don't have to install those dependencies on first boot. Producing golden images might be a complex task, but they enable your devices to save time and resources during provisioning and configuration.\n• Produce silver images. Unlike golden images, devices running silver images complete all provisioning and configuration processes during their first boot. Producing silver images can be less complex than producing golden images, but the devices running a silver image spend more time and resources during provisioning and configuration.\n\nYou can customize the OS and firmware images as part of your continuous integration and continuous deployment (CI/CD) processes, and automatically make the customized images available to your devices after validation. The CI/CD processes that you implement with a tool such as Cloud Build, GitHub Actions, GitLab CI/CD, or Jenkins, can perform the following sequence of tasks:\n• Perform an automated validation against the customized images.\n• Publish the customized images in a repository where your devices can obtain them.\n\nIf your CI/CD environment and the OS or firmware for which you need to build images use different hardware architectures, you can use tools like QEMU to emulate those architectures. For example, you can emulate the hardware architecture of the ARM family on an x86_64 architecture.\n\nTo customize your OS or firmware images, you need to be able to modify them and verify those modifications in a test environment before installing them in your edge devices. Tools like chroot let you virtually change but not physically change the root directory before running a command.\n\nThis best practice helps you customize OS and firmware images before making the images available to your devices.\n\nIf your devices support heterogeneous workloads, you can use the following tools to orchestrate those workloads and manage their lifecycle:\n• A workload orchestration system: Using a workload orchestration system, such as Kubernetes, is suitable for workloads that have complex orchestration or lifecycle management requirements. These systems are also suitable for workloads that span multiple components. In both cases, it means you don't have to implement that orchestration and workload lifecycle management logic by yourself. If your devices are resource-constrained, you can install a lightweight Kubernetes distribution that needs fewer resources than the canonical one, such as MicroK8s, K3s, or Google Distributed Cloud installed with the edge profile.\n• None An init system: Using an init system, like systemd, is suitable for workloads with the following characteristics:\n• Workloads that can't be placed in containers\n\nAfter you have system in place to orchestrate your workloads, you can also use it to run tasks that are part of your provisioning and configuration processes. If you need to run a configuration management tool as part of your provisioning and configuration processes, for example, you can use the workload orchestration system as you would with any other workload.\n\nThis best practice helps ensure that you can orchestrate the workloads running on your devices.\n\nWhen you need to verify if your devices need to connect to external systems, such as other devices or to a backend, consider the recommendations in the following subsections.\n\nThis best practice helps you:\n• Avoid potential backdoors that circumvent the security perimeter of your devices.\n• Verify that your devices don't expose unauthorized interfaces that an attacker might exploit.\n• Authenticate other parties that are making information requests before exchanging any information.\n• Rely on trusted execution environments to handle secrets, such as encryption keys, authentication keys, and passwords.\n• Verify the integrity and the authenticity of any OS or firmware image before use.\n• Verify the validity, the integrity, and the authenticity of any user-provided configuration.\n• Limit the attack surface by not installing unnecessary software and removing any that already exists on your devices.\n• Limit the use of privileged operations and accounts.\n• Verify the integrity of the device's case if that case needs to resist physical manipulation and tampering.\n• Avoid leaving privileged access open, such as the following:\n• Virtual or physical serial ports and serial consoles with elevated privileges, even if the ports are accessible only if someone physically tampers with the device.\n• Endpoints that respond to requests coming from the network and that can run privileged operations.\n• Don't rely on hardcoded credentials in your OS or firmware images, configuration, or source code.\n• Don't reveal any information that might help an adversary gather information to gain elevated privileges. For example, you should encrypt data on your devices and turn off unneeded tracing and logging systems on production devices.\n\nGathering information about the state of your devices without manual intervention is essential for the reliability of your environment. Ensure that your devices automatically report all the data that you need. There are two main reasons to gather and monitor data:\n• To help you ensure that your devices are working as intended.\n\nFor example, you can collect monitoring metrics and events with Cloud Monitoring.\n\nTo help you investigate and troubleshoot issues, we recommend that you design and implement processes to gather high resolution diagnostic data, such as detailed monitoring, tracing and debugging information, on top of the processes that monitor your devices during their normal operation. Gathering high resolution diagnostic data and transferring it by using a network can be expensive in terms of device resources, such as computing, data storage, and electrical power. For this reason, we recommend that you enable processes to gather high resolution diagnostic data only when needed, and only for the devices that need further investigation. For example, if one of your devices is not working as intended, and the regular monitoring data that the device reports is not enough to thoroughly diagnose the issue, you can enable high resolution data gathering for that device so it reports more information that can help you investigate the causes of the issue.\n\nThis best practice ensures that you don't leave devices in an unknown state, and that you have a enough data to determine whether and how your devices are performing.\n\nWhen you design your provisioning and configuration processes, ensure that your devices are capable of unattended booting and that you have the necessary infrastructure in place. By implementing an unattended booting mechanism that supports both the first boot and the delivery of over-the-air upgrades, you increase the maintainability of your infrastructure. Using unattended booting frees you from manually attending to each device as it boots or upgrades. Manually attending a large fleet of devices is error-prone because operators might miss or incorrectly perform actions, or they might not have enough time to perform the required actions for every device in the fleet.\n\nAlso, you don't have to prepare each device in advance to boot the correct OS or firmware image. You can release a new version of an OS or firmware image, for example, and make that version available as one of the options that your devices can choose when they take their boot instructions from the network.\n\nThis best practice helps you ensure that your devices can perform boots and upgrades that are automated and unattended.\n\nEven with fully automated provisioning and configuration processes, errors can occur that prevent those processes from correctly completing, thus leaving your devices in an inconsistent state. Help ensure that your devices are able to recover from such failures by implementing retry and fallback mechanisms. When a device fails to complete a task that's part of the provisioning and configuration processes, for example, it should automatically attempt to recover from that failure. After the device recovers from the failure or falls back to a working state, it can resume running processes from the point at which the processes failed.\n\nThis best practice helps you design and implement resilient provisioning and configuration processes.\n\nSupport the whole lifecycle of your devices\n\nWhen designing your provisioning and configuration processes, ensure that those processes can manage the entire device lifecycle. Effectively managing device lifecycles includes planning for termination and disposal, even if your devices are supposed to run for a relatively long time.\n\nIf you don't manage the lifecycle of your devices, it could create issues, like the following:\n• Sustained high costs: Introducing lifecycle management support after your provisioning and configuration processes are in place can increase costs. By planning this support early in the design, you might lower those costs. If your provisioning and configuration processes don't support the whole lifecycle of your devices, for example, you might have to manually intervene on each device to properly handle each phase of their lifecycle. Manual intervention can be expensive, and often doesn't scale.\n• Increased rigidity: Not supporting lifecycle management might eventually lead to the inability to update or manage your devices. If you lack a mechanism to safely and efficiently turn off your devices, for example, it might be challenging to manage their end of life and ultimate disposal.\n• For more reference architectures, diagrams, and best practices, explore the Cloud Architecture Center. ."
    },
    {
        "link": "https://nextplatform.com/2024/07/23/scaling-the-datacenter-five-best-practices-for-csps",
        "document": "Sponsored Feature: In today’s dynamic technological environment, service providers such as cloud service providers (CSPs), managed service providers (MSPs), software-as-a-service (SaaS) providers, and enterprise private cloud operators face a myriad of challenges in the modern datacenter.\n\nThe vast landscape of technologies that constitute what a modern datacenter is and how it operates efficiently is evolving swiftly, with cost management quickly becoming a perpetual concern amongst all service providers. Outlined below, are five best practices for CSPs to scale the modern datacenter.\n\nOver the past three decades, the datacenter landscape has witnessed a consistent growth in output. This growth has been fueled and driven by the rapidly increasing demand for AI server technologies; however, the constant issue of environmental sustainability still looms. New servers containing the latest CPUs and GPUs are quickly approaching the limits of air cooling, which will require a new approach in liquid cooling, which is used to keep the microprocessors and accelerators running within their design limits. In addition, if the datacenter power budget has become a persistent issue, CSPs should consider using liquid cooling to reduce the overall datacenter power usage effectiveness (PUE) and minimize the HVAC cooling power.\n\nMany datacenters have a power budget of 10 kilowatts to 12 kilowatts per rack, which becomes very challenging for a full rack of servers, GPU servers, and storage systems. New systems optimized for AI may each draw up to 10kW per server, resulting in an increased power per rack of up to 100 kilowatts. A properly tested liquid cooling solution allows for higher-density servers and GPU accelerated servers; the external heat exchanger is much more efficient than the more conventional HVAC cooling. A liquid cooling infrastructure must be planned before the rack delivery time. Working with a company experienced in liquid cooling at the rack level is critical to an efficient datacenter.\n\nThe coming generation of high-performance liquid cooling infrastructure is prepared to assume the task of supporting CSPs in the datacenter. New solutions from Supermicro have been engineered and tested to support high-density and high thermal design power (TDP) of CPUs and of GPUs. These solutions have undergone rigorous validation and testing procedures at various levels, encompassing system, rack, and cluster evaluations, ensuring the highest level of consistency and reliability.\n\nThe biggest virtue that has remained a constant in the technological landscape is growth. Despite this, waiting for the latest and greatest technology has proven to be a futile tactic as new technologies and improvements are introduced. Being able to strategically plan around and account for critical technological transitions and implementing an upgrade or migration strategy can maximize the benefits to the buyer.\n\nFurthermore, the expansion of services and simultaneous growth of technology do not always correlate with an increase in staffing and resources. It is essential for CSPs to partner with a reliable supplier that provides cutting-edge servers, storage, and networking solutions that are pre-tested and assembled into a rack with the correct software stack. This relationship can help alleviate some of the challenges that accompany the datacenter, leading to quicker deployment of new services or enhancement of existing ones. As a leader in supplying rack-scale solutions to CSPs of all sizes, Supermicro has significant and relevant experience in product development as well as supply chain logistics, service and support, and sizing and testing. Having access to a supplier with deep partnerships who can share the transition plans, cost impacts, and supply chain issues with you is critical.\n\nIn addition, a disaggregated or modular server and rack approach can mean upgrading specific components or servers without replacing the all of the components or the entire chassis. New generations of servers that can perform substantially more work per watt may also need more power. The design of a new datacenter should not be limited by rack power requirements when the initial servers and racks are installed. By working closely with a supplier such as Supermicro, CSPs will be able to better understand the criteria and means necessary for potential technology in the datacenter.\n• Staying Up to Date with the Latest Server Designs\n\nTo support the problem of cost management, adopting new technologies can increase performance at lower costs. For example, depending on the required service level agreements (SLAs), code base, and matrix processing level, AI workloads can be done on CPUs or GPUs. Some workloads can be moved from the CPU to an auxiliary data processing unit (DPU), which acts as both a network interface and a data processing unit at the same time.\n\nSome workloads would however benefit from a custom approach using a field-programmable gate array (FPGA). The introduction of CXL 2.0 (Compute Express Link) provides another layer in the memory hierarchy directly attached below DRAM but above SSDs. Additionally, this enables the concept of pooled memory, which can be flexibly allocated to one of the CPUs on the given system, and mitigates the issue of stranded memory, which is directly attached to a CPU but not fully utilized. These new technologies may benefit the workload and software stack for the intended service. Testing new technologies in a proof-of-concept (POC) setting before large-scale deployment is also essential. Working with a hardware partner on early POC testing with these new technologies is key to gaining competitive advantages over your competitors.\n\nWhile the initial conversation may be about which server or servers to acquire for the desired workloads, the conversation will quickly shift to one centered around rack scale integration. As the number of racks at a site increases, it is essential to understand the workings and limitations of the entire datacenter. The datacenter must be considered as a whole unit, ranging from topics like the separation of cold and hot aisles, forced air cooling, and the size of chillers and fans, all the way to electrical distribution. The discussion of cooling technologies must be considered at the start because the datacenter’s physical infrastructure will be different depending on the CSP s choice of air or liquid- cooling.\n\nTo accurately assess your current datacenter’s efficiency, use instrumentation to measure CPU, storage, and network utilization. There are also tools available to do this at the cluster level. These tools can provide valuable information on where existing bottlenecks are occurring and where over- or under–utilization situations are not optimal. In addition, the temperature of the CPUs and servers can also be measured, which can identify potential issues prior to the problems causing failures\n\nA datacenter for a cloud provider will most likely be used by many customers simultaneously, and therefore, a job management scheduler will be needed to maintain the efficiency of the datacenter’s operations. With finite resources, not all requests for compute, storage, or networking may be satisfied, and jobs or applications will have to be scheduled or fit in as the required resources become more available, or until additional software can be acquired.\n\nIn terms of managing the supply chain, it is said that identifying “the weakest link” in managing suppliers is optimal. While we’re not advocating a supply chain hierarchy or caste, simplifying the supply chain for key suppliers is an ideal best practice for ordering, installation, and support. A single supplier who can provide servers, storage, networking, third-party software solutions, and rack integration, and can even integrate unique third-party hardware into a single system is ideal.\n\nIt is an industry secret that almost all large original equipment manufacturers (OEMs) have outsourced their products’ manufacturing, design, and supply chain to original design manufacturers (ODMs) and contact manufacturers (CMs). The OEMs are primarily focused on marketing and selling these products. It is valuable to work with a company that designs all its products, from chassis to motherboards as well as power supplies, and manufactures them in locations geographically close to customers’ locations. From the perspective of the customer, this means that a datacenter supplier can be much more flexible, provide faster time to delivery, and ultimately reduce the total cost of ownership through fewer intermediaries, faster transportation, and economies of scale.\n\nLike adopting new technologies in the datacenter, putting all your eggs into one supplier basket can be a risky decision. Selecting a datacenter solution provider is not the time or place for on-the-job learning or working with a company that is more focused on its own managed service offerings or making laptops. Working instead with a B2B company such as Supermicro that is focused specifically on the datacenter and has been working for decades with service providers and large-scale HPC clusters of servers, and powering solutions for the largest hyperscalers, OEMs, and enterprises is of great benefit to CSPs.\n\nThe efficient operation of a datacenter as a CSP requires very meticulous planning and a close working relationship with full-service providers. There are several decisions to make that will affect the start–up times, SLAs, and overall efficiency of the datacenter. Whether designing and implementing a publicly shared datacenter or an on-premises datacenter, plan carefully, educate yourself on and understand server and rack technology, and explore the vast landscape of new technologies and solutions that will keep the datacenter running for years to come.\n\nMichael McNerney is senior vice president of marketing and network security at Supermicro."
    },
    {
        "link": "https://techtarget.com/searchdatacenter/Server-hardware-guide-to-architecture-products-and-management",
        "document": "The explosive growth of cloud computing platforms and the preceding wave of virtualization have significantly altered the role of the server among SMBs and enterprises.\n\nVirtualization enabled organizations to load multiple virtual machines (VMs) on a single, physical box, boosting hardware utilization and offering the ability to consolidate IT resources. Virtualization paved the way to the cloud, as IT managers were able to transfer VMs offsite to run on hyperscale providers, such as AWS, Google and Microsoft. Business-critical applications -- such as CRM, HR and ERP -- previously operating on in-house servers, also journeyed to the cloud in the form of SaaS.\n\nThat said, businesses of all sizes will continue to own and operate servers. Some organizations prefer to maintain in-house servers and private clouds for particularly sensitive workloads or applications too costly or difficult to migrate to a public cloud platform.\n\nRead on to learn more about the different types of servers, their evolving architectures and what IT buyers should look for regarding key requirements.\n\nThe mainframes of the 1950s and 1960s can be considered the original servers, but the history of servers in the modern sense starts in the 1990s with the invention of web and rack-mounted servers. Today, servers come in a range of form factors and offer varying capabilities, finding a home in settings from small businesses to large enterprises. The following is a rundown of some of the prevalent types of server hardware:\n• Tower servers. A tower server resides in an upright, standalone cabinet, resembling a tower-style PC. These servers provide the benefit of easier cooling because they offer a relatively low component density. They are also comparatively inexpensive, making them an option for smaller businesses on a limited budget. However, tower servers take up more space than other server types.\n• Rack servers. A rack server is designed to be mounted on a server rack in a data center. Rack servers often play an all-around computing role in the data center, supporting a multitude of workloads. These servers take up less space than a tower server. Rack servers and server racks are built to consistent size standards so that servers from multiple vendors can be stacked together. Standardization also makes adding new servers or replacing old ones a straightforward task for engineers. Cable management, however, can prove a challenge when maintaining rack servers, which are tethered to power supplies, networking equipment and storage devices. A rack server is designed to fit into a standard-size metal frame.\n• Blade servers. A blade server is a compact device that houses multiple thin, modular circuit boards called server blades. Each blade contains a single server, which is often assigned to one application. Since blade servers tend to be dedicated, admins have greater control regarding how they are accessed and how data is transferred among devices. A blade server segregates processors, memory, I/O, disk, power and other components into separate modules. Blade servers offer greater processing density than other server types, providing a potential price and performance advantage. Other blade server benefits include cooling -- with each blade being cooled individually by fans -- minimal wiring, low-power use and storage consolidation. Blade server systems are also simpler to repair than rack servers due to their hot-swappable, modular components. On the downside, blade servers historically have been built on proprietary architectures, making vendor lock-in a possible pitfall for buyers.\n• Hyperconverged infrastructure. HCI systems aim to provide a simpler alternative to traditional IT infrastructure, pulling together compute power, storage and hypervisor technology in an integrated system. With a typical hyperconvergence offering, a midlevel data center engineer should be able to complete the tasks of initial hardware configuration, hypervisor deployment and software-defined storage implementation in about an hour. Vendors' products offer setup wizards to gather the appropriate information. The implementation processes are mostly automated.\n• Mainframes. The rise of client-server architectures in the 1990s was forecast to obliterate mainframes, but those high-end servers continue to exist. Today's mainframes offer the ability to support large volumes of simultaneous transactions and heavy I/O loads without taking a performance hit. Financial services firms conducting concurrent, real-time transactions are among the typical mainframe customers. The primary drawbacks to mainframes are their size and price tag.\n\nThe key components of server hardware architecture include the motherboard, processor, random access memory (RAM) and storage. The motherboard resides at the heart of the server, providing the central nexus through which system components are interconnected and external devices are attached. Advanced Technology Extended and Low Profile Extension are the main types of motherboards, with Balance Technology Extended, Pico BTX and Mini Information Technology Extended motherboards addressing the needs of smaller form factors. The processor, or central processing unit (CPU), resides on the motherboard. CPU components include the arithmetic logic unit, floating point unit, registers and cache memory. A server might also contain a graphics processing unit (GPU), which can support applications such as machine learning and simulations. Tensor processing units and neural processing units offer additional levels of processor specialization. RAM microchips also plug into the motherboard, serving as a system's main memory. RAM holds the OS, applications and in-use data for fast access by the processor. As for storage, a server might use a hard disk drive (HDD), a solid-state drive (SSD), the cloud or a mix. A server's form factor, meanwhile, influences the role it will play. An enterprise data center, for example, might consider the differences between rack and blade server form factors. An organization seeking to run a heterogenous data center might choose rack servers because various makes and models can exist together in a common physical deployment approach. Rack servers also offer a range of power connections and network cabling choices. Large rack servers also offer extensibility, accommodating additional processors, memory and local storage disks. Blade servers, meanwhile, are geared toward single-vendor IT settings that seek to unite compute, storage and networking within a single system. The blade server approach offers the benefits of faster deployment and simplified management. Other trends in server architecture include disaggregation and composable infrastructure, which can be viewed as a continuation of converged infrastructure. Composable infrastructure uses software-defined methods to logically pool computer, storage and network fabric resources in a data center. Those resources become the basis for shared services, which admins can draw upon to compose compute instances on the fly.\n\nManaging servers and keeping them in working order encompasses a range of activities and tools. Server monitoring systems, for example, provide critical data that admins can use to detect issues before they escalate to problems that could result in outages. Some of the essential components and tools of server monitoring include capacity management tools that track the usage of resources, such as CPU, memory and storage. Specialized energy consumption tools, meanwhile, enable admins to gauge a server's efficiency. Tools in this category include meters embedded in uninterruptable power supplies, which report the power consumption of connected devices. Other options include external power meters, online energy consumption calculators and hardware vendors' configuration tools. Although proper maintenance should help extend server life, hardware degradation is inevitable. A server's physical components will eventually break down, with power, temperature, management and memory among the problem areas. At some point, servers will need to be decommissioned and replaced. At the high end of the server spectrum, proper mainframe decommissioning calls for several steps. Initially, IT managers should take inventory of the applications running on the mainframe server. The next step is to outsource the applications to a service provider specializing in mainframes or replatforming the mainframe's applications to operate on x86 servers. Admins must also create a security plan, which could include deleting the mainframe's data or destroying hard drives.\n\nEnd-of-life hardware will need replacing at some point, but until then, admins must troubleshoot a server to keep it operating. An important first step is determining the scope of the problem -- who are the affected users, and are they reporting any consistent error messages? Once the problem is isolated to a particular server, admins can consult monitoring tools for alerts, check network connections and review the server environment's topology. Troubleshooting and the use of monitoring tools can help organizations maintain system availability. But there are other steps organizations can take as they develop approaches to prevent and recover from server failure. Those include maintaining adequate server ventilation, temperature control and following a routine maintenance regimen. Failure to update machines also ranks among the top common server hardware issues that can result in poor performance. Organizations should develop a process for regularly updating applications, firmware and OSes. Special attention should also be paid to server security and power. The latter calls for the installation of uninterruptible power supplies and the purchase and testing of on-site generators. These aren't the only places in which a server problem could originate, but they are the most likely. Check server status to troubleshoot issues and retain log data in case of future problems.\n\nWhether you are purchasing a server for an enterprise or a small business, the fundamentals are essentially the same: Understand your requirements and use cases and match those up against the products you evaluate. Vendors offer a wide range of server form factors, configurations and models, many of which are optimized for particular types of workloads. Buyers must be willing to conduct a careful analysis or hire a consultant or channel partner, such as a value-added reseller, to identify the server that best suits their needs. Organizations sizing up server options should also familiarize themselves with purchasing methods. Vendors sell servers on their website, through a direct sales force, using online retailers and in partnership with authorized channel companies. Some vendors use a combination of sales outlets. Cost is always a consideration, particularly for budget-constrained organizations, such as small businesses. SMBs should consider several factors when calculating server hardware costs. The first task is to determine hardware requirements based on anticipated uses. Although this will help scope the hardware costs, buyers must also examine server OS costs, which can rival the hardware price tag, and the cost of applications intended to reside on the server. Also factor server maintenance and administration costs into the total cost of ownership equation."
    },
    {
        "link": "https://doc.milestonesys.com/latest/en-US/system/sad/sad_servercomponents.htm",
        "document": "The management server is the central VMS component. It stores the configuration of the surveillance system in a SQL Server database, either on SQL Server on the management server computer itself or on separate SQL Server on the network. It also handles user authentication, user permissions, the rule system and more. To improve system performance, you can run several management servers as a Milestone Federated Architecture™. The management server runs as a service and is typically installed on a dedicated server. You can get failover support on the management server by installing the management server in a Microsoft Windows cluster. The cluster ensures that another server takes over the management server function in case the first server fails.\n\nRecording servers are computers where you have installed the Recording Server software, and configured it to communicate with the management server. A surveillance system typically consists of several recording servers. The recording server is responsible for all communication, recording, and event handling related to devices such as cameras, video and audio encoders, I/O modules, and metadata sources. Examples of actions the recording server handles:\n• Retrieve video, audio, metadata and I/O event streams from the devices\n• Provide operators with access to live and recorded video, audio and metadata\n• Trigger system and video events on device failures or events The recording server is also responsible for communicating with other Milestone products when using the Milestone Interconnect™ technology. For more information, see Milestone Interconnect. The recording server supports encryption of data streams to the clients and services as well as encryption of the connection with the management server.For more information, see the certificates guide about how to secure your XProtect VMS installations. The failover recording server is responsible for taking over the recording task in case a recording server fails. The failover recording server operates in two modes: In a cold standby failover recording server setup, you group multiple failover recording servers in a failover group. The entire failover group is dedicated to take over from any of several preselected recording servers, if one of these becomes unavailable. You can also specify a secondary failover server group that takes over from the primary group if all the recording servers in the primary group are busy In a hot standby failover recording server setup, you dedicate a failover recording server to take over from one recording server only. With this approach, the failover recording server is continuously synchronized with the correct/current configuration of the recording server it is dedicated to and it can take over much faster than a cold standby failover recording server.\n\nThe event server handles the tasks related to events, alarms, and maps and also third-party integrations via the Milestone Integration Platform .\n• All system events are consolidated in the event server so there is a single place and interface for partners to make integrations that use system events\n• The event server offers third-party access for sending events to the system via the Generic events or Analytics events interface\n• The event server hosts the alarm feature, alarm logic, alarm state and handling of the alarm database. The alarm database is stored in the same database as the management server uses\n• The event server also hosts maps. You configure and use maps in the\n• You can install third-party developed plug-ins on the event server and utilize access to system events You can get failover support on the event server by installing the event server in a Microsoft Windows cluster. The cluster ensures that another server takes over the event server function in case the first server fails.\n\nThe log server stores all log messages for the entire system. The log server typically uses the same SQL Server as the management server but has its own SQL Server database. The log server is also typically installed on the same server as the management server. If you need to increase the performance of the management server or log server, you can install the log server on a separate server and use separate SQL Server. The system can through the log server write three types of log messages:\n• System logs: the system administrator can choose to log errors, warnings, and information, or a combination of these. The default is to log errors only\n• Audit logs: the system administrator can choose to log user activity in clients in addition to login and administration logs\n• Rule-triggered logs: the system administrator can use the rule log to create logs on specific events\n\nThe management server, the event server, and the log server use SQL Server databases on one or two SQL Server installations to store, for example, configuration, alarms, events and log messages. The installation wizard installs Microsoft SQL Server Express 2022 unless SQL Server is already installed on the computer. When you install XProtect VMS as an upgrade, the wizard keeps the previous SQL Server installation. For very large systems or systems with many transactions to and from the SQL Server databases, Milestone recommends that you use the Microsoft® SQL Server® Standard or Microsoft® SQL Server® Enterprise edition of SQL Server on a dedicated computer on the network and on a dedicated hard disk drive that is not used for other purposes. Installing SQL Server on its own drive improves the entire system performance.\n\nXProtect Mobile server handles logins to the system from XProtect Mobile client or XProtect Web Client. A XProtect Mobile server distributes video streams from recording servers to XProtect Mobile client or XProtect Web Client. This offers a secure setup where recording servers are never connected to the Internet. When a XProtect Mobile server receives video streams from recording servers, it also handles the complex conversion of codecs and formats allowing streaming of video on the mobile device."
    },
    {
        "link": "https://sciencedirect.com/topics/computer-science/server-architecture",
        "document": ""
    },
    {
        "link": "https://learn.microsoft.com/en-us/mem/configmgr/develop/core/understand/architectural-overview",
        "document": "Configuration Manager is a configuration management product that requires servers to administer client computers. The following sections describe both Configuration Manager server and client architecture. Gaining an understanding of the concepts relating to both server and client architecture will help you understand how you can customize Configuration Manager for specific uses in your organization.\n\nThe Configuration Manager server architecture can be divided into two separate tiers:\n• The WBEM interface to the Configuration Manager architecture (SMS Provider)\n\nConfiguration Manager components are analogous to the mechanisms and devices that enable the elevator, the phone system, and the electrical system in an office building to work properly. When you make changes through the Configuration Manager console, Configuration Manager services and components start working to complete the operation successfully, whether it's software distribution, hardware inventory, or any other administrator-initiated or schedules Configuration Manager task, feature, or tool.\n\nThe WBEM interface to the Configuration Manager architecture is a description of the Configuration Manager framework, much as building plans describe a building. As you become more familiar with Configuration Manager, you might find that your organization needs to provide Configuration Manager functionality in a slightly different fashion. You might need to gather additional inventory information and store it in your Configuration Manager database. The WBEM interface enables you to customer Configuration Manager for optimal change and configuration management.\n\nTo fully understand Configuration Manager features, you need a basic understanding of the elements that make up Configuration Manager. Service components, thread components, and data stores are the major elements of Configuration Manager server architecture. Each of these elements does a specific function to complete the work that you assign and schedule.\n\nIn Configuration Manager, components are threads, services, and applications that run on both server and client computers and provide Configuration Manager functionality. Service and thread components accomplish the many tasks Configuration Manager requires to function – tasks such as communication for inter- and intra-site connectivity, configuration, resource discovery, client installation, database maintenance, status, site system installation, and reporting.\n\nA dynamic computing environment must have a central location that stores the critical operations information. Also, server and client components need access to their configuration data, scheduled times of operation, and the data in the Configuration Manager site database to accomplish tasks. For example, Collection Evaluator operation requires information such as which collections to evaluate, when to evaluate them, and what resources belong to each specific collection. To do these tasks, Collection Evaluator needs access to both configuration data and data stored in the Configuration Manager site database.\n\nIn Configuration Manager, there are two basic types of data stores: configuration data and system data.\n\nConfiguration Manager gathers configuration data from Configuration Manager default settings, changes you make through the Configuration Manager console, and changes Configuration Manager services make. Configuration is a dynamic system that enables you to make decisions about how and with the site will operate. As you make configuration changes, Configuration Manager updates the site control file and the registry. The site control file contains configuration for a Configuration Manager site. Many Configuration Manager features, such as Software Inventory, function on a schedule. After Configuration Manager server service and thread components are enabled, they periodically check the site control file for their configuration and schedule as they continue to work.\n\nConfiguration Manager gathers system data from the various resources in the site. Systems within an organization change constantly as hardware and software are upgraded and repaired, new systems are brought on line, and old systems are retired. Configuration Manager stores the information in the Configuration Manager site database. This database stores all of the data pertinent to Configuration Manager functions including DDRs, MIF files, network discovery data, and site configuration data.\n\nThe WBEM interface with the SMS architecture\n\nConfiguration Manager provides an open architecture that enables you to write applications and scripts that automate and customize Configuration Manager features, such as Software Distribution. You can also create and install customized programs that you can start from the Configuration Manager console.\n\nTerms and concepts that relate to Configuration Manager architecture originate from various sources. Some originated with the Desktop Management Task Force (DMTF) and were created to describe managed objects. Others are standard COM and Web-Based Enterprise Management (WBEM) initiative terms and concepts. Still others are specific to Configuration Manager.\n\nConfiguration Manager uses the WBEM architecture to manage objects. WBEM is an industry initiative adopted by the DMTF that is also supported by many non-Windows computer and network device manufacturers. The WBEM initiative complements Active Directory that locates and manages entity policies. WBEM also provides a unifying mechanism through which management applications can interact with the managed entities (like Configuration Manager objects) – without you having to understand the underlying management protocols that these entities use.\n\nIn Configuration Manager, objects are items such as client computers, advertisements, and packages stored in the Configuration Manager database. The WBEM initiative outlines the architecture used by Windows Management, Microsoft's implementation of one of the DMTF object management standards.\n\nThe CIM Object Manager stores the metadata, Windows Management provides access to the Configuration Manager configuration and operations data with an extensible, platform-independent interface. And managed object, such as a disk drive or a collection is represented by an instance of a Configuration Manager class. Each Configuration Manager managed object is represented by a CIM class.\n\nTo view and manipulate objects, Configuration Manager makes a request to the CIM Object Manager (the central WBEM component). Configuration Manager uses the site database to store managed object data. However, Configuration Manager uses the CIM Object Manager interface and the SMS Provider to view and manipulate that managed data. You can't view or manipulate the Configuration Manager database directly. Instead, you gain access to the underlying Configuration Manager site database through the CIM Object Manager, which in turn communicates with the SMS Provider.\n\nA Configuration Manager client computer is any computer in your organization that has the Configuration Manager client software installed. Computers serving as Configuration Manager site servers and site systems can also be installed as Configuration Manager client computers, in addition to any other servers in your organization where you install Configuration Manager client software.\n• None Runs almost entirely as services, processes, or applications started from Configuration Manager services.\n• None Runs from the client computer (as opposed to over the network).\n• None Maintains history information for most function so the client computer (such as software and hardware inventory)."
    },
    {
        "link": "https://techdocs.broadcom.com/us/en/symantec-security-software/endpoint-security-and-management/it-management-suite/ITMS/Related-Suites/Server-Management-Suite/components-of-server-management-suite-v31091026-d759e50.html",
        "document": "Symantec Management Platform provides a set of services that IT-related solutions can leverage. By leveraging these services, the solutions that are built on the platform can focus on their unique tasks. They also can take advantage of the more general services that the platform provides. The platform services also provide a high degree of consistency between the solutions, so that users do not need to learn multiple product interfaces. Symantec Management Platform provides the following services:\n• None Execution of scheduled or event-triggered tasks and policies\n\nDeployment Solution helps to reduce the cost of deploying and managing servers, desktops, and notebooks from a centralized location in your environment. It offers operating system deployment, configuration, personality migration of computers, and software deployment across different hardware platforms and operating systems. Deployment Solution provides integrated, disk imaging, and personality migration from the Symantec Management Console. Using Symantec Ghost , you can perform initial computer deployment using standard images and migrate user data and application settings to new computers.\n\nITMS Management Views replace the default console views for computers and software that existed in Symantec Management Platform version 7.0. For tasks and policies, the Management views add drag-and-drop functionality. In addition, you can now search the tree rather than drilling down to find specific tasks or policies. The Management views are incorporated into the existing console. . For more information, see the\n\nInventory Solution lets you gather inventory data about the computers, users, operating systems, and installed software applications in your environment. You can collect inventory data from the computers that run Windows, UNIX, Linux, and Mac. After you gather inventory data, you can analyze it using predefined or custom reports. Management view, in the summary flipbook. For example, you can gather information for all the Symantec Endpoint Protection Windows and Mac clients that are installed on managed and unmanaged computers in your environment. Then you can view the gathered data in the Resource Manager or in theManagement view, in thesummary flipbook.\n\nMonitor Solution for Servers lets you monitor various aspects of computer operating systems, applications, and devices. These aspects can include events, processes, and performance. This ability helps you ensure that your servers and your devices work and reduces the costs of server and network monitoring. Monitor Pack for Servers works with the Monitor Solution core components of the Symantec Management Platform. It lets you monitor operating system performance, services, and events of your Windows, UNIX, and Linux server environment.\n\nPatch Management Solution for Linux lets you scan Red Hat and Novell Linux computers for security vulnerabilities. The solution then reports on the findings and lets you automate the download and distribution of needed errata, or software updates. The solution downloads the required patches and provides wizards to help you deploy them. Patch Management Solution for Mac lets you scan Mac computers for the updates that they require. The solution then reports on the findings and lets you automate the downloading and distribution of needed updates. You can distribute all or some of the updates. Patch Management Solution for Windows lets you scan Windows computers for the updates that they require, and view the results of the scan. The system lets you automate the download and distribution of software updates. You can create filters of the computers and apply the patch to the computers that need it.\n\nVirtual Machine Management helps you to view virtual resource information in your network and perform management tasks on those virtual resources. You can create virtual environments of servers, storage devices, and network resources on a single physical server. Each virtual environment is isolated and functions independently from the physical server and from the other virtual environments. Virtualization enhances the efficiency and productivity of the hardware resources and helps to reduce administrative costs.\n\nSymantec Workflow is a security process development framework that you can use to create both automated business processes and security processes. These processes provide for increased repeatability, control, and accountability while reducing overall workload. The Symantec Workflow framework also lets you create Workflow processes that integrate Symantec tools into your organization's unique business processes. Once deployed, Symantec Workflow processes can respond automatically to environmental variables. Symantec Workflow processes can also allow for human interface points when a process calls for someone to make a decision with accountability."
    }
]