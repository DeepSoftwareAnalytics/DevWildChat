[
    {
        "link": "https://docs.python.org/3/library/logging.html",
        "document": "This module defines functions and classes which implement a flexible event logging system for applications and libraries.\n\nThe key benefit of having the logging API provided by a standard library module is that all Python modules can participate in logging, so your application log can include your own messages integrated with messages from third-party modules.\n\nIf you run myapp.py, you should see this in myapp.log:\n\nThe key feature of this idiomatic usage is that the majority of code is simply creating a module level logger with , and using that logger to do any needed logging. This is concise, while allowing downstream code fine-grained control if needed. Logged messages to the module-level logger get forwarded to handlers of loggers in higher-level modules, all the way up to the highest-level logger known as the root logger; this approach is known as hierarchical logging.\n\nFor logging to be useful, it needs to be configured: setting the levels and destinations for each logger, potentially changing how specific modules log, often based on command-line arguments or application configuration. In most cases, like the one above, only the root logger needs to be so configured, since all the lower level loggers at module level eventually forward their messages to its handlers. provides a quick way to configure the root logger that handles many use cases.\n\nThe module provides a lot of functionality and flexibility. If you are unfamiliar with logging, the best way to get to grips with it is to view the tutorials (see the links above and on the right).\n\nThe basic classes defined by the module, together with their attributes and methods, are listed in the sections below.\n• None Loggers expose the interface that application code directly uses.\n• None Handlers send the log records (created by loggers) to the appropriate destination.\n• None Filters provide a finer grained facility for determining which log records to output.\n• None Formatters specify the layout of log records in the final output.\n\nLoggers have the following attributes and methods. Note that Loggers should NEVER be instantiated directly, but always through the module-level function . Multiple calls to with the same name will always return a reference to the same Logger object. The is potentially a period-separated hierarchical value, like (though it could also be just plain , for example). Loggers that are further down in the hierarchical list are children of loggers higher up in the list. For example, given a logger with a name of , loggers with names of , , and are all descendants of . In addition, all loggers are descendants of the root logger. The logger name hierarchy is analogous to the Python package hierarchy, and identical to it if you organise your loggers on a per-module basis using the recommended construction . That’s because in a module, is the module’s name in the Python package namespace. This is the logger’s name, and is the value that was passed to to obtain the logger. This attribute should be treated as read-only. The threshold of this logger, as set by the method. Do not set this attribute directly - always use , which has checks for the level passed to it. The parent logger of this logger. It may change based on later instantiation of loggers which are higher up in the namespace hierarchy. This value should be treated as read-only. If this attribute evaluates to true, events logged to this logger will be passed to the handlers of higher level (ancestor) loggers, in addition to any handlers attached to this logger. Messages are passed directly to the ancestor loggers’ handlers - neither the level nor filters of the ancestor loggers in question are considered. If this evaluates to false, logging messages are not passed to the handlers of ancestor loggers. Spelling it out with an example: If the propagate attribute of the logger named evaluates to true, any event logged to via a method call such as will [subject to passing that logger’s level and filter settings] be passed in turn to any handlers attached to loggers named , and the root logger, after first being passed to any handlers attached to . If any logger in the chain , , has its attribute set to false, then that is the last logger whose handlers are offered the event to handle, and propagation stops at that point. The constructor sets this attribute to . If you attach a handler to a logger and one or more of its ancestors, it may emit the same record multiple times. In general, you should not need to attach a handler to more than one logger - if you just attach it to the appropriate logger which is highest in the logger hierarchy, then it will see all events logged by all descendant loggers, provided that their propagate setting is left set to . A common scenario is to attach handlers only to the root logger, and to let propagation take care of the rest. The list of handlers directly attached to this logger instance. This attribute should be treated as read-only; it is normally changed via the and methods, which use locks to ensure thread-safe operation. This attribute disables handling of any events. It is set to in the initializer, and only changed by logging configuration code. This attribute should be treated as read-only. Sets the threshold for this logger to level. Logging messages which are less severe than level will be ignored; logging messages which have severity level or higher will be emitted by whichever handler or handlers service this logger, unless a handler’s level has been set to a higher severity level than level. When a logger is created, the level is set to (which causes all messages to be processed when the logger is the root logger, or delegation to the parent when the logger is a non-root logger). Note that the root logger is created with level . The term ‘delegation to the parent’ means that if a logger has a level of NOTSET, its chain of ancestor loggers is traversed until either an ancestor with a level other than NOTSET is found, or the root is reached. If an ancestor is found with a level other than NOTSET, then that ancestor’s level is treated as the effective level of the logger where the ancestor search began, and is used to determine how a logging event is handled. If the root is reached, and it has a level of NOTSET, then all messages will be processed. Otherwise, the root’s level will be used as the effective level. See Logging Levels for a list of levels. Changed in version 3.2: The level parameter now accepts a string representation of the level such as ‘INFO’ as an alternative to the integer constants such as . Note, however, that levels are internally stored as integers, and methods such as e.g. and will return/expect to be passed integers. Indicates if a message of severity level would be processed by this logger. This method checks first the module-level level set by and then the logger’s effective level as determined by . Indicates the effective level for this logger. If a value other than has been set using , it is returned. Otherwise, the hierarchy is traversed towards the root until a value other than is found, and that value is returned. The value returned is an integer, typically one of , etc. Returns a logger which is a descendant to this logger, as determined by the suffix. Thus, would return the same logger as would be returned by . This is a convenience method, useful when the parent logger is named using e.g. rather than a literal string. Returns a set of loggers which are immediate children of this logger. So for example might return a set containing loggers named and , but a logger named wouldn’t be included in the set. Likewise, might return a set including a logger named , but it wouldn’t include one named . Logs a message with level on this logger. The msg is the message format string, and the args are the arguments which are merged into msg using the string formatting operator. (Note that this means that you can use keywords in the format string, together with a single dictionary argument.) No % formatting operation is performed on msg when no args are supplied. There are four keyword arguments in kwargs which are inspected: exc_info, stack_info, stacklevel and extra. If exc_info does not evaluate as false, it causes exception information to be added to the logging message. If an exception tuple (in the format returned by ) or an exception instance is provided, it is used; otherwise, is called to get the exception information. The second optional keyword argument is stack_info, which defaults to . If true, stack information is added to the logging message, including the actual logging call. Note that this is not the same stack information as that displayed through specifying exc_info: The former is stack frames from the bottom of the stack up to the logging call in the current thread, whereas the latter is information about stack frames which have been unwound, following an exception, while searching for exception handlers. You can specify stack_info independently of exc_info, e.g. to just show how you got to a certain point in your code, even when no exceptions were raised. The stack frames are printed following a header line which says: This mimics the which is used when displaying exception frames. The third optional keyword argument is stacklevel, which defaults to . If greater than 1, the corresponding number of stack frames are skipped when computing the line number and function name set in the created for the logging event. This can be used in logging helpers so that the function name, filename and line number recorded are not the information for the helper function/method, but rather its caller. The name of this parameter mirrors the equivalent one in the module. The fourth keyword argument is extra which can be used to pass a dictionary which is used to populate the of the created for the logging event with user-defined attributes. These custom attributes can then be used as you like. For example, they could be incorporated into logged messages. For example: would print something like The keys in the dictionary passed in extra should not clash with the keys used by the logging system. (See the section on LogRecord attributes for more information on which keys are used by the logging system.) If you choose to use these attributes in logged messages, you need to exercise some care. In the above example, for instance, the has been set up with a format string which expects ‘clientip’ and ‘user’ in the attribute dictionary of the . If these are missing, the message will not be logged because a string formatting exception will occur. So in this case, you always need to pass the extra dictionary with these keys. While this might be annoying, this feature is intended for use in specialized circumstances, such as multi-threaded servers where the same code executes in many contexts, and interesting conditions which arise are dependent on this context (such as remote client IP address and authenticated user name, in the above example). In such circumstances, it is likely that specialized s would be used with particular s. If no handler is attached to this logger (or any of its ancestors, taking into account the relevant attributes), the message will be sent to the handler set on . Changed in version 3.2: The stack_info parameter was added. Changed in version 3.5: The exc_info parameter can now accept exception instances. Changed in version 3.8: The stacklevel parameter was added. Logs a message with level on this logger. The arguments are interpreted as for . Logs a message with level on this logger. The arguments are interpreted as for . There is an obsolete method which is functionally identical to . As is deprecated, please do not use it - use instead. Logs a message with level on this logger. The arguments are interpreted as for . Logs a message with level on this logger. The arguments are interpreted as for . Logs a message with integer level level on this logger. The other arguments are interpreted as for . Logs a message with level on this logger. The arguments are interpreted as for . Exception info is added to the logging message. This method should only be called from an exception handler. Adds the specified filter filter to this logger. Removes the specified filter filter from this logger. Apply this logger’s filters to the record and return if the record is to be processed. The filters are consulted in turn, until one of them returns a false value. If none of them return a false value, the record will be processed (passed to handlers). If one returns a false value, no further processing of the record occurs. Adds the specified handler hdlr to this logger. Removes the specified handler hdlr from this logger. Finds the caller’s source filename and line number. Returns the filename, line number, function name and stack information as a 4-element tuple. The stack information is returned as unless stack_info is . The stacklevel parameter is passed from code calling the and other APIs. If greater than 1, the excess is used to skip stack frames before determining the values to be returned. This will generally be useful when calling logging APIs from helper/wrapper code, so that the information in the event log refers not to the helper/wrapper code, but to the code that calls it. Handles a record by passing it to all handlers associated with this logger and its ancestors (until a false value of propagate is found). This method is used for unpickled records received from a socket, as well as those created locally. Logger-level filtering is applied using . This is a factory method which can be overridden in subclasses to create specialized instances. Checks to see if this logger has any handlers configured. This is done by looking for handlers in this logger and its parents in the logger hierarchy. Returns if a handler was found, else . The method stops searching up the hierarchy whenever a logger with the ‘propagate’ attribute set to false is found - that will be the last logger which is checked for the existence of handlers. Changed in version 3.7: Loggers can now be pickled and unpickled.\n\ncan be used by and for more sophisticated filtering than is provided by levels. The base filter class only allows events which are below a certain point in the logger hierarchy. For example, a filter initialized with ‘A.B’ will allow events logged by loggers ‘A.B’, ‘A.B.C’, ‘A.B.C.D’, ‘A.B.D’ etc. but not ‘A.BB’, ‘B.A.B’ etc. If initialized with the empty string, all events are passed. Returns an instance of the class. If name is specified, it names a logger which, together with its children, will have its events allowed through the filter. If name is the empty string, allows every event. Is the specified record to be logged? Returns false for no, true for yes. Filters can either modify log records in-place or return a completely different record instance which will replace the original log record in any future processing of the event. Note that filters attached to handlers are consulted before an event is emitted by the handler, whereas filters attached to loggers are consulted whenever an event is logged (using , , etc.), before sending an event to handlers. This means that events which have been generated by descendant loggers will not be filtered by a logger’s filter setting, unless the filter has also been applied to those descendant loggers. You don’t actually need to subclass : you can pass any instance which has a method with the same semantics. Changed in version 3.2: You don’t need to create specialized classes, or use other classes with a method: you can use a function (or other callable) as a filter. The filtering logic will check to see if the filter object has a attribute: if it does, it’s assumed to be a and its method is called. Otherwise, it’s assumed to be a callable and called with the record as the single parameter. The returned value should conform to that returned by . Changed in version 3.12: You can now return a instance from filters to replace the log record rather than modifying it in place. This allows filters attached to a to modify the log record before it is emitted, without having side effects on other handlers. Although filters are used primarily to filter records based on more sophisticated criteria than levels, they get to see every record which is processed by the handler or logger they’re attached to: this can be useful if you want to do things like counting how many records were processed by a particular logger or handler, or adding, changing or removing attributes in the being processed. Obviously changing the LogRecord needs to be done with some care, but it does allow the injection of contextual information into logs (see Using Filters to impart contextual information).\n\nThe LogRecord has a number of attributes, most of which are derived from the parameters to the constructor. (Note that the names do not always correspond exactly between the LogRecord constructor parameters and the LogRecord attributes.) These attributes can be used to merge data from the record into the format string. The following table lists (in alphabetical order) the attribute names, their meanings and the corresponding placeholder in a %-style format string. If you are using {}-formatting ( ), you can use as the placeholder in the format string. If you are using $-formatting ( ), use the form . In both cases, of course, replace with the actual attribute name you want to use. In the case of {}-formatting, you can specify formatting flags by placing them after the attribute name, separated from it with a colon. For example: a placeholder of would format a millisecond value of as . Refer to the documentation for full details on the options available to you. You shouldn’t need to format this yourself. The tuple of arguments merged into to produce , or a dict whose values are used for the merge (when there is only one argument, and it is a dictionary). Human-readable time when the was created. By default this is of the form ‘2003-07-08 16:49:45,896’ (the numbers after the comma are millisecond portion of the time). Time when the was created (as returned by / 1e9). You shouldn’t need to format this yourself. Exception tuple (à la ) or, if no exception has occurred, . Name of function containing the logging call. Source line number where the logging call was issued (if available). The logged message, computed as . This is set when is invoked. Millisecond portion of the time when the was created. You shouldn’t need to format this yourself. The format string passed in the original logging call. Merged with to produce , or an arbitrary object (see Using arbitrary objects as messages). Name of the logger used to log the call. Full pathname of the source file where the logging call was issued (if available). Process name (if available). Time in milliseconds when the LogRecord was created, relative to the time the logging module was loaded. You shouldn’t need to format this yourself. Stack frame information (where available) from the bottom of the stack in the current thread, up to and including the stack frame of the logging call which resulted in the creation of this record. Thread name (if available). name (if available).\n\nIn addition to the classes described above, there are a number of module-level functions. Return a logger with the specified name or, if name is , return the root logger of the hierarchy. If specified, the name is typically a dot-separated hierarchical name like ‘a’, ‘a.b’ or ‘a.b.c.d’. Choice of these names is entirely up to the developer who is using logging, though it is recommended that be used unless you have a specific reason for not doing that, as mentioned in Logger Objects. All calls to this function with a given name return the same logger instance. This means that logger instances never need to be passed between different parts of an application. Return either the standard class, or the last class passed to . This function may be called from within a new class definition, to ensure that installing a customized class will not undo customizations already applied by other code. For example: Return a callable which is used to create a . Added in version 3.2: This function has been provided, along with , to allow developers more control over how the representing a logging event is constructed. See for more information about the how the factory is called. This is a convenience function that calls , on the root logger. The handling of the arguments is in every way identical to what is described in that method. The only difference is that if the root logger has no handlers, then is called, prior to calling on the root logger. For very short scripts or quick demonstrations of facilities, and the other module-level functions may be convenient. However, most programs will want to carefully and explicitly control the logging configuration, and should therefore prefer creating a module-level logger and calling (or other level-specific methods) on it, as described at the beginnning of this documentation. Logs a message with level on the root logger. The arguments and behavior are otherwise the same as for . Logs a message with level on the root logger. The arguments and behavior are otherwise the same as for . There is an obsolete function which is functionally identical to . As is deprecated, please do not use it - use instead. Logs a message with level on the root logger. The arguments and behavior are otherwise the same as for . Logs a message with level on the root logger. The arguments and behavior are otherwise the same as for . Logs a message with level on the root logger. The arguments and behavior are otherwise the same as for . Exception info is added to the logging message. This function should only be called from an exception handler. Logs a message with level level on the root logger. The arguments and behavior are otherwise the same as for . Provides an overriding level level for all loggers which takes precedence over the logger’s own level. When the need arises to temporarily throttle logging output down across the whole application, this function can be useful. Its effect is to disable all logging calls of severity level and below, so that if you call it with a value of INFO, then all INFO and DEBUG events would be discarded, whereas those of severity WARNING and above would be processed according to the logger’s effective level. If is called, it effectively removes this overriding level, so that logging output again depends on the effective levels of individual loggers. Note that if you have defined any custom logging level higher than (this is not recommended), you won’t be able to rely on the default value for the level parameter, but will have to explicitly supply a suitable value. Changed in version 3.7: The level parameter was defaulted to level . See bpo-28524 for more information about this change. Associates level level with text levelName in an internal dictionary, which is used to map numeric levels to a textual representation, for example when a formats a message. This function can also be used to define your own levels. The only constraints are that all levels used must be registered using this function, levels should be positive integers and they should increase in increasing order of severity. If you are thinking of defining your own levels, please see the section on Custom Levels. Returns a mapping from level names to their corresponding logging levels. For example, the string “CRITICAL” maps to . The returned mapping is copied from an internal mapping on each call to this function. Returns the textual or numeric representation of logging level level. If level is one of the predefined levels , , , or then you get the corresponding string. If you have associated levels with names using then the name you have associated with level is returned. If a numeric value corresponding to one of the defined levels is passed in, the corresponding string representation is returned. The level parameter also accepts a string representation of the level such as ‘INFO’. In such cases, this functions returns the corresponding numeric value of the level. If no matching numeric or string value is passed in, the string ‘Level %s’ % level is returned. Levels are internally integers (as they need to be compared in the logging logic). This function is used to convert between an integer level and the level name displayed in the formatted log output by means of the format specifier (see LogRecord attributes), and vice versa. Changed in version 3.4: In Python versions earlier than 3.4, this function could also be passed a text level, and would return the corresponding numeric value of the level. This undocumented behaviour was considered a mistake, and was removed in Python 3.4, but reinstated in 3.4.2 due to retain backward compatibility. Returns a handler with the specified name, or if there is no handler with that name. Returns an immutable set of all known handler names. Creates and returns a new instance whose attributes are defined by attrdict. This function is useful for taking a pickled attribute dictionary, sent over a socket, and reconstituting it as a instance at the receiving end. Does basic configuration for the logging system by creating a with a default and adding it to the root logger. The functions , , , and will call automatically if no handlers are defined for the root logger. This function does nothing if the root logger already has handlers configured, unless the keyword argument force is set to . This function should be called from the main thread before other threads are started. In versions of Python prior to 2.7.1 and 3.2, if this function is called from multiple threads, it is possible (in rare circumstances) that a handler will be added to the root logger more than once, leading to unexpected results such as messages being duplicated in the log. The following keyword arguments are supported. Specifies that a be created, using the specified filename, rather than a . If filename is specified, open the file in this mode. Defaults to . Use the specified format string for the handler. Defaults to attributes , and separated by colons. Use the specified date/time format, as accepted by . If format is specified, use this style for the format string. One of , or for printf-style, or respectively. Defaults to . Set the root logger level to the specified level. Use the specified stream to initialize the . Note that this argument is incompatible with filename - if both are present, a is raised. If specified, this should be an iterable of already created handlers to add to the root logger. Any handlers which don’t already have a formatter set will be assigned the default formatter created in this function. Note that this argument is incompatible with filename or stream - if both are present, a is raised. If this keyword argument is specified as true, any existing handlers attached to the root logger are removed and closed, before carrying out the configuration as specified by the other arguments. If this keyword argument is specified along with filename, its value is used when the is created, and thus used when opening the output file. If this keyword argument is specified along with filename, its value is used when the is created, and thus used when opening the output file. If not specified, the value ‘backslashreplace’ is used. Note that if is specified, it will be passed as such to , which means that it will be treated the same as passing ‘errors’. Changed in version 3.2: The style argument was added. Changed in version 3.3: The handlers argument was added. Additional checks were added to catch situations where incompatible arguments are specified (e.g. handlers together with stream or filename, or stream together with filename). Changed in version 3.8: The force argument was added. Changed in version 3.9: The encoding and errors arguments were added. Informs the logging system to perform an orderly shutdown by flushing and closing all handlers. This should be called at application exit and no further use of the logging system should be made after this call. When the logging module is imported, it registers this function as an exit handler (see ), so normally there’s no need to do that manually. Tells the logging system to use the class klass when instantiating a logger. The class should define such that only a name argument is required, and the should call . This function is typically called before any loggers are instantiated by applications which need to use custom logger behavior. After this call, as at any other time, do not instantiate loggers directly using the subclass: continue to use the API to get your loggers. Set a callable which is used to create a . factory – The factory callable to be used to instantiate a log record. Added in version 3.2: This function has been provided, along with , to allow developers more control over how the representing a logging event is constructed. The factory has the following signature: The full pathname of the file where the logging call was made. The line number in the file where the logging call was made. The arguments for the logging message. The name of the function or method which invoked the logging call. A stack traceback such as is provided by , showing the call hierarchy."
    },
    {
        "link": "https://docs.python.org/3/howto/logging.html",
        "document": "This page contains tutorial information. For links to reference information and a logging cookbook, please see Other resources.\n\nLogging is a means of tracking events that happen when some software runs. The software’s developer adds logging calls to their code to indicate that certain events have occurred. An event is described by a descriptive message which can optionally contain variable data (i.e. data that is potentially different for each occurrence of the event). Events also have an importance which the developer ascribes to the event; the importance can also be called the level or severity. When to use logging¶ You can access logging functionality by creating a logger via , and then calling the logger’s , , , and methods. To determine when to use logging, and to see which logger methods to use when, see the table below. It states, for each of a set of common tasks, the best tool to use for that task. The best tool for the task Display console output for ordinary usage of a command line script or program Report events that occur during normal operation of a program (e.g. for status monitoring or fault investigation) A logger’s (or method for very detailed output for diagnostic purposes) in library code if the issue is avoidable and the client application should be modified to eliminate the warning A logger’s method if there is nothing the client application can do about the situation, but the event should still be noted Report an error regarding a particular runtime event Report suppression of an error without raising an exception (e.g. error handler in a long-running server process) A logger’s , or method as appropriate for the specific error and application domain The logger methods are named after the level or severity of the events they are used to track. The standard levels and their applicability are described below (in increasing order of severity): Detailed information, typically of interest only when diagnosing problems. Confirmation that things are working as expected. An indication that something unexpected happened, or indicative of some problem in the near future (e.g. ‘disk space low’). The software is still working as expected. Due to a more serious problem, the software has not been able to perform some function. A serious error, indicating that the program itself may be unable to continue running. The default level is , which means that only events of this severity and higher will be tracked, unless the logging package is configured to do otherwise. Events that are tracked can be handled in different ways. The simplest way of handling tracked events is to print them to the console. Another common way is to write them to a disk file. A very simple example is: # will print a message to the console # will not print anything If you type these lines into a script and run it, you’ll see: printed out on the console. The message doesn’t appear because the default level is . The printed message includes the indication of the level and the description of the event provided in the logging call, i.e. ‘Watch out!’. The actual output can be formatted quite flexibly if you need that; formatting options will also be explained later. Notice that in this example, we use functions directly on the module, like , rather than creating a logger and calling functions on it. These functions operation on the root logger, but can be useful as they will call for you if it has not been called yet, like in this example. In larger programs you’ll usually want to control the logging configuration explicitly however - so for that reason as well as others, it’s better to create loggers and call their methods. A very common situation is that of recording logging events in a file, so let’s look at that next. Be sure to try the following in a newly started Python interpreter, and don’t just continue from the session described above: 'This message should go to the log file' 'And non-ASCII stuff, too, like Øresund and Malmö' Changed in version 3.9: The encoding argument was added. In earlier Python versions, or if not specified, the encoding used is the default value used by . While not shown in the above example, an errors argument can also now be passed, which determines how encoding errors are handled. For available values and the default, see the documentation for . And now if we open the file and look at what we have, we should find the log messages: DEBUG:__main__:This message should go to the log file INFO:__main__:So should this WARNING:__main__:And this, too ERROR:__main__:And non-ASCII stuff, too, like Øresund and Malmö This example also shows how you can set the logging level which acts as the threshold for tracking. In this case, because we set the threshold to , all of the messages were printed. If you want to set the logging level from a command-line option such as: and you have the value of the parameter passed for in some variable loglevel, you can use: to get the value which you’ll pass to via the level argument. You may want to error check any user input value, perhaps as in the following example: # assuming loglevel is bound to the string value obtained from the # command line argument. Convert to upper case to allow the user to The call to should come before any calls to a logger’s methods such as , , etc. Otherwise, that logging event may not be handled in the desired manner. If you run the above script several times, the messages from successive runs are appended to the file example.log. If you want each run to start afresh, not remembering the messages from earlier runs, you can specify the filemode argument, by changing the call in the above example to: The output will be the same as before, but the log file is no longer appended to, so the messages from earlier runs are lost. To log variable data, use a format string for the event description message and append the variable data as arguments. For example: As you can see, merging of variable data into the event description message uses the old, %-style of string formatting. This is for backwards compatibility: the logging package pre-dates newer formatting options such as and . These newer formatting options are supported, but exploring them is outside the scope of this tutorial: see Using particular formatting styles throughout your application for more information. To change the format which is used to display messages, you need to specify the format you want to use: 'This message should appear on the console' DEBUG:This message should appear on the console INFO:So should this WARNING:And this, too Notice that the ‘root’ which appeared in earlier examples has disappeared. For a full set of things that can appear in format strings, you can refer to the documentation for LogRecord attributes, but for simple usage, you just need the levelname (severity), message (event description, including variable data) and perhaps to display when the event occurred. This is described in the next section. That concludes the basic tutorial. It should be enough to get you up and running with logging. There’s a lot more that the logging package offers, but to get the best out of it, you’ll need to invest a little more of your time in reading the following sections. If you’re ready for that, grab some of your favourite beverage and carry on. If your logging needs are simple, then use the above examples to incorporate logging into your own scripts, and if you run into problems or don’t understand something, please post a question on the comp.lang.python Usenet group (available at https://groups.google.com/g/comp.lang.python) and you should receive help before too long. Still here? You can carry on reading the next few sections, which provide a slightly more advanced/in-depth tutorial than the basic one above. After that, you can take a look at the Logging Cookbook.\n\nThe logging library takes a modular approach and offers several categories of components: loggers, handlers, filters, and formatters.\n• None Loggers expose the interface that application code directly uses.\n• None Handlers send the log records (created by loggers) to the appropriate destination.\n• None Filters provide a finer grained facility for determining which log records to output.\n• None Formatters specify the layout of log records in the final output. Log event information is passed between loggers, handlers, filters and formatters in a instance. Logging is performed by calling methods on instances of the class (hereafter called loggers). Each instance has a name, and they are conceptually arranged in a namespace hierarchy using dots (periods) as separators. For example, a logger named ‘scan’ is the parent of loggers ‘scan.text’, ‘scan.html’ and ‘scan.pdf’. Logger names can be anything you want, and indicate the area of an application in which a logged message originates. A good convention to use when naming loggers is to use a module-level logger, in each module which uses logging, named as follows: This means that logger names track the package/module hierarchy, and it’s intuitively obvious where events are logged just from the logger name. The root of the hierarchy of loggers is called the root logger. That’s the logger used by the functions , , , and , which just call the same-named method of the root logger. The functions and the methods have the same signatures. The root logger’s name is printed as ‘root’ in the logged output. It is, of course, possible to log messages to different destinations. Support is included in the package for writing log messages to files, HTTP GET/POST locations, email via SMTP, generic sockets, queues, or OS-specific logging mechanisms such as syslog or the Windows NT event log. Destinations are served by handler classes. You can create your own log destination class if you have special requirements not met by any of the built-in handler classes. By default, no destination is set for any logging messages. You can specify a destination (such as console or file) by using as in the tutorial examples. If you call the functions , , , and , they will check to see if no destination is set; and if one is not set, they will set a destination of the console ( ) and a default format for the displayed message before delegating to the root logger to do the actual message output. The default format set by for messages is: You can change this by passing a format string to with the format keyword argument. For all options regarding how a format string is constructed, see Formatter Objects. The flow of log event information in loggers and handlers is illustrated in the following diagram. At least one handler objects have a threefold job. First, they expose several methods to application code so that applications can log messages at runtime. Second, logger objects determine which log messages to act upon based upon severity (the default filtering facility) or filter objects. Third, logger objects pass along relevant log messages to all interested log handlers. The most widely used methods on logger objects fall into two categories: configuration and message sending. These are the most common configuration methods:\n• None specifies the lowest-severity log message a logger will handle, where debug is the lowest built-in severity level and critical is the highest built-in severity. For example, if the severity level is INFO, the logger will handle only INFO, WARNING, ERROR, and CRITICAL messages and will ignore DEBUG messages.\n• None and add and remove handler objects from the logger object. Handlers are covered in more detail in Handlers.\n• None and add and remove filter objects from the logger object. Filters are covered in more detail in Filter Objects. You don’t need to always call these methods on every logger you create. See the last two paragraphs in this section. With the logger object configured, the following methods create log messages:\n• None , , , , and all create log records with a message and a level that corresponds to their respective method names. The message is actually a format string, which may contain the standard string substitution syntax of , , , and so on. The rest of their arguments is a list of objects that correspond with the substitution fields in the message. With regard to , the logging methods care only about a keyword of and use it to determine whether to log exception information.\n• None creates a log message similar to . The difference is that dumps a stack trace along with it. Call this method only from an exception handler.\n• None takes a log level as an explicit argument. This is a little more verbose for logging messages than using the log level convenience methods listed above, but this is how to log at custom log levels. returns a reference to a logger instance with the specified name if it is provided, or if not. The names are period-separated hierarchical structures. Multiple calls to with the same name will return a reference to the same logger object. Loggers that are further down in the hierarchical list are children of loggers higher up in the list. For example, given a logger with a name of , loggers with names of , , and are all descendants of . Loggers have a concept of effective level. If a level is not explicitly set on a logger, the level of its parent is used instead as its effective level. If the parent has no explicit level set, its parent is examined, and so on - all ancestors are searched until an explicitly set level is found. The root logger always has an explicit level set ( by default). When deciding whether to process an event, the effective level of the logger is used to determine whether the event is passed to the logger’s handlers. Child loggers propagate messages up to the handlers associated with their ancestor loggers. Because of this, it is unnecessary to define and configure handlers for all the loggers an application uses. It is sufficient to configure handlers for a top-level logger and create child loggers as needed. (You can, however, turn off propagation by setting the propagate attribute of a logger to .) objects are responsible for dispatching the appropriate log messages (based on the log messages’ severity) to the handler’s specified destination. objects can add zero or more handler objects to themselves with an method. As an example scenario, an application may want to send all log messages to a log file, all log messages of error or higher to stdout, and all messages of critical to an email address. This scenario requires three individual handlers where each handler is responsible for sending messages of a specific severity to a specific location. The standard library includes quite a few handler types (see Useful Handlers); the tutorials use mainly and in its examples. There are very few methods in a handler for application developers to concern themselves with. The only handler methods that seem relevant for application developers who are using the built-in handler objects (that is, not creating custom handlers) are the following configuration methods:\n• None The method, just as in logger objects, specifies the lowest severity that will be dispatched to the appropriate destination. Why are there two methods? The level set in the logger determines which severity of messages it will pass to its handlers. The level set in each handler determines which messages that handler will send on.\n• None selects a Formatter object for this handler to use.\n• None and respectively configure and deconfigure filter objects on handlers. Application code should not directly instantiate and use instances of . Instead, the class is a base class that defines the interface that all handlers should have and establishes some default behavior that child classes can use (or override). Formatter objects configure the final order, structure, and contents of the log message. Unlike the base class, application code may instantiate formatter classes, although you could likely subclass the formatter if your application needs special behavior. The constructor takes three optional arguments – a message format string, a date format string and a style indicator. If there is no message format string, the default is to use the raw message. If there is no date format string, the default date format is: with the milliseconds tacked on at the end. The is one of , , or . If one of these is not specified, then will be used. If the is , the message format string uses styled string substitution; the possible keys are documented in LogRecord attributes. If the style is , the message format string is assumed to be compatible with (using keyword arguments), while if the style is then the message format string should conform to what is expected by . The following message format string will log the time in a human-readable format, the severity of the message, and the contents of the message, in that order: Formatters use a user-configurable function to convert the creation time of a record to a tuple. By default, is used; to change this for a particular formatter instance, set the attribute of the instance to a function with the same signature as or . To change it for all formatters, for example if you want all logging times to be shown in GMT, set the attribute in the Formatter class (to for GMT display). Programmers can configure logging in three ways:\n• None Creating loggers, handlers, and formatters explicitly using Python code that calls the configuration methods listed above.\n• None Creating a logging config file and reading it using the function.\n• None Creating a dictionary of configuration information and passing it to the function. For the reference documentation on the last two options, see Configuration functions. The following example configures a very simple logger, a console handler, and a simple formatter using Python code: Running this module from the command line produces the following output: The following Python module creates a logger, handler, and formatter nearly identical to those in the example listed above, with the only difference being the names of the objects: Here is the logging.conf file: The output is nearly identical to that of the non-config-file-based example: You can see that the config file approach has a few advantages over the Python code approach, mainly separation of configuration and code and the ability of noncoders to easily modify the logging properties. The function takes a default parameter, , which defaults to for reasons of backward compatibility. This may or may not be what you want, since it will cause any non-root loggers existing before the call to be disabled unless they (or an ancestor) are explicitly named in the configuration. Please refer to the reference documentation for more information, and specify for this parameter if you wish. The dictionary passed to can also specify a Boolean value with key , which if not specified explicitly in the dictionary also defaults to being interpreted as . This leads to the logger-disabling behaviour described above, which may not be what you want - in which case, provide the key explicitly with a value of . Note that the class names referenced in config files need to be either relative to the logging module, or absolute values which can be resolved using normal import mechanisms. Thus, you could use either (relative to the logging module) or (for a class defined in package and module , where is available on the Python import path). In Python 3.2, a new means of configuring logging has been introduced, using dictionaries to hold configuration information. This provides a superset of the functionality of the config-file-based approach outlined above, and is the recommended configuration method for new applications and deployments. Because a Python dictionary is used to hold configuration information, and since you can populate that dictionary using different means, you have more options for configuration. For example, you can use a configuration file in JSON format, or, if you have access to YAML processing functionality, a file in YAML format, to populate the configuration dictionary. Or, of course, you can construct the dictionary in Python code, receive it in pickled form over a socket, or use whatever approach makes sense for your application. Here’s an example of the same configuration as above, in YAML format for the new dictionary-based approach: For more information about logging using a dictionary, see Configuration functions. What happens if no configuration is provided¶ If no logging configuration is provided, it is possible to have a situation where a logging event needs to be output, but no handlers can be found to output the event. The event is output using a ‘handler of last resort’, stored in . This internal handler is not associated with any logger, and acts like a which writes the event description message to the current value of (therefore respecting any redirections which may be in effect). No formatting is done on the message - just the bare event description message is printed. The handler’s level is set to , so all events at this and greater severities will be output. Changed in version 3.2: For versions of Python prior to 3.2, the behaviour is as follows:\n• None If is (production mode), the event is silently dropped.\n• None If is (development mode), a message ‘No handlers could be found for logger X.Y.Z’ is printed once. To obtain the pre-3.2 behaviour, can be set to . When developing a library which uses logging, you should take care to document how the library uses logging - for example, the names of loggers used. Some consideration also needs to be given to its logging configuration. If the using application does not use logging, and library code makes logging calls, then (as described in the previous section) events of severity and greater will be printed to . This is regarded as the best default behaviour. If for some reason you don’t want these messages printed in the absence of any logging configuration, you can attach a do-nothing handler to the top-level logger for your library. This avoids the message being printed, since a handler will always be found for the library’s events: it just doesn’t produce any output. If the library user configures logging for application use, presumably that configuration will add some handlers, and if levels are suitably configured then logging calls made in library code will send output to those handlers, as normal. A do-nothing handler is included in the logging package: (since Python 3.1). An instance of this handler could be added to the top-level logger of the logging namespace used by the library (if you want to prevent your library’s logged events being output to in the absence of logging configuration). If all logging by a library foo is done using loggers with names matching ‘foo.x’, ‘foo.x.y’, etc. then the code: should have the desired effect. If an organisation produces a number of libraries, then the logger name specified can be ‘orgname.foo’ rather than just ‘foo’. It is strongly advised that you do not log to the root logger in your library. Instead, use a logger with a unique and easily identifiable name, such as the for your library’s top-level package or module. Logging to the root logger will make it difficult or impossible for the application developer to configure the logging verbosity or handlers of your library as they wish. It is strongly advised that you do not add any handlers other than to your library’s loggers. This is because the configuration of handlers is the prerogative of the application developer who uses your library. The application developer knows their target audience and what handlers are most appropriate for their application: if you add handlers ‘under the hood’, you might well interfere with their ability to carry out unit tests and deliver logs which suit their requirements.\n\nThe numeric values of logging levels are given in the following table. These are primarily of interest if you want to define your own levels, and need them to have specific values relative to the predefined levels. If you define a level with the same numeric value, it overwrites the predefined value; the predefined name is lost. Levels can also be associated with loggers, being set either by the developer or through loading a saved logging configuration. When a logging method is called on a logger, the logger compares its own level with the level associated with the method call. If the logger’s level is higher than the method call’s, no logging message is actually generated. This is the basic mechanism controlling the verbosity of logging output. Logging messages are encoded as instances of the class. When a logger decides to actually log an event, a instance is created from the logging message. Logging messages are subjected to a dispatch mechanism through the use of handlers, which are instances of subclasses of the class. Handlers are responsible for ensuring that a logged message (in the form of a ) ends up in a particular location (or set of locations) which is useful for the target audience for that message (such as end users, support desk staff, system administrators, developers). Handlers are passed instances intended for particular destinations. Each logger can have zero, one or more handlers associated with it (via the method of ). In addition to any handlers directly associated with a logger, all handlers associated with all ancestors of the logger are called to dispatch the message (unless the propagate flag for a logger is set to a false value, at which point the passing to ancestor handlers stops). Just as for loggers, handlers can have levels associated with them. A handler’s level acts as a filter in the same way as a logger’s level does. If a handler decides to actually dispatch an event, the method is used to send the message to its destination. Most user-defined subclasses of will need to override this . Defining your own levels is possible, but should not be necessary, as the existing levels have been chosen on the basis of practical experience. However, if you are convinced that you need custom levels, great care should be exercised when doing this, and it is possibly a very bad idea to define custom levels if you are developing a library. That’s because if multiple library authors all define their own custom levels, there is a chance that the logging output from such multiple libraries used together will be difficult for the using developer to control and/or interpret, because a given numeric value might mean different things for different libraries.\n\nFormatting of message arguments is deferred until it cannot be avoided. However, computing the arguments passed to the logging method can also be expensive, and you may want to avoid doing it if the logger will just throw away your event. To decide what to do, you can call the method which takes a level argument and returns true if the event would be created by the Logger for that level of call. You can write code like this: so that if the logger’s threshold is set above , the calls to and are never made. In some cases, can itself be more expensive than you’d like (e.g. for deeply nested loggers where an explicit level is only set high up in the logger hierarchy). In such cases (or if you want to avoid calling a method in tight loops), you can cache the result of a call to in a local or instance variable, and use that instead of calling the method each time. Such a cached value would only need to be recomputed when the logging configuration changes dynamically while the application is running (which is not all that common). There are other optimizations which can be made for specific applications which need more precise control over what logging information is collected. Here’s a list of things you can do to avoid processing during logging which you don’t need: What you don’t want to collect How to avoid collecting it Information about where calls were made from. Set to . This avoids calling , which may help to speed up your code in environments like PyPy (which can’t speed up code that uses ). Current process name when using to manage multiple processes. Current name when using . Also note that the core logging module only includes the basic handlers. If you don’t import and , they won’t take up any memory."
    },
    {
        "link": "https://docs.python.org/3/library/logging.config.html",
        "document": "This section describes the API for configuring the logging module.\n\nThe following functions configure the logging module. They are located in the module. Their use is optional — you can configure the logging module using these functions or by making calls to the main API (defined in itself) and defining handlers which are declared either in or . Takes the logging configuration from a dictionary. The contents of this dictionary are described in Configuration dictionary schema below. If an error is encountered during configuration, this function will raise a , , or with a suitably descriptive message. The following is a (possibly incomplete) list of conditions which will raise an error:\n• None A which is not a string or which is a string not corresponding to an actual logging level.\n• None A value which is not a boolean.\n• None An id which does not have a corresponding destination.\n• None Inability to resolve to an internal or external object. Parsing is performed by the class, whose constructor is passed the dictionary used for configuration, and has a method. The module has a callable attribute which is initially set to . You can replace the value of with a suitable implementation of your own. calls passing the specified dictionary, and then calls the method on the returned object to put the configuration into effect: For example, a subclass of could call in its own , then set up custom prefixes which would be usable in the subsequent call. would be bound to this new subclass, and then could be called exactly as in the default, uncustomized state. Reads the logging configuration from a -format file. The format of the file should be as described in Configuration file format. This function can be called several times from an application, allowing an end user to select from various pre-canned configurations (if the developer provides a mechanism to present the choices and load the chosen configuration). It will raise if the file doesn’t exist and if the file is invalid or empty.\n• None fname – A filename, or a file-like object, or an instance derived from . If a -derived instance is passed, it is used as is. Otherwise, a is instantiated, and the configuration read by it from the object passed in . If that has a method, it is assumed to be a file-like object and read using ; otherwise, it is assumed to be a filename and passed to .\n• None defaults – Defaults to be passed to the can be specified in this argument.\n• None disable_existing_loggers – If specified as , loggers which exist when this call is made are left enabled. The default is because this enables old behaviour in a backward-compatible way. This behaviour is to disable any existing non-root loggers unless they or their ancestors are explicitly named in the logging configuration.\n• None encoding – The encoding used to open file when fname is filename. Changed in version 3.4: An instance of a subclass of is now accepted as a value for . This facilitates:\n• None Use of a configuration file where logging configuration is just part of the overall application configuration.\n• None Use of a configuration read from a file, and then modified by the using application (e.g. based on command-line parameters or other aspects of the runtime environment) before being passed to . Changed in version 3.12: An exception will be thrown if the provided file doesn’t exist or is invalid or empty. Starts up a socket server on the specified port, and listens for new configurations. If no port is specified, the module’s default is used. Logging configurations will be sent as a file suitable for processing by or . Returns a instance on which you can call to start the server, and which you can when appropriate. To stop the server, call . The argument, if specified, should be a callable which should verify whether bytes received across the socket are valid and should be processed. This could be done by encrypting and/or signing what is sent across the socket, such that the callable can perform signature verification and/or decryption. The callable is called with a single argument - the bytes received across the socket - and should return the bytes to be processed, or to indicate that the bytes should be discarded. The returned bytes could be the same as the passed in bytes (e.g. when only verification is done), or they could be completely different (perhaps if decryption were performed). To send a configuration to the socket, read in the configuration file and send it to the socket as a sequence of bytes preceded by a four-byte length string packed in binary using . Because portions of the configuration are passed through , use of this function may open its users to a security risk. While the function only binds to a socket on , and so does not accept connections from remote machines, there are scenarios where untrusted code could be run under the account of the process which calls . Specifically, if the process calling runs on a multi-user machine where users cannot trust each other, then a malicious user could arrange to run essentially arbitrary code in a victim user’s process, simply by connecting to the victim’s socket and sending a configuration which runs whatever code the attacker wants to have executed in the victim’s process. This is especially easy to do if the default port is used, but not hard even if a different port is used. To avoid the risk of this happening, use the argument to to prevent unrecognised configurations from being applied. Changed in version 3.4: The argument was added. If you want to send configurations to the listener which don’t disable existing loggers, you will need to use a JSON format for the configuration, which will use for configuration. This method allows you to specify as in the configuration you send. Stops the listening server which was created with a call to . This is typically called before calling on the return value from .\n\nDescribing a logging configuration requires listing the various objects to create and the connections between them; for example, you may create a handler named ‘console’ and then say that the logger named ‘startup’ will send its messages to the ‘console’ handler. These objects aren’t limited to those provided by the module because you might write your own formatter or handler class. The parameters to these classes may also need to include external objects such as . The syntax for describing these objects and connections is defined in Object connections below. The dictionary passed to must contain the following keys:\n• None version - to be set to an integer value representing the schema version. The only valid value at present is 1, but having this key allows the schema to evolve while still preserving backwards compatibility. All other keys are optional, but if present they will be interpreted as described below. In all cases below where a ‘configuring dict’ is mentioned, it will be checked for the special key to see if a custom instantiation is required. If so, the mechanism described in User-defined objects below is used to create an instance; otherwise, the context is used to determine what to instantiate.\n• None formatters - the corresponding value will be a dict in which each key is a formatter id and each value is a dict describing how to configure the corresponding instance. The configuring dict is searched for the following optional keys which correspond to the arguments passed to create a object: An optional key indicates the name of the formatter’s class (as a dotted module and class name). The instantiation arguments are as for , thus this key is most useful for instantiating a customised subclass of . For example, the alternative class might present exception tracebacks in an expanded or condensed format. If your formatter requires different or extra configuration keys, you should use User-defined objects.\n• None filters - the corresponding value will be a dict in which each key is a filter id and each value is a dict describing how to configure the corresponding Filter instance. The configuring dict is searched for the key (defaulting to the empty string) and this is used to construct a instance.\n• None handlers - the corresponding value will be a dict in which each key is a handler id and each value is a dict describing how to configure the corresponding Handler instance. The configuring dict is searched for the following keys:\n• None (mandatory). This is the fully qualified name of the handler class.\n• None (optional). The level of the handler.\n• None (optional). The id of the formatter for this handler.\n• None (optional). A list of ids of the filters for this handler. Changed in version 3.11: can take filter instances in addition to ids. All other keys are passed through as keyword arguments to the handler’s constructor. For example, given the snippet: the handler with id is instantiated as a , using as the underlying stream. The handler with id is instantiated as a with the keyword arguments .\n• None loggers - the corresponding value will be a dict in which each key is a logger name and each value is a dict describing how to configure the corresponding Logger instance. The configuring dict is searched for the following keys:\n• None (optional). The level of the logger.\n• None (optional). The propagation setting of the logger.\n• None (optional). A list of ids of the filters for this logger. Changed in version 3.11: can take filter instances in addition to ids.\n• None (optional). A list of ids of the handlers for this logger. The specified loggers will be configured according to the level, propagation, filters and handlers specified.\n• None root - this will be the configuration for the root logger. Processing of the configuration will be as for any logger, except that the setting will not be applicable.\n• None incremental - whether the configuration is to be interpreted as incremental to the existing configuration. This value defaults to , which means that the specified configuration replaces the existing configuration with the same semantics as used by the existing API. If the specified value is , the configuration is processed as described in the section on Incremental Configuration.\n• None disable_existing_loggers - whether any existing non-root loggers are to be disabled. This setting mirrors the parameter of the same name in . If absent, this parameter defaults to . This value is ignored if incremental is . It is difficult to provide complete flexibility for incremental configuration. For example, because objects such as filters and formatters are anonymous, once a configuration is set up, it is not possible to refer to such anonymous objects when augmenting a configuration. Furthermore, there is not a compelling case for arbitrarily altering the object graph of loggers, handlers, filters, formatters at run-time, once a configuration is set up; the verbosity of loggers and handlers can be controlled just by setting levels (and, in the case of loggers, propagation flags). Changing the object graph arbitrarily in a safe way is problematic in a multi-threaded environment; while not impossible, the benefits are not worth the complexity it adds to the implementation. Thus, when the key of a configuration dict is present and is , the system will completely ignore any and entries, and process only the settings in the entries, and the and settings in the and entries. Using a value in the configuration dict lets configurations to be sent over the wire as pickled dicts to a socket listener. Thus, the logging verbosity of a long-running application can be altered over time with no need to stop and restart the application. The schema describes a set of logging objects - loggers, handlers, formatters, filters - which are connected to each other in an object graph. Thus, the schema needs to represent connections between the objects. For example, say that, once configured, a particular logger has attached to it a particular handler. For the purposes of this discussion, we can say that the logger represents the source, and the handler the destination, of a connection between the two. Of course in the configured objects this is represented by the logger holding a reference to the handler. In the configuration dict, this is done by giving each destination object an id which identifies it unambiguously, and then using the id in the source object’s configuration to indicate that a connection exists between the source and the destination object with that id. So, for example, consider the following YAML snippet: # configuration for formatter with id 'brief' goes here # configuration for formatter with id 'precise' goes here # configuration of handler with id 'h1' goes here # configuration of handler with id 'h2' goes here The ids for loggers are the logger names which would be used programmatically to obtain a reference to those loggers, e.g. . The ids for Formatters and Filters can be any string value (such as , above) and they are transient, in that they are only meaningful for processing the configuration dictionary and used to determine connections between objects, and are not persisted anywhere when the configuration call is complete. The above snippet indicates that logger named should have two handlers attached to it, which are described by the handler ids and . The formatter for is that described by id , and the formatter for is that described by id . The schema supports user-defined objects for handlers, filters and formatters. (Loggers do not need to have different types for different instances, so there is no support in this configuration schema for user-defined logger classes.) Objects to be configured are described by dictionaries which detail their configuration. In some places, the logging system will be able to infer from the context how an object is to be instantiated, but when a user-defined object is to be instantiated, the system will not know how to do this. In order to provide complete flexibility for user-defined object instantiation, the user needs to provide a ‘factory’ - a callable which is called with a configuration dictionary and which returns the instantiated object. This is signalled by an absolute import path to the factory being made available under the special key . Here’s a concrete example: The above YAML snippet defines three formatters. The first, with id , is a standard instance with the specified format string. The second, with id , has a longer format and also defines the time format explicitly, and will result in a initialized with those two format strings. Shown in Python source form, the and formatters have configuration sub-dictionaries: respectively, and as these dictionaries do not contain the special key , the instantiation is inferred from the context: as a result, standard instances are created. The configuration sub-dictionary for the third formatter, with id , is: and this contains the special key , which means that user-defined instantiation is wanted. In this case, the specified factory callable will be used. If it is an actual callable it will be used directly - otherwise, if you specify a string (as in the example) the actual callable will be located using normal import mechanisms. The callable will be called with the remaining items in the configuration sub-dictionary as keyword arguments. In the above example, the formatter with id will be assumed to be returned by the call: The values for keys such as , and in the above example should not be configuration dictionaries or references such as or , because they will not be processed by the configuration machinery, but passed to the callable as-is. The key has been used as the special key because it is not a valid keyword parameter name, and so will not clash with the names of the keyword arguments used in the call. The also serves as a mnemonic that the corresponding value is a callable. Changed in version 3.11: The member of and can take filter instances in addition to ids. You can also specify a special key whose value is a dictionary is a mapping of attribute names to values. If found, the specified attributes will be set on the user-defined object before it is returned. Thus, with the following configuration: the returned formatter will have attribute set to and attribute set to . The values for attributes such as and in the above example should not be configuration dictionaries or references such as or , because they will not be processed by the configuration machinery, but set as attribute values as-is. Handlers are configured in alphabetical order of their keys, and a configured handler replaces the configuration dictionary in (a working copy of) the dictionary in the schema. If you use a construct such as , then initially points to the configuration dictionary for the handler named , and later (once that handler has been configured) it points to the configured handler instance. Thus, could resolve to either a dictionary or a handler instance. In general, it is wise to name handlers in a way such that dependent handlers are configured _after_ any handlers they depend on; that allows something like to be used in configuring a handler that depends on handler . If that dependent handler were named , problems would result, because the configuration of would be attempted before that of , and would not yet have been configured. However, if the dependent handler were named , it would be configured after , with the result that would resolve to configured handler , and not its configuration dictionary. There are times where a configuration needs to refer to objects external to the configuration, for example . If the configuration dict is constructed using Python code, this is straightforward, but a problem arises when the configuration is provided via a text file (e.g. JSON, YAML). In a text file, there is no standard way to distinguish from the literal string . To facilitate this distinction, the configuration system looks for certain special prefixes in string values and treat them specially. For example, if the literal string is provided as a value in the configuration, then the will be stripped off and the remainder of the value processed using normal import mechanisms. The handling of such prefixes is done in a way analogous to protocol handling: there is a generic mechanism to look for prefixes which match the regular expression whereby, if the is recognised, the is processed in a prefix-dependent manner and the result of the processing replaces the string value. If the prefix is not recognised, then the string value will be left as-is. As well as external objects, there is sometimes also a need to refer to objects in the configuration. This will be done implicitly by the configuration system for things that it knows about. For example, the string value for a in a logger or handler will automatically be converted to the value , and the , and entries will take an object id and resolve to the appropriate destination object. However, a more generic mechanism is needed for user-defined objects which are not known to the module. For example, consider , which takes a argument which is another handler to delegate to. Since the system already knows about this class, then in the configuration, the given just needs to be the object id of the relevant target handler, and the system will resolve to the handler from the id. If, however, a user defines a which has an handler, the configuration system would not know that the referred to a handler. To cater for this, a generic resolution system allows the user to specify: # configuration of file handler goes here The literal string will be resolved in an analogous way to strings with the prefix, but looking in the configuration itself rather than the import namespace. The mechanism allows access by dot or by index, in a similar way to that provided by . Thus, given the following snippet: in the configuration, the string would resolve to the dict with key , the string would resolve to the dict with key in the dict, and so on. The string would resolve to and the string would resolve to the value . The value could be accessed using either or, equivalently, . The latter form only needs to be used if the key contains spaces or non-alphanumeric characters. Please note that the characters and are not allowed in the keys. If an index value consists only of decimal digits, access will be attempted using the corresponding integer value, falling back to the string value if needed. Given a string , this will resolve to . If the string is specified as , the system will attempt to retrieve the value from , and fall back to if that fails. Import resolution, by default, uses the builtin function to do its importing. You may want to replace this with your own importing mechanism: if so, you can replace the attribute of the or its superclass, the class. However, you need to be careful because of the way functions are accessed from classes via descriptors. If you are using a Python callable to do your imports, and you want to define it at class level rather than instance level, you need to wrap it with . For example: You don’t need to wrap with if you’re setting the import callable on a configurator instance. If you want to configure a , noting that this is normally used in conjunction with a , you can configure both together. After the configuration, the instance will be available as the attribute of the created handler, and that in turn will be available to you using and passing the name you have used for the in your configuration. The dictionary schema for configuring the pair is shown in the example YAML snippet below. The and keys are optional. If the key is present, the corresponding value can be one of the following:\n• None An object implementing the and public API. For instance, this may be an actual instance of or a subclass thereof, or a proxy obtained by . This is of course only possible if you are constructing or modifying the configuration dictionary in code.\n• None A string that resolves to a callable which, when called with no arguments, returns the queue instance to use. That callable could be a subclass or a function which returns a suitable queue instance, such as .\n• None A dict with a key which is constructed in the usual way as discussed in User-defined objects. The result of this construction should be a instance. If the key is absent, a standard unbounded instance is created and used. If the key is present, the corresponding value can be one of the following:\n• None A subclass of . This is of course only possible if you are constructing or modifying the configuration dictionary in code.\n• None A string which resolves to a class which is a subclass of , such as .\n• None A dict with a key which is constructed in the usual way as discussed in User-defined objects. The result of this construction should be a callable with the same signature as the initializer. If the key is absent, is used. The values under the key are the names of other handlers in the configuration (not shown in the above snippet) which will be passed to the queue listener. Any custom queue handler and listener classes will need to be defined with the same initialization signatures as and .\n\nThe configuration file format understood by is based on functionality. The file must contain sections called , and which identify by name the entities of each type which are defined in the file. For each such entity, there is a separate section which identifies how that entity is configured. Thus, for a logger named in the section, the relevant configuration details are held in a section . Similarly, a handler called in the section will have its configuration held in a section called , while a formatter called in the section will have its configuration specified in a section called . The root logger configuration must be specified in a section called . The API is older than the API and does not provide functionality to cover certain aspects of logging. For example, you cannot configure objects, which provide for filtering of messages beyond simple integer levels, using . If you need to have instances of in your logging configuration, you will need to use . Note that future enhancements to configuration functionality will be added to , so it’s worth considering transitioning to this newer API when it’s convenient to do so. Examples of these sections in the file are given below. The root logger must specify a level and a list of handlers. An example of a root logger section is given below. The entry can be one of or . For the root logger only, means that all messages will be logged. Level values are evaluated in the context of the package’s namespace. The entry is a comma-separated list of handler names, which must appear in the section. These names must appear in the section and have corresponding sections in the configuration file. For loggers other than the root logger, some additional information is required. This is illustrated by the following example. The and entries are interpreted as for the root logger, except that if a non-root logger’s level is specified as , the system consults loggers higher up the hierarchy to determine the effective level of the logger. The entry is set to 1 to indicate that messages must propagate to handlers higher up the logger hierarchy from this logger, or 0 to indicate that messages are not propagated to handlers up the hierarchy. The entry is the hierarchical channel name of the logger, that is to say the name used by the application to get the logger. Sections which specify handler configuration are exemplified by the following. The entry indicates the handler’s class (as determined by in the package’s namespace). The is interpreted as for loggers, and is taken to mean ‘log everything’. The entry indicates the key name of the formatter for this handler. If blank, a default formatter ( ) is used. If a name is specified, it must appear in the section and have a corresponding section in the configuration file. The entry, when evaluated in the context of the package’s namespace, is the list of arguments to the constructor for the handler class. Refer to the constructors for the relevant handlers, or to the examples below, to see how typical entries are constructed. If not provided, it defaults to . The optional entry, when evaluated in the context of the package’s namespace, is the keyword argument dict to the constructor for the handler class. If not provided, it defaults to . Sections which specify formatter configuration are typified by the following. The arguments for the formatter configuration are the same as the keys in the dictionary schema formatters section. The entry, when evaluated in the context of the package’s namespace, is a dictionary of default values for custom formatting fields. If not provided, it defaults to . Due to the use of as described above, there are potential security risks which result from using the to send and receive configurations via sockets. The risks are limited to where multiple users with no mutual trust run code on the same machine; see the documentation for more information. Useful handlers included with the logging module."
    },
    {
        "link": "https://docs.python.org/3/library/logging.handlers.html",
        "document": "The following useful handlers are provided in the package. Note that three of the handlers ( , and ) are actually defined in the module itself, but have been documented here along with the other handlers.\n\nThe class, located in the module, is a which watches the file it is logging to. If the file changes, it is closed and reopened using the file name. A file change can happen because of usage of programs such as newsyslog and logrotate which perform log file rotation. This handler, intended for use under Unix/Linux, watches the file to see if it has changed since the last emit. (A file is deemed to have changed if its device or inode have changed.) If the file has changed, the old file stream is closed, and the file opened to get a new stream. This handler is not appropriate for use under Windows, because under Windows open log files cannot be moved or renamed - logging opens the files with exclusive locks - and so there is no need for such a handler. Furthermore, ST_INO is not supported under Windows; always returns zero for this value. Returns a new instance of the class. The specified file is opened and used as the stream for logging. If mode is not specified, is used. If encoding is not , it is used to open the file with that encoding. If delay is true, then file opening is deferred until the first call to . By default, the file grows indefinitely. If errors is provided, it determines how encoding errors are handled. Changed in version 3.6: As well as string values, objects are also accepted for the filename argument. Changed in version 3.9: The errors parameter was added. Checks to see if the file has changed. If it has, the existing stream is flushed and closed and the file opened again, typically as a precursor to outputting the record to the file. Outputs the record to the file, but first calls to reopen the file if it has changed.\n\nThe class, located in the module, is the base class for the rotating file handlers, and . You should not need to instantiate this class, but it has attributes and methods you may need to override. The parameters are as for . The attributes are: If this attribute is set to a callable, the method delegates to this callable. The parameters passed to the callable are those passed to . The namer function is called quite a few times during rollover, so it should be as simple and as fast as possible. It should also return the same output every time for a given input, otherwise the rollover behaviour may not work as expected. It’s also worth noting that care should be taken when using a namer to preserve certain attributes in the filename which are used during rotation. For example, expects to have a set of log files whose names contain successive integers, so that rotation works as expected, and deletes old log files (based on the parameter passed to the handler’s initializer) by determining the oldest files to delete. For this to happen, the filenames should be sortable using the date/time portion of the filename, and a namer needs to respect this. (If a namer is wanted that doesn’t respect this scheme, it will need to be used in a subclass of which overrides the method to fit in with the custom naming scheme.) If this attribute is set to a callable, the method delegates to this callable. The parameters passed to the callable are those passed to . Modify the filename of a log file when rotating. This is provided so that a custom filename can be provided. The default implementation calls the ‘namer’ attribute of the handler, if it’s callable, passing the default name to it. If the attribute isn’t callable (the default is ), the name is returned unchanged. default_name – The default name for the log file. The default implementation calls the ‘rotator’ attribute of the handler, if it’s callable, passing the source and dest arguments to it. If the attribute isn’t callable (the default is ), the source is simply renamed to the destination.\n• None source – The source filename. This is normally the base filename, e.g. ‘test.log’.\n• None dest – The destination filename. This is normally what the source is rotated to, e.g. ‘test.log.1’. The reason the attributes exist is to save you having to subclass - you can use the same callables for instances of and . If either the namer or rotator callable raises an exception, this will be handled in the same way as any other exception during an call, i.e. via the method of the handler. If you need to make more significant changes to rotation processing, you can override the methods. For an example, see Using a rotator and namer to customize log rotation processing.\n\nThe class, located in the module, supports rotation of disk log files at certain timed intervals. Returns a new instance of the class. The specified file is opened and used as the stream for logging. On rotating it also sets the filename suffix. Rotating happens based on the product of when and interval. You can use the when to specify the type of interval. The list of possible values is below. Note that they are not case sensitive. Roll over at midnight, if atTime not specified, else at time atTime When using weekday-based rotation, specify ‘W0’ for Monday, ‘W1’ for Tuesday, and so on up to ‘W6’ for Sunday. In this case, the value passed for interval isn’t used. The system will save old log files by appending extensions to the filename. The extensions are date-and-time based, using the strftime format or a leading portion thereof, depending on the rollover interval. When computing the next rollover time for the first time (when the handler is created), the last modification time of an existing log file, or else the current time, is used to compute when the next rotation will occur. If the utc argument is true, times in UTC will be used; otherwise local time is used. If backupCount is nonzero, at most backupCount files will be kept, and if more would be created when rollover occurs, the oldest one is deleted. The deletion logic uses the interval to determine which files to delete, so changing the interval may leave old files lying around. If delay is true, then file opening is deferred until the first call to . If atTime is not , it must be a instance which specifies the time of day when rollover occurs, for the cases where rollover is set to happen “at midnight” or “on a particular weekday”. Note that in these cases, the atTime value is effectively used to compute the initial rollover, and subsequent rollovers would be calculated via the normal interval calculation. If errors is specified, it’s used to determine how encoding errors are handled. Calculation of the initial rollover time is done when the handler is initialised. Calculation of subsequent rollover times is done only when rollover occurs, and rollover occurs only when emitting output. If this is not kept in mind, it might lead to some confusion. For example, if an interval of “every minute” is set, that does not mean you will always see log files with times (in the filename) separated by a minute; if, during application execution, logging output is generated more frequently than once a minute, then you can expect to see log files with times separated by a minute. If, on the other hand, logging messages are only output once every five minutes (say), then there will be gaps in the file times corresponding to the minutes where no output (and hence no rollover) occurred. Changed in version 3.6: As well as string values, objects are also accepted for the filename argument. Changed in version 3.9: The errors parameter was added. Does a rollover, as described above. Outputs the record to the file, catering for rollover as described above. Returns a list of filenames which should be deleted as part of rollover. These are the absolute paths of the oldest backup log files written by the handler.\n\nThe class, located in the module, supports sending logging messages to a remote or local Unix syslog. Returns a new instance of the class intended to communicate with a remote Unix machine whose address is given by address in the form of a tuple. If address is not specified, is used. The address is used to open a socket. An alternative to providing a tuple is providing an address as a string, for example ‘/dev/log’. In this case, a Unix domain socket is used to send the message to the syslog. If facility is not specified, is used. The type of socket opened depends on the socktype argument, which defaults to and thus opens a UDP socket. To open a TCP socket (for use with the newer syslog daemons such as rsyslog), specify a value of . Note that if your server is not listening on UDP port 514, may appear not to work. In that case, check what address you should be using for a domain socket - it’s system dependent. For example, on Linux it’s usually ‘/dev/log’ but on OS/X it’s ‘/var/run/syslog’. You’ll need to check your platform and use the appropriate address (you may need to do this check at runtime if your application needs to run on several platforms). On Windows, you pretty much have to use the UDP option. On macOS 12.x (Monterey), Apple has changed the behaviour of their syslog daemon - it no longer listens on a domain socket. Therefore, you cannot expect to work on this system. See gh-91070 for more information. Closes the socket to the remote host. Tries to create a socket and, if it’s not a datagram socket, connect it to the other end. This method is called during handler initialization, but it’s not regarded as an error if the other end isn’t listening at this point - the method will be called again when emitting an event, if there is no socket at that point. The record is formatted, and then sent to the syslog server. If exception information is present, it is not sent to the server. Changed in version 3.2.1: (See: bpo-12168.) In earlier versions, the message sent to the syslog daemons was always terminated with a NUL byte, because early versions of these daemons expected a NUL terminated message - even though it’s not in the relevant specification (RFC 5424). More recent versions of these daemons don’t expect the NUL byte but strip it off if it’s there, and even more recent daemons (which adhere more closely to RFC 5424) pass the NUL byte on as part of the message. To enable easier handling of syslog messages in the face of all these differing daemon behaviours, the appending of the NUL byte has been made configurable, through the use of a class-level attribute, . This defaults to (preserving the existing behaviour) but can be set to on a instance in order for that instance to not append the NUL terminator. Changed in version 3.3: (See: bpo-12419.) In earlier versions, there was no facility for an “ident” or “tag” prefix to identify the source of the message. This can now be specified using a class-level attribute, defaulting to to preserve existing behaviour, but which can be overridden on a instance in order for that instance to prepend the ident to every message handled. Note that the provided ident must be text, not bytes, and is prepended to the message exactly as is. Encodes the facility and priority into an integer. You can pass in strings or integers - if strings are passed, internal mapping dictionaries are used to convert them to integers. The symbolic values are defined in and mirror the values defined in the header file. Maps a logging level name to a syslog priority name. You may need to override this if you are using custom levels, or if the default algorithm is not suitable for your needs. The default algorithm maps , , , and to the equivalent syslog names, and all other level names to ‘warning’.\n\nThe class, located in the module, supports sending logging messages to a local Windows NT, Windows 2000 or Windows XP event log. Before you can use it, you need Mark Hammond’s Win32 extensions for Python installed. Returns a new instance of the class. The appname is used to define the application name as it appears in the event log. An appropriate registry entry is created using this name. The dllname should give the fully qualified pathname of a .dll or .exe which contains message definitions to hold in the log (if not specified, is used - this is installed with the Win32 extensions and contains some basic placeholder message definitions. Note that use of these placeholders will make your event logs big, as the entire message source is held in the log. If you want slimmer logs, you have to pass in the name of your own .dll or .exe which contains the message definitions you want to use in the event log). The logtype is one of , or , and defaults to . At this point, you can remove the application name from the registry as a source of event log entries. However, if you do this, you will not be able to see the events as you intended in the Event Log Viewer - it needs to be able to access the registry to get the .dll name. The current version does not do this. Determines the message ID, event category and event type, and then logs the message in the NT event log. Returns the event category for the record. Override this if you want to specify your own categories. This version returns 0. Returns the event type for the record. Override this if you want to specify your own types. This version does a mapping using the handler’s typemap attribute, which is set up in to a dictionary which contains mappings for , , , and . If you are using your own levels, you will either need to override this method or place a suitable dictionary in the handler’s typemap attribute. Returns the message ID for the record. If you are using your own messages, you could do this by having the msg passed to the logger being an ID rather than a format string. Then, in here, you could use a dictionary lookup to get the message ID. This version returns 1, which is the base message ID in .\n\nThe class, located in the module, supports sending logging messages to a queue, such as those implemented in the or modules. Along with the class, can be used to let handlers do their work on a separate thread from the one which does the logging. This is important in web applications and also other service applications where threads servicing clients need to respond as quickly as possible, while any potentially slow operations (such as sending an email via ) are done on a separate thread. Returns a new instance of the class. The instance is initialized with the queue to send messages to. The queue can be any queue-like object; it’s used as-is by the method, which needs to know how to send messages to it. The queue is not required to have the task tracking API, which means that you can use instances for queue. If you are using , you should avoid using and instead use . Enqueues the result of preparing the LogRecord. Should an exception occur (e.g. because a bounded queue has filled up), the method is called to handle the error. This can result in the record silently being dropped (if is ) or a message printed to (if is ). Prepares a record for queuing. The object returned by this method is enqueued. The base implementation formats the record to merge the message, arguments, exception and stack information, if present. It also removes unpickleable items from the record in-place. Specifically, it overwrites the record’s and attributes with the merged message (obtained by calling the handler’s method), and sets the , and attributes to . You might want to override this method if you want to convert the record to a dict or JSON string, or send a modified copy of the record while leaving the original intact. The base implementation formats the message with arguments, sets the and attributes to the formatted message and sets the and attributes to to allow pickling and to prevent further attempts at formatting. This means that a handler on the side won’t have the information to do custom formatting, e.g. of exceptions. You may wish to subclass and override this method to e.g. avoid setting to . Note that the / / changes are related to ensuring the record is pickleable, and you might or might not be able to avoid doing that depending on whether your are pickleable. (Note that you may have to consider not only your own code but also code in any libraries that you use.) Enqueues the record on the queue using ; you may want to override this if you want to use blocking behaviour, or a timeout, or a customized queue implementation. When created via configuration using , this attribute will contain a instance for use with this handler. Otherwise, it will be .\n\nThe class, located in the module, supports receiving logging messages from a queue, such as those implemented in the or modules. The messages are received from a queue in an internal thread and passed, on the same thread, to one or more handlers for processing. While is not itself a handler, it is documented here because it works hand-in-hand with . Along with the class, can be used to let handlers do their work on a separate thread from the one which does the logging. This is important in web applications and also other service applications where threads servicing clients need to respond as quickly as possible, while any potentially slow operations (such as sending an email via ) are done on a separate thread. Returns a new instance of the class. The instance is initialized with the queue to send messages to and a list of handlers which will handle entries placed on the queue. The queue can be any queue-like object; it’s passed as-is to the method, which needs to know how to get messages from it. The queue is not required to have the task tracking API (though it’s used if available), which means that you can use instances for queue. If you are using , you should avoid using and instead use . If is , a handler’s level is respected (compared with the level for the message) when deciding whether to pass messages to that handler; otherwise, the behaviour is as in previous Python versions - to always pass each message to each handler. Changed in version 3.5: The argument was added. The base implementation uses . You may want to override this method if you want to use timeouts or work with custom queue implementations. This implementation just returns the passed-in record. You may want to override this method if you need to do any custom marshalling or manipulation of the record before passing it to the handlers. This just loops through the handlers offering them the record to handle. The actual object passed to the handlers is that which is returned from . This starts up a background thread to monitor the queue for LogRecords to process. This asks the thread to terminate, and then waits for it to do so. Note that if you don’t call this before your application exits, there may be some records still left on the queue, which won’t be processed. Writes a sentinel to the queue to tell the listener to quit. This implementation uses . You may want to override this method if you want to use timeouts or work with custom queue implementations."
    },
    {
        "link": "https://coralogix.com/blog/python-logging-best-practices-tips",
        "document": "Python is a highly skilled language with a large developer community, which is essential in data science, machine learning, embedded applications, and back-end web and cloud applications.\n\nAnd logging is critical to understanding software behavior in Python. Once logs are in place, log monitoring can be utilized to make sense of what is happening in the software. Python includes several logging libraries that create and direct logs to their assigned targets.\n\n\n\nThis article will go over Python logging best practices to help you get the best log monitoring setup for your organization.\n\nLogging in Python, like other programming languages, is implemented to indicate events that have occurred in software. Logs should include descriptive messages and variable data to communicate the state of the software at the time of logging.\n\nThey also communicate the severity of the event using unique log levels. Logs can be generated using the Python standard library.\n\nThe Python standard library provides a logging module to log events from applications and libraries. Once the Python JSON logger is configured, it becomes part of the Python interpreter process that is running the code.\n\nIn other words, Python logging is global. You can also configure the Python logging subsystem using an external configuration file. The specifications for the logging configuration format are found in the Python standard library documentation.\n\nThe logging library is modular and offers four categories of components:\n• Loggers expose the interface used by the application code.\n• Handlers are created by loggers and send log records to the appropriate destination.\n• Filters can determine which log records are output.\n• Formatters specify the layout of the final log record output.\n\nMultiple logger objects are organized into a tree representing various parts of your system and the different third-party libraries you have installed. When you send a message to one of the loggers, the message gets output on that logger’s handlers using a formatter attached to each handler.\n\n\n\nThe message then propagates the logger tree until it hits the root logger or a logger in the tree configured with .propagate=False. This hierarchy allows logs to be captured up the subtree of loggers, and a single handler could catch all logging messages.\n\nThe logging.Logger objects offer the primary interface to the logging library. These objects provide the logging methods to issue log requests along with the methods to query and modify their state. From here on out, we will refer to Logger objects as loggers.\n\nThe factory function logging.getLogger(name) is typically used to create loggers. By using the factory function, clients can rely on the library to manage loggers and access loggers via their names instead of storing and passing references to loggers.\n\nThe name argument in the factory function is typically a dot-separated hierarchical name, i.e. a.b.c. This naming convention enables the library to maintain a hierarchy of loggers. Specifically, when the factory function creates a logger, the library ensures a logger exists for each level of the hierarchy specified by the name, and every logger in the hierarchy is linked to its parent and child loggers.\n\nEach logger has a threshold logging level to determine whether a log request should be processed. A logger processes a log request if the numeric value of the requested logging level is greater than or equal to the severity of the logger’s threshold logging level.\n\n\n\nClients can retrieve and change the threshold logging level of a logger via Logger.getEffectiveLevel() and Logger.setLevel(level) methods, respectively. When the factory function is used to create a logger, the function sets a logger’s threshold logging level to the threshold logging level of its parent logger as determined by its name.\n\nLog levels allow you to define event severity for each log so they are easily analyzed. Python supports predefined values, which can be found by calling logging.getLevelName(). Predefined log levels include CRITICAL, ERROR, WARNING, INFO, and DEBUG from highest to lowest severity. Developers can also maintain a dictionary of log levels by defining custom levels using logging.getLogger().\n\nPython comes with different methods to read events from the software: print() and logging. Both will communicate event data but pass this information to different storage locations using different methods.\n\nThe print function sends data exclusively to the console. This can be convenient for fast testing as a function is developed, but it is not practical for use in functional software. There are two critical reasons to not use print() in software:\n• If your code is used by other tools or scripts, the user will not know the context of the print messages.\n• When running Python software in containers like Docker, the print messages will not be seen since containers cannot access the console.\n\nThe logging library also provides many features contributing to Python logging best practices. These include identifying the line of the file, function, and time of log events, distinguishing log events by their importance, and providing formatting to keep log messages consistent.\n\nHere are a few code snippets to illustrate how to use the Python logging library.\n\nIn snippet 1, a logger is created with a log level of INFO. Any logs that have a severity less than INFO will not print (i.e. DEBUG logs). A new handler is created and assigned to the logger. New handlers can be added to send logging outputs to streams like sys.stdout or any file-like object.\n\n\n\nA formatter is created and added to the handler to transform log messages into placeholder data. In this formatter, the time of the log request (as an epoch timestamp), the logging level, the logger’s name, the module name, and the log message will all print.\n\nIn snippet 2, an info log states the app has started. When the app is started in the folder /home/kali with the logger created in snippet 1, the following log entry will be generated in the std.out stream:\n\nThis snippet logs an informational message every time data is written successfully via write_data. If a write fails, the snippet logs an error message that includes the stack trace in which the exception occurred. The logs here use positional arguments to enhance the value of the logs and provide more contextual information.\n\nWith the logger created using snippet 1, successful execution of write_data would create a log similar to:\n\nIf the execution fails, then the created log will appear like:\n\nFileNotFoundError: [Errno 2] No such file or directory: ‘/tmp1/tmp_data.txt’\n\nAlternatively to positional arguments, the same outputs could be achieved using complete names as in:\n\nEvery logger offers a shorthand method to log requests by level. Each pre-defined log level is available in shorthand; for example, Logger.error(msg, *args, **kwargs).\n\nIn addition to these shorthand methods, loggers also offer a general method to specify the log level in the arguments. This method is useful when using custom logging levels.\n\nAnother useful method is used for logs inside exception handlers. It issues log requests with the logging level ERROR and captures the current exception as part of the log entry.\n\nIn each of the methods above, the msg and args arguments are combined to create log messages captured by log entries. They each support the keyword argument exc_info to add exception information to log entries and stack_info and stacklevel to add call stack information to log entries. Also, they support the keyword argument extra, which is a dictionary, to pass values relevant to filters, handlers, and formatters.\n\nHow to get started with Python logging\n\nTo get the most out of your Python logging, they need to be set up consistently and ready to analyze. When setting up your Python logging, use these best practices below.\n\nThe logging.getLogger() factory function helps the library manage the mapping from logger names to logger instances and maintain a hierarchy of loggers. In turn, this mapping and hierarchy offer the following benefits:\n• Clients can use the factory function to access the same logger in different application parts by merely retrieving the logger by its name.\n• Only a finite number of loggers are created at runtime (under normal circumstances).\n• Log requests can be propagated up the logger hierarchy.\n• When unspecified, the threshold logging level of a logger can be inferred from its ascendants.\n• The configuration of the logging library can be updated at runtime by merely relying on the logger names.\n\n\n\nUse the shorthand logging.<logging level>() method to log at pre-defined logging levels. Besides making the code a bit shorter, the use of these functions helps partition the logging statements into two sets:\n• Those that issue log requests with pre-defined logging levels.\n• Those that issue log requests with custom logging levels.\n\nThe pre-defined logging levels capture almost all logging scenarios that occur. Most developers are universally familiar with these logging levels across different programming languages, making them easy to understand. The use of these values reduces deployment, configuration, and maintenance burdens.\n\nWhile creating loggers, we can create a logger for each class or create a logger for each module. While the first option enables fine-grained configuration, it leads to more loggers in a program, i.e., one per class. In contrast, the second option can help reduce the number of loggers in a program. So, unless such fine-grained configuration is necessary, create module-level loggers.\n\nUse logging.LoggerAdapter() to inject contextual information into log records. The class can also modify the log message and data provided as part of the request. Since the logging library does not manage these adapters, they cannot be accessed with common names. Use them to inject contextual information local to a module or class.\n• Use filters or .setLogRecordFactor() to inject global contextual information\n\nTwo options exist to seamlessly inject global contextual information (common across an app) into log records. The first option is to use the filter support to modify the log record arguments provided to filters. For example, the following filter injects version information into incoming log records.\n\nThere are two downsides to this option. First, if filters depend on the data in log records, then filters that inject data into log records should be executed before filters that use the injected data. Thus, the order of filters added to loggers and handlers becomes crucial. Second, the option “abuses” the support to filter log records to extend log records.\n\nThe second option is to initialize the logging library with a log record creating a factory function via logging.setLogRecordFactory(). Since the injected contextual information is global, it can be injected into log records when created in the factory function. This ensures the data will be available to every filter, formatter, logger, and handler in the program.\n\nThe downside of this option is that we have to ensure factory functions contributed by different components in a program play nicely with each other. While log record factory functions could be chained, such chaining increases the complexity of programs.\n• Use .disable() to inhibit processing of low-level requests\n\nA logger will process a log request based on the effective logging level. The effective logging level is the higher of two logging levels: the logger’s threshold level and the library-wide level. Set the library-wide logging level using the logging.disable(level) function. This is set to 0 by default so that every log request will be processed.\n\nUsing this function, the software will throttle the logging output of an app by increasing the logging level across the whole app. This can be important to keep log volumes in check in production software.\n\nPython’s logging library is more complicated than simple print() statements. The library has many great features that provide a complete solution for obtaining log data needed to achieve full-stack observability in your software.\n\nHere we show the high-level advantages and disadvantages of the library.\n\nThe Python logging library is highly configurable. Logs can be formatted before printing, can have placeholder data filled in automatically, and can be turned on and off as needed. Logs can also be sent to a number of different locations for easier reading and debugging. All of these settings are codified, so are well-defined for each logger.\n\nIn failures, it is useful to log debugging information showing where and when a failure occurred. These tracebacks can be generated automatically in the Python logging library to help speed up troubleshooting and fixes.\n\nLog levels used in different scenarios can be subjective across a development team. For proper analysis, it is important to keep log levels consistent. Create a well-defined strategy for your team about when to use each logging level available and when a custom level is appropriate.\n\nSince the logging module is so flexible, logging configurations can quickly get complicated. Create a strategy for your team for how each logging module will be defined to keep logs consistent across developers.\n\nLet’s look at an example of a basic logger in Python:\n\nLine 2: create a basicConf function and pass some arguments to create the log file. In this case, we indicate the severity level, date format, filename and file mode to have the function overwrite the log file.\n\nLine 3 to 5: messages for each logging level.\n\nThe default format for log records is SEVERITY: LOGGER: MESSAGE. Hence, if you run the code above as is, you’ll get this output:\n\nRegarding the output, you can set the destination of the log messages. As a first step, you can print messages to the screen using this sample code:\n\nIf your goals are aimed at the Cloud, you can take advantage of Python’s set of logging handlers to redirect content. Currently in beta release, you can write logs to Stackdriver Logging from Python applications by using Google’s Python logging handler included with the Stackdriver Logging client library, or by using the client library to access the API directly. When developing your logger, take into account that the root logger doesn’t use your log handler. Since the Python Client for Stackdriver Logging library also does logging, you may get a recursive loop if the root logger uses your Python log handler.\n\nWhen we use a logging library, we perform/trigger the following common tasks while using the associated concepts (highlighted in bold).\n• A client issues a log request by executing a logging statement. Often, such logging statements invoke a function/method in the logging (library) API by providing the log data and the logging level as arguments. The logging level specifies the importance of the log request. Log data is often a log message, which is a string, along with some extra data to be logged. Often, the logging API is exposed via logger objects.\n• To enable the processing of a request as it threads through the logging library, the logging library creates a log record that represents the log request and captures the corresponding log data.\n• Based on how the logging library is configured (via a logging configuration), the logging library filters the log requests/records. This filtering involves comparing the requested logging level to the threshold logging level and passing the log records through user-provided filters.\n• Handlers process the filtered log records to either store the log data (e.g., write the log data into a file) or perform other actions involving the log data (e.g., send an email with the log data). In some logging libraries, before processing log records, a handler may again filter the log records based on the handler’s logging level and user-provided handler-specific filters. Also, when needed, handlers often rely on user-provided formatters to format log records into strings, i.e., log entries.\n\nIndependent of the logging library, the above tasks are performed in an order similar to that shown in Figure 1.\n\nFigure 1: The flow of tasks when logging via a logging library\n\nEvery logger offers the following logging methods to issue log requests.\n\nEach of these methods is a shorthand to issue log requests with corresponding pre-defined logging levels as the requested logging level.\n\nIn addition to the above methods, loggers also offer the following two methods:\n• issues log requests with explicitly specified logging levels. This method is useful when using custom logging levels.\n• issues log requests with the logging level and that capture the current exception as part of the log entries. Consequently, clients should invoke this method only from an exception handler.\n\nand arguments in the above methods are combined to create log messages captured by log entries. All of the above methods support the keyword argument to add exception information to log entries and and to add call stack information to log entries. Also, they support the keyword argument , which is a dictionary, to pass values relevant to filters, handlers, and formatters.\n\nWhen executed, the above methods perform/trigger all of the tasks shown in Figure 1 and the following two tasks:\n• After deciding to process a log request based on its logging level and the threshold logging level, the logger creates a object to represent the log request in the downstream processing of the request. objects capture the and arguments of logging methods and the exception and call stack information along with source code information. They also capture the keys and values in the extra argument of the logging method as fields.\n• After every handler of a logger has processed a log request, the handlers of its ancestor loggers process the request (in the order they are encountered walking up the logger hierarchy). The field controls this aspect, which is by default.\n\nBeyond logging levels, filters provide a finer means to filter log requests based on the information in a log record, e.g., ignore log requests issued in a specific class. Clients can add and remove filters to/from loggers using and methods, respectively.\n\nThe logging classes introduced in the previous section provide methods to configure their instances and, consequently, customize the use of the logging library. Snippet 1 demonstrates how to use configuration methods. These methods are best used in simple single-file programs.\n\nWhen involved programs (e.g., apps, libraries) use the logging library, a better option is to externalize the configuration of the logging library. Such externalization allows users to customize certain facets of logging in a program (e.g., specify the location of log files, use custom loggers/handlers/formatters/filters) and, hence, ease the deployment and use of the program. We refer to this approach to configuration as data-based approach.\n\nClients can configure the logging library by invoking function. The argument is a dictionary and the following optional keys can be used to specify a configuration.\n\nfilters key maps to a dictionary of strings and dictionaries. The strings serve as filter ids used to refer to filters in the configuration (e.g., adding a filter to a logger) while the mapped dictionaries serve as filter configurations. The string value of the name key in filter configurations is used to construct .\n\nThis configuration snippet results in the creation of a filter that admits all records created by the logger named ‘app.io’ or its descendants.\n\nformatters key maps to a dictionary of strings and dictionaries. The strings serve as formatter ids used to refer to formatters in the configuration (e.g., adding a formatter to a handler) while the mapped dictionaries serve as formatter configurations. The string values of the datefmt and format keys in formatter configurations are used as the date and log entry formatting strings, respectively, to construct instances. The boolean value of the (optional) validate key controls the validation of the format strings during the construction of a formatter.\n\nThis configuration snippet results in the creation of two formatters. A simple formatter with the specified log entry and date formatting strings and detailed formatter with specified log entry formatting string and default date formatting string.\n\nhandlers key maps to a dictionary of strings and dictionaries. The strings serve as handler ids used to refer to handlers in the configuration (e.g., adding a handler to a logger) while the mapped dictionaries serve as handler configurations. The string value of the class key in a handler configuration names the class to instantiate to construct a handler. The string value of the (optional) level key specifies the logging level of the instantiated handler. The string value of the (optional) formatter key specifies the id of the formatter of the handler. Likewise, the list of values of the (optional) filters key specifies the ids of the filters of the handler. The remaining keys are passed as keyword arguments to the handler’s constructor.\n\nThis configuration snippet results in the creation of two handlers:\n• A handler that formats log requests with INFO and higher logging level log via the formatter and emits the resulting log entry into the standard error stream. The key is passed as keyword arguments to constructor.\n\nThe value of the key illustrates how to access objects external to the configuration. The prefixed string refers to the object that is accessible when the string without the ext:// prefix (i.e., sys.stderr) is processed via the normal importing mechanism. Refer to Access to external objects for more details. Refer to Access to internal objects for details about a similar mechanism based on prefix to refer to objects internal to a configuration.\n• An alert handler that formats ERROR and CRITICAL log requests via the formatter and emails the resulting log entry to the given email addresses. The keys , , , and subject are passed as keyword arguments to ’s constructor.\n\nloggers key maps to a dictionary of strings that serve as logger names and dictionaries that serve as logger configurations. The string value of the (optional) key specifies the logging level of the logger. The boolean value of the (optional) key specifies the propagation setting of the logger. The list of values of the (optional) key specifies the ids of the filters of the logger. Likewise, the list of values of the (optional) key specifies the ids of the handlers of the logger.\n\nThis configuration snippet results in the creation of two loggers. The first logger is named app, its threshold logging level is set to WARNING, and it is configured to forward log requests to and handlers. The second logger is named app.io, and its threshold logging level is set to INFO. Since a log request is propagated to the handlers associated with every ascendant logger, every log request with INFO or a higher logging level made via the app.io logger will be propagated to and handled by both and handlers.\n\nroot key maps to a dictionary of configuration for the root logger. The format of the mapped dictionary is the same as the mapped dictionary for a logger.\n\nincremental key maps to either True or False (default). If True, then only logging levels and propagate options of loggers, handlers, and root loggers are processed, and all other bits of the configuration is ignored. This key is useful to alter existing logging configuration. Refer to Incremental Configuration for more details.\n\ndisable_existing_loggers key maps to either (default) or . If , then all existing non-root loggers are disabled as a result of processing this configuration.\n\nAlso, the argument should map the key to 1.\n\nHere’s the complete configuration composed of the above snippets.\n\nThe configuration schema for filters supports a pattern to specify a function to create a filter. In this pattern, a filter configuration maps the () key to the fully qualified name of a filter creating factory function along with a set of keys and values to be passed as keyword arguments to the factory function. In addition, attributes and values can be added to custom filters by mapping the key to a dictionary of attribute names and values.\n\nFor example, the below configuration will cause the invocation of to create a custom filter and the addition of local attribute with the value to this filter.\n\nConfiguration schemas for formatters, handlers, and loggers also support the above pattern. In the case of handlers/loggers, if this pattern and the key occur in the configuration dictionary, then this pattern is used to create handlers/loggers. Refer to User-defined Objects for more details.\n\nThe logging library also supports loading configuration from a -format file via the < function. Since this is an older API that does not provide all of the functionalities offered by the dictionary-based configuration scheme, the use of the function is recommended; hence, we’re not discussing the function and the file format in this tutorial.\n\nWhile the above APIs can be used to update the logging configuration when the client is running (e.g., web services), programming such update mechanisms from scratch can be cumbersome. The function alleviates this issue. This function starts a socket server that accepts new configurations over the wire and loads them via or functions. Refer to for more details.\n\nSince the configuration provided to is nothing but a collection of nested dictionaries, a logging configuration can be easily represented in JSON and YAML format. Consequently, programs can use the module in Python’s standard library or external YAML processing libraries to read and write logging configurations from files.\n\nFor example, the following snippet suffices to load the logging configuration stored in JSON format.\n\nIn the supported configuration scheme, we cannot configure filters to filter beyond simple name-based filtering. For example, we cannot create a filter that admits only log requests created between 6 PM and 6 AM. We need to program such filters in Python and add them to loggers and handlers via factory functions or the method.\n\nWhile logging statements help capture information at locations in a program, they contribute to the cost of the program in terms of execution time (logging statements in loops) and storage (logging lots of data). Although cost-free yet useful logging is impossible, we can reduce the cost of logging by making choices that are informed by performance considerations.\n\nAfter adding logging statements to a program, we can use the support to configure logging (described earlier) to control the execution of logging statements and the associated execution time. In particular, consider the following configuration capabilities when making decisions about logging-related performance.\n• Change logging levels of loggers: This change helps suppress log messages below a certain log level. This helps reduce the execution cost associated with unnecessary creation of log records.\n• Change handlers: This change helps replace slower handlers with faster handlers (e.g., during testing, use a transient handler instead of a persistent handler) and even remove context-irrelevant handlers. This reduces the execution cost associated with unnecessary handling of log records.\n• Change format: This change helps exclude unnecessary parts of a log record from the log (e.g., exclude IP addresses when executing in a single node setting). This reduces the execution cost associated with unnecessary handling of parts of log records.\n\nThe above changes the range over coarser to finer aspects of logging support in Python.\n\nWhile the support to configure logging is powerful, it cannot help control the performance impact of implementation choices baked into the source code. Here are a few such logging-related implementation choices and the reasons why you should consider them when making decisions about logging-related performance.\n\nUpon adding the logging module to Python’s standard library, there were concerns about the execution cost associated with inactive logging statements — logging statements that issue log requests with logging level lower than the threshold logging level of the target logger. For example, how much extra time will a logging statement that invokes add to a program’s execution time when the threshold logging level of logger is ? This concern led to client-side coding patterns (as shown below) that used the threshold logging level of the target logger to control the execution of the logging statement.\n\nToday, this concern is not valid because the logging methods in the class perform similar checks and process the log requests only if the checks pass. For example, as shown below, the above check is performed in the method.\n\nConsequently, inactive logging statements effectively turn into no-op statements and do not contribute to the execution cost of the program.\n\nEven so, one should consider the following two aspects when adding logging statements.\n• Each invocation of a logging method incurs a small overhead associated with the invocation of the logging method and the check to determine if the logging request should proceed, e.g., a million invocations of when threshold logging level of logger was took half a second on a typical laptop. So, while the cost of an inactive logging statement is trivial, the total execution cost of numerous inactive logging statements can quickly add up to be non-trivial.\n• While disabling a logging statement inhibits the processing of log requests, it does not inhibit the calculation/creation of arguments to the logging statement. So, if such calculations/creations are expensive, then they can contribute non-trivially to the execution cost of the program even when the corresponding logging statement is inactive.\n\nClients can construct log messages in two ways: eagerly and lazily.\n• The client constructs the log message and passes it on to the method, e.g., .\n\nThis approach offers formatting flexibility via and the method, but it involves the eager construction of log messages, i.e., before the logging statements are deemed as active.\n• The client provides a -style message format string (as a argument) and the values (as a argument) to construct the log message to the logging method, e.g., . After the logging statement is deemed as active, the logger constructs the log message using the string formatting operator .\n\nThis approach relies on an older and quirky string formatting feature of Python but it involves the lazy construction of log messages.\n\nWhile both approaches result in the same outcome, they exhibit different performance characteristics due to the eagerness and laziness of message construction.\n\nFor example, on a typical laptop, a million inactive invocations of takes 2197ms while a million inactive invocations of takes 1111ms when is a list of four integers. In the case of a million active invocations, the first approach takes 11061ms and the second approach took 10149ms. A savings of 9–50% of the time taken for logging!\n\nSo, the second (lazy) approach is more performant than the first (eager) approach in cases of both inactive and active logging statements. Further, the gains would be larger when the message construction is non-trivial, e.g., use of many arguments, conversion of complex arguments to strings.\n\nBy default, when a log record is created, the following data is captured in the log record:\n• Identifier and name of the current thread.\n• Name of the current process in the multiprocessing framework.\n• Filename, line number, function name, and call stack info of the logging statement.\n\nUnless these bits of data are logged, gathering them unnecessarily increases the execution cost. So, if these bits of data will not be logged, then configure the logging framework to not gather them by setting the following flags.\n\nDo not block the main thread of execution\n\nThere are situations where we may want to log data in the main thread of execution without spending almost any time logging the data. Such situations are common in web services, e.g., a request processing thread needs to log incoming web requests without significantly increasing its response time. We can tackle these situations by separating concerns across threads: a client/main thread creates a log record while a logging thread logs the record. Since the task of logging is often slower as it involves slower resources (e.g., secondary storage) or other services (e.g., logging services such as Coralogix, pub-sub systems such as Kafka), this separation of concerns helps minimize the effort of logging on the execution time of the main/client thread.\n\nThe Python logging library helps handle such situations via the and classes as follows.\n• A pair of and instances are initialized with a queue.\n• When the instance receives a log record from the client, it merely places the log request in its queue while executing in the client’s thread. Given the simplicity of the task performed by the , the client thread hardly pauses.\n• When a log record is available in the queue, the listener retrieves the log record and executes the handlers registered with the listener to handle the log record. In terms of execution, the listener and the registered handlers execute in a dedicated thread that is different from the client thread.\n\nNote: While comes with a default threading strategy, developers are not required to use this strategy to use . Instead, developers can use alternative threading strategies that meet their needs.\n\nThat about wraps it up for this Python logging guide. If you’re looking for a log management solution to centralize your Python logs, check out our easy-to-configure Python integration."
    },
    {
        "link": "https://coralogix.com/blog/python-logging-best-practices-tips",
        "document": "Python is a highly skilled language with a large developer community, which is essential in data science, machine learning, embedded applications, and back-end web and cloud applications.\n\nAnd logging is critical to understanding software behavior in Python. Once logs are in place, log monitoring can be utilized to make sense of what is happening in the software. Python includes several logging libraries that create and direct logs to their assigned targets.\n\n\n\nThis article will go over Python logging best practices to help you get the best log monitoring setup for your organization.\n\nLogging in Python, like other programming languages, is implemented to indicate events that have occurred in software. Logs should include descriptive messages and variable data to communicate the state of the software at the time of logging.\n\nThey also communicate the severity of the event using unique log levels. Logs can be generated using the Python standard library.\n\nThe Python standard library provides a logging module to log events from applications and libraries. Once the Python JSON logger is configured, it becomes part of the Python interpreter process that is running the code.\n\nIn other words, Python logging is global. You can also configure the Python logging subsystem using an external configuration file. The specifications for the logging configuration format are found in the Python standard library documentation.\n\nThe logging library is modular and offers four categories of components:\n• Loggers expose the interface used by the application code.\n• Handlers are created by loggers and send log records to the appropriate destination.\n• Filters can determine which log records are output.\n• Formatters specify the layout of the final log record output.\n\nMultiple logger objects are organized into a tree representing various parts of your system and the different third-party libraries you have installed. When you send a message to one of the loggers, the message gets output on that logger’s handlers using a formatter attached to each handler.\n\n\n\nThe message then propagates the logger tree until it hits the root logger or a logger in the tree configured with .propagate=False. This hierarchy allows logs to be captured up the subtree of loggers, and a single handler could catch all logging messages.\n\nThe logging.Logger objects offer the primary interface to the logging library. These objects provide the logging methods to issue log requests along with the methods to query and modify their state. From here on out, we will refer to Logger objects as loggers.\n\nThe factory function logging.getLogger(name) is typically used to create loggers. By using the factory function, clients can rely on the library to manage loggers and access loggers via their names instead of storing and passing references to loggers.\n\nThe name argument in the factory function is typically a dot-separated hierarchical name, i.e. a.b.c. This naming convention enables the library to maintain a hierarchy of loggers. Specifically, when the factory function creates a logger, the library ensures a logger exists for each level of the hierarchy specified by the name, and every logger in the hierarchy is linked to its parent and child loggers.\n\nEach logger has a threshold logging level to determine whether a log request should be processed. A logger processes a log request if the numeric value of the requested logging level is greater than or equal to the severity of the logger’s threshold logging level.\n\n\n\nClients can retrieve and change the threshold logging level of a logger via Logger.getEffectiveLevel() and Logger.setLevel(level) methods, respectively. When the factory function is used to create a logger, the function sets a logger’s threshold logging level to the threshold logging level of its parent logger as determined by its name.\n\nLog levels allow you to define event severity for each log so they are easily analyzed. Python supports predefined values, which can be found by calling logging.getLevelName(). Predefined log levels include CRITICAL, ERROR, WARNING, INFO, and DEBUG from highest to lowest severity. Developers can also maintain a dictionary of log levels by defining custom levels using logging.getLogger().\n\nPython comes with different methods to read events from the software: print() and logging. Both will communicate event data but pass this information to different storage locations using different methods.\n\nThe print function sends data exclusively to the console. This can be convenient for fast testing as a function is developed, but it is not practical for use in functional software. There are two critical reasons to not use print() in software:\n• If your code is used by other tools or scripts, the user will not know the context of the print messages.\n• When running Python software in containers like Docker, the print messages will not be seen since containers cannot access the console.\n\nThe logging library also provides many features contributing to Python logging best practices. These include identifying the line of the file, function, and time of log events, distinguishing log events by their importance, and providing formatting to keep log messages consistent.\n\nHere are a few code snippets to illustrate how to use the Python logging library.\n\nIn snippet 1, a logger is created with a log level of INFO. Any logs that have a severity less than INFO will not print (i.e. DEBUG logs). A new handler is created and assigned to the logger. New handlers can be added to send logging outputs to streams like sys.stdout or any file-like object.\n\n\n\nA formatter is created and added to the handler to transform log messages into placeholder data. In this formatter, the time of the log request (as an epoch timestamp), the logging level, the logger’s name, the module name, and the log message will all print.\n\nIn snippet 2, an info log states the app has started. When the app is started in the folder /home/kali with the logger created in snippet 1, the following log entry will be generated in the std.out stream:\n\nThis snippet logs an informational message every time data is written successfully via write_data. If a write fails, the snippet logs an error message that includes the stack trace in which the exception occurred. The logs here use positional arguments to enhance the value of the logs and provide more contextual information.\n\nWith the logger created using snippet 1, successful execution of write_data would create a log similar to:\n\nIf the execution fails, then the created log will appear like:\n\nFileNotFoundError: [Errno 2] No such file or directory: ‘/tmp1/tmp_data.txt’\n\nAlternatively to positional arguments, the same outputs could be achieved using complete names as in:\n\nEvery logger offers a shorthand method to log requests by level. Each pre-defined log level is available in shorthand; for example, Logger.error(msg, *args, **kwargs).\n\nIn addition to these shorthand methods, loggers also offer a general method to specify the log level in the arguments. This method is useful when using custom logging levels.\n\nAnother useful method is used for logs inside exception handlers. It issues log requests with the logging level ERROR and captures the current exception as part of the log entry.\n\nIn each of the methods above, the msg and args arguments are combined to create log messages captured by log entries. They each support the keyword argument exc_info to add exception information to log entries and stack_info and stacklevel to add call stack information to log entries. Also, they support the keyword argument extra, which is a dictionary, to pass values relevant to filters, handlers, and formatters.\n\nHow to get started with Python logging\n\nTo get the most out of your Python logging, they need to be set up consistently and ready to analyze. When setting up your Python logging, use these best practices below.\n\nThe logging.getLogger() factory function helps the library manage the mapping from logger names to logger instances and maintain a hierarchy of loggers. In turn, this mapping and hierarchy offer the following benefits:\n• Clients can use the factory function to access the same logger in different application parts by merely retrieving the logger by its name.\n• Only a finite number of loggers are created at runtime (under normal circumstances).\n• Log requests can be propagated up the logger hierarchy.\n• When unspecified, the threshold logging level of a logger can be inferred from its ascendants.\n• The configuration of the logging library can be updated at runtime by merely relying on the logger names.\n\n\n\nUse the shorthand logging.<logging level>() method to log at pre-defined logging levels. Besides making the code a bit shorter, the use of these functions helps partition the logging statements into two sets:\n• Those that issue log requests with pre-defined logging levels.\n• Those that issue log requests with custom logging levels.\n\nThe pre-defined logging levels capture almost all logging scenarios that occur. Most developers are universally familiar with these logging levels across different programming languages, making them easy to understand. The use of these values reduces deployment, configuration, and maintenance burdens.\n\nWhile creating loggers, we can create a logger for each class or create a logger for each module. While the first option enables fine-grained configuration, it leads to more loggers in a program, i.e., one per class. In contrast, the second option can help reduce the number of loggers in a program. So, unless such fine-grained configuration is necessary, create module-level loggers.\n\nUse logging.LoggerAdapter() to inject contextual information into log records. The class can also modify the log message and data provided as part of the request. Since the logging library does not manage these adapters, they cannot be accessed with common names. Use them to inject contextual information local to a module or class.\n• Use filters or .setLogRecordFactor() to inject global contextual information\n\nTwo options exist to seamlessly inject global contextual information (common across an app) into log records. The first option is to use the filter support to modify the log record arguments provided to filters. For example, the following filter injects version information into incoming log records.\n\nThere are two downsides to this option. First, if filters depend on the data in log records, then filters that inject data into log records should be executed before filters that use the injected data. Thus, the order of filters added to loggers and handlers becomes crucial. Second, the option “abuses” the support to filter log records to extend log records.\n\nThe second option is to initialize the logging library with a log record creating a factory function via logging.setLogRecordFactory(). Since the injected contextual information is global, it can be injected into log records when created in the factory function. This ensures the data will be available to every filter, formatter, logger, and handler in the program.\n\nThe downside of this option is that we have to ensure factory functions contributed by different components in a program play nicely with each other. While log record factory functions could be chained, such chaining increases the complexity of programs.\n• Use .disable() to inhibit processing of low-level requests\n\nA logger will process a log request based on the effective logging level. The effective logging level is the higher of two logging levels: the logger’s threshold level and the library-wide level. Set the library-wide logging level using the logging.disable(level) function. This is set to 0 by default so that every log request will be processed.\n\nUsing this function, the software will throttle the logging output of an app by increasing the logging level across the whole app. This can be important to keep log volumes in check in production software.\n\nPython’s logging library is more complicated than simple print() statements. The library has many great features that provide a complete solution for obtaining log data needed to achieve full-stack observability in your software.\n\nHere we show the high-level advantages and disadvantages of the library.\n\nThe Python logging library is highly configurable. Logs can be formatted before printing, can have placeholder data filled in automatically, and can be turned on and off as needed. Logs can also be sent to a number of different locations for easier reading and debugging. All of these settings are codified, so are well-defined for each logger.\n\nIn failures, it is useful to log debugging information showing where and when a failure occurred. These tracebacks can be generated automatically in the Python logging library to help speed up troubleshooting and fixes.\n\nLog levels used in different scenarios can be subjective across a development team. For proper analysis, it is important to keep log levels consistent. Create a well-defined strategy for your team about when to use each logging level available and when a custom level is appropriate.\n\nSince the logging module is so flexible, logging configurations can quickly get complicated. Create a strategy for your team for how each logging module will be defined to keep logs consistent across developers.\n\nLet’s look at an example of a basic logger in Python:\n\nLine 2: create a basicConf function and pass some arguments to create the log file. In this case, we indicate the severity level, date format, filename and file mode to have the function overwrite the log file.\n\nLine 3 to 5: messages for each logging level.\n\nThe default format for log records is SEVERITY: LOGGER: MESSAGE. Hence, if you run the code above as is, you’ll get this output:\n\nRegarding the output, you can set the destination of the log messages. As a first step, you can print messages to the screen using this sample code:\n\nIf your goals are aimed at the Cloud, you can take advantage of Python’s set of logging handlers to redirect content. Currently in beta release, you can write logs to Stackdriver Logging from Python applications by using Google’s Python logging handler included with the Stackdriver Logging client library, or by using the client library to access the API directly. When developing your logger, take into account that the root logger doesn’t use your log handler. Since the Python Client for Stackdriver Logging library also does logging, you may get a recursive loop if the root logger uses your Python log handler.\n\nWhen we use a logging library, we perform/trigger the following common tasks while using the associated concepts (highlighted in bold).\n• A client issues a log request by executing a logging statement. Often, such logging statements invoke a function/method in the logging (library) API by providing the log data and the logging level as arguments. The logging level specifies the importance of the log request. Log data is often a log message, which is a string, along with some extra data to be logged. Often, the logging API is exposed via logger objects.\n• To enable the processing of a request as it threads through the logging library, the logging library creates a log record that represents the log request and captures the corresponding log data.\n• Based on how the logging library is configured (via a logging configuration), the logging library filters the log requests/records. This filtering involves comparing the requested logging level to the threshold logging level and passing the log records through user-provided filters.\n• Handlers process the filtered log records to either store the log data (e.g., write the log data into a file) or perform other actions involving the log data (e.g., send an email with the log data). In some logging libraries, before processing log records, a handler may again filter the log records based on the handler’s logging level and user-provided handler-specific filters. Also, when needed, handlers often rely on user-provided formatters to format log records into strings, i.e., log entries.\n\nIndependent of the logging library, the above tasks are performed in an order similar to that shown in Figure 1.\n\nFigure 1: The flow of tasks when logging via a logging library\n\nEvery logger offers the following logging methods to issue log requests.\n\nEach of these methods is a shorthand to issue log requests with corresponding pre-defined logging levels as the requested logging level.\n\nIn addition to the above methods, loggers also offer the following two methods:\n• issues log requests with explicitly specified logging levels. This method is useful when using custom logging levels.\n• issues log requests with the logging level and that capture the current exception as part of the log entries. Consequently, clients should invoke this method only from an exception handler.\n\nand arguments in the above methods are combined to create log messages captured by log entries. All of the above methods support the keyword argument to add exception information to log entries and and to add call stack information to log entries. Also, they support the keyword argument , which is a dictionary, to pass values relevant to filters, handlers, and formatters.\n\nWhen executed, the above methods perform/trigger all of the tasks shown in Figure 1 and the following two tasks:\n• After deciding to process a log request based on its logging level and the threshold logging level, the logger creates a object to represent the log request in the downstream processing of the request. objects capture the and arguments of logging methods and the exception and call stack information along with source code information. They also capture the keys and values in the extra argument of the logging method as fields.\n• After every handler of a logger has processed a log request, the handlers of its ancestor loggers process the request (in the order they are encountered walking up the logger hierarchy). The field controls this aspect, which is by default.\n\nBeyond logging levels, filters provide a finer means to filter log requests based on the information in a log record, e.g., ignore log requests issued in a specific class. Clients can add and remove filters to/from loggers using and methods, respectively.\n\nThe logging classes introduced in the previous section provide methods to configure their instances and, consequently, customize the use of the logging library. Snippet 1 demonstrates how to use configuration methods. These methods are best used in simple single-file programs.\n\nWhen involved programs (e.g., apps, libraries) use the logging library, a better option is to externalize the configuration of the logging library. Such externalization allows users to customize certain facets of logging in a program (e.g., specify the location of log files, use custom loggers/handlers/formatters/filters) and, hence, ease the deployment and use of the program. We refer to this approach to configuration as data-based approach.\n\nClients can configure the logging library by invoking function. The argument is a dictionary and the following optional keys can be used to specify a configuration.\n\nfilters key maps to a dictionary of strings and dictionaries. The strings serve as filter ids used to refer to filters in the configuration (e.g., adding a filter to a logger) while the mapped dictionaries serve as filter configurations. The string value of the name key in filter configurations is used to construct .\n\nThis configuration snippet results in the creation of a filter that admits all records created by the logger named ‘app.io’ or its descendants.\n\nformatters key maps to a dictionary of strings and dictionaries. The strings serve as formatter ids used to refer to formatters in the configuration (e.g., adding a formatter to a handler) while the mapped dictionaries serve as formatter configurations. The string values of the datefmt and format keys in formatter configurations are used as the date and log entry formatting strings, respectively, to construct instances. The boolean value of the (optional) validate key controls the validation of the format strings during the construction of a formatter.\n\nThis configuration snippet results in the creation of two formatters. A simple formatter with the specified log entry and date formatting strings and detailed formatter with specified log entry formatting string and default date formatting string.\n\nhandlers key maps to a dictionary of strings and dictionaries. The strings serve as handler ids used to refer to handlers in the configuration (e.g., adding a handler to a logger) while the mapped dictionaries serve as handler configurations. The string value of the class key in a handler configuration names the class to instantiate to construct a handler. The string value of the (optional) level key specifies the logging level of the instantiated handler. The string value of the (optional) formatter key specifies the id of the formatter of the handler. Likewise, the list of values of the (optional) filters key specifies the ids of the filters of the handler. The remaining keys are passed as keyword arguments to the handler’s constructor.\n\nThis configuration snippet results in the creation of two handlers:\n• A handler that formats log requests with INFO and higher logging level log via the formatter and emits the resulting log entry into the standard error stream. The key is passed as keyword arguments to constructor.\n\nThe value of the key illustrates how to access objects external to the configuration. The prefixed string refers to the object that is accessible when the string without the ext:// prefix (i.e., sys.stderr) is processed via the normal importing mechanism. Refer to Access to external objects for more details. Refer to Access to internal objects for details about a similar mechanism based on prefix to refer to objects internal to a configuration.\n• An alert handler that formats ERROR and CRITICAL log requests via the formatter and emails the resulting log entry to the given email addresses. The keys , , , and subject are passed as keyword arguments to ’s constructor.\n\nloggers key maps to a dictionary of strings that serve as logger names and dictionaries that serve as logger configurations. The string value of the (optional) key specifies the logging level of the logger. The boolean value of the (optional) key specifies the propagation setting of the logger. The list of values of the (optional) key specifies the ids of the filters of the logger. Likewise, the list of values of the (optional) key specifies the ids of the handlers of the logger.\n\nThis configuration snippet results in the creation of two loggers. The first logger is named app, its threshold logging level is set to WARNING, and it is configured to forward log requests to and handlers. The second logger is named app.io, and its threshold logging level is set to INFO. Since a log request is propagated to the handlers associated with every ascendant logger, every log request with INFO or a higher logging level made via the app.io logger will be propagated to and handled by both and handlers.\n\nroot key maps to a dictionary of configuration for the root logger. The format of the mapped dictionary is the same as the mapped dictionary for a logger.\n\nincremental key maps to either True or False (default). If True, then only logging levels and propagate options of loggers, handlers, and root loggers are processed, and all other bits of the configuration is ignored. This key is useful to alter existing logging configuration. Refer to Incremental Configuration for more details.\n\ndisable_existing_loggers key maps to either (default) or . If , then all existing non-root loggers are disabled as a result of processing this configuration.\n\nAlso, the argument should map the key to 1.\n\nHere’s the complete configuration composed of the above snippets.\n\nThe configuration schema for filters supports a pattern to specify a function to create a filter. In this pattern, a filter configuration maps the () key to the fully qualified name of a filter creating factory function along with a set of keys and values to be passed as keyword arguments to the factory function. In addition, attributes and values can be added to custom filters by mapping the key to a dictionary of attribute names and values.\n\nFor example, the below configuration will cause the invocation of to create a custom filter and the addition of local attribute with the value to this filter.\n\nConfiguration schemas for formatters, handlers, and loggers also support the above pattern. In the case of handlers/loggers, if this pattern and the key occur in the configuration dictionary, then this pattern is used to create handlers/loggers. Refer to User-defined Objects for more details.\n\nThe logging library also supports loading configuration from a -format file via the < function. Since this is an older API that does not provide all of the functionalities offered by the dictionary-based configuration scheme, the use of the function is recommended; hence, we’re not discussing the function and the file format in this tutorial.\n\nWhile the above APIs can be used to update the logging configuration when the client is running (e.g., web services), programming such update mechanisms from scratch can be cumbersome. The function alleviates this issue. This function starts a socket server that accepts new configurations over the wire and loads them via or functions. Refer to for more details.\n\nSince the configuration provided to is nothing but a collection of nested dictionaries, a logging configuration can be easily represented in JSON and YAML format. Consequently, programs can use the module in Python’s standard library or external YAML processing libraries to read and write logging configurations from files.\n\nFor example, the following snippet suffices to load the logging configuration stored in JSON format.\n\nIn the supported configuration scheme, we cannot configure filters to filter beyond simple name-based filtering. For example, we cannot create a filter that admits only log requests created between 6 PM and 6 AM. We need to program such filters in Python and add them to loggers and handlers via factory functions or the method.\n\nWhile logging statements help capture information at locations in a program, they contribute to the cost of the program in terms of execution time (logging statements in loops) and storage (logging lots of data). Although cost-free yet useful logging is impossible, we can reduce the cost of logging by making choices that are informed by performance considerations.\n\nAfter adding logging statements to a program, we can use the support to configure logging (described earlier) to control the execution of logging statements and the associated execution time. In particular, consider the following configuration capabilities when making decisions about logging-related performance.\n• Change logging levels of loggers: This change helps suppress log messages below a certain log level. This helps reduce the execution cost associated with unnecessary creation of log records.\n• Change handlers: This change helps replace slower handlers with faster handlers (e.g., during testing, use a transient handler instead of a persistent handler) and even remove context-irrelevant handlers. This reduces the execution cost associated with unnecessary handling of log records.\n• Change format: This change helps exclude unnecessary parts of a log record from the log (e.g., exclude IP addresses when executing in a single node setting). This reduces the execution cost associated with unnecessary handling of parts of log records.\n\nThe above changes the range over coarser to finer aspects of logging support in Python.\n\nWhile the support to configure logging is powerful, it cannot help control the performance impact of implementation choices baked into the source code. Here are a few such logging-related implementation choices and the reasons why you should consider them when making decisions about logging-related performance.\n\nUpon adding the logging module to Python’s standard library, there were concerns about the execution cost associated with inactive logging statements — logging statements that issue log requests with logging level lower than the threshold logging level of the target logger. For example, how much extra time will a logging statement that invokes add to a program’s execution time when the threshold logging level of logger is ? This concern led to client-side coding patterns (as shown below) that used the threshold logging level of the target logger to control the execution of the logging statement.\n\nToday, this concern is not valid because the logging methods in the class perform similar checks and process the log requests only if the checks pass. For example, as shown below, the above check is performed in the method.\n\nConsequently, inactive logging statements effectively turn into no-op statements and do not contribute to the execution cost of the program.\n\nEven so, one should consider the following two aspects when adding logging statements.\n• Each invocation of a logging method incurs a small overhead associated with the invocation of the logging method and the check to determine if the logging request should proceed, e.g., a million invocations of when threshold logging level of logger was took half a second on a typical laptop. So, while the cost of an inactive logging statement is trivial, the total execution cost of numerous inactive logging statements can quickly add up to be non-trivial.\n• While disabling a logging statement inhibits the processing of log requests, it does not inhibit the calculation/creation of arguments to the logging statement. So, if such calculations/creations are expensive, then they can contribute non-trivially to the execution cost of the program even when the corresponding logging statement is inactive.\n\nClients can construct log messages in two ways: eagerly and lazily.\n• The client constructs the log message and passes it on to the method, e.g., .\n\nThis approach offers formatting flexibility via and the method, but it involves the eager construction of log messages, i.e., before the logging statements are deemed as active.\n• The client provides a -style message format string (as a argument) and the values (as a argument) to construct the log message to the logging method, e.g., . After the logging statement is deemed as active, the logger constructs the log message using the string formatting operator .\n\nThis approach relies on an older and quirky string formatting feature of Python but it involves the lazy construction of log messages.\n\nWhile both approaches result in the same outcome, they exhibit different performance characteristics due to the eagerness and laziness of message construction.\n\nFor example, on a typical laptop, a million inactive invocations of takes 2197ms while a million inactive invocations of takes 1111ms when is a list of four integers. In the case of a million active invocations, the first approach takes 11061ms and the second approach took 10149ms. A savings of 9–50% of the time taken for logging!\n\nSo, the second (lazy) approach is more performant than the first (eager) approach in cases of both inactive and active logging statements. Further, the gains would be larger when the message construction is non-trivial, e.g., use of many arguments, conversion of complex arguments to strings.\n\nBy default, when a log record is created, the following data is captured in the log record:\n• Identifier and name of the current thread.\n• Name of the current process in the multiprocessing framework.\n• Filename, line number, function name, and call stack info of the logging statement.\n\nUnless these bits of data are logged, gathering them unnecessarily increases the execution cost. So, if these bits of data will not be logged, then configure the logging framework to not gather them by setting the following flags.\n\nDo not block the main thread of execution\n\nThere are situations where we may want to log data in the main thread of execution without spending almost any time logging the data. Such situations are common in web services, e.g., a request processing thread needs to log incoming web requests without significantly increasing its response time. We can tackle these situations by separating concerns across threads: a client/main thread creates a log record while a logging thread logs the record. Since the task of logging is often slower as it involves slower resources (e.g., secondary storage) or other services (e.g., logging services such as Coralogix, pub-sub systems such as Kafka), this separation of concerns helps minimize the effort of logging on the execution time of the main/client thread.\n\nThe Python logging library helps handle such situations via the and classes as follows.\n• A pair of and instances are initialized with a queue.\n• When the instance receives a log record from the client, it merely places the log request in its queue while executing in the client’s thread. Given the simplicity of the task performed by the , the client thread hardly pauses.\n• When a log record is available in the queue, the listener retrieves the log record and executes the handlers registered with the listener to handle the log record. In terms of execution, the listener and the registered handlers execute in a dedicated thread that is different from the client thread.\n\nNote: While comes with a default threading strategy, developers are not required to use this strategy to use . Instead, developers can use alternative threading strategies that meet their needs.\n\nThat about wraps it up for this Python logging guide. If you’re looking for a log management solution to centralize your Python logs, check out our easy-to-configure Python integration."
    },
    {
        "link": "https://signoz.io/guides/python-logging-best-practices",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/25187083/python-logging-to-multiple-handlers-at-different-log-levels",
        "document": "I'm scratching my head on a python logging config that I can't get right.\n\nLet's say I have the following package installed:\n\nAs the script can be used interactively or run from a crontab, I have the following requirements:\n• None no print statements, everything goes through logging;\n• None log using a , always at DEBUG level;\n• None log using a , always at INFO level;\n• None log to console, with level set by default to INFO or overridden through a command-line option.\n\nProblem is, I can change the log level through the command-line and the console log level is changed accordingly, but the other handlers are also changed, which I don't want... :-/\n\nIn a logging config file, I'm not sure I understand the precedence between the root logger's level, other loggers' level and handlers' level settings.\n\nHere is some sample code. Any clues will be appreciated :-)\n\nIn the meantime, I discovered this answer and successfully modified my code this way:\n\nNow, I'll try to translate this in a logging.config file...\n\nHere is the best logging config and code combination I found.\n\nIn the mypackage.logging.conf file, the \"mypackage\" logger is:\n• set up only with the file and email handlers;\n• its propagate is set to false;\n• its level is set to DEBUG;\n• while the file and email handlers are respectively set to INFO and DEBUG.\n• None a console handler is created with the log level from the command-line option, then added to the \"mypackage\" logger.\n\nThings would be simpler if the handlers were \"addressable\" by name when loaded from a config file.\n\nThen, we could have the mypackage console handler set up in the config file and its log level changed in the code like this:\n\nThere would no need to re-create a formatter either..."
    },
    {
        "link": "https://stackoverflow.com/questions/61404432/python-logger-multiple-logger-instances-with-multiple-levels-best-practice",
        "document": "I have the following requirements:\n• To have one global logger which you can configure (setup level, additional handlers,..)\n• To have per module logger which you can configure (setup level, additional handlers,..)\n\nIn other words we need more logs with different configuration\n\nTherefore I did the following\n\nThen I create logger as following:\n\nBefore I will try what works and what not and test it more deeply I want to ask you if this is good practice to do it or do you have any other recommendation how to achieve our requrements?\n\nThanks for help"
    },
    {
        "link": "https://last9.io/blog/python-logging-best-practices",
        "document": "Logging is an essential part of Python development. It helps developers track application behavior and troubleshoot issues. This guide covers key logging practices to improve your code's observability and make debugging easier.\n\nWe'll explore setting up logging, common pitfalls to avoid, and advanced techniques for handling logs in larger projects. Whether building a small script or a complex application, you'll find practical tips to enhance your logging approach.\n\nPython logging is like the Swiss Army knife of software development. It's a powerful feature in the Python standard library that allows you to track events, debug issues, and monitor the health of your applications.\n\nThink of it as your application's diary, but instead of teenage angst, it's full of valuable insights that can save you countless hours of head-scratching and keyboard-smashing.\n\nBut why is logging so crucial?\n\nImagine trying to solve a murder mystery without any clues - that's what debugging or troubleshooting without proper logging feels like.\n• Help you grasp your application's flow\n• Notify you of potential issues before they escalate\n\nThe logging module is the heart of Python's logging system. It's like the command center for all your logging operations, providing a flexible framework for generating log messages from your Python programs.\n\nLet's understand how to use it effectively.\n\nIn this example, we're using basicConfig() to set up a basic configuration for our logging. We're setting the default level to INFO and specifying a format for our log messages. Then, we're creating a logger and using it to log messages at different levels.\n\nLoggers are the storytellers of your application. They're responsible for capturing events and routing them to the appropriate handlers. Think of them as the journalists of your code, always ready to report on what's happening.\n\nUsing the name as the logger name is a common practice. It allows you to have a unique logger for each module in your application, which can be incredibly helpful when trying to track down issues in a large codebase.\n\nThe threshold level determines which messages get recorded. It's like a bouncer for your logs, deciding which messages are important enough to make it into the VIP section (your log files or console).\n\nThis sets the logger to capture all messages at the DEBUG level and above. Any messages below this level (if there were any) would be ignored.\n\nPython provides several log levels, each serving a different purpose:\n• INFO: Confirmation that the application is functioning as expected.\n• WARNING: A sign that something unexpected occurred or a potential problem might arise soon.\n• ERROR: Indicates a significant issue that has prevented certain functions from executing.\n• CRITICAL: A severe error suggesting that the program may be unable to continue running.\n\nHere's how you might use these in practice:\n\nIn the above example, we're using different log levels to provide context about what's happening in our function.\n\nThe DEBUG message gives us detailed information about the operation, the ERROR message alerts us to a serious problem, the INFO message confirms that the operation was successful, and the WARNING message indicates that something went wrong, but it's not necessarily a critical error.\n\nWhile print() statements might seem tempting for quick debugging, logging offers several advantages:\n• Granular control over output: Easily adjust the verbosity of your output without altering your code.\n• Easy to disable or redirect output: Disable logging or redirect it to a file with ease, no code changes are required.\n• Includes contextual information: Automatically includes details like timestamps, line numbers, and function names in your logs.\n• Thread-safe: Unlike print statements, logging is thread-safe, making it essential for multi-threaded applications.\n\nThe logging version gives us more flexibility and provides more context out of the box.\n\nLet's look at some more advanced examples to really get our logging muscles flexing.\n\nThis setup gives us a logger that outputs to the console with a specific format.\n\nThis setup will write all our log messages to a file named 'spam.log'.\n\nThis example shows how you can set up logging within a class, which can be very useful for object-oriented programming.\n\nPython's logging module offers various methods to suit different needs:\n• Logger objects: Create and configure instances for more control.\n• Formatter objects: Customize the format of log messages with objects.\n\nLet's look at an example that combines these:\n\nThis setup uses a RotatingFileHandler to manage log files (creating new files when the current one gets too large) and a StreamHandler for console output. It also uses different log levels for file and console output.\n\nHow to get started with Python logging\n• Configure handlers and formatters (as shown in previous examples)\n\nRemember, the key to effective logging is consistency. Establish logging conventions for your project and stick to them.\n• Flexible and Customizable: Tailor logging to fit your needs, from log levels to formats.\n• Built-in to Python: No need for external libraries or dependencies.\n• Supports Multiple Output Destinations: Log events can be directed to consoles, files, networks, and more.\n• Hierarchical: Organize loggers in a hierarchy for precise control over log events.\n• Complex Setup for Advanced Use Cases: The extensive configuration options can lead to complexity.\n• Potential Performance Impact: Improper configuration or excessive logging can slow down your application.\n• Overwhelming Volume: Logging too much information can make it challenging to pinpoint relevant log events.\n\nWhile Python's built-in logging is powerful, sometimes you need more. Here are some popular logging platforms:\n• Splunk: An advanced platform for searching, monitoring, and analyzing machine-generated data.\n\nThese platforms can help you aggregate logs from multiple sources, perform advanced searches, and create visualizations.\n• Loggers: The entry point for logging operations. They expose the interface that the application code directly uses.\n• Handlers: Determine where log messages go (console, file, email, etc.).\n• Formatters: Define the structure and content of log messages.\n• Filters: Provide fine-grained control over which log records to output.\n\nHere's how these concepts work together:\n\nCustom Filter Example:\n\nIn this example, we've created a custom filter that only allows messages containing the word 'important'. This demonstrates how you can use filters to have fine-grained control over your logs.\n\nThis configuration sets up logging to both a file and the console, with different formats and levels for each.\n\nThis approach allows you to easily create consistent loggers throughout your application.\n\nThis approach allows you to keep your logging configuration separate from your code, making it easier to modify logging behavior without changing your application code.\n\nWhile logging is crucial for understanding and debugging your application, it's important to implement it in a way that doesn't significantly impact your application's performance. Here are some tips to keep your logging efficient:\n• Use appropriate log levels: In production, you probably don't need DEBUG level logs. Set the log level appropriately to reduce the amount of log data generated.\n• Implement log rotation: Use or to manage log file sizes and prevent them from consuming too much disk space.\n• Consider using asynchronous logging for high-volume scenarios: Libraries like can help manage logging in high-throughput applications.\n• Do not execute inactive logging statements: Use lazy logging to avoid unnecessary performance hits.\n\nThis way, expensive function calls are only made if the debug level is actually enabled.\n• Use formatting or instead of f-strings for log messages: While f-strings are convenient, they are always evaluated, even if the log level is not enabled.\n• Batch logging calls in critical paths: If you have a loop that's logging on each iteration, consider batching the log calls.\n• Use sampling for high-volume logs: In some cases, you might want to log only a sample of events to reduce volume.\n\nThis will log approximately 10% of the items processed.\n• Contextual logging with the parameter: Add extra contextual information to your logs.\n\nThis includes the user ID in each log message, making it easier to trace actions related to specific users.\n• Using for adding context: For more complex scenarios, use to consistently add context to log messages.\n\nThis prefixes each log message with the IP address.\n\nThe method automatically includes the stack trace in the log message.\n• Be consistent: Use the same logging format and conventions throughout your application.\n• Log at the appropriate level: Use DEBUG for detailed information, INFO for general information, WARNING for unexpected events, ERROR for serious problems, and CRITICAL for fatal errors.\n• Include context: Log relevant details that will help you understand the state of your application when the log was created.\n• Use structured logging: Consider using JSON or another structured format for your logs to make them easier to parse and analyze.\n• Don't log sensitive information: Be careful not to log passwords, API keys, or other sensitive data.\n• Configure logging as early as possible: Set up your logging configuration at the start of your application to ensure all modules use the same configuration.\n• Use logger hierarchies: Take advantage of Python's logger hierarchy to control logging behavior across your application.\n• Review and rotate logs: Regularly review your logs and implement log rotation to manage log file sizes.\n• Use unique identifiers: Include unique identifiers (like request IDs) in your logs to help trace requests across multiple services.\n• Test your logging: Make sure your logging works as expected, especially in error scenarios.\n\nLogging is an essential part of any robust Python application. When done right, it can be your best friend in understanding your application's behavior, debugging issues, and maintaining your sanity as a developer.\n\nRemember, the goal of logging is to provide visibility into your application's operations. Good logging practices can save you hours of debugging time and help you catch issues before they become critical problems.\n\nAs you continue your Python journey, keep refining your logging practices. Experiment with different configurations, try out various logging platforms and always be on the lookout for ways to make your logs more informative and efficient.\n\nHappy logging! May your logs be informative, your debug sessions be short, and your production deploys be uneventful. Now go forth and log like a pro!"
    }
]