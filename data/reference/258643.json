[
    {
        "link": "https://registry.khronos.org/OpenGL/specs/es/3.2/es_spec_3.2.pdf",
        "document": ""
    },
    {
        "link": "https://khronos.org/developers/linkto/opengl-es-3.2-reference-guide",
        "document": ""
    },
    {
        "link": "https://developer.android.com/develop/ui/views/graphics/opengl/about-opengl",
        "document": "Android includes support for high performance 2D and 3D graphics with the Open Graphics Library (OpenGL®), specifically, the OpenGL ES API. OpenGL is a cross-platform graphics API that specifies a standard software interface for 3D graphics processing hardware. OpenGL ES is a flavor of the OpenGL specification intended for embedded devices. Android supports several versions of the OpenGL ES API:\n• OpenGL ES 2.0 - This API specification is supported by Android 2.2 (API level 8) and higher.\n• OpenGL ES 3.0 - This API specification is supported by Android 4.3 (API level 18) and higher.\n• OpenGL ES 3.1 - This API specification is supported by Android 5.0 (API level 21) and higher.\n• OpenGL ES 3.2 - This API specification is supported by Android 7.0 (API level 24) and higher.\n\nCaution: Regardless of the Android platform version, a device cannot support the OpenGL ES 3.0 API unless the device manufacturer provides an implementation of this graphics pipeline. If you specify in the manifest that OpenGL ES 3.0 is required, you can be sure that that version will be present on the device. If you specify that a lower-level version is required but you want to use 3.0 features if they're available, you should check at run time to see what version of OpenGL the device supports. For information on how to do this, see Checking OpenGL ES version.\n\nNote: Android includes support for OpenGL ES 1.0 and 1.1, but these versions of the API are deprecated and shouldn't be used by modern applications.\n\nNote: The specific API provided by the Android framework is similar to the J2ME JSR239 OpenGL ES API, but isn't identical. If you are familiar with J2ME JSR239 specification, be alert for variations.\n\nAndroid supports OpenGL both through its framework API and the Native Development Kit (NDK). This topic focuses on the Android framework interfaces. For more information about the NDK, see the Android NDK.\n\nThere are two foundational classes in the Android framework that let you create and manipulate graphics with the OpenGL ES API: and . If your goal is to use OpenGL in your Android application, understanding how to implement these classes in an activity should be your first objective.\n\nOnce you have established a container view for OpenGL ES using and , you can begin calling OpenGL APIs using the following classes:\n• OpenGL ES 2.0 API Class\n• - This package provides the interface to OpenGL ES 2.0 and is available starting with Android 2.2 (API level 8).\n• OpenGL ES 3.0/3.1/3.2 API Packages\n• - This package provides the interface to the OpenGL ES 3.0/3.1 classes. Version 3.0 is available starting with Android 4.3 (API level 18). Version 3.1 is available starting with Android 5.0 (API level 21). Version 3.2 is available starting with Android 7.0 (API level 24).\n\nIf you want to start building an app with OpenGL ES right away, follow the Displaying graphics with OpenGL ES class.\n\nIf your application uses OpenGL features that are not available on all devices, you must include these requirements in your AndroidManifest.xml file. Here are the most common OpenGL manifest declarations:\n• OpenGL ES version requirements - If your application requires a specific version of OpenGL ES, you must declare that requirement by adding the following settings to your manifest as shown below. <!-- Tell the system this app requires OpenGL ES 2.0. --> <uses-feature android:glEsVersion=\"0x00020000\" android:required=\"true\" /> Adding this declaration causes Google Play to restrict your application from being installed on devices that do not support OpenGL ES 2.0. If your application is exclusively for devices that support OpenGL ES 3.0, you can also specify this in your manifest: <!-- Tell the system this app requires OpenGL ES 3.0. --> <uses-feature android:glEsVersion=\"0x00030000\" android:required=\"true\" /> <!-- Tell the system this app requires OpenGL ES 3.1. --> <uses-feature android:glEsVersion=\"0x00030001\" android:required=\"true\" /> <!-- Tell the system this app requires OpenGL ES 3.2. --> <uses-feature android:glEsVersion=\"0x00030002\" android:required=\"true\" /> Note: The OpenGL ES 3.x API is backwards-compatible with the 2.0 API, which means you can be more flexible with your implementation of OpenGL ES in your application. By declaring the OpenGL ES 2.0 API as a requirement in your manifest, you can use that API version as a default, check for the availability of the 3.x API at run time and then use OpenGL ES 3.x features if the device supports it. For more information about checking the OpenGL ES version supported by a device, see Checking OpenGL ES version.\n• Texture compression requirements - If your application uses texture compression formats, you must declare the formats your application supports in your manifest file using . For more information about available texture compression formats, see Texture compression support. Declaring texture compression requirements in your manifest hides your application from users with devices that do not support at least one of your declared compression types. For more information on how Google Play filtering works for texture compressions, see the Google Play and texture compression filtering section of the documentation.\n\nOne of the basic problems in displaying graphics on Android devices is that their screens can vary in size and shape. OpenGL assumes a square, uniform coordinate system and, by default, happily draws those coordinates onto your typically non-square screen as if it is perfectly square.\n\nThe illustration above shows the uniform coordinate system assumed for an OpenGL frame on the left, and how these coordinates actually map to a typical device screen in landscape orientation on the right. To solve this problem, you can apply OpenGL projection modes and camera views to transform coordinates so your graphic objects have the correct proportions on any display.\n\nIn order to apply projection and camera views, you create a projection matrix and a camera view matrix and apply them to the OpenGL rendering pipeline. The projection matrix recalculates the coordinates of your graphics so that they map correctly to Android device screens. The camera view matrix creates a transformation that renders objects from a specific eye position.\n\nProjection and camera view in OpenGL ES 2.0 and higher\n\nIn the ES 2.0 and 3.0 APIs, you apply projection and camera view by first adding a matrix member to the vertex shaders of your graphics objects. With this matrix member added, you can then generate and apply projection and camera viewing matrices to your objects.\n• Add matrix to vertex shaders - Create a variable for the view projection matrix and include it as a multiplier of the shader's position. In the following example vertex shader code, the included member allows you to apply projection and camera viewing matrices to the coordinates of objects that use this shader. // This matrix member variable provides a hook to manipulate // the coordinates of objects that use this vertex shader. // The matrix must be included as part of gl_Position // Note that the uMVPMatrix factor *must be first* in order // for the matrix multiplication product to be correct. // This matrix member variable provides a hook to manipulate // the coordinates of objects that use this vertex shader. // The matrix must be included as part of gl_Position // Note that the uMVPMatrix factor *must be first* in order // for the matrix multiplication product to be correct. Note: The example above defines a single transformation matrix member in the vertex shader into which you apply a combined projection matrix and camera view matrix. Depending on your application requirements, you may want to define separate projection matrix and camera viewing matrix members in your vertex shaders so you can change them independently.\n• Access the shader matrix - After creating a hook in your vertex shaders to apply projection and camera view, you can then access that variable to apply projection and camera viewing matrices. The following code shows how to modify the method of a implementation to access the matrix variable defined in the vertex shader above.\n• Create projection and camera viewing matrices - Generate the projection and viewing matrices to be applied the graphic objects. The following example code shows how to modify the and methods of a implementation to create camera view matrix and a projection matrix based on the screen aspect ratio of the device.\n• Apply projection and camera viewing matrices - To apply the projection and camera view transformations, multiply the matrices together and then set them into the vertex shader. The following example code shows how modify the method of a implementation to combine the projection matrix and camera view created in the code above and then apply it to the graphic objects to be rendered by OpenGL.\n\nFor a complete example of how to apply projection and camera view with OpenGL ES 2.0, see the Displaying graphics with OpenGL ES class.\n\nIn OpenGL, the face of a shape is a surface defined by three or more points in three-dimensional space. A set of three or more three-dimensional points (called vertices in OpenGL) have a front face and a back face. How do you know which face is front and which is the back? Good question. The answer has to do with winding, or, the direction in which you define the points of a shape.\n\nFigure 1. Illustration of a coordinate list which translates into a counterclockwise drawing order.\n\nIn this example, the points of the triangle are defined in an order such that they are drawn in a counterclockwise direction. The order in which these coordinates are drawn defines the winding direction for the shape. By default, in OpenGL, the face which is drawn counterclockwise is the front face. The triangle shown in Figure 1 is defined so that you are looking at the front face of the shape (as interpreted by OpenGL) and the other side is the back face.\n\nWhy is it important to know which face of a shape is the front face? The answer has to do with a commonly used feature of OpenGL, called face culling. Face culling is an option for the OpenGL environment which allows the rendering pipeline to ignore (not calculate or draw) the back face of a shape, saving time, memory and processing cycles:\n\nIf you try to use the face culling feature without knowing which sides of your shapes are the front and back, your OpenGL graphics are going to look a bit thin, or possibly not show up at all. So, always define the coordinates of your OpenGL shapes in a counterclockwise drawing order.\n\nNote: It is possible to set an OpenGL environment to treat the clockwise face as the front face, but doing so requires more code and is likely to confuse experienced OpenGL developers when you ask them for help. So don’t do that.\n\nThe OpenGL ES 1.0 and 1.1 API specifications have been supported since Android 1.0. Graphics programming with OpenGL ES 1.0/1.1 API is significantly different than using the 2.0 and higher versions. OpenGL ES 2.0 is supported by all Android devices beginning with Android 2.2 (API level 8) and is the earliest version recommended for new applications being developed with OpenGL ES. OpenGL ES 3.0 is supported with Android 4.3 (API level 18) and higher, on devices that provide an implementation of the OpenGL ES 3.0 API. For information about the relative number of Android-powered devices that support a given version of OpenGL ES, see the OpenGL ES version dashboard.\n\nYou should carefully consider the graphics requirements and choose the API version that works best for your application. For more information, see Choosing an OpenGL API version.\n\nThe OpenGL ES 3.0 API provides additional features and better performance than the 2.0 API and is also backward compatible. This means that you can potentially write your application targeting OpenGL ES 2.0 and conditionally include OpenGL ES 3.0 graphics features if they are available. For more information on checking for availability of the 3.0 API, see Checking OpenGL ES version\n\nTexture compression can significantly increase the performance of your OpenGL application by reducing memory requirements and making more efficient use of memory bandwidth. The Android framework provides support for the ETC1 compression format as a standard feature, including a utility class and the compression tool (located in the Android SDK at ). For an example of an Android application that uses texture compression, see the code sample in Android SDK ( ).\n\nThe ETC1 format is supported by all Android devices that support OpenGL ES 2.0 or higher.\n\nNote: The ETC1 texture compression format does not support textures with a transparency (alpha channel). If your application requires textures with transparency, you should investigate other texture compression formats available on your target devices. A method of rendering alpha channel textures using ETC1 is to bind two ETC1 texture objects: the first with color data, the second with alpha channel data and then combine the values from the two textures in the fragment shader.\n\nThe ETC2/EAC texture compression formats are guaranteed to be available when using the OpenGL ES 3.0 API. This texture format offers excellent compression ratios with high visual quality and the format also supports transparency (alpha channel).\n\nBeyond the ETC formats, Android devices have varied support for texture compression based on their GPU chipsets and OpenGL implementations. You should investigate texture compression support on the devices you are are targeting to determine what compression types your application should support. In order to determine what texture formats are supported on a given device, you must query the device and review the OpenGL extension names, which identify what texture compression formats (and other OpenGL features) are supported by the device. Some commonly supported texture compression formats are as follows:\n• Adaptable Scalable Texture Compression (ASTC) - A texture compression format designed to supersede prior formats. More flexible than previous formats due to support for various block sizes.\n• S3TC (DXTn/DXTC) - S3 texture compression (S3TC) has several format variations (DXT1 to DXT5) and is less widely available. The format supports RGB textures with 4-bit alpha or 8-bit alpha channels. These formats are represented by the following OpenGL extension name: Some devices only support the DXT1 format variation; this limited support is represented by the following OpenGL extension name:\n\nThe following texture compression formats are considered legacy formats and aren't recommended for use in new applications:\n• ATITC (ATC) - ATI texture compression (ATITC or ATC) is available on a wide variety of devices and supports fixed rate compression for RGB textures with and without an alpha channel. This format may be represented by several OpenGL extension names, for example:\n• PVRTC - PowerVR texture compression (PVRTC) is available on a wide variety of devices and supports 2-bit and 4-bit per pixel textures with or without an alpha channel. This format is represented by the following OpenGL extension name:\n• 3DC - 3DC texture compression (3DC) is a less widely available format that supports RGB textures with an alpha channel. This format is represented by the following OpenGL extension name:\n\nWarning: These texture compression formats are not supported on all devices. Support for these formats can vary by manufacturer and device. For information on how to determine what texture compression formats are on a particular device, see the next section.\n\nNote: Once you decide which texture compression formats your application will support, make sure you declare them in your manifest using <supports-gl-texture> . Using this declaration enables filtering by external services such as Google Play, so that your app is installed only on devices that support the formats your app requires. For details, see OpenGL manifest declarations.\n\nImplementations of OpenGL vary by Android device in terms of the extensions to the OpenGL ES API that are supported. These extensions include texture compressions, but typically also include other extensions to the OpenGL feature set.\n\nTo determine what texture compression formats, and other OpenGL extensions, are supported on a particular device:\n• Run the following code on your target devices to determine what texture compression formats are supported: Warning: The results of this call vary by device model! You must run this call on several target devices to determine what compression types are commonly supported.\n• Review the output of this method to determine what OpenGL extensions are supported on the device.\n\nThe AEP ensures that your application supports a standardized set of OpenGL extensions above and beyond the core set described in the OpenGL 3.1 specification. Packaging these extensions together encourages a consistent set of functionality across devices, while allowing developers to take full advantage of the latest crop of mobile GPU devices.\n\nThe AEP also improves support for images, shader storage buffers, and atomic counters in fragment shaders.\n\nFor your app to be able to use the AEP, the app's manifest must declare that the AEP is required. In addition, the platform version must support it.\n\nAll of the additional features specified in the AEP are included in the base OpenGL ES 3.2 specification. If your app requires OpenGL ES 3.2 you do not need to require the AEP.\n\nDeclare the AEP requirement in the manifest as follows:\n\nTo verify that the platform version supports the AEP, use the method, passing in as the argument. The following code snippet shows an example of how to do so:\n\nIf the method returns true, AEP is supported.\n\nFor more information about the AEP, visit its page at the Khronos OpenGL ES Registry.\n\nThere are several versions of OpenGL ES available on Android devices. You can specify the minimum version of the API your application requires in your manifest, but you may also want to take advantage of features in a newer API at the same time. For example, the OpenGL ES 3.0 API is backward-compatible with the 2.0 version of the API, so you may want to write your application so that it uses OpenGL ES 3.0 features, but falls back to the 2.0 API if the 3.0 API is not available.\n\nBefore using OpenGL ES features from a version higher than the minimum required in your application manifest, your application should check the version of the API available on the device. You can do this in one of two ways:\n• Attempt to create the higher-level OpenGL ES context ( ) and check the result.\n• Create a minimum-supported OpenGL ES context and check the version value.\n\nThe following example code demonstrates how to check the available OpenGL ES version by creating an and checking the result. This example shows how to check for OpenGL ES 3.0 version:\n\nIf the method show above returns null, your code should create a OpenGL ES 2.0 context instead and fall back to using only that API.\n\nThe following code example demonstrates how to check the OpenGL ES version by creating a minimum supported context first, and then checking the version string:\n\nWith this approach, if you discover that the device supports a higher-level API version, you must destroy the minimum OpenGL ES context and create a new context with the higher available API version.\n\nOpenGL ES version 2.0, and version 3.0 both provide high performance graphics interfaces for creating 3D games, visualizations and user interfaces. Graphics progamming for OpenGL ES 2.0 and 3.0 is largely similar, with version 3.0 representing a superset of the 2.0 API with additional features. Programming for the OpenGL ES 1.0/1.1 API versus OpenGL ES 2.0 and 3.0 differs significantly, and isn't recommended for new applications. Developers should carefully consider the following factors before starting development with these APIs:\n• Device Compatibility - Developers should consider the types of devices, Android versions and the OpenGL ES versions available to their customers. For more information on OpenGL compatibility across devices, see the OpenGL versions and device compatibility section.\n• Texture Support - The OpenGL ES 3.0 API has the best support for texture compression because it guarantees availability of the ETC2 compression format, which supports transparency. The 2.0 API implementations include support for ETC1, however this texture format doesn't support transparency. To implement transparency with compressed textures you must either use two ETC1 textures (split between color and alpha) or provide resources in other compression formats supported by the devices you are targeting. For more information, see Texture compression support.\n\nWhile compatibility, and texture support may influence your decision, you should pick an OpenGL API version based on what you think provides the best experience for your users."
    },
    {
        "link": "https://docs.nvidia.com/drive/archive/5.1.6.0L/nvvib_docs/DRIVE_OS_Linux_SDK_Development_Guide/baggage/es_spec_3.2.withchanges.pdf",
        "document": ""
    },
    {
        "link": "https://developer.nvidia.com/docs/drive/drive-os/archives/6.0.3/linux/sdk/api_reference/es_spec_3.2.withchanges.pdf",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/44031117/how-to-render-androids-yuv-yv12-camera-image-on-the-background-in-libgdx-with-o",
        "document": "This question refers to this one : How to render Android's YUV-NV21 camera image on the background in libgdx with OpenGLES 2.0 in real-time?\n\nIt is well explained in the best answer given by the author, but I have a little different issue concerning YV12 instead of NV12. (Here are some specs : https://wiki.videolan.org/YUV and https://www.fourcc.org/yuv.php )\n\nWhat about YUV-YV12 ? The Y buffer is the same, but the UV is not entrelaced so i looks like 2 buffers for V and U. But then, who to do to give them to the Shader ? Using an Pixmap.Format.Intensity texture I think, setting GL_LUMINANCE ?\n\nI don't understand how the NV12 \"UVUV\" buffer is converted into RGBA with RGB = V and A = U using GL_LUMINANCE and Pixmap format with GL_LUMINANCEALPHA ?\n\nYV12 is using \"VVUU\" buffer, so it is easy to split in V and U buffers, but how to bind them and get u and v in the shader ?\n\nThanks for any help, this sample is awesome ! but I need something a bit different and for that I need to understand deep in details the shader binding behavior."
    },
    {
        "link": "https://stackoverflow.com/questions/42153819/how-to-load-and-display-an-image-in-opengl-es-3-0-using-c",
        "document": "After working around, I finally make it. Below is my C++ source code. I use stb_image library to load the image."
    },
    {
        "link": "https://forums.raspberrypi.com/viewtopic.php?t=111137",
        "document": "glReadPixels only returns RGBA. To do a colourspace conversion probably render your scene to a texture then render that as fullscreen quad with a basic shader set to return Y in the red, U in green and V in blue. Not sure off-hand what NV12 format looks like but converting YUV to RGB is trivial so I would assume going the other way would be too.Edit:RGB to YUV is roughly (values dependant on your colourspace)NV12 only has UV as 1/4 size and they are interleaved after the Y array so you'd have to split them up, and for each 2x2 U and V samples I would probably store the average."
    },
    {
        "link": "https://community.khronos.org/t/shader-does-not-render-texture-with-opengl-es-version-3-2-and-opengl-egl/109697",
        "document": "I have been working with OpenGL ES version 3.2 and OpenGL EGL to try and render some simple geometries and have been successful. Now I am trying to move to some more advance feature(s), the use of shaders to render a 2D texture. However, I have only been able to get an image file that is black (my clear color value) when I try and use a texture that I am attaching to the framebuffer. There is a lot of code so I am posting what I hope are the critical points where it may be failing. I have no idea what is going on with this and any help/hints would be greatly appreciated. #version 320 es layout(location = 0) in vec3 position; layout(location = 0) out vec2 vTexcoord; layout(location = 1) in vec2 texcoord; void main() { gl_Position = vec4(position, 1.0); vTexcoord = texcoord; } The relevant portion of the C++ code that is called after the successful setting of EGL and GLES preliminaries such as setting context, etc., follows: // Where shader program has already been successfully compiled // and program defines the uint32_t identifier of this constexpr int32_t WIDTH = 480; constexpr int32_t HEIGHT = 620; GLfloat textureData[4 * WIDTH * HEIGHT]; for (int32_t y = 0; y < HEIGHT; y++) { for (int32_t x = 0; x < WIDTH; x++) { int32_t index = 4 * (y * WIDTH + x); float value = static_cast<float>(x) / WIDTH; textureData[index] = value; // red component textureData[index + 1] = value; // green component textureData[index + 2] = value; // blue component textureData[index + 3] = 1.0f; // alpha component } } GLfloat vertices[] = { -1.0f, -1.0f, 0.0f, 1.0f, -1.0f, 0.0f, 1.0f, 1.0f, 0.0f, -1.0f, 1.0f, 0.0f, }; GLushort indices[] = { 0, 1, 2, 2, 3, 0 }; ... GLuint texture; glGenTextures(1, &texture); glBindTexture(GL_TEXTURE_2D, texture); glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, WIDTH, HEIGHT, 0, GL_RGBA, GL_FLOAT, textureData); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST); // Set uniform for texture GLint textureLocation = glGetUniformLocation(program, \"uTexture\"); glUniform1i(textureLocation, 0); // Create and bind vertex buffer GLuint vertexBuffer; glGenBuffers(1, &vertexBuffer); glBindBuffer(GL_ARRAY_BUFFER, vertexBuffer); glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); // Create and bind index buffer GLuint indexBuffer; glGenBuffers(1, &indexBuffer); glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, indexBuffer); glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW); // Set up vertex attribute pointers GLint positionLocation = glGetAttribLocation(program, \"position\"); glEnableVertexAttribArray(positionLocation); glVertexAttribPointer(positionLocation, 3, GL_FLOAT, GL_FALSE, 5 * sizeof(GLfloat), 0); GLint texcoordLocation = glGetAttribLocation(program, \"texcoord\"); glEnableVertexAttribArray(texcoordLocation); glVertexAttribPointer(texcoordLocation, 2, GL_FLOAT, GL_FALSE, 5 * sizeof(GLfloat), (void*)(3 * sizeof(GLfloat))); // Clear the screen glClearColor(0.0f, 0.0f, 0.0f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); // Use the shader program glUseProgram(program); // Bind the texture glActiveTexture(GL_TEXTURE0); glBindTexture(GL_TEXTURE_2D, texture); // Draw the quad glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_SHORT, 0); // Read from framebuffer as floating point values std::vector<float> framebufferData(WIDTH * HEIGHT * 4); glReadPixels(0, 0, WIDTH, HEIGHT, GL_RGBA, GL_FLOAT, framebufferData.data()); // Write the framebuffer to a ppm file std::ofstream ppmFile(out_file.c_str(), std::ios::out | std::ios::binary); ppmFile << \"P6\n\n\" << WIDTH << \" \" << HEIGHT << \"\n\n255\n\n\"; for (size_t i = 0; i < WIDTH * HEIGHT * 4; i += 4) { unsigned char r = static_cast<unsigned char>(std::min(framebufferData[i] * 255.0f, 255.0f)); unsigned char g = static_cast<unsigned char>(std::min(framebufferData[i + 1] * 255.0f, 255.0f)); unsigned char b = static_cast<unsigned char>(std::min(framebufferData[i + 2] * 255.0f, 255.0f)); ppmFile << r << g << b; } // Clean up glDeleteTextures(1, &texture); glDeleteBuffers(1, &vertexBuffer); glDeleteBuffers(1, &indexBuffer); glDeleteProgram(program); The textureData array should define a gradient from black to white, however the output ppm file is a solid black rectangle. Apologies for the long message and if I am unclear in my question, but this has got me stumped and I am hoping one of you GLES experts will know where I am messing up. Thanks in advance for any help.\n\nuse a texture that I am attaching to the framebuffer In the code you’ve posted you are not using any framebuffer objects. You are drawing textured triangles. Your vertices do not have UV coordinates, each vertex should be a 5-tuple instead of a triple of floats - at least that is what your call later on say. As written some values will be read from past the end of your vertex buffer.\n\nThank you @carsten_neumann for the reply. I will add the framebuffer as well as adding the UV coords to the vertex so that there is a 5-tuple of triple floats. Question. Is it possible to add these UV coords to a separate buffer or is it always the case that texture coords need to be appended to vertices resulting in a 5-tuple of triple floats?\n\nYou don’t have to use a framebuffer object (only if you want to render to something else than the default framebuffer, aka the application window). I was just trying to point out that the way you had phrased things made it sound like you were doing something that isn’t reflected in the code you posted. Vertex attributes can be stored in multiple buffers or different regions of one buffer or interleaved in one buffer. Any layout that you can describe with really. When using different buffers you change the buffer object bound to the binding point before calling , which captures the bound buffer object along with the type, offset, stride information from its arguments.\n\nSorry about taking so long to get back to this question. I have been very busy lately. I added the changes, minus the framebuffer object. The C++ code snippet follows: // Where shader program has already been successfully compiled // and program defines the uint32_t identifier of this constexpr int32_t WIDTH = 501; constexpr int32_t HEIGHT = 501; constexpr int32_t TEX_WIDTH = 23; constexpr int32_t TEX_HEIGHT = 73; // Defines texture data as vertical gradient from black to white GLfloat textureData[4 * TEX_WIDTH * TEX_HEIGHT]; for (int32_t y = 0; y < TEX_HEIGHT; y++) { for (int32_t x = 0; x < TEX_WIDTH; x++) { int32_t index = 4 * (y * TEX_WIDTH + x); float value = static_cast<float>(x) / TEX_WIDTH; textureData[index] = value; // red component textureData[index + 1] = value; // green component textureData[index + 2] = value; // blue component textureData[index + 3] = 1.0f; // alpha component } } GLfloat vertices[] = { -1.0f, -1.0f, 0.0f, 1.0f, -1.0f, 0.0f, 1.0f, 1.0f, 0.0f, -1.0f, 1.0f, 0.0f, }; // Added UV coordinates float texCoords[] = { 0.0f, 0.0f, // Bottom left 1.0f, 0.0f, // Bottom right 1.0f, 1.0f, // Top right 0.0f, 1.0f // Top left }; GLushort indices[] = { 0, 1, 2, 2, 3, 0 }; ... GLuint texture; glGenTextures(1, &texture); glBindTexture(GL_TEXTURE_2D, texture); glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, TEX_WIDTH, TEX_HEIGHT, 0, GL_RGBA, GL_FLOAT, textureData); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST); // Set uniform for texture GLint textureLocation = glGetUniformLocation(program, \"uTexture\"); glUniform1i(textureLocation, 0); // Create and bind vertex buffer GLuint vertexBuffer; glGenBuffers(1, &vertexBuffer); glBindBuffer(GL_ARRAY_BUFFER, vertexBuffer); glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); // Create and bind index buffer GLuint indexBuffer; glGenBuffers(1, &indexBuffer); glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, indexBuffer); glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW); // Bind texture coordinate buffer object and load data GLuint texBuffer; glGenBuffers(1, &texBuffer); glBindBuffer(GL_ARRAY_BUFFER, texBuffer); glBufferData(GL_ARRAY_BUFFER, sizeof(texCoords), texCoords, GL_STATIC_DRAW); // Set up vertex attribute pointers GLint positionLocation = glGetAttribLocation(program, \"position\"); glEnableVertexAttribArray(positionLocation); glVertexAttribPointer(positionLocation, 3, GL_FLOAT, GL_FALSE, 0, 0); GLint texcoordLocation = glGetAttribLocation(program, \"texcoord\"); glEnableVertexAttribArray(texcoordLocation); glVertexAttribPointer(texcoordLocation, 2, GL_FLOAT, GL_FALSE, 0, 0); glViewport(0, 0, WIDTH, HEIGHT); // Clear the screen glClearColor(0.0f, 0.0f, 0.0f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); // Use the shader program glUseProgram(program); // Bind the texture glActiveTexture(GL_TEXTURE0); glBindTexture(GL_TEXTURE_2D, texture); // Draw the quad glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_SHORT, 0); // Read from framebuffer as floating point values std::vector<float> framebufferData(WIDTH * HEIGHT * 4); glReadPixels(0, 0, WIDTH, HEIGHT, GL_RGBA, GL_FLOAT, framebufferData.data()); // Write the framebuffer to a ppm file std::ofstream ppmFile(out_file.c_str(), std::ios::out | std::ios::binary); ppmFile << \"P6\n\n\" << WIDTH << \" \" << HEIGHT << \"\n\n255\n\n\"; for (size_t i = 0; i < WIDTH * HEIGHT * 4; i += 4) { unsigned char r = static_cast<unsigned char>(std::min(framebufferData[i] * 255.0f, 255.0f)); unsigned char g = static_cast<unsigned char>(std::min(framebufferData[i + 1] * 255.0f, 255.0f)); unsigned char b = static_cast<unsigned char>(std::min(framebufferData[i + 2] * 255.0f, 255.0f)); ppmFile << r << g << b; } // Clean up glDeleteTextures(1, &texture); glDeleteBuffers(1, &vertexBuffer); glDeleteBuffers(1, &indexBuffer); glDeleteProgram(program); The texture displays the gradient in the top left corner but not across the entire output image file and I would like it to display across the entire view. Also the texture doesn’t appear to perform any actual texture sampling. Any ideas/hints would be greatly appreciated.\n\nWhen using different buffers you change the buffer object bound to the binding point before calling , which captures the bound buffer object along with the type, offset, stride information from its arguments. Your code currently has the buffer containing the texture coordinates bound for both calls to , so OpenGL will happily read positions and texture coordinates from the buffer you are storing the latter in. Going forward, please consider working on you skills to debug problems; it is a critical skill for any sort of software development and finding and understanding the issue behind a strange bug can be very rewarding. For graphics problems it is often helpful when running into a problem to simplify the program until the problem goes away - e.g. here to remove the use of texture coordinates and only use positions (with a fragment shader that output a solid color). Since that would still not brought you back to where the same area as before was drawn it could have led you to look closely at how you specify your positions.\n\n If you are still stuck include a clear description of what you have done to isolate the problem in your post, that way others can focus on things you haven’t tried yet. That also helps to distinguish your post from the somewhat dreaded “It doesn’t work.” followed by a wall of code. Those posts often don’t result in helpful responses"
    },
    {
        "link": "https://nxp.com/docs/en/user-guide/IMX_GRAPHICS_USERS_GUIDE.pdf",
        "document": ""
    }
]