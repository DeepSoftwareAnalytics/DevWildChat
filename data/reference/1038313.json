[
    {
        "link": "https://fiberplane.com/blog/asynchronous-tasks-in-cloudflare-part1",
        "document": "In modern data APIs, asynchronous background tasks are not just common—they’re fundamental to almost every operation. Whether it’s database operations, sending emails, or processing webhooks, these behind-the-scenes tasks form the backbone of most services.\n\nMost importantly: The way you manage asynchronous operations in your API handlers can significantly impact your service’s overall performance and reliability.\n\nWhen a request hits our API and we run all of its logic sequentially, the response can end up being unnecessarily delayed. This increases latency for the end user or, worse, can result in failed responses—such as when one of the asynchronous tasks encounters an error.\n\nHandling asynchronous tasks is like running a marathon for our system: it requires endurance, coordination, retries, and error handling at the right points. There are multiple ways to handle asynchronous tasks. In part 1 of this blog post series, I will explore how we can manage asynchronous tasks within a single Cloudflare Worker.\n\nImagine this: We are organizing a marathon and need to build an api to register participants. We set up a simple data API with Hono on Cloudflare Workers, which stores the runners’ information in a database. After a successful registration, we send a confirmation email with all the important details for the race.\n\nIn this example, we will store registration data in a Neon database, and we’ll use Resend to send out emails for us.\n\nLet’s start by looking at running all tasks sequentially.\n\nIn the code above, we first insert the runner’s data into the database and then send the confirmation email. All the asynchronous tasks (functions) are called sequentially. Although this code works, it slows down the return to the user.\n\nIf we look into the trace provided in Fiberplane Studio, we see that tasks are running in sequential order, and the total response time depends on the whole sequence. The trace visualization helps us understand the performance impact of sequential execution on our application:\n\nOne way to return quickly is by not waiting for other tasks to complete successfully. Generally, this can be useful if, from a business perspective, it doesn’t matter whether the email is eventually sent or not. If you don’t need a guarantee that the email will be sent (e.g., for non-critical notifications), you can simply fire the promise and proceed without waiting for it.\n\nWhile it is essential for a successful registration to store the runner’s details in the database, we can argue that the email we send is not mission-critical, so we don’t need to wait for the promise to return. Taking this into account, we could modify the code like so:\n\nHowever, Cloudflare Workers are ephemeral, and your Worker might terminate after you return a response. This means dangling promises may not finish successfully, which could result in the email never being sent. Therefore, when working with Cloudflare Workers, we should ensure they are built in a way that gives promises a chance to complete.\n\nAs we can see here in the trace, the response returns after the database insert, but we cannot know whether the email was sent, because the worker shut down before the promise was resolved. The trace helps us identify potential issues with unfinished tasks in our fire-and-forget approach.\n\nOn another note: Do you want your support team assisting runners who didn’t receive a confirmation email and are now worried they haven’t signed up for the race? (Probably not!)\n\nIf we want to return a response to the user while still performing background tasks in our Worker, we can use . This allows us to send a response immediately and handle tasks like email and database updates in the background.\n\nIf there are multiple tasks to run in parallel, we can use to group them.\n\nIf we now look at the trace, we see the response returns immediately, and after a short while, the client instrumentation also shows the other two tasks related to the trace. This method leads to the parallel execution of both tasks, and the trace clearly shows how this improves our response time compared to the sequential approach:\n\nHowever, in our example, we want to ensure that the user’s data is stored in the database before returning a response to the user. Without this step, we can’t confirm their registration in the system. So, we actually do have a dependency on the database insert and a required sequential flow.\n\nHowever, instead of waiting for the email promise before returning, we could use to keep the worker open and return as follows:\n\nSimilar to the fire and forget trace, we see that the response is sent after the database insert, but the email is sent after the response is returned and starts after the database insert finishes. The trace confirms our intended execution order, all while showing improved response times.\n\nIn our scenario, the approach using for sending the email is the most suitable. However, there are still a few more things to consider.\n\nWhat happens if sending the email fails? How can we ensure that tasks are retried if they encounter an error? While we could implement custom logic for retries and error handling, this isn’t always the most effective solution.\n\nThis is where we could break down tasks across separate workers to streamline handling and retries. In the next part of this blog post series, we’ll explore how to manage asynchronous tasks across multiple workers and ensure tasks are retried if they fail."
    },
    {
        "link": "https://developers.cloudflare.com/workers/examples",
        "document": "Set up an A/B test by controlling what response is served based on cookies. This version supports passing the request through to test and control on the origin, bypassing random assignment."
    },
    {
        "link": "https://stackoverflow.com/questions/77579881/async-database-queries-with-cloudflare-workers",
        "document": "I have a use case where I'm wanting to run potentially long-running queries via CloudFlare workers. These seem to work great for the simple quick stuff, but I'm thinking about queries which could potentially take a couple of minutes. For example, a quick query:\n\nI don't really want to be sat waiting for the worker to run a long query, so I'm considering something whereby the worker issues the query and sticks the results into a cache somewhere that I can poll for (so maybe this is just a JavaScript problem?)\n\nHowever, this is just an idea that I've come up with. Are there any other more 'best practice' ways of potentially solving this?"
    },
    {
        "link": "https://developers.cloudflare.com/workers/runtime-apis/streams",
        "document": "The Streams API ↗ is a web standard API that allows JavaScript to programmatically access and process streams of data.\n\nWorkers do not need to prepare an entire response body before returning a . You can use a to stream a response body after sending the front matter (that is, HTTP status line and headers). This allows you to minimize:\n• The buffering done in the Worker.\n\nMinimizing buffering is especially important for processing or transforming response bodies larger than the Worker's memory limit. For these cases, streaming is the only implementation strategy.\n\nBy default, Cloudflare Workers is capable of streaming responses using the Streams APIs ↗. To maintain the streaming behavior, you should only modify the response body using the methods in the Streams APIs. If your Worker only forwards subrequest responses to the client verbatim without reading their body text, then its body handling is already optimal and you do not have to use these APIs.\n\nThe worker can create a object using a as the body. Any data provided through the will be streamed to the client as it becomes available.\n\nA and the method can be used to modify the response body as it is being streamed:\n\nThis example calls but does not it. This is so it does not block the forward progress of the remainder of the function. It continues to run asynchronously until the response is complete or the client disconnects.\n\nThe runtime can continue running a function ( ) after a response is returned to the client. This example pumps the subrequest response body to the final response body. However, you can use more complicated logic, such as adding a prefix or a suffix to the body or to process it somehow.\n• Write your Worker code in ES modules syntax for an optimized experience."
    },
    {
        "link": "https://stackoverflow.com/questions/65082602/what-are-the-use-cases-to-store-readablestream-in-the-distributed-data-store-lik",
        "document": "The difference between passing a , , or is not what data is stored, but rather how the data gets there. Note that you can store data as a and then later read it as an or vice versa ( s are converted to/from bytes using UTF-8). When you pass a to , the system reads data from the stream and stores that data; it does not store the stream itself. Similarly, when using , you can specify as the second parameter to get a back; when you read from this stream, it will produce the value's content.\n\nThe main case where you would want to use streams is when you want to directly store the body of an HTTP request into a KV value, or when you want to directly return a KV value as the body of an HTTP response. Using streams in these cases avoids the need to hold the entire value in memory all at once; instead, bytes can stream through as they arrive.\n\nFor example, instead of doing:\n\nYou should do this:\n\nThis is especially important when the value is many megabytes in size. The former version would read the entire value into memory to construct one large (including decoding UTF-8 to UTF-16), only to immediately write that value back out into a KV (converting UTF-16 back to UTF-8). The latter version copies bytes straight from the incoming connection into KV without ever storing the whole value in memory at once.\n\nSimilarly, for a response, instead of doing:\n\nYou can do:\n\nThis way, the response bytes get streamed from KV to the HTTP connection. This not only saves memory and CPU time, but also means that the client starts receiving bytes faster, because your Worker doesn't wait until all bytes are received before it starts forwarding them."
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/Using_Fetch",
        "document": "The Fetch API provides a JavaScript interface for making HTTP requests and processing the responses. Fetch is the modern replacement for : unlike , which uses callbacks, Fetch is promise-based and is integrated with features of the modern web such as service workers and Cross-Origin Resource Sharing (CORS). With the Fetch API, you make a request by calling , which is available as a global function in both and contexts. You pass it a object or a string containing the URL to fetch, along with an optional argument to configure the request. The function returns a which is fulfilled with a object representing the server's response. You can then check the request status and extract the body of the response in various formats, including text and JSON, by calling the appropriate method on the response. Here's a minimal function that uses to retrieve some JSON data from a server: async function getData() { const url = \"https://example.org/products.json\"; try { const response = await fetch(url); if (!response.ok) { throw new Error(`Response status: ${response.status}`); } const json = await response.json(); console.log(json); } catch (error) { console.error(error.message); } } We declare a string containing the URL and then call , passing the URL with no extra options. The function will reject the promise on some errors, but not if the server responds with an error status like : so we also check the response status and throw if it is not OK. Otherwise, we fetch the response body content as JSON by calling the method of , and log one of its values. Note that like itself, is asynchronous, as are all the other methods to access the response body content. In the rest of this page we'll look in more detail at the different stages of this process.\n\nThe request body is the payload of the request: it's the thing the client is sending to the server. You cannot include a body with requests, but it's useful for requests that send content to the server, such as or requests. For example, if you want to upload a file to the server, you might make a request and include the file as the request body. To set a request body, pass it as the option: You can supply the body as an instance of any of the following types: Other objects are converted to strings using their method. For example, you can use a object to encode form data (see setting headers for more information): const response = await fetch(\"https://example.org/post\", { method: \"POST\", headers: { \"Content-Type\": \"application/x-www-form-urlencoded\", }, // Automatically converted to \"username=example&password=password\" body: new URLSearchParams({ username: \"example\", password: \"password\" }), // ... }); Note that just like response bodies, request bodies are streams, and making the request reads the stream, so if a request contains a body, you can't make it twice: const request = new Request(\"https://example.org/post\", { method: \"POST\", body: JSON.stringify({ username: \"example\" }), }); const response1 = await fetch(request); console.log(response1.status); // Will throw: \"Body has already been consumed.\" const response2 = await fetch(request); console.log(response2.status); Instead, you would need to create a clone of the request before sending it: See Locked and disturbed streams for more information.\n\nCredentials are cookies, TLS client certificates, or authentication headers containing a username and password. To control whether or not the browser sends credentials, as well as whether the browser respects any response headers, set the option, which can take one of the following three values:\n• : never send credentials in the request or include credentials in the response.\n• (the default): only send and include credentials for same-origin requests. Note that if a cookie's attribute is set to or , then the cookie will not be sent cross-site, even if is set to . Including credentials in cross-origin requests can make a site vulnerable to CSRF attacks, so even if is set to , the server must also agree to their inclusion by including the header in its response. Additionally, in this situation the server must explicitly specify the client's origin in the response header (that is, is not allowed). This means that if is set to and the request is cross-origin, then:\n• If the request is a simple request, then the request will be sent with credentials, but the server must set the and response headers, or the browser will return a network error to the caller. If the server does set the correct headers, then the response, including credentials, will be delivered to the caller.\n• If the request is not a simple request, then the browser will send a preflighted request without credentials, and the server must set the and response headers, or the browser will return a network error to the caller. If the server does set the correct headers, then the browser will follow up with the real request, including credentials, and will deliver the real response, including credentials, to the caller.\n\nRequest and response bodies are actually objects, and whenever you read them, you're streaming the content. This is good for memory efficiency, because the browser doesn't have to buffer the entire response in memory before the caller retrieves it using a method like . This also means that the caller can process the content incrementally as it is received. For example, consider a request that fetches a large text file and processes it in some way, or displays it to the user: const url = \"https://www.example.org/a-large-file.txt\"; async function fetchText(url) { try { const response = await fetch(url); if (!response.ok) { throw new Error(`Response status: ${response.status}`); } const text = await response.text(); console.log(text); } catch (e) { console.error(e); } } If we use , as above, we must wait until the whole file has been received before we can process any of it. If we stream the response instead, we can process chunks of the body as they are received from the network: const url = \"https://www.example.org/a-large-file.txt\"; async function fetchTextAsStream(url) { try { const response = await fetch(url); if (!response.ok) { throw new Error(`Response status: ${response.status}`); } const stream = response.body.pipeThrough(new TextDecoderStream()); for await (const value of stream) { console.log(value); } } catch (e) { console.error(e); } } In this example, we iterate asynchronously over the stream, processing each chunk as it arrives. Note that when you access the body directly like this, you get the raw bytes of the response and must transform it yourself. In this case we call to pipe the response through a , which decodes the UTF-8-encoded body data as text."
    },
    {
        "link": "https://stackoverflow.com/questions/62121310/how-to-handle-streaming-data-using-fetch",
        "document": "I have used async for with great success in handling output streams from processes with node.js, but I'm struggling to get something that I was hoping could \"just work\" with the browser API.\n\nThis works great to async'ly handle chunks of output streaming from a process:\n\nI tried to do something similar in a browser where I want to gain access to the data while it is being sent to me from the server.\n\nThis does not work in Chrome ( ).\n\nFor my use case this is not necessary, so I am simply using in my client code for now. This is analogous to the typical use of instead of on the awaited fetch, so no processing can begin until the entire response is received by the browser. This is not ideal for obvious reasons.\n\nI was looking at Oboe.js (I think the relevant impl is somewhere near here) which pretty much deals with this but its internals are fairly ugly so it looks like that might be the only way to do this for now?\n\nIf async iteration isn't implemented (meaning async for cannot be used yet) isn't there another way to use the ReadableStream in a practical way?"
    },
    {
        "link": "https://digitalocean.com/community/tutorials/how-to-use-the-javascript-fetch-api-to-get-data",
        "document": "There was a time when was used to make API requests. It didn’t include Promises, and it didn’t make for clean JavaScript code. Using jQuery, you could use the cleaner syntax of .\n\nNow, JavaScript has its own built-in way to make API requests. This is the Fetch API, a new standard to make server requests with Promises, but which also includes additional features.\n\nIn this tutorial, you will create both GET and POST requests using the Fetch API.\n\nTo complete this tutorial, you will need the following:\n• A local development environment for Node.js. Follow How to Install Node.js and Create a Local Development Environment.\n• A basic understanding of coding in JavaScript, which you can learn more about from the How to Code in JavaScript series.\n• An understanding of Promises in JavaScript. Read the Promises section of this article on the event loop, callbacks, Promises, and async/await in JavaScript.\n\nOne approach to using the Fetch API is by passing the URL of the API as a parameter:\n\nThe method returns a Promise. After the method, include the Promise method :\n\nIf the Promise returned is , the function within the method is executed. That function contains the code for handling the data received from the API.\n\nAfter the method, include the method:\n\nThe API you call using may be down or other errors may occur. If this happens, the promise will be returned. The method is used to handle . The code within will be executed if an error occurs when calling the API of your choice.\n\nWith an understanding of the syntax for using the Fetch API, you can now move on to using on a real API.\n\nStep 2 — Using Fetch to get Data from an API\n\nThe following code samples will be based on the JSONPlaceholder API. Using the API, you will get ten users and display them on the page using JavaScript. This tutorial will retrieve data from the JSONPlaceholder API and display it in list items inside the author’s list.\n\nBegin by creating an HTML file and adding a heading and unordered list with the of :\n\nNow add tags to the bottom of your HTML file and use a DOM selector to grab the . Use with as the argument:\n\nRemember, is the for the previously created .\n\nNext, create a that is a :\n\nAll the appended list items will be added to . A is not part of the active document tree structure. This has the benefit of not causing performance-affecting redraws when the Document Object Model is changed.\n\nCreate a constant variable called which will hold the API URL that will return ten random users:\n\nNow using the Fetch API, call the JSONPlaceholder API using with as the argument:\n\nYou are calling the Fetch API and passing in the URL to the JSONPlaceholder API. Then a response is received. However, the response you get is not JSON, but an object with a series of methods that can be used depending on what you want to do with the information. To convert the object returned into JSON, use the method.\n\nAdd the method which will contain a function with a parameter called :\n\nThe parameter takes the value of the object returned from . Use the method to convert into JSON data:\n\nThe JSON data still needs to be processed. Add another statement with a function that has an argument called :\n\nWithin this function, create a variable called that is set equal to :\n\nFor each author in , you will want to create a list item that displays their name. The method is suited for this pattern:\n\nWithin your function, create a variable called that will be set equal to with (the HTML element) as the argument. Also, create an for and a for :\n\nThe element will contain the of the . The element will contain the email of the . The property and string interpolation will allow you to do this:\n\nNext, connect these DOM elements with :\n\nNote that each list item is being appended to the . Once the is complete, the is appended to the unordered list element.\n\nWith both functions completed, you can now add the function. This function will log the potential error to the console:\n\nThis is the full code of the request you created:\n\nYou just successfully performed a GET request using the JSONPlaceholder API and the Fetch API. In the next step, you will perform POST requests.\n\nFetch defaults to GET requests, but you can use all other types of requests, change the headers, and send data. Let’s create a POST request.\n\nFirst, include a constant variable that holds the link to the JSONPlaceholder API:\n\nNext, you need to set your object and pass it as the second argument of the fetch function. This will be an object called with the key and value (or your name):\n\nSince this is a POST request, you will need to state that explicitly. Create an object called :\n\nThis object needs to include three keys: , , and :\n\nThe key will have the value . will be set equal to the format of the object that was just created. will have the value of .\n\nThe interface is a property of the Fetch API, which allows you to perform actions on HTTP request and response headers. This article called How To Define Routes and HTTP Request Methods in Express can provide you with more information.\n\nWith this code in place, the POST request can be made using the Fetch API. You will include and as arguments for your POST request:\n\nThe function will include code that handles the response received from the JSONPlaceholder API:\n\nThis is the full code of the request you created:\n\nWith this approach, can be used as the sole argument for , replacing and .\n\nNow you know two methods for creating and executing POST requests with the Fetch API.\n\nThe Fetch API is a modern and flexible interface for making network requests in JavaScript. It is promise-based, making it easier to handle asynchronous operations efficiently. However, it is not the only option for making network requests in JavaScript.\n\nAxios is a popular library for making HTTP requests in JavaScript. It is promise-based and has a simple and clean API. It also provides the ability to intercept requests and responses, transform data, and cancel requests.\n\nMany JavaScript frameworks, such as React, Vue.js, and Angular, have their own built-in methods for making network requests. These methods are often based on the Fetch API or Axios, but they may have additional features or be more tightly integrated with the framework’s ecosystem.\n\nIf you’re working on a simple project and prefer a lightweight, native solution, use Fetch API. However, for projects requiring automatic JSON parsing, interceptors, and better error handling, Axios is the better choice.\n\nYou can check out How to Use Vue.js and Axios to Display Data from an API for an Axios-based approach.\n\nReact applications often use Fetch API inside useEffect() to fetch data when a component mounts:\n\nFor better performance in React, consider using JavaScript Performance API.\n\nIn Vue.js, Fetch API is commonly used inside the lifecycle hook:\n\nAlternatively, many Vue.js projects prefer using Axios for its simplicity, as shown in How to Use Vue.js and Axios to Display Data from an API.\n\nIn Angular, Fetch API can be used within services using , but if using native Fetch API, you can implement it inside a component:\n\nFor large applications, Angular’s built-in is recommended for better scalability.\n\n1. What does Fetch API do in JavaScript?\n\nThe Fetch API provides a modern and flexible interface for making network requests in JavaScript. It allows you to fetch resources like JSON data, HTML, images, and more from a server. Unlike older methods like XMLHttpRequest, Fetch API is promise-based, making it easier to handle asynchronous operations efficiently.\n\n2. What is an example of Fetch API?\n\nA simple example of using Fetch API to request JSON data from an API:\n\nThis fetches a sample post from a placeholder API and logs it to the console. You can also check out How to Use Vue.js and Axios to Display Data from an API for another way to retrieve and display API data.\n\n3. How to fetch JSON data from an API in JavaScript?\n\nThis converts the response to JSON using and then processes the data. If you’re working with performance optimizations, you may also find JavaScript Performance API useful.\n\n4. How to fetch data from an API with JavaScript?\n\no fetch data asynchronously, use inside an function with :\n\nThis ensures cleaner code and better error handling. For advanced API integrations, consider learning about GraphQL API as an alternative to REST APIs.\n\n5. What is the difference between REST API and Fetch API?\n\nIn simpler terms, Fetch API is a tool used to interact with a REST API or any other data source available over the web.\n\nWhile the Fetch API is not yet supported by all the browsers, it is a great alternative to .\n\nThis tutorial provides a step-by-step guide on using Fetch API in JavaScript. However, if you’re working on a larger project, you may want to explore Axios for better error handling or GraphQL for more efficient data fetching.\n• Learn how to optimize API performance with JavaScript Performance API.\n• Explore GraphQL for an alternative to REST APIs.\n• Read How to Use Vue.js and Axios to Display Data from an API for a comparison with Axios.\n\nBy integrating these concepts, you can efficiently fetch and manage data in any JavaScript project.\n\nIf you would like to learn how to call Web APIs using React, check out this article on this very topic."
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Web/API/Response/json",
        "document": "This feature is well established and works across many devices and browser versions. It’s been available across browsers since March 2017 .\n\nThe method of the interface takes a stream and reads it to completion. It returns a promise which resolves with the result of parsing the body text as .\n\nNote that despite the method being named , the result is not JSON but is instead the result of taking JSON as input and parsing it to produce a JavaScript object."
    },
    {
        "link": "https://stackoverflow.com/questions/37663674/using-fetch-api-to-access-json",
        "document": "I am trying to use fetch api to bring back some data, however am unable to map it to the console once I have retrieved it.\n\nThe error i get is\n\nso I tried to parse the response,(ie var data=JSON.parse) which did not work, with the error\n\nInterestingly, when doing the same thing with a XMLHttp request, I was required to parse it, so I would also be interested to know why the difference between these two methods of retrieving the data.\n\nIf anyone could point me in the right direction, I would be really grateful."
    }
]