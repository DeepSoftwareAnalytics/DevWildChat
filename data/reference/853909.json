[
    {
        "link": "https://psycopg.org/docs/extras.html",
        "document": "This module is a generic place used to hold little helper functions and classes until a better place in the distribution is found.\n\nA few objects that change the way the results are returned by the cursor or modify the object behavior in some other way. Typically subclasses are passed as cursor_factory argument to so that the connection’s method will generate objects of this class. Alternatively a subclass can be used one-off by passing it as the cursor_factory argument to the method. If you want to use a subclass you can pass it as the connection_factory argument of the function. The dict cursors allow to access to the attributes of retrieved records using an interface similar to the Python dictionaries instead of the tuples. The records still support indexing as the original tuple: A cursor that keeps a list of column name -> index mappings. Not very useful since Psycopg 2.5: you can use instead of . A row object that allow by-column-name access to data. A cursor that uses a real dict as the base type for rows. Note that this cursor is extremely specialized and does not allow the normal access (using integer indices) to fetched data. If you need to access database rows both as a dictionary and a list, then use the generic instead of . Not very useful since Psycopg 2.5: you can use instead of . methods will return named tuples instead of regular tuples, so their elements can be accessed both as regular numeric items as well as attributes. Not very useful since Psycopg 2.5: you can use instead of . A connection that logs all queries to a file or logger object. Filter the query before logging it. This is the method to overwrite to filter unwanted queries out of the log or to add some extra data to the output. The default implementation just does nothing. Initialize the connection to log to . The parameter can be an open file object or a Logger/LoggerAdapter instance from the standard logging module. A cursor that logs queries using its connection logging facilities. Queries that are executed with are not logged. This is just an example of how to sub-class to provide some extra filtering for the logged queries. Both the and methods are overwritten to make sure that only queries executing for more than ms are logged. Note that this connection uses the specialized cursor . Filter the query before logging it. This is the method to overwrite to filter unwanted queries out of the log or to add some extra data to the output. The default implementation just does nothing. Initialize the connection to log to . The parameter can be an open file object or a Logger/LoggerAdapter instance from the standard logging module.\n\nSee Replication protocol support for an introduction to the topic. The following replication types are defined: This connection factory class can be used to open a special type of connection that is used for logical replication. This connection factory class can be used to open a special type of connection that is used for physical replication. Both and use for actual communication with the server. The individual messages in the replication stream are represented by objects (both logical and physical type): The actual data received from the server. An instance of either or , depending on the value of option passed to on the connection. See for details. The raw size of the message payload (before possible unicode conversion). LSN position of the start of the message. LSN position of the current end of WAL on the server. A object representing the server timestamp at the moment when the message was sent. A reference to the corresponding object. A cursor used for communication on replication connections.\n• None slot_name – name of the replication slot to be created\n• None slot_type – type of replication: should be either or\n• None output_plugin – name of the logical decoding output plugin to be used by the slot; required for logical replication connections, disallowed for physical When creating a slot on a logical replication connection, a logical replication slot is created by default. Logical replication requires name of the logical decoding output plugin to be specified. When creating a slot on a physical replication connection, a physical replication slot is created by default. No output plugin parameter is required or allowed when creating a physical replication slot. In either case the type of slot being created can be specified explicitly using slot_type parameter. Replication slots are a feature of PostgreSQL server starting with version 9.4. slot_name – name of the replication slot to drop Replication slots are a feature of PostgreSQL server starting with version 9.4.\n• None slot_name – name of the replication slot to use; required for logical replication, physical replication can work with or without a slot\n• None slot_type – type of replication: should be either or\n• None start_lsn – the optional LSN position to start replicating from, can be an integer or a string of hexadecimal digits in the form\n• None timeline – WAL history timeline to start streaming from (optional, can only be used with physical replication)\n• None options – a dictionary of options to pass to logical replication slot (not allowed with physical replication)\n• None decode – a flag indicating that unicode conversion should be performed on messages received from the server\n• None status_interval – time between feedback packets sent to the server If a slot_name is specified, the slot must exist on the server and its type must match the replication type used. If not specified using slot_type parameter, the type of replication is defined by the type of replication connection. Logical replication is only allowed on logical replication connection, but physical replication can be used with both types of connection. On the other hand, physical replication doesn’t require a named replication slot to be used, only logical replication does. In any case logical replication and replication slots are a feature of PostgreSQL server starting with version 9.4. Physical replication can be used starting with 9.0. If start_lsn is specified, the requested stream will start from that LSN. The default is which passes the LSN causing replay to begin at the last point for which the server got flush confirmation from the client, or the oldest available point for a new slot. The server might produce an error if a WAL file for the given LSN has already been recycled or it may silently start streaming from a later position: the client can verify the actual position using information provided by the attributes. The exact server behavior depends on the type of replication and use of slots. The timeline parameter can only be specified with physical replication and only starting with server version 9.3. A dictionary of options may be passed to the logical decoding plugin on a logical replication slot. The set of supported options depends on the output plugin that was used to create the slot. Must be for physical replication. If decode is set to the messages received from the server would be converted according to the connection . This parameter should not be set with physical replication or with logical replication plugins that produce binary output. Replication stream should periodically send feedback to the database to prevent disconnect via timeout. Feedback is automatically sent when is called or during run of the . To specify the feedback interval use status_interval parameter. The value of this parameter must be set to at least 1 second, but it can have a fractional part. After starting the replication, to actually consume the incoming server messages use or implement a loop around in case of asynchronous connection. Start replication on the connection using provided command.\n• None command – The full replication command. It can be a string or a instance for dynamic generation.\n• None decode – a flag indicating that unicode conversion should be performed on messages received from the server.\n• None status_interval – time between feedback packets sent to the server\n• None keepalive_interval – interval (in seconds) to send keepalive messages to the server This method can only be used with synchronous connection. For asynchronous connections see . Before using this method to consume the stream call first. This method enters an endless loop reading messages from the server and passing them to one at a time, then waiting for more messages from the server. In order to make this method break out of the loop and return, can throw a exception. Any unhandled exception will make it break out of the loop as well. The msg object passed to is an instance of class. See for details about message decoding. This method also sends feedback messages to the server every keepalive_interval (in seconds). The value of this parameter must be set to at least 1 second, but it can have a fractional part. If the keepalive_interval is not specified, the value of status_interval specified in the or will be used. The client must confirm every processed message by calling method on the corresponding replication cursor. A reference to the cursor is provided in the as an attribute. The following example is a sketch implementation of callable for logical replication: When using replication with slots, failure to constantly consume and report success to the server appropriately can eventually lead to “disk full” condition on the server, because the server retains all the WAL segments that might be needed to stream the changes via all of the currently open replication slots. Changed in version 2.8.3: changed the default value of the keepalive_interval parameter to .\n• None write_lsn – a LSN position up to which the client has written the data locally\n• None flush_lsn – a LSN position up to which the client has processed the data reliably (the server is allowed to discard all and every data that predates this LSN)\n• None apply_lsn – a LSN position up to which the warm standby server has applied the changes (physical replication master-slave protocol only) Use this method to report to the server that all messages up to a certain LSN position have been processed on the client and may be discarded on the server. If the reply or force parameters are not set, this method will just update internal structures without sending the feedback message to the server. The library sends feedback message automatically when status_interval timeout is reached. For this to work, you must call on the same Cursor that you called on (the one in ) or your feedback will be lost. With the synchronous connection a call to handles all the complexity of handling the incoming messages and sending keepalive replies, but at times it might be beneficial to use low-level interface for better control, in particular to on multiple sockets. The following methods are provided for asynchronous operation: Try to read the next message from the server without blocking and return an instance of or , in case there are no more data messages from the server at the moment. This method should be used in a loop with asynchronous connections (after calling once). For synchronous connections see . The returned message’s is an instance of decoded according to connection iff decode was set to in the initial call to on this connection, otherwise it is an instance of with no decoding. It is expected that the calling code will call this method repeatedly in order to consume all of the messages that might have been buffered until is returned. After receiving from this method the caller should use or on the corresponding connection to block the process until there is more data from the server. Last, but not least, this method sends feedback messages when status_interval timeout is reached or when keepalive message with reply request arrived from the server. Call the corresponding connection’s method and return the result. This is a convenience method which allows replication cursor to be used directly in or calls. A object representing the timestamp at the moment of last communication with the server (a data or keepalive message in either direction). A object representing the timestamp at the moment when the last feedback message sent to the server. LSN position of the current end of WAL on the server at the moment of last data or keepalive message received from the server. An actual example of asynchronous operation might look like this: Exception used to break out of the endless loop in . Subclass of . Intentionally not inherited from as occurrence of this exception does not indicate an error.\n\nChanged in version 2.5.4: added support. In previous versions values are returned as strings. See the FAQ for a workaround. Psycopg can adapt Python objects to and from the PostgreSQL and types. With PostgreSQL 9.2 and following versions adaptation is available out-of-the-box. To use JSON data with previous database versions (either with the 9.1 json extension, but even if you want to convert text fields to JSON) you can use the function. The Python module is used by default to convert Python objects to JSON and to parse data from the database. In order to pass a Python object to the database as query argument you can use the adapter: Reading from the database, and values will be automatically converted to Python objects. If you are using the PostgreSQL data type but you want to read it as string in Python instead of having it parsed, your can either cast the column to in the query (it is an efficient operation, that doesn’t involve a copy): or you can register a no-op function with : You can use to adapt any Python dictionary to JSON, either registering or any subclass or factory creating a compatible adapter: This setting is global though, so it is not compatible with similar adapters such as the one registered by . Any other object supported by JSON can be registered the same way, but this will clobber the default adaptation rule, so be careful to unwanted side effects. If you want to customize the adaptation from Python to PostgreSQL you can either provide a custom function to : or you can subclass it overriding the method: Customizing the conversion from PostgreSQL to Python can be done passing a custom function to . For the builtin data types ( from PostgreSQL 9.2, from PostgreSQL 9.4) use and . For example, if you want to convert the float values from into you can use: Or, if you want to use an alternative JSON module implementation, such as the faster UltraJSON, you can use: An wrapper to adapt a Python object to data type. can be used to wrap any object supported by the provided dumps function. If none is provided, the standard is used. The default is to call or the dumps function provided in the constructor. You can override this method to create a customized JSON wrapper.\n• None conn_or_curs – a connection or cursor used to find the and oids; the typecasters are registered in a scope limited to this object, unless globally is set to . It can be if the oids are provided\n• None globally – if register the typecasters only on conn_or_curs, otherwise register them globally\n• None loads – the function used to parse the data into a Python object. If use , where is the module chosen according to the Python version (see above)\n• None oid – the OID of the type if known; If not, it will be queried on conn_or_curs\n• None array_oid – the OID of the array type if known; if not, it will be queried on conn_or_curs\n• None name – the name of the data type to look for in conn_or_curs The connection or cursor passed to the function will be used to query the database and look for the OID of the type (or an alternative type if name if provided). No query is performed if oid and array_oid are provided. Raise if the type is not found. Changed in version 2.5.4: added the name parameter to enable support. Create and register typecasters for PostgreSQL 9.2 and following. Since PostgreSQL 9.2 is a builtin type, hence its oid is known and fixed. This function allows specifying a customized loads function for the default type without querying the database. All the parameters have the same meaning of . Create and register typecasters for PostgreSQL 9.4 and following. As in , the function allows to register a customized loads function for the type at its known oid for PostgreSQL 9.4 and following versions. All the parameters have the same meaning of . The data type is a key-value store embedded in PostgreSQL. It has been available for several server versions but with the release 9.0 it has been greatly improved in capacity and usefulness with the addition of many functions. It supports GiST or GIN indexes allowing search by keys or key/value pairs as well as regular BTree indexes for equality, uniqueness etc. Psycopg can convert Python objects to and from structures. Only dictionaries with string/unicode keys and values are supported. is also allowed as value but not as a key. Psycopg uses a more efficient representation when dealing with PostgreSQL 9.0 but previous server versions are supported as well. By default the adapter/typecaster are disabled: they can be enabled using the function.\n• None conn_or_curs – a connection or cursor: the typecaster will be registered only on this object unless globally is set to\n• None globally – register the adapter globally, not only on conn_or_curs\n• None unicode – if , keys and values returned from the database will be instead of . The option is not available on Python 3\n• None oid – the OID of the type if known. If not, it will be queried on conn_or_curs.\n• None array_oid – the OID of the array type if known. If not, it will be queried on conn_or_curs. The connection or cursor passed to the function will be used to query the database and look for the OID of the type (which may be different across databases). If querying is not desirable (e.g. with asynchronous connections) you may specify it in the oid parameter, which can be found using a query such as . Analogously you can obtain a value for array_oid using a query such as . Note that, when passing a dictionary from Python to the database, both strings and unicode keys and values are supported. Dictionaries returned from the database have keys/values according to the unicode parameter. The contrib module must be already installed in the database (executing the script in your directory). Raise if the type is not found. Changed in version 2.4: added the oid parameter. If not specified, the typecaster is installed also if is not installed in the schema. Using it is possible to cast a PostgreSQL composite type (either created with the command or implicitly defined after a table row type) into a Python named tuple, or into a regular tuple if is not found. Nested composite types are handled as expected, provided that the type of the composite components are registered as well. Adaptation from Python tuples to composite types is automatic instead and requires no adapter registration. If you want to convert PostgreSQL composite types into something different than a you can subclass the overriding . For example, if you want to convert your type into a Python dictionary you can use:\n• None name – the name of a PostgreSQL composite type, e.g. created using the command\n• None conn_or_curs – a connection or cursor used to find the type oid and components; the typecaster is registered in a scope limited to this object, unless globally is set to\n• None globally – if (default) register the typecaster only on conn_or_curs, otherwise register it globally\n• None factory – if specified it should be a subclass: use it to customize how to cast composite types the registered or factory instance responsible for the conversion Changed in version 2.4.3: added support for array of composite types The class is usually created by the function. You may want to create and register manually instances of the class if querying the database at registration time is not desirable (such as when using an asynchronous connections). Return a new Python object representing the data being casted. values is the list of attributes, already casted into their Python representation. You can subclass this method to customize the composite cast. The name of the PostgreSQL type. The schema where the type is defined. The oid of the PostgreSQL type. The oid of the PostgreSQL array type, if available. The type of the Python objects returned. If is available, it is a named tuple with attributes equal to the type components. Otherwise it is just the object. List of component names of the type to be casted. List of component type oids of the type to be casted. Psycopg offers a Python type and supports adaptation between them and PostgreSQL types. Builtin types are supported out-of-the-box; user-defined types can be adapted using .\n• None bounds – one of the literal strings , , , , representing whether the lower or upper bounds are included\n• None empty – if , the range is empty This Python type is only used to pass and retrieve range values to and from PostgreSQL and doesn’t attempt to replicate the PostgreSQL range features: it doesn’t perform normalization and doesn’t implement all the operators supported by the database. objects are immutable, hashable, and support the operator (checking if an element is within the range). They can be tested for equivalence. Empty ranges evaluate to in boolean context, nonempty evaluate to . Changed in version 2.5.3: objects can be sorted although, as on the server-side, this ordering is not particularly meangingful. It is only meant to be used by programs assuming objects using as primary key can be sorted on them. In previous versions comparing s raises . Although it is possible to instantiate objects, the class doesn’t have an adapter registered, so you cannot normally pass these instances as query arguments. To use range objects as query arguments you can either use one of the provided subclasses, such as or create a custom subclass using . if the range is empty. The lower bound of the range. if empty or unbound. The upper bound of the range. if empty or unbound. if the lower bound is included in the range. if the upper bound is included in the range. if the range doesn’t have a lower bound. if the range doesn’t have an upper bound. The following subclasses map builtin PostgreSQL types to Python objects: they have an adapter registered so their instances can be passed as query arguments. values read from database queries are automatically casted into instances of these classes. Python lacks a representation for date so Psycopg converts the value to and such. When written into the database these dates will assume their literal value (e.g. instead of ). Check Infinite dates handling for an example of an alternative adapter to map to . An alternative dates adapter will be used automatically by the adapter and so on. Custom types (created with ) can be adapted to a custom subclass: Create and register an adapter and the typecasters to convert between a PostgreSQL type and a PostgreSQL subclass.\n• None pgrange – the name of the PostgreSQL type. Can be schema-qualified\n• None pyrange – a strict subclass, or just a name to give to a new class\n• None conn_or_curs – a connection or cursor used to find the oid of the range and its subtype; the typecaster is registered in a scope limited to this object, unless globally is set to\n• None globally – if (default) register the typecaster only on conn_or_curs, otherwise register it globally If a string is passed to pyrange, a new subclass is created with such name and will be available as the attribute of the returned object. The function queries the database on conn_or_curs to inspect the pgrange type and raises if the type is not found. If querying the database is not advisable, use directly the class and register the adapter and typecasters using the provided functions. Helper class to convert between and PostgreSQL range types. Objects of this class are usually created by . Manual creation could be useful if querying the database is not advisable: in this case the oids must be provided. The object responsible to cast arrays, if available, else . # Python UUID can be used in SQL queries Create the UUID type and an uuid.UUID adapter.\n• None oids – oid for the PostgreSQL type, or 2-items sequence with oids of the type and the array. If not specified, use PostgreSQL standard oids.\n• None conn_or_curs – where to register the typecaster. If not specified, register it globally. By default Psycopg casts the PostgreSQL networking data types ( , , ) into ordinary strings; array of such types are converted into lists of strings. Changed in version 2.7: in previous version array of networking types were not treated as arrays. conn_or_curs – the scope where to register the type casters. If register them globally. After the function is called, PostgreSQL values will be converted into or objects, values into into or . Create the INET type and an Inet adapter.\n• None oid – oid for the PostgreSQL type, or 2-items sequence with oids of the type and the array. If not specified, use PostgreSQL standard oids.\n• None conn_or_curs – where to register the typecaster. If not specified, register it globally. Deprecated since version 2.7: this function will not receive further development and may disappear in future versions. Wrap a string to allow for correct SQL-quoting of inet values. Note that this adapter does NOT check the passed value to make sure it really is an inet-compatible address but DOES call adapt() on it to make sure it is impossible to execute an SQL-injection by passing an evil value to the initializer. Deprecated since version 2.7: this object will not receive further development and may disappear in future versions.\n\nThe current implementation of is (using an extremely charitable understatement) not particularly performing. These functions can be used to speed up the repeated execution of a statement against a set of parameters. By reducing the number of server roundtrips the performance can be orders of magnitude better than using . Execute sql several times, against all parameters set (sequences or mappings) found in argslist. The function is semantically similar to but has a different implementation: Psycopg will join the statements into fewer multi-statement commands, each one containing at most page_size statements, resulting in a reduced number of server roundtrips. After the execution of the function the property will not contain a total result. can be also used in conjunction with PostgreSQL prepared statements using , , . Instead of executing: it is possible to execute something like: \"PREPARE stmt AS big and complex SQL with $1 $2 params\" which may bring further performance benefits: if the operation to perform is complex, every single execution will be faster as the query plan is already cached; furthermore the amount of data to send on the server will be lesser (one per param set instead of the whole, likely longer, statement). Execute a statement using with a sequence of parameters.\n• None cur – the cursor to use to execute the query.\n• None sql – the query to execute. It must contain a single placeholder, which will be replaced by a VALUES list. Example: .\n• None argslist – sequence of sequences or dictionaries with the arguments to send to the query. The type and content must be consistent with template.\n• None the snippet to merge to every item in argslist to compose the query.\n• None If the argslist items are sequences it should contain positional placeholders (e.g. , or ” if there are constants value…).\n• None If the argslist items are mappings it should contain named placeholders (e.g. ). If not specified, assume the arguments are sequence and use a simple positional template (i.e. ), with the number of placeholders sniffed by the first element in argslist.\n• None page_size – maximum number of argslist items to include in every statement. If there are more items the function will execute more than one statement.\n• None fetch – if return the query results into a list (like in a ). Useful for queries with clause. After the execution of the function the property will not contain a total result. While is an obvious candidate for this function it is possible to use it with other statements, for example:"
    },
    {
        "link": "https://psycopg.org/docs/usage.html",
        "document": "The basic Psycopg usage is common to all the database adapters implementing the DB API 2.0 protocol. Here is an interactive session showing some of the basic commands:\n\nThe main entry points of Psycopg are:\n• None The function creates a new database session and returns a new instance.\n• None The class encapsulates a database session. It allows to:\n• None create new instances using the method to execute database commands and queries,\n• None terminate transactions using the methods or .\n• None The class allows interaction with the database:\n• None send commands to the database using methods such as and ,\n• None retrieve data from the database by iteration or using methods such as , , .\n\nPsycopg converts Python variables to SQL values using their types: the Python type determines the function used to convert the object into a string representation suitable for PostgreSQL. Many standard Python types are already adapted to the correct SQL representation. Passing parameters to an SQL statement happens in functions such as by using placeholders in the SQL statement, and passing a sequence of values as the second argument of the function. For example the Python function call: is converted into a SQL command similar to: Named arguments are supported too using placeholders in the query and specifying the values into a mapping. Using named arguments allows to specify the values in any order and to repeat the same value in several places in the query: Using characters , , in the argument names is not supported. When parameters are used, in order to include a literal in the query you can use the string: While the mechanism resembles regular Python strings manipulation, there are a few subtle differences you should care about when passing parameters to a query.\n• None The Python string operator must not be used: the method accepts a tuple or dictionary of values as second parameter. Never use or to merge values into queries:\n• None For positional variables binding, the second argument must always be a sequence, even if it contains a single variable (remember that Python requires a comma to create a single element tuple):\n• None The placeholder must not be quoted. Psycopg will add quotes where needed:\n• None The variables placeholder must always be a , even if a different placeholder (such as a for integers or for floats) may look more appropriate:\n• None Only query values should be bound via this method: it shouldn’t be used to merge table or field names to the query (Psycopg will try quoting the table name as a string value, generating invalid SQL). If you need to generate dynamically SQL queries (for instance choosing dynamically a table name) you can use the facilities provided by the module: The problem with the query parameters¶ The SQL representation of many data types is often different from their Python string representation. The typical example is with single quotes in strings: in SQL single quotes are used as string literal delimiters, so the ones appearing inside the string itself must be escaped, whereas in Python single quotes can be left unescaped if the string is delimited by double quotes. Because of the difference, sometime subtle, between the data types representations, a naïve approach to query strings composition, such as using Python strings concatenation, is a recipe for terrible problems: ProgrammingError: syntax error at or near \"Reilly\" If the variables containing the data to send to the database come from an untrusted source (such as a form published on a web site) an attacker could easily craft a malformed string, either gaining access to unauthorized data or performing destructive operations on the database. This form of attack is called SQL injection and is known to be one of the most widespread forms of attack to database servers. Before continuing, please print this page as a memo and hang it onto your desk. Psycopg can automatically convert Python objects to and from SQL literals: using this feature your code will be more robust and reliable. We must stress this point: Never, never, NEVER use Python string concatenation ( ) or string parameters interpolation ( ) to pass variables to a SQL query string. Not even at gunpoint. The correct way to pass variables in a SQL command is using the second argument of the method: Unlike in Python, the backslash ( ) is not used as an escape character except in patterns used with and where they are needed to escape the and characters. This can lead to confusing situations: 'SELECT * FROM mytable WHERE path LIKE The solution is to specify an character of (empty string) in your query: \"SELECT * FROM mytable WHERE path LIKE\n\nIn Psycopg transactions are handled by the class. By default, the first time a command is sent to the database (using one of the s created by the connection), a new transaction is created. The following database commands will be executed in the context of the same transaction – not only the commands issued by the first cursor, but the ones issued by all the cursors created by the same connection. Should any command fail, the transaction will be aborted and no further command will be executed until a call to the method. The connection is responsible for terminating its transaction, calling either the or method. Committed changes are immediately made persistent in the database. If the connection is closed (using the method) or destroyed (using or by letting it fall out of scope) while a transaction is in progress, the server will discard the transaction. However doing so is not advisable: middleware such as PgBouncer may see the connection closed uncleanly and dispose of it. It is possible to set the connection in autocommit mode: this way all the commands executed will be immediately committed and no rollback is possible. A few commands (e.g. , , on stored procedures using transaction control…) require to be run outside any transaction: in order to be able to run these commands from Psycopg, the connection must be in autocommit mode: you can use the property. By default even a simple will start a transaction: in long-running programs, if no further action is taken, the session will remain “idle in transaction”, an undesirable condition for several reasons (locks are held by the session, tables bloat…). For long lived scripts, either make sure to terminate a transaction as soon as possible or use an autocommit connection. A few other transaction properties can be set session-wide by the : for instance it is possible to have read-only transactions or change the isolation level. See the method for all the details. Starting from version 2.5, psycopg2’s connections and cursors are context managers and can be used with the statement: When a connection exits the block, if no exception has been raised by the block, the transaction is committed. In case of exception the transaction is rolled back. When a cursor exits the block it is closed, releasing any resource eventually associated with it. The state of the transaction is not affected. A connection can be used in more than one statement and each block is effectively wrapped in a separate transaction: Unlike file objects or other resources, exiting the connection’s block doesn’t close the connection, but only the transaction associated to it. If you want to make sure the connection is closed after a certain point, you should still use a try-catch block: Changed in version 2.9: starts a transaction also on autocommit connections.\n\nWhen a database query is executed, the Psycopg usually fetches all the records returned by the backend, transferring them to the client process. If the query returns a huge amount of data, a proportionally large amount of memory will be allocated by the client. If the dataset is too large to be practically handled on the client side, it is possible to create a server side cursor. Using this kind of cursor it is possible to transfer to the client only a controlled amount of data, so that a large dataset can be examined without keeping it entirely in memory. Server side cursor are created in PostgreSQL using the command and subsequently handled using , and commands. Psycopg wraps the database server side cursor in named cursors. A named cursor is created using the method specifying the name parameter. Such cursor will behave mostly like a regular cursor, allowing the user to move in the dataset using the method and to read the data using and methods. Normally you can only scroll forward in a cursor: if you need to scroll backwards you should declare your cursor . Named cursors are also iterable like regular cursors. Note however that before Psycopg 2.4 iteration was performed fetching one record at time from the backend, resulting in a large overhead. The attribute now controls how many records are fetched at time during the iteration: the default value of 2000 allows to fetch about 100KB per roundtrip assuming records of 10-20 columns of mixed number and strings; you may decrease this value if you are dealing with huge records. Named cursors are usually created , meaning they live only as long as the current transaction. Trying to fetch from a named cursor after a or to create a named cursor when the connection is in mode will result in an exception. It is possible to create a cursor by specifying a value for the parameter to or by setting the attribute to before calling on the cursor. It is extremely important to always such cursors, otherwise they will continue to hold server-side resources until the connection will be eventually closed. Also note that while cursors lifetime extends well after , calling will automatically close the cursor. It is also possible to use a named cursor to consume a cursor created in some other way than using the executed by . For example, you may have a PL/pgSQL function returning a cursor: You can read the cursor content by calling the function with a regular, non-named, Psycopg cursor: and then use a named cursor in the same transaction to “steal the cursor”: # do something with record\n\nThe Psycopg module and the objects are thread-safe: many threads can access the same database either using separate sessions and creating a per thread or using the same connection and creating separate s. In DB API 2.0 parlance, Psycopg is level 2 thread safe. The difference between the above two approaches is that, using different connections, the commands will be executed in different sessions and will be served by different server processes. On the other hand, using many cursors on the same connection, all the commands will be executed in the same session (and in the same transaction if the connection is not in autocommit mode), but they will be serialized. The above observations are only valid for regular threads: they don’t apply to forked processes nor to green threads. connections shouldn’t be used by a forked processes, so when using a module such as or a forking web deploy method such as FastCGI make sure to create the connections after the fork. Connections shouldn’t be shared either by different green threads: see Support for coroutine libraries for further details.\n\nPsycopg exposes the two-phase commit features available since PostgreSQL 8.1 implementing the two-phase commit extensions proposed by the DB API 2.0. The DB API 2.0 model of two-phase commit is inspired by the XA specification, according to which transaction IDs are formed from three components: For a particular global transaction, the first two components will be the same for all the resources. Every resource will be assigned a different branch qualifier. According to the DB API 2.0 specification, a transaction ID is created using the method. Once you have a transaction id, a distributed transaction can be started with , prepared using and completed using or . Transaction IDs can also be retrieved from the database using and completed using the above and . PostgreSQL doesn’t follow the XA standard though, and the ID for a PostgreSQL prepared transaction can be any string up to 200 characters long. Psycopg’s objects can represent both XA-style transactions IDs (such as the ones created by the method) and PostgreSQL transaction IDs identified by an unparsed string. The format in which the Xids are converted into strings passed to the database is the same employed by the PostgreSQL JDBC driver: this should allow interoperation between tools written in Python and in Java. For example a recovery tool written in Python would be able to recognize the components of transactions produced by a Java program. For further details see the documentation for the above methods."
    },
    {
        "link": "https://pypi.org/project/psycopg2",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://stackoverflow.com/questions/43317376/how-can-i-use-psycopg2-extras-in-sqlalchemy",
        "document": "I want to upload a huge number of entries (~600k) into a simple table in a PostgreSQL DB, with one foreign key, a timestamp and 3 float per each entry. However, it takes 60 ms per each entry to execute the core bulk insert described here, thus the whole execution would take 10 h. I have found out, that it is a performance issue of method, however it has been solved with the method in psycopg2 2.7.\n\nThe code I run is the following:\n\nI see that it is a common problem, however I have not managed to find a solution in sqlalchemy itself. Is there any way to tell sqlalchemy to call in some occasions? Is there any other way to implement huge inserts without constructing the SQL statements by myself?\n\nThanks for the help!"
    },
    {
        "link": "https://access.crunchydata.com/documentation/psycopg3/3.1.9/pdf/psycopg3.pdf",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/8134602/psycopg2-insert-multiple-rows-with-one-query",
        "document": "but I want some simpler way.\n\nThe only way I know is\n\nI need to insert multiple rows with one query (number of rows is not constant), so I need to execute query like this one:\n\nThe pythonic way of doing it in Psycopg 2.6: Explanation: If the data to be inserted is given as a list of tuples like in then it is already in the exact required format as\n• None the syntax of the clause expects a list of records as in The only necessary work is to provide a records list template to be filled by psycopg # We use the data list to be sure of the template length records_list_template = ','.join(['%s'] * len(data)) and place it in the query Now to the usual arguments substitution Or just testing what will be sent to the server\n\nThe classic is about 60 times slower than @ant32 's implementation (called \"folded\") as explained in this thread: https://www.postgresql.org/message-id/20170130215151.GA7081%40deb76.aryehleib.com This implementation was added to psycopg2 in version 2.7 and is called : To insert multiple rows, using the multirow syntax with is about 10x faster than using psycopg2 . Indeed, just runs many individual statements. @ant32 's code works perfectly in Python 2. But in Python 3, returns bytes, takes either bytes or strings, and expects instance. So in Python 3 you may need to modify @ant32 's code, by adding : args_str = ','.join(cur.mogrify(\"(%s,%s,%s,%s,%s,%s,%s,%s,%s)\", x).decode('utf-8') for x in tup) cur.execute(\"INSERT INTO table VALUES \" + args_str) Or by using bytes (with or ) only: args_bytes = b','.join(cur.mogrify(\"(%s,%s,%s,%s,%s,%s,%s,%s,%s)\", x) for x in tup) cur.execute(b\"INSERT INTO table VALUES \" + args_bytes)\n\ncursor.copy_from is the fastest solution I've found for bulk inserts by far. Here's a gist I made containing a class named IteratorFile which allows an iterator yielding strings to be read like a file. We can convert each input record to a string using a generator expression. So the solution would be For this trivial size of args it won't make much of a speed difference, but I see big speedups when dealing with thousands+ of rows. It will also be more memory efficient than building a giant query string. An iterator would only ever hold one input record in memory at a time, where at some point you'll run out of memory in your Python process or in Postgres by building the query string.\n\nAs of 2022-11-16, the answers by @Clodoaldo Neto (for Psycopg 2.6), @Joseph Sheedy, @J.J, @Bart Jonk, @kevo Njoki, @TKoutny and @Nihal Sharma contain SQL injection vulnerabilities and should not be used. The fastest proposal so far ( ) should not be used either because it is difficult to escape the data correctly. This is easily apparent when trying to insert characters like , , , , or . The author of psycopg2 also recommends against : copy_from() and copy_to() are really just ancient and incomplete methods The fastest method is , which can insert data straight from CSV files. with open(\"mydata.csv\") as f: cursor.copy_expert(\"COPY mytable (my_id, a, b) FROM STDIN WITH csv\", f) is also the fastest method when generating the CSV file on-the-fly. For reference, see the following class, which takes care to limit memory usage. import io, csv class CSVFile(io.TextIOBase): # Create a CSV file from rows. Can only be read once. def __init__(self, rows, size=8192): self.row_iter = iter(rows) self.buf = io.StringIO() self.available = 0 self.size = size def read(self, n): # Buffer new CSV rows until enough data is available buf = self.buf writer = csv.writer(buf) while self.available < n: try: row_length = writer.writerow(next(self.row_iter)) self.available += row_length self.size = max(self.size, row_length) except StopIteration: break # Read requested amount of data from buffer write_pos = buf.tell() read_pos = write_pos - self.available buf.seek(read_pos) data = buf.read(n) self.available -= len(data) # Shrink buffer if it grew very large if read_pos > 2 * self.size: remaining = buf.read() buf.seek(0) buf.write(remaining) buf.truncate() else: buf.seek(write_pos) return data This class can then be used like: rows = [(1, \"a\", \"b\"), (2, \"c\", \"d\")] cursor.copy_expert(\"COPY mytable (my_id, a, b) FROM STDIN WITH csv\", CSVFile(rows)) If all your data fits into memory, you can also generate the entire CSV data directly without the class, but if you do not know how much data you are going to insert in the future, you probably should not do that. f = io.StringIO() writer = csv.writer(f) for row in rows: writer.writerow(row) f.seek(0) cursor.copy_expert(\"COPY mytable (my_id, a, b) FROM STDIN WITH csv\", f)\n• 39 milliseconds - generating the entire CSV file at once\n\nAll of these techniques are called 'Extended Inserts\" in Postgres terminology, and as of the 24th of November 2016, it's still a ton faster than psychopg2's executemany() and all the other methods listed in this thread (which i tried before coming to this answer). Here's some code which doesnt use cur.mogrify and is nice and simply to get your head around: valueSQL = [ '%s', '%s', '%s', ... ] # as many as you have columns. sqlrows = [] rowsPerInsert = 3 # more means faster, but with diminishing returns.. for row in getSomeData: # row == [1, 'a', 'yolo', ... ] sqlrows += row if ( len(sqlrows)/len(valueSQL) ) % rowsPerInsert == 0: # sqlrows == [ 1, 'a', 'yolo', 2, 'b', 'swag', 3, 'c', 'selfie' ] insertSQL = 'INSERT INTO \"twitter\" VALUES ' + ','.join(['(' + ','.join(valueSQL) + ')']*rowsPerInsert) cur.execute(insertSQL, sqlrows) con.commit() sqlrows = [] insertSQL = 'INSERT INTO \"twitter\" VALUES ' + ','.join(['(' + ','.join(valueSQL) + ')']*len(sqlrows)) cur.execute(insertSQL, sqlrows) con.commit() But it should be noted that if you can use copy_from(), you should use copy_from ;)\n\nThe cursor.copyfrom solution as provided by @jopseph.sheedy (https://stackoverflow.com/users/958118/joseph-sheedy) above (https://stackoverflow.com/a/30721460/11100064) is indeed lightning fast. However, the example he gives are not generically usable for a record with any number of fields and it took me while to figure out how to use it correctly. The IteratorFile needs to be instantiated with tab-separated fields like this ( is a list of dicts where each dict is a record): To generalise for an arbitrary number of fields we will first create a line string with the correct amount of tabs and field placeholders : and then use to fill in the field values : :"
    },
    {
        "link": "https://stackoverflow.com/questions/75959463/how-to-insert-multiple-records-in-a-table-with-psycopg2-extras-execute-values",
        "document": "Two ways to deal with this.\n\nThen you won't have specify the id value in the query. If the sequence is dedicated to that table.column then also:\n\nThat will create a dependency so that if the table is dropped the sequence will be also.\n\nLeave the sequence freestanding and just pull the values as needed using procedures below.\n\nExample of how compares to over inserting 10000 values. This is done in using the cell magic."
    },
    {
        "link": "https://geeksforgeeks.org/python-psycopg2-insert-multiple-rows-with-one-query",
        "document": "This article is about inserting multiple rows in our table of a specified database with one query. There are multiple ways of executing this task, let’s see how we can do it from the below approaches.\n\nIn this method, we import the psycopg2 package and form a connection using the psycopg2.connect() method, we connect to the ‘Classroom’ database. after forming a connection we create a cursor using the connect().cursor() method, it’ll help us fetch rows. after that we execute the insert SQL statement, which is of the form :\n\nThe SQL statement is executed by cursor.execute() method. we fetch rows using the cursor.fetchall() method.\n\nThe code is the same as the previous example, but the difference is cursor.mogrify() method.\n\nAfter the arguments have been bound, return a query string. The string returned is the same as what would be sent to the database if you used the execute() method or anything similar. The string that is returned is always a bytes string and this is faster than executemany() method.\n\ncursor.mogrify() returns a bytes string but we want it to be in string format so we just need to decode the result of mogrify back to a string by using the decode(‘utf-8’) hack.\n\nThe approach of this example is the same as before but instead of using cursor.mogrify() we use cursor.executemany() method. executemany() is slower compared to mogrify() method.\n\nIt is used to Apply a database action (query or command) to all parameter tuples or mappings in the vars list sequence. The function is especially useful for database update instructions because it discards any result set produced by the query. The query’s parameters are bound using the same principles as the execute() function."
    },
    {
        "link": "https://datacareer.de/blog/improve-your-psycopg2-executions-for-postgresql-in-python",
        "document": "I’m sure everybody who worked with Python and a PostgreSQL database is familiar or definitely heard about the psycopg2 library. It is the most popular PostgreSQL database adapter for the Python programming language. In my work, I come in contact with this library every day and execute hundreds of automated statements. As always, I try to improve my code and execution speed, because, in the cloud, time is money. The longer your code runs the more it will cost you. So in this post, I will go over some ways on how you can decrease your database inserts and executions up to 10 times and if you run your code in the cloud, save you some money!\n\nWith Psycopg2 we have four ways to execute a command for a (large) list of items:\n\nNow let’s go over each of these methods and see how much time it takes to insert 10'000, 100'000, and 1'000'000 items. Before each run, we will truncate the table to make sure we are working under the same conditions.\n\nThe execute() method is the standard function to execute a database operation. To insert a large number of rows, we have to loop over each row in our dataset and call the execute method.\n\nAs we know, loops are quite slow, so it’s not surprising if we look at the results:\n\nAs we can see the increase in time is almost linear.\n\nPsycopg2 offers another execution method called executemany(). One would think that this would be some kind of improvement over the simple execute() method. But if we look at the official documentation a warning says otherwise: In its current implementation this method is not faster than executing in a loop. For better performance you can use the functions described in Fast execution helpers. As we can see this method is somehow a little bit faster than a simple execute() statement loop. But I think we can do better. Another approach offered by the Psycopg2 library is execute_batch(). It reduces the number of server roundtrips, improving the performance in contrast to the executemany() function. The method achieves this, by joining the statements together until the page_size is reached (usually 8kB in Postgres). Let’s check the performance. Look at that! We are already almost 4! times faster! In our last approach, we are going to build a custom string, which we will then run in a single execute() statement. @measure_time def method_string_building(self, values): argument_string = \",\".join(\"('%s', '%s')\" % (x, y) for (x, y) in values) self.cursor.execute(\"INSERT INTO {table} VALUES\".format(table=TABLE_NAME) + argument_string) self.connection.commit()\n\n\n\n As you can we successfully reduced the amount of time needed by another factor of 4! In comparison to the loop execute() method we reduced the insertion time for 10'000 rows by a factor of 13! Here is the final code used: import time import psycopg2 import psycopg2.extras TABLE_NAME = 'TestTable' def measure_time(func): def time_it(*args, **kwargs): time_started = time.time() func(*args, **kwargs) time_elapsed = time.time() print(\"{execute} running time is {sec} seconds for inserting {rows} rows.\".format(execute=func.__name__, sec=round( time_elapsed - time_started, 4), rows=len( kwargs.get('values')))) return time_it class PsycopgTest(): def __init__(self, num_rows): self.num_rows = num_rows def create_dummy_data(self): values = [] for i in range(self.num_rows): values.append((i + 1, 'test')) return values def connect(self): conn_string = \"host={0} user={1} dbname={2} password={3}\".format('localhost', 'postgres', 'psycopg2_test', '') self.connection = psycopg2.connect(conn_string) self.cursor = self.connection.cursor() def create_table(self): self.cursor.execute( \"CREATE TABLE IF NOT EXISTS {table} (id INT PRIMARY KEY, NAME text)\".format(table=TABLE_NAME)) self.connection.commit() def truncate_table(self): self.cursor.execute(\"TRUNCATE TABLE {table} RESTART IDENTITY\".format(table=TABLE_NAME)) self.connection.commit() @measure_time def method_execute(self, values): \"\"\"Loop over the dataset and insert every row separately\"\"\" for value in values: self.cursor.execute(\"INSERT INTO {table} VALUES (%s, %s)\".format(table=TABLE_NAME), value) self.connection.commit() @measure_time def method_execute_many(self, values): self.cursor.executemany(\"INSERT INTO {table} VALUES (%s, %s)\".format(table=TABLE_NAME), values) self.connection.commit() @measure_time def method_execute_batch(self, values): psycopg2.extras.execute_batch(self.cursor, \"INSERT INTO {table} VALUES (%s, %s)\".format(table=TABLE_NAME), values) self.connection.commit() @measure_time def method_string_building(self, values): argument_string = \",\".join(\"('%s', '%s')\" % (x, y) for (x, y) in values) self.cursor.execute(\"INSERT INTO {table} VALUES\".format(table=TABLE_NAME) + argument_string) self.connection.commit() def main(): psyco = PsycopgTest(10000) psyco.connect() values = psyco.create_dummy_data() psyco.create_table() psyco.truncate_table() psyco.method_execute(values=values) # psyco.method_execute_many(values=values) # psyco.method_execute_batch(values=values) # psyco.method_string_building(values=values) if __name__ == '__main__': main() \n\nAs we saw there are huge performance gaps in the different execution methods for Psycopg2. If you are working with small amounts of data, it won’t matter that much. But as the size of the data grows, it will definitely get more interesting to explore and use these alternative methods to speed up the process up to 13 times!"
    },
    {
        "link": "https://github.com/NaysanSaran/pandas2postgresql/blob/master/notebooks/Psycopg2_Bulk_Insert_execute_values.ipynb",
        "document": "To see all available qualifiers, see our documentation .\n\nSaved searches Use saved searches to filter your results more quickly\n\nWe read every piece of feedback, and take your input very seriously.\n\nYou signed in with another tab or window. Reload to refresh your session.\n\nYou signed out in another tab or window. Reload to refresh your session.\n\nYou switched accounts on another tab or window. Reload to refresh your session."
    }
]