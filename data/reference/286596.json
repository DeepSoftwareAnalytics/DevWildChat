[
    {
        "link": "https://spark.apache.org/docs/latest/rdd-programming-guide.html",
        "document": "\n• Where to Go from Here\n\nAt a high level, every Spark application consists of a driver program that runs the user’s function and executes various parallel operations on a cluster. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.\n\nA second abstraction in Spark is shared variables that can be used in parallel operations. By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: broadcast variables, which can be used to cache a value in memory on all nodes, and accumulators, which are variables that are only “added” to, such as counters and sums.\n\nThis guide shows each of these features in each of Spark’s supported languages. It is easiest to follow along with if you launch Spark’s interactive shell – either for the Scala shell or for the Python one.\n\nThe parameter is a name for your application to show on the cluster UI. is a Spark, Mesos or YARN cluster URL, or a special “local” string to run in local mode. In practice, when running on a cluster, you will not want to hardcode in the program, but rather launch the application with and receive it there. However, for local testing and unit tests, you can pass “local” to run Spark in-process.\n\nSpark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\n\nOne important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to (e.g. ). Note: some places in the code use the term slices (a synonym for partitions) to maintain backward compatibility.\n\nPySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat. Text file RDDs can be created using ’s method. This method takes a URI for the file (either a local path on the machine, or a , , etc URI) and reads it as a collection of lines. Here is an example invocation: Once created, can be acted on by dataset operations. For example, we can add up the sizes of all the lines using the and operations as follows: . Some notes on reading files with Spark:\n• If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.\n• All of Spark’s file-based input methods, including , support running on directories, compressed files, and wildcards as well. For example, you can use , , and .\n• The method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks. Apart from text files, Spark’s Python API also supports several other data formats:\n• lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with , which would return one record per line in each file.\n• and support saving an RDD in a simple format consisting of pickled Python objects. Batching is used on pickle serialization, with default batch size 10. Note this feature is currently marked and is intended for advanced users. It may be replaced in future with read/write support based on Spark SQL, in which case Spark SQL is the preferred approach. PySpark SequenceFile support loads an RDD of key-value pairs within Java, converts Writables to base Java types, and pickles the resulting Java objects using pickle. When saving an RDD of key-value pairs to SequenceFile, PySpark does the reverse. It unpickles Python objects into Java objects and then converts them to Writables. The following Writables are automatically converted: Arrays are not handled out-of-the-box. Users need to specify custom subtypes when reading or writing. When writing, users also need to specify custom converters that convert arrays to custom subtypes. When reading, the default converter will convert custom subtypes to Java , which then get pickled to Python tuples. To get Python for arrays of primitive types, users need to specify custom converters. Similarly to text files, SequenceFiles can be saved and loaded by specifying the path. The key and value classes can be specified, but for standard Writables this is not required. PySpark can also read any Hadoop InputFormat or write any Hadoop OutputFormat, for both ‘new’ and ‘old’ Hadoop MapReduce APIs. If required, a Hadoop configuration can be passed in as a Python dict. Here is an example using the Elasticsearch ESInputFormat: # the result is a MapWritable that is converted to a Python dict Note that, if the InputFormat simply depends on a Hadoop configuration and/or input path, and the key and value classes can easily be converted according to the above table, then this approach should work well for such cases. If you have custom serialized binary data (such as loading data from Cassandra / HBase), then you will first need to transform that data on the Scala/Java side to something which can be handled by pickle’s pickler. A Converter trait is provided for this. Simply extend this trait and implement your transformation code in the method. Remember to ensure that this class, along with any dependencies required to access your , are packaged into your Spark job jar and included on the PySpark classpath. See the Python examples and the Converter examples for examples of using Cassandra / HBase and with custom converters. Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat. Text file RDDs can be created using ’s method. This method takes a URI for the file (either a local path on the machine, or a , , etc URI) and reads it as a collection of lines. Here is an example invocation: Once created, can be acted on by dataset operations. For example, we can add up the sizes of all the lines using the and operations as follows: . Some notes on reading files with Spark:\n• If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.\n• All of Spark’s file-based input methods, including , support running on directories, compressed files, and wildcards as well. For example, you can use , , and . When multiple files are read, the order of the partitions depends on the order the files are returned from the filesystem. It may or may not, for example, follow the lexicographic ordering of the files by path. Within a partition, elements are ordered according to their order in the underlying file.\n• The method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks. Apart from text files, Spark’s Scala API also supports several other data formats:\n• lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with , which would return one record per line in each file. Partitioning is determined by data locality which, in some cases, may result in too few partitions. For those cases, provides an optional second argument for controlling the minimal number of partitions.\n• For SequenceFiles, use SparkContext’s method where and are the types of key and values in the file. These should be subclasses of Hadoop’s Writable interface, like IntWritable and Text. In addition, Spark allows you to specify native types for a few common Writables; for example, will automatically read IntWritables and Texts.\n• For other Hadoop InputFormats, you can use the method, which takes an arbitrary and input format class, key class and value class. Set these the same way you would for a Hadoop job with your input source. You can also use for InputFormats based on the “new” MapReduce API ( ).\n• and support saving an RDD in a simple format consisting of serialized Java objects. While this is not as efficient as specialized formats like Avro, it offers an easy way to save any RDD. Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat. Text file RDDs can be created using ’s method. This method takes a URI for the file (either a local path on the machine, or a , , etc URI) and reads it as a collection of lines. Here is an example invocation: Once created, can be acted on by dataset operations. For example, we can add up the sizes of all the lines using the and operations as follows: . Some notes on reading files with Spark:\n• If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.\n• All of Spark’s file-based input methods, including , support running on directories, compressed files, and wildcards as well. For example, you can use , , and .\n• The method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks. Apart from text files, Spark’s Java API also supports several other data formats:\n• lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with , which would return one record per line in each file.\n• For SequenceFiles, use SparkContext’s method where and are the types of key and values in the file. These should be subclasses of Hadoop’s Writable interface, like IntWritable and Text.\n• For other Hadoop InputFormats, you can use the method, which takes an arbitrary and input format class, key class and value class. Set these the same way you would for a Hadoop job with your input source. You can also use for InputFormats based on the “new” MapReduce API ( ).\n• and support saving an RDD in a simple format consisting of serialized Java objects. While this is not as efficient as specialized formats like Avro, it offers an easy way to save any RDD.\n\nRDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel that returns a distributed dataset).\n\nAll transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through will be used in a and return only the result of the to the driver, rather than the larger mapped dataset.\n\nBy default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the (or ) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.\n\nOne of the harder things about Spark is understanding the scope and life cycle of variables and methods when executing code across a cluster. RDD operations that modify variables outside of their scope can be a frequent source of confusion. In the example below we’ll look at code that uses to increment a counter, but similar issues can occur for other operations as well.\n\nConsider the naive RDD element sum below, which may behave differently depending on whether execution is happening within the same JVM. A common example of this is when running Spark in mode ( ) versus deploying a Spark application to a cluster (e.g. via spark-submit to YARN):\n\nThe behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s closure. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case ). This closure is serialized and sent to each executor.\n\nThe variables within the closure sent to each executor are now copies and thus, when counter is referenced within the function, it’s no longer the counter on the driver node. There is still a counter in the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of counter will still be zero since all operations on counter were referencing the value within the serialized closure.\n\nIn local mode, in some circumstances, the function will actually execute within the same JVM as the driver and will reference the same original counter, and may actually update it.\n\nTo ensure well-defined behavior in these sorts of scenarios one should use an . Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster. The Accumulators section of this guide discusses these in more detail.\n\nIn general, closures - constructs like loops or locally defined methods, should not be used to mutate some global state. Spark does not define or guarantee the behavior of mutations to objects referenced from outside of closures. Some code that does this may work in local mode, but that’s just by accident and such code will not behave as expected in distributed mode. Use an Accumulator instead if some global aggregation is needed.\n\nAnother common idiom is attempting to print out the elements of an RDD using or . On a single machine, this will generate the expected output and print all the RDD’s elements. However, in mode, the output to being called by the executors is now writing to the executor’s instead, not the one on the driver, so on the driver won’t show these! To print all elements on the driver, one can use the method to first bring the RDD to the driver node thus: . This can cause the driver to run out of memory, though, because fetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the : .\n\nThe following table lists some of the common transformations supported by Spark. Refer to the RDD API doc (Scala, Java, Python, R) and pair RDD functions doc (Scala, Java) for details.\n\nThe following table lists some of the common actions supported by Spark. Refer to the RDD API doc (Scala, Java, Python, R)\n\nThe Spark RDD API also exposes asynchronous versions of some actions, like for , which immediately return a to the caller instead of blocking on completion of the action. This can be used to manage or wait for the asynchronous execution of the action.\n\nCertain operations within Spark trigger an event known as the shuffle. The shuffle is Spark’s mechanism for re-distributing data so that it’s grouped differently across partitions. This typically involves copying data across executors and machines, making the shuffle a complex and costly operation.\n\nTo understand what happens during the shuffle, we can consider the example of the operation. The operation generates a new RDD where all values for a single key are combined into a tuple - the key and the result of executing a reduce function against all values associated with that key. The challenge is that not all values for a single key necessarily reside on the same partition, or even the same machine, but they must be co-located to compute the result.\n\nIn Spark, data is generally not distributed across partitions to be in the necessary place for a specific operation. During computations, a single task will operate on a single partition - thus, to organize all the data for a single reduce task to execute, Spark needs to perform an all-to-all operation. It must read from all partitions to find all the values for all keys, and then bring together values across partitions to compute the final result for each key - this is called the shuffle.\n\nAlthough the set of elements in each partition of newly shuffled data will be deterministic, and so is the ordering of partitions themselves, the ordering of these elements is not. If one desires predictably ordered data following shuffle then it’s possible to use:\n• to sort each partition using, for example,\n\nOperations which can cause a shuffle include repartition operations like and , ‘ByKey operations (except for counting) like and , and join operations like and .\n\nThe Shuffle is an expensive operation since it involves disk I/O, data serialization, and network I/O. To organize data for the shuffle, Spark generates sets of tasks - map tasks to organize the data, and a set of reduce tasks to aggregate it. This nomenclature comes from MapReduce and does not directly relate to Spark’s and operations.\n\nInternally, results from individual map tasks are kept in memory until they can’t fit. Then, these are sorted based on the target partition and written to a single file. On the reduce side, tasks read the relevant sorted blocks.\n\nCertain shuffle operations can consume significant amounts of heap memory since they employ in-memory data structures to organize records before or after transferring them. Specifically, and create these structures on the map side, and operations generate these on the reduce side. When data does not fit in memory Spark will spill these tables to disk, incurring the additional overhead of disk I/O and increased garbage collection.\n\nShuffle also generates a large number of intermediate files on disk. As of Spark 1.3, these files are preserved until the corresponding RDDs are no longer used and are garbage collected. This is done so the shuffle files don’t need to be re-created if the lineage is re-computed. Garbage collection may happen only after a long period of time, if the application retains references to these RDDs or if GC does not kick in frequently. This means that long-running Spark jobs may consume a large amount of disk space. The temporary storage directory is specified by the configuration parameter when configuring the Spark context.\n\nShuffle behavior can be tuned by adjusting a variety of configuration parameters. See the ‘Shuffle Behavior’ section within the Spark Configuration Guide.\n\nOne of the most important capabilities in Spark is persisting (or caching) a dataset in memory across operations. When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use.\n\nYou can mark an RDD to be persisted using the or methods on it. The first time it is computed in an action, it will be kept in memory on the nodes. Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.\n\nIn addition, each persisted RDD can be stored using a different storage level, allowing you, for example, to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space), replicate it across nodes. These levels are set by passing a object (Scala, Java, Python) to . The method is a shorthand for using the default storage level, which is (store deserialized objects in memory). The full set of storage levels is:\n\nNote: In Python, stored objects will always be serialized with the Pickle library, so it does not matter whether you choose a serialized level. The available storage levels in Python include , , , , , , and .\n\nSpark also automatically persists some intermediate data in shuffle operations (e.g. ), even without users calling . This is done to avoid recomputing the entire input if a node fails during the shuffle. We still recommend users call on the resulting RDD if they plan to reuse it.\n\nSpark’s storage levels are meant to provide different trade-offs between memory usage and CPU efficiency. We recommend going through the following process to select one:\n• If your RDDs fit comfortably with the default storage level ( ), leave them that way. This is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible.\n• If not, try using and selecting a fast serialization library to make the objects much more space-efficient, but still reasonably fast to access. (Java and Scala)\n• Don’t spill to disk unless the functions that computed your datasets are expensive, or they filter a large amount of the data. Otherwise, recomputing a partition may be as fast as reading it from disk.\n• Use the replicated storage levels if you want fast fault recovery (e.g. if using Spark to serve requests from a web application). All the storage levels provide full fault tolerance by recomputing lost data, but the replicated ones let you continue running tasks on the RDD without waiting to recompute a lost partition.\n\nSpark automatically monitors cache usage on each node and drops out old data partitions in a least-recently-used (LRU) fashion. If you would like to manually remove an RDD instead of waiting for it to fall out of the cache, use the method. Note that this method does not block by default. To block until resources are freed, specify when calling this method.\n\nNormally, when a function passed to a Spark operation (such as or ) is executed on a remote cluster node, it works on separate copies of all the variables used in the function. These variables are copied to each machine, and no updates to the variables on the remote machine are propagated back to the driver program. Supporting general, read-write shared variables across tasks would be inefficient. However, Spark does provide two limited types of shared variables for two common usage patterns: broadcast variables and accumulators.\n\nBroadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.\n\nSpark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage. The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.\n\nBroadcast variables are created from a variable by calling . The broadcast variable is a wrapper around , and its value can be accessed by calling the method. The code below shows this:\n\nAfter the broadcast variable is created, it should be used instead of the value in any functions run on the cluster so that is not shipped to the nodes more than once. In addition, the object should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later).\n\nTo release the resources that the broadcast variable copied onto executors, call . If the broadcast is used again afterwards, it will be re-broadcast. To permanently release all resources used by the broadcast variable, call . The broadcast variable can’t be used after that. Note that these methods do not block by default. To block until resources are freed, specify when calling them.\n\nAccumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.\n\nAs a user, you can create named or unnamed accumulators. As seen in the image below, a named accumulator (in this instance ) will display in the web UI for the stage that modifies that accumulator. Spark displays the value for each accumulator modified by a task in the “Tasks” table.\n\nTracking accumulators in the UI can be useful for understanding the progress of running stages (NOTE: this is not yet supported in Python).\n\nFor accumulator updates performed inside actions only, Spark guarantees that each task’s update to the accumulator will only be applied once, i.e. restarted tasks will not update the value. In transformations, users should be aware of that each task’s update may be applied more than once if tasks or job stages are re-executed.\n\nAccumulators do not change the lazy evaluation model of Spark. If they are being updated within an operation on an RDD, their value is only updated once that RDD is computed as part of an action. Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like . The below code fragment demonstrates this property:\n\nThe application submission guide describes how to submit applications to a cluster. In short, once you package your application into a JAR (for Java/Scala) or a set of or files (for Python), the script lets you submit it to any supported cluster manager.\n\nThe org.apache.spark.launcher package provides classes for launching Spark jobs as child processes using a simple Java API.\n\nSpark is friendly to unit testing with any popular unit test framework. Simply create a in your test with the master URL set to , run your operations, and then call to tear it down. Make sure you stop the context within a block or the test framework’s method, as Spark does not support two contexts running concurrently in the same program.\n\nWhere to Go from Here\n\nYou can see some example Spark programs on the Spark website. In addition, Spark includes several samples in the directory (Scala, Java, Python, R). You can run Java and Scala examples by passing the class name to Spark’s script; for instance:\n\nFor Python examples, use instead:\n\nFor R examples, use instead:\n\nFor help on optimizing your programs, the configuration and tuning guides provide information on best practices. They are especially important for making sure that your data is stored in memory in an efficient format. For help on deploying, the cluster mode overview describes the components involved in distributed operation and supported cluster managers.\n\nFinally, full API documentation is available in Scala, Java, Python and R."
    },
    {
        "link": "https://spark.apache.org/docs/latest/api/python/reference/index.html",
        "document": ""
    },
    {
        "link": "https://docs.databricks.com/aws/en/reference/spark",
        "document": "Databricks is built on top of Apache Spark , a unified analytics engine for big data and machine learning. For more information, see Apache Spark on Databricks.\n\nApache Spark has DataFrame APIs for operating on large datasets, which include over 100 operators, in several languages.\n• PySpark APIs for Python developers. See Tutorial: Load and transform data using Apache Spark DataFrames. Key classes include:\n• SparkSession - The entry point to programming Spark with the Dataset and DataFrame API.\n• DataFrame - A distributed collection of data grouped into named columns. See DataFrames and DataFrame-based MLlib.\n• (Deprecated) SparkR APIs for R developers. Key classes include:\n• SparkSession - SparkSession is the entry point into SparkR. See Starting Point: SparkSession.\n• SparkDataFrame - A distributed collection of data grouped into named columns. See Datasets and DataFrames, Creating DataFrames, and Creating SparkDataFrames.\n• Scala APIs for Scala developers. Key classes include:\n• SparkSession - The entry point to programming Spark with the Dataset and DataFrame API. See Starting Point: SparkSession.\n• Dataset - A strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations. Each also has an untyped view called a DataFrame, which is a of Row. See Datasets and DataFrames, Creating Datasets, Creating DataFrames, and DataFrame functions.\n• Java APIs for Java developers. Key classes include:\n• SparkSession - The entry point to programming Spark with the Dataset and DataFrame API. See Starting Point: SparkSession.\n• Dataset - A strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations. Each also has an untyped view called a DataFrame, which is a of Row. See Datasets and DataFrames, Creating Datasets, Creating DataFrames, and DataFrame functions.\n\nTo learn how to use the Apache Spark APIs on Databricks , see:\n• For Java, you can run Java code as a JAR job."
    },
    {
        "link": "https://stackoverflow.com/questions/61211403/using-the-apache-spark-rdd-map-method-java-api-to-produce-a-non-columnar-resul",
        "document": "Please note: I believe I'm correct in trying to use the RDD method here, but if there is another way to accomplish what I'm looking for, I'm all ears!\n\nBrand new to Spark 2.4.x here, and using the Java (not Scala) API.\n\nI'm trying to wrap my brain around the RDD method, specifically on and not restricted only to RDDs. The canonical example of its use from the official docs is:\n\nSo it seems that, in this case, that after the RDD is created, it has a single column (whose name I'm unsure of) where each column value is a different line of the file, and that each row in the RDD represents a different line of the file as well. Meaning, is an matrix where is the number of rows/lines in the file.\n\nIt also seems that when the function is executed, it is fed each row's one-and-only column as the input string and returns an integer representing the line length of that string as a new column value in a different dataset, which is also (just holding line length info instead of the actual lines/strings).\n\nOK, so I get that trivial example. But what if we have datasets, meaning, lots of rows and lots of columns, and we want to write functions that transform them into other datasets?\n\nFor example, let's say I have the following \"input\" dataset:\n\nWhere is a numeric/floating-point type and both and are strings. So here we have an shaped dataset; rows and always 3 columns in each row.\n\nHow would I write a map function that also returned an dataset, with the same columns/column names/schema, but different values (based on the function)?\n\nFor instance, say I wanted a new dataset with the same schema, but that added to the column if the row's value equals the string ?\n\nHence, using the arbitrary dataset above, the new dataset coming out of this map function would look like:\n\nI'm tempted to do something like:\n\nHowever, several issues here:\n• is now only an dataset consisting of the correctly-computed prices for each row in , whereas I want something that has the same that looks like the 2nd arbitrary dataset up above\n• How does the know that its first string argument is the column, and not the column?\n• The Function API seems to only go up to either or (it's hard to discern what is available to the Java API and what is exclusive to the Scala API). This means I can only write functions that take in 5 or 6 arguments, whereas I might have datasets with 10+ columns in them, and I might very well need most of those column values \"injected\" into the function so I can compute the return value of the new dataset. What options do I have available in this case?"
    },
    {
        "link": "https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/index.html",
        "document": ""
    },
    {
        "link": "https://spark.apache.org/docs/latest/rdd-programming-guide.html",
        "document": "\n• Where to Go from Here\n\nAt a high level, every Spark application consists of a driver program that runs the user’s function and executes various parallel operations on a cluster. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.\n\nA second abstraction in Spark is shared variables that can be used in parallel operations. By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: broadcast variables, which can be used to cache a value in memory on all nodes, and accumulators, which are variables that are only “added” to, such as counters and sums.\n\nThis guide shows each of these features in each of Spark’s supported languages. It is easiest to follow along with if you launch Spark’s interactive shell – either for the Scala shell or for the Python one.\n\nThe parameter is a name for your application to show on the cluster UI. is a Spark, Mesos or YARN cluster URL, or a special “local” string to run in local mode. In practice, when running on a cluster, you will not want to hardcode in the program, but rather launch the application with and receive it there. However, for local testing and unit tests, you can pass “local” to run Spark in-process.\n\nSpark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\n\nOne important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to (e.g. ). Note: some places in the code use the term slices (a synonym for partitions) to maintain backward compatibility.\n\nPySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat. Text file RDDs can be created using ’s method. This method takes a URI for the file (either a local path on the machine, or a , , etc URI) and reads it as a collection of lines. Here is an example invocation: Once created, can be acted on by dataset operations. For example, we can add up the sizes of all the lines using the and operations as follows: . Some notes on reading files with Spark:\n• If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.\n• All of Spark’s file-based input methods, including , support running on directories, compressed files, and wildcards as well. For example, you can use , , and .\n• The method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks. Apart from text files, Spark’s Python API also supports several other data formats:\n• lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with , which would return one record per line in each file.\n• and support saving an RDD in a simple format consisting of pickled Python objects. Batching is used on pickle serialization, with default batch size 10. Note this feature is currently marked and is intended for advanced users. It may be replaced in future with read/write support based on Spark SQL, in which case Spark SQL is the preferred approach. PySpark SequenceFile support loads an RDD of key-value pairs within Java, converts Writables to base Java types, and pickles the resulting Java objects using pickle. When saving an RDD of key-value pairs to SequenceFile, PySpark does the reverse. It unpickles Python objects into Java objects and then converts them to Writables. The following Writables are automatically converted: Arrays are not handled out-of-the-box. Users need to specify custom subtypes when reading or writing. When writing, users also need to specify custom converters that convert arrays to custom subtypes. When reading, the default converter will convert custom subtypes to Java , which then get pickled to Python tuples. To get Python for arrays of primitive types, users need to specify custom converters. Similarly to text files, SequenceFiles can be saved and loaded by specifying the path. The key and value classes can be specified, but for standard Writables this is not required. PySpark can also read any Hadoop InputFormat or write any Hadoop OutputFormat, for both ‘new’ and ‘old’ Hadoop MapReduce APIs. If required, a Hadoop configuration can be passed in as a Python dict. Here is an example using the Elasticsearch ESInputFormat: # the result is a MapWritable that is converted to a Python dict Note that, if the InputFormat simply depends on a Hadoop configuration and/or input path, and the key and value classes can easily be converted according to the above table, then this approach should work well for such cases. If you have custom serialized binary data (such as loading data from Cassandra / HBase), then you will first need to transform that data on the Scala/Java side to something which can be handled by pickle’s pickler. A Converter trait is provided for this. Simply extend this trait and implement your transformation code in the method. Remember to ensure that this class, along with any dependencies required to access your , are packaged into your Spark job jar and included on the PySpark classpath. See the Python examples and the Converter examples for examples of using Cassandra / HBase and with custom converters. Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat. Text file RDDs can be created using ’s method. This method takes a URI for the file (either a local path on the machine, or a , , etc URI) and reads it as a collection of lines. Here is an example invocation: Once created, can be acted on by dataset operations. For example, we can add up the sizes of all the lines using the and operations as follows: . Some notes on reading files with Spark:\n• If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.\n• All of Spark’s file-based input methods, including , support running on directories, compressed files, and wildcards as well. For example, you can use , , and . When multiple files are read, the order of the partitions depends on the order the files are returned from the filesystem. It may or may not, for example, follow the lexicographic ordering of the files by path. Within a partition, elements are ordered according to their order in the underlying file.\n• The method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks. Apart from text files, Spark’s Scala API also supports several other data formats:\n• lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with , which would return one record per line in each file. Partitioning is determined by data locality which, in some cases, may result in too few partitions. For those cases, provides an optional second argument for controlling the minimal number of partitions.\n• For SequenceFiles, use SparkContext’s method where and are the types of key and values in the file. These should be subclasses of Hadoop’s Writable interface, like IntWritable and Text. In addition, Spark allows you to specify native types for a few common Writables; for example, will automatically read IntWritables and Texts.\n• For other Hadoop InputFormats, you can use the method, which takes an arbitrary and input format class, key class and value class. Set these the same way you would for a Hadoop job with your input source. You can also use for InputFormats based on the “new” MapReduce API ( ).\n• and support saving an RDD in a simple format consisting of serialized Java objects. While this is not as efficient as specialized formats like Avro, it offers an easy way to save any RDD. Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat. Text file RDDs can be created using ’s method. This method takes a URI for the file (either a local path on the machine, or a , , etc URI) and reads it as a collection of lines. Here is an example invocation: Once created, can be acted on by dataset operations. For example, we can add up the sizes of all the lines using the and operations as follows: . Some notes on reading files with Spark:\n• If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.\n• All of Spark’s file-based input methods, including , support running on directories, compressed files, and wildcards as well. For example, you can use , , and .\n• The method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks. Apart from text files, Spark’s Java API also supports several other data formats:\n• lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with , which would return one record per line in each file.\n• For SequenceFiles, use SparkContext’s method where and are the types of key and values in the file. These should be subclasses of Hadoop’s Writable interface, like IntWritable and Text.\n• For other Hadoop InputFormats, you can use the method, which takes an arbitrary and input format class, key class and value class. Set these the same way you would for a Hadoop job with your input source. You can also use for InputFormats based on the “new” MapReduce API ( ).\n• and support saving an RDD in a simple format consisting of serialized Java objects. While this is not as efficient as specialized formats like Avro, it offers an easy way to save any RDD.\n\nRDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel that returns a distributed dataset).\n\nAll transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through will be used in a and return only the result of the to the driver, rather than the larger mapped dataset.\n\nBy default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the (or ) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.\n\nOne of the harder things about Spark is understanding the scope and life cycle of variables and methods when executing code across a cluster. RDD operations that modify variables outside of their scope can be a frequent source of confusion. In the example below we’ll look at code that uses to increment a counter, but similar issues can occur for other operations as well.\n\nConsider the naive RDD element sum below, which may behave differently depending on whether execution is happening within the same JVM. A common example of this is when running Spark in mode ( ) versus deploying a Spark application to a cluster (e.g. via spark-submit to YARN):\n\nThe behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s closure. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case ). This closure is serialized and sent to each executor.\n\nThe variables within the closure sent to each executor are now copies and thus, when counter is referenced within the function, it’s no longer the counter on the driver node. There is still a counter in the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of counter will still be zero since all operations on counter were referencing the value within the serialized closure.\n\nIn local mode, in some circumstances, the function will actually execute within the same JVM as the driver and will reference the same original counter, and may actually update it.\n\nTo ensure well-defined behavior in these sorts of scenarios one should use an . Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster. The Accumulators section of this guide discusses these in more detail.\n\nIn general, closures - constructs like loops or locally defined methods, should not be used to mutate some global state. Spark does not define or guarantee the behavior of mutations to objects referenced from outside of closures. Some code that does this may work in local mode, but that’s just by accident and such code will not behave as expected in distributed mode. Use an Accumulator instead if some global aggregation is needed.\n\nAnother common idiom is attempting to print out the elements of an RDD using or . On a single machine, this will generate the expected output and print all the RDD’s elements. However, in mode, the output to being called by the executors is now writing to the executor’s instead, not the one on the driver, so on the driver won’t show these! To print all elements on the driver, one can use the method to first bring the RDD to the driver node thus: . This can cause the driver to run out of memory, though, because fetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the : .\n\nThe following table lists some of the common transformations supported by Spark. Refer to the RDD API doc (Scala, Java, Python, R) and pair RDD functions doc (Scala, Java) for details.\n\nThe following table lists some of the common actions supported by Spark. Refer to the RDD API doc (Scala, Java, Python, R)\n\nThe Spark RDD API also exposes asynchronous versions of some actions, like for , which immediately return a to the caller instead of blocking on completion of the action. This can be used to manage or wait for the asynchronous execution of the action.\n\nCertain operations within Spark trigger an event known as the shuffle. The shuffle is Spark’s mechanism for re-distributing data so that it’s grouped differently across partitions. This typically involves copying data across executors and machines, making the shuffle a complex and costly operation.\n\nTo understand what happens during the shuffle, we can consider the example of the operation. The operation generates a new RDD where all values for a single key are combined into a tuple - the key and the result of executing a reduce function against all values associated with that key. The challenge is that not all values for a single key necessarily reside on the same partition, or even the same machine, but they must be co-located to compute the result.\n\nIn Spark, data is generally not distributed across partitions to be in the necessary place for a specific operation. During computations, a single task will operate on a single partition - thus, to organize all the data for a single reduce task to execute, Spark needs to perform an all-to-all operation. It must read from all partitions to find all the values for all keys, and then bring together values across partitions to compute the final result for each key - this is called the shuffle.\n\nAlthough the set of elements in each partition of newly shuffled data will be deterministic, and so is the ordering of partitions themselves, the ordering of these elements is not. If one desires predictably ordered data following shuffle then it’s possible to use:\n• to sort each partition using, for example,\n\nOperations which can cause a shuffle include repartition operations like and , ‘ByKey operations (except for counting) like and , and join operations like and .\n\nThe Shuffle is an expensive operation since it involves disk I/O, data serialization, and network I/O. To organize data for the shuffle, Spark generates sets of tasks - map tasks to organize the data, and a set of reduce tasks to aggregate it. This nomenclature comes from MapReduce and does not directly relate to Spark’s and operations.\n\nInternally, results from individual map tasks are kept in memory until they can’t fit. Then, these are sorted based on the target partition and written to a single file. On the reduce side, tasks read the relevant sorted blocks.\n\nCertain shuffle operations can consume significant amounts of heap memory since they employ in-memory data structures to organize records before or after transferring them. Specifically, and create these structures on the map side, and operations generate these on the reduce side. When data does not fit in memory Spark will spill these tables to disk, incurring the additional overhead of disk I/O and increased garbage collection.\n\nShuffle also generates a large number of intermediate files on disk. As of Spark 1.3, these files are preserved until the corresponding RDDs are no longer used and are garbage collected. This is done so the shuffle files don’t need to be re-created if the lineage is re-computed. Garbage collection may happen only after a long period of time, if the application retains references to these RDDs or if GC does not kick in frequently. This means that long-running Spark jobs may consume a large amount of disk space. The temporary storage directory is specified by the configuration parameter when configuring the Spark context.\n\nShuffle behavior can be tuned by adjusting a variety of configuration parameters. See the ‘Shuffle Behavior’ section within the Spark Configuration Guide.\n\nOne of the most important capabilities in Spark is persisting (or caching) a dataset in memory across operations. When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use.\n\nYou can mark an RDD to be persisted using the or methods on it. The first time it is computed in an action, it will be kept in memory on the nodes. Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.\n\nIn addition, each persisted RDD can be stored using a different storage level, allowing you, for example, to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space), replicate it across nodes. These levels are set by passing a object (Scala, Java, Python) to . The method is a shorthand for using the default storage level, which is (store deserialized objects in memory). The full set of storage levels is:\n\nNote: In Python, stored objects will always be serialized with the Pickle library, so it does not matter whether you choose a serialized level. The available storage levels in Python include , , , , , , and .\n\nSpark also automatically persists some intermediate data in shuffle operations (e.g. ), even without users calling . This is done to avoid recomputing the entire input if a node fails during the shuffle. We still recommend users call on the resulting RDD if they plan to reuse it.\n\nSpark’s storage levels are meant to provide different trade-offs between memory usage and CPU efficiency. We recommend going through the following process to select one:\n• If your RDDs fit comfortably with the default storage level ( ), leave them that way. This is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible.\n• If not, try using and selecting a fast serialization library to make the objects much more space-efficient, but still reasonably fast to access. (Java and Scala)\n• Don’t spill to disk unless the functions that computed your datasets are expensive, or they filter a large amount of the data. Otherwise, recomputing a partition may be as fast as reading it from disk.\n• Use the replicated storage levels if you want fast fault recovery (e.g. if using Spark to serve requests from a web application). All the storage levels provide full fault tolerance by recomputing lost data, but the replicated ones let you continue running tasks on the RDD without waiting to recompute a lost partition.\n\nSpark automatically monitors cache usage on each node and drops out old data partitions in a least-recently-used (LRU) fashion. If you would like to manually remove an RDD instead of waiting for it to fall out of the cache, use the method. Note that this method does not block by default. To block until resources are freed, specify when calling this method.\n\nNormally, when a function passed to a Spark operation (such as or ) is executed on a remote cluster node, it works on separate copies of all the variables used in the function. These variables are copied to each machine, and no updates to the variables on the remote machine are propagated back to the driver program. Supporting general, read-write shared variables across tasks would be inefficient. However, Spark does provide two limited types of shared variables for two common usage patterns: broadcast variables and accumulators.\n\nBroadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.\n\nSpark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage. The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.\n\nBroadcast variables are created from a variable by calling . The broadcast variable is a wrapper around , and its value can be accessed by calling the method. The code below shows this:\n\nAfter the broadcast variable is created, it should be used instead of the value in any functions run on the cluster so that is not shipped to the nodes more than once. In addition, the object should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later).\n\nTo release the resources that the broadcast variable copied onto executors, call . If the broadcast is used again afterwards, it will be re-broadcast. To permanently release all resources used by the broadcast variable, call . The broadcast variable can’t be used after that. Note that these methods do not block by default. To block until resources are freed, specify when calling them.\n\nAccumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.\n\nAs a user, you can create named or unnamed accumulators. As seen in the image below, a named accumulator (in this instance ) will display in the web UI for the stage that modifies that accumulator. Spark displays the value for each accumulator modified by a task in the “Tasks” table.\n\nTracking accumulators in the UI can be useful for understanding the progress of running stages (NOTE: this is not yet supported in Python).\n\nFor accumulator updates performed inside actions only, Spark guarantees that each task’s update to the accumulator will only be applied once, i.e. restarted tasks will not update the value. In transformations, users should be aware of that each task’s update may be applied more than once if tasks or job stages are re-executed.\n\nAccumulators do not change the lazy evaluation model of Spark. If they are being updated within an operation on an RDD, their value is only updated once that RDD is computed as part of an action. Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like . The below code fragment demonstrates this property:\n\nThe application submission guide describes how to submit applications to a cluster. In short, once you package your application into a JAR (for Java/Scala) or a set of or files (for Python), the script lets you submit it to any supported cluster manager.\n\nThe org.apache.spark.launcher package provides classes for launching Spark jobs as child processes using a simple Java API.\n\nSpark is friendly to unit testing with any popular unit test framework. Simply create a in your test with the master URL set to , run your operations, and then call to tear it down. Make sure you stop the context within a block or the test framework’s method, as Spark does not support two contexts running concurrently in the same program.\n\nWhere to Go from Here\n\nYou can see some example Spark programs on the Spark website. In addition, Spark includes several samples in the directory (Scala, Java, Python, R). You can run Java and Scala examples by passing the class name to Spark’s script; for instance:\n\nFor Python examples, use instead:\n\nFor R examples, use instead:\n\nFor help on optimizing your programs, the configuration and tuning guides provide information on best practices. They are especially important for making sure that your data is stored in memory in an efficient format. For help on deploying, the cluster mode overview describes the components involved in distributed operation and supported cluster managers.\n\nFinally, full API documentation is available in Scala, Java, Python and R."
    },
    {
        "link": "https://bootcampai.medium.com/best-practices-for-optimizing-data-processing-at-scale-with-apache-spark-7cb046939ae0",
        "document": "Large-scale data analysis has become a transformative tool for many industries, with applications that include fraud detection for the banking industry, clinical research for healthcare, and predictive maintenance and quality control for manufacturing. However, processing such vast amounts of data can be a challenge, even with the power of modern computing hardware. Many tools are now available to address the challenge, with one of the most popular being Apache Spark, an open source analytics engine designed to speed up the processing of very large data sets.\n\nSpark provides a powerful architecture capable of handling immense amounts of data. There are several Spark optimization techniques that streamline processes and data handling, including performing tasks in memory and storing frequently accessed data in a cache, thus reducing latency during retrieval. Spark is also designed for scalability; data processing can be distributed across multiple computers, increasing the available computing power. Spark is relevant to many projects: It supports a variety of programming languages (e.g., Java, Scala, R, and Python) and includes various libraries (e.g., MLlib for machine learning, GraphX for working with graphs, and Spark Streaming for processing streaming data).\n\nWhile Spark’s default settings provide a good starting point, there are several adjustments that can enhance its performance — thus allowing many businesses to use it to its full potential. There are two areas to consider when thinking about optimization techniques in Spark: computation efficiency and optimizing the communication between nodes.\n\nBefore discussing optimization techniques in detail, it’s helpful to look at how Spark handles data. The fundamental data structure in Spark is the resilient distributed data set, or RDD. Understanding how RDDs work is key when considering how to use Apache Spark. An RDD represents a fault-tolerant, distributed collection of data capable of being processed in parallel across a cluster of computers. RDDs are immutable; their contents cannot be changed once they are created.\n\nSpark’s fast processing speeds are enabled by RDDs. While many frameworks rely on external storage systems such as a Hadoop Distributed File System (HDFS) for reusing and sharing data between computations, RDDs support in-memory computation. Performing processing and data sharing in memory avoids the substantial overhead caused by replication, serialization, and disk read/write operations, not to mention network latency, when using an external storage system. Spark is often seen as a successor to MapReduce, the data processing component of Hadoop, an earlier framework from Apache. While the two systems share similar functionality, Spark’s in-memory processing allows it to run up to 100 times faster than MapReduce, which processes data on disk.\n\nTo work with the data in an RDD, Spark provides a rich set of transformations and actions. Transformations produce new RDDs from the data in existing ones using operations such as , , or . The function creates a new RDD with elements that satisfy a given condition, while creates a new RDD by combining two existing RDDs based on a common key. is used to apply a transformation to each element in a data set, for example, applying a mathematical operation such as calculating a percentage to every record in an RDD, outputting the results in a new RDD. An action, on the other hand, does not create a new RDD, but returns the result of a computation on the data set. Actions include operations such as , , or . The action returns the number of elements in an RDD, while returns just the first element. simply retrieves all of the elements in an RDD.\n\nTransformations further differ from actions in that they are lazy. The execution of transformations is not immediate. Instead, Spark keeps track of the transformations that need to be applied to the base RDD, and the actual computation is triggered only when an action is called.\n\nUnderstanding RDDs and how they work can provide valuable insight into Spark tuning and optimization; however, even though an RDD is the foundation of Spark’s functionality, it might not be the most efficient data structure for many applications.\n\nWhile an RDD is the basic data structure of Spark, it is a lower-level API that requires a more verbose syntax and lacks the optimizations provided by higher-level data structures. Spark shifted toward a more user-friendly and optimized API with the introduction of DataFrames — higher-level abstractions built on top of RDDs. The data in a DataFrame is organized into named columns, structuring it more like the data in a relational database. DataFrame operations also benefit from Catalyst, Spark SQL’s optimized execution engine, which can increase computational efficiency, potentially improving performance. Transformations and actions can be run on DataFrames the way they are in RDDs.\n\nBecause of their higher-level API and optimizations, DataFrames are typically easier to use and offer better performance; however, due to their lower-level nature, RDDs can still be useful for defining custom operations, as well as debugging complex data processing tasks. RDDs offer more granular control over partitioning and memory usage. When dealing with raw, unstructured data, such as text streams, binary files, or custom formats, RDDs can be more flexible, allowing for custom parsing and manipulation in the absence of a predefined structure.\n\nCaching is an essential technique that can lead to significant improvements in computational efficiency. Frequently accessed data and intermediate computations can be cached, or persisted, in a memory location that allows for faster retrieval. Spark provides built-in caching functionality, which can be particularly beneficial for machine learning algorithms, graph processing, and any other application in which the same data must be accessed repeatedly. Without caching, Spark would recompute an RDD or DataFrame and all of its dependencies every time an action was called.\n\nThe following Python code block uses PySpark, Spark’s Python API, to cache a DataFrame named :\n\nIt is important to keep in mind that caching requires careful planning, because it utilizes the memory resources of Spark’s worker nodes, which perform such tasks as executing computations and storing data. If the data set is significantly larger than the available memory, or you’re caching RDDs or DataFrames without reusing them in subsequent steps, the potential overflow and other memory management issues could introduce bottlenecks in performance.\n\nSpark’s architecture is built around partitioning, the division of large amounts of data into smaller, more manageable units called partitions. Partitioning enables Spark to process large amounts of data in parallel by distributing computation across multiple nodes, each handling a subset of the total data.\n\nWhile Spark provides a default partitioning strategy typically based on the number of available CPU cores, it also provides options for custom partitioning. Users might instead specify a custom partitioning function, such as dividing data on a certain key.\n\nOne of the most important factors affecting the efficiency of parallel processing is the number of partitions. If there aren’t enough partitions, the available memory and resources may be underutilized. On the other hand, too many partitions can lead to increased performance overhead due to task scheduling and coordination. The optimal number of partitions is usually set as a factor of the total number of cores available in the cluster.\n\nPartitions can be set using and . In this example, the DataFrame is repartitioned into 200 partitions:\n\nThe method increases or decreases the number of partitions in an RDD or DataFrame and performs a full shuffle of the data across the cluster, which can be costly in terms of processing and network latency. The method decreases the number of partitions in an RDD or DataFrame and, unlike , does not perform a full shuffle, instead combining adjacent partitions to reduce the overall number.\n\nIn some situations, certain partitions may contain significantly more data than others, leading to a condition known as skewed data. Skewed data can cause inefficiencies in parallel processing due to an uneven workload distribution among the worker nodes. To address skewed data in Spark, clever techniques such as splitting or salting can be used.\n\nIn some cases, skewed partitions can be separated into multiple partitions. If a numerical range causes the data to be skewed, the range can often be split up into smaller sub-ranges. For example, if a large number of students scored between 65% to 75% on an exam, the test scores can be divided into several sub-ranges, such as 65% to 68%, 69% to 71%, and 72% to 75%.\n\nIf a specific key value is causing the skew, the DataFrame can be divided based on that key. In the example code below, a skew in the data is caused by a large number of records that have an value of “12345.” The transformation is used twice: once to select all records with an value of “12345,” and once to select all records where the value is not “12345.” The records are placed into two new DataFrames: , which contains only the rows that have an value of “12345,” and , which contains all of the other rows. Data processing can be performed on and separately, after which the resulting data can be combined:\n\nAnother method of distributing data more evenly across partitions is to add a “salt” to the key or keys that are causing the skew. The salt value, typically a random number, is appended to the original key, and the salted key is used for partitioning. This forces a more even distribution of data.\n\nTo illustrate this concept, let’s imagine our data is split into partitions for three cities in the US state of Illinois: Chicago has many more residents than the nearby cities of Oak Park or Long Grove, causing the data to be skewed.\n\nTo distribute the data more evenly, using PySpark, we combine the column with a randomly generated integer to create a new key, called . “Chicago” becomes “Chicago1,” “Chicago2,” and “Chicago3,” with the new keys each representing a smaller number of records. The new keys can be used with actions or transformations such as or :\n\nA is a common operation in which two data sets are combined based on one or more common keys. Rows from two different data sets can be merged into a single data set by matching values in the specified columns. Because data shuffling across multiple nodes is required, a can be a costly operation in terms of network latency.\n\nIn scenarios in which a small data set is being joined with a larger data set, Spark offers an optimization technique called broadcasting. If one of the data sets is small enough to fit into the memory of each worker node, it can be sent to all nodes, reducing the need for costly shuffle operations. The operation simply happens locally on each node.\n\nIn the following example, the small DataFrame is broadcast across all of the worker nodes, and the operation with the large DataFrame is performed locally on each node:\n\nmust be small enough to fit into the memory of each worker node; a DataFrame that is too large will cause out-of-memory errors.\n\nWhen working with high-dimensional data, minimizing computational overhead is essential. Any rows or columns that are not absolutely required should be removed. Two key techniques that reduce computational complexity and memory usage are early filtering and column pruning:\n\nEarly filtering: Filtering operations should be applied as early as possible in the data processing pipeline. This cuts down on the number of rows that need to be processed in subsequent transformations, reducing the overall computational load and memory resources.\n\nColumn pruning: Many computations involve only a subset of columns in a data set. Columns that are not necessary for data processing should be removed. Column pruning can significantly decrease the amount of data that needs to be processed and stored.\n\nThe following code shows an example of the operation used to prune columns. Only the columns and are loaded into memory. The code also demonstrates how to use the operation to only include rows in which the value of is greater than 21:\n\nPython user-defined functions (UDFs) are custom functions written in Python that can be applied to RDDs or DataFrames. With UDFs, users can define their own custom logic or computations; however, there are performance considerations. Each time a Python UDF is invoked, data needs to be serialized and then deserialized between the Spark JVM and the Python interpreter, which leads to additional overhead due to data serialization, process switching, and data copying. This can significantly impact the speed of your data processing pipeline.\n\nOne of the most effective PySpark optimization techniques is to use PySpark’s built-in functions whenever possible. PySpark comes with a rich library of functions, all of which are optimized.\n\nIn cases in which complex logic can’t be implemented with the built-in functions, using vectorized UDFs, also known as Pandas UDFs, can help to achieve better performance. Vectorized UDFs operate on entire columns or arrays of data, rather than on individual rows. This batch processing often leads to improved performance over row-wise UDFs.\n\nConsider a task in which all of the elements in a column must be multiplied by two. In the following example, this operation is performed using a Python UDF:\n\nThe function is a Python UDF which takes an integer and multiplies it by two. This function is registered as a UDF using and applied to the column within the DataFrame .\n\nThe same multiplication operation can be implemented in a more efficient manner using PySpark’s built-in functions:\n\nIn cases in which the operation cannot be performed using built-in functions and a Python UDF is necessary, a vectorized UDF can offer a more efficient alternative:\n\nThis method applies the function to an entire series of data at once, reducing the serialization overhead. Note that the input and return of the function are both Pandas Series. A Pandas Series is a one-dimensional labeled array that can be used to represent the data in a single column in a DataFrame.\n\nAs machine learning and big data become more commonplace, engineers are adopting Apache Spark to handle the vast amounts of data that these technologies need to process. Boosting the performance of Spark involves a range of strategies, all designed to optimize the usage of available resources. Implementing the techniques discussed here will help Spark process large volumes of data much more efficiently."
    },
    {
        "link": "https://quora.com/What-are-the-best-practices-for-using-Scala-and-Spark",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://sparkcodehub.com/spark-rdd-transformations",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/51994256/what-are-the-best-practices-around-accessing-rdds-in-spark-sql-api",
        "document": "Spark sql and data frame APIs are high level APIs and easy to use with catalyst optimizer I.e the code you will write in theses APIs will be automatically be optimised by its catalyst engine.\n\nRDD is a low level api and can only be used where you need to handle completely unstructured raw data that can’t be handled by the high level APIs.\n\nEven if you will write the code using data frame and sql it will automatically be converted into rdds by spark.\n\nIt is advisable to write using high level APIs as you can utilise spark catalyst optimizer. If you will write using rdd then it won’t be optimized and you need to optimize your code.\n\nThough I am not sure whether rdd api will be removed in future but currently most of the work is going on for data frame and sql support"
    }
]