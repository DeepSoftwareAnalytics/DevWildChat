[
    {
        "link": "https://medium.com/@sandeep4.verma/system-design-scalable-url-shortener-service-like-tinyurl-106f30f23a82",
        "document": "A URL shortener service creates a short url/aliases/tiny url against a long url.Moreover, when user click on the tiny url, he gets redirected to original url.\n\nTiny url are exceedingly handy to share through sms/tweets (where there is limit to number of characters that can be messaged/tweeted) and also when they are printed in books/magazines etc.(Less character implies less printing cost). In addition, it is easy and less error prone to type a short url when compared to its longer version.\n\nFor e.g. I have created a tiny url to my medium profile using a free short link generation service called tinyrl.com (Try clicking shortened link and also try generating few short urls on your own at tinyurl.com)\n\nThere can be myriad of features even in seemingly small service like URL shortening. Hence it’s always a good idea to ask interviewer what features are expected in the service. This will then become the basis for functional requirements.\n\nHow long a tiny url would be ? Will it ever expire ?\n\nAssume once a url created it will remain forever in system.\n\nCan a customer create a tiny url of his/her choice or will it always be service generated ? If user is allowed to create customer shortened links, what would be the maximum size of custom url ?\n\nYes user can create a tiny url of his/her choice. Assume maximum character limit to be 16.\n\nHow many url shortening request expected per month ?\n\nDo we expect service to provide metrics like most visited links ?\n\nYes. Service should also aggregate metrics like number of URL redirections per day and other analytics for targeted advertisements.\n• Service should be able to create shortened url/links against a long url\n• Click to the short URL should redirect the user to the original long URL\n• Shortened link should be as small as possible\n• Users can create custom url with maximum character limit of 16\n• Service should collect metrics like most clicked links\n• Once a shortened link is generated it should stay in system for lifetime\n• Service should be up and running all the time\n• URL redirection should be fast and should not degrade at any point of time (Even during peak loads)\n• Service should expose REST API’s so that it can be integrated with third party applications\n\nAssuming lifetime of service to be 100 years and with 100 million shortened links creation per month, total number of data points/objects in system will be = 100 million/month * 100 (years) * 12 (months) = 120 billion\n\nAssuming size of each data object (Short url, long url, created date etc.) to be 500 bytes long, then total require storage = 120 billion * 500 bytes =60TB\n\nFollowing Pareto Principle, better known as the 80:20 rule for caching. (80% requests are for 20% data)\n\nSince we get 8000 read/redirection requests per second, we will be getting 700 million requests per day:\n\nTo cache 20% of these requests, we will need ~70GB of memory.\n\nFollowing is high level design of our URL service. This is a rudimentary design. We will optimise this further as we move along.\n• There is only one WebServer which is single point of failure (SPOF)\n• There is only single database which might not be sufficient for 60 TB of storage and high load of 8000/s read requests\n\nTo cater above limitations we :\n• Added cache system to reduce load on the database.\n\nWe will delve further into each component when we will go through the algorithms in later sections\n\nLet’s starts by making two functions accessible through REST API:\n\nlong_url: A long URL that needs to be shortened.\n\napi_key: A unique API key provided to each user, to protect from the spammers, access, and resource control for the user, etc.\n\ncustom_url(optional): The custom short link URL, user want to use.\n\nReturn Value: The short Url generated, or error code in case of the inappropriate parameter.\n\nNote : “HTTP 302 Redirect” status is sent back to the browser instead of “HTTP 301 Redirect”. A 301 redirect means that the page has permanently moved to a new location. A 302 redirect means that the move is only temporary. Thus, returning 302 redirect will ensure all requests for redirection reaches to our backend and we can perform analytics (Which is a functional requirement)\n\nshort_url: The short URL generated from the above function.\n\nReturn Value: The original long URL, or invalid URL error code.\n\nLet’s see the data we need to store:\n• Name: The name of the user\n• Email: The email id of the user\n• Creation Date: The date on which the user was registered\n• UserId: The unique user id or API key of the user who created the short URL\n\nFor shortening a url we can use following two solutions (URL encoding and Key Generation service). Let’s walk through each of them one by one.\n\nA base is a number of digits or characters that can be used to represent a particular number.\n\nBase 10 are digits [0–9], which we use in everyday life and\n\nLet’s do a back of the envelope calculation to find out how many characters shall we keep in our tiny url.\n\nSince we required to produce 120 billion URLs, with 7 characters in base62 we will get ~3500 Billion URLs. Hence each of tiny url generated will have 7 characters\n\nHow to get unique ‘7 character’ long random URLs in base62\n\nOnce we have decided number of characters to use in Tiny URL (7 characters) and also the base to use (base 62 [0–9][a-z][A-Z] ), then the next challenge is how to generate unique URLs which are 7 characters long.\n\nWe could just make a random choice for each character and check if this tiny url exists in DB or not. If it doesn’t exist return the tiny url else continue rolling/retrying.As more and more 7 characters short links are generated in Database, we would require 4 rolls before finding non-existing one short link which will slow down tiny url generation process.\n\nThink of the seven-bit short url as a hexadecimal number (0–9, a-z, A-Z) (For e.g. aKc3K4b) . Each short url can be mapped to a decimal integer by using base conversion and vice versa.\n\nHow do we do the base conversion? This is easiest to show by an example.\n\nTake the number 125 in base 10.\n\nIt has a 1 in the 100s place, a 2 in the 10s place, and a 5 in the 1s place. In general, the places in a base-10 number are:\n\nThe places in a base-62 number are:\n\nSo to convert 125 to base-62, we distribute that 125 across these base-62 “places.” The highest “place” that can take some is 62¹, which is 62. 125/62 is 2, with a remainder of 1. So we put a 2 in the 62’s place and a 1 in the 1’s place. So our answer is 21.\n\nWhat about a higher number — say, 7,912?\n\nNow we have enough to put something in the 3,844’s place (the 62²’s place). 7,912 / 3,844 is 2 with a remainder of 224. So we put a 2 in the 3,844’s place, and we distribute that remaining 224 across the remaining places — the 62’s place and the 1’s place. 224 / 62 is 3 with a remainder of 38. So we put a 3 in the 62’s place and a 38 in the 1’s place. We have this three-digit number: 23- 38.\n\nNow, that “38” represents one numeral in our base-62 number. So we need to convert that 38 into a specific choice from our set of numerals: a-z, A-Z, and 0–9.\n\nLet’s number each of our 62 numerals, like so:\n\nAs you can see, our “38th” numeral is “C.” So we convert that 38 to a “C.” That gives us 23C.\n\nSo we can start with a counter (A Large number 100000000000 in base 10 which 1L9zO9O in base 62) and increment counter every-time we get request for new short url (100000000001, 100000000002, 100000000003 etc.) .This way we will always get a unique short url.\n\nSimilarly, when we get tiny url link for redirection we can convert this base62 tiny url to a integer in base10\n\nThe MD5 message-digest algorithm is a widely used hash function producing a 128-bit hash value(or 32 hexadecimal digits). We can use these 32 hexadecimal digit for generating 7 characters long tiny url.\n• Encode the long URL using the MD5 algorithm and take only the first 7 characters to generate TinyURL.\n• The first 7 characters could be the same for different long URLs so check the DB to verify that TinyURL is not used already\n• Try next 7 characters of previous choice of 7 characters already exist in DB and continue until you find a unique value\n\nWith all these shortening algorithms, let’s revisit our design goals!\n• Being able to store a lot of short links (120 billion)\n• Our TinyURL should be as short as possible (7 characters)\n• Application should be resilient to load spikes (For both url redirections and short link generation)\n• Following a short link should be fast\n\nAll the above techniques we discussed above will help us in achieving goal (1) and (2). But will fail at step (3) and step (4). Let’s see how ?\n• A single web-server is a single point of failure (SPOF). If this web-server goes down, none of our users will be able to generate tiny urls/or access original (long) urls from tiny urls. This can be handled by adding more web-servers for redundancy and then bringing a load balancer in front but even with this design choice next challenge will come from database\n• With database we have two options :\n\nIf we choose RDBMs as our database to store the data then it is efficient to check if an URL exists in database and handle concurrent writes. But RDBMs are difficult to scale (We will use RDBMS for scaling in one of our Technique and will see how to scale using RDBMS)\n\nIf we opt for “NOSQL”, we can leverage scaling power of “NOSQL”-style databases but these systems are eventually consistent (Unlike RDBMs which provides ACID consistency)\n\nWe can use putIfAbsent(TinyURL, long URL) or INSERT-IF-NOT-EXIST condition while inserting the tiny URL but this requires support from DB which is available in RDBMS but not in NoSQL. Data is eventually consistent in NoSQL so putIfAbsent feature support might not be available in the NoSQL database(We can handle this using some of the features of NOSQL which we have discussed later on in this article). This can cause consistency issue (Two different long URLs having the same tiny url).\n\nWe can use a relational database as our backend store but since we don’t have joins between objects and we require huge storage size (60TB) along with high writes (40 URL’s per seconds) and read (8000/s) speed, a NoSQL store (MongoDB, Cassandra etc.) will be better choice.\n\nWe can certainly scale using SQL (By using custom partitioning and replication which is by default available in MongoDB and Cassandra) but this difficult to develop and maintain\n\nLet’s discuss how to use MongoDB to scale database for shortening algorithm given in Technique — 1\n\nMongoDB supports distributing data across multiple machines using shards. Since we have to support large data sets and high throughput, we can leverage sharding feature of MongoDB.\n\nOnce we generate 7 characters long tiny url, we can use this tinyURL as shard key and employ sharding strategy as hashed sharding. MongoDB automatically computes the hashes (Using tiny url ) when resolving queries. Applications do not need to compute hashes. Data distribution based on hashed values facilitates more even data distribution, especially in data sets where the shard key changes monotonically.\n\nWe can scale reads/writes by increasing number of shards for our collection containing tinyURL data(Schema having three fields Short Url,Original Url,UserId).\n\nSince MongoDB supports transaction for a single document, we can maintain consistency and because we are using hash based shard key, we can efficiently use putIfAbsent (As a hash will always be corresponding to a shard)\n\nTo speed up reads (checking whether a Short URL exists in DB or what is Original url corresponding to a short URL) we can create indexing on ShortURL.\n\nWe will also use cache to further speed up reads (We will discuss caching in later part of this article)\n\nWe used a counter (A large number) and then converted it into a base62 7 character tinyURL.As counters always get incremented so we can get a new value for every new request (Thus we don’t need to worry about getting same tinyURL for different long/original urls)\n\nSharding is a scale-out approach in which database tables are partitioned, and each partition is put on a separate RDBMS server. For SQL, this means each node has its own separate SQL RDBMS managing its own separate set of data partitions. This data separation allows the application to distribute queries across multiple servers simultaneously, creating parallelism and thus increasing the scale of that workload. However, this data and server separation also creates challenges, including sharding key choice, schema design, and application rewrites. Additional challenges of sharding MySQL include data maintenance, infrastructure maintenance, and business challenges.\n\nBefore an RDBMS can be sharded, several design decisions must be made. Each of these is critical to both the performance of the sharded array, as well as the flexibility of the implementation going forward. These design decisions include the following:\n\nWe can use sharding key as auto-incrementing counter and divide them into ranges for example from 1 to 10M, server 2 ranges from 10M to 20M, and so on.\n\nWe can start the counter from 100000000000. So counter for each SQL database instance will be in range 100000000000+1 to 100000000000+10M , 100000000000+10M to 100000000000+20M and so on.\n\nWe can start with 100 database instances and as and when any instance reaches maximum limit (10M), we can stop saving objects there and spin up a new server instance. In case one instance is not available/or down or when we require high throughput for write we can spawn multiple new server instances.\n\nWhere to keep which information about active database instances ?\n\nSolution: We can use a distributed service Zookeeper to solve the various challenges of a distributed system like a race condition, deadlock, or particle failure of data. Zookeeper is basically a distributed coordination service that manages a large set of hosts. It keeps track of all the things such as the naming of the servers, active database servers, dead servers, configuration information (Which server is managing which range of counters)of all the hosts. It provides coordination and maintains the synchronization between the multiple servers. \n\nLet’s discuss how to maintain a counter for distributed hosts using Zookeeper.\n• In Zookeeper maintain the range and divide the 1st billion into 100 ranges of 10 million each i.e. range 1->(1–1,000,0000), range 2->(1,000,0001–2,000,0000)…. range 1000->(999,000,0001–1,000,000,0000) (Add 100000000000 to each range for counter)\n• When servers will be added these servers will ask for the unused range from Zookeepers. Suppose the W1 server is assigned range 1, now W1 will generate the tiny URL incrementing the counter and using the encoding technique. Every time it will be a unique number so there is no possibility of collision and also there is no need to keep checking the DB to ensure that if the URL already exists or not. We can directly insert the mapping of a long URL and short URL into the DB.\n• In the worst case, if one of the servers goes down then only that range of data is affected. We can replicate data of master to it’s slave and while we try to bring master back, we can divert read queries to it’s slaves\n• If one of the database reaches its maximum range or limit then we can move that database instance out from active database instances which can accept write and add a new database with a new a new fresh range and add this to Zookeeper. This will only be used for reading purpose\n• The Addition of a new database is also easy. Zookeeper will assign an unused counter range to this new database.\n• We will take the 2nd billion when the 1st billion is exhausted to continue the process.\n\nHow to check whether short URL is present in database or not ?\n\nSolution : When we get tiny url (For example 1L9zO9O) we can use base62ToBase10 function to get the counter value (100000000000). Once we have this values we can get which database this counter ranges belongs to from zookeeper(Let’s say it database instance 1 ) . Then we can send SQL query to this server (Select * from tinyUrl where id=10000000000111).This will provide us sql row data (*if present)\n\nHow to get original url corresponding to tiny url ?\n\nSolution : We can leverage the above solution to get back the sql row data. This data will have shortUrl, originalURL and userId.\n\nWe can leverage the scaling Technique 1 (Using MongoDB). We can also use Cassandra in place of MongoDB. In Cassandra instead of using shard key we will use partition key to distribute our data.\n\nWe can have a standalone Key Generation Service (KGS) that generates random seven-letter strings beforehand and stores them in a database (let’s call it key-DB). Whenever we want to shorten a URL, we will take one of the already-generated keys and use it. This approach will make things quite simple and fast. Not only are we not encoding the URL, but we won’t have to worry about duplications or collisions. KGS will make sure all the keys inserted into key-DB are unique\n\nCan concurrency cause problems? As soon as a key is used, it should be marked in the database to ensure that it is not used again. If there are multiple servers reading keys concurrently, we might get a scenario where two or more servers try to read the same key from the database. How can we solve this concurrency problem?\n\nServers can use KGS to read/mark keys in the database. KGS can use two tables to store keys: one for keys that are not used yet, and one for all the used keys. As soon as KGS gives keys to one of the servers, it can move them to the used keys table. KGS can always keep some keys in memory to quickly provide them whenever a server needs them.\n\nFor simplicity, as soon as KGS loads some keys in memory, it can move them to the used keys table. This ensures each server gets unique keys. If KGS dies before assigning all the loaded keys to some server, we will be wasting those keys–which could be acceptable, given the huge number of keys we have.\n\nKGS also has to make sure not to give the same key to multiple servers. For that, it must synchronize (or get a lock on) the data structure holding the keys before removing keys from it and giving them to a server.\n\nIsn’t KGS a single point of failure? Yes, it is. To solve this, we can have a standby replica of KGS. Whenever the primary server dies, the standby server can take over to generate and provide keys.\n\nCan each app server cache some keys from key-DB? Yes, this can surely speed things up. Although, in this case, if the application server dies before consuming all the keys, we will end up losing those keys. This can be acceptable since we have 68B unique six-letter keys.\n\nHow would we perform a key lookup? We can look up the key in our database to get the full URL. If its present in the DB, issue an “HTTP 302 Redirect” status back to the browser, passing the stored URL in the “Location” field of the request. If that key is not present in our system, issue an “HTTP 404 Not Found” status or redirect the user back to the homepage.\n\nShould we impose size limits on custom aliases? Our service supports custom aliases. Users can pick any ‘key’ they like, but providing a custom alias is not mandatory. However, it is reasonable (and often desirable) to impose a size limit on a custom alias to ensure we have a consistent URL database. Let’s assume users can specify a maximum of 16 characters per customer key\n\nWe can cache URLs that are frequently accessed. We can use some off-the-shelf solution like Memcached, which can store full URLs with their respective hashes. Before hitting backend storage, the application servers can quickly check if the cache has the desired URL.\n\nHow much cache memory should we have? We can start with 20% of daily traffic and, based on clients’ usage patterns, we can adjust how many cache servers we need. As estimated above, we need 70GB memory to cache 20% of daily traffic. Since a modern-day server can have 256GB memory, we can easily fit all the cache into one machine. Alternatively, we can use a couple of smaller servers to store all these hot URLs.\n\nWhich cache eviction policy would best fit our needs? When the cache is full, and we want to replace a link with a newer/hotter URL, how would we choose? Least Recently Used (LRU) can be a reasonable policy for our system. Under this policy, we discard the least recently used URL first. We can use a Linked Hash Map or a similar data structure to store our URLs and Hashes, which will also keep track of the URLs that have been accessed recently.\n\nTo further increase the efficiency, we can replicate our caching servers to distribute the load between them.\n\nHow can each cache replica be updated? Whenever there is a cache miss, our servers would be hitting a backend database. Whenever this happens, we can update the cache and pass the new entry to all the cache replicas. Each replica can update its cache by adding the new entry. If a replica already has that entry, it can simply ignore it.\n\nWe can add a Load balancing layer at three places in our system:\n\nInitially, we could use a simple Round Robin approach that distributes incoming requests equally among backend servers. This LB is simple to implement and does not introduce any overhead. Another benefit of this approach is that if a server is dead, LB will take it out of the rotation and will stop sending any traffic to it.\n\nA problem with Round Robin LB is that we don’t take the server load into consideration. If a server is overloaded or slow, the LB will not stop sending new requests to that server. To handle this, a more intelligent LB solution can be placed that periodically queries the backend server about its load and adjusts traffic based on that.\n\nA customer can also provide a tiny url of his/her choice. We can allow customer to choose just alphanumerics and the “special characters” $-_.+!*’(),. in tiny url(With let’s say 8 minimum characters). When customer provides a custom url we can save it to some other instance of database (Different one from where system generates tiny url against original url) and treat these tiny URLs as special URLs. When we get redirection request we can divert these request to special instances of WebServer . Since this will be a paid service we can expect very few tiny URLs to be of this kind and we don’t need to worry about scaling WebServers/Database in this case.\n\nHow many times a short URL has been used ? How would we store these statistics?\n\nSince we used “HTTP 302 Redirect” status to the browser instead of “HTTP 301 Redirect”, thus each redirection for tiny url will reach service’s backend. We can push this data(tiny url, users etc.) to a Kafka queue and perform analytics in real time"
    },
    {
        "link": "https://geeksforgeeks.org/system-design-url-shortening-service",
        "document": "The need for an efficient and concise URL management system has become a big matter in this technical age. URL shortening services, such as bit.ly, and TinyURL, play a massive role in transforming lengthy web addresses into shorter, shareable links. As the demand for such services grows, it has become vital to understand the System Design of URL shorteners and master the art of designing a scalable and reliable URL-shortening system.\n\nA URL shortening service takes a long, complex web address and converts it into a shorter, more manageable link. This shorter URL redirects users to the original destination, making sharing links easier and cleaner—especially on platforms with character limits, like Twitter. Common examples include services like Bit.ly and TinyURL, which create concise links that are easy to remember and track.\n\nHow Would You Design a URL Shortener Service Like TinyURL?\n\nURL shortening services like bit.ly or TinyURL are very popular to generate shorter aliases for long URLs. You need to design this kind of web service where if a user gives a long URL then the service returns a short URL and if the user gives a short URL then it returns the original long URL.\n\nFor example, shortening the given URL through TinyURL:\n\nWe get the result given below:\n• None Given a long URL, the service should generate a shorter and unique alias for it.\n• None When the user hits a short link, the service should redirect to the original link.\n• None The system should be highly available. This is really important to consider because if the service goes down, all the URL redirection will start failing.\n• None URL redirection should happen in real-time with minimal latency.\n• None Shortened links should not be predictable.\n\nLet’s assume our service has 30M new URL shortenings per month. Let’s assume we store every URL shortening request (and associated shortened link) for 5 years. For this period the service will generate about 1.8 B records.\n\nNote: Let’s consider we are using 7 characters to generate a short URL. These characters are a combination of 62 characters [A-Z, a-z, 0-9] something like http://ad.com/abXdef2.\n\nDiscuss the data capacity model to estimate the storage of the system. We need to understand how much data we might have to insert into our system. Think about the different columns or attributes that will be stored in our database and calculate the storage of data for five years. Let’s make the assumption given below for different attributes.\n• None Consider the average long URL size of 2KB ie for 2048 characters.\n\nNote: We need to think about the reads and writes that will happen on our system for this amount of data. This will decide what kind of database (RDBMS or NoSQL) we need to use.\n\nTo convert a long URL into a unique short URL we can use some hashing techniques like Base62 or MD5. We will discuss both approaches.\n• None Base62 encoder allows us to use the combination of characters and numbers which contains A-Z, a-z, 0–9 total( 26 + 26 + 10 = 62).\n• None So for 7 characters short URL, we can serve 62^7 ~= 3500 billion URLs which is quite enough in comparison to base10 (base10 only contains numbers 0-9 so you will get only 10M combinations).\n• None We can generate a random number for the given long URL and convert it to base62 and use the hash as a short URL id.\n\nMD5 also gives base62 output but the MD5 hash gives a lengthy output which is more than 7 characters.\n• None MD5 hash generates 128-bit long output so out of 128 bits we will take 43 bits to generate a tiny URL of 7 characters.\n• None MD5 can create a lot of collisions. For two or many different long URL inputs we may get the same unique id for a short URL and that could cause data corruption.\n• None So we need to perform some checks to ensure that this unique id doesn’t exist in the database already.\n\nLet’s discuss the mapping of a long URL into a short URL in our database:\n\nAssume we generate the Tiny URL using base62 encoding then we need to perform the steps given below:\n• None The tiny URL should be unique so firstly check the existence of this tiny URL in the database (doing get(tiny) on DB). If it’s already present there for some other long URL then generate a new short URL.\n• None If the short URL isn’t present in DB then put the long URL and TinyURL in DB (put(TinyURL, long URL)).\n• None Encode the long URL using the MD5 approach and take only the first 7 chars to generate TinyURL.\n• None The first 7 characters could be the same for different long URLs so check the DB (as we have discussed in Technique 1) to verify that TinyURL is not used already.\n\nThis approach saves some space in the database but how?\n• None If two users want to generate a tiny URL for the same long URL then the first technique will generate two random numbers and it requires two rows in the database.\n• None In the second technique, both the longer URL will have the same MD5 so it will have the same first 43 bits.\n• None This means we will get some deduping and we will end up with saving some space since we only need to store one row instead of two rows in the database.\n\nUsing a counter is a good decision for a scalable solution because counters always get incremented so we can get a new value for every new request.\n• None A single host or server (say database) will be responsible for maintaining the counter.\n• None When the worker host receives a request it talks to the counter host, which returns a unique number and increments the counter. When the next request comes the counter host again returns the unique number and this goes on.\n• None Every worker host gets a unique number which is used to generate TinyURL.\n• User Interface/Clients:\n• None The user interface allows users to enter a long URL and receive a shortened link. This could be a simple web form or a RESTful API.\n• Application Server:\n• None The application server receives the long URL from the user interface and generates a unique, shorter alias or key for the URL. It then stores the alias and the original URL in a database. The application server also tracks click events on the shortened links.\n• Load Balancer:\n• None To handle a large number of requests, we can use a load balancer to distribute incoming traffic across multiple instances of the application server. We can add a Load balancing layer at three places in our service:\n• Database:\n• None The database stores the alias or key and the original URL. The database should be scalable to handle a large number of URLs and clicks. We can use NoSQL databases such as MongoDB or Cassandra, which can handle large amounts of data and can scale horizontally.\n• None As soon as a key is used, it should be marked in the database to ensure it doesn’t get used again. If there are multiple servers reading keys concurrently, we might get a scenario where two or more servers try to read the same key from the database\n• Caching:\n• None Since reading from the database can be slow and resource-intensive, we can add a caching layer to speed up read operations. We can use in-memory caches like Redis or Memcached to store the most frequently accessed URLs.\n• Cleanup Service:\n• None This service helps in cleaning the old data from the databases\n• Redirection:\n• None When a user clicks on a shortened link, the application server looks up the original URL from the database using the alias or key. It then redirects the user to the original URL using HTTP 301 status code, which is a permanent redirect.\n• Analytics:\n• None The application server should track click events on the shortened links and provide analytics to the user. This includes the number of clicks, the referrer, the browser, and the device used to access the link.\n• Security:\n• None The service should be designed to prevent malicious users from generating short links to phishing or malware sites. It should also protect against DDoS attacks and brute force attacks. We can use firewalls, rate-limiting, and authentication mechanisms to ensure the security of the service.\n\nLet us explore some of the choices for System Design of Databases of URL Shortner:\n• None We can use RDBMS which uses ACID properties but you will be facing the scalability issue with relational databases.\n• None Now if you think you can use issue in RDBMS then that will increase the complexity of the system.\n• None There are 30M active users so there will be conversions and a lot of Short URL resolution and redirections.\n• None Read and write will be heavy for these 30M users so scaling the RDBMS using shard will increase the complexity of the design.\n\nSo let’s take a look at NoSQL Database:\n• None The only problem with using the NoSQL database is its\n• None We write something and it takes some time to replicate to a different node but our system needs high availability and NoSQL fits this requirement.\n• None NoSQL can easily handle the 30M of active users and it is easy to scale. We just need to keep adding the nodes when we want to expand the storage.\n\nIn a URL shortening service, caching and load balancing are essential for managing high demand and optimizing response times. The service could greatly benefit from read-through caching or write-through caching mechanisms.\n• None A read-through cache automatically loads data into the cache when a miss occurs\n• None While a write-through cache updates the cache whenever the database is updated.\n\nIn this scenario, a read-through cache would be especially useful since shortened URLs are likely accessed multiple times. Redis or Memcached would be good choices for the caching layer due to their speed and support for frequently accessed data.\n\nFor load balancing, algorithms like Round Robin or Least Connections are effective choices. Round Robin evenly distributes incoming traffic across servers, making it easy to implement and scalable. However, for services with variable request times, Least Connections can be better, as it allocates requests based on the server’s current load.\n\nOverall, a URL shortening service is a simple but useful application that can be built using a variety of technologies and architectures. By following the above architecture, we can build a scalable, reliable, and secure URL-shortening service."
    },
    {
        "link": "https://designgurus.io/blog/url-shortening",
        "document": "2. Requirements and Goals of the System\n\nOur URL shortening system should meet the following requirements:\n• Given a URL, our service should generate a shorter and unique alias of it. This is called a short link. This link should be short enough to be easily copied and pasted into applications.\n• When users access a short link, our service should redirect them to the original link.\n• Users should optionally be able to pick a custom short link for their URL.\n• Links will expire after a standard default timespan. Users should be able to specify the expiration time.\n• The system should be highly available. This is required because, if our service is down, all the URL redirections will start failing.\n• URL redirection should happen in real-time with minimal latency.\n• Shortened links should not be guessable (not predictable).\n• Our service should also be accessible through REST APIs by other services.\n\nOur system will be read-heavy. There will be lots of redirection requests compared to new URL shortenings. Let’s assume a 100:1 ratio between read and write.\n\nTraffic estimates: Assuming, we will have 500M new URL shortenings per month, with 100:1 read/write ratio, we can expect 50B redirections during the same period:\n\nWhat would be Queries Per Second (QPS) for our system? New URLs shortenings per second:\n\nConsidering 100:1 read/write ratio, URLs redirections per second will be:\n\nStorage estimates: Let’s assume we store every URL shortening request (and associated shortened link) for 5 years. Since we expect to have 500M new URLs every month, the total number of objects we expect to store will be 30 billion:\n\nLet’s assume that each stored object will be approximately 500 bytes (just a ballpark estimate--we will dig into it later). We will need 15TB of total storage:\n\nBandwidth estimates: For write requests, since we expect 200 new URLs every second, total incoming data for our service will be 100KB per second:\n\nFor read requests, since every second we expect ~20K URLs redirections, total outgoing data for our service would be 10MB per second:\n\nMemory estimates: If we want to cache some of the hot URLs that are frequently accessed, how much memory will we need to store them? If we follow the 80-20 rule, meaning 20% of URLs generate 80% of traffic, we would like to cache these 20% hot URLs.\n\nSince we have 20K requests per second, we will be getting 1.7 billion requests per day:\n\nTo cache 20% of these requests, we will need 170GB of memory.\n\nOne thing to note here is that since there will be many duplicate requests (of the same URL), our actual memory usage will be less than 170GB.\n\nHigh-level estimates: Assuming 500 million new URLs per month and 100:1 read:write ratio, following is the summary of the high level estimates for our service:\n\nWe can have SOAP or REST APIs to expose the functionality of our service. Following could be the definitions of the APIs for creating and deleting URLs:\n\nParameters:\n\n api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota.\n\n original_url (string): Original URL to be shortened.\n\n custom_alias (string): Optional custom key for the URL.\n\n user_name (string): Optional user name to be used in the encoding.\n\n expire_date (string): Optional expiration date for the shortened URL.\n\nReturns: (string)\n\n A successful insertion returns the shortened URL; otherwise, it returns an error code.\n\nWhere “url_key” is a string representing the shortened URL to be retrieved; a successful deletion returns ‘URL Removed’.\n\nHow do we detect and prevent abuse? A malicious user can put us out of business by consuming all URL keys in the current design. To prevent abuse, we can limit users via their api_dev_key. Each api_dev_key can be limited to a certain number of URL creations and redirections per some time period (which may be set to a different duration per developer key).\n\nA few observations about the nature of the data we will store:\n• We need to store billions of records.\n• Each object we store is small (less than 1K).\n• There are no relationships between records—other than storing which user created a URL.\n\nWe would need two tables: one for storing information about the URL mappings and one for the user's data who created the short link."
    },
    {
        "link": "https://systemdesign.one/url-shortening-system-design",
        "document": "The target audience for this article falls into the following roles:\n\nThe prerequisite to reading this article is fundamental knowledge of system design components. This article does not cover an in-depth guide on individual system design components.\n\nDisclaimer: The system design questions are subjective. This article is written based on the research I have done on the topic and might differ from real-world implementations. Feel free to share your feedback and ask questions in the comments.\n\nThe system design of the URL shortener is similar to the design of Pastebin. I highly recommend reading the related article to improve your system design skills.\n\nGet the powerful template to approach system design for FREE on newsletter sign-up:\n\nAt a high level, the URL shortener executes the following operations:\n• the server generates a unique short URL for each long URL\n• the server encodes the short URL for readability\n• the server persists the short URL in the data store\n• the server redirects the client to the original long URL against the short URL\n\nThe following terminology might be useful for you:\n• Microservices: designing software that is made up of small independent services, which have a specific purpose\n• Service Discovery: the process of automatically detecting devices and services on a network\n• CDN: a group of geographically distributed servers that speed up the delivery of web content by bringing the content closer to the users\n• API: a software intermediary that allows two applications or services to talk to each other\n• Encoding: the process of converting data from one form to another to preserve the usability of data\n• Encryption: secure encoding of data using a key to protect the confidentiality of data\n• Hashing: a one-way summary of data that cannot be reversed and is used to validate the integrity of data\n• Bloom filter: a memory-efficient probabilistic data structure to check whether an element is present in a set\n\nA URL shortening service is a website that substantially shortens a Uniform Resource Locator (URL). The short URL redirects the client to the URL of the original website. Some popular public-facing URL shortening services are tinyurl.com and bitly.com.\n\nThe reasons to shorten a URL are the following:\n• some instant messaging services limit the count of characters on the URL\n\nQuestions to ask the Interviewer\n• What are the use cases of the system?\n• What is the amount of Daily Active Users (DAU) for writes?\n• How many years should we persist the short URL by default?\n• What is the anticipated read: write ratio of the system?\n• What is the usage pattern of the shortened URL?\n• Who will use the URL shortener service?\n• What is the reasonable length of a short URL?\n• URL shortening and redirection to the original long URL\n• Most of the shortened URLs will be accessed only once after the creation\n• A client (user) enters a long URL into the system and the system returns a shortened URL\n• The client visiting the short URL must be redirected to the original long URL\n• Multiple users entering the same long URL must receive the same short URL (1-to-1 mapping)\n• The short URL should be readable\n• The short URL should be collision-free\n• The short URL should be non-predictable\n• The client should be able to choose a custom short URL\n• The short URL should be web-crawler friendly (SEO)\n• The short URL should support analytics (not real-time) such as the number of redirections from the shortened URL\n• The client optionally defines the expiry time of the short URL\n• The user sets the visibility of the short URL\n\nThe components in the system expose the Application Programming Interface (API) endpoints through Representational State Transfer (REST) or Remote Procedure Call (RPC). The best practice to expose public APIs is through REST because of loose coupling and easiness to debug , .\n\nOnce the services have hardened and performance should be tuned further, switch to RPC for internal communications between services. The tradeoffs of RPC are tight coupling and difficulty to debug.\n\nThe client executes an HTTP PUT request to the server to shorten a URL. The HTTP PUT method is used because the PUT is idempotent and the idempotency quality resonates with the given requirement of 1-to-1 mapping between the long and short URL.\n\nThe description of URL shortening HTTP request headers is the following:\n\nThe description of URL shortening HTTP request body parameters is the following:\n\nThe server responds with a status code of 200 OK for success. The HTTP response payload contains the shortened URL and related metadata , .\n\nThe description of URL shortening HTTP Response headers is the following:\n\nThe description of URL shortening HTTP Response payload parameters is the following:\n\nThe client receives a status code 401 unauthorized if the client did not provide any credentials or does not have valid credentials.\n\nThe client sees a status code 403 forbidden if the client has valid credentials but not sufficient privileges to act on the resource.\n\nThe client executes an HTTP GET request to redirect from the short URL to the original long URL. There is no request body for an HTTP GET request.\n\nThe description of URL redirection HTTP request headers is the following:\n\nThe server responds with status code 301 Moved Permanently. The status code 301 indicates that the short URL is permanently moved to the long URL. The web crawler updates its records when the status code 301 is received , .\n\nThe 301 status code stores the cache on the client by default. The URL redirection requests do not reach the server because of the cache on the client side. If the redirection requests do not reach the server, analytics data cannot be collected. The cache-control header on the HTTP response is set to public to prevent caching on the client side. The public cache lives on the content delivery network (CDN) or the reverse proxy servers. Analytics data is collected from the servers.\n\nThe server sets the location HTTP header to the long URL. The browser then automatically makes another HTTP request to the received long URL. The original website (long URL) is displayed to the client.\n\nThe description of URL redirection HTTP response headers is the following:\n\nThe HTTP response status code 302 is a temporary redirect. The server sets the location header value to the long URL. The 302 status code does not improve the SEO rankings of the original website.\n\nThe HTTP response status code 307 is a temporary redirect. The 307 status code lets the client reuse the HTTP method and the body of the original request on redirection. For instance, when the client received a 307 status code in response to a redirection request, the client can execute a PUT request against the original long URL. Contrarily, the status codes 301 and 302 automatically convert the HTTP requests to HTTP GET requests.\n\nThe server responds with the status code 404 Not Found when the client executes a redirection request on the short URL that does not exist in the database.\n\nIn summary, use the 301 status code in the HTTP response to meet the system requirements.\n\nGet the powerful template to approach system design for FREE on newsletter sign-up:\n\nThe URL shortener system is read-heavy. In other words, the dominant usage pattern is the redirection from short URLs to long URLs.\n\nThe major entities of the database (data store) are the Users table and the URL table. The relationship between the Users and the URL tables is 1-to-many. A user might generate multiple short URLs but a short URL is generated only by a single user.\n\nA NoSQL data store such as DynamoDB or MongoDB is used to store the URL table. The reasons for choosing NoSQL for storing the URL table are the following:\n• No need for complex joins\n• Store many TB (or PB) of data\n\nA SQL database such as Postgres or MySQL is used to store the Users table. The reasons to choose a SQL store for storing the Users table are the following:\n\nWhen the client requests to identify the owner of a short URL, the server queries the Users table and the URL table from different data stores and joins the tables at the application level.\n\nThe number of registered users is relatively limited compared to the number of short URLs generated. The capacity planning (back of the envelope) calculations focus on the URL data. However, the calculated numbers are approximations. A few helpful tips on capacity planning for a system design are the following:\n• round off the numbers for quicker calculations\n• write down the units when you do conversions\n\nThe URL shortener is read-heavy. The Daily Active Users (DAU) for writes is 100 million. The Query Per Second (QPS) of reads is approximately 100 thousand.\n\nThe shortened URL persists by default for 5 years in the data store. Each character size is assumed to be 1 byte.\n\nIn total, a URL record is approximately 2.5 KB in size.\n\nSet the replication factor of the storage to a value of at least three for improved durability and disaster recovery.\n\nIngress is the network traffic that enters the server (client requests). Egress is the network traffic that exits the servers (server responses).\n\nThe URL redirection traffic (egress) is cached to improve the latency. Following the 80/20 rule, 80% of egress is served by 20% of URL data stored on cache servers. The remaining 20% of the egress is served by the data store to improve the latency. A Time-to-live (TTL) of 1 day is reasonable.\n\nGet the powerful template to approach system design for FREE on newsletter sign-up:\n\nEncoding is the process of converting data from one form to another. The following are the reasons to encode a shortened URL:\n• improve the readability of the short URL\n\nThe encoding format to be used in the URL shortener must yield a deterministic (no randomness) output. The potential data encoding formats that satisfy the URL shortening use case are the following:\n\nThe base58 encoding format is similar to base62 encoding except that base58 avoids non-distinguishable characters such as O (uppercase O), 0 (zero), and I (capital I), l (lowercase L). The characters in base62 encoding consume 6 bits (2⁶ = 64). A short URL of 7 characters in length in base62 encoding consumes 42 bits.\n\nThe following generic formula is used to count the total number of short URLs that are produced using a specific encoding format and the number of characters in the output:\n\nThe combination of encoding formats and the output length generates the following total count of short URLs:\n\nThe total count of short URLs is directly proportional to the length of the encoded output. However, the length of the short URL must be kept as short as possible for better readability. The base62 encoded output of 7-character length generates 3.5 trillion short URLs. A total count of 3.5 trillion short URLs is exhausted in 100 years when 1000 short URLs are used per second. The guidelines on the encoded output format to improve the readability of a short URL are the following:\n• the encoded output contains only alphanumeric characters\n• the length of the short URL must not exceed 9 characters\n\nThe time complexity of base conversion is O(k), where k is the number of characters (k = 7). The time complexity of base conversion is reduced to constant time O(1) because the number of characters is fixed.\n\nThe server shortens the long URL entered by the client. The shortened URL is encoded for improved readability. The server persists the encoded short URL into the database. The simplified block diagram of a single-machine URL shortener is the following:\n\nThe single-machine solution does not meet the scalability requirements of the URL shortener system. The key generation function is moved out of the server to a dedicated Key Generation Service (KGS) to scale out the system.\n\nThe different solutions to shortening a URL are the following:\n\nThe Key Generation Service (KGS) queries the random identifier (ID) generation service to shorten a URL. The service generates random IDs using a random function or Universally Unique Identifiers (UUID). Multiple instances of the random ID generation service must be provisioned to meet the demand for scalability.\n\nThe random ID generation service must be stateless to easily replicate the service for scaling. The ingress is distributed to a random ID generation service using a load balancer. The potential load-balancing algorithms to route the traffic are the following:\n\nThe consistent hashing or the modulo-hash function-based load balancing algorithms might result in unbalanced (hot) replicas when the same long URL is entered by a large number of clients at the same time. The KGS must verify if the generated short URL already exists in the database because of the randomness in the output.\n\nThe random ID generation solution has the following tradeoffs:\n• the probability of collisions is high due to randomness\n• breaks the 1-to-1 mapping between a short URL and a long URL\n• coordination between servers is required to prevent a collision\n• frequent verification of the existence of a short URL in the database is a bottleneck\n\nAn alternative to the random ID generation solution is using Twitter’s Snowflake. The length of the snowflake output is 64 bits. The base62 encoding of snowflake output yields an 11-character output because each base62 encoded character consumes 6 bits. The snowflake ID is generated by a combination of the following entities (real-world implementation might vary):\n\nThe downsides of using snowflake ID for URL shortening are the following:\n• probability of collision is higher due to the overlapping bits\n• generated short URL becomes predictable due to known bits\n• increases the complexity of the system due to time synchronization between servers\n\nIn summary, do not use the random ID generator solution for shortening a URL.\n\nThe KGS queries the hashing function service to shorten a URL. The hashing function service accepts a long URL as an input and executes a hash function such as the message-digest algorithm (MD5) to generate a short URL. The length of the MD5 hash function output is 128 bits. The hashing function service is replicated to meet the scalability demand of the system.\n\nThe hashing function service must be stateless to easily replicate the service for scaling. The ingress is distributed to the hashing function service using a load balancer. The potential load-balancing algorithms to route the traffic are the following:\n\nThe hash-based load balancing algorithms result in hot replicas when the same long URL is entered by a large number of clients at the same time. The non-hash-based load-balancing algorithms result in redundant operations because the MD5 hashing function produces the same output (short URL) for the same input (long URL).\n\nThe base62 encoding of MD5 output yields 22 characters because each base62 encoded character consumes 6 bits and MD5 output is 128 bits. The encoded output must be truncated by considering only the first 7 characters (42 bits) to keep the short URL readable. However, the encoded output of multiple long URLs might yield the same prefix (first 7 characters), resulting in a collision. Random bits are appended to the suffix of the encoded output to make it nonpredictable at the expense of short URL readability.\n\nAn alternative hashing function for URL shortening is SHA256. However, the probability of a collision is higher due to an output length of 256 bits. The tradeoffs of the hashing function solution are the following:\n\nIn summary, do not use the hashing function solution for shortening a URL.\n\nThe KGS queries the token service to shorten a URL. An internal counter function of the token service generates the short URL and the output is monotonically increasing.\n\nThe token service must be horizontally partitioned (shard) to meet the scalability requirements of the system. The potential sharding schemes for the token service are the following:\n\nThe list and modulus partitioning schemes do not meet the scalability requirements of the system because both schemes limit the number of token service instances. The sharding based on consistent hashing fits the system requirements as the token service scales out by provisioning new instances.\n\nThe ingress is distributed to the token service using a load balancer. The percent-encoded long URLs are load-balanced using consistent hashing to preserve the 1-to-1 mapping between the long and short URLs. However, a load balancer based on consistent hashing might result in hot shards when the same long URL is entered by a large number of clients at the same time.\n\nThe output of the token service instances must be non-overlapping to prevent a collision. A highly reliable distributed service such as Apache Zookeeper or Amazon DynamoDB is used to coordinate the output range of token service instances. The service that coordinates the output range between token service instances is named the token range service.\n\nWhen the key-value store is chosen as the token range service, the quorum must be set to a higher value to increase the consistency of the token range service. The stronger consistency prevents a range collision by preventing fetching the same output range by multiple token services.\n\nWhen an instance of the token service is provisioned, the fresh instance executes a request for an output range from the token range service. When the fetched output range is fully exhausted, the token service requests a fresh output range from the token range service.\n\nThe token range service might become a bottleneck if queried frequently. Either the output range or the number of token range service replicas must be incremented to improve the reliability of the system. The token range solution is collision-free and scalable. However, the short URL is predictable due to the monotonically increasing output range. The following actions degrade the predictability of the shortened URL:\n• append random bits to the suffix of the output\n\nThe time complexity of short URL generation using token service is constant O(1). In contrast, the KGS must perform one of the following operations before shortening a URL to preserve the 1-to-1 mapping:\n• query the database to check the existence of the long URL\n• use the putIfAbsent procedure to check the existence of the long URL\n\nQuerying the database is an expensive operation because of the disk input/output (I/O) and most of the NoSQL data stores do not support the putIfAbsent procedure due to eventual consistency.\n\nA bloom filter is used to prevent expensive data store lookups on URL shortening. The time complexity of a bloom filter query is constant O(1). The KGS populates the bloom filter with the long URL after shortening the long URL. When the client enters a customized short URL, the KGS queries the bloom filter to check if the long URL exists before persisting the custom short URL into the data store. However, the bloom filter query might yield false positives, resulting in a database lookup. In addition, the bloom filter increases the operational complexity of the system.\n\nWhen the client enters an already existing long URL, the KGS must return the appropriate short URL but the database is partitioned with the short URL as the partition key. The short URL as the partition key resonates with the read and write paths of the URL shortener.\n\nA naive solution to finding the short URL is to build an index on the long URL column of the data store. However, the introduction of a database index degrades the write performance and querying remains complex due to sharding using the short URL key.\n\nThe optimal solution is to introduce an additional data store (inverted index) with mapping from the long URLs to the short URLs (key-value schema). The additional data store improves the time complexity of finding the short URL of an already existing long URL record. On the other hand, an additional data store increases storage costs. The additional data store is partitioned using consistent hashing. The partition key is the long URL to quickly find the URL record. A key-value store such as DynamoDB is used as the additional data store.\n\nThe token service stores some short URLs (keys) in memory so that the token service quickly provides the keys to an incoming request. The keys in the token service must be distributed by an atomic data structure to handle concurrent requests. The output range stored in token service memory is marked as used to prevent a collision. The downside of storing keys in memory is losing the specific output range of keys on a server failure. The output range must be moved out to an external cache server to scale out the token service and improve its reliability.\n\nThe output of the token service must be encoded within the token server using an encoding service to prevent external network communication. An additional function executes the encoding of token service output.\n\nIn summary, use the token range solution for shortening a URL.\n\nThe server redirects the shortened URL to the original long URL. The simplified block diagram of a single-machine URL redirection is the following:\n\nThe single-machine solution does not meet the scalability requirements of URL redirection for a read-heavy system. The disk I/O due to frequent database access is a potential bottleneck.\n\nThe URL redirection traffic (Egress) is cached following the 80/20 rule to improve latency. The cache stores the mapping between the short URLs and the long URLs. The cache handles uneven traffic and traffic spikes in URL redirection. The server must query the cache before hitting the data store. The cache-aside pattern is used to update the cache. When a cache miss occurs, the server queries the data store and populates the cache. The tradeoff of using the cache-aside pattern is the delay in initial requests. As the data stored in the cache is memory bound, the Least Recently Used (LRU) policy is used to evict the cache when the cache server is full.\n\nThe cache is introduced at the following layers of the system for scalability:\n\nA shared cache such as CDN or a dedicated cache server reduces the load on the system. On the other hand, the private cache is only accessible by the client and does not significantly improve the system’s performance. On top of that, the definition of TTL for the private cache is crucial because private cache invalidation is difficult. Dedicated cache servers such as Redis or Memcached are provisioned between the following system components to further improve latency:\n\nThe typical usage pattern of a URL shortener by the client is to shorten a URL and access the short URL only once. The cache update on a single access usage pattern results in cache thrashing. A bloom filter on short URLs is introduced on cache servers and CDN to prevent cache thrashing. The bloom filter is updated on the initial access to a short URL. The cache servers are updated only when the bloom filter is already set (multiple requests to the same short URL).\n\nThe cache and the data store must not be queried if the short URL does not exist. A bloom filter on the short URL is introduced to prevent unnecessary queries. If the short URL is absent in the bloom filter, return an HTTP status code of 404. If the short URL is set in the bloom filter, delegate the redirection request to the cache server or the data store.\n\nThe cache servers are scaled out by performing the following operations:\n• partition the cache servers (use the short URL as the partition key)\n• replicate the cache servers to handle heavy loads using leader-follower topology\n• redirect the write operations to the leader\n• redirect all the read operations to the follower replicas\n\nWhen multiple identical requests arrive at the cache server at the same time, the cache server will collapse the requests and will forward a single request to the origin server on behalf of the clients. The response is reused among all the clients to save bandwidth and system resources.\n\nThe following intermediate components are introduced to satisfy the scalability demand for URL redirection:\n\nThe reverse proxy is used as an API Gateway. The reverse proxy executes SSL termination and compression at the expense of increased system complexity. When an extremely popular short URL is accessed by thousands of clients at the same time, the reverse proxy collapse forwards the requests to reduce the system load. The load balancer must be introduced between the following system components to route traffic between the replicas or shards:\n\nThe CDN serves the content from locations closer to the client at the expense of increased financial costs. The Pull CDN approach fits the URL redirection requirements. A dedicated controller service is provisioned to automatically scale out or scale down the system components based on the system load.\n\nThe microservices architecture improves the fault tolerance of the system. The services such as Etcd or Zookeeper help services find each other (known as service discovery). In addition, the Zookeeper is configured to monitor the health of the services in the system by sending regular heartbeat signals. The downside of microservices architecture is the increased operational complexity.\n\nGet the powerful template to approach system design for FREE on newsletter sign-up:\n\nThe availability of the system is improved by the following configuration:\n• load balancer runs either in active-active or active-passive mode\n• KGS runs either in active-active or active-passive mode\n• back up the storage servers at least once a day to object storage such as AWS S3 to aid disaster recovery\n• rate-limiting the traffic to prevent DDoS attacks and malicious users\n\nRate limiting the system prevents malicious clients from degrading the service. The following entities are used to identify the client for rate limiting:\n\nThe API developer key is transferred either as a JSON Web Token (JWT) parameter or as a custom HTTP header. The Internet Protocol (IP) address is also used to rate limit the client.\n\nScaling a system is an iterative process. The following actions are repeatedly performed to scale a system:\n• profile for bottlenecks or a single point of failure (SPOF)\n\nThe read and write paths of the URL shortener are segregated to improve the latency and prevent network bandwidth from becoming a bottleneck.\n\nThe general guidelines to horizontally scale a service are the following:\n\nThe microservices architecture improves the fault tolerance of the system. The microservices are isolated from each other and the services will fail independently. Simply put, a service failure means reduced functionality without the whole system going down. For instance, the failure of a metric service will not affect the URL redirection requests. The event-driven architecture also helps to isolate the services and improve the reliability of the system, and scale by naturally supporting multiple consumers.\n\nThe introduction of a message queue such as Apache Kafka further improves the fault tolerance of the URL shortener. The message queue must be provisioned in the write and read paths of the system. However, the message queue must be checkpointed frequently for reliability.\n\nThe reasons for using a message queue are the following:\n\nThe tradeoffs of using a message queue are the following:\n\nFurther actions to optimize the fault tolerance of the system are the following:\n• services must exponentially backoff when a failure occurs for a faster recovery\n• snapshot or checkpoint stateful services such as a bloom filter\n\nThe following stateful services are partitioned for scalability:\n\nThe KGS must acquire a mutex (lock) or a semaphore on the atomic data structure distributing the short URLs to handle concurrency. The lock prevents the distribution of the same short URL to distinct shortening requests from the KGS, resulting in a collision. Multiple data structures distribute the short URL in a single instance of the token service to improve latency. The lock must be released when the short URL is used.\n\nWhen multiple clients enter the same long URL at the same time, all the clients must receive the same short URL. A naive approach to solving this problem is by introducing a message queue and grouping the duplicate requests. An alternative suboptimal approach is to use the collapsed forwarding feature of a reverse proxy. However, multiple reverse proxies might exist in a distributed system. The optimal solution is using a distributed lock such as the Chubby or the Redis lock. The distributed lock must be acquired on the long URL. The distributed lock internally uses Raft or Paxos consensus algorithm.\n\nThe distributed lock service must be used when the client enters a custom URL as well to prevent a collision. A reasonable TTL must be set on the distributed lock to prevent starvation. The tradeoff of using a distributed lock is the slight degradation of latency. The distributed lock service is unnecessary if the 1-to-1 mapping between the URLs is not a system requirement. The URL shortening workflow in a highly concurrent system is the following:\n• KGS checks the bloom filter for the existence of a long URL\n• Populate the bloom filter with the long URL\n\nThread safety in the bloom filter is achieved using a concurrent BitSet. The tradeoff is the increased latency to some write operations. A lock must be acquired on the bloom filter to handle concurrency. A variant of the bloom filter named naive striped bloom filter supports highly concurrent reads at the expense of extra space.\n\nThe generation of analytics on the usage pattern of the clients is one of the crucial features of a URL shortening service. Some of the popular URL-shortening services like Bitly make money by offering analytics on URL redirections. The HTTP headers of URL redirection requests are used to collect data for the generation of analytics. In addition, the IP address of the client identifies the country or location. The most popular HTTP headers useful for analytics are the following:\n\nThe workflow for the generation of analytics is the following:\n• The server responds with the long URL\n• The server puts a message on the message queue\n• The archive service executes a batch operation to move messages from the message queue to HDFS\n• Hadoop is executed on the collected data on HDFS to generate offline analytics\n\nA data warehousing solution such as Amazon Redshift or Snowflake might be used for the analytics database.\n\nThe expired records in the database are removed to save storage costs. Active removal of expired records in the database might overload the database and degrade the service. The approaches to the removal of the expired records from the database are the following:\n\nWhen the client tries to access an expired short URL, remove the record from the database. The server responds to the client with a status code of 404 Not found. On the other hand, if the client never visits an expired record, the record sits there forever and consumes storage space.\n\nA dedicated cleanup service must be executed during non-peak (low traffic) hours to remove the expired short URLs. The cleanup service scans the whole database for expired records. If the traffic suddenly spikes during the execution of the cleanup service, the cleanup service must be stopped, and the system fallbacks to the lazy removal approach.\n\nThe unused data in the database is archived to save storage costs. The data that is frequently accessed is classified as hot data and the data that was not accessed for a significant amount of time (assume a time frame of 3 years) is classified as cold data.\n\nThe last_visited timestamp column of the database is used for data classification. The cold data is compressed and stored in object storage (AWS S3) during non-peak hours to avoid degradation of the service. However, maintenance of object storage and data classification is an additional operational effort. The unused data must be archived only if you are certain that the data will not be accessed in the future.\n\nMonitoring is essential to identify system failures before they lead to actual problems to increase the availability of the system. Monitoring is usually implemented by installing an agent on each of the servers (services). The agent collects and aggregates the metrics and publishes the result data to a central monitoring service. Dashboards are configured to visualize the data.\n\nCentralized logging is required to quickly isolate the cause of a failure instead of hopping between servers. The popular log aggregation and monitoring services are fluentd and datadog. The sidecar design pattern is used to run the agent and collect metrics. The following metrics must be monitored in general to get the best results:\n\nThe following list covers some of the most popular security measures:\n• use the principle of least privilege\n\nThe URL shortening service is a popular system design interview question. Although the use cases of a URL shortener seem trivial, building an internet-scale URL shortener is a challenging task.\n\nWhat to learn next?\n\nGet the powerful template to approach system design for FREE on newsletter sign-up:\n\nIf you would like to challenge your knowledge on the topic, visit the article: Knowledge Test\n\nCC BY-NC-ND 4.0: This license allows reusers to copy and distribute the content in this article in any medium or format in unadapted form only, for noncommercial purposes, and only so long as attribution is given to the creator. The original article must be backlinked."
    },
    {
        "link": "https://stackoverflow.com/questions/59692297/designing-a-url-shortening-service-like-tinyurl",
        "document": "I'm reading an online document that explains how to design a url shortening service. The website is https://www.educative.io/courses/grokking-the-system-design-interview .\n\nIn the section, Encoding actual URL, they said -> \"We can compute a unique hash (e.g., MD5 or SHA256, etc.) of the given URL. The hash can then be encoded for displaying. This encoding could be base36 ([a-z ,0-9]) or base62 ([A-Z, a-z, 0-9]) and if we add ‘+’ and ‘/’ we can use Base64 encoding. A reasonable question would be, what should be the length of the short key? 6, 8, or 10 characters.\"\n\n\"If we use the MD5 algorithm as our hash function, it’ll produce a 128-bit hash value. After base64 encoding, we’ll get a string having more than 21 characters (since each base64 character encodes 6 bits of the hash value).Since we only have space for 8 characters per short key, how will we choose our key then? We can take the first 6 (or 8) letters for the key. This could result in key duplication, to resolve that, we can choose some other characters out of the encoding string or swap some characters.\"\n\nI used online MD5 hash generator (http://onlinemd5.com/) and Base64 encoder (https://www.base64encode.org/) to verify the above. I used \"www.yahoo.com\" as the input string for MD5 hash and output is 1B03577ED104F16AADC00A639D33CB44 . Then I Base64 encoded it and got MUIwMzU3N0VEMTA0RjE2QUFEQzAwQTYzOUQzM0NCNDQ= with UTF-8 destination charset and Unix newline seperator.\n\nCan anyone explain if I'm doing it correctly? I see the number of characters are way more than 21."
    },
    {
        "link": "https://geeksforgeeks.org/java-database-connectivity-with-mysql",
        "document": "In Java, we can connect our Java application with the MySQL database through the Java code. JDBC ( Java Database Connectivity) is one of the standard APIs for database connectivity, using it we can easily run our query, statement, and also fetch data from the database.\n\n\n\n\n\n 3. To set up the connectivity, the user should have MySQL Connector to the Java (JAR file), \n\n the 'JAR' file must be in classpath while compiling and running the code of JDBC. \n\n\n\n\n\nStep 1 – Search for MySQL community downloads.\n\nStep 2 – Go to the Connector/J.\n\nStep 3 – Select the Operating System platform-independent.\n\nStep 4 – Download the zip file Platform Independent (Architecture Independent), ZIP Archive.\n\nStep 5 – Extract the zip file.\n\nStep 6 – Get the mysql-connector-java-8.0.20.jar file from the folder.\n\nSetting up Database Connectivity with MySQL using\n\nUsers have to follow the following steps:\n\nStep 1 – Users have to create a database in MySQL (for example let the name of the database be ‘mydb’ ).\n\nStep 2 – Create a table in that database.\n\nThis is MySQL code for creating a table.\n\nStep 3 – Now, we want to access the data of this table using Java database connectivity.\n• None Now, inside gfg created two more directories one named as ‘ src ‘ and the other ‘ lib\n• None Put the MySQL connector java jar file in the lib folder.\n\nStep 4 – We will write connectivity code in the src folder, To write connectivity code user must know the following information:\n• Driver class :- The driver class for connectivity of MySQL database “com.mysql.cj.jdbc.Driver”, a fter the driver has been registered, we can obtain a Connection instance that is connected to a particular database by calling DriverManager.getConnection() :, in this method, we need to pass URL for connection and name and password of the database.\n• URL for Connection:- The connection URL for the mysql database is jdbc:mysql://localhost:3306/mydb (‘mydb’ is the name of database).\n\nTo get more clarification follow the connectivity code below:\n\nStep 5 – In this src code, we will set up the connection and get all the data from the table. we have created the ‘check.java‘ file in the src folder."
    },
    {
        "link": "https://hevodata.com/learn/how-to-connect-mongodb-jdbc",
        "document": "Being a cross-platform document-first NoSQL database program, MongoDB operates on JSON-like documents. On the other hand, JDBC is a Java application programming interface (API) used while executing queries in association with the database.\n\nUsing JDBC, you can seamlessly access any data source from any relational database in spreadsheet format or a flat file. In fact, all major service providers have their own JDBC drivers, which include different sets of Java classes to enable a particular connection to a particular database. Using the MongoDB JDBC connectivity, it’s easier to place a query with the database, introduce updates to the database, and call upon stored processes.\n\nIn this tutorial article, we will explore the MongoDB JDBC connect in detail. And to maintain a coerce understanding of the subject, introduce you to the concept of JDBC Connector and the basics and features of MongoDB in brief. Let’s begin.\n\nMongoDB is a document-oriented no-SQL database. Initially released in 2009, MongoDB has successfully altered the concept of rows and columns in conventional relational data models with documents.\n\nMongoDB provides today’s developers with the flexibility to work with evolving data models because its document-based and, allows embedded documents, arrays, and represents complex hierarchical data structures and relationships using a single record.\n\nMoreover, the fact that MongoDB is schema-free, the keys defined in the documents are not fixed; hence, data migration at a large scale can be ruled out. MongoDB stores data in BSON format, which is similar to JSON. A basic document storage structure looks like this:\n\nSome key features of MongoDB are as follows:\n\nIt’s Document-oriented: MongoDB does not break documents into multiple relational structures like RDBMS but instead stores the main subject in a minimal number of documents. It stores data in documents named “Computer” and not in CPU, RAM, Hard disk, etc. which are relational structures.\n\nIt’s a General Purpose Database with Flexible Schema Design: MongoDB can execute heterogeneous loads and multiple purposes within an application. Its document-oriented approaches with non-defined attributes that can be modified on the fly are a key contrast between MongoDB and any other relational database.\n\nIt’s Scalable: MongoDB is built to scale both horizontally and vertically. In terms of horizontal scalability, MongoDB uses sharding, and architecture users can share the load between multiple instances, achieving both read and write scalability. Load-balancing also happens automatically and transparently to the user by the shared balancer eliminating the need for complex data pipelines.\n\nJDBC stands for Java DataBase Connectivity. JDBC is a Java API that is used to connect and execute queries with the database. Developers use JDBC to access any kind of tabular, also including the data stored in a Relational Database.\n\nJDBC is generally used to connect java applications with databases and is usually installed automatically with the JDK software. On the other hand, the JDBC connector is a program that makes various possible databases to get accessed by Java application servers, run on Java 2 Platform, which is an enterprise edition Java programming language from Sun Microsystems.\n\nIn this section, you will go through the steps to set up a connection. To establish a Connect, follow the seven steps given below:\n\nIn the development phase, while establishing the MongoDB JDBC connect, it’s required to ensure that the JDBC Driver is added to the built path. To add JDBC driver to the built path, first, click on the package name in the Package Explorer section, then hit “Alt” + “Enter” and click on Java Build Path.\n\nNow, find and click on the “As External JARs” button and then find the newly downloaded JDBC Driver jar files. In the example given below, we are using MongoDB; hence we will choose unityjdbc.jar in addition to the mongo-java-driver-2.12.2.jar.\n\nUntil now in the process to establish Connect, we have successfully added an appropriate JDBC driver to the Java Built Path. Now, it’s required to import the java.sql.* classes as well for MongoDB JDBC connect.\n\nThe import statements given below will declare Java Class(es) import statements. After declaring the Java class, the class name is used, in the code, without having to classify the package it belongs to.\n\nA simple import statement can also be used as shown in the code below:\n\nFirst, to begin using the MongoDB JDBC Connect, it’s necessary to register the driver. Use the “Class.forName” method to register, as shown in the example below:\n\nNow that we have the MongoDB JDBC connection registered, it’s required to establish the connection to the database. Use the “DriverManager.getConnection” method as shown in the example below:\n\nNow the MongoDB JDBC connect is ready to use and get interact with. Use the JDBC createStatement(), prepareCall(), & prepareStatement() methods to establish a back and forth MongoDB JDBC connection that enables you to send and receive data in your database. Look at the code given below:\n\nStep 6: Now, Iterate Through the ResultSet\n\nThe ResultSet has records of data returning from a database query that is already executed. Now, when looking at the JavaDocs, a ResultSet is maintained and a cursor is pointing to its current row of data. At first glance, the cursor seems to be positioned before the first row. Now the “next” method is used to move the cursor to the next row.\n\nLook at the code given below for Integration to understand better:\n\nStep 7: At Last, Close the Connection\n\nIt’s necessary to ensure that the database connections and the resources at the end of the program are closed. It’s considered to be a poor programming practice if not able to comply. Look at the code given below:\n\nMoreover, a sample JDBC program to establish MongoDB JDBC connect looks like this:\n\nAnd, the output for MongoDB JDBC connect looks like this:\n\nIn this tutorial article, we discussed MongoDB JSON connectivity using JSON Connector. This combination is a powerful way to manage and execute queries for data stored in your database. And, if you want to learn more about the Connect in more detail, either of these two articles can help a great deal.\n\nHowever, as a Developer, extracting complex data from a diverse set of data sources like Databases, CRMs, Project management Tools, Streaming Services, Marketing Platforms to your MongoDB Database, or maintaining the Connect can seem to be quite challenging. This is where a simpler alternative like Hevo can save your day!\n\nAlso, don’t forget to share your experience of establishing Connect in the comments section below!\n\nWant to take Hevo for a spin? Sign Up or a 14-day free trial and experience the feature-rich Hevo suite firsthand. Also checkout our unbeatable pricing to choose the best plan for your organization."
    },
    {
        "link": "https://mongodb.com/docs/atlas/data-federation/query/sql/drivers/jdbc/connect",
        "document": "To connect to your federated database instance, create a connection string and open a connection from your application. The connection string for the JDBC driver follows the format of the standard MongoDB connection string, except with the prefix:\n\nYou can get the connection string from the Atlas UI. To get the connection string from the Atlas UI, do the following:\n• None In the Atlas UI, go to the Data Federation page and click Connect for the federated database instance that you want to connect to.\n• None Under Access your data through tools, select Atlas SQL.\n• None Under Select your driver, select JDBC Driver from the dropdown.\n• None Under Get Connection String, select the database that you want to connect to and copy the connection string.\n\nThe following example demonstrates how to open a connection. In addition to the connection string, you must also specify the database to use through a object parameter. To learn more, see Connection Strings and Connection Properties .\n\nThe driver can only connect to Atlas and not to a instance. Any special characters in the connection string for the JDBC driver must be URL encoded."
    },
    {
        "link": "https://codejava.net/java-se/jdbc/java-connecting-to-mongodb-database-examples",
        "document": "This tutorial helps you write Java code that makes connection to MongoDB database.\n\nYou know, MongoDB is the leading NoSQL database system which has become very popular for recent years due to its dynamic schema nature and advantages over big data like high performance, horizontal scalability, replication, etc. Unlike traditional relational database systems which provide JDBC-compliant drivers, MongoDB comes with its own non-JDBC driver called Mongo Java Driver. That means we cannot use JDBC API to interact with MongoDB from Java. Instead, we have to use its own Mongo Java Driver API.\n\nClick here to download latest version of Mongo Java Driver (version 2.11.1 as of this writing). The JAR file name is mongo-java-driver-VERSION.jar (around 400KB). Copy the downloaded JAR file into your classpath.\n\nOnline API documentation for Mongo Java Driver can be found here.\n\nThe MongoClientclass is used to make a connection with a MongoDB server and perform database-related operations. Here are some examples:\n• Creating a instance that connects to a default MongoDB server running on localhost and default port:\n• Connecting to a named MongoDB server listening on the default port (27017): \n\n Or:\n\n\n\nAfter the connection is established, we can obtain a database and make authentication (if the server is running in secure mode), for example:\n\nBy default, MongoDB server is running in trusted mode which doesn’t require authentication.\n\nThis Java program connects to a MongoDB server running on localhost at default port, then lists all database names available on the server. For each database, it lists all collection names (a collection is equivalent to a table in relational database), and finally closes the connection. This program would produce the following output:\n\nIt’s also possible to use a String that represents a database connection URI to connect to the MongoDB server, for example:\n\nSyntax of the URI is as follows:\n\nHere are some connection string URI examples:\n• Connecting to the MongoDB server running on localhost at the default port:\n• Connecting to the admin database on a named MongoDB server db1.server.com running on port 27027 with user root and password secret:\n• Connecting to the users database on server db2.server.com:\n• Connecting to the products database on a named MongoDB server db3.server.com running on port 27027 with user tom and password secret:\n• Connecting to a replica set of three servers:\n\nTo see the coding in action, I recommend you to watch this video:\n• How to Read Database Meta Data in JDBC\n• How to call stored procedure with JDBC"
    },
    {
        "link": "https://medium.com/mongodb/best-databases-for-java-exploring-mongodb-libraries-and-jdbc-8ed7d63c7067",
        "document": "In modern Java development, selecting the best databases for Java applications is critical to ensuring efficient and scalable solutions. While relational databases have long dominated the ecosystem through APIs like Java Database Connectivity (JDBC) and libraries like Hibernate, NoSQL databases like MongoDB offer unparalleled flexibility.\n• Why MongoDB is considered one of the best databases for Java.\n• The essentials of working with MongoDB drivers and their modern alternatives.\n• Practical steps to integrate MongoDB with Java applications using frameworks like Spring Data, Quarkus, Micronaut, etc.\n\nWhy MongoDB stands out among the best databases for Java\n\nDevelopers often face the challenge of mapping objects to relational data structures when working with Java. MongoDB addresses this gap by storing data in BSON (Binary JSON), which closely resembles the core Java side’s POJO (Plain Old Java Object) model.\n• Schema flexibility: Unlike relational databases, MongoDB allows dynamic schemas, making it ideal for evolving data models.\n• Simplified integration: With the MongoDB Java Driver, developers can bypass complex JDBC mappings.\n• Rich query capabilities: MongoDB supports advanced queries, aggregations, and geospatial operations, making it one of the best databases for modern Java use cases.\n\nMongoDB: The best database for Java developers\n\nMongoDB is a top choice for Java developers, offering flexibility and performance by storing data as BSON documents, similar to JSON. Unlike relational databases, where you generally have to write SQL statements and perform complex queries, MongoDB allows dynamic schemas, making it perfect for evolving data structures in modern apps.\n\nWith MongoDB Atlas, a fully managed service on AWS, Google Cloud, and Azure, developers enjoy features like automated scaling, backups, and monitoring — eliminating the need for traditional JDBC setups.\n\nMongoDB’s powerful query language supports complex aggregations, geospatial queries, text searches, and vector-based searches. With the MongoDB Java Driver and Spring Data MongoDB, developers can easily build scalable, efficient, and data-driven applications.\n\nIntegration of MongoDB with the Java programming language opens the door to powerful, flexible, and efficient database interactions tailored to modern application requirements. In this tutorial, we will begin by exploring the integration options with the simplest and most direct method of making use of the MongoDB Java drivers.\n\nAfter establishing the database connectivity with the basics, we will understand how we can make the Java database connectivity connection with the popular Java frameworks. The frameworks include Spring Data, Quarkus, and Micronaut. We will also explore other frameworks, such as Helidon and Hibernate OGM, and learn how to establish a connection with MongoDB.\n\nFinally, we will delve into how MongoDB can integrate with Java through JDBC. By leveraging MongoDB’s Data Federation, developers can query MongoDB collections using SQL via JDBC, providing a bridge between traditional relational database tools and the flexible NoSQL capabilities of MongoDB.\n\nStep-by-step guide: Connect Java applications to MongoDB using the Java Driver\n\nConnecting the Java application with MongoDB’s Java Driver is the most basic way to connect the application with the database. To connect using the MongoDB Java Driver, add the Java code below to the Maven or Gradle applications to install the dependencies related to the driver class making the connection.\n\nOnce the driver has been installed in the system, the next step is to make the database connection with the MongoDB URI. In this article, we will focus on the connection with the Atlas URI.\n\nTo get your Atlas connection string, follow the steps to create your first free cluster. Once done, keep your connection string safe as we will be using it in different applications with different frameworks.\n\nAfter successfully installing the required drivers, the next step is to establish a connection to the same database server. For connecting your Java application to MongoDB Atlas, use the following code snippet to initiate a secure connection to your Atlas cluster.\n\nIt is important to note that the connection could directly be hardcoded inside the class. For security reasons, it is always recommended to get the connection string from the environment variables or application files.\n\nAfter the connection is established successfully, the next step is to perform operations on the collections for retrieving data, etc. To do so, the below code examples perform the basic CRUD operations on the documents.\n\nTo understand more about how to perform operations on the documents, you can clone the GitHub repository or go through the Getting Started With MongoDB and Java tutorial.\n\nJava has long been a cornerstone of enterprise development, known for its robust and versatile capabilities. Frameworks like Spring Data, Quarkus, Hibernate, and Helidon have simplified development, boosted productivity, and streamlined database integration.\n\nWith evolving technology, MongoDB has become a top choice for modern applications. Its NoSQL architecture complements Java frameworks by offering unmatched flexibility and scalability for handling diverse data types. Whether leveraging the reactive programming of Quarkus, the simplicity of Spring Data, or Helidon’s lightweight microservices, MongoDB ensures seamless integration for modern development needs.\n\nIn the upcoming sections, we’ll explore how Java frameworks connect with MongoDB, enabling efficient data handling, from basic operations to advanced queries, for scalable and flexible applications.\n\nOne of the most efficient and widely adopted methods for connecting Java applications to MongoDB is by leveraging Spring Data MongoDB. This approach simplifies database interactions, offering a consistent and developer-friendly way to work with data while retaining the unique characteristics of MongoDB, making it a top choice among the best databases for Java applications.\n\nSpring Data MongoDB provides two primary ways to establish the database connection: MongoRepository and MongoTemplate. These methods empower developers to perform CRUD operations and implement complex queries seamlessly, making the integration process straightforward. By automating much of the boilerplate code, Spring Data ensures developers can focus on building robust applications rather than managing database intricacies.\n\nTo set up the connection, you’ll need to configure your project files based on your build tool. For Maven projects, update the pom.xml, and for Gradle projects, modify the Gradle build configuration files. Detailed steps for downloading and configuring the MongoDB Java Driver are available in MongoDB’s official documentation. Once the drivers are installed, the next step involves adding the necessary configuration to your application.properties file to establish the database connection and get started with MongoDB and Spring Data MongoDB.\n\nPlatforms like Spring Initialiser allow you to generate dependency installation files easily. For example, if you wish to develop a simple REST-based application using Spring Boot, go to Spring Initializr, select the Spring Data MongoDB and Spring Web dependencies, and click on “Generate.”\n\nAfter the dependencies are installed, put the connection starting into the application.properties files as:\n\nThe next step is making the connection with the MongoDB Atlas database. To do so, the below code snippet will configure the Spring application to connect to the database and then allow it to perform the operations.\n\nOnce the connection is successfully established, you can leverage either MongoRepository or MongoTemplate to interact with the database. These tools allow you to perform essential CRUD (Create, Read, Update, Delete) operations effortlessly while also supporting advanced features such as search queries and complex aggregations. This flexibility ensures that your application can handle both simple and intricate data requirements seamlessly.\n\nFor example, if you wish to perform a simple find() operation, the below code can be used.\n\nSimilarly, to perform complex query aggregations, use the below code snippet example, which performs the aggregation to find the total spent.\n\nUsing the MongoTemplate operations, you can perform complex aggregations, text search, and vector search in addition to the basic CRUD operations.\n\nTo learn more, the GitHub repository and our article on advanced aggregations with Spring Boot have the complete code to perform CRUD and complex aggregation operations.\n\nAfter connecting with the Spring Boot application and learning how to perform complex queries and aggregations, let us understand another framework called Quarkus to connect with the MongoDB database.\n\nQuarkus, known for its lightweight framework and robust extensions, is an excellent choice for building high-performance applications. When paired with MongoDB, a scalable document-oriented database, it enables the creation of advanced search solutions that surpass traditional keyword-based methods, offering a more context-aware and intuitive search experience.\n\nTo make the connection with the database in the Quarkus application, similar to the Spring application, you need the Quarkus initialiser to generate the pom.xml or Gradle configuration files as:\n\nAfter the project has installed the dependency, similar to the Spring application, we need to set the connection starting in the application.properties file as:\n\nOnce the connection is made, the next step is to perform operations on all the records and documents inside the collection. Generally, Quarkus uses Panache to do so.\n\nPanache in Quarkus simplifies data access by minimizing boilerplate with built-in CRUD operations and type-safe queries. It supports both the Active Record and Repository patterns, allowing flexibility in architecture. Panache is tightly integrated with Quarkus, leveraging fast startup times and a low memory footprint, making it ideal for cloud-native and serverless environments.\n\nAs said in the above example, using Panache reduces the boilerplate code for you. You just need to put the business logic in the resource files. For example, if you wish to perform CRUD, you can use the below code:\n\nSimilarly, you can perform other operations as explained in our article on building a complete application with Quarkus and Panache. Apart from this, you can also perform vector search using the Quarkus applications. Our article on building applications to perform vector search is a good starting point to learn.\n\nConnecting your Micronaut Java application with the MongoDB database\n\nAfter exploring how to connect Java applications to databases using frameworks like Spring Data and Quarkus, let’s turn our attention to Micronaut — a modern Java framework designed for building server-side applications in a microservice architecture.\n\nMicronaut is engineered to optimize performance by minimizing code reflection, which results in faster application startup times and lower memory usage. Its key features include aspect-oriented programming, inversion of control (IoC), and automatic code configuration, making it a powerful choice for developers aiming to build lightweight, efficient applications.\n\nTo connect your application to a database using Micronaut, you’ll first need to add the required dependency to your pom.xml file (for Maven projects) or build configuration (for Gradle projects). This simple step ensures your project is set up to leverage Micronaut’s capabilities for seamless database integration.\n\nThe first step is to go to the initialiser and create a sample application, as given in the image below:\n\nOnce the project is initialised, make the connection with the Java database and perform the operations. To make the connection with the Java database, copy the connection string as:\n\nOnce the Java application has established a successful connection with the database, you are all set to execute queries to perform the data modification operations. For example, if you wish to execute queries to perform the CRUD operations, you can use the below sample snippet:\n\nAs the above example code snippet shows, Micronaut’s built-in support for MongoDB integration simplifies database interactions, enabling developers to perform CRUD operations and run queries with minimal boilerplate code. Additionally, Micronaut’s lightweight and fast startup nature, combined with features like compile-time dependency injection and configuration, makes it an excellent choice for building scalable and efficient applications that use MongoDB as their database.\n\nHeldion also uses another Java framework to create faster and more responsive microservice-based applications. This helps to develop lightweight, speedy applications using a microservices architecture.\n\nIt features a powerful and fast web core supported by Java virtual threads, providing a solid foundation for modern applications. By following a few simple steps, you will learn how to set up a Java application that seamlessly integrates with MongoDB, enabling you to concentrate on building scalable, high-performance services effortlessly.\n\nTo create the Helidon project from scratch, the Helidon initialiser helps you create the project with all the dependencies. The below screenshot represents how to create the first sample project.\n\nAfter the dependency has been installed, the next step is to make the connection of the Java application with the MongoDB database. In this example, we will use the MongoDB connection string. To do so, use the below code snippet inside the src/main/resources/META-INF/microprofile-config.properties file.\n\nOnce the connection is established, the application is all set to create the REST API calls to perform the basic CRUD, as well as complex queries or other large aggregations or other business operations. The article on an introduction to MongoDB and Helidon will be a good starting point for creating your first sample applications with Helidon.\n\nHibernate has been a trusted ORM framework for Java developers working with SQL databases, simplifying interactions between Java objects and relational database tables. With features like HQL, caching, and transaction management, it remains a cornerstone for structured data management in SQL databases like MySQL and PostgreSQL.\n\nAs modern applications demand the flexibility of NoSQL databases, Hibernate extended its capabilities with Hibernate OGM (Object/Grid Mapper). Hibernate OGM allows developers to use familiar Hibernate concepts to interact with NoSQL databases like MongoDB, bridging the gap between SQL and NoSQL.\n\nJava developers with Hibernate experience can start using MongoDB without a steep learning curve.\n\nExisting Hibernate projects can be extended to support MongoDB with minimal modifications, enabling hybrid solutions with both SQL and NoSQL databases.\n\nIt is, however, important to note that Hibernate OGM is no longer actively developed or supported, with its last update released in 2018. There are no current plans for further development.\n\nMongoDB integrates with various Java frameworks, streamlining development. Additionally, MongoDB JDBC enables developers to query MongoDB collections using SQL, easing the transition for those familiar with relational databases.\n\nMongoDB’s Atlas Data Federation allows you to query, transform, and move data across multiple sources, both within and outside of MongoDB Atlas. It supports data access from data sources in various storage formats, including Atlas clusters, online archives, AWS S3 buckets, and HTTP stores, enabling flexible data management and insights.\n\nYou can use JDBC drivers for querying across these data source sources, though note that the Atlas SQL interface only supports read operations across data sources, not writes. For detailed connection steps, refer to the documentation on using JDBC drivers.\n\nHowever, it is important to note that the Atlas SQL Interface only supports read operations. With the Atlas SQL Interface, you cannot write data to your Atlas cluster.\n\nYou can use the JDBC driver to connect to SQL-based Java applications that accept a connection interface via JDBC API. To learn more and follow the steps to make the connection, you can follow the documentation on connecting with JDBC drivers and perform the query operations.\n\nWhy is MongoDB considered the best database for Java?\n\nThe modern application built today performs operations on unstructured data. The schemaless and flexible storage of MongoDB makes it ideal for the storage of unstructured data. MongoDB stores the data in the BSON format which is similar to the POJOs (Plain Old Java Objects). This simplifies database operations and reduces the need for complex object-relational mapping (ORM).\n• Schema flexibility: MongoDB’s dynamic schemas allow for easier adaptation to evolving data models.\n• Simplified integration: The MongoDB Java Driver eliminates the complexities of traditional JDBC-based integration.\n• Rich query capabilities: MongoDB supports advanced queries, aggregations, and geospatial operations, which are essential for modern applications.\n\nWhat is Spring Data MongoDB, and how does it help Java developers?\n\nSpring Data MongoDB is a part of the larger Spring Data project and provides easy integration of MongoDB with Spring-based applications. It offers a repository model to simplify CRUD operations and supports complex queries, indexing, and aggregation. With Spring Data MongoDB, developers can bypass the need for boilerplate code and focus more on business logic.\n\nIn addition to Spring Data and Quarkus, frameworks like Hibernate (via Hibernate OGM), Micronaut, and Helidon can be used to integrate MongoDB with Java. These frameworks offer varying degrees of support for MongoDB, with some providing additional capabilities like reactive programming (in Quarkus) or microservice architecture (in Micronaut).\n\nIn conclusion, MongoDB stands out as one of the best databases for Java applications, combining the flexibility of NoSQL with seamless integration options for modern Java frameworks. Its JSON-like BSON format aligns naturally with Java’s POJO model, eliminating the complexities of traditional database object-relational mapping. With robust features like dynamic schemas, advanced query capabilities, and compatibility with tools like Spring Data, Quarkus, and Hibernate, MongoDB empowers developers to build scalable and efficient database applications tailored to evolving data needs.\n\nBy leveraging MongoDB’s Java Driver and its fully managed Atlas platform, Java developers can streamline development while maintaining high performance and reliability. Whether working on basic CRUD operations, complex aggregations, or vector-based searches, MongoDB ensures a developer-friendly experience with powerful APIs and frameworks, making it an essential choice for modern Java developers."
    }
]