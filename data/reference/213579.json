[
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html",
        "document": "Applies a 2D convolution over an input signal composed of several input planes.\n\nIn the simplest case, the output value of the layer with input size (N,Cin​,H,W) and output (N,Cout​,Hout​,Wout​) can be precisely described as:\n\nwhere ⋆ is the valid 2D cross-correlation operator, N is a batch size, C denotes a number of channels, H is a height of input planes in pixels, and W is width in pixels.\n\nOn certain ROCm devices, when using float16 inputs this module will use different precision for backward.\n• None controls the stride for the cross-correlation, a single number or a tuple.\n• None controls the amount of padding applied to the input. It can be either a string {‘valid’, ‘same’} or an int / a tuple of ints giving the amount of implicit padding applied on both sides.\n• None controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this link has a nice visualization of what does.\n• None controls the connections between inputs and outputs. and must both be divisible by . For example,\n• None At groups=1, all inputs are convolved to all outputs.\n• None At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.\n• None At groups= , each input channel is convolved with its own set of filters (of size in_channelsout_channels​).\n\nThe parameters , , , can either be:\n\nWhen and , where is a positive integer, this operation is also known as a “depthwise convolution”. In other words, for an input of size (N,Cin​,Lin​), a depthwise convolution with a depthwise multiplier can be performed with the arguments (Cin​=Cin​,Cout​=Cin​×K,...,groups=Cin​).\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting . See Reproducibility for more information."
    },
    {
        "link": "https://pytorch.org/docs/stable/nn.html",
        "document": "These are the basic building blocks for graphs:\n\nApplies a 1D convolution over an input signal composed of several input planes. Applies a 2D convolution over an input signal composed of several input planes. Applies a 3D convolution over an input signal composed of several input planes. Applies a 1D transposed convolution operator over an input image composed of several input planes. Applies a 2D transposed convolution operator over an input image composed of several input planes. Applies a 3D transposed convolution operator over an input image composed of several input planes. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. Combines an array of sliding local blocks into a large containing tensor.\n\nApplies a 1D max pooling over an input signal composed of several input planes. Applies a 2D max pooling over an input signal composed of several input planes. Applies a 3D max pooling over an input signal composed of several input planes. Applies a 1D average pooling over an input signal composed of several input planes. Applies a 2D average pooling over an input signal composed of several input planes. Applies a 3D average pooling over an input signal composed of several input planes. Applies a 2D fractional max pooling over an input signal composed of several input planes. Applies a 3D fractional max pooling over an input signal composed of several input planes. Applies a 1D power-average pooling over an input signal composed of several input planes. Applies a 2D power-average pooling over an input signal composed of several input planes. Applies a 3D power-average pooling over an input signal composed of several input planes. Applies a 1D adaptive max pooling over an input signal composed of several input planes. Applies a 2D adaptive max pooling over an input signal composed of several input planes. Applies a 3D adaptive max pooling over an input signal composed of several input planes. Applies a 1D adaptive average pooling over an input signal composed of several input planes. Applies a 2D adaptive average pooling over an input signal composed of several input planes. Applies a 3D adaptive average pooling over an input signal composed of several input planes.\n\nPads the input tensor using the reflection of the input boundary. Pads the input tensor using the reflection of the input boundary. Pads the input tensor using the reflection of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with a constant value. Pads the input tensor boundaries with a constant value. Pads the input tensor boundaries with a constant value. Pads the input tensor using circular padding of the input boundary. Pads the input tensor using circular padding of the input boundary. Pads the input tensor using circular padding of the input boundary.\n\nCreates a criterion that measures the mean absolute error (MAE) between each element in the input x and target y. Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input x and target y. This criterion computes the cross entropy loss between input logits and target. Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities: This loss combines a layer and the in one single class. Creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch or 0D , and a label 1D mini-batch or 0D y (containing 1 or -1). Measures the loss given an input tensor x and a labels tensor y (containing 1 or -1). Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x (a 2D mini-batch ) and output y (which is a 2D of target class indices). Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. Creates a criterion that optimizes a two-class classification logistic loss between input tensor x and target tensor y (containing 1 or -1). Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input x and target y of size (N,C). Creates a criterion that measures the loss given input tensors x1​, x2​ and a label y with values 1 or -1. Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input x (a 2D mini-batch ) and output y (which is a 1D tensor of target class indices, 0≤y≤x.size(1)−1): Creates a criterion that measures the triplet loss given an input tensors x1, x2, x3 and a margin with a value greater than 0. Creates a criterion that measures the triplet loss given input tensors a, p, and n (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").\n\nClip the gradient norm of an iterable of parameters. Clip the gradient norm of an iterable of parameters. Clip the gradients of an iterable of parameters at specified value. Compute the norm of an iterable of tensors. Scale the gradients of an iterable of parameters given a pre-calculated total norm and desired max norm. Utility functions to flatten and unflatten Module parameters to and from a single vector. Flatten an iterable of parameters into a single vector. Copy slices of a vector into an iterable of parameters. Fuse a convolutional module and a BatchNorm module into a single, new convolutional module. Fuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters. Fuse a linear module and a BatchNorm module into a single, new linear module. Fuse linear module parameters and BatchNorm module parameters into new linear module parameters. Convert of to The conversion recursively applies to nested , including . Utility functions to apply and remove weight normalization from Module parameters. Apply weight normalization to a parameter in the given module. Apply spectral normalization to a parameter in the given module. Given a module class object and args / kwargs, instantiate the module without initializing parameters / buffers. Abstract base class for creation of new pruning techniques. Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. Prune (currently unpruned) units in a tensor at random. Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. Prune entire (currently unpruned) channels in a tensor at random. Prune entire (currently unpruned) channels in a tensor based on their L -norm. Prune tensor by removing units with the lowest L1-norm. Prune tensor by removing random channels along the specified dimension. Prune tensor by removing channels with the lowest L -norm along the specified dimension. Globally prunes tensors corresponding to all parameters in by applying the specified . Prune tensor corresponding to parameter called in by applying the pre-computed mask in . Remove the pruning reparameterization from a module and the pruning method from the forward hook. Check if a module is pruned by looking for pruning pre-hooks. Parametrizations implemented using the new parametrization functionality in . Apply an orthogonal or unitary parametrization to a matrix or a batch of matrices. Apply weight normalization to a parameter in the given module. Apply spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the Parametrizations tutorial for more information on how to implement your own parametrizations. Remove the parametrizations on a tensor in a module. Context manager that enables the caching system within parametrizations registered with . A sequential container that holds and manages the original parameters or buffers of a parametrized . Utility functions to call a given Module in a stateless manner. Perform a functional call on the module by replacing the module parameters and buffers with the provided ones. Holds the data and list of of a packed sequence."
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html",
        "document": "Your models should also subclass this class.\n\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes:\n\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call , etc.\n\nThe hook will be called every time before is invoked. If is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won’t be passed to the hooks and only to the . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature: If is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature:\n• None hook (Callable) – The user defined hook to be registered.\n• None prepend (bool) – If true, the provided will be fired before all existing hooks on this . Otherwise, the provided will be fired after all existing hooks on this . Note that global hooks registered with will fire before all hooks registered by this method. Default:\n• None with_kwargs (bool) – If true, the will be passed the kwargs given to the forward function. Default: a handle that can be used to remove the added hook by calling\n\nThe hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature: The and are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of in subsequent computations. will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in and will be for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module’s forward function. Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.\n• None hook (Callable) – The user-defined hook to be registered.\n• None prepend (bool) – If true, the provided will be fired before all existing hooks on this . Otherwise, the provided will be fired after all existing hooks on this . Note that global hooks registered with will fire before all hooks registered by this method. a handle that can be used to remove the added hook by calling\n\nThe hook will be called every time the gradients for the module are computed. The hook should have the following signature: The is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of in subsequent computations. Entries in will be for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module’s forward function. Modifying inputs inplace is not allowed when using backward hooks and will raise an error.\n• None hook (Callable) – The user-defined hook to be registered.\n• None prepend (bool) – If true, the provided will be fired before all existing hooks on this . Otherwise, the provided will be fired after all existing hooks on this . Note that global hooks registered with will fire before all hooks registered by this method. a handle that can be used to remove the added hook by calling"
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html",
        "document": "Click here to download the full example code\n\nNeural networks can be constructed using the package.\n\nNow that you had a glimpse of , depends on to define models and differentiate them. An contains layers, and a method that returns the .\n\nFor example, look at this network that classifies digit images:\n\nIt is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n\nA typical training procedure for a neural network is as follows:\n• None Define the neural network that has some learnable parameters (or weights)\n• None Compute the loss (how far is the output from being correct)\n• None Update the weights of the network, typically using a simple update rule:\n\nA loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target. There are several different loss functions under the nn package . A simple loss is: which computes the mean-squared error between the output and the target. # make it the same shape as output Now, if you follow in the backward direction, using its attribute, you will see a graph of computations that looks like this: So, when we call , the whole graph is differentiated w.r.t. the neural net parameters, and all Tensors in the graph that have will have their Tensor accumulated with the gradient. For illustration, let us follow a few steps backward: <MseLossBackward0 object at 0x7fa1139a9d80> <AddmmBackward0 object at 0x7fa1139a8b50> <AccumulateGrad object at 0x7fa1139ab2b0>\n\nTo backpropagate the error all we have to do is to . You need to clear the existing gradients though, else gradients will be accumulated to existing gradients. Now we shall call , and have a look at conv1’s bias gradients before and after the backward. # zeroes the gradient buffers of all parameters conv1.bias.grad before backward None conv1.bias.grad after backward tensor([ 0.0081, -0.0080, -0.0039, 0.0150, 0.0003, -0.0105]) Now, we have seen how to use loss functions. The neural network package contains various modules and loss functions that form the building blocks of deep neural networks. A full list with documentation is here. The only thing left to learn is:\n• None Updating the weights of the network"
    },
    {
        "link": "https://discuss.pytorch.org/t/creating-a-custom-convolutional-layer/191654",
        "document": "I’m trying to write a custom convolutional layer using nn.Conv2d as a guide. The python source for nn.Conv2d calls torch.functional.conv2d (torch/csrc/api/include/torch/nn/functional/conv.h#L72) which is a C++ function. I found torch::nn::Conv2dImpl (torch/csrc/api/src/nn/modules/conv.cpp#L81) but that I don’t see where that is used. I could do what I want all in python, but it will be a lot slower. I was hoping the implementation for Conv2d would be well enough factored that it would be easy to implement my own custom version, but maybe that isn’t the case. My goal is a performant custom PyTorch layer where an offset patch is applied to the input before weights are applied instead of applying a single bias scalar after the weights. Is there a better place to ask this?\n\nConv2d takes an image and convolves A over it then adds a scalar bias: y=Ax+b I want an offset image patch B that is the same size as A such that I calculate y=A(x-B). That’s an implementation detail that isn’t super important for my actual question. I kind didn’t want to mention it because I thought it would be a distraction. I need to do something between the convolution and the weight multiplication. I haven’t found any hooks in the code to do that. Conv2d’s implementation is in C++ for a reason. I’m assuming it is for performance.\n\nPerhaps I am a bit unclear by your written out definition above and this: I want an offset image patch B that is the same size as A such that I calculate y=A(x-B). x is the image, correct? Which would make A the kernels(or weights) and b the bias. So is B an input image or a second set of weights? By the way, you might find it helpful to know you can pass in a custom set of kernels and your image by using . See here: But to make those kernels learnable, you’ll need to wrap them in in your init.\n\nI suppose in those equations x is a convolution patch of the image. A, B, and x are the same shape. B is a learnable parameter containing a weight for each element of the convolution patch x. B is subtracted from x before taking the dot product with A. The details of this aren’t super important to my actual question. When you say “pass in a custom set of kernels” do you mean the weights of A? I know that. It doesn’t help me change the function being computed. I don’t see anything in the F.conv2d code that takes a lambda. F.conv2d delegates its implementation to C++. I can find the declaration for that function, but I can’t find the definition. Does anyone know where it is defined?\n\nThe convolution operation is pretty straight forward. First, your image is converted into a Toeplitz matrix, based on the kernel, stride, dilation and padding settings. Then that and the kernels undergo a matmul. As far as I know, the process behind the scenes does so a little more efficiently than what I just described. So let’s suppose A and B are learnable parameters. Do you want B applied as a kernel, or just a simple elementwise subtraction? If the latter, you just define A and B in your init as learnable parameters via the class. B should be the same shape as the incoming image. A, however, would not be the same shape, unless under very specific circumstances, such as when the stride and kernel are the same size as the image. The kernel is defined based on the settings passed into the convolution layer. In the forward pass, you can just subtract B from the image before running the convolution. For example: def forward(self, x): x = x - self.B return F.conv2d(x, self.A, ...) #fill in your desired conv settings here\n\nI have an implementation using tensors. It is slow. What takes around 5s with Conv2d takes around 116s with my custom Conv2dPS. Are there any obvious performance improvements I can make to this? class Conv2dPS(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, padding=0): super(Conv2dPS, self).__init__() # Initialize parameters for the convolution layer # in_channels: number of input channels # out_channels: number of output channels # kernel_size: size of the convolution kernel # Define A (weights) for the convolution layer # Use nn.Parameter to create a learnable parameter for the weights self.A = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size)) # Define B (offset) as a learnable parameter # B should have the same dimensions as the kernel self.B = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size)) # Initialize A and B with appropriate initialization methods, e.g., Xavier initialization nn.init.xavier_uniform_(self.A) nn.init.xavier_uniform_(self.B) # Set padding for the convolution operation self.padding = padding def convolve(self, x): batch_size = x.shape[0] image_channels = x.shape[1] image_height = x.shape[2] image_width = x.shape[3] out_channels = self.A.shape[0] in_channels = self.A.shape[1] kernel_height = self.A.shape[2] kernel_width = self.A.shape[3] assert(image_channels == in_channels) assert(kernel_height == kernel_width) # F.unfold takes an input tensor and extracts sliding local blocks (or patches) from it. # These blocks are the regions of the input tensor over which the convolution operation # (filter application) will take place. # x_unfolded, will have a shape of [batch_size, in_channels * kernel_height * kernel_width, num_patches] # The output will look something like this: # tensor([[[ 1., 2., 3., 5., 6., 7., 9., 10., 11.], # [ 2., 3., 4., 6., 7., 8., 10., 11., 12.], # [ 5., 6., 7., 9., 10., 11., 13., 14., 15.], # [ 6., 7., 8., 10., 11., 12., 14., 15., 16.]]]) # The first patch is the first element of each row: [1, 2, 5, 6] x_unfolded = F.unfold(x, kernel_size=kernel_height, padding=self.padding) unfolded_batch_size = x_unfolded.shape[0] unfolded_patch_size = x_unfolded.shape[1] num_patches = x_unfolded.shape[2] assert(unfolded_batch_size == batch_size) assert(unfolded_patch_size == in_channels * kernel_height * kernel_width) # Reshape x_unfolded into a format that aligns with the convolution weights A # transpose dimensions 1 and 2 above into [batch, num_patches, in_channels * kernel_height * kernel_width] # then view as [batch, num_patches, in_channels, kernel_height, kernel_width] x_unfolded = x_unfolded.permute(0, 2, 1).view(batch_size, num_patches, in_channels, kernel_height, kernel_width) # Expand x_unfolded across output_channels to match the dimensions of B_expanded x_expanded = x_unfolded.unsqueeze(2).expand(batch_size, num_patches, out_channels, in_channels, kernel_height, kernel_width) return x_expanded def subtract_offset(self, x_convolve): batch_size = x_convolve.shape[0] num_patches = x_convolve.shape[1] x_out_channels = x_convolve.shape[2] x_in_channels = x_convolve.shape[3] x_kernel_height = x_convolve.shape[4] x_kernel_width = x_convolve.shape[5] out_channels = self.B.shape[0] in_channels = self.B.shape[1] kernel_height = self.B.shape[2] kernel_width = self.B.shape[3] assert(x_out_channels == out_channels) assert(x_in_channels == in_channels) assert(x_kernel_height == kernel_height) assert(x_kernel_width == kernel_width) # Reshape B to match the dimensions of x_unfolded, but keeping its unique values per filter # Current shape of B: [out_channels, in_channels, kernel_height, kernel_width] # We need B to have the shape: [batch_size, in_channels, patch_size, num_patches] # Expand B across the batch_size and num_patches dimensions B_reshaped = self.B.view(1, 1, out_channels, in_channels, kernel_height, kernel_width) B_expanded = B_reshaped.expand(batch_size, num_patches, out_channels, in_channels, kernel_height, kernel_width) # Subtract B_expanded from each patch x_offset = x_convolve - B_expanded return x_offset def multiply_weights(self, x_offset): batch_size = x_offset.shape[0] num_patches = x_offset.shape[1] x_out_channels = x_offset.shape[2] x_in_channels = x_offset.shape[3] x_kernel_height = x_offset.shape[4] x_kernel_width = x_offset.shape[5] out_channels = self.A.shape[0] in_channels = self.A.shape[1] kernel_height = self.A.shape[2] kernel_width = self.A.shape[3] assert(x_out_channels == out_channels) assert(x_in_channels == in_channels) assert(x_kernel_height == kernel_height) assert(x_kernel_width == kernel_width) # Multiply A with x_offset and sum over the kernel dimensions # A shape: [out_channels, in_channels, kernel_height, kernel_width] # x_offset shape: [batch_size, num_patches, in_channels, kernel_height, kernel_width] return self.A.unsqueeze(0).unsqueeze(1) * x_offset def forward(self, x): batch_size = x.shape[0] image_channels = x.shape[1] image_height = x.shape[2] image_width = x.shape[3] out_channels = self.A.shape[0] in_channels = self.A.shape[1] kernel_height = self.A.shape[2] kernel_width = self.A.shape[3] assert(image_channels == in_channels) assert(kernel_height == kernel_width) # 1. Extract image patches x_convolve = self.convolve(x) # 2. Calculate P_offset = P - B x_offset = self.subtract_offset(x_convolve) # 3. Perform the convolution operation y = A * P_offset output = torch.sum(self.multiply_weights(x_offset), dim=[3, 4, 5]) # 4. Reshape the output # Calculate the dimensions of the output feature map output_height = (image_height + 2 * self.padding - (kernel_height - 1) - 1) + 1 output_width = (image_width + 2 * self.padding - (kernel_width - 1) - 1) + 1 # Reshape output to the shape (batch_size, out_channels, output_height, output_width) output = output.view(batch_size, out_channels, output_height, output_width) return output\n\nI’m used to looking at source code to figure things out. I can’t find the C++ code that F.conv2d calls in github. That is my question. Where is that C++ code? I’m doing non-standard things with CNN’s. I’ve implemented it using tensors in python but it’s about 20x slower – which is sort of what I expected and why I asked my question. I’m going to define words to make sure we are talking about the same thing. A standard CNN convolves the function y=Ax+b over an image. That function is the “convolutional operation”.\n\n “A” is the kernel. It is a matrix, defines the convolution window size, and is multiplied against image values in a convolutional window. “b” is the bias. It is a scalar value. A filter is a single node with its own <A, b>. It generates an output for each convolution position. “A” is the kernel. It is a matrix, defines the convolution window size, and is multiplied against image values in a convolutional window. “B” is an offset. It is a matrix that is the same size as A, and is subtracted from image values in a convolutional window before A is applied. A filter is a single node with its own <A, B>. It generates an output for each convolution position. The specific details of what I’m doing aren’t really important to my question, which is really how do I implement a custom Conv2d layer that runs quickly? At some point I want to make a Conv2d layer that implements a quadratic convolution operation y = Ax^2 + Bx + C instead of a linear function y=Ax+b. Or maybe something else. I want to be able to make Conv2d use different convolution operations. The details of what those operations are shouldn’t matter. Ideally I’d have a Conv2d that can take a lambda. Yes, I know I’m doing weird stuff. I’ll read about the Toeplitz matrix and see if that can speed anything up for me.\n\nThat works. It doesn’t require two convolutions. AB is constant for each filter across convolution. I feel like I should have seen that solution. Thank you for breaking me out of my myopia. class Conv2dPS(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, padding=0): super(Conv2dPS, self).__init__() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, bias=False) self.A = self.conv.weight # Alias for clarity self.B = nn.Parameter(torch.zeros(out_channels, in_channels, kernel_size, kernel_size)) def forward(self, x): # Standard convolution Ax = self.conv(x) # Calculate AB as a dot product and then sum it # Reshape or expand as necessary to match the dimensions of Ax # element-wise multiplication for dot product. # Gives [outputs, channels, kernel_width, kernel_height] AB = self.A * self.B # sum each output (filter) over channel, kernel_width, kernel_height # Gives [outputs] AB = torch.sum(AB, dim=(1, 2, 3)) # expand for substraction from Ax # Gives [batch, outputs, window_x, window_y] AB = AB.unsqueeze(0).unsqueeze(2).unsqueeze(3) # Add AB to the convolution result # Ensure correct reshaping/broadcasting to match Ax dimensions return Ax - AB"
    },
    {
        "link": "https://michhar.github.io/learning-from-learning-yolov3",
        "document": "UPDATE 2020-06-06: YOLO v4 has been recently published and implemented in PyTorch and can be found at https://github.com/Tianxiaomo/pytorch-YOLOv4.\n\ntl:dr: YOLO (for \"you only look once\") v3 is a relatively recent (April 2018) architecture design for object detection. PyTorch (recently merged with Caffe2 and production as of November 2018) is a very popular deep learning library with Python and C++ bindings for both training and inference that is differentiated from Tensorflow by having a dynamic graph. This post is about my lessons working on PyTorch YOLO v3 and a little insight into creating a good YOLO v3 custom model on custom data (We love you COCO, but we have our own interets, now.).\n\nIn order to understand the anchors or anchor boxes, a little background is needed on the YOLO v3 algorithm (sources are the original YOLO and YOLO v3 papers).\n\nIn full-sized YOLO v3 there are 9 anchor boxes specified in total as can be seen in the files on the PyTorch repo.\n\nThere are 3 scales at which YOLO \"sees\" an image when passes through the network (these correspond to the three layers). Note, this allows YOLO to see big, medium and small sized objects all at once.\n\nAt each of the three scales, the image is broken in to a grid of 13x13 squares or cells (remember, our input image is converted to a square in this implementation before running through the network). For each cell in a 13x13 grid, three anchor boxes are used (this corresponds to the three anchor boxes from above). In other words, each cell has three anchor boxes overlayed on it and this happens at three different scales (all within the same pass through the network, even! Hence, \"you only look once\" :-) ).\n\nSo, when we list the array of 9 anchor boxes from above, the first three width/heights ( ), belong to the first scaling process, the second three ( ) to the second scaling process and, as follows, the final three to the third scaling process ( ). Each set of three width/heights correspond to the width/heights of the three bounding boxes used for each grid cell at each of the three scales.\n\nTo round out this story, the three anchor boxes are used to predict whether there is an object there (object/no object). The grid cell is used to predict classes. These are combined at the end of the network to figure out the shape of objects (bounding boxes) from anchor boxes and their classes from grid cells.\n\nThis diagram shows this very well (anchor boxes on top path and grid cell predictions on bottom path):\n\nWith this all being said, the lesson is to always calculate the anchor boxes on each new dataset before training. The sizes of labeled objects (which determines sizes of anchor boxes) will be crucial to a good training experiment and well as inference which uses the same anchor box sizes.\n\nAnchor boxes are calculated using Kmeans clustering for every new dataset as is shown in code here (adapted from a Keras implementation of YOLO v3).\n\nIn transfer learning we begin with a base model which gives us the weight values to start our training. Objects from the training set of the base model, upon which the base model was trained, gets us closer to a new learned network for objects in the real world. So, instead of starting with random weights to begin our training we begin from a \"smarter\" set of values.\n• One tidbit I learned was to skip making batch normalization (BN) layers trainable.\n\nI recently learned from A refresher on batch (re-)normalization that:\n\n\"When the mini-batch mean (µB) and mini-batch standard deviation (σB) diverge from the mean and standard deviation over the entire training set too often, BatchNorm breaks.\"\n\nAnd that there are perils in hyperparameter tuning in conjunction with retraining BN layers and a few extra steps required to fix this (with a technique call batch renormalization) - so for simplicity sake, I left out retraining on BN layers, but look at batch renormalization techniques in the post above for addressing the complex issue if you wish.\n\nHow to allow layers in a PyTorch model to be trainable (minus BNs).\n• How much of network to \"open up\" or set as trainable (the parameters that is)? - it's recommended at times to open it more (likely all of the parameters in fine-tuning phase) if the object or objects are very different from any COCO classes, which is called domain adaptation (NB: the base model from darknet is trained on COCO dataset). So, for instance, if the base model has never seen a caterpillar before (not in COCO), you may want to let more layers be trainable.\n\nHow to allow even more layers in the PyTorch model to be trainable (could set to 0 to train whole network):\n• Another learning is that if the network is not converging, try opening up all of the layers during fine-tuning.\n\nSome of these I learned the hard way, others from the wonderful PyTorch forums and StackOverflow.\n• Be careful of conversions from a 0-255 to a 0-1 range as you don't want to do that more than once in code.\n• Keep this simple at first with only the resize and normalization. Try with several types of augmentation next, increasing in complexity with each experiment.\n\nStart with just resize and standard pixel intensity normalize. (NB: the transforms operate on PIL images, then convert to 3D array and finally to )\n\nThen get fancier with hue, saturation and brightness shifts, for example (look in for the amounts if following along in code).\n\nWhere Normalize is a pixel intensity normalization (here, not to unit norm because we do that elsewhere) (based on accepted answer on StackOverflow):\n• A great option for augmentation is to double or triple the size of a dataset with a library like which can handle bounding boxes and polygons now.\n\nThere are some great learning rate schedulers to decrease learning rate with training on a schedule or automatically in the and set of methods therein.\n\nThe following is more of an implementation detail, but nonetheless, found it helpful to not make the mistake.\n• Place the learning rate scheduler at the level of the epoch update, not the inner loop over batches of data (where the optimizer is).\n• YOLOv3: You Only Look Once v3. Improvments over v1, v2 and YOLO9000 which include Ref:\n• Predicts more bounding boxes per image (hence a bit slower than previous YOLO architectures)\n• Can perform multi-label classification (no more mutually exclusive labels)\n• Performance on par with other architectures (a bit faster than SSD, even, in many cases)\n• Tiny-YOLOv3: A reduced network architecture for smaller models designed for mobile, IoT and edge device scenarios\n• Anchors: There are 5 anchors per box. The anchor boxes are designed for a specific dataset using K-means clustering, i.e., a custom dataset must use K-means clustering to generate anchor boxes. It does not assume the aspect ratios or shapes of the boxes. Ref\n• Loss: using (for loss confidence) or mean squared error\n• IOU: intersection over union between predicted bounding boxes and ground truth boxes\n• 37 Reasons why your Neural Network is not working"
    },
    {
        "link": "https://geeksforgeeks.org/yolov3-from-scratch-using-pytorch",
        "document": "This article discusses about YOLO (v3), and how it differs from the original YOLO and also covers the implementation of the YOLO (v3) object detector in Python using the PyTorch library.\n\nObject detection is a fundamental task in computer vision that is a combination of identifying objects within an image and localizing them by drawing a bounding box around them. It takes an image as input and produces one or more bounding boxes with the class label attached to each bounding box. A lot of research has been done in this domain and various algorithms like YOLO, R-CNN, Fast R-CNN, and Faster R-CNN have been proposed. In this article, we will discuss the YOLO (v3) algorithm.\n\nYOLO (You Only Look Once)\n\nYOLO (You Only Look Once) are proposed by J. Redmon and A. Farhadi in 2015 to deal with the problems of slow processing speed and complex architectures of state-of-the-art models at that time. In terms of speed, YOLO is one of the best models for object recognition, able to recognize objects and process frames at a rate of up to 150 FPS for small networks. However, In terms of the accuracy of mAP, YOLO was not the state-of-the-art model but has a fairly good Mean Average Precision (mAP) of 63% when trained on PASCAL VOC 2007 and PASCAL VOC 2012. However, Fast R-CNN which was the state of the art at that time has an mAP of 71%.\n\nLater, YOLO (v2) and YOLO 9000 were proposed by J. Redmon and A. Farhadi in 2016 which at 67 FPS gave mAP of 76.8% on VOC 2007 dataset. Furthermore, in 2018, J. Redmon and A. Farhadi released YOLO (v3), which further improved object detection accuracy and speed. YOLO (v3) introduced a new backbone architecture, called Darknet-53, which improved feature extraction and added additional anchor boxes to better detect objects at different scales. It also introduced a new loss function, which improved object localization and reduced false positives.\n\nYOLO (v3) was proposed with several improvements compared to YOLO (v1) and YOLO (v2) as reported by their authors. Some of the key improvements are listed below:\n• Backbone architecture: In YOLO (v3), the authors have improved the backbone architecture by introducing Darknet-53 which is more capable of extracting high-level features and capturing complex patterns in images compare to Darknet-19 used in YOLO (v1) and YOLO (v2).\n• Multi-scale prediction: YOLO (v3) predicts objects at three different scales using anchor boxes of different sizes. This helped the model to improve the prediction compared to YOLO (v1) and YOLO (v2).\n• Smoother bounding box predictions: YOLO (v3) uses a technique called bounding box regression to improve the accuracy of bounding box predictions. This technique predicts the offsets between the anchor boxes and the ground truth boxes, resulting in smoother and more accurate bounding box predictions.\n• Faster training: YOLO (v3) is faster to train because it uses batch normalization and residual connections like YOLO (v2) to stabilize the training process and reduce overfitting.\n\nNow in this section we will look into implementation of YOLO (v3) object detector in PyTorch. We will first discuss about the dataset we can use to train the model. Then we will discuss about its architecture design, its components and there implementation. Later we will discuss about training the network and test it with a random image.\n\nWe will first include the libraries we will be using in this article.\n\nWe will also define some helper functions as follows:\n\nWe will also define some constants to use later.\n\nTo train this network, you can make use of PASCAL Visual Object Classes dataset. This dataset is usually used for object detection and recognition tasks and consists of 16,550 training data and 4,952 testing data, containing objects annotated from a total of 20 classes.\n\nNow, we will define the dataset class to load the dataset from the folders. In this class while loading the data with its label we have to make sure of the following parts:\n• Bounding box label data should be in the [x, y, width, height, class_label] format where (x, y) represents the center coordinate of the object within the image, width is the width of the object’s bounding box, height is the height of the object’s bounding box, and class_label indicates the class to which the object belongs. We will follow this format because while applying transforms to the input image we need the bounding box data to be in this format to match the input transforms.\n• While reading the input image we have to convert it into 3-channel (RGB format) input because some of the input is in grayscale.\n• While loading the data, we will have target data for each box at different scales and we have to assign which anchor is responsible and which cell is responsible for the identification of that object (Generating a conditional probability map).\n\nNow, for training and testing, we will need to define transforms on which the input data will be processed before feeding it to the network. For this, we will make use of the argumentation library in Pytorch which provides efficient transforms for both image and bounding boxes.\n\nNow, let us take a sample image and display it with labels.\n\nNow, we will look into the architecture design of YOLO (v3). The authors of YOLO (v3) introduced a new version of Darknet named Darknet-54, containing 54 layers, as the backbone of this architecture. Figure 1 describes the architecture of Darknet-54 used in YOLO (v3) to extract features from the image. This network is a hybrid of Darknet-19 and residual blocks along with some short connections in the network.\n\nThe bounding boxes are predicted at three different points in this network and on three different scales or grid sizes. The idea behind this approach is that the small objects will get easily detected on smaller grids and large objects will be detected on larger grid. In YOLO (v3) the grid sizes author used were [13, 26, 52] with image of size 416×416.\n\nThis network contains three main components namely, CNN block, residual block, and scale prediction. We will first code the components of the network and then use them to define our YOLO (v3) network. The CNN block will be defined as follows.\n\nNow we will define residual block. We will be looping the layers in the residual block based on number defined in the architecture.\n\nNow, we will define the scale prediction block.\n\nNow, we will use these components to code YOLO (v3) network.\n\nWe can use to the following code to test the YOLO (v3) mode generated and check if we are getting the output of correct shape.\n\nFor training the model, we need to define a loss function on which our model can optimize. The paper discusses that the YOLO (v3) architecture was optimized on a combination of four losses: no object loss, object loss, box coordinate loss, and class loss. The loss function is defined as:\n• λ , λ , λ and λ are constants that weight the different components of the loss function (they are set to 1 in the paper).\n• L penalizes the errors in the bounding box coordinates.\n• L penalizes the errors in the class predictions.\n\nDefine the losses, optimizer, and activation function, and forwarding step for the YOLOv3 model\n\nNow we will define the loss function in Pytorch.\n\nNow, let us define the training loop we will be using to train the model.\n\nNow, let us train the model for 20 epochs with a learning rate of 1e-4 and batch size of 32.\n\nAfter training the model, we will test it on a sample input image and see the results.\n• YOLO : You Only Look Once – Real Time Object Detection"
    },
    {
        "link": "https://github.com/xuexingyu24/YOLO-V3-in-Pytorch-A-Tutorial-on-Implementation-of-YOLO-V3-Algorithm/blob/master/Yolo_V3_Inference_Step_by_Step.ipynb",
        "document": "To see all available qualifiers, see our documentation .\n\nSaved searches Use saved searches to filter your results more quickly\n\nWe read every piece of feedback, and take your input very seriously.\n\nYou signed in with another tab or window. Reload to refresh your session.\n\nYou signed out in another tab or window. Reload to refresh your session.\n\nYou switched accounts on another tab or window. Reload to refresh your session."
    },
    {
        "link": "https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch",
        "document": "Tutorial on building YOLO v3 detector from scratch detailing how to create the network architecture from a configuration file, load the weights and designing input/output pipelines.\n\nImage Credits: Karol Majek. Check out his YOLO v3 real time detection video here\n\nObject detection is a domain that has benefited immensely from the recent developments in deep learning. Recent years have seen people develop many algorithms for object detection, some of which include YOLO, SSD, Mask RCNN and RetinaNet.\n\nObject detection is a domain that has benefited immensely from the recent developments in deep learning. Recent years have seen people develop many algorithms for object detection, some of which include YOLO, SSD, Mask RCNN and RetinaNet.\n\nFor the past few months, I've been working on improving object detection at a research lab. One of the biggest takeaways from this experience has been realizing that the best way to go about learning object detection is to implement the algorithms by yourself, from scratch. This is exactly what we'll do in this tutorial.\n\nWe will use PyTorch to implement an object detector based on YOLO v3, one of the faster object detection algorithms out there.\n\nThe code for this tutorial is designed to run on Python 3.5, and PyTorch 0.4. It can be found in it's entirety at this Github repo.\n\nThis tutorial is broken into 5 parts:\n• Part 1 (This one): Understanding How YOLO works\n• Part 2 : Creating the layers of the network architecture\n• Part 3 : Implementing the the forward pass of the network\n• Part 5 : Designing the input and the output pipelines\n• You should understand how convolutional neural networks work. This also includes knowledge of Residual Blocks, skip connections, and Upsampling.\n• What is object detection, bounding box regression, IoU and non-maximum suppression.\n• Basic PyTorch usage. You should be able to create simple neural networks with ease.\n\nI've provided the link at the end of the post in case you fall short on any front.\n\nYOLO stands for You Only Look Once. It's an object detector that uses features learned by a deep convolutional neural network to detect an object. Before we get out hands dirty with code, we must understand how YOLO works.\n\nYOLO makes use of only convolutional layers, making it a fully convolutional network (FCN). It has 75 convolutional layers, with skip connections and upsampling layers. No form of pooling is used, and a convolutional layer with stride 2 is used to downsample the feature maps. This helps in preventing loss of low-level features often attributed to pooling.\n\nBeing a FCN, YOLO is invariant to the size of the input image. However, in practice, we might want to stick to a constant input size due to various problems that only show their heads when we are implementing the algorithm.\n\nA big one amongst these problems is that if we want to process our images in batches (images in batches can be processed in parallel by the GPU, leading to speed boosts), we need to have all images of fixed height and width. This is needed to concatenate multiple images into a large batch (concatenating many PyTorch tensors into one)\n\nThe network downsamples the image by a factor called the stride of the network. For example, if the stride of the network is 32, then an input image of size 416 x 416 will yield an output of size 13 x 13. Generally, stride of any layer in the network is equal to the factor by which the output of the layer is smaller than the input image to the network.\n\nTypically, (as is the case for all object detectors) the features learned by the convolutional layers are passed onto a classifier/regressor which makes the detection prediction (coordinates of the bounding boxes, the class label.. etc).\n\nIn YOLO, the prediction is done by using a convolutional layer which uses 1 x 1 convolutions.\n\nNow, the first thing to notice is our output is a feature map. Since we have used 1 x 1 convolutions, the size of the prediction map is exactly the size of the feature map before it. In YOLO v3 (and it's descendants), the way you interpret this prediction map is that each cell can predict a fixed number of bounding boxes.\n\nDepth-wise, we have (B x (5 + C)) entries in the feature map. B represents the number of bounding boxes each cell can predict. According to the paper, each of these B bounding boxes may specialize in detecting a certain kind of object. Each of the bounding boxes have 5 + C attributes, which describe the center coordinates, the dimensions, the objectness score and C class confidences for each bounding box. YOLO v3 predicts 3 bounding boxes for every cell.\n\nYou expect each cell of the feature map to predict an object through one of it's bounding boxes if the center of the object falls in the receptive field of that cell. (Receptive field is the region of the input image visible to the cell. Refer to the link on convolutional neural networks for further clarification).\n\nThis has to do with how YOLO is trained, where only one bounding box is responsible for detecting any given object. First, we must ascertain which of the cells this bounding box belongs to.\n\nTo do that, we divide the input image into a grid of dimensions equal to that of the final feature map.\n\nLet us consider an example below, where the input image is 416 x 416, and stride of the network is 32. As pointed earlier, the dimensions of the feature map will be 13 x 13. We then divide the input image into 13 x 13 cells.\n\n\n\n Then, the cell (on the input image) containing the center of the ground truth box of an object is chosen to be the one responsible for predicting the object. In the image, it is the cell which marked red, which contains the center of the ground truth box (marked yellow).\n\nNow, the red cell is the 7th cell in the 7th row on the grid. We now assign the 7th cell in the 7th row on the feature map (corresponding cell on the feature map) as the one responsible for detecting the dog.\n\nNow, this cell can predict three bounding boxes. Which one will be assigned to the dog's ground truth label? In order to understand that, we must wrap out head around the concept of anchors.\n\nIt might make sense to predict the width and the height of the bounding box, but in practice, that leads to unstable gradients during training. Instead, most of the modern object detectors predict log-space transforms, or simply offsets to pre-defined default bounding boxes called anchors.\n\nThen, these transforms are applied to the anchor boxes to obtain the prediction. YOLO v3 has three anchors, which result in prediction of three bounding boxes per cell.\n\nComing back to our earlier question, the bounding box responsible for detecting the dog will be the one whose anchor has the highest IoU with the ground truth box.\n\nThe following formulae describe how the network output is transformed to obtain bounding box predictions.\n\n\n\n bx, by, bw, bh are the x,y center co-ordinates, width and height of our prediction. tx, ty, tw, th is what the network outputs. cx and cy are the top-left co-ordinates of the grid. pw and ph are anchors dimensions for the box.\n\nNotice we are running our center coordinates prediction through a sigmoid function. This forces the value of the output to be between 0 and 1. Why should this be the case? Bear with me.\n\nNormally, YOLO doesn't predict the absolute coordinates of the bounding box's center. It predicts offsets which are:\n• Relative to the top left corner of the grid cell which is predicting the object.\n• Normalised by the dimensions of the cell from the feature map, which is, 1.\n\nFor example, consider the case of our dog image. If the prediction for center is (0.4, 0.7), then this means that the center lies at (6.4, 6.7) on the 13 x 13 feature map. (Since the top-left co-ordinates of the red cell are (6,6)).\n\nBut wait, what happens if the predicted x,y co-ordinates are greater than one, say (1.2, 0.7). This means center lies at (7.2, 6.7). Notice the center now lies in cell just right to our red cell, or the 8th cell in the 7th row. This breaks theory behind YOLO because if we postulate that the red box is responsible for predicting the dog, the center of the dog must lie in the red cell, and not in the one beside it.\n\nTherefore, to remedy this problem, the output is passed through a sigmoid function, which squashes the output in a range from 0 to 1, effectively keeping the center in the grid which is predicting.\n\nThe dimensions of the bounding box are predicted by applying a log-space transform to the output and then multiplying with an anchor.\n\n\n\n How the detector output is transformed to give the final prediction. Image Credits. http://christopher5106.github.io/\n\nThe resultant predictions, bw and bh, are normalised by the height and width of the image. (Training labels are chosen this way). So, if the predictions bx and by for the box containing the dog are (0.3, 0.8), then the actual width and height on 13 x 13 feature map is (13 x 0.3, 13 x 0.8).\n\nObject score represents the probability that an object is contained inside a bounding box. It should be nearly 1 for the red and the neighboring grids, whereas almost 0 for, say, the grid at the corners.\n\nThe objectness score is also passed through a sigmoid, as it is to be interpreted as a probability.\n\nClass confidences represent the probabilities of the detected object belonging to a particular class (Dog, cat, banana, car etc). Before v3, YOLO used to softmax the class scores.\n\nHowever, that design choice has been dropped in v3, and authors have opted for using sigmoid instead. The reason is that Softmaxing class scores assume that the classes are mutually exclusive. In simple words, if an object belongs to one class, then it's guaranteed it cannot belong to another class. This is true for COCO database on which we will base our detector.\n\nHowever, this assumptions may not hold when we have classes like Women and Person. This is the reason that authors have steered clear of using a Softmax activation.\n\nYOLO v3 makes prediction across 3 different scales. The detection layer is used make detection at feature maps of three different sizes, having strides 32, 16, 8 respectively. This means, with an input of 416 x 416, we make detections on scales 13 x 13, 26 x 26 and 52 x 52.\n\nThe network downsamples the input image until the first detection layer, where a detection is made using feature maps of a layer with stride 32. Further, layers are upsampled by a factor of 2 and concatenated with feature maps of a previous layers having identical feature map sizes. Another detection is now made at layer with stride 16. The same upsampling procedure is repeated, and a final detection is made at the layer of stride 8.\n\nAt each scale, each cell predicts 3 bounding boxes using 3 anchors, making the total number of anchors used 9. (The anchors are different for different scales)\n\nThe authors report that this helps YOLO v3 get better at detecting small objects, a frequent complaint with the earlier versions of YOLO. Upsampling can help the network learn fine-grained features which are instrumental for detecting small objects.\n\nFor an image of size 416 x 416, YOLO predicts ((52 x 52) + (26 x 26) + 13 x 13)) x 3 = 10647 bounding boxes. However, in case of our image, there's only one object, a dog. How do we reduce the detections from 10647 to 1?\n\nFirst, we filter boxes based on their objectness score. Generally, boxes having scores below a threshold are ignored.\n\nNMS intends to cure the problem of multiple detections of the same image. For example, all the 3 bounding boxes of the red grid cell may detect a box or the adjacent cells may detect the same object.\n\nIf you don't know about NMS, I've provided a link to a website explaining the same.\n\nYOLO can only detect objects belonging to the classes present in the dataset used to train the network. We will be using the official weight file for our detector. These weights have been obtained by training the network on COCO dataset, and therefore we can detect 80 object categories.\n\nThat's it for the first part. This post explains enough about the YOLO algorithm to enable you to implement the detector. However, if you want to dig deep into how YOLO works, how it's trained and how it performs compared to other detectors, you can read the original papers, the links of which I've provided below.\n\nThat's it for this part. In the next part, we implement various layers required to put together the detector.\n• YOLO V1: You Only Look Once: Unified, Real-Time Object Detection\n\nAyoosh Kathuria is currently an intern at the Defense Research and Development Organization, India, where he is working on improving object detection in grainy videos. When he's not working, he's either sleeping or playing pink floyd on his guitar. You can connect with him on LinkedIn or look at more of what he does at GitHub"
    },
    {
        "link": "https://github.com/westerndigitalcorporation/YOLOv3-in-PyTorch",
        "document": "The repo implements YOLOv3 using the PyTorch framework. Both inference and training modules are implemented.\n\nYou-Only-Look-Once (YOLO) newtork was introduced by Joseph Redmon et al. Three versions were implemented in C, with the framework called darknet (paper: v1, v2, v3).\n\nThis repo implements the Nueral Network (NN) model of YOLOv3 in the PyTorch framework, aiming to ease the pain when the network needs to be modified or retrained.\n\nThere are a number of implementations existing in the open source domain, e.g., eriklindernoren/PyTorch-YOLOv3, ayooshkathuria/pytorch-yolo-v3, ultralytics/yolov3, etc. However, majority of them relies on \"importing\" the configuration file from the original darknet framework. In this work, the model is built from scratch using PyTorch.\n\nAdditionally, both inference and training part are implemented. The original weights trained by the authors are converted to .pt file. It can be used as a baseline for transfer learning.\n\nThis project is licensed under BSD 3-Clause \"Revised\" License.\n\nBefore cloning the repo to your local machine, make sure that is installed. See details about , see this link.\n\nAfter is installed. Run the following command to see sample detection results.\n\nDetections will be saved in the folder.\n\nThe repo is tested in . Additionally, the following packages are required:\n\nThe repo is structured as following:\n\nfolder contains the source codes. folder contains the original weight file trained by Joseph Redmon et al. file lists the names of the categories defined in the COCO dataset. folder contains the font used by the Pillow module.\n\nThe weight trained by Joseph Redmon et al. is used as a starting point. The last few layers of the network can be unfreezed for transfer learning or finetuning.\n\nTo train on COCO dataset, first you have to download the dataset from COCO dataset website. Both images and the annotations are needed. Secondly, , which serves as the Python API for COCO dataset needs to be installed. Please follow the instructions on their github repo to install .\n\nAfter the COCO dataset is properly downloaded and the API setup, the training can be done by:\n\nYou can see the network to converge within 1-2 epochs of training.\n\nTo run inference on one image folder, run:\n\nThe option will save a detection file to the output folder. The formate matches COCO detection format for easy benchmarking. The option\n\nprovides numerous options to tweak the functions. Run to check the provided options. The help file is pasted here for your convenience. But it might not be up-to-date.\n\nExecution time is measured using the desktop machine described below (\"My Machine\"). Test case is detecting COCO val2017 dataset with batch size of 1. The data in the \"Time from paper (ms)\" column is taken from the original YOLOv3 paper (link), which is not verified on My Machine.\n\nThe configuration of My Machine:\n\nThe figure of merit used in object detection is COCO AP. For a good explanation of COCO AP (also called mAP), see this post.\n\nEspecially, AP is extensively used by Redmon et al. to compare the performance of different models. Here we inherit this metric. AP is measured using COCO val2017 dataset.\n\n* metric extracted from paper and not verified.\n\nThis project is licensed under BSD 3-Clause \"Revised\" License - see the LICENSE file for details"
    }
]