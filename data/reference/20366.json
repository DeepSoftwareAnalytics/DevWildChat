[
    {
        "link": "https://spring.io/guides/gs/multi-module",
        "document": "This guide shows you how to create a multi-module project with Spring Boot. The project will have a library jar and a main application that uses the library. You could also use it to see how to build a library (that is, a jar file that is not an application) on its own.\n\nThis guide walks through building two projects, one of which is a dependency to the other. Consequently, you need to create two child projects under a root project. But first, create the build configuration at the top level. For Maven you will want a with listing the subdirectories: For Gradle, you will want a including the same directories: and (optionally) you could include an empty (to help IDEs identify the root directory). In the directory that you want to be your root directory, create the following subdirectory structure (for example, with on *nix systems): In the root of the project, you will need to set up a build system, and this guide shows you how to use Maven or Gradle.\n\nOne of the two projects serves as a library that the other project (the application) will use. In a the directory, create the following subdirectory structure (for example, by using on *nix systems): Now you need to configure a build tool (Maven or Gradle). In both cases, note that the Spring Boot plugin is not used in the library project at all. The main function of the plugin is to create an executable “über-jar”, which we neither need nor want for a library. Although the Spring Boot Maven plugin is not being used, you do want to take advantage of Spring Boot dependency management, so that is configured by using the from Spring Boot as a parent project. An alternative would be to import the dependency management as a Bill of Materials (BOM) in the section of the file. For the Library project, you need not add dependencies. The basic dependency provides everything you need. You can get a Maven build file with the necessary dependencies directly from the Spring Initializr. The following listing shows the file that is created when you choose Maven: You can get a Gradle build file with the necessary dependencies directly from the Spring Initializr. The following listing shows the file that is created when you choose Gradle: If you generated the Library project from it will contain a wrapper script for the build system ( or depending on the choice you made). You can move that script and its associated configuration up to the root directory: It is better that the library depends on the most narrowed dependencies, and not a starter. For our own use there has all the code that we need. Removing the of the existing entry makes sure the library doesn’t bring up too much dependencies. The Library project has no class with a main method (because it is not an application). Consequently, you have to tell the build system to not try to build an executable jar for the Library project. (By default, the Spring Initializr builds executable projects.) To tell Maven to not build an executable jar for the Library project, you must remove the following block from the created by the Spring Initializr: The following listing shows the final file for the Library project: To tell Gradle to not build an executable jar for the Library project, you must add the following blocks to the created by the Spring Initializr: The task tries to create an executable jar, and that requires a method. As a result, you need to disable it by disabling the the Spring Boot plugin, while keeping it for its dependency management features. Also, now that we have disabled the Spring Boot plugin, it no longer automatically configures the task to enable the option. This is important if you are using an expression that refers to a parameter name. The following enables this option: The following listing shows the final file for the Library project:\n\nYou will want to write unit tests for your library components. If you provide re-usable Spring configuration as part of the library, you might also want to write an integration test, to make sure that the configuration works. To do that, you can use JUnit and the annotation. The following listing (from ) shows how to do so: In the preceding listing, we have configured the for the test by using the default attribute of the annotation. We do not recommend putting in a library, because there might be a clash at runtime with the application that uses the library (only one is ever loaded from the classpath). You could put in the test classpath but not include it in the jar (for instance, by placing it in ).\n\nThe Application project uses the Library project, which offers a service that other projects can use. In the directory, create the following subdirectory structure (for example, with on *nix systems): Do not use the same package as the library (or a parent of the library package) unless you want to include all Spring components in the library by in the application. For the Application project, you need the Spring Web and Spring Boot Actuator dependencies. You can get a Maven build file with the necessary dependencies directly from the Spring Initializr. The following listing shows the file that is created when you choose Maven: You can get a Gradle build file with the necessary dependencies directly from the Spring Initializr. The following listing shows the file that is created when you choose Gradle: You can delete the and/or wrappers and their associated configuration files: The Application project needs to have a dependency on the Library project. You need to modify your Application build file accordingly. For Maven, add the following dependency: The following listing shows the finished file: For Gradle, add the following dependency: The following listing shows the finished file:\n\nThe main class in the application can be a that uses the from the library to render a message. The following listing (from ) shows such a class: is a convenience annotation that adds all of the following:\n• : Tags the class as a source of bean definitions for the application context.\n• : Tells Spring Boot to start adding beans based on classpath settings, other beans, and various property settings. For example, if is on the classpath, this annotation flags the application as a web application and activates key behaviors, such as setting up a .\n• : Tells Spring to look for other components, configurations, and services in the package, letting it find the controllers. The method uses Spring Boot’s method to launch an application. Did you notice that there was not a single line of XML? There is no file, either. This web application is 100% pure Java and you did not have to deal with configuring any plumbing or infrastructure. Because is inside a different package ( ) than ( ), cannot automatically detect it. There are different ways to let `MyService be picked up:\n• Fetch everything from its package by using .\n• Specifying the parent package by name: . (This guide uses this method) If your application also uses JPA or Spring Data, the and (and related) annotations inherit only their base package from when not explicitly specified. That is, once you specify or , you might also have to also explicitly use and with their package scans explicitly configured.\n\nTest the end-to-end result by starting the application. You can start the application in your IDE or use the command line. Once the application is running, visit the client application in the browser, at . There, you should see reflected in the response. If you use Gradle, the following command (really two commands run in sequence) will first build the library and then run the application: If you use Maven, the following command (really two commands run in sequence) will first build the library and then run the application:"
    },
    {
        "link": "https://stackoverflow.com/questions/48775836/how-to-have-another-parent-dependency-as-well-as-spring-boot-parent-in-maven-pom",
        "document": "When you set up a Spring Boot project, in your you are required to have a parent reference to Spring Boot, e.g.\n\nWhat I would like to have is my project be a submodule Spring Boot project of a wider project (i.e. I would like to package them together under one project folder in a Repo).\n\nNow, normally in order to setup multi-module project, you put the parent pom info like above into your submodule .\n\nBut with the requirement of having the Spring Boot parent as a requirement, it is not possible to do so.\n\nI've seen suggestions already on the net recommending to move my submodule out and have it as a separate project independently but I think this would be my last resort right now unless that is the only way to go.\n\nDoes anyone have any ideas on how to achieve what I want or whether what I want to achieve is practically feasible or not?"
    },
    {
        "link": "https://stackoverflow.com/questions/67567486/best-practices-for-inheritance-structure-of-a-maven-project-with-modules",
        "document": "I'm try to figuring out with the best way to define inheritance structure for a Maven Project organized in modules.\n\n Let me introduce the scenario.\n\n\n\nI'm developing a Web Application which can be extended for different customer.\n\n Each personalization contains custom GUI components, data warehouse logic, etc..\n\nI create the primary pom.xml, defined as packaging, which rappresent the main project container and includes shared dependencies and modules definition:\n\nBasically, I usually make changes at Core Library and Base application module.Everytime I release a new version on any of those modules, I need to update pom.xml of each customer's module in order to update them at the last released version of mentioned artifacts.Following an example of a customer web application **pom.xml**:\n\nAt the moment I have to specify:\n• Customer module's version (1.11.05 in the example above)\n• cms-base-app module's verion (1.11.05 in the example above)\n\nThose two artifacts, in my idea, has to share the same version number because Customer Application extends Base Web Application.\n\nEverything works fine as expected but I'm sure there's a best way for doing that.\n\n\n\nThere's a way to specify version number only on cms-base-app pom.xml file and make each customer's module inherit from that? \n\n\n\n Thanks for your suggestions."
    },
    {
        "link": "https://kurular4.medium.com/why-you-should-use-parent-pom-for-your-multi-module-java-projects-b575017fab2e",
        "document": "In this article, we are going to look deep into why we should use parent for our Java projects especially those composed of multi-modules like microservice projects.\n\nWhen we start a new project, most probably it will require different functionalities which we will not be keeping in a single module as otherwise would cause a lot of pain. Or we have a microservice project that will be composed of several services each of which will be different module in our project. Let’s see example pom files for a microservice project.\n\nParent pom generally has spring-boot-starter-parent as parent to use what version spring package gives us to have consistent environment. You will also define groupId and version here to share it across sub modules. Packaging will be pom as this is not a actual java module that contains source code. Last part is the <modules> part where you define your sub modules. We will go into more detail in the next section.\n\nAs you see, we defined parent as the parent we created above. You do not need to define groupId or version here. If you want to set different version for this module apart from parent one, you can still define it. Next, we will see the advantages and how to get the most out of it.\n\nLet’s look at how we can we can organize our project and advantages over sample pom files.\n\nWhen you have multiple related modules in your project, it may be convenient to run their lifecycle methods together instead of building them one by one which may take too much time if you have a lot of modules. With parent module, you can define them under <modules> tag and manage build lifecycles from one place as you can see below.\n\nBut this is optional. Sometimes, you just want to use parent pom to have uniform dependency versioning or have some place to define commons. In that case, you do not have to define child modules in the parent pom.\n\nAnother advantage is that you can version all your modules directly from your parent pom and keep them consistent. When you define version in your parent pom, you do not have to put version in your child poms eventhough you can.\n\nIn your project, different modules will have different dependencies. Some of them will be special to one module while some are used in different modules. It is important to have consistency in dependency versioning as different versions of the same dependency may cause conflicts and undesired results. By defining dependency versions in the parent pom, you will just need to add the dependency without version to your sub module without having to go through other modules that use the same dependency to find out what version they use. Same applies for plugins. See below for an example.\n\nNote that you do not have to define sub modules under <modules> tag in parent pom to pass down versions to sub modules.\n\nWe will face two different case here and will ask ourselves a question. Are this dependency going to be definetely in all sub modules or only in some of them? If the answer is “yes, it will be in all modules”, then you need to define the dependency in your parent pom under <dependencies> tag as follows and do not have to define dependency in your child pom.\n\nIf the answer is “no, just some modules will use it” which is the most common one, then define dependencies in pom under <dependencyManagement> tag as follows and define the dependency in sub modules without version (you can also define with version if you want to use seperate version in that module).\n\nAgain, same applies for plugins with <plugins> and <pluginManagement> tags.\n\nOne practical use for Spring projects is that you can use spring-boot-starter-parent as your parent’s parent and define spring cloud dependencies in dependencyManagement tag in parent pom so that you will have consistent environment across your modules.\n\n3- Declaring commons in one place and using everywhere\n\nIn your pom file, you can define several different configurations like distribution management. Without parent pom, you have to define them in their own pom files which turns out to be unnecessary work. When you define the common configurations in the parent pom, it will be inherited to sub modules. See the following example\n\nWe defined distribution management here and sub modules can use this without having to redefine it.\n\nIn this article, we talked about why we should use parent pom to organize our project and advantages of it. I hope you will find it useful."
    },
    {
        "link": "https://baeldung.com/maven-multi-module",
        "document": "In this tutorial, we’ll learn how to build a multi-module project with Maven.\n\nFirst, we’ll discuss what a multi-module project is, and have a look at the benefits of following this approach. Then we’ll set up our sample project. For a good introduction to Maven, check out this tutorial.\n\nA multi-module project is built from an aggregator POM that manages a group of submodules. In most cases, the aggregator is located in the project’s root directory and must have packaging of type pom.\n\nThe submodules are regular Maven projects, and they can be built separately or through the aggregator POM.\n\nBy building the project through the aggregator POM, each project that has a packaging type different from pom will result in a built archive file.\n\nThe significant advantage of using this approach is that we may reduce duplication.\n\nLet’s say we have an application that consists of several modules, a front-end module and a back-end module. Now imagine we work on them and change the functionality, which affects them both. In that case, without a specialized build tool, we’d have to build both components separately or write a script to compile the code, run tests, and show the results. Then, after we got even more modules in the project, it would become harder to manage and maintain.\n\nIn the real world, projects may need certain Maven plugins to perform various operations during the build lifecycle, to share dependencies and profiles, and to include other BOM projects.\n\nTherefore, when leveraging multi-modules, we can build our application’s modules in a single command, and if the order matters, Maven will figure it out for us. We can also share a vast amount of configuration with other modules.\n\nMaven supports inheritance in a way that each pom.xml file has the implicit parent POM. It’s called Super POM and can be located in the Maven binaries. These two files are merged by Maven and form the Effective POM.\n\nWe can create our own pom.xml file, which will serve us as the parent project. Then we can include in it all configuration with dependencies, and set it as the parent of our child modules, so they’ll inherit from it.\n\nBesides the inheritance, Maven provides the notion of aggregation. A parent POM that leverages this functionality is called an aggregate POM. Basically, this kind of POM declares its modules explicitly in its pom.xml file.\n\nSubmodules, or subprojects, are regular Maven projects that inherit from the parent POM. As we already know, inheritance lets us share the configuration and dependencies with submodules. However, if we’d like to build or release our project in one shot, we have to declare our submodules explicitly in the parent POM. Ultimately, our parent POM will be the parent, as well as the aggregate POM.\n\nNow that we understand Maven’s submodules and hierarchy, let’s build a sample application to demonstrate them. We’ll use Maven’s command-line interface to generate our projects.\n\nThis app will consist of three modules that’ll represent:\n• The core part of our domain\n• A webapp containing user-facing web assets of some sort\n\nSince we’ll focus on Maven, the implementation of these services will remain undefined.\n\nOnce the parent is generated, we have to open the pom.xml file located in the parent’s directory and add the packaging as pom:\n\nBy setting the packaging to pom type, we’re declaring that the project will serve as a parent or an aggregator; it won’t produce further artifacts.\n\nNow, as our aggregator is done, we can generate our submodules.\n\nHowever, we need to note, this is where all the configuration to be shared is located, which will eventually be re-used in child modules. Among other things, we can make use of dependencyManagement or pluginManagement here.\n\nAs our parent POM was named parent-project, we need to make sure we’re in the parent’s directory and run generate commands:\n\nNotice the command used. It’s the same as we used for the parent. The thing here is, these modules are regular Maven projects, yet Maven recognized that they’re nested. When we changed the directory to the parent-project, it found that the parent has the packaging of type pom, and it will modify the pom.xml files accordingly.\n\nIn the parent-project‘s pom.xml it will add all the submodules inside the modules section:\n\nand in the individual submodules’ pom.xml, it will add the parent-project in the parent section:\n\nNext, Maven will generate the three submodules successfully.\n\nIt’s important to note that submodules can have only one parent. However, we can import many BOMs. More details about the BOM files can be found in this article.\n\nNow we can build all three modules at once. In the parent’s project directory, we’ll run:\n\nThis will build all the modules. We should see the following output of the command:\n\nThe Reactor lists the parent-project, but since it’s pom type it’s excluded, and the build results in two separate .jar files and one .war file for all the other modules. In this case, build occurs in three of them.\n\nMoreover, Maven Reactor will analyze our project and build it in the proper order. So if our webapp module depends on the service module, Maven will first build the service, then the webapp.\n\nDependency management is a mechanism for centralizing the dependency information for a multi-module parent project and its children.\n\nWhen you have a set of projects or modules that inherit a common parent, you can put all the required information about the dependencies in the common pom.xml file. This will simplify the references to the artifacts in the child POMs.\n\nLet’s take a look at a sample parent’s pom.xml:\n\nBy declaring the spring-core version in the parent, all submodules that depend on spring-core can declare the dependency using only the groupId and artifactId, and the version will be inherited:\n\nMoreover, you can provide exclusions for dependency management in parent’s pom.xml, so that specific libraries will not be inherited by child modules:\n\nFinally, if a child module needs to use a different version of a managed dependency, you can override the managed version in the child’s pom.xml file:\n\nPlease note that while child modules inherit from their parent project, a parent project does not necessarily have any modules that it aggregates. On the other hand, a parent project may also aggregate projects that do not inherit from it.\n\nFor more information on inheritance and aggregation please refer to this documentation.\n\nWe can change the packaging type of each submodule. For example, let’s change the packaging of the webapp module to WAR by updating the pom.xml file:\n\nand adding maven-war-plugin in the plugins list:\n\nNow we can test the build of our project by using the mvn clean install command. The output of the Maven logs should be similar to this:\n\nIn this article, we discussed the benefits of using Maven multi-modules. We also distinguished between regular Maven’s parent POM and an aggregate POM. Finally, we explored how to set up a simple multi-module to start to play with.\n\nMaven is a great tool, but it’s complex on its own. If we want to learn more details about Maven, we can look at the Sonatype Maven reference or Apache Maven guides. If we seek advanced usages of Maven’s multi-modules set-up, we can look at how the Spring Boot project leverages its usage."
    },
    {
        "link": "https://docs.spring.io/spring-boot/docs/2.2.9.RELEASE/reference/htmlsingle",
        "document": "Various properties can be specified inside your file, inside your file, or as command line switches. This appendix provides a list of common Spring Boot properties and references to the underlying classes that consume them. Spring Boot provides various conversion mechanism with advanced value formatting, make sure to review the properties conversion section. Property contributions can come from additional jar files on your classpath, so you should not consider this an exhaustive list. Also, you can define your own properties. Arbitrary properties to add to the info endpoint. Location of the logging configuration file. For instance, `classpath:logback.xml` for Logback. Whether to clean the archive log files on startup. Only supported with the default logback setup. Maximum number of days archive log files are kept. Only supported with the default logback setup. Maximum log file size. Only supported with the default logback setup. Log file name (for instance, `myapp.log`). Names can be an exact location or relative to the current directory. Location of the log file. For instance, `/var/log`. Total size of log backups to be kept. Only supported with the default logback setup. Log groups to quickly change multiple loggers at the same time. For instance, `logging.group.db=org.hibernate,org.springframework.jdbc`. Appender pattern for output to the console. Supported only with the default Logback setup. Appender pattern for log date format. Supported only with the default Logback setup. Appender pattern for output to a file. Supported only with the default Logback setup. Appender pattern for log level. Supported only with the default Logback setup. Pattern for rolled-over log file names. Supported only with the default Logback setup. Register a shutdown hook for the logging system when it is initialized. Whether subclass-based (CGLIB) proxies are to be created (true), as opposed to standard Java interface-based proxies (false). Whether to enable admin features for the application. JMX name of the application admin MBean. The bit depth to use for ANSI colors. Supported values are 4 (16 color) or 8 (256 color). Height of the banner image in chars (default based on image height). Whether images should be inverted for dark terminal themes. Banner image file location (jpg or png can also be used). The pixel mode to use when rendering the image. Width of the banner image in chars. Whether to skip search of BeanInfo classes. Limit on the number of bytes that can be buffered whenever the input stream needs to be aggregated. By default this is not set, in which case individual codec defaults apply. Most codecs are limited to 256K by default. Config file locations used in addition to the defaults. Whether unique runtime object names should be ensured. Whether bean definition overriding, by registering a definition with the same name as an existing definition, is allowed. Mode used to display the banner when the application runs. Whether initialization should be performed lazily. Whether to log information about the application when it starts. Whether the application should have a shutdown hook registered. Sources (class names, package names, or XML resource locations) to include in the ApplicationContext. Flag to explicitly request a specific type of web application. If not set, auto-detected based on the classpath. Expected character encoding the application must use. Whether to always apply the MessageFormat rules, parsing even messages without arguments. Comma-separated list of basenames (essentially a fully-qualified classpath location), each following the ResourceBundle convention with relaxed support for slash based locations. If it doesn't contain a package qualifier (such as \"org.mypackage\"), it will be resolved from the classpath root. Loaded resource bundle files cache duration. When not set, bundles are cached forever. If a duration suffix is not specified, seconds will be used. Whether to fall back to the system Locale if no files for a specific Locale have been found. if this is turned off, the only fallback will be the default file (e.g. \"messages.properties\" for basename \"messages\"). Whether to use the message code as the default message instead of throwing a \"NoSuchMessageException\". Recommended during development only. Fails if ApplicationPidFileWriter is used but it cannot write the PID file. Location of the PID file to write (if ApplicationPidFileWriter is used). Comma-separated list of profile expressions that at least one should match for the document to be included. Comma-separated list of active profiles. Can be overridden by a command line switch. Unconditionally activate the specified comma-separated list of profiles (or list of profiles if using YAML). Whether to automatically start the scheduler after initialization. Path to the SQL file to use to initialize the database schema. Delay after which the scheduler is started once initialization completes. Setting this property makes sense if no jobs should be run before the entire application has started up. Whether to wait for running jobs to complete on shutdown. Whether the Reactor Debug Agent should be enabled when reactor-tools is present. Whether core threads are allowed to time out. This enables dynamic growing and shrinking of the pool. Time limit for which threads may remain idle before being terminated. Maximum allowed number of threads. If tasks are filling up the queue, the pool can expand up to that size to accommodate the load. Ignored if the queue is unbounded. Queue capacity. An unbounded capacity does not increase the pool and therefore ignores the \"max-size\" property. Whether the executor should wait for scheduled tasks to complete on shutdown. Maximum time the executor should wait for remaining tasks to complete. Prefix to use for the names of newly created threads. Whether the executor should wait for scheduled tasks to complete on shutdown. Maximum time the executor should wait for remaining tasks to complete. Prefix to use for the names of newly created threads. Comma-separated list of cache names to create if supported by the underlying cache manager. Usually, this disables the ability to create additional caches on-the-fly. The spec to use to create caches. See CaffeineSpec for more details on the spec format. Entry expiration. By default the entries never expire. Note that this value is ultimately converted to seconds. The location of the configuration file to use to initialize EhCache. The location of the configuration file to use to initialize Infinispan. The location of the configuration file to use to initialize the cache manager. The configuration file is dependent of the underlying cache implementation. Fully qualified name of the CachingProvider implementation to use to retrieve the JSR-107 compliant cache manager. Needed only if more than one JSR-107 implementation is available on the classpath. Entry expiration. By default the entries never expire. Whether to use the key prefix when writing to Redis. Cache type. By default, auto-detected according to the environment. Session JNDI name. When set, takes precedence over other Session settings. Protocol used by the SMTP server. Whether to test that the mail server is available on startup. Format to use when serializing Date objects. Whether to disable the escaping of HTML characters such as '<', '>', etc. Whether to exclude inner classes during serialization. Whether to enable serialization of complex map keys (i.e. non-primitives). Whether to exclude all fields from consideration for serialization or deserialization that do not have the \"Expose\" annotation. Naming policy that should be applied to an object's field during serialization and deserialization. Whether to generate non executable JSON by prefixing the output with some special text. Whether to be lenient about parsing JSON that doesn't conform to RFC 4627. Whether to output serialized JSON that fits in a page for pretty printing. Date format string or a fully-qualified date format class name. For instance, `yyyy-MM-dd HH:mm:ss`. Controls the inclusion of properties during serialization. Configured with one of the values in Jackson's JsonInclude.Include enumeration. Jackson on/off features that affect the way Java objects are deserialized. One of the constants on Jackson's PropertyNamingStrategy. Can also be a fully-qualified class name of a PropertyNamingStrategy subclass. Jackson on/off features that affect the way Java objects are serialized. Time zone used when formatting dates. For instance, \"America/Los_Angeles\" or \"GMT+10\". Jackson visibility thresholds that can be used to limit which methods (and fields) are auto-detected. Couchbase nodes (host or IP address) to bootstrap from. Name of the bucket to connect to. Number of sockets per node against the key/value service. Whether to enable SSL support. Enabled automatically if a \"keyStore\" is provided unless specified otherwise. Path to the JVM key store that holds the certificates. Password used to access the key store. Whether to enable the PersistenceExceptionTranslationPostProcessor. Name of the Cassandra cluster. Whether to enable JMX reporting. Default to false as Cassandra JMX reporting is not compatible with Dropwizard Metrics. Heartbeat interval after which a message is sent on an idle connection to make sure it's still alive. If a duration suffix is not specified, seconds will be used. Idle timeout before an idle connection is removed. If a duration suffix is not specified, seconds will be used. Maximum number of requests that get queued if no connection is available. Pool timeout when trying to acquire a connection from a host's pool. Schema action to take at startup. Automatically create views and indexes. Use the meta-data provided by \"@ViewIndexed\", \"@N1qlPrimaryIndexed\" and \"@N1qlSecondaryIndexed\". Consistency to apply by default on generated queries. Comma-separated list of the Elasticsearch endpoints to connect to. Whether the client should use SSL to connect to the endpoints. Fully qualified name of the FieldNamingStrategy to use. Mongo server host. Cannot be set with URI. Login password of the mongo server. Cannot be set with URI. Mongo server port. Cannot be set with URI. Mongo database URI. Cannot be set with host, port and credentials. Login user of the mongo server. Cannot be set with URI. Whether to enable embedded mode if the embedded driver is available. Register OpenSessionInViewInterceptor. Binds a Neo4j Session to the thread for the entire processing of the request.\", URI used by the driver. Auto-detected by default. Whether to use Neo4j native types wherever possible. Base path to be used by Spring Data REST to expose repository resources. Content type to use as a default when none is specified. Strategy to use to determine which repositories get exposed. Whether to enable enum value translation through the Spring Data REST default resource bundle. Name of the URL query string parameter that indicates how many results to return at once. Name of the URL query string parameter that indicates what page to return. Whether to return a response body after creating an entity. Whether to return a response body after updating an entity. Name of the URL query string parameter that indicates what direction to sort results. Solr host. Ignored if \"zk-host\" is set. Whether to expose and assume 1-based page number indexes. Defaults to \"false\", meaning a page number of 0 in the request equals the first page. General prefix to be prepended to the page number and page size parameters. Delimiter to be used between the qualifier and the actual page number and size properties. Whether to stop if an error occurs while initializing the database. Password of the database to execute DML scripts (if different). Username of the database to execute DML scripts (if different). Commons DBCP2 specific settings bound to an instance of DBCP2's BasicDataSource Fully qualified name of the JDBC driver. Auto-detected based on the URL by default. Whether to generate a random datasource name. Hikari specific settings bound to an instance of Hikari's HikariDataSource Initialize the datasource with available DDL and DML scripts. JNDI location of the datasource. Class, url, username & password are ignored when set. Name of the datasource. Default to \"testdb\" when using an embedded database. Platform to use in the DDL or DML scripts (such as schema-${platform}.sql or data-${platform}.sql). Password of the database to execute DDL scripts (if different). Username of the database to execute DDL scripts (if different). Tomcat datasource specific settings bound to an instance of Tomcat JDBC's DataSource Fully qualified name of the connection pool implementation to use. By default, it is auto-detected from the classpath. Properties to pass to the XA data source. Whether to enable connection requests from multiple execution threads. Proxy host the HTTP client should use. Proxy port the HTTP client should use. Comma-separated list of the Elasticsearch instances to use. Whether to enable the console. Path at which the console is available. URL of the InfluxDB instance to which to connect. Number of rows that should be fetched from the database when more rows are needed. Use -1 to use the JDBC driver's default configuration. Maximum number of rows. Use -1 to use the JDBC driver's default configuration. Query timeout. Default is to use the JDBC driver's default configuration. If a duration suffix is not specified, seconds will be used. SQL dialect to use. Auto-detected by default. Target database to operate on, auto-detected by default. Can be alternatively set using the \"databasePlatform\" property. Name of the target database to operate on, auto-detected by default. Can be alternatively set using the \"Database\" enum. Whether to initialize the schema on startup. DDL mode. This is actually a shortcut for the \"hibernate.hbm2ddl.auto\" property. Defaults to \"create-drop\" when using an embedded database and no schema manager was detected. Otherwise, defaults to \"none\". Fully qualified name of the implicit naming strategy. Fully qualified name of the physical naming strategy. Whether to use Hibernate's newer IdentifierGenerator for AUTO, TABLE and SEQUENCE. This is actually a shortcut for the \"hibernate.id.new_generator_mappings\" property. When not specified will default to \"true\". Register OpenEntityManagerInViewInterceptor. Binds a JPA EntityManager to the thread for the entire processing of the request. Additional native properties to set on the JPA provider. Whether to enable logging of SQL statements. Comma-separated list of features to enable. Uses the defaults of the configured version by default. Client name to be set on connections with CLIENT SETNAME. Maximum number of redirects to follow when executing commands across the cluster. Comma-separated list of \"host:port\" pairs to bootstrap from. This represents an \"initial\" list of cluster nodes and is required to have at least one entry. Database index used by the connection factory. Maximum number of connections that can be allocated by the pool at a given time. Use a negative value for no limit. Maximum number of \"idle\" connections in the pool. Use a negative value to indicate an unlimited number of idle connections. Maximum amount of time a connection allocation should block before throwing an exception when the pool is exhausted. Use a negative value to block indefinitely. Target for the minimum number of idle connections to maintain in the pool. This setting only has an effect if both it and time between eviction runs are positive. Time between runs of the idle object evictor thread. When positive, the idle object evictor thread starts, otherwise no idle object eviction is performed. Maximum number of connections that can be allocated by the pool at a given time. Use a negative value for no limit. Maximum number of \"idle\" connections in the pool. Use a negative value to indicate an unlimited number of idle connections. Maximum amount of time a connection allocation should block before throwing an exception when the pool is exhausted. Use a negative value to block indefinitely. Target for the minimum number of idle connections to maintain in the pool. This setting only has an effect if both it and time between eviction runs are positive. Time between runs of the idle object evictor thread. When positive, the idle object evictor thread starts, otherwise no idle object eviction is performed. Name of the Redis server. Connection URL. Overrides host, port, and password. User is ignored. Example: redis://user:[email protected]:6379 Timeout, in seconds, for borrowing connections from the pool. Whether to ignore the transacted flag when creating session. Time, in seconds, between runs of the pool's maintenance thread. Time, in seconds, after which connections are cleaned up from the pool. Time, in seconds, that a connection can be pooled for before being destroyed. 0 denotes no limit. Reap timeout, in seconds, for borrowed connections. 0 denotes no limit. Unique name used to identify the resource during recovery. Timeout, in seconds, for borrowing connections from the pool. Whether to use concurrent connection validation. Default isolation level of connections provided by the pool. Time, in seconds, between runs of the pool's maintenance thread. Time, in seconds, after which connections are cleaned up from the pool. Time, in seconds, that a connection can be pooled for before being destroyed. 0 denotes no limit. Reap timeout, in seconds, for borrowed connections. 0 denotes no limit. SQL query or statement used to validate a connection before returning it. Unique name used to identify the resource during recovery. Specify whether sub-transactions are allowed. Interval between checkpoints, expressed as the number of log writes between two checkpoints. A checkpoint reduces the log file size at the expense of adding some overhead in the runtime. How long should normal shutdown (no-force) wait for transactions to complete. Whether a VM shutdown should trigger forced shutdown of the transaction core. Directory in which the log files should be stored. Defaults to the current working directory. Maximum timeout that can be allowed for transactions. Delay after which recovery can cleanup pending ('orphaned') log entries. Number of retry attempts to commit the transaction before throwing an exception. Whether sub-transactions should be joined when possible. Transaction manager implementation that should be started. Whether to use different (and concurrent) threads for two-phase commit on the participating resources. The transaction manager's unique name. Defaults to the machine's IP address. If you plan to run more than one transaction manager against one database you must set this property to a unique value. Number of connections to create when growing the pool. Time, in seconds, to wait before trying to acquire a connection again after an invalid connection was acquired. Timeout, in seconds, for acquiring connections from the pool. Whether the transaction manager should allow mixing XA and non-XA transactions. Whether the transaction timeout should be set on the XAResource when it is enlisted. Whether resources should be enlisted and delisted automatically. Whether producers and consumers should be cached. Underlying implementation class name of the XA resource. Whether the provider can run many transactions on the same connection and supports transaction interleaving. Whether this resource is disabled, meaning it's temporarily forbidden to acquire a connection from its pool. Properties that should be set on the underlying implementation. Whether recovery failures should be ignored. Time, in seconds, after which connections are cleaned up from the pool. Maximum size of the pool. 0 denotes no limit. Password to use to connect to the JMS provider. Whether connections in the ACCESSIBLE state can be shared within the context of a transaction. Whether connections should be tested when acquired from the pool. Position that this resource should take during two-phase commit (always first is Integer.MIN_VALUE, always last is Integer.MAX_VALUE). Unique name used to identify the resource during recovery. Whether TMJOIN should be used when starting XAResources. User to use to connect to the JMS provider. Number of connections to create when growing the pool. Time, in seconds, to wait before trying to acquire a connection again after an invalid connection was acquired. Timeout, in seconds, for acquiring connections from the pool. Whether the transaction manager should allow mixing XA and non-XA transactions. Whether the transaction timeout should be set on the XAResource when it is enlisted. Whether resources should be enlisted and delisted automatically. Underlying implementation class name of the XA resource. Whether the database can run many transactions on the same connection and supports transaction interleaving. Whether this resource is disabled, meaning it's temporarily forbidden to acquire a connection from its pool. Properties that should be set on the underlying implementation. Whether Connection.isValid() is called when acquiring a connection from the pool. Whether recovery failures should be ignored. Time, in seconds, after which connections are cleaned up from the pool. Maximum size of the pool. 0 denotes no limit. Target size of the prepared statement cache. 0 disables the cache. Whether connections in the ACCESSIBLE state can be shared within the context of a transaction. SQL query or statement used to validate a connection before returning it. Position that this resource should take during two-phase commit (always first is Integer.MIN_VALUE, and always last is Integer.MAX_VALUE). Unique name used to identify the resource during recovery. Whether TMJOIN should be used when starting XAResources. Whether to allow multiple LRC resources to be enlisted into the same transaction. Whether to enable asynchronously execution of two phase commit. Interval in seconds at which to run the recovery process in the background. Whether to recover only the current node. Should be enabled if you run multiple instances of the transaction manager on the same JMS and JDBC resources. Whether to log the creation and commit call stacks of transactions executed without a single enlisted resource. Set the fully qualified name of the exception analyzer implementation to use. Whether to enable filtering of logs so that only mandatory logs are written. Whether logs are forced to disk. Maximum amount of seconds the TM waits for transactions to get done before aborting them at shutdown time. JNDI name of the TransactionSynchronizationRegistry. JNDI name of the UserTransaction. Name of the journal. Can be 'disk', 'null', or a class name. Name of the first fragment of the journal. Name of the second fragment of the journal. Maximum size in megabytes of the journal fragments. ASCII ID that must uniquely identify this TM instance. Defaults to the machine's IP address. Skip corrupted transactions log entries. Use only at last resort when all you have to recover is a pair of corrupted files. Whether to log a warning for transactions executed without a single enlisted resource. Default transaction timeout. If a duration suffix is not specified, seconds will be used. Whether to roll back on commit failures. Description to tag an existing schema with when applying a baseline. Whether to automatically call baseline when migrating a non-empty schema. Version to tag an existing schema with when executing baseline. Whether to batch SQL statements when executing them. Requires Flyway Pro or Flyway Enterprise. Whether to check that migration scripts location exists. Whether to disable cleaning of the database. Whether to automatically call clean when a validation error occurs. Maximum number of retries when attempting to connect to the database. Rules for the built-in error handling to override specific SQL states and error codes. Requires Flyway Pro or Flyway Enterprise. Whether to group all pending migrations together in the same transaction when applying them. Whether to ignore future migrations when reading the schema history table. Whether to ignore ignored migrations when reading the schema history table. Whether to ignore missing migrations when reading the schema history table. Whether to ignore pending migrations when reading the schema history table. SQL statements to execute to initialize a connection immediately after obtaining it. Username recorded in the schema history table as having applied the migration. Locations of migrations scripts. Can contain the special \"{vendor}\" placeholder to use vendor-specific locations. Whether to allow mixing transactional and non-transactional statements within the same migration. Whether to enable support for Oracle SQL*Plus commands. Requires Flyway Pro or Flyway Enterprise. Whether to issue a warning rather than an error when a not-yet-supported Oracle SQL*Plus statement is encountered. Requires Flyway Pro or Flyway Enterprise. Whether to allow migrations to be run out of order. Login password of the database to migrate. Placeholders and their replacements to apply to sql migration scripts. Whether to skip default callbacks. If true, only custom callbacks are used. Whether to skip default resolvers. If true, only custom resolvers are used. Whether to stream SQL migrations when executing them. Requires Flyway Pro or Flyway Enterprise. Name of the schema history table that will be used by Flyway. Tablespace in which the schema history table is created. Ignored when using a database that does not support tablespaces. Defaults to the default tablespace of the connection used by Flyway. Target version up to which migrations should be considered. JDBC url of the database to migrate. If not set, the primary configured data source is used. Login user of the database to migrate. Whether to automatically call validate when performing a migration. Comma-separated list of runtime contexts to use. Name of table to use for tracking concurrent Liquibase usage. Name of table to use for tracking change history. Whether to first drop the database schema. Comma-separated list of runtime labels to use. Schema to use for Liquibase objects. Tablespace to use for Liquibase objects. Login password of the database to migrate. File to which rollback SQL is written when an update is performed. Whether rollback should be tested before update is performed. JDBC URL of the database to migrate. If not set, the primary configured data source is used. Login user of the database to migrate. URL of the ActiveMQ broker. Auto-generated by default. Time to wait before considering a close complete. Whether the default broker URL should be in memory. Ignored if an explicit broker has been specified. Whether to stop message delivery before re-delivering messages from a rolled back transaction. This implies that message order is not preserved when this is enabled. Whether to trust all packages. Comma-separated list of specific packages to trust (when not trusting all packages). Whether to block when a connection is requested and the pool is full. Set it to false to throw a \"JMSException\" instead. Blocking period before throwing an exception if the pool is still full. Whether a JmsPoolConnectionFactory should be created, instead of a regular ConnectionFactory. Maximum number of pooled sessions per connection in the pool. Time to sleep between runs of the idle connection eviction thread. When negative, no idle connection eviction thread runs. Whether to use only one anonymous \"MessageProducer\" instance. Set it to false to create one \"MessageProducer\" every time one is required. Time to wait on message sends for a response. Set it to 0 to wait forever. Journal file directory. Not necessary if persistence is turned off. Whether to enable embedded mode if the Artemis server APIs are available. Comma-separated list of queues to create on startup. Server ID. By default, an auto-incremented counter is used. Comma-separated list of topics to create on startup. Whether to block when a connection is requested and the pool is full. Set it to false to throw a \"JMSException\" instead. Blocking period before throwing an exception if the pool is still full. Whether a JmsPoolConnectionFactory should be created, instead of a regular ConnectionFactory. Maximum number of pooled sessions per connection in the pool. Time to sleep between runs of the idle connection eviction thread. When negative, no idle connection eviction thread runs. Whether to use only one anonymous \"MessageProducer\" instance. Set it to false to create one \"MessageProducer\" every time one is required. Execute all Spring Batch jobs in the context on startup. Comma-separated list of job names to execute on startup (for instance, `job1,job2`). By default, all Jobs found in the context are executed. Path to the SQL file to use to initialize the database schema. Table prefix for all the batch meta-data tables. The location of the configuration file to use to initialize Hazelcast. Path to the SQL file to use to initialize the database schema. Size of the session cache (per JMS Session type). Connection factory JNDI name. When set, takes precedence to others connection factory auto-configurations. Acknowledge mode of the container. By default, the listener is transacted with automatic acknowledgment. Timeout to use for receive calls. Use -1 for a no-wait receive or 0 for no timeout at all. The latter is only feasible if not running within a transaction manager and is generally discouraged since it prevents clean shutdown. Whether the default destination type is topic. Default destination to use on send and receive operations that do not have a destination parameter. Delivery delay to use for send calls. Priority of a message when sending. Enables QoS (Quality of Service) when set. Whether to enable explicit QoS (Quality of Service) when sending a message. When enabled, the delivery mode, priority and time-to-live properties will be used when sending a message. QoS is automatically enabled when at least one of those settings is customized. Timeout to use for receive calls. Time-to-live of a message when sending. Enables QoS (Quality of Service) when set. ID to pass to the server when making requests. Used for server-side logging. Whether to fail fast if the broker is not available on startup. Additional admin-specific properties used to configure the client. Password of the private key in the key store file. Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. Applies to all components unless overridden. ID to pass to the server when making requests. Used for server-side logging. Frequency with which the consumer offsets are auto-committed to Kafka if 'enable.auto.commit' is set to true. What to do when there is no initial offset in Kafka or if the current offset no longer exists on the server. Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. Overrides the global property, for consumers. ID to pass to the server when making requests. Used for server-side logging. Whether the consumer's offset is periodically committed in the background. Maximum amount of time the server blocks before answering the fetch request if there isn't sufficient data to immediately satisfy the requirement given by \"fetch-min-size\". Minimum amount of data the server should return for a fetch request. Unique string that identifies the consumer group to which this consumer belongs. Expected time between heartbeats to the consumer coordinator. Isolation level for reading messages that have been written transactionally. Maximum number of records returned in a single call to poll(). Additional consumer-specific properties used to configure the client. Password of the private key in the key store file. Number of records between offset commits when ackMode is \"COUNT\" or \"COUNT_TIME\". Time between offset commits when ackMode is \"TIME\" or \"COUNT_TIME\". Number of threads to run in the listener containers. Whether to log the container configuration during initialization (INFO level). Whether the container should fail to start if at least one of the configured topics are not present on the broker. Time between checks for non-responsive consumers. If a duration suffix is not specified, seconds will be used. Multiplier applied to \"pollTimeout\" to determine if a consumer is non-responsive. Timeout to use when polling the consumer. Number of acknowledgments the producer requires the leader to have received before considering a request complete. Default batch size. A small batch size will make batching less common and may reduce throughput (a batch size of zero disables batching entirely). Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. Overrides the global property, for producers. Total memory size the producer can use to buffer records waiting to be sent to the server. ID to pass to the server when making requests. Used for server-side logging. Compression type for all data generated by the producer. Additional producer-specific properties used to configure the client. When greater than zero, enables retrying of failed sends. Password of the private key in the key store file. When non empty, enables transaction support for producer. Additional properties, common to producers and consumers, used to configure the client. Password of the private key in the key store file. Whether or not to auto-start the streams factory bean. Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. Overrides the global property, for streams. Maximum memory size to be used for buffering across all threads. ID to pass to the server when making requests. Used for server-side logging. Additional Kafka properties used to configure the streams. The replication factor for change log topics and repartition topics created by the stream processing application. Password of the private key in the key store file. Default topic to which messages are sent. Comma-separated list of addresses to which the client should connect. Duration to wait to obtain a channel if the cache size has been reached. If 0, always create a new channel. Number of channels to retain in the cache. When \"check-timeout\" > 0, max channels per connection. Number of connections to cache. Only applies when mode is CONNECTION. Connection timeout. Set it to zero to wait forever. Whether to create an AmqpAdmin bean. Whether to start the container automatically on startup. Whether rejected deliveries are re-queued by default. How often idle container events should be published. Whether to fail if the queues declared by the container are not available on the broker. Maximum number of unacknowledged messages that can be outstanding at each consumer. Duration between the first and second attempt to deliver a message. Multiplier to apply to the previous retry interval. Whether retries are stateless or stateful. Whether to start the container automatically on startup. Batch size, expressed as the number of physical messages, to be used by the container. Whether rejected deliveries are re-queued by default. How often idle container events should be published. Whether to fail if the queues declared by the container are not available on the broker and/or whether to stop the container if one or more queues are deleted at runtime. Maximum number of unacknowledged messages that can be outstanding at each consumer. Duration between the first and second attempt to deliver a message. Multiplier to apply to the previous retry interval. Whether retries are stateless or stateful. Login to authenticate against the broker. Type of publisher confirms to use. Requested heartbeat timeout; zero for none. If a duration suffix is not specified, seconds will be used. SSL algorithm to use. By default, configured by the Rabbit client library. Path to the key store that holds the SSL certificate. Password used to access the key store. Password used to access the trust store. Name of the default queue to receive messages from when none is specified explicitly. Name of the default exchange to use for send operations. Duration between the first and second attempt to deliver a message. Multiplier to apply to the previous retry interval. Value of a default routing key to use for send operations. Login user to authenticate to the broker. Virtual host to use when connecting to the broker. Path that serves as the base URI for the services. Load on startup priority of the Spring Web Services servlet. Comma-separated list of locations of WSDLs and accompanying XSDs to be exposed as beans. Whether application/hal+json responses should be sent to requests that accept application/json. Preferred JSON mapper to use for HTTP message conversion. By default, auto-detected according to the environment. Charset of HTTP requests and responses. Added to the \"Content-Type\" header if not set explicitly. Whether to force the encoding to the configured charset on HTTP requests and responses. Whether to force the encoding to the configured charset on HTTP requests. Defaults to true when \"force\" has not been specified. Whether to force the encoding to the configured charset on HTTP responses. Locale in which to encode mapping. Whether logging of (potentially sensitive) request details at DEBUG and TRACE level is allowed. Path that serves as the base URI for the application. If specified, overrides the value of \"@ApplicationPath\". Init parameters to pass to Jersey through the servlet or filter. Load on startup priority of the Jersey servlet. Amount of time before asynchronous request handling times out. If this value is not set, the default timeout of the underlying implementation is used. Whether a request parameter (\"format\" by default) should be used to determine the requested media type. Map file extensions to media types for content negotiation. For instance, yml to text/yaml. Query parameter name to use when \"favor-parameter\" is enabled. Date format to use. For instance, `dd/MM/yyyy`. Whether to dispatch OPTIONS requests to the FrameworkServlet doService method. Whether to dispatch TRACE requests to the FrameworkServlet doService method. Whether the content of the \"default\" model should be ignored during redirect scenarios. Locale to use. By default, this locale is overridden by the \"Accept-Language\" header. Define how the locale should be resolved. Whether to enable warn logging of exceptions resolved by a \"HandlerExceptionResolver\", except for \"DefaultHandlerExceptionResolver\". Whether to publish a ServletRequestHandledEvent at the end of each request. Load on startup priority of the dispatcher servlet. Whether a \"NoHandlerFoundException\" should be thrown if no Handler was found to process a request. Indicate that the response message is intended for a single user and must not be stored by a shared cache. Indicate that any cache may store the response. Maximum time the response should be cached, in seconds if no duration suffix is not specified. Indicate that once it has become stale, a cache must not use the response without re-validating it with the server. Indicate that the cached response can be reused only if re-validated with the server. Indicate to not cache the response in any case. Indicate intermediaries (caches and others) that they should not transform the response content. Same meaning as the \"must-revalidate\" directive, except that it does not apply to private caches. Maximum time the response should be cached by shared caches, in seconds if no duration suffix is not specified. Maximum time the response may be used when errors are encountered, in seconds if no duration suffix is not specified. Maximum time the response can be served after it becomes stale, in seconds if no duration suffix is not specified. Cache period for the resources served by the resource handler. If a duration suffix is not specified, seconds will be used. Can be overridden by the 'spring.resources.cache.cachecontrol' properties. Whether to enable caching in the Resource chain. Whether to enable resolution of already compressed resources (gzip, brotli). Checks for a resource name with the '.gz' or '.br' file extensions. Whether to enable the Spring Resource Handling chain. By default, disabled unless at least one strategy has been enabled. Whether to enable the content Version Strategy. Comma-separated list of patterns to apply to the content Version Strategy. Whether to enable the fixed Version Strategy. Comma-separated list of patterns to apply to the fixed Version Strategy. Version string to use for the fixed Version Strategy. Whether to enable support of multipart uploads. Threshold after which files are written to disk. Whether to resolve the multipart request lazily at the time of file or parameter access. Date format to use. For instance, `dd/MM/yyyy`. Whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name. Whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name. Whether to check that the templates location exists. Whether to enable MVC view resolution for this technology. Whether all request attributes should be added to the model prior to merging with the template. Whether all HttpSession attributes should be added to the model prior to merging with the template. Whether to expose a RequestContext for use by Spring's macro library, under the name \"springMacroRequestContext\". Whether to prefer file system access for template loading. File system access enables hot detection of template changes. Prefix that gets prepended to view names when building a URL. Name of the RequestContext attribute for all views. Well-known FreeMarker keys which are passed to FreeMarker's Configuration. Suffix that gets appended to view names when building a URL. View names that can be resolved. Whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name. Whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name. Whether to check that the templates location exists. Whether to enable MVC view resolution for this technology. Whether all request attributes should be added to the model prior to merging with the template. Whether all HttpSession attributes should be added to the model prior to merging with the template. Whether to expose a RequestContext for use by Spring's macro library, under the name \"springMacroRequestContext\". Prefix that gets prepended to view names when building a URL. Name of the RequestContext attribute for all views. Suffix that gets appended to view names when building a URL. View names that can be resolved. Whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name. Whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name. Whether to check that the templates location exists. Whether to enable MVC view resolution for this technology. Whether all request attributes should be added to the model prior to merging with the template. Whether all HttpSession attributes should be added to the model prior to merging with the template. Whether to expose a RequestContext for use by Spring's macro library, under the name \"springMacroRequestContext\". Name of the RequestContext attribute for all views. View names that can be resolved. Whether to check that the template exists before rendering it. Whether to check that the templates location exists. Whether to enable Thymeleaf view resolution for Web frameworks. Comma-separated list of view names (patterns allowed) that should be excluded from resolution. Template mode to be applied to templates. See also Thymeleaf's TemplateMode enum. Prefix that gets prepended to view names when building a URL. Comma-separated list of view names (patterns allowed) that should be the only ones executed in CHUNKED mode when a max chunk size is set. Comma-separated list of view names (patterns allowed) that should be executed in FULL mode even if a max chunk size is set. Maximum size of data buffers used for writing to the response. Templates will execute in CHUNKED mode by default if this is set. Whether hidden form inputs acting as markers for checkboxes should be rendered before the checkbox element itself. Whether Thymeleaf should start writing partial output as soon as possible or buffer until template processing is finished. Suffix that gets appended to view names when building a URL. Order of the template resolver in the chain. By default, the template resolver is first in the chain. Order start at 1 and should only be set if you have defined additional \"TemplateResolver\" beans. Comma-separated list of view names (patterns allowed) that can be resolved. Network address to which the server should bind. Comma-separated list of user agents for which responses should not be compressed. Comma-separated list of MIME types that should be compressed. Minimum \"Content-Length\" value that is required for compression to be performed. Whether to enable the default error page displayed in browsers in case of a server error. Whether to enable HTTP/2 support, if the current environment supports it. Number of acceptor threads to use. When the value is -1, the default, the number of acceptors is derived from the operating environment. Custom log format, see org.eclipse.jetty.server.CustomRequestLog. If defined, overrides the \"format\" configuration key. Date format to place in log file name. Log filename. If not specified, logs redirect to \"System.err\". Request paths that should not be logged. Number of days before rotated log files are deleted. Time that the connection can be idle before it is closed. Maximum size of the form content in any HTTP post request. Number of selector threads to use. When the value is -1, the default, the number of selectors is derived from the operating environment. Value to use for the Server response header (if empty, no header is sent). Display name of the application. Class name of the servlet to use for JSPs. If registered is true and this class * is on the classpath then it will be registered. Init parameters used to configure the JSP servlet. Whether the JSP servlet is registered. Whether to use \"HttpOnly\" cookies for session cookies. Maximum age of the session cookie. If a duration suffix is not specified, seconds will be used. Whether to always mark the session cookie as secure. Whether to persist session data between restarts. Session timeout. If a duration suffix is not specified, seconds will be used. Alias that identifies the key in the key store. Password used to access the key in the key store. Path to the key store that holds the SSL certificate (typically a jks file). Password used to access the key store. Password used to access the trust store. Maximum queue length for incoming connection requests when all possible request processing threads are in use. Whether to buffer output such that it is flushed only periodically. Whether to check for log file existence so it can be recreated it if an external process has renamed it. Whether logging of the request will only be enabled if \"ServletRequest.getAttribute(conditionIf)\" does not yield null. Whether logging of the request will only be enabled if \"ServletRequest.getAttribute(conditionUnless)\" yield null. Directory in which log files are created. Can be absolute or relative to the Tomcat base dir. Character set used by the log file. Default to the system default character set. Date format to place in the log file name. Whether to use IPv6 canonical representation format as defined by RFC 5952. Locale used to format timestamps in log entries and in log file name suffix. Default to the default locale of the Java process. Number of days to retain the access log files before they are removed. Whether to defer inclusion of the date stamp in the file name until rotate time. Set request attributes for the IP address, Hostname, protocol, and port used for the request. Comma-separated list of additional patterns that match jars to ignore for TLD scanning. The special '?' and '*' characters can be used in the pattern to match one and only one character and zero or more characters respectively. Delay between the invocation of backgroundProcess methods. If a duration suffix is not specified, seconds will be used. Tomcat base directory. If not specified, a temporary directory is used. Amount of time the connector will wait, after accepting a connection, for the request URI line to be presented. Name of the HTTP header from which the remote host is extracted. Regular expression that matches proxies that are to be trusted. Maximum number of connections that the server accepts and processes at any given time. Once the limit has been reached, the operating system may still accept connections based on the \"acceptCount\" property. Maximum size of the form content in any HTTP post request. Whether Tomcat's MBean Registry should be enabled. Name of the HTTP header used to override the original port value. Maximum number of idle processors that will be retained in the cache and reused with a subsequent request. When set to -1 the cache will be unlimited with a theoretical maximum size equal to the maximum number of connections. Header that holds the incoming protocol, usually named \"X-Forwarded-Proto\". Value of the protocol header indicating whether the incoming request uses SSL. Whether requests to the context root should be redirected by appending a / to the path. Comma-separated list of additional unencoded characters that should be allowed in URI paths. Only \"< > [ \\ ] ^ ` { | }\" are allowed. Comma-separated list of additional unencoded characters that should be allowed in URI query strings. Only \"< > [ \\ ] ^ ` { | }\" are allowed. Name of the HTTP header from which the remote IP is extracted. For instance, `X-FORWARDED-FOR`. Whether static resource caching is permitted for this web application. Character encoding to use to decode the URI. Whether HTTP 1.1 and later location headers generated by a call to sendRedirect will use relative or absolute redirects. Whether to enable the access log. Whether the server should decode percent encoded slash characters. Enabling encoded slashes can have security implications due to different servers interpreting the slash differently. Only enable this if you have a legacy application that requires it. Whether the 'Connection: keep-alive' header should be added to all responses, even if not required by the HTTP specification. Size of each buffer. The default is derived from the maximum amount of memory that is available to the JVM. Whether the URL should be decoded. When disabled, percent-encoded characters in the URL will be left as-is. Whether to allocate buffers outside the Java heap. The default is derived from the maximum amount of memory that is available to the JVM. Whether servlet filters should be initialized on startup. Number of I/O threads to create for the worker. The default is derived from the number of available processors. Maximum number of cookies that are allowed. This limit exists to prevent hash collision based DOS attacks. Maximum number of headers that are allowed. This limit exists to prevent hash collision based DOS attacks. Maximum size of the HTTP post content. When the value is -1, the default, the size is unlimited. Maximum number of query or path parameters that are allowed. This limit exists to prevent hash collision based DOS attacks. Amount of time a connection can sit idle without processing a request, before it is closed by the server. Number of worker threads. The default is 8 times the number of I/O threads. Whether read-only operations should use an anonymous environment. Disabled by default unless a username is set. Base suffix from which all operations should originate. URI that can either be an OpenID Connect discovery endpoint or an OAuth 2.0 Authorization Server Metadata endpoint defined by RFC 8414. JSON Web Key URI to use to verify the JWT token. JSON Web Algorithm used for verifying the digital signatures. Location of the file containing the public key used to verify a JWT. Client id used to authenticate with the token introspection endpoint. Client secret used to authenticate with the token introspection endpoint. OAuth 2.0 endpoint through which token introspection is accomplished. Password for the default user name. Granted roles for the default user name. Sessions flush mode. Determines when session changes are written to the session store. Name of the map used to store sessions. Sessions save mode. Determines how session changes are tracked and saved to the session store. Sessions flush mode. Determines when session changes are written to the session store. Sessions save mode. Determines how session changes are tracked and saved to the session store. Path to the SQL file to use to initialize the database schema. Name of the database table used to store sessions. Collection name used to store sessions. The configure action to apply when no user defined ConfigureRedisAction bean is present. Sessions flush mode. Determines when session changes are written to the session store. Namespace for keys used to store sessions. Sessions save mode. Determines how session changes are tracked and saved to the session store. Session timeout. If a duration suffix is not specified, seconds will be used. Network address to which the server should bind. Path under which RSocket handles requests (only works with websocket transport). Whether to enable storage of audit events. Whether to skip SSL verification for Cloud Foundry actuator endpoint security calls. Maximum time that a response can be cached. Whether to enable the auditevents endpoint. Maximum time that a response can be cached. Whether to enable the beans endpoint. Maximum time that a response can be cached. Whether to enable the caches endpoint. Maximum time that a response can be cached. Whether to enable the conditions endpoint. Maximum time that a response can be cached. Whether to enable the configprops endpoint. Keys that should be sanitized. Keys can be simple strings that the property ends with or regular expressions. Maximum time that a response can be cached. Whether to enable the env endpoint. Keys that should be sanitized. Keys can be simple strings that the property ends with or regular expressions. Maximum time that a response can be cached. Whether to enable the flyway endpoint. Maximum time that a response can be cached. Whether to enable the health endpoint. Roles used to determine whether or not a user is authorized to be shown details. When empty, all authenticated users are authorized. When to show components. If not specified the 'show-details' setting will be used. Mapping of health statuses to HTTP status codes. By default, registered health statuses map to sensible defaults (for example, UP maps to 200). Comma-separated list of health statuses in order of severity. Maximum time that a response can be cached. Whether to enable the heapdump endpoint. Maximum time that a response can be cached. Whether to enable the httptrace endpoint. Maximum time that a response can be cached. Whether to enable the info endpoint. Maximum time that a response can be cached. Whether to enable the integrationgraph endpoint. Jolokia settings. Refer to the documentation of Jolokia for more details. Whether to enable the jolokia endpoint. Maximum time that a response can be cached. Whether to enable the liquibase endpoint. Maximum time that a response can be cached. Whether to enable the logfile endpoint. External Logfile to be accessed. Can be used if the logfile is written by output redirect and not by the logging system itself. Maximum time that a response can be cached. Whether to enable the loggers endpoint. Maximum time that a response can be cached. Whether to enable the mappings endpoint. Maximum time that a response can be cached. Whether to enable the metrics endpoint. Maximum time that a response can be cached. Whether to enable the prometheus endpoint. Maximum time that a response can be cached. Whether to enable the scheduledtasks endpoint. Whether to enable the sessions endpoint. Whether to enable the shutdown endpoint. Maximum time that a response can be cached. Whether to enable the threaddump endpoint. Whether to enable or disable all endpoints by default. Endpoints JMX domain name. Fallback to 'spring.jmx.default-domain' if set. Endpoint IDs that should be excluded or '*' for all. Endpoint IDs that should be included or '*' for all. Additional static properties to append to all ObjectNames of MBeans representing Endpoints. Base path for Web endpoints. Relative to server.servlet.context-path or management.server.servlet.context-path if management.server.port is configured. Whether credentials are supported. When not set, credentials are not supported. Comma-separated list of headers to allow in a request. '*' allows all headers. Comma-separated list of methods to allow. '*' allows all methods. When not set, defaults to GET. Comma-separated list of origins to allow. '*' allows all origins. When not set, CORS support is disabled. Comma-separated list of headers to include in a response. How long the response from a pre-flight request can be cached by clients. If a duration suffix is not specified, seconds will be used. Endpoint IDs that should be excluded or '*' for all. Endpoint IDs that should be included or '*' for all. Mapping between endpoint IDs and the path that should expose them. Path used to compute the available disk space. Minimum disk space that should be available. Mode to use to expose git information. Maximum value that meter IDs starting-with the specified name are expected to observe. The longest match wins. Values can be specified as a long or as a Duration value (for timer meters, defaulting to ms if no unit specified). Minimum value that meter IDs starting-with the specified name are expected to observe. The longest match wins. Values can be specified as a long or as a Duration value (for timer meters, defaulting to ms if no unit specified). Whether meter IDs starting with the specified name should publish percentile histograms. For monitoring systems that support aggregable percentile calculation based on a histogram, this can be set to true. For other systems, this has no effect. The longest match wins, the key `all` can also be used to configure all meters. Specific computed non-aggregable percentiles to ship to the backend for meter IDs starting-with the specified name. The longest match wins, the key `all` can also be used to configure all meters. Specific SLA boundaries for meter IDs starting-with the specified name. The longest match wins. Counters will be published for each specified boundary. Values can be specified as a long or as a Duration value (for timer meters, defaulting to ms if no unit specified). Whether meter IDs starting-with the specified name should be enabled. The longest match wins, the key `all` can also be used to configure all meters. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. Tag that will be mapped to \"@host\" when shipping metrics to AppOptics. Read timeout for requests to this backend. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Frequency for refreshing config settings from the LWC service. Time to live for subscriptions from the LWC service. URI for the Atlas LWC endpoint to retrieve current subscriptions. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. URI for the Atlas LWC endpoint to evaluate the data for a subscription. Whether to enable streaming to Atlas LWC. Time to live for meters that do not have any activity. After this period the meter will be considered expired and will not get reported. Number of threads to use with the metrics publishing scheduler. Read timeout for requests to this backend. Datadog application key. Not strictly required, but improves the Datadog experience by sending meter descriptions, types, and base units to Datadog. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether to publish descriptions metadata to Datadog. Turn this off to minimize the amount of metadata sent. Whether exporting of metrics to this backend is enabled. Tag that will be mapped to \"host\" when shipping metrics to Datadog. Read timeout for requests to this backend. URI to ship metrics to. If you need to publish metrics to an internal proxy en-route to Datadog, you can define the location of the proxy with this. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. ID of the custom device that is exporting metrics to Dynatrace. Whether exporting of metrics to this backend is enabled. Group for exported metrics. Used to specify custom device group name in the Dynatrace UI. Read timeout for requests to this backend. Technology type for exported metrics. Used to group metrics under a logical technology name in the Dynatrace UI. URI to ship metrics to. Should be used for SaaS, self managed instances or to en-route through an internal proxy. Whether to create the index automatically if it does not exist. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. Index date format used for rolling indices. Appended to the index name, preceded by a '-'. Read timeout for requests to this backend. Name of the timestamp field. Whether exporting of metrics to Ganglia is enabled. Host of the Ganglia server to receive exported metrics. Port of the Ganglia server to receive exported metrics. Ganglia protocol version. Must be either 3.1 or 3.0. Time to live for metrics on Ganglia. Set the multi-cast Time-To-Live to be one greater than the number of hops (routers) between the hosts. Whether exporting of metrics to Graphite is enabled. Host of the Graphite server to receive exported metrics. Port of the Graphite server to receive exported metrics. Protocol to use while shipping data to Graphite. For the default naming convention, turn the specified tag keys into part of the metric prefix. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. Read timeout for requests to this backend. Humio tags describing the data source in which metrics will be stored. Humio tags are a distinct concept from Micrometer's tags. Micrometer's tags are used to divide metrics along dimensional boundaries. URI to ship metrics to. If you need to publish metrics to an internal proxy en-route to Humio, you can define the location of the proxy with this. Whether to create the Influx database if it does not exist before attempting to publish metrics to it. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Whether to enable GZIP compression of metrics batches published to Influx. Connection timeout for requests to this backend. Tag that will be mapped to \"host\" when shipping metrics to Influx. Whether exporting of metrics to this backend is enabled. Read timeout for requests to this backend. Time period for which Influx should retain data in the current database. For instance 7d, check the influx documentation for more details on the duration format. Retention policy to use (Influx writes to the DEFAULT retention policy if one is not specified). How many copies of the data are stored in the cluster. Must be 1 for a single node instance. Time range covered by a shard group. For instance 2w, check the influx documentation for more details on the duration format. Whether exporting of metrics to JMX is enabled. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. Read timeout for requests to this backend. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. The event type that should be published. This property will be ignored if 'meter-name-event-type-enabled' is set to 'true'. Whether to send the meter name as the event type instead of using the 'event-type' configuration property value. Can be set to 'true' if New Relic guidelines are not being followed or event types consistent with previous Spring Boot releases are required. Read timeout for requests to this backend. Whether to enable publishing descriptions as part of the scrape payload to Prometheus. Turn this off to minimize the amount of data sent on each scrape. Whether exporting of metrics to Prometheus is enabled. Frequency with which to push metrics. Operation that should be performed on shutdown. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. Read timeout for requests to this backend. Uniquely identifies the app instance that is publishing metrics to SignalFx. Defaults to the local host name. Whether, in the absence of any other exporter, exporting of metrics to an in-memory backend is enabled. Whether exporting of metrics to StatsD is enabled. Host of the StatsD server to receive exported metrics. Total length of a single payload should be kept within your network's MTU. How often gauges will be polled. When a gauge is polled, its value is recalculated and if the value has changed (or publishUnchangedMeters is true), it is sent to the StatsD server. Port of the StatsD server to receive exported metrics. Whether to send unchanged meters to the StatsD server. API token used when publishing metrics directly to the Wavefront API host. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. Global prefix to separate metrics originating from this app's white box instrumentation from those originating from other Wavefront integrations when viewed in the Wavefront UI. Read timeout for requests to this backend. Unique identifier for the app instance that is the source of metrics being published to Wavefront. Defaults to the local host name. Common tags that are applied to every meter. Whether auto-configured MeterRegistry implementations should be bound to the global static registry on Metrics. For testing, set this to 'false' to maximize test independence. Maximum number of unique URI tag values allowed. After the max number of tag values is reached, metrics with additional tag values are denied by filter. Whether percentile histograms should be published. Name of the metric for sent requests. Maximum number of unique URI tag values allowed. After the max number of tag values is reached, metrics with additional tag values are denied by filter. Whether percentile histograms should be published. Whether the trailing slash should be ignored when recording metrics. Name of the metric for received requests. Add the \"X-Application-Context\" HTTP header in each response. Network address to which the management endpoints should bind. Requires a custom management.server.port. Management endpoint HTTP port (uses the same port as the application by default). Configure a different port to use management-specific SSL. Alias that identifies the key in the key store. Password used to access the key in the key store. Path to the key store that holds the SSL certificate (typically a jks file). Password used to access the key store. Password used to access the trust store. Items to be included in the trace. Defaults to request headers (excluding Authorization but including Cookie), response headers (including Set-Cookie), and time taken. Context path used to handle the remote connection. The host of the proxy to use to connect to the remote application. The port of the proxy to use to connect to the remote application. HTTP header used to transfer the shared secret. Additional patterns that should be excluded from triggering a full restart. Additional paths to watch for changes. Patterns that should be excluded from triggering a full restart. Whether to log the condition evaluation delta upon restart. Amount of time to wait between polling for classpath changes. Amount of quiet time required without any classpath changes before a restart is triggered. Name of a specific file that, when changed, triggers the restart check. Must be a simple name (without any path) of a file that appears on your classpath. If not specified, any classpath file change triggers the restart. Spring Boot jars include metadata files that provide details of all supported configuration properties. The files are designed to let IDE developers offer contextual help and “code completion” as users are working with or files. The majority of the metadata file is generated automatically at compile time by processing all items annotated with . However, it is possible to write part of the metadata manually for corner cases or more advanced use cases. Configuration metadata files are located inside jars under . They use a JSON format with items categorized under either “groups” or “properties” and additional values hints categorized under \"hints\", as shown in the following example: {\"groups\": [ { \"name\": \"server\", \"type\": \"org.springframework.boot.autoconfigure.web.ServerProperties\", \"sourceType\": \"org.springframework.boot.autoconfigure.web.ServerProperties\" }, { \"name\": \"spring.jpa.hibernate\", \"type\": \"org.springframework.boot.autoconfigure.orm.jpa.JpaProperties$Hibernate\", \"sourceType\": \"org.springframework.boot.autoconfigure.orm.jpa.JpaProperties\", \"sourceMethod\": \"getHibernate()\" } ... ],\"properties\": [ { \"name\": \"server.port\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.boot.autoconfigure.web.ServerProperties\" }, { \"name\": \"server.address\", \"type\": \"java.net.InetAddress\", \"sourceType\": \"org.springframework.boot.autoconfigure.web.ServerProperties\" }, { \"name\": \"spring.jpa.hibernate.ddl-auto\", \"type\": \"java.lang.String\", \"description\": \"DDL mode. This is actually a shortcut for the \\\"hibernate.hbm2ddl.auto\\\" property.\", \"sourceType\": \"org.springframework.boot.autoconfigure.orm.jpa.JpaProperties$Hibernate\" } ... ],\"hints\": [ { \"name\": \"spring.jpa.hibernate.ddl-auto\", \"values\": [ { \"value\": \"none\", \"description\": \"Disable DDL handling.\" }, { \"value\": \"validate\", \"description\": \"Validate the schema, make no changes to the database.\" }, { \"value\": \"update\", \"description\": \"Update the schema if necessary.\" }, { \"value\": \"create\", \"description\": \"Create the schema and destroy previous data.\" }, { \"value\": \"create-drop\", \"description\": \"Create and then destroy the schema at the end of the session.\" } ] } ]} Each “property” is a configuration item that the user specifies with a given value. For example, and might be specified in , as follows: The “groups” are higher level items that do not themselves specify a value but instead provide a contextual grouping for properties. For example, the and properties are part of the group. It is not required that every “property” has a “group”. Some properties might exist in their own right. Finally, “hints” are additional information used to assist the user in configuring a given property. For example, when a developer is configuring the property, a tool can use the hints to offer some auto-completion help for the , , , , and values. The JSON object contained in the array can contain the attributes shown in the following table: The full name of the group. This attribute is mandatory. The class name of the data type of the group. For example, if the group were based on a class annotated with , the attribute would contain the fully qualified name of that class. If it were based on a method, it would be the return type of that method. If the type is not known, the attribute may be omitted. A short description of the group that can be displayed to users. If no description is available, it may be omitted. It is recommended that descriptions be short paragraphs, with the first line providing a concise summary. The last line in the description should end with a period ( ). The class name of the source that contributed this group. For example, if the group were based on a method annotated with , this attribute would contain the fully qualified name of the class that contains the method. If the source type is not known, the attribute may be omitted. The full name of the method (include parenthesis and argument types) that contributed this group (for example, the name of a annotated method). If the source method is not known, it may be omitted. The JSON object contained in the array can contain the attributes described in the following table: The full name of the property. Names are in lower-case period-separated form (for example, ). This attribute is mandatory. The full signature of the data type of the property (for example, ) but also a full generic type (such as ). You can use this attribute to guide the user as to the types of values that they can enter. For consistency, the type of a primitive is specified by using its wrapper counterpart (for example, becomes ). Note that this class may be a complex type that gets converted from a as values are bound. If the type is not known, it may be omitted. A short description of the property that can be displayed to users. If no description is available, it may be omitted. It is recommended that descriptions be short paragraphs, with the first line providing a concise summary. The last line in the description should end with a period ( ). The class name of the source that contributed this property. For example, if the property were from a class annotated with , this attribute would contain the fully qualified name of that class. If the source type is unknown, it may be omitted. The default value, which is used if the property is not specified. If the type of the property is an array, it can be an array of value(s). If the default value is unknown, it may be omitted. Specify whether the property is deprecated. If the field is not deprecated or if that information is not known, it may be omitted. The next table offers more detail about the attribute. The JSON object contained in the attribute of each element can contain the following attributes: The level of deprecation, which can be either (the default) or . When a property has a deprecation level, it should still be bound in the environment. However, when it has an deprecation level, the property is no longer managed and is not bound. A short description of the reason why the property was deprecated. If no reason is available, it may be omitted. It is recommended that descriptions be short paragraphs, with the first line providing a concise summary. The last line in the description should end with a period ( ). The full name of the property that replaces this deprecated property. If there is no replacement for this property, it may be omitted. Prior to Spring Boot 1.3, a single boolean attribute can be used instead of the element. This is still supported in a deprecated fashion and should no longer be used. If no reason and replacement are available, an empty object should be set. Deprecation can also be specified declaratively in code by adding the annotation to the getter exposing the deprecated property. For instance, assume that the property was confusing and was renamed to . The following example shows how to handle that situation: There is no way to set a . is always assumed, since code is still handling the property. The preceding code makes sure that the deprecated property still works (delegating to the property behind the scenes). Once the and methods can be removed from your public API, the automatic deprecation hint in the metadata goes away as well. If you want to keep a hint, adding manual metadata with an deprecation level ensures that users are still informed about that property. Doing so is particularly useful when a is provided. The JSON object contained in the array can contain the attributes shown in the following table: The full name of the property to which this hint refers. Names are in lower-case period-separated form (such as ). If the property refers to a map (such as ), the hint either applies to the keys of the map ( ) or the values ( ) of the map. This attribute is mandatory. A list of valid values as defined by the object (described in the next table). Each entry defines the value and may have a description. A list of providers as defined by the object (described later in this document). Each entry defines the name of the provider and its parameters, if any. The JSON object contained in the attribute of each element can contain the attributes described in the following table: A valid value for the element to which the hint refers. If the type of the property is an array, it can also be an array of value(s). This attribute is mandatory. A short description of the value that can be displayed to users. If no description is available, it may be omitted. It is recommended that descriptions be short paragraphs, with the first line providing a concise summary. The last line in the description should end with a period ( ). The JSON object contained in the attribute of each element can contain the attributes described in the following table: The name of the provider to use to offer additional content assistance for the element to which the hint refers. Any additional parameter that the provider supports (check the documentation of the provider for more details). Objects with the same “property” and “group” name can appear multiple times within a metadata file. For example, you could bind two separate classes to the same prefix, with each having potentially overlapping property names. While the same names appearing in the metadata multiple times should not be common, consumers of metadata should take care to ensure that they support it. To improve the user experience and further assist the user in configuring a given property, you can provide additional metadata that:\n• Describes the list of potential values for a property.\n• Associates a provider, to attach a well defined semantic to a property, so that a tool can discover the list of potential values based on the project’s context. The attribute of each hint refers to the of a property. In the initial example shown earlier, we provide five values for the property: , , , , and . Each value may have a description as well. If your property is of type , you can provide hints for both the keys and the values (but not for the map itself). The special and suffixes must refer to the keys and the values, respectively. Assume a maps magic values to an integer, as shown in the following example: The magic values are (in this example) are and . In order to offer additional content assistance for the keys, you could add the following JSON to the manual metadata of the module: {\"hints\": [ { \"name\": \"sample.contexts.keys\", \"values\": [ { \"value\": \"sample1\" }, { \"value\": \"sample2\" } ] } ]} We recommend that you use an for those two values instead. If your IDE supports it, this is by far the most effective approach to auto-completion. Providers are a powerful way to attach semantics to a property. In this section, we define the official providers that you can use for your own hints. However, your favorite IDE may implement some of these or none of them. Also, it could eventually provide its own. As this is a new feature, IDE vendors must catch up with how it works. Adoption times naturally vary. The following table summarizes the list of supported providers: Permits any additional value to be provided. Auto-completes the classes available in the project. Usually constrained by a base class that is specified by the parameter. Handles the property as if it were defined by the type defined by the mandatory parameter. Auto-completes valid logger names and logger groups. Typically, package and class names available in the current project can be auto-completed as well as defined groups. Auto-completes the available bean names in the current project. Usually constrained by a base class that is specified by the parameter. Auto-completes the available Spring profile names in the project. Only one provider can be active for a given property, but you can specify several providers if they can all manage the property in some way. Make sure to place the most powerful provider first, as the IDE must use the first one in the JSON section that it can handle. If no provider for a given property is supported, no special content assistance is provided, either. The special any provider value permits any additional values to be provided. Regular value validation based on the property type should be applied if this is supported. This provider is typically used if you have a list of values and any extra values should still be considered as valid. The following example offers and as auto-completion values for : {\"hints\": [ { \"name\": \"system.state\", \"values\": [ { \"value\": \"on\" }, { \"value\": \"off\" } ], \"providers\": [ { \"name\": \"any\" } ] } ]} Note that, in the preceding example, any other value is also allowed. The class-reference provider auto-completes classes available in the project. This provider supports the following parameters: The fully qualified name of the class that should be assignable to the chosen value. Typically used to filter out-non candidate classes. Note that this information can be provided by the type itself by exposing a class with the appropriate upper bound. Specify whether only concrete classes are to be considered as valid candidates. The following metadata snippet corresponds to the standard property that defines the class name to use: The handle-as provider lets you substitute the type of the property to a more high-level type. This typically happens when the property has a type, because you do not want your configuration classes to rely on classes that may not be on the classpath. This provider supports the following parameters: The fully qualified name of the type to consider for the property. This parameter is mandatory. The following types can be used:\n• Any : Lists the possible values for the property. (We recommend defining the property with the type, as no further hint should be required for the IDE to auto-complete the values)\n• : Supports auto-completion of charset/encoding values (such as )\n• : auto-completion of locales (such as )\n• : Supports auto-completion of content type values (such as )\n• : Supports auto-completion of Spring’s Resource abstraction to refer to a file on the filesystem or on the classpath (such as ) If multiple values can be provided, use a or Array type to teach the IDE about it. The following metadata snippet corresponds to the standard property that defines the path to the changelog to use. It is actually used internally as a but cannot be exposed as such, because we need to keep the original String value to pass it to the Liquibase API. The logger-name provider auto-completes valid logger names and logger groups. Typically, package and class names available in the current project can be auto-completed. If groups are enabled (default) and if a custom logger group is identified in the configuration, auto-completion for it should be provided. Specific frameworks may have extra magic logger names that can be supported as well. This provider supports the following parameters: Specify whether known groups should be considered. Since a logger name can be any arbitrary name, this provider should allow any value but could highlight valid package and class names that are not available in the project’s classpath. The following metadata snippet corresponds to the standard property. Keys are logger names, and values correspond to the standard log levels or any custom level. As Spring Boot defines a few logger groups out-of-the-box, dedicated value hints have been added for those. {\"hints\": [ { \"name\": \"logging.level.keys\", \"values\": [ { \"value\": \"root\", \"description\": \"Root logger used to assign the default logging level.\" }, { \"value\": \"sql\", \"description\": \"SQL logging group including Hibernate SQL logger.\" }, { \"value\": \"web\", \"description\": \"Web logging group including codecs.\" } ], \"providers\": [ { \"name\": \"logger-name\" } ] }, { \"name\": \"logging.level.values\", \"values\": [ { \"value\": \"trace\" }, { \"value\": \"debug\" }, { \"value\": \"info\" }, { \"value\": \"warn\" }, { \"value\": \"error\" }, { \"value\": \"fatal\" }, { \"value\": \"off\" } ], \"providers\": [ { \"name\": \"any\" } ] } ]} The spring-bean-reference provider auto-completes the beans that are defined in the configuration of the current project. This provider supports the following parameters: The fully qualified name of the bean class that should be assignable to the candidate. Typically used to filter out non-candidate beans. The following metadata snippet corresponds to the standard property that defines the name of the bean to use: The binder is not aware of the metadata. If you provide that hint, you still need to transform the bean name into an actual Bean reference using by the . The spring-profile-name provider auto-completes the Spring profiles that are defined in the configuration of the current project. The following metadata snippet corresponds to the standard property that defines the name of the Spring profile(s) to enable: 10.B.3. Generating Your Own Metadata by Using the Annotation Processor You can easily generate your own configuration metadata file from items annotated with by using the jar. The jar includes a Java annotation processor which is invoked as your project is compiled. To use the processor, include a dependency on . With Maven the dependency should be declared as optional, as shown in the following example: If you have defined in your application, make sure to configure the to prevent the goal from adding the dependency into the fat jar: With Gradle 4.5 and earlier, the dependency should be declared in the configuration, as shown in the following example: With Gradle 4.6 and later, the dependency should be declared in the configuration, as shown in the following example: If you are using an file, the task should be configured to depend on the task, as shown in the following example: This dependency ensures that the additional metadata is available when the annotation processor runs during compilation. The processor picks up both classes and methods that are annotated with . The Javadoc for field values within configuration classes is used to populate the attribute. You should only use plain text with field Javadoc, since they are not processed before being added to the JSON. If the class has a single constructor with at least one parameters, one property is created per constructor parameter. Otherwise, properties are discovered through the presence of standard getters and setters with special handling for collection types (that is detected even if only a getter is present). The annotation processor also supports the use of the , , and lombok annotations. The annotation processor cannot auto-detect default values for s and s. In the cases where a or property has a non-empty default value, manual metadata should be provided. Consider the following class: In order to document default values for properties in the class above, you could add the following content to the manual metadata of the module: Only the of the property is required to document additional fields with manual metadata. If you are using AspectJ in your project, you need to make sure that the annotation processor runs only once. There are several ways to do this. With Maven, you can configure the explicitly and add the dependency to the annotation processor only there. You could also let the AspectJ plugin run all the processing and disable annotation processing in the configuration, as follows: The annotation processor automatically considers inner classes as nested properties. Consider the following class: @ConfigurationProperties(prefix=\"server\") public class ServerProperties { private String name; private Host host; // ... getter and setters public static class Host { private String ip; private int port; // ... getter and setters } } The preceding example produces metadata information for , , and properties. You can use the annotation on a field to indicate that a regular (non-inner) class should be treated as if it were nested. This has no effect on collections and maps, as those types are automatically identified, and a single metadata property is generated for each of them. Spring Boot’s configuration file handling is quite flexible, and it is often the case that properties may exist that are not bound to a bean. You may also need to tune some attributes of an existing key. To support such cases and let you provide custom \"hints\", the annotation processor automatically merges items from into the main metadata file. If you refer to a property that has been detected automatically, the description, default value, and deprecation information are overridden, if specified. If the manual property declaration is not identified in the current module, it is added as a new property. The format of the file is exactly the same as the regular . The additional properties file is optional. If you do not have any additional properties, do not add the file. This appendix contains details of all of the auto-configuration classes provided by Spring Boot, with links to documentation and source code. Remember to also look at the conditions report in your application for more details of which features are switched on. (To do so, start the app with or or, in an Actuator application, use the endpoint). The following auto-configuration classes are from the module: The following auto-configuration classes are from the module: This appendix describes the auto-configuration annotations that Spring Boot provides to test slices of your application. The following table lists the various annotations that can be used to test slices of your application and the auto-configuration that they import by default: The modules lets Spring Boot support executable jar and war files. If you use the Maven plugin or the Gradle plugin, executable jars are automatically generated, and you generally do not need to know the details of how they work. If you need to create executable jars from a different build system or if you are just curious about the underlying technology, this appendix provides some background. Java does not provide any standard way to load nested jar files (that is, jar files that are themselves contained within a jar). This can be problematic if you need to distribute a self-contained application that can be run from the command line without unpacking. To solve this problem, many developers use “shaded” jars. A shaded jar packages all classes, from all jars, into a single “uber jar”. The problem with shaded jars is that it becomes hard to see which libraries are actually in your application. It can also be problematic if the same filename is used (but with different content) in multiple jars. Spring Boot takes a different approach and lets you actually nest jars directly. Spring Boot Loader-compatible jar files should be structured in the following way: Application classes should be placed in a nested directory. Dependencies should be placed in a nested directory. Spring Boot Loader-compatible war files should be structured in the following way: Dependencies should be placed in a nested directory. Any dependencies that are required when running embedded but are not required when deploying to a traditional web container should be placed in . The core class used to support loading nested jars is . It lets you load jar content from a standard jar file or from nested child jar data. When first loaded, the location of each is mapped to a physical file offset of the outer jar, as shown in the following example: The preceding example shows how can be found in in at position . from the nested jar can actually be found in at position , and is at position . Armed with this information, we can load specific nested entries by seeking to the appropriate part of the outer jar. We do not need to unpack the archive, and we do not need to read all entry data into memory. Spring Boot Loader strives to remain compatible with existing code and libraries. extends from and should work as a drop-in replacement. The method returns a that opens a connection compatible with and can be used with Java’s . The class is a special bootstrap class that is used as an executable jar’s main entry point. It is the actual in your jar file, and it is used to setup an appropriate and ultimately call your method. There are three launcher subclasses ( , , and ). Their purpose is to load resources ( files and so on) from nested jar files or war files in directories (as opposed to those explicitly on the classpath). In the case of and , the nested paths are fixed. looks in , and looks in and . You can add extra jars in those locations if you want more. The looks in in your application archive by default. You can add additional locations by setting an environment variable called or in (which is a comma-separated list of directories, archives, or directories within archives). You need to specify an appropriate as the attribute of . The actual class that you want to launch (that is, the class that contains a method) should be specified in the attribute. The following example shows a typical for an executable jar file: For a war file, it would be as follows: You need not specify entries in your manifest file. The classpath is deduced from the nested jars. has a few special features that can be enabled with external properties (System properties, environment variables, manifest entries, or ). The following table describes these properties: Comma-separated Classpath, such as . Earlier entries take precedence, like a regular on the command line. Used to resolve relative paths in . For example, given , then is a classpath location (along with all jar files in that directory). This property is also used to locate a file, as in the following example It defaults to . Name of main class to launch (for example, ). Name of properties file (for example, ). It defaults to . Path to properties file (for example, ). It defaults to . Boolean flag to indicate that all properties should be added to System properties. It defaults to . When specified as environment variables or manifest entries, the following names should be used: Build plugins automatically move the attribute to when the fat jar is built. If you use that, specify the name of the class to launch by using the attribute and leaving out . The following rules apply to working with :\n• is searched for in , then in the root of the classpath, and then in . The first location where a file with that name exists is used.\n• is the directory location of an additional properties file (overriding the default) only when is not specified.\n• can contain directories (which are scanned recursively for jar and zip files), archive paths, a directory within an archive that is scanned for jar files (for example, ), or wildcard patterns (for the default JVM behavior). Archive paths can be relative to or anywhere in the file system with a prefix.\n• (if empty) defaults to (meaning a local directory or a nested one if running from an archive). Because of this, behaves the same as when no additional configuration is provided.\n• can not be used to configure the location of (the classpath used to search for the latter is the JVM classpath when is launched).\n• Placeholder replacement is done from System and environment variables plus the properties file itself on all values before use.\n• The search order for properties (where it makes sense to look in more than one place) is environment variables, system properties, , the exploded archive manifest, and the archive manifest. You need to consider the following restrictions when working with a Spring Boot Loader packaged application:\n• Zip entry compression: The for a nested jar must be saved by using the method. This is required so that we can seek directly to individual content within the nested jar. The content of the nested jar file itself can still be compressed, as can any other entries in the outer jar.\n• System classLoader: Launched applications should use when loading classes (most libraries and frameworks do so by default). Trying to load nested jar classes with fails. always uses the system classloader. For this reason, you should consider a different logging implementation. If the preceding restrictions mean that you cannot use Spring Boot Loader, consider the following alternatives: This appendix provides details of the dependencies that are managed by Spring Boot. The following table provides details of all of the dependency versions that are provided by Spring Boot in its CLI (Command Line Interface), Maven dependency management, and Gradle plugin. When you declare a dependency on one of these artifacts without declaring a version, the version listed in the table is used."
    },
    {
        "link": "https://docs.spring.io/spring-boot/docs/2.2.10.RELEASE/reference/htmlsingle",
        "document": "Various properties can be specified inside your file, inside your file, or as command line switches. This appendix provides a list of common Spring Boot properties and references to the underlying classes that consume them. Spring Boot provides various conversion mechanism with advanced value formatting, make sure to review the properties conversion section. Property contributions can come from additional jar files on your classpath, so you should not consider this an exhaustive list. Also, you can define your own properties. Arbitrary properties to add to the info endpoint. Location of the logging configuration file. For instance, `classpath:logback.xml` for Logback. Whether to clean the archive log files on startup. Only supported with the default logback setup. Maximum number of days archive log files are kept. Only supported with the default logback setup. Maximum log file size. Only supported with the default logback setup. Log file name (for instance, `myapp.log`). Names can be an exact location or relative to the current directory. Location of the log file. For instance, `/var/log`. Total size of log backups to be kept. Only supported with the default logback setup. Log groups to quickly change multiple loggers at the same time. For instance, `logging.group.db=org.hibernate,org.springframework.jdbc`. Appender pattern for output to the console. Supported only with the default Logback setup. Appender pattern for log date format. Supported only with the default Logback setup. Appender pattern for output to a file. Supported only with the default Logback setup. Appender pattern for log level. Supported only with the default Logback setup. Pattern for rolled-over log file names. Supported only with the default Logback setup. Register a shutdown hook for the logging system when it is initialized. Whether subclass-based (CGLIB) proxies are to be created (true), as opposed to standard Java interface-based proxies (false). Whether to enable admin features for the application. JMX name of the application admin MBean. The bit depth to use for ANSI colors. Supported values are 4 (16 color) or 8 (256 color). Height of the banner image in chars (default based on image height). Whether images should be inverted for dark terminal themes. Banner image file location (jpg or png can also be used). The pixel mode to use when rendering the image. Width of the banner image in chars. Whether to skip search of BeanInfo classes. Limit on the number of bytes that can be buffered whenever the input stream needs to be aggregated. By default this is not set, in which case individual codec defaults apply. Most codecs are limited to 256K by default. Config file locations used in addition to the defaults. Whether unique runtime object names should be ensured. Whether bean definition overriding, by registering a definition with the same name as an existing definition, is allowed. Mode used to display the banner when the application runs. Whether initialization should be performed lazily. Whether to log information about the application when it starts. Whether the application should have a shutdown hook registered. Sources (class names, package names, or XML resource locations) to include in the ApplicationContext. Flag to explicitly request a specific type of web application. If not set, auto-detected based on the classpath. Expected character encoding the application must use. Whether to always apply the MessageFormat rules, parsing even messages without arguments. Comma-separated list of basenames (essentially a fully-qualified classpath location), each following the ResourceBundle convention with relaxed support for slash based locations. If it doesn't contain a package qualifier (such as \"org.mypackage\"), it will be resolved from the classpath root. Loaded resource bundle files cache duration. When not set, bundles are cached forever. If a duration suffix is not specified, seconds will be used. Whether to fall back to the system Locale if no files for a specific Locale have been found. if this is turned off, the only fallback will be the default file (e.g. \"messages.properties\" for basename \"messages\"). Whether to use the message code as the default message instead of throwing a \"NoSuchMessageException\". Recommended during development only. Fails if ApplicationPidFileWriter is used but it cannot write the PID file. Location of the PID file to write (if ApplicationPidFileWriter is used). Comma-separated list of profile expressions that at least one should match for the document to be included. Comma-separated list of active profiles. Can be overridden by a command line switch. Unconditionally activate the specified comma-separated list of profiles (or list of profiles if using YAML). Whether to automatically start the scheduler after initialization. Path to the SQL file to use to initialize the database schema. Delay after which the scheduler is started once initialization completes. Setting this property makes sense if no jobs should be run before the entire application has started up. Whether to wait for running jobs to complete on shutdown. Whether the Reactor Debug Agent should be enabled when reactor-tools is present. Whether core threads are allowed to time out. This enables dynamic growing and shrinking of the pool. Time limit for which threads may remain idle before being terminated. Maximum allowed number of threads. If tasks are filling up the queue, the pool can expand up to that size to accommodate the load. Ignored if the queue is unbounded. Queue capacity. An unbounded capacity does not increase the pool and therefore ignores the \"max-size\" property. Whether the executor should wait for scheduled tasks to complete on shutdown. Maximum time the executor should wait for remaining tasks to complete. Prefix to use for the names of newly created threads. Whether the executor should wait for scheduled tasks to complete on shutdown. Maximum time the executor should wait for remaining tasks to complete. Prefix to use for the names of newly created threads. Comma-separated list of cache names to create if supported by the underlying cache manager. Usually, this disables the ability to create additional caches on-the-fly. The spec to use to create caches. See CaffeineSpec for more details on the spec format. Entry expiration. By default the entries never expire. Note that this value is ultimately converted to seconds. The location of the configuration file to use to initialize EhCache. The location of the configuration file to use to initialize Infinispan. The location of the configuration file to use to initialize the cache manager. The configuration file is dependent of the underlying cache implementation. Fully qualified name of the CachingProvider implementation to use to retrieve the JSR-107 compliant cache manager. Needed only if more than one JSR-107 implementation is available on the classpath. Entry expiration. By default the entries never expire. Whether to use the key prefix when writing to Redis. Cache type. By default, auto-detected according to the environment. Session JNDI name. When set, takes precedence over other Session settings. Protocol used by the SMTP server. Whether to test that the mail server is available on startup. Format to use when serializing Date objects. Whether to disable the escaping of HTML characters such as '<', '>', etc. Whether to exclude inner classes during serialization. Whether to enable serialization of complex map keys (i.e. non-primitives). Whether to exclude all fields from consideration for serialization or deserialization that do not have the \"Expose\" annotation. Naming policy that should be applied to an object's field during serialization and deserialization. Whether to generate non executable JSON by prefixing the output with some special text. Whether to be lenient about parsing JSON that doesn't conform to RFC 4627. Whether to output serialized JSON that fits in a page for pretty printing. Date format string or a fully-qualified date format class name. For instance, `yyyy-MM-dd HH:mm:ss`. Controls the inclusion of properties during serialization. Configured with one of the values in Jackson's JsonInclude.Include enumeration. Jackson on/off features that affect the way Java objects are deserialized. One of the constants on Jackson's PropertyNamingStrategy. Can also be a fully-qualified class name of a PropertyNamingStrategy subclass. Jackson on/off features that affect the way Java objects are serialized. Time zone used when formatting dates. For instance, \"America/Los_Angeles\" or \"GMT+10\". Jackson visibility thresholds that can be used to limit which methods (and fields) are auto-detected. Couchbase nodes (host or IP address) to bootstrap from. Name of the bucket to connect to. Number of sockets per node against the key/value service. Whether to enable SSL support. Enabled automatically if a \"keyStore\" is provided unless specified otherwise. Path to the JVM key store that holds the certificates. Password used to access the key store. Whether to enable the PersistenceExceptionTranslationPostProcessor. Name of the Cassandra cluster. Whether to enable JMX reporting. Default to false as Cassandra JMX reporting is not compatible with Dropwizard Metrics. Heartbeat interval after which a message is sent on an idle connection to make sure it's still alive. If a duration suffix is not specified, seconds will be used. Idle timeout before an idle connection is removed. If a duration suffix is not specified, seconds will be used. Maximum number of requests that get queued if no connection is available. Pool timeout when trying to acquire a connection from a host's pool. Schema action to take at startup. Automatically create views and indexes. Use the meta-data provided by \"@ViewIndexed\", \"@N1qlPrimaryIndexed\" and \"@N1qlSecondaryIndexed\". Consistency to apply by default on generated queries. Comma-separated list of the Elasticsearch endpoints to connect to. Whether the client should use SSL to connect to the endpoints. Fully qualified name of the FieldNamingStrategy to use. Mongo server host. Cannot be set with URI. Login password of the mongo server. Cannot be set with URI. Mongo server port. Cannot be set with URI. Mongo database URI. Cannot be set with host, port and credentials. Login user of the mongo server. Cannot be set with URI. Whether to enable embedded mode if the embedded driver is available. Register OpenSessionInViewInterceptor. Binds a Neo4j Session to the thread for the entire processing of the request.\", URI used by the driver. Auto-detected by default. Whether to use Neo4j native types wherever possible. Base path to be used by Spring Data REST to expose repository resources. Content type to use as a default when none is specified. Strategy to use to determine which repositories get exposed. Whether to enable enum value translation through the Spring Data REST default resource bundle. Name of the URL query string parameter that indicates how many results to return at once. Name of the URL query string parameter that indicates what page to return. Whether to return a response body after creating an entity. Whether to return a response body after updating an entity. Name of the URL query string parameter that indicates what direction to sort results. Solr host. Ignored if \"zk-host\" is set. Whether to expose and assume 1-based page number indexes. Defaults to \"false\", meaning a page number of 0 in the request equals the first page. General prefix to be prepended to the page number and page size parameters. Delimiter to be used between the qualifier and the actual page number and size properties. Whether to stop if an error occurs while initializing the database. Password of the database to execute DML scripts (if different). Username of the database to execute DML scripts (if different). Commons DBCP2 specific settings bound to an instance of DBCP2's BasicDataSource Fully qualified name of the JDBC driver. Auto-detected based on the URL by default. Whether to generate a random datasource name. Hikari specific settings bound to an instance of Hikari's HikariDataSource Initialize the datasource with available DDL and DML scripts. JNDI location of the datasource. Class, url, username & password are ignored when set. Name of the datasource. Default to \"testdb\" when using an embedded database. Platform to use in the DDL or DML scripts (such as schema-${platform}.sql or data-${platform}.sql). Password of the database to execute DDL scripts (if different). Username of the database to execute DDL scripts (if different). Tomcat datasource specific settings bound to an instance of Tomcat JDBC's DataSource Fully qualified name of the connection pool implementation to use. By default, it is auto-detected from the classpath. Properties to pass to the XA data source. Whether to enable connection requests from multiple execution threads. Proxy host the HTTP client should use. Proxy port the HTTP client should use. Comma-separated list of the Elasticsearch instances to use. Whether to enable the console. Path at which the console is available. URL of the InfluxDB instance to which to connect. Number of rows that should be fetched from the database when more rows are needed. Use -1 to use the JDBC driver's default configuration. Maximum number of rows. Use -1 to use the JDBC driver's default configuration. Query timeout. Default is to use the JDBC driver's default configuration. If a duration suffix is not specified, seconds will be used. SQL dialect to use. Auto-detected by default. Target database to operate on, auto-detected by default. Can be alternatively set using the \"databasePlatform\" property. Name of the target database to operate on, auto-detected by default. Can be alternatively set using the \"Database\" enum. Whether to initialize the schema on startup. DDL mode. This is actually a shortcut for the \"hibernate.hbm2ddl.auto\" property. Defaults to \"create-drop\" when using an embedded database and no schema manager was detected. Otherwise, defaults to \"none\". Fully qualified name of the implicit naming strategy. Fully qualified name of the physical naming strategy. Whether to use Hibernate's newer IdentifierGenerator for AUTO, TABLE and SEQUENCE. This is actually a shortcut for the \"hibernate.id.new_generator_mappings\" property. When not specified will default to \"true\". Register OpenEntityManagerInViewInterceptor. Binds a JPA EntityManager to the thread for the entire processing of the request. Additional native properties to set on the JPA provider. Whether to enable logging of SQL statements. Comma-separated list of features to enable. Uses the defaults of the configured version by default. Client name to be set on connections with CLIENT SETNAME. Maximum number of redirects to follow when executing commands across the cluster. Comma-separated list of \"host:port\" pairs to bootstrap from. This represents an \"initial\" list of cluster nodes and is required to have at least one entry. Database index used by the connection factory. Maximum number of connections that can be allocated by the pool at a given time. Use a negative value for no limit. Maximum number of \"idle\" connections in the pool. Use a negative value to indicate an unlimited number of idle connections. Maximum amount of time a connection allocation should block before throwing an exception when the pool is exhausted. Use a negative value to block indefinitely. Target for the minimum number of idle connections to maintain in the pool. This setting only has an effect if both it and time between eviction runs are positive. Time between runs of the idle object evictor thread. When positive, the idle object evictor thread starts, otherwise no idle object eviction is performed. Maximum number of connections that can be allocated by the pool at a given time. Use a negative value for no limit. Maximum number of \"idle\" connections in the pool. Use a negative value to indicate an unlimited number of idle connections. Maximum amount of time a connection allocation should block before throwing an exception when the pool is exhausted. Use a negative value to block indefinitely. Target for the minimum number of idle connections to maintain in the pool. This setting only has an effect if both it and time between eviction runs are positive. Time between runs of the idle object evictor thread. When positive, the idle object evictor thread starts, otherwise no idle object eviction is performed. Name of the Redis server. Connection URL. Overrides host, port, and password. User is ignored. Example: redis://user:[email protected]:6379 Timeout, in seconds, for borrowing connections from the pool. Whether to ignore the transacted flag when creating session. Time, in seconds, between runs of the pool's maintenance thread. Time, in seconds, after which connections are cleaned up from the pool. Time, in seconds, that a connection can be pooled for before being destroyed. 0 denotes no limit. Reap timeout, in seconds, for borrowed connections. 0 denotes no limit. Unique name used to identify the resource during recovery. Timeout, in seconds, for borrowing connections from the pool. Whether to use concurrent connection validation. Default isolation level of connections provided by the pool. Time, in seconds, between runs of the pool's maintenance thread. Time, in seconds, after which connections are cleaned up from the pool. Time, in seconds, that a connection can be pooled for before being destroyed. 0 denotes no limit. Reap timeout, in seconds, for borrowed connections. 0 denotes no limit. SQL query or statement used to validate a connection before returning it. Unique name used to identify the resource during recovery. Specify whether sub-transactions are allowed. Interval between checkpoints, expressed as the number of log writes between two checkpoints. A checkpoint reduces the log file size at the expense of adding some overhead in the runtime. How long should normal shutdown (no-force) wait for transactions to complete. Whether a VM shutdown should trigger forced shutdown of the transaction core. Directory in which the log files should be stored. Defaults to the current working directory. Maximum timeout that can be allowed for transactions. Delay after which recovery can cleanup pending ('orphaned') log entries. Number of retry attempts to commit the transaction before throwing an exception. Whether sub-transactions should be joined when possible. Transaction manager implementation that should be started. Whether to use different (and concurrent) threads for two-phase commit on the participating resources. The transaction manager's unique name. Defaults to the machine's IP address. If you plan to run more than one transaction manager against one database you must set this property to a unique value. Number of connections to create when growing the pool. Time, in seconds, to wait before trying to acquire a connection again after an invalid connection was acquired. Timeout, in seconds, for acquiring connections from the pool. Whether the transaction manager should allow mixing XA and non-XA transactions. Whether the transaction timeout should be set on the XAResource when it is enlisted. Whether resources should be enlisted and delisted automatically. Whether producers and consumers should be cached. Underlying implementation class name of the XA resource. Whether the provider can run many transactions on the same connection and supports transaction interleaving. Whether this resource is disabled, meaning it's temporarily forbidden to acquire a connection from its pool. Properties that should be set on the underlying implementation. Whether recovery failures should be ignored. Time, in seconds, after which connections are cleaned up from the pool. Maximum size of the pool. 0 denotes no limit. Password to use to connect to the JMS provider. Whether connections in the ACCESSIBLE state can be shared within the context of a transaction. Whether connections should be tested when acquired from the pool. Position that this resource should take during two-phase commit (always first is Integer.MIN_VALUE, always last is Integer.MAX_VALUE). Unique name used to identify the resource during recovery. Whether TMJOIN should be used when starting XAResources. User to use to connect to the JMS provider. Number of connections to create when growing the pool. Time, in seconds, to wait before trying to acquire a connection again after an invalid connection was acquired. Timeout, in seconds, for acquiring connections from the pool. Whether the transaction manager should allow mixing XA and non-XA transactions. Whether the transaction timeout should be set on the XAResource when it is enlisted. Whether resources should be enlisted and delisted automatically. Underlying implementation class name of the XA resource. Whether the database can run many transactions on the same connection and supports transaction interleaving. Whether this resource is disabled, meaning it's temporarily forbidden to acquire a connection from its pool. Properties that should be set on the underlying implementation. Whether Connection.isValid() is called when acquiring a connection from the pool. Whether recovery failures should be ignored. Time, in seconds, after which connections are cleaned up from the pool. Maximum size of the pool. 0 denotes no limit. Target size of the prepared statement cache. 0 disables the cache. Whether connections in the ACCESSIBLE state can be shared within the context of a transaction. SQL query or statement used to validate a connection before returning it. Position that this resource should take during two-phase commit (always first is Integer.MIN_VALUE, and always last is Integer.MAX_VALUE). Unique name used to identify the resource during recovery. Whether TMJOIN should be used when starting XAResources. Whether to allow multiple LRC resources to be enlisted into the same transaction. Whether to enable asynchronously execution of two phase commit. Interval in seconds at which to run the recovery process in the background. Whether to recover only the current node. Should be enabled if you run multiple instances of the transaction manager on the same JMS and JDBC resources. Whether to log the creation and commit call stacks of transactions executed without a single enlisted resource. Set the fully qualified name of the exception analyzer implementation to use. Whether to enable filtering of logs so that only mandatory logs are written. Whether logs are forced to disk. Maximum amount of seconds the TM waits for transactions to get done before aborting them at shutdown time. JNDI name of the TransactionSynchronizationRegistry. JNDI name of the UserTransaction. Name of the journal. Can be 'disk', 'null', or a class name. Name of the first fragment of the journal. Name of the second fragment of the journal. Maximum size in megabytes of the journal fragments. ASCII ID that must uniquely identify this TM instance. Defaults to the machine's IP address. Skip corrupted transactions log entries. Use only at last resort when all you have to recover is a pair of corrupted files. Whether to log a warning for transactions executed without a single enlisted resource. Default transaction timeout. If a duration suffix is not specified, seconds will be used. Whether to roll back on commit failures. Description to tag an existing schema with when applying a baseline. Whether to automatically call baseline when migrating a non-empty schema. Version to tag an existing schema with when executing baseline. Whether to batch SQL statements when executing them. Requires Flyway Pro or Flyway Enterprise. Whether to check that migration scripts location exists. Whether to disable cleaning of the database. Whether to automatically call clean when a validation error occurs. Maximum number of retries when attempting to connect to the database. Rules for the built-in error handling to override specific SQL states and error codes. Requires Flyway Pro or Flyway Enterprise. Whether to group all pending migrations together in the same transaction when applying them. Whether to ignore future migrations when reading the schema history table. Whether to ignore ignored migrations when reading the schema history table. Whether to ignore missing migrations when reading the schema history table. Whether to ignore pending migrations when reading the schema history table. SQL statements to execute to initialize a connection immediately after obtaining it. Username recorded in the schema history table as having applied the migration. Locations of migrations scripts. Can contain the special \"{vendor}\" placeholder to use vendor-specific locations. Whether to allow mixing transactional and non-transactional statements within the same migration. Whether to enable support for Oracle SQL*Plus commands. Requires Flyway Pro or Flyway Enterprise. Whether to issue a warning rather than an error when a not-yet-supported Oracle SQL*Plus statement is encountered. Requires Flyway Pro or Flyway Enterprise. Whether to allow migrations to be run out of order. Login password of the database to migrate. Placeholders and their replacements to apply to sql migration scripts. Whether to skip default callbacks. If true, only custom callbacks are used. Whether to skip default resolvers. If true, only custom resolvers are used. Whether to stream SQL migrations when executing them. Requires Flyway Pro or Flyway Enterprise. Name of the schema history table that will be used by Flyway. Tablespace in which the schema history table is created. Ignored when using a database that does not support tablespaces. Defaults to the default tablespace of the connection used by Flyway. Target version up to which migrations should be considered. JDBC url of the database to migrate. If not set, the primary configured data source is used. Login user of the database to migrate. Whether to automatically call validate when performing a migration. Comma-separated list of runtime contexts to use. Name of table to use for tracking concurrent Liquibase usage. Name of table to use for tracking change history. Whether to first drop the database schema. Comma-separated list of runtime labels to use. Schema to use for Liquibase objects. Tablespace to use for Liquibase objects. Login password of the database to migrate. File to which rollback SQL is written when an update is performed. Whether rollback should be tested before update is performed. JDBC URL of the database to migrate. If not set, the primary configured data source is used. Login user of the database to migrate. URL of the ActiveMQ broker. Auto-generated by default. Time to wait before considering a close complete. Whether the default broker URL should be in memory. Ignored if an explicit broker has been specified. Whether to stop message delivery before re-delivering messages from a rolled back transaction. This implies that message order is not preserved when this is enabled. Whether to trust all packages. Comma-separated list of specific packages to trust (when not trusting all packages). Whether to block when a connection is requested and the pool is full. Set it to false to throw a \"JMSException\" instead. Blocking period before throwing an exception if the pool is still full. Whether a JmsPoolConnectionFactory should be created, instead of a regular ConnectionFactory. Maximum number of pooled sessions per connection in the pool. Time to sleep between runs of the idle connection eviction thread. When negative, no idle connection eviction thread runs. Whether to use only one anonymous \"MessageProducer\" instance. Set it to false to create one \"MessageProducer\" every time one is required. Time to wait on message sends for a response. Set it to 0 to wait forever. Journal file directory. Not necessary if persistence is turned off. Whether to enable embedded mode if the Artemis server APIs are available. Comma-separated list of queues to create on startup. Server ID. By default, an auto-incremented counter is used. Comma-separated list of topics to create on startup. Whether to block when a connection is requested and the pool is full. Set it to false to throw a \"JMSException\" instead. Blocking period before throwing an exception if the pool is still full. Whether a JmsPoolConnectionFactory should be created, instead of a regular ConnectionFactory. Maximum number of pooled sessions per connection in the pool. Time to sleep between runs of the idle connection eviction thread. When negative, no idle connection eviction thread runs. Whether to use only one anonymous \"MessageProducer\" instance. Set it to false to create one \"MessageProducer\" every time one is required. Execute all Spring Batch jobs in the context on startup. Comma-separated list of job names to execute on startup (for instance, `job1,job2`). By default, all Jobs found in the context are executed. Path to the SQL file to use to initialize the database schema. Table prefix for all the batch meta-data tables. The location of the configuration file to use to initialize Hazelcast. Path to the SQL file to use to initialize the database schema. Size of the session cache (per JMS Session type). Connection factory JNDI name. When set, takes precedence to others connection factory auto-configurations. Acknowledge mode of the container. By default, the listener is transacted with automatic acknowledgment. Timeout to use for receive calls. Use -1 for a no-wait receive or 0 for no timeout at all. The latter is only feasible if not running within a transaction manager and is generally discouraged since it prevents clean shutdown. Whether the default destination type is topic. Default destination to use on send and receive operations that do not have a destination parameter. Delivery delay to use for send calls. Priority of a message when sending. Enables QoS (Quality of Service) when set. Whether to enable explicit QoS (Quality of Service) when sending a message. When enabled, the delivery mode, priority and time-to-live properties will be used when sending a message. QoS is automatically enabled when at least one of those settings is customized. Timeout to use for receive calls. Time-to-live of a message when sending. Enables QoS (Quality of Service) when set. ID to pass to the server when making requests. Used for server-side logging. Whether to fail fast if the broker is not available on startup. Additional admin-specific properties used to configure the client. Password of the private key in the key store file. Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. Applies to all components unless overridden. ID to pass to the server when making requests. Used for server-side logging. Frequency with which the consumer offsets are auto-committed to Kafka if 'enable.auto.commit' is set to true. What to do when there is no initial offset in Kafka or if the current offset no longer exists on the server. Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. Overrides the global property, for consumers. ID to pass to the server when making requests. Used for server-side logging. Whether the consumer's offset is periodically committed in the background. Maximum amount of time the server blocks before answering the fetch request if there isn't sufficient data to immediately satisfy the requirement given by \"fetch-min-size\". Minimum amount of data the server should return for a fetch request. Unique string that identifies the consumer group to which this consumer belongs. Expected time between heartbeats to the consumer coordinator. Isolation level for reading messages that have been written transactionally. Maximum number of records returned in a single call to poll(). Additional consumer-specific properties used to configure the client. Password of the private key in the key store file. Number of records between offset commits when ackMode is \"COUNT\" or \"COUNT_TIME\". Time between offset commits when ackMode is \"TIME\" or \"COUNT_TIME\". Number of threads to run in the listener containers. Whether to log the container configuration during initialization (INFO level). Whether the container should fail to start if at least one of the configured topics are not present on the broker. Time between checks for non-responsive consumers. If a duration suffix is not specified, seconds will be used. Multiplier applied to \"pollTimeout\" to determine if a consumer is non-responsive. Timeout to use when polling the consumer. Number of acknowledgments the producer requires the leader to have received before considering a request complete. Default batch size. A small batch size will make batching less common and may reduce throughput (a batch size of zero disables batching entirely). Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. Overrides the global property, for producers. Total memory size the producer can use to buffer records waiting to be sent to the server. ID to pass to the server when making requests. Used for server-side logging. Compression type for all data generated by the producer. Additional producer-specific properties used to configure the client. When greater than zero, enables retrying of failed sends. Password of the private key in the key store file. When non empty, enables transaction support for producer. Additional properties, common to producers and consumers, used to configure the client. Password of the private key in the key store file. Whether or not to auto-start the streams factory bean. Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. Overrides the global property, for streams. Maximum memory size to be used for buffering across all threads. ID to pass to the server when making requests. Used for server-side logging. Additional Kafka properties used to configure the streams. The replication factor for change log topics and repartition topics created by the stream processing application. Password of the private key in the key store file. Default topic to which messages are sent. Comma-separated list of addresses to which the client should connect. Duration to wait to obtain a channel if the cache size has been reached. If 0, always create a new channel. Number of channels to retain in the cache. When \"check-timeout\" > 0, max channels per connection. Number of connections to cache. Only applies when mode is CONNECTION. Connection timeout. Set it to zero to wait forever. Whether to create an AmqpAdmin bean. Whether to start the container automatically on startup. Whether rejected deliveries are re-queued by default. How often idle container events should be published. Whether to fail if the queues declared by the container are not available on the broker. Maximum number of unacknowledged messages that can be outstanding at each consumer. Duration between the first and second attempt to deliver a message. Multiplier to apply to the previous retry interval. Whether retries are stateless or stateful. Whether to start the container automatically on startup. Batch size, expressed as the number of physical messages, to be used by the container. Whether rejected deliveries are re-queued by default. How often idle container events should be published. Whether to fail if the queues declared by the container are not available on the broker and/or whether to stop the container if one or more queues are deleted at runtime. Maximum number of unacknowledged messages that can be outstanding at each consumer. Duration between the first and second attempt to deliver a message. Multiplier to apply to the previous retry interval. Whether retries are stateless or stateful. Login to authenticate against the broker. Type of publisher confirms to use. Requested heartbeat timeout; zero for none. If a duration suffix is not specified, seconds will be used. SSL algorithm to use. By default, configured by the Rabbit client library. Path to the key store that holds the SSL certificate. Password used to access the key store. Password used to access the trust store. Name of the default queue to receive messages from when none is specified explicitly. Name of the default exchange to use for send operations. Duration between the first and second attempt to deliver a message. Multiplier to apply to the previous retry interval. Value of a default routing key to use for send operations. Login user to authenticate to the broker. Virtual host to use when connecting to the broker. Path that serves as the base URI for the services. Load on startup priority of the Spring Web Services servlet. Comma-separated list of locations of WSDLs and accompanying XSDs to be exposed as beans. Whether application/hal+json responses should be sent to requests that accept application/json. Preferred JSON mapper to use for HTTP message conversion. By default, auto-detected according to the environment. Charset of HTTP requests and responses. Added to the \"Content-Type\" header if not set explicitly. Whether to force the encoding to the configured charset on HTTP requests and responses. Whether to force the encoding to the configured charset on HTTP requests. Defaults to true when \"force\" has not been specified. Whether to force the encoding to the configured charset on HTTP responses. Locale in which to encode mapping. Whether logging of (potentially sensitive) request details at DEBUG and TRACE level is allowed. Path that serves as the base URI for the application. If specified, overrides the value of \"@ApplicationPath\". Init parameters to pass to Jersey through the servlet or filter. Load on startup priority of the Jersey servlet. Amount of time before asynchronous request handling times out. If this value is not set, the default timeout of the underlying implementation is used. Whether a request parameter (\"format\" by default) should be used to determine the requested media type. Map file extensions to media types for content negotiation. For instance, yml to text/yaml. Query parameter name to use when \"favor-parameter\" is enabled. Date format to use. For instance, `dd/MM/yyyy`. Whether to dispatch OPTIONS requests to the FrameworkServlet doService method. Whether to dispatch TRACE requests to the FrameworkServlet doService method. Whether the content of the \"default\" model should be ignored during redirect scenarios. Locale to use. By default, this locale is overridden by the \"Accept-Language\" header. Define how the locale should be resolved. Whether to enable warn logging of exceptions resolved by a \"HandlerExceptionResolver\", except for \"DefaultHandlerExceptionResolver\". Whether to publish a ServletRequestHandledEvent at the end of each request. Load on startup priority of the dispatcher servlet. Whether a \"NoHandlerFoundException\" should be thrown if no Handler was found to process a request. Indicate that the response message is intended for a single user and must not be stored by a shared cache. Indicate that any cache may store the response. Maximum time the response should be cached, in seconds if no duration suffix is not specified. Indicate that once it has become stale, a cache must not use the response without re-validating it with the server. Indicate that the cached response can be reused only if re-validated with the server. Indicate to not cache the response in any case. Indicate intermediaries (caches and others) that they should not transform the response content. Same meaning as the \"must-revalidate\" directive, except that it does not apply to private caches. Maximum time the response should be cached by shared caches, in seconds if no duration suffix is not specified. Maximum time the response may be used when errors are encountered, in seconds if no duration suffix is not specified. Maximum time the response can be served after it becomes stale, in seconds if no duration suffix is not specified. Cache period for the resources served by the resource handler. If a duration suffix is not specified, seconds will be used. Can be overridden by the 'spring.resources.cache.cachecontrol' properties. Whether to enable caching in the Resource chain. Whether to enable resolution of already compressed resources (gzip, brotli). Checks for a resource name with the '.gz' or '.br' file extensions. Whether to enable the Spring Resource Handling chain. By default, disabled unless at least one strategy has been enabled. Whether to enable the content Version Strategy. Comma-separated list of patterns to apply to the content Version Strategy. Whether to enable the fixed Version Strategy. Comma-separated list of patterns to apply to the fixed Version Strategy. Version string to use for the fixed Version Strategy. Whether to enable support of multipart uploads. Threshold after which files are written to disk. Whether to resolve the multipart request lazily at the time of file or parameter access. Date format to use. For instance, `dd/MM/yyyy`. Whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name. Whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name. Whether to check that the templates location exists. Whether to enable MVC view resolution for this technology. Whether all request attributes should be added to the model prior to merging with the template. Whether all HttpSession attributes should be added to the model prior to merging with the template. Whether to expose a RequestContext for use by Spring's macro library, under the name \"springMacroRequestContext\". Whether to prefer file system access for template loading. File system access enables hot detection of template changes. Prefix that gets prepended to view names when building a URL. Name of the RequestContext attribute for all views. Well-known FreeMarker keys which are passed to FreeMarker's Configuration. Suffix that gets appended to view names when building a URL. View names that can be resolved. Whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name. Whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name. Whether to check that the templates location exists. Whether to enable MVC view resolution for this technology. Whether all request attributes should be added to the model prior to merging with the template. Whether all HttpSession attributes should be added to the model prior to merging with the template. Whether to expose a RequestContext for use by Spring's macro library, under the name \"springMacroRequestContext\". Prefix that gets prepended to view names when building a URL. Name of the RequestContext attribute for all views. Suffix that gets appended to view names when building a URL. View names that can be resolved. Whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name. Whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name. Whether to check that the templates location exists. Whether to enable MVC view resolution for this technology. Whether all request attributes should be added to the model prior to merging with the template. Whether all HttpSession attributes should be added to the model prior to merging with the template. Whether to expose a RequestContext for use by Spring's macro library, under the name \"springMacroRequestContext\". Name of the RequestContext attribute for all views. View names that can be resolved. Whether to check that the template exists before rendering it. Whether to check that the templates location exists. Whether to enable Thymeleaf view resolution for Web frameworks. Comma-separated list of view names (patterns allowed) that should be excluded from resolution. Template mode to be applied to templates. See also Thymeleaf's TemplateMode enum. Prefix that gets prepended to view names when building a URL. Comma-separated list of view names (patterns allowed) that should be the only ones executed in CHUNKED mode when a max chunk size is set. Comma-separated list of view names (patterns allowed) that should be executed in FULL mode even if a max chunk size is set. Maximum size of data buffers used for writing to the response. Templates will execute in CHUNKED mode by default if this is set. Whether hidden form inputs acting as markers for checkboxes should be rendered before the checkbox element itself. Whether Thymeleaf should start writing partial output as soon as possible or buffer until template processing is finished. Suffix that gets appended to view names when building a URL. Order of the template resolver in the chain. By default, the template resolver is first in the chain. Order start at 1 and should only be set if you have defined additional \"TemplateResolver\" beans. Comma-separated list of view names (patterns allowed) that can be resolved. Network address to which the server should bind. Comma-separated list of user agents for which responses should not be compressed. Comma-separated list of MIME types that should be compressed. Minimum \"Content-Length\" value that is required for compression to be performed. Whether to enable the default error page displayed in browsers in case of a server error. Whether to enable HTTP/2 support, if the current environment supports it. Number of acceptor threads to use. When the value is -1, the default, the number of acceptors is derived from the operating environment. Custom log format, see org.eclipse.jetty.server.CustomRequestLog. If defined, overrides the \"format\" configuration key. Date format to place in log file name. Log filename. If not specified, logs redirect to \"System.err\". Request paths that should not be logged. Number of days before rotated log files are deleted. Time that the connection can be idle before it is closed. Maximum size of the form content in any HTTP post request. Number of selector threads to use. When the value is -1, the default, the number of selectors is derived from the operating environment. Value to use for the Server response header (if empty, no header is sent). Display name of the application. Class name of the servlet to use for JSPs. If registered is true and this class * is on the classpath then it will be registered. Init parameters used to configure the JSP servlet. Whether the JSP servlet is registered. Whether to use \"HttpOnly\" cookies for session cookies. Maximum age of the session cookie. If a duration suffix is not specified, seconds will be used. Whether to always mark the session cookie as secure. Whether to persist session data between restarts. Session timeout. If a duration suffix is not specified, seconds will be used. Alias that identifies the key in the key store. Password used to access the key in the key store. Path to the key store that holds the SSL certificate (typically a jks file). Password used to access the key store. Password used to access the trust store. Maximum queue length for incoming connection requests when all possible request processing threads are in use. Whether to buffer output such that it is flushed only periodically. Whether to check for log file existence so it can be recreated it if an external process has renamed it. Whether logging of the request will only be enabled if \"ServletRequest.getAttribute(conditionIf)\" does not yield null. Whether logging of the request will only be enabled if \"ServletRequest.getAttribute(conditionUnless)\" yield null. Directory in which log files are created. Can be absolute or relative to the Tomcat base dir. Character set used by the log file. Default to the system default character set. Date format to place in the log file name. Whether to use IPv6 canonical representation format as defined by RFC 5952. Locale used to format timestamps in log entries and in log file name suffix. Default to the default locale of the Java process. Number of days to retain the access log files before they are removed. Whether to defer inclusion of the date stamp in the file name until rotate time. Set request attributes for the IP address, Hostname, protocol, and port used for the request. Comma-separated list of additional patterns that match jars to ignore for TLD scanning. The special '?' and '*' characters can be used in the pattern to match one and only one character and zero or more characters respectively. Delay between the invocation of backgroundProcess methods. If a duration suffix is not specified, seconds will be used. Tomcat base directory. If not specified, a temporary directory is used. Amount of time the connector will wait, after accepting a connection, for the request URI line to be presented. Name of the HTTP header from which the remote host is extracted. Regular expression that matches proxies that are to be trusted. Maximum number of connections that the server accepts and processes at any given time. Once the limit has been reached, the operating system may still accept connections based on the \"acceptCount\" property. Maximum size of the form content in any HTTP post request. Whether Tomcat's MBean Registry should be enabled. Name of the HTTP header used to override the original port value. Maximum number of idle processors that will be retained in the cache and reused with a subsequent request. When set to -1 the cache will be unlimited with a theoretical maximum size equal to the maximum number of connections. Header that holds the incoming protocol, usually named \"X-Forwarded-Proto\". Value of the protocol header indicating whether the incoming request uses SSL. Whether requests to the context root should be redirected by appending a / to the path. Comma-separated list of additional unencoded characters that should be allowed in URI paths. Only \"< > [ \\ ] ^ ` { | }\" are allowed. Comma-separated list of additional unencoded characters that should be allowed in URI query strings. Only \"< > [ \\ ] ^ ` { | }\" are allowed. Name of the HTTP header from which the remote IP is extracted. For instance, `X-FORWARDED-FOR`. Whether static resource caching is permitted for this web application. Character encoding to use to decode the URI. Whether HTTP 1.1 and later location headers generated by a call to sendRedirect will use relative or absolute redirects. Whether to enable the access log. Whether the server should decode percent encoded slash characters. Enabling encoded slashes can have security implications due to different servers interpreting the slash differently. Only enable this if you have a legacy application that requires it. Whether the 'Connection: keep-alive' header should be added to all responses, even if not required by the HTTP specification. Size of each buffer. The default is derived from the maximum amount of memory that is available to the JVM. Whether the URL should be decoded. When disabled, percent-encoded characters in the URL will be left as-is. Whether to allocate buffers outside the Java heap. The default is derived from the maximum amount of memory that is available to the JVM. Whether servlet filters should be initialized on startup. Number of I/O threads to create for the worker. The default is derived from the number of available processors. Maximum number of cookies that are allowed. This limit exists to prevent hash collision based DOS attacks. Maximum number of headers that are allowed. This limit exists to prevent hash collision based DOS attacks. Maximum size of the HTTP post content. When the value is -1, the default, the size is unlimited. Maximum number of query or path parameters that are allowed. This limit exists to prevent hash collision based DOS attacks. Amount of time a connection can sit idle without processing a request, before it is closed by the server. Number of worker threads. The default is 8 times the number of I/O threads. Whether read-only operations should use an anonymous environment. Disabled by default unless a username is set. Base suffix from which all operations should originate. URI that can either be an OpenID Connect discovery endpoint or an OAuth 2.0 Authorization Server Metadata endpoint defined by RFC 8414. JSON Web Key URI to use to verify the JWT token. JSON Web Algorithm used for verifying the digital signatures. Location of the file containing the public key used to verify a JWT. Client id used to authenticate with the token introspection endpoint. Client secret used to authenticate with the token introspection endpoint. OAuth 2.0 endpoint through which token introspection is accomplished. Password for the default user name. Granted roles for the default user name. Sessions flush mode. Determines when session changes are written to the session store. Name of the map used to store sessions. Sessions save mode. Determines how session changes are tracked and saved to the session store. Sessions flush mode. Determines when session changes are written to the session store. Sessions save mode. Determines how session changes are tracked and saved to the session store. Path to the SQL file to use to initialize the database schema. Name of the database table used to store sessions. Collection name used to store sessions. The configure action to apply when no user defined ConfigureRedisAction bean is present. Sessions flush mode. Determines when session changes are written to the session store. Namespace for keys used to store sessions. Sessions save mode. Determines how session changes are tracked and saved to the session store. Session timeout. If a duration suffix is not specified, seconds will be used. Network address to which the server should bind. Path under which RSocket handles requests (only works with websocket transport). Whether to enable storage of audit events. Whether to skip SSL verification for Cloud Foundry actuator endpoint security calls. Maximum time that a response can be cached. Whether to enable the auditevents endpoint. Maximum time that a response can be cached. Whether to enable the beans endpoint. Maximum time that a response can be cached. Whether to enable the caches endpoint. Maximum time that a response can be cached. Whether to enable the conditions endpoint. Maximum time that a response can be cached. Whether to enable the configprops endpoint. Keys that should be sanitized. Keys can be simple strings that the property ends with or regular expressions. Maximum time that a response can be cached. Whether to enable the env endpoint. Keys that should be sanitized. Keys can be simple strings that the property ends with or regular expressions. Maximum time that a response can be cached. Whether to enable the flyway endpoint. Maximum time that a response can be cached. Whether to enable the health endpoint. Roles used to determine whether or not a user is authorized to be shown details. When empty, all authenticated users are authorized. When to show components. If not specified the 'show-details' setting will be used. Mapping of health statuses to HTTP status codes. By default, registered health statuses map to sensible defaults (for example, UP maps to 200). Comma-separated list of health statuses in order of severity. Maximum time that a response can be cached. Whether to enable the heapdump endpoint. Maximum time that a response can be cached. Whether to enable the httptrace endpoint. Maximum time that a response can be cached. Whether to enable the info endpoint. Maximum time that a response can be cached. Whether to enable the integrationgraph endpoint. Jolokia settings. Refer to the documentation of Jolokia for more details. Whether to enable the jolokia endpoint. Maximum time that a response can be cached. Whether to enable the liquibase endpoint. Maximum time that a response can be cached. Whether to enable the logfile endpoint. External Logfile to be accessed. Can be used if the logfile is written by output redirect and not by the logging system itself. Maximum time that a response can be cached. Whether to enable the loggers endpoint. Maximum time that a response can be cached. Whether to enable the mappings endpoint. Maximum time that a response can be cached. Whether to enable the metrics endpoint. Maximum time that a response can be cached. Whether to enable the prometheus endpoint. Maximum time that a response can be cached. Whether to enable the scheduledtasks endpoint. Whether to enable the sessions endpoint. Whether to enable the shutdown endpoint. Maximum time that a response can be cached. Whether to enable the threaddump endpoint. Whether to enable or disable all endpoints by default. Endpoints JMX domain name. Fallback to 'spring.jmx.default-domain' if set. Endpoint IDs that should be excluded or '*' for all. Endpoint IDs that should be included or '*' for all. Additional static properties to append to all ObjectNames of MBeans representing Endpoints. Base path for Web endpoints. Relative to server.servlet.context-path or management.server.servlet.context-path if management.server.port is configured. Whether credentials are supported. When not set, credentials are not supported. Comma-separated list of headers to allow in a request. '*' allows all headers. Comma-separated list of methods to allow. '*' allows all methods. When not set, defaults to GET. Comma-separated list of origins to allow. '*' allows all origins. When not set, CORS support is disabled. Comma-separated list of headers to include in a response. How long the response from a pre-flight request can be cached by clients. If a duration suffix is not specified, seconds will be used. Endpoint IDs that should be excluded or '*' for all. Endpoint IDs that should be included or '*' for all. Mapping between endpoint IDs and the path that should expose them. Path used to compute the available disk space. Minimum disk space that should be available. Mode to use to expose git information. Maximum value that meter IDs starting-with the specified name are expected to observe. The longest match wins. Values can be specified as a long or as a Duration value (for timer meters, defaulting to ms if no unit specified). Minimum value that meter IDs starting-with the specified name are expected to observe. The longest match wins. Values can be specified as a long or as a Duration value (for timer meters, defaulting to ms if no unit specified). Whether meter IDs starting with the specified name should publish percentile histograms. For monitoring systems that support aggregable percentile calculation based on a histogram, this can be set to true. For other systems, this has no effect. The longest match wins, the key `all` can also be used to configure all meters. Specific computed non-aggregable percentiles to ship to the backend for meter IDs starting-with the specified name. The longest match wins, the key `all` can also be used to configure all meters. Specific SLA boundaries for meter IDs starting-with the specified name. The longest match wins. Counters will be published for each specified boundary. Values can be specified as a long or as a Duration value (for timer meters, defaulting to ms if no unit specified). Whether meter IDs starting-with the specified name should be enabled. The longest match wins, the key `all` can also be used to configure all meters. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. Tag that will be mapped to \"@host\" when shipping metrics to AppOptics. Read timeout for requests to this backend. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Frequency for refreshing config settings from the LWC service. Time to live for subscriptions from the LWC service. URI for the Atlas LWC endpoint to retrieve current subscriptions. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. URI for the Atlas LWC endpoint to evaluate the data for a subscription. Whether to enable streaming to Atlas LWC. Time to live for meters that do not have any activity. After this period the meter will be considered expired and will not get reported. Number of threads to use with the metrics publishing scheduler. Read timeout for requests to this backend. Datadog application key. Not strictly required, but improves the Datadog experience by sending meter descriptions, types, and base units to Datadog. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether to publish descriptions metadata to Datadog. Turn this off to minimize the amount of metadata sent. Whether exporting of metrics to this backend is enabled. Tag that will be mapped to \"host\" when shipping metrics to Datadog. Read timeout for requests to this backend. URI to ship metrics to. If you need to publish metrics to an internal proxy en-route to Datadog, you can define the location of the proxy with this. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. ID of the custom device that is exporting metrics to Dynatrace. Whether exporting of metrics to this backend is enabled. Group for exported metrics. Used to specify custom device group name in the Dynatrace UI. Read timeout for requests to this backend. Technology type for exported metrics. Used to group metrics under a logical technology name in the Dynatrace UI. URI to ship metrics to. Should be used for SaaS, self managed instances or to en-route through an internal proxy. Whether to create the index automatically if it does not exist. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. Index date format used for rolling indices. Appended to the index name, preceded by a '-'. Read timeout for requests to this backend. Name of the timestamp field. Whether exporting of metrics to Ganglia is enabled. Host of the Ganglia server to receive exported metrics. Port of the Ganglia server to receive exported metrics. Ganglia protocol version. Must be either 3.1 or 3.0. Time to live for metrics on Ganglia. Set the multi-cast Time-To-Live to be one greater than the number of hops (routers) between the hosts. Whether exporting of metrics to Graphite is enabled. Host of the Graphite server to receive exported metrics. Port of the Graphite server to receive exported metrics. Protocol to use while shipping data to Graphite. For the default naming convention, turn the specified tag keys into part of the metric prefix. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. Read timeout for requests to this backend. Humio tags describing the data source in which metrics will be stored. Humio tags are a distinct concept from Micrometer's tags. Micrometer's tags are used to divide metrics along dimensional boundaries. URI to ship metrics to. If you need to publish metrics to an internal proxy en-route to Humio, you can define the location of the proxy with this. Whether to create the Influx database if it does not exist before attempting to publish metrics to it. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Whether to enable GZIP compression of metrics batches published to Influx. Connection timeout for requests to this backend. Tag that will be mapped to \"host\" when shipping metrics to Influx. Whether exporting of metrics to this backend is enabled. Read timeout for requests to this backend. Time period for which Influx should retain data in the current database. For instance 7d, check the influx documentation for more details on the duration format. Retention policy to use (Influx writes to the DEFAULT retention policy if one is not specified). How many copies of the data are stored in the cluster. Must be 1 for a single node instance. Time range covered by a shard group. For instance 2w, check the influx documentation for more details on the duration format. Whether exporting of metrics to JMX is enabled. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. Read timeout for requests to this backend. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. The event type that should be published. This property will be ignored if 'meter-name-event-type-enabled' is set to 'true'. Whether to send the meter name as the event type instead of using the 'event-type' configuration property value. Can be set to 'true' if New Relic guidelines are not being followed or event types consistent with previous Spring Boot releases are required. Read timeout for requests to this backend. Whether to enable publishing descriptions as part of the scrape payload to Prometheus. Turn this off to minimize the amount of data sent on each scrape. Whether exporting of metrics to Prometheus is enabled. Frequency with which to push metrics. Operation that should be performed on shutdown. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. Read timeout for requests to this backend. Uniquely identifies the app instance that is publishing metrics to SignalFx. Defaults to the local host name. Whether, in the absence of any other exporter, exporting of metrics to an in-memory backend is enabled. Whether exporting of metrics to StatsD is enabled. Host of the StatsD server to receive exported metrics. Total length of a single payload should be kept within your network's MTU. How often gauges will be polled. When a gauge is polled, its value is recalculated and if the value has changed (or publishUnchangedMeters is true), it is sent to the StatsD server. Port of the StatsD server to receive exported metrics. Whether to send unchanged meters to the StatsD server. API token used when publishing metrics directly to the Wavefront API host. Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made. Connection timeout for requests to this backend. Whether exporting of metrics to this backend is enabled. Global prefix to separate metrics originating from this app's white box instrumentation from those originating from other Wavefront integrations when viewed in the Wavefront UI. Read timeout for requests to this backend. Unique identifier for the app instance that is the source of metrics being published to Wavefront. Defaults to the local host name. Common tags that are applied to every meter. Whether auto-configured MeterRegistry implementations should be bound to the global static registry on Metrics. For testing, set this to 'false' to maximize test independence. Maximum number of unique URI tag values allowed. After the max number of tag values is reached, metrics with additional tag values are denied by filter. Whether percentile histograms should be published. Name of the metric for sent requests. Maximum number of unique URI tag values allowed. After the max number of tag values is reached, metrics with additional tag values are denied by filter. Whether percentile histograms should be published. Whether the trailing slash should be ignored when recording metrics. Name of the metric for received requests. Add the \"X-Application-Context\" HTTP header in each response. Network address to which the management endpoints should bind. Requires a custom management.server.port. Management endpoint HTTP port (uses the same port as the application by default). Configure a different port to use management-specific SSL. Alias that identifies the key in the key store. Password used to access the key in the key store. Path to the key store that holds the SSL certificate (typically a jks file). Password used to access the key store. Password used to access the trust store. Items to be included in the trace. Defaults to request headers (excluding Authorization but including Cookie), response headers (including Set-Cookie), and time taken. Context path used to handle the remote connection. The host of the proxy to use to connect to the remote application. The port of the proxy to use to connect to the remote application. HTTP header used to transfer the shared secret. Additional patterns that should be excluded from triggering a full restart. Additional paths to watch for changes. Patterns that should be excluded from triggering a full restart. Whether to log the condition evaluation delta upon restart. Amount of time to wait between polling for classpath changes. Amount of quiet time required without any classpath changes before a restart is triggered. Name of a specific file that, when changed, triggers the restart check. Must be a simple name (without any path) of a file that appears on your classpath. If not specified, any classpath file change triggers the restart. Spring Boot jars include metadata files that provide details of all supported configuration properties. The files are designed to let IDE developers offer contextual help and “code completion” as users are working with or files. The majority of the metadata file is generated automatically at compile time by processing all items annotated with . However, it is possible to write part of the metadata manually for corner cases or more advanced use cases. Configuration metadata files are located inside jars under . They use a JSON format with items categorized under either “groups” or “properties” and additional values hints categorized under \"hints\", as shown in the following example: {\"groups\": [ { \"name\": \"server\", \"type\": \"org.springframework.boot.autoconfigure.web.ServerProperties\", \"sourceType\": \"org.springframework.boot.autoconfigure.web.ServerProperties\" }, { \"name\": \"spring.jpa.hibernate\", \"type\": \"org.springframework.boot.autoconfigure.orm.jpa.JpaProperties$Hibernate\", \"sourceType\": \"org.springframework.boot.autoconfigure.orm.jpa.JpaProperties\", \"sourceMethod\": \"getHibernate()\" } ... ],\"properties\": [ { \"name\": \"server.port\", \"type\": \"java.lang.Integer\", \"sourceType\": \"org.springframework.boot.autoconfigure.web.ServerProperties\" }, { \"name\": \"server.address\", \"type\": \"java.net.InetAddress\", \"sourceType\": \"org.springframework.boot.autoconfigure.web.ServerProperties\" }, { \"name\": \"spring.jpa.hibernate.ddl-auto\", \"type\": \"java.lang.String\", \"description\": \"DDL mode. This is actually a shortcut for the \\\"hibernate.hbm2ddl.auto\\\" property.\", \"sourceType\": \"org.springframework.boot.autoconfigure.orm.jpa.JpaProperties$Hibernate\" } ... ],\"hints\": [ { \"name\": \"spring.jpa.hibernate.ddl-auto\", \"values\": [ { \"value\": \"none\", \"description\": \"Disable DDL handling.\" }, { \"value\": \"validate\", \"description\": \"Validate the schema, make no changes to the database.\" }, { \"value\": \"update\", \"description\": \"Update the schema if necessary.\" }, { \"value\": \"create\", \"description\": \"Create the schema and destroy previous data.\" }, { \"value\": \"create-drop\", \"description\": \"Create and then destroy the schema at the end of the session.\" } ] } ]} Each “property” is a configuration item that the user specifies with a given value. For example, and might be specified in , as follows: The “groups” are higher level items that do not themselves specify a value but instead provide a contextual grouping for properties. For example, the and properties are part of the group. It is not required that every “property” has a “group”. Some properties might exist in their own right. Finally, “hints” are additional information used to assist the user in configuring a given property. For example, when a developer is configuring the property, a tool can use the hints to offer some auto-completion help for the , , , , and values. The JSON object contained in the array can contain the attributes shown in the following table: The full name of the group. This attribute is mandatory. The class name of the data type of the group. For example, if the group were based on a class annotated with , the attribute would contain the fully qualified name of that class. If it were based on a method, it would be the return type of that method. If the type is not known, the attribute may be omitted. A short description of the group that can be displayed to users. If no description is available, it may be omitted. It is recommended that descriptions be short paragraphs, with the first line providing a concise summary. The last line in the description should end with a period ( ). The class name of the source that contributed this group. For example, if the group were based on a method annotated with , this attribute would contain the fully qualified name of the class that contains the method. If the source type is not known, the attribute may be omitted. The full name of the method (include parenthesis and argument types) that contributed this group (for example, the name of a annotated method). If the source method is not known, it may be omitted. The JSON object contained in the array can contain the attributes described in the following table: The full name of the property. Names are in lower-case period-separated form (for example, ). This attribute is mandatory. The full signature of the data type of the property (for example, ) but also a full generic type (such as ). You can use this attribute to guide the user as to the types of values that they can enter. For consistency, the type of a primitive is specified by using its wrapper counterpart (for example, becomes ). Note that this class may be a complex type that gets converted from a as values are bound. If the type is not known, it may be omitted. A short description of the property that can be displayed to users. If no description is available, it may be omitted. It is recommended that descriptions be short paragraphs, with the first line providing a concise summary. The last line in the description should end with a period ( ). The class name of the source that contributed this property. For example, if the property were from a class annotated with , this attribute would contain the fully qualified name of that class. If the source type is unknown, it may be omitted. The default value, which is used if the property is not specified. If the type of the property is an array, it can be an array of value(s). If the default value is unknown, it may be omitted. Specify whether the property is deprecated. If the field is not deprecated or if that information is not known, it may be omitted. The next table offers more detail about the attribute. The JSON object contained in the attribute of each element can contain the following attributes: The level of deprecation, which can be either (the default) or . When a property has a deprecation level, it should still be bound in the environment. However, when it has an deprecation level, the property is no longer managed and is not bound. A short description of the reason why the property was deprecated. If no reason is available, it may be omitted. It is recommended that descriptions be short paragraphs, with the first line providing a concise summary. The last line in the description should end with a period ( ). The full name of the property that replaces this deprecated property. If there is no replacement for this property, it may be omitted. Prior to Spring Boot 1.3, a single boolean attribute can be used instead of the element. This is still supported in a deprecated fashion and should no longer be used. If no reason and replacement are available, an empty object should be set. Deprecation can also be specified declaratively in code by adding the annotation to the getter exposing the deprecated property. For instance, assume that the property was confusing and was renamed to . The following example shows how to handle that situation: There is no way to set a . is always assumed, since code is still handling the property. The preceding code makes sure that the deprecated property still works (delegating to the property behind the scenes). Once the and methods can be removed from your public API, the automatic deprecation hint in the metadata goes away as well. If you want to keep a hint, adding manual metadata with an deprecation level ensures that users are still informed about that property. Doing so is particularly useful when a is provided. The JSON object contained in the array can contain the attributes shown in the following table: The full name of the property to which this hint refers. Names are in lower-case period-separated form (such as ). If the property refers to a map (such as ), the hint either applies to the keys of the map ( ) or the values ( ) of the map. This attribute is mandatory. A list of valid values as defined by the object (described in the next table). Each entry defines the value and may have a description. A list of providers as defined by the object (described later in this document). Each entry defines the name of the provider and its parameters, if any. The JSON object contained in the attribute of each element can contain the attributes described in the following table: A valid value for the element to which the hint refers. If the type of the property is an array, it can also be an array of value(s). This attribute is mandatory. A short description of the value that can be displayed to users. If no description is available, it may be omitted. It is recommended that descriptions be short paragraphs, with the first line providing a concise summary. The last line in the description should end with a period ( ). The JSON object contained in the attribute of each element can contain the attributes described in the following table: The name of the provider to use to offer additional content assistance for the element to which the hint refers. Any additional parameter that the provider supports (check the documentation of the provider for more details). Objects with the same “property” and “group” name can appear multiple times within a metadata file. For example, you could bind two separate classes to the same prefix, with each having potentially overlapping property names. While the same names appearing in the metadata multiple times should not be common, consumers of metadata should take care to ensure that they support it. To improve the user experience and further assist the user in configuring a given property, you can provide additional metadata that:\n• Describes the list of potential values for a property.\n• Associates a provider, to attach a well defined semantic to a property, so that a tool can discover the list of potential values based on the project’s context. The attribute of each hint refers to the of a property. In the initial example shown earlier, we provide five values for the property: , , , , and . Each value may have a description as well. If your property is of type , you can provide hints for both the keys and the values (but not for the map itself). The special and suffixes must refer to the keys and the values, respectively. Assume a maps magic values to an integer, as shown in the following example: The magic values are (in this example) are and . In order to offer additional content assistance for the keys, you could add the following JSON to the manual metadata of the module: {\"hints\": [ { \"name\": \"sample.contexts.keys\", \"values\": [ { \"value\": \"sample1\" }, { \"value\": \"sample2\" } ] } ]} We recommend that you use an for those two values instead. If your IDE supports it, this is by far the most effective approach to auto-completion. Providers are a powerful way to attach semantics to a property. In this section, we define the official providers that you can use for your own hints. However, your favorite IDE may implement some of these or none of them. Also, it could eventually provide its own. As this is a new feature, IDE vendors must catch up with how it works. Adoption times naturally vary. The following table summarizes the list of supported providers: Permits any additional value to be provided. Auto-completes the classes available in the project. Usually constrained by a base class that is specified by the parameter. Handles the property as if it were defined by the type defined by the mandatory parameter. Auto-completes valid logger names and logger groups. Typically, package and class names available in the current project can be auto-completed as well as defined groups. Auto-completes the available bean names in the current project. Usually constrained by a base class that is specified by the parameter. Auto-completes the available Spring profile names in the project. Only one provider can be active for a given property, but you can specify several providers if they can all manage the property in some way. Make sure to place the most powerful provider first, as the IDE must use the first one in the JSON section that it can handle. If no provider for a given property is supported, no special content assistance is provided, either. The special any provider value permits any additional values to be provided. Regular value validation based on the property type should be applied if this is supported. This provider is typically used if you have a list of values and any extra values should still be considered as valid. The following example offers and as auto-completion values for : {\"hints\": [ { \"name\": \"system.state\", \"values\": [ { \"value\": \"on\" }, { \"value\": \"off\" } ], \"providers\": [ { \"name\": \"any\" } ] } ]} Note that, in the preceding example, any other value is also allowed. The class-reference provider auto-completes classes available in the project. This provider supports the following parameters: The fully qualified name of the class that should be assignable to the chosen value. Typically used to filter out-non candidate classes. Note that this information can be provided by the type itself by exposing a class with the appropriate upper bound. Specify whether only concrete classes are to be considered as valid candidates. The following metadata snippet corresponds to the standard property that defines the class name to use: The handle-as provider lets you substitute the type of the property to a more high-level type. This typically happens when the property has a type, because you do not want your configuration classes to rely on classes that may not be on the classpath. This provider supports the following parameters: The fully qualified name of the type to consider for the property. This parameter is mandatory. The following types can be used:\n• Any : Lists the possible values for the property. (We recommend defining the property with the type, as no further hint should be required for the IDE to auto-complete the values)\n• : Supports auto-completion of charset/encoding values (such as )\n• : auto-completion of locales (such as )\n• : Supports auto-completion of content type values (such as )\n• : Supports auto-completion of Spring’s Resource abstraction to refer to a file on the filesystem or on the classpath (such as ) If multiple values can be provided, use a or Array type to teach the IDE about it. The following metadata snippet corresponds to the standard property that defines the path to the changelog to use. It is actually used internally as a but cannot be exposed as such, because we need to keep the original String value to pass it to the Liquibase API. The logger-name provider auto-completes valid logger names and logger groups. Typically, package and class names available in the current project can be auto-completed. If groups are enabled (default) and if a custom logger group is identified in the configuration, auto-completion for it should be provided. Specific frameworks may have extra magic logger names that can be supported as well. This provider supports the following parameters: Specify whether known groups should be considered. Since a logger name can be any arbitrary name, this provider should allow any value but could highlight valid package and class names that are not available in the project’s classpath. The following metadata snippet corresponds to the standard property. Keys are logger names, and values correspond to the standard log levels or any custom level. As Spring Boot defines a few logger groups out-of-the-box, dedicated value hints have been added for those. {\"hints\": [ { \"name\": \"logging.level.keys\", \"values\": [ { \"value\": \"root\", \"description\": \"Root logger used to assign the default logging level.\" }, { \"value\": \"sql\", \"description\": \"SQL logging group including Hibernate SQL logger.\" }, { \"value\": \"web\", \"description\": \"Web logging group including codecs.\" } ], \"providers\": [ { \"name\": \"logger-name\" } ] }, { \"name\": \"logging.level.values\", \"values\": [ { \"value\": \"trace\" }, { \"value\": \"debug\" }, { \"value\": \"info\" }, { \"value\": \"warn\" }, { \"value\": \"error\" }, { \"value\": \"fatal\" }, { \"value\": \"off\" } ], \"providers\": [ { \"name\": \"any\" } ] } ]} The spring-bean-reference provider auto-completes the beans that are defined in the configuration of the current project. This provider supports the following parameters: The fully qualified name of the bean class that should be assignable to the candidate. Typically used to filter out non-candidate beans. The following metadata snippet corresponds to the standard property that defines the name of the bean to use: The binder is not aware of the metadata. If you provide that hint, you still need to transform the bean name into an actual Bean reference using by the . The spring-profile-name provider auto-completes the Spring profiles that are defined in the configuration of the current project. The following metadata snippet corresponds to the standard property that defines the name of the Spring profile(s) to enable: 10.B.3. Generating Your Own Metadata by Using the Annotation Processor You can easily generate your own configuration metadata file from items annotated with by using the jar. The jar includes a Java annotation processor which is invoked as your project is compiled. To use the processor, include a dependency on . With Maven the dependency should be declared as optional, as shown in the following example: If you have defined in your application, make sure to configure the to prevent the goal from adding the dependency into the fat jar: With Gradle 4.5 and earlier, the dependency should be declared in the configuration, as shown in the following example: With Gradle 4.6 and later, the dependency should be declared in the configuration, as shown in the following example: If you are using an file, the task should be configured to depend on the task, as shown in the following example: This dependency ensures that the additional metadata is available when the annotation processor runs during compilation. If you are using AspectJ in your project, you need to make sure that the annotation processor runs only once. There are several ways to do this. With Maven, you can configure the explicitly and add the dependency to the annotation processor only there. You could also let the AspectJ plugin run all the processing and disable annotation processing in the configuration, as follows: The processor picks up both classes and methods that are annotated with . If the class is also annotated with , a single constructor is expected and one property is created per constructor parameter. Otherwise, properties are discovered through the presence of standard getters and setters with special handling for collection and map types (that is detected even if only a getter is present). The annotation processor also supports the use of the , , and lombok annotations. Consider the following example: @ConfigurationProperties(prefix=\"server\") public class ServerProperties { /** * Name of the server. */ private String name; /** * IP address to listen to. */ private String ip = \"127.0.0.1\"; /** * Port to listener to. */ private int port = 9797; // ... getter and setters } This exposes three properties where has no default and and defaults to and respectively. The Javadoc on fields is used to populate the attribute. For instance, the description of is \"IP address to listen to.\". You should only use plain text with field Javadoc, since they are not processed before being added to the JSON. The annotation processor applies a number of heuristics to extract the default value from the source model. Default values have to be provided statically. In particular, do not refer to a constant defined in another class. Also, the annotation processor cannot auto-detect default values for s and s. For cases where the default value could not be detected, manual metadata should be provided. Consider the following example: In order to document default values for properties in the class above, you could add the following content to the manual metadata of the module: Only the of the property is required to document additional metadata for existing properties. The annotation processor automatically considers inner classes as nested properties. Rather than documenting the and at the root of the namespace, we could create a sub-namespace for it. Consider the updated example: @ConfigurationProperties(prefix=\"server\") public class ServerProperties { private String name; private Host host; // ... getter and setters public static class Host { private String ip; private int port; // ... getter and setters } } The preceding example produces metadata information for , , and properties. You can use the annotation on a field to indicate that a regular (non-inner) class should be treated as if it were nested. This has no effect on collections and maps, as those types are automatically identified, and a single metadata property is generated for each of them. Spring Boot’s configuration file handling is quite flexible, and it is often the case that properties may exist that are not bound to a bean. You may also need to tune some attributes of an existing key. To support such cases and let you provide custom \"hints\", the annotation processor automatically merges items from into the main metadata file. If you refer to a property that has been detected automatically, the description, default value, and deprecation information are overridden, if specified. If the manual property declaration is not identified in the current module, it is added as a new property. The format of the file is exactly the same as the regular . The additional properties file is optional. If you do not have any additional properties, do not add the file. This appendix contains details of all of the auto-configuration classes provided by Spring Boot, with links to documentation and source code. Remember to also look at the conditions report in your application for more details of which features are switched on. (To do so, start the app with or or, in an Actuator application, use the endpoint). The following auto-configuration classes are from the module: The following auto-configuration classes are from the module: This appendix describes the auto-configuration annotations that Spring Boot provides to test slices of your application. The following table lists the various annotations that can be used to test slices of your application and the auto-configuration that they import by default: The modules lets Spring Boot support executable jar and war files. If you use the Maven plugin or the Gradle plugin, executable jars are automatically generated, and you generally do not need to know the details of how they work. If you need to create executable jars from a different build system or if you are just curious about the underlying technology, this appendix provides some background. Java does not provide any standard way to load nested jar files (that is, jar files that are themselves contained within a jar). This can be problematic if you need to distribute a self-contained application that can be run from the command line without unpacking. To solve this problem, many developers use “shaded” jars. A shaded jar packages all classes, from all jars, into a single “uber jar”. The problem with shaded jars is that it becomes hard to see which libraries are actually in your application. It can also be problematic if the same filename is used (but with different content) in multiple jars. Spring Boot takes a different approach and lets you actually nest jars directly. Spring Boot Loader-compatible jar files should be structured in the following way: Application classes should be placed in a nested directory. Dependencies should be placed in a nested directory. Spring Boot Loader-compatible war files should be structured in the following way: Dependencies should be placed in a nested directory. Any dependencies that are required when running embedded but are not required when deploying to a traditional web container should be placed in . The core class used to support loading nested jars is . It lets you load jar content from a standard jar file or from nested child jar data. When first loaded, the location of each is mapped to a physical file offset of the outer jar, as shown in the following example: The preceding example shows how can be found in in at position . from the nested jar can actually be found in at position , and is at position . Armed with this information, we can load specific nested entries by seeking to the appropriate part of the outer jar. We do not need to unpack the archive, and we do not need to read all entry data into memory. Spring Boot Loader strives to remain compatible with existing code and libraries. extends from and should work as a drop-in replacement. The method returns a that opens a connection compatible with and can be used with Java’s . The class is a special bootstrap class that is used as an executable jar’s main entry point. It is the actual in your jar file, and it is used to setup an appropriate and ultimately call your method. There are three launcher subclasses ( , , and ). Their purpose is to load resources ( files and so on) from nested jar files or war files in directories (as opposed to those explicitly on the classpath). In the case of and , the nested paths are fixed. looks in , and looks in and . You can add extra jars in those locations if you want more. The looks in in your application archive by default. You can add additional locations by setting an environment variable called or in (which is a comma-separated list of directories, archives, or directories within archives). You need to specify an appropriate as the attribute of . The actual class that you want to launch (that is, the class that contains a method) should be specified in the attribute. The following example shows a typical for an executable jar file: For a war file, it would be as follows: You need not specify entries in your manifest file. The classpath is deduced from the nested jars. has a few special features that can be enabled with external properties (System properties, environment variables, manifest entries, or ). The following table describes these properties: Comma-separated Classpath, such as . Earlier entries take precedence, like a regular on the command line. Used to resolve relative paths in . For example, given , then is a classpath location (along with all jar files in that directory). This property is also used to locate a file, as in the following example It defaults to . Name of main class to launch (for example, ). Name of properties file (for example, ). It defaults to . Path to properties file (for example, ). It defaults to . Boolean flag to indicate that all properties should be added to System properties. It defaults to . When specified as environment variables or manifest entries, the following names should be used: Build plugins automatically move the attribute to when the fat jar is built. If you use that, specify the name of the class to launch by using the attribute and leaving out . The following rules apply to working with :\n• is searched for in , then in the root of the classpath, and then in . The first location where a file with that name exists is used.\n• is the directory location of an additional properties file (overriding the default) only when is not specified.\n• can contain directories (which are scanned recursively for jar and zip files), archive paths, a directory within an archive that is scanned for jar files (for example, ), or wildcard patterns (for the default JVM behavior). Archive paths can be relative to or anywhere in the file system with a prefix.\n• (if empty) defaults to (meaning a local directory or a nested one if running from an archive). Because of this, behaves the same as when no additional configuration is provided.\n• can not be used to configure the location of (the classpath used to search for the latter is the JVM classpath when is launched).\n• Placeholder replacement is done from System and environment variables plus the properties file itself on all values before use.\n• The search order for properties (where it makes sense to look in more than one place) is environment variables, system properties, , the exploded archive manifest, and the archive manifest. You need to consider the following restrictions when working with a Spring Boot Loader packaged application:\n• Zip entry compression: The for a nested jar must be saved by using the method. This is required so that we can seek directly to individual content within the nested jar. The content of the nested jar file itself can still be compressed, as can any other entries in the outer jar.\n• System classLoader: Launched applications should use when loading classes (most libraries and frameworks do so by default). Trying to load nested jar classes with fails. always uses the system classloader. For this reason, you should consider a different logging implementation. If the preceding restrictions mean that you cannot use Spring Boot Loader, consider the following alternatives: This appendix provides details of the dependencies that are managed by Spring Boot. The following table provides details of all of the dependency versions that are provided by Spring Boot in its CLI (Command Line Interface), Maven dependency management, and Gradle plugin. When you declare a dependency on one of these artifacts without declaring a version, the version listed in the table is used."
    },
    {
        "link": "https://docs.redhat.com/it/documentation/red_hat_fuse/7.0/html-single/release_notes/index",
        "document": ""
    },
    {
        "link": "https://github.com/LogNet/grpc-spring-boot-starter/issues/159",
        "document": "i have setup a sample maven springboot app to test out spring security implementation. no success so far.\n\nhere is the git repo with the app code - https://github.com/swarupdonepudi/grpc-sb-hello-world\n\nI start the service using and the app starts successfully.\n\nbut when i send a request to the service using bloomrpc client, i get the below response back.\n\ncan someone please help me to figure out what i am doing wrong?\n\nhere is the debug log output\n\nas can be seen from the above log, the header is being passed to the service just fine as i can see authorization in inbound metadata\n\nbut the response message says no Auth object in the context"
    },
    {
        "link": "https://documentation.coremedia.com/cmcc-10/artifacts/2101/release-notes-manuals/release-notes-en.pdf",
        "document": ""
    }
]