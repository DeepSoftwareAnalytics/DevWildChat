[
    {
        "link": "https://github.com/bytedeco/javacv/issues/1210",
        "document": "Try to test how to load RTSP video stream my code is like this:\n\n\n\n and got error:\n\n W/System.err: org.bytedeco.javacv.FrameGrabber$Exception: avformat_open_input() error -5: Could not open input \"rtsp://wowzaec2demo.streamlock.net/vod/mp4:BigBuckBunny_115k.mov\". (Has setFormat() been called?)\n\nI tried\n\n \n\n \n\n both of them didn't work\n\n This public server and that is working in my VLC, can anyone help me with this?"
    },
    {
        "link": "https://github.com/bytedeco/javacv/issues/1377",
        "document": "GoPros and some other cameras provide metadata in H.264 streams which are not currently being read out by FFmpegFrameGrabber. This data is generally in some type of binary format, and for those purposes providing an interface to the raw packet / ByteArray would be sufficient."
    },
    {
        "link": "https://stackoverflow.com/questions/31034161/recieving-rtsp-stream-with-javacv-library",
        "document": "I am currently trying to obtain an RTSP stream from an IP camera on my network so that I can apply facial recognition algorithms to the frames (I am using the JavaCV library for this).\n\nWhen I attempt to obtain the RTSP stream, I start getting an error depending upon the method that I tried to use.\n\nHere is the code that should work:\n\nThat particular piece of code gives me this error:\n• None Using OpenCVFrameGrabber instead - error says \"Could not create camera capture\"\n\nWhat am I doing wrong, is this a code issue or a camera issue?"
    },
    {
        "link": "https://groups.google.com/g/javacv/c/5ItbSEUmiYg",
        "document": "Sign in to reply to author\n\nYou do not have permission to delete messages in this group\n\nEither email addresses are anonymous for this group or you need the view member email addresses permission to view the original message"
    },
    {
        "link": "https://ffmpeg.org/ffmpeg-protocols.html",
        "document": "This document describes the input and output protocols provided by the libavformat library.\n\nThe libavformat library provides some generic global options, which can be set on all the protocols. In addition each protocol may support so-called private options, which are specific for that component.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the options or using the API for programmatic use.\n\nThe list of supported options follows:\n\nProtocols are configured elements in FFmpeg that enable access to resources that require specific protocols.\n\nWhen you configure your FFmpeg build, all the supported protocols are enabled by default. You can list all available ones using the configure option \"–list-protocols\".\n\nYou can disable all the protocols using the configure option \"–disable-protocols\", and selectively enable a protocol using the option \"–enable-protocol= \", or you can disable a particular protocol using the option \"–disable-protocol= \".\n\nThe option \"-protocols\" of the ff* tools will display the list of supported protocols.\n\nAll protocols accept the following options:\n\nA description of the currently available protocols follows.\n\nFFmpeg must be compiled with –enable-librabbitmq to support AMQP. A separate AMQP broker must also be run. An example open-source AMQP broker is RabbitMQ.\n\nAfter starting the broker, an FFmpeg client may stream data to the broker using the command:\n\nWhere hostname and port (default is 5672) is the address of the broker. The client may also set a user/password for authentication. The default for both fields is \"guest\". Name of virtual host on broker can be set with vhost. The default value is \"/\".\n\nMuliple subscribers may stream from the broker using the command:\n\nIn RabbitMQ all data published to the broker flows through a specific exchange, and each subscribing client has an assigned queue/buffer. When a packet arrives at an exchange, it may be copied to a client’s queue depending on the exchange and routing_key fields.\n\nThe following options are supported:\n\nFill data in a background thread, to decouple I/O operation from demux thread.\n\nRead angle 2 of playlist 4 from BluRay mounted to /mnt/bluray, start from chapter 2:\n\nCache the input stream to temporary file. It brings seeking capability to live streams.\n\nRead and seek from many resources in sequence as if they were a unique resource.\n\nA URL accepted by this protocol has the syntax:\n\nwhere , , ..., are the urls of the resource to be concatenated, each one possibly specifying a distinct protocol.\n\nFor example to read a sequence of files , , with use the command:\n\nNote that you may need to escape the character \"|\" which is special for many shells.\n\nRead and seek from many resources in sequence as if they were a unique resource.\n\nA URL accepted by this protocol has the syntax:\n\nwhere is the url containing a line break delimited list of resources to be concatenated, each one possibly specifying a distinct protocol. Special characters must be escaped with backslash or single quotes. See (ffmpeg-utils)the \"Quoting and escaping\" section in the ffmpeg-utils(1) manual.\n\nFor example to read a sequence of files , , listed in separate lines within a file with use the command:\n\nWhere contains the lines:\n\nData in-line in the URI. See http://en.wikipedia.org/wiki/Data_URI_scheme.\n\nFor example, to convert a GIF file given inline with :\n\nIf is not specified, by default the stdout file descriptor will be used for writing, stdin for reading. Unlike the pipe protocol, fd protocol has seek support if it corresponding to a regular file. fd protocol doesn’t support pass file descriptor via URL for security.\n\nThis protocol accepts the following options:\n\nRead from or write to a file.\n\nA file URL can have the form:\n\nwhere is the path of the file to read.\n\nAn URL that does not have a protocol prefix will be assumed to be a file URL. Depending on the build, an URL that looks like a Windows path with the drive letter at the beginning will also be assumed to be a file URL (usually not the case in builds for unix-like systems).\n\nFor example to read from a file with use the command:\n\nThis protocol accepts the following options:\n\nRead from or write to remote resources using FTP protocol.\n\nThis protocol accepts the following options.\n\nNOTE: Protocol can be used as output, but it is recommended to not do it, unless special care is taken (tests, customized server configuration etc.). Different FTP servers behave in different way during seek operation. ff* tools may produce incomplete content due to server limitations.\n\nRead Apple HTTP Live Streaming compliant segmented stream as a uniform one. The M3U8 playlists describing the segments can be remote HTTP resources or local files, accessed using the standard file protocol. The nested protocol is declared by specifying \"+ \" after the hls URI scheme name, where is either \"file\" or \"http\".\n\nUsing this protocol is discouraged - the hls demuxer should work just as well (if not, please report the issues) and is more complete. To use the hls demuxer instead, simply use the direct URLs to the m3u8 files.\n\nThis protocol accepts the following options:\n\nControl seekability of connection. If set to 1 the resource is supposed to be seekable, if set to 0 it is assumed not to be seekable, if set to -1 it will try to autodetect if it is seekable. Default value is -1. If set to 1 use chunked Transfer-Encoding for posts, default is 1. Set custom HTTP headers, can override built in default headers. The value must be a string encoding the headers. Set a specific content type for the POST messages or for listen mode. Override the User-Agent header. If not specified the protocol will use a string describing the libavformat build. (\"Lavf/<version>\") Use persistent connections if set to 1, default is 0. Exports the HTTP response version number. Usually \"1.0\" or \"1.1\". Set the cookies to be sent in future requests. The format of each cookie is the same as the value of a Set-Cookie HTTP response field. Multiple cookies can be delimited by a newline character. If set to 1 request ICY (SHOUTcast) metadata from the server. If the server supports this, the metadata has to be retrieved by the application by reading the and options. The default is 1. If the server supports ICY metadata, this contains the ICY-specific HTTP reply headers, separated by newline characters. If the server supports ICY metadata, and was set to 1, this contains the last non-empty metadata packet sent by the server. It should be polled in regular intervals by applications interested in mid-stream metadata updates. Set an exported dictionary containing Icecast metadata from the bitstream, if present. Only useful with the C API. Set HTTP authentication type. No option for Digest, since this method requires getting nonce parameters from the server first and can’t be used straight away like Basic. Choose the HTTP authentication type automatically. This is the default. Basic authentication sends a Base64-encoded string that contains a user name and password for the client. Base64 is not a form of encryption and should be considered the same as sending the user name and password in clear text (Base64 is a reversible encoding). If a resource needs to be protected, strongly consider using an authentication scheme other than basic authentication. HTTPS/TLS should be used with basic authentication. Without these additional security enhancements, basic authentication should not be used to protect sensitive or valuable information. Send an Expect: 100-continue header for POST. If set to 1 it will send, if set to 0 it won’t, if set to -1 it will try to send if it is applicable. Default value is -1. An exported dictionary containing the content location. Only useful with the C API. Try to limit the request to bytes preceding this offset. When used as a client option it sets the HTTP method for the request. When used as a server option it sets the HTTP method that is going to be expected from the client(s). If the expected and the received HTTP method do not match the client will be given a Bad Request response. When unset the HTTP method is not checked for now. This will be replaced by autodetection in the future. Reconnect automatically when disconnected before EOF is hit. If set then eof is treated like an error and causes reconnection, this is useful for live / endless streams. Reconnect automatically in case of TCP/TLS errors during connect. A comma separated list of HTTP status codes to reconnect on. The list can include specific status codes (e.g. ’503’) or the strings ’4xx’ / ’5xx’. If set then even streamed/non seekable streams will be reconnected on errors. Set the maximum delay in seconds after which to give up reconnecting. Set the maximum number of times to retry a connection. Default unset. Set the maximum total delay in seconds after which to give up reconnecting. If enabled, and a Retry-After header is encountered, its requested reconnection delay will be honored, rather than using exponential backoff. Useful for 429 and 503 errors. Default enabled. If set to 1 enables experimental HTTP server. This can be used to send data when used as an output option, or read data from a client with HTTP POST when used as an input option. If set to 2 enables experimental multi-client HTTP server. This is not yet implemented in ffmpeg.c and thus must not be used as a command line option. # Server side (sending): ffmpeg -i somefile.ogg -c copy -listen 1 -f ogg http:// : # Client side (receiving): ffmpeg -i http:// : -c copy somefile.ogg # Client can also be done with wget: wget http:// : -O somefile.ogg # Server side (receiving): ffmpeg -listen 1 -i http:// : -c copy somefile.ogg # Client side (sending): ffmpeg -i somefile.ogg -chunked_post 0 -c copy -f ogg http:// : # Client can also be done with wget: wget --post-file=somefile.ogg http:// : The resource requested by a client, when the experimental HTTP server is in use. The HTTP code returned to the client, when the experimental HTTP server is in use. Set the threshold, in bytes, for when a readahead should be prefered over a seek and new HTTP request. This is useful, for example, to make sure the same connection is used for reading large video packets with small audio packets in between.\n\nSome HTTP requests will be denied unless cookie values are passed in with the request. The option allows these cookies to be specified. At the very least, each cookie must specify a value along with a path and domain. HTTP requests that match both the domain and path will automatically include the cookie value in the HTTP Cookie header field. Multiple cookies can be delimited by a newline.\n\nThe required syntax to play a stream specifying a cookie is:\n\nThis protocol accepts the following options:\n\nInterPlanetary File System (IPFS) protocol support. One can access files stored on the IPFS network through so-called gateways. These are http(s) endpoints. This protocol wraps the IPFS native protocols (ipfs:// and ipns://) to be sent to such a gateway. Users can (and should) host their own node which means this protocol will use one’s local gateway to access files on the IPFS network.\n\nThis protocol accepts the following options:\n\nOne can use this protocol in 2 ways. Using IPFS:\n\nOr the IPNS protocol (IPNS is mutable IPFS):\n\nComputes the MD5 hash of the data to be written, and on close writes this to the designated output or stdout if none is specified. It can be used to test muxers without writing an actual file.\n\nNote that some formats (typically MOV) require the output protocol to be seekable, so they will fail with the MD5 output protocol.\n\nIf isn’t specified, is the number corresponding to the file descriptor of the pipe (e.g. 0 for stdin, 1 for stdout, 2 for stderr). If is not specified, by default the stdout file descriptor will be used for writing, stdin for reading.\n\nFor example to read from stdin with :\n\nFor writing to stdout with :\n\nThis protocol accepts the following options:\n\nNote that some formats (typically MOV), require the output protocol to be seekable, so they will fail with the pipe output protocol.\n\nThe Pro-MPEG CoP#3 FEC is a 2D parity-check forward error correction mechanism for MPEG-2 Transport Streams sent over RTP.\n\nThis protocol must be used in conjunction with the muxer and the protocol.\n\nThe destination UDP ports are for the column FEC stream and for the row FEC stream.\n\nThis protocol accepts the following options:\n\nThe Real-Time Messaging Protocol (RTMP) is used for streaming multimedia content across a TCP/IP network.\n\nAdditionally, the following parameters can be set via command line options (or in code via s):\n\nFor example to read with a multimedia resource named \"sample\" from the application \"vod\" from an RTMP server \"myserver\":\n\nTo publish to a password protected server, passing the playpath and app names separately:\n\nThe Encrypted Real-Time Messaging Protocol (RTMPE) is used for streaming multimedia content within standard cryptographic primitives, consisting of Diffie-Hellman key exchange and HMACSHA256, generating a pair of RC4 keys.\n\nThe Real-Time Messaging Protocol (RTMPS) is used for streaming multimedia content across an encrypted connection.\n\nThe Real-Time Messaging Protocol tunneled through HTTP (RTMPT) is used for streaming multimedia content within HTTP requests to traverse firewalls.\n\nThe Encrypted Real-Time Messaging Protocol tunneled through HTTP (RTMPTE) is used for streaming multimedia content within HTTP requests to traverse firewalls.\n\nThe Real-Time Messaging Protocol tunneled through HTTPS (RTMPTS) is used for streaming multimedia content within HTTPS requests to traverse firewalls.\n\nThis protocol accepts the following options.\n\nFor more information see: http://www.samba.org/.\n\nRead from or write to remote resources using SFTP protocol.\n\nThis protocol accepts the following options.\n\nReal-Time Messaging Protocol and its variants supported through librtmp.\n\nRequires the presence of the librtmp headers and library during configuration. You need to explicitly configure the build with \"–enable-librtmp\". If enabled this will replace the native RTMP protocol.\n\nThis protocol provides most client functions and a few server functions needed to support RTMP, RTMP tunneled in HTTP (RTMPT), encrypted RTMP (RTMPE), RTMP over SSL/TLS (RTMPS) and tunneled variants of these encrypted types (RTMPTE, RTMPTS).\n\nwhere is one of the strings \"rtmp\", \"rtmpt\", \"rtmpe\", \"rtmps\", \"rtmpte\", \"rtmpts\" corresponding to each RTMP variant, and , , and have the same meaning as specified for the RTMP native protocol. contains a list of space-separated options of the form = .\n\nSee the librtmp manual page (man 3 librtmp) for more information.\n\nFor example, to stream a file in real-time to an RTMP server using :\n\nTo play the same stream using :\n\nThe required syntax for an RTP URL is:\n\nspecifies the RTP port to use.\n\ncontains a list of &-separated options of the form = .\n\nThe following URL options are supported:\n• If is not set the RTCP port will be set to the RTP port value plus 1.\n• If (the local RTP port) is not set any available port will be used for the local RTP and RTCP ports.\n• If (the local RTCP port) is not set it will be set to the local RTP port value plus 1.\n\nRTSP is not technically a protocol handler in libavformat, it is a demuxer and muxer. The demuxer supports both normal RTSP (with data transferred over RTP; this is used by e.g. Apple and Microsoft) and Real-RTSP (with data transferred over RDT).\n\nThe muxer can be used to send a stream using RTSP ANNOUNCE to a server supporting it (currently Darwin Streaming Server and Mischa Spiegelmock’s RTSP server).\n\nThe required syntax for a RTSP url is:\n\nOptions can be set on the / command line, or set in code via s or in .\n\nThe following options are supported.\n\nThe following options are supported.\n\nWhen receiving data over UDP, the demuxer tries to reorder received packets (since they may arrive out of order, or packets may get lost totally). This can be disabled by setting the maximum demuxing delay to zero (via the field of AVFormatContext).\n\nWhen watching multi-bitrate Real-RTSP streams with , the streams to display can be chosen with and for video and audio respectively, and can be switched on the fly by pressing and .\n\nThe following examples all make use of the and tools.\n• Watch a stream over UDP, with a max reordering delay of 0.5 seconds:\n• Send a stream in realtime to a RTSP server, for others to watch:\n\nSession Announcement Protocol (RFC 2974). This is not technically a protocol handler in libavformat, it is a muxer and demuxer. It is used for signalling of RTP streams, by announcing the SDP for the streams regularly on a separate port.\n\nThe syntax for a SAP url given to the muxer is:\n\nThe RTP packets are sent to on port , or to port 5004 if no port is specified. is a -separated list. The following options are supported:\n\nTo broadcast a stream on the local subnet, for watching in VLC:\n\nAnd for watching in , over IPv6:\n\nThe syntax for a SAP url given to the demuxer is:\n\nis the multicast address to listen for announcements on, if omitted, the default 224.2.127.254 (sap.mcast.net) is used. is the port that is listened on, 9875 if omitted.\n\nThe demuxers listens for announcements on the given address and port. Once an announcement is received, it tries to receive that particular stream.\n\nTo play back the first stream announced on the normal SAP multicast address:\n\nTo play back the first stream announced on one the default IPv6 SAP multicast address:\n\nThe protocol accepts the following options:\n\nThe supported syntax for a SRT URL is:\n\ncontains a list of &-separated options of the form = .\n\nThis protocol accepts the following options.\n\nConnection timeout; SRT cannot connect for RTT > 1500 msec (2 handshake exchanges) with the default connect timeout of 3 seconds. This option applies to the caller and rendezvous connection modes. The connect timeout is 10 times the value set for the rendezvous mode (which can be used as a workaround for this connection problem with earlier versions). Flight Flag Size (Window Size), in bytes. FFS is actually an internal parameter and you should set it to not less than and . The default value is relatively large, therefore unless you set a very large receiver buffer, you do not need to change this option. Default value is 25600. Sender nominal input rate, in bytes per seconds. Used along with , when is set to relative (0), to calculate maximum sending rate when recovery packets are sent along with the main media stream: * (100 + ) / 100 if is not set while is set to relative (0), the actual input rate is evaluated inside the library. Default value is 0. IP Type of Service. Applies to sender only. Default value is 0xB8. IP Time To Live. Applies to sender only. Default value is 64. Timestamp-based Packet Delivery Delay. Used to absorb bursts of missed packet retransmissions. This flag sets both and to the same value. Note that prior to version 1.3.0 this is the only flag to set the latency, however this is effectively equivalent to setting , when side is sender and when side is receiver, and the bidirectional stream sending is not supported. Maximum sending bandwidth, in bytes per seconds. -1 infinite (CSRTCC limit is 30mbps) 0 relative to input rate (see ) >0 absolute limit value Default value is 0 (relative) Connection mode. opens client connection. starts server to listen for incoming connections. use Rendez-Vous connection mode. Default value is caller. Maximum Segment Size, in bytes. Used for buffer allocation and rate calculation using a packet counter assuming fully filled packets. The smallest MSS between the peers is used. This is 1500 by default in the overall internet. This is the maximum size of the UDP packet and can be only decreased, unless you have some unusual dedicated network settings. Default value is 1500. If set to 1, Receiver will send ‘UMSG_LOSSREPORT‘ messages periodically until a lost packet is retransmitted or intentionally dropped. Default value is 1. Recovery bandwidth overhead above input rate, in percents. See . Default value is 25%. HaiCrypt Encryption/Decryption Passphrase string, length from 10 to 79 characters. The passphrase is the shared secret between the sender and the receiver. It is used to generate the Key Encrypting Key using PBKDF2 (Password-Based Key Derivation Function). It is used only if is non-zero. It is used on the receiver only if the received data is encrypted. The configured passphrase cannot be recovered (write-only). If true, both connection parties must have the same password set (including empty, that is, with no encryption). If the password doesn’t match or only one side is unencrypted, the connection is rejected. Default is true. The number of packets to be transmitted after which the encryption key is switched to a new key. Default is -1. -1 means auto (0x1000000 in srt library). The range for this option is integers in the 0 - . The interval between when a new encryption key is sent and when switchover occurs. This value also applies to the subsequent interval between when switchover occurs and when the old encryption key is decommissioned. Default is -1. -1 means auto (0x1000 in srt library). The range for this option is integers in the 0 - . The sender’s extra delay before dropping packets. This delay is added to the default drop delay time interval value. Special value -1: Do not drop packets on the sender at all. Sets the maximum declared size of a packet transferred during the single call to the sending function in Live mode. Use 0 if this value isn’t used (which is default in file mode). Default is -1 (automatic), which typically means MPEG-TS; if you are going to use SRT to send any different kind of payload, such as, for example, wrapping a live stream in very small frames, then you can use a bigger maximum frame size, though not greater than 1456 bytes. The latency value (as described in ) that is set by the sender side as a minimum value for the receiver. Sender encryption key length, in bytes. Only can be set to 0, 16, 24 and 32. Enable sender encryption if not 0. Not required on receiver (set to 0), key size obtained from sender in HaiCrypt handshake. Default value is 0. The time that should elapse since the moment when the packet was sent and the moment when it’s delivered to the receiver application in the receiving function. This time should be a buffer time large enough to cover the time spent for sending, unexpectedly extended RTT time, and the time needed to retransmit the lost UDP packet. The effective latency value will be the maximum of this options’ value and the value of set by the peer side. Before version 1.3.0 this option is only available as . Set raise error timeouts for read, write and connect operations. Note that the SRT library has internal timeouts which can be controlled separately, the value set here is only a cap on those. Too-late Packet Drop. When enabled on receiver, it skips missing packets that have not been delivered in time and delivers the following packets to the application when their time-to-play has come. It also sends a fake ACK to the sender. When enabled on sender and enabled on the receiving peer, the sender drops the older packets that have no chance of being delivered in time. It was automatically enabled in the sender if the receiver supports it. Receive buffer must not be greater than . The value up to which the Reorder Tolerance may grow. When Reorder Tolerance is > 0, then packet loss report is delayed until that number of packets come in. Reorder Tolerance increases every time a \"belated\" packet has come, but it wasn’t due to retransmission (that is, when UDP packets tend to come out of order), with the difference between the latest sequence and this packet’s sequence, and not more than the value of this option. By default it’s 0, which means that this mechanism is turned off, and the loss report is always sent immediately upon experiencing a \"gap\" in sequences. The minimum SRT version that is required from the peer. A connection to a peer that does not satisfy the minimum version requirement will be rejected. The version format in hex is 0xXXYYZZ for x.y.z in human readable form. A string limited to 512 characters that can be set on the socket prior to connecting. This stream ID will be able to be retrieved by the listener side from the socket that is returned from srt_accept and was connected by a socket with that set stream ID. SRT does not enforce any special interpretation of the contents of this string. This option doesn’t make sense in Rendezvous connection; the result might be that simply one side will override the value from the other side and it’s the matter of luck which one would win Alias for ‘ ’ to avoid conflict with ffmpeg command line option. The type of Smoother used for the transmission for that socket, which is responsible for the transmission and congestion control. The Smoother type must be exactly the same on both connecting parties, otherwise the connection is rejected. When set, this socket uses the Message API, otherwise it uses Buffer API. Note that in live mode (see ) there’s only message API available. In File mode you can chose to use one of two modes: Stream API (default, when this option is false). In this mode you may send as many data as you wish with one sending instruction, or even use dedicated functions that read directly from a file. The internal facility will take care of any speed and congestion control. When receiving, you can also receive as many data as desired, the data not extracted will be waiting for the next call. There is no boundary between data portions in the Stream mode. Message API. In this mode your single sending instruction passes exactly one piece of data that has boundaries (a message). Contrary to Live mode, this message may span across multiple UDP packets and the only size limitation is that it shall fit as a whole in the sending buffer. The receiver shall use as large buffer as necessary to receive the message, otherwise the message will not be given up. When the message is not complete (not all packets received or there was a packet loss) it will not be given up. Sets the transmission type for the socket, in particular, setting this option sets multiple other parameters to their default values as required for a particular transmission type. live: Set options as for live transmission. In this mode, you should send by one sending instruction only so many data that fit in one UDP packet, and limited to the value defined first in (1316 is default in this mode). There is no speed control in this mode, only the bandwidth control, if configured, in order to not exceed the bandwidth with the overhead transmission (retransmitted and control packets). file: Set options as for non-live transmission. See for further explanations The number of seconds that the socket waits for unsent data when closing. Default is -1. -1 means auto (off with 0 seconds in live mode, on with 180 seconds in file mode). The range for this option is integers in the 0 - . When true, use Timestamp-based Packet Delivery mode. The default behavior depends on the transmission type: enabled in live mode, disabled in file mode.\n\nFor more information see: https://github.com/Haivision/srt.\n\nVirtually extract a segment of a file or another stream. The underlying stream must be seekable.\n\nExtract a chapter from a DVD VOB file (start and end sectors obtained externally and multiplied by 2048):\n\nWrites the output to multiple protocols. The individual outputs are separated by |\n\nThe required syntax for a TCP url is:\n\ncontains a list of &-separated options of the form = .\n\nThe list of supported options follows.\n\nThe following example shows how to setup a listening TCP connection with , which is then accessed with :\n\nThe required syntax for a TLS/SSL url is:\n\nThe following parameters can be set via command line options (or in code via s):\n\nTo create a TLS/SSL server that serves an input stream.\n\nTo play back a stream from the TLS/SSL server using :\n\nThe required syntax for an UDP URL is:\n\ncontains a list of &-separated options of the form = .\n\nIn case threading is enabled on the system, a circular buffer is used to store the incoming data, which allows one to reduce loss of data due to UDP socket buffer overruns. The and options are related to this buffer.\n\nThe list of supported options follows.\n• Use to stream over UDP to a remote endpoint:\n• Use to stream in mpegts format over UDP using 188 sized UDP packets, using a large input buffer:\n• Use to receive over UDP from a remote endpoint:\n\nThe required syntax for a Unix socket URL is:\n\nThe following parameters can be set via command line options (or in code via s):\n\nThis library supports unicast streaming to multiple clients without relying on an external server.\n\nThe required syntax for streaming or connecting to a stream is:\n\nMultiple clients may connect to the stream using:\n\nStreaming to multiple clients is implemented using a ZeroMQ Pub-Sub pattern. The server side binds to a port and publishes data. Clients connect to the server (via IP address/port) and subscribe to the stream. The order in which the server and client start generally does not matter.\n\nffmpeg must be compiled with the –enable-libzmq option to support this protocol.\n\nOptions can be set on the / command line. The following options are supported:\n\nFor details about the authorship, see the Git history of the project (https://git.ffmpeg.org/ffmpeg), e.g. by typing the command in the FFmpeg source directory, or browsing the online repository at https://git.ffmpeg.org/ffmpeg.\n\nMaintainers for the specific components are listed in the file in the source code tree.\n\nThis document was generated on March 22, 2025 using makeinfo."
    },
    {
        "link": "http://bytedeco.org/news/2015/04/04/javacv-frame-converters",
        "document": "To supplement the long-standing functionality of , , , and their implementation classes found in JavaCV, we are introducing the concept of starting with version 0.11, released today. Other more minor changes have been made to all the components since version 0.10, so be sure to check out the repositories on GitHub: JavaCPP, JavaCPP Presets, JavaCV, ProCamCalib, and ProCamTracker.\n\nA , in short, allows the user to share easily the same audio samples or video image data among different APIs, but without coupling their applications to all of those APIs. For example, JavaCV currently ships with , , and to let users represent image data as either , , , or . The plain old data class adopted by JavaCV is , which does not itself depend on either Android, FFmpeg, Java 2D, or OpenCV. A user could for example grab and record frames using and , without requiring anything more than FFmpeg. We could further display those frames with , creating a dependency on Java 2D, or alternatively, on Android, if we transfer the frames to video memory with . To further process the data, one might start using , automatically creating a dependency on OpenCV that was not present until that point. We intentionally designed the API to be as easy to use as possible, but at the same time as efficient as possible, eliminating data copies whenever the underlying APIs allow. A typical usage scenario might look like this:\n\nIt is also possible to pass a in a generic way to prevent unnecessary processing of data, for example:\n\nAs a side note, to implement we used code previously found inside the helper class. On Android, this was problematic because it would cause the log file to fill up with warnings about classes missing from the package. Moving that code out fixed this annoyance. As a less happy consequence, to recover the functionality that was lost, existing users need to refactor their code based on a combination of and . To limit these kinds of breaking changes, the features are still very limited, but in a way, this is intentional. It allows users to focus on what is missing, rather than on figuring out how to use everything. We are aware of no precedent to a framework like JavaCV that has attempted to bridge the gap between multimedia and computer vision across platforms.\n\nIn any case, we hope that overall you find this “frame processing” technique practical for your own applications, and as usual, if you have any questions, problems, or would like to contribute, but are unsure how to proceed, please feel free to share your concerns on the mailing list or via “issues” on GitHub. Enjoy and thank you for your continued interest!"
    },
    {
        "link": "https://github.com/bytedeco/javacv/issues/1172",
        "document": "I've been testing converting instances to instances and noticed it's very slow, because, no matter what, every pixel channel is copied manually from 's buffer to (even when pixel layout and values don't have to be altered at all).\n\nI created a small example with a streamlined approach, that can be used instead, if there's no gamma conversion required and 's pixel format matches (won't work for every pixel format, obviously, but works for BGR/RGB formats). You can comment/uncomment parts where conversion is done and test it on your machine.\n\n{ = ; = ; = ; = (); ( [] ) . , . { = ( ); . ( , ); // does nothing in the current ffmpeg version . ( , ); . ( ); . (); = ( , . (), . ()); . ( . ()); . ( . ()); . ( . ()); . ( ); . ( , ); . (); = ; = . (); ; (( = . ( , , , )) != ) { = ( ); = ( ); . ( , ); ++ ; } = ( . () - ) / ; . . ( + + + ( . ( * ) / ) + + ( . ( / * ) / ) + ); . (); . (); } [] = ; ( ) { = ( ) . [ ]. ( ); ( == ) = [ . ()]; . ( ); = . ( . ); = ( , , , . , . ); = . ( ( . , . , . , . , . , [] { , , }), ); [] = (( ) . ()). (); . ( , , , , . ); ( , , , ); } [] = ; ( ) { = ( ) . (). (); ( == ) = [ . (). ]; . ( . (), , , , . ); = ( . (), . (), , ); = ( ) . [ ]. ( ); . ( ); ; } }"
    },
    {
        "link": "https://bytedeco.org/javacv/apidocs/org/bytedeco/javacv/Java2DFrameConverter.html",
        "document": "A utility class to copy data between and . Since does not support NIO buffers, we cannot share allocated memory with"
    },
    {
        "link": "https://stackoverflow.com/questions/62185618/javacv-ffmpegframegrabber-java2dframeconverter-creating-weird-looking-image",
        "document": "I'm new to JavaCV, so the issue is probably very obvious. I'm trying to do the easy said, difficult done, task of getting the images and audio from a video so I can start making a video editor. After lots of confusion and errors, I am finally getting a result, but it is as odd as the errors. The image appears to be squished in the x direction, with the extra space to the right being transparent (so the image size matches the video's size). Additionally, it has a lot of extra transparent pixels and is multicolored in an odd way.\n\nWhat the image should look like: https://gofile.io/d/1lQnNd\n\nWhat the image looks like: https://gofile.io/d/kc09G7\n\nHere is my code:\n\nIf you are interested, here is the console:\n\nAdditionally, if I just do instead of (which I believe leaves out the audio), the frame's image property is null (that is what the statement is there for). I am not sure if that belongs in a new question, but help on that is also appreciated."
    },
    {
        "link": "https://stackoverflow.com/questions/60771002/i-am-using-org-bytedeco-javacv-ffmpegframegrabber-java2dframeconverter-for-co",
        "document": "in this image, I am getting an error when I am going to start grabber.start() on my local, it's working absolute fine at the time sonar-scanner test case run throws an exception which is attached in image.\n\nERROR: comes when i star a grabber(grabber.start() in code )\n\nA fatal error has been detected by the Java Runtime Environment: # # SIGSEGV (0xb) at pc=0x000000000000dc56, pid=447, tid=0x00007f45200a3b10 Exception in thread \"Thread-8\" java.io.EOFException at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2960) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1540) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431) at"
    },
    {
        "link": "https://stackoverflow.com/questions/912623/how-can-i-speed-up-java-datagramsocket-performance",
        "document": "I'm using Java DatagramSocket class to send a UDP data gram to an endpoint. The datagram must arrive at the endpoint in 60ms intervals. I'm finding that DatagramSocket.Send can often take > 1 ms (close to 2) to package and send packets no greater than 56 bytes. This causes my packets to be delivered at 62 ms intervals, rather than 60ms intervals. This is on a windows vista machine. Here is how I'm measuring the time: Does any one have tips or tricks to speed this process?"
    },
    {
        "link": "https://stackoverflow.com/questions/1753334/use-datagrams-in-java-to-send-video-audio-from-client-to-server",
        "document": "Hey everyone, I'm having a bit of a problem with UDP and Datagrams. I'm supposed to make a server that will get a request from the client to send a file in the same directory. The UDP Server will then get this file (a video), put it into a datagram and send it. I think I know how to do it, but I can't put the file in the datagram. I'm putting it in Binary form, so keep that in mind.\n\nHere's my code so far: edit: This is the server by the way, and I keep having trouble with BufferedInputReader and OutputReader, so keep that in mind :)"
    },
    {
        "link": "https://moldstud.com/articles/p-leveraging-java-for-real-time-video-streaming-platforms",
        "document": "Real-time video analytics allows businesses to extract valuable insights from live video streams. By analyzing video data in real-time, businesses can detect and respond to events as they happen, improve operational efficiency, enhance customer experience, and increase overall security. Some key benefits of real-time video analytics include:\n\nJava technology is widely used for building real-time video analytics solutions due to its scalability, performance, and robustness. Java provides a rich set of libraries and tools that can be leveraged to process video data efficiently. Here are some key steps to implement real-time video analytics with Java technology:\n\nThe first step in implementing real-time video analytics is to acquire video data from cameras or other sources. Java provides libraries such as OpenCV that can be used to capture video streams and extract frames for analysis.\n\nOnce the video data is acquired, it needs to be preprocessed to enhance the quality and extract relevant information. Java offers various image processing libraries that can be used to preprocess video frames before analysis.\n\nObject detection and tracking are key components of real-time video analytics. Java provides machine learning libraries such as Deeplearning4j that can be used to build models for object detection and tracking in video streams.\n\nAfter detecting and tracking objects in real-time, the next step is to analyze the data and visualize the insights. Java libraries such as JavaFX can be used to create interactive visualizations of the video analytics results.\n• Real-time video analytics is crucial for businesses to make faster and more informed decisions.\n• Implementing real-time video analytics with Java involves video data acquisition, data preprocessing, object detection and tracking, and data analysis and visualization.\n\nBy leveraging Java technology for real-time video analytics, businesses can gain valuable insights from live video streams and improve decision-making processes. The combination of Java's powerful libraries and tools make it a great choice for building scalable and efficient video analytics solutions.\n\nAs a software development company, it is crucial to optimize video streaming solutions to ensure smooth playback while minimizing bandwidth consumption.\n\nBandwidth is a limited and valuable resource, especially in today's digital age where multiple devices are connected to the internet simultaneously. Optimizing bandwidth in Java-based video streaming solutions can result in faster load times, improved video quality, and a better overall user experience. By reducing the amount of data being transferred, companies can also save on costs associated with data usage and server infrastructure.\n\nWhen it comes to Java-based video streaming solutions, there are several strategies that developers can employ to minimize bandwidth usage:\n• Implement Adaptive Bitrate Streaming: This technique adjusts the video quality based on the user's internet connection, ensuring smooth playback without buffering.\n• Utilize Efficient Compression Algorithms: Use codecs like H.264 or H.265 to compress video files without compromising quality.\n• Cache Data Locally: Store frequently accessed content on the user's device to reduce the need for repeated data transfers.\n\nBy implementing these strategies, companies can benefit from:\n• Competitive Edge: Stand out in a crowded market with superior streaming performance\n\nIn conclusion, optimizing bandwidth usage in Java-based video streaming solutions is essential for delivering high-quality video content efficiently. By implementing techniques such as adaptive bitrate streaming and efficient compression algorithms, companies can enhance the user experience while minimizing costs. It is crucial for software development companies to stay ahead of the curve and prioritize bandwidth optimization in today's competitive digital landscape.\n\nThe Importance of Real-Time Capabilities in Video Streaming Apps\n\nVideo streaming apps have become increasingly popular in recent years, thanks to the widespread availability of high-speed internet connections and the growing demand for on-the-go entertainment. However, developing a video streaming app that offers smooth playback, minimal buffering, and high-quality content can be a challenging task. This is where Java's real-time capabilities come into play, allowing developers to create robust and responsive applications that deliver an immersive viewing experience for users.\n• Low Latency: Java tools offer low latency solutions that ensure minimal delay between the time a video is broadcasted and when it is received by the viewer, resulting in seamless playback.\n• High Performance: Java frameworks are optimized for performance, allowing developers to build video streaming apps that can handle high volumes of data and maintain consistent playback quality.\n• Scalability: Java tools provide scalable solutions that can accommodate growing user bases and increasing data traffic without compromising on performance.\n• Reliability: Java real-time capabilities ensure the reliability of video streaming apps, minimizing the risk of downtime or technical glitches that can disrupt the user experience.\n\nAdvantages of Using Java Tools for Developing Video Streaming Apps\n\nThere are several advantages to using Java tools for developing video streaming apps:\n• Java's cross-platform compatibility allows developers to create apps that can run on multiple operating systems and devices, reaching a wider audience.\n• Java's extensive libraries and frameworks provide developers with a wealth of resources for building robust and feature-rich video streaming applications.\n• Java's real-time capabilities enable developers to create responsive and interactive user interfaces that enhance the overall viewing experience.\n• Java's strong community support and documentation make it easier for developers to troubleshoot issues and find solutions to common challenges.\n• Optimize Performance: Utilize Java's real-time capabilities to optimize the performance of your video streaming app, ensuring smooth playback and minimal buffering.\n• Enhance User Experience: Leverage Java tools to create engaging and interactive user interfaces that enhance the overall viewing experience for users.\n• Ensure Reliability: Take advantage of Java's reliability features to minimize downtime and technical issues that can negatively impact the user experience.\n• Stay Updated: Stay informed about the latest Java tools and frameworks for real-time application development to keep your video streaming app competitive in the market.\n\nIn conclusion, Java real-time capabilities offer a powerful set of tools for developing high-performance video streaming apps that deliver superior user experience. By leveraging Java's scalability, reliability, and performance optimization features, developers can create innovative and engaging applications that stand out in the competitive streaming market. With Java's extensive resources and community support, building a successful video streaming app has never been easier. So why wait? Start exploring the world of Java real-time capabilities and take your video streaming apps to the next level!"
    },
    {
        "link": "https://profiletree.com/java-socket-programming",
        "document": "Java Socket Programming empowers developers to build powerful network applications by establishing communication channels between devices over a network. By utilising sockets, developers can create robust client-server applications, facilitate real-time communication, and integrate various components of distributed systems while ensuring data exchange and security across networks.\n\nJava Socket Programming is your ticket to the open seas, a direct line to any port in the digital storm. With a flick of your code-forged wrist, you can send messages across continents, build real-time chat apps, and even control robots on Mars – all through the magic of inter-process communication.\n\nIn this section, we will explore the fundamentals of socket programming in Java, including understanding sockets and how they are used for communication between a client and server. We will also dive into the different socket classes available in Java’s net package.\n\nSockets in Java allow two-way chat between computers. Think of them as a pipe for data. One computer sends data into the pipe, and the other takes it out. They are used when we want apps on different systems to ‘talk’ with each other, and you will find sockets at each end of this link.\n\nThere are two types of sockets: server sockets and client sockets. The server socket waits for requests from clients, while the client socket asks for connections from servers.\n\nFor all this to work, Java uses a set of rules known as TCP/IP protocol, which makes sure that our message gets sent back and forth correctly between different ends or points in network communication.\n\nCommunication Between Client and Server Using Sockets\n\nIn Java programming, the ServerSocket class waits for client requests. Once a request lands, it sets up a link to the client using sockets. This is socket programming at work! Sockets are like two-way streets where information can flow in both directions.\n\nThey are software endpoints that help in exchanging messages between a client and server. One side sends data, while the other receives it. The Socket class speeds up this transfer process by panelling all lines of communication into one place! So, each system – be it on different Java Runtime Environments (JREs) or not – performs its tasks without delay.\n\nAnd there’s more! We also have User Datagram Protocol (UDP) and Transfer Control Protocol (TCP). These two protocols set out rules for how your data should move from one point to another via these sockets.\n\nLet’s dive into Java socket classes. These are vital parts of Java socket programming. Here are the key facts you should know:\n• Java has two main classes for socket programming – Socket and ServerSocket.\n• The Socket class helps the client to connect with the server.\n• This class also lets the client send messages.\n• On the other side, we have the ServerSocket class.\n• This class is for use by servers, not clients.\n• Servers use it to wait and listen for a client’s call.\n• When a client tries to connect, this class swings into action.\n• Its accept() method blocks any other task until a client connects.\n• One more thing – java.net package takes care of all low-level details in our programs.\n\nThere are different socket programming techniques that you need to be familiar with, such as blocking vs non-blocking sockets and asynchronous channels, to enhance your Java networking skills. After learning about these techniques, you should also explore error-handling and exception-handling strategies for a smoother socket communication experience.\n\nIn Java socket programming, understanding the distinction between blocking and non-blocking sockets is crucial. Here is an overview of their differences and appropriate use cases.\n\nChoosing between blocking and non-blocking sockets depends on the specific requirements of your application and its performance needs.\n\nAsynchronous channels are an important concept in socket programming. They allow us to perform socket operations in an asynchronous manner, which means that we can continue with other tasks while waiting for a response from the server or client.\n• Asynchronous socket channels in Java are used for socket programming in an asynchronous manner.\n• The code for implementing an asynchronous socket channel is similar to synchronous socket programming.\n• Asynchronous channels make use of non-blocking I/O operations, which allows for better performance and scalability.\n• In Java programming, the ServerSocketChannel.accept method blocks until a connection is accepted and returns a SocketChannel object.\n• NIO2 Asynchronous Socket Channel is an API in Java that provides a practical guide for implementing asynchronous socket channels.\n• The AsynchronousSocketChannel class in Java represents an asynchronous channel for stream-oriented connecting sockets.\n\nIn socket programming, it is important to handle errors and exceptions properly. One common exception that occurs in socket programming is the SocketException. This exception is thrown when there is an error in creating or accessing a socket.\n\nThe SocketException falls under the Java Exception Hierarchy, which means it is a subclass of the IOException class. When this exception occurs, it indicates that there was a problem with the socket connection.\n\nTo handle this exception, you can use try-catch blocks to catch any SocketExceptions that may occur during your code execution. Within the catch block, you can specify what actions to take when this exception happens.\n\nIn advanced socket programming, we explore advanced techniques and features that can enhance network communication in Java. This includes configuring socket options, implementing secure socket communication using SSL/TLS, and utilizing broadcasting and multicasting capabilities with sockets.\n\nSocket options and configurations allow developers to customise the behaviour of socket communication in Java. Here are some important things to know:\n• Socket options: Java provides various socket options that can be used to modify the behaviour of socket communication. These options include timeouts, buffer sizes, and keep-alive settings.\n• Configuration files: Developers can use configuration files to set default socket options for their applications. These files typically contain key-value pairs that specify the desired option values.\n• SocketImpl class: The SocketImpl class in Java provides methods that allow developers to implement custom socket behaviour. This class is useful when system-specific implementations are required for certain socket operations.\n• DatagramSocketImpl class: The DatagramSocketImpl class is similar to SocketImpl but specifically designed for datagram communication. It allows developers to customize the behaviour of UDP/IP sockets.\n• TCP/IP and UDP/IP communication: Java sockets support both TCP/IP and UDP/IP protocols. This means that developers can create client-server applications using either protocol based on their specific requirements.\n\nSecure socket communication, also known as SSL/TLS, is an important aspect of advanced socket programming in Java. It ensures data security and network security by encrypting the communication between a client and a server.\n\nSSL (Secure Sockets Layer) and TLS (Transport Layer Security) are communication protocols that establish secure connections over a network. They use encryption algorithms to protect sensitive information exchanged between websites/services and users.\n\nSSL certificates play a crucial role in this process by verifying the authenticity of websites/service providers, ensuring secure transactions, and maintaining internet security. In Java, the Java Secure Socket Extension (JSSE) provides a framework for implementing TLS to create secure connections in socket programming.\n\nWhen working with socket programming in Java, it is possible to implement broadcasting and multicasting. Broadcasting allows one-to-all communication, where a message sent by the server can be received by multiple clients.\n\nOn the other hand, multicasting enables one-to-many communication, where messages are sent from a single source to a specific group of receivers.\n\nTo implement broadcasting in Java socket programming, you can use DatagramSocket and DatagramPacket classes. The server can send messages using broadcast IP addresses such as 255.255.255.255 or a subnet-specific address like 192.168.0.255.\n\nFor multicasting, Java programming provides MulticastSocket class that makes it easy to send and receive multicast datagrams on an IP multicast group address. The server joins a specific multicast group using its IP address and port number while clients subscribe to the same group to receive messages from the server.\n\nBest Practices and Tips for Socket Programming in Java\n\nComing up next, we dive into the essential best practices and hidden gems to guide you towards writing efficient, secure, and reliable socket applications. Here are some important things to keep in mind:\n• Plan your communication protocol: Before starting your socket programming project, it’s crucial to plan out the communication protocol between the client and server. This includes deciding on the message formats, data encoding, and error-handling mechanisms.\n• Use buffered streams: When reading from or writing to sockets, it’s recommended to use buffered streams instead of raw input/output streams. Buffered streams can significantly improve performance by reducing the number of network calls.\n• Implement proper error handling: Socket programming can be prone to errors, such as connection timeouts or unexpected disconnections. Make sure to implement robust error-handling mechanisms to handle these situations, including retry strategies and logging gracefully.\n• Minimize network traffic: To improve efficiency, minimize the amount of data sent over the network by using compact message formats and only transmitting necessary information. This can help reduce bandwidth usage and increase overall system performance.\n• Secure socket communication: If you’re dealing with sensitive data, consider implementing secure socket communication using SSL/TLS protocols. This ensures that the data exchanged between the client and server is encrypted and protected from unauthorized access.\n• Thoroughly test your code: Before deploying your socket programming application in a production environment, thoroughly test it under various scenarios like high traffic or network failures. This will help identify any potential issues or bottlenecks that need to be addressed.\n\nIn conclusion, Java Socket Programming is a powerful tool for creating network applications that allow computers to communicate with each other. By using sockets, we can establish connections between clients and servers, enabling the exchange of data over a network.With its simplicity and versatility, Java socket programming is an essential skill for developers interested in building robust network applications."
    },
    {
        "link": "https://jsums.edu/nmeghanathan/files/2019/01/CSC435-Sp2019-Module-2-Socket-Programming-in-Java.pdf",
        "document": ""
    }
]