[
    {
        "link": "https://huggingface.co/transformers/v4.4.2/model_doc/gpt.html",
        "document": "OpenAI GPT model was proposed in Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever. It’s a causal (unidirectional) transformer pre-trained using language modeling on a large corpus will long range dependencies, the Toronto Book Corpus. The abstract from the paper is the following: Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pretraining of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied.\n• None GPT is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n• None GPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the example script. Write With Transformer is a webapp created and hosted by Hugging Face showcasing the generative capabilities of several models. GPT is one of them. The original code can be found here. If you want to reproduce the original tokenization process of the paper, you will need to install and : If you don’t install and , the will default to tokenize using BERT’s followed by Byte-Pair Encoding (which should be fine for most usage, don’t worry).\n\nThe Original OpenAI GPT Model transformer with a sequence classification head on top (linear layer). uses the last token in order to do the classification, as other causal models (e.g. GPT-2) do. Since it does classification on the last token, it requires to know the position of the last token. If a is defined in the configuration, it finds the last token that is not a padding token in each row. If no is defined, it simply takes the last value in each row of the batch. Since it cannot guess the padding tokens when are passed instead of , it does the same (take the last value in each row of the batch). This model inherits from . Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.) This model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior. config ( ) – Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the method to load the model weights. Although the recipe for forward pass needs to be defined within this function, one should call the instance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\n• None Indices of input sequence tokens in the vocabulary. Indices can be obtained using . See and for details.\n• None Mask to avoid performing attention on padding token indices. Mask values selected in :\n• None 1 for tokens that are not masked,\n• None 0 for tokens that are masked.\n• None Segment token indices to indicate first and second portions of the inputs. Indices are selected in :\n• None Indices of positions of each input sequence tokens in the position embeddings. Selected in the range .\n• None Mask to nullify selected heads of the self-attention modules. Mask values selected in :\n• None 1 indicates the head is not masked,\n• None 0 indicates the head is masked.\n• None inputs_embeds ( of shape , ) – Optionally, instead of passing you can choose to directly pass an embedded representation. This is useful if you want more control over how to convert indices into associated vectors than the model’s internal embedding lookup matrix.\n• None output_attentions ( , ) – Whether or not to return the attentions tensors of all attention layers. See under returned tensors for more detail.\n• None output_hidden_states ( , ) – Whether or not to return the hidden states of all layers. See under returned tensors for more detail.\n• None return_dict ( , ) – Whether or not to return a instead of a plain tuple.\n• None labels ( of shape , ) – Labels for computing the sequence classification/regression loss. Indices should be in . If a regression loss is computed (Mean-Square loss), If a classification loss is computed (Cross-Entropy). A (if is passed or when ) or a tuple of comprising various elements depending on the configuration ( ) and inputs.\n• None loss ( of shape , , returned when is provided) – Classification (or regression if config.num_labels==1) loss.\n• None logits ( of shape ) – Classification (or regression if config.num_labels==1) scores (before SoftMax).\n• None hidden_states ( , , returned when is passed or when ) – Tuple of (one for the output of the embeddings + one for the output of each layer) of shape . Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n• None attentions ( , , returned when is passed or when ) – Tuple of (one for each layer) of shape . Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\nThe bare OpenAI GPT transformer model outputting raw hidden-states without any specific head on top. This model inherits from . Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.) This model is also a tf.keras.Model subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and behavior.\n• None having all inputs as keyword arguments (like PyTorch models), or\n• None having all inputs as a list, tuple or dict in the first positional arguments. This second option is useful when using method which currently requires having all the tensors in the first argument of the model call function: . If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the first positional argument :\n• None a single Tensor with only and nothing else:\n• None a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: or\n• None a dictionary with one or several input Tensors associated to the input names given in the docstring: config ( ) – Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the method to load the model weights. Although the recipe for forward pass needs to be defined within this function, one should call the instance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\n• None Indices of input sequence tokens in the vocabulary. Indices can be obtained using . See and for details.\n• None Mask to avoid performing attention on padding token indices. Mask values selected in :\n• None 1 for tokens that are not masked,\n• None 0 for tokens that are masked.\n• None Segment token indices to indicate first and second portions of the inputs. Indices are selected in :\n• None Indices of positions of each input sequence tokens in the position embeddings. Selected in the range .\n• None head_mask ( or of shape or , ) – Mask to nullify selected heads of the self-attention modules. Mask values selected in :\n• None 1 indicates the head is not masked,\n• None 0 indicates the head is masked.\n• None inputs_embeds ( or of shape , ) – Optionally, instead of passing you can choose to directly pass an embedded representation. This is useful if you want more control over how to convert indices into associated vectors than the model’s internal embedding lookup matrix.\n• None output_attentions ( , ) – Whether or not to return the attentions tensors of all attention layers. See under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will be used instead.\n• None output_hidden_states ( , ) – Whether or not to return the hidden states of all layers. See under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will be used instead.\n• None return_dict ( , ) – Whether or not to return a instead of a plain tuple. This argument can be used in eager mode, in graph mode the value will always be set to True.\n• None training ( , , defaults to ) – Whether or not to use the model in training mode (some modules like dropout modules have different behaviors between training and evaluation). A (if is passed or when ) or a tuple of comprising various elements depending on the configuration ( ) and inputs.\n• None last_hidden_state ( of shape ) – Sequence of hidden-states at the output of the last layer of the model.\n• None hidden_states ( , , returned when is passed or when ) – Tuple of (one for the output of the embeddings + one for the output of each layer) of shape . Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n• None attentions ( , , returned when is passed or when ) – Tuple of (one for each layer) of shape . Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\nOpenAI GPT Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings). This model inherits from . Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.) This model is also a tf.keras.Model subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and behavior.\n• None having all inputs as keyword arguments (like PyTorch models), or\n• None having all inputs as a list, tuple or dict in the first positional arguments. This second option is useful when using method which currently requires having all the tensors in the first argument of the model call function: . If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the first positional argument :\n• None a single Tensor with only and nothing else:\n• None a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: or\n• None a dictionary with one or several input Tensors associated to the input names given in the docstring: config ( ) – Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the method to load the model weights. Although the recipe for forward pass needs to be defined within this function, one should call the instance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\n• None Indices of input sequence tokens in the vocabulary. Indices can be obtained using . See and for details.\n• None Mask to avoid performing attention on padding token indices. Mask values selected in :\n• None 1 for tokens that are not masked,\n• None 0 for tokens that are masked.\n• None Segment token indices to indicate first and second portions of the inputs. Indices are selected in :\n• None Indices of positions of each input sequence tokens in the position embeddings. Selected in the range .\n• None head_mask ( or of shape or , ) – Mask to nullify selected heads of the self-attention modules. Mask values selected in :\n• None 1 indicates the head is not masked,\n• None 0 indicates the head is masked.\n• None inputs_embeds ( or of shape , ) – Optionally, instead of passing you can choose to directly pass an embedded representation. This is useful if you want more control over how to convert indices into associated vectors than the model’s internal embedding lookup matrix.\n• None output_attentions ( , ) – Whether or not to return the attentions tensors of all attention layers. See under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will be used instead.\n• None output_hidden_states ( , ) – Whether or not to return the hidden states of all layers. See under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will be used instead.\n• None return_dict ( , ) – Whether or not to return a instead of a plain tuple. This argument can be used in eager mode, in graph mode the value will always be set to True.\n• None training ( , , defaults to ) – Whether or not to use the model in training mode (some modules like dropout modules have different behaviors between training and evaluation).\n• None labels ( of shape , ) – Labels for computing the cross entropy classification loss. Indices should be in . A (if is passed or when ) or a tuple of comprising various elements depending on the configuration ( ) and inputs.\n• None loss ( of shape , , where n is the number of non-masked labels, returned when is provided) – Language modeling loss (for next-token prediction).\n• None logits ( of shape ) – Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n• None hidden_states ( , , returned when is passed or when ) – Tuple of (one for the output of the embeddings + one for the output of each layer) of shape . Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n• None attentions ( , , returned when is passed or when ) – Tuple of (one for each layer) of shape . Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\nOpenAI GPT Model transformer with a language modeling and a multiple-choice classification head on top e.g. for RocStories/SWAG tasks. The two heads are two linear layers. The language modeling head has its weights tied to the input embeddings, the classification head takes as input the input of a specified classification token index in the input sequence). This model inherits from . Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.) This model is also a tf.keras.Model subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and behavior.\n• None having all inputs as keyword arguments (like PyTorch models), or\n• None having all inputs as a list, tuple or dict in the first positional arguments. This second option is useful when using method which currently requires having all the tensors in the first argument of the model call function: . If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the first positional argument :\n• None a single Tensor with only and nothing else:\n• None a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: or\n• None a dictionary with one or several input Tensors associated to the input names given in the docstring: config ( ) – Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the method to load the model weights. Although the recipe for forward pass needs to be defined within this function, one should call the instance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\n• None Indices of input sequence tokens in the vocabulary. Indices can be obtained using . See and for details.\n• None Mask to avoid performing attention on padding token indices. Mask values selected in :\n• None 1 for tokens that are not masked,\n• None 0 for tokens that are masked.\n• None Segment token indices to indicate first and second portions of the inputs. Indices are selected in :\n• None Indices of positions of each input sequence tokens in the position embeddings. Selected in the range .\n• None head_mask ( or of shape or , ) – Mask to nullify selected heads of the self-attention modules. Mask values selected in :\n• None 1 indicates the head is not masked,\n• None 0 indicates the head is masked.\n• None inputs_embeds ( or of shape , ) – Optionally, instead of passing you can choose to directly pass an embedded representation. This is useful if you want more control over how to convert indices into associated vectors than the model’s internal embedding lookup matrix.\n• None output_attentions ( , ) – Whether or not to return the attentions tensors of all attention layers. See under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will be used instead.\n• None output_hidden_states ( , ) – Whether or not to return the hidden states of all layers. See under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will be used instead.\n• None return_dict ( , ) – Whether or not to return a instead of a plain tuple. This argument can be used in eager mode, in graph mode the value will always be set to True.\n• None training ( , , defaults to ) – Whether or not to use the model in training mode (some modules like dropout modules have different behaviors between training and evaluation).\n• None mc_token_ids ( or of shape , , default to index of the last token of the input) – Index of the classification token in each input sequence. Selected in the range . A (if is passed or when ) or a tuple of comprising various elements depending on the configuration ( ) and inputs.\n• None logits ( of shape ) – Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n• None mc_logits ( of shape ) – Prediction scores of the multiple choice classification head (scores for each choice before SoftMax).\n• None hidden_states ( , , returned when is passed or when ) – Tuple of (one for the output of the embeddings + one for the output of each layer) of shape . Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n• None attentions ( , , returned when is passed or when ) – Tuple of (one for each layer) of shape . Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads. # Add a [CLS] to the vocabulary (we should train it also!) # Update the model embeddings with the new vocabulary size # The newly token the last token of the vocabulary \"Hello, my dog is cute [CLS]\" \"Hello, my cat is cute [CLS]\"\n\nThe OpenAI GPT Model transformer with a sequence classification head on top (linear layer). uses the last token in order to do the classification, as other causal models (e.g. GPT-2) do. Since it does classification on the last token, it requires to know the position of the last token. If a is defined in the configuration, it finds the last token that is not a padding token in each row. If no is defined, it simply takes the last value in each row of the batch. Since it cannot guess the padding tokens when are passed instead of , it does the same (take the last value in each row of the batch). This model inherits from . Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.) This model is also a tf.keras.Model subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and behavior.\n• None having all inputs as keyword arguments (like PyTorch models), or\n• None having all inputs as a list, tuple or dict in the first positional arguments. This second option is useful when using method which currently requires having all the tensors in the first argument of the model call function: . If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the first positional argument :\n• None a single Tensor with only and nothing else:\n• None a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: or\n• None a dictionary with one or several input Tensors associated to the input names given in the docstring: config ( ) – Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the method to load the model weights. Although the recipe for forward pass needs to be defined within this function, one should call the instance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\n• None Indices of input sequence tokens in the vocabulary. Indices can be obtained using . See and for details.\n• None Mask to avoid performing attention on padding token indices. Mask values selected in :\n• None 1 for tokens that are not masked,\n• None 0 for tokens that are masked.\n• None Segment token indices to indicate first and second portions of the inputs. Indices are selected in :\n• None Indices of positions of each input sequence tokens in the position embeddings. Selected in the range .\n• None head_mask ( or of shape or , ) – Mask to nullify selected heads of the self-attention modules. Mask values selected in :\n• None 1 indicates the head is not masked,\n• None 0 indicates the head is masked.\n• None inputs_embeds ( or of shape , ) – Optionally, instead of passing you can choose to directly pass an embedded representation. This is useful if you want more control over how to convert indices into associated vectors than the model’s internal embedding lookup matrix.\n• None output_attentions ( , ) – Whether or not to return the attentions tensors of all attention layers. See under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will be used instead.\n• None output_hidden_states ( , ) – Whether or not to return the hidden states of all layers. See under returned tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the config will be used instead.\n• None return_dict ( , ) – Whether or not to return a instead of a plain tuple. This argument can be used in eager mode, in graph mode the value will always be set to True.\n• None training ( , , defaults to ) – Whether or not to use the model in training mode (some modules like dropout modules have different behaviors between training and evaluation).\n• None labels ( of shape , ) – Labels for computing the cross entropy classification loss. Indices should be in . A (if is passed or when ) or a tuple of comprising various elements depending on the configuration ( ) and inputs.\n• None loss ( of shape , , returned when is provided) – Classification (or regression if config.num_labels==1) loss.\n• None logits ( of shape ) – Classification (or regression if config.num_labels==1) scores (before SoftMax).\n• None hidden_states ( , , returned when is passed or when ) – Tuple of (one for the output of the embeddings + one for the output of each layer) of shape . Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n• None attentions ( , , returned when is passed or when ) – Tuple of (one for each layer) of shape . Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads."
    },
    {
        "link": "https://datacamp.com/tutorial/using-gpt-models-via-the-openai-api-in-python",
        "document": "Master the basics of data analysis with Python in just four hours. This online course will introduce the Python interface and explore popular packages."
    },
    {
        "link": "https://jaykmody.com/blog/gpt-from-scratch",
        "document": "In this post, we'll implement a GPT from scratch in just 60 lines of . We'll then load the trained GPT-2 model weights released by OpenAI into our implementation and generate some text.\n• This post assumes familiarity with Python, NumPy, and some basic experience with neural networks.\n• This implementation is for educational purposes, so it's missing lots of features/improvements on purpose to keep it as simple as possible while remaining complete.\n• All the code for this blog post can be found at github.com/jaymody/picoGPT.\n\nEDIT (Feb 9th, 2023): Added a \"What's Next\" section and updated the intro with some notes.\n\n EDIT (Feb 28th, 2023): Added some additional sections to \"What's Next\".\n\nGPT stands for Generative Pre-trained Transformer. It's a type of neural network architecture based on the Transformer. Jay Alammar's How GPT3 Works is an excellent introduction to GPTs at a high level, but here's the tl;dr:\n• Pre-trained: A GPT is trained on lots of text from books, the internet, etc ...\n\nLarge Language Models (LLMs) like OpenAI's GPT-3 are just GPTs under the hood. What makes them special is they happen to be 1) very big (billions of parameters) and 2) trained on lots of data (hundreds of gigabytes of text).\n\nFundamentally, a GPT generates text given a prompt. Even with this very simple API (input = text, output = text), a well-trained GPT can do some pretty awesome stuff like write your emails, summarize a book, give you instagram caption ideas, explain black holes to a 5 year old, code in SQL, and even write your will.\n\nSo that's a high-level overview of GPTs and their capabilities. Let's dig into some more specifics.\n\nThe function signature for a GPT looks roughly like this:\n\nThe input is some text represented by a sequence of integers that map to tokens in the text:\n\nTokens are sub-pieces of the text, which are produced using some kind of tokenizer. We can map tokens to integers using a vocabulary:\n• We use a tokenizer to break it down into smaller pieces called tokens.\n• We use a vocabulary to map those tokens to integers.\n\nIn practice, we use more advanced methods of tokenization than simply splitting by whitespace, such as Byte-Pair Encoding or WordPiece, but the principle is the same:\n• There is a that maps string tokens to integer indices\n• There is an method that converts\n• There is a method that converts\n\nThe output is a 2D array, where is the model's predicted probability that the token at is the next token . For example:\n\nTo get a next token prediction for the whole sequence, we simply take the token with the highest probability in :\n\nTaking the token with the highest probability as our prediction is known as greedy decoding or greedy sampling.\n\nThe task of predicting the next logical word in a sequence is called language modeling. As such, we can call a GPT a language model.\n\nGenerating a single word is cool and all, but what about entire sentences, paragraphs, etc ...?\n\nWe can generate full sentences by iteratively getting the next token prediction from our model. At each iteration, we append the predicted token back into the input:\n\nThis process of predicting a future value (regression), and adding it back into the input (auto), is why you might see a GPT described as autoregressive.\n\nWe can introduce some stochasticity (randomness) to our generations by sampling from the probability distribution instead of being greedy:\n\nThis allows us to generate different sentences given the same input. When combined with techniques like top-k, top-p, and temperature, which modify the distribution prior to sampling, the quality of our outputs is greatly increased. These techniques also introduce some hyperparameters that we can play around with to get different generation behaviors (for example, increasing temperature makes our model take more risks and thus be more \"creative\").\n\nWe train a GPT like any other neural network, using gradient descent with respect to some loss function. In the case of a GPT, we take the cross entropy loss over the language modeling task:\n\nThis is a heavily simplified training setup, but it illustrates the point. Notice the addition of to our function signature (we left this out in the previous sections for simplicity). During each iteration of the training loop:\n• We compute the language modeling loss for the given input text example\n• The loss determines our gradients, which we compute via backpropagation\n• We use the gradients to update our model parameters such that the loss is minimized (gradient descent)\n\nNotice, we don't use explicitly labelled data. Instead, we are able to produce the input/label pairs from just the raw text itself. This is referred to as self-supervised learning.\n\nSelf-supervision enables us to massively scale training data. Just get our hands on as much raw text as possible and throw it at the model. For example, GPT-3 was trained on 300 billion tokens of text from the internet and books:\n\nOf course, you need a sufficiently large model to be able to learn from all this data, which is why GPT-3 has 175 billion parameters and probably cost between $1m-10m in compute cost to train.\n\nThis self-supervised training step is called pre-training, since we can reuse the \"pre-trained\" models weights to further train the model on downstream tasks, such as classifying if a tweet is toxic or not. Pre-trained models are also sometimes called foundation models.\n\nTraining the model on downstream tasks is called fine-tuning, since the model weights have already been pre-trained to understand language, it's just being fine-tuned to the specific task at hand.\n\nThe \"pre-training on a general task + fine-tuning on a specific task\" strategy is called transfer learning.\n\nIn principle, the original GPT paper was only about the benefits of pre-training a transformer model for transfer learning. The paper showed that pre-training a 117M GPT achieved state-of-the-art performance on various NLP (natural language processing) tasks when fine-tuned on labelled datasets.\n\nIt wasn't until the GPT-2 and GPT-3 papers that we realized a GPT model pre-trained on enough data with enough parameters was capable of performing any arbitrary task by itself, no fine-tuning needed. Just prompt the model, perform autoregressive language modeling, and voila, the model magically gives us an appropriate response. This is referred to as in-context learning, because the model is using just the context of the prompt to perform the task. In-context learning can be zero shot, one shot, or few shot:\n\nGenerating text given a prompt is also sometimes referred to as conditional generation, since our model is generating some output conditioned on some input.\n\nGPTs are not limited to NLP tasks. You can condition the model on anything you want. For example, you can turn a GPT into a chatbot (i.e. ChatGPT) by conditioning it on the conversation history. You can also further condition the chatbot to behave a certain way by prepending the prompt with some kind of description (i.e. \"You are a chatbot. Be polite, speak in full sentences, don't say harmful things, etc ...\"). Conditioning the model like this can even give your chatbot a persona. This is often referred to as a system prompt. However, this is not robust, you can still \"jailbreak\" the model and make it misbehave.\n\nWith that out of the way, let's finally get to the actual implementation.\n\nClone the repository for this tutorial:\n\nNote: This code was tested with .\n\nA quick breakdown of each of the files:\n• contains the code for OpenAI's BPE Tokenizer, taken straight from their gpt-2 repo.\n• contains the code to download and load the GPT-2 model weights, tokenizer, and hyperparameters.\n• contains the actual GPT model and generation code, which we can run as a python script.\n• is the same as , but in even fewer lines of code. Why? Because why not.\n\nWe'll be reimplementing from scratch, so let's delete it and recreate it as an empty file:\n\nAs a starting point, paste the following code into :\n\nBreaking down each of the 4 sections:\n• The function is the actual GPT code we'll be implementing. You'll notice that the function signature includes some extra stuff in addition to :\n• , , , and are the parameters of our model.\n• is a hyperparameter that is needed during the forward pass.\n• The function is the autoregressive decoding algorithm we saw earlier. We use greedy sampling for simplicity. is a progress bar to help us visualize the decoding process as it generates tokens one at a time.\n• The function handles:\n• Encoding the input prompt into token IDs using the tokenizer\n• just turns our file into a CLI application, so we can eventually run our code with:\n\nLet's take a closer look at , , and , in a notebook, or an interactive python session, run:\n\nThis will download the necessary model and tokenizer files to and load , , and into our code.\n\nis the BPE tokenizer used by GPT-2:\n\nUsing the vocabulary of the tokenizer (stored in ), we can take a peek at what the actual tokens look like:\n\nNotice, sometimes our tokens are words (e.g. ), sometimes they are words but with a space in front of them (e.g. , the represents a space), sometimes there are part of a word (e.g. capes is split into and ), and sometimes they are punctuation (e.g. ).\n\nOne nice thing about BPE is that it can encode any arbitrary string. If it encounters something that is not present in the vocabulary, it just breaks it down into substrings it does understand:\n\nWe can also check the size of the vocabulary:\n\nThe vocabulary, as well as the byte-pair merges which determines how strings are broken down, is obtained by training the tokenizer. When we load the tokenizer, we're loading the already trained vocab and byte-pair merges from some files, which were downloaded alongside the model files when we ran . See (the vocabulary) and (byte-pair merges).\n\nis a dictionary that contains the hyper-parameters of our model:\n\nWe'll use these symbols in our code's comments to show the underlying shape of things. We'll also use to denote the length of our input sequence (i.e. ).\n\nis a nested json dictionary that hold the trained weights of our model. The leaf nodes of the json are NumPy arrays. If we print , replacing the arrays with their shapes, we get:\n\nThese are loaded from the original OpenAI tensorflow checkpoint:\n\nThe following code converts the above tensorflow variables into our dictionary.\n\nFor reference, here's the shapes of but with the numbers replaced by the they represent:\n\nYou'll probably want to come back to reference this dictionary to check the shape of the weights as we implement our GPT. We'll match the variable names in our code with the keys of this dictionary for consistency.\n\nLast thing before we get into the actual GPT architecture itself, let's implement some of the more basic neural network layers that are non-specific to GPTs.\n\nThe non-linearity (activation function) of choice for GPT-2 is GELU (Gaussian Error Linear Units), an alternative for ReLU:\n\nIt is approximated by the following function:\n\nLike ReLU, GELU operates element-wise on the input:\n\nWe use the trick for numerical stability.\n\nSoftmax is used to a convert set of real numbers (between \\(-\\infty\\) and \\(\\infty\\)) to probabilities (between 0 and 1, with the numbers all summing to 1). We apply over the last axis of the input.\n\nLayer normalization standardizes values to have a mean of 0 and a variance of 1:\n\n\\[ \\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2}} + \\beta \\]where \\(\\mu\\) is the mean of \\(x\\), \\(\\sigma^2\\) is the variance of \\(x\\), and \\(\\gamma\\) and \\(\\beta\\) are learnable parameters.\n\nLayer normalization ensures that the inputs for each layer are always within a consistent range, which is supposed to speed up and stabilize the training process. Like Batch Normalization, the normalized output is then scaled and offset with two learnable vectors gamma and beta. The small epsilon term in the denominator is used to avoid a division by zero error.\n\nLayer norm is used instead of batch norm in the transformer for various reasons. The differences between various normalization techniques is outlined in this excellent blog post.\n\nWe apply layer normalization over the last axis of the input.\n\nLinear layers are often referred to as projections (since they are projecting from one vector space to another vector space).\n\nThe GPT architecture follows that of the transformer:\n\nBut uses only the decoder stack (the right part of the diagram):\n\nNote, the middle \"cross-attention\" layer is also removed since we got rid of the encoder.\n\nAt a high level, the GPT architecture has three sections:\n\nIn code, it looks like this:\n\nLet's break down each of these three sections into more detail.\n\nToken IDs by themselves are not very good representations for a neural network. For one, the relative magnitudes of the token IDs falsely communicate information (for example, if and in our vocab, then we are implying that ). Secondly, a single number is not a lot of dimensionality for a neural network to work with.\n\nTo address these limitations, we'll take advantage of word vectors, specifically via a learned embedding matrix:\n\nRecall, is a matrix. It acts as a lookup table, where the \\(i\\)th row in the matrix corresponds to the learned vector for the \\(i\\)th token in our vocabulary. uses integer array indexing to retrieve the vectors corresponding to each token in our input.\n\nLike any other parameter in our network, is learned. That is, it is randomly initialized at the start of training and then updated via gradient descent.\n\nOne quirk of the transformer architecture is that it doesn't take into account position. That is, if we randomly shuffled our input and then accordingly unshuffled the output, the output would be the same as if we never shuffled the input in the first place (the ordering of inputs doesn't have any effect on the output).\n\nOf course, the ordering of words is a crucial part of language (duh), so we need some way to encode positional information into our inputs. For this, we can just use another learned embedding matrix:\n\nRecall, is a matrix. The \\(i\\)th row of the matrix contains a vector that encodes information about the \\(i\\)th position in the input. Similar to , this matrix is learned during gradient descent.\n\nNotice, this restricts our model to a maximum sequence length of . That is, must hold.\n\nWe can add our token and positional embeddings to get a combined embedding that encodes both token and positional information.\n\nThis is where all the magic happens and the \"deep\" in deep learning comes in. We pass our embedding through a stack of transformer decoder blocks.\n\nStacking more layers is what allows us to control how deep our network is. GPT-3 for example, has a whopping 96 layers. On the other hand, choosing a larger value allows us to control how wide our network is (for example, GPT-3 uses an embedding size of 12288).\n\nIn our final step, we project the output of the final transformer block to a probability distribution over our vocab:\n• We first pass through a final layer normalization layer before doing the projection to vocab. This is specific to the GPT-2 architecture (this is not present in the original GPT and Transformer papers).\n• We are reusing the embedding matrix for the projection. Other GPT implementations may choose to use a separate learned weight matrix for the projection, however sharing the embedding matrix has a couple of advantages:\n• You save some parameters (although at GPT-3 scale, this is negligible).\n• Since the matrix is both responsible for mapping both to words and from words, in theory, it may learn a richer representation compared to having two separate matrixes.\n• We don't apply at the end, so our outputs will be logits instead of probabilities between 0 and 1. This is done for several reasons:\n• is monotonic, so for greedy sampling is equivalent to making redundant\n• is irreversible, meaning we can always go from to by applying , but we can't go back to from , so for maximum flexibility, we output the\n• Numerically stability (for example, to compute cross entropy loss, taking is numerically unstable compared to\n\nThe projection to vocab step is also sometimes called the language modeling head. What does \"head\" mean? Once your GPT is pre-trained, you can swap out the language modeling head with some other kind of projection, like a classification head for fine-tuning the model on some classification task. So your model can have multiple heads, kind of like a hydra.\n\nSo that's the GPT architecture at a high level, let's actually dig a bit deeper into what the decoder blocks are doing.\n\nThe transformer decoder block consists of two sublayers:\n\nEach sublayer utilizes layer normalization on their inputs as well as a residual connection (i.e. add the input of the sublayer to the output of the sublayer).\n• Multi-head causal self attention is what facilitates the communication between the inputs. Nowhere else in the network does the model allow inputs to \"see\" each other. The embeddings, position-wise feed forward network, layer norms, and projection to vocab all operate on our inputs position-wise. Modeling relationships between inputs is tasked solely to attention.\n• The Position-wise feed forward neural network is just a regular 2 layer fully connected neural network. This just adds a bunch of learnable parameters for our model to work with to facilitate learning.\n• In the original transformer paper, layer norm is placed on the output while we place layer norm on the input to match GPT-2. This is referred to as pre-norm and has been shown to be important in improving the performance of the transformer.\n• Residual connections (popularized by ResNet) serve a couple of different purposes:\n• Makes it easier to optimize neural networks that are deep (i.e. networks that have lots of layers). The idea here is that we are providing \"shortcuts\" for the gradients to flow back through the network, making it easier to optimize the earlier layers in the network.\n• Without residual connections, deeper models see a degradation in performance when adding more layers (possibly because it's hard for the gradients to flow all the way back through a deep network without losing information). Residual connections seem to give a bit of an accuracy boost for deeper networks.\n• Can help with the vanishing/exploding gradients problem.\n\nLet's dig a little deeper into the 2 sublayers.\n\nThis is just a simple multi-layer perceptron with 2 layers:\n\nNothing super fancy here, we just project from up to a higher dimension and then back down to .\n\nRecall, from our dictionary, that our params look like this:\n\nThis layer is probably the most difficult part of the transformer to understand. So let's work our way up to \"Multi-Head Causal Self Attention\" by breaking each word down into its own section:\n\nI have another blog post on this topic, where we derive the scaled dot product equation proposed in the original transformer paper from the ground up:\n\n \\[\\text{attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\\]As such, I'm going to skip an explanation for attention in this post. You can also reference Lilian Weng's Attention? Attention! and Jay Alammar's The Illustrated Transformer which are also great explanations for attention.\n\nWe'll just adapt our attention implementation from my blog post:\n\nWhen , , and all come from the same source, we are performing self-attention (i.e. letting our input sequence attend to itself):\n\nFor example, if our input is \"Jay went to the store, he bought 10 apples.\" , we would be letting the word \"he\" attend to all the other words, including \"Jay\", meaning the model can learn to recognize that \"he\" is referring to \"Jay\".\n\nWe can enhance self attention by introducing projections for , , and the attention output:\n\nThis enables our model to learn a mapping for , , and that best helps attention distinguish relationships between inputs.\n\nWe can reduce the number of matrix multiplication from 4 to just 2 if we combine , and into a single matrix , perform the projection, and then split the result:\n\nThis is a bit more efficient as modern accelerators (GPUs) can take better advantage of one large matrix multiplication rather than 3 separate small ones happening sequentially.\n\nFinally, we add bias vectors to match the implementation of GPT-2, use our function, and rename our parameters to match our dictionary:\n\nRecall, from our dictionary, our params look like this:\n\nThere is a bit of an issue with our current self-attention setup, our inputs can see into the future! For example, if our input is , during self attention we are allowing \"wear\" to see \"capes\". This means our output probabilities for \"wear\" will be biased since the model already knows the correct answer is \"capes\". This is no good since our model will just learn that the correct answer for input \\(i\\) can be taken from input \\(i+1\\).\n\nTo prevent this, we need to somehow modify our attention matrix to hide or mask our inputs from being able to see into the future. For example, let's pretend our attention matrix looks like this:\n\nEach row corresponds to a query and the columns to a key. In this case, looking at the row for \"wear\", you can see that it is attending to \"capes\" in the last column with a weight of 0.295. To prevent this, we want to set that entry to :\n\nIn general, to prevent all the queries in our input from looking into the future, we set all positions \\(i, j\\) where \\(j > i\\) to :\n\nWe call this masking. One issue with our above masking approach is our rows no longer sum to 1 (since we are setting them to 0 after the has been applied). To make sure our rows still sum to 1, we need to modify our attention matrix before the is applied.\n\nThis can be achieved by setting entries that are to be masked to \\(-\\infty\\) prior to the :\n\nwhere is the matrix (for ):\n\nWe use instead of as can cause .\n\nAdding to our attention matrix instead of just explicitly setting the values to works because practically, any number plus is just .\n\nWe can compute the matrix in NumPy with .\n\nPutting it all together, we get:\n\nWe can further improve our implementation by performing separate attention computations, splitting our queries, keys, and values into heads:\n\nThere are three steps added here:\n• Merge the outputs of each head:\n\nNotice, this reduces the dimension from to for each attention computation. This is a tradeoff. For reduced dimensionality, our model gets additional subspaces to work when modeling relationships via attention. For example, maybe one attention head is responsible for connecting pronouns to the person the pronoun is referencing. Maybe another might be responsible for grouping sentences by periods. Another could simply be identifying which words are entities, and which are not. Although, it's probably just another neural network black box.\n\nThe code we wrote performs the attention computations over each head sequentially in a loop (one at a time), which is not very efficient. In practice, you'd want to do these in parallel. For simplicity, we'll just leave this sequential.\n\nWith that, we're finally done our GPT implementation! Now, all that's left to do is put it all together and run our code.\n\nPutting it All Together\n\nPutting everything together, we get gpt2.py, which in its entirety is a mere 120 lines of code (60 lines if you remove comments and whitespace).\n\nWe can test our implementation with:\n\nwhich gives the output:\n\nWe can test that our implementation gives identical results to OpenAI's official GPT-2 repo using the following Dockerfile (Note: this won't work on M1 Macbooks because of tensorflow shenanigans and also warning, it downloads all 4 GPT-2 model sizes, which is a lot of GBs of stuff to download):\n\nwhich should give an identical result:\n\nThis implementation is cool and all, but it's missing a ton of bells and whistles:\n\nThat's it. You can now use the code with GPUs and even TPUs! Just make sure you install JAX correctly.\n\nAgain, if we replace NumPy with JAX:\n\nThen computing the gradients is as easy as:\n\nOnce again, if we replace NumPy with JAX :\n\nThen, making our function batched is as easy as:\n\nOur implementation is quite inefficient. The quickest and most impactful optimization you can make (outside of GPU + batching support) would be to implement a kv cache. Also, we implemented our attention head computations sequentially, when we should really be doing it in parallel .\n\nThere's many many more inference optimizations. I recommend Lillian Weng's Large Transformer Model Inference Optimization and Kipply's Transformer Inference Arithmetic as a starting point.\n\nTraining a GPT is pretty standard for a neural network (gradient descent w.r.t a loss function). Of course, you also need to use the standard bag of tricks when training a GPT (i.e. use the Adam optimizer, find the optimal learning rate, regularization via dropout and/or weight decay, use a learning rate scheduler, use the correct weight initialization, batching, etc ...).\n\nThe real secret sauce to training a good GPT model is the ability to scale the data and the model, which is where the real challenge is.\n\nFor scaling data, you'll want a corpus of text that is big, high quality, and diverse.\n• Big means billions of tokens (terabytes of data). For example, check out The Pile, which is an open source pre-training dataset for large language models.\n• High quality means you want to filter out duplicate examples, unformatted text, incoherent text, garbage text, etc ...\n• Diverse means varying sequence lengths, about lots of different topics, from different sources, with differing perspectives, etc ... Of course, if there are any biases in the data, it will reflect in the model, so you need to be careful of that as well.\n\nScaling the model to billions of parameters involves a cr*p ton of engineering (and money lol). Training frameworks can get absurdly long and complex. A good place to start would be Lillian Weng's How to Train Really Large Models on Many GPUs. On the topic there's also the NVIDIA's Megatron Framework, Cohere's Training Framework, Google's PALM, the open source mesh-transformer-jax (used to train EleutherAI's open source models), and many many more.\n\nOh boy, how does one even evaluate LLMs? Honestly, it's really hard problem. HELM is pretty comprehensive and a good place to start, but you should always be skeptical of benchmarks and evaluation metrics.\n\nI recommend taking a look at Phil Wang's X-Transformer's. It has the latest and greatest research on the transformer architecture. This paper is also a pretty good summary (see Table 1). Facebook's recent LLaMA paper is also probably a good reference for standard architecture improvements (as of February 2023).\n\nOur current implementation requires us to specify the exact number of tokens we'd like to generate ahead of time. This is not a very good approach as our generations end up being too long, too short, or cutoff mid-sentence.\n\nTo resolve this, we can introduce a special end of sentence (EOS) token. During pre-training, we append the EOS token to the end of our input (i.e. ). During generation, we simply stop whenever we encounter the EOS token (or if we hit some maximum sequence length):\n\nGPT-2 was not pre-trained with an EOS token, so we can't use this approach in our code, but most LLMs nowadays use an EOS token.\n\nWe briefly touched on fine-tuning in the training section. Recall, fine-tuning is when we re-use the pre-trained weights to train the model on some downstream task. We call this process transfer-learning.\n\nIn theory, we could use zero-shot or few-shot prompting to get the model to complete our task, however, if you have access to a labelled dataset, fine-tuning a GPT is going to yield better results (results that can scale given additional data and higher quality data).\n\nThere are a couple different topics related to fine-tuning, I've broken them down below:\n\nIn classification fine-tuning, we give the model some text and we ask it to predict which class it belongs to. For example, consider the IMDB dataset, which contains movie reviews that rate the movie as either good, or bad:\n\nTo fine-tune our model, we replace the language modeling head with a classification head, which we apply to the last token output:\n\nWe only use the last token output because we only need to produce a single probability distribution for the entire input instead of distributions as in the case of language modeling. We take the last token in particular (instead of say the first token or a combination of all the tokens) because the last token is the only token that is allowed to attend to the entire sequence and thus has information about the input text as a whole.\n\nAs per usual, we optimize w.r.t. the cross entropy loss:\n\nSome tasks can't be neatly categorized into classes. For example, consider the task of summarization. We can fine-tune these types of task by simply performing language modeling on the input concatenated with the label. For example, here's what a single summarization training sample might look like:\n\nWe train the model as we do during pre-training (optimize w.r.t language modeling loss).\n\nAt predict time, we feed the model the everything up to and then perform auto-regressive language modeling to generate the summary.\n\nThe choice of the delimiters and are arbitrary. How you choose to format the text is up to you, as long as it is consistent between training and inference.\n\nNotice, we can also formulate classification tasks as generative tasks (for example with IMDB):\n\nHowever, this will probably perform worse than doing classification fine-tuning directly (loss includes language modeling on the entire sequence, not just the final prediction, so the loss specific to the prediction will get diluted)\n\nMost state-of-the-art large language models these days also undergo an additional instruction fine-tuning step after being pre-trained. In this step, the model is fine-tuned (generative) on thousands of instruction prompt + completion pairs that were human labeled. Instruction fine-tuning can also be referred to as supervised fine-tuning, since the data is human labelled (i.e. supervised).\n\nSo what's the benefit of instruction fine-tuning? While predicting the next word in a wikipedia article makes the model is good at continuing sentences, it doesn't make it particularly good at following instructions, or having a conversation, or summarizing a document (all the things we would like a GPT to do). Fine-tuning them on human labelled instruction + completion pairs is a way to teach the model how it can be more useful, and make them easier to interact with. This call this AI alignment, as we are aligning the model to do and behave as we want it to. Alignment is an active area of research, and includes more than just following instructions (bias, safety, intent, etc ...).\n\nWhat does this instruction data look like exactly? Google's FLAN models were trained on various academic NLP datasets (which are already human labelled):\n\nOpenAI's InstructGPT on the other hand was trained on prompts collected from their own API. They then paid workers to write completions for those prompts. Here's a breakdown of the data:\n\nWhen we talk about fine-tuning in the above sections, it is assumed that we are updating all of the model parameters. While this yields the best performance, it is costly both in terms of compute (need to back propagate over the entire model) and in terms of storage (for each fine-tuned model, you need to store a completely new copy of the parameters). For instruction fine-tuning, this is fine, we want maximum performance, but if you then wanted to fine-tune 100 different models for various downstream tasks, then you'd have a problem.\n\nThe most simple approach to this problem is to only update the head and freeze (i.e. make untrainable) the rest of the model. This would speed up training and greatly reduce the number of new parameters, however it would not perform nearly as well as a full fine-tune (we are lacking the deep in deep learning). We could instead selectively freeze specific layers (i.e. freeze all layers except the last 4, or freeze every other layer, or freeze all parameters except multi-head attention parameters), which would help restore some of the depth. This will perform a lot better, but we become a lot less parameter efficient and reduce our training speed ups.\n\nInstead, we can utilize parameter-efficient fine-tuning (PEFT) methods. PEFT is active area of research, and there are lots of different methods to choose from.\n\nAs an example, take the Adapters paper. In this approach, we add an additional \"adapter\" layer after the FFN and MHA layers in the transformer block. The adapter layer is just a simple 2 layer fully connected neural network, where the input and output dimensions are , and the hidden dimension is smaller than :\n\nThe size of the hidden dimension is a hyper-parameter that we can set, enabling us to tradeoff parameters for performance. For a BERT model, the paper showed that using this approach can reduce the number of trained parameters to 2% while only sustaining a small hit in performance (<1%) when compared to a full fine-tune."
    },
    {
        "link": "https://github.com/FoundationVision/VAR",
        "document": "\n• 2024-12: 🔥 We Release our Text-to-Image research based on VAR, please check Infinity.\n\nWe provide a demo website for you to play with VAR models and generate images interactively. Enjoy the fun of visual autoregressive modeling!\n\nWe provide a demo website for you to play with VAR Text-to-Image and generate images interactively. Enjoy the fun of visual autoregressive modeling!\n\nWe also provide demo_sample.ipynb for you to see more technical details about VAR.\n\nVisual Autoregressive Modeling (VAR) redefines the autoregressive learning on images as coarse-to-fine \"next-scale prediction\" or \"next-resolution prediction\", diverging from the standard raster-scan \"next-token prediction\".\n\nWe provide VAR models for you to play with, which are on or can be downloaded from the following links:\n\nYou can load these models to generate images via the codes in demo_sample.ipynb. Note: you need to download vae_ch160v4096z32.pth first.\n• assume the ImageNet is in `/path/to/imagenet`. It should be like this: NOTE: The arg should be passed to the training script.\n• (Optional) install and compile and for faster attention computation. Our code will automatically use them if installed. See models/basic_var.py#L15-L30.\n\nTo train VAR-{d16, d20, d24, d30, d36-s} on ImageNet 256x256 or 512x512, you can run the following command:\n\nA folder named will be created to save the checkpoints and logs. You can monitor the training process by checking the logs in and , or using .\n\nIf your experiment is interrupted, just rerun the command, and the training will automatically resume from the last checkpoint in (see utils/misc.py#L344-L357).\n\nFor FID evaluation, use to sample 50,000 images (50 per class) and save them as PNG (not JPEG) files in a folder. Pack them into a file via in utils/misc.py#L344. Then use the OpenAI's FID evaluation toolkit and reference ground truth npz file of 256x256 or 512x512 to evaluate FID, IS, precision, and recall.\n\nNote a relatively small is used for trade-off between image quality and diversity. You can adjust it to , or sample with for better visual quality. We'll provide the sampling script later.\n\nIn this pargraph, we cross link third-party repositories or research which use VAR and report results. You can let us know by raising an issue\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\nIf our work assists your research, feel free to give us a star ⭐ or cite us using:"
    },
    {
        "link": "https://reddit.com/r/MLQuestions/comments/18rlrt0/why_dont_autoregressive_gpt_models_have_the_same",
        "document": "GPT is trained by giving it the first token in a sequence, having it predict the next, then giving it the first two and predicting the third, etc., right? It's an autoregressive model.\n\nIsn't that the main drawback with RNNs? Why doesn't GPT have the same drawback? When GPT generates text is take its previous output as an input...\n\nEDIT: Okay I got my answer from GPT-4 haha. For GPT training you put in the sequence all at once and use backwards time attention (each token only attends to previous tokens). Then you get a whole sequence of output vectors. From each vector element of the sequence you predict the next token in the sequence. This is all done in parallel in one step, and you can even parallelize across sequences. RNNs can batch independent sequences together but they can't compute the sequence all in one pass."
    },
    {
        "link": "https://stackoverflow.com/questions/52799113/iterating-over-tokens-within-lists-within-lists-using-for-loops-in-python-spacy",
        "document": "I'm relatively new, so I might be making some really basic mistake, but from what I understand, you would iterate over tokens within a list-within-a-list in python as follows:\n\nHowever, when using SpaCy, it seems like the first for-loop is iterating over the tokens rather than the lists.\n\nactually prints each word that is over 5 letters rather than each sentence which is longer than 5 words (which it should be doing if it was really on the \"first level\" for-loop.\n\nreturns the error: \"TypeError: 'spacy.tokens.token.Token' object is not iterable.\"\n\nThe reason I want to do this in the nested for-loops is that I want newalice to be a list of lists (I still want to be able to iterate over the sentences, I just wanted to get rid of words I don't care about).\n\nI don't know if I'm making some really basic error in my code, or if SpaCy is doing something weird, but either way I'd really appreciate any help on how to iterate over items in a list-in-a-list in SpaCy while keeping the integrity of the original lists."
    },
    {
        "link": "https://stackoverflow.com/questions/52668183/how-to-use-append-to-store-values-within-a-loop-in-python",
        "document": "I'm defining a function ( ) that contains a for loop whose result is a random number ( ). So, if the loop runs 10 times, for example, it will generate 10 different numbers. I would like to store those numbers in a list within the loop that I could then print to see what numbers were generated.\n\nI thought of using append, though I don't know how to do this. This is my code so far, though the print statement doesn't work (I get an error saying I didn't use append correctly)."
    },
    {
        "link": "https://clemsonciti.github.io/rcde_workshops/pytorch_llm/02-small_language_model.html",
        "document": "This notebook was partly inspired by this blog post on character-level bigram models: https://medium.com/@fareedkhandev/create-gpt-from-scratch-using-python-part-1-bd89ccf6206a\n\nIn this notebook, we take a first look at the language modeling task. “Language Modeling” has two parts:\n• None “Language” is what it sounds like. For our purposes, we will always represent language with text. We will also talk about\n• None : pieces of text. These could be words, word chunks, or individual characters.\n• None : a sequence of tokens about something. These could be individual tweets, legal contracts, love letters, emails, or journal abstracts.\n• None : a collection of documents. We will be using a PubMed dataset containing 50 thousand abstracts.\n• None : the set of all unique tokens in our dataset.\n• None “Modeling” refers to creating a mathematical structure that, in some way, corresponds to observed data. In this case, the data is language, so the model should quantiatively capture something about the nature of language. We need to make this more concrete. Let’s try to make the idea of mathematically modeling language more concrete. We will develop models for the vector of tokens that appear in a document. We denote this as $\\( p(\\langle w_i\\rangle_{i=1}^{L}) \\)\\( where \\)w_i\\( is the token at position \\)i\\( in a document and \\)L$ is the number of words in the document. The angle bracket with limits notation here denotes the vector of all tokens specified by the limits. If we knew this joint distribution, we could sample new documents \\(d\\): $\\( d \\sim p(\\langle w_i\\rangle_{i=1}^{L}) \\)\\( This is called _language generation_ because \\)d\\( is not in the dataset that we used to learn \\)p(\\langle w_i\\rangle_{i=1}^{L})$, but it “looks like” it is from that dataset. Let’s make a simplifying assumption. Let’s assume that the probability for token \\(i\\) only depends on the previous tokens as shown in this figure (Notice: no arrows going from right to left.) Mathematically, this can be expressed as: $\\( p(w_i | \\langle w_j\\rangle_{j\n\neq i}) = p(w_i | \\langle w_j\\rangle_{j=1}^{i-1}) \\)\\( This gives us a natural way to sample documents because it implies that \\)\\( p(\\langle w_i\\rangle_{i=1}^{L}) = p(w_1)\\prod_{j=2}^L p(w_j | \\langle w_i\\rangle_{i=1}^{j-1}) \\)$ So, to generate a new document, we can\n• None sample the next token conditioned on the prompt and append it to the prompt\n• None sample the next token conditioned on the appended prompt and append it to the appended prompt This is how ChatGPT works! This approach goes by the names or . This is not how all language modeling works. BERT, for instance, uses masked language modeling, where random tokens in a sequence are sampled by considering the tokens at all other positions. Word2Vec models tokens using a neighborhood of nearby tokens. Also, we still haven’t said anything about how you actually write down the functional form of \\(p(w_i | \\langle w_j\\rangle_{j=1}^{i-1})\\). There are many possible architectures (an incomplete list in approximate historical ordering): We will spend the next notebook digging deep into the last option. Before we do, though, let’s try to get a better understanding of language models by looking closely at a simple Markov model.\n\nBefore we move on to attention, transformers, and LLMs, let’s first write down and fit a very simple language model for the PubMed dataset. This will provide a baseline for more sophisticated techniques and will give us a better understanding of how autoregressive language modeling works. Most of the lessons learned will transfer directly to the LLM case. The simplest, non-trivial model comes from assuming that the distribution for token \\(i\\) only depends on token \\(i-1\\). Graphically: With this Markov assumption, the conditional distribution for token \\(i\\) simplifies to $\\( p(w_i | \\langle w_j\\rangle_{j=1}^{i-1}) = p(w_i | w_{i-1}) \\)$ The probability distribution for the entire sequence is then $\\( p(\\langle w_i\\rangle_{i=1}^{L}) = p(w_{1})\\prod_{i=2}^{L}p(w_{i}|{w}_{i-1}) \\)$ allowing us to generate sequences as described above. In what ways might this be an inadequate model for human language? How can we estimate this model mathematically? We start by observing that the model only depends on a set of probabilities describing the likelihood of one word given another word. These probabilities are called transition matrix elements, $\\( T_{\\alpha\\beta} = p(w_i=\\alpha | w_{i-1}=\\beta)\\\\ \\)\\( where the matrix elements satisfy \\)\\( T_{\\alpha\\beta} \\geq 0 \\\\ \\sum_\\alpha T_{\\alpha\\beta} =1 \\)\\( where \\)\\alpha\\( and \\)\\beta\\( are two tokens in our vocabulary. If the vocab size is \\)V\\(, the estimation task comes down to inferring the \\)V\\times V$ transition matrix elements describing the probability of going from any word to any other word. One straightforward way to estimate these probabilities would be to list all of the neighbor pairs of tokens in our dataset and for each conditioning token \\(\\beta\\) look at the share into each choice of \\(\\alpha\\). This can be made to work, though we would have to deal with the fact that many token pairs will never appear. In the code below, we will take a different approach. We will estimate the probabilities using a maximum likelihood based approach with gradient descent. For the Markov model, the two approaches are formally equivalent up to how they deal with the missing pairs. However, the gradient descent approach will generalize to more complicated models including transformer-based LLMs!\n\nMake sure you have the dataset.py in your working directory. For the Markov model, we need to know the size of our vocabulary. Yikes, that’s a big vocabulary! The size of the transition matrix will be . Let’s estimate how much memory that would take to store: # memory needed to store the transition matrix (in gigabytes) That’s huge, but let’s just try it anyway. Let’s write down our pytorch model. Just a little notation first:\n• None \\(L\\), \\(L_\\mathrm{batch}\\): The document sequence length or the sequence length of the batch, respectively.\n• None \\(V\\): the size of our vocab Without further ado, let’s write down the model: # nn.Embedding is just a matrix. Each input token will have a learnable # parameter vector with one element for each output token. # the transition matrix elements are computed by taking the softmax along the output dimension # let's start with the assumption that most transitions are very improbable # turns out we never actually need to compute the softmax for MLE Let’s try it on some actual data to make sure it works. The output tensor has shape . We interpret these outputs as the logits of the next word. The probability of the next word is then # check that the total probability over possible next tokens sums to 1: # idx is (N, L) array of indices in the current context # trim last time step. It is prediction for token after end of sequence Let’s use our model to generate some sequences! \"We compared the prevalence of\" \"We compared the prevalence of\" \"We compared the prevalence of\" # trim off unwanted [SEP] tokens which act like our special end-of-sequence token. If we’re to improve it, we need an objective to optimize. Remember, our goal is to learn good values for the transition matrix elements. We will do this by minimizing the cross entropy loss for next token prediction. This loss measures how likely the actual next tokens are under the predicted probability distribution over tokens. It turns out, we never actually have to use the next token probabilities. This is because cross entropy only depends on log probabilities. So, rather than take exponentials of the logits, only to take the log again while computing cross entropy, we just stick with logits. Pytorch’s built-in cross entropy loss function expects this. # remember what our batch of inputs abstracts looks like: # cut the last prediction off because this corresponds to a token after the last token in the input sequence # cut the first word off the targets because we can't predict the distribution for the first word from the autoregressive model Make sure these shapes make sense to you! Use the built in pytorch function to compute cross entropy for each position in the sequence # pytorch expects the intput to have shape `sequence_length x batch_size x vocab_size` This is the loss for each token. But remember, some of those tokens are just padding to make the batch tensor rectangular. We shouldn’t count those. We can use the data structure output by our dataset to take care of this. # need to trim the first because our predictions are for tokens 2 through the end. We need to zero out the loss coming from the padding tokens and compute the average loss only counting the non-padding tokens. Let’s put all of this logic into a function. # let's put all this together in a custom loss function - logits: The next token prediction logits. Last element removed. Shape (N, V, L-1) - targets: Ids of the correct next tokens. First element removed (N, L-1)\n\nThis is boilerplate pytorch optimization code, so we will zip over it. Pytorch’s documentation has a useful Beginner’s guide, here. The learning seems to have generalized well. # generate some more samples now that we've trained the model The model is still terrible, though it has started to learn some very basic patterns.\n\nWith all the setup in place, it’s easy to start experimenting with different models. We saw how huge the embedding matrix was and we worried that this would lead to bad performance. One way to get around this is to create a low-rank version of the markov model. # We project down to size `embed_dim` then back up to `vocab_size` # the total number of parameters is 2 * vocab_size * embed_dim which # can be much smaller than embed_dim * embed_dim # zero out some of the embedding vector elements randomly to prevent overfitting # let's start with the assumption that most transitions are very improbable # turns out we never actually need to compute the softmax for MLE # test how well the model generalizes: The cross entropy is just a little worse. Let’s see about the generated samples: # generate some more samples for the low-rank model Still pretty terrible – maybe a bit worse than the full-rank model. But much more parameter efficieint. Can you think of other ways to improve the model?"
    },
    {
        "link": "https://medium.com/@javaid.nabi/all-you-need-to-know-about-llm-text-generation-03b138e0ed19",
        "document": "All You Need To Know About LLM Text Generation\n\nThe impressive capability of LLMs to comprehend and draw inferences from context and generate human-like relevant responses for diverse tasks such as translation, summarization, question answering, and creative endeavors like poetry or code generation has left people amazed worldwide.\n\nBehind all this technological revolution is Transformer architecture and in particular Autoregressive Transformer model. This model generates tokens (kind of sub-words) based on the input prompt and the previous sequence of the output tokens it has generated so far.\n\nLanguage models are pre-trained to predict the next token in a text corpus. How to choose the next token based on the probability distribution over a fixed vocabulary, which is where decoding strategies come into play. The process of selecting output tokens to generate text is known as decoding and the good thing is that you can customize the decoding and thus generate the text as per your needs. Based on the decoding method the next token can simply be picking the most probable token or considering multiple top candidates etc. By effective decoding strategy, LLM output can transform from just a next-token predictor to a useful text generator for various tasks. Thus, it is important to understand ‘What are different decoding methods or strategies? and How do they impact the LLM output generation?’.\n\nBut before we discuss it further, let us start with basic text generation sample code from the Huggingface library using the GPT2 model:\n\nGiven an input text “Once upon a time”, the sample code generates an output sequence length for T=10:\n\nOnce upon a time, the world was a place of great beauty and\n\nThe logic of generating text is in the function .If we run the code above multiple times, it produces the same result [* more about it later]. Let us try to understand the auto-regressive mechanism for text generation along with various sampling techniques under the hood of API.\n\nA typical auto-regressive generation algorithm is defined as follows:\n\nWe can represent the API by implementing the basic blocks of autoregressive algorithm as:\n\nThere are 3 main steps in the loop:\n• — Getting output probability distribution over the vocabulary list\n• Appending the output token to the input for auto-regression\n\nThis is the autoregressive text generation mechanism. That is all we need to create our custom API.\n\nNow, it is time to talk about , ‘sampling’ method that is used to choose the next token. What should be the best sampling mechanism? Is there a way to generate more coherent and varied text?\n\nThe simplest way to implement is to choose the next token with the highest probability at each time step. This is also called Greedy Search. This straightforward approach of choosing the highest probability token makes it fast and efficient but can lead to repetitive or predictable text. So if you are looking for creative results this approach is least likely to work for you.\n\nIn terms of code, our function can be written as:\n\nIf we use the above sampling function, then every time we run the code, we get the same output. As mentioned earlier [*running the Huggingface code exhibits similar behavior]. The reason is by default API implements greedy search. Can we do better? Yes.\n\nBeam Search (BS) — Instead of the next token with the highest probability, BS maintains a beam of the K most probable sequences at each time step, where the K is referred to as the beam width. This procedure is repeated until a predefined maximum length is reached or an end-of-sequence token appears. This method produces better quality text, depending on the beam size, but can be slower due to more computations than greedy search.\n\nWe start with the given sequence “Once upon a time”, for beam width K= 2, the next likely two tokens are “a” and “the”. Next iteration, we have two sequences (“a”, “cat”) with a probability of 0.20 (0.5*0.4) and a sequence (“the”, “people”) with a higher probability of 0.21 (0.3*0.7). So beam search can choose the higher probability sequence “the people” as the generated sequence. If we increase the beam width K, the algorithm can explore more sequences thus producing better-quality text but at the cost of computation. So there is a tradeoff between the two.\n\nThe above methods that choose the most probable next token at each step are called Deterministic methods. These methods produce output text ensuring predictability but often at the expense of diversity. So if you are looking for creative writing, we need some better mechanisms.\n\nTo overcome the limitation of deterministic methods for generating varied and creative text, the stochastic methods introduce randomness into the selection process. These methods generate text with less predictability and hence non-repetitive.\n\nRandom Sampling — The simplest implementation of is a random sampling from the distribution.\n\nIn the above sampling method, every time we run the code, we get a different output. The output is not deterministic anymore, but this simple approach can generate incoherent text. We need a better strategy to get varied but coherent output.\n\nTemperature Sampling — This simple method helps to increase the likelihood of high-probability words and decrease the likelihood of low-probability words by lowering the so-called of the softmax. The softmax function transforms the logits (raw outputs) of the LLM into a probability distribution across all tokens in the vocabulary.\n\nSoftmax transforms the logit zᵢ of the iᵗʰ token into a probability qᵢ by comparing zᵢ with all other logits in the vocabulary. Temperature T is normally set to 1.\n• Low Temperature T<1, makes the distribution sharper by increasing the difference between logits. Tokens with higher probabilities are augmented and tokens with lower probabilities are dampened.\n• High Temperature T>1, makes the distribution flatter by bringing the logits closer to each other. As the logits get closer, and become more equally distributed, more tokens get a chance of being selected. For creative text generation output, a higher temperature is preferred.\n\nLet's visualize the impact of the temperature on the softmax probability distribution. Consider an example of five tokens with logit values of [1,2,3,4,5]. The following code snippet generates the temperature-controlled softmax probability distribution for each token.\n\nIn the figure below, at Temperature = 1, we have the original probability distribution of the tokens. As temperature reduces (<1), the probability gets sharper towards ‘Token 4’ with the highest probability while for the rest of the tokens it is close to zero. This makes ‘Token 4’ to be the most likely next token. Whereas, when temperature increases (>1), the probability distribution of all tokens gets flattened. Due to this, there is more likelihood of any of the tokens getting chosen as the next token prediction."
    },
    {
        "link": "https://vgel.me/posts/faster-inference",
        "document": "In my last post, we made a transformer by hand. There, we used the classic autoregressive sampler, along the lines of:\n\nThis approach to inference is elegant and cuts to the heart of how LLMs work—they're autoregressive, consuming their own output. And for our toy model with merely thousands of parameters, it worked completely fine. Unfortunately, for real models it's far too slow . Why is that, and how can we make it faster?\n\nThis post is a long and wide-ranging survey of a bunch of different ways to make LLMs go brrrr, from better hardware utilization to clever decoding tricks. It's not completely exhaustive, and isn't the most in-depth treatment of every topic—I'm not an expert on all these things! But hopefully you'll find the information here a useful jumping off point to learn more about the topics you're interested in. (I tried to include links to relevant papers and blog posts where applicable.)\n• Why is simple inference so slow?\n\nWhy is simple inference so slow?\n\nThere are two main reasons that inference with the plain autoregressive function is slow: an algorithmic one, and a hardware one.\n\nAlgorithmically, has to process an increasing number of tokens every cycle, because each cycle we append a new token to the context. That means to generate 100 tokens from a 10 token prompt, you don't need to run on only 109 tokens. You need to run it on 10 + 11 + 12 + 13 + ... + 109 = 5,950 tokens! (The initial prompt can be processed in parallel, which is part of why prompt tokens are usually cheaper in inference APIs.) It also means that the model slows down as it generates, since each successive token generation has a longer and longer prefix:\n\nAttention, at least vanilla attention, is also a quadratic algorithm: all tokens attend to all tokens, leading to N² scaling, making everything worse.\n\nSo that's the algorithmic reason. What's the hardware reason? Well, it's simple: LLMs are just huge. Even a relatively small model like gpt2 (117M parameters) is hundreds of megabytes, and all that data has to live in RAM. RAM is really slow, and modern processors (both CPUs and GPUs) make up for that by having lots of cache close to the processor that's faster to access . The details of this differ based on type and model of processor, but the gist is that LLM weights do not fit in cache, so a lot of time is spent waiting to load weights from RAM. This has some unintuitive effects! For example, looking at the graph above, operating on 10 tokens isn't necessarily much slower than operating on a single token, even though the activation tensors are 10x larger, because the main time sink is moving the model weights around, not doing calculations!\n\nAs a sidebar, what do we mean exactly when we say slow? There's a whole zoo of metrics people talk about when it comes to LLM inference:\n• Time to First Token—how long is there between receiving the prompt and returning the first token?\n• Generation Latency—how long is there between receiving the prompt and returning the final token?\n• Throughput—how many distinct generations can we pass through the pipeline at once?\n• Hardware Utilization—how efficiently are we using the compute, memory bandwidth, and other capabilities of the hardware?\n\nDifferent optimizations affect these metrics differently. For example, batching improves throughput and better utilizes the hardware, but can increase and generation latency.\n\nA straightforward way to speed up inference (especially if you're VC funded :-)) is to just buy better hardware, or if you can't afford that, to take better advantage of the hardware you have.\n\nIf you're buying better hardware, most likely that would be some sort of accelerator—usually a GPU, or sometimes/if you're Google, a .\n\nUsing an accelerator can produce dramatic speedups (hence the name), but keep in mind that there's a transfer bottleneck between the CPU and the accelerator. If your model doesn't fit in the accelerator's memory, it will need to be swapped out throughout the forward pass, which can slow things down dramatically. (This is one of the reasons Apple's M1/M2/M3 chips punch above their weight for inference—they have unified CPU and GPU memory.)\n\nAnother thing to keep in mind with both CPU and accelerator inference is whether you're taking full advantage of the hardware—a properly optimized program can squeeze more out of weaker hardware than a poorly optimized one can get out of the best hardware.\n\nFor example, you could write attention in PyTorch as , which will give you correct results. But if you instead use , it will delegate the calculation to FlashAttention when available, which can produce 3x speedups using a handwritten kernel that better takes advantage of cache.\n\nA more general version of this is compilers like , TinyGrad, and ONNX, which can fuse naive Python code into kernels optimized for your hardware. For example, I could write the following function:\n• A linear scan of to calculate for each element\n• Another memory allocation of for\n• A linear scan of to calculate for each element\n• A memory allocation of for the result tensor\n• A linear scan of and to add them into the result\n\nEach of these things is slow, and some of the steps require jumping the boundary between Python and native code, which doesn't help. So what if I compile this function using ?\n\nIf I go into that debug trace directory and open the file there, has generated an optimized C++ kernel for my CPU that fuses into a single kernel. (If I had run this with a GPU available, would have generated a CUDA kernel for the GPU instead.)\n\nNow, the steps are:\n• A memory allocation of for the result tensor\n• A linear scan of ( ) to calculate and and add them together into the result\n\nMuch simpler and faster for large inputs!\n\nNote that specialized the code above for the specific size of tensor we passed in ( ). If we passed in tensors of many different sizes, would instead generate code generic over the size, but having a constant size can enable the compiler to generate better code in some cases (e.g. via loop unrolling or better vectorization).\n\nHere's another function where does something surprising:\n\nThis function has data-dependent control flow, meaning we do something different based on the runtime value of a variable. If we compile this in the same way we compiled , we get two graphs (and thus two debug directories):\n\nThe first kernel implements the and parts of the function:\n\nAnd the second kernel implements the branch, since this is the branch that was taken with the example input:\n\nThis is called a graph break, and it's not good! The compiled function is slower due to it, since we have to leave the optimized kernel and return to Python to evaluate the branch. On top of that, the other branch ( ) hasn't been compiled yet, since it hasn't been taken! That means it will be compiled on the fly when needed, which could be bad if it happens at an inopportune time (such as in the middle of serving a user request).\n\nTools like are a great way to optimize your code to get better performance out of your hardware, without dipping down to CUDA to write kernels the old-fashioned way.\n\nIn the unoptimized version of , we pass the model a single sequence at once, and at each step ask it to append a token:\n\nTo batch generation, we instead pass the model multiple sequences at once, generating a completion for each in the same forward pass. This requires the sequences to be padded on either the left or right with filler tokens to equal length. The padding tokens (which can be anything, I'm using [end] here) are masked in the attention mask so that they don't influence generation.\n\nBecause batching sequences in this way allows the model weights to be used for multiple sequences at once, running the entire batch of sequences together takes less time than running each sequence separately. For example, on my machine, using GPT-2 to generate a next token for:\n\nNotice how in the example above, \"Mark is quick. He moves quickly.\" finished before the other sequences, but because the batch as a whole wasn't done, we were forced to continue generating tokens for it (\"Random\"). This isn't a problem for correctness—we can simply clip the generated sequence to the token—but it is unfortunate, since GPU resources are being used to generate tokens we will just throw away.\n\nContinuous batching fixes this by inserting new sequences into the batch as other sequences complete, after their tokens. Instead of generating random tokens after the token, a new sequence can be inserted in that row of the batch, with attention masking to prevent the sequence from being influenced by the tokens from the previous sequence in the row. (Essentially, the prior sequence acts like additional padding.)\n\nFloating point numbers come in different sizes, and that matters for performance. Most of the time for regular software (e.g., Javascript numbers and Python floats), we use 64 bit (double precision) IEEE 754 floating point. Most ML, however, has traditionally used 32 bit (single precision) IEEE 754:\n\nModels train and infer fine with fp32, and this saves 4 bytes (50%) per parameter, which is huge—a 7B parameter model would take up 56Gb in fp64, and only 28 Gb in fp32. Remember that large amounts of time during training and inference are spent moving data from RAM to cache and registers—the less data there is to move, the better. So while fp32 is better than fp64, can we do even better?\n\nfp16, or half precision, is the obvious next step—another 50% savings! You have two main options here: fp16, and bfloat16 (short for brain float, since it was developed by Google Brain), which has better range but worse hardware support.\n\nIt's easiest to see the distinction with a diagram showing the size of each field:\n\nWhen reducing the fields of a fp32, fp16 and bfloat16 made different tradeoffs: fp16 tried to balance between range and precision by shrinking both the exponent and fraction fields, whereas bfloat16 preserved the range of fp32 by keeping an 8-bit exponent, while sacrificing precision by shrinking the fraction field smaller than fp16. The loss of range can sometimes be a problem for training in fp16, but for inference either works, and fp16 is probably a better choice if your GPU doesn't support bfloat16.\n\nCan we go even smaller? Of course!\n\nOne approach is to quantize a model trained in a larger format, like fp16. The llama.cpp project (and the associated ML library ggml) defines a whole zoo of quantization formats (the README is currently out of date, so make sure to check the k-quants PR as well), which can go down to less than 5 bits per weight from an fp32 or fp16 model.\n\nThese quantizations work a bit differently than fp16 / bfloat16—there isn't enough room to fit a whole number in that space, so instead the weights are quantized in blocks, where an fp16 acts as the block scale, and then the block of quantized weights are each multiplied against that scale. (In some formats, there's also a min value, and sometimes the scale and min are themselves quantized to still be smaller than fp16—it's complicated! See the k-quants PR for more details about how it's implemented in GGML, and this post for more details about why quantization is challenging.)\n\nbitsandbytes also implements quantization for non-llama.cpp projects. (I don't have much experience with it personally, though, besides dealing with it as a transitive dependency when it doesn't want to install on Lambda Labs instances :-))\n\nHowever, the smaller you go with quantization of a model trained with wider parameters, the more it can start to affect the model's performance, reducing the quality of responses. It's best to go with the least amount of quantization that will give you acceptable inference speed.\n\nHowever, it's also possible to finetune or train models with datatypes smaller than fp16. For example, you can train quantized low rank adapters with qLoRA, and a 2022 paper demonstrated training 175B parameter language models in (simulated) fp8, achieving very similar results to fp16.\n\nNote that, as of 2023, GPUs don't natively support datatypes smaller than fp16, except int8 (8 bit integer). You can train and infer with int8 to some extent, but most quantization requires converting the weights from the quantized format to another type (like fp16) for calculation, and then back when they're no longer needed, which incurs some performance cost. This can pay for itself based on how much memory your GPU has and how fast that memory is, but it's worth being aware of—quantization isn't free.\n\nTo explain this one, I'm going to borrow some diagrams from my last post about how Transformers work. If this section feels too quick, please read that post for a (much) more in depth explanation! This explanation is also based on GPT-2, since it's the model I covered in that post. Other architectures work slightly differently—I'll explain the relevant differences, but most don't make too much of a difference for understanding KV caching.\n\nInside a Transformer, the activations run through a feed-forward layer to generate a matrix, where each row corresponds to a token:\n\nThen, the matrix is split into , , and , which are combined with attention like this:\n\nTo produce a matrix like this:\n\nNow depending on where this layer is in the Transformer, these rows might be used (after passing through an MLP) as the input to the next Transformer block, or be the predictions for the next token—but note that there's a row for every token! That's because Transformers are trained to predict the next token for every single token in the context window!\n\nDuring training, this behavior is desirable—it means more information is flowing into the Transformer since many tokens are being graded instead of just one. But usually during inference, all we care about is that bottom row, the prediction for the final token.\n\nSo how can we get just that out of a Transformer trained to predict the entire context? Well, let's go back to the attention calculation. What if was only one row—the row corresponding to the last token?\n\nThen, we'd get this as the attention result—just the result for the last token, exactly like what we want.\n\nSo that's great, but to only generate the last row of , that means we can only run the layer that generates the matrix on a single row as well. So where do the rest of the rows of and come from, since we still need them? The answer is in the name—KV caching—we reuse them from the previous token generation step! Inside the model, we save the KV values calculated during attention in each Transformer block. Then on the next generation, only a single token will be passed in, and the cached KV rows will be stacked on top of the K and V row for the new token to produce the single row Q and multi-row K and V that we want.\n\nHere's an example of KV caching with the HuggingFace API, which actually returns the KV cache by default as part of the model forward pass. The cache is a tuple with a tuple for each layer. The and tensors are each of shape .\n\nIf we pass this returned KV cache to a model forward pass, the model will treat the tokens we passed in to generate the cache as present even though we don't provide them again. Note that we only pass a single token here, and only get a single row of logits back!\n\nCompare that to if we only pass the single token without passing —we get a completion, but it's not conditioned on those previous tokens that the KV cache was generated from.\n\nKV caching helps with the algorithmic side of LLM slowness—since we're now only passing in a single token on each step, we don't have to redo everything for each new token. However, it doesn't completely banish the problem, since the KV cache still grows in size each step, slowing down the attention calculation. The size of the KV cache can also pose its own, new problem—for example, with a 1,000 token KV cache, even with the smallest GPT-2 there are 18,432,000 values being cached. If each is an fp32, that's almost 74MB of cache, for a single generation, for a comparatively tiny model! With modern large models, especially running on a server that needs to handle many simultaneous clients, the KV cache can quickly become unmanageable, so a few techniques have popped up to make it better.\n\nMulti-Query attention is a change to the model architecture that shrinks the size of the KV cache by assigning multiple heads to Q, and only a single head to K and V. It needs to be trained into the model from the beginning—it's not just an inference-time optimization—but it's worth being aware of if you're trying to choose a model, because models with MQA can support more tokens in the KV cache than models trained with normal attention. To understand that, first we need to understand multi-head attention, so let's digress into that for a second.\n\nModern LLMs don't usually perform attention on the entire QKV matrix at once like how I described above—instead, the KQV matrix is split into multiple smaller \"heads\". That means instead of how it's shown in the diagram above, it looks more like this:\n\nThen each head is combined in attention as before:\n\nThe individual small result matrices are then stuck back together to recreate a final result matrix of shape , just like the result of vanilla attention. This process allows each head to be used for a different task (e.g., one head could handle punctuation in acronyms and another French articles), instead of wastefully dedicating an entire Transformer block to a single task.\n\nSo what is Multi-Query Attention? Instead of Q, K, and V all being split into separate heads, only Q is split. K and V are smaller, the size of a single head, and that single K and V is shared among all the Q heads.\n\nYou might think this would be a serious problem for the model, but it actually has only a small effect on perplexity. This table from the MQA paper shows slightly worse results than the Multi-Head Attention baseline, but better than alternatives involving shrinking all the dimensions of MHA.\n\nThe benefit is, because K and V are so much smaller than in MHA, the KV cache is proportionally smaller as well. Both LLAMA-2 and Falcon use MQA, for this reason.\n\nMistral 7B uses a variant called Grouped-Query Attention which is a hybrid between MQA and MHA. If MHA is and MQA is , then GQA is where . GQA claims less effect on perplexity and better training stability than MQA.\n\nThe other issue with a large KV cache is that it often needs to be stored as in contiguous tensors, regardless of whether all of the cache is currently in use. That leads to multiple problems:\n• More space than necessary needs to be allocated up front, since we need to anticipate the maximum size of the KV cache before it's needed.\n• That reserved space can't be used by other requests, even if it isn't needed yet.\n• Requests with the same prefix can't share KV cache for that prefix, since they may diverge later.\n\nPagedAttention fixes these problems by taking inspiration from how operating systems handle a similar issue with userspace program memory.\n\nLet's take a moment to explore OS paging, as a primer. Like tensors, programs want to see their memory as a contiguous linear space. (If I allocate a million-byte array , I expect the address of to exactly equal , no more, no less! Much code depends on this.) However, physical memory isn't always so forgiving—operating systems have to worry about pesky things like \"fragmentation\" and \"hey you asked for a 1TiB allocation i cannot put this anywhere\".\n\nSo the operating system collaborates with hardware, using the to map virtual pages to physical pages in a page table. When you access an address in a userspace program, that address gets translated from your program's address space via the page table (and TLB cache) to a physical address before being read from or written to. Importantly, that physical address may not exist yet—it may be generated on-demand for a write. For example, let's map 16 pages of memory in C:\n\nNow if we pagemap-dump the running program, we get this list of page addresses and associated physical addresses (along with other metadata):\n\nNotice that all the pages have a physical address of zero—they don't exist yet! This is called memory overcommit. The kernel doesn't know if we're going to use these pages, so it doesn't bother to set up mappings for them yet. Trying to read from them will just return an unspecified value (this is part of why reading uninitialized memory is UB in C).\n\nHowever, if I then touch every page by writing to it...\n\n...the dump looks different!\n\nNow all the pages have a physical address, because they've been written to. However, note that the physical addresses aren't contiguous like the virtual addresses! (The largest physical address is 0x7e2662, which is the mapping for virtual address 0x7fbe0fe76000, page #13.) They're scattered all over the place, wherever they can fit. And if our C program had only touched e.g. half the pages, only those pages would've been mapped, the others would have remained unmapped. This means that no physical memory is reserved until the exact moment that it's needed.\n\nI could share only part of this memory with another process. Imagine I mapped 4 pages of shared memory:\n\nThe other process might see these pages as:\n\nThe virtual addresses are different, but the physical addresses are identical! Each program might see these pages interleaved in different contexts, but the underlying data can be stored, deduplicated, in a single place in physical memory.\n\nSo how does this apply to PagedAttention? PagedAttention is the exact same idea—they say so in the paper.\n\nInstead of pages, we have blocks of KV cache for tokens, and instead of processes accessing those pages, we have LLM requests accessing those blocks of tokens.\n\nAt startup, PagedAttention allocates a block table for the request, analogous to the role of a hardware MMU. This block table starts out empty, with no blocks mapped, just like our C process.\n\nInstead of associating requests with a large tensor of KV cache items, each request only has a comparatively small list of block indices, analogous to virtual addresses in OS paging. Those indices point at blocks stored in the global block table. Just like OS pages, they can be out of order, placed wherever they can fit:\n\nDuring the attention computation, the PagedAttention kernel walks the request's list of block indices, and goes and fetches those blocks from the global block table to compute attention as normal in the correct order.\n\nImportantly, because the blocks have been decoupled from individual requests, they can be shared, just like the shared memory example in OS paging. If two requests use the same long prefix (such as k-shot examples for multiple parallel translation tasks, the newest Twitter prompt hack, the chain so far for self-consistency Chain of Thought, etc.), the KV cache blocks for that prefix can be shared by multiple requests, simply by placing the index of that block in the appropriate part of each request's list of block indices.\n\nTo understand speculative decoding, you need to remember three things.\n\nFirst, running a small number of tokens through a model takes about the same amount of time as running a single token, thanks to memory access overhead:\n\nSecond, LLMs generate a prediction for every token in the context:\n\nFinally third, some words are very easy to predict. For example, after the word \"going\", you don't need to be GPT-4 to know that the word \"to\" is an extremely likely next token.\n\nWe can take advantage of these facts to optimize generation! Imagine if, whenever the most recent token was \" going\", we optimistically tacked \" to\" onto the end as well before running generation. Then, after running the model forward, we check if the model's prediction after \"going\" was indeed \"to\". if so, we got a token (\"to\") for free! And if not, no sweat, we simply accept the token predicted for \"going\" instead, with no increased perplexity, since that token is the exact token the model would have generated without our trick.\n\nThis will absolutely work, and buy you a (minuscule) speed boost. To improve it, you could imagine making more heuristics: predict \"and\" after a comma, and \"the\" after \"is\". You could even make multi-word heuristics: if you see \"The early bird\", why not optimistically add \"catches the worm\"? Even if they're doing a twist on the phrase, you could still win \"catches\" and \"the\", the entire phrase doesn't need to be accepted.\n\nBut wait a second—why make these heuristics by hand? We're trying to come up with likely completions of a token... that's exactly what language models are good at! If we use a language model to generate our optimistic tokens, we could pick up even more complex patterns, even ones from the earlier context.\n\nWe just need to use a \"draft\" model that's small enough (and therefore quick enough to run) that it will pay for itself by avoiding passes through the larger \"oracle\" model. A good rule of thumb is for this model to be ~1/10 the size of the oracle model. It should also use the same tokenizer (to avoid needing to detokenize and retokenize the sequence over and over).\n\nHere's what the generate loop looks like with a draft model:\n\nHere, I used the above loop with GPT-2-XL (1558M parameters) as the oracle model, and GPT-2-small (124M parameters) as the draft model, with . The green tokens were generated by the draft model, and the blue tokens are where the draft model was incorrect and the oracle model's corrected token had to be used.\n\nWhat is a tensor in machine learning?\n\nNote how the draft model is particularly good at quickly copying text from the question (\"A tensor is a\"), completing common N-grams (\"It\" -> \"is a\", \"can\" -> \"can be\"), and inserting stock phrases (\"a set of data points\"), so that the oracle model only needs to step in for a few key words.\n\nHere's another example, with the same setup but a different prompt:\n\nHere, the draft model does very well in the alphabet part, actually hitting the draft limit several times (the draft model would have correctly predicted \"L\", for example, but we limit it to 8 tokens at once). But once it got into the prose generation below, the draft model couldn't keep up as well.\n\nThe digits of Pi are 3.14159\n\nAt first the models are in agreement, but fairly quickly they diverge as the draft model becomes inaccurate, and generation becomes unbearably slow. For every token, 8 draft tokens are generated and then immediately discarded.\n\nThis shows that speculative decoding performance can be very context dependent! If the draft model is well-correlated with the oracle model and the text is easy to predict, you'll get lots of drafted tokens and fast inference. But if the models aren't correlated, speculative decoding can actually make inference slower, because you're wasting time generating draft tokens that will just be rejected.\n\nAn approach I came up with to mitigate the issues with using a fixed number of draft tokens is threshold decoding.\n\nInstead of always decoding up to the maximum number of draft tokens, we keep a moving probability threshold, calibrated based on how many tokens are being accepted right now. Draft tokens are generated until the cumulative probability of the draft so far (based on the draft model logits) falls below this threshold.\n\nFor example, if the threshold was 0.5, and the we generated a draft token \" the\" with a probability of 0.75, we'd keep going. If the next token, \" next\", had a probability of 0.5, the cumulative probability 0.375 would be lower than the threshold, so we'd stop and submit the two draft tokens to the oracle.\n\nThen, based on how far into the draft is accepted, the threshold is adjusted up or down to try and calibrate the draft model's confidence with the actual acceptance rate. Right now this is just done by a simple moving average and some thresholding, but there's probably a more principled way to do it based on real statistics.\n\nThis is the code (using my homegrown framework, apologies):\n\nThis table compares threshold decoding to regular speculative decoding with different fixed draft lengths (along with KV caching, to make the comparison fair), for the test prompts I showed earlier. The rightmost column is a sparkline showing how generation speed changes over time.\n\nNote how, for example, n_draft=16 on the \"Index: A B C\" prompt has a strong start, but falls off hard in the later prose section as it overgenerates incorrect tokens. Threshold decoding, meanwhile, is able to ramp up to take advantage of the easy alphabet tokens, and then ramp back down to not overgenerate on the harder prose section:\n\nThe first is to restructure the draft batch as a tree, instead of a single generation. This helps because longer draft batches on complex text can quickly diverge from the base model. It can instead make more sense to do multiple, shorter drafts, branching off from each other, and then verify them all against the oracle model using a specially-crafted attention mask. Generating multiple draft sequences lets you reuse prior tokens and sample the draft model in batches, further accelerating the process.\n\nThe second improvement is to speculatively decode the draft model as well—it's usually a Transformer after all. This could be a yet-smaller Transformer (they recommend 15-20x smaller than the oracle model), or even a simple N-gram model.\n\nGrammar-guided generation lets you constrain a model's output to follow some grammar, giving you output that guaranteed to match some grammar—such as JSON. At first, this seems unrelated to speed—reliability is nice, but speed too? That can't be possible! But it is—let's dig into how it works to see why.\n\nImagine you're generating JSON with an LLM, and the generation so far is:\n\nGPT-4 could generate any of 100k+ tokens here, but only a few are actually valid: whitespace, an open bracket, a quote, a digit, , etc. During regular (non-guided) generation, you'd just hope the model had learned that properly and didn't generate syntactically invalid JSON. During guided generation, however, the sampler only samples from those valid tokens, ignoring any others, even if the invalid tokens are more likely.\n\nEven better, with libraries like Outlines or jsonformer, you can give the guided generation sampler a schema, and it will sample within that schema! For example, if a key requires a digit, it will only sample digits after that key name. Note how the returned object in this example exactly matches the Pydantic schema:\n\nThis is great for reliability, but what does it have to do with speed? Well, let's take a look at that response again. If we compare it to what's required by the schema, barely any tokens are actually ambiguous—for most of the response, the model would only have one possible token to pick from. In that case, the sampler can just pick that token, bypassing the model entirely! Only a small fraction of the tokens (the ones highlighted in green) actually required a model forward call:\n\nBetween the JSON grammar and the schema, we already know the first 7 tokens are , so we can automatically add those into the context before the first model call. Then after the model generates , we know the next tokens up until the model needs to actually generate the age ( ), so we append those as well before calling the model again to generate . We can keep going that way all the way down. (We can even save a token by completing as because no other weapon starts with !)\n\nThe complete response was 41 tokens, but only 11 of them had to come from the model, the others were automatically inserted and only needed to be processed as prompt tokens, which are much faster. This is a nice speedup, and more reliable to boot—a win-win. If you need to generate structured data with LLMs, especially OSS LLMs that can use custom samplers, you should definitely be using a library like Outlines.\n\nLookahead decoding is a new approach to speculative decoding that doesn't require a draft model. Instead, the model itself is used in two branches: a lookahead branch, which predicts and extends candidate N-grams (short sequences of N tokens), and a verification branch, which verifies the candidates. The lookahead branch is similar to the draft model in regular speculative decoding, and the verification branch has the same role as the oracle model.\n\nBut unlike regular speculative decoding, this is all done not just in a single model, but in a single model call using a specially-crafted attention mask:\n\nI won't go too in depth because the lmsys blog post announcing lookahead decoding already has some nice animations (even if the explanation is somewhat dense).\n\nPrompt lookup decoding is another technique, where the draft model is replaced by simple string matching over the context. They claim it works well for tasks like code editing or RAG where the output necessarily contains lots of verbatim copying from the input. I assume it'd also work well in staged speculative decoding, to speculatively decode a draft model.\n\nThere are a few optimizations that I'm going to speed through since they're not very relevant if you don't have the resources to pretrain a model with them from the start.\n\nAttention is algorithmically slow because it's quadratic: as the sequence grows in length, each of the N tokens needs to attend to each of the N tokens. Sparse attention attempts to remedy this by calculating less attention. For example, Mistral 7B uses sliding window attention, where tokens in some layers can only attend to nearby tokens. Longformer also explored some interesting sparse attention patterns, like giving all tokens access to specific positions, dilating the sliding window, using different size windows on different layers, and other tricks. (Longformer predates Mistral, but as far as I can tell Mistral didn't use all the tricks Longformer did—I'm not sure why.)\n\nSometimes this kind of attention can be finetuned in or bolted on without tuning after the fact, but for the best performance it needs to be trained into the model from the start, like Mistral did.\n\nRecently there's been a renewed surge of interest in non-Transformer LLMs. If you're new to the field you may not be familiar with RNNs/LSTMs, but they were the dominant sequence modeling architecture before Attention is All You Need was published and Transformers took off. Unlike Transformers where the whole context is available to the model at once, RNNs do a linear scan over the sequence, building up a hidden state that models what has come before. (There are also reversed and bidirectional RNNs, it's a whole thing.)\n\nThey were outmatched by Transformers due to difficulty scaling them and problems like forgetting, but some recent papers have tried to bring them back, or invent new sub-quadratic architectures that can finally dethrone Transformers. These include RWKV (new type of RNN), Mamba (state-space model), Hyena (convolutional), and Recurrent Memory Transformers (use a Transformer for segments, then special memory tokens for global context). So far the biggest and best models are still Transformers, but that might not be true in the future!\n\nUnfortunately I didn't get to cover everything I wanted to in this post, since it's already quite long—I didn't touch on structured sparsity, Mixture of Experts, activation quantization, static vs dynamic quantization, or lots of other great topics. However, I think the post is a good survey of different areas of LLM optimization. LLMs are currently slow and hard to run, but the situation is improving all the time—Lookahead Decoding was released while I was writing this post! It seems likely that in a few years, between better hardware, better training methods, better quantization, more inference optimizations, and the continuing hard work of the open source community, we could be running models than handily beat GPT-4 on consumer hardware, and these techniques and more will be instrumental to making that happen. (Of course, GPT-5 will probably be out by then... but always upwards!)\n\nHopefully you found the post useful and/or entertaining. If you enjoyed it, you may also enjoy:\n• My other blog posts, such as I made a transformer by hand (no training!), GPT-3 will ignore tools when it disagrees with them, Does GPT-4 think better in Javascript? and I'm worried about adversarial training data\n• My other projects and writing\n• My Twitter, where I post about new blog posts, smaller AI-related thoughts (e.g. 1, 2), whatever fiction I've been reading, and other things.\n\nIf you have thoughts about this post, please feel free to get in touch! I love hearing from people who read my posts.\n\nAs always, thanks to everyone who contributed to and reviewed this post:\n• @wjessup (http://willjessup.com/) for a very thorough review of the draft.\n• @MF_FOOM for reviewing and commenting (and helping me rent some GPUs for an experiment 🫡 even though the experiment didn't work rip)\n• Everyone on Twitter who liked and commented on the various posts I made while working on this! Really helps."
    }
]