[
    {
        "link": "https://github.com/luozhouyang/python-string-similarity",
        "document": "A library implementing different string similarity and distance measures. A dozen of algorithms (including Levenshtein edit distance and sibblings, Jaro-Winkler, Longest Common Subsequence, cosine similarity etc.) are currently implemented. Check the summary table below for the complete list...\n\nThe main characteristics of each implemented algorithm are presented below. The \"cost\" column gives an estimation of the computational cost to compute the similarity between two strings of length m and n respectively.\n\n[1] In this library, Levenshtein edit distance, LCS distance and their sibblings are computed using the dynamic programming method, which has a cost O(m.n). For Levenshtein distance, the algorithm is sometimes called Wagner-Fischer algorithm (\"The string-to-string correction problem\", 1974). The original algorithm uses a matrix of size m x n to store the Levenshtein distance between string prefixes.\n\nIf the alphabet is finite, it is possible to use the method of four russians (Arlazarov et al. \"On economic construction of the transitive closure of a directed graph\", 1970) to speedup computation. This was published by Masek in 1980 (\"A Faster Algorithm Computing String Edit Distances\"). This method splits the matrix in blocks of size t x t. Each possible block is precomputed to produce a lookup table. This lookup table can then be used to compute the string similarity (or distance) in O(nm/t). Usually, t is choosen as log(m) if m > n. The resulting computation cost is thus O(mn/log(m)). This method has not been implemented (yet).\n\n[2] In \"Length of Maximal Common Subsequences\", K.S. Larsen proposed an algorithm that computes the length of LCS in time O(log(m).log(n)). But the algorithm has a memory requirement O(m.n²) and was thus not implemented here.\n\n[3] There are two variants of Damerau-Levenshtein string distance: Damerau-Levenshtein with adjacent transpositions (also sometimes called unrestricted Damerau–Levenshtein distance) and Optimal String Alignment (also sometimes called restricted edit distance). For Optimal String Alignment, no substring can be edited more than once.\n\nAlthough the topic might seem simple, a lot of different algorithms exist to measure text similarity or distance. Therefore the library defines some interfaces to categorize them.\n• StringSimilarity : Implementing algorithms define a similarity between strings (0 means strings are completely different).\n• NormalizedStringSimilarity : Implementing algorithms define a similarity between 0.0 and 1.0, like Jaro-Winkler for example.\n• StringDistance : Implementing algorithms define a distance between strings (0 means strings are identical), like Levenshtein for example. The maximum distance value depends on the algorithm.\n• NormalizedStringDistance : This interface extends StringDistance. For implementing classes, the computed distance value is between 0.0 and 1.0. NormalizedLevenshtein is an example of NormalizedStringDistance.\n\nGenerally, algorithms that implement NormalizedStringSimilarity also implement NormalizedStringDistance, and similarity = 1 - distance. But there are a few exceptions, like N-Gram similarity and distance (Kondrak)...\n\nThe MetricStringDistance interface : A few of the distances are actually metric distances, which means that verify the triangle inequality d(x, y) <= d(x,z) + d(z,y). For example, Levenshtein is a metric distance, but NormalizedLevenshtein is not.\n\nA lot of nearest-neighbor search algorithms and indexing structures rely on the triangle inequality.\n\nA few algorithms work by converting strings into sets of n-grams (sequences of n characters, also sometimes called k-shingles). The similarity or distance between the strings is then the similarity or distance between the sets.\n\nSome of them, like jaccard, consider strings as sets of shingles, and don't consider the number of occurences of each shingle. Others, like cosine similarity, work using what is sometimes called the profile of the strings, which takes into account the number of occurences of each shingle.\n\nFor these algorithms, another use case is possible when dealing with large datasets:\n• compute the set or profile representation of all the strings\n• compute the similarity between sets or profiles\n\nThe Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other.\n\nIt is a metric string distance. This implementation uses dynamic programming (Wagner–Fischer algorithm), with only 2 rows of data. The space requirement is thus O(m) and the algorithm runs in O(m.n).\n\nThis distance is computed as levenshtein distance divided by the length of the longest string. The resulting value is always in the interval [0.0 1.0] but it is not a metric anymore!\n\nThe similarity is computed as 1 - normalized distance.\n\nAn implementation of Levenshtein that allows to define different weights for different character substitutions.\n\nThis algorithm is usually used for optical character recognition (OCR) applications. For OCR, the cost of substituting P and R is lower then the cost of substituting P and M for example because because from and OCR point of view P is similar to R.\n\nIt can also be used for keyboard typing auto-correction. Here the cost of substituting E and R is lower for example because these are located next to each other on an AZERTY or QWERTY keyboard. Hence the probability that the user mistyped the characters is higher.\n\nSimilar to Levenshtein, Damerau-Levenshtein distance with transposition (also sometimes calls unrestricted Damerau-Levenshtein distance) is the minimum number of operations needed to transform one string into the other, where an operation is defined as an insertion, deletion, or substitution of a single character, or a transposition of two adjacent characters.\n\nIt does respect triangle inequality, and is thus a metric distance.\n\nThis is not to be confused with the optimal string alignment distance, which is an extension where no substring can be edited more than once.\n\nThe Optimal String Alignment variant of Damerau–Levenshtein (sometimes called the restricted edit distance) computes the number of edit operations needed to make the strings equal under the condition that no substring is edited more than once, whereas the true Damerau–Levenshtein presents no such restriction. The difference from the algorithm for Levenshtein distance is the addition of one recurrence for the transposition operations.\n\nNote that for the optimal string alignment distance, the triangle inequality does not hold and so it is not a true metric.\n\nJaro-Winkler is a string edit distance that was developed in the area of record linkage (duplicate detection) (Winkler, 1990). The Jaro–Winkler distance metric is designed and best suited for short strings such as person names, and to detect typos.\n\nJaro-Winkler computes the similarity between 2 strings, and the returned value lies in the interval [0.0, 1.0]. It is (roughly) a variation of Damerau-Levenshtein, where the substitution of 2 close characters is considered less important then the substitution of 2 characters that a far from each other.\n\nThe distance is computed as 1 - Jaro-Winkler similarity.\n\nThe longest common subsequence (LCS) problem consists in finding the longest subsequence common to two (or more) sequences. It differs from problems of finding common substrings: unlike substrings, subsequences are not required to occupy consecutive positions within the original sequences.\n\nIt is used by the diff utility, by Git for reconciling multiple changes, etc.\n\nThe LCS distance between strings X (of length n) and Y (of length m) is n + m - 2 |LCS(X, Y)| min = 0 max = n + m\n\nLCS distance is equivalent to Levenshtein distance when only insertion and deletion is allowed (no substitution), or when the cost of the substitution is the double of the cost of an insertion or deletion.\n\nThis class implements the dynamic programming approach, which has a space requirement O(m.n), and computation cost O(m.n).\n\nIn \"Length of Maximal Common Subsequences\", K.S. Larsen proposed an algorithm that computes the length of LCS in time O(log(m).log(n)). But the algorithm has a memory requirement O(m.n²) and was thus not implemented here.\n\nDistance metric based on Longest Common Subsequence, from the notes \"An LCS-based string metric\" by Daniel Bakkelund. http://heim.ifi.uio.no/~danielry/StringMetric.pdf\n\nThe distance is computed as 1 - |LCS(s1, s2)| / max(|s1|, |s2|)\n\nNormalized N-Gram distance as defined by Kondrak, \"N-Gram Similarity and Distance\", String Processing and Information Retrieval, Lecture Notes in Computer Science Volume 3772, 2005, pp 115-126.\n\nThe algorithm uses affixing with special character '\n\n' to increase the weight of first characters. The normalization is achieved by dividing the total similarity score the original length of the longest word.\n\nIn the paper, Kondrak also defines a similarity measure, which is not implemented (yet).\n\nA few algorithms work by converting strings into sets of n-grams (sequences of n characters, also sometimes called k-shingles). The similarity or distance between the strings is then the similarity or distance between the sets.\n\nThe cost for computing these similarities and distances is mainly domnitated by k-shingling (converting the strings into sequences of k characters). Therefore there are typically two use cases for these algorithms:\n\nOr, for large datasets, pre-compute the profile of all strings. The similarity can then be computed between profiles:\n\nPay attention, this only works if the same KShingling object is used to parse all input strings !\n\nQ-gram distance, as defined by Ukkonen in \"Approximate string-matching with q-grams and maximal matches\" http://www.sciencedirect.com/science/article/pii/0304397592901434\n\nThe distance between two strings is defined as the L1 norm of the difference of their profiles (the number of occurences of each n-gram): SUM( |V1_i - V2_i| ). Q-gram distance is a lower bound on Levenshtein distance, but can be computed in O(m + n), where Levenshtein requires O(m.n)\n\nThe similarity between the two strings is the cosine of the angle between these two vectors representation, and is computed as V1 . V2 / (|V1| * |V2|)\n\nLike Q-Gram distance, the input strings are first converted into sets of n-grams (sequences of n characters, also called k-shingles), but this time the cardinality of each n-gram is not taken into account. Each input string is simply a set of n-grams. The Jaccard index is then computed as |V1 inter V2| / |V1 union V2|.\n\nDistance is computed as 1 - similarity. Jaccard index is a metric distance.\n\nSimilar to Jaccard index, but this time the similarity is computed as 2 * |V1 inter V2| / (|V1| + |V2|).\n\nVery similar to Jaccard and Sorensen-Dice measures, but this time the similarity is computed as |V1 inter V2| / Min(|V1|,|V2|). Tends to yield higher similarity scores compared to the other overlapping coefficients. Always returns the highest similarity score (1) if one given string is the subset of the other.\n\nSIFT4 is a general purpose string distance algorithm inspired by JaroWinkler and Longest Common Subsequence. It was developed to produce a distance measure that matches as close as possible to the human perception of string distance. Hence it takes into account elements like character substitution, character distance, longest common subsequence etc. It was developed using experimental testing, and without theoretical background.\n\nUse java-string-similarity in your project and want it to be mentioned here? Don't hesitate to drop me a line!"
    },
    {
        "link": "https://github.com/luozhouyang/python-string-similarity/blob/master/README.md",
        "document": "A library implementing different string similarity and distance measures. A dozen of algorithms (including Levenshtein edit distance and sibblings, Jaro-Winkler, Longest Common Subsequence, cosine similarity etc.) are currently implemented. Check the summary table below for the complete list...\n\nThe main characteristics of each implemented algorithm are presented below. The \"cost\" column gives an estimation of the computational cost to compute the similarity between two strings of length m and n respectively.\n\n[1] In this library, Levenshtein edit distance, LCS distance and their sibblings are computed using the dynamic programming method, which has a cost O(m.n). For Levenshtein distance, the algorithm is sometimes called Wagner-Fischer algorithm (\"The string-to-string correction problem\", 1974). The original algorithm uses a matrix of size m x n to store the Levenshtein distance between string prefixes.\n\nIf the alphabet is finite, it is possible to use the method of four russians (Arlazarov et al. \"On economic construction of the transitive closure of a directed graph\", 1970) to speedup computation. This was published by Masek in 1980 (\"A Faster Algorithm Computing String Edit Distances\"). This method splits the matrix in blocks of size t x t. Each possible block is precomputed to produce a lookup table. This lookup table can then be used to compute the string similarity (or distance) in O(nm/t). Usually, t is choosen as log(m) if m > n. The resulting computation cost is thus O(mn/log(m)). This method has not been implemented (yet).\n\n[2] In \"Length of Maximal Common Subsequences\", K.S. Larsen proposed an algorithm that computes the length of LCS in time O(log(m).log(n)). But the algorithm has a memory requirement O(m.n²) and was thus not implemented here.\n\n[3] There are two variants of Damerau-Levenshtein string distance: Damerau-Levenshtein with adjacent transpositions (also sometimes called unrestricted Damerau–Levenshtein distance) and Optimal String Alignment (also sometimes called restricted edit distance). For Optimal String Alignment, no substring can be edited more than once.\n\nAlthough the topic might seem simple, a lot of different algorithms exist to measure text similarity or distance. Therefore the library defines some interfaces to categorize them.\n• StringSimilarity : Implementing algorithms define a similarity between strings (0 means strings are completely different).\n• NormalizedStringSimilarity : Implementing algorithms define a similarity between 0.0 and 1.0, like Jaro-Winkler for example.\n• StringDistance : Implementing algorithms define a distance between strings (0 means strings are identical), like Levenshtein for example. The maximum distance value depends on the algorithm.\n• NormalizedStringDistance : This interface extends StringDistance. For implementing classes, the computed distance value is between 0.0 and 1.0. NormalizedLevenshtein is an example of NormalizedStringDistance.\n\nGenerally, algorithms that implement NormalizedStringSimilarity also implement NormalizedStringDistance, and similarity = 1 - distance. But there are a few exceptions, like N-Gram similarity and distance (Kondrak)...\n\nThe MetricStringDistance interface : A few of the distances are actually metric distances, which means that verify the triangle inequality d(x, y) <= d(x,z) + d(z,y). For example, Levenshtein is a metric distance, but NormalizedLevenshtein is not.\n\nA lot of nearest-neighbor search algorithms and indexing structures rely on the triangle inequality.\n\nA few algorithms work by converting strings into sets of n-grams (sequences of n characters, also sometimes called k-shingles). The similarity or distance between the strings is then the similarity or distance between the sets.\n\nSome of them, like jaccard, consider strings as sets of shingles, and don't consider the number of occurences of each shingle. Others, like cosine similarity, work using what is sometimes called the profile of the strings, which takes into account the number of occurences of each shingle.\n\nFor these algorithms, another use case is possible when dealing with large datasets:\n• compute the set or profile representation of all the strings\n• compute the similarity between sets or profiles\n\nThe Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other.\n\nIt is a metric string distance. This implementation uses dynamic programming (Wagner–Fischer algorithm), with only 2 rows of data. The space requirement is thus O(m) and the algorithm runs in O(m.n).\n\nThis distance is computed as levenshtein distance divided by the length of the longest string. The resulting value is always in the interval [0.0 1.0] but it is not a metric anymore!\n\nThe similarity is computed as 1 - normalized distance.\n\nAn implementation of Levenshtein that allows to define different weights for different character substitutions.\n\nThis algorithm is usually used for optical character recognition (OCR) applications. For OCR, the cost of substituting P and R is lower then the cost of substituting P and M for example because because from and OCR point of view P is similar to R.\n\nIt can also be used for keyboard typing auto-correction. Here the cost of substituting E and R is lower for example because these are located next to each other on an AZERTY or QWERTY keyboard. Hence the probability that the user mistyped the characters is higher.\n\nSimilar to Levenshtein, Damerau-Levenshtein distance with transposition (also sometimes calls unrestricted Damerau-Levenshtein distance) is the minimum number of operations needed to transform one string into the other, where an operation is defined as an insertion, deletion, or substitution of a single character, or a transposition of two adjacent characters.\n\nIt does respect triangle inequality, and is thus a metric distance.\n\nThis is not to be confused with the optimal string alignment distance, which is an extension where no substring can be edited more than once.\n\nThe Optimal String Alignment variant of Damerau–Levenshtein (sometimes called the restricted edit distance) computes the number of edit operations needed to make the strings equal under the condition that no substring is edited more than once, whereas the true Damerau–Levenshtein presents no such restriction. The difference from the algorithm for Levenshtein distance is the addition of one recurrence for the transposition operations.\n\nNote that for the optimal string alignment distance, the triangle inequality does not hold and so it is not a true metric.\n\nJaro-Winkler is a string edit distance that was developed in the area of record linkage (duplicate detection) (Winkler, 1990). The Jaro–Winkler distance metric is designed and best suited for short strings such as person names, and to detect typos.\n\nJaro-Winkler computes the similarity between 2 strings, and the returned value lies in the interval [0.0, 1.0]. It is (roughly) a variation of Damerau-Levenshtein, where the substitution of 2 close characters is considered less important then the substitution of 2 characters that a far from each other.\n\nThe distance is computed as 1 - Jaro-Winkler similarity.\n\nThe longest common subsequence (LCS) problem consists in finding the longest subsequence common to two (or more) sequences. It differs from problems of finding common substrings: unlike substrings, subsequences are not required to occupy consecutive positions within the original sequences.\n\nIt is used by the diff utility, by Git for reconciling multiple changes, etc.\n\nThe LCS distance between strings X (of length n) and Y (of length m) is n + m - 2 |LCS(X, Y)| min = 0 max = n + m\n\nLCS distance is equivalent to Levenshtein distance when only insertion and deletion is allowed (no substitution), or when the cost of the substitution is the double of the cost of an insertion or deletion.\n\nThis class implements the dynamic programming approach, which has a space requirement O(m.n), and computation cost O(m.n).\n\nIn \"Length of Maximal Common Subsequences\", K.S. Larsen proposed an algorithm that computes the length of LCS in time O(log(m).log(n)). But the algorithm has a memory requirement O(m.n²) and was thus not implemented here.\n\nDistance metric based on Longest Common Subsequence, from the notes \"An LCS-based string metric\" by Daniel Bakkelund. http://heim.ifi.uio.no/~danielry/StringMetric.pdf\n\nThe distance is computed as 1 - |LCS(s1, s2)| / max(|s1|, |s2|)\n\nNormalized N-Gram distance as defined by Kondrak, \"N-Gram Similarity and Distance\", String Processing and Information Retrieval, Lecture Notes in Computer Science Volume 3772, 2005, pp 115-126.\n\nThe algorithm uses affixing with special character '\n\n' to increase the weight of first characters. The normalization is achieved by dividing the total similarity score the original length of the longest word.\n\nIn the paper, Kondrak also defines a similarity measure, which is not implemented (yet).\n\nA few algorithms work by converting strings into sets of n-grams (sequences of n characters, also sometimes called k-shingles). The similarity or distance between the strings is then the similarity or distance between the sets.\n\nThe cost for computing these similarities and distances is mainly domnitated by k-shingling (converting the strings into sequences of k characters). Therefore there are typically two use cases for these algorithms:\n\nOr, for large datasets, pre-compute the profile of all strings. The similarity can then be computed between profiles:\n\nPay attention, this only works if the same KShingling object is used to parse all input strings !\n\nQ-gram distance, as defined by Ukkonen in \"Approximate string-matching with q-grams and maximal matches\" http://www.sciencedirect.com/science/article/pii/0304397592901434\n\nThe distance between two strings is defined as the L1 norm of the difference of their profiles (the number of occurences of each n-gram): SUM( |V1_i - V2_i| ). Q-gram distance is a lower bound on Levenshtein distance, but can be computed in O(m + n), where Levenshtein requires O(m.n)\n\nThe similarity between the two strings is the cosine of the angle between these two vectors representation, and is computed as V1 . V2 / (|V1| * |V2|)\n\nLike Q-Gram distance, the input strings are first converted into sets of n-grams (sequences of n characters, also called k-shingles), but this time the cardinality of each n-gram is not taken into account. Each input string is simply a set of n-grams. The Jaccard index is then computed as |V1 inter V2| / |V1 union V2|.\n\nDistance is computed as 1 - similarity. Jaccard index is a metric distance.\n\nSimilar to Jaccard index, but this time the similarity is computed as 2 * |V1 inter V2| / (|V1| + |V2|).\n\nVery similar to Jaccard and Sorensen-Dice measures, but this time the similarity is computed as |V1 inter V2| / Min(|V1|,|V2|). Tends to yield higher similarity scores compared to the other overlapping coefficients. Always returns the highest similarity score (1) if one given string is the subset of the other.\n\nSIFT4 is a general purpose string distance algorithm inspired by JaroWinkler and Longest Common Subsequence. It was developed to produce a distance measure that matches as close as possible to the human perception of string distance. Hence it takes into account elements like character substitution, character distance, longest common subsequence etc. It was developed using experimental testing, and without theoretical background.\n\nUse java-string-similarity in your project and want it to be mentioned here? Don't hesitate to drop me a line!"
    },
    {
        "link": "https://newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python",
        "document": "Learn the different similarity measures and text embedding techniques. Play around with code examples and develop a general intuition.\n\nIn this article, you will learn about different similarity metrics and text embedding techniques. By the end, you'll have a good grasp of when to use what metrics and embedding techniques. You’ll also get to play around with them to help establish a general intuition.\n\nYou can find the accompanying web app here.\n\nSimilarity is the distance between two vectors where the vector dimensions represent the features of two objects. In simple terms, similarity is the measure of how different or alike two data objects are. If the distance is small, the objects are said to have a high degree of similarity and vice versa. Generally, it is measured in the range 0 to 1. This score in the range of [0, 1] is called the similarity score.\n\nAn important point to remember about similarity is that it’s subjective and highly dependent on the domain and use case. For example, two cars can be similar because of simple things like the manufacturing company, color, price range, or technical details like fuel type, wheelbase, horsepower. So, special care should be taken when calculating similarity across features that are unrelated to each other or not relevant to the problem.\n\nAs simple as the idea may be, similarity forms the basis of many machine learning techniques. For instance, the K-Nearest-Neighbors classifier uses similarity to classify new data objects, similarly, K-means clustering utilizes similarity measures to assign data points to appropriate clusters. Even recommendation engines use neighborhood-based collaborative filtering methods which use similarity to identify a user’s neighbors.\n\nThe use of similarity measures is quite prominent in the field of natural language processing. Everything from information retrieval systems, search engines, paraphrase detection to text classification, automated document linking, spell correction makes use of similarity measures.\n\nTake a look at the following sentences:\n\nAs humans, it is very obvious to us that the two sentences mean the same thing despite being written in completely different formats. But how do we make an algorithm come to that same conclusion?\n\nThe first part of this problem is representation. How do we represent the text? We could leave the text as it is or convert it into feature vectors using a suitable text embedding technique. Once we have the text representation, we can compute the similarity score using one of the many distance/similarity measures.\n\nLet’s dive deeper into the two aspects of the problem, starting with the similarity measures.\n\nJaccard index, also known as Jaccard similarity coefficient, treats the data objects like sets. It is defined as the size of the intersection of two sets divided by the size of the union. Let’s continue with our previous example:\n\nSentence 2: There is nothing in the bottle.\n\nTo calculate the similarity using Jaccard similarity, we will first perform text normalization to reduce words their roots/lemmas. There are no words to reduce in the case of our example sentences, so we can move on to the next part. Drawing a Venn diagram of the sentences we get:\n\nSize of the intersection of the two sets: 3\n\nSize of the union of the two sets: 1+3+3 = 7\n\nUsing the Jaccard index, we get a similarity score of 3/7 = 0.42\n\nTesting the function for our example sentences\n\nEuclidean distance, or L2 norm, is the most commonly used form of the Minkowski distance. Generally speaking, when people talk about distance, they refer to Euclidean distance. It uses the Pythagoras theorem to calculate the distance between two points as indicated in the figure below:\n\nThe larger the distance d between two vectors, the lower the similarity score and vice versa.\n\nLet’s compute the similarity between our example statements using Euclidean distance:\n\nTo compute the Euclidean distance we need vectors, so we’ll use spaCy’s in-built Word2Vec model to create text embeddings. (We’ll learn more about this later in the article)\n\nOkay, so we have the Euclidean distance of 1.86, but what does that mean? See, the problem with using distance is that it’s hard to make sense if there is nothing to compare to. The distances can vary from 0 to infinity, we need to use some way to normalize them to the range of 0 to 1.\n\nAlthough we have our typical normalization formula that uses mean and standard deviation, it is sensitive to outliers. That means if there are a few extremely large distances, every other distance will become smaller as a consequence of the normalization operation. So the best option here is to use something like the Euler’s constant as follows:\n\nCosine Similarity computes the similarity of two vectors as the cosine of the angle between two vectors. It determines whether two vectors are pointing in roughly the same direction. So if the angle between the vectors is 0 degrees, then the cosine similarity is 1.\n\nIt is given as:\n\nWhere ||v|| represents the length of the vector v, 𝜃 denotes the angle between v and w, and ‘.’ denotes the dot product operator.\n\nWhat Metric To Use?\n\nJaccard similarity takes into account only the set of unique words for each text document. This makes it the likely candidate for assessing the similarity of documents when repetition is not an issue. A prime example of such an application is comparing product descriptions. For instance, if a term like “HD” or “thermal efficiency” is used multiple times in one description and just once in another, the Euclidean distance and cosine similarity would drop. On the other hand, if the total number of unique words stays the same, the Jaccard similarity will remain unchanged.\n\nBoth Euclidean and cosine similarity metrics drop if an additional ‘empty’ is added to our first example sentence:\n\nThat being said, Jaccard similarity is rarely used when working with text data as it does not work with text embeddings. This means that is limited to assessing the lexical similarity of text, i.e., how similar documents are on a word level.\n\nAs far as cosine and Euclidean metrics are concerned, the differentiating factor between the two is that cosine similarity is not affected by the magnitude/length of the feature vectors. Let’s say we are creating a topic tagging algorithm. If a word (e.g. senate) occurs more frequently in document 1 than it does in document 2, we could assume that document 1 is more related to the topic of Politics. However, it could also be the case that we are working with news articles of different lengths. Then, the word ‘senate’ probably occurred more in document 1 simply because it was way longer. As we saw earlier when the word ‘empty’ was repeated, cosine similarity is less sensitive to a difference in lengths.\n\nIn addition to that, Euclidean distance doesn’t work well with the sparse vectors of text embeddings. So cosine similarity is generally preferred over Euclidean distance when working with text data. The only length-sensitive text similarity use case that comes to mind is plagiarism detection.\n\nHumans can easily understand and derive meaning from words, but computers don’t have this natural neuro-linguistic ability. To make words machine-understandable we need to encode them into a numeric form, so the computer can apply mathematical formulas and operations to make sense of them. Even beyond the task of text similarity, representing documents in the form of numbers and vectors is an active area of study.\n\nSimply put, word embedding is the vector representation of a word. They aim to capture the meaning, context, and semantic relationships of the words. A lot of the word embeddings are created based on the notion of the “distributional hypothesis” introduced by Zellig Harris: words that are used close to one another typically have the same meaning.\n\nThe most straightforward way to numerically represent words is through the one-hot encoding method. The idea is simple, create a vector with the size of the total number of unique words in the corpora. Each unique word has a unique feature and will be represented by a 1 with 0s everywhere else.\n\nDocuments contain large chunks of text with the possibility of repetition. Simply marking the presence or absence of words leads to loss of information. In the \"bag of words\" representation (also called count vectorizing), each word is represented by its count instead of 1. Regardless of that, both these approaches create huge, sparse vectors that capture absolutely no relational information.\n\nThe scikit-learn module implements this method, let’s use it to calculate the similarity of the following news headlines:\n\nTo make for better output, let’s create a function that creates a heatmap of the similarity scores.\n\nNow that we have our data and helper function, we can test countvectorizer\n\nTF-IDF vectors are an extension of the one-hot encoding model. Instead of considering the frequency of words in one document, the frequency of words across the whole corpus is taken into account. The big idea is that words that occur a lot everywhere carry very little meaning or significance. For instance, trivial words like “and”, “or”, “is” don’t carry as much significance as nouns and proper nouns that occur less frequently.\n\nMathematically, Term Frequency (TF) is the number of times a word appears in a document divided by the total number of words in the document. And Inverse Document Frequency (IDF) = log(N/n) where N is the total number of documents and n is the number of documents a term has appeared in. The TF-IDF value for a word is the product of the term frequency and the inverse document frequency.\n\nAlthough TF-IDF vectors offer a slight improvement over simple count vectorizing, they still have very high dimensionality and don’t capture semantic relationships.\n\nScikit-learn also offers a `TfidfVectorizer` class for creating TF-IDF vectors from the text.\n\nWord2Vec is a predictive method for forming word embeddings. Unlike the previous methods that need to be “trained” on the working corpus, Word2Vec is a pre-trained two-layer neural network. It takes as input the text corpus and outputs a set of feature vectors that represent words in that corpus. It uses one of two neural network-based methods:\n\nContinuous Bag Of Words takes the context of each word as the input and tries to predict the word corresponding to the context. Here, context simply means the surrounding words.\n\nFor example, consider the greeting: “Have a nice day”\n\nLet’s say we use the word ‘nice’ as the input to the neural network and we are trying to predict the word ‘day’. We will use the one-hot encoding of the input word ‘nice’, then measure and optimize for the output error of the target word ‘day’. In this process of trying to predict the target word, this shallow network learns its vector representation.\n\nJust like how this model used a single context word to predict the target, it can be extended to use multiple context words to do the same:\n\nSo CBOW generates word representations based on the context words, but there’s another way to do the same. We can use the target word, i.e., the word we want to generate the representation for, to predict the context.\n\nThat’s what Skip-gram does. In the process of predicting the context words, the Skip-gram model learns the vector representation of the target word. representations are generated using the context words.\n\nWhen To Use What?\n\nIntuitively, the CBOW task is much simpler as it is using multiple inputs to predict one target while Skip-gram relies on one-word inputs. This is reflected in the faster convergence time of CBOW, in the original paper the authors wrote that CBOW took hours to train, while Skip-gram took 3 days.\n\nContinuing on the same train of thought, CBOW is better at learning syntactic relationships between words while skip-gram is better at understanding the semantic relationships. In practical terms, this means that for a word like ‘dog’, CBOW would return morphologically similar words like plurals like ‘dogs’. On the other hand, Skip-gram would consider morphologically different but semantically similar words like ‘cat’ or ‘hamster’.\n\nAnd lastly, as Skip-gram relies on single-word input it is less sensitive to overfitting frequent words. Because even if some words appear more times during the training they considered one at a time. CBOW is prone to overfit frequent words because they can appear several times in the same set of context words. This characteristic also allows Skip-gram to be more efficient in terms amount of documents required to achieve good performance.\n\nTLDR: Skip-gram works better when working with a small amount of data, focuses on semantic similarity of words, and represents rare words well. On the other hand, CBOW is faster, focuses more on the morphological similarity of words, and needs more data to achieve similar performance.\n\nWord2Vec is used in spaCy to create word vectors, one important thing to note: In order to create word vectors we need larger spaCy models. For example, For example, the medium or large English model, but not the small one. So if we want to use vectors, we will go with a model that ends in 'md' or 'lg'. More details about this can be found here.\n\nInstall spaCy and download one of the larger models:\n\nCreate a pipeline object and use it to create to the Docs for the headlines:\n\nWe can look up the embedding vector for the `Doc` or individual tokens using the `.vector` attribute. Let’s see what the headline embeddings look like.\n\nThe result is a 300-dimensional vector of the first headline. We can use these vectors to calculate the cosine similarity of the headlines. spaCy `doc` object have their own `similarity` method that calculates the cosine similarity.\n\nSo far all the text embeddings we have covered learn a global word embedding. They first build a vocabulary of all the unique words in the document, then learn similar representations for words that appear together frequently. The issue with such word representations is that the words’ contextual meaning is ignored. In practice, this approach to word representation does not address polysemy or the coexistence of many possible meanings for a given word or phrase. For instance, consider the following statement :\n\n“After stealing gold from the bank vault, the bank robber was seen fishing on the river bank.”\n\nTraditional word embedding techniques will only learn one representation for the word ‘bank’. But ‘bank’ has two different meanings in the sentence and needs to have two different representations in the embedding space. Contextual embedding methods like BERT and ELMo learn sequence-level semantics by considering the sequence of all words in the document. As a result, these techniques learn different representations for polysemous words like ‘bank’ in the above example, based on their context.\n\nELMo computes the embeddings from the internal states of a two-layer bidirectional Language Model (LM), thus the name “ELMo”: Embeddings from Language Models. It assigns each word a representation that is a function of the entire corpus of sentences. ELMo embeddings are a function of all of the internal layers of the biLM. Different layers encode different kinds of information for the same word. For example, the lower levels work well for Part-Of-Speech tagging, while the higher levels are better at dealing with polysemous words.\n\nConcatenating the activations of all layers allows ELMo to combine a wide range of word representations that perform better on downstream tasks. In addition to that, ELMo works on the character level instead of words. This enables it to take advantage of sub-word units to derive meaningful embeddings for even out-of-vocabulary words.\n\nThis means that the way ELMo is used is quite different compared to traditional embedding methods. Instead of having a dictionary of words and their corresponding vectors, ELMo creates embeddings on the fly.\n\nThere are many implementations of ELMo, we’ll be trying out the simpe-elmo module. We’ll also need to download a pre-trained model to create the embeddings\n\nNow, let's create an ElmoModel instance and load the pre-trained model we just downloaded.\n\nThe Tensor’s second dimension of 92 corresponds to the 92 characters in the sentence. To get the word embeddings we can average the embeddings of the characters for each word. Our main concern is ELMo’s ability to extract contextual information so let’s focus only on the three instances of the word “bank”:\n\nWe can now evaluate how similar the three instances are:\n\nSo far we have discussed how word embeddings represent the meaning of the words in a text document. But sometimes we need to go a step further and encode the meaning of the whole sentence to readily understand the context in which the words are used. This sentence representation is important for many downstream tasks. It enables us to understand the meaning of the sentence without calculating the individual embeddings of the words. It also allows us to make comparisons on the sentence level.\n\nUsing simple mathematical manipulation, it is possible to adapt sentence embeddings for tasks such as semantic search, clustering, intent detection, paraphrase detection. In addition to that, cross-lingual sentence embedding models can be used for parallel text mining or translation pair detection. For example, the TAUS Data Marketplace uses a data cleaning technique that uses sentence embeddings to compute the semantic similarity between parallel segments of text in different languages to assess translation quality.\n\nA straightforward approach for creating sentence embeddings is to use a word embedding model to encode all words of the given sentence and take the average of all the word vectors. While this provides a strong baseline, it falls short of capturing information related to word order and other aspects of overall sentence semantics.\n\nThe Doc2Vec model (or Paragraph Vector) extends the idea of the Word2Vec algorithm. The algorithm follows the assumption that a word’s meaning is given by the words that appear close by. Similar to Word2Vec, Doc2Vec has two variants.\n\nEach word and sentence of the training corpus are one-hot encoded and stored in matrices D and W, respectively. The training process involves passing a sliding window over the sentence, trying to predict the next word based on the previous words and the sentence vector (or Paragraph Matrix in the figure above). This prediction of the next word is done by concatenating the sentence and word vectors and passing the result into a softmax layer. The sentence vectors change with sentences, while the word vectors remain the same. Both are updated during training.\n\nThe inference process also involves the same sliding window approach. The difference is that all the vectors of the models are fixed except the sentence vector. After all the predictions of the next word are computed for a sentence, the sentence embedding is the resultant sentence vector.\n\nThe DBOW model ignores the word order and has a simpler architecture. Each sentence in the training corpus is converted into a one-hot representation. During training, a random sentence is selected from the corpus, and from the sentence, a random number of words. The model tries to predict these words using only the sentence ID, and the sentence vector is updated(Paragraph ID and Paragraph Matrix in the figure).\n\nDuring inference, a new sentence ID is trained with random words from the sentence. The sentence vector is updated in each step, and the resulting sentence vector is the embedding for that sentence.\n\nWhat Variant To Use?\n\nThe DM model takes into account the word order, the DBOW model doesn’t. Also, the DBOW model doesn’t use word vectors so the semantics of the words are not preserved. As a result, it’s harder for it to detect similarities between words. And because of its simpler architecture, the DBOW model requires more training to obtain accurate vectors. The main drawback of the DM model is the time and the resources needed to generate an embedding, which is higher than with the DBOW model.\n\nWhat approach produces better Sentence Embeddings? In the original paper, the authors state that the DM model is “consistently better than” DBOW. However, later studies showed that the DBOW approach is better for most tasks. For that reason, the implementation in the Gensim of Doc2Vec uses the DBOW approach as the default algorithm.\n\nInstall Gensim, get the “text8” dataset to train the Doc2Vec model.\n\nTag the text data, then use it to build the model vocabulary and train the model.\n\nUse the model to get the sentence embeddings of the headlines and calculate the cosine similarity between them.\n\nMuch like ELMo, there are a few bidirectional LSTM based sentence encoders (InferSent, etc) but LSTMs have certain problems. Firstly, they use a hidden vector to represent the memory at the current state of the input. But for long input sequences such as sentences, a single vector isn't enough to provide all the information needed to predict the next state correctly. This bottleneck of the size of the hidden vector makes LSTM based methods more susceptible to mistakes, as in practice it can only hold information from a limited number of steps back. The Attention mechanism in transformers doesn’t have this bottleneck issue as it has access to all the previous hidden states for making predictions.\n\nAnother problem with LSTMs is the time to train. As the output is always dependent on the previous input, the training is done sequentially. This makes parallelization harder and results in longer training times. The Transformer architecture parallelized the use of the Attention mechanism in a neural network allowing for lower training time.\n\nTransformer-based general language understanding models perform much better than all their predecessors. When BERT was introduced, it achieved state-of-the-art results in a wide range of tasks such as question answering or language inference with minor tweaks in its architecture. That being said, it has a massive computational overhead. For example, finding the most similar pair of sentences in a collection of 10,000 requires about 50 million inference computations (~65 hours). The structure of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.\n\nSentence-BERT (SBERT) is a modified BERT network that uses siamese and triplet network structures to derive semantically meaningful sentence embeddings. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.\n\nWe’ll try out the RoBERTa based models implemented in the sentence-transformer module.\n\nUse it to calculate the headline embeddings and their pairwise similarity.\n\nGoogle’s Universal Sentence Encoder(USE) leverages a one-to-many multi-tasking learning framework to learn a universal sentence embedding by switching between several tasks. The 6 tasks, skip-thoughts prediction of the next/previous sentence, neural machine translation, constituency parsing, and natural language inference, share the same sentence embedding. This reduces training time greatly while preserving the performance on a variety of transfer tasks.\n\nOne version of the Universal Sentence Encoder model uses a deep average network (DAN) encoder, while another version uses a Transformer.\n\nThe more complicated Transformer architecture performs better than the simpler DAN model on a variety of sentiment and similarity classification tasks. The compute time for the Transformer version increases noticeably as sentence length increases. On the other hand, the compute time for the DAN model stays nearly constant as sentence length increases.\n\nWhat Embedding To Use?\n\nAs a general rule of thumb, traditional embeddings techniques like Word2Vec and Doc2Vec offer good results when the task only requires the global meaning of the text. This is reflected in their outperformance of state-of-the-art deep learning techniques on tasks such as semantic text similarity or paraphrase detection. On the other hand, when the task needs somethings more specific than just the global meaning, take for example sentiment analysis or sequence labeling, more complex contextual methods perform better.\n\nSo always start with a simple and fast method as the baseline before moving on to more complex methods if required."
    },
    {
        "link": "https://geeksforgeeks.org/python-similarity-metrics-of-strings",
        "document": "In Python, we often need to measure the similarity between two strings. For example, consider the strings “geeks” and “geeky” —we might want to know how closely they match, whether for tasks like comparing user inputs or finding duplicate entries. Let’s explore different methods to compute string similarity.\n\nSequenceMatcher class in the difflib module provides a simple way to measure string similarity based on the ratio of matching subsequences.\n• None SequenceMatcher() compares two strings and calculates the ratio of matching characters.\n• None The ratio method returns a float between 0 and 1, indicating how similar the strings are.\n• None This method is simple to use and works well for general string similarity tasks.\n\nLet’s explore some more methods and see how we can find similarity metrics of strings.\n\nLevenshtein distance measures the number of edits (insertions, deletions, or substitutions) needed to convert one string into another.\n• None It is more accurate for cases where string transformations are involved.\n• None This method is widely used in text processing and is efficient for moderate string lengths.\n\nJaccard similarity compares the common elements between two sets and calculates their ratio to the union of the sets.\n• None The strings are converted into sets of characters.\n• None The intersection and union of the sets are used to calculate the similarity ratio.\n• None This method is effective for comparing unique characters and is easy to implement.\n\nCosine similarity measures the angle between two vectors in a multidimensional space, where each string is represented as a vector of character counts.\n• None The strings are represented as frequency vectors using the\n• None The dot product and magnitudes of the vectors are used to compute the similarity.\n• None This method is useful for comparing strings with weighted character counts.\n\nHamming distance measures the number of differing characters at corresponding positions in two strings of equal length.\n• None function pairs characters from both strings for comparison.\n• None This method requires strings of equal length and is efficient for this specific task."
    },
    {
        "link": "https://docs.python.org/3/library/string.html",
        "document": "A string containing all ASCII characters that are considered whitespace. This includes the characters space, tab, linefeed, return, formfeed, and vertical tab.\n\nBy design, string.printable.isprintable() returns False . In particular, string.printable is not printable in the POSIX sense (see LC_CTYPE ).\n\nString of ASCII characters which are considered printable by Python. This is a combination of digits , ascii_letters , punctuation , and whitespace .\n\nString of ASCII characters which are considered punctuation characters in the C locale: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ .\n\nThe uppercase letters 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' . This value is not locale-dependent and will not change.\n\nThe lowercase letters 'abcdefghijklmnopqrstuvwxyz' . This value is not locale-dependent and will not change.\n\nThe concatenation of the ascii_lowercase and ascii_uppercase constants described below. This value is not locale-dependent.\n\nConverts the value (returned by get_field() ) given a conversion type (as in the tuple returned by the parse() method). The default version understands ‘s’ (str), ‘r’ (repr) and ‘a’ (ascii) conversion types.\n\nformat_field() simply calls the global format() built-in. The method is provided so that subclasses can override it.\n\nImplement checking for unused arguments if desired. The arguments to this function is the set of all argument keys that were actually referred to in the format string (integers for positional arguments, and strings for named arguments), and a reference to the args and kwargs that was passed to vformat. The set of unused args can be calculated from these parameters. check_unused_args() is assumed to raise an exception if the check fails.\n\nIf the index or keyword refers to an item that does not exist, then an IndexError or KeyError should be raised.\n\nSo for example, the field expression ‘0.name’ would cause get_value() to be called with a key argument of 0. The name attribute will be looked up after get_value() returns by calling the built-in getattr() function.\n\nFor compound field names, these functions are only called for the first component of the field name; subsequent components are handled through normal attribute and indexing operations.\n\nThe args parameter is set to the list of positional arguments to vformat() , and the kwargs parameter is set to the dictionary of keyword arguments.\n\nRetrieve a given field value. The key argument will be either an integer or a string. If it is an integer, it represents the index of the positional argument in args; if it is a string, then it represents a named argument in kwargs.\n\nGiven field_name as returned by parse() (see above), convert it to an object to be formatted. Returns a tuple (obj, used_key). The default version takes strings of the form defined in PEP 3101 , such as “0[name]” or “label.title”. args and kwargs are as passed in to vformat() . The return value used_key has the same meaning as the key parameter to get_value() .\n\nThe values in the tuple conceptually represent a span of literal text followed by a single replacement field. If there is no literal text (which can happen if two replacement fields occur consecutively), then literal_text will be a zero-length string. If there is no replacement field, then the values of field_name, format_spec and conversion will be None .\n\nLoop over the format_string and return an iterable of tuples (literal_text, field_name, format_spec, conversion). This is used by vformat() to break the string into either literal text, or replacement fields.\n\nIn addition, the Formatter defines a number of methods that are intended to be replaced by subclasses:\n\nThis function does the actual work of formatting. It is exposed as a separate function for cases where you want to pass in a predefined dictionary of arguments, rather than unpacking and repacking the dictionary as individual arguments using the *args and **kwargs syntax. vformat() does the work of breaking up the format string into character data and replacement fields. It calls the various methods described below.\n\nThe primary API method. It takes a format string and an arbitrary set of positional and keyword arguments. It is just a wrapper that calls vformat() .\n\nThe built-in string class provides the ability to do complex variable substitutions and value formatting via the format() method described in PEP 3101 . The Formatter class in the string module allows you to create and customize your own string formatting behaviors using the same implementation as the built-in format() method.\n\nThe method and the class share the same syntax for format strings (although in the case of , subclasses can define their own format string syntax). The syntax is related to that of formatted string literals, but it is less sophisticated and, in particular, does not support arbitrary expressions.\n\nFormat strings contain “replacement fields” surrounded by curly braces . Anything that is not contained in braces is considered literal text, which is copied unchanged to the output. If you need to include a brace character in the literal text, it can be escaped by doubling: and .\n\nThe grammar for a replacement field is as follows:\n\nIn less formal terms, the replacement field can start with a field_name that specifies the object whose value is to be formatted and inserted into the output instead of the replacement field. The field_name is optionally followed by a conversion field, which is preceded by an exclamation point , and a format_spec, which is preceded by a colon . These specify a non-default format for the replacement value.\n\nSee also the Format Specification Mini-Language section.\n\nThe field_name itself begins with an arg_name that is either a number or a keyword. If it’s a number, it refers to a positional argument, and if it’s a keyword, it refers to a named keyword argument. An arg_name is treated as a number if a call to on the string would return true. If the numerical arg_names in a format string are 0, 1, 2, … in sequence, they can all be omitted (not just some) and the numbers 0, 1, 2, … will be automatically inserted in that order. Because arg_name is not quote-delimited, it is not possible to specify arbitrary dictionary keys (e.g., the strings or ) within a format string. The arg_name can be followed by any number of index or attribute expressions. An expression of the form selects the named attribute using , while an expression of the form does an index lookup using .\n\nThe conversion field causes a type coercion before formatting. Normally, the job of formatting a value is done by the method of the value itself. However, in some cases it is desirable to force a type to be formatted as a string, overriding its own definition of formatting. By converting the value to a string before calling , the normal formatting logic is bypassed.\n\nThree conversion flags are currently supported: which calls on the value, which calls and which calls .\n\nThe format_spec field contains a specification of how the value should be presented, including such details as field width, alignment, padding, decimal precision and so on. Each value type can define its own “formatting mini-language” or interpretation of the format_spec.\n\nMost built-in types support a common formatting mini-language, which is described in the next section.\n\nA format_spec field can also include nested replacement fields within it. These nested replacement fields may contain a field name, conversion flag and format specification, but deeper nesting is not allowed. The replacement fields within the format_spec are substituted before the format_spec string is interpreted. This allows the formatting of a value to be dynamically specified.\n\nSee the Format examples section for some examples.\n\n“Format specifications” are used within replacement fields contained within a format string to define how individual values are presented (see Format String Syntax and f-strings). They can also be passed directly to the built-in function. Each formattable type may define how the format specification is to be interpreted. Most built-in types implement the following options for format specifications, although some of the formatting options are only supported by the numeric types. A general convention is that an empty format specification produces the same result as if you had called on the value. A non-empty format specification typically modifies the result. The general form of a standard format specifier is: If a valid align value is specified, it can be preceded by a fill character that can be any character and defaults to a space if omitted. It is not possible to use a literal curly brace (” ” or “ ”) as the fill character in a formatted string literal or when using the method. However, it is possible to insert a curly brace with a nested replacement field. This limitation doesn’t affect the function. The meaning of the various alignment options is as follows: Forces the field to be left-aligned within the available space (this is the default for most objects). Forces the field to be right-aligned within the available space (this is the default for numbers). Forces the padding to be placed after the sign (if any) but before the digits. This is used for printing fields in the form ‘+000000120’. This alignment option is only valid for numeric types, excluding . It becomes the default for numbers when ‘0’ immediately precedes the field width. Forces the field to be centered within the available space. Note that unless a minimum field width is defined, the field width will always be the same size as the data to fill it, so that the alignment option has no meaning in this case. The sign option is only valid for number types, and can be one of the following: indicates that a sign should be used for both positive as well as negative numbers. indicates that a sign should be used only for negative numbers (this is the default behavior). indicates that a leading space should be used on positive numbers, and a minus sign on negative numbers. The option coerces negative zero floating-point values to positive zero after rounding to the format precision. This option is only valid for floating-point presentation types. Changed in version 3.11: Added the option (see also PEP 682). The option causes the “alternate form” to be used for the conversion. The alternate form is defined differently for different types. This option is only valid for integer, float and complex types. For integers, when binary, octal, or hexadecimal output is used, this option adds the respective prefix , , , or to the output value. For float and complex the alternate form causes the result of the conversion to always contain a decimal-point character, even if no digits follow it. Normally, a decimal-point character appears in the result of these conversions only if a digit follows it. In addition, for and conversions, trailing zeros are not removed from the result. The option signals the use of a comma for a thousands separator for floating-point presentation types and for integer presentation type . For other presentation types, this option is an error. For a locale aware separator, use the integer presentation type instead. Changed in version 3.1: Added the option (see also PEP 378). The option signals the use of an underscore for a thousands separator for floating-point presentation types and for integer presentation type . For integer presentation types , , , and , underscores will be inserted every 4 digits. For other presentation types, specifying this option is an error. Changed in version 3.6: Added the option (see also PEP 515). width is a decimal integer defining the minimum total field width, including any prefixes, separators, and other formatting characters. If not specified, then the field width will be determined by the content. When no explicit alignment is given, preceding the width field by a zero ( ) character enables sign-aware zero-padding for numeric types, excluding . This is equivalent to a fill character of with an alignment type of . Changed in version 3.10: Preceding the width field by no longer affects the default alignment for strings. The precision is a decimal integer indicating how many digits should be displayed after the decimal point for presentation types and , or before and after the decimal point for presentation types or . For string presentation types the field indicates the maximum field size - in other words, how many characters will be used from the field content. The precision is not allowed for integer presentation types. Finally, the type determines how the data should be presented. The available string presentation types are: String format. This is the default type for strings and may be omitted. The available integer presentation types are: Character. Converts the integer to the corresponding unicode character before printing. Hex format. Outputs the number in base 16, using lower-case letters for the digits above 9. Hex format. Outputs the number in base 16, using upper-case letters for the digits above 9. In case is specified, the prefix will be upper-cased to as well. Number. This is the same as , except that it uses the current locale setting to insert the appropriate number separator characters. In addition to the above presentation types, integers can be formatted with the floating-point presentation types listed below (except and ). When doing so, is used to convert the integer to a floating-point number before formatting. The available presentation types for and values are: Scientific notation. For a given precision , formats the number in scientific notation with the letter ‘e’ separating the coefficient from the exponent. The coefficient has one digit before and digits after the decimal point, for a total of significant digits. With no precision given, uses a precision of digits after the decimal point for , and shows all coefficient digits for . If , the decimal point is omitted unless the option is used. Scientific notation. Same as except it uses an upper case ‘E’ as the separator character. Fixed-point notation. For a given precision , formats the number as a decimal number with exactly digits following the decimal point. With no precision given, uses a precision of digits after the decimal point for , and uses a precision large enough to show all coefficient digits for . If , the decimal point is omitted unless the option is used. Fixed-point notation. Same as , but converts to and to . General format. For a given precision , this rounds the number to significant digits and then formats the result in either fixed-point format or in scientific notation, depending on its magnitude. A precision of is treated as equivalent to a precision of . The precise rules are as follows: suppose that the result formatted with presentation type and precision would have exponent . Then, if , where is -4 for floats and -6 for , the number is formatted with presentation type and precision . Otherwise, the number is formatted with presentation type and precision . In both cases insignificant trailing zeros are removed from the significand, and the decimal point is also removed if there are no remaining digits following it, unless the option is used. With no precision given, uses a precision of significant digits for . For , the coefficient of the result is formed from the coefficient digits of the value; scientific notation is used for values smaller than in absolute value and values where the place value of the least significant digit is larger than 1, and fixed-point notation is used otherwise. Positive and negative infinity, positive and negative zero, and nans, are formatted as , , , and respectively, regardless of the precision. General format. Same as except switches to if the number gets too large. The representations of infinity and NaN are uppercased, too. Number. This is the same as , except that it uses the current locale setting to insert the appropriate number separator characters. Percentage. Multiplies the number by 100 and displays in fixed ( ) format, followed by a percent sign. For this is like the type, except that when fixed-point notation is used to format the result, it always includes at least one digit past the decimal point, and switches to the scientific notation when . When the precision is not specified, the latter will be as large as needed to represent the given value faithfully. For , this is the same as either or depending on the value of for the current decimal context. The overall effect is to match the output of as altered by the other format modifiers. The result should be correctly rounded to a given precision of digits after the decimal point. The rounding mode for matches that of the builtin. For , the rounding mode of the current context will be used. The available presentation types for are the same as those for ( is not allowed). Both the real and imaginary components of a complex number are formatted as floating-point numbers, according to the specified presentation type. They are separated by the mandatory sign of the imaginary part, the latter being terminated by a suffix. If the presentation type is missing, the result will match the output of (complex numbers with a non-zero real part are also surrounded by parentheses), possibly altered by other format modifiers.\n\nThis section contains examples of the syntax and comparison with the old -formatting. In most of the cases the syntax is similar to the old -formatting, with the addition of the and with used instead of . For example, can be translated to . The new format syntax also supports new and different options, shown in the following examples. is formed from the real part 'The complex number (3-5j) is formed from the real part 3.0 and the imaginary part -5.0.' Aligning the text and specifying a width: Replacing , , and and specifying a sign: # show only the minus -- same as '{:f}; {:f}' Replacing and and converting the value to different bases: # with 0x, 0o, or 0b as prefix: Using the comma as a thousands separator:"
    },
    {
        "link": "https://geeksforgeeks.org/edit-distance-dp-5",
        "document": "Given two strings s1 and s2 and below operations that can be performed on s1. The task is to find the minimum number of edits (operations) to convert ‘s1‘ into ‘s2‘.\n• Insert : Insert any character before or after any index of s1\n• Replace: Replace a character at any index of s1 with some other character.\n\nNote: All of the above operations are of equal cost.\n\nThe idea is to process all characters one by one starting from either from left or right sides of both strings. \n\nLet us process from the right end of the strings, there are two possibilities for every pair of characters being traversed, either they match or they don’t match. If last characters of both string matches then we simply recursively calculate the answer for rest of part of the strings. When last characters do not match, we perform all three operations to match the last characters, i.e. insert, replace, and remove. And then recursively calculate the result for the remaining part of the string. Upon completion of these operations, we will select the minimum answer and add 1 to it. Below is the recursive tree for this problem considering the case when the last characters never match. When the last characters of strings matches. Make a recursive call editDistance(m-1, n-1) to calculate the answer for remaining part of the strings. When the last characters of strings don’t matches. Make three recursive calls as show below:\n• None Insert s2[n-1] at last of s1 : editDistance(m, n-1)\n\n// of operations to convert s1 to s2 // If first string is empty, the only option is to // insert all characters of second string into first // If second string is empty, the only option is to // remove all characters of first string // If last characters of two strings are same, nothing // much to do. Get the count for // If last characters are not same, consider all three // operations on last character of first string, // recursively compute minimum cost for all three // operations and take minimum of three values. // Function to find minimum of three numbers // of operations to convert s1 to s2 // If first string is empty, the only option is to // insert all characters of second string into first // If second string is empty, the only option is to // remove all characters of first string // If last characters of two strings are same, nothing // much to do. Get the count for // If last characters are not same, consider all three // operations on last character of first string, // recursively compute minimum cost for all three // operations and take minimum of three values. // of operations to convert s1 to s2. // If first string is empty, the only option is to // insert all characters of second string into first // If second string is empty, the only option is to // remove all characters of first string // If last characters of two strings are same, nothing // much to do. Get the count for // If last characters are not same, consider all three // operations on last character of first string, // recursively compute minimum cost for all three // operations and take minimum of three values. # of operations to convert s1 to s2. # If first string is empty, the only option is to # insert all characters of second string into first # If second string is empty, the only option is to # remove all characters of first string # If last characters of two strings are same, nothing # much to do. Get the count for # If last characters are not same, consider all three # operations on last character of first string, # recursively compute minimum cost for all three # operations and take minimum of three values. // of operations to convert s1 to s2. // If first string is empty, the only option is to // insert all characters of second string into first // If second string is empty, the only option is to // remove all characters of first string // If last characters of two strings are same, nothing // much to do. Get the count for // If last characters are not same, consider all three // operations on last character of first string, // recursively compute minimum cost for all three // operations and take minimum of three values. // of operations to convert s1 to s2. // If first string is empty, the only option is to // insert all characters of second string into first // If second string is empty, the only option is to // remove all characters of first string // If last characters of two strings are same, nothing // much to do. Get the count for // If last characters are not same, consider all three // operations on last character of first string, // recursively compute minimum cost for all three // operations and take minimum of three values.\n\n[Better Approach 1] Using Top-Down DP (Memoization) – O(m*n) time and O(m*n) space\n\n// of operations to convert s1 to s2 // If first string is empty, the only option is to // insert all characters of second string into first // If second string is empty, the only option is to // remove all characters of first string // If last characters of two strings are same, nothing // much to do. Get the count for // If last characters are not same, consider all three // operations on last character of first string, // recursively compute minimum cost for all three // operations and take minimum of three values. // of operations to convert s1 to s2 // If first string is empty, the only option is to // insert all characters of second string into first // If second string is empty, the only option is to // remove all characters of first string // If last characters of two strings are same, nothing // much to do. Get the count for // If last characters are not same, consider all three // operations on last character of first string, // recursively compute minimum cost for all three // operations and take minimum of three values. # of operations to convert s1 to s2 # If first string is empty, the only option is to # insert all characters of second string into first # If second string is empty, the only option is to # remove all characters of first string # If last characters of two strings are same, nothing # much to do. Get the count for # If last characters are not same, consider all three # operations on last character of first string, # recursively compute minimum cost for all three # operations and take minimum of three values. // of operations to convert s1 to s2 // If first string is empty, the only option is to // insert all characters of second string into first // If second string is empty, the only option is to // remove all characters of first string // If last characters of two strings are same, nothing // much to do. Get the count for // If last characters are not same, consider all three // operations on last character of first string, // recursively compute minimum cost for all three // operations and take minimum of three values. // of operations to convert s1 to s2 // If first string is empty, the only option is to // insert all characters of second string into first // If second string is empty, the only option is to // remove all characters of first string // If last characters of two strings are same, nothing // much to do. Get the count for // If last characters are not same, consider all three // operations on last character of first string, // recursively compute minimum cost for all three // operations and take minimum of three values.\n\n[Better Approach 2] Using Bottom-Up DP (Tabulation)-O(m*n) time and O(m*n) space\n\nBelow are the steps to convert the recursive approach to Bottom up approach:\n\n1. Choosing Dimensions of Table: The state of smaller sub-problems depends on the input parameters m and n because at least one of them will decrease after each recursive call. So we need to construct a 2D table dp[][] to store the solution of the sub-problems.\n\n2. Choosing Correct size of Table: The range of parameters goes from 0 to m and 0 to n. So we choose dimensions as (m + 1)*(n + 1)\n\n3. Filling the table: It consist of two stages, table initialisation and building the solution from the smaller subproblems:\n• Table initialisation: Before building the solution, we need to initialise the table with the known values i.e. base case. Here m = 0 n = 0 is the situation of the base case, so we initialise first-column dp[i][0] i dp[0][j] j\n• Building the solution of larger problems from the smaller subproblems: We can easily define the iterative structure by using the recursive structure of the above recursive solution.\n\n4. Returning final solution: After filling the table iteratively, our final solution gets stored at the bottom right corner of the 2-D table i.e. we return dp[m][n] as an output.\n\n// of operations to convert s1 to s2 // Fill the known entries in dp[][] // If one string is empty, then answer // is length of the other string // of operations to convert s1 to s2 // of operations to convert s1 to s2 // Fill the known entries in dp[][] // If one string is empty, then answer // is length of the other string # of operations to convert s1 to s2 # of operations to convert s1 to s2 # Fill the known entries in dp[][] # If one string is empty, then answer # is length of the other string // of operations to convert s1 to s2 // of operations to convert s1 to s2 // Fill the known entries in dp[][] // If one string is empty, then answer // is length of the other string // of operations to convert s1 to s2 // of operations to convert s1 to s2 // Fill the known entries in dp[][] // If one string is empty, then answer // is length of the other string\n\n[Expected Approach 1] Using Space Optimised DP-O(m*n) time and space O(n)\n\n[Expected Approach 2] Using Space Optimised DP – O(m*n) Time and O(n) Space\n\nIf we do not consider the replace operation, then edit distance problem is same as the Longest Common Subsequence (LCS) problem. With only insert and delete operations allowed, the edit distance between two strings is ( M + N – 2* LCS)\n\nYou may refer edit distance based articles to know interview based questions based on edit distance."
    },
    {
        "link": "https://stackoverflow.com/questions/2460177/edit-distance-in-python",
        "document": "I'm programming a spellcheck program in Python. I have a list of valid words (the dictionary) and I need to output a list of words from this dictionary that have an edit distance of 2 from a given invalid word.\n\nI know I need to start by generating a list with an edit distance of one from the invalid word(and then run that again on all the generated words). I have three methods, inserts(...), deletions(...) and changes(...) that should output a list of words with an edit distance of 1, where inserts outputs all valid words with one more letter than the given word, deletions outputs all valid words with one less letter, and changes outputs all valid words with one different letter.\n\nI've checked a bunch of places but I can't seem to find an algorithm that describes this process. All the ideas I've come up with involve looping through the dictionary list multiple times, which would be extremely time consuming. If anyone could offer some insight, I'd be extremely grateful."
    },
    {
        "link": "https://algocademy.com/blog/edit-distance-levenshtein-distance-a-comprehensive-guide",
        "document": "In the world of computer science and algorithmic problem-solving, the concept of Edit Distance, also known as Levenshtein Distance, plays a crucial role in various applications. This powerful algorithm is essential for anyone looking to enhance their coding skills, especially those preparing for technical interviews at major tech companies. In this comprehensive guide, we’ll dive deep into the Edit Distance algorithm, exploring its implementation, applications, and significance in the realm of programming.\n\nEdit Distance, or Levenshtein Distance, is a measure of the similarity between two strings. It quantifies how many single-character edits (insertions, deletions, or substitutions) are needed to transform one string into another. This concept was introduced by Vladimir Levenshtein in 1965 and has since become a fundamental algorithm in computer science.\n\nFor example, the Edit Distance between “kitten” and “sitting” is 3, as it takes three edits to transform “kitten” into “sitting”:\n\nThe Edit Distance algorithm uses dynamic programming to efficiently calculate the minimum number of edits required. Let’s break down the steps:\n• Create a matrix with dimensions (m+1) x (n+1), where m and n are the lengths of the two strings.\n• Initialize the first row and column of the matrix.\n• Fill the rest of the matrix using the following rule:\n• If the characters match, copy the value from the diagonal.\n• If they don’t match, take the minimum of the three adjacent cells and add 1.\n• The value in the bottom-right cell of the matrix is the Edit Distance.\n\nThis implementation creates a 2D matrix (dp) to store intermediate results. The final Edit Distance is found in the bottom-right cell of the matrix.\n\nThe time complexity of this algorithm is O(mn), where m and n are the lengths of the two strings. This is because we need to fill each cell in the m x n matrix. The space complexity is also O(mn) due to the 2D matrix used to store intermediate results.\n\nWe can optimize the space complexity to O(min(m,n)) by only keeping track of the current and previous rows in the matrix. Here’s an optimized version:\n\nThis optimized version reduces the space complexity to O(min(m,n)) while maintaining the same time complexity of O(mn).\n\nThe Edit Distance algorithm has numerous practical applications across various domains:\n• Spell Checking: It can suggest corrections for misspelled words by finding words with the smallest Edit Distance.\n• DNA Sequence Alignment: In bioinformatics, it’s used to compare DNA sequences and identify similarities.\n• Plagiarism Detection: It can help identify similarities between texts, potentially indicating plagiarism.\n• Fuzzy String Matching: Useful in search engines and databases for finding approximate matches.\n• Natural Language Processing: It’s used in various NLP tasks, including machine translation and speech recognition.\n\nThere are several variations of the Edit Distance algorithm, each with its own specific use cases:\n\nHamming Distance is a simpler version that only considers substitution operations and works on strings of equal length. It’s often used in information theory and error detection.\n\nThis variation extends the Levenshtein Distance by including transposition of adjacent characters as a valid operation. It’s particularly useful for spell checking.\n\nThe Edit Distance algorithm is a popular topic in technical interviews, especially at major tech companies. Here are some tips for tackling Edit Distance problems in interviews:\n• Understand the problem: Make sure you grasp what the question is asking. Is it a standard Edit Distance problem, or a variation?\n• Start with a brute force approach: Begin by explaining a recursive solution, even if it’s not optimal. This shows your problem-solving process.\n• Optimize with dynamic programming: Explain how you can use memoization or tabulation to optimize the solution.\n• Consider space optimization: If asked, discuss how you can optimize the space complexity.\n• Analyze time and space complexity: Be prepared to discuss the complexity of your solution.\n• Handle edge cases: Consider what happens with empty strings or strings of very different lengths.\n• Test your solution: Provide test cases and walk through your code to ensure it works correctly.\n\nTo further enhance your understanding and prepare for interviews, here are some practice problems related to Edit Distance:\n• One Edit Distance: Given two strings, determine if they are one edit distance apart.\n• Delete Operation for Two Strings: Find the minimum number of steps required to make two strings the same.\n• Minimum ASCII Delete Sum for Two Strings: Find the lowest ASCII sum of deleted characters to make two strings equal.\n• Longest Common Subsequence: Find the length of the longest subsequence present in both strings.\n• Wildcard Matching: Implement wildcard pattern matching with support for ‘?’ and ‘*’.\n\nThe Edit Distance algorithm is a fundamental concept in computer science with wide-ranging applications. Understanding this algorithm and its variations is crucial for anyone looking to excel in coding interviews and develop strong problem-solving skills.\n\nAs you continue your journey in coding education and skills development, remember that mastering algorithms like Edit Distance is just one piece of the puzzle. Keep practicing, exploring new concepts, and applying your knowledge to real-world problems. Platforms like AlgoCademy can provide valuable resources and guidance as you progress from beginner-level coding to tackling complex algorithmic challenges.\n\nBy thoroughly understanding Edit Distance and similar dynamic programming concepts, you’ll be well-prepared for technical interviews at major tech companies and equipped to solve a wide range of programming problems. Keep honing your skills, and don’t hesitate to dive deep into the intricacies of algorithms â€“ it’s the key to becoming a proficient and confident programmer."
    },
    {
        "link": "https://medium.com/data-science/learn-to-implement-edit-distance-from-scratch-7a6f34412d07",
        "document": "What is Edit Distance and how to implement it?\n\nEdit Distance or Levenstein distance (the most common) is a metric to calculate the similarity between a pair of sequences. The distance between two sequences is measured as the number of edits (insertion, deletion, or substitution) that are required to convert one sequence to another.\n\nIn this section, we will learn to implement the Edit Distance.\n\nThe Levenstein distance is calculated using the following:\n\nWhere tail means rest of the sequence except for the 1st character, in Python lingo it is .\n\nThe explanations of the conditions are:\n• If b is an empty sequence ( ), then cost is the length of a ( ).\n• If a is an empty sequence ( ), then cost is the length of b ( ).\n• If the 1st characters of a & b are the same ( ), then the cost is the cost of subsequence tail(a) ( ) and tail(b) ( ).\n• Finally, the cost is the minimum of insertion, deletion, or substitution operation, which are as defined:\n• indicates a character is deleted from a\n• indicates a character is inserted to a\n\nNote: here in the formula above, the cost of insertion, deletion, or substitution has been kept the same i.e. . But, the cost of substitution is generally considered as , which we will use in the implementation.\n\nWe can directly convert the above formula into a Recursive function to calculate the Edit distance between two sequences, but the time complexity of such a solution is 𝑂(3(𝑚+𝑛)).\n\nSo, once we get clarity on how does Edit distance work, we will write a more optimized solution for it using Dynamic Programming having a time complexity of 𝑂(𝑚∗𝑛).\n\nBelow is the Recursive function. I will also, add some narration i.e. the function to print out the operations (insertion, deletion, or substitution) it is performing.\n\nLet’s test this function for some examples\n\nThe reason for Edit distance to be is: characters remain same (hence the 0 cost), then are inserted resulted in the total cost of so far. Then, no change was made for , so no change in cost and finally, , which resulted in an additional cost of 2.\n\nHence, the total cost is .\n\nDynamic programming can be applied to the problems that have overlapping subproblems. Like in our case, where to get the Edit distance between & , we first compute the same for sub-sequences & , then for & and so on...\n\nOnce, we solve a particular subproblem we store its result, which later on is used to solve the overall problem.\n\nTo know more about Dynamic Programming you can refer to my short tutorial — Introduction to Dynamic Programming.\n\nLet’s now understand how to break the problem into sub-problems, store the results and then solve the overall problem.\n\nIn the image below — across the rows we have which we want to convert into (which is across the columns) with minimum conversion cost.\n\nThe character before the two sequences indicate the empty string or the beginning of the string.\n\nNow, we will fill this Matrix with the cost of different sub-sequence to get the overall solution. But, first, let’s look at the base cases:\n• When is empty, then the cost to get is just the cost of adding the characters which are present in the . The first row in the Maxtrix indicates that is empty.\n• If both the sequences are empty, then the cost is .\n• If we add character to the we get the cost of .\n• In the same way, we will fill our first row, where the value in each column is , i.e. the cost of appending 1 more character is added.\n• Note: the value 7 in the last column represents that if is empty then the cost of converting to is . Also, the cost of converting an empty sequence to a subsequence is .\n• Opposite to this, we have a case when is empty, but is not. Then the values across the rows represent the cost of deleting the characters to get an empty sequence.\n• Note: here the cost represents the total cost to delete all the characters of to get an empty .\n\nNow the matrix with base cases’ costs filled will be as follows:\n\nSolving for Sub-problems and fill up the matrix.\n• The value under (’n’, ’n’) is , since these, both characters are the same and hence no cost occurred for conversion.\n• The below matrix shows the cost to convert to is , since the cost of substrings & is , we only add the cost of adding to .\n• Similar to above, the cost of converting to is , since the cost of substrings & is , we only add the cost of deleting from .\n• After few iterations, the matrix will look as shown below. Note: the cost user sub-sequences & is , since they are identical.\n• So far we have only looked at insertion and deletion operations, but now we will also, consider a substitution example. To solve for the subsequences & we will first calculate the cost of sub-sequences & (which is as we noted above), hence the total cost is 0+2=20+2=2 which is the cost of substituting to .\n• The complete matrix is below and the total cost is mentioned in the last column of the last row — which is .\n\nAlso, by tracing the minimum cost from the last column of the last row to the first column of the first row we can get the operations that were performed to reach this minimum cost.\n\nThe below function gets the operations performed to get the minimum cost.\n\nExecute the above function on sample sequences.\n\nThe dataset we are going to use contains files containing the list of packages with their versions installed for two versions of Python language which are 3.6 and 3.9.\n\nThe records of Pandas package in the two files are:\n\nIn this exercise for each of the package mentioned in one file, we will find the most suitable one from the second file. The suitability will be based on the Levenstein distance or the Edit distance metric.\n\nGet the pairwise distance between the requirement files\n\nNow, that we have built a function to calculate the edit distance between two sequences, we will use it to calculate the score between two packages from two different requirement files.\n\nThen, for each package mentioned in the requirement file of the Python 3.6 version, we will find the best matching package from the Python 3.9 version file.\n\nSome of the output records:\n\nCheck the accuracy of the above solution\n\nTo do so, we will simply crop off the version part of the package names from both py36 and its best-matching package from py39 and then check if they are the same or not.\n\nLet’s look at the below example to understand why we have such a low accuracy.\n\nThere is no matching record of ‘xlrd’ in the py39 list that is it was never installed for the Python 3.9 version.\n\nThe number of records in py36 is 276, while it is only 146 in py39, hence we can find the matching names only for 53% (146/276)of the records of py36 list.\n\nAlso, the data used was uploaded on Kaggle and the working notebook can be accessed using — https://www.kaggle.com/pikkupr/implement-edit-distance-from-sratch\n\nHope the explanations were clear and you learned from this notebook and let me know in the comments if you have any questions."
    },
    {
        "link": "https://cs.jhu.edu/~langmea/resources/lecture_notes/dp_and_edit_dist.pdf",
        "document": ""
    }
]