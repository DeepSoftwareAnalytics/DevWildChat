[
    {
        "link": "https://geeksforgeeks.org/semaphores-in-process-synchronization",
        "document": "Semaphores are a tool used in operating systems to help manage how different processes (or programs) share resources, like memory or data, without causing conflicts. A semaphore is a special kind of synchronization data that can be used only through specific synchronization primitives. Semaphores are used to implement critical sections, which are regions of code that must be executed by only one process at a time. By using semaphores, processes can coordinate access to shared resources, such as shared memory or I/O devices.\n\nA semaphore is a synchronization tool used in concurrent programming to manage access to shared resources. It is a lock-based mechanism designed to achieve process synchronization, built on top of basic locking techniques.\n\nSemaphores use a counter to control access, allowing synchronization for multiple instances of a resource. Processes can attempt to access one instance, and if it is not available, they can try other instances. Unlike basic locks, which allow only one process to access one instance of a resource. Semaphores can handle more complex synchronization scenarios, involving multiple processes or threads. It help prevent problems like race conditions by controlling when and how processes access shared data.\n\nThe process of using Semaphores provides two operations:\n• wait (P): The wait operation decrements the value of the semaphore\n• signal (V): The signal operation increments the value of the semaphore.\n\nWhen the value of the semaphore is zero, any process that performs a wait operation will be blocked until another process performs a signal operation.\n\nWhen a process performs a wait operation on a semaphore, the operation checks whether the value of the semaphore is >0. If so, it decrements the value of the semaphore and lets the process continue its execution; otherwise, it blocks the process on the semaphore. A signal operation on a semaphore activates a process blocked on the semaphore if any, or increments the value of the semaphore by 1. Due to these semantics, semaphores are also called counting semaphores. The initial value of a semaphore determines how many processes can get past the wait operation.\n\nSemaphores are required for process synchronization to make sure that multiple processes can safely share resources without interfering with each other. They help control when a process can access a shared resource, preventing issues like race conditions.\n\nSemaphores are of two Types:\n• Binary Semaphore: This is also known as a mutex lock, as they are locks that provide mutual exclusion. It can have only two values – 0 and 1. Its value is initialized to 1. It is used to implement the solution of critical section problems with multiple processes and a single resource.\n• Counting Semaphore: Counting semaphores can be used to control access to a given resource consisting of a finite number of instances. The semaphore is initialized to the number of resources available. Its value can range over an unrestricted domain.\n\nTo learn more refer: Types of Semaphores\n\nA semaphore is a simple yet powerful synchronization tool used to manage access to shared resources in a system with multiple processes. It works by maintaining a counter that controls access to a specific resource, ensuring that no more than the allowed number of processes access the resource at the same time.\n\nThere are two primary operations that a semaphore can perform:\n• Wait (P operation) : This operation checks the semaphore’s value. If the value is greater than 0, the process is allowed to continue, and the semaphore’s value is decremented by 1. If the value is 0, the process is blocked (waits) until the semaphore value becomes greater than 0.\n• Signal (V operation) : After a process is done using the shared resource, it performs the signal operation. This increments the semaphore’s value by 1, potentially unblocking other waiting processes and allowing them to access the resource.\n\nNow let us see how it does so. First, look at two operations that can be used to access and change the value of the semaphore variable.\n\nA critical section is surrounded by both operations to implement process synchronization. The below image demonstrates the basic mechanism of how semaphores are used to control access to a critical section in a multi-process environment, ensuring that only one process can access the shared resource at a time\n\nNow, let us see how it implements mutual exclusion. Let there be two processes P1 and P2 and a semaphore s is initialized as 1. Now if suppose P1 enters in its critical section then the value of semaphore s becomes 0. Now if P2 wants to enter its critical section then it will wait until s > 0, this can only happen when P1 finishes its critical section and calls V operation on semaphore s.\n\nThis way mutual exclusion is achieved. Look at the below image for details which is a Binary semaphore.\n\nThe description above is for binary semaphore which can take only two values 0 and 1 and ensure mutual exclusion. There is one other type of semaphore called counting semaphore which can take values greater than one.\n\nNow suppose there is a resource whose number of instances is 4. Now we initialize S = 4 and the rest is the same as for binary semaphore. Whenever the process wants that resource it calls P or waits for function and when it is done it calls V or signal function. If the value of S becomes zero then a process has to wait until S becomes positive. For example, Suppose there are 4 processes P1, P2, P3, and P4, and they all call wait operation on S(initialized with 4). If another process P5 wants the resource then it should wait until one of the four processes calls the signal function and the value of semaphore becomes positive.\n• None One of the biggest limitations of semaphore is\n• None Deadlock, suppose a process is trying to wake up another process that is not in a sleep state. Therefore, a deadlock may be blocked indefinitely.\n• None The operating system has to keep track of all calls to wait and signal the semaphore.\n\nThe main problem with semaphores is that they require busy waiting, If a process is in the critical section, then other processes trying to enter the critical section will be waiting until the critical section is not occupied by any process. Whenever any process waits then it continuously checks for semaphore value (look at this line while (s==0); in P operation) and wastes CPU cycle.\n\nThere is also a chance of “spinlock” as the processes keep on spinning while waiting for the lock. To avoid this another implementation is provided below.\n\nIn this implementation whenever the process waits it is added to a waiting queue of processes associated with that semaphore. This is done through the system call block() on that process. When a process is completed it calls the signal function and one process in the queue is resumed. It uses the wakeup() system call.\n• Mutual Exclusion : Semaphore ensures that only one process accesses a shared resource at a time.\n• Resource Management : Limits access to a finite set of resources, like printers, devices, etc.\n• Reader-Writer Problem : Allows multiple readers but restricts the writers until no reader is present.\n• Avoiding Deadlocks : Prevents deadlocks by controlling the order of allocation of resources.\n• None Semaphore is a simple and effective mechanism for process synchronization\n• None Supports coordination between multiple processes. By controlling the access to critical sections, semaphores help in managing multiple processes without them interfering with each other.\n• None When used correctly, semaphores can help avoid deadlocks by managing access to resources efficiently and ensuring that no process is indefinitely blocked from accessing necessary resources.\n• None Semaphores help prevent race conditions by ensuring that only one process can access a shared resource at a time.\n• None Provides a flexible and robust way to manage shared resources.\n• None It Can lead to performance degradation due to overhead associated with wait and signal operations.\n• None If semaphores are not managed carefully, they can lead to . This often occurs when semaphores are not released properly or when processes acquire semaphores in an inconsistent order.\n• None It can cause performance issues in a program if not used properly.\n• None It can be difficult to debug and maintain. Debugging systems that rely heavily on semaphores can be challenging, as it is hard to track the state of each semaphore and ensure that all processes are correctly synchronized\n• None It can be prone to race conditions and other synchronization problems if not used correctly.\n• None It can be vulnerable to certain types of attacks, such as denial of service attacks.\n\nThese are the classical synchronization problem using the concept of semaphores.\n\nThe producer-consumer problem involves two types of processes producers that generate data and consumers that process data. The Producer-Consumer Problem is like a restaurant where the chef (producer) makes food and the customer (consumer) eats it. The counter (buffer: A fixed-size queue where producers place items and consumers remove items.) holds food temporarily. A special lock (semaphore) ensures the chef doesn’t overflow the counter and the customer doesn’t take food when it’s not available. In This way, everything runs smoothly and efficiently and gives faster results.\n• None empty: Counts the number of empty slots in the buffer\n• None full: Counts the number of full slots in the buffer\n\nDescription: Traffic lights at an intersection can be managed using semaphores to control the flow of traffic and ensure safe crossing.\n\nTraffic Lights: Represented by semaphores that control the green, yellow, and red lights for different directions.\n\nSemaphores Used: Each light direction is controlled by a semaphore that manages the timing and transitions between light states.\n\nLight Controller: Uses semaphores to cycle through green, yellow, and red lights. The controller ensures that only one direction has the green light at a time and manages transitions to avoid conflicts.\n• Multiple Transactions: Processes that need to access shared resources (accounts)\n• Semaphore value: 1 (only one transaction can access and modify an account at a time is crucial for maintaining data integrity).\n• Acquire Semaphore: A transaction must need to acquire the semaphore first before modifying an account balance.\n• The account : Then operations like debiting or crediting the account are performed by the transaction.\n• Release Semaphore: After Completing the transactions the semaphore is released to allow other transactions to proceed.\n• Multiple Print Jobs: Processes that need to access shared resources (printers)\n• Semaphore Value: 1 (only one print job can access the printer at a time)\n• Acquire Semaphore: First, acquire a semaphore for the printer to begin the print job.\n• Release Semaphore: Now the semaphore is released after the job is done.\n• Multiple Trains: Processes that need to access shared resources (tracks)\n• Semaphore Value: 1 (only one train can access the track at a time)\n• Release Semaphore: The semaphores are released after the train passes the track.\n• Multiple Philosophers: Process that needs to access shared resources(forks).\n• Forks: Shared resources represented by semaphores, that need to eat. Each philosopher needs two forks to eat.\n• Semaphore Value: 1 (Only one philosopher can access a fork at a time preventing deadlock and starvation).\n• Acquire Forks: Philosophers acquire the semaphore for both the left and right forks before eating.\n• Eat : When both the forks are acquired then the philosopher eats.\n• Release Forks: After eating, both the forks are released for others to use them.\n\nIn Reader-Writer problem Synchronizing access to shared data where multiple readers can read the data simultaneously, but writers need exclusive access to modify it. In simple terms imagine a library where multiple readers and writers come and all readers want to read a book, and some people(writers) want to update or edit the book. That’s why we need a system to ensure that these actions are done smoothly without errors or conflicts.\n• Multiple Readers and Writers: Processes that need to access a shared resource(data).\n• Semaphore Value for Reader: Process that reads shared data, Can access simultaneously(value>1, more than one reader can access at the same time)\n• Semaphore Value for Writers: Processes that modify shared data need exclusive access(value =1, one at a time)\n\nProcess Synchronization is an important of concurrent computing, ensuring that multiple processes can execute without interfering with each other. Semaphores can be used to tackle classic synchronization challenges in computing. It plays a vital role In managing access to shared resources and coordinating the activities of concurrent processes. It also helps to improve system performance, maintain data integrity, and remove the conflicts between the processes. Effective process synchronization ensures consistency, stability, and efficiency in multi-threaded and multi-process systems.\n\nWhy it is important to protect Critical Sections?\n\nWhat is the difference between process synchronization and mutual exclusion?"
    },
    {
        "link": "https://stackoverflow.com/questions/32153151/how-to-create-semaphores-in-shared-memory-in-c",
        "document": "What most of these share is that you create the semaphore in one process via a virtual path which dubs as the semaphore's name. If permissions are correctly set, you can the open the semaphore in another process by using the same virtual path. These virtual paths aren't usually real filesystem paths even if they look familiar.\n\nOn POSIX/System V-based systems, you typically have two options. The differences between the two options are very well explained in this answer.\n\nThese are path-based semaphores that can be obtained with :\n\nNote that it is important to cleanup the semaphores, as System V semaphores stay around until explicitly unlinked. Which is kind of a problem when a process crashes without cleaning up its semaphores (e.g. FreeBSD comes with a utility do remove dangling System V IPC objects).\n\nThese are actually less widely implemented, so check if your kernel supports them. The named version of these is are obtained via .\n\nEdit: Named POSIX semaphores need to be destroyed and unlinked using and . Unnamed semaphores (obtained from get destroyed on last close. Note that it is save to unlink an open semaphore, it will get destroyed on last close.\n\nWindows has its own semaphore APIs: Semaphores are created by .\n\nWindows uses the same naming trick as POSIX but with different namespace conventions.\n\nDon't forget to add error checking when adapting the examples above. Also note that I have deliberately ignored permissions to simplify the code. Don't forget to add the relevant flags.\n\nIn addition to all this you can also (as commonly done before the arrival of real semaphores) abuse file locks as a form of binary mutex."
    },
    {
        "link": "https://stackoverflow.com/questions/8359322/how-to-share-semaphores-between-processes-using-shared-memory",
        "document": "I wrote a sample application where a parent (producer) spawns N child (consumer) threads and uses a global array to communicate data between them. The global array is a circular memory and is protected when data is written on to the array."
    },
    {
        "link": "https://cs.purdue.edu/homes/ci/cs240/Week15.pdf",
        "document": ""
    },
    {
        "link": "https://geeksforgeeks.org/use-posix-semaphores-c",
        "document": "How to use POSIX semaphores in C language\n\nSemaphores are very useful in process synchronization and multithreading. But how to use one in real life, for example say in C Language?\n\nWell, we have the POSIX semaphore library in Linux systems. Let’s learn how to use it.\n\nThe basic code of a semaphore is simple as presented here. But this code cannot be written directly, as the functions require to be atomic and writing code directly would lead to a context switch without function completion and would result in a mess.\n\nThe POSIX system in Linux presents its own built-in semaphore library. To use it, we have to :\n• None Compile the code by linking with -lpthread -lrt\n• None To lock a semaphore or wait we can use the sem_wait\n\nTo release or signal a semaphore, we use the sem_post function:\n\nA semaphore is initialised by using sem_init(for processes or threads) or sem_open (for IPC).\n• sem : Specifies the semaphore to be initialized.\n• pshared : This argument specifies whether or not the newly initialized semaphore is shared between processes or between threads. A non-zero value means the semaphore is shared between processes and a value of zero means it is shared between threads.\n• value : Specifies the value to assign to the newly initialized semaphore.\n\nTo destroy a semaphore, we can use sem_destroy.\n\nTo declare a semaphore, the data type is sem_t.\n\nCompilation should be done with gcc a.c -lpthread -lrt\n\n2 threads are being created, one 2 seconds after the first one.\n\nBut the first thread will sleep for 4 seconds after acquiring the lock.\n\nThus the second thread will not enter immediately after it is called, it will enter 4 – 2 = 2 secs after it is called. So the output is:"
    },
    {
        "link": "https://geeksforgeeks.org/use-posix-semaphores-c",
        "document": "How to use POSIX semaphores in C language\n\nSemaphores are very useful in process synchronization and multithreading. But how to use one in real life, for example say in C Language?\n\nWell, we have the POSIX semaphore library in Linux systems. Let’s learn how to use it.\n\nThe basic code of a semaphore is simple as presented here. But this code cannot be written directly, as the functions require to be atomic and writing code directly would lead to a context switch without function completion and would result in a mess.\n\nThe POSIX system in Linux presents its own built-in semaphore library. To use it, we have to :\n• None Compile the code by linking with -lpthread -lrt\n• None To lock a semaphore or wait we can use the sem_wait\n\nTo release or signal a semaphore, we use the sem_post function:\n\nA semaphore is initialised by using sem_init(for processes or threads) or sem_open (for IPC).\n• sem : Specifies the semaphore to be initialized.\n• pshared : This argument specifies whether or not the newly initialized semaphore is shared between processes or between threads. A non-zero value means the semaphore is shared between processes and a value of zero means it is shared between threads.\n• value : Specifies the value to assign to the newly initialized semaphore.\n\nTo destroy a semaphore, we can use sem_destroy.\n\nTo declare a semaphore, the data type is sem_t.\n\nCompilation should be done with gcc a.c -lpthread -lrt\n\n2 threads are being created, one 2 seconds after the first one.\n\nBut the first thread will sleep for 4 seconds after acquiring the lock.\n\nThus the second thread will not enter immediately after it is called, it will enter 4 – 2 = 2 secs after it is called. So the output is:"
    },
    {
        "link": "https://stackoverflow.com/questions/32153151/how-to-create-semaphores-in-shared-memory-in-c",
        "document": "What most of these share is that you create the semaphore in one process via a virtual path which dubs as the semaphore's name. If permissions are correctly set, you can the open the semaphore in another process by using the same virtual path. These virtual paths aren't usually real filesystem paths even if they look familiar.\n\nOn POSIX/System V-based systems, you typically have two options. The differences between the two options are very well explained in this answer.\n\nThese are path-based semaphores that can be obtained with :\n\nNote that it is important to cleanup the semaphores, as System V semaphores stay around until explicitly unlinked. Which is kind of a problem when a process crashes without cleaning up its semaphores (e.g. FreeBSD comes with a utility do remove dangling System V IPC objects).\n\nThese are actually less widely implemented, so check if your kernel supports them. The named version of these is are obtained via .\n\nEdit: Named POSIX semaphores need to be destroyed and unlinked using and . Unnamed semaphores (obtained from get destroyed on last close. Note that it is save to unlink an open semaphore, it will get destroyed on last close.\n\nWindows has its own semaphore APIs: Semaphores are created by .\n\nWindows uses the same naming trick as POSIX but with different namespace conventions.\n\nDon't forget to add error checking when adapting the examples above. Also note that I have deliberately ignored permissions to simplify the code. Don't forget to add the relevant flags.\n\nIn addition to all this you can also (as commonly done before the arrival of real semaphores) abuse file locks as a form of binary mutex."
    },
    {
        "link": "https://stackoverflow.com/questions/13223935/programming-in-unix-semaphores-shared-memory-in-c",
        "document": "You declare your variable as follows:\n\nBut the actual type of your array is . Although those can both be considered 2-dimensional arrays, they are not at all the same type and do not share the same structure in memory:\n• is a pointer to a pointer to an integer. When used as a 2-dimensional array, it describes a 2-level array. The first level (dereferencing the pointer once) contains an array of pointers that each point to 1-dimensional arrays of integers.\n• is a flat 2-dimentional array. It is a single pointer to an array of integers. It has the same memory structure as the 1-dimensional array even though it offers the convenience of using 2 levels of subscripts.\n\nPassing around pointers to -style arrays in C is non-intuitive and very difficult to get right. I don't advise doing it. You might consider passing pointers to your type instead. Still, it is possible to get it to work with this declaration:\n\nthen you have to change the parameter in and to instead or and use it as instead of just . This gets ugly. Like I said, consider using your type instead.\n\nThis problem should have been caught by the compiler. It should have been giving you lots of type mismatch warnings. Make sure you always compile with and aim to have your program compile with no errors and no warnings."
    },
    {
        "link": "https://thiyagaraaj.com/tutorials/unix-network-programming-example-programs/semaphore-in-unix-using-c-programming",
        "document": ""
    },
    {
        "link": "https://geeksforgeeks.org/semaphores-in-process-synchronization",
        "document": "Semaphores are a tool used in operating systems to help manage how different processes (or programs) share resources, like memory or data, without causing conflicts. A semaphore is a special kind of synchronization data that can be used only through specific synchronization primitives. Semaphores are used to implement critical sections, which are regions of code that must be executed by only one process at a time. By using semaphores, processes can coordinate access to shared resources, such as shared memory or I/O devices.\n\nA semaphore is a synchronization tool used in concurrent programming to manage access to shared resources. It is a lock-based mechanism designed to achieve process synchronization, built on top of basic locking techniques.\n\nSemaphores use a counter to control access, allowing synchronization for multiple instances of a resource. Processes can attempt to access one instance, and if it is not available, they can try other instances. Unlike basic locks, which allow only one process to access one instance of a resource. Semaphores can handle more complex synchronization scenarios, involving multiple processes or threads. It help prevent problems like race conditions by controlling when and how processes access shared data.\n\nThe process of using Semaphores provides two operations:\n• wait (P): The wait operation decrements the value of the semaphore\n• signal (V): The signal operation increments the value of the semaphore.\n\nWhen the value of the semaphore is zero, any process that performs a wait operation will be blocked until another process performs a signal operation.\n\nWhen a process performs a wait operation on a semaphore, the operation checks whether the value of the semaphore is >0. If so, it decrements the value of the semaphore and lets the process continue its execution; otherwise, it blocks the process on the semaphore. A signal operation on a semaphore activates a process blocked on the semaphore if any, or increments the value of the semaphore by 1. Due to these semantics, semaphores are also called counting semaphores. The initial value of a semaphore determines how many processes can get past the wait operation.\n\nSemaphores are required for process synchronization to make sure that multiple processes can safely share resources without interfering with each other. They help control when a process can access a shared resource, preventing issues like race conditions.\n\nSemaphores are of two Types:\n• Binary Semaphore: This is also known as a mutex lock, as they are locks that provide mutual exclusion. It can have only two values – 0 and 1. Its value is initialized to 1. It is used to implement the solution of critical section problems with multiple processes and a single resource.\n• Counting Semaphore: Counting semaphores can be used to control access to a given resource consisting of a finite number of instances. The semaphore is initialized to the number of resources available. Its value can range over an unrestricted domain.\n\nTo learn more refer: Types of Semaphores\n\nA semaphore is a simple yet powerful synchronization tool used to manage access to shared resources in a system with multiple processes. It works by maintaining a counter that controls access to a specific resource, ensuring that no more than the allowed number of processes access the resource at the same time.\n\nThere are two primary operations that a semaphore can perform:\n• Wait (P operation) : This operation checks the semaphore’s value. If the value is greater than 0, the process is allowed to continue, and the semaphore’s value is decremented by 1. If the value is 0, the process is blocked (waits) until the semaphore value becomes greater than 0.\n• Signal (V operation) : After a process is done using the shared resource, it performs the signal operation. This increments the semaphore’s value by 1, potentially unblocking other waiting processes and allowing them to access the resource.\n\nNow let us see how it does so. First, look at two operations that can be used to access and change the value of the semaphore variable.\n\nA critical section is surrounded by both operations to implement process synchronization. The below image demonstrates the basic mechanism of how semaphores are used to control access to a critical section in a multi-process environment, ensuring that only one process can access the shared resource at a time\n\nNow, let us see how it implements mutual exclusion. Let there be two processes P1 and P2 and a semaphore s is initialized as 1. Now if suppose P1 enters in its critical section then the value of semaphore s becomes 0. Now if P2 wants to enter its critical section then it will wait until s > 0, this can only happen when P1 finishes its critical section and calls V operation on semaphore s.\n\nThis way mutual exclusion is achieved. Look at the below image for details which is a Binary semaphore.\n\nThe description above is for binary semaphore which can take only two values 0 and 1 and ensure mutual exclusion. There is one other type of semaphore called counting semaphore which can take values greater than one.\n\nNow suppose there is a resource whose number of instances is 4. Now we initialize S = 4 and the rest is the same as for binary semaphore. Whenever the process wants that resource it calls P or waits for function and when it is done it calls V or signal function. If the value of S becomes zero then a process has to wait until S becomes positive. For example, Suppose there are 4 processes P1, P2, P3, and P4, and they all call wait operation on S(initialized with 4). If another process P5 wants the resource then it should wait until one of the four processes calls the signal function and the value of semaphore becomes positive.\n• None One of the biggest limitations of semaphore is\n• None Deadlock, suppose a process is trying to wake up another process that is not in a sleep state. Therefore, a deadlock may be blocked indefinitely.\n• None The operating system has to keep track of all calls to wait and signal the semaphore.\n\nThe main problem with semaphores is that they require busy waiting, If a process is in the critical section, then other processes trying to enter the critical section will be waiting until the critical section is not occupied by any process. Whenever any process waits then it continuously checks for semaphore value (look at this line while (s==0); in P operation) and wastes CPU cycle.\n\nThere is also a chance of “spinlock” as the processes keep on spinning while waiting for the lock. To avoid this another implementation is provided below.\n\nIn this implementation whenever the process waits it is added to a waiting queue of processes associated with that semaphore. This is done through the system call block() on that process. When a process is completed it calls the signal function and one process in the queue is resumed. It uses the wakeup() system call.\n• Mutual Exclusion : Semaphore ensures that only one process accesses a shared resource at a time.\n• Resource Management : Limits access to a finite set of resources, like printers, devices, etc.\n• Reader-Writer Problem : Allows multiple readers but restricts the writers until no reader is present.\n• Avoiding Deadlocks : Prevents deadlocks by controlling the order of allocation of resources.\n• None Semaphore is a simple and effective mechanism for process synchronization\n• None Supports coordination between multiple processes. By controlling the access to critical sections, semaphores help in managing multiple processes without them interfering with each other.\n• None When used correctly, semaphores can help avoid deadlocks by managing access to resources efficiently and ensuring that no process is indefinitely blocked from accessing necessary resources.\n• None Semaphores help prevent race conditions by ensuring that only one process can access a shared resource at a time.\n• None Provides a flexible and robust way to manage shared resources.\n• None It Can lead to performance degradation due to overhead associated with wait and signal operations.\n• None If semaphores are not managed carefully, they can lead to . This often occurs when semaphores are not released properly or when processes acquire semaphores in an inconsistent order.\n• None It can cause performance issues in a program if not used properly.\n• None It can be difficult to debug and maintain. Debugging systems that rely heavily on semaphores can be challenging, as it is hard to track the state of each semaphore and ensure that all processes are correctly synchronized\n• None It can be prone to race conditions and other synchronization problems if not used correctly.\n• None It can be vulnerable to certain types of attacks, such as denial of service attacks.\n\nThese are the classical synchronization problem using the concept of semaphores.\n\nThe producer-consumer problem involves two types of processes producers that generate data and consumers that process data. The Producer-Consumer Problem is like a restaurant where the chef (producer) makes food and the customer (consumer) eats it. The counter (buffer: A fixed-size queue where producers place items and consumers remove items.) holds food temporarily. A special lock (semaphore) ensures the chef doesn’t overflow the counter and the customer doesn’t take food when it’s not available. In This way, everything runs smoothly and efficiently and gives faster results.\n• None empty: Counts the number of empty slots in the buffer\n• None full: Counts the number of full slots in the buffer\n\nDescription: Traffic lights at an intersection can be managed using semaphores to control the flow of traffic and ensure safe crossing.\n\nTraffic Lights: Represented by semaphores that control the green, yellow, and red lights for different directions.\n\nSemaphores Used: Each light direction is controlled by a semaphore that manages the timing and transitions between light states.\n\nLight Controller: Uses semaphores to cycle through green, yellow, and red lights. The controller ensures that only one direction has the green light at a time and manages transitions to avoid conflicts.\n• Multiple Transactions: Processes that need to access shared resources (accounts)\n• Semaphore value: 1 (only one transaction can access and modify an account at a time is crucial for maintaining data integrity).\n• Acquire Semaphore: A transaction must need to acquire the semaphore first before modifying an account balance.\n• The account : Then operations like debiting or crediting the account are performed by the transaction.\n• Release Semaphore: After Completing the transactions the semaphore is released to allow other transactions to proceed.\n• Multiple Print Jobs: Processes that need to access shared resources (printers)\n• Semaphore Value: 1 (only one print job can access the printer at a time)\n• Acquire Semaphore: First, acquire a semaphore for the printer to begin the print job.\n• Release Semaphore: Now the semaphore is released after the job is done.\n• Multiple Trains: Processes that need to access shared resources (tracks)\n• Semaphore Value: 1 (only one train can access the track at a time)\n• Release Semaphore: The semaphores are released after the train passes the track.\n• Multiple Philosophers: Process that needs to access shared resources(forks).\n• Forks: Shared resources represented by semaphores, that need to eat. Each philosopher needs two forks to eat.\n• Semaphore Value: 1 (Only one philosopher can access a fork at a time preventing deadlock and starvation).\n• Acquire Forks: Philosophers acquire the semaphore for both the left and right forks before eating.\n• Eat : When both the forks are acquired then the philosopher eats.\n• Release Forks: After eating, both the forks are released for others to use them.\n\nIn Reader-Writer problem Synchronizing access to shared data where multiple readers can read the data simultaneously, but writers need exclusive access to modify it. In simple terms imagine a library where multiple readers and writers come and all readers want to read a book, and some people(writers) want to update or edit the book. That’s why we need a system to ensure that these actions are done smoothly without errors or conflicts.\n• Multiple Readers and Writers: Processes that need to access a shared resource(data).\n• Semaphore Value for Reader: Process that reads shared data, Can access simultaneously(value>1, more than one reader can access at the same time)\n• Semaphore Value for Writers: Processes that modify shared data need exclusive access(value =1, one at a time)\n\nProcess Synchronization is an important of concurrent computing, ensuring that multiple processes can execute without interfering with each other. Semaphores can be used to tackle classic synchronization challenges in computing. It plays a vital role In managing access to shared resources and coordinating the activities of concurrent processes. It also helps to improve system performance, maintain data integrity, and remove the conflicts between the processes. Effective process synchronization ensures consistency, stability, and efficiency in multi-threaded and multi-process systems.\n\nWhy it is important to protect Critical Sections?\n\nWhat is the difference between process synchronization and mutual exclusion?"
    },
    {
        "link": "https://geeksforgeeks.org/inter-process-communication-ipc",
        "document": "Processes need to communicate with each other in many situations, for example, to count occurrences of a word in text file, output of grep command needs to be given to wc command, something like grep -o -i <word> <file> | wc -l. Inter-Process Communication or IPC is a mechanism that allows processes to communicate. It helps processes synchronize their activities, share information, and avoid conflicts while accessing shared resources.\n\nLet us first talk about types of types of processes.\n• Independent process: An independent process is not affected by the execution of other processes. Independent processes are processes that do not share any data or resources with other processes. No inte-process communication required here.\n• Co-operating process: Interact with each other and share data or resources. A co-operating process can be affected by other executing processes. Inter-process communication (IPC) is a mechanism that allows processes to communicate with each other and synchronize their actions. The communication between these processes can be seen as a method of cooperation between them.\n\nInter process communication (IPC) allows different programs or processes running on a computer to share information with each other. IPC allows processes to communicate by using different techniques like sharing memory, sending messages, or using files. It ensures that processes can work together without interfering with each other. Cooperating processes require an Inter Process Communication (IPC) mechanism that will allow them to exchange data and information.\n\nThe two fundamental models of Inter Process Communication are:\n\nFigure 1 below shows a basic structure of communication between processes via the shared memory method and via the message passing method.\n\nAn operating system can implement both methods of communication. First, we will discuss the shared memory methods of communication and then message passing. Communication between processes using shared memory requires processes to share some variable, and it completely depends on how the programmer will implement it. One way of communication using shared memory can be imagined like this: Suppose process1 and process2 are executing simultaneously, and they share some resources or use some information from another process. Process1 generates information about certain computations or resources being used and keeps it as a record in shared memory. When process2 needs to use the shared information, it will check in the record stored in shared memory and take note of the information generated by process1 and act accordingly. Processes can use shared memory for extracting information as a record from another process as well as for delivering any specific information to other processes.\n\nFigure 1 below shows a basic structure of communication between processes via the shared memory method and via the message passing method.\n\nLet’s discuss an example of communication between processes using the shared memory method.\n\nInter-Process Communication refers to the techniques and methods that allow processes to exchange data and coordinate their activities. Since processes typically operate independently in a multitasking environment, IPC is essential for them to communicate effectively without interfering with one another. There are several methods of IPC, each designed to suit different scenarios and requirements. These methods include shared memory, message passing, semaphores, and signals, etc.\n\nTo read more refer – methods of Inter Process Communication\n\nIn IPC, synchronization is essential for controlling access to shared resources and guaranteeing that processes do not conflict with one another. Data consistency is ensured and problems like race situations are avoided with proper synchronization.\n• None Enables processes to communicate with each other and share resources, leading to increased efficiency and flexibility.\n• None Facilitates coordination between multiple processes, leading to better overall system performance.\n• None Allows for the creation of distributed systems that can span multiple computers or networks.\n• None Can be used to implement various and communication protocols, such as semaphores, pipes, and sockets.\n• None Increases system complexity, making it harder to design, implement, and debug.\n• None Can introduce security vulnerabilities, as processes may be able to access or modify data belonging to other processes.\n• None Requires careful management of system resources, such as memory and time, to ensure that IPC operations do not degrade overall system performance. \n\n Can lead to data inconsistencies if multiple processes try to access or modify the same data at the same time.\n• None Overall, the advantages of IPC outweigh the disadvantages, as it is a necessary mechanism for modern operating systems and enables processes to work together and share resources in a flexible and efficient manner. However, care must be taken to design and implement IPC systems carefully, in order to avoid potential security vulnerabilities and performance issues.\n\nA fundamental component of contemporary operating systems, IPC allows processes to efficiently coordinate operations, share resources, and communicate. IPC is beneficial for developing adaptable and effective systems, despite its complexity and possible security threats.\n\nWhy is synchronization important in IPC?\n\nWhat are the main methods of IPC?"
    },
    {
        "link": "https://geeksforgeeks.org/ipc-shared-memory",
        "document": "Inter-Process Communication (IPC) is a fundamental concept in operating systems that allows multiple processes to communicate and synchronize their actions. Among the various methods of IPC shared memory is one of the most efficient mechanisms especially when it comes to performance-critical applications. This article delves into the concept of IPC through shared memory explaining its working, advantages, and disadvantages.\n\nInter-process Communication through shared memory is a concept where two or more processes can access the common memory and communication is done via this shared memory where changes made by one process can be viewed by another process.\n\nThe problem with pipes, fifo and message queue – is that for two processes to exchange information. The information has to go through the kernel.\n• None The server reads from the input file.\n• None The server writes this data in a message using either a pipe, FIFO, or message queue.\n• None The client reads the data from the IPC channel, again requiring the data to be copied from the kernel’s IPC buffer to the client’s buffer.\n• None Finally, the data is copied from the client’s buffer.\n\nFour copies of data are required (2 read and 2 write). So, shared memory provides a way to let two or more processes share a memory segment. With Shared Memory the data is only copied twice from the input file into shared memory and from shared memory to the output file.\n\nInter-Process Communication (IPC) refers to the set of techniques that allow the processes to exchange data and signals with one another. IPC is crucial for modern operating systems that support multitasking as it will enable the different methods to cooperate and share resources effectively. The Common IPC mechanisms include message passing, semaphores, pipes, signals, and shared memory.\n\nThe Shared memory is a memory segment that multiple processes can access concurrently. It is one of the fastest IPC methods because the processes communicate by the reading and writing to a shared block of the memory. Unlike other IPC mechanisms that involve the more complex synchronization and data exchange procedures shared memory provides the straightforward way for the processes to share data.\n\nThe Shared memory IPC works by creating a memory segment that is accessible by the multiple processes. Here’s a basic outline of how it operates:\n• Creation of Shared Memory Segment: A process usually the parent, creates a shared memory segment using the system calls like shmget() in Unix-like systems. This segment is assigned the unique identifier (shmid).\n• Attaching to the Shared Memory Segment: The Processes that need to access the shared memory attach themselves to this segment using shmat() system call. Once attached the processes can directly read from and write to the shared memory.\n• Synchronization: Since multiple processes can access the shared memory simultaneously synchronization mechanisms like semaphores are often used to the prevent race conditions and ensure data consistency.\n• Detaching and Deleting the Segment: When a process no longer needs access to the shared memory it can detach from the segment using shmdt() system call. The shared memory segment can be removed entirely from system using shmctl() once all processes have the detached.\n• Speed: The Shared memory IPC is much faster than other IPC methods like message passing because processes directly read and write to the shared memory location.\n• Low Overhead: It eliminates the overhead associated with the message passing where data has to be copied from the one process to another.\n• Flexibility: The Shared memory can be used to share complex data structures like arrays, linked lists and matrices between the processes.\n• Large Data Transfers: The Shared memory is particularly useful for the transferring large amounts of data between the processes as it avoids the need for the multiple data copies.\n• Complex Synchronization: The Shared memory requires explicit synchronization to the prevent race conditions which can make the code more complex and error-prone.\n• Security Risks: Since shared memory is accessible by the multiple processes it can pose security risks if not managed properly as unauthorized processes may access or modify the data.\n• Manual Cleanup: The Shared memory segments must be manually detached and removed which can lead to the resource leaks if not done correctly.\n• Portability: The Shared memory IPC is platform-dependent and may not be easily portable across the different operating systems.\n\nThe system calls that are used in the program are:\n\nThere are two processes: Producer and Consumer . The producer produces some items and the Consumer consumes that item. The two processes share a common space or memory location known as a buffer where the item produced by the Producer is stored and from which the Consumer consumes the item if needed. There are two versions of this problem: the first one is known as the unbounded buffer problem in which the Producer can keep on producing items and there is no limit on the size of the buffer, the second one is known as the bounded buffer problem in which the Producer can produce up to a certain number of items before it starts waiting for Consumer to consume it. We will discuss the bounded buffer problem. First, the Producer and the Consumer will share some common memory, then the producer will start producing items. If the total produced item is equal to the size of the buffer, the producer will wait to get it consumed by the Consumer. Similarly, the consumer will first check for the availability of the item. If no item is available, the Consumer will wait for the Producer to produce it. If there are items available, Consumer will consume them. The pseudo-code to demonstrate is provided below:\n\nShared Data Between the two Processes\n\nIn the above code, the Producer will start producing again when the (free_index+1) mod buff max will be free because if it is not free, this implies that there are still items that can be consumed by the Consumer so there is no need to produce more. Similarly, if free index and full index point to the same index, this implies that there are no items to consume.\n\nNote that the atomic class is used to make sure that the shared variables free_index and full_index are updated atomically. The mutex is used to protect the critical section where the shared buffer is accessed. The sleep_for function is used to simulate the production and consumption of items.\n\nThe Shared memory IPC is a powerful tool for the process communication offering the high speed and low overhead for the data exchange between the processes. However, it comes with the challenges related to synchronization, security and resource management. Understanding these aspects is crucial for the effectively implementing shared memory IPC in a multitasking environment.\n\nWhat system calls are used to manage shared memory in Unix-like systems?\n\nHow does shared memory compare to other IPC methods in terms of speed?\n\nWhat are the main security concerns with shared memory IPC?"
    },
    {
        "link": "https://tutorialspoint.com/inter_process_communication/inter_process_communication_semaphores.htm",
        "document": "The first question that comes to mind is, why do we need semaphores? A simple answer, to protect the critical/common region shared among multiple processes.\n\nLet us assume, multiple processes are using the same region of code and if all want to access parallelly then the outcome is overlapped. Say, for example, multiple users are using one printer only (common/critical section), say 3 users, given 3 jobs at same time, if all the jobs start parallelly, then one user output is overlapped with another. So, we need to protect that using semaphores i.e., locking the critical section when one process is running and unlocking when it is done. This would be repeated for each user/process so that one job is not overlapped with another job.\n\nBasically semaphores are classified into two types −\n\nBinary Semaphores − Only two states 0 & 1, i.e., locked/unlocked or available/unavailable, Mutex implementation.\n\nCounting Semaphores − Semaphores which allow arbitrary resource count are called counting semaphores.\n\nAssume that we have 5 printers (to understand assume that 1 printer only accepts 1 job) and we got 3 jobs to print. Now 3 jobs would be given for 3 printers (1 each). Again 4 jobs came while this is in progress. Now, out of 2 printers available, 2 jobs have been scheduled and we are left with 2 more jobs, which would be completed only after one of the resource/printer is available. This kind of scheduling as per resource availability can be viewed as counting semaphores.\n\nTo perform synchronization using semaphores, following are the steps −\n\nStep 1 − Create a semaphore or connect to an already existing semaphore (semget())\n\nStep 2 − Perform operations on the semaphore i.e., allocate or release or wait for the resources (semop())\n\nNow, let us check this with the system calls we have.\n\nThis system call creates or allocates a System V semaphore set. The following arguments need to be passed −\n• None The first argument, key, recognizes the message queue. The key can be either an arbitrary value or one that can be derived from the library function ftok().\n• None The second argument, nsems, specifies the number of semaphores. If binary then it is 1, implies need of 1 semaphore set, otherwise as per the required count of number of semaphore sets.\n• None The third argument, semflg, specifies the required semaphore flag/s such as IPC_CREAT (creating semaphore if it does not exist) or IPC_EXCL (used with IPC_CREAT to create semaphore and the call fails, if a semaphore already exists). Need to pass the permissions as well.\n\nThis call would return valid semaphore identifier (used for further calls of semaphores) on success and -1 in case of failure. To know the cause of failure, check with errno variable or perror() function.\n\nVarious errors with respect to this call are EACCESS (permission denied), EEXIST (queue already exists cant create), ENOENT (queue doesnt exist), ENOMEM (not enough memory to create the queue), ENOSPC (maximum sets limit exceeded), etc.\n\nThis system call performs the operations on the System V semaphore sets viz., allocating resources, waiting for the resources or freeing the resources. Following arguments need to be passed −\n• None The first argument, semid, indicates semaphore set identifier created by semget().\n• None The second argument, semops, is the pointer to an array of operations to be performed on the semaphore set. The structure is as follows −\n\nElement, sem_op, in the above structure, indicates the operation that needs to be performed −\n• None If sem_op is ve, allocate or obtain resources. Blocks the calling process until enough resources have been freed by other processes, so that this process can allocate.\n• None If sem_op is zero, the calling process waits or sleeps until semaphore value reaches 0.\n• None The third argument, nsemops, is the number of operations in that array.\n\nThis system call performs control operation for a System V semaphore. The following arguments need to be passed −\n• None The first argument, semid, is the identifier of the semaphore. This id is the semaphore identifier, which is the return value of semget() system call.\n• None The second argument, semnum, is the number of semaphore. The semaphores are numbered from 0.\n• None The third argument, cmd, is the command to perform the required control operation on the semaphore.\n• None The fourth argument, of type, union semun, depends on the cmd. For few cases, the fourth argument is not applicable.\n\nLet us check the union semun −\n\nThe semid_ds data structure which is defined in sys/sem.h is as follows −\n\nNote − Please refer manual pages for other data structures.\n• None IPC_STAT − Copies the information of the current values of each member of struct semid_ds to the passed structure pointed by arg.buf. This command requires read permission to the semaphore.\n• None IPC_SET − Sets the user ID, group ID of the owner, permissions, etc. pointed to by the structure semid_ds.\n• None IPC_INFO − Returns the information about the semaphore limits and parameters in the structure semid_ds pointed by arg.__buf.\n• None SEM_INFO − Returns a seminfo structure containing information about the consumed system resources by the semaphore.\n\nThis call would return value (non-negative value) depending upon the passed command. Upon success, IPC_INFO and SEM_INFO or SEM_STAT returns the index or identifier of the highest used entry as per Semaphore or the value of semncnt for GETNCNT or the value of sempid for GETPID or the value of semval for GETVAL 0 for other operations on success and -1 in case of failure. To know the cause of failure, check with errno variable or perror() function.\n\nBefore looking at the code, let us understand its implementation −\n• None Create two processes say, child and parent.\n• None Create shared memory mainly needed to store the counter and other flags to indicate end of read/write process into the shared memory.\n• None The counter is incremented by count by both parent and child processes. The count is either passed as a command line argument or taken as default (if not passed as command line argument or the value is less than 10000). Called with certain sleep time to ensure both parent and child accesses the shared memory at the same time i.e., in parallel.\n• None Since, the counter is incremented in steps of 1 by both parent and child, the final value should be double the counter. Since, both parent and child processes performing the operations at same time, the counter is not incremented as required. Hence, we need to ensure the completeness of one process completion followed by other process.\n• None All the above implementations are performed in the file shm_write_cntr.c\n• None Check if the counter value is implemented in file shm_read_cntr.c\n• None To ensure completion, the semaphore program is implemented in file shm_write_cntr_with_sem.c. Remove the semaphore after completion of the entire process (after read is done from other program)\n• None Since, we have separate files to read the value of counter in the shared memory and dont have any effect from writing, the reading program remains the same (shm_read_cntr.c)\n• None It is always better to execute the writing program in one terminal and reading program from another terminal. Since, the program completes execution only after the writing and reading process is complete, it is ok to run the program after completely executing the write program. The write program would wait until the read program is run and only finishes after it is done.\n\nNow, let us check the shared memory reading program.\n\nIf you observe the above output, the counter should be 20000, however, since before completion of one process task other process is also processing in parallel, the counter value is not as expected. The output would vary from system to system and also it would vary with each execution. To ensure the two processes perform the task after completion of one task, it should be implemented using synchronization mechanisms.\n\nNow, let us check the same application using semaphores.\n\nNow, we will check the counter value by the reading process."
    },
    {
        "link": "https://mohitdtumce.medium.com/understanding-interprocess-communication-ipc-pipes-message-queues-shared-memory-rpc-902f918fba58",
        "document": "There are several methods for communication within a single machine. These methods are known as Interprocess Communication (IPC) and allow different processes to communicate with each other. Some common methods of IPC include Pipes, Named Pipes, Message Queues, Shared Memory, Remote Procedure Calls (RPC), Semaphores and Sockets.\n• Pipes are a form of IPC that allow two or more processes to communicate with each other by creating a unidirectional channel between them (i.e. data to be transferred between processes in one direction only). One process writes to the pipe, and the other process reads from it.\n• Pipes are implemented using system calls in most modern operating systems, including Linux, macOS, and Windows.\n• There are two types of pipes: Anonymous Pipes and Named Pipes (FIFOs).\n• Messages and Buffer: When using anonymous pipes, messages sent from a producer process to a subscriber process are stored in a buffer maintained by the kernel (typically implementing it as a fixed-size byte stream). The buffer’s size can vary depending on the operating system and its configuration. If the buffer becomes full and the producer tries to write more data, the write operation may block until space becomes available. Similarly, if the buffer becomes empty and the subscriber tries to read data, the read operation may block until new data is available.\n• Creation of Named Pipes: Anonymous pipes are created using the system call in Unix-like operating systems. This creates a pair of file descriptors: one for the read end (receiving) of the pipe and one for the write end (sending) of the pipe. These file descriptors allow processes to interact with the pipe, but they do not directly represent the buffer managed by the kernel.\n• Sending and Receiving Data: Data flows unidirectionally, with the parent process writing data to the write end, and the child process reading from the read end. The pipe acts as a buffer, facilitating the transfer of data between the two processes.\n• Use Case: Anonymous pipes are commonly used between a parent process and its child process. After creating the pipe, the parent process forks a child process, and both processes have access to the read and write ends of the pipe.\n• File-Based Communication: Named pipes use the file system as a medium for communication between processes. They are created as special files in a directory, just like regular files. However, these files are not meant to store data on disk. Instead, they act as virtual channels through which data can flow between processes.\n• Creation: To create a named pipe, you use a ) system call provided by the operating system. Once created, the named pipe file appears in the file system, and processes can interact with it like they would with any other file.\n• Two Ends: A named pipe has two ends: a reading end and a writing end. These ends function as entry and exit points for data. One process can write data to the writing end of the named pipe, and another process can read the same data from the reading end. This creates a unidirectional flow of data.\n• Synchronization: Named pipes handle synchronization automatically. If a process tries to read from an empty buffer, it will be blocked until data becomes available. Similarly, if a process tries to write to a full buffer, it will be blocked until there’s space in the buffer.\n• Communication Beyond Parent-Child: One of the significant advantages of named pipes is that they can facilitate communication between unrelated processes. While they can still be used between parent and child processes, they extend their functionality to enable data exchange between processes that might not share a direct hierarchical relationship.\n• Lifespan and Cleanup: Named pipes persist beyond the lifespan of the processes that use them. They remain as files in the file system until explicitly removed. This allows different processes to communicate even if they start and stop at different times.\n\nMessage Queues allows processes to exchange data in the form of messages between two processes. It allows processes to communicate asynchronously by sending messages to each other where the messages are stored in a queue, waiting to be processed, and are deleted after being processed.\n• Creation: To use a message queue, a process first needs to create or open a message queue. This is done using system calls like which takes a key (message queue identifier) and creates or opens a message queue associated with that key. If the message queue doesn't already exist, it's created; if it does exist, the process gains access to it.\n• Structure of Messages: Each message in the queue has a specific structure. Every message has a positive long integer type field, a non-negative length, and the actual data bytes (corresponding to the length). The message type is a numerical identifier that helps the receiving process identify the type of message it wants to retrieve.\n• Sending Messages: To send a message, a process uses the system call. It specifies the message queue's identifier, the message type, the data to be sent, and some control flags. The message is then added to the message queue, waiting for the recipient process to retrieve it.\n• Receiving Messages: To receive a message, a process uses the system call. It specifies the message queue's identifier, the message type it wants to receive, a buffer to hold the message data, and other control flags. The kernel retrieves the message from the message queue that matches the specified type and copies it into the buffer.\n• Synchronization and Blocking: Message queues offer built-in synchronization. If a process tries to receive a message from an empty queue, it can be blocked until a message of the specified type becomes available. Similarly, if a process tries to send a message to a full queue, it can be blocked until there’s space.\n• Message Priority: Some implementations of message queues support message priorities. This means that higher-priority messages are retrieved before lower-priority ones.\n• Cleanup and Deletion: When a process is done using a message queue, it can close it using the system call. If no processes are using the message queue, it can be removed from the system using the same system call.\n• Persistence: Depending on the system, message queues might persist even if the processes using them terminate. This allows processes to communicate even if they start and stop at different times.\n\nMessage queues and pipes are both mechanisms for inter-process communication (IPC), but they differ in several important ways:\n• Message Queues: Message queues provide a structured way for processes to send and receive discrete messages. Messages can be of different types and sizes, and they are usually stored in a queue data structure. This allows for asynchronous communication, where processes can operate independently and handle messages at their own pace.\n• Pipes: Pipes are unidirectional communication channels that transfer a stream of bytes from one process to another. They are typically used for simple data exchange between a producer process (writing end) and a consumer process (reading end), and they rely on a sequential, byte-oriented data flow.\n• Message Queues: Messages in message queues can have complex structures, including various data types and structures. This structured approach makes them suitable for sending well-defined messages.\n• Pipes: Pipes handle data as an unstructured stream of bytes. They are less suitable for sending structured data and are often used for transmitting textual or binary data.\n• Message Queues: Message queues offer built-in synchronization. If a process tries to read from an empty queue, it will wait until a message arrives. Similarly, if a process tries to send a message to a full queue, it will wait until there’s space.\n• Pipes: Pipes don’t inherently provide synchronization. If the reading process tries to read from an empty pipe, it might block or receive an end-of-file indication. Similarly, if the writing process tries to write to a full pipe, it might block or encounter an error.\n• Message Queues: Message queues can support bidirectional communication, allowing two processes to communicate with each other using two separate queues.\n• Pipes: Pipes are unidirectional by nature. To establish bidirectional communication, two pipes are needed — one for each direction.\n• Message Queues: In some systems, message queues can persist even if the sending or receiving processes terminate. This allows messages to be stored for future retrieval.\n• Pipes: Pipes are ephemeral and exist only as long as the processes that created them are running. Once the processes close their pipe ends, the pipe is removed.\n• Message Queues: Message queues are not limited by process hierarchy. They can be used between related processes (parent-child) or unrelated processes.\n• Pipes: Pipes are commonly used between a parent process and its child, with the parent process typically creating the pipe and forking the child.\n\nIn essence, message queues are more versatile and suitable for structured communication between processes, whereas pipes are simpler and are often used for direct data streaming between related processes. The choice between message queues and pipes depends on the complexity of the data being exchanged, the synchronization requirements, and the relationship between the communicating processes.\n\nA memory section is shared between different processes. In other words, one process writes to this memory and another process can read from this memory. This allows for fast communication between processes as data doesn’t have to be copied around. However, due to the potential for synchronization issues, careful management of synchronization mechanisms is crucial to maintaining data integrity and preventing conflicts.\n\nLet’s dive into the practical aspects of how shared memory works in Unix-like systems:\n\nTo use shared memory, you first need to create it. This is typically done using the system call, which allocates a shared memory segment. The call takes parameters such as a key (an identifier for the shared memory) and the size of the memory segment you want to create.\n\nOnce the shared memory is created, processes that want to use it need to attach to it. This is done using the system call. It returns a pointer to the shared memory segment, allowing the process to access it.\n\nWith the shared memory attached, processes can now read from and write to the shared memory region just like any other memory. Data written by one process can be immediately accessed by another process attached to the same segment.\n\nShared memory can be accessed by multiple processes simultaneously, which can lead to race conditions. Synchronization mechanisms like semaphores or mutexes are used to ensure data consistency.\n\nExample: Using a semaphore to control access to the shared memory.\n\nWhen a process is done using the shared memory, it should detach from it using the system call.\n\nWhen shared memory is no longer needed by any process, it should be de-allocated using the system call with the command.\n\nShared memory segments, like other IPC resources, have ownership and permissions associated with them. Proper permissions ensure that only authorized processes can access the shared memory.\n\nSystem calls related to shared memory return error codes that should be checked to handle various scenarios, such as when creating or attaching to shared memory fails.\n\nShared memory is not limited by process hierarchy. Different processes can access the shared memory as long as they have the required permissions.\n\nIt’s important to properly clean up shared memory resources when they are no longer needed. Detaching and de-allocating shared memory segments prevents resource leaks.\n\nSemaphores are a synchronization mechanism used to coordinate the activities of multiple processes in a computer system. They are used to enforce mutual exclusion, avoid race conditions and implement synchronization between processes.\n\nSemaphores provide two operations: wait (P) and signal (V). The wait operation decrements the value of the semaphore, and the signal operation increments the value of the semaphore. When the value of the semaphore is zero, any process that performs a wait operation will be blocked until another process performs a signal operation.\n\nSemaphores are used to implement critical sections, which are regions of code that must be executed by only one process at a time. By using semaphores, processes can coordinate access to shared resources, such as shared memory or I/O devices.\n\nSemaphores differ from other IPC methods such as Pipes, Message Queues and Shared Memory in that they are not used for direct communication between processes. Instead, they are used to coordinate access to shared resources and ensure that only one process can access a shared resource at a time.\n\nSockets are an inter-process communication (IPC) mechanism that allows two or more processes to communicate with each other by creating a bidirectional channel between them. A socket is one endpoint of a two-way communication link between two programs running on the network. The socket mechanism provides a means of IPC by establishing named contact points between which the communication takes place.\n\nPOSIX sockets are a type of socket available in the POSIX API. There are two types of POSIX sockets: IPC sockets (aka Unix domain sockets) and network sockets. IPC sockets enable channel-based communication for processes on the same physical device (host), whereas network sockets enable this kind of IPC for processes that can run on different hosts, thereby bringing networking into play.\n\nPOSIX sockets differ from other IPC methods such as Pipes, Message Queues and Shared Memory in that they can be used for both local and network communication.\n\nMiddleware is software that lies between an operating system and the applications running on it. It enables communication and data management for distributed applications. Some common examples of middleware include database middleware, application server middleware, message-oriented middleware, web middleware, and transaction-processing monitors.\n\nEach program typically provides messaging services so that different applications can communicate using messaging frameworks like simple object access protocol (SOAP), web services, representational state transfer (REST), and JavaScript object notation (JSON). While all middleware performs communication functions, the type a company chooses to use will depend on what service is being used and what type of information needs to be communicated\n• Database middleware provides a way for applications to access data stored in databases. Some examples of database middleware include Open Database Connectivity (ODBC) and Java Database Connectivity (JDBC).\n• Application server middleware provides a platform for building and deploying enterprise applications. Some examples of application server middleware include Apache Tomcat and IBM WebSphere.\n• Message-oriented middleware provides a way for applications to communicate with each other using messages. Some examples of message-oriented middleware include Apache Kafka and RabbitMQ.\n• Web middleware provides a way for web applications to communicate with each other. Some examples of web middleware include Express.js and Ruby on Rails.\n• Transaction-processing monitors provide a way to manage transactions in distributed systems. Some examples of transaction-processing monitors include IBM CICS and Oracle Tuxedo.\n\nGoogle Protocol Buffers (protobuf) is a language-neutral, platform-neutral extensible mechanism for serializing structured data. It was developed by Google for internal use and provided a code generator for multiple languages under an open-source license. The design goals for Protocol Buffers emphasized simplicity and performance. In particular, it was designed to be smaller and faster than XML.\n• Protocol Buffers are widely used at Google for storing and interchanging all kinds of structured information. The method serves as a basis for a custom remote procedure call (RPC) system that is used for nearly all inter-machine communication at Google.\n• To use protobuf, you first define your data structure in a .proto file using the protobuf interface definition language (IDL). This file specifies the fields and their types for the messages you want to serialize. Once you have defined your messages, you can use the protobuf compiler (protoc) to generate source code in the language of your choice. This generated code includes classes for each message type, with methods for reading and writing the message to a binary format.\n\nHope you enjoyed reading. I’m always open to suggestions and new ideas. Please write to me :)"
    },
    {
        "link": "https://linkedin.com/pulse/interprocess-communicationipc-semaphores-pratik-parvati",
        "document": "Shared memory an IPC mechanism is about two processes sharing a common segment of memory that they can both read to and write from to communicate with one another. If you are new here, please read my previous article IPC using shared memory; this article is a prolongation to my previous article. Here I am talking more about semaphores and their usage, working with shared resources.\n\nThere is a problem with using shared memory - shared memory (aka shared resource) when accessed by two or more processes without the mechanism of guaranteeing safe read and write operations leaves your code open to nasty race conditions. This problem arises due to preemptive multitasking, where OS takes care of managing when and for how long each process gets to execute. One simple way of solving this problem is semaphores.\n\nTo avoid a series of issues caused by multiple processes (or programs) accessing a shared resource at the same time, we need a method that can be authorised by generating and using a token, so that only one execution thread can access the critical section of the code at any given time. The critical section is a code segment where the shared variables can be accessed and the atomic action is required in this section.\n\nA semaphore is implemented as an integer variable with atomic increment and decrement operations; so long as the value is not negative the thread will continue, it will block otherwise. The increment operation is called V, or signal; the decrement is called P, or wait.\n• wait(): decreases the counter by one; if the counter is negative, then it puts the thread on a queue and blocks.\n• signal(): increments the counter; wakes up one waiting process.\n\nThe most common and simplest kind of semaphore is called a binary semaphore because they have two states locked or unlocked. It guarantees that only one process will be in a critical section at a time.\n\nA counting semaphore is associated with an integer count and two operations, namely, increment and decrement. The increment is equivalent to a set operation for a flag semaphore and a decrement is equivalent to a reset.\n\nThe signal and wait operations are implemented as follows\n\nDiagram showing how the P and V operatons act as a gate into critical sections of code.\n• Compile either with -lrt or -lpthread\n• pshared indicates whether this semaphore is to be shared between the threads of a process, or between processes:\n• zero: semaphore is shared between the threads of a process; should be located at an address visible to all threads.\n• non-zero: semaphore is shared among processes and shouldbe located in a region of shared memory.\n• The sem_wait() function locks the specified semaphore by performing a semaphore lock operation on that semaphore.\n• The sem_trywait() function locks the specified semaphore only if that semaphore is currently not locked; that is, if the semaphore value is currently non-zero.\n• When this operation results in a positive semaphore value, no threads were blocked waiting for the semaphore to be unlocked; the semaphore value is simply incremented.\n• When this operation results in a semaphore value of zero, one of the threads waiting for the semaphore is allowed to return successfully from its invocation of the sem_wait() function.\n• Subsequent use of the specified semaphore by another semaphore function results in that function failing with errno set to EINVAL.\n\nThe sem_trywait() return zero if the calling process successfully performed the semaphore lock operation on the semaphore designated by sem. If the call was unsuccessful, the state of the semaphore is unchanged and it returns -1.\n\nIn the next article; I will write about another IPC mechanism message queue."
    }
]