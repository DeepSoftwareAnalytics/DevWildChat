[
    {
        "link": "https://docs.python.org/3/library/shutil.html",
        "document": "The module offers a number of high-level operations on files and collections of files. In particular, functions are provided which support file copying and removal. For operations on individual files, see also the module.\n\nChanged in version 3.5: Added support for the xztar format. High-level utilities to create and read compressed and archived files are also provided. They rely on the and modules. Create an archive file (such as zip or tar) and return its name. base_name is the name of the file to create, including the path, minus any format-specific extension. format is the archive format: one of “zip” (if the module is available), “tar”, “gztar” (if the module is available), “bztar” (if the module is available), or “xztar” (if the module is available). root_dir is a directory that will be the root directory of the archive, all paths in the archive will be relative to it; for example, we typically chdir into root_dir before creating the archive. base_dir is the directory where we start archiving from; i.e. base_dir will be the common prefix of all files and directories in the archive. base_dir must be given relative to root_dir. See Archiving example with base_dir for how to use base_dir and root_dir together. root_dir and base_dir both default to the current directory. If dry_run is true, no archive is created, but the operations that would be executed are logged to logger. owner and group are used when creating a tar archive. By default, uses the current owner and group. logger must be an object compatible with PEP 282, usually an instance of . The verbose argument is unused and deprecated. This function is not thread-safe when custom archivers registered with do not support the root_dir argument. In this case it temporarily changes the current working directory of the process to root_dir to perform archiving. Changed in version 3.8: The modern pax (POSIX.1-2001) format is now used instead of the legacy GNU format for archives created with . Changed in version 3.10.6: This function is now made thread-safe during creation of standard and tar archives. Return a list of supported formats for archiving. Each element of the returned sequence is a tuple . By default provides these formats:\n• None zip: ZIP file (if the module is available).\n• None tar: Uncompressed tar file. Uses POSIX.1-2001 pax format for new archives.\n• None gztar: gzip’ed tar-file (if the module is available).\n• None bztar: bzip2’ed tar-file (if the module is available).\n• None xztar: xz’ed tar-file (if the module is available). You can register new formats or provide your own archiver for any existing formats, by using . Register an archiver for the format name. function is the callable that will be used to unpack archives. The callable will receive the base_name of the file to create, followed by the base_dir (which defaults to ) to start archiving from. Further arguments are passed as keyword arguments: owner, group, dry_run and logger (as passed in ). If function has the custom attribute set to , the root_dir argument is passed as a keyword argument. Otherwise the current working directory of the process is temporarily changed to root_dir before calling function. In this case is not thread-safe. If given, extra_args is a sequence of pairs that will be used as extra keywords arguments when the archiver callable is used. description is used by which returns the list of archivers. Defaults to an empty string. Changed in version 3.12: Added support for functions supporting the root_dir argument. Remove the archive format name from the list of supported formats. Unpack an archive. filename is the full path of the archive. extract_dir is the name of the target directory where the archive is unpacked. If not provided, the current working directory is used. format is the archive format: one of “zip”, “tar”, “gztar”, “bztar”, or “xztar”. Or any other format registered with . If not provided, will use the archive file name extension and see if an unpacker was registered for that extension. In case none is found, a is raised. The keyword-only filter argument is passed to the underlying unpacking function. For zip files, filter is not accepted. For tar files, it is recommended to set it to , unless using features specific to tar and UNIX-like filesystems. (See Extraction filters for details.) The filter will become the default for tar files in Python 3.14. Never extract archives from untrusted sources without prior inspection. It is possible that files are created outside of the path specified in the extract_dir argument, e.g. members that have absolute filenames starting with “/” or filenames with two dots “..”. Changed in version 3.7: Accepts a path-like object for filename and extract_dir. Registers an unpack format. name is the name of the format and extensions is a list of extensions corresponding to the format, like for Zip files. function is the callable that will be used to unpack archives. The callable will receive:\n• None the path of the archive, as a positional argument;\n• None the directory the archive must be extracted to, as a positional argument;\n• None possibly a filter keyword argument, if it was given to ;\n• None additional keyword arguments, specified by extra_args as a sequence of tuples. description can be provided to describe the format, and will be returned by the function. Unregister an unpack format. name is the name of the format. Return a list of all registered formats for unpacking. Each element of the returned sequence is a tuple . By default provides these formats:\n• None zip: ZIP file (unpacking compressed files works only if the corresponding module is available).\n• None gztar: gzip’ed tar-file (if the module is available).\n• None bztar: bzip2’ed tar-file (if the module is available).\n• None xztar: xz’ed tar-file (if the module is available). You can register new formats or provide your own unpacker for any existing formats, by using . In this example, we create a gzip’ed tar-file archive containing all files found in the directory of the user: In this example, similar to the one above, we show how to use , but this time with the usage of base_dir. We now have the following directory structure: In the final archive, should be included, but should not. Therefore we use the following: Listing the files in the resulting archive gives us:"
    },
    {
        "link": "https://geeksforgeeks.org/python-shutil-copytree-method",
        "document": "Shutil module in Python provides many functions of high-level operations on files and collections of files. It comes under Python’s standard utility modules. This module helps in automating process of copying and removal of files and directories.\n\n method recursively copies an entire directory tree rooted at source (src) to the destination directory. The destination directory, named by (dst) must not already exist. It will be created during copying. Permissions and times of directories are copied with copystat() and individual files are copied using shutil.copy2().\n\nSyntax: shutil.copytree(src, dst, symlinks = False, ignore = None, copy_function = copy2, igonre_dangling_symlinks = False)\n\n Parameters: \n\n src: A string representing the path of the source directory.\n\n dest: A string representing the path of the destination.\n\n symlinks (optional) : This parameter accepts True or False, depending on which the metadata of the original links or linked links will be copied to the new tree.\n\n ignore (optional) : If ignore is given, it must be a callable that will receive as its arguments the directory being visited by , and a list of its contents, as returned by .\n\n copy_function (optional): The default value of this parameter is copy2. We can use other copy function like for this parameter.\n\n igonre_dangling_symlinks (optional) : This parameter value when set to True is used to put a silence on the exception raised if the file pointed by the symlink doesn’t exist.\n\n Return Value: This method returns a string which represents the path of newly created directory.\n\nExample #1 :\n\n Using method to copy file from source to destination"
    },
    {
        "link": "https://note.nkmk.me/en/python-shutil-copy-copytree",
        "document": "In Python, you can copy a file with or , and a directory (folder) with .\n\nIf you want to move or delete files and directories, refer to the following articles.\n\nThis article does not cover detailed specifications, like handling symbolic links. For comprehensive information, please check the official Python documentation.\n\nThe sample code in this article imports the module as shown below. It is part of the standard library, so no additional installation is necessary.\n\nTo copy a file, use or .\n\nAlthough both functions have identical usage, attempts to copy metadata such as creation and modification times, which does not.\n\nThe first argument should be the path of the source file, and the second should be the path of the destination directory or file. The function returns the path of the newly copied file.\n\nPaths can be represented as either strings or path-like objects like . When specifying a directory as a string, the trailing delimiter ( ) is optional.\n\nConsider the following files and directories:\n\nIf the second argument is an existing directory, the source file will be copied into it. If a file with the same name already exists in this directory, the source file will overwrite it.\n\nIf you provide a new path as the second argument, the file will be copied with that filename. Ensure all directories in the path exist before copying to avoid errors from non-existent directories.\n\nBe aware that if you attempt to specify a new directory as the second argument, for example, , it will be interpreted as a filename and copied as a file named . Specifying will result in an error. You must first create a directory.\n\nIf the second argument is an existing file, it will be overwritten by the source file.\n\nThe results are as follows. The content of , which was provided as the source in the first argument, is copied and replaces the content of the destination files.\n\nThe difference between and\n\nThe primary difference between and is their handling of metadata (creation time, modification time, etc.). While does not copy metadata, attempts to do so.\n\nIn the case of , the copied file's modification time is the time when the copy was made. You can verify this by comparing the modification times of the source and destination files using .\n• Get file timestamp in Python (os.stat, os.path.getmtime, and more)\n\nIn contrast, attempts to copy as much metadata as possible. For example, the modification times of the source and destination files are the same.\n\nIt is important to remember that cannot copy all metadata.\n\nUse to recursively copy a directory along with all its files and subdirectories.\n\nSpecify the path of the source directory as the first argument, and the path of the destination directory as the second. The function returns the path of the destination directory.\n\nConsider the following files and directories:\n\nIf you specify a new path (non-existent path) as the second argument, all directories, including intermediate ones, are created, and the contents are copied.\n\nBy default, if you specify an existing directory as the destination, an error will occur. To avoid this, use the argument described next.\n\nAs mentioned above, by default, specifying an existing directory as the second argument raises an error.\n\nHowever, by setting the argument to , this error will be prevented. If the destination directory contains a file with the same name as one in the source directory, the destination file will be replaced.\n\nYou can specify the function used for copying with the argument.\n\nThe default is , which tries to preserve as much metadata as possible. If you prefer not to duplicate the metadata, you can specify instead.\n\nSpecify files and directories to ignore:\n\nBy specifying to the argument, you can exclude certain files or directories from the copying process.\n\nYou can specify multiple glob-style patterns in . You can use wildcards such as . Files and directories whose names match the provided patterns will not be copied.\n• How to use glob() in Python\n\nConsider the following files and directories:\n\nHere, files and directories starting with and files with the extension are ignored.\n\nCopy multiple files based on certain conditions with wildcards and regex\n\nFor a practical example, let's examine how to copy multiple files based on certain conditions.\n\nPlease note that using in can be time-consuming with a large number of files and directories. For improved efficiency, consider specifying conditions with other special characters when feasible.\n\nIf possible, using the argument of to specify conditions can simplify the process.\n\nUse wildcards to specify conditions\n\nThe module allows you to use wildcard characters like to generate a list of file and directory names. For detailed usage, refer to the following article.\n• How to use glob() in Python\n\nConsider the following files and directories:\n\nFor example, you can extract and copy files with the extension, including those in subdirectories. The destination directory must be created beforehand.\n\nIn the example above, there's no problem. However, under different conditions, directories might be included in the results. Therefore, is used to target only files.\n• Check if a file or a directory exists in Python\n\nIf you want to copy directories, use and instead of and .\n\nIf you want to maintain the directory structure of the source, you can do as follows.\n\nSpecify the argument in , use to extract the directory name, and use to concatenate paths. You don't need to create the destination directory in advance because it's generated with .\n• Get the filename, directory, extension from a path string in Python\n\nUse regex to specify conditions\n\nFor complex conditions that can't be handled by glob alone, use the module to specify conditions with regular expressions.\n• Regular expressions with the re module in Python\n\nThe basic approach remains the same as when using alone.\n\nConsider the following files and directories:\n\nUse with and to recursively list all files and directories, and then filter them by regex with .\n\nFor example, to extract and copy files with names consisting only of digits and either a or extension, including files in subdirectories, use to match digits, to match one or more repetitions, and to match either or .\n\nIf you want to retain the directory structure of the source, do as follows."
    },
    {
        "link": "https://docs.python.org/2/library/shutil.html",
        "document": "The module offers a number of high-level operations on files and collections of files. In particular, functions are provided which support file copying and removal. For operations on individual files, see also the module.\n\nEven the higher-level file copying functions ( , ) can’t copy all file metadata. On POSIX platforms, this means that file owner and group are lost as well as ACLs. On Mac OS, the resource fork and other metadata are not used. This means that resources will be lost and file type and creator codes will not be correct. On Windows, file owners, ACLs and alternate data streams are not copied.\n\nCopy the contents of the file-like object fsrc to the file-like object fdst. The integer length, if given, is the buffer size. In particular, a negative length value means to copy the data without looping over the source data in chunks; by default the data is read in chunks to avoid uncontrolled memory consumption. Note that if the current file position of the fsrc object is not 0, only the contents from the current file position to the end of the file will be copied. Copy the contents (no metadata) of the file named src to a file named dst. dst must be the complete target file name; look at for a copy that accepts a target directory path. If src and dst are the same files, is raised. The destination location must be writable; otherwise, an exception will be raised. If dst already exists, it will be replaced. Special files such as character or block devices and pipes cannot be copied with this function. src and dst are path names given as strings. Copy the permission bits from src to dst. The file contents, owner, and group are unaffected. src and dst are path names given as strings. Copy the permission bits, last access time, last modification time, and flags from src to dst. The file contents, owner, and group are unaffected. src and dst are path names given as strings. Copy the file src to the file or directory dst. If dst is a directory, a file with the same basename as src is created (or overwritten) in the directory specified. Permission bits are copied. src and dst are path names given as strings. Identical to except that also attempts to preserve file metadata. uses to copy the file metadata. Please see for more information. This factory function creates a function that can be used as a callable for ’s ignore argument, ignoring files and directories that match one of the glob-style patterns provided. See the example below. Recursively copy an entire directory tree rooted at src. The destination directory, named by dst, must not already exist; it will be created as well as missing parent directories. Permissions and times of directories are copied with , individual files are copied using . If symlinks is true, symbolic links in the source tree are represented as symbolic links in the new tree, but the metadata of the original links is NOT copied; if false or omitted, the contents and metadata of the linked files are copied to the new tree. If ignore is given, it must be a callable that will receive as its arguments the directory being visited by , and a list of its contents, as returned by . Since is called recursively, the ignore callable will be called once for each directory that is copied. The callable must return a sequence of directory and file names relative to the current directory (i.e. a subset of the items in its second argument); these names will then be ignored in the copy process. can be used to create such a callable that ignores names based on glob-style patterns. If exception(s) occur, an is raised with a list of reasons. The source code for this should be considered an example rather than the ultimate tool. Changed in version 2.3: is raised if any exceptions occur during copying, rather than printing a message. Changed in version 2.5: Create intermediate directories needed to create dst, rather than raising an error. Copy permissions and times of directories using . Changed in version 2.6: Added the ignore argument to be able to influence what is being copied. Delete an entire directory tree; path must point to a directory (but not a symbolic link to a directory). If ignore_errors is true, errors resulting from failed removals will be ignored; if false or omitted, such errors are handled by calling a handler specified by onerror or, if that is omitted, they raise an exception. If onerror is provided, it must be a callable that accepts three parameters: function, path, and excinfo. The first parameter, function, is the function which raised the exception; it will be , , or . The second parameter, path, will be the path name passed to function. The third parameter, excinfo, will be the exception information return by . Exceptions raised by onerror will not be caught. Changed in version 2.6: Explicitly check for path being a symbolic link and raise in that case. Recursively move a file or directory (src) to another location (dst). If the destination is an existing directory, then src is moved inside that directory. If the destination already exists but is not a directory, it may be overwritten depending on semantics. If the destination is on the current filesystem, then is used. Otherwise, src is copied (using ) to dst and then removed. This exception collects exceptions that are raised during a multi-file operation. For , the exception argument is a list of 3-tuples (srcname, dstname, exception). This example is the implementation of the function, described above, with the docstring omitted. It demonstrates many of the other functions provided by this module. # XXX What about devices, sockets etc.? # catch the Error from the recursive copytree so that we can Another example that uses the helper: This will copy everything except files and files or directories whose name starts with . Another example that uses the ignore argument to add a logging call: # nothing will be ignored\n\nHigh-level utilities to create and read compressed and archived files are also provided. They rely on the and modules. Create an archive file (eg. zip or tar) and returns its name. base_name is the name of the file to create, including the path, minus any format-specific extension. format is the archive format: one of “zip” (if the module or external executable is available), “tar”, “gztar” (if the module is available), or “bztar” (if the module is available). root_dir is a directory that will be the root directory of the archive; ie. we typically chdir into root_dir before creating the archive. base_dir is the directory where we start archiving from; ie. base_dir will be the common prefix of all files and directories in the archive. root_dir and base_dir both default to the current directory. owner and group are used when creating a tar archive. By default, uses the current owner and group. logger must be an object compatible with PEP 282, usually an instance of . Return a list of supported formats for archiving. Each element of the returned sequence is a tuple . By default provides these formats:\n• None zip: ZIP file (if the module or external executable is available).\n• None gztar: gzip’ed tar-file (if the module is available).\n• None bztar: bzip2’ed tar-file (if the module is available). You can register new formats or provide your own archiver for any existing formats, by using . Register an archiver for the format name. function is a callable that will be used to invoke the archiver. If given, extra_args is a sequence of that will be used as extra keywords arguments when the archiver callable is used. description is used by which returns the list of archivers. Defaults to an empty list. Remove the archive format name from the list of supported formats. In this example, we create a gzip’ed tar-file archive containing all files found in the directory of the user:"
    },
    {
        "link": "https://geeksforgeeks.org/shutil-module-in-python",
        "document": "Shutil module offers high-level operation on a file like a copy, create, and remote operation on the file. It comes under Python’s standard utility modules. This module helps in automating the process of copying and removal of files and directories. In this article, we will learn this module.\n\nshutil.copy() method in Python is used to copy the content of the source file to the destination file or directory. It also preserves the file’s permission mode but other metadata of the file like the file’s creation and modification times is not preserved.\n\nThe source must represent a file but the destination can be a file or a directory. If the destination is a directory then the file will be copied into the destination using the base filename from the source. Also, the destination must be writable. If the destination is a file and already exists then it will be replaced with the source file otherwise a new file will be created.\n\nExample 2: If the destination is a directory.\n\nCopying the Metadata along with File\n\nshutil.copy2() method in Python is used to copy the content of the source file to the destination file or directory. This method is identical to shutil.copy() method but it also tries to preserve the file’s metadata.\n\nExample 2: If the destination is a directory\n\nCopying the content of one file to another\n\nshutil.copyfile() method in Python is used to copy the content of the source file to the destination file. The metadata of the file is not copied. Source and destination must represent a file and destination must be writable. If the destination already exists then it will be replaced with the source file otherwise a new file will be created.\n\nIf source and destination represent the same file then SameFileError exception will be raised.\n\nshutil.copytree() method recursively copies an entire directory tree rooted at source (src) to the destination directory. The destination directory, named by (dst) must not already exist. It will be created during copying.\n\nParameters:\n\nsrc: A string representing the path of the source directory.\n\ndest: A string representing the path of the destination.\n\nsymlinks (optional) : This parameter accepts True or False, depending on which the metadata of the original links or linked links will be copied to the new tree.\n\nignore (optional) : If ignore is given, it must be a callable that will receive as its arguments the directory being visited by copytree(), and a list of its contents, as returned by os.listdir().\n\ncopy_function (optional): The default value of this parameter is copy2. We can use other copy function like copy() for this parameter.\n\nignore_dangling_symlinks (optional) : This parameter value when set to True is used to put a silence on the exception raised if the file pointed by the symlink doesn’t exist. Return Value: This method returns a string which represents the path of newly created directory.\n\nshutil.rmtree() is used to delete an entire directory tree, the path must point to a directory (but not a symbolic link to a directory).\n\nshutil.which() method tells the path to an executable application that would be run if the given cmd was called. This method can be used to find a file on a computer which is present on the PATH.\n\nSyntax: shutil.which(cmd, mode = os.F_OK | os.X_OK, path = None)\n\nParameters:\n\ncmd: A string representing the file.\n\nmode: This parameter specifies mode by which method should execute. os.F_OK tests existence of the path and os.X_OK Checks if path can be executed or we can say mode determines if the file exists and executable.\n\npath: This parameter specifies the path to be used, if no path is specified then the results of os.environ() are used\n\nReturn Value: This method returns the path to an executable application"
    },
    {
        "link": "https://docs.python.org/3/library/os.path.html",
        "document": "Source code: Lib/genericpath.py, Lib/posixpath.py (for POSIX) and Lib/ntpath.py (for Windows).\n\nThis module implements some useful functions on pathnames. To read or write files see , and for accessing the filesystem see the module. The path parameters can be passed as strings, or bytes, or any object implementing the protocol.\n\nUnlike a Unix shell, Python does not do any automatic path expansions. Functions such as and can be invoked explicitly when an application desires shell-like path expansion. (See also the module.)\n\nSince different operating systems have different path name conventions, there are several versions of this module in the standard library. The module is always the path module suitable for the operating system Python is running on, and therefore usable for local paths. However, you can also import and use the individual modules if you want to manipulate a path that is always in one of the different formats. They all have the same interface:\n\nOn Unix and Windows, return the argument with an initial component of or replaced by that user’s home directory. On Unix, an initial is replaced by the environment variable if it is set; otherwise the current user’s home directory is looked up in the password directory through the built-in module . An initial is looked up directly in the password directory. On Windows, will be used if set, otherwise a combination of and will be used. An initial is handled by checking that the last directory component of the current user’s home directory matches , and replacing it if so. If the expansion fails or if the path does not begin with a tilde, the path is returned unchanged. Changed in version 3.8: No longer uses on Windows.\n\nJoin one or more path segments intelligently. The return value is the concatenation of path and all members of *paths, with exactly one directory separator following each non-empty part, except the last. That is, the result will only end in a separator if the last part is either empty or ends in a separator. If a segment is an absolute path (which on Windows requires both a drive and a root), then all previous segments are ignored and joining continues from the absolute path segment. On Windows, the drive is not reset when a rooted path segment (e.g., ) is encountered. If a segment is on a different drive or is an absolute path, all previous segments are ignored and the drive is reset. Note that since there is a current directory for each drive, represents a path relative to the current directory on drive ( ), not . Changed in version 3.6: Accepts a path-like object for path and paths."
    },
    {
        "link": "https://geeksforgeeks.org/python-os-path-join-method",
        "document": "The os.path.join() method is a function in the os module that joins one or more path components intelligently. It constructs a full path by concatenating various components while automatically inserting the appropriate path separator (/ for Unix-based systems and \\ for Windows).\n\nExplanation: In this case, the presence of the absolute path “/etc” resets the earlier components, resulting in only the absolute path being displayed.\n\nThe method processes the path components and resolves them into a single, absolute path. It handles:\n• Path Separators : It uses the correct path separator based on the operating system.\n• Absolute Paths : If an absolute path is provided at any level, it resets the previous components to that absolute path.\n\nBelow are some examples and use cases by which we can join file paths and handle file paths safely in Python OS.\n• Path 1 : This output is correct as the function constructs the path based on the provided components.\n• Path 2 is an absolute path. When an absolute path is included in the ), and the result reflects only the absolute path combined with the last component (\n• Path 3 : Again, the last component is treated as an absolute path, which resets the previous components. As a result, only\n\nExplanation: In this example, the method is utilized to form a complete file path by joining the base directory and the filename. The constructed path is then used to read the content of the file named .\n\nExplanation: In this example, the `os.path.join()` method is employed to generate the full path for each file in the current working directory. The complete paths are then printed, allowing for a comprehensive listing of all files in the directory.\n\nIterating Over Paths with a For Loop\n\nExplanation: In this example, the `os.path.join()` method is utilized within a loop to dynamically create the full path for each file name listed. The constructed paths are then printed to indicate the processing of each respective file.\n\nWhat is os.path in Python?\n\nWhat does path.join() do?"
    },
    {
        "link": "https://pythoncheatsheet.org/cheatsheet/file-directory-path",
        "document": "There are two main modules in Python that deal with path manipulation. One is the module and the other is the module.\n\nOn Windows, paths are written using backslashes ( ) as the separator between folder names. On Unix based operating system such as macOS, Linux, and BSDs, the forward slash ( ) is used as the path separator. Joining paths can be a headache if your code needs to work on different platforms.\n\nFortunately, Python provides easy ways to handle this. We will showcase how to deal with both, and\n\nAnd using on *nix:\n\nalso provides a shortcut to joinpath using the operator:\n\nNotice the path separator is different between Windows and Unix based operating system, that’s why you want to use one of the above methods instead of adding strings together to join paths together.\n\nJoining paths is helpful if you need to create different file paths under the same directory.\n\nOh no, we got a nasty error! The reason is that the ‘delicious’ directory does not exist, so we cannot make the ‘walnut’ and the ‘waffles’ directories under it. To fix this, do:\n\nAnd all is good :)\n\nThere are two ways to specify a file path.\n• An absolute path, which always begins with the root folder\n• A relative path, which is relative to the program’s current working directory\n\nThere are also the dot ( ) and dot-dot ( ) folders. These are not real folders, but special names that can be used in a path. A single period (“dot”) for a folder name is shorthand for “this directory.” Two periods (“dot-dot”) means “the parent folder.”\n\nTo see if a path is an absolute path:\n\nYou can extract an absolute path with both and\n\nYou can get a relative path from a starting path to another path.\n\nUsing and together on Windows:\n\nThe module provides functions for copying files, as well as entire folders.\n\nWhile will copy a single file, will copy an entire folder and every folder and file contained in it:\n\nThe destination path can also specify a filename. In the following example, the source file is moved and renamed:\n\nIf there is no eggs folder, then will rename bacon.txt to a file named eggs:\n• None Calling or will delete the file at path.\n• None Calling or will delete the folder at path. This folder must be empty of any files or folders.\n• None Calling will remove the folder at path, and all files and folders it contains will also be deleted."
    },
    {
        "link": "https://stackoverflow.com/questions/45684631/what-is-the-best-practice-for-working-with-files-and-paths",
        "document": "I am creating a python script that essentially takes a 'path to a file' as an argument by the user. It does some post processing and creates a new file in the same directory as the original file.\n\nI'm using the path I received itself to create a File Handler for\n\n\n\nI've been told to use to navigate to the folder instead and create my final file there instead of using paths directly. What is the best practice in such a scenario? Is there a risk in not changing the working directory?"
    },
    {
        "link": "https://stackoverflow.com/questions/67112343/pathlib-vs-os-path-join-in-python",
        "document": "When I need to define a file system path in my script, I use to guarantee that the path will be consistent on different file systems:\n\nI also know that there is library that basically does the same:\n\nWhat is the difference between these two ways to handle paths? Which one is better?"
    },
    {
        "link": "https://docs.python.org/3/library/configparser.html",
        "document": "This module provides the class which implements a basic configuration language which provides a structure similar to what’s found in Microsoft Windows INI files. You can use this to write Python programs which can be customized by end users easily.\n\nLet’s take a very basic configuration file that looks like this: The structure of INI files is described in the following section. Essentially, the file consists of sections, each of which contains keys with values. classes can read and write such files. Let’s start by creating the above configuration file programmatically. As you can see, we can treat a config parser much like a dictionary. There are differences, outlined later, but the behavior is very close to what you would expect from a dictionary. Now that we have created and saved a configuration file, let’s read it back and explore the data it holds. As we can see above, the API is pretty straightforward. The only bit of magic involves the section which provides default values for all other sections . Note also that keys in sections are case-insensitive and stored in lowercase . It is possible to read several configurations into a single , where the most recently added configuration has the highest priority. Any conflicting keys are taken from the more recent configuration while the previously existing keys are retained. The example below reads in an file, which will override any conflicting keys from the file. This behaviour is equivalent to a call with several files passed to the filenames parameter.\n\nAs with a dictionary, you can use a section’s method to provide fallback values: Please note that default values have precedence over fallback values. For instance, in our example the key was specified only in the section. If we try to get it from the section , we will always get the default, even if we specify a fallback: One more thing to be aware of is that the parser-level method provides a custom, more complex interface, maintained for backwards compatibility. When using this method, a fallback value can be provided via the keyword-only argument: 'No such things as monsters' 'No such things as monsters' The same argument can be used with the , and methods, for example:\n\nA configuration file consists of sections, each led by a header, followed by key/value entries separated by a specific string ( or by default ). By default, section names are case sensitive but keys are not . Leading and trailing whitespace is removed from keys and values. Values can be omitted if the parser is configured to allow it , in which case the key/value delimiter may also be left out. Values can also span multiple lines, as long as they are indented deeper than the first line of the value. Depending on the parser’s mode, blank lines may be treated as parts of multiline values or ignored. By default, a valid section name can be any string that does not contain ‘\n\n’. To change this, see . The first section name may be omitted if the parser is configured to allow an unnamed top level section with . In this case, the keys/values may be retrieved by as in . Configuration files may include comments, prefixed by specific characters ( and by default ). Comments may appear on their own on an otherwise empty line, possibly indented. are they treated as numbers? integers, floats and booleans are held as can use the API to get converted values directly I sleep all night and I work all day # By default only in an empty line. # Inline comments can be harmful because they prevent users # from using the delimiting characters as parts of values. # That being said, this can be customized. long as they are indented deeper than the first line # Did I mention we can indent comments, too?\n\nMapping protocol access is a generic name for functionality that enables using custom objects as if they were dictionaries. In case of , the mapping interface implementation is using the notation. in particular returns a proxy for the section’s data in the parser. This means that the values are not copied but they are taken from the original parser on demand. What’s even more important is that when values are changed on a section proxy, they are actually mutated in the original parser. objects behave as close to actual dictionaries as possible. The mapping interface is complete and adheres to the ABC. However, there are a few differences that should be taken into account:\n• None By default, all keys in sections are accessible in a case-insensitive manner . E.g. yields only ’ed option key names. This means lowercased keys by default. At the same time, for a section that holds the key , both expressions return :\n• None All sections include values as well which means that on a section may not leave the section visibly empty. This is because default values cannot be deleted from the section (because technically they are not there). If they are overridden in the section, deleting causes the default value to be visible again. Trying to delete a default value causes a .\n• None cannot be removed from the parser:\n• None trying to delete it raises ,\n• None - the second argument is not a fallback value. Note however that the section-level methods are compatible both with the mapping protocol and the classic configparser API.\n• None is compatible with the mapping protocol (returns a list of section_name, section_proxy pairs including the DEFAULTSECT). However, this method can also be invoked with arguments: . The latter call returns a list of option, value pairs for a specified , with all interpolations expanded (unless is provided). The mapping protocol is implemented on top of the existing legacy API so that subclasses overriding the original interface still should have mappings working as expected.\n\nThere are nearly as many INI format variants as there are applications using it. goes a long way to provide support for the largest sensible set of INI styles available. The default functionality is mainly dictated by historical background and it’s very likely that you will want to customize some of the features. The most common way to change the way a specific config parser works is to use the options:\n• This option accepts a dictionary of key-value pairs which will be initially put in the section. This makes for an elegant way to support concise configuration files that don’t specify values which are the same as the documented default. Hint: if you want to specify default values for a specific section, use before you read the actual file.\n• This option has a major impact on how the mapping protocol will behave and how the written configuration files look. With the standard dictionary, every section is stored in the order they were added to the parser. Same goes for options within sections. An alternative dictionary type can be used for example to sort sections and options on write-back. Please note: there are ways to add a set of key-value pairs in a single operation. When you use a regular dictionary in those operations, the order of the keys will be ordered. For example:\n• Some configuration files are known to include settings without values, but which otherwise conform to the syntax supported by . The allow_no_value parameter to the constructor can be used to indicate that such values should be accepted: # Settings with values are treated as before: # Settings which aren't specified still raise an error: :\n• Delimiters are substrings that delimit keys from values within a section. The first occurrence of a delimiting substring on a line is considered a delimiter. This means values (but not keys) can contain the delimiters. See also the space_around_delimiters argument to .\n• Comment prefixes are strings that indicate the start of a valid comment within a config file. comment_prefixes are used only on otherwise empty lines (optionally indented) whereas inline_comment_prefixes can be used after every valid value (e.g. section names, options and empty lines as well). By default inline comments are disabled and and are used as prefixes for whole line comments. Changed in version 3.2: In previous versions of behaviour matched and . Please note that config parsers don’t support escaping of comment prefixes so using inline_comment_prefixes may prevent users from specifying option values with characters used as comment prefixes. When in doubt, avoid setting inline_comment_prefixes. In any circumstances, the only way of storing comment prefix characters at the beginning of a line in multiline values is to interpolate the prefix, for example: # the default BasicInterpolation could be used as well interpolation not necessary = if # is not at line start if # is not at line start\n• When set to , the parser will not allow for any section or option duplicates while reading from a single source (using , or ). It is recommended to use strict parsers in new applications. Changed in version 3.2: In previous versions of behaviour matched .\n• In config parsers, values can span multiple lines as long as they are indented more than the key that holds them. By default parsers also let empty lines to be parts of values. At the same time, keys can be arbitrarily indented themselves to improve readability. In consequence, when configuration files get big and complex, it is easy for the user to lose track of the file structure. Take for instance: is still a part of the multiline value of 'key' This can be especially problematic for the user to see if she’s using a proportional font to edit the file. That is why when your application does not need values with empty lines, you should consider disallowing them. This will make empty lines split keys every time. In the example above, it would produce two keys, and .\n• None default_section, default value: (that is: ) The convention of allowing a special section of default values for other sections or interpolation purposes is a powerful concept of this library, letting users create complex declarative configurations. This section is normally called but this can be customized to point to any other valid section name. Some typical values include: or . The name provided is used for recognizing default sections when reading from any source and is used when writing configuration back to a file. Its current value can be retrieved using the attribute and may be modified at runtime (i.e. to convert files from one format to another).\n• Interpolation behaviour may be customized by providing a custom handler through the interpolation argument. can be used to turn off interpolation completely, provides a more advanced variant inspired by . More on the subject in the dedicated documentation section. has a default value of .\n• Config parsers provide option value getters that perform type conversion. By default , , and are implemented. Should other getters be desirable, users may define them in a subclass or pass a dictionary where each key is a name of the converter and each value is a callable implementing said conversion. For instance, passing would add on both the parser object and all section proxies. In other words, it will be possible to write both and . If the converter needs to access the state of the parser, it can be implemented as a method on a config parser subclass. If the name of this method starts with , it will be available on all section proxies, in the dict-compatible form (see the example above). More advanced customization may be achieved by overriding default values of these parser attributes. The defaults are defined on the classes, so they may be overridden by subclasses or by attribute assignment. By default when using , config parsers consider the following values : , , , and the following values : , , , . You can override this by specifying a custom dictionary of strings and their Boolean outcomes. For example: This method transforms option names on every read, get, or set operation. The default converts the name to lowercase. This also means that when a configuration file gets written, all keys will be lowercase. Override this method if that’s unsuitable. For example: The optionxform function transforms option names to a canonical form. This should be an idempotent function: if the name is already in canonical form, it should be returned unchanged. A compiled regular expression used to parse section headers. The default matches to the name . Whitespace is considered part of the section name, thus will be read as a section of name . Override this attribute if that’s unsuitable. For example: While ConfigParser objects also use an attribute for recognizing option lines, it’s not recommended to override it because that would interfere with constructor options allow_no_value and delimiters.\n\nMainly because of backwards compatibility concerns, provides also a legacy API with explicit / methods. While there are valid use cases for the methods outlined below, mapping protocol access is preferred for new projects. The legacy API is at times more advanced, low-level and downright counterintuitive. An example of writing to a configuration file: # Please note that using RawConfigParser's set functions, you can assign # non-string values to keys internally, but will receive an error when # attempting to write to a file or when you get it in non-raw mode. Setting # values using the mapping protocol or ConfigParser's set() does not allow # such assignments to take place. An example of reading the configuration file again: # getfloat() raises an exception if the value is not a float # getint() and getboolean() also do this for their respective types # Notice that the next output does not interpolate '%(bar)s' or '%(baz)s'. # This is because we are using a RawConfigParser(). To get interpolation, use : # Set the optional *raw* argument of get() to True if you wish to disable # The optional *vars* argument is a dict with members that will take # The optional *fallback* argument can be used to provide a fallback value 'No such things as monsters.' # -> \"No such things as monsters.\" Default values are available in both types of ConfigParsers. They are used in interpolation if an option used is not defined elsewhere. # New instance with 'bar' and 'baz' defaulting to 'Life' and 'hard' each\n\nThe main configuration parser. When defaults is given, it is initialized into the dictionary of intrinsic defaults. When dict_type is given, it will be used to create the dictionary objects for the list of sections, for the options within a section, and for the default values. When delimiters is given, it is used as the set of substrings that divide keys from values. When comment_prefixes is given, it will be used as the set of substrings that prefix comments in otherwise empty lines. Comments can be indented. When inline_comment_prefixes is given, it will be used as the set of substrings that prefix comments in non-empty lines. When strict is (the default), the parser won’t allow for any section or option duplicates while reading from a single source (file, string or dictionary), raising or . When empty_lines_in_values is (default: ), each empty line marks the end of an option. Otherwise, internal empty lines of a multiline option are kept as part of the value. When allow_no_value is (default: ), options without values are accepted; the value held for these is and they are serialized without the trailing delimiter. When default_section is given, it specifies the name for the special section holding default values for other sections and interpolation purposes (normally named ). This value can be retrieved and changed at runtime using the instance attribute. This won’t re-evaluate an already parsed config file, but will be used when writing parsed settings to a new config file. Interpolation behaviour may be customized by providing a custom handler through the interpolation argument. can be used to turn off interpolation completely, provides a more advanced variant inspired by . More on the subject in the dedicated documentation section. All option names used in interpolation will be passed through the method just like any other option name reference. For example, using the default implementation of (which converts option names to lower case), the values and are equivalent. When converters is given, it should be a dictionary where each key represents the name of a type converter and each value is a callable implementing the conversion from string to the desired datatype. Every converter gets its own corresponding method on the parser object and section proxies. When allow_unnamed_section is (default: ), the first section name can be omitted. See the “Unnamed Sections” section. It is possible to read several configurations into a single , where the most recently added configuration has the highest priority. Any conflicting keys are taken from the more recent configuration while the previously existing keys are retained. The example below reads in an file, which will override any conflicting keys from the file. Changed in version 3.1: The default dict_type is . Changed in version 3.2: allow_no_value, delimiters, comment_prefixes, strict, empty_lines_in_values, default_section and interpolation were added. Changed in version 3.5: The converters argument was added. Changed in version 3.7: The defaults argument is read with , providing consistent behavior across the parser: non-string keys and values are implicitly converted to strings. Changed in version 3.8: The default dict_type is , since it now preserves insertion order. Changed in version 3.13: Raise a when allow_no_value is , and a key without a value is continued with an indented line. Changed in version 3.13: The allow_unnamed_section argument was added. Return a list of the sections available; the default section is not included in the list. Add a section named section to the instance. If a section by the given name already exists, is raised. If the default section name is passed, is raised. The name of the section must be a string; if not, is raised. Indicates whether the named section is present in the configuration. The default section is not acknowledged. Return a list of options available in the specified section. If the given section exists, and contains the given option, return ; otherwise return . If the specified section is or an empty string, DEFAULT is assumed. Attempt to read and parse an iterable of filenames, returning a list of filenames which were successfully parsed. If filenames is a string, a object or a path-like object, it is treated as a single filename. If a file named in filenames cannot be opened, that file will be ignored. This is designed so that you can specify an iterable of potential configuration file locations (for example, the current directory, the user’s home directory, and some system-wide directory), and all existing configuration files in the iterable will be read. If none of the named files exist, the instance will contain an empty dataset. An application which requires initial values to be loaded from a file should load the required file or files using before calling for any optional files: Changed in version 3.2: Added the encoding parameter. Previously, all files were read using the default encoding for . Read and parse configuration data from f which must be an iterable yielding Unicode strings (for example files opened in text mode). Optional argument source specifies the name of the file being read. If not given and f has a attribute, that is used for source; the default is . Optional argument source specifies a context-specific name of the string passed. If not given, is used. This should commonly be a filesystem path or a URL. Load configuration from any object that provides a dict-like method. Keys are section names, values are dictionaries with keys and values that should be present in the section. If the used dictionary type preserves order, sections and their keys will be added in order. Values are automatically converted to strings. Optional argument source specifies a context-specific name of the dictionary passed. If not given, is used. This method can be used to copy state between parsers. Get an option value for the named section. If vars is provided, it must be a dictionary. The option is looked up in vars (if provided), section, and in DEFAULTSECT in that order. If the key is not found and fallback is provided, it is used as a fallback value. can be provided as a fallback value. All the interpolations are expanded in the return values, unless the raw argument is true. Values for interpolation keys are looked up in the same manner as the option. Changed in version 3.2: Arguments raw, vars and fallback are keyword only to protect users from trying to use the third argument as the fallback fallback (especially when using the mapping protocol). A convenience method which coerces the option in the specified section to an integer. See for explanation of raw, vars and fallback. A convenience method which coerces the option in the specified section to a floating-point number. See for explanation of raw, vars and fallback. A convenience method which coerces the option in the specified section to a Boolean value. Note that the accepted values for the option are , , , and , which cause this method to return , and , , , and , which cause it to return . These string values are checked in a case-insensitive manner. Any other value will cause it to raise . See for explanation of raw, vars and fallback. When section is not given, return a list of section_name, section_proxy pairs, including DEFAULTSECT. Otherwise, return a list of name, value pairs for the options in the given section. Optional arguments have the same meaning as for the method. Changed in version 3.8: Items present in vars no longer appear in the result. The previous behaviour mixed actual parser options with variables provided for interpolation. If the given section exists, set the given option to the specified value; otherwise raise . option and value must be strings; if not, is raised. Write a representation of the configuration to the specified file object, which must be opened in text mode (accepting strings). This representation can be parsed by a future call. If space_around_delimiters is true, delimiters between keys and values are surrounded by spaces. Comments in the original configuration file are not preserved when writing the configuration back. What is considered a comment, depends on the given values for comment_prefix and inline_comment_prefix. Remove the specified option from the specified section. If the section does not exist, raise . If the option existed to be removed, return ; otherwise return . Remove the specified section from the configuration. If the section in fact existed, return . Otherwise return . Transforms the option name option as found in an input file or as passed in by client code to the form that should be used in the internal structures. The default implementation returns a lower-case version of option; subclasses may override this or client code can set an attribute of this name on instances to affect this behavior. You don’t need to subclass the parser to use this method, you can also set it on an instance, to a function that takes a string argument and returns a string. Setting it to , for example, would make option names case sensitive: Note that when reading configuration files, whitespace around the option names is stripped before is called. A special object representing a section name used to reference the unnamed section (see Unnamed Sections). The maximum depth for recursive interpolation for when the raw parameter is false. This is relevant only when the default interpolation is used."
    },
    {
        "link": "https://realpython.com/python-json",
        "document": "Python’s module provides you with the tools you need to effectively handle JSON data. You can convert Python data types to a JSON-formatted string with or write them to files using . Similarly, you can read JSON data from files with and parse JSON strings with .\n\nJSON, or JavaScript Object Notation, is a widely-used text-based format for data interchange. Its syntax resembles Python dictionaries but with some differences, such as using only double quotes for strings and lowercase for Boolean values. With built-in tools for validating syntax and manipulating JSON files, Python makes it straightforward to work with JSON data.\n\nBy the end of this tutorial, you’ll understand that:\n• JSON in Python is handled using the standard-library module, which allows for data interchange between JSON and Python data types.\n• JSON is a good data format to use with Python as it’s human-readable and straightforward to serialize and deserialize, which makes it ideal for use in APIs and data storage.\n• You write JSON with Python using to serialize data to a file.\n• You can minify and prettify JSON using Python’s module.\n\nSince its introduction, JSON has rapidly emerged as the predominant standard for the exchange of information. Whether you want to transfer data with an API or store information in a document database, it’s likely you’ll encounter JSON. Fortunately, Python provides robust tools to facilitate this process and help you manage JSON data efficiently.\n\nWhile JSON is the most common format for data distribution, it’s not the only option for such tasks. Both XML and YAML serve similar purposes. If you’re interested in how the formats differ, then you can check out the tutorial on how to serialize your data with Python.\n\nThe acronym JSON stands for JavaScript Object Notation. As the name suggests, JSON originated from JavaScript. However, JSON has transcended its origins to become language-agnostic and is now recognized as the standard for data interchange. The popularity of JSON can be attributed to native support by the JavaScript language, resulting in excellent parsing performance in web browsers. On top of that, JSON’s straightforward syntax allows both humans and computers to read and write JSON data effortlessly. To get a first impression of JSON, have a look at this example code: You’ll learn more about the JSON syntax later in this tutorial. For now, recognize that the JSON format is text-based. In other words, you can create JSON files using the code editor of your choice. Once you set the file extension to , most code editors display your JSON data with syntax highlighting out of the box: The screenshot above shows how VS Code displays JSON data using the Bearded color theme. You’ll have a closer look at the syntax of the JSON format next! In the previous section, you got a first impression of how JSON data looks. And as a Python developer, the JSON structure probably reminds you of common Python data structures, like a dictionary that contains a string as a key and a value. If you understand the syntax of a dictionary in Python, you already know the general syntax of a JSON object. Note: Later in this tutorial, you’ll learn that you’re free to use lists and other data types at the top level of a JSON document. The similarity between Python dictionaries and JSON objects is no surprise. One idea behind establishing JSON as the go-to data interchange format was to make working with JSON as convenient as possible, independently of which programming language you use: [A collection of key-value pairs and arrays] are universal data structures. Virtually all modern programming languages support them in one form or another. It makes sense that a data format that is interchangeable with programming languages is also based on these structures. (Source) To explore the JSON syntax further, create a new file named and add a more complex JSON structure as the content of the file: In the code above, you see data about a dog named Frieda, which is formatted as JSON. The top-level value is a JSON object. Just like Python dictionaries, you wrap JSON objects inside curly braces ( ). In line 1, you start the JSON object with an opening curly brace ( ), and then you close the object at the end of line 20 with a closing curly brace ( ). Note: Although whitespace doesn’t matter in JSON, it’s customary for JSON documents to be formatted with two or four spaces to indicate indentation. If the file size of the JSON document is important, then you may consider minifying the JSON file by removing the whitespace. You’ll learn more about minifying JSON data later in the tutorial. Inside the JSON object, you can define zero, one, or more key-value pairs. If you add multiple key-value pairs, then you must separate them with a comma ( ). A key-value pair in a JSON object is separated by a colon ( ). On the left side of the colon, you define a key. A key is a string you must wrap in double quotes ( ). Unlike Python, JSON strings don’t support single quotes ( ). The values in a JSON document are limited to the following data types: Either or without quotes Just like in dictionaries and lists, you’re able to nest data in JSON objects and arrays. For example, you can include an object as the value of an object. Also, you’re free to use any other allowed value as an item in a JSON array. As a Python developer, you may need to pay extra attention to the Boolean values. Instead of using or in title case, you must use the lowercase JavaScript-style Booleans or . Unfortunately, there are some other details in the JSON syntax that you may stumble over as a developer. You’ll have a look at them next. The JSON standard doesn’t allow any comments, trailing commas, or single quotes for strings. This can be confusing to developers who are used to Python dictionaries or JavaScript objects. Here’s a smaller version of the JSON file from before with invalid syntax:\n• Line 5 has a trailing comma after the final key-value pair.\n• Line 10 contains a trailing comma in the array. Using double quotes is something you can get used to as a Python developer. Comments can be helpful in explaining your code, and trailing commas can make moving lines around in your code less fragile. This is why some developers like to use Human JSON (Hjson) or JSON with comments (JSONC). Hjson gives you the freedom to use comments, ditch commas between properties, or create quoteless strings. Apart from the curly braces ( ), the Hjson syntax look like a mix of YAML and JSON. JSONC is a bit stricter than Hjson. Compared to regular JSON, JSONC allows you to use comments and trailing commas. You may have encountered JSONC when editing the file of VS Code. Inside its configuration files, VS Code works in a JSONC mode. For common JSON files, VS Code is more strict and points out JSON syntax errors. If you want to make sure you write valid JSON, then your coding editor can be of great help. The invalid JSON document above contains marks for each occurrence of incorrect JSON syntax: When you don’t want to rely on your code editor, you can also use online tools to verify that the JSON syntax you write is correct. Popular online tools for validating JSON are JSON Lint and JSON Formatter. Later in the tutorial, you’ll learn how to validate JSON documents from the comfort of your terminal. But before that, it’s time to find out how you can work with JSON data in Python.\n\nPython supports the JSON format through the built-in module named . The module is specifically designed for reading and writing strings formatted as JSON. That means you can conveniently convert Python data types into JSON data and the other way around. The act of converting data into the JSON format is referred to as serialization. This process involves transforming data into a series of bytes for storage or transmission over a network. The opposite process, deserialization, involves decoding data from the JSON format back into a usable form within Python. You’ll start with the serialization of Python code into JSON data with the help of the module. One of the most common actions when working with JSON in Python is to convert a Python dictionary into a JSON object. To get an impression of how this works, hop over to your Python REPL and follow along with the code below: After importing the module, you can use to convert a Python dictionary to a JSON-formatted string, which represents a JSON object. It’s important to understand that when you use , you get a Python string in return. In other words, you don’t create any kind of JSON data type. The result is similar to what you’d get if you used Python’s built-in function: Using gets more interesting when your Python dictionary doesn’t contain strings as keys or when values don’t directly translate to a JSON format: In the dictionary, the keys , , and are numbers. Once you use , the dictionary keys become strings in the JSON-formatted string. Note: When you convert a dictionary to JSON, the dictionary keys will always be strings in JSON. The Boolean Python values of your dictionary become JSON Booleans. As mentioned before, the tiny but significant difference between JSON Booleans and Python Booleans is that JSON Booleans are lowercase. The cool thing about Python’s module is that it takes care of the conversion for you. This can come in handy when you’re using variables as dictionary keys: When converting Python data types into JSON, the module receives the evaluated values. While doing so, sticks tightly to the JSON standard. For example, when converting integer keys like to the string . The module allows you to convert common Python data types to JSON. Here’s an overview of all Python data types and values that you can convert to JSON values: Note that different Python data types like lists and tuples serialize to the same JSON data type. This can cause problems when you convert JSON data back to Python, as the data type may not be the same as before. You’ll explore this pitfall later in this tutorial when you learn how to read JSON. Dictionaries are probably the most common Python data type that you’ll use as a top-level value in JSON. But you can convert the data types listed above just as smoothly as dictionaries using . Take a Boolean or a list, for example: A JSON document may contain a single scalar value, like a number, at the top level. That’s still valid JSON. But more often than not, you want to work with a collection of key-value pairs. Similar to how not every data type can be used as a dictionary key in Python, not all keys can be converted into JSON key strings: You can’t use dictionaries, lists, or tuples as JSON keys. For dictionaries and lists, this rule makes sense as they’re not hashable. But even when a tuple is hashable and allowed as a key in a dictionary, you’ll get a when you try to use a tuple as a JSON key: : keys must be str, int, float, bool or None, not tuple By providing the argument, you can prevent getting a when creating JSON data with unsupported Python keys: When you set in to , then Python skips the keys that are not supported and would otherwise raise a . The result is a JSON-formatted string that only contains a subset of the input dictionary. In practice, you usually want your JSON data to resemble the input object as close as possible. So, you must use with caution to not lose information when calling . Note: If you’re ever in a situation where you need to convert an unsupported object into JSON, then you can consider creating a subclass of the and implementing a method. When you use , you can use additional arguments to control the look of the resulting JSON-formatted string. For example, you can sort the dictionary keys by setting the parameter to : When you set to , then Python sorts the keys alphabetically for you when serializing a dictionary. Sorting the keys of a JSON object can come in handy when your dictionary keys formerly represented the column names of a database, and you want to display them in an organized fashion to the user. Another notable parameter of is , which you’ll probably use the most when serializing JSON data. You’ll explore later in this tutorial in the prettify JSON section. When you convert Python data types into the JSON format, you usually have a goal in mind. Most commonly, you’ll use JSON to persist and exchange data. To do so, you need to save your JSON data outside of your running Python program. Conveniently, you’ll explore saving JSON data to a file next. The JSON format can come in handy when you want to save data outside of your Python program. Instead of spinning up a database, you may decide to use a JSON file to store data for your workflows. Again, Python has got you covered. To write Python data into an external JSON file, you use . This is a similar function to the one you saw earlier, but without the s at the end of its name: In lines 3 to 22, you define a dictionary that you write to a JSON file in line 25 using a context manager. To properly indicate that the file contains JSON data, you set the file extension to . When you use , then it’s good practice to define the encoding. For JSON, you commonly want to use as the encoding when reading and writing files: The RFC requires that JSON be represented using either UTF-8, UTF-16, or UTF-32, with UTF-8 being the recommended default for maximum interoperability. (Source) The function has two required arguments:\n• The object you want to write\n• The file you want to write into Other than that, there are a bunch of optional parameters for . The optional parameters of are the same as for . You’ll investigate some of them later in this tutorial when you prettify and minify JSON files.\n\nIn the former sections, you learned how to serialize Python data into JSON-formatted strings and JSON files. Now, you’ll see what happens when you load JSON data back into your Python program. In parallel to and , the library provides two functions to deserialize JSON data into a Python object: As a rule of thumb, you work with when your data is already present in your Python program. You use with external files that are saved on your disk. The conversion from JSON data types and values to Python follows a similar mapping as before when you converted Python objects into the JSON format: When you compare this table to the one in the previous section, you may recognize that Python offers a matching data type for all JSON types. That’s very convenient because this way, you can be sure you won’t lose any information when deserializing JSON data to Python. Note: Deserialization is not the exact reverse of the serialization process. The reason for this is that JSON keys are always strings, and not all Python data types can be converted to JSON data types. This discrepancy means that certain Python objects may not retain their original type when serialized and then deserialized. To get a better feeling for the conversion of data types, you’ll start with serializing a Python object to JSON and then convert the JSON data back to Python. That way, you can spot differences between the Python object you serialize and the Python object you end up with after deserializing the JSON data. To investigate how to load a Python dictionary from a JSON object, revisit the example from before. Start by creating a dictionary and then serialize the Python dictionary to a JSON string using : By passing into , you’re creating a string with a JSON object that you save in . If you want to convert back to a Python dictionary, then you can use : By using , you can convert JSON data back into Python objects. With the knowledge about JSON that you’ve gained so far, you may already suspect that the content of the dictionary is not identical to the content of : The difference between and is subtle but can be impactful in your Python programs. In JSON, the keys must always be strings. When you converted to using , the integer key became the string . When you used , there was no way for Python to know that the string key should be an integer again. That’s why your dictionary key remained a string after deserialization. You’ll investigate a similar behavior by doing another conversion roundtrip with other Python data types! To explore how different data types behave in a roundtrip from Python to JSON and back, take a portion of the dictionary from a former section. Note how the dictionary contains different data types as values: The dictionary contains a bunch of common Python data types as values. For example, a string in line 2, a Boolean in line 3, a in line 7, and a tuple in line 8, just to name a few. Next, convert to a JSON-formatted string and back to Python again. Afterward, have a look at the newly created dictionary: You can convert every JSON data type perfectly into a matching Python data type. The JSON Boolean deserializes into , converts back into , and objects and arrays become dictionaries and lists. Still, there’s one exception that you may encounter in roundtrips: When you serialize a Python tuple, it becomes a JSON array. When you load JSON, a JSON array correctly deserializes into a list because Python has no way of knowing that you want the array to be a tuple. Problems like the one described above can always be an issue when you’re doing data roundtrips. When the roundtrip happens in the same program, you may be more aware of the expected data types. Data type conversions may be even more obfuscated when you’re dealing with external JSON files that originated in another program. You’ll investigate a situation like this next! In a previous section, you created a file that saved a file. If you need to refresh your memory, you can expand the collapsible section below that shows the code again: Take a look at the data types of the dictionary. Is there a data type in a value that the JSON format doesn’t support? When you want to write content to a JSON file, you use . The counterpart to is . As the name suggests, you can use to load a JSON file into your Python program. Jump back into the Python REPL and load the JSON file from before: Just like when writing files, it’s a good idea to use a context manager when reading a file in Python. That way, you don’t need to bother with closing the file again. When you want to read a JSON file, then you use inside the statement’s block. The argument for the function must be either a text file or a binary file. The Python object that you get from depends on the top-level data type of your JSON file. In this case, the JSON file contains an object at the top level, which deserializes into a dictionary. When you deserialize a JSON file as a Python object, then you can interact with it natively—for example, by accessing the value of the key with square bracket notation ( ). Still, there’s a word of caution here. Import the original dictionary from before and compare it to : When you load a JSON file as a Python object, then any JSON data type happily deserializes into Python. That’s because Python knows about all data types that the JSON format supports. Unfortunately, it’s not the same the other way around. As you learned before, there are Python data types like that you can convert into JSON, but you’ll end up with an data type in the JSON file. Once you convert the JSON data back to Python, then an array deserializes into the Python data type. Generally, being cautious about data type conversions should be the concern of the Python program that writes the JSON. With the knowledge you have about JSON files, you can always anticipate which Python data types you’ll end up with as long as the JSON file is valid. If you use , then the content of the file you load must contain valid JSON syntax. Otherwise, you’ll receive a . Luckily, Python caters to you with more tools you can use to interact with JSON. For example, it allows you to check a JSON file’s validity from the convenience of the terminal.\n\nSo far, you’ve explored the JSON syntax and have already spotted some common JSON pitfalls like trailing commas and single quotes for strings. When writing JSON, you may have also spotted some annoying details. For example, neatly indented Python dictionaries end up being a blob of JSON data. In the last section of this tutorial, you’ll try out some techniques to make your life easier as you work with JSON data in Python. To start, you’ll give your JSON object a well-deserved glow-up. One huge advantage of the JSON format is that JSON data is human-readable. Even more so, JSON data is human-writable. This means you can open a JSON file in your favorite text editor and change the content to your liking. Well, that’s the idea, at least! Editing JSON data by hand is not particularly easy when your JSON data looks like this in the text editor: Even with word wrapping and syntax highlighting turned on, JSON data is hard to read when it’s a single line of code. And as a Python developer, you probably miss some whitespace. But worry not, Python has got you covered! When you call or to serialize a Python object, then you can provide the argument. Start by trying out with different indentation levels: The default value for is . When you call without or with as a value, you’ll end up with one line of a compact JSON-formatted string. If you want linebreaks in your JSON string, then you can set to or provide an empty string. Although probably less useful, you can even provide a negative number as the indentation or any other string. More commonly, you’ll provide values like or for : When you use positive integers as the value for when calling , then you’ll indent every level of the JSON object with the given count as spaces. Also, you’ll have newlines for each key-value pair. Note: To actually see the whitespace in the REPL, you can wrap the calls in function calls. The parameter works exactly the same for as it does for . Go ahead and write the dictionary into a JSON file with an indentation of spaces: When you set the indentation level when serializing JSON data, then you end up with prettified JSON data. Have a look at how the file looks in your editor: Python can work with JSON files no matter how they’re indented. As a human, you probably prefer a JSON file that contains newlines and is neatly indented. A JSON file that looks like this is way more convenient to edit. The convenience of being able to edit JSON data in the editor comes with a risk. When you move key-value pairs around or add strings with one quote instead of two, you end up with an invalid JSON. To swiftly check if a JSON file is valid, you can leverage Python’s . You can run the module as an executable in the terminal using the switch. To see in action, also provide as the positional argument: When you run only with an option, then Python validates the JSON file and outputs the JSON file’s content in the terminal if the JSON is valid. Running in the example above means that contains valid JSON syntax. Note: The prints the JSON data with an indentation of 4 by default. You’ll explore this behavior in the next section. To make complain, you need to invalidate your JSON document. You can make the JSON data of invalid by removing the comma ( ) between the key-value pairs: After saving , run again to validate the file: The module successfully stumbles over the missing comma in . Python notices that there’s a delimiter missing once the property name enclosed in double quotes starts in line 3 at position 5. Go ahead and try fixing the JSON file again. You can also be creative with invalidating and check how reports your error. But keep in mind that only reports the first error. So you may need to go back and forth between fixing a JSON file and running . Once is valid, you may notice that the output always looks the same. Of course, like any well-made command-line interface, offers you some options to control the program. In the previous section, you used to validate a JSON file. When the JSON syntax was valid, showed the content with newlines and an indentation of four spaces. To control how prints the JSON, you can set the option. If you followed along with the tutorial, then you’ve got a file that doesn’t contain newlines or indentation. Alternatively, you can download in the materials by clicking the link below: Free Bonus: Click here to download the free sample code that shows you how to work with JSON data in Python. When you pass in to , then you can pretty print the content of the JSON file in your terminal. When you set , then you can control which indentation level uses to display the code: Seeing the prettified JSON data in the terminal is nifty. But you can step up your game even more by providing another option to the run! By default, writes the output to , just like you commonly do when calling the function. But you can also redirect the output of into a file by providing a positional argument: With as the value of the option, you write the output into the JSON file instead of showing the content in the terminal. If the file doesn’t exist yet, then Python creates the file on the way. If the target file already exists, then you overwrite the file with the new content. Note: You can prettify a JSON file in place by using the same file as and arguments. You can verify that the file exists by running the terminal command: The whitespace you added to comes with a price. Compared to the original, unindented file, the file size of is now around double that. Here, the 308-byte increase may not be significant. But when you’re dealing with big JSON data, then a good-looking JSON file will take up quite a bit of space. Having a small data footprint is especially useful when serving data over the web. Since the JSON format is the de facto standard for exchanging data over the web, it’s worth keeping the file size as small as possible. And again, Python’s has got your back! As you know by now, Python is a great helper when working with JSON. You can minify JSON data with Python in two ways:\n• Use the module in your Python code Before, you used with the option to add whitespace. Instead of using here, you can use provide to do the opposite and remove any whitespace between the key-value pairs of your JSON: After calling the module, you provide a JSON file as the and another JSON file as the . If the target JSON file exists, then you overwrite its contents. Otherwise, you create a new file with the filename you provide. Just like with , you provide the same file as a source and target file to minify the file in-place. In the example above, you minify into . Run the command to see how many bytes you squeezed out of the original JSON file: Compared to , the file size of is 337 bytes smaller. That’s even 29 bytes less than the original file that didn’t contain any indentation. To investigate where Python managed to remove even more whitespace from the original JSON, open the Python REPL again and minify the content of the original file with Python’s module: In the code above, you use Python’s to get the content of as text. Then, you use to deserialize to , which is a Python dictionary. You could use to get a Python dictionary right away, but you need the JSON data as a string first to compare it properly. That’s also why you use to create and then use instead of leveraging directly to save the minified JSON data in . As you learned before, needs JSON data as the first argument and then accepts a value for the indentation. The default value for is , so you could skip setting the argument explicitly like you do above. But with , you’re making your intention clear that you don’t want any indentation, which will be a good thing for others who read your code later. The parameter for allows you to define a tuple with two values:\n• The separator between the key-value pairs or list items. By default, this separator is a comma followed by a space ( ).\n• The separator between the key and the value. By default, this separator is a colon followed by a space ( ). By setting to , you continue to use valid JSON separators. But you tell Python not to add any spaces after the comma ( ) and the colon ( ). That means that the only whitespace left in your JSON data can be whitespace appearing in key names and values. That’s pretty tight! With both and containing your JSON strings, it’s time to compare them: You can already spot the difference between and when you look at the output. You then use the function to verify that the size of is indeed smaller. If you’re curious about why the length of the JSON strings almost exactly matches the file size of the written files, then looking into Unicode & character encodings in Python is a great idea. Both and are excellent helpers when you want to make JSON data look prettier, or if you want to minify JSON data to save some bytes. With the module, you can conveniently interact with JSON data in your Python programs. That’s great when you need to have more control over the way you interact with JSON. The module comes in handy when you want to work with JSON data directly in your terminal."
    },
    {
        "link": "https://stackoverflow.com/questions/46283355/config-parser-python-how-to-read-from-json-file",
        "document": "I have created a file in the same folder as my is. It looks something like:\n\nI'm looking to insert it into my script that got headers of:\n\nI'm wondering how can I make it possible to read from my into my script? This is my first time using this and I'm also new into Python. I feel like this could be an easy way to modify information WITHOUT having to change the code every time.\n\nTried to do this:\n\nthe print is saying good information but when it comes to \"given_name\": config.given_name then im getting a error saying"
    },
    {
        "link": "https://geeksforgeeks.org/reading-and-writing-json-to-a-file-in-python",
        "document": "The full form of JSON is Javascript Object Notation. It means that a script (executable) file which is made of text in a programming language, is used to store and transfer the data. Python supports JSON through a built-in package called JSON. To use this feature, we import the JSON package in Python script. The text in JSON is done through quoted-string which contains the value in key-value mapping within { }. It is similar to the dictionary in Python.\n\nSerializing JSON refers to the transformation of data into a series of bytes (hence serial) to be stored or transmitted across a network. To handle the data flow in a file, the JSON library in Python uses dump() or dumps() function to convert the Python objects into their respective JSON object, so it makes it easy to write data to files. See the following table given below.\n\nMethod 1: Writing JSON to a file in Python using json.dumps()\n\nThe JSON package in Python has a function called json.dumps() that helps in converting a dictionary to a JSON object. It takes two parameters:\n• dictionary – the name of a dictionary which should be converted to a JSON object.\n• indent – defines the number of units for indentation\n\nAfter converting the dictionary to a JSON object, simply write it to a file using the “write” function.\n\nMethod 2: Writing JSON to a file in Python using json.dump()\n\nAnother way of writing JSON to a file is by using json.dump() method The JSON package has the “dump” function which directly writes the dictionary to a file in the form of JSON, without needing to convert it into an actual JSON object. It takes 2 parameters:\n• dictionary – the name of a dictionary which should be converted to a JSON object.\n• file pointer – pointer of the file opened in write or append mode.\n\nDeserialization is the opposite of Serialization, i.e. conversion of JSON objects into their respective Python objects. The load() method is used for it. If you have used JSON data from another program or obtained it as a string format of JSON, then it can easily be deserialized with load(), which is usually used to load from a string, otherwise, the root object is in a list or Dict.\n\nThe JSON package has json.load() function that loads the JSON content from a JSON file into a dictionary. It takes one parameter:"
    },
    {
        "link": "https://freecodecamp.org/news/how-to-use-the-json-module-in-python",
        "document": "JSON (JavaScript Object Notation) is a popular, lightweight data interchange standard. It represents data structures made up of key-value pairs that's quite straightforward and human-readable.\n\nJSON has become the industry standard for data interchange between online services. And it's widely utilized in modern programming languages, including Python.\n\nJSON data is frequently expressed as nested dictionaries, lists, and scalar values such as texts, numbers, booleans, and null. It is named JSON because it closely mimics the syntax used in JavaScript objects.\n\nIn this tutorial, you will explore the JSON module in Python and learn how to effectively work with JSON data.\n\nJSON plays an important role in Python programming because it allows efficient data serialization and deserialization. It enables Python programs to effortlessly communicate with web services, exchange data, and store structured information.\n\nDevelopers can use JSON to seamlessly link their Python programs with a variety of APIs, databases, and external systems that use JSON for data representation.\n\nIf you're looking to learn how to interact with web services using Python, check out my tutorial on the requests module.\n\nThe built-in JSON module in Python provides a powerful set of methods and classes that make working with JSON data simple. Developers can use it to encode Python objects into JSON strings and decode JSON strings back into Python objects.\n\nHow to Store JSON Data in a File\n\nWhen working with JSON data in Python, you'll often need to save the data or share it with others. Storing JSON data in a file enables quick retrieval and data persistence.\n\nIn this section, you'll learn how to use Python's function to save JSON data to a file. This process involves serializing the JSON data and saving it to a file, which you can subsequently read and use as needed.\n\nThe function in Python allows you to store JSON data directly into a file. This function takes two parameters: the data to be serialized and the file object where the data will be written.\n\nTo write JSON data to a file, you need to follow a few steps. First, you need to open a file in write mode, specifying the file path. Then, you can use the function to serialize the data and write it to the file. Finally, you need to close the file to ensure that all the data is properly saved.\n\nLet's learn how to store data in a file using the horoscope API response as an example.\n\nAssume you have made a GET request to the following URL: https://horoscope-app-api.vercel.app/api/v1/get-horoscope/daily?sign=capricorn&day=today, which provides the daily horoscope for the Capricorn sign.\n\nIn the code above, you use the library to make a GET request to the Horoscope API. You then extract the JSON data from the response using the method. Finally, you open a file named in write mode using the statement, and you use to store the data in the file.\n\nCheck out this tutorial to learn how to find out your horoscope using Python.\n\nIf you open the file, you'll see contents similar to below:\n\nHow to Retrieve Data from a JSON File\n\nYou'll often need to read data from a JSON file. For example, you may need to read configuration settings from a JSON file. Python's JSON module provides the function, which allows you to read and deserialize JSON data from a file.\n\nIn this section, you will learn how to use the function to retrieve JSON data from a file and work with it in your Python programs.\n\nThe function accepts a file object as an argument and returns deserialized JSON data in the form of Python objects such as dictionaries, lists, strings, numbers, booleans, and null values.\n\nTo read JSON data from a file, you need to open the file in read mode, extract the data using the function, and store it in a variable for further processing. It's important to ensure that the file being read contains valid JSON data – otherwise, it may raise an exception.\n\nLet's see how you can retrieve the data from the previously created file:\n\nIn the code above, you open the file in read mode using the statement. You then use the function to deserialize the JSON data from the file into the data variable. Finally, you access specific fields of the JSON data (e.g., \"date\" and \"horoscope_data\") and process them as needed.\n\nHow to Format the JSON Output\n\nWhen you read data from a JSON file and print it, the output is displayed as a single line, which may not resemble the structured format of JSON.\n\nThe JSON module provides you with a function to serialize Python objects into a JSON formatted string. It provides various options for customization, including formatting the output to make it more human-readable.\n\nThe function provides several options to customize the output. The most commonly used is the which allows you to specify the number of spaces used for indentation.\n\nAs you can see, the JSON data is now formatted with proper indentation, enhancing its readability. This technique can be applied to any JSON data, allowing you to present JSON output in a more organized and visually appealing way.\n\nPython's JSON module provides a convenient command line tool called that allows you to format and pretty-print JSON data directly from the command line. It is a useful utility for quickly visualizing the structure and contents of JSON data in a more readable and organized format.\n\nTo use , you can execute the following command in your command-line interface:\n• invokes the module using the Python interpreter.\n• represents the path to the JSON file you want to format.\n• is an optional argument that specifies the file to which you want to save the formatted JSON output. If not provided, the formatted output will be displayed on the console.\n\nLet's say you have a file with the following contents:\n\nNotice that the above JSON file has an indentation of two spaces.\n\nTo pretty-print this JSON file using , you can execute the following command:\n\nThe output will be:\n\nAs you can see in the example, executing the module with the input file path formats the JSON data and displays the formatted output on the console.\n\nYou can also redirect the formatted output to an output file by specifying the output file name as the second argument:\n\nThis command formats the JSON data from and saves the formatted output to .\n\nThe JSON module in Python allows you to encode and decode custom objects by using JSON encoder and decoder classes. You can define custom serialization and deserialization logic for your objects using these classes.\n\nclass allows you to customize the encoding process. To define how your custom object should be encoded into JSON format, you can extend the and change its method.\n\nHere's an example of how you can extend the class and customize the encoding process for a custom object:\n\nIn this example, you define a custom class with and attributes. You then create a subclass of called and override its method. Within the method, you check if the object being encoded is an instance of . If it is, you provide a JSON-serializable representation of the object by returning a dictionary containing the and attributes. If the object is not of type , you call the method of the superclass to handle other types.\n\nBy using and specifying the parameter as your custom encoder class , you can encode the object into a JSON string. The output will be:\n\nSimilarly, you can specify custom decoding logic in the JSON decoder class, . To define how JSON data should be decoded into your custom object, extend the and override its function.\n\nHow to Create JSON from a Python Dictionary\n\nYou can use the function provided by the JSON module to create JSON from a Python dictionary. This function takes a Python object, typically a dictionary, and converts it into a JSON string representation.\n\nIn this example, you have a Python dictionary representing some data. By calling , you convert the dictionary into a JSON string. The output will be:\n\nHow to Create a Python Dictionary from JSON\n\nTo create a Python dictionary from JSON data, you can use the function provided by the JSON module. This function takes a JSON string and converts it into a corresponding Python object, typically a dictionary.\n\nIn this example, you have a JSON string representing some data. By calling , you convert the JSON string into a Python dictionary. You can then access the values in the dictionary using their respective keys.\n\nThe output will be:\n\nUnderstanding the Python JSON module is necessary for working with JSON data because it is widely used for data exchange and storage in a variety of applications.\n\nYou can efficiently handle JSON data, interface with APIs, and deal with configuration files if you learn how to use the JSON module."
    }
]