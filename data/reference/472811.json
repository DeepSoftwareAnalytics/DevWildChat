[
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-master",
        "document": ""
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/table/tableapi",
        "document": ""
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-release-1.16/api/python//reference/pyflink.table/api/pyflink.table.Table.group_by.html",
        "document": ""
    },
    {
        "link": "https://ververica.com/blog/all-you-need-to-know-about-pyflink",
        "document": "PyFlink serves as a Python API for Apache Flink, providing users with a medium to develop Flink programs in Python and deploy them on a Flink cluster.\n\nIn this post, we will introduce PyFlink from the following aspects:\n• The structure of a fundamental PyFlink job and some basic knowledge surrounding it\n• The operational mechanisms of PyFlink jobs, the high-level architecture, and its internal workings\n\nBy the end of this article, you should have a firm grasp on PyFlink and its potential applications.\n\nIf you find yourself needing real-time computing solutions, such as real-time ETL, real-time feature engineering, real-time data warehouse, real-time prediction, and you're comfortable with the Python language or want to use some handy Python libraries in the process, PyFlink is an excellent starting point as it merges the worlds of Flink and Python.\n\nPyFlink was first introduced into Flink in Flink 1.9, dating back to 2019. This inaugural version offered only limited functionalities. Since then, the Flink community has strived to continually enhance PyFlink. After nearly four years of diligent development, it has become more and more mature. Currently, it encompasses most functionalities found in the Flink Java API. Additionally, PyFlink exclusively provides several features, like Python user-defined function support, among other functionalities.\n\nPyFlink is integrated into current versions of Ververica Platform. If you want to get a feel for PyFlink’s capabilities and are working in a Kuberbetes capable environment, you can download Community Edition for free and spin up a minikube playground in minutes.\n\nIf you prefer to work with vanilla Flink, then you can install PyFlink from PyPI:\n\nFor the latest Flink 1.17 you’ll need a Python version later than Python 3.6, up to and including Python 3.10; Flink 1.16 supports Python versions from 3.6 to 3.9. Note that Python/PyFlink must be available to each node in the cluster. The most flexible way to do this is to pass in a Python environment when you submit a PyFlink job, but if you have many deep Python dependencies it may be simpler just to preinstall the Python environment to each cluster node.\n\nYou can alternatively build PyFlink from source, which you may want to do if you maintain your own fork of Flink or need to cherry-pick commits which are still not released.\n\nIf you are new to Flink, there are a few basic concepts it’s good to understand and which are relevant also to PyFlink:\n• Flink offers two different APIs, the procedural and relatively low level DataStream API and the relational/declarative Table API. Don’t be misled by their names: both APIs can be applied to either stream or batch processing, and both have PyFlink APIs.\n• Flink is a distributed computing engine. It has no storage besides the state which provides the immediate context during processing. Data is assumed to flow from an external data source to (typically, but it’s not required) an external data sink. A Flink/PyFlink job needs at least a data source.\n• At the heart of any Flink/PyFlink application are the data transformations that compute the desired results from the source data – which can involve reshaping or sampling data, merging and enriching, comparing or modeling, processing transactions, or any of the countless other ways you might want to perform computations over unbounded data streams or massive data sets.\n\nThe first step for any PyFlink job is to define the data source, and optionally the data sink to which the execution results will be written.\n\nPyFlink fully supports both the Table API and the DataStream API. Both APIs provide many different ways to define sources and sinks, and a single job can combine both APIs, for example converting between Table API reads and DataStream API writes, or DataStream API reads and Table API writes.\n\nBelow is a typical read and write example for each API. The examples assume Kafka streams provide the source/sink.\n\nRefer to the Apache Table API documentation for more details about Table API connectors, and to the Apache DataStream API documentation for more about DataStream API connectors. The Apache API conversion documentation shows how to combine Table API/DataStream API reads/writes.\n\nThere are a few things to notice:\n• The Table API examples define source/sink properties as key/value pairs. All Table API connectors follow that pattern To use a different connector, or to define a new connector that is not officially supported in PyFlink, just configure appropriate key/value pairs.\n• The DataStream API connectors are less regular; each connector provides a stack of completely different APIs. Refer to the specific connector page to see which APIs are provided. To use a connector not supported by PyFlink you need to write a Python wrapper for the corresponding Java API, see the supported connectors for examples.\n\nThe DataStream API includes the following functionality:\n• map: Convert one element into another\n• flat map: Takes one element as input and produce zero, one, or more elements\n• filter: Evaluates a boolean function for each element and filter out the ones which return false\n• windowing: Group elements into different windows and perform calculations for each group\n• connect: Connect two different streams, allows sharing state between two streams\n• process: Similar to flat map, however, is more flexible as it allows access to low level operations, e.g. timer, state, etc.\n• broadcast: Broadcast one stream to all the subtasks of another stream\n• side output: In addition to the main stream, produce additional side output result stream\n• async io: This is still not supported in PyFlink.\n\nThe Table API is a relational API with a SQL-like flavor. It includes the following functionality:\n• aggregation: Similar to SQL GROUP BY, group elements on the grouping keys and perform aggregations for each group\n• window aggregation: Group elements into different windows and perform aggregations for each window\n• temporal join: Join a stream with a versioned table, similar to lookup join, however, it allows join a table at a certain point in time\n• window join: Join elements of two streams belonging to the same window\n• interval join: Join elements of two streams with a time constraint\n• topn and windowed topn: N smallest or largest values ordered by columns\n• deduplication and windowed deduplication: Removes elements that duplicate over a set of columns\n• pattern recognition: Detect elements of a specific pattern in one stream\n\nAgain there are a few things to notice:\n• If you need fine-grained control of the transformations or access to low level functionality, e.g. timer, state, etc, choose the DataStream API. Otherwise, Table API is a good choice in most cases.\n• The Table API also supports executing SQL queries directly , providing access to functions not currently available via the API, e.g. deduplication, pattern recognition, topn, etc. Although the API will continue to grow, using SQL provides an immediate solution.\n\nFlink is a distributed compute engine which executes Flink/PyFlink jobs in a standalone cluster.. Flink jobs are executed lazily; you must explicitly submit jobs for execution. This is a little different from the more interactive/exploratory scripting style that many Python users are used to.\n\nFor example, if you have a PyFlink job defined by a Python script word_count.py, you can execute it locally via the Flink console with or by right clicking and executing in the Flink IDE. Flink will launch a mini Flink cluster which runs in a single process and executes the PyFlink job.\n\nYou can also submit a PyFlink job to a remote cluster using Flink’s command line tool.\n\nHere is a simple example that shows how to submit a PyFlink job to an Apache YARN cluster for execution:\n\nSee the Apache documentation for more about job submission in Flink.\n\nYou can read more about how to define and run a Python script as a PyFlink job in the LINK of PyFlink blog post.\n\nAt the beginning, Python user-defined functions are executed in separate Python processes which are launched during job startup. This is not easy to debug as users have to make some changes to the Python user-defined functions to enable remote debugging.\n\nSince Flink 1.14, it has supported to execute Python user-defined functions in the same Python process on the client side in local mode. Users could set breakpoints in any places where they want to debug, e.g. PyFlink framework code, Python user-defined functions, etc. This makes debugging PyFlink jobs very easy, just like debugging any other usual Python programs.\n\nUsers could also use logging inside the Python user-defined functions for debugging purposes. It should be noted that the logging messages will appear in the logging file of the TaskManagers instead of the console.\n\nBesides, it also supports Metrics in the Python user-defined functions. This is very useful for long running programs and could be used to monitor specific statistics and configure alerts.\n\nFor a production job you will almost certainly want to refer to third party Python libraries. Possibly you may also need to use data connectors whose jar files are not part of the Flink distribution - for example connectors for Kafka, HBase, Hive, and Elasticsearch are not bundled in the Flink distribution.\n\nBecause PyFlink jobs are executed in a distributed cluster, dependencies also need to be managed across the cluster. PyFlink provides a number of ways to manage dependencies.\n\nYou can include JAR files with a PyFlink job:\n\nYou must include all the transitive dependencies. For connectors, use the fat JAR whose name usually includes sql, e.g. flink-sql-connector-kafka-1.16.0.jar for the Kafka connector in preference to flink-connector-kafka-1.16.0.jar.\n\nAdd the Python dependencies to the PyFlink venv virtual environment:\n\nThe environment, with the specified libraries included, will be distributed across the cluster nodes during execution.\n\nIf you need to include a large number of Python libraries it’s good practice to pass them in archived form to the virtual environment:\n\nYou can also configure dependencies on the command line to give you extra flexibility:\n\nSee Python Dependency Management in the Apache PyFlink documentation for more details.\n\nLike Python itself, PyFlink offers great flexibility and adaptability. As you explore the APIs, here are some useful tips.\n\nIf your Python code depends on a big resource, e.g. a machine learning model, use the open()to load it once at during job initialization:\n\nThis is more efficient than opening it directly in your Python function:\n\nThe simplistic approach causes the resource to be serialized and distributed with the Python function itself and loaded with each invocation; using open() ensures it is only loaded once.\n\nWatermarks trigger the calculation of specific operators e.g. window, pattern recognition, etc when event time is enabled. Be sure to define the watermark generator, otherwise your job may have no output.\n\nPyFlink gives you several different ways to define the watermark generator:\n• SQL DDL: See Watermark section for more details.\n• Table API: Refer to this example for more details.\n• DataStream API: Refer to this example for more details.\n\nIf your watermark generator is defined correctly but the watermark isn’t advancing as expected, then possibly your job does not have enough data. This can be true during testing if you have a small test sample. Try setting the parallelism of the job to 1 or configure source idleness to work around the problem during the test phase. See ‘Timely Stream Processing’ for more about watermark behavior.\n\nThe Web UI is a rich source of information – showing how long the job has run, whether there are any exceptions, the number of input / output elements for each operator, etc.\n\nHow you access it depends on the deployment mode:\n• Local: The web port is set randomly. You can find it in the log file at /path/to/python-installation-directory/lib/python3.7/site-packages/pyflink/log/<deployment>.local.log):\n• Standalone: Configured via configuration rest.port which is 8081 by default.\n• Apache YARN: From the Web Ui of the YARN Resource Manager, find the application corresponding to the PyFlink job and then click the link under the “Tracking UI” column.\n• Kubernetes: The Web UI may be exposed via any of the following: ClusterIP, NodePort and LoadBalancer. See the Kubernetes documentation for more details.\n\nSome background understanding may help you answer questions like:\n• What’s the difference between Python API and Java API and which one should I use?\n• How to use a custom connector in PyFlink?\n• Where to find the logging messages printed in the Python user-defined functions?\n• How to tune the performance of PyFlink jobs?\n\nNote that we will not talk about basic Flink concepts here, for example the architecture of Flink, stateful streaming processing, event time and watermark which are described in detail in the official Flink documentation.\n\nPyFlink is composed of two main parts:\n• Job execution: Accepts a JobGraph and converts it into a graph of Flink operators which run in a distributed manner\n\nThink of JobGraph as the protocol between a client and a Flink cluster. It contains all the necessary information to execute a job:\n• A graph of transformations which represents the processing logic the user wants to perform\n• The name and configuration of the job\n• Dependencies required to execute the job, e.g. JAR files, Python dependencies, etc\n\nAt present, there is no multiple language support for JobGraph, which only supports Java. PyFlink reuses the existing job compiling stack of the Java API by leveraging Py4J to enable Python programs running in a Python process to access the Java objects in a JVM.\n\nMethods are called as if the Java objects resided in the Python process. Each Java API is wrapped by a corresponding Python API. When a Python program makes a PyFlink API call the corresponding Java object is created in the JVM and the method is called on it.\n\nInternally it will create a corresponding Java object in JVM and then call the corresponding API on the Java object. So it reuses the same job compiling stack as the Java API.\n• If you use the PyFlink Table API but execute only Java code the performance should be just the same as the Java Table API\n• If there is a Java class you want to use, e.g. custom connectors, that is not yet supported in PyFlink, you can just wrap it yourself\n\nMostly, wrapping the Java API works well. However, there are some exceptional cases. Let’s look at the following example:\n\nHere, all the Python methods can be mapped to Flink’s Java API except for map() which passes a lambda function ds.map(lambda x: x[1]). Java expects a Java MapFunction. To make this work in Java, we need to serialize lambda x: x[1] and wrap it with a Java wrapper object that spawns a Python process to execute it during job execution.\n\nDuring execution, a Flink job is composed of a series of Flink operators. Each operator accepts inputs from upstream operators, transforms them and produces outputs to the downstream operators. For transformations where the processing logic is Python, a specific Python operator will be generated:\n• During initialization phase, the operator will spawn a Python process and send the metadata i.e. the Python functions to be executed, to the Python process\n• After receiving data from upstream operators, the operator will send it to the Python process for execution. The data is sent to the Python process asynchronously; the operator doesn’t wait to receive the execution results for one data item before sending the next one.\n• The operator supports access to the Python state, but the Python operator runs in the JVM. Unlike data communication, state access is synchronous. The state may be cached in the Python process to improve the performance.\n• The Python operator also supports the use of logging in the Python functions. The logging messages are sent to the Python operator which runs in the JVM, and so the messages will finally appear in the log file of the TaskManagers.\n• The Python functions will be serialized during job compiling and deserialized during job execution. Keep resource usage light (see the notes on using open() above), and only use instance variables that are serializable.\n• Multiple Python functions will be chained where possible to avoid unnecessary serialization/deserialization as well as communication overhead.\n\nLaunching Python functions in a separate process works well in most cases, but again there are some exceptional cases:\n• The additional serialization/deserialization and communication overhead can be a problem with large data e.g. image processing where the image size may be very large, long strings, etc.\n• Inter-process communication also means the latency may be higher. Additionally the Python operator usually needs to buffer data to improve the network performance which adds more latency.\n• The extra process and inter-process communication creates challenges for stability.\n\nTo address these problems, Flink 1.15 thread mode is introduced as an option for executing Python functions in the JVM. By default thread mode is disabled; to use it, configure python.execution-mode: thread.\n\nWith thread mode enabled, Python functions are executed very differently than in process mode:\n• Data is processed one row at a time which increases latency.\n• But, serialization/deserialization and communication overhead are removed.\n\nNote that thread mode has specific limitations, which is why it’s not enabled by default:\n• It only supports the CPython interpreter because it depends on the CPython runtime to execute Python functions.\n• Because the CPython runtime can only be loaded once in a process, thread mode doesn’t support session mode well, where multiple jobs may need to use separate Python interpreters\n\nSee the blog post Exploring the thread mode in PyFlink for more details about thread mode.\n\nState access is supported for Python functions. This example uses state to calculate the average value of each group:\n\nHere both sum_state and cnt_state are PyFlink state objects. States can be accessed during job execution and also recovered after job failover:\n\nFrom the above diagram, we can see that:\n• The source of truth for state is the Python Operator running in the JVM\n• State access is synchronous from the user perspective\n\nThe following optimizations have been introduced to improve the performance of state access:\n• Async write: Maintains a LRU cache of the latest states and state modifications which is written back to the Python Operator asynchronously\n• Lazy read: As well as the LRU cache, MapState is read lazily to avoid unnecessary state requests\n\nIn general tuning PyFlink jobs is the same as tuning Flink Java jobs. One exception is tuning Python operator performance.\n\nPython operators launch a separate Python process to execute Python functions. Python functions that depend on large resources can potentially occupy a lot of memory.If too little memory is configured for the Python process then the stability of the job will be affected.\n\nIf a PyFlink job is run in a Kubernetes or Apache YARN deployment which strictly limits memory usage, the Python process may crash because its memory requirement exceeds the limit.\n\nYou need to design your Python code carefully. Additionally, use the following configuration options to help tune Python memory usage:\n• taskmanager.memory.managed.fraction: Fraction of total memory to be used as managed memory. (Memory of Python process is also part of managed memory)\n• taskmanager.memory.jvm-overhead.fraction: Fraction of total memory to be reserved for JVM Overhead. (Reserved memory which is not used explicitly)\n• taskmanager.memory.managed.consumer-weights: Managed memory weights for different kinds of consumers. This configuration can be used to adjust the fraction of managed memory allocated to the Python process.\n\nIn process mode, the Python operator sends data to the Python process in batches. To improve network performance it buffers data before sending it.t.\n\nDuring a checkpoint, it must wait before all the buffered data is processed. If there are many elements in a batch and the Python processing logic is inefficient then the checkpoint time will be extended. If you notice very long or even failed checkpoints, try tuning the bundle size configuration python.fn-execution.bundle.size.\n\nThread mode can improve performance in cases where the data size is large or when you need to reduce latency. Set configuration python.execution-mode: thread to enable it.\n\nPyFlink already has rich functionality. In the next phase of its evolution the community focus will be on:\n• Better support for interactive programing, e.g. retrieving only the few leading rows of an unbounded table.\n• Improved ease of use, e.g. make the API more Pythonic, improve the documentation, and add more examples."
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/hive-compatibility/hive-dialect/queries/group-by",
        "document": ""
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/tableapi",
        "document": ""
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/group-agg",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/50940064/apache-flink-how-to-group-every-n-rows-with-the-table-api",
        "document": "Recently I am trying to use Apache Flink for fast batch processing. I have a table with a column:value and an irrelevant index column\n\nBasically I want to calculate the mean and range of every 5 rows of value. Then I am going to calculate the mean and standard deviation based on those mean I just calculated. So I guess the best way is to use window.\n\nIt looks like this\n\nBut I don't know what to write in . I have tried but it said there is no such input. I just want it to group by the order as it reads from the source. But it has to be a time attribute so I cannot use - the index column as ordering as well.\n\nDo I have to add a timestamp to do this? Is it necessary in batch processing and will it slow down the calculation? What is the best way to solve this?\n\nUpdate : I tried to use a sliding window in the table API and it gets me Exception.\n\nDoes that mean that sliding window is not supported in Table API? If I recall correctly there is no window function in DataSet API. Then how do I calculate moving range in batch process?"
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/table/tableapi",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/78150208/using-flink-sql-with-table-aggregate-function-udtagg",
        "document": "I am using Flink 1.18 with Java and I am trying use a User-defined table aggregate function (UDTAGG). The documentation only includes an example with Table API for UDTAGG. However, when I try achieving same results by using a query in Flink SQL, I get planning errors.\n\nMy records consist of \"Event\" logs, where each event has a timestamp, eventId and some payload:\n\nI am trying to group events by their EventId values and apply my user-defined table aggregate function to them. The following program, which uses the Table API, works fine:\n\nNow, I am trying to use a Flink SQL query to do the same and get a type mismatch error from Calcite:\n\nAny suggestions on how a UDTAGG should be used via the SQL API in Flink?"
    }
]