[
    {
        "link": "https://pytorch.org/blog/new-library-updates-in-pytorch-1.13",
        "document": "We are bringing a number of improvements to the current PyTorch libraries, alongside the PyTorch 1.13 release. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch.\n\nAlong with 1.13, we are releasing updates to the PyTorch Libraries, please find them below.\n\nHybrid Demucs is a music source separation model that uses both spectrogram and time domain features. It has demonstrated state-of-the-art performance in the Sony® Music DeMixing Challenge. (citation: https://arxiv.org/abs/2111.03600)\n\nThe TorchAudio v0.13 release includes the following features\n• MUSDB_HQ Dataset, which is used in Hybrid Demucs training (docs)\n• Three factory functions suitable for different sample rate ranges\n• Tutorial that steps through music source separation using the pretrained pipeline (docs)\n\n* Trained on the training data of MUSDB-HQ dataset.\n\n** Trained on both training and test sets of MUSDB-HQ and 150 extra songs from an internal database that were specifically produced for Meta.\n\nSpecial thanks to Alexandre Defossez for the guidance.\n\nTorchAudio adds support for various audio-related datasets used in downstream tasks for benchmarking self-supervised learning models. With the addition of several new datasets, there is now support for the downstream tasks in version 1 of the SUPERB benchmark, which can be found in the s3prl repository.\n\nFor these datasets, we also add metadata support through a function, enabling faster dataset iteration or preprocessing without the need to load waveforms. The function returns the same features as , except it returns the relative waveform path rather than the loaded waveform.\n\nTorchAudio released a CTC beam search decoder in release 0.12, with KenLM language model support. This release, there is added functionality for creating custom Python language models that are compatible with the decoder, using the wrapper.\n\nFor more information on using a custom language model, please refer to the documentation and tutorial.\n\ntorchaudio.io.StreamWriter is a class for encoding media including audio and video. This can handle a wide variety of codecs, chunk-by-chunk encoding and GPU encoding.\n\nFor more information, refer to the documentation and the following tutorials\n\nFor a complete list of changes and new features, please visit our repository’s 0.5.0 release note.\n\nwas introduced in the last release to execute graph, with support for dynamic sharding for multi-process/distributed data loading, multiple backend ReadingServices, and graph in-place modification (e.g. shuffle control).\n\nIn this release, we further consolidated the API for and a detailed documentation is now available here. We continue to welcome early adopters and feedback, as well as potential contributors. If you are interested in trying it out, we encourage you to install the nightly version of TorchData.\n\nWe extended our support to load data from additional cloud storage providers via DataPipes, now covering AWS, Google Cloud Storage, and Azure. A tutorial is also available. We are open to feedback and feature requests.\n\nWe also performed a simple benchmark, comparing the performance of data loading from AWS S3 and attached volume on an AWS EC2 instance.\n\ntorch::deploy is now in Beta! torch::deploy is a C++ library for Linux based operating systems that allows you to run multiple Python interpreters in a single process. You can run your existing eager PyTorch models without any changes for production inference use cases. Highlights include:\n• Existing models work out of the box–no need to modify your python code to support tracing.\n• Full support for your existing Python environment including C extensions.\n• No need to cross process boundaries to load balance in multi-GPU serving environments.\n• Model weight can be shared between multiple Python interpreters.\n\ntorch::deploy now links against standard PyTorch Python distributions so all accelerators that PyTorch core supports such as CUDA and AMD/HIP work out of the box.\n• Can install any device variant of PyTorch via pip/conda like normal.\n\ntorch::deploy now has basic support for aarch64 Linux systems.\n• We’re looking to gather feedback on it and learn more about arm use cases for eager PyTorch models.\n• Learn more / share your use case at https://github.com/pytorch/multipy/issues/64\n\nTorchEval is a library built for users who want highly performant implementations of common metrics to evaluate machine learning models. It also provides an easy to use interface for building custom metrics with the same toolkit. Building your metrics with TorchEval makes running distributed training loops with torch.distributed a breeze.\n\nLearn more with our docs, see our examples, or check out our GitHub repo.\n\nPlease watch for upcoming blogs in early November that will introduce TorchMultimodal, a PyTorch domain library for training SoTA multi-task multimodal models at scale, in more details; in the meantime, play around with the library and models through our tutorial.\n\nWe’ve provided a simplified and more intuitive API for setting fused optimizer settings via apply_optimizer_in_backward. This new approach enables the ability to specify optimizer settings on a per-parameter basis and sharded modules will configure FBGEMM’s TableBatchedEmbedding modules accordingly. Additionally, this now let’s TorchRec’s planner account for optimizer memory usage. This should alleviate reports of sharding jobs OOMing after using Adam using a plan generated from planner.\n\nWe’re introducing the shard API, which now allows you to shard only the embedding modules within a model, and provides an alternative to the current main entry point - DistributedModelParallel. This lets you have a finer grained control over the rest of the model, which can be useful for customized parallelization logic, and inference use cases (which may not require any parallelization on the dense layers). We’re also introducing construct_module_sharding_plan, providing a simpler interface to the TorchRec sharder.\n\nApplying quantization or mixed precision to tensors in a collective call during model parallel training greatly improves training efficiency, with little to no effect on model quality. TorchRec now integrates with the quantized comms library provided by FBGEMM GPU and provides an interface to construct encoders and decoders (codecs) that surround the all_to_all, and reduce_scatter collective calls in the output_dist of a sharded module. We also allow you to construct your own codecs to apply to your sharded module. The codces provided by FBGEMM allow FP16, BF16, FP8, and INT8 compressions, and you may use different quantizations for the forward pass and backward pass.\n\nAlong with PyTorch 1.13, we are releasing the beta version of TorchSnapshot, which is a performant, memory-efficient checkpointing library for PyTorch applications, designed with large, complex distributed workloads in mind. Highlights include:\n• Performance: TorchSnapshot provides a fast checkpointing implementation employing various optimizations, including zero-copy serialization for most tensor types, overlapped device-to-host copy and storage I/O, parallelized storage I/O\n• Memory Use: TorchSnapshot’s memory usage adapts to the host’s available resources, greatly reducing the chance of out-of-memory issues when saving and loading checkpoints\n• Usability: Simple APIs that are consistent between distributed and non-distributed workloads\n\nLearn more with our tutorial.\n\nWe are happy to introduce torchvision v0.14 (release note). This version introduces a new model registration API to help users retrieving and listing models and weights. It also includes new image and video classification models such as MViT, S3D, Swin Transformer V2, and MaxViT. Last but not least, we also have new primitives and augmentation such as PolynomicalLR scheduler and SimpleCopyPaste.\n\nFollowing up on the multi-weight support API that was released on the previous version, we have added a new model registration API to help users retrieve models and weights. There are now 4 new methods under the torchvision.models module: get_model, get_model_weights, get_weight, and list_models. Here are examples of how we can use them:\n\nWe added two new video classification models, MViT and S3D. MViT is a state of the art video classification transformer model which has 80.757% accuracy on the Kinetics400 dataset, while S3D is a relatively small model with good accuracy for its size. These models can be used as follows:\n\nHere is the table showing the accuracy of the new video classification models tested in the Kinetics400 dataset.\n\nWe would like to thank Haoqi Fan, Yanghao Li, Christoph Feichtenhofer and Wan-Yen Lo for their work on PyTorchVideo and their support during the development of the MViT model. We would like to thank Sophia Zhi for her contribution implementing the S3D model in torchvision.\n\nFor Classification Models, we’ve added the Swin Transformer V2 architecture along with pre-trained weights for its tiny/small/base variants. In addition, we have added support for the MaxViT transformer. Here is an example on how to use the models:\n\nHere is the table showing the accuracy of the models tested on ImageNet1K dataset.\n\nWe would like to thank Ren Pang and Teodor Poncu for contributing the 2 models to torchvision.\n\nIn this release we’ve added the SimpleCopyPaste augmentation in our reference scripts and we up-streamed the PolynomialLR scheduler to PyTorch Core. We would like to thank Lezwon Castelino and Federico Pozzi for their contributions. We are continuing our efforts to modernize TorchVision by adding more SoTA primitives, Augmentations and architectures with the help of our community. If you are interested in contributing, have a look at the following issue.\n\nTorch-TensorRT is the PyTorch integration for TensorRT, providing high performance inference on NVIDIA GPUs. Torch-TRT allows for optimizing models directly in PyTorch for deployment providing up to 6x performance improvement.\n\nTorch-TRT is an AoT compiler which ingests an nn.Module or TorchScript module, optimizes compatible subgraphs in TensorRT & leaves the rest to run in PyTorch. This gives users the performance of TensorRT, but the usability and familiarity of Torch.\n\nTorch-TensorRT is part of the PyTorch ecosystem, and was released as v1.0 in November ‘21. There are currently two distinct front-ends: Torchscript & FX. Each provides the same value proposition and underlying operation with the primary difference being the input & output formats (TS vs FX / Python).\n\nThe Torchscript front-end was included in v1.0 and should be considered stable. The FX front-end is first released in v1.2 and should be considered a Beta.\n\nTorch-TensorRT is an integration for PyTorch that leverages inference optimizations of TensorRT on NVIDIA GPUs. It takes advantage of TensorRT optimizations, such as FP16 and INT8 reduced precision, graph optimization, operation fusion, etc. while offering a fallback to native PyTorch when TensorRT does not support the model subgraphs. Currently, there are two frontend paths existing in the library that help to convert a PyTorch model to tensorRT engine. One path is through Torch Script (TS) and the other is through FX frontend. That being said, the models are traced by either TS or FX into their IR graph and then converted to TensorRT from it.\n\nLearn more with our tutorial.\n\nTorchX 0.3 updates include a new list API, experiment tracking, elastic training and improved scheduler support. There’s also a new Multi-Objective NAS tutorial using TorchX + Ax.\n\nThe newly added list command and API allows you to list recently launched jobs and their statuses for a given scheduler directly from within TorchX.\n• This removes the need for using secondary tools to list the jobs.\n• Full programmatic access to recent jobs for integration with custom tools.\n\nLearn more with our documentation.\n\nTorchX Tracker is a new prototype library that provides a flexible and customizable experiment and artifact tracking interface. This allows you to track inputs and outputs for jobs across multiple steps to make it easier to use TorchX with pipelines and other external systems.\n\nElasticity on Ray and Kubernetes – automatic scale up of distributed training jobs when using a supported scheduler. Learn more with our documentation.\n\nThe AWS Batch scheduler integration is now in beta.\n• log fetching and listing jobs is now supported.\n\nDrop in replacement for AdamW optimizer that reduces GPU memory, enables two main features:\n• Ability to successfully train the entire model pipeline in full BFloat16. Kahan summation ensures precision. This can improve training throughput, especially on huge models, by reduced memory and increased computation speed.\n• Ability to change the variance state to BFloat16. This can reduce overall memory required for model training with additional speed improvements."
    },
    {
        "link": "https://i32n.com/docs/pytorch/tutorials/recipes/recipes/tuning_guide.html",
        "document": "Click here to download the full example code\n\nPerformance Tuning Guide is a set of optimizations and best practices which can accelerate training and inference of deep learning models in PyTorch. Presented techniques often can be implemented by changing only a few lines of code and can be applied to a wide range of deep learning models across all domains.\n\ntorch.utils.data.DataLoader supports asynchronous data loading and data augmentation in separate worker subprocesses. The default setting for is , which means that the data loading is synchronous and done in the main process. As a result the main training process has to wait for the data to be available to continue the execution. Setting enables asynchronous data loading and overlap between the training and data loading. should be tuned depending on the workload, CPU, GPU, and location of training data. accepts argument, which defaults to . When using a GPU it’s better to set , this instructs to use pinned memory and enables faster and asynchronous memory copy from the host to the GPU. PyTorch saves intermediate buffers from all operations which involve tensors that require gradients. Typically gradients aren’t needed for validation or inference. torch.no_grad() context manager can be applied to disable gradient calculation within a specified block of code, this accelerates execution and reduces the amount of required memory. torch.no_grad() can also be used as a function decorator. Disable bias for convolutions directly followed by a batch norm¶ torch.nn.Conv2d() has parameter which defaults to (the same is true for Conv1d and Conv3d ). If a layer is directly followed by a layer, then the bias in the convolution is not needed, instead use . Bias is not needed because in the first step subtracts the mean, which effectively cancels out the effect of bias. This is also applicable to 1d and 3d convolutions as long as (or other normalization layer) normalizes on the same dimension as convolution’s bias. Models available from torchvision already implement this optimization. Use parameter.grad = None instead of model.zero_grad() or optimizer.zero_grad()¶ to zero out gradients, use the following method instead: The second code snippet does not zero the memory of each individual parameter, also the subsequent backward pass uses assignment instead of addition to store gradients, this reduces the number of memory operations. Setting gradient to has a slightly different numerical behavior than setting it to zero, for more details refer to the documentation. Pointwise operations (elementwise addition, multiplication, math functions - , , etc.) can be fused into a single kernel to amortize memory access time and kernel launch time. PyTorch JIT can fuse kernels automatically, although there could be additional fusion opportunities not yet implemented in the compiler, and not all device types are supported equally. Pointwise operations are memory-bound, for each operation PyTorch launches a separate kernel. Each kernel loads data from the memory, performs computation (this step is usually inexpensive) and stores results back into the memory. Fused operator launches only one kernel for multiple fused pointwise ops and loads/stores data only once to the memory. This makes JIT very useful for activation functions, optimizers, custom RNN cells etc. In the simplest case fusion can be enabled by applying torch.jit.script decorator to the function definition, for example: Refer to TorchScript documentation for more advanced use cases. PyTorch 1.5 introduced support for memory format for convolutional networks. This format is meant to be used in conjunction with AMP to further accelerate convolutional neural networks with Tensor Cores. Support for is experimental, but it’s expected to work for standard computer vision models (e.g. ResNet-50, SSD). To convert models to format follow Channels Last Memory Format Tutorial. The tutorial includes a section on converting existing models. Buffer checkpointing is a technique to mitigate the memory capacity burden of model training. Instead of storing inputs of all layers to compute upstream gradients in backward propagation, it stores the inputs of a few layers and the others are recomputed during backward pass. The reduced memory requirements enables increasing the batch size that can improve utilization. Checkpointing targets should be selected carefully. The best is not to store large layer outputs that have small re-computation cost. The example target layers are activation functions (e.g. , , ), up/down sampling and matrix-vector operations with small accumulation depth. Many PyTorch APIs are intended for debugging and should be disabled for regular training runs:\n\nNUMA or non-uniform memory access is a memory layout design used in data center machines meant to take advantage of locality of memory in multi-socket machines with multiple memory controllers and blocks. Generally speaking, all deep learning workloads, training or inference, get better performance without accessing hardware resources across NUMA nodes. Thus, inference can be run with multiple instances, each instance runs on one socket, to raise throughput. For training tasks on single node, distributed training is recommended to make each training process run on one socket. In general cases the following command executes a PyTorch script on cores on the Nth node only, and avoids cross-socket memory access to reduce memory access overhead. More detailed descriptions can be found here. OpenMP is utilized to bring better performance for parallel computation tasks. is the easiest switch that can be used to accelerate computations. It determines number of threads used for OpenMP computations. CPU affinity setting controls how workloads are distributed over multiple cores. It affects communication overhead, cache line invalidation overhead, or page thrashing, thus proper setting of CPU affinity brings performance benefits. or determines how to bind OpenMP* threads to physical processing units. Detailed information can be found here. With the following command, PyTorch run the task on N OpenMP threads. Typically, the following environment variables are used to set for CPU affinity with GNU OpenMP implementation. specifies whether threads may be moved between processors. Setting it to CLOSE keeps OpenMP threads close to the primary thread in contiguous place partitions. determines how OpenMP threads are scheduled. binds threads to specific CPUs. By default, PyTorch uses GNU OpenMP (GNU ) for parallel computation. On Intel platforms, Intel OpenMP Runtime Library ( ) provides OpenMP API specification support. It sometimes brings more performance benefits compared to . Utilizing environment variable can switch OpenMP library to : Similar to CPU affinity settings in GNU OpenMP, environment variables are provided in to control CPU affinity settings. binds OpenMP threads to physical processing units. sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping. In most cases, setting to 1 or 0 yields good performances. The following commands show a common settings with Intel OpenMP Runtime Library. For deep learning workloads, or can get better performance by reusing memory as much as possible than default function. Jemalloc is a general purpose implementation that emphasizes fragmentation avoidance and scalable concurrency support. TCMalloc also features a couple of optimizations to speed up program executions. One of them is holding memory in caches to speed up access of commonly-used objects. Holding such caches even after deallocation also helps avoid costly system calls if such memory is later re-allocated. Use environment variable to take advantage of one of them. Use oneDNN Graph with TorchScript for inference¶ oneDNN Graph can significantly boost inference performance. It fuses some compute-intensive operations such as convolution, matmul with their neighbor operations. In PyTorch 2.0, it is supported as a beta feature for & data-types. oneDNN Graph receives the model’s graph and identifies candidates for operator-fusion with respect to the shape of the example input. A model should be JIT-traced using an example input. Speed-up would then be observed after a couple of warm-up iterations for inputs with the same shape as the example input. The example code-snippets below are for resnet50, but they can very well be extended to use oneDNN Graph with custom models as well. # Only this extra line of code is required to use oneDNN Graph Using the oneDNN Graph API requires just one extra line of code for inference with Float32. If you are using oneDNN Graph, please avoid calling . # sample input should be of the same shape as expected inputs # Using resnet50 from torchvision in this example for illustrative purposes, # but the line below can indeed be modified to use custom models as well. # Tracing the model with example input Once a model is JIT-traced with a sample input, it can then be used for inference after a couple of warm-up runs. # speedup would be observed after warm-up runs While the JIT fuser for oneDNN Graph also supports inference with datatype, performance benefit with oneDNN Graph is only exhibited by machines with AVX512_BF16 instruction set architecture (ISA). The following code snippets serves as an example of using datatype for inference with oneDNN Graph: # AMP for JIT mode is enabled by default, and is divergent with its eager mode counterpart # Conv-BatchNorm folding for CNN-based Vision Models should be done with ``torch.fx.experimental.optimization.fuse`` when AMP is used # Please note that optimization.fuse need not be called when AMP is not used # speedup would be observed in subsequent runs. For small scale models or memory-bound models, such as DLRM, training on CPU is also a good choice. On a machine with multiple sockets, distributed training brings a high-efficient hardware resource usage to accelerate the training process. Torch-ccl, optimized with Intel(R) (collective communications library) for efficient distributed deep learning training implementing such collectives like , , , implements PyTorch C10D API and can be dynamically loaded as external . Upon optimizations implemented in PyTorch DDP module, accelerates communication operations. Beside the optimizations made to communication kernels, also features simultaneous computation-communication functionality.\n\nNVIDIA cuDNN supports many algorithms to compute a convolution. Autotuner runs a short benchmark and selects the kernel with the best performance on a given hardware for a given input size. For convolutional networks (other types currently not supported), enable cuDNN autotuner before launching the training loop by setting:\n• None the auto-tuner decisions may be non-deterministic; different algorithm may be selected for different runs. For more details see PyTorch: Reproducibility\n• None in some rare cases, such as with highly variable input sizes, it’s better to run convolutional networks with autotuner disabled to avoid the overhead associated with algorithm selection for each input size. Avoid unnecessary synchronizations, to let the CPU run ahead of the accelerator as much as possible to make sure that the accelerator work queue contains many operations. When possible, avoid operations which require synchronizations, for example:\n• None python control flow which depends on results of operations performed on CUDA tensors e.g. Instead of calling to generate a random tensor, produce the output directly on the target device: . This is applicable to all functions which create new tensors and accept argument: torch.rand(), torch.zeros(), torch.full() and similar. Mixed precision leverages Tensor Cores and offers up to 3x overall speedup on Volta and newer GPU architectures. To use Tensor Cores AMP should be enabled and matrix/tensor dimensions should satisfy requirements for calling kernels that use Tensor Cores.\n• None set sizes to multiples of 8 (to map onto dimensions of Tensor Cores)\n• None see Deep Learning Performance Documentation for more details and guidelines specific to layer type\n• None if layer size is derived from other parameters rather than fixed, it can still be explicitly padded e.g. vocabulary size in NLP models\n• \n• None native PyTorch AMP is available starting from PyTorch 1.6: documentation, examples, tutorial Models for speech recognition or for NLP are often trained on input tensors with variable sequence length. Variable length can be problematic for PyTorch caching allocator and can lead to reduced performance or to unexpected out-of-memory errors. If a batch with a short sequence length is followed by an another batch with longer sequence length, then PyTorch is forced to release intermediate buffers from previous iteration and to re-allocate new buffers. This process is time consuming and causes fragmentation in the caching allocator which may result in out-of-memory errors. A typical solution is to implement preallocation. It consists of the following steps:\n• None generate a (usually random) batch of inputs with maximum sequence length (either corresponding to max length in the training dataset or to some predefined threshold)\n• None execute a forward and a backward pass with the generated batch, do not execute an optimizer or a learning rate scheduler, this step preallocates buffers of maximum size, which can be reused in subsequent training iterations\n\nPyTorch has two ways to implement data-parallel training: offers much better performance and scaling to multiple-GPUs. For more information refer to the relevant section of CUDA Best Practices from PyTorch documentation. Skip unnecessary all-reduce if training with and gradient accumulation¶ By default torch.nn.parallel.DistributedDataParallel executes gradient all-reduce after every backward pass to compute the average gradient over all workers participating in the training. If training uses gradient accumulation over N steps, then all-reduce is not necessary after every training step, it’s only required to perform all-reduce after the last call to backward, just before the execution of the optimizer. provides no_sync() context manager which disables gradient all-reduce for particular iteration. should be applied to first iterations of gradient accumulation, the last iteration should follow the default execution and perform the required gradient all-reduce. Match the order of layers in constructors and during the execution if using DistributedDataParallel``(find_unused_parameters=True)¶ torch.nn.parallel.DistributedDataParallel with uses the order of layers and parameters from model constructors to build buckets for gradient all-reduce. overlaps all-reduce with the backward pass. All-reduce for a particular bucket is asynchronously triggered only when all gradients for parameters in a given bucket are available. To maximize the amount of overlap, the order in model constructors should roughly match the order during the execution. If the order doesn’t match, then all-reduce for the entire bucket waits for the gradient which is the last to arrive, this may reduce the overlap between backward pass and all-reduce, all-reduce may end up being exposed, which slows down the training. with (which is the default setting) relies on automatic bucket formation based on order of operations encountered during the backward pass. With it’s not necessary to reorder layers or parameters to achieve optimal performance. Load imbalance typically may happen for models processing sequential data (speech recognition, translation, language models etc.). If one device receives a batch of data with sequence length longer than sequence lengths for the remaining devices, then all devices wait for the worker which finishes last. Backward pass functions as an implicit synchronization point in a distributed setting with DistributedDataParallel backend. There are multiple ways to solve the load balancing problem. The core idea is to distribute workload over all workers as uniformly as possible within each global batch. For example Transformer solves imbalance by forming batches with approximately constant number of tokens (and variable number of sequences in a batch), other models solve imbalance by bucketing samples with similar sequence length or even by sorting dataset by sequence length."
    },
    {
        "link": "https://discuss.pytorch.org/t/how-to-implement-a-custom-layer/153590",
        "document": "Hi\n\n I’d like to test a custom layer. I wrote the init and forward methods, but when I test it on a MNIST testcase, the training loss doesn’t change. What did I miss? Should I also write a backward method ?\n\n Is there any good tutorial with examples somewhere ?\n\nThanks for your help"
    },
    {
        "link": "https://pytorch.org/tutorials",
        "document": ""
    },
    {
        "link": "https://upstream.i32n.com/docs/pytorch/tutorials/intermediate/pruning_tutorial.html",
        "document": "State-of-the-art deep learning techniques rely on over-parametrized models that are hard to deploy. On the contrary, biological neural networks are known to use efficient sparse connectivity. Identifying optimal techniques to compress models by reducing the number of parameters in them is important in order to reduce memory, battery, and hardware consumption without sacrificing accuracy. This in turn allows you to deploy lightweight models on device, and guarantee privacy with private on-device computation. On the research front, pruning is used to investigate the differences in learning dynamics between over-parametrized and under-parametrized networks, to study the role of lucky sparse subnetworks and initializations (“lottery tickets”) as a destructive neural architecture search technique, and more.\n\nIn this tutorial, you will learn how to use to sparsify your neural networks, and how to extend it to implement your own custom pruning technique.\n\nTo prune a module (in this example, the layer of our LeNet architecture), first select a pruning technique among those available in (or implement your own by subclassing ). Then, specify the module and the name of the parameter to prune within that module. Finally, using the adequate keyword arguments required by the selected pruning technique, specify the pruning parameters. In this example, we will prune at random 30% of the connections in the parameter named in the layer. The module is passed as the first argument to the function; identifies the parameter within that module using its string identifier; and indicates either the percentage of connections to prune (if it is a float between 0. and 1.), or the absolute number of connections to prune (if it is a non-negative integer). Pruning acts by removing from the parameters and replacing it with a new parameter called (i.e. appending to the initial parameter ). stores the unpruned version of the tensor. The was not pruned, so it will remain intact. The pruning mask generated by the pruning technique selected above is saved as a module buffer named (i.e. appending to the initial parameter ). For the forward pass to work without modification, the attribute needs to exist. The pruning techniques implemented in compute the pruned version of the weight (by combining the mask with the original parameter) and store them in the attribute . Note, this is no longer a parameter of the , it is now simply an attribute. Finally, pruning is applied prior to each forward pass using PyTorch’s . Specifically, when the is pruned, as we have done here, it will acquire a for each parameter associated with it that gets pruned. In this case, since we have so far only pruned the original parameter named , only one hook will be present. For completeness, we can now prune the too, to see how the parameters, buffers, hooks, and attributes of the change. Just for the sake of trying out another pruning technique, here we prune the 3 smallest entries in the bias by L1 norm, as implemented in the pruning function. We now expect the named parameters to include both (from before) and . The buffers will include and . The pruned versions of the two tensors will exist as module attributes, and the module will now have two .\n\nThe same parameter in a module can be pruned multiple times, with the effect of the various pruning calls being equal to the combination of the various masks applied in series. The combination of a new mask with the old mask is handled by the ’s method. Say, for example, that we now want to further prune , this time using structured pruning along the 0th axis of the tensor (the 0th axis corresponds to the output channels of the convolutional layer and has dimensionality 6 for ), based on the channels’ L2 norm. This can be achieved using the function, with and . # As we can verify, this will zero out all the connections corresponding to # 50% (3 out of 6) of the channels, while preserving the action of the The corresponding hook will now be of type , and will store the history of pruning applied to the parameter.\n\nSo far, we only looked at what is usually referred to as “local” pruning, i.e. the practice of pruning tensors in a model one by one, by comparing the statistics (weight magnitude, activation, gradient, etc.) of each entry exclusively to the other entries in that tensor. However, a common and perhaps more powerful technique is to prune the model all at once, by removing (for example) the lowest 20% of connections across the whole model, instead of removing the lowest 20% of connections in each layer. This is likely to result in different pruning percentages per layer. Let’s see how to do that using from . Now we can check the sparsity induced in every pruned parameter, which will not be equal to 20% in each layer. However, the global sparsity will be (approximately) 20%.\n\nTo implement your own pruning function, you can extend the module by subclassing the base class, the same way all other pruning methods do. The base class implements the following methods for you: , , , , and . Beyond some special cases, you shouldn’t have to reimplement these methods for your new pruning technique. You will, however, have to implement (the constructor), and (the instructions on how to compute the mask for the given tensor according to the logic of your pruning technique). In addition, you will have to specify which type of pruning this technique implements (supported options are , , and ). This is needed to determine how to combine masks in the case in which pruning is applied iteratively. In other words, when pruning a prepruned parameter, the current pruning technique is expected to act on the unpruned portion of the parameter. Specifying the will enable the (which handles the iterative application of pruning masks) to correctly identify the slice of the parameter to prune. Let’s assume, for example, that you want to implement a pruning technique that prunes every other entry in a tensor (or – if the tensor has previously been pruned – in the remaining unpruned portion of the tensor). This will be of because it acts on individual connections in a layer and not on entire units/channels ( ), or across different parameters ( ). \"\"\"Prune every other entry in a tensor Now, to apply this to a parameter in an , you should also provide a simple function that instantiates the method and applies it. \"\"\"Prunes tensor corresponding to parameter called `name` in `module` by removing every other entry in the tensors. Modifies module in place (and also return the modified module) 1) adding a named buffer called `name+'_mask'` corresponding to the binary mask applied to the parameter `name` by the pruning method. The parameter `name` is replaced by its pruned version, while the original (unpruned) parameter is stored in a new parameter named module (nn.Module): module containing the tensor to prune name (string): parameter name within `module` on which pruning"
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html",
        "document": "Neural networks comprise of layers/modules that perform operations on data. The torch.nn namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the nn.Module. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.\n\nIn the following sections, we’ll build a neural network to classify images in the FashionMNIST dataset.\n\nWe want to be able to train our model on an accelerator such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.\n\nMany layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s or methods. In this example, we iterate over each parameter, and print its size and a preview of its values."
    },
    {
        "link": "https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html",
        "document": "Click here to download the full example code\n\nDeep learning uses artificial neural networks (models), which are computing systems that are composed of many layers of interconnected units. By passing data through these interconnected units, a neural network is able to learn how to approximate the computations required to transform inputs into outputs. In PyTorch, neural networks can be constructed using the package.\n\nPyTorch provides the elegantly designed modules and classes, including , to help you create and train neural networks. An contains layers, and a method that returns the . In this recipe, we will use to define a neural network intended for the MNIST dataset."
    },
    {
        "link": "https://pytorch.org/docs/stable/notes/modules.html",
        "document": "PyTorch uses modules to represent neural networks. Modules are:\n• None Building blocks of stateful computation. PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for easy construction of elaborate, multi-layer neural networks.\n• None Tightly integrated with PyTorch’s autograd system. Modules make it simple to specify learnable parameters for PyTorch’s Optimizers to update.\n• None Easy to work with and transform. Modules are straightforward to save and restore, transfer between CPU / GPU / TPU devices, prune, quantize, and more.\n\nThis note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch, many topics in this note are elaborated on in other notes or tutorials, and links to many of those documents are provided here as well.\n\nTo get started, let’s look at a simpler, custom version of PyTorch’s module. This module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules:\n• None It inherits from the base Module class. All modules should subclass for composability with other modules.\n• None It defines some “state” that is used in computation. Here, the state consists of randomly-initialized and tensors that define the affine transformation. Because each of these is defined as a , they are registered for the module and will automatically be tracked and returned from calls to . Parameters can be considered the “learnable” aspects of the module’s computation (more on this later). Note that modules are not required to have state, and can also be stateless.\n• None It defines a forward() function that performs the computation. For this affine transformation module, the input is matrix-multiplied with the parameter (using the short-hand notation) and added to the parameter to produce the output. More generally, the implementation for a module can perform arbitrary computation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be constructed and called: Note that the module itself is callable, and that calling it invokes its function. This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module. The “forward pass” is responsible for applying the computation represented by the module to the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of module outputs with respect to its inputs, which can be used for “training” parameters through gradient descent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it is not required to manually implement a function for each module. The process of training module parameters through successive forward / backward passes is covered in detail in Neural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call to or , where the latter includes each parameter’s name: In general, the parameters registered by a module are aspects of the module’s computation that should be “learned”. A later section of this note shows how to update these parameters using one of PyTorch’s Optimizers. Before we get to that, however, let’s first examine how modules can be composed with one another.\n\nModules can contain other modules, making them useful building blocks for developing more elaborate functionality. The simplest way to do this is using the module. It allows us to chain together multiple modules: Note that automatically feeds the output of the first module as input into the , and the output of that as input into the second module. As shown, it is limited to in-order chaining of modules with a single input and output. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives full flexibility on how submodules are used for a module’s computation. For example, here’s a simple neural network implemented as a custom module: This module is composed of two “children” or “submodules” ( and ) that define the layers of the neural network and are utilized for computation within the module’s method. Immediate children of a module can be iterated through via a call to or : To go deeper than just the immediate children, and recursively iterate through a module and its child modules: Sometimes, it’s necessary for a module to dynamically define submodules. The and modules are useful here; they register submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules. This means that calls to and will recursively include child parameters, allowing for convenient optimization of all parameters within the network: It’s also easy to move all parameters to a different device or change their precision using : More generally, an arbitrary function can be applied to a module and its submodules recursively by using the function. For example, to apply custom initialization to parameters of a module and its submodules: # Note that no_grad() is used here to avoid tracking this computation in the autograd graph. # Apply the function recursively on the module and its submodules. These examples show how elaborate neural networks can be formed through module composition and conveniently manipulated. To allow for quick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of performant modules within the namespace that perform common neural network operations like pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out:\n\nOnce a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s Optimizers from : # Create the network (from previous section) and optimizer # to output the constant zero function # After training, switch the module to eval mode to do inference, compute performance metrics, etc. # (see discussion below for a description of training and evaluation modes) In this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according to its absolute value by employing as a loss function. While this is not a very interesting task, the key parts of training are present:\n• None An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network’s parameters are associated with it.\n• None\n• None calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network’s parameters have changed. In particular, examining the value of ‘s parameter shows that its values are now much closer to 0 (as may be expected): Note that the above process is done entirely while the network module is in “training mode”. Modules default to training mode and can be switched between training and evaluation modes using and . They can behave differently depending on which mode they are in. For example, the module maintains a running mean and variance during training that are not updated when the module is in evaluation mode. In general, modules should be in training mode during training and only switched to evaluation mode for inference or evaluation. Below is an example of a custom module that behaves differently between the two modes: Training neural networks can often be tricky. For more information, check out:\n\nIn the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation. Now, if we want to save the trained model to disk, we can do so by saving its (i.e. “state dictionary”): # Load the module later on A module’s contains state that affects its computation. This includes, but is not limited to, the module’s parameters. For some modules, it may be useful to have state beyond parameters that affects module computation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent” and “non-persistent”. Following is an overview of the various types of state a module can have:\n• None Parameters: learnable aspects of computation; contained within the\n• \n• None Persistent buffers: contained within the (i.e. serialized when saving & loading)\n• None Non-persistent buffers: not contained within the (i.e. left out of serialization) As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want the current value of the running mean to be considered part of the module’s so that it will be restored when loading a serialized form of the module, but we don’t want it to be learnable. This snippet shows how to use to accomplish this: Now, the current value of the running mean is considered part of the module’s and will be properly restored when loading the module from disk: # Serialized form will contain the 'mean' tensor As mentioned previously, buffers can be left out of the module’s by marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with : # Moves all module parameters and buffers to the specified device / dtype Buffers of a module can be iterated over using or . The following class demonstrates the various ways of registering parameters and buffers within a module: # Setting a nn.Parameter as an attribute of the module automatically registers the tensor # as a parameter of the module. # Reserves the \"param3\" attribute as a parameter, preventing it from being set to anything # except a parameter. \"None\" entries like this will not be present in the module's state_dict. # Registers a persistent buffer (one that appears in the module's state_dict). # Registers a non-persistent buffer (one that does not appear in the module's state_dict). # Reserves the \"buffer3\" attribute as a buffer, preventing it from being set to anything # except a buffer. \"None\" entries like this will not be present in the module's state_dict. # Adding a submodule registers its parameters as parameters of the module. # Note that non-persistent buffer \"buffer2\" and reserved attributes \"param3\" and \"buffer3\" do # not appear in the state_dict. For more information, check out:\n\nBy default, parameters and floating-point buffers for modules provided by are initialized during module instantiation as 32-bit floating point values on the CPU using an initialization scheme determined to perform well historically for the module type. For certain use cases, it may be desired to initialize with a different dtype, device (e.g. GPU), or initialization technique. Note that the device and dtype options demonstrated above also apply to any floating-point buffers registered for the module: While module writers can use any device or dtype to initialize parameters in their custom modules, good practice is to use and by default as well. Optionally, you can provide full flexibility in these areas for your custom module by conforming to the convention demonstrated above that all modules follow:\n• None Provide a constructor kwarg that applies to any parameters / buffers registered by the module.\n• None Provide a constructor kwarg that applies to any parameters / floating-point buffers registered by the module.\n• None Only use initialization functions (i.e. functions from ) on parameters and buffers within the module’s constructor. Note that this is only required to use ; see this page for an explanation. For more information, check out:\n\nIn Neural Network Training with Modules, we demonstrated the training process for a module, which iteratively performs forward and backward passes, updating module parameters each iteration. For more control over this process, PyTorch provides “hooks” that can perform arbitrary computation during a forward or backward pass, even modifying how the pass is done if desired. Some useful examples for this functionality include debugging, visualizing activations, examining gradients in-depth, etc. Hooks can be added to modules you haven’t written yourself, meaning this functionality can be applied to third-party or PyTorch-provided modules. PyTorch provides two types of hooks for modules:\n• None Forward hooks are called during the forward pass. They can be installed for a given module with and . These hooks will be called respectively just before the forward function is called and just after it is called. Alternatively, these hooks can be installed globally for all modules with the analogous and functions.\n• None Backward hooks are called during the backward pass. They can be installed with and . These hooks will be called when the backward for this Module has been computed. will allow the user to access the gradients for outputs while will allow the user to access the gradients both the inputs and outputs. Alternatively, they can be installed globally for all modules with and . All hooks allow the user to return an updated value that will be used throughout the remaining computation. Thus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or modify some inputs/outputs without having to change the module’s function. Below is an example demonstrating usage of forward and backward hooks: # Allows for examination and modification of the input before the forward pass. # Note that inputs are always wrapped in a tuple. # Allows for examination of inputs / outputs and modification of the outputs # after the forward pass. Note that inputs are always wrapped in a tuple while outputs # Allows for examination of grad_inputs / grad_outputs and modification of # grad_inputs used in the rest of the backwards pass. Note that grad_inputs and # grad_outputs are always wrapped in tuples. # Run input through module before and after adding hooks. # Note that the modified input results in a different output. # Remove hooks; note that the output here matches the output before adding hooks."
    },
    {
        "link": "https://d2l.ai/chapter_builders-guide/custom-layer.html",
        "document": "One factor behind deep learning’s success is the availability of a wide range of layers that can be composed in creative ways to design architectures suitable for a wide variety of tasks. For instance, researchers have invented layers specifically for handling images, text, looping over sequential data, and performing dynamic programming. Sooner or later, you will need a layer that does not exist yet in the deep learning framework. In these cases, you must build a custom layer. In this section, we show you how.\n\nTo start, we construct a custom layer that does not have any parameters of its own. This should look familiar if you recall our introduction to modules in Section 6.1. The following class simply subtracts the mean from its input. To build it, we simply need to inherit from the base layer class and implement the forward propagation function. Let’s verify that our layer works as intended by feeding some data through it. We can now incorporate our layer as a component in constructing more complex models. As an extra sanity check, we can send random data through the network and check that the mean is in fact 0. Because we are dealing with floating point numbers, we may still see a very small nonzero number due to quantization. Here we utilize the method which returns both the output of the network as well as the parameters. In this case we only focus on the output.\n\nNow that we know how to define simple layers, let’s move on to defining layers with parameters that can be adjusted through training. We can use built-in functions to create parameters, which provide some basic housekeeping functionality. In particular, they govern access, initialization, sharing, saving, and loading model parameters. This way, among other benefits, we will not need to write custom serialization routines for every custom layer. Now let’s implement our own version of the fully connected layer. Recall that this layer requires two parameters, one to represent the weight and the other for the bias. In this implementation, we bake in the ReLU activation as a default. This layer requires two input arguments: and , which denote the number of inputs and outputs, respectively. Next, we instantiate the class and access its model parameters. Next, we instantiate the class and access its model parameters. Next, we instantiate the class and access its model parameters. Next, we instantiate the class and access its model parameters. We can directly carry out forward propagation calculations using custom layers. We can also construct models using custom layers. Once we have that we can use it just like the built-in fully connected layer.\n\nWe can design custom layers via the basic layer class. This allows us to define flexible new layers that behave differently from any existing layers in the library. Once defined, custom layers can be invoked in arbitrary contexts and architectures. Layers can have local parameters, which can be created through built-in functions."
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html",
        "document": "In the constructor we instantiate four parameters and assign them as In the forward function we accept a Tensor of input data and we must return a Tensor of output data. We can use Modules defined in the constructor as well as arbitrary operators on Tensors. Just like any class in Python, you can also define custom method on PyTorch modules # Construct our model by instantiating the class defined above # Construct our loss function and an Optimizer. The call to model.parameters() # in the SGD constructor will contain the learnable parameters (defined # with torch.nn.Parameter) which are members of the model. # Forward pass: Compute predicted y by passing x to the model # Zero gradients, perform a backward pass, and update the weights."
    }
]