[
    {
        "link": "https://huggingface.co/docs/transformers/en/index",
        "document": "and get access to the augmented documentation experience\n\nü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:\n\nüìù Natural Language Processing: text classification, named entity recognition, question answering, language modeling, code generation, summarization, translation, multiple choice, and text generation.\n\n üñºÔ∏è Computer Vision: image classification, object detection, and segmentation.\n\n üó£Ô∏è Audio: automatic speech recognition and audio classification.\n\n üêô Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n\nü§ó Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model‚Äôs life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.\n\nJoin the growing community on the Hub, forum, or Discord today!\n\nIf you are looking for custom support from the Hugging Face team\n\nThe documentation is organized into five sections:\n‚Ä¢ None GET STARTED provides a quick tour of the library and installation instructions to get up and running.\n‚Ä¢ None TUTORIALS are a great place to start if you‚Äôre a beginner. This section will help you gain the basic skills you need to start using the library.\n‚Ä¢ None HOW-TO GUIDES show you how to achieve a specific goal, like finetuning a pretrained model for language modeling or how to write and share a custom model.\n‚Ä¢ None CONCEPTUAL GUIDES offers more discussion and explanation of the underlying concepts and ideas behind models, tasks, and the design philosophy of ü§ó Transformers.\n‚Ä¢ \n‚Ä¢ MAIN CLASSES details the most important classes like configuration, model, tokenizer, and pipeline.\n‚Ä¢ MODELS details the classes and functions related to each model implemented in the library.\n\nThe table below represents the current support in the library for each of those models, whether they have a Python tokenizer (called ‚Äúslow‚Äù). A ‚Äúfast‚Äù tokenizer backed by the ü§ó Tokenizers library, whether they have support in Jax (via Flax), PyTorch, and/or TensorFlow."
    },
    {
        "link": "https://huggingface.co/transformers/v3.0.2/model_doc/auto.html",
        "document": "In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the method.\n\nAutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary:\n\nInstantiating one of , and will directly create a class of the relevant architecture (ex: will create a instance of ).\n\nis a generic model class that will be instantiated as one of the question answering model classes of the library when created with the class method. The method takes care of returning the correct model class instance based on the property of the config object, or when it‚Äôs missing, falling back to using pattern matching on the string: This class cannot be instantiated using (throws an error). Instantiates one of the base model classes of the library from a configuration. Loading a model from its configuration file does not load the model weights. It only affects the model‚Äôs configuration. Use to load the model weights ( ) instance of a class derived from : The model class to instantiate is selected based on the configuration class: Instantiates one of the question answering model classes of the library from a pre-trained model configuration. The method takes care of returning the correct model class instance based on the property of the config object, or when it‚Äôs missing, falling back to using pattern matching on the string: The model is set in evaluation mode by default using (Dropout modules are deactivated) To train the model, you should first set it back in training mode with\n‚Ä¢ None a string with the of a pre-trained model to load from cache or download, e.g.: .\n‚Ä¢ None a string with the of a pre-trained model that was user-uploaded to our S3, e.g.: .\n‚Ä¢ None a path to a containing model weights saved using , e.g.: .\n‚Ä¢ None a path or url to a (e.g. ). In the case of a PyTorch checkpoint, should be set to True and a configuration object should be provided as argument. Set to True if the Checkpoint is a PyTorch checkpoint. All remaning positional arguments will be passed to the underlying model‚Äôs method Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n‚Ä¢ None the model is a model provided by the library (loaded with the string of a pretrained model), or\n‚Ä¢ None the model was saved using and is reloaded by suppling the save directory.\n‚Ä¢ None the model is loaded by suppling a local directory as and a configuration JSON file named is found in the directory. an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file. This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using and is not a simpler option. Path to a directory in which a downloaded pre-trained model configuration should be cached if the standard cache should not be used. Force to (re-)download the model weights and configuration files and override the cached versions if they exists. Do not delete incompletely recieved file. Attempt to resume the download if such a file exists. A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‚Äòhttp‚Äô: ‚Äòfoo.bar:3128‚Äô, ‚Äòhttp://hostname‚Äô: ‚Äòfoo.bar:4012‚Äô}. The proxies are used on each request. Set to to also return a dictionnary containing missing keys, unexpected keys and error messages. Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ). Behave differently depending on whether a is provided or automatically loaded:\n‚Ä¢ None If a configuration is provided with , will be directly passed to the underlying model‚Äôs method (we assume all relevant updates to the configuration have already been done)\n‚Ä¢ None If a configuration is not provided, will be first passed to the configuration class initialization function ( ). Each key of that corresponds to a configuration attribute will be used to override said attribute with the supplied value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model‚Äôs function. # Download model and configuration from S3 and cache. # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n\nInstantiates one of the base model classes of the library from a configuration. Loading a model from its configuration file does not load the model weights. It only affects the model‚Äôs configuration. Use to load the model weights ( ) instance of a class derived from : The model class to instantiate is selected based on the configuration class: Instantiates one of the question answering model classes of the library from a pre-trained model configuration. The method takes care of returning the correct model class instance based on the property of the config object, or when it‚Äôs missing, falling back to using pattern matching on the string: The model is set in evaluation mode by default using (Dropout modules are deactivated) To train the model, you should first set it back in training mode with\n‚Ä¢ None a string with the of a pre-trained model to load from cache or download, e.g.: .\n‚Ä¢ None a path to a containing model weights saved using , e.g.: .\n‚Ä¢ None a path or url to a (e.g. ). In this case, should be set to True and a configuration object should be provided as argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards. All remaning positional arguments will be passed to the underlying model‚Äôs method Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n‚Ä¢ None the model is a model provided by the library (loaded with the string of a pretrained model), or\n‚Ä¢ None the model was saved using and is reloaded by suppling the save directory.\n‚Ä¢ None the model is loaded by suppling a local directory as and a configuration JSON file named is found in the directory. an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file. This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using and is not a simpler option. Path to a directory in which a downloaded pre-trained model configuration should be cached if the standard cache should not be used. Force to (re-)download the model weights and configuration files and override the cached versions if they exists. A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‚Äòhttp‚Äô: ‚Äòfoo.bar:3128‚Äô, ‚Äòhttp://hostname‚Äô: ‚Äòfoo.bar:4012‚Äô}. The proxies are used on each request. Set to to also return a dictionnary containing missing keys, unexpected keys and error messages. Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ). Behave differently depending on whether a is provided or automatically loaded:\n‚Ä¢ None If a configuration is provided with , will be directly passed to the underlying model‚Äôs method (we assume all relevant updates to the configuration have already been done)\n‚Ä¢ None If a configuration is not provided, will be first passed to the configuration class initialization function ( ). Each key of that corresponds to a configuration attribute will be used to override said attribute with the supplied value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model‚Äôs function. # Download model and configuration from S3 and cache. # Loading from a TF checkpoint file instead of a PyTorch model (slower)"
    },
    {
        "link": "https://huggingface.co/docs",
        "document": ""
    },
    {
        "link": "https://huggingface.co/docs/transformers/v4.39.0/index",
        "document": "and get access to the augmented documentation experience\n\nü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:\n\nüìù Natural Language Processing: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.\n\n üñºÔ∏è Computer Vision: image classification, object detection, and segmentation.\n\n üó£Ô∏è Audio: automatic speech recognition and audio classification.\n\n üêô Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n\nü§ó Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model‚Äôs life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.\n\nJoin the growing community on the Hub, forum, or Discord today!\n\nIf you are looking for custom support from the Hugging Face team\n\nThe documentation is organized into five sections:\n‚Ä¢ None GET STARTED provides a quick tour of the library and installation instructions to get up and running.\n‚Ä¢ None TUTORIALS are a great place to start if you‚Äôre a beginner. This section will help you gain the basic skills you need to start using the library.\n‚Ä¢ None HOW-TO GUIDES show you how to achieve a specific goal, like finetuning a pretrained model for language modeling or how to write and share a custom model.\n‚Ä¢ None CONCEPTUAL GUIDES offers more discussion and explanation of the underlying concepts and ideas behind models, tasks, and the design philosophy of ü§ó Transformers.\n‚Ä¢ \n‚Ä¢ MAIN CLASSES details the most important classes like configuration, model, tokenizer, and pipeline.\n‚Ä¢ MODELS details the classes and functions related to each model implemented in the library.\n\nThe table below represents the current support in the library for each of those models, whether they have a Python tokenizer (called ‚Äúslow‚Äù). A ‚Äúfast‚Äù tokenizer backed by the ü§ó Tokenizers library, whether they have support in Jax (via Flax), PyTorch, and/or TensorFlow."
    },
    {
        "link": "https://medium.com/nerd-for-tech/all-you-need-to-know-comprehensive-faq-on-hugging-face-transformers-93b9268f59fa",
        "document": "Imagine you‚Äôre the sales director at an online store selling nutrition products. Each month, you‚Äôre swamped with various kinds of data: sales figures, trend graphs, customer service calls, market activity, new product launches, and updates on customer preferences. Your job is to sift through this mountain of data, pinpoint valuable insights, discard irrelevant details, and use this refined information to make strategic decisions. These decisions might include identifying the right products for certain customers, adjusting prices, planning marketing campaigns, or finding potential buyers for new products.\n\nTypically, handling this data would involve a lot of heavy lifting by IT staff. They‚Äôd need to carefully organize and store the data, then pull specific bits of information based on your needs. You or your data analysts would then create charts and tables to make sense of this raw data. But it‚Äôs important to remember that raw data isn‚Äôt the same as actionable insights. You need to analyze and interpret this information, tailoring your approach to meet various strategic objectives.\n\nThe workload for all this can be overwhelming. However, by using transformers, you can streamline much of this process‚Ä¶"
    },
    {
        "link": "https://stackoverflow.com/questions/77159136/efficiently-using-hugging-face-transformers-pipelines-on-gpu-with-large-datasets",
        "document": "I think you can ignore this message. I found it being reported on different websites this year, but if I get it correctly, this Github issue on the Huggingface transformers (https://github.com/huggingface/transformers/issues/22387) shows that the warning can be safely ignored. In addition, batching or using might not remove the warning or automatically use the resources in the best way. You can do in here (https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/base.py#L1100) to ignore the warning, as explained by Martin Weyssow above.\n\nHow can I modify my code to batch my data and use parallel computing to make better use of my GPU resources:\n\nYou can add batching like this:\n\nand most importantly, you can experiment with the batch size that will result to the highest GPU usage possible on your device and particular task.\n\nHuggingface provides here some rules to help users figure out how to batch: https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching. Making the best resource/GPU usage possible might take some experimentation and it depends on the use case you work on every time.\n\nWhat does this warning mean, and why should I use a dataset for efficiency?\n\nThis means the GPU utilization is not optimal, because the data is not grouped together and it is thus not processed efficiently. Using a dataset from the Huggingface library will utilize your resources more efficiently. However, it is not so easy to tell what exactly is going on, especially considering that we don‚Äôt know exactly how the data looks like, what the device is and how the model deals with the data internally. The warning might go away by using the library, but that does not necessarily mean that the resources are optimally used.\n\nWhat code or function or library should be used with hugging face transformers?\n\nHere is a code example with and the library: https://huggingface.co/docs/transformers/v4.27.1/pipeline_tutorial#using-pipelines-on-a-dataset. It mentions that using iterables will fill your GPU as fast as possible and batching might also help with computational time improvements.\n\nIn your case it seems you are doing a relatively small POC (doing inference for under 10,000 documents with a medium size model), so I don‚Äôt think you need to use pipelines. I assume the sentiment analysis model is a classifier and you want to keep using as shown in the post, so here is how you can combine both. This is usually fast enough for my experiments and prints no warnings about the resources.\n\nAs soon as your inference starts, either with this snippet or with the library code, you can run in a terminal and check what the GPU usage is and play around with the parameters to optimize it. Beware that running the code on your local machine with a GPU vs running it on a larger machine, e.g., a Linux server with perhaps a more powerful GPU might lead to different performance and might need different tuning. If you wish to run the code for larger document collections, you can split the data in order to avoid GPU memory errors locally, or in order to speed up the inference with concurrent runs in a server."
    },
    {
        "link": "https://huggingface.co/docs/transformers/en/main_classes/pipelines",
        "document": "and get access to the augmented documentation experience\n\nThe pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the task summary for examples of use.\n\nThere are two categories of pipeline abstractions to be aware about:\n‚Ä¢ The pipeline() which is the most powerful object encapsulating all other pipelines.\n‚Ä¢ Task-specific pipelines are available for audio, computer vision, natural language processing, and multimodal tasks.\n\nThe pipeline abstraction is a wrapper around all the other available pipelines. It is instantiated as any other pipeline but can provide additional quality of life.\n\nIf you want to use a specific model from the hub you can ignore the task if the model on the hub already defines it:\n\nTo call a pipeline on many items, you can call it with a list.\n\nTo iterate over full datasets it is recommended to use a directly. This means you don‚Äôt need to allocate the whole dataset at once, nor do you need to do batching yourself. This should work just as fast as custom loops on GPU. If it doesn‚Äôt don‚Äôt hesitate to create an issue.\n\nFor ease of use, a generator is also possible:\n\nAll pipelines can use batching. This will work whenever the pipeline uses its streaming ability (so when passing lists or or ).\n\nExample where it‚Äôs most a slowdown:\n\nThis is a occasional very long sentence compared to the other. In that case, the whole batch will need to be 400 tokens long, so the whole batch will be [64, 400] instead of [64, 4], leading to the high slowdown. Even worse, on bigger batches, the program simply crashes.\n\nThere are no good (general) solutions for this problem, and your mileage may vary depending on your use cases. Rule of thumb:\n\nFor users, a rule of thumb is:\n‚Ä¢ None Measure performance on your load, with your hardware. Measure, measure, and keep measuring. Real numbers are the only way to go.\n‚Ä¢ None If you are latency constrained (live product doing inference), don‚Äôt batch.\n‚Ä¢ None If you are using CPU, don‚Äôt batch.\n‚Ä¢ None If you are using throughput (you want to run your model on a bunch of static data), on GPU, then:\n‚Ä¢ If you have no clue about the size of the sequence_length (‚Äúnatural‚Äù data), by default don‚Äôt batch, measure and try tentatively to add it, add OOM checks to recover when it will fail (and it will at some point if you don‚Äôt control the sequence_length.)\n‚Ä¢ If your sequence_length is super regular, then batching is more likely to be VERY interesting, measure and push it until you get OOMs.\n‚Ä¢ The larger the GPU the more likely batching is going to be more interesting\n‚Ä¢ None As soon as you enable batching, make sure you can handle OOMs nicely.\n\nand are slightly specific in the sense, that a single input might yield multiple forward pass of a model. Under normal circumstances, this would yield issues with argument.\n\nIn order to circumvent this issue, both of these pipelines are a bit specific, they are instead of regular . In short:\n\nThis should be very transparent to your code because the pipelines are used in the same way.\n\nThis is a simplified view, since the pipeline can handle automatically the batch to ! Meaning you don‚Äôt have to care about how many forward passes you inputs are actually going to trigger, you can optimize the independently of the inputs. The caveats from the previous section still apply.\n\nModels can be run in FP16 which can be significantly faster on GPU while saving memory. Most models will not suffer noticeable performance loss from this. The larger the model, the less likely that it will.\n\nTo enable FP16 inference, you can simply pass or to the pipeline constructor. Note that this only works for models with a PyTorch backend. Your inputs will be converted to FP16 internally.\n\nIf you want to override a specific pipeline.\n\nDon‚Äôt hesitate to create an issue for your task at hand, the goal of the pipeline is to be easy to use and support most cases, so could maybe support your use case.\n\nIf you want to try simply you can:\n\nThat should enable you to do all the custom code you want.\n\nPipelines available for audio tasks include the following.\n\nPipelines available for computer vision tasks include the following.\n\nPipelines available for natural language processing tasks include the following.\n\nPipelines available for multimodal tasks include the following."
    },
    {
        "link": "https://stackoverflow.com/questions/79131911/how-to-replace-the-deprecated-conversational-pipeline-in-hugging-face-transfor",
        "document": "I'm working on a chatbot using Hugging Face's Transformers library. I was previously using the conversational pipeline, but it seems that it has been deprecated. Here is the code I was using:\n\nNow that the conversational pipeline is deprecated, I'm unsure how to achieve the same functionality. I've read that I might need to use the text-generation pipeline or the model's generate method directly, but I'm not sure how to implement this with my existing chat history structure.\n\nSo, What is the recommended way to create a conversational chatbot using the latest version of Transformers?\n\nAny guidance or code examples would be greatly appreciated!"
    },
    {
        "link": "https://realpython.com/huggingface-transformers",
        "document": "Transformers is a powerful Python library created by Hugging Face that allows you to download, manipulate, and run thousands of pretrained, open-source AI models. These models cover multiple tasks across modalities like natural language processing, computer vision, audio, and multimodal learning. Using pretrained open-source models can reduce costs, save the time needed to train models from scratch, and give you more control over the models you deploy.\n\nIn this tutorial, you‚Äôll learn how to:\n\nThroughout this tutorial, you‚Äôll gain a conceptual understanding of Hugging Face‚Äôs AI offerings and learn how to work with the Transformers library through hands-on examples. When you finish, you‚Äôll have the knowledge and tools you need to start using models for your own use cases. Before starting, you‚Äôll benefit from having an intermediate understanding of Python and popular deep learning libraries like and .\n\nBefore using Transformers, you‚Äôll want to have a solid understanding of the Hugging Face ecosystem. In this first section, you‚Äôll briefly explore everything that Hugging Face offers with a particular emphasis on model cards. Hugging Face is a hub for state-of-the-art AI models. It‚Äôs primarily known for its wide range of open-source transformer-based models that excel in natural language processing (NLP), computer vision, and audio tasks. The platform offers several resources and services that cater to developers, researchers, businesses, and anyone interested in exploring AI models for their own use cases. There‚Äôs a lot you can do with Hugging Face, but the primary offerings can be broken down into a few categories:\n‚Ä¢ Models: Hugging Face hosts a vast repository of pretrained AI models that are readily accessible and highly customizable. This repository is called the Model Hub, and it hosts models covering a wide range of tasks, including text classification, text generation, translation, summarization, speech recognition, image classification, and more. The platform is community-driven and allows users to contribute their own models, which facilitates a diverse and ever-growing selection.\n‚Ä¢ Datasets: Hugging Face has a library of thousands of datasets that you can use to train, benchmark, and enhance your models. These range from small-scale benchmarks to massive, real-world datasets that encompass a variety of domains, such as text, image, and audio data. Like the Model Hub, ü§ó Datasets supports community contributions and provides the tools you need to search, download, and use data in your machine learning projects.\n‚Ä¢ Spaces: Spaces allows you to deploy and share machine learning applications directly on the Hugging Face website. This service supports a variety of frameworks and interfaces, including Streamlit, Gradio, and Jupyter notebooks. It is particularly useful for showcasing model capabilities, hosting interactive demos, or for educational purposes, as it allows you to interact with models in real time.\n‚Ä¢ Paid offerings: Hugging Face also offers several paid services for enterprises and advanced users. These include the Pro Account, the Enterprise Hub, and Inference Endpoints. These solutions offer private model hosting, advanced collaboration tools, and dedicated support to help organizations scale their AI operations effectively. These resources empower you to accelerate your AI projects and encourage collaboration and innovation within the community. Whether you‚Äôre a novice looking to experiment with pretrained models, or an enterprise seeking robust AI solutions, Hugging Face offers tools and platforms that cater to a wide range of needs. This tutorial focuses on Transformers, a Python library that lets you run just about any model in the Model Hub. Before using , you‚Äôll need to understand what model cards are, and that‚Äôs what you‚Äôll do next. Model cards are the core components of the Model Hub, and you‚Äôll need to understand how to search and read them to use models in Transformers. Model cards are nothing more than files that accompany each model to provide useful information. You can search for the model card you‚Äôre looking for on the Models page: On the left side of the Models page, you can search for model cards based on the task you‚Äôre interested in. For example, if you‚Äôre interested in zero-shot text classification, you can click the button under the section: In this search, you can see 266 different zero-shot text classification models, which is a paradigm where language models assign labels to text without explicit training or seeing any examples. In the upper-right corner, you can sort the search results based on model likes, downloads, creation dates, updated dates, and popularity trends. Each model card button tells you the model‚Äôs task, when it was last updated, and how many downloads and likes it has. When you click a model card button, say the one for the model, the model card will open and display all of the model‚Äôs information: Even though a model card can display just about anything, Hugging Face has outlined the information that a good model card should provide. This includes detailed information about the model, its uses and limitations, the training parameters and experiment details, the dataset used to train the model, and the model‚Äôs evaluation performance. A high-quality model card also includes metadata such as the model‚Äôs license, references to the training data, and links to research papers that describe the model in detail. In some model cards, you‚Äôll also get to tinker with a deployed instance of the model via the Inference API. You can see an example of this in the model card: Tinker with Hugging Face models using the Inference API You pass a block of text along with the class names you want to categorize the text into. You then click , and the model assigns a score between 0 and 1 to each class. The numbers represent how likely the model thinks the text belongs to the corresponding class. In this example, the model assigns high scores to the classes and . This makes sense because the input text describes an urgent phone issue. To determine whether a model card is appropriate for your use case, you can review the information within the model card, including the metadata and Inference API features. These are great resources to help you familiarize yourself with the model and determine it‚Äôs suitability. And with that primer on Hugging Face and model cards, you‚Äôre ready to start running these models in Transformers.\n\nHugging Face‚Äôs Transformers library provides you with APIs and tools you can use to download, run, and train state-of-the-art open-source AI models. Transformers supports the majority of models available in Hugging Face‚Äôs Model Hub, and encompasses diverse tasks in natural language processing, computer vision, and audio processing. Because it‚Äôs built on top of PyTorch, TensorFlow, and JAX, Transformers gives you the flexibility to use these frameworks to run and customize models at any stage. Using open-source models through Transformers has several advantages:\n‚Ä¢ Cost reduction: Proprietary AI companies like OpenAI, Cohere, and Anthropic often charge you a token fee to use their models via an API. This means you pay for every token that goes in and out of the model, and your API costs can add up quickly. By deploying your own instance of a model with Transformers, you can significantly reduce your costs because you only pay for the infrastructure that hosts the model.\n‚Ä¢ Data security: When you build applications that process sensitive data, it‚Äôs a good idea to keep the data within your enterprise rather than send it to a third party. While closed-source AI providers often have data privacy agreements, anytime sensitive data leaves your ecosystem, you risk that data ending up in the wrong person‚Äôs hands. Deploying a model with Transformers within your enterprise gives you more control over data security.\n‚Ä¢ Time and resource savings: Because Transformers models are pretrained, you don‚Äôt have to spend the time and resources required to train an AI model from scratch. Moreover, it usually only takes a few lines of code to run a model with Transformers, which saves you the time it takes to write model code from scratch. Overall, Transformers is a fantastic resource that enables you to run a suite of powerful open-source AI models efficiently. In the next section, you‚Äôll get hands-on experience with the library and see how straightforward it is to run and customize models. Transformers is available on PyPI and you can install it with pip. Open a terminal or command prompt, create a new virtual environment, and then run the following command: This command will install the latest version of Transformers from PyPI onto your machine. You‚Äôll also leverage PyTorch to interact with models at a lower level. Note: Installing PyTorch can take a considerable amount of time. It typically requires downloading several hundred megabytes of dependencies unless they‚Äôre already cached or included with your Python distribution. You can install PyTorch with the following command: To verify that the installations were successful, start a Python REPL and import and : If the imports run without errors, then you‚Äôve successfully installed the dependencies needed for this tutorial, and you‚Äôre ready to get started with pipelines! Pipelines are the simplest way to use models out of the box in Transformers. In particular, the function offers you a high-level abstraction over models in the Hugging Face Model Hub. To see how this works, suppose you want to use a sentiment classification model. Sentiment classification models take in text as input and output a score that indicates the likelihood that the text has negative, neutral, or positive sentiment. One popular sentiment classification model available in the hub is the model. Note: Just about every machine learning classifier outputs scores often referred to as ‚Äúlikelihoods‚Äù or ‚Äúprobabilities‚Äù. Keep in mind that ‚Äúlikelihood‚Äù and ‚Äúprobability‚Äù are mathematical terms that have similar but different definitions. Classifier outputs aren‚Äôt true likelihoods or probabilities according to these definitions. All you need to remember is that the closer a score is to 1, the more confident the model is that the input belongs to the corresponding class. Similarly, the closer a score is to 0, the more confident the model is that the input doesn‚Äôt belong to the corresponding class. You can run this model with the following code: \"I'm really excited about using Hugging Face to run AI models!\" \"Most of the Earth is covered in water.\" In this block, you import and load the model by specifying the parameter in . When you do this, returns a callable object, stored as , that you can use to classify text. Once created, accepts text as input, and it outputs a sentiment label and score that indicates how likely the text belongs to the label. Note: As you‚Äôll see in a moment, every model you download might require different parameters. In this first example, the only required input is the text you want to classify, but other models can require more inputs to make a prediction. Be sure to check out the model card if you‚Äôre not sure how to use for a particular model. The model scores range from 0 to 1. In the first example, predicts that the text has positive sentiment with high confidence. In the second and third examples, predicts the texts are negative and neutral, respectively. If you want to classify multiple texts in one function call, you can pass a list into : \"What a great time to be alive!\" \"How are you doing today?\" Here, you create a list of texts called and pass it into . The model wrapped by returns a label and score for each line of text in the order specified by . You can see that the model has done a nice job of classifying the sentiment for each line of text! While every model in the hub has a slightly different interface, is flexible enough to handle all of them. For example, a step up in complexity from sentiment classification is zero-shot text classification. Instead of classifying text as , , or , zero-shot text classification models can classify text into arbitrary categories. Here‚Äôs how you could instantiate a zero-shot text classifier with : In this example, you first load the zero-shot text classification model into an object called . You then define and , which are required for to make predictions. The values in tell the model which categories the text can be classified into, and tells the model how to compare the candidate labels to the text input. In this case, tells the model that it should try to figure out which of the candidate labels the input text is most likely about. You can use like this: \"My account was charged twice for a single order.\" {'sequence': 'My account was charged twice for a single order.', Here, you define and pass it into along with and . By setting to , you allow the model to classify the text into multiple categories instead of just one. This means each label can receive a score between 0 and 1 that‚Äôs independent of the other labels. When is , the model scores sum to 1, which means the text can only belong to one label. In this example, the model assigned a score of about 0.98 to , 0.0125 to , 0.008 to , and 0.0002 to . From this, you can see that the model believes is most likely about , and this checks out! To further demonstrate the power of pipelines, you‚Äôll use to classify an image. Image classification is a sub-task of computer vision where a model predicts the likelihood that an image belongs to a specified class. Similar to NLP, image classifiers in the Model Hub can be pretrained on a specific set of labels or they can be trained for zero-shot classification. In order to use image classifiers from Transformers, you must install Python‚Äôs image processing library, Pillow: After installing Pillow, you should be able to instantiate the default image classification model like this: No model was supplied, defaulted to google/vit-base-patch16-224 Notice here that you don‚Äôt pass the argument into . Instead, you specify the as , and returns the model by default. This model is pretrained on a fixed set of labels, so you can specify the labels as you do with zero-shot classification. Now, suppose you want to use to classify the following image of llamas, which you can download from the materials for this tutorial: There are a few ways to pass images into , but the most straightforward approach is to pass the image path into the pipeline. Ensure the image is in the same directory as your Python process, and run the following: Here, you pass the path into and store the results as . The model returns the five most likely labels. You then look at the first class prediction, , which is the class the model thinks the image most likely belongs to. The model predicts that the image should be labeled as with a score of about 0.99. The next two most likely labels are and , but the scores for these labels are very low. It‚Äôs pretty amazing how confident the model is at predicting on an image it has never seen before! The most important takeaway is how straightforward it is to use models out of the box with . All you do is pass raw inputs like text or images into pipelines, along with the minimum amount of additional input the model needs to run, such as the hypothesis template or candidate labels. The pipeline handles the rest for you. While pipelines are great for getting started with models, you might find yourself needing more control over the internal details of a model. In the next section, you‚Äôll learn how to break out pipelines into their individual components with auto classes. Looking Under the Hood With Auto Classes As you‚Äôve seen so far, pipelines make it easy to use models out of the box. However, you may want to further customize models through techniques like fine-tuning. Fine-tuning is a technique that adapts a pretrained model to a specific task with potentially different but related data. For example, you could take an existing image classifier in the Model Hub and further train it to classify images that are proprietary to your company. For customization tasks like fine-tuning, Transformers allows you to access the lower-level components that make up pipelines via auto classes. This section won‚Äôt go over fine-tuning or other customizations specifically, but you‚Äôll get a deeper understanding of how pipelines work under the hood by looking at their auto classes. Suppose you want more granular access and understanding of the sentiment classifier pipeline you saw in the previous section. The first component of this pipeline, and almost every NLP pipeline, is the tokenizer. Tokens can be words, subwords, or even characters, depending on the design of the tokenizer. A tokenizer is a component that processes input text and converts it into a format that the model can understand. It does this by breaking the text into tokens and associating those tokens with an ID. You can access tokenizers using the class. To see how this works, take a look at this example: \"I really want to go to an island. Do you want to go?\" In this block, you first import the class from Transformers. You then instantiate and store the tokenizer for the model using the class method. Lastly, you pass some into the tokenizer and look at the IDs it associates with each token. Each integer in is the ID of a token within the tokenizer‚Äôs vocabulary. For example, you can already tell that ID corresponds to the token because it‚Äôs repeated multiple times. This might seem a bit cryptic at first, but we can better understand this by using to convert the IDs back to tokens: With , we see that ID and ID convert to tokens and , respectively. The prefix is a special symbol used to denote the beginning of a new word in contexts where whitespace is used as a separator. By passing into , you recover the original text input with the additional tokens and , which denote the beginning and end of the text. You can see how many tokens are in the tokenizer‚Äôs vocabulary by looking at the attribute: This particular tokenizer has 50,265 tokens. If you wanted to fine-tune this model, and there were new tokens in your training data, you‚Äôd have to add them to the tokenizer with : You first define a list called , which has two tokens that aren‚Äôt in the tokenizer‚Äôs vocabulary by default. When you call , both of the new tokens map to ID . Token ID corresponds to , which is the default token for input tokens that aren‚Äôt in the vocabulary. When you pass into , the tokens are added to the vocabulary and assigned new IDs of and . You can also use auto classes to access the model object. For the model, you can load the model object directly using : Here, you add to your imports and instantiate the model object for using . When you call in the console, you can see the full string representation of the model. The Roberta model consists of a series of layers that you can access and modify directly. As an example, take a look at the layer: It‚Äôs out of the scope of this tutorial to look at all of the intricacies of the Roberta model, but pay close attention to the layer here. You may have noticed that the first input to in the layer is ‚Äîthe exact size of the tokenizer‚Äôs vocabulary. This is because the first embedding layer maps each token in the vocabulary to a PyTorch tensor of size 768. In other words, maps all 50,265 tokens in the vocabulary to a PyTorch tensor with 768 elements. To get a better understanding of how this works, you can convert input text to embeddings directly using the layer: In this block, you define and convert each token to its corresponding ID using . You then pass the token IDs into Roberta‚Äôs embeddings layer and store the results as . Notice how the size of is . This is because you passed one text input into the embedding layer that had nine tokens in it, and each token was converted to a tensor with 768 elements. When you look at the string representation, the first row is the embedding for the token, the second row is for the token, the third for the token, and so on. If you wanted to fine-tune the Roberta model with new tokens, you‚Äôd first add the new tokens to the tokenizer as you did previously, and then you‚Äôd have to update and train the embeddings layer with a 768-element tensor for each new token. In the full model, the embedding tensor is passed through multiple layers where it‚Äôs reshaped, manipulated, and eventually converted to a predicted score for each sentiment class. You can piece together these auto classes to create the entire pipeline: Here, you first import along with the auto classes you saw previously. Additionally, you import , which has configuration and metadata for the model. You then store the pipeline name in and instantiate the configuration object, tokenizer, and model object. Next, you define and tokenize it with . You then pass the tokenized input, , to the model object and store the results as . You use the context manager to speed up model inference by disabling gradient calculations. After that, you convert the raw model output to scores and then transform the scores to sum to using . Lastly, you loop through each element in and output the value along with the associated label, which comes from . The results tell you that the model assigns a predicted probability of about to the class for the input text. You can verify that this code gives the same results as the pipeline you used in the earlier example: In this block, you run the same through the full pipeline and get the exact same predicted score for the label. You now understand how pipelines work under the hood and how you can access and manipulate pipeline components with auto classes. This gives you the tools to create custom pipelines through techniques like fine-tuning, and a deeper understanding of the underlying model. In the next section, you‚Äôll shift gears and learn how to improve pipeline performance by leveraging GPUs.\n\nNearly every model submitted to Hugging Face is a neural network and, more specifically, a transformer. These neural networks comprise multiple layers with millions, billions, and sometimes even trillions of parameters. For example, the model you used in the first section has 435 million parameters, and this is a relatively small language model. The core computation of any neural network is matrix multiplication, and performing matrix multiplication over multiple millions of parameters can be computationally expensive. Because of this, training and inference for most large neural networks is done on graphics processing units (GPUs). GPUs are specialized hardware that can significantly speed up the training and inference time of neural networks compared to the CPU. This is because GPUs have thousands of small and efficient cores designed to process multiple tasks simultaneously, while CPUs have fewer cores optimized for sequential serial processing. This makes GPUs especially powerful for the matrix and vector operations required in neural networks. While all of the models available in Transformers can run on CPUs, like the ones you saw in the previous section, most of them were likely trained and are optimized to run on GPUs. In the next section, you‚Äôll see just how simple Transformers makes it for you to run pipelines on GPUs. This will dramatically improve the performance of your pipelines and allow you to make predictions with lightning speed. You might not have access to a GPU on your local machine, so in this section, you‚Äôll use Google Colab, a Python notebook interface offered by Google for running pipelines on GPUs for free! To get started, sign into Google Colab and create a new notebook. Once created, your notebook should look something like this: Next, click the dropdown next to the button and click : Select the GPU available to you, and click . This will connect your notebook to a machine in the cloud with a GPU. Note that if you‚Äôre using the free version of Google Colab, GPU availability can vary. This means that if Google Colab receives a high volume of users requesting GPU access, you might have to wait to get access. Next, you‚Äôll need to upload the and files from this tutorial‚Äôs materials to your notebook session. To do this, right-click under the folder tab and choose : Once uploaded, you should see the two files under the folder tab: Keep in mind that these files are temporary, and they‚Äôll be deleted whenever your notebook session terminates. If you want to persist files, you can upload them to your Google Drive account and mount it to your notebook. Lastly, you need to install the libraries specified in the file. You can do this directly from a notebook cell: Press + , and your requirements should install. Although Google Colab caches popular packages to speed up subsequent installations, you may need to wait a few moments for the installation to finish. Once that completes, you‚Äôre ready to start running pipelines on the GPU! Now that you have a running notebook with access to a GPU, Transformers makes it easy for you to run pipelines on the GPU. In this section, you‚Äôll run car reviews from the file through a sentiment classification pipeline on both the CPU and GPU, and you‚Äôll see how much faster the pipeline runs on the GPU. If you‚Äôre interested, you can read more about the car reviews dataset on Kaggle. To start, define the path to the data and import your dependencies in a new cell: Don‚Äôt worry if you‚Äôre not familiar with some of these dependencies‚Äîyou‚Äôll see how they‚Äôre used in a moment. Next, you can use the Polars library to read in the car reviews and store them in a list: Here, you read in the column from and store it in . You then look at the length of and see there are 8,499 reviews. Here‚Äôs what the first review says: \" It's been a great delivery vehicle for my care of. Havent repaired anything or replaced anything You‚Äôre going to use the sentiment classifier to predict the sentiment of these reviews using both a CPU and GPU, and you‚Äôll time how long it takes for both. To help you with these experiments, define the following helper function: to run inference on a list of texts\"\"\" The goal of is to evaluate how long it takes a to make predictions on a list of . It first converts the texts to a generator called , and passes the generator to the text classification pipeline. This turns the pipeline into an iterable that can be looped over to get predictions. The determines how many predictions the model makes in one pass. You wrap the text classification pipeline iterator with to see your pipeline‚Äôs progress as it makes predictions. Lastly, you iterate over and use the statement since you‚Äôre only interested in how long it takes to run. After all predictions are made, displays the total runtime. Next, you‚Äôll instantiate two pipelines, one that runs on the CPU and the other on the GPU: Transformers makes it easy for you to specify what hardware your pipeline runs on with the argument. When you pass an integer to the argument, you‚Äôre telling the pipeline which GPU it should run on. When is , the pipeline runs on the CPU, and any non-negative number tells the pipeline which GPU to use based on its ordinal rank. In this case, you likely only have one GPU, so setting tells the pipeline to run on the first and only GPU. If you had a second GPU, you could set , and so on. You‚Äôre now ready to time these two pipelines starting with the CPU: Here, you time how long it takes to make predictions on the first 1000 reviews. The results show it took about 5 minutes and 49 seconds. This means made roughly 2.86 predictions per second. Now you can run the same experiment for : Woah! On the exact same 1000 reviews, took about 16 seconds to make all the predictions! That‚Äôs nearly 61 predictions per second and roughly 21 times faster than . Keep in mind that the exact run times will vary, but you can run this experiment multiple times to gauge the average time. You can further optimize the pipeline‚Äôs performance by experimenting with , which is a parameter that determines how many inputs the model processes at one time. For example, if the is , then the model will make predictions of four inputs simultaneously. Check out the performance of on different batch sizes: Here, you iterate over a list of batch sizes and see how long it takes the pipeline to run through all 8,499 reviews on the corresponding batch size. From the output, you can see a batch size of resulted in the best performance at about 91 predictions per second. In general, deciding on an optimal batch size for inference requires experimentation. You should run experiments like this multiple times on different datasets to see which batch size is best on average. You now know how to run and evaluate pipelines on GPUs! Keep in mind that while most models available in Transformers run best on GPUs, it‚Äôs not always feasible to do so. In practice, GPUs can be expensive and resource-intensive, so you have to decide whether the performance gain is necessary and worth the cost for your application. Always experiment and make sure you have a solid understanding of the hardware you want to use."
    },
    {
        "link": "https://huggingface.co/docs/transformers/en/index",
        "document": "and get access to the augmented documentation experience\n\nü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:\n\nüìù Natural Language Processing: text classification, named entity recognition, question answering, language modeling, code generation, summarization, translation, multiple choice, and text generation.\n\n üñºÔ∏è Computer Vision: image classification, object detection, and segmentation.\n\n üó£Ô∏è Audio: automatic speech recognition and audio classification.\n\n üêô Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n\nü§ó Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model‚Äôs life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.\n\nJoin the growing community on the Hub, forum, or Discord today!\n\nIf you are looking for custom support from the Hugging Face team\n\nThe documentation is organized into five sections:\n‚Ä¢ None GET STARTED provides a quick tour of the library and installation instructions to get up and running.\n‚Ä¢ None TUTORIALS are a great place to start if you‚Äôre a beginner. This section will help you gain the basic skills you need to start using the library.\n‚Ä¢ None HOW-TO GUIDES show you how to achieve a specific goal, like finetuning a pretrained model for language modeling or how to write and share a custom model.\n‚Ä¢ None CONCEPTUAL GUIDES offers more discussion and explanation of the underlying concepts and ideas behind models, tasks, and the design philosophy of ü§ó Transformers.\n‚Ä¢ \n‚Ä¢ MAIN CLASSES details the most important classes like configuration, model, tokenizer, and pipeline.\n‚Ä¢ MODELS details the classes and functions related to each model implemented in the library.\n\nThe table below represents the current support in the library for each of those models, whether they have a Python tokenizer (called ‚Äúslow‚Äù). A ‚Äúfast‚Äù tokenizer backed by the ü§ó Tokenizers library, whether they have support in Jax (via Flax), PyTorch, and/or TensorFlow."
    }
]