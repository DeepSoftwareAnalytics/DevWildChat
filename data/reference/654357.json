[
    {
        "link": "https://docs.gitlab.com/ci/docker/using_docker_build",
        "document": "You can use GitLab CI/CD with Docker to create Docker images. For example, you can create a Docker image of your application, test it, and push it to a container registry.\n\nTo run Docker commands in your CI/CD jobs, you must configure GitLab Runner to support commands. This method requires mode.\n\nIf you want to build Docker images without enabling mode on the runner, you can use a Docker alternative.\n\nTo enable Docker commands for your CI/CD jobs, you can use:\n\nTo include Docker commands in your CI/CD jobs, you can configure your runner to use the executor. In this configuration, the user runs the Docker commands, but needs permission to do so.\n• None Register a runner. Select the executor. For example:\n• None On the server where GitLab Runner is installed, install Docker Engine. View a list of supported platforms.\n• None Add the user to the group:\n• None Verify that has access to Docker:\n• None In GitLab, add to to verify that Docker is working:\n\nYou can now use commands (and install Docker Compose if needed).\n\nWhen you add to the group, you effectively grant full root permissions. For more information, see security of the group.\n• Your registered runner uses the Docker executor or the Kubernetes executor.\n• The executor uses a container image of Docker, provided by Docker, to run your CI/CD jobs.\n\nThe Docker image includes all of the tools and can run the job script in context of the image in privileged mode.\n\nYou should use Docker-in-Docker with TLS enabled, which is supported by GitLab.com instance runners.\n\nYou should always pin a specific version of the image, like . If you use a tag like , you have no control over which version is used. This can cause incompatibility problems when new versions are released.\n\nUse the Docker executor with Docker-in-Docker\n\nYou can use the Docker executor to run jobs in a Docker container.\n\nDocker-in-Docker with TLS enabled in the Docker executor\n\nThe Docker daemon supports connections over TLS. TLS is the default in Docker 19.03.12 and later.\n\nTo use Docker-in-Docker with TLS enabled:\n• None Register GitLab Runner from the command line. Use and mode:\n• This command registers a new runner to use the image (if none is specified at the job level). To start the build and service containers, it uses the mode. If you want to use Docker-in-Docker, you must always use in your Docker containers.\n• This command mounts for the service and build container, which is needed for the Docker client to use the certificates in that directory. For more information, see the Docker image documentation. The previous command creates a entry similar to the following example:\n• None You can now use in the job script. You should include the service: # When you use the dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # by setting the DOCKER_HOST in # The 'docker' hostname is the alias of the service container as described at # Specify to Docker where to create the certificates. Docker # creates them automatically on boot, and creates # `/certs/client` to share between the service and job # container, thanks to volume mount from config.toml\n\nUse a Unix socket on a shared volume between Docker-in-Docker and build container\n\nDirectories defined in in the Docker-in-Docker with TLS enabled in the Docker executor approach are persistent between builds. If multiple CI/CD jobs using a Docker executor runner have Docker-in-Docker services enabled, then each job writes to the directory path. This approach might result in a conflict.\n\nTo address this conflict, use a Unix socket on a volume shared between the Docker-in-Docker service and the build container. This approach improves performance and establishes a secure connection between the service and client.\n\nThe following is a sample with temporary volume shared between build and service containers:\n\nThe Docker-in-Docker service creates a . The Docker client connects to through a Docker Unix socket volume.\n\nDocker-in-Docker with TLS disabled in the Docker executor\n\nSometimes there are legitimate reasons to disable TLS. For example, you have no control over the GitLab Runner configuration that you are using.\n\nAssuming that the runner’s is similar to:\n\nYou can now use in the job script. You should include the service:\n\nDocker-in-Docker with proxy enabled in the Docker executor\n\nYou might need to configure proxy settings to use the command.\n\nFor more information, see Proxy settings when using dind service.\n\nUse the Kubernetes executor with Docker-in-Docker\n\nYou can use the Kubernetes executor to run jobs in a Docker container.\n\nTo use Docker-in-Docker with TLS enabled in Kubernetes:\n• None Using the Helm chart, update the file to specify a volume mount.\n• None You can now use in the job script. You should include the service: # When using dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # The 'docker' hostname is the alias of the service container as described at # If you're using GitLab Runner 12.7 or earlier with the Kubernetes executor and Kubernetes 1.6 or earlier, # the variable must be set to tcp://localhost:2376 because of how the # Specify to Docker where to create the certificates. Docker # creates them automatically on boot, and creates # `/certs/client` to share between the service and job # container, thanks to volume mount from config.toml # These are usually specified by the entrypoint, however the\n\nTo use Docker-in-Docker with TLS disabled in Kubernetes, you must adapt the example above to:\n• Remove the section from the file.\n• Change the port from to with .\n• Instruct Docker to start with TLS disabled with .\n• None Using the Helm chart, update the file:\n• None You can now use in the job script. You should include the service: # When using dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # The 'docker' hostname is the alias of the service container as described at # If you're using GitLab Runner 12.7 or earlier with the Kubernetes executor and Kubernetes 1.6 or earlier, # the variable must be set to tcp://localhost:2376 because of how the # This instructs Docker not to start over TLS.\n\nDocker-in-Docker is the recommended configuration, but you should be aware of the following issues:\n• None The command: This command is not available in this configuration by default. To use in your job scripts, follow the Docker Compose installation instructions.\n• None Cache: Each job runs in a new environment. Because every build gets its own instance of the Docker engine, concurrent jobs do not cause conflicts. However, jobs can be slower because there’s no caching of layers. See Docker layer caching.\n• None Storage drivers: By default, earlier versions of Docker use the storage driver, which copies the file system for each job. Docker 17.09 and later use , which is the recommended storage driver. See Using the OverlayFS driver for details.\n• None Root file system: Because the container and the runner container do not share their root file system, you can use the job’s working directory as a mount point for child containers. For example, if you have files you want to share with a child container, you could create a subdirectory under and use it as your mount point. For a more detailed explanation, see issue #41227.\n\nUse the Docker executor with Docker socket binding\n\nTo use Docker commands in your CI/CD jobs, you can bind-mount into the container. Docker is then available in the context of the image.\n\nIf you bind the Docker socket you can’t use as a service. Volume bindings also affect services, making them incompatible.\n\nTo make Docker available in the context of the image, you need to mount into the launched containers. To do this with the Docker executor, add to the Volumes in the section.\n\nYour configuration should look similar to this example:\n\nTo mount while registering your runner, include the following options:\n\nFor the executor, use a configuration similar to this example:\n\nFor complex Docker-in-Docker setups like Code Quality scanning using CodeClimate, you must match host and container paths for proper execution. For more details, see Use private runners for CodeClimate-based scanning.\n\nWhen the Docker daemon starts inside the service container, it uses the default configuration. You might want to configure a registry mirror for performance improvements and to ensure you do not exceed Docker Hub rate limits.\n\nThe service in the file\n\nYou can append extra CLI flags to the service to set the registry mirror:\n\nThe service in the GitLab Runner configuration file\n\nIf you are a GitLab Runner administrator, you can specify the to configure the registry mirror for the Docker daemon. The service must be defined for the Docker or Kubernetes executor.\n\nThe Docker executor in the GitLab Runner configuration file\n\nIf you are a GitLab Runner administrator, you can use the mirror for every service. Update the configuration to specify a volume mount.\n\nFor example, if you have a file with the following content:\n\nUpdate the file to mount the file to . This mounts the file for every container created by GitLab Runner. The configuration is detected by the service.\n\nThe Kubernetes executor in the GitLab Runner configuration file\n\nIf you are a GitLab Runner administrator, you can use the mirror for every service. Update the configuration to specify a ConfigMap volume mount.\n\nFor example, if you have a file with the following content:\n\nCreate a ConfigMap with the content of this file. You can do this with a command like:\n\nAfter the ConfigMap is created, you can update the file to mount the file to . This update mounts the file for every container created by GitLab Runner. The service detects this configuration.\n\nWhen you use Docker socket binding, you avoid running Docker in privileged mode. However, the implications of this method are:\n• None By sharing the Docker daemon, you effectively disable all the container’s security mechanisms and expose your host to privilege escalation. This can cause container breakout. For example, if a project ran , it would remove the GitLab Runner containers.\n• None Concurrent jobs might not work. If your tests create containers with specific names, they might conflict with each other.\n• None Any containers created by Docker commands are siblings of the runner, rather than children of the runner. This might cause complications for your workflow.\n• None Sharing files and directories from the source repository into containers might not work as expected. Volume mounting is done in the context of the host machine, not the build container. For example:\n\nYou do not need to include the service, like you do when you use the Docker-in-Docker executor:\n\nWhen you use Docker-in-Docker, the standard authentication methods do not work, because a fresh Docker daemon is started with the service. You should authenticate with registry.\n\nWhen using Docker-in-Docker, Docker downloads all layers of your image every time you create a build. You can make your builds faster with Docker layer caching.\n\nBy default, when using , Docker uses the storage driver, which copies the file system on every run. You can avoid this disk-intensive operation by using a different driver, for example .\n• None Check whether the module is loaded: If you see no result, then the module is not loaded. To load the module, use: If the module loaded, you must make sure the module loads on reboot. On Ubuntu systems, do this by adding the following line to :\n\nUse the OverlayFS driver per project\n\nYou can enable the driver for each project individually by using the CI/CD variable in :\n\nUse the OverlayFS driver for every project\n\nIf you use your own runners, you can enable the driver for every project by setting the environment variable in the section of the file:\n\nIf you’re running multiple runners, you must modify all configuration files.\n\nRead more about the runner configuration and using the OverlayFS storage driver.\n\nTo build Docker images without enabling privileged mode on the runner, you can use one of these alternatives:\n\nTo use Buildah with GitLab CI/CD, you need a runner with one of the following executors:\n\nIn this example, you use Buildah to:\n\nIn the last step, Buildah uses the under the root directory of the project to build the Docker image. Finally, it pushes the image to the project’s container registry:\n\nIf you are using GitLab Runner Operator deployed to an OpenShift cluster, try the tutorial for using Buildah to build images in rootless container.\n\nAfter you’ve built a Docker image, you can push it to the GitLab container registry.\n\nError: docker: Cannot connect to the Docker daemon at tcp://docker:2375\n\nThis error is common when you are using Docker-in-Docker v19.03 or later:\n\nThis error occurs because Docker starts on TLS automatically.\n• If this is your first time setting it up, see use the Docker executor with the Docker image.\n• If you are upgrading from v18.09 or earlier, see the upgrade guide.\n\nThis error can also occur with the Kubernetes executor when attempts are made to access the Docker-in-Docker service before it has fully started up. For a more detailed explanation, see issue 27215.\n\nYou might get an error that says docker: error during connect: Post https://docker:2376/v1.40/containers/create: dial tcp: lookup docker on x.x.x.x:53: no such host .\n\nThis issue can occur when the service’s image name includes a registry hostname. For example:\n\nA service’s hostname is derived from the full image name. However, the shorter service hostname is expected. To allow service resolution and access, add an explicit alias for the service name :\n\nError: Cannot connect to the Docker daemon at unix:///var/run/docker.sock\n\nYou might get the following error when trying to run a command to access a service:\n\nMake sure your job has defined these environment variables:\n\nYou may also want to update the image that provides the Docker client. For example, the images are obsolete and should be replaced with .\n\nAs described in runner issue 30944, this error can happen if your job previously relied on environment variables derived from the deprecated Docker parameter, such as . Your job fails with this error if:\n• Your CI/CD image relies on a legacy variable, such as .\n• The runner feature flag is set to .\n\nThis error appears when you use the deprecated variable, :\n\nTo prevent users from receiving this error, you should:\n\nThis error appears when the service has failed to start:\n\nCheck the job log to see if appears. For example:\n\nThis indicates the GitLab Runner does not have permission to start the service:\n• Check that is set in the .\n• Make sure the CI job has the right Runner tags to use these privileged runners.\n\nThere is a known incompatibility introduced by Docker Engine 20.10.\n\nWhen the host uses Docker Engine 20.10 or newer, then the service in a version older than 20.10 does not work as expected.\n\nWhile the service itself will start without problems, trying to build the container image results in the error:\n\nTo resolve this issue, update the container to version at least 20.10.x, for example .\n\nThe opposite configuration ( service and Docker Engine on the host in version 19.06.x or older) works without problems. For the best strategy, you should to frequently test and update job environment versions to the newest. This brings new features, improved security and - for this specific case - makes the upgrade on the underlying Docker Engine on the runner’s host transparent for the job.\n\nThis error can appear when Docker commands like or are executed in a Docker-in-Docker environment where custom or private certificates are used (for example, Zscaler certificates):\n\nThis error occurs because Docker commands in a Docker-in-Docker environment use two separate containers:\n• The build container runs the Docker client ( ) and executes your job’s script commands.\n• The service container (often named ) runs the Docker daemon that processes most Docker commands.\n\nWhen your organization uses custom certificates, both containers need these certificates. Without proper certificate configuration in both containers, Docker operations that connect to external registries or services will fail with certificate errors.\n• None Store your root certificate as a CI/CD variable named . The certificate should be in this format:\n• None Configure your pipeline to install the certificate in the service container before starting the Docker daemon. For example:"
    },
    {
        "link": "https://docs.gitlab.com/runner/executors/docker",
        "document": "GitLab Runner uses the Docker executor to run jobs on Docker images.\n\nYou can use the Docker executor to:\n• Maintain the same build environment for each job.\n• Use the same image to test commands locally without the requirement of running a job in the CI server.\n\nThe Docker executor uses Docker Engine to run each job in a separate and isolated container. To connect to Docker Engine, the executor uses:\n• The image and services you define in .\n• The configurations you define in .\n\nThe Docker executor uses a Docker image based on Alpine Linux that contains the tools to run the prepare, pre-job, and post-job steps. To view the definition of the special Docker image, see the GitLab Runner repository.\n\nThe Docker executor divides the job into several steps:\n• Pre-job: Clones, restores cache, and downloads artifacts from previous stages. Runs on a special Docker image.\n• Job: Runs your build in the Docker image you configure for the runner.\n\nThe Docker executor supports the following configurations.\n\nFor known issues and additional requirements of Windows configurations, see Use Windows containers.\n\nThese configurations are not supported:\n\nTo use the Docker executor, define Docker as the executor in .\n\nThe following sample shows Docker defined as the executor and example configurations. For more information about these values, see Advanced configuration\n• The image where your job runs must have a working shell in its operating system . Supported shells are:\n\nTo configure the Docker executor, you define the Docker images and services in and .\n\nUse the following keywords:\n• : The name of the Docker image that the runner uses to run jobs.\n• Enter an image from the local Docker Engine, or any image in Docker Hub. For more information, see the Docker documentation.\n• To define the image version, use a colon ( ) to add a tag. If you don’t specify a tag, Docker uses as the version.\n• : The additional image that creates another container and links to the . For more information about types of services, see Services.\n\nDefine an image that the runner uses for all jobs and a list of services to use during build time.\n\nTo define different images and services per job:\n\nIf you don’t define an in , the runner uses the defined in .\n\nTo add images and services to all jobs run by a runner, update in the . If you don’t define an in , the runner uses the image defined in .\n\nThis example uses the array of tables syntax.\n• To access images from a private registry, you must authenticate GitLab Runner.\n\nTo define an image from a private registry, provide the registry name and the image in .\n\nIn this example, GitLab Runner searches the registry for the image .\n\nYou must configure a network to connect services to a CI/CD job.\n\nTo configure a network, you can either:\n• Recommended. Configure the runner to create a network for each job.\n\nYou can configure the runner to create a network for each job.\n\nWhen you enable this networking mode, the runner creates and uses a user-defined Docker bridge network for each job. Docker environment variables are not shared across the containers. For more information about user-defined bridge networks, see the Docker documentation.\n\nTo use this networking mode, enable in either the feature flag or the environment variable in the .\n\nDo not set the .\n\nTo set the default Docker address pool, use in . If CIDR ranges are already used in the network, Docker networks may conflict with other networks on the host, including other Docker networks.\n\nThis feature works only when the Docker daemon is configured with IPv6 enabled. To enable IPv6 support, set to in the Docker configuration. For more information, see the Docker documentation.\n\nThe runner uses the alias to resolve the job container.\n\nHow the runner creates a network for each job\n• Connects the service and containers to the bridge network.\n• Removes the network at the end of the job.\n\nThe container running the job and the containers running the service resolve each other’s hostnames and aliases. This functionality is provided by Docker.\n\nYou can configure a network mode that uses Docker legacy container links and the default Docker to link the job container with the services. This network mode is the default if is not enabled.\n\nTo configure the network, specify the networking mode in the file:\n• : Use the host’s network stack inside the container.\n\nIf you use any other value, these are taken as the name of an already existing Docker network, which the build container connects to.\n\nDuring name resolution, Docker updates the file in the container with the service container hostname and alias. However, the service container is not able to resolve the container name. To resolve the container name, you must create a network for each job.\n\nOverriding the MTU of the created network\n\nFor some environments, like virtual machines in OpenStack, a custom MTU is necessary. The Docker daemon does not respect the MTU in (see Moby issue 34981). You can set in your to any valid value so the Docker daemon can use the correct MTU for the newly created network. You must also enable for the override to take effect.\n\nThe following configuration sets the MTU to for the network created for each job. Make sure to adjust the value to your specific environment requirements.\n\nTo restrict Docker images and services, specify a wildcard pattern in the and parameters. For more details on syntax, see doublestar documentation.\n\nFor example, to allow images from your private Docker registry only:\n\nTo restrict to a list of images from your private Docker registry:\n\nTo access a service hostname, add the service to in .\n\nFor example, to use a Wordpress instance to test an API integration with your application, use tutum/wordpress as the service image:\n\nWhen the job runs, the service starts. You can access it from your build container under the hostname and .\n\nIn addition to the specified service aliases, the runner assigns the name of the service image as an alias to the service container. You can use any of these aliases.\n\nThe runner uses the following rules to create the alias based on the image name:\n• Everything after is stripped.\n• For the first alias, the slash ( ) is replaced with double underscores ( ).\n• For the second alias, the slash ( ) is replaced with a single dash ( ).\n\nIf you use a private service image, the runner strips any specified port and applies the rules. The service results in the hostname and .\n\nTo change database names or set account names, you can define environment variables for the service.\n• Variables are passed to all containers. The runner cannot pass variables to specific containers.\n• Secure variables are passed to the build container.\n\nFor more information about configuration variables, see the documentation of each image provided in their corresponding Docker Hub page.\n\nYou can use the option to mount a directory in RAM. This speeds up the time required to test if there is a lot of I/O related work, such as with databases.\n\nIf you use the and options in the runner configuration, you can specify multiple paths, each with its own options. For more information, see the Docker documentation.\n\nFor example, to mount the data directory for the official MySQL container in RAM, configure the :\n\nFor more information about using different services see:\n\nAfter the service starts, GitLab Runner waits for the service to respond. The Docker executor tries to open a TCP connection to the exposed service port in the service container.\n• In GitLab 15.11 and earlier, only the first exposed port is checked.\n• In GitLab 16.0 and later, the first 20 exposed ports are checked.\n\nThe service variable can be used to perform the health check on a specific port:\n\nTo see how this is implemented, use the health check Go command.\n\nSpecify arguments to supply to the Docker volume driver when you create volumes for builds. For example, you can use these arguments to limit the space for each build to run, in addition to all other driver specific options. The following example shows a where the limit that each build can consume is set to 50 GB.\n\nYou can expose hardware devices on the GitLab Runner host to the container that runs the job. To do this, configure the runner’s and options.\n• To expose devices to and helper containers, use the option.\n• To expose devices to services containers, use the option. To restrict a service container’s device access to specific images, use exact image names or glob patterns. This action prevents direct access to host system devices.\n\nFor more information on device access, see Docker documentation.\n\nIn this example, the section exposes to build containers. This configuration allows pipelines to access USB devices attached to the host machine, such as Android smartphones controlled over the Android Debug Bridge ( ).\n\nSince build job containers can directly access host USB devices, simultaneous pipeline executions may conflict with each other when accessing the same hardware. To prevent these conflicts, use .\n\nThis example shows how to expose and devices to container images from a private Docker registry. These devices are commonly used for hardware-accelerated virtualization and rendering. To mitigate risks involved with providing users direct access to hardware resources, restrict device access to trusted images in the namespace:\n\nConfigure directories for the container build and cache\n\nTo define where data is stored in the container, configure and directories in the section in .\n\nIf you modify the storage path, to mark the path as persistent you must define it in , under the section in .\n\nBy default, the Docker executor stores builds and caches in the following directories:\n\nUse to remove unused containers and volumes created by the runner.\n\nFor a list of options, run the script with the option:\n\nThe default option is , which removes all unused containers (dangling and unreferenced) and volumes.\n\nTo manage cache storage efficiently, you should:\n• Run with regularly (for example, once a week).\n• Maintain some recent containers in the cache for performance while you reclaim disk space.\n\nThe script does not remove Docker images because they are not tagged by the GitLab Runner.\n• None Confirm what disk space can be reclaimed:\n• None To remove all unused containers, networks, images (dangling and unreferenced), and untagged volumes, run .\n\nThe Docker executor provides persistent storage when it runs containers. All directories defined in are persistent between builds.\n\nThe directive supports the following types of storage:\n• For dynamic storage, use . The is persistent between subsequent runs of the same concurrent job for that project. The data is attached to a custom cache volume: .\n• For host-bound storage, use . The is bound to on the host system. The optional specifies that this storage is read-only or read-write (default).\n\nIf you make the directory a host-bound storage, your builds are stored in: , where:\n• is a shortened version of the Runner’s token (first 8 letters).\n• is a unique number that identifies the local job ID of the particular runner in context of the project.\n\nThe Docker executor supports sharing the IPC namespace of containers with other locations. This maps to the flag. More details on IPC settings in Docker documentation\n\nThe Docker executor supports several options that allows fine-tuning of the build container. One of these options is the mode.\n\nThe configured flag is passed to the build container and all services. With this flag, you can use the Docker-in-Docker approach.\n\nFirst, configure your runner ( ) to run in mode:\n\nThen, make your build script ( ) to use Docker-in-Docker container:\n\nYou might need to configure Docker in Docker with TLS, or disable TLS to avoid an error similar to the following:\n\nIn this version, only Docker-in-Docker rootless images are allowed to run as services in privileged mode.\n\nThe and configuration parameters limit which containers are allowed to run in privileged mode.\n\nTo use rootless Docker-in-Docker with restricted privileged mode:\n• None In the , configure the runner to use and :\n• None In , edit your build script to use Docker-in-Docker rootless container:\n\nOnly the Docker-in-Docker rootless images you list in are allowed to run in privileged mode. All other containers for jobs and services run in unprivileged mode.\n\nBecause they run as non-root, it’s almost safe to use with privileged mode images like Docker-in-Docker rootless or BuildKit rootless.\n\nFor more information about security issues, see Security risks for Docker executors.\n\nBy default the Docker executor doesn’t override the of a Docker image. It passes or as to start a container that runs the job script.\n\nTo ensure a job can run, its Docker image must:\n• Define an that starts a shell when passed / as argument\n\nThe Docker Executor runs the job’s container with an equivalent of the following command:\n\nIf your Docker image doesn’t support this mechanism, you can override the image’s ENTRYPOINT in the project configuration as follows:\n\nFor more information, see Override the Entrypoint of an image and How and interact in Docker.\n\nYou can use to create a Docker image that runs the build script in a custom environment, or in secure mode.\n\nFor example, you can create a Docker image that uses an that doesn’t execute the build script. Instead, the Docker image executes a predefined set of commands to build the Docker image from your directory. You run the build container in privileged mode, and secure the build environment of the runner.\n• None Create a bash script ( ) that is used as the :\n• None Push the image to the Docker registry.\n• None In your project use the following :\n\nIf you have GitLab Runner installed on Linux, your jobs can use Podman to replace Docker as the container runtime in the Docker executor.\n• To run services with Podman as an executor, enable the feature flag. Docker container links are legacy and are not supported by Podman. For services that create a network alias, you must install the package.\n• None On your Linux host, install GitLab Runner. If you installed GitLab Runner by using your system’s package manager, it automatically creates a user.\n• None Sign in as the user who runs GitLab Runner. You must do so in a way that doesn’t go around . You can use SSH with the correct user. This ensures you can run as this user.\n• None Make sure that your system fulfills the prerequisites for a rootless Podman setup. Specifically, make sure your user has correct entries in and .\n• None Copy the socket string in the key through which the Podman API is being accessed.\n• None Make sure the Podman socket remains available after the GitLab Runner user is logged out:\n• None Edit the GitLab Runner file and add the socket value to the host entry in the section. For example:\n\nUse Podman to build container images from a Dockerfile\n\nThe following example uses Podman to build a container image and push the image to the GitLab Container registry.\n\nThe default container image in the Runner is set to , so that the CI job uses that image to execute the included commands.\n\nUse Buildah to build container images from a Dockerfile\n\nThe following example shows how to use Buildah to build a container image and push the image to the GitLab Container registry.\n\nSpecify which user runs the job\n\nBy default, the runner runs jobs as the user in the container. To specify a different, non-root user to run the job, use the directive in the Dockerfile of the Docker image.\n\nWhen you use that Docker image to execute your job, it runs as the specified user:\n\nConfigure the pull policy in the to define how runners pull Docker images from registries. You can set a single policy, a list of policies, or allow specific pull policies.\n\nUse the following values for the :\n• : Pull an image even if a local image exists. Default.\n• : Pull an image only when a local version does not exist.\n• : Never pull an image and use only local images.\n\nThe option, which is on by default, always initiates a pull before creating the container. This option makes sure the image is up-to-date, and prevents you from using outdated images even if a local image exists.\n\nUse this pull policy if:\n• Runners must always pull the most recent images.\n• Runners are publicly available and configured for auto-scale or as an instance runner in your GitLab instance.\n\nDo not use this policy if runners must use locally stored images.\n\nSet as the in the :\n\nWhen you set the pull policy to , the runner first checks if a local image exists. If there is no local image, the runner pulls an image from the registry.\n\nUse the policy to:\n• Use local images but also pull images if a local image does not exist.\n• Reduce time that runners analyze the difference in image layers for heavy and rarely updated images. In this case, you must manually remove the image regularly from the local Docker Engine store to force the image update.\n\nDo not use this policy:\n• For instance runners where different users that use the runner may have access to private images. For more information about security issues, see Usage of private Docker images with if-not-present pull policy.\n• If jobs are frequently updated and must be run in the most recent image version. This may result in a network load reduction that outweighs the value of frequent deletion of local images.\n\nSet the policy in the :\n• Local images must contain an installed Docker Engine and a local copy of used images.\n\nWhen you set the pull policy to , image pulling is disabled. Users can only use images that have been manually pulled on the Docker host where the runner runs.\n• To control the images used by runner users.\n• For private runners that are dedicated to a project that can only use specific images that are not publicly available on any registries.\n\nDo not use the pull policy for auto-scaled Docker executors. The pull policy is usable only when using a pre-defined cloud instance images for chosen cloud provider.\n\nSet the policy in the :\n\nYou can list multiple pull policies to execute if a pull fails. The runner processes pull policies in the order listed until a pull attempt is successful or the list is exhausted. For example, if a runner uses the pull policy and the registry is not available, you can add the as a second pull policy. This configuration lets the runner use a locally cached Docker image.\n\nFor information about the security implications of this pull policy, see Usage of private Docker images with if-not-present pull policy.\n\nTo set multiple pull policies, add them as a list in the :\n\nIn the file, you can specify a pull policy. This policy determines how a CI/CD job fetches images.\n\nTo restrict which pull policies can be used in the file, use .\n\nFor example, to allow only the and pull policies, add them to the :\n• If you don’t specify , the list matches the values specified in the keyword.\n• If you don’t specify , the default is .\n• The existing keyword must not include a pull policy that is not specified in . If it does, the job returns an error.\n\nTo configure a runner to retry a failed image pull, specify the same policy more than once in the .\n\nFor example, this configuration retries the pull one time:\n\nThis setting is similar to the directive in the files of individual projects, but only takes effect if specifically the Docker pull fails initially.\n\nTo use Windows containers with the Docker executor, note the following information about limitations, supported Windows versions, and configuring a Windows Docker executor.\n\nWith the support for PowerShell Core introduced in the Windows helper image, it is now possible to leverage the variants for the helper image.\n\nKnown issues with Docker executor on Windows\n\nThe following are some limitations of using Windows containers with Docker executor:\n• None Docker-in-Docker is not supported, because it’s not supported by Docker itself.\n• None When mounting a volume directory it has to exist, or Docker fails to start the container, see #3754 for additional detail.\n• None executor can be run only using GitLab Runner running on Windows.\n• None Linux containers on Windows are not supported, because they are still experimental. Read the relevant issue for more details.\n• None Because of a limitation in Docker, if the destination path drive letter is not , paths are not supported for: This means values such as are not supported, but is supported. However, if the destination path is on the drive, paths are also supported (for example ). To configure where the Docker daemon keeps images and containers, update the parameter in the file of the Docker daemon. For more information, see Configure Docker with a configuration file.\n\nGitLab Runner only supports the following versions of Windows which follows our support lifecycle for Windows:\n\nFor future Windows Server versions, we have a future version support policy.\n\nYou can only run containers based on the same OS version that the Docker daemon is running on. For example, the following images can be used:\n\nGitLab Runner uses Docker to detect what version of Windows Server is running. Hence, a Windows Server running GitLab Runner must be running a recent version of Docker.\n\nA known version of Docker that doesn’t work with GitLab Runner is . Docker does not identify the version of Windows Server resulting in the following error:\n\nRead more about troubleshooting this.\n\nBelow is an example of the configuration for a Docker executor running Windows.\n\nFor other configuration options for the Docker executor, see the advanced configuration section.\n\nIn GitLab Runner 12.9 and later, you can use services by enabling a network for each job.\n\nThe Docker executor supports running the CI/CD steps natively by using the API provided by .\n\nTo enable this mode of execution, you must specify CI/CD jobs using the keyword instead of the legacy keyword. Additionally, you must enable the feature flag. You can enable this feature flag at either the job or pipeline level.\n• None In GitLab 17.9 and later, the build image must have the package installed or the will fail to pull the steps defined in the job. Debian-based Linux distribution for example do not install by default.\n• None In GitLab versions before 17.9, the build image must include a binary in . To achieve this, you can either:\n• Create your own custom build image and include the binary in it.\n• Use the image if it includes the dependencies you need to run your job.\n• None Running a step that runs a Docker container must adhere to the same configuration parameters and constraints as traditional . For example, you must use Docker-in-Docker.\n• None This mode of execution does not yet support running ."
    },
    {
        "link": "https://docs.gitlab.com/ci/docker/using_docker_images",
        "document": "You can run your CI/CD jobs in Docker containers hosted on dedicated CI/CD build servers or your local machine.\n\nTo run CI/CD jobs in a Docker container, you need to:\n• Register a runner and configure it to use the Docker executor.\n• Specify the container image where you want to run the CI/CD jobs in the file.\n• Optional. Run other services, like MySQL, in containers. Do this by specifying services in your file.\n\nRegister a runner that uses the Docker executor\n\nTo use GitLab Runner with Docker you need to register a runner that uses the Docker executor.\n\nThis example shows how to set up a temporary template to supply services:\n\nThen use this template to register the runner:\n\nThe registered runner uses the Docker image and runs two services, and , both of which are accessible during the build process.\n\nWhat is an image\n\nThe keyword is the name of the Docker image the Docker executor uses to run CI/CD jobs.\n\nBy default, the executor pulls images from Docker Hub. However, you can configure the registry location in the file. For example, you can set the Docker pull policy to use local images.\n\nFor more information about images and Docker Hub, see the Docker overview.\n\nAny image used to run a CI/CD job must have the following applications installed:\n\nYou can define an image that’s used for all jobs, and a list of services that you want to use during runtime:\n\nThe image name must be in one of the following formats:\n\nYou can use a string or a map for the or entries:\n• Strings must include the full image name (including the registry, if you want to download the image from a registry other than Docker Hub).\n• Maps must contain at least the option, which is the same image name as used for the string setting.\n\nFor example, the following two definitions are equal:\n• None A map for and . The is required:\n\nWhen a CI job runs in a Docker container, the , , and commands run in the directory. Your image may have a different default defined. To move to your , save the as an environment variable so you can reference it in the container during the job’s runtime.\n\nOverride the entrypoint of an image\n\nBefore explaining the available entrypoint override methods, let’s describe how the runner starts. It uses a Docker image for the containers used in the CI/CD jobs:\n• The runner starts a Docker container using the defined entrypoint. The default from that may be overridden in the file.\n• The runner attaches itself to a running container.\n• The runner prepares a script (the combination of , , and ).\n• The runner sends the script to the container’s shell and receives the output.\n\nTo override the entrypoint of a Docker image, in the file:\n• For Docker 17.06 and later, set to an empty value.\n• For Docker 17.03 and earlier, set to , , or an equivalent shell available in the image.\n\nThe syntax of is similar to Dockerfile .\n\nLet’s assume you have a image with a SQL database in it. You want to use it as a base image for your job because you want to execute some tests with this database binary. Let’s also assume that this image is configured with as an entrypoint. When the container starts without additional options, it runs the database’s process. The runner expects that the image has no entrypoint or that the entrypoint is prepared to start a shell command.\n\nWith the extended Docker configuration options, instead of:\n• Creating your own image based on .\n• Using the new image in your CI job.\n\nYou can now define an in the file.\n\nFor Docker 17.06 and later:\n\nIn the file, you can define:\n• In the section, the container image used to run CI/CD jobs\n• In the section, the services container\n\nThe image and services defined this way are added to all jobs run by that runner.\n\nTo access private container registries, the GitLab Runner process can use:\n• Credentials Store. For more information, see the relevant Docker documentation.\n• Credential Helpers. For more information, see the relevant Docker documentation.\n\nWhen you use the GitLab Container Registry on the same GitLab instance, GitLab provides default credentials for this registry. With these credentials, the is used for authentication. To use the job token, the user starting the job must have at least the Developer role for the project where the private image is hosted. The project hosting the private image must also allow the other project to authenticate with the job token. This access is disabled by default. For more details, see CI/CD job token.\n\nTo define which option should be used, the runner process reads the configuration in this order:\n• A file in directory of the user running the process. If the flag is provided to run the child processes as unprivileged user, the home directory of the main runner process user is used.\n• Available for Kubernetes executor in GitLab Runner 13.1 and later.\n• Credentials Store and Credential Helpers require binaries to be added to the GitLab Runner , and require access to do so. Therefore, these features are not available on instance runners, or any other runner where the user does not have access to the environment where the runner is installed.\n\nYou can access a private registry using two approaches. Both require setting the CI/CD variable with appropriate authentication information.\n• Per-job: To configure one job to access a private registry, add as a CI/CD variable.\n• Per-runner: To configure a runner so all its jobs can access a private registry, add as an environment variable in the runner’s configuration.\n\nSee below for examples of each.\n\nAs an example, let’s assume you want to use the image. This image is private and requires you to sign in to a private container registry.\n\nLet’s also assume that these are the sign-in credentials:\n\nUse one of the following methods to determine the value for :\n• None Do a on your local machine: Then copy the content of . If you don’t need access to the registry from your computer, you can do a :\n• None In some setups, it’s possible the Docker client uses the available system key store to store the result of . In that case, it’s impossible to read , so you must prepare the required base64-encoded version of and create the Docker configuration JSON manually. Open a terminal and execute the following command: # The use of printf (as opposed to echo) prevents encoding a newline in the password. If your username includes special characters like , you must escape them with a backslash ( ) to prevent authentication problems. Create the Docker JSON configuration content as follows:\n\nTo configure a single job with access for , follow these steps:\n• None Create a CI/CD variable with the content of the Docker configuration file as the value:\n• None You can now use any private image from defined in or in your file: In the example above, GitLab Runner looks at for the image .\n\nYou can add configuration for as many registries as you want, adding more registries to the hash as described above.\n\nThe full combination is required everywhere for the runner to match the . For example, if is specified in the file, then the must also specify . Specifying only does not work.\n\nIf you have many pipelines that access the same registry, you should set up registry access at the runner level. This allows pipeline authors to have access to a private registry just by running a job on the appropriate runner. It also helps simplify registry changes and credential rotations.\n\nThis means that any job on that runner can access the registry with the same privilege, even across projects. If you need to control access to the registry, you need to be sure to control access to the runner.\n• None Modify the runner’s file as follows:\n• The double quotes included in the data must be escaped with backslashes. This prevents them from being interpreted as TOML.\n• The option is a list. Your runner may have existing entries and you should add this to the list, not replace it.\n• None To use a Credentials Store, you need an external helper program to interact with a specific keychain or external store. Make sure the helper program is available in the GitLab Runner .\n• None Make GitLab Runner use it. You can accomplish this by using one of the following options:\n• None Create a CI/CD variable with the content of the Docker configuration file as the value:\n• None Or, if you’re running self-managed runners, add the above JSON to . GitLab Runner reads this configuration file and uses the needed helper for this specific repository.\n\nis used to access all the registries. If you use both images from a private registry and public images from Docker Hub, pulling from Docker Hub fails. Docker daemon tries to use the same credentials for all the registries.\n\nAs an example, let’s assume that you want to use the image. This image is private and requires you to sign in to a private container registry.\n\nTo configure access for , follow these steps:\n• None Make sure is available in the GitLab Runner .\n• None Have any of the following AWS credentials setup. Make sure that GitLab Runner can access the credentials.\n• None Make GitLab Runner use it. You can accomplish this by using one of the following options:\n• None Create a CI/CD variable with the content of the Docker configuration file as the value: This configures Docker to use the Credential Helper for a specific registry. Instead, you can configure Docker to use the Credential Helper for all Amazon Elastic Container Registry (ECR) registries: If you use , set the region explicitly in the AWS shared configuration file ( ). The region must be specified when the ECR Credential Helper retrieves the authorization token.\n• None Or, if you’re running self-managed runners, add the previous JSON to . GitLab Runner reads this configuration file and uses the needed helper for this specific repository.\n• None You can now use any private image from defined in and/or in your file: In the example, GitLab Runner looks at for the image .\n\nYou can add configuration for as many registries as you want, adding more registries to the hash.\n\nUse checksum to keep your image secure\n\nUse the image checksum in your job definition in your file to verify the integrity of the image. A failed image integrity verification prevents you from using a modified container.\n\nTo use the image checksum you have to append the checksum at the end:\n\nTo get the image checksum, on the image tab, view the column. For example, view the Ruby image. The checksum is a random string, like .\n\nYou can also get the checksum of any image on your system with the command :\n\nYou can create a custom GitLab Runner Docker image to package AWS CLI and Amazon ECR Credential Helper. This setup facilitates secure and streamlined interactions with AWS services, especially for containerized applications. For example, use this setup to manage, deploy, and update Docker images on Amazon ECR. This setup helps avoid time consuming, error-prone configurations, and manual credential management.\n• None Create a with the following content:\n• None To build the custom GitLab Runner Docker image in a , include the following example below:"
    },
    {
        "link": "https://docs.gitlab.com/runner/executors/shell",
        "document": "You can use the Shell executor to execute builds locally on the machine where GitLab Runner is installed. It supports all systems on which the Runner can be installed. That means that it’s possible to use scripts generated for Bash, PowerShell Core, Windows PowerShell, and Windows Batch (deprecated).\n\nThe scripts can be run as unprivileged user if the is added to the command. This feature is only supported by Bash.\n\nThe source project is checked out to: .\n\nThe caches for project are stored in .\n• is the value of as passed to the command or the current directory where the Runner is running\n• is a shortened version of the Runner’s token (first 8 letters)\n• is a unique number, identifying the local job ID on the particular Runner in context of the project (accessible through the pre-defined variable)\n• is the namespace where the project is stored on GitLab\n• is the name of the project as it is stored on GitLab\n\nTo overwrite the and specify the and options under the section in .\n\nIf GitLab Runner is installed on Linux from the official or packages, the installer tries to use the user if found. If the installer is unable to find the user, it creates a user and uses it instead.\n\nAll shell builds are then executed as either the or user.\n\nIn some testing scenarios, your builds may need to access some privileged resources, like Docker Engine or VirtualBox. In that case you need to add the user to the respective group:\n\nGitLab Runner supports certain shells. To select a shell, specify it in your file. For example:\n\nGenerally it’s unsafe to run jobs with shell executors. The jobs are run with the user’s permissions ( ) and can “steal” code from other projects that are run on this server. Depending on your configuration, the job could execute arbitrary commands on the server as a highly privileged user. Use it only for running builds from users you trust on a server you trust and own.\n\nThe shell executor starts the script for each job in a new process. On UNIX systems, it sets the main process as a process group.\n\nOn UNIX system sends to the process and its child processes, and after 10 minutes sends . This allows for graceful termination for the process. Windows doesn’t have a equivalent, so the kill signal is sent twice. The second is sent after 10 minutes."
    },
    {
        "link": "https://medium.com/@BuildWithLal/dockerized-gitlab-ci-register-docker-executor-as-a-gitlab-runner-71799352c9ac",
        "document": "Continuing the series of Dockerized GitLab, in this post i’ll show you how to register Docker executor as a GitLab runner with your GitLab server for building, testing and deploying your dockerized projects.\n\nIn our first post, we set up a GitLab server using barebone docker commands. In the next post, we transitioned those barebone docker commands into a docker compose file for better usability and managing multiple containers easily.\n\nIn our last post, we added GitLab runner as a container, integrated with GitLab server and then we registered a Shell executor to run our project pipelines.\n\nShell pipeline executor from our last post.\n\nIn this post, we are going to register docker executor inside our GitLab runner service so we can build, test and deploy our dockerized projects.\n\nBefore registering our docker executor, we need a minor update in our file by mounting the file from our host machine into the GitLab runner container.\n\nYour updated should look like this.\n\nWhen you install docker in a machine. Two diffrent programs comes in:\n\n 1. Docker Server\n\n 2. Docker Client\n\nDocker Server recieves commands over a socket (either over a network or through a file)\n\nDocker Client communicates over a network and sends message to the Docker server to say make a container, start a container, stop a container etc.\n\nWhen the docker client and server are running on the same computer, they can connect through a special file called socket. And since they can communicate through a file and Docker can efficiently share files between host and containers, it means you can run the client inside Docker itself.\n\nThe Docker daemon can listen for Docker Engine API requests via three different types of Socket: , , and . By default, a unix domain socket (or IPC socket) is created at when you install Docker.\n\nWe need to mount from our host machine into GitLab Runner container. When we want to execute a job using docker, GitLab Runner can create a container on our host machine using this file, run the job and then terminate once the job is done.\n\nIf your docker server and client are across different machines, you can communicate it through TCP but since we have it both on the same host machine, we are communicating through socket file i.e\n\nOnce you have updated , hit to stop running containers and then run\n\nGo to your project runners by going to\n\nin the URL is the the repository name i’ve created earlier.\n\nExisting runner is the Shell executor we created in our last post.\n\nFill out the runner details. Make sure to add so we can run different jobs by specifying a specific runner using these tags.\n\nClick on Create runner button which will redirect you to this page\n\nLogin to your GitLab runner container using docker\n\nOnce logged in to the GitLab runner container, run command from the above Step 1. Make sure to add these flags to the end of the command.\n\nadding will mount the docker socket file from the GitLab runner container into the executor container so when we need to run docker CLI commands, docker CLI will have access to docker engine on the host machine via file.\n\nadding will put the executor container on the host network so when cloning the repository, the executor can access it via localhost.\n\nWe put the container network on host network (PC network)in 2 different places.\n• First is in the file where put the network of GitLab runner container on the our host machine (PC network) so GitLab server can communicate with GitLab runner over localhost for managing pipelines.\n• Second, we put the container network on the host network (PC network) when we are registering our docker executor. This network comes handy when the pipeline runs using docker executor, job pulls the docker image and then tries to clone the project repository inside that executor container. At that time, the docker executor tries to access the repository over localhost so it is necessary for the executor container to be on the same network as GitLab server which is localhost.\n• When asking for the , leave it as it is by hitting Enter unless your GitLab server and Runner are on different machines.\n• Enter your favourite name for the Runner.\n• Since we want to register a docker executor, enter when asking for executor option.\n• When selecting docker executor, you also need to set a default docker image in case you miss it in our pipeline file when building pipeline for your project. I have set default docker image to which you can override inside your file\n\nAfter you fill out all of the above details, you should have your docker runner registered and the runner page should look like this.\n\nIf you go back to your project runners page under\n\nyou should have a new runner alongside the Shell runner which we created in the previous post.\n\nLets try adding a pipeline with 3 different jobs\n\nYour pipeline editor should look like this under Build → Pipeline Editor\n\nBy adding to each job, it will make sure to pick the correct runner for this job. So we are adding to the first job because we want to that job to be run by our Shell executor. While rest of the 2 jobs should be executed by the docker executor so we are adding\n\nIn th second job with , you can use docker CLI to build your project using docker and push your docker image to some container registry.\n\nCommit these changes and switch to Jobs under Build. Build → Jobs\n\nYou should have all your jobs running OR already passed.\n\nGo to the details of each job and you can notice that each job is picked and executed by the relevant runner\n\nJob is executed by the Shell executor by printing the current date and OS details.\n\nJob is executed by the Docker runner by using the default docker image i.e\n\nJob is executed by the Docker runner by overriding the default docker image to for utilizing the docker CLI.\n\nIf you check all these details from command, the docker server details are coming from my host machine where i have my actual docker engine installed. The reason we used docker image inside the file is to have docker CLI available. Any instruction you provide to the docker CLI will be passed to the docker Engine on the host machine by using our mounted socket file\n\nWhen a job is executing by docker executor, you can monitor containers on your machine and you should have some new containers alongside your existing containers. These new containers are coming from the docker executor and it will be terminated once the job is completed.\n\nThese runner containers are the sibling containers of your GitLab runner container and not child containers of the GitLab runner container because we have mounted the file from the host machine so all of the containers are managed by the host machine docker engine.\n\nEven if try build and creating container inside our pipeline job, the containers will be actually created on our host docker engine because we are using same docker.sock inside docker executor and its job.\n\nLets update our pipeline by creating some containers inside our job.\n\nOnce you commit the above changes and pipeline runs, you can monitor your docker containers on your host machine by running\n\nThe output should look like this.\n\nYou can see that our executor container and the containers we created inside our job are created on host docker engine.\n\nThere are some known issues with docker socket binding and managing all containers using a single docker engine highlighted by GitLab documentation here.\n• If a pipeline job ran , it would remove the GitLab server and runner containers and may be other critical containers as well.\n• If your tests create containers with specific names, they might conflict with each other.\n\nSocket binding is not the only way to use docker executor with GitLab runner. There is also another way called Docker in Docker aka dind which we are going to discuss in our next post.\n\nBuild and Deploy your project using Docker Executor\n\nNow you can add a to your respository and can try to build your project using docker.\n\nWe tried to build our project using\n\nThe same way you can login to a container registry using and then push your docker build image to using\n\nAs we discussed earlier that using created child containers on the host docker engine which could have some potential issues. To avoid using inside a CI job, there is another way called Docker-In-Docker where job’s containers will be creating as a child containers of another service container (Docker-in-Docker) instead of directly on the host docker engine.\n\nThank you for making it till the end 🎉\n\nIf this article helped you, make sure to:"
    },
    {
        "link": "https://stackoverflow.com/questions/28490874/docker-run-image-multiple-commands",
        "document": "I'm trying to run MULTIPLE commands like this.\n\nBut this gives me \"No such file or directory\" error because it is interpreted as...\n\nIt seems that some ESCAPE characters like \"\" or () are needed.\n\nSo I also tried\n\nI have searched for Docker Run Reference but have not find any hints about ESCAPE characters."
    },
    {
        "link": "https://stackoverflow.com/questions/33416286/how-to-run-2-commands-with-docker-exec",
        "document": "WARNING: This is advanced++ bash programming hack, not easy to understand or read for beginner.\n\nbash (on the host) has a real hacking feature by using ...\n\nAfter having a look at the documentation above, I see it's not well documented, nor at reading ... So this is a real hidden hack!\n\n: It displays the source code of the given function. Which is very well suitable for remote definition of the same function.\n\nOh... you think... you mean: « I can export my local function to the remote container?»\n\nCherry on the cake: you get all quoting / escaping quotes done magically by bash within string expansion.\n\nAssuming the remote exec environment could parse it: a container without bash wont be able to evaluate bash specific syntax. Nor it could exec command not installed. (Though you could install them on the fly.)\n\nWith that, you can hack remote call, and perform 2, or complex calls.\n\nLets imaging, you want get the list of user, that don't have the sub-folder in their home:\n\nFirst let's get the list of folder in\n\nA more real list could be more based on user. As folder in may be mounted and not related to a user's home.\n\nAnd now, let's combine all those commands, first multi-line code:\n\nThis is not exactly as simple a putting semi-colon at each end-of-line\n\ntips: can help you to achieve it. Left as an exercise for future hacker.\n\non my container it looks like:\n\nour crafted oneliner is becoming a nice small script.\n\nSo let's save it, on the host as a nice function\n\nlet's have it defined on the host\n\ntest it with our new hacker tool\n\nexec that cool function on the container\n\nis the trick you've learnt from @Solx\n\ndouble-quote mandatory for shell expansion (on host side before to be sent to the container)\n\nRemember I said, bash will handle quote escaping for you? That will happen right here.\n\nthe semi-colon split the two things:\n\nYes it works through ssh too.\n\n Yes, function could have argument, yes multiple functions could exported that way..."
    },
    {
        "link": "https://reddit.com/r/docker/comments/18lcvcj/best_practices_for_multiple_consecutive_commands",
        "document": "Hi! I'm new to docker and still figuring things out. I've just set up a compose.yaml with restic for backups of a server, and then I'm going to have cron periodically call a script that runs backup, cleanup, and verify in sequence. The default entrypoint of the container is the restic command itself, so currently to run multiple restic commands I'm calling multiple times:\n\nHowever, it feels inefficient to spin up a new container for each command in sequence, which I believe is what this does. What's the best practice for something like this? Can I start the container once and then send multiple commands to it? Or should I change the entrypoint to /bin/sh and then just string everything together in one big docker run?"
    },
    {
        "link": "https://docs.docker.com/engine/containers/multi-service_container",
        "document": "A container's main running process is the and/or at the end of the . It's best practice to separate areas of concern by using one service per container. That service may fork into multiple processes (for example, Apache web server starts multiple worker processes). It's ok to have multiple processes, but to get the most benefit out of Docker, avoid one container being responsible for multiple aspects of your overall application. You can connect multiple containers using user-defined networks and shared volumes.\n\nThe container's main process is responsible for managing all processes that it starts. In some cases, the main process isn't well-designed, and doesn't handle \"reaping\" (stopping) child processes gracefully when the container exits. If your process falls into this category, you can use the option when you run the container. The flag inserts a tiny init-process into the container as the main process, and handles reaping of all processes when the container exits. Handling such processes this way is superior to using a full-fledged init process such as or to handle process lifecycle within your container.\n\nIf you need to run more than one service within a container, you can achieve this in a few different ways.\n\nPut all of your commands in a wrapper script, complete with testing and debugging information. Run the wrapper script as your . The following is a naive example. First, the wrapper script:\n\nIf you have one main process that needs to start first and stay running but you temporarily need to run some other processes (perhaps to interact with the main process) then you can use bash's job control. First, the wrapper script:\n\n# Start the primary process and put it in the background # the my_helper_process might need to know how to wait on the # primary process to start before it does its work and returns # now we bring the primary process back into the foreground\n\nUse a process manager like . This is more involved than the other options, as it requires you to bundle and its configuration into your image (or base your image on one that includes ), along with the different applications it manages. Then you start , which manages your processes for you.\n\nThe following Dockerfile example shows this approach. The example assumes that these files exist at the root of the build context:\n\nIf you want to make sure both processes output their and to the container logs, you can add the following to the file:"
    },
    {
        "link": "https://serverfault.com/questions/685697/multiple-commands-in-docker-cmd-directive",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    }
]