[
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-to-sample",
        "document": "The texture's template type, which may be a single- or multi-component vector. The format is based on the texture's DXGI_FORMAT.\n\nThis function is supported in the following shader models.\n• TextureCubeArray is available in Shader Model 4.1 or higher.\n• Shader Model 4.1 is available in Direct3D 10.1 or higher.\n\nThis partial code example is based on the BasicHLSL11.fx file in the BasicHLSL11 Sample.\n\nTexture sampling uses the texel position to look up a texel value. An offset can be applied to the position before lookup. The sampler state contains the sampling and filtering options. This method can be invoked within a pixel shader, but it is not supported in a vertex shader or a geometry shader.\n\nUse an offset only at an integer miplevel; otherwise, you may get different results depending on hardware implementation or driver settings.\n\nTexture coordinates are floating-point values that reference texture data, which is also known as normalized texture space. Address wrapping modes are applied in this order (texture coordinates + offsets + wrap mode) to modify texture coordinates outside the [0...1] range.\n\nFor texture arrays, an additional value in the location parameter specifies an index into a texture array. This index is treated as a scaled float value (instead of the normalized space for standard texture coordinates). The conversion to an integer index is done in the following order (float + round-to-nearest-even integer + clamp to the array range).\n\nThe offset parameter modifies the texture coordinates, in texel space. Even though texture coordinates are normalized floating-point numbers, the offset applies an integer offset. Also note that the texture offsets need to be static.\n\nThe data format returned is determined by the texture format. For example, if the texture resource was defined with the DXGI_FORMAT_A8B8G8R8_UNORM_SRGB format, the sampling operation converts sampled texels from gamma 2.0 to 1.0, filter, and writes the result as a floating-point value in the range [0..1]."
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-writing-shaders-9",
        "document": "When in operation, a programmable vertex shader replaces the vertex processing done by the Microsoft Direct3D graphics pipeline. While using a vertex shader, state information regarding transformation and lighting operations is ignored by the fixed function pipeline. When the vertex shader is disabled and fixed function processing is returned, all current state settings apply.\n\nTessellation of high-order primitives should be done before the vertex shader executes. Implementations that perform surface tessellation after the shader processing must do so in a way that is not apparent to the application and shader code.\n\nAs a minimum, a vertex shader must output vertex position in homogeneous clip space. Optionally, the vertex shader can output texture coordinates, vertex color, vertex lighting, fog factors, and so on.\n\nPixel processing is performed by pixel shaders on individual pixels. Pixel shaders work in concert with vertex shaders; the output of a vertex shader provides the inputs for a pixel shader. Other pixel operations (fog blending, stencil operations, and render-target blending) occur after execution of the shader.\n\nA pixel shader completely replaces the pixel-blending functionality specified by the multi-texture blender including operations previously defined by the texture stage states. Texture sampling and filtering operations which were controlled by the standard texture stage states for minification, magnification, mip filtering, and the wrap addressing modes, can be initialized in shaders. The application is free to change these states without requiring the regeneration of the currently bound shader. Setting state can be made even easier if your shaders are designed within an effect.\n\nFor pixel shader versions ps_1_1 - ps_2_0, diffuse and specular colors are saturated (clamped) in the range 0 to 1 before use by the shader.\n\nColor values input to the pixel shader are assumed to be perspective correct, but this is not guaranteed (for all hardware). Colors sampled from texture coordinates are iterated in a perspective correct manner, and are clamped to the 0 to 1 range during iteration.\n\nFor pixel shader versions ps_1_1 - ps_1_4, the result emitted by the pixel shader is the contents of register r0. Whatever it contains when the shader completes processing is sent to the fog stage and render-target blender.\n\nFor pixel shader versions ps_2_0 and above, output color is emitted from oC0 - oC4.\n\nThe simplest variable declaration includes a type and a variable name, such as this floating-point declaration:\n\nYou can initialize a variable in the same statement.\n\nAn array of variables can be declared,\n\nor declared and initialized in the same statement.\n\nHere are a few declarations that demonstrate many of the characteristics of high-level shader language (HLSL) variables:\n\nData declarations can use any valid type including:\n\nA shader can have top-level variables, arguments, and functions.\n\nTop-level variables are declared outside of all functions. Top-level arguments are parameters to a top-level function. A top-level function is any function called by the application (as opposed to a function that is called by another function).\n\nVertex and pixel shaders accept two kinds of input data: varying and uniform. The varying input is the data that is unique to each execution of the shader. For a vertex shader, the varying data (for example: position, normal, etc.) comes from the vertex streams. The uniform data (for example: material color, world transform, etc.) is constant for multiple executions of a shader. For those familiar with the assembly shader models, uniform data is specified by constant registers and varying data by the v and t registers.\n\nUniform data can be specified by two methods. The most common method is to declare global variables and use them within a shader. Any use of global variables within a shader will result in adding that variable to the list of uniform variables required by that shader. The second method is to mark an input parameter of the top-level shader function as uniform. This marking specifies that the given variable should be added to the list of uniform variables.\n\nUniform variables used by a shader are communicated back to the application via the constant table. The constant table is the name for the symbol table that defines how the uniform variables used by a shader fit into the constant registers. The uniform function parameters appear in the constant table prepended with a dollar sign ($), unlike the global variables. The dollar sign is required to avoid name collisions between local uniform inputs and global variables of the same name.\n\nThe constant table contains the constant register locations of all uniform variables used by the shader. The table also includes the type information and the default value, if specified.\n\nVarying input parameters (of a top-level shader function) must be marked either with a semantic or uniform keyword indicating the value is constant for the execution of the shader. If a top-level shader input is not marked with a semantic or uniform keyword, then the shader will fail to compile.\n\nThe input semantic is a name used to link the given input to an output of the previous part of the graphics pipeline. For example, the input semantic POSITION0 is used by the vertex shaders to specify where the position data from the vertex buffer should be linked.\n\nPixel and vertex shaders have different sets of input semantics due to the different parts of the graphics pipeline that feed into each shader unit. Vertex shader input semantics describe the per-vertex information (for example: position, normal, texture coordinates, color, tangent, binormal, etc.) to be loaded from a vertex buffer into a form that can be consumed by the vertex shader. The input semantics directly map to the vertex declaration usage and the usage index.\n\nPixel shader input semantics describe the information that is provided per pixel by the rasterization unit. The data is generated by interpolating between outputs of the vertex shader for each vertex of the current primitive. The basic pixel shader input semantics link the output color and texture coordinate information to input parameters.\n\nInput semantics can be assigned to shader input by two methods:\n• Appending a colon and the semantic name to the parameter declaration.\n• Defining an input structure with input semantics assigned to each structure member.\n\nVertex and pixel shaders provide output data to the subsequent graphics pipeline stage. Output semantics are used to specify how data generated by the shader should be linked to the inputs of the next stage. For example, the output semantics for a vertex shader are used to link the outputs of the interpolators in the rasterizer to generate the input data for the pixel shader. The pixel shader outputs are the values provided to the alpha blending unit for each of the render targets or the depth value written to the depth buffer.\n\nVertex shader output semantics are used to link the shader both to the pixel shader and to the rasterizer stage. A vertex shader that is consumed by the rasterizer and not exposed to the pixel shader must generate position data as a minimum. Vertex shaders that generate texture coordinate and color data provide that data to a pixel shader after interpolation is done.\n\nPixel shader output semantics bind the output colors of a pixel shader with the correct render target. The pixel shader output color is linked to the alpha blend stage, which determines how the destination render targets are modified. The pixel shader depth output can be used to change the destination depth values at the current raster location. The depth output and multiple render targets are only supported with some shader models.\n\nThe syntax for output semantics is identical to the syntax for specifying input semantics. The semantics can be either specified directly on parameters declared as \"out\" parameters or assigned during the definition of a structure that either returned as an \"out\" parameter or the return value of a function.\n\nSemantics identify where data comes from. Semantics are optional identifiers that identify shader inputs and outputs. Semantics appear in one of three places:\n• After an argument in a function's input argument list.\n\nThis example uses a structure to provide one or more vertex shader inputs, and another structure to provide one or more vertex shader outputs. Each of the structure members uses a semantic.\n\nThe input structure identifies the data from the vertex buffer that will provide the shader inputs. This shader maps the data from the position, normal, and blendweight elements of the vertex buffer into vertex shader registers. The input data type does not have to exactly match the vertex declaration data type. If it doesn't exactly match, the vertex data will automatically be converted into the HLSL's data type when it is written into the shader registers. For instance, if the normal data were defined to be of type UINT by the application, it would be converted into a float3 when read by the shader.\n\nIf the data in the vertex stream contains fewer components than the corresponding shader data type, the missing components will be initialized to 0 (except for w, which is initialized to 1).\n\nInput semantics are similar to the values in the D3DDECLUSAGE.\n\nThe output structure identifies the vertex shader output parameters of position and color. These outputs will be used by the pipeline for triangle rasterization (in primitive processing). The output marked as position data denotes the position of a vertex in homogeneous space. As a minimum, a vertex shader must generate position data. The screen space position is computed after the vertex shader completes by dividing the (x, y, z) coordinate by w. In screen space, -1 and 1 are the minimum and maximum x and y values of the boundaries of the viewport, while z is used for z-buffer testing.\n\nOutput semantics are also similar to the values in D3DDECLUSAGE. In general, an output structure for a vertex shader can also be used as the input structure for a pixel shader, provided the pixel shader does not read from any variable marked with the position, point size, or fog semantics. These semantics are associated with per-vertex scalar values that are not used by a pixel shader. If these values are needed for the pixel shader, they can be copied into another output variable that uses a pixel shader semantic.\n\nGlobal variables are assigned to registers automatically by the compiler. Global variables are also called uniform parameters because the contents of the variable is the same for all pixels processed each time the shader is called. The registers are contained in the constant table, which can be read using the ID3DXConstantTable interface.\n\nInput semantics for pixel shaders map values into specific hardware registers for transport between vertex shaders and pixel shaders. Each register type has specific properties. Because there are currently only two semantics for color and texture coordinates, it is common for most data to be marked as a texture coordinate even when it is not.\n\nNotice that the vertex shader output structure used an input with position data, which is not used by the pixel shader. HLSL allows valid output data of a vertex shader that is not valid input data for a pixel shader, provided that it is not referenced in the pixel shader.\n\nInput arguments can also be arrays. Semantics are automatically incremented by the compiler for each element of the array. For instance, consider the following explicit declaration:\n\nThe explicit declaration given above is equivalent to the following declaration that will have semantics automatically incremented by the compiler:\n\nJust like input semantics, output semantics identify data usage for pixel shader output data. Many pixel shaders write to only one output color. Pixel shaders can also write out a depth value into one or more multiple render targets at the same time (up to four). Like vertex shaders, pixel shaders use a structure to return more than one output. This shader writes 0 to the color components, as well as to the depth component.\n\nPixel shader output colors must be of type float4. When writing multiple colors, all output colors must be used contiguously. In other words, COLOR1 cannot be an output unless COLOR0 has already been written. Pixel shader depth output must be of type float1.\n\nA sampler contains sampler state. Sampler state specifies the texture to be sampled, and controls the filtering that is done during sampling. Three things are required to sample a texture:\n\nSamplers can be initialized with textures and sampler state as shown here:\n\nHere's an example of the code to sample a 2D texture:\n\nThe texture is declared with a texture variable tex0.\n\nIn this example, a sampler variable named s_2D is declared. The sampler contains the sampler state inside of curly braces. This includes the texture that will be sampled and, optionally, the filter state (that is, wrap modes, filter modes, etc.). If the sampler state is omitted, a default sampler state is applied specifying linear filtering and a wrap mode for the texture coordinates. The sampler function takes a two-component floating-point texture coordinate, and returns a two-component color. This is represented with the float2 return type and represents data in the red and green components.\n\nFour types of samplers are defined (see Keywords) and texture lookups are performed by the intrinsic functions: tex1D(s, t) (DirectX HLSL), tex2D(s, t) (DirectX HLSL), tex3D(s, t) (DirectX HLSL), texCUBE(s, t) (DirectX HLSL). Here is an example of 3D sampling:\n\nThis sampler declaration uses default sampler state for the filter settings and address mode.\n\nHere is the corresponding cube sampling example:\n\nAnd finally, here is the 1D sampling example:\n\nBecause the runtime does not support 1D textures, the compiler will use a 2D texture with the knowledge that the y-coordinate is unimportant. Since tex1D(s, t) (DirectX HLSL) is implemented as a 2D texture lookup, the compiler is free to choose the y-component in an efficient manner. In some rare scenarios, the compiler cannot choose an efficient y-component, in which case it will issue a warning.\n\nThis particular example is inefficient because the compiler must move the input coordinate into another register (because a 1D lookup is implemented as a 2D lookup and the texture coordinate is declared as a float1). If the code is rewritten using a float2 input instead of a float1, the compiler can use the input texture coordinate because it knows that y is initialized to something.\n\nAll texture lookups can be appended with \"bias\" or \"proj\" (that is, tex2Dbias (DirectX HLSL), texCUBEproj (DirectX HLSL)). With the \"proj\" suffix, the texture coordinate is divided by the w-component. With \"bias,\" the mip level is shifted by the w-component. Thus, all texture lookups with a suffix always take a float4 input. tex1D(s, t) (DirectX HLSL) and tex2D(s, t) (DirectX HLSL) ignore the yz- and z-components respectively.\n\nSamplers may also be used in array, although no back end currently supports dynamic array access of samplers. Therefore, the following is valid because it can be resolved at compile time:\n\nHowever, this example is not valid.\n\nDynamic access of samplers is primarily useful for writing programs with literal loops. The following code illustrates sampler array accessing:\n\nFunctions break large tasks into smaller ones. Small tasks are easier to debug and can be reused, once proven. Functions can be used to hide details of other functions, which makes a program composed of functions easier to follow.\n\nHLSL functions are similar to C functions in several ways: They both contain a definition and a function body and they both declare return types and argument lists. Like C functions, HLSL validation does type checking on the arguments, argument types, and the return value during shader compilation.\n\nUnlike C functions, HLSL entry point functions use semantics to bind function arguments to shader inputs and outputs (HLSL functions called internally ignore semantics). This makes it easier to bind buffer data to a shader, and bind shader outputs to shader inputs.\n\nA function contains a declaration and a body, and the declaration must precede the body.\n\nThe function declaration includes everything in front of the curly braces:\n\nThe return type can be any of the HLSL basic data types such as a float4:\n\nThe return type can be a structure that has already been defined:\n\nIf the function does not return a value, void can be used as the return type.\n\nThe return type always appears first in a function declaration.\n\nAn argument list declares the input arguments to a function. It may also declare values that will be returned. Some arguments are both input and output arguments. Here is an example of a shader that takes four input arguments.\n\nThis function returns a final color, that is a blend of a texture sample and the light color. The function takes four inputs. Two inputs have semantics: LightDir has the TEXCOORD1 semantic, and texcrd has the TEXCOORD0 semantic. The semantics mean that the data for these variables will come from the vertex buffer. Even though the LightDir variable has a TEXCOORD1 semantic, the parameter is probably not a texture coordinate. The TEXCOORDn semantic type is often used to supply a semantic for a type that is not predefined (there is no vertex shader input semantic for a light direction).\n\nThe other two inputs LightColor and samp are labeled with the uniform keyword. These are uniform constants that will not change between draw calls. The values for these parameters come from shader global variables.\n\nArguments can be labeled as inputs with the in keyword, and output arguments with the out keyword. Arguments cannot be passed by reference; however, an argument can be both an input and an output if it is declared with the inout keyword. Arguments passed to a function that are marked with the inout keyword are considered copies of the original until the function returns, and they are copied back. Here's an example using inout:\n\nThis function increments the values in A and B and returns them.\n\nThe function body is all of the code after the function declaration.\n\nThe body consists of statements which are surrounded by curly braces. The function body implements all of the functionality using variables, literals, expressions, and statements.\n\nThe shader body does two things: it performs a matrix multiply and returns a float4 result. The matrix multiply is accomplished with the mul (DirectX HLSL) function, which performs a 4x4 matrix multiply. mul (DirectX HLSL) is called an intrinsic function because it is already built into the HLSL library of functions. Intrinsic functions will be covered in more detail in the next section.\n\nThe matrix multiply combines an input vector Pos and a composite matrix WorldViewProj. The result is position data transformed into screen space. This is the minimum vertex shader processing we can do. If we were using the fixed function pipeline instead of a vertex shader, the vertex data could be drawn after doing this transform.\n\nThe last statement in a function body is a return statement. Just like C, this statement returns control from the function to the statement that called the function.\n\nFunction return types can be any of the simple data types defined in HLSL, including bool, int half, float, and double. Return types can be one of the complex data types such as vectors and matrices. HLSL types that refer to objects cannot be used as return types. This includes pixelshader, vertexshader, texture, and sampler.\n\nHere is an example of a function that uses a structure for a return type.\n\nThe float4 return type has been replaced with the structure VS_OUTPUT, which now contains a single float4 member.\n\nA return statement signals the end of a function. This is the simplest return statement. It returns control from the function to the calling program. It returns no value.\n\nA return statement can return one or more values. This example returns a literal value:\n\nThis example returns the scalar result of an expression:\n\nThis example returns a float4 constructed from a local variable and a literal:\n\nThis example returns a float4 that is constructed from the result returned from an intrinsic function, and a few literal values:\n\nThis example returns a structure that contains one or more members:\n\nMost current vertex and pixel shader hardware is designed to run a shader line by line, executing each instruction once. HLSL supports flow control, which includes static branching, predicated instructions, static looping, dynamic branching, and dynamic looping.\n\nPreviously, using an if statement resulted in assembly-language shader code that implements both the if side and the else side of the code flow. Here is an example of the in HLSL code that was compiled for vs_1_1:\n\nAnd here is the resulting assembly code:\n\nSome hardware allows for either static or dynamic looping, but most require linear execution. On the models that do not support looping, all loops must be unrolled. An example is the DepthOfField Sample sample that uses unrolled loops even for ps_1_1 shaders.\n\nHLSL now includes support for each of these types of flow control:\n\nStatic branching allows blocks of shader code to be switched on or off based on a Boolean shader constant. This is a convenient method for enabling or disabling code paths based on the type of object currently being rendered. Between draw calls, you can decide which features you want to support with the current shader and then set the Boolean flags required to get that behavior. Any statements that are disabled by a Boolean constant are skipped during shader execution.\n\nThe most familiar branching support is dynamic branching. With dynamic branching, the comparison condition resides in a variable, which means that the comparison is done for each vertex or each pixel at run time (as opposed to the comparison occurring at compile time, or between two draw calls). The performance hit is the cost of the branch plus the cost of the instructions on the side of the branch taken. Dynamic branching is implemented in shader model 3 or higher. Optimizing shaders that work with these models is similar to optimizing code that runs on a CPU."
    },
    {
        "link": "https://docs.unity3d.com/6000.0/Documentation/Manual/writing-shader-writing-shader-programs-hlsl.html",
        "document": "Resources for writing HLSL shaderA program that runs on the GPU. More info\n\nSee in Glossary programs inside a block in a custom ShaderLabUnity’s language for defining the structure of Shader objects. More info\n\nSee in Glossary shader."
    },
    {
        "link": "https://xoxor4d.github.io/tutorials/hlsl-techniques",
        "document": "Techniques define shaders, shader-inputs and statemaps that are being used for your current pass. Always make sure that every constant you pass into your shader is actively being used within your shader and referenced by your material template , otherwise your shader will fail to compile.\n\n You can mix up Vertex- and PixelShaders as long as the VertexShader ouputs the information that the PixelShader needs. You could also use a stock Vertex- with a custom PixelShader if you so wish, but its somewhat impractical. Multiple Shader-passes are also supported, but more on that later. Lets have a look at a technique that is using 2 textures (referencing the dynamic material template that was shown here: Material Templates - In-depth)\n\n// defines the statemap to be used for the current pass >> root\\raw\\statemaps // custom shader to be used :: shader name (without \"vs_3_0_\" suffix and .ext) // you only need to append the \".hlsl\" extension when you want to use stock shaders // assign material parameters to pre-defined global shader constants in here (defined in shader_vars.h) // eg. assign the distortionScale set within AssetManager and exposed by your material template: // assign material parameters to pre-defined global shader constants in here (defined in shader_vars.h) // ^ same for the second \"colorMap\" that was applied to the specularMap slot // assign code-variables exposed by the engine to shader semantics // the UV of the current Vertex\n\nSamplers are defined in root\\raw\\shader_bin\\shader_src\\shader_vars.h and either mapped to constant registers or mapped dynamically.\n\n Each sampler type uses different hard-coded sampler stats that define how it samples the given input image (eg. texture filtering or sampling technique). \n\n I advice you to use a sampler that fits your input but you can obv. experiment with them if you want.\n\nTextures exposed by code are not always valid. They represent the different Rendertargets that the game uses to render \n\n to off-screen “textures” and are mostly used for post-process effects but also for depth, shadows, lightmaps etc.\n\n Most of them are only valid when the specific use-case (in code), that they are used for, is active.\n\n Rendering to code-textures can only be done via engine modifications so these can only be read from.\n\nLike mentioned earlier, you pass material parameters into your shader by assigning them to pre-defined, global shader variables (defined in shader_vars.h). The same applies to textures that you assign to samplers.\n\nThere are also variables exposed by code, that hold information about the current vertex within the pipeline. Vertex-information that you need within your shader needs to be put into its intended [Semantic] defined at the bottom of your technique."
    },
    {
        "link": "https://vfxdoc.readthedocs.io/en/latest/shaders/hlsl",
        "document": "This language is close to the C language, and uses a reduced instruction set dedicated to GPU computing. Depending on the engine you use, shaders file contents can vary but you will find here the common denonimator for writing hlsl.\n\nAuthoring shaders can take many forms in terms of tools, depending on the engine you are working with, but the two main ways to author them are : text-based (the old-fashioned way) and using a node-based shader editor (most useful for artists).\n\nThese two methods have their pros and cons but the learning curve of text-based shader editing is way more daunting than a graphical approach, but instead gives a better understanding and more possibilities than a node-based graph.\n\nThe main advantage of using a node-based shader editor lies in the preview and the experimental context it enables for the user. Most of the node-based shader tools that know success enable per-node preview and this is precisely where the power come from. Prototyping made easy. A single tree can be reused and modified, and all the intermediate steps update accordingly. This is a must to learn how shading works, because it's visual, and visual is the way artists understand more how stuff works.\n\nHowever, in production node-based tools have proven poor maintenance, especially if the shaders are the fruit of N iterations of trial and error. Refactoring and understanding a tentacular graph can become nearly impossible when in production.\n\nLet's start with a base template that displays a textured mesh with a tiling texture which color is multiplied by vertex colors:\n\nBase declarations are in our example, arbitrary. It is not always the case but in our example we used a statement to tell the compiler that :\n• the pixel shader (ps) function will be called\n• the vertex shader (vs) function will be called\n\nand are arbitrary in the example and can be different from one engine to another. For example Unity uses with the use of keyword to define the vertex shader, and for a pixel shader and for its specific, high-level surface definition.\n\nSo if you take a look at the end of the example you will see\n\nand that are declarations of the vertex and pixel shader.\n\nStructures are declared most of the time using a statement with structure element in enclosed braces. In the example we define two structs and (vertex-to-pixel) that will contain the data to pass from one shader function to another.\n\nStructures are declaring elements of a given type that are passed through a specific data channel, defined by a semantic : a keyword that describes the channel where the data is passed.\n\nFor the structure we have 3 elements:\n• position which is a vector of 3 float values (X,Y,Z) passed to the POSITION semantic which represents the vertex position.\n• uv, vector of 2 components (U,V) passed to the TEXCOORD0 semantic, corresponding to the first UV channel.\n• color, vector of 3 components (R,G,B) passed to the COLOR semantic, corresponding to the vertex color channel.\n\nFor the case of an vertex declaration, these semantics correspond to the data contained into the mesh. But when it comes to declare a structure of data that will be used between the vertex shader and pixel shader, the semantics are not the same. In this case, we need to use the semantic for the screen-space position (projected position of the vertex on screen), then any TEXCOORD(N) semantic for other values such as UVs or color.\n\nPassing data through these semantics will tell the rasterizer that, for every pixel of the triangle, the value need to be interpolated (for example vertex color interpolated from every vertex of a triangle)\n\nTEXCOORD(N) is limited though so you cannot use an infinity of values passed from vertex to pixel. Every channel can contain from 1 to 4 channels (float, float2, float3 or float4).\n\nIt is utterly interesting to use these interpolants to benefit from automatic vertex interpolation (for UVs, normals, colors, etc.)\n\nUniforms are the external parameters of the shader and are called uniforms because they do not vary from one vertex to another, or from one pixel to another, they are .\n\nIn a material system, some of these values will be exposed so the user can set them.\n\nIn our example we have 3 uniforms for our shader:\n\nThese are declared at the base level of the code, not in a function block (in this case they would be variables local to the scope of the enclosing braces they are in).\n• MyTexture is a sampler2D that can be used to read its texture using a function\n• UVTile is a float2 parameter that will be used to set the tiling ratio for the texture.\n• worldViewProjection is a matrix of 4 by 4 elements that the engine shall use to project the model on screen (basically the transformation matrix that will place the object in the world, and look at it using a camera so it gets projected on screen).\n\nThe vertex shader is the function that will take all the vertices of the mesh and will transform them to screen-space projected positions, so the rasterizer can define which pixels on screen have to be drawn for this model.\n\nIn this function we do the following:\n• we transform the model vertices from local space to screen-space using the worldViewProjection matrix.\n• we transform the UV by scaling them by the UVTile scale factor, so we can tile the texture when we will read it.\n• we pass the vertex color to the pixel shader without modification\n\nPixel shader function is executed after all triangles are rasterized on screen, and for each pixel will compute the output color for it.\n\nThis function will output a float4 value (color + alpha) and uses the semantic SV_TARGET that corresponds to a single render target (often, the backbuffer) where the pixels will be drawn\n\nIn this function we do the following:\n• we store locally a float4 named color containing the result of a texture read ( ) using the we just scaled in the vertex shader, and using the uniform\n• we return a color that corresponds to the value of the color previously read, multiplied by the vertex color"
    },
    {
        "link": "https://gamedev.stackexchange.com/questions/95736/hlsl-pixel-shader-color-depending-on-condition",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-writing-shaders-9",
        "document": "When in operation, a programmable vertex shader replaces the vertex processing done by the Microsoft Direct3D graphics pipeline. While using a vertex shader, state information regarding transformation and lighting operations is ignored by the fixed function pipeline. When the vertex shader is disabled and fixed function processing is returned, all current state settings apply.\n\nTessellation of high-order primitives should be done before the vertex shader executes. Implementations that perform surface tessellation after the shader processing must do so in a way that is not apparent to the application and shader code.\n\nAs a minimum, a vertex shader must output vertex position in homogeneous clip space. Optionally, the vertex shader can output texture coordinates, vertex color, vertex lighting, fog factors, and so on.\n\nPixel processing is performed by pixel shaders on individual pixels. Pixel shaders work in concert with vertex shaders; the output of a vertex shader provides the inputs for a pixel shader. Other pixel operations (fog blending, stencil operations, and render-target blending) occur after execution of the shader.\n\nA pixel shader completely replaces the pixel-blending functionality specified by the multi-texture blender including operations previously defined by the texture stage states. Texture sampling and filtering operations which were controlled by the standard texture stage states for minification, magnification, mip filtering, and the wrap addressing modes, can be initialized in shaders. The application is free to change these states without requiring the regeneration of the currently bound shader. Setting state can be made even easier if your shaders are designed within an effect.\n\nFor pixel shader versions ps_1_1 - ps_2_0, diffuse and specular colors are saturated (clamped) in the range 0 to 1 before use by the shader.\n\nColor values input to the pixel shader are assumed to be perspective correct, but this is not guaranteed (for all hardware). Colors sampled from texture coordinates are iterated in a perspective correct manner, and are clamped to the 0 to 1 range during iteration.\n\nFor pixel shader versions ps_1_1 - ps_1_4, the result emitted by the pixel shader is the contents of register r0. Whatever it contains when the shader completes processing is sent to the fog stage and render-target blender.\n\nFor pixel shader versions ps_2_0 and above, output color is emitted from oC0 - oC4.\n\nThe simplest variable declaration includes a type and a variable name, such as this floating-point declaration:\n\nYou can initialize a variable in the same statement.\n\nAn array of variables can be declared,\n\nor declared and initialized in the same statement.\n\nHere are a few declarations that demonstrate many of the characteristics of high-level shader language (HLSL) variables:\n\nData declarations can use any valid type including:\n\nA shader can have top-level variables, arguments, and functions.\n\nTop-level variables are declared outside of all functions. Top-level arguments are parameters to a top-level function. A top-level function is any function called by the application (as opposed to a function that is called by another function).\n\nVertex and pixel shaders accept two kinds of input data: varying and uniform. The varying input is the data that is unique to each execution of the shader. For a vertex shader, the varying data (for example: position, normal, etc.) comes from the vertex streams. The uniform data (for example: material color, world transform, etc.) is constant for multiple executions of a shader. For those familiar with the assembly shader models, uniform data is specified by constant registers and varying data by the v and t registers.\n\nUniform data can be specified by two methods. The most common method is to declare global variables and use them within a shader. Any use of global variables within a shader will result in adding that variable to the list of uniform variables required by that shader. The second method is to mark an input parameter of the top-level shader function as uniform. This marking specifies that the given variable should be added to the list of uniform variables.\n\nUniform variables used by a shader are communicated back to the application via the constant table. The constant table is the name for the symbol table that defines how the uniform variables used by a shader fit into the constant registers. The uniform function parameters appear in the constant table prepended with a dollar sign ($), unlike the global variables. The dollar sign is required to avoid name collisions between local uniform inputs and global variables of the same name.\n\nThe constant table contains the constant register locations of all uniform variables used by the shader. The table also includes the type information and the default value, if specified.\n\nVarying input parameters (of a top-level shader function) must be marked either with a semantic or uniform keyword indicating the value is constant for the execution of the shader. If a top-level shader input is not marked with a semantic or uniform keyword, then the shader will fail to compile.\n\nThe input semantic is a name used to link the given input to an output of the previous part of the graphics pipeline. For example, the input semantic POSITION0 is used by the vertex shaders to specify where the position data from the vertex buffer should be linked.\n\nPixel and vertex shaders have different sets of input semantics due to the different parts of the graphics pipeline that feed into each shader unit. Vertex shader input semantics describe the per-vertex information (for example: position, normal, texture coordinates, color, tangent, binormal, etc.) to be loaded from a vertex buffer into a form that can be consumed by the vertex shader. The input semantics directly map to the vertex declaration usage and the usage index.\n\nPixel shader input semantics describe the information that is provided per pixel by the rasterization unit. The data is generated by interpolating between outputs of the vertex shader for each vertex of the current primitive. The basic pixel shader input semantics link the output color and texture coordinate information to input parameters.\n\nInput semantics can be assigned to shader input by two methods:\n• Appending a colon and the semantic name to the parameter declaration.\n• Defining an input structure with input semantics assigned to each structure member.\n\nVertex and pixel shaders provide output data to the subsequent graphics pipeline stage. Output semantics are used to specify how data generated by the shader should be linked to the inputs of the next stage. For example, the output semantics for a vertex shader are used to link the outputs of the interpolators in the rasterizer to generate the input data for the pixel shader. The pixel shader outputs are the values provided to the alpha blending unit for each of the render targets or the depth value written to the depth buffer.\n\nVertex shader output semantics are used to link the shader both to the pixel shader and to the rasterizer stage. A vertex shader that is consumed by the rasterizer and not exposed to the pixel shader must generate position data as a minimum. Vertex shaders that generate texture coordinate and color data provide that data to a pixel shader after interpolation is done.\n\nPixel shader output semantics bind the output colors of a pixel shader with the correct render target. The pixel shader output color is linked to the alpha blend stage, which determines how the destination render targets are modified. The pixel shader depth output can be used to change the destination depth values at the current raster location. The depth output and multiple render targets are only supported with some shader models.\n\nThe syntax for output semantics is identical to the syntax for specifying input semantics. The semantics can be either specified directly on parameters declared as \"out\" parameters or assigned during the definition of a structure that either returned as an \"out\" parameter or the return value of a function.\n\nSemantics identify where data comes from. Semantics are optional identifiers that identify shader inputs and outputs. Semantics appear in one of three places:\n• After an argument in a function's input argument list.\n\nThis example uses a structure to provide one or more vertex shader inputs, and another structure to provide one or more vertex shader outputs. Each of the structure members uses a semantic.\n\nThe input structure identifies the data from the vertex buffer that will provide the shader inputs. This shader maps the data from the position, normal, and blendweight elements of the vertex buffer into vertex shader registers. The input data type does not have to exactly match the vertex declaration data type. If it doesn't exactly match, the vertex data will automatically be converted into the HLSL's data type when it is written into the shader registers. For instance, if the normal data were defined to be of type UINT by the application, it would be converted into a float3 when read by the shader.\n\nIf the data in the vertex stream contains fewer components than the corresponding shader data type, the missing components will be initialized to 0 (except for w, which is initialized to 1).\n\nInput semantics are similar to the values in the D3DDECLUSAGE.\n\nThe output structure identifies the vertex shader output parameters of position and color. These outputs will be used by the pipeline for triangle rasterization (in primitive processing). The output marked as position data denotes the position of a vertex in homogeneous space. As a minimum, a vertex shader must generate position data. The screen space position is computed after the vertex shader completes by dividing the (x, y, z) coordinate by w. In screen space, -1 and 1 are the minimum and maximum x and y values of the boundaries of the viewport, while z is used for z-buffer testing.\n\nOutput semantics are also similar to the values in D3DDECLUSAGE. In general, an output structure for a vertex shader can also be used as the input structure for a pixel shader, provided the pixel shader does not read from any variable marked with the position, point size, or fog semantics. These semantics are associated with per-vertex scalar values that are not used by a pixel shader. If these values are needed for the pixel shader, they can be copied into another output variable that uses a pixel shader semantic.\n\nGlobal variables are assigned to registers automatically by the compiler. Global variables are also called uniform parameters because the contents of the variable is the same for all pixels processed each time the shader is called. The registers are contained in the constant table, which can be read using the ID3DXConstantTable interface.\n\nInput semantics for pixel shaders map values into specific hardware registers for transport between vertex shaders and pixel shaders. Each register type has specific properties. Because there are currently only two semantics for color and texture coordinates, it is common for most data to be marked as a texture coordinate even when it is not.\n\nNotice that the vertex shader output structure used an input with position data, which is not used by the pixel shader. HLSL allows valid output data of a vertex shader that is not valid input data for a pixel shader, provided that it is not referenced in the pixel shader.\n\nInput arguments can also be arrays. Semantics are automatically incremented by the compiler for each element of the array. For instance, consider the following explicit declaration:\n\nThe explicit declaration given above is equivalent to the following declaration that will have semantics automatically incremented by the compiler:\n\nJust like input semantics, output semantics identify data usage for pixel shader output data. Many pixel shaders write to only one output color. Pixel shaders can also write out a depth value into one or more multiple render targets at the same time (up to four). Like vertex shaders, pixel shaders use a structure to return more than one output. This shader writes 0 to the color components, as well as to the depth component.\n\nPixel shader output colors must be of type float4. When writing multiple colors, all output colors must be used contiguously. In other words, COLOR1 cannot be an output unless COLOR0 has already been written. Pixel shader depth output must be of type float1.\n\nA sampler contains sampler state. Sampler state specifies the texture to be sampled, and controls the filtering that is done during sampling. Three things are required to sample a texture:\n\nSamplers can be initialized with textures and sampler state as shown here:\n\nHere's an example of the code to sample a 2D texture:\n\nThe texture is declared with a texture variable tex0.\n\nIn this example, a sampler variable named s_2D is declared. The sampler contains the sampler state inside of curly braces. This includes the texture that will be sampled and, optionally, the filter state (that is, wrap modes, filter modes, etc.). If the sampler state is omitted, a default sampler state is applied specifying linear filtering and a wrap mode for the texture coordinates. The sampler function takes a two-component floating-point texture coordinate, and returns a two-component color. This is represented with the float2 return type and represents data in the red and green components.\n\nFour types of samplers are defined (see Keywords) and texture lookups are performed by the intrinsic functions: tex1D(s, t) (DirectX HLSL), tex2D(s, t) (DirectX HLSL), tex3D(s, t) (DirectX HLSL), texCUBE(s, t) (DirectX HLSL). Here is an example of 3D sampling:\n\nThis sampler declaration uses default sampler state for the filter settings and address mode.\n\nHere is the corresponding cube sampling example:\n\nAnd finally, here is the 1D sampling example:\n\nBecause the runtime does not support 1D textures, the compiler will use a 2D texture with the knowledge that the y-coordinate is unimportant. Since tex1D(s, t) (DirectX HLSL) is implemented as a 2D texture lookup, the compiler is free to choose the y-component in an efficient manner. In some rare scenarios, the compiler cannot choose an efficient y-component, in which case it will issue a warning.\n\nThis particular example is inefficient because the compiler must move the input coordinate into another register (because a 1D lookup is implemented as a 2D lookup and the texture coordinate is declared as a float1). If the code is rewritten using a float2 input instead of a float1, the compiler can use the input texture coordinate because it knows that y is initialized to something.\n\nAll texture lookups can be appended with \"bias\" or \"proj\" (that is, tex2Dbias (DirectX HLSL), texCUBEproj (DirectX HLSL)). With the \"proj\" suffix, the texture coordinate is divided by the w-component. With \"bias,\" the mip level is shifted by the w-component. Thus, all texture lookups with a suffix always take a float4 input. tex1D(s, t) (DirectX HLSL) and tex2D(s, t) (DirectX HLSL) ignore the yz- and z-components respectively.\n\nSamplers may also be used in array, although no back end currently supports dynamic array access of samplers. Therefore, the following is valid because it can be resolved at compile time:\n\nHowever, this example is not valid.\n\nDynamic access of samplers is primarily useful for writing programs with literal loops. The following code illustrates sampler array accessing:\n\nFunctions break large tasks into smaller ones. Small tasks are easier to debug and can be reused, once proven. Functions can be used to hide details of other functions, which makes a program composed of functions easier to follow.\n\nHLSL functions are similar to C functions in several ways: They both contain a definition and a function body and they both declare return types and argument lists. Like C functions, HLSL validation does type checking on the arguments, argument types, and the return value during shader compilation.\n\nUnlike C functions, HLSL entry point functions use semantics to bind function arguments to shader inputs and outputs (HLSL functions called internally ignore semantics). This makes it easier to bind buffer data to a shader, and bind shader outputs to shader inputs.\n\nA function contains a declaration and a body, and the declaration must precede the body.\n\nThe function declaration includes everything in front of the curly braces:\n\nThe return type can be any of the HLSL basic data types such as a float4:\n\nThe return type can be a structure that has already been defined:\n\nIf the function does not return a value, void can be used as the return type.\n\nThe return type always appears first in a function declaration.\n\nAn argument list declares the input arguments to a function. It may also declare values that will be returned. Some arguments are both input and output arguments. Here is an example of a shader that takes four input arguments.\n\nThis function returns a final color, that is a blend of a texture sample and the light color. The function takes four inputs. Two inputs have semantics: LightDir has the TEXCOORD1 semantic, and texcrd has the TEXCOORD0 semantic. The semantics mean that the data for these variables will come from the vertex buffer. Even though the LightDir variable has a TEXCOORD1 semantic, the parameter is probably not a texture coordinate. The TEXCOORDn semantic type is often used to supply a semantic for a type that is not predefined (there is no vertex shader input semantic for a light direction).\n\nThe other two inputs LightColor and samp are labeled with the uniform keyword. These are uniform constants that will not change between draw calls. The values for these parameters come from shader global variables.\n\nArguments can be labeled as inputs with the in keyword, and output arguments with the out keyword. Arguments cannot be passed by reference; however, an argument can be both an input and an output if it is declared with the inout keyword. Arguments passed to a function that are marked with the inout keyword are considered copies of the original until the function returns, and they are copied back. Here's an example using inout:\n\nThis function increments the values in A and B and returns them.\n\nThe function body is all of the code after the function declaration.\n\nThe body consists of statements which are surrounded by curly braces. The function body implements all of the functionality using variables, literals, expressions, and statements.\n\nThe shader body does two things: it performs a matrix multiply and returns a float4 result. The matrix multiply is accomplished with the mul (DirectX HLSL) function, which performs a 4x4 matrix multiply. mul (DirectX HLSL) is called an intrinsic function because it is already built into the HLSL library of functions. Intrinsic functions will be covered in more detail in the next section.\n\nThe matrix multiply combines an input vector Pos and a composite matrix WorldViewProj. The result is position data transformed into screen space. This is the minimum vertex shader processing we can do. If we were using the fixed function pipeline instead of a vertex shader, the vertex data could be drawn after doing this transform.\n\nThe last statement in a function body is a return statement. Just like C, this statement returns control from the function to the statement that called the function.\n\nFunction return types can be any of the simple data types defined in HLSL, including bool, int half, float, and double. Return types can be one of the complex data types such as vectors and matrices. HLSL types that refer to objects cannot be used as return types. This includes pixelshader, vertexshader, texture, and sampler.\n\nHere is an example of a function that uses a structure for a return type.\n\nThe float4 return type has been replaced with the structure VS_OUTPUT, which now contains a single float4 member.\n\nA return statement signals the end of a function. This is the simplest return statement. It returns control from the function to the calling program. It returns no value.\n\nA return statement can return one or more values. This example returns a literal value:\n\nThis example returns the scalar result of an expression:\n\nThis example returns a float4 constructed from a local variable and a literal:\n\nThis example returns a float4 that is constructed from the result returned from an intrinsic function, and a few literal values:\n\nThis example returns a structure that contains one or more members:\n\nMost current vertex and pixel shader hardware is designed to run a shader line by line, executing each instruction once. HLSL supports flow control, which includes static branching, predicated instructions, static looping, dynamic branching, and dynamic looping.\n\nPreviously, using an if statement resulted in assembly-language shader code that implements both the if side and the else side of the code flow. Here is an example of the in HLSL code that was compiled for vs_1_1:\n\nAnd here is the resulting assembly code:\n\nSome hardware allows for either static or dynamic looping, but most require linear execution. On the models that do not support looping, all loops must be unrolled. An example is the DepthOfField Sample sample that uses unrolled loops even for ps_1_1 shaders.\n\nHLSL now includes support for each of these types of flow control:\n\nStatic branching allows blocks of shader code to be switched on or off based on a Boolean shader constant. This is a convenient method for enabling or disabling code paths based on the type of object currently being rendered. Between draw calls, you can decide which features you want to support with the current shader and then set the Boolean flags required to get that behavior. Any statements that are disabled by a Boolean constant are skipped during shader execution.\n\nThe most familiar branching support is dynamic branching. With dynamic branching, the comparison condition resides in a variable, which means that the comparison is done for each vertex or each pixel at run time (as opposed to the comparison occurring at compile time, or between two draw calls). The performance hit is the cost of the branch plus the cost of the instructions on the side of the branch taken. Dynamic branching is implemented in shader model 3 or higher. Optimizing shaders that work with these models is similar to optimizing code that runs on a CPU."
    },
    {
        "link": "https://docs.unity3d.com/6000.0/Documentation/Manual/SL-MultipleProgramVariants-make-conditionals.html",
        "document": "To mark parts of your shaderA program that runs on the GPU. More info\n\nSee in Glossary code conditional based on whether you enable or disable a shader keyword in a set, use an HLSL if statement.\n\nYou can enable and disable keywords using the Inspector or C# scripting.\n\nWhat Unity does with your shader code depends on which shader directive you use.\n\nIf you use , Unity creates a uniform integer variable for each keyword. When you enable a keyword, Unity sets the integer for that variable to , and your GPU switches to using the code in the statement for that keyword. This is dynamic branching.\n\nIf you use or , Unity creates separate shader variantsA verion of a shader program that Unity generates according to a specific combination of shader keywords and their status. A Shader object can contain multiple shader variants. More info\n\nSee in Glossary for each keyword state. Each variant contains the code from an branch for that keyword. When you enable a keyword, Unity sends the matching variant to your GPU. This is static branching.\n\nRefer to Conditionals in shaders for more information about when to use which shader directive.\n\nFor each keyword in a set, Unity automatically adds a keyword. For example, if you declare a keyword, Unity adds a keyword.\n\nYou can use this to check if a keyword exists, regardless of whether it’s enabled or disabled.\n\nBranch when all keywords in a set are disabled\n\nTo execute code when all keywords in a set are disabled, Unity must create an additional shader variant or uniform integer for that state.\n\nIf you use or to create a single keyword, Unity automatically creates an additional variant or uniform integer. For example:\n\nIf you use or to create a set of two or more keywords, or you use , add when you declare a keyword set so that Unity creates an additional variant or uniform integer. For example:\n\nUse other statements to make shader behavior conditional\n\nYou can also use the following HLSL pre-processor directives to create conditional code:\n• #if, #elif, #else and #endif .\n\nUsing these instead of makes it more difficult to change the keyword directive later. For example, if you need to reduce the number of shader variants, it’s more difficult to change to ."
    },
    {
        "link": "https://github.com/Unity-Technologies/Graphics/blob/master/Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl",
        "document": ""
    },
    {
        "link": "https://medium.com/@lemapp09/beginning-game-development-shader-graph-versus-hlsl-7b1e7211e72d",
        "document": "Note: A Real World example follows this article.\n\nIn the realm of game development with Unity, shaders play a critical role in defining the visual aesthetics of a game. Unity offers two primary approaches for creating shaders: the Shader Graph, which is a visual scripting tool, and traditional coding using C# and HLSL. This article explores these two methodologies, comparing their advantages and disadvantages to help developers choose the right approach for their projects.\n• Overview of Shader Graph\n\nShader Graph in Unity is a visual tool that allows developers to create shaders by building a network of nodes. Each node represents a function or operation, such as generating noise, blending textures, or manipulating colors. Users can visually connect these nodes to construct complex effects.\n\nPros of Shader Graph:\n\n- Intuitiveness and Accessibility: Shader Graph’s node-based interface is user-friendly, especially for beginners or those less familiar with programming. It visually represents shader operations, making it easier to understand the flow and effects.\n\n- Real-Time Feedback: Changes are visible in real-time, allowing developers to see the effects of adjustments immediately without needing to compile code.\n\n- Safety: The graphical interface minimizes syntax errors and other common mistakes that can occur in code-based shader writing.\n\nCons of Shader Graph:\n\n- Performance Overhead: While generally efficient, Shader Graph can introduce some performance overhead compared to manually optimized shaders.\n\n- Flexibility Limits: Advanced shader techniques might be challenging or impossible to implement with Shader Graph due to the limitations of available nodes.\n\n2. Writing Custom Shaders with C# and HLSL\n\nWriting shaders traditionally involves coding in HLSL within Unity’s C# scripts. This method provides direct access to the graphics pipeline, allowing more control and precision.\n\nPros of Writing Custom Shaders:\n\n- Maximum Control: Developers have complete control over every aspect of how the shader behaves, optimizing performance and achieving specific visual effects that might be beyond the scope of Shader Graph.\n\n- Optimization: Experienced shader programmers can write highly efficient shaders that are better optimized than those created through Shader Graph.\n\nCons of Writing Custom Shaders:\n\n- Steep Learning Curve: Requires a good understanding of both programming and graphics principles.\n\n- Development Time: Writing and debugging shader code is generally more time-consuming and can slow down the development process.\n\n- Error-Prone: Greater likelihood of bugs and errors due to complex syntax and less intuitive processes.\n\n3. Practical Considerations\n\nChoosing between Shader Graph and traditional coding depends on several factors:\n\n- Project Needs: Complex, performance-critical projects might benefit from the precise control offered by traditional shaders.\n\n- Team Skills: Teams with strong programming skills might prefer writing custom shaders, while those with more visual or design focus might find Shader Graph more accessible.\n\n- Development Speed: Projects on tight deadlines may benefit from the speed and ease of use provided by Shader Graph, especially during prototyping phases.\n\nBoth Shader Graph and traditional shader coding have their place in Unity development. Shader Graph offers a more accessible and visually intuitive approach, suitable for rapid development and those new to shader programming. On the other hand, custom shader coding offers unmatched control and optimization possibilities, ideal for projects requiring high performance and unique visual effects. Understanding the strengths and limitations of each method will help developers choose the most appropriate one for their specific needs and goals.\n\nThis article provides a foundation for understanding these tools and making an informed decision based on the project requirements and team expertise.\n\nFollowing is a example of a shader that changes from red to pink over time. First we’ll explore HLSL and then we’ll explore Shader Graph.\n\nCreating a simple HLSL shader in Unity that transitions a color from red to pink over time involves manipulating the fragment shader to interpolate between two color values based on the passing time. We’ll use Unity’s ShaderLab framework to set up the shader, incorporating HLSL for the color interpolation logic in the fragment shader.\n\nHere’s a basic example of how you could set up this shader:\n\nExplanation:\n\n- Properties Block: This defines a shader property `_TimeScale` that allows you to control the speed of the color transition in the material inspector in Unity.\n\n- Vertex and Fragment Shaders:\n\n — Vertex Shader (`vert`): This processes vertex data, primarily passing it through unchanged but transforming vertex positions to clip space.\n\n — Fragment Shader (`frag`): This is where the color interpolation happens. It takes a base red color and a target pink color and interpolates between these two based on the sine of the scaled time. This creates a smooth transition back and forth between the two colors.\n\n- Time Control: The transition effect is controlled by the `_Time.y` variable (which increases continuously) and is modulated by `_TimeScale` to adjust the speed of the color change.\n\nUsage:\n\n1. Create a new material that uses this shader.\n\n2. Apply this material to a GameObject in your scene.\n\n3. Adjust the `_TimeScale` property on the material to control the speed of the color transition.\n\nThis simple shader demonstrates the basics of color manipulation over time in HLSL within Unity. You can further explore and modify the `lerp` function parameters and the mathematical function used for `t` to achieve different types of color transition effects.\n\nCreating a shader in Unity that changes from red to pink over time using Shader Graph is a straightforward process that visually demonstrates how you can manipulate colors over time. Here’s a step-by-step guide to set this up:\n\nStep 1: Create a New Shader Graph\n\n1. Open Unity and ensure you have the Shader Graph package installed. If it’s not installed, you can add it via the Unity Package Manager.\n\n2. Create a New Shader Graph: Right-click in the Project window, go to `Create > Shader > PBR Graph` (or `Unlit Graph` if you don’t need lighting effects). Name your shader, for example, “RedToPinkShader”.\n\nStep 2: Set Up Your Shader Graph\n\n1. Open the Shader Graph by double-clicking on the shader asset you just created.\n\n2. Create a Color Node for Red: Right-click in the Shader Graph editor, select `Create Node`, and search for `Color`. Create the node and set its value to pure red `(RGB: 1, 0, 0)`.\n\n3. Create a Color Node for Pink: Repeat the above step, but this time set the color to pink `(RGB: 1, 0.5, 0.5)`.\n\nStep 3: Add Time and Interpolation\n\n1. Create a Time Node: This node provides the shader with a continuously increasing time value. Right-click and choose `Create Node`, then find and add the `Time` node.\n\n2. Manipulate Time for Effect: You might want to manipulate the time to control the speed of the color change. Add a `Multiply` node (search and add from the menu), connect the `Time` node to it, and set a float value to scale the time, affecting the speed.\n\n3. Create a Sine Node: To create a smooth back-and-forth transition, add a `Sine` node and connect the output of the `Multiply` node to its input.\n\n4. Adjust Sine Output: Since the `Sine` function oscillates between -1 and 1, use an `Add` node to add 1 (making the range 0 to 2), then use a `Divide` node to divide by 2, bringing the range back to 0 to 1.\n\n5. Interpolate Between Colors: Add a `Lerp` (Linear Interpolation) node. Connect the red color to the first input, pink to the second, and the output of the adjusted `Sine` node to the T input (which controls the blend factor).\n\nStep 4: Connect to Master Node\n\n1. Connect the Output: Take the output from the `Lerp` node and connect it to the `Base Color` input of the `PBR Master` node (or `Color` input if you’re using an Unlit Graph).\n\n2. Save the Shader: Click the Save Asset button in the top left of the Shader Graph window.\n\nStep 5: Apply Your Shader\n\n1. Create a Material: In the Project window, right-click and select `Create > Material`.\n\n2. Assign Your Shader: In the material’s inspector, change the shader to the one you just created (“RedToPinkShader”).\n\n3. Apply the Material: Drag the material onto any mesh object in your scene to see the effect.\n\nNow, as you play your scene in Unity, you will see the material smoothly transitioning between red and pink based on the sine of the scaled time, creating a visually appealing effect. You can adjust the time scaling to make the transition faster or slower. This visual approach provided by Shader Graph makes it easy to understand and manipulate shader properties without writing code."
    }
]