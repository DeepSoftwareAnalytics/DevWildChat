[
    {
        "link": "https://tldp.org/LDP/abs/html",
        "document": "This tutorial assumes no previous knowledge of scripting or programming, yet progresses rapidly toward an intermediate/advanced level of instruction . . . all the while sneaking in little nuggets of UNIX® wisdom and lore. It serves as a textbook, a manual for self-study, and as a reference and source of knowledge on shell scripting techniques. The exercises and heavily-commented examples invite active reader participation, under the premise that the only way to really learn scripting is to write scripts .\n\nThis book is suitable for classroom use as a general introduction to programming concepts.\n\nThis document is herewith granted to the Public Domain."
    },
    {
        "link": "https://sap1ens.com/blog/2017/07/01/bash-scripting-best-practices",
        "document": "I was always afraid of writing shell scripts. Bash seemed to be a programming language that doesn’t allow a slightest mistake… Extra space here and there and everything blows up.\n\nLike with every skill, persistence and repetition help. I’ve started writing more and more bash scripts a few years ago. But it’s important to remember one simple rule - when things become really complex you need to switch to Python/Ruby/scripting language of your choice. Please do!\n\nAnyway, today I want to share some of the very practical conventions, best practices and recommendations I gathered over these years.\n\nIt’s not an introduction to bash, you should have some background already (ideally some war stories as well).\n\nAlso, I’m not an expert! It’s ok to not agree with me. And I’m pretty sure almost everything I mention can be improved. So please help me and leave some feedback ;-)\n\nSo, first few lines in the bash script are actually the most important lines! This is how I typically start:\n\nFirst line (shebang) tells which interpreter to use. Try to avoid things like because they’re not portable. More explanations can be found here.\n\nNext, you MUST ALWAYS INCLUDE !!! I don’t know how to attract your attention more here. It’s very important to stop the script when an error occurs. Otherwise things can go really catastrophic. Yes, by default bash doesn’t stop when an error happens!\n\nYou’d also like to use , because if you don’t, expressions like will always succeed! It’s probably not what you want.\n\nA few more instructions you might use:\n• : detects uninitialised variables in your script (and exits with an error). Generally very useful to have, but it will also reject environment variables, which is a pretty common thing to use, so I don’t include this option by default\n• : prints every expression before executing it. Really handy for debugging / build scripts. I usually set it like this: , so it only works when explicitly requested\n\nOne more things to notice: it’s common to use those instructions in a short form: , , etc. I prefer the longer format, because it’s more readable and less cryptic, especially for people without a lot of bash experience.\n\nHow do you refer to a variable in your script? Probably something like ?\n\nThis is the most reliable notation: .\n\nQuotes help to prevent issues when variable contains spaces (for example, in filenames).\n\nCurly braces are not needed in this particular example, but help you with more complex situations like:\n\nMore examples can be found here under section.\n\nIt’s always helpful to separate variables that should be mutable from immutable ones. instruction can help you with that, practically making constants from variables.\n\nIt’s very simple to use:\n\nFun fact: there is no [normal] way to unset a readonly variable in bash! Make sure to remember this.\n\nShould we use single or double square brackets in conditionals? What’s the difference between and ?\n\nThey’re mostly equal, but double square brackets usually provide cleaner syntax and a few more additional features. Compare this:\n\nWith double square brackets you don’t need to escape parenthesis and unquoted variables work just fine even if they contain spaces (meaning no word splitting or glob expansion).\n\nDouble square brackets are less portable though: is supported by all POSIX shells and only works in bash 2.x+, zsh and some other shells.\n\nUsually you should use double square brackets unless you really know what you’re doing.\n\nYou can find great detailed explanation here.\n\nNow let’s look at some functions. Here’s what I think is a good example of a function:\n\nWhat I like about this function:\n• is more concise than\n• function name has an underscore as a prefix. It seems like a good idea to always have a special naming convention for your bash functions to avoid any potential clashes with built-in operators or functions you might include from other files\n• in bash functions arguments are accessible using index-based variables, first argument is , second is , etc. Of course you can refer to them like that, but when you have 5-6 index-based variables in a 20-30 lines function it can become really hard to keep the mapping in mind. So, you should always name them to make things very explicit. It’s also applicable to your “main” function, variables like , , etc. would be strings passed from CLI and it’s also a great idea to name them\n• operator restricts the scope of variables, protecting us from leaking those variables to a global namespace. If you only need this variable inside a function - make it !\n\nIn bash you can include (actually execute) external script using command. I’m still not sure how I feel about this:\n• It’s nice to be able to create a file with a set of utility functions that’s shared across various scripts. I generally support DRY principle. It also can be handy for defining configuration parameters in one concise file and then including that file to an actual script with the business logic\n• But at the same time I’m not very happy with the idea of including and executing a file with, potentially, unknown content. Yes, you can’t always control that and bash doesn’t give any mechanisms to protect you!\n\nAnyway, if you decide to use for includes, here’s the right way:\n\nYou make your life easier by always using consistent path for includes, because variable will always be resolved to the actual script location, not the current location.\n\nShellCheck has more than 200 checks and it integrates with your test frameworks and CLI tools. It doesn’t necessarily follow all the conventions I mentioned here and it definitely has more rules than I can cover.\n\nSo, I think it’s possible to write readable and reliable bash scripts. It’s important to remember when not to - some tools should be implemented with more advanced scripting languages. It doesn’t make any sense to try to squeeze out as much as you can from bash with very exotic syntax or shell commands. You still want your scripts to be simple and straightforward."
    },
    {
        "link": "https://geeksforgeeks.org/bash-scripting-introduction-to-bash-and-bash-scripting",
        "document": "Bash is a command-line interpreter or Unix Shell and it is widely used in GNU/Linux Operating System. It is written by Brian Jhan Fox. It is used as a default login shell for most Linux distributions. Scripting is used to automate the execution of the tasks so that humans do not need to perform them individually. Bash scripting is a great way to automate different types of tasks in a system. Developers can avoid doing repetitive tasks using bash scripting.\n\nBash scripting supports variables, conditional statements, and loops just like programming languages. Below are some of the applications of Bash Scripts –\n\nThe advantages of using Bash Scripts are given below –\n• None It helps to avoid doing repetitive tasks\n• None A sequence of commands can be run as a single command.\n\nThe disadvantages of the Bash Scripts are given below –\n• None Any mistake while writing can be costly.\n• None A new process launched for almost every shell command executed\n\nTo write a Bash Script we will follow the steps –\n• None First, we will create a file with the .sh extension.\n• None Next, we will write down the bash scripts within it\n• None After that, we will provide execution permission to it.\n\nTo create and write a file with the .sh extension we can use gedit text editor. The command for it will be –\n\nThe first line of our script file will be –\n\nThis will tell, the system to use Bash for execution. Then we can write our own scripts.\n\nLet’s write down just a simple script that will print some lines in the terminal. The code for it will be –\n\nNow we will save the file and provide the execution permission to it. To do so use the following command –\n\nNext to execute the following script we will use the following command –\n\nHere is the terminal shell pictorial depiction after executing the above commands as follows:\n\nHere the script file name is gfg.sh.\n\nNow we can also write more complicated commands using Bash Scripts. Here is an example below in which we are using a condition statement –\n\nHere is the terminal shell pictorial depiction after executing the above script as follows:\n\nIn the above way, we can execute multiple Bash commands all at once.\n\nNow Let’s look into the other important concepts related to Bash Scripting.\n\nIn the above example, we have saved the file using gfg.sh name and also provided execute permission using chmod command. Now, let’s understand why we have done that.\n\nWhile writing bash scripts we should save our file with the .sh extension, so that the Linux system can execute it. When we first create a file with the .sh extension, it doesn’t have any execute permission and without the execute permission the script will not work. So, we should provide execute permission to it using the chmod command.\n\nThe filename of a bash script can be anything but by convention, it is recommended to use snake case ( my_bash_script.sh ) or hyphens ( my-bash-script.sh ) for naming a script file.\n\nWe can use variables in bash scripting. Below is a sample program to understand the usage of variables in Bash scripts.\n\nSo, here is have declared two variables Name and another one is Age. These variables are accessible using $Name and $Age. That means, we can declare a variable in a bash script using VariableName=Value and can access it using $VariableName. Here is the terminal shell pictorial depiction after executing the above script as follows:\n\nThere are two types of variables present within Bash Scripting. Conventionally, If a variable, is declared inside a function then it is generally a local variable and if it is declared outside then it is a global variable. In the case of a bash script, this concept is a little bit different, here any variable whether it is written inside a function or outside a function by default is a global variable. If we want to make a local variable then we need to use the keyword “local”.\n\nNote: It is best practice to always use a local variable inside a function to avoid any unnecessary confusion.\n\nAn example of the same is given below –\n\nHere in this above example, var2 is a local variable, so when we are accessing it from the function it is doing fine but when we are trying to access it outside the function, it is giving us an empty result in the output.\n\nOn the other hand, unlike programming languages, even though var3 is defined inside a function still it is acting as a global variable and it can be accessed outside the function. Below is the terminal shell depiction after executing the script –\n\nInput & output are fundamental concepts for shell scripting. A script can take one or more inputs and can also produce zero or many outputs. It may even produce some errors. Let’s understand this with an example –\n\nSo, in this above example the first time, the script could not find any file with that file name, and the else block gets executed. It created the file and put some data into that file. When we run it a second time with the same file name, then it finds the file. So, is the if block gets executed and that displays the contents of the file. Reading the file contents is input and on the first time putting data into the file is considered to be output. Here we have used > for storing the content in a file. The > notation is used to redirect stdout to a file. On the other hand, we can use 2> notation to redirect stderr, and &> to redirect both stdout and stderr.\n\nBelow is the terminal shell pictorial depiction after executing the following script –\n\nIn programming, A function is a block of code that performs some tasks and it can be called multiple times for performing tasks. The simplest example of the use of function in Bash scripting can be given as –\n\nThe above example shows a function that prints something when called.\n\nSo, the basic syntax for writing functions within a Bash Script will be –\n\nBesides this, we can also have functions with passing arguments and with return values.\n\nIn programming, Decision Making is one of the important concepts. The programmer provides one or more conditions for the execution of a block of code. If the conditions are satisfied then those block of codes only gets executed.\n\nTwo types of decision-making statements are used within shell scripting. They are –\n\nIf else statement is a conditional statement. It can be used to execute two different codes based on whether the given condition is satisfied or not.\n\nThere are a couple of varieties present within the if-else statement. They are –\n\nThe syntax for the simplest one will be –\n\nIn the above example, during the condition checking the name matches and the condition becomes true. Hence, the block of code present within the if block gets executed. In case the name doesn’t match then will not have an output. Below is the terminal shell pictorial depiction after executing the following script –\n\ncase-sac is basically working the same as switch statement in programming. Sometimes if we have to check multiple conditions, then it may get complicated using if statements. At those moments we can use a case-sac statement. The syntax will be –\n\nIn the above example, the case-sac statement executed the statement which is a part of the matched pattern here the ‘Name’. Below is the terminal shell pictorial depiction after executing the following script –\n\nThe string comparison means in the shell scripting we can take decisions by doing comparisons within strings as well. Here is a descriptive table with all the operators –\n\nArithmetic operators are used for checking the arithmetic-based conditions. Like less than, greater than, equals to, etc. Here is a descriptive table with all the operators –\n\nBelow is a simple example of the same –\n\nIn this example first one (-eq )is a numeric comparison that checks for equality. The second one ( == ) is also check for equality but in strings. Below is the terminal shell pictorial depiction after executing the following script –\n\nWe will learn more about this concept in the next articles.\n\nIn this article we discuss Bash scripting which plays a vital role in automating tasks within the Linux environment. As a command-line interpreter, Bash offers simplicity and ease of use for developers to create scripts that streamline routine operations, manipulate files, and execute complex tasks. While Bash scripts can enhance productivity by avoiding repetitive actions and executing sequences of commands efficiently, they also come with considerations such as managing permissions, handling variables, and implementing decision-making structures. By mastering Bash scripting, users can harness its power to optimize their workflow and enhance system administration tasks on Linux platforms."
    },
    {
        "link": "https://stackoverflow.com/questions/69252356/bash-what-is-the-most-efficient-way-to-handle-i-o",
        "document": "I have a bash script which does a lot of string manipulations. As I know, reading from a file is slow. So instead of doing it every time I need its contents, I read the whole file at the beginning of the script\n\nBut every time I need to feed the lines to a program which accepts input (e.g., , , ), I anyway have to print them and create a pipeline. Here's an example which finds the first line which contains a colon in a file\n\nSo I started wondering, didn't I just make it slower by making additional calls to and creating a pipeline? What's the best way to handle this situation?"
    },
    {
        "link": "https://tldp.org/LDP/abs/abs-guide.pdf",
        "document": ""
    },
    {
        "link": "https://docs.rackspace.com/docs/linux-server-security-best-practices",
        "document": "The first step after you create a Linux® Cloud Server is to set the security\n\n on it. You should perform this crucial step on every server to prevent bad actors\n\n from obtaining unwanted access. This action results in a more secure environment that\n\n helps prevent you and your business from being compromised. Performing these basic\n\n steps and hardening the security on your server discourages bad actors\n\n and makes them move on to a new target.\n\nBy default, the root user is created as the first user on every Linux system.\n\n You should disable it via Secure Shell (SSH). Disabling this root user via SSH\n\n makes it harder for a bad actor to gain access to the system. Because the root user\n\n is created by default on every Linux server, bad actors already have half the\n\n information they need to log in to your server if the root user is enabled via\n\n SSH. This situation allows for brute-force SSH attacks until the password hash breaks.\n\nTo avoid this situation, you should create a secondary user for when you need\n\n to log in and administer the system. Each end user on the system should have\n\n their own login credentials for logging purposes. Depending on the actions that\n\n the end user needs to perform, they might need permission to complete\n\n administrative actions. This section provides examples about how to add a user with\n\n sudo permission on both Debian® and Red Hat® Enterprise Linux-based systems.\n\nBefore you create any users, ensure that you use strong passwords that require a\n\n minimum character length (and maybe even include expiration dates). Here are common\n\n guidelines advocated by proponents of software system security:\n• Use a minimum password length of 12 to 14 characters, if permitted.\n• Include lowercase and uppercase letters, numbers, and symbols,\n\n if permitted.\n• Avoid using the same password for multiple users, accounts, or software systems.\n• Avoid character repetition, keyboard patterns, dictionary words, letter or\n\n number sequences, user names, relative or pet names, romantic links (current\n\n or past), or personal information (for example, ID numbers, ancestors'\n\n names, or dates).\n• Avoid using information that is or might become publicly associated with the\n\n user or the account.\n• Avoid using information that the user's colleagues or acquaintances might\n\n know to be associated with the user.\n• Do not use passwords that consist of weak components.\n\nFor Debian and Ubuntu® operating systems, add a user by following these steps:\n• Create a new user and set their password:\n• Give the new user permissions for privileged operations on the system.\n\n This user is the primary user for logging in remotely and making changes to the server. Use one of the following methods to implement permissions for the user. a. Run the following command to add the user to the user group. Alternatively, you can modify the sudoers file to give the user permissions. a. Run the following command as root to edit the list of user permissions: Note: On some distributions, systems use the text editor for\n\n . Because is not a user-friendly editor, you might need to\n\n consult a vi tutorial for help. b. Add the following line directly after the line containing :\n• Switch to the new user and test their permissions by using to run a\n\n command that requires root access, such as the following command: A prompt asks you to enter the new user's password for verification before\n\n executing the command.\n• You can also verify that your user can elevate to the account by running the following\n\n command:\n\nIf you perform these steps correctly, your user now has access and can elevate permissions.\n\n If not executed properly, you get a message indicating that the user is not in the sudoers file\n\n when you attempt to authenticate.\n\nFor Red Hat and CentOS® operating systems, add a user by following these steps:\n• Create a new user with and set the user’s password with :\n• Give the new user permissions for privileged operations on the system.\n\n This user is the primary user for logging in remotely and making changes to the server. Use one of the following methods to implement permissions for the user. a. Run the following command to add the user to the group Alternatively, you can modify the sudoers file to give the user permissions. Note: On some distributions, the text editor that the system uses for\n\n is . Because is not a user-friendly editor, you might need to\n\n consult a vi tutorial for help. b. Add the following line directly after the line containing\n\n :\n• Switch to the new user and test their permissions by using to run a\n\n command that requires root access: A prompt asks you to enter the new user's password for verification before\n\n executing the command.\n• You can also verify that your user can elevate to the account by running the following\n\n command:\n\nIf you performed these steps correctly, your user now has access and can elevate permissions.\n\n If not executed properly, you get a message indicating that the user is not in the sudoers file\n\n when you attempt to authenticate.\n\nFor a login method that is more secure than using a password, create an SSH key pair to\n\n use with the user that you previously created. These instructions work with any Linux\n\n distribution.\n\nNote: These instructions are for Linux and macOS® desktops. If you are\n\n connecting from a Windows® desktop, follow the instructions in\n\n Generate RSA keys with SSH PUTTYgen\n\n and\n\n Log in with a SSH private key on Windows\n\n to generate and add the SSH key pair.\n• Run the following command to generate a key pair on your local Linux or\n\n Mac® computer: When asked where to save the key, use the default location. Adding a password is optional\n\n and is more secure, but can be inconvenient. This operation creates two files. The default names are for your private\n\n key and for your public key.\n• After you have created the key pair on your local computer, upload the\n\n public key to your remote server for the user that you created previously. Warning: Be sure to upload the public key, and not the private\n\n key.\n• Connect to the remote server by using\n\n and run the following command to\n\n verify that no extra keys were added that you do not expect:\n\nAt this point, you have added and password authentication for the user. The next\n\n section goes over optional steps on how to disable password authentication.\n\nNow that you have a new user with permissions and an SSH key\n\n pair, you can work with the SSH daemon (server) configuration to improve security.\n\nNote: For Managed Operations and RackConnect customers only: To ensure that our automated systems have access to your server when needed, we request that you do not change the SSH configuration and that you skip to the next section. When connecting to your server, the Rackspace Support team logs in as the user and uses password authentication on port 22. In addition, rebuilding existing servers or building a new server from a snapshot requires that you enable root logins by setting the option to . If you need to change these values, speak with a member of your Rackspace Support team so that the change is made in a way that does not impact our ability to provide you with a Fanatical Experience™.\n\nThe example commands for the rest of the article assume that you're are logged in as\n\n your new user, using to perform privileged operations.\n\nThis section covers common options in the SSH configuration file that help improve\n\n security. This information is used to configure your firewall later.\n\nThis section outlines only a few options to change and describes what they do. For details\n\n on other configuration options, see the\n\n OpenSSH documentation.\n\nThis section focuses on the following options:\n• : This option is the port on which the SSH daemon listens (port 22 by\n\n default).\n• : This flag enables (yes) or disables (no) root login via\n\n SSH. By default, this line is commented and allows root login.\n• : This flag enables (yes) or disables (no) SSH keys\n\n for authentication. By default, this line is commented and allows \n\n access.\n• : This flag enables (yes) or disables (no) password\n\n authentication. By default, this option is enabled.\n\nSSH uses port 22 by default for communication. Bad actors try port 22 with the\n\n username on every server that they attack. For this reason,\n\n disabling the root user via SSH and changing SSH to listen on a nonstandard port\n\n helps prevent a breach.\n\nChanging the port won't stop a determined intruder, but it does cause most\n\n superficial scans for SSH connection opportunities to overlook your server.\n\n Similarly, removing SSH access for the root user interferes with casual\n\n brute-force attacks via SSH.\n\nYou should also consider which authentication method to use when logging in.\n\n By default, all Linux systems use password authentication. Multiple ways exists\n\n to perform authentication on the server, but the main two are by using a password\n\n and SSH keys.\n\nSSH keys are generated in pairs, one public and the other private, and you can use them\n\n only in combination with each other. You should store the private key in a safe\n\n location on the computer from which you connect, and you should never give it out. You\n\n can give out the public key, and it is the key that you place on the server to which you\n\n are connecting. The private key on your local computer runs through an algorithm\n\n when you make a connection, granting access if the key pair hash matches up with the\n\n public key.\n\nBy this point, you have added a new user with permissions, created an SSH\n\n key pair, and uploaded your public SSH key. You can now change your SSH configuration\n\n file to improve your security. To do this, you can change SSH to listen on a\n\n custom port, restrict root login via SSH, enable public key authentication,\n\n and disable password authentication by using the following steps:\n• Note: By default, the and lines are commented out\n\n as indicated by the symbol. When commented out, these lines are read as default\n\n options, even if changes are made to the line. To implement these changes, you need\n\n to uncomment the associated lines by removing the symbol at the beginning of the\n\n associated line. Additionally, before disabling ensure that\n\n you have configured an SSH key or you cannot connect to the server. Replace 2222 with the port you want to use. Ensure the new port isn't already in use by another\n\n program by using netstat. Important: As mentioned previously, you should not make this change to the\n\n file if your server has a Managed Operations service level.\n\n These changes could deny Rackspace access to your server.\n• Test the altered SSH configuration for errors by running the following command:\n\nIf you receive no errors, SSH is now configured to run on a custom port and accept only non-root users that pass a valid SSH key. For these settings to apply and persist, you must restart the SSH service.\n\n However, do not restart the service yet. Restarting SSH now might lock you out of the server, requiring you to use rescue mode or the web console to restore the configuration. You must configure the firewall before restarting the server. We discuss the firewall in the next section.\n\nNote: RackConnect customers: To manage firewall rules, use the RackConnect management instead of on the server. You shouldn't change the SSH port if you use RackConnect. For more information about firewalls and RackConnect, see\n\n Managing RackConnect v2.0 network policies.\n\nEach Linux distribution uses a different software firewall solution. In Red Hat Enterprise Linux (RHEL) and CentOS 7, the default firewall is . On Debian and Ubuntu-based distributions, the default firewall solution is Uncomplicated Firewall (UFW). For RHEL and CentOS 6, the default solution is . Refer to the following section for your server's OS.\n• Open the new SSH port by running the following commands: Replace with the port that you used for the SSH daemon.\n• Restart the service by running the following command:\n• Verify that the your custom SSH port opened on the server:\n\nIf you followed these steps, you should see something similar to the following output:\n• Open the new SSH port by running the following commands: Replace with the port that you used for the SSH daemon.\n• Restart the service by running the following command:\n• Verify that your custom SSH port opened on the server by running the following command:\n\nIf you followed these steps, you should see something similar to the following output:\n\nNote: RHEL and CentOS 6 will be marked End of Life in November 2020. For best security practices, we strongly advise that you consider a newer OS version to host your application or website.\n• Open the new SSH port by running the following command: Replace with the port that you used for the SSH daemon.\n• Run the following command to restart the SSH daemon so that the daemon\n\n applies the new configuration you set up:\n• Verify that your custom SSH port opened on the server by running the following command:\n\nIf you followed these steps, you should see something similar to the following output:\n\nAfter you make the changes for your OS, open another terminal window on your local machine and log in to the server as the user that you created previously. Remember to specify the newly changed port. Keep your original connection active in case you need to troubleshoot the configuration.\n\nTo connect to SSH with the new configuration, you might need to specify the port number and key to use. For example:\n\nThe option specifies the port, and the option specifies the private key to use for the connection.\n\nIf you're connecting from a Windows desktop, when you create the connection in PuTTY, you can specify the port number and a private key.\n\nMost would-be intruders run multiple attacks against the same port to try to\n\n find something that they can exploit in the software running on that port.\n\n Fortunately, you can set up an intrusion prevention tool like\n\n fail2ban on your server to\n\n block repeated attacks on a port.\n\nNote: Managed Operations servers have installed and configured\n\n by default to watch over SSH login attempts. Contact your Support team if you\n\n have any questions or concerns.\n\nmonitors logs and automatically blocks connections if it sees too many\n\n from the same host in a short period of time. To set up and configure \n\n on your server, use the following steps:\n• To install on your server, run one of the following commands.\n• Use the following command to copy your default config. The newly created file overrides\n\n the default config and allows you to modify the file safely.\n• Modify your file to customize your security level in . In this file, you can set the following options:\n• : This parameter allows you to specify any IP address that should not be banned.\n• : This parameter allows you to specify the number of seconds to ban an IP address.\n• : This parameter checks for indications of triggers in the specified time.\n• : This parameter sets the number of retries allows before the IP address is banned.\n• In the file created, copy and paste the following text: These options ban an IP address after three failed attempts to connect via SSH for 24 hours.\n\n If you know your local IP address, we strongly recommended that you add this IP address in\n\n the preceding section as an parameter.\n• Start and enable on your server by using the following commands: RHEL and CentOS 7 or Debian and Ubuntu:\n\nAn intrusion detection system (IDS) can help an administrator monitor systems\n\n for suspicious activity and possible exploits. An IDS is more robust than a\n\n prevention tool like but can be more complicated to set up and\n\n maintain.\n\nA popular open-source IDS is OSSEC. OSSEC maintains\n\n agents on multiple systems that report back to the main server, allowing\n\n investigation of logs and alerts from a potentially compromised server even if\n\n that server is shut down.\n\nIf you suspect that a system is already compromised, you can investigate with\n\n procedures such as checking for backdoors and intruders\n\n and rescue mode investigation.\n\nKeep your OS up to date (patching)\n\nKeeping your kernel, packages, and dependencies up-to-date is very important, especially for security-related modules and packages. Some updates, such as kernel updates, require you to reboot your server. You should schedule maintenances to take place during times that are least disruptive to users because these maintenances cause a short period of downtime.\n\nImportant: While keeping your system up to date is of vital importance, ensure that the updates you're applying do not negatively impact your production environment.\n\nTo check for and install updates on Ubuntu operating systems and Debian, run the following commands:\n\nTo check for and install updates on CentOS, Red Hat, and Fedora systems, run the following command:\n\nFind out when the Linux distribution release that you are running on\n\n your servers reaches its end-of-life (EOL). When a release reaches its EOL, the\n\n distribution's maintainers no longer support it or supply package updates\n\n through their official repositories. You should plan your migration to a newer\n\n release well before your current release reaches its EOL.\n\nUse the following links to find out when your Linux distribution release is\n\n set to reach its EOL:\n\nAlthough securing your servers is an essential part of operations on the Internet,\n\n securing your account is also necessary. Your account name, password, and API\n\n keys are essential parts of how you interact with the Rackspace Cloud\n\n offerings. Just like any other password or access credentials, you want to keep\n\n them secure. However, you also need to allow your team to take action and perform\n\n necessary tasks.\n\nBy using\n\n Role Based Access Control (RBAC),\n\n you can create users and grant permissions to individuals or applications that\n\n are responsible for using various Rackspace services. By leveraging RBAC, you\n\n can give your team and contractors access to only the utilities that they need\n\n and revoke the access if necessary.\n\nFollowing are some usage scenarios:\n• Give contractors access to set up the environment that you have hired them\n\n to create, but limit their ability to view or change any credit card\n\n information or delete your account.\n• Allow your accountant to see the bill but not to delete your servers.\n• Hire a Database administrator (DBA) and give the DBA access to your DBaaS instances.\n• Allow a client to upload files directly to your Cloud Files account.\n• Configure your servers to register and use specific users for your\n\n monitoring and backup agents that are separate from your admin account.\n\nFor more information about RBAC, see the Role-Based Access Control (RBAC) FAQ."
    },
    {
        "link": "https://sternumiot.com/iot-blog/linux-security-hardrining-19-best-practices-with-linux-commands",
        "document": "Linux security hardening involves implementing a series of measures and best practices to reduce vulnerabilities and strengthen the security posture of a Linux server. This process aims to minimize potential attack vectors and ensure the server’s integrity, confidentiality, and availability. Hardening a Linux server typically includes configuring system settings, applying security patches, and disabling unnecessary services and applications.\n\nThe basic steps involved in hardening a Linux server include updating the system and all installed software to the latest versions to address known vulnerabilities; configuring access controls, such as setting strong passwords and managing user privileges; and improving network security by configuring firewalls (e.g., iptables or nftables), disabling unused network ports, and encrypting data communications.\n\nLinux hardening provides many benefits, which are not limited to security. Here are a few reasons every Linux server should undergo hardening:\n• Enhanced security: Hardening significantly reduces the risk of unauthorized access and data breaches.\n• Reduced attack surface: Limits the number of entry points for attackers by disabling unnecessary services and applications.\n• Incident response effectiveness: Facilitates quicker detection and response to security incidents through improved logging and monitoring.\n• Data integrity and confidentiality: Ensures that sensitive data remains secure and unaltered through encryption and access controls.\n• Operational continuity: Enhances the server’s ability to remain operational and functional during security incidents.\n\nTips and Best Practices for Linux Security Hardening (with Linux Commands)\n\nHere are some of the ways that organizations can harden their Linux servers.\n\nEncrypting file systems ensures that data stored on the disk is secure and inaccessible to unauthorized users, even if the physical media is compromised. A widely used method for encrypting disk partitions on Linux is LUKS (Linux Unified Key Setup), which offers strong encryption for block devices. LUKS operates at the partition level, providing full disk encryption.\n\nHere’s how to set this up:\n• Install LUKS: Use the following script:\n• Set up a LUKS-encrypted partition: Before setting up encryption, identify the partition to encrypt using lsblk or fdisk. Use the following commands to initialize encryption on the chosen partition, open it, create a file system on it, and mount the encrypted partition:\n• Access and manage encrypted data: To unmount and close the encrypted partition:\n• Automate mounting on boot: To automatically mount the encrypted partition on boot, you’ll need to store the LUKS passphrase securely (e.g., using a key file) and configure for auto-mounting. This allows the partition to be available without manual unlocking after each reboot, ensuring seamless access to the encrypted data.\n\nEliminating unnecessary components helps in minimizing the attack surface of Linux servers. This process involves identifying and disabling or uninstalling services, applications, and modules that are not required for the server’s operation. It reduces potential entry points for attackers, decreases system complexity, and improves the overall security posture:\n• Audit installed packages: to list all installed packages. Review the list and remove any packages that are not necessary for your server’s roles or functions. It is also important to install the security updates available for your operating system.\n• Disable unused services: . Disable services that are not needed for your server’s operation using appropriate commands (\n• Unbind unnecessary network services: For network-bound services that cannot be disabled, configure them to listen only on localhost (127.0.0.1) if remote access is not required. This can often be done within the service’s configuration file.\n• Limit module loading: Restrict the loading of kernel modules by blacklisting those that are unnecessary for your server’s functionality. This can be achieved by adding entries to\n• Secure boot configuration: are password-protected and configured securely to prevent unauthorized modifications during the boot process.\n\nBy reducing the number of open ports and securing those that need to remain open, administrators can significantly decrease the server’s exposure to potential attacks. Here are strategies to achieve this goal:\n• Conduct a port audit: to identify all open ports on the server. Review the list of open ports and determine which services are associated with each port.\n• Close unnecessary ports: For any service that is not required, disable the service or configure it to stop listening on the network. This can usually be done within the service’s configuration file or by stopping and disabling the service using\n• Implement firewall rules: to create rules that explicitly allow traffic only on necessary ports and deny all other traffic by default. Be sure to allow essential services such as SSH (on a non-default port), HTTP/HTTPS for web servers, and any application-specific ports that must be accessible. Use restrictive settings, which ensure all ports are inaccessible except those explicitly open by the security admin\n• Use TCP wrappers for additional control: For services that support it, use TCP wrappers ( ) to restrict access to specific services based on IP addresses or hostnames, adding an extra layer of control over who can connect to your server.\n• Regularly review network configuration: Periodically re-audit your server’s open ports and firewall rules to ensure that only required ports remain open and that firewall rules are still relevant based on changes in your server’s roles or applications.\n• Isolate sensitive services: For critical services, consider running them in isolated environments such as containers or virtual machines with dedicated network interfaces. This limits access to these services and reduces risk in case of compromise.\n• Enable connection rate limiting: Configure the firewall or use tools like to limit connection attempts to sensitive services like SSH. This helps mitigate brute-force attacks by temporarily banning IPs that make too many failed connection attempts within a short period. In addition, it is recommended to use third-party DDoS protection such as Cloudflare to mitigate risks associated with such attacks.\n\nPassword management is essential for ensuring that all user accounts on the system adhere to strong password standards. This helps prevent unauthorized access due to weak or compromised passwords. Here are strategies for managing password policies on Linux systems:\n• Enforce password complexity: Configure Pluggable Authentication Modules (PAM) to enforce password complexity requirements. This can include rules for minimum length, and the inclusion of uppercase letters, lowercase letters, numbers, and special characters. An example PAM configuration in\n\nThis configuration enforces a minimum length of 12 characters and requires at least one digit (dcredit), one uppercase letter (ucredit), one lowercase letter (lcredit), and one special character (ocredit).\n• Implement password aging: command to set password expiration policies for user accounts. This forces users to regularly update their passwords, reducing the risk of long-term use of compromised passwords. Here’s an example command to set a maximum password age of 90 days:\n• Limit password reuse: To prevent users from reusing old passwords, configure PAM with the . Here’s an example PAM configuration to remember the last five used passwords:\n• Educate users about secure password practices: Regularly inform and educate users about the importance of strong passwords and secure authentication practices. Encourage the use of passphrase generators or managers where appropriate.\n• Monitor failed login attempts: Configure the system to monitor and alert administrators about excessive failed login attempts, which could indicate brute-force attacks.\n\nLocking user accounts after a certain number of unsuccessful login attempts helps prevent brute-force attacks. By implementing account lockout policies, systems can automatically disable access for accounts that exhibit suspicious login behavior, reducing the risk of unauthorized access. To implement this:\n• Configure PAM for account lockout: Use the PAM framework to set up account lockout policies. Edit the\n• None Example configuration to lock an account after 5 failed login attempts for 10 minutes:\n• Use the faillog command: command can display and modify the login failure log, set limits on allowed failed login attempts, and reset counters. Use this command to manage account lockouts and review failed login attempts.\n• Implement manual unlock procedures: Establish procedures for manually unlocking accounts after verification of the user’s identity. This ensures that legitimate users can regain access while maintaining security controls.\n\nImplementing Secure Shell (SSH) with key-based authentication enhances security by replacing traditional password-based logins with cryptographic keys. This method reduces the risk of brute-force attacks and unauthorized access since attackers must possess the correct private key to gain entry. Here’s how to set up key-based authentication for SSH:\n• Generate a key pair: On the client machine, use the command to create a public and private key pair. The command and its default parameters will save the keys in the\n• Copy the public key to the server: Transfer the public key to the server’s authorized keys file. Use the\n\nAlternatively, manually append the public key to the ~/.ssh/authorized_keys file on the server:\n• Configure the SSH daemon: ) on the server to enhance security settings. Disable password-based authentication by setting:\n\nAfter making changes, restart the SSH service:\n• Set proper permissions: file have the correct permissions to prevent unauthorized access:\n• Disable root login: To further secure the server, disable root login over SSH by adding or modifying the following line in the\n\nEnabling mandatory access control (MAC) systems such as SELinux or AppArmor adds an additional layer of security by enforcing policies that restrict how programs can access resources.\n\nHere’s how to set up and manage SELinux:\n• Installation: On many Linux distributions, SELinux is already installed (note that it is not installed by default on Ubuntu). Verify its status using:\n• Configuration: ) to enable it in enforcing mode:\n\nApply changes by rebooting the system:\n• Policies: Use predefined policies to restrict applications. For example, to apply a policy for an Apache server, use:\n• Installation: Install AppArmor if it is not already present. On Debian-based systems, use:\n• Configuration: Ensure AppArmor is enabled at boot by editing the GRUB configuration (/etc/default/grub) and adding apparmor=1 security=apparmor to the GRUB_CMDLINE_LINUX line:\n• Profiles: Load and enforce AppArmor profiles for applications. For example, to enforce a profile for the MySQL service, use:\n\nEnsuring that only authorized users have access, and that they are authenticated securely, is fundamental to protecting the system from unauthorized access and potential breaches. To ensure a thorough review process:\n• Audit user accounts: Regularly audit all user accounts on the system using commands like . Look for any accounts that are no longer in use, or were created for testing purposes and forgotten. These should be disabled or removed to prevent unauthorized access.\n• Enforce strong password policies: Implement strong password policies using Pluggable Authentication Modules (PAM) configuration. This can include password complexity requirements, minimum password length, and password expiration policies. Tools like can be used to enforce these policies.\n• Use two-factor authentication (2FA): For critical systems, consider implementing two-factor authentication for an added layer of security. This requires users to provide two forms of identification: something they know (like a password) and something they have (like a token or mobile phone app generating one-time codes).\n• Limit the use of the root account: Avoid using the root account for day-to-day administration by creating individual user accounts with sudo privileges for administrators. This reduces the risk associated with having multiple people share the root account and provides an audit trail of administrative actions.\n• Regularly review the sudoers file: file to control which users have sudo access and what commands they can execute as root. Use the\n• Manage SSH keys: If using SSH key-based authentication, regularly review and remove any outdated or unused public keys from\n• Disable empty password accounts: Ensure no accounts exist with empty passwords by setting them to a locked state or assigning strong passwords where necessary.\n• Implement an account lockout policy: Configure account lockout policies to temporarily disable accounts after a certain number of failed login attempts, reducing the risk of brute-force attacks.\n\nEnabling and correctly configuring iptables, the default Linux firewall, is a critical step towards securing a Linux server. This firewall allows you to define rules for how incoming and outgoing network traffic should be handled. By setting up iptables, you can protect the server from unauthorized access and various network attacks. Here are some tips for configuring it:\n• Default policies: chains. This ensures that any traffic not explicitly allowed will be denied.\n• Allow essential traffic: Define rules to allow essential inbound and outbound traffic for your server’s operation. Commonly allowed inbound traffic includes SSH (port 22), HTTP (port 80), and HTTPS (port 443). Remember to adjust port numbers if you’re using non-standard ports.\n• Allow the loopback interface: ) is essential for internal system communication. Ensure that traffic on this interface is allowed.\n• Establish stateful inspection: Use connection tracking to allow established connections to continue while still filtering new incoming connections based on your defined rules.\n• Log dropped packets: Configure logging for dropped packets to help identify potentially malicious activity or adjust the firewall rules as needed.\n• Save the rules: After configuring these rules, ensure that they persist across reboots by saving them with the appropriate command for your distribution (\n• Regularly review the firewall rules: Periodically review and update the rules to adapt to changes in the server’s configuration or respond to emerging security threats.\n\nEncryption involves transforming readable data into an encoded format that can only be deciphered by someone who possesses the correct decryption key. By encrypting data in transit, organizations can protect sensitive information from being intercepted and accessed by unauthorized individuals. To ensure full encryption (in addition to enabling SSH as described above):\n• Implement TLS/SSL for web services: Use Transport Layer Security (TLS) or its predecessor, Secure Sockets Layer (SSL), to secure HTTP traffic. Obtain a certificate from a trusted Certificate Authority (CA) and configure your web server to use HTTPS.\n• Enable IPsec for network communication: This protocol suite authenticates and encrypts each IP packet in a communication session. Use IPsec to secure data communication between Linux servers, especially when transmitting over untrusted networks.\n• Use VPNs for remote connections: Virtual Private Networks (VPNs) create a secure tunnel between remote users or sites over the Internet. By routing traffic through this encrypted tunnel, VPNs ensure that data remains confidential and protected from eavesdropping.\n• Use email encryption: Implement protocols such as S/MIME (Secure/Multipurpose Internet Mail Extensions) or PGP/GPG (Pretty Good Privacy/GNU Privacy Guard).\n\nDisabling USB ports helps in preventing unauthorized access and data exfiltration from Linux systems. USB devices can be used to introduce malware or copy sensitive information without permission. Here’s how to disable USB usage:\n• Block the USB storage module: Prevent the kernel from loading the module, which is responsible for recognizing USB storage devices. Edit or create a file in\n• None . This will stop the system from loading the USB storage driver, effectively disabling the use of USB storage devices.\n• Use Udev rules: Udev, the device manager for the Linux kernel, allows you to write rules that can enable or disable access to certain devices. Create a\n\nThis rule matches removable devices added (ACTION==”add”) that are recognized as SCSI disk devices (KERNEL==”sd[a-z][0-9]*”) on the USB subsystem (SUBSYSTEMS==”usb”), then disables them.\n• Remove or disable unnecessary drivers: If the system does not require any USB functionality, consider removing or disabling unnecessary drivers from the kernel configuration if you are compiling your own kernel.\n• Physically disable or block ports: For systems that should never use USB ports (e.g., servers in a data center), physically disabling or blocking access to USB ports is an effective measure. This can be done by disconnecting internal headers on motherboards or using physical locks available for USB ports.\n• Restrict access with permissions: As an additional layer of protection, you can set strict permissions on device nodes for USB storage devices: This command removes all permissions for accessing first SCSI disk device nodes, preventing users from accessing attached USB storage.\n\nPhysical server security involves implementing measures to prevent unauthorized physical access, theft, vandalism, and environmental hazards that could compromise server integrity and data confidentiality. Here are some key considerations for enhancing physical server security:\n• Access control: Implement strict access control measures to restrict entry to server rooms and data centers. Use electronic access systems with key cards or biometric authentication to track and control who enters the server environment.\n• Surveillance: Deploy surveillance cameras around critical areas, including entrances/exits and server racks. Continuous monitoring can deter potential intruders and provide evidence in case of security incidents.\n• Environmental controls: Ensure that servers are housed in a controlled environment with optimal temperature and humidity levels maintained through climate control systems. Protect against environmental risks such as fire, flooding, and power surges with appropriate detection systems (smoke detectors) and uninterruptible power supplies (UPS).\n• Rack security: Secure servers within locked racks or cages to prevent unauthorized removal or tampering with the hardware. Consider using tamper-evident seals on server cases for additional security.\n• Visitor management: Establish a visitor management protocol that includes signing in/out procedures, escorted access for visitors, and temporary badges that clearly identify non-staff members.\n• Secure disposal of equipment: When decommissioning servers or storage devices, ensure secure disposal methods are used to prevent data recovery from hard drives or other storage media. This may include physical destruction or professional data wiping services.\n\nThe BIOS (Basic Input/Output System) or UEFI (Unified Extensible Firmware Interface) firmware initializes and tests hardware during the boot process and provides runtime services for operating systems. Unauthorized modifications to BIOS settings can compromise the entire system, allowing attackers to bypass security mechanisms or introduce malware. Here’s how to implement BIOS protection:\n• Enable a BIOS password: Set a strong password for accessing the BIOS setup utility. This prevents unauthorized users from changing critical settings such as boot order, enabling/disabling hardware components, or other security-related configurations.\n• Disable boot from external devices: Configure the BIOS to prioritize booting from the internal hard drive and disable boot options for external devices such as USB drives, CDs/DVDs, and network PXE boot. This reduces the risk of unauthorized bootable media being used to bypass OS security.\n• Enable secure boot: This feature ensures only digitally signed software can be executed during the boot process. Enabling secure boot helps protect against rootkits and other low-level malware by verifying the integrity of the bootloader and operating system kernel.\n• Use TPM for additional security: If the system includes a Trusted Platform Module (TPM), enable it in the BIOS settings. The TPM is a hardware-based security device that can securely store cryptographic keys, passwords, and digital certificates. It enhances data encryption, disk encryption, and platform authentication.\n• Document and audit BIOS configurations: Maintain documentation of approved BIOS configurations for your systems and perform regular audits to ensure compliance with organizational security policies. Any unauthorized changes should be investigated promptly.\n\nLocking the boot directory helps protect critical boot files from unauthorized modification. The boot directory contains essential components like the kernel image and bootloader configuration, which, if tampered with, could compromise the entire system’s integrity. Here are steps to secure the boot directory:\n• Set ownership and permissions: Change the ownership of the directory to root and set strict file permissions to prevent non-root users from making modifications.\n• Secure the GRUB configuration: ) is critical for system startup. Protect it by setting root ownership and read-only permissions.\n• Regularly monitor for unauthorized changes: Use file integrity monitoring tools such as AIDE (Advanced Intrusion Detection Environment) or Tripwire to detect any unauthorized changes to files within the /boot directory.\n• Limit access to BIOS/UEFI settings: Ensure that BIOS or UEFI settings are password-protected and configured to prevent booting from external devices. This prevents attackers from bypassing the locked /boot directory by using a bootable USB drive or CD/DVD.\n\n15. Keep Linux Kernel and Software Up to Date\n\nKeeping the Linux kernel and software up to date helps address vulnerabilities that could be exploited by attackers, ensuring that the system is protected against known security threats. Here are some steps to ensure your Linux kernel and software are kept current:\n• Enable automatic updates: Most Linux distributions offer automatic update features, which can be enabled to ensure that all software packages, including the kernel, receive timely updates. For Debian-based systems, use ; for Red Hat-based systems, consider using\n• Regularly check for updates: In addition to automatic updates, manually check for available updates regularly using your package manager ( for CentOS/RHEL). This ensures that you are aware of any pending updates.\n• Update the kernel: The Linux kernel receives regular updates to fix security vulnerabilities and improve performance. Ensure these updates are applied and reboot the system to activate the new kernel version.\n\nRegularly reviewing system and application logs is a vital practice for maintaining security and operational integrity on Linux servers. Logs provide insights into system behavior, unauthorized access attempts, and potential issues. Here are strategies for effective log management:\n• Centralize log management: Use centralized log management solutions to aggregate logs from multiple sources. Tools like syslog-ng or rsyslog can forward logs to a central server, simplifying analysis and monitoring.\n• Monitor critical logs: Prioritize monitoring of critical logs, such as authentication logs (/var/log/auth.log), system messages (/var/log/messages), and web server access and error logs (/var/log/apache2/access.log, /var/log/nginx/error.log). Set up alerts for unusual patterns or specific events.\n• Implement log rotation: To prevent log files from consuming excessive disk space, implement log rotation using logrotate. Configure rotation policies based on file size or time, and compress older logs to save space.\n• Secure log files: Ensure that log files are accessible only to authorized users by setting appropriate permissions. Consider encrypting sensitive logs to protect data in transit and at rest.\n• Use log analysis tools: Leverage tools like GoAccess for web server logs, Fail2Ban for detecting brute-force attempts, or ELK Stack (Elasticsearch, Logstash, Kibana) for comprehensive log analysis and visualization.\n• Regularly audit logs: Schedule regular audits of your logs to identify security incidents, operational issues, or areas for improvement in your logging strategy.\n\nBy systematically reviewing logs and employing effective log management practices, administrators can enhance their ability to detect security threats early, troubleshoot issues promptly, and maintain a secure Linux environment.\n\nPerforming system auditing is an essential practice for identifying security vulnerabilities, ensuring compliance with security policies, and detecting unauthorized changes or activities within Linux systems. System auditing involves collecting, analyzing, and reporting on various system events and configurations. Here’s how to effectively perform system auditing:\n• Use auditing tools: Use Linux auditing tools such as , the Linux Audit Daemon, which provides detailed logging of security events. Configure rules to monitor access to sensitive files, use of privileged commands, and changes to critical system files.\n• Regular vulnerability scanning: Implement regular vulnerability scanning using tools like OpenVAS or Nessus. These tools can identify known vulnerabilities in software packages, configurations, and services running on your system.\n• Check for integrity: Use file integrity monitoring tools such as AIDE (Advanced Intrusion Detection Environment) or Tripwire. These tools help detect unauthorized changes to critical system files, directories, and configurations by comparing current file states against a known good baseline.\n• Document auditing processes: Keep detailed documentation of your auditing processes, including the scope of audits, tools used, schedules, and procedures for responding to findings.\n• Act on audit findings: Promptly address any issues discovered during audits by applying necessary patches, tightening security configurations, removing unnecessary services or software packages, and updating policies as needed.\n\nEven though Linux is less susceptible to malware than other operating systems, using antivirus software and monitoring tools is crucial for maintaining a secure environment. Here are some strategies for implementing these tools:\n• Install antivirus software: Choose a reputable antivirus solution compatible with Linux. Some popular options include ClamAV, Sophos, and ESET. To install ClamAV:\n• Enable real-time monitoring: Real-time monitoring tools help detect and respond to threats quickly.\n• Use an intrusion detection system (IDS): IDS tools such as AIDE (Advanced Intrusion Detection Environment) and Tripwire monitor file changes and alert administrators of suspicious activities. For example, start by installing AIDE:\n• Monitor system logs: Regularly review system logs for unusual activities using tools like . Implement centralized logging with tools such as Graylog or ELK stack (Elasticsearch, Logstash, and Kibana) for enhanced analysis. To set up\n• Network monitoring: Tools like Wireshark, Nagios, and Zabbix provide comprehensive network monitoring (using respective modules or scripts for capturing network metrics), helping detect abnormal traffic patterns indicative of an attack. To install Nagios:\n• System resource monitoring: to monitor CPU, memory, and disk usage, identifying potential performance issues and security incidents. Alerting tools should be used to notify relevant teams in case of abnormal metrics.\n\nRuntime Application Self-Protection (RASP) embeds protection mechanisms within an application to detect and mitigate threats in real time. Here’s how to deploy RASP:\n• Integrate RASP with applications: Follow the provider’s guidelines to integrate RASP with your application. This typically involves adding a library or agent to the application’s runtime environment. Next, initialize the RASP agent during the application startup. This ensures that the protection mechanisms are active from the moment the application begins running.\n• Customize security policies: Configure the RASP solution to define acceptable behavior and responses to detected threats. Customize these policies to match the specific needs of your application. For example, set a policy to block SQL injection attacks by inspecting database queries at runtime.\n• Monitor and respond to alerts: Continuously monitor the alerts and logs generated by the RASP solution. Establish a protocol for responding to different types of threats. One option is to configure automated responses to certain threats, such as blocking IP addresses or terminating malicious sessions.\n• Regular updates: Keep the RASP solution updated with the latest security patches and threat intelligence to ensure it can defend against new vulnerabilities and attack vectors.\n• Testing and validation: Regularly test the effectiveness of the RASP solution by simulating attacks and validating that the protective measures work as intended.\n\nSternum is an IoT security and observability platform. Sternum provides deterministic security with runtime protection against known and unknown threats; complete observability that provides data about individual devices and the entire device fleet; and anomaly detection powered by AI to provide real-time operational intelligence.\n\nSternum operates at the bytecode level, making it universally compatible with any IoT device or operating system including RTOS, Linux, OpenWrt, Zephyr, Micrium, and FreeRTOS. It has low overhead of only 1-3%, even on legacy devices.\n\nVisit our dedicated Linux solutions page to learn more about Sternum’s agentless embedded Linux security"
    },
    {
        "link": "https://goteleport.com/blog/5-ssh-best-practices",
        "document": ""
    },
    {
        "link": "https://serverfault.com/questions/334448/why-is-ssh-password-authentication-a-security-risk",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://beyondtrust.com/blog/entry/server-security-best-practices-for-unix-linux-systems",
        "document": "Server security, and the protection of Unix and Linux environments, has never been more urgent. While the adoption of PaaS, IaaS, and SaaS models has been underway for years, demand for all things cloud has become supercharged in this era of accelerated digital transformation to accommodate a vast increase in remote work and distributed ways of conducting business. This speed of evolution and increase in complexity contributes to server vulnerabilities and gaps in protections that attackers—whether external threat actor or insider—can exploit to inflict substantial damage. In the blog, we delve into the world of Unix/Linux server security and examine the fundamentals of securing your servers, covering key topics such as:\n• Common use cases for Unix and Linux\n• The pros and cons of open source solutions and automation tools for Unix/Linux server security\n• What Linux Security Modules are and how they can be used to improve security\n• 10 best practices for hardening and securing Unix & Linux environments\n\nWhy Linux & Unix Security is also Cloud Security Linux is the most widely used operating system in the Cloud, supporting a broad range of workloads. It's used for web, database, and mail servers, as well as for blockchains, containers, TVs, IoT, SCADA, and CI/CD pipelines, among others. The increasing volume of internet-facing cloud applications makes it more critical than ever to ensure these highly valued systems are protected against a growing threat landscape. The nature of distributed systems means that they are prone to insecure configuration, creating gaps in security defenses. For example, a simple misconfiguration in a web application could allow threat actors to remotely execute commands on the underlying server. A determined threat actor will use this access to persistently probe the system, seek elevation of privilege, then ultimately take control of the entire system. Given time, advanced persistent threats (APTs) like this can morph from an isolated annoyance to a broad-scale breach that can shake the global digital ecosystem, such as we have seen with the SolarWinds attack. There is not only a problem with scale for the Unix or Linux configuration; as the number of systems and platforms grow, but infrastructure operators must also manage a rapidly increasing number of privileged identities (human and machine). This exponential increase in identities puts organizations in a difficult situation. Just a single improperly stored SSH key could serve as an attack vector through which an organization could be compromised. Simply put, the odds are stacking against you - the attacker aim is a greater than 0% chance of success, whereas the security team needs to aim for 100% coverage all the time. Now think back to where Linux and Unix systems run--they are the cornerstones of critical business processes across almost every vertical. Some of these systems control continuous processes, where the recovery time from a shutdown can extend out to weeks. If these systems are unavailable due to a cyberattack or misconfiguration, it will cause significant reputational and financial impact to the organization.\n\nWhen it comes to managing these privileged identities in any environment, there are 5Ws to keep in mind to achieve appropriate security measures and processes for your organization: Who is the identity of the Linux or Unix user? What is the target asset? What is the connectivity method? Why does the Linux or Unix user need access to this system? Why should this be actioned? When should this change be done? When is this change acceptable to the business? Where will you perform this change from? Where will the full audit trail of this change live?\n\nMany open-source solutions are pre-installed on Unix/Linux platforms to help address the questions and challenges discussed in the sections above. For instance, the sudo application, which is free in most distributions, enables a standard user to execute a command as a superuser (root) or another specified user. It can be compared to the right click ‘runas’ option in Windows. This allows for a straightforward principle of least privilege (PoLP) to be applied, ensuring users can elevate privileges on demand. However, this simple solution proves inadequate for the diverse demands of computing today, let alone infrastructures which are dynamically expanding at scale. It is worth noting a major sudo vulnerability was recently discovered by the Qualys research team. This vulnerability has existed since 2011 but was not fixed until 2021. You can read more about it in this blog. As the number of systems increases, management with sudo becomes more challenging. Sudo requires you to manage every host separately, and quickly gets complex, time-consuming, and ultimately, untenable, to administer. This lack of centralization inevitably leads to gaps, and gaps lead to risk. (Free automation tools exist to address the scaling problem, and we will explore them later..) Additionally, despite its use, sudo has limitations in managing privilege and root identity and may scalabe well in larger environments. Consequently, team members share root and other accounts, leading to workarounds, or even worse, breaches. Once the root account is shared, you run into an audit/compliance problem – you have completely lost the ability to prove an indelible audit trail for any privileged activity. Sudo activity logs are not tamper-proof, making it impossible to prove compliance or effectively investigate a forensic issue. To avoid getting to this point of sudo impasse, or to escape it, the best and most successful path is to migrate to using a true least privilege server management solution. This will empower your organization to run all users without any standing privileges, or without sharing accounts. Operators and administrators can continue running the applications they have always used, but elevation is only performed for the individual commands based on business policy. From the user’s point of view, this is as simple as swapping out a command-to-action elevation. All events are then fully recorded for accountability, and a granular policy allows you to control what actions are available. All privileged activity information is then captured in a tamper-proof audit trail, ready for use by auditors or incident response teams. As you can see in my example below, there is a minimal impact to the existing workflow:\n\nLSMs, or Linux Security Modules, are shipped with the Linux kernel and implement Mandatory Access Controls (MAC) rather than the traditional Discretionary Access Controls (DAC) for the entire system. The LSM inserts hooks at every point in the kernel where a user system call is requesting access to an internal, important kernel object. This framework then enables different implementations of a Mandatory Access Control-based security model. There are several LSM modules included in the Linux kernel, each having a slightly different approach and management style, but they will all equally enhance security on your system. Looking at SELinux specifically, this solution provides granular security policies, which go further than the traditional default existing permissions of Read, Write, Execute, and assign permissions to files or directories. LSMs are particularly powerful as all system calls are checked against a Policy Database and denied by default. This is achieved by applying context to files and network ports (for example) and applying labels to these objects. Policies then reference these mappings, to provide a single policy. Luckily, Administrators don’t need to write rules and labels from scratch. Many templated rules exist today to simplify deployment and testing on your system.\n\nLinux Security Modules are proven to stop attackers in their tracks. For example, if you have a vulnerable PHP site running on an Apache Linux server, if the LSM is correctly deployed, any outbreaks in the PHP vulnerability further into the machine will be automatically blocked. When the attacker has command and control through the vulnerability, they will attempt to elevate or move laterally. Important files like sudoers, passwd, known_hosts, resolve.conf, etc. will all reference different labels to the PHP security policy the attacker is in. The PHP policy will not permit any read, write, or execute rights to any other object. Such attacks like this are interesting—and dangerous—because the server could be patched, fully updated, and managed securely, but an attacker could still gain access through another vulnerability. LSMs provide a critical security layer here—but you are only as strong as your weakest link.\n• What about managing those root accounts to stop privilege elevation?\n• Does IT Security know the ‘good’ baseline of system activity and session data?\n• Does IT Security have an audit trail of the 5Ws? This is where LSMs reach their limit, and additional layers of security are necessary. BeyondTrust’s Privileged Management for Unix and Linux product (part of our Endpoint Privilege Management solution also includes our Privilege Management for Windows & Mac product) can create a known baseline of activity on the endpoint and a central audit trail. All administrators of the system can sign on as a standard user, and only elevate the tools and applications required to complete the job. In addition, BeyondTrust Password Safe will automatically scan, onboard, and manage privileged accounts (human, application, machine, etc.). Not only the built-in root account, but also the identities and SSH Keys of other known or discovered privileged users. By taking these accounts under management, Password Safe ensures all credentials are released just-in-time and changed after their use.\n\nAutomation tools, specifically Infrastructure as Code (IaC), have really simplified the configuration management of large environments. What used to be a painfully long and tedious undertaking (such as changing a configuration file on 100 servers, or even copying a file to them), can now be achieved in one tool across many platforms and operating systems. Teams can apply this technology to simplify account management on these systems. New users, groups, and home directories can be created and defined from a central parent node. However, these built-in solutions, like SSSD, struggle in complex environments and lack advance features like Group Policy, multi-factor authentication (MFA), and central reporting. These tools also introduce additional risks. For instance, the highly privileged account powering the automation tools is often static, with vast amounts of standing privilege. These types of accounts are arguably the most desirable target for a determined threat actor in any cyberattack, as it allows total control of the organization’s infrastructure. This also does not replace the need for central identity management. Without centralized identity management, every System Administrator will need to maintain at least two accounts, each with unique passwords and policies. Using a tool like Active Directory (AD) Bridge, which is also part of BeyondTrust’s Endpoint Privilege Management solution, System Administrators can consolidate all user identities into Active Directory. By centralizing all user identification and authorization into AD, the business can achieve a single source of truth, a single point of management of all systems, and not just be limited to Windows endpoints. With BeyondTrust AD Bridging, all configuration, rollouts, and updates are performed by a central web application. This platform makes life much easier for identity teams – allowing them to reduce the time taken to process joiners, movers, and leavers, as well as reduce the overhead in producing entitlement reports for auditing and compliance purposes. Not only are IT Operations saving significant amounts of time by leveraging AD Bridge, but security teams are also enforcing a one password policy, while making it easier for the users of the system by using single sign on.\n\nWhen choosing a Privileged Access Management (PAM) vendor, it's critical to validate they have a holistic view on PAM and offer robust security capabilities, which not only scale, but are also platform-agnostic, practical to implement, and deliver measurable ROI. Here are some key Unix/Linux privilege management concepts that BeyondTrust can help your team achieve:\n• Manage the accounts, passwords, and keys within your *nix environment to ensure password policies are adhered to and not left static and weak. This helps protect against many types of password attacks, such as password re-use, password cracking, pass-the-hash, and more.\n• Implement the least privilege principle to eliminate excessive administrative privilege This helps protect against Unix and Linux privilege escalation attacks as well as lateral movement.\n• Eliminate the dangerous use of shared accounts to achieve nonrepudiation\n• Control access to your *nix environment, only grant sessions for approved, authenticated individuals, from approved locations\n• Provide command line filtering and protect against errant or malicious commands\n• Log user activities to ensure compliance across the organization\n• Manage and record privileged activity remotely for complete audit trails on problems, changes, and incidents. BeyondTrust gives you an unimpeachable audit trail of all session activity.\n• Get alerts and notifications of unusual activity so you can be proactive in your security, such as adjusting configurations or pausing or terminating a suspicious in-progress session.\n• Enable centralized and unified administration to help your IT staff work as effectively and securely as possible BeyondTrust supports you with over 30 years of innovation in server privilege management, and the most complete PAM platform. With BeyondTrust, you can mature privileged access security controls across your entire IT estate at the pace whichworks for you. Below, are three BeyondTrust solutions which are key to several areas in the Unix and Linux security space. It is important to note, you do not need to start in any specific order when starting your PAM journey, but BeyondTrust can certainly recommend your next steps based on where you are today. Additionally, our solutions integrate into a single platform and can be combined to unlock additional synergies.\n• Privilege management for Unix/Linux, for true least privilege on Unix, Linux, and network systems. Controls root access, audits user activity, and enables session monitoring and replay capabilities.\n• Active Directory Bridge, for extending Microsoft Active Directory authentication to Unix and Linux systems, for single sign on and Group Policy configuration.\n• Password Safe, for discovering, auditing, and monitoring privileged accounts, credentials, and secrets of all types, such as root accounts, SSH keys, local administrator, DevOps tools, to name a few."
    }
]