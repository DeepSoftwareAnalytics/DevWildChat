[
    {
        "link": "https://stackoverflow.com/questions/16469410/data-compression-algorithms",
        "document": "There are a ton of compression algorithms out there. What you need here is a lossless compression algorithm. A lossless compression algorithm compresses data such that it can be decompressed to achieve exactly what was given before compression. The opposite would be a lossy compression algorithm. Lossy compression can remove data from a file. PNG images use lossless compression while JPEG images can and often do use lossy compression.\n\nSome of the most widely known compression algorithms include:\n\nZIP archives use a combination of Huffman coding and LZ77 to give fast compression and decompression times and reasonably good compression ratios.\n\nLZ77 is pretty much a generalized form of RLE and it will often yield much better results.\n\nHuffman allows the most repeating bytes to represent the least number of bits. Imagine a text file that looked like this:\n\nA typical implementation of Huffman would result in the following map:\n\nSo the file would be compressed to this:\n\n18 bytes go down to 5. Of course, the table must be included in the file. This algorithm works better with more data :P\n\nAlex Allain has a nice article on the Huffman Compression Algorithm in case the Wiki doesn't suffice.\n\nFeel free to ask for more information. This topic is pretty darn wide."
    },
    {
        "link": "https://medium.com/@xinyijun/performance-optimization-two-perspectives-65b08bb3bf6b",
        "document": "We usually evaluate system performance from two dimensions: latency and throughput. Latency measures how quickly a system responds to inputs, while throughput measures how many inputs a system can handle within a specific time interval. High performance means achieving low latency, high throughput, or both. Low latency enhances user experience, while high throughput improves resource efficiency and reduces costs. Our goal is to achieve the highest performance with the resources available. While C++ is designed for high-performance programming, simply writing a program in C++ does not guarantee optimal performance.\n\nThis article focuses on improving the performance of C++ programs to achieve the best possible results. It begins by highlighting the importance of profiling and verification in performance optimization, then explores optimization strategies from both top-down and resource perspectives. Finally, it addresses common performance bottlenecks and their solutions, concluding with several general optimization techniques.\n\nWhy Are Profiling and Verification So Important?\n\nBefore performing performance optimization, we should perform profiling to identify the hot path and perform targeted optimizations. Why? Because often, we may need to sacrifice something, like code readability, to gain better performance. We want to achieve the greatest benefits at the cost of readability in a small piece of code. We don’t want to ruin the readability of a large amount of code for minimal returns. Profiling helps us locate the code that is most worth optimizing.\n\nAfter performing performance optimizations, we must verify our efforts. Otherwise, the results may not be what we want because many factors can affect performance and influence each other.\n\nNote: This article will not dive into tools like and .\n\nThe top-down perspective divides software into several layers: Business Layer, Algorithm Layer, Programming Language Layer, Operating System Layer, and CPU Layer. It optimizes performance starting from the top layer to the bottom layer. Let’s explore these layers one by one.\n\nWe should analyze the specific business logic as clearly as possible and try our best to simplify it. This can not only decrease complexity, shorten development time, and reduce maintenance costs, but it can also improve performance. Generally, the higher the layer, the more significant the optimization effects. Therefore, a small optimization in the business layer may result in a big performance improvement for the whole system.\n\nA good algorithm can greatly improve a system's performance. For example, an algorithm with time complexity is usually much faster than one with time complexity.\n\nMost of the time, we don’t need to invent a new algorithm. Instead, we just choose or combine existing algorithms to meet our needs. The key is to compare our business logic with common patterns to identify specific characteristics that can be leveraged to optimize general algorithms.\n\nWhen optimizing algorithms, consider these two questions: Has the current algorithm’s performance reached the business’s theoretical upper limit? Has the current algorithm fully exploited the business’s characteristics? If the answer to either question is no, it means there may still be room for improvement.\n\nThis layer depends on the specific language. This article focuses on C++. We’ll discuss optimization based on two categories: language features and the usage of the standard library.\n\nOptimization based on language features:\n\nMemory Allocation/Deallocation: Stack memory allocation is much faster than heap memory allocation, and so does memory deallocation. \n\nVirtual Function: Calling a virtual function requires accessing the virtual table to get its address, which requires one more memory access compared to a non-virtual function. Consider using static polymorphism instead.\n\nArgument Passing: For large objects (with a size larger than two machine words), passing arguments by pointer or reference may be faster than passing by value. However, for smaller objects, passing by value is more efficient.\n\nReturn Value Optimization(RVO): The compiler can do RVO with some constraints: The caller should assign the return value to a newly created object in the assignment statement, and all return statements in the function should return the same object.\n\nMove: Use move semantics to avoid copying large objects.\n\nKeyword : Use the keyword to define variables or functions whenever possible to help the compiler perform compile-time computations.\n\nKeyword : Use the keyword to define variables or member functions whenever possible to help the compiler perform optimization.\n\nKeyword : Use the keyword to avoid function call overhead and enable cross-function optimization. However, it’s just a hint, and the compiler might not actually inline the function. To force inlining, consider using compiler-specific extensions.\n\nCast: Use instead of whenever possible, as static_cast has no overhead, while does a runtime type check. Additionally, and have no overhead.\n\nFunction scope: Define functions within an anonymous namespace or use the keyword when possible. This can help the compiler eliminate unused functions, enable cross-function optimization, reduce code size, and improve code cache hit rate.\n\nVariable Scope: Keep the variable scope as small as possible and define variables as close as possible to their first use. This helps avoid unnecessary initialization.\n\nIncrement Operation: Prefix increment might be faster than postfix increment because the former returns a reference, while the latter returns a value and may require a move or copy.\n\nLocal Static Initialization: In C++11, local static initialization is thread-safe, but this comes with a cost: every call must check whether the variable has been initialized.\n\nEmbedding assembly code: Sometimes, there is no equivalent C/C++ construct for a specific assembly instruction, or the compiler does not generate optimal code. In such cases, we can embed assembly code within C/C++ programs.\n\nOptimization based on the standard library’s usage:\n\nSmart Pointer: A is as fast as a raw pointer, while a involves additional overhead due to extra memory consumption and atomic operations. If you need a , use instead of the constructor. performs one heap memory allocation, while the constructor requires two.\n\nProvide Hint: Provide a hint to call functions like to avoid reallocations or rehashing.\n\nHeterogeneous Lookup: For Associative containers, use the feature to avoid temporary object creation in functions like . \n\nConstruction In Place: Functions like enable the construction of objects in place, while might create temporary objects and require an extra move or copy operation.\n\nAvoid Default Construction: If the key doesn't exist, the operator[] for std::map calls the default constructor of the value type. Consider using instead.\n\nAvoid Copy: Use to avoid copy operations.\n\nThis layer depends on the specific operating system. We’ll focus on Linux. The operating system abstracts the underlying hardware and provides services to applications through system calls. Generally, it’s advisable to avoid making system calls whenever possible to enhance performance, as they are expensive: switching from user mode to kernel mode and back involves saving and restoring many states, which is time-consuming. However, there are instances where system calls can actually help improve performance. Let’s go through some of them.\n\nThread Affinity: The function sets thread affinity so that a thread runs only on specific CPU cores. This can increase the efficiency of the on-core CPU cache and TLB.\n\nZero Copy: The system call can be used to implement zero-copy. If the and system calls are used instead, there will be two more copies and one more system call.\n\nLock Physical Pages: The system call can prevent physical pages from being reclaimed and ensure the performance of key applications.\n\nAccessing Files: The system call can map file content into user address space. This can avoid many calls to the and system calls.\n\nAvoid Unnecessary Copy: The system call requires copying the entire file descriptor set from user space to kernel space and back each time. To avoid these unnecessary copies, use the family of system calls.\n\nThe CPU’s performance depends largely on the efficiency of the instruction pipeline and the cache. We’ll focus on improving performance from these two aspects.\n\nPerformance of the Instruction Pipeline\n\nThe two most important factors slowing down the performance of instruction pipelines are branch statements¹ and data dependency². Below are some optimization techniques targeted at improving the instruction pipeline’s performance, mainly from these two aspects.\n\nLookup Table: Use a lookup table to eliminate branch statements completely.\n\nMerge Conditions: Use bitwise operations to merge conditions and reduce branches from the CPU’s perspective. For example, use instead of .\n\nThe and macros: Provide hints to the compiler to generate a better instruction layout, which may avoid a branch prediction failure.\n\nConditional Move: Compared to a prediction failure, the conditional move instruction might perform much better. Usually, the compiler does a good job of using conditional move instructions, checking the assembly code to confirm this.\n\nLoop Unrolling: It can reduce stalls caused by data dependency and the overhead of a loop.\n\nInline Functions: Inline functions can avoid function call overhead and enable cross-function optimization. It may also mediate the impact of data dependency and benefit the instruction cache.\n\nSIMD Intrinsics: A SIMD instruction can operate on multiple copies of data simultaneously, therefore enhancing the performance. SIMD intrinsics are C wrappers of the underlying SIMD instructions.\n\nAvoid Unnecessary Time-consuming Operations: \n\n— Division is super slow, consider using multiplication instead. \n\n— Conversion between single-precision and double-precision floats is super slow, avoid mixing different float types in an expression.\n\n— Conversion from floating number to integer is super slow.\n\nPerformance of the Cache\n\nThe two most important factors impacting CPU cache performance are cache hit rate and cache coherence³. Leveraging spatial and temporal locality can increase the cache hit rate, and minimizing data sharing can reduce the cost of cache coherence. Below are some optimization techniques focused on these aspects.\n\nSequential Memory Access: Sequential memory access has a good cache hit rate because of its spatial locality. For example, accessing a matrix by row may be much faster than accessing it by column.\n\nData Structure: Use cache-friendly data structures like arrays instead of linked lists, and avoid mixing data and metadata together.\n\nData Type: Use smaller data types whenever possible to achieve a better cache hit rate.\n\nMemory Layout: An inappropriate member sequence in a class or struct can result in padding space, causing unnecessary waste. Members used together should be defined next to each other to improve data cache performance.\n\nData Alignment: Avoid laying out data across cache lines. While the compiler and most libraries typically handle this, it can occur when we manually lay out data.\n\nWarm Up: Using appropriate data to warm up can increase the hit rate of various caches, including the instruction cache, data cache, branch predictor cache, and TLB cache.\n\nData Prefetch: The CPU's automatic prefetch works well most of the time. If it doesn’t, consider inserting prefetch instructions manually.\n\nFalse Sharing: This occurs when two threads operate on different data that reside in the same cache line. If one CPU core modifies its data, it will invalidate the same cache line in the other CPU cores, requiring a data refresh. This greatly degrades performance. To avoid false sharing, ensure that frequently accessed data by different threads are placed in separate cache lines.\n\nData Sharing: Similar to false sharing, real data sharing between threads also greatly degrades CPU cache performance. Consider using local data and merging it at the end.\n\nError Handling: To improve the performance of the instruction cache, place all error-handling branches in a separate function and call it in the branch. Ensure the error-handling function is not inlined by using appropriate compiler attributes.\n\nTLB Cache: Use big pages to increase the TLB hit ratio, this can also greatly reduce the number of page faults.\n\nThere are usually five types of resources: CPU, GPU (not discussed in this article), memory, disk, and network. At any time, there is always a bottleneck resource in a system. Once we identify this bottleneck, we can optimize its performance to enhance the overall system performance. There are two ways to achieve this: improving the efficiency of the bottleneck resource or exchanging it with other resources.\n\nThe bottleneck resource doesn’t necessarily have high throughput. For example, random disk operations can cause low throughput, making the disk a bottleneck.\n\nCPU: Use the command to check whether a thread's CPU usage remains consistently high.\n\nMemory: If a swap partition is being used, it indicates significant pressure on the memory. Another precise way to identify whether memory is a bottleneck is to use the command to check if the number of major page faults⁴ for a process continuously increases rapidly.\n\nDisk: Use the command to check whether the throughput has approached its maximum. If not, check whether the value is consistently high.\n\nNetwork: Use to check whether the bandwidth is approaching its maximum.\n\nCPU\n\nPlease refer to the CPU section under the top-down perspective.\n\nMemory\n\nAvoid Copy: Prevent copying large amounts of memory, share or move the data instead. \n\nCopy on Write: Postpone copying until it’s necessary. The Linux kernel uses this technique to save memory. Some older implementations of also utilize copy-on-write.\n\nLock Physical Pages: Prevent frequent swapping in and out.\n\nMemory Access Pattern: Use the system call to inform the kernel about the memory access pattern to use memory more efficiently.\n\nBig Page: Enable this feature to reduce the number of physical pages used for the page table overhead.\n\nDisk\n\nSequential Access: For optimal throughput, access the disk sequentially.\n\nAppropriate Filesystem: Choose an appropriate filesystem for your business to improve performance.\n\nNetwork\n\nAppropriate Protocol: Choosing an appropriate protocol for your business may improve performance.\n\nProtocol Stack Parameters Tuning: Tune the protocol stack parameters to improve network latency or throughput.\n\nUse One Resource to Exchange the Bottleneck Resource\n\nCPU Exchange Memory\n\nCompression & Decompression: If a file is randomly accessed and too large to be entirely loaded into memory, consider compression and decompression.\n\nMemory Exchange CPU\n\nCache Result: Use memory to cache the results of complex computations.\n\nCPU Exchange Disk\n\nCompression & Decompression: Use this to speed up loading and storing files on the disk.\n\nCPU Exchange Network\n\nCompression & Decompression: Use this to decrease bandwidth usage and reduce network latency.\n\nMemory Exchange Disk\n\nKernel Parameter Tuning: Tuning the kernel to cache more file contents to avoid frequent swapping in and out.\n\nBuild Index: An Index consumes extra memory but can help reduce unnecessary disk operations.\n\nMemory Exchange Network\n\nLarger Socket Buffer: Allocating more memory to the socket buffer may improve both latency and throughput.\n\nI/O operations are typically slow, and synchronous I/O operations can significantly degrade system performance. It’s often better to use asynchronous I/O operations instead. For example, logging is usually performed asynchronously.\n\nHeap memory management is complex, leading to time-consuming memory allocation and deallocation. Here are some ways to optimize it:\n\nUse Stack Memory: Stack memory management is simple and handled by the compiler. All allocations and deallocations within a single function can often be merged into a single instruction separately, resulting in extremely fast performance. Additionally, the stack typically offers better CPU cache performance than the heap. However, the default thread stack size is about 10MB, so pay attention to avoid a stack overflow.\n\nMerge Memory Allocations: Combining multiple memory allocations into a single one reduces the number of allocations and might also improve CPU cache performance. \n\nUse Appropriate Memory Management Library: Consider using or instead of . \n\nCustomize Memory Allocator: A custom memory allocator designed to fit your application’s specific needs can greatly improve the performance of heap memory operations. \n\nAsync Deallocation: Memory deallocation doesn’t need to be done synchronously; it can be performed asynchronously in a separate thread.\n\nWhen multiple threads share the same data, they must acquire a lock to synchronize access, which is a common bottleneck.\n\nAvoid Sharing Data: This approach can fundamentally solve the problem, but it might be difficult or even impossible to implement.\n\nKeep Critical Section Minimum: Decouple operations and move unrelated operations outside the critical section to minimize the time a lock is held.\n\nUse Read-write Lock: This can improve read concurrency compared to a mutex.\n\nLock with Small Granularity: Employ multiple smaller locks instead of a single big lock.\n\nAtomic Operations: When applicable, using atomic variables can yield better performance.\n\nLock-free Algorithm: Difficult to implement, error-prone, and even impossible.\n\nA system call is expensive and frequently making system calls is a common bottleneck. Minimize their use as much as possible.\n\nFile Operations: Use the system call to map files into the user address space and avoid frequently calling the and system calls.\n\nSend Data: Use the system call to directly send file content and avoid calling both the and system calls.\n\nBatch operations: Use the / system calls to batch the operations.\n\nReuse Connections: Use connection pools to avoid frequently creating and destroying network connections.\n\nMultithread/multiprocess: Use multithread/multiprocess to fully leverage a multi-core CPU.\n\nThread Switch: Since threads in a process share the same address space, switching between them improves the efficiency of the on-core CPU cache and TLB compared to threads from different processes. Consider using multithreading instead of multiprocessing.\n\nAsync Write Operation: Making write operations asynchronous when their results are not needed immediately can improve performance.\n\nCache Results: Cache the results of time-consuming operations for later reuse. For example, cache the results of complex computations or database queries.\n\nBuild Index: Building an index can speed up read operations while slowing down write operations.\n\nBatch Operations: Batch opeartions to improve throughput. \n\nAppropriate Library: Choose a library that performs optimally for a specific situation.\n\nAppropriate Compiler: Choose an appropriate compiler. For example, use the compiler if using an Intel CPU.\n\nCompiler Option: Choose appropriate compiler optimization options. For example, using the option allows application code to use one more general register, which can boost performance but makes debugging more difficult.\n\nStatic Library: Compared to a static library, calling a function in a dynamic library incurs a setup cost for the first call. Subsequent calls involve an additional jump, and the instruction cache is also impacted.\n\nHyperthreading: Enable it for potentially increased throughput; disable it for latency.\n\nProfile-Guided Optimization⁵: Depending on the specific situation, this optimization technical may be useful.\n• For a conditional branch statement, the CPU must decide which branch to execute before knowing the condition's result. While modern CPUs typically perform well in branch prediction, they can sometimes fail. A branch prediction failure causes the entire instruction pipeline to stall, resulting in a pipeline restart and a loss of 10 to 30 CPU cycles.\n• Data dependency causes the CPU to stall while waiting for the result of the previous instruction, even if free processor units are available to execute instructions.\n• When multiple CPU cores share the same data, modifying the data on one core requires refreshing the local cache on the other cores. If this occurs frequently, CPU performance can degrade significantly.\n• A major page fault occurs when the system needs to allocate a physical page and load its contents from the disk.\n• This optimization method involves running a program with a representative dataset to generate profiling data. The program is then recompiled using this profiling data to produce an optimized version."
    },
    {
        "link": "https://github.com/MapleCCC/liblzw",
        "document": "LZW is an archive format that utilizes power of LZW compression algorithm. LZW compression algorithm is a dictionary-based loseless algorithm. It's an old algorithm suitable for beginner to practice.\n\nInternal algorithm processes byte data. So it's applicable to any file types, besides text file. Although it may not be able to achieve substantial compression rate for some file types that are already compressed efficiently, such as PDF files and MP4 files. It treats data as byte stream, unaware of the text-level pattern, which makes it less compression-efficient compared to other more advanced compression algorithms.\n\nLZW compression algorithm is dynamic. It doesn't collect data statistics before hand. Instead, it learns the data pattern while conducting the compression, building a code table on the fly. The compression ratio approaches maximum after enough time. The algorithmic complexity is strictly linear to the size of the text. A more in-depth algorithmic analysis can be found in the following sections.\n\nAn alternative implementation that utilizes more efficient self-made customized , and to replace C++ builtin general-purpose , and can be found in the branch . Future enhancement includes customized functions to replace builtin general-purpose .\n\nPre-built binaries (Windows only) are available on the Releases page. Release version conforms to the semantic versioning convention.\n\nPrerequisites: Git, Python3.6+, , and a modern C++ compiler: or .\n\nAlternatively, you can build as a static library for embedding in other applications.\n• When commiting new code, make sure to apply format specified in config file.\n• Don't directly modify . It's automatically generated from the template . If you want to add something to README, modify the template instead.\n• Also remember to add to as pre-commit hook script.\n\nPrerequisites: Git, Python3.6+, , , and a modern C++ compiler: or .\n\nThe pre-commit hook script basically does four things:\n• Transform math equation in to image url in\n• Append content of and to\n\nBesides relying on the pre-commit hook script, you can manually format code and transform math equations in .\n\nThe advantages of pre-commit hook script, compared to manual triggering scripts, is that it's convenient and un-disruptive, as it only introduces changes to staged files, instead of to all the files in the repo.\n• Python3.6+, Pytest and Hypothesis are needed for integrate test You can optionally create a virtual environment for isolation purpose $ python -m virtualenv .venv $ .venv/Scripts/activate $ python -m pip install -U pytest, hypothesis $ make integrate-test It makes very thorough testing. It currently take about ten to twenty seconds.\n\nBefore talking about performance, we first conduct some complexity analysis. This help us understand the internal structure of the problem, and which and how various parts of the task contribute to the performance hotspots.\n\nLZW algorithm treats data as binary stream, regardless of its text encoding. During the algorithm process, a running word is maintained. Before algorithm starts, the running word is empty. Every new byte consumed by the algorithm would be appended to the running word. The running word would be reset to some value under some circumstances.\n\nFor each byte consumed by the algorithm, deciding upon whether the running word is in the code dict, the algorithm has two branches to go.\n\nIf the new running word formed by appending the new byte at the end is in the code dict, the algorithm proceeds to branch, which basically do nothing, and continue to next iteration. On the other hand, if the new running word constituted by the new byte is not in the code dict, the algorithm goes to branch.\n\nIn the branch, three things happen. First, the code in the code dict corresponding to the old running word (before the new byte is appended) is emitted. Then the new running word is entered into the code dict as a new string, and assigned a new code, ready to use in the future. Lastly, the running word is reset to contain only the new byte. The algorithm then proceeds to the next iteration.\n\nThe cost model for these two branches are respectively:\n\nSuppose the source text byte length is . Among the bytes consumed by the algorithm, there are bytes for whom the algorithm goes to branch A, and goes to branch for the other bytes.\n\nFor simplicity, we assume that , , , , and are fixed cost that don't vary upon different string sizes. This assumption is invalid/broken for large input, but that kind of case is very rare, so we are good with such hurtless simplification, as long as the strings are of reasonable lengths.\n\nThe total cost model of compression process can then be summarized as:\n\nFor input data that doesn't have many repeated byte pattern, is small compared to (i.e. ). The cost model approximates to:\n\nIf the underlying data structure implementation of code dict is hash table, then and are both operations. The total cost is then.\n\nFor input data that has many repeated byte pattern, is not negligible compared to . The largest possible comes from input consisting of single byte pattern. In such case, (The detailed deduction process is delegated to the readers). The total cost is still .\n\nWe see that the time complexity of compression algorithm is not affected by the byte repetition pattern. It's always linear time. This nice property holds true as long as the underlying implementation of the code dict scales in sublinear factor.\n\nContrary to the compression algorithm, LZW decompression algorithm consumes a stream of codes, decodes them on the fly, and emitted byte data. A string dict is built and maintained along with the process. Similar to the compression process, a running word is maintained during the process. The running word is empty at the beginning.\n\nDeciding on whether the code can be found in the string dict, the algorithm has two branches to go.\n\nIf the code currently consumed can be found in the string dict, the algorithm goes to branch , where basically four things happen. First, the decoded string coresponding to the current code is lookuped in the string dict. The decoded string is emitted as byte data. Then a new string constructed by appending the first byte of the decoded string to the running word is added to the string dict, ready for future use. The running word is then reset to be the decoded string. The algorithm then goes on to the next iteration.\n\nOn the other hand, if the code currently consumed is not present in the string dict, a special situation happens and needs special care. Algorithm goes to branch , where three things happen. A new string constructed by appending the first byte of the running word to the running word is added to the string dict for future use. This new string is emitted as byte data. Then the running word is reset to be the new string. The algorithm then goes on to the next iteration.\n\nThe cost model for these two branches is then:\n\nSuppose the code stream length is . Among the codes consumed by the algorithm, there are codes for whom the algorithm goes to branch A, and goes to branch for the other codes.\n\nFor simplicity, we assume that , , , , and are fixed cost that don't vary upon different string sizes. This assumption is invalid/broken for large input, but that kind of case is very rare, so we are good with such hurtless simplification, as long as the strings are of reasonable lengths.\n\nThe probability of going to branch is relatively rare, so the major cost comes from branch . The total cost model for the decompression algorithm is then:\n\nIt's the same with that of the compression algorithm! The total cost model for the decompression algorithm turns out to be identical to that of the compression algorithm! They are both linear . (under the precondition that the underlying implementation of string dict and code dict scales in sublinear factor)\n\nThe algorithmic detail of LZW compression algorithm doesn't have large parameter tuning space or rich variants for twisting. The only performance improvement we can try to make is concentrated on the underlying data structure implementation of the code dict and the string dict. The exported interface consists of , , and . Many data structure supports these three operations. When deciding which data structure to use, we want the cost to be as small as possible, because these three operations is invoked in nearly every iteration of the algorithm process. Three candidates are to be examined in detail:\n• Balanced tree has the advantages of being easy to implement, while having reasonable average performance bound for many operations. It's too mediocre for our demanding task.\n• Hash table, if implemented carefully, can exhibit excel performance in various frequent operations. The major drawback of hash table data structure is that it's space demanding, and its performance is sensitive to input data pattern. The good news is that we can tune the hashing function and collision resolution scheme to workaround skewed input data.\n• Trie is a specialized data structure especially good at handling text related data. Its major drawback is that correct and efficient implementation needs some attention and care.\n\nWe break the trade-off and decide to use hash table to implement the code dict and string dict.\n\nFor simplicity, we do not customize our own hashing functions. is used instead. This could serve as a minor optimization candidate for future.\n\nIdeally, the hash table implemented code dict and string dict exhibit time complexity for all , , and operations.\n\nInstead of loading the whole file content into memory, which could lead to large memory footprint and hence resource pressure, our code adopts a more flexible IO method. We load input data in stream style. We read in data when needed and no too much more. This ensures that our code scales well with large input data size. Also our code is more memory space efficient.\n\nA little trick we use here is to reserve sufficient space for the code dict and string dict at initialization. This effectively reduce the cost of resizing hash table, which is expensive, to say the least."
    },
    {
        "link": "https://cs.stackexchange.com/questions/91384/how-would-the-use-of-an-external-lookup-table-affect-file-compression",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://stackoverflow.com/questions/1138345/an-efficient-compression-algorithm-for-short-text-strings",
        "document": "Huffman has a static cost, the Huffman table, so I disagree it's a good choice.\n\nThere are adaptative versions which do away with this, but the compression rate may suffer. Actually, the question you should ask is \"what algorithm to compress text strings with these characteristics\". For instance, if long repetitions are expected, simple Run-Lengh Encoding might be enough. If you can guarantee that only English words, spaces, punctiation and the occasional digits will be present, then Huffman with a pre-defined Huffman table might yield good results.\n\nGenerally, algorithms of the Lempel-Ziv family have very good compression and performance, and libraries for them abound. I'd go with that.\n\nWith the information that what's being compressed are URLs, then I'd suggest that, before compressing (with whatever algorithm is easily available), you CODIFY them. URLs follow well-defined patterns, and some parts of it are highly predictable. By making use of this knowledge, you can codify the URLs into something smaller to begin with, and ideas behind Huffman encoding can help you here.\n\nFor example, translating the URL into a bit stream, you could replace \"http\" with the bit 1, and anything else with the bit \"0\" followed by the actual procotol (or use a table to get other common protocols, like https, ftp, file). The \"://\" can be dropped altogether, as long as you can mark the end of the protocol. Etc. Go read about URL format, and think on how they can be codified to take less space."
    },
    {
        "link": "https://stackoverflow.com/questions/51624933/how-exactly-do-lookup-tables-work-and-how-to-implement-them",
        "document": "What you're looking for is an efficient mechanism by which you can look up the value that corresponds to a given key.\n\nYour current mechanism (a long list of if/else-if commands) is rather inefficient, in that if you have N possible values to choose from, you will (on average) have to compare your candidate key against (N/2) other keys before you find the one that matches and you can stop looking. (This is known as O(N) complexity)\n\nSo what are the other choices?\n\nThe simplest one is literally just an array of values, e.g.\n\n... with a lookup table like that, you take a key (which in this case is a number between 0 and 999, inclusive), and look up the corresponding value with a single array-lookup:\n\nThat's super-efficient (O(1) complexity -- it always finishes in constant time, regardless of how big the array is!), but it only works in cases where your keys are unsigned integers in a continuous (and relatively small) range of values. For example, if your keys were strings, this approach wouldn't apply.\n\nFor more flexibility, the next option would be STL's . gives you fast key->value lookups from any key-type to any value-type. Internally it is implemented as a tree: each key-value pair is inserted into the tree in such a way that the tree remains sorted with the smallest keys at the left of the tree and the largest keys at the right. Because of that, looking up a key (and its associated value) in a is just a matter of starting at the tree's root node and comparing the key at that node to the key you are looking up: is it less than your key? Then move to the right-hand child. Or it greater than your key? Then move to the left-hand child. Repeat that until you get to the bottom of the tree, at which point you'll either find the key-value pair you were looking for or you'll find that it's not present. This is an algorithm of O(log(N)) complexity, because for a tree with N values in it, it takes log(N) comparisons for the lookup to complete. O(log(N)) is considered pretty good efficiency.\n\nThe final data structure you mentioned is a hash table (as seen in ). A hash table does things a bit differently -- internally it is an array, but in order to avoid the limitations of the lookup-table approach, it also comes with an algorithm for figuring out where in its array a given key/value pair is to be stored. It does this by calculating a hash code for the key-object you pass in -- and then using that code to compute an offset into the array (e.g. ) and looking at that slot in the array to see if the requested key-value pair is there. If it is, then it's done (O(1) performance again!); or if the slot is empty, then it knows that your key isn't in the table and can return failure immediately (O(1) again). If the slot is occupied by some other key/value pair, then the hashtable will need to fall back to another algorithm to sort out the ; different hash tables handle that different ways but it's generally still fairly efficient."
    },
    {
        "link": "https://github.com/MapleCCC/liblzw",
        "document": "LZW is an archive format that utilizes power of LZW compression algorithm. LZW compression algorithm is a dictionary-based loseless algorithm. It's an old algorithm suitable for beginner to practice.\n\nInternal algorithm processes byte data. So it's applicable to any file types, besides text file. Although it may not be able to achieve substantial compression rate for some file types that are already compressed efficiently, such as PDF files and MP4 files. It treats data as byte stream, unaware of the text-level pattern, which makes it less compression-efficient compared to other more advanced compression algorithms.\n\nLZW compression algorithm is dynamic. It doesn't collect data statistics before hand. Instead, it learns the data pattern while conducting the compression, building a code table on the fly. The compression ratio approaches maximum after enough time. The algorithmic complexity is strictly linear to the size of the text. A more in-depth algorithmic analysis can be found in the following sections.\n\nAn alternative implementation that utilizes more efficient self-made customized , and to replace C++ builtin general-purpose , and can be found in the branch . Future enhancement includes customized functions to replace builtin general-purpose .\n\nPre-built binaries (Windows only) are available on the Releases page. Release version conforms to the semantic versioning convention.\n\nPrerequisites: Git, Python3.6+, , and a modern C++ compiler: or .\n\nAlternatively, you can build as a static library for embedding in other applications.\n• When commiting new code, make sure to apply format specified in config file.\n• Don't directly modify . It's automatically generated from the template . If you want to add something to README, modify the template instead.\n• Also remember to add to as pre-commit hook script.\n\nPrerequisites: Git, Python3.6+, , , and a modern C++ compiler: or .\n\nThe pre-commit hook script basically does four things:\n• Transform math equation in to image url in\n• Append content of and to\n\nBesides relying on the pre-commit hook script, you can manually format code and transform math equations in .\n\nThe advantages of pre-commit hook script, compared to manual triggering scripts, is that it's convenient and un-disruptive, as it only introduces changes to staged files, instead of to all the files in the repo.\n• Python3.6+, Pytest and Hypothesis are needed for integrate test You can optionally create a virtual environment for isolation purpose $ python -m virtualenv .venv $ .venv/Scripts/activate $ python -m pip install -U pytest, hypothesis $ make integrate-test It makes very thorough testing. It currently take about ten to twenty seconds.\n\nBefore talking about performance, we first conduct some complexity analysis. This help us understand the internal structure of the problem, and which and how various parts of the task contribute to the performance hotspots.\n\nLZW algorithm treats data as binary stream, regardless of its text encoding. During the algorithm process, a running word is maintained. Before algorithm starts, the running word is empty. Every new byte consumed by the algorithm would be appended to the running word. The running word would be reset to some value under some circumstances.\n\nFor each byte consumed by the algorithm, deciding upon whether the running word is in the code dict, the algorithm has two branches to go.\n\nIf the new running word formed by appending the new byte at the end is in the code dict, the algorithm proceeds to branch, which basically do nothing, and continue to next iteration. On the other hand, if the new running word constituted by the new byte is not in the code dict, the algorithm goes to branch.\n\nIn the branch, three things happen. First, the code in the code dict corresponding to the old running word (before the new byte is appended) is emitted. Then the new running word is entered into the code dict as a new string, and assigned a new code, ready to use in the future. Lastly, the running word is reset to contain only the new byte. The algorithm then proceeds to the next iteration.\n\nThe cost model for these two branches are respectively:\n\nSuppose the source text byte length is . Among the bytes consumed by the algorithm, there are bytes for whom the algorithm goes to branch A, and goes to branch for the other bytes.\n\nFor simplicity, we assume that , , , , and are fixed cost that don't vary upon different string sizes. This assumption is invalid/broken for large input, but that kind of case is very rare, so we are good with such hurtless simplification, as long as the strings are of reasonable lengths.\n\nThe total cost model of compression process can then be summarized as:\n\nFor input data that doesn't have many repeated byte pattern, is small compared to (i.e. ). The cost model approximates to:\n\nIf the underlying data structure implementation of code dict is hash table, then and are both operations. The total cost is then.\n\nFor input data that has many repeated byte pattern, is not negligible compared to . The largest possible comes from input consisting of single byte pattern. In such case, (The detailed deduction process is delegated to the readers). The total cost is still .\n\nWe see that the time complexity of compression algorithm is not affected by the byte repetition pattern. It's always linear time. This nice property holds true as long as the underlying implementation of the code dict scales in sublinear factor.\n\nContrary to the compression algorithm, LZW decompression algorithm consumes a stream of codes, decodes them on the fly, and emitted byte data. A string dict is built and maintained along with the process. Similar to the compression process, a running word is maintained during the process. The running word is empty at the beginning.\n\nDeciding on whether the code can be found in the string dict, the algorithm has two branches to go.\n\nIf the code currently consumed can be found in the string dict, the algorithm goes to branch , where basically four things happen. First, the decoded string coresponding to the current code is lookuped in the string dict. The decoded string is emitted as byte data. Then a new string constructed by appending the first byte of the decoded string to the running word is added to the string dict, ready for future use. The running word is then reset to be the decoded string. The algorithm then goes on to the next iteration.\n\nOn the other hand, if the code currently consumed is not present in the string dict, a special situation happens and needs special care. Algorithm goes to branch , where three things happen. A new string constructed by appending the first byte of the running word to the running word is added to the string dict for future use. This new string is emitted as byte data. Then the running word is reset to be the new string. The algorithm then goes on to the next iteration.\n\nThe cost model for these two branches is then:\n\nSuppose the code stream length is . Among the codes consumed by the algorithm, there are codes for whom the algorithm goes to branch A, and goes to branch for the other codes.\n\nFor simplicity, we assume that , , , , and are fixed cost that don't vary upon different string sizes. This assumption is invalid/broken for large input, but that kind of case is very rare, so we are good with such hurtless simplification, as long as the strings are of reasonable lengths.\n\nThe probability of going to branch is relatively rare, so the major cost comes from branch . The total cost model for the decompression algorithm is then:\n\nIt's the same with that of the compression algorithm! The total cost model for the decompression algorithm turns out to be identical to that of the compression algorithm! They are both linear . (under the precondition that the underlying implementation of string dict and code dict scales in sublinear factor)\n\nThe algorithmic detail of LZW compression algorithm doesn't have large parameter tuning space or rich variants for twisting. The only performance improvement we can try to make is concentrated on the underlying data structure implementation of the code dict and the string dict. The exported interface consists of , , and . Many data structure supports these three operations. When deciding which data structure to use, we want the cost to be as small as possible, because these three operations is invoked in nearly every iteration of the algorithm process. Three candidates are to be examined in detail:\n• Balanced tree has the advantages of being easy to implement, while having reasonable average performance bound for many operations. It's too mediocre for our demanding task.\n• Hash table, if implemented carefully, can exhibit excel performance in various frequent operations. The major drawback of hash table data structure is that it's space demanding, and its performance is sensitive to input data pattern. The good news is that we can tune the hashing function and collision resolution scheme to workaround skewed input data.\n• Trie is a specialized data structure especially good at handling text related data. Its major drawback is that correct and efficient implementation needs some attention and care.\n\nWe break the trade-off and decide to use hash table to implement the code dict and string dict.\n\nFor simplicity, we do not customize our own hashing functions. is used instead. This could serve as a minor optimization candidate for future.\n\nIdeally, the hash table implemented code dict and string dict exhibit time complexity for all , , and operations.\n\nInstead of loading the whole file content into memory, which could lead to large memory footprint and hence resource pressure, our code adopts a more flexible IO method. We load input data in stream style. We read in data when needed and no too much more. This ensures that our code scales well with large input data size. Also our code is more memory space efficient.\n\nA little trick we use here is to reserve sufficient space for the code dict and string dict at initialization. This effectively reduce the cost of resizing hash table, which is expensive, to say the least."
    },
    {
        "link": "https://stackoverflow.com/questions/29890348/how-to-efficiently-decompress-huffman-coded-file",
        "document": "Don't bother writing it yourself, unless this is a didactic exercise. Use zlib, lz4, or any of several other free compression/decompression libraries out there that are far better tested than anything you'll be able to do.\n\nYou are only talking about Huffman coding, indicating that you would only get a small portion of the available compression. Most of the compression in the libraries mentioned come from matching strings. Look up \"LZ77\".\n\nAs for efficient Huffman decoding, you can look at how zlib's inflate does it. It creates a lookup table for the most-significant nine bits of the code. Each entry in the table has either a symbol and numbers of bits for that code (less than or equal to nine), or if the provided nine bits is a prefix of a longer code, that entry has a pointer to another table to resolve the rest of the code and the number of bits needed for that secondary table. (There are several of these secondary tables.) There are multiple entries for the same symbol if the code length is less than nine. In fact, 29-n multiple entries for an n-bit code.\n\nSo to decode you get nine bits from the input and get the entry from the table. If it is a symbol, then you remove the number of bits indicated for the code from your stream and emit the symbol. If it is a pointer to a secondary table, then you remove nine bits from the stream, get the number of bits indicated by the table, and look it up there. Now you will definitely get a symbol to emit, and the number of remaining bits to remove from the stream."
    },
    {
        "link": "https://cs.stackexchange.com/questions/91384/how-would-the-use-of-an-external-lookup-table-affect-file-compression",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://dev.to/polmonroig/how-to-implement-a-simple-lossless-compression-in-c-458c",
        "document": "Compression algorithms are one of the most important computer science discoveries. It enables us to save data using less space and transfer it faster. Moreover, compression techniques are so enhanced that even lossy compressions give us an unnoticeable loss of the data being managed. Nevertheless, we are not going to talk about lossy compression algorithms, but loss-less algorithms, in particular, a very famous one called Huffman Encoding. You may know it because it is used in the JPEG image compression. In this post we will discover the magic behind this compression algorithm, we will go step by step until we end up designing a very simple implementation in C++.\n\nHuffman encoding is a code system based on the prefix property. To encode a text we can move each distinct character of the text into a set, thus we will have a set of characters. To compress each symbol we need a function that is able to convert a character into code (e.g. a binary string). Given a set of symbols Σ we can define a function ϕ: Σ → {0,1}+ that maps each symbol into a code. The symbols in Σ contain the set of distinct characters in the text that needs to be compressed. The most simple prefix encoding would be to assign each letter a binary number, which is a simple ASCII to binary integer conversion. This is a very simple encoding since it is a function that maps a character to itself, but it surely does not compress at all. Prefix codes are very easy to decode, they only need to be read (left-to-right), this ensures us a decompression runtime complexity of O(n). A common way to represent this type of encoding is in a binary tree called the prefix tree.\n\n For example, let's suppose we have the following set and encoding scheme.\n• Encoding: ϕ(A) = 1, ϕ(B)=01, ϕ(C)=000, ϕ(D)=001 then we can represent it using this\n\nAs we can see in the tree, to decode/encode a text (e.g. 00010010….) we must traverse the tree until we find a leaf (where the character is found). If the current prefix is a 0 we must go left and if it is a 1 we must go right. That simple!\n\nAfter creating the tree it is easier to save the equivalencies (code — character) in a simple table.\n\nA prefix tree has the following properties:\n• Labels on the path from the root to a leaf specify the code for that leaf.\n\nOkay, so what do these strange prefix trees have to do we Huffman trees? Well, it turns out Huffman trees are prefix trees, but not just simple prefix trees, they represent the optimal prefix trees. Given a text, an optimal prefix code is a prefix code that minimizes the total number of bits needed to encode that text, in other words, it is the encoding that makes the text smaller (fewer bits = more compression). Note that if you are using a Huffman tree to compress data you should also save the tree in which it was encoded.\n\nNow, how do we find this optimal tree? Well, we need to follow the following steps.\n• Find the frequencies of each character and save them in a table\n• For each character, we create a prefix tree consisting of only the leaf node. This node should contain the value of the character and its frequency in the text.\n• We should have a list of trees now, one per character. Next, we are going to select the two smallest trees, we consider a tree to be smaller to another one if its frequency is lower (in case of a tie we select the one with fewer nodes), and we are going to merge them into one; that is one of the two should become the left subtree and one the right subtree, afterward, a new parent node is created.\n\nWell, that's it, after joining every tree you should be left with only one. If you were paying attention you must have noticed that I didn’t specify how to select the smaller tree from the list of all the trees. That is because it depends on the implementation. The fast way to do it is saving the trees in a MinHeap (priority queue in C++), each insertion and deletion in the heap has an O(log n) complexity but the lookup is always constant. Thus the total complexity of the encoding algorithm is O(n log n) because we must insert a new tree n times.\n\nThe Huffman compression algorithm is a greedy algorithm, that is it always tries to make the optimal choice in a local space, to implement we can create a class called HuffmanTree.\n\nA HuffmanTree will contain, as we said before, the value (character), its weight (frequency), and the size (number of nodes). Finally, it also has a pointer to the left subtree and the right subtree, we used a shared pointer to promote modern C++ smart pointers and avoid worrying about memory leaks.\n\nYou may be wondering why would we want to implement three different constructors? Well, the first one creates a new tree with a given value and weight.\n\nThe second constructor is just a copy constructor, that creates a new one based on the old one.\n\nFinally, we need a constructor that merges two different trees.\n\nThe HuffmanTree class has overloaded a comparison operator, but if you were paying attention, it should be self-explanatory.\n\nFinally, we need to make the core of the algorithm, as you can see we first create a HuffmanTree per character, then we merge trees until we are only left with one.\n\nThat’s all, you have successfully implemented a Huffman Tree, I hope you haven’t lost in the way!"
    }
]