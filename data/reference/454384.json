[
    {
        "link": "https://web.cs.ucla.edu/~ehalperin/cozygene/publications/papers/labels.pdf",
        "document": ""
    },
    {
        "link": "https://researchgate.net/publication/313449295_Three-hop_distance_estimation_in_social_graphs",
        "document": "Abstract —In this paper , we study a 3-hop approach to dis- tance estimation that uses two intermediate landmarks, where each landmark only stores distances to vertices in its local neighborhood and to the other landmarks. We show ho w to suitably represent and compress the distance data stored f or each landmark, for the 2-hop and 3-hop case. Overall, we ﬁnd that some cases, while being comparable or slightly worse than 2-hop methods in others. Furthermore, our light compression schemes improve the practical applicability of both the 2-hop and 3-hop distances: given two vertices, determine the length of a shortest connecting path. In many applications, traversing the graph at runtime to ﬁnd a shortest path is too slow , while precomputing distances between all pairs requires too much memory. In many applications, it sufﬁces to hav e a good estimate of the distance, since this distance is often just a rough proxy for the similarity or relatedness of two nodes (e.g., in expert search in social networks, or search in web graphs) and may be used as one of many features in ranking. For this reason, many researchers have studied how to estimate pairwise distances, which allows for much faster and more space efﬁcient solutions than the exact case. Most of the approaches are heuristic in nature, and are engineered to work well for speciﬁc graphs and query loads. In particular, social graphs tend to have certain properties, such as highly skewed coefﬁcients, and small diameters. Many of the queries issued in social applications tend to be for pairs of nodes that have smaller than average distance. Thus, methods dev eloped for such graphs may not work well on other types of graphs or query loads, and often do not have good worst-case bounds. Probably the most commonly studied and used approach for distance estimation is based on the idea of , which are designated vertices that store their distance from all other nodes in an undirected graph. Given a landmark , 2 Work done while a graduate student at NYU. the distance between any nodes can be upper- bounded by using the triangle inequality; the best approximation is then obtained by taking the minimum The precision of this estimate depends on the number of landmarks as well as their selection strategy; the best land- marks are those that are on, or close to, shortest paths between many pairs and . If and are often close to each other, then it is necessary to have landmarks in many neighborhoods of the graph. Unfortunately, this approach does not scale well as the graph size increases. This is because for larger graphs, we need more and more landmarks to get good estimates, resulting in total space requirements that are super-linear in the number of vertices. Conv ersely, if we are restricted to space linear in the number of vertices, then this limits us to a ﬁxed number of landmarks independent of graph In this paper, we study a estimates via two intermediate landmark nodes, and that can support a larger number of landmarks in linear space. Our methodology is based on a technique described by Bast et al. [1] in the context of road networks, where they apply the above method in a multi-layered fashion to obtain shortest paths , not just distance estimates. In contrast, here we engineer this method to suit the properties of social graphs. The basic idea is very simple: we choose a set of landmarks, and then precompute and store for each landmark its distance from all other landmarks. In addition, for each node we store its distance to a limited number of carefully selected close-by landmarks. Given two nodes , we can then upper-bound are any landmarks for which have been stored. The best estimate is obtained by taking the smallest bound over all possible choices of This approach raises a number of questions that need to be explored. First, we need suitable schemes to select three-hop landmarks, and to choose for which close-by landmarks each node should store distances. Second, we need to show that three-hop methods actually do better. While three-hop methods for the two-hop method, given space linear in the number of vertices , this does not directly imply better estimates. two-hop methods that use a single application of the triangle inequality, potentially canceling out an y beneﬁts due to having\n\nmore landmarks. Finally, gi ven the different structure of two- hop and three-hop methods, we need to decide how to best store and encode the distance data. Simply assuming that each distance costs a ﬁxed number of bits (say one 32-bit int) would not result in a fair comparison of the approaches: In practical settings the absolute space consumption of the data should use the available space in an efﬁcient manner . This includes compressing the data structures in a way that allows fast answering of arbitrary distance queries to allow for more stored information in a given amount of space. In the remainder of this paper, we address these questions. marks, including centrality and degree. W e also study how to select which landmarks a vertex should store distances to. While our baseline simply selects the closest landmarks for each vertex, we show ho w to select a better set of landmarks such that no landmark obscures (i.e., is on the shortest path to) another landmark. W e then study how to reduce the space consumption of the methods, by exploring compression beneﬁt both two-hop and three-hop schemes, and enable a fair comparison that gives each method the same amount of space. Overall, our main technical contributions are as estimation in graphs that provides improvements in precision in many scenarios. (2) W e reevaluate existing methods for landmark selection in the case of three-hop methods. (3) W e show how to store and compress the main data structures used in two-hop and three-hop schemes. (4) W e run extensive terms of estimation accuracy , efﬁciency , and scalability on a number of data sets and query load, showing both strengths and weaknesses of the approach. states the problem and describes the two-hop approach. Then There are three classes of previous work that are closely The state of the art in point-to-point shortest path (PPSP) an search guided by lower bounds. In most common scenarios, this succeeds in reducing the search space for the efﬁciency over unidirectional or bidirectional v ersions of Dijk- stra’s algorithm. Most of the literature on AL T algorithms [2], [3], [4] has focused on road networks, which tend to have properties such as near planarity, lo w degree, and the presence of hierarchy, that beneﬁt these algorithms. Ho wever , they often do not perform well on social networks, web graphs, or co- authorship networks, which do not exhibit these properties. distances in road networks using an idea similar to ours: It deﬁnes a locality relation for pairs of vertices and uses that must retrieve the exact distance for any such pair . They give two implementations of this idea that are tailored to road networks and extend it by recursively applying the scheme (resulting in a -hop technique for ). In contrast, our three-hop data structure, due to the fact that almost all distance queries would be “local” in a social graph, resulting in too many online path computations for the approach in [1]. computation, but at the cost of often very signiﬁcant storage overheads that depends on the structure of the graph. [10], [11], [12], [13], where graphs are mapped to low- functions that approximate distances in the original graph within guaranteed bounds. Another line of work approximates on space requirements and the quality of the approximation. However , most methods are difﬁcult to implement, and no empirical evaluation is giv en. Our methods here do not provide theoretical guarantees, but they are simple to implement and work well in practice in terms of their approximation error and space consumption on common classes of graphs. Landmark-Based Methods: The most widely used heuristic [21], [22], which are subsets of the nodes for which we store distances to all other, or in some cases some selected other, nodes. Giv en a query, we can then apply the triangle inequality on these precomputed distances to get upper and lower bounds on the exact distance. While most work has focused on upper bounds, there has also been some study of obtained from landmarks tend to have a much larger error than Global landmark approaches, which can also be seen as and all other nodes. Although theoretic guarantees on the selected landmarks are possible [22], much accuracy can be gained in practice by selecting landmarks more carefully estimation errors for a given amount of space, but may result in redundancies between different landmarks. Hence [18] sug-\n\n“coverage” by landmarks, and an algorithm that ﬁrst partitions a graph and then chooses good landmarks for each partition. Local landmark approaches store for each landmark only the approach to select landmarks, where each level only stores distances to nodes that are within a certain distance from the landmarks is presented in [16]. Any graph can be preprocessed distance data of size , for any constant number of levels . Using these sketches, any point-to-point query can be answered in time to provide a - approximation of the correct distance. The work in [16] can be simpliﬁed [21], allowing for an easier implementation while retaining theoretical guarantees. In practice the obtained estimation errors are usually much better than the theoretical guarantees [21], but signiﬁcantly larger than the best global landmark methods using the same space [23]. approximate shortest paths, not just estimate their lengths, us- ing landmarks [20], [24]. There is also some loosely related but different work in the database community on query processing Our work can be seen as a combination of ideas from the global and local landmark approaches. Our landmarks store distances to all other landmarks, but only to those non- landmark nodes that are close by. While almost all global and local landmark approaches use two hops, we use three hops, following the work in [1]. W e focus on how to best represent and compress distance data in landmark approaches, which started when we realized that a fair comparison to other methods requires an exploration of this issue. T o the best of our knowledge, ours is the ﬁrst work considering how to compress landmark-related data. There is, however , a signiﬁcant amount of related work on graph compression in the data compression and web search edge locality, and man y other structural properties of the graphs for better compression. One very common idea is to store the adjacency list of one vertex as a delta with respect to that of another close-by or similar vertex, and then apply further compression using, e.g., variable length or run-length Throughout this paper, denotes an undirected graph where is the set of vertices and the set of edges. Unless stated otherwise, we assume graphs to be simple, connecting two nodes is an acyclic sequence of edges | is the number of edges in the sequence. The distance d , v is the minimum length of any path connecting c , v , and the length of a path In this work we address the following problem: Given an | | that allows fast approximate answers to distance queries for arbitrary pairs of vertices . In particular, we want space to be linear with a small constant, i.e., not signiﬁcantly larger than the original graph, and the time for estimations to be constant or at least signiﬁcantly faster than linear. One solution to the above problem are global landmarks, which we shall call landmarks. This approach selects a set of nodes as landmarks and stores for each node in its distance to every node in , resulting in a during preprocessing, we execute a BFS from each landmark, how to select the landmarks. By the triangle inequality, the distance between an y nodes here on upper bounds, which tend to be much closer to the real answer than lower bounds [19]. Given a query , we can thus return an upper bound in time , by performing two Both the two- and three-hop approach require a set of global methods is known to be NP-hard, and various greedy heuristics have been proposed [19]. These algorithms usually impose an ordering on the set of nodes, and then select the top- | nodes as landmarks. W e consider the following methods: Random Order Degree , which is believed to indicate an improved chance to be on many shortest paths; , measuring the inverse of the distances from the particular node to all other node; , the number of shortest paths passing through a vertex; and , which successively selects landmarks that connect to the largest fraction of the remaining graph and then removes them and their neighbors [18]. For the two-hop approach, Betweenness clearly outperforms all other methods, whereas Random Order is very weak [19]. Howev er, Betweenness as well as Closeness are expensive to compute, a drawback that can be remedied W e now introduce our three-hop approach to distance esti- mation. Recall that the two-hop approach stores distances from\n\nevery node to landmark. Ho wever, man y of these stored distances never contribute by being part of the minimal upper bound of any distance estimate. Our three-hop approach tries to remove these unnecessary stored distances. This is of course also the motivation for the local two-hop approach in [21], [16], but as shown in [23] this does not come close to the best global schemes. Our approach stores distances between every pair of landmarks, while ev ery non-landmark node only This decreases space consumption, while trying to preserve T o illustrate the idea, we start with a simpliﬁed version. First, we choose a set of landmarks . For each vertex we store the identiﬁer of the nearest landmark , and the distance d v , l . In addition, we maintain distances between any two The tightness of the bound depends on the position of landmarks and ; the closer they are to a shortest path, the smaller the error. If both landmarks reside on the same shortest path, then we obtain the exact distance. Of course, even if lies on or close to a shortest path to , it may be far away from any shortest path to some other verte x can hence improve this method by choosing more landmarks for each node. Thus, for each node , we pick a small set v .L landmarks from among all landmarks. As we will show , even improves signiﬁcantly over . marks for each vertex. T o efﬁciently compute these landmarks, we initiate a BFS from each landmark, and maintain a priority queue of the nearest landmarks at each vertex. While this solution performs reasonably well, there are situations where there exists a shortest path from containing another we associate is obscured by . Then a waste of space. Instead, we should add another landmark that is further away but not obscured, or store less than landmarks at , if there are no more unobscured landmarks. Our second association algorithm, , thus assigns the closest unobscured landmarks to each vertex. It can be implemented by an extended BFS that maintains two queues (one red for obscured vertices, one blue for unobscured) at no Formally , the overall algorithm now works as follows: First, we select a set of landmarks . Then, for every vertex or RedBlue, and store their distances from distances between all landmarks in in a matrix. Given a The approach analysis is as follows: Landmark selection time depends of course on the methodology. Simple ap- proaches, such as , run in time, while more complicated approaches take much more time but can usually be engineered to run fast enough in practice. Distances between landmarks in are computed in by running BFS from each landmark. Selection of the sets v v can be done during this computation, with an additional factor in the worst case of if using a minimum heap at each node, and a constant factor if using linear time selection instead. (W e implemented the heap version as is small and the worst case unlikely .) At each node, we store at space. For constant , this becomes The three-hop approach can thus asymptotically afford to keep signiﬁcantly more landmarks than the two-hop approach, though we need to see how this affects precision. A fair comparison of the two requires experimentation that varies | | , , and the algorithms for landmark selection and for assignment of landmarks to nodes (in the case of three-hop). Experiments also need to make sure to suitably represent, and if needed compress, the different structures, instead of just Although similar, the two-hop and the three-hop approach store different kinds of data. Hence, a fair practical comparison of the two methods should assign both methods the same structures for both methods to address this issue. For the two-hop approach we need to represent and com- press a array of integers. Each query accesses the two complete rows of the inv olved vertices. Hence, we can employ compression techniques that allow us to decompress bit integer for each cell. Howev er, even in v ery large graphs, distances usually do not exceed a very modest value (due to the distance value in the matrix. A baseline (BL) approach thus W e can do better with two additional observations. First, even if maxdist is, say , , most entries in the matrix are signiﬁcantly smaller. Thus, we can use v ariable-length encod- ing schemes to save additional space. There are many such compression schemes [39]; we use Rice coding and choose its parameter globally for the entire graph. W e call this scheme Space can be reduced further using neighbor-list compres- sion (NLC), an idea previously used in the context of web graph compression [31], [32]. So far, we hav e compressed each row of the array individually . In NLC, we represent a\n\nrow by storing the difference to a neighboring node. Consider two neighboring vertices . For any landmark , the corresponding distances to can differ by at most a row by (i) a pointer to another row , and (ii) a list of values , indicating the differences to that ro w. These values are best represented using a simple Huffman Of course, this requires some lists to be stored by them- selves, without NLC. In general, for each row , we need to (i) decide whether to express it in absolute terms, or relative to a neighbor using NLC, and (ii) if so, choose the right neighbor. W e have to ensure that any reference chains remain acyclic and of reasonable length. This applies equally to two-hop and three-hop methods, and we describe details below . W e ﬁrst discuss how to store the symmetric matrix of all distances between landmarks. W e thus maintain only the upper triangle as an array of 2  entries. This allows accessing any As in the two-hop case, distances in this matrix are quite small. W e thus resort to a ﬁxed-length encoding of suitable size and do not further compress this array. There are tw o reasons. First, the query algorithm does not access this array in a row-wise fashion, but almost at random. Second, this array is usually smaller than the array storing distances between ordinary nodes and landmarks, but is more frequently accesses; thus, speed is more important than compressed size. v , we need to store distances to at most the information identifying these landmarks. Thus, for each vertex , the corresponding landmarks may be different, and we switch from a matrix representation to lists of -pairs where is the ID of a landmark, and is the distance from it. These entries could be compressed individu- ally, or by e xpressing records of one vertex relatively to those of a neighbor since neighbors often share many landmarks. However , this requires somewhat different encoding schemes. In a ﬁrst step, we replace the na a tight encoding of ﬁxed size. Note that the ranges of the distance values and landmark-IDs are quite different. The former is rather small ( ), while the latter is larger (up to | | ). Thus, in a baseline approach (BL), we could store landmark IDs in bits and distances in In the next step, single row compression (SRC) further compacts individual entries. As in two-hop above, distance values are encoded using Rice coding with a globally chosen parameter. In this case, the beneﬁts of Rice encoding are e ven more pronounced as the landmarks are chosen to be close by. In contrast, the lar ger landmark IDs do not beneﬁt as much from this step. However , if we sort entries by landmark IDs, then we can encode the gaps between landmark ids using IDs by summing up the gaps. The list of records for neighboring nodes can differ in two ways. The nodes could be associated with different landmarks; however , the algorithms in Section IV emphasize close-by landmarks and thus neighboring nodes are frequently asso- ciated with the same landmarks. Second, even if landmarks are the same, distances could differ by at most two-hop case. In general, one can expect to identify at least one neighbor whose information is very similar (almost the As in the two-hop case, we can again express records in terms of the differences to a neighboring node. in this case, each record can differ from its counterpart at the neighbor in one of four ways: (i) the distance at is equal, (ii) it deviates by , (iii) it deviates by , and (iv) the particular landmark is not associated with the neighbor. W e thus store an array of two-bit entries encoding this information, and for any entries of type (iv) we store the ID and distance to the For both two-hop and three-hop methods, we need to decide when to use NLC, and if so, which neighbor to choose as reference. W e have three requirements. Reference chains need to be acyclic to be decodable. Second, reference chains must be constrained to a reasonable length for efﬁcient decoding; for this, we introduce a parameter . Third, we would like to get the best compression possible. W e devise a simple heuristic that ensures the ﬁrst two requirements and greedily approximates the third one. The algorithm iterates over all vertices, considering only neighbors that have already been compressed and decides if the vertex is encoded absolutely , or relatively to one of these neighbors. This step employs a gain function described below . Connecting only to neighbors that have been processed ensures there are no cycles. T o restrict the length of reference chains, we only connect to neighbors if the resulting chain length is less than , as suggested in [31], [32]. The gain function expressing the beneﬁt of a relative en- coding is deﬁned as follows. Let of bits required for storing the information at some vertex using SRC. Similarly, let denote number of bits for a NLC encoding based on some already processed neighbor than zero, we would beneﬁt from a relative representation, and should thus encode relatively to the neighbor that leaves more options (with regards to ) for nodes to be processed later, and thus we introduce a threshold minGain for the beneﬁt, below which we always use ab- such that , but v .cl > v .cl . Again, one could equally well decide to encode relatively to , but we should actually fa vor v as this leaves more ﬂexibility for processing later nodes. W e generalize this by introducing a penalty factor for the\n\nWhile this heuristic could possibly be improved, it proved W e conducted a series of experiments to evaluate the effectiveness of three-hop landmarks compared to the tw o- that for small distances and for a small space budget three- show that our proposed compression methods reduce the space For our experimental ev aluation we use three real world graphs obtained from snap.stanford.edu. All graphs contain a large connected component consisting of well over 90% of all vertices. W e preprocess all graphs to obtain the largest laboration network where each node represents a scientist and there is an edge if two scientists coauthored a paper. It has 197K vertices, 950K edges and has diameter 16, while Orkut has 3M vertices, 117M edges and a diameter of 10. For the evaluation of the distance estimation accurac y we generated a set of 50K queries for each of the three graphs. For each query we selected one node at random, and a around ). Our assumption is that we are constrained to store only a constant number of bits per vertex to answer lists cannot be stored in memory, and we also need to estimate d . . . 7 } . Larger distances are not only extremely rare but are also not as important to estimate as accurately as smaller distances. T o measure the overall quality of each method we use a single query load of mixed distances for some of the experiments. In context of social search, distance queries between close vertices are expected to be much more frequent; i.e., the people that social network users search for are usually within their close vicinity. Moreo ver, the error is much more important to be smaller for small distance queries than for larger. W e therefore use a po wer law distribution over the distances and introduce based on that distribution. All algorithms were implemented in Java using fastutil [32] for compression and specialized data structures for integers. The source code is available upon request from the authors. T o make a fair comparison between the -hop and -hop approaches we conducted experiments to select the optimal parameters for each method. For -hop we found that selecting the landmarks with the highest betweenness gives the best distance estimates on average, reproducing the results in [19]. The same holds for global landmark selection in -hop. For the evaluation of the -hop approach we need to choose the size of the global landmark set given a ﬁxed space budget. A larger set makes the assigned id range of the global landmarks larger and therefore the landmark id lists of each vertex harder to compress. Therefore the estimation quality reaches a peak as grows and starts decaying for of values up to for each graph. W e found that the Loc-gowalla, and Orkut respectiv ely, independent of the space budget or number of local landmarks per vertex. For both and -hop landmark methods NLC is most com- monly the optimal compression method. Because it reduces the space consumption more than BL and SRC, NLC gives lower estimate errors for the same space consumption. Also, for -hop RedBlue provides tighter distance bounds than Next for the same space budget. The above two parameters are discussed in detail in the following subsections. W e compare both methods using the parameters from Sec- tion VI-B to show that given a restricted space b udget, - hop outperforms -hop for all graphs. Figure 1 presents the comparison between the two methods for all three graphs for the query load for different individual distances. W e observe that on average, -hop outperforms -hop in all three graphs. W e also observe that the average error is always larger both for smaller distances and for smaller space budgets. Howev er, the behavior of the two methods is not the same for all three graphs. For Astro-phys the beneﬁt of using 3 -hop is signiﬁcantly larger than in the case of Loc-gowalla and Orkut. In fact, for Orkut -hop slightly outperforms -hop initially. The error lines of the two methods intersect close to estimates for the same space budget. For distances Astro-phys and Loc-gowalla the -hop error is smaller than the -hop error for any space consumption but the smaller the space consumption the greater the beneﬁt of using our -hop method. For larger distance there is no beneﬁt in using -hop. For Orkut the beneﬁt of -hop is only for distances and we observe a signiﬁcant disadvantage in using -hop for W e can draw two basic conclusions from these plots. First, the smaller the distance we aim to estimate, the better - hop method works than -hop. Due to the higher number of landmarks for -hop, vertices are more likely to have a distance queries tend to overlap and hence be more exact. In comparison, the smaller number of landmarks in -hop results in higher average errors for such queries, Second, -hop leads to a greater beneﬁt when only limited space is available. This is particularity beneﬁcial for very large graphs since even a small amount of bits per vertex introduces a very signiﬁcant Regarding query processing, we do not observe a signiﬁcant difference in speed. This is because the -hop methods have\n\nFig. 1. A verage absolute error of 2-Hop and 3-Hop (y-axis). T op: For queries when increasing the space availability (x-axis); i.e. the number of bits that the methods store for each vertex. Bottom: For different distance queries (x-axis) and three ﬁxed space b udgets per vertex. For Astro-phys, Loc-gowalla, and Orkut (left to right). linear query time in the number of global landmarks used, whereas -hop has quadratic query time in the number of local landmarks which can be chosen much smaller. proposed for -hop with respect to their average estimation error and their space consumption for different numbers of k . W e experiment with . As can be seen, Most of the time, it decreases the required space per vertex dramatically. In rare cases ( in Orkut, and in Astro-phys), however , the space required for RedBlue is higher than that of Next: RedBlue may choose different or fewer landmarks per vertex and may yield a smaller number of local landmarks and a smaller error for a ﬁxed graph, and . However , it might happen that the resulting landmark id lists cannot be compressed as well, due to larger gaps. Finally , we note that the beneﬁt of using RedBlue is larger for larger values of . This behavior is consistent in all graphs for all . W e applied the compression schemes from Section V on all three graphs for both -hop methods to com- pare their performance. The top plots of Figure 3 show the compression schemes for the -hop method. The three graphs behave very similar for all sizes of landmark sets that are used in the experiment. BL as expected consumes the most space for the same number of landmarks and NLC the least. SRC has a behavior rather similar to NLC. The bottom plots show the corresponding space consumption in the 3 compression schemes for the -hop method, where we ﬁxed the total number of landmarks to be , and , respectively . W e observe a much greater beneﬁt of the two compression schemes in the -hop case: The plots show that NLC is almost twice as good as BL, whereas SRC lies somewhere in between the numbers for -hop, and is almost as good for -hop. In this work, we studied the problem of efﬁcient and framework that ﬁrst selects a set of global landmarks and then assigns a small subset of them to each vertex. Moreover , we describe new compression schemes that apply both to previous two-hop methods and our new three-hop method. W e draw two main conclusions from our experimental evaluation on sev eral real world graphs. First, our three-hop small distances, but does not improve accuracy in some other for two-hop as well as three-hop approaches, and is in fact essential when comparing the practical applicability of the different approaches in a fair way . This [3] A. V . Goldberg and C. Harrelson, “Computing the shortest path: shortest paths by exploiting symmetry in graphs,” in , 2009. [6] A. W . Fu, H. Wu, J. Cheng, and R. C. W ong, “IS-LABEL: an queries on large networks by pruned landmark labeling, ” in ,"
    },
    {
        "link": "https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm",
        "document": "Dijkstra's algorithm ( DYKE-strəz) is an algorithm for finding the shortest paths between nodes in a weighted graph, which may represent, for example, a road network. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.[4][5][6]\n\nDijkstra's algorithm finds the shortest path from a given source node to every other node.[7]: 196–206 It can be used to find the shortest path to a specific destination node, by terminating the algorithm after determining the shortest path to the destination node. For example, if the nodes of the graph represent cities, and the costs of edges represent the average distances between pairs of cities connected by a direct road, then Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. A common application of shortest path algorithms is network routing protocols, most notably IS-IS (Intermediate System to Intermediate System) and OSPF (Open Shortest Path First). It is also employed as a subroutine in algorithms such as Johnson's algorithm.\n\nThe algorithm uses a min-priority queue data structure for selecting the shortest paths known so far. Before more advanced priority queue structures were discovered, Dijkstra's original algorithm ran in time, where is the number of nodes.[8] Fredman & Tarjan 1984 proposed a Fibonacci heap priority queue to optimize the running time complexity to . This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights. However, specialized cases (such as bounded/integer weights, directed acyclic graphs etc.) can be improved further. If preprocessing is allowed, algorithms such as contraction hierarchies can be up to seven orders of magnitude faster.\n\nDijkstra's algorithm is commonly used on graphs where the edge weights are positive integers or real numbers. It can be generalized to any graph where the edge weights are partially ordered, provided the subsequent labels (a subsequent label is produced when traversing an edge) are monotonically non-decreasing.[10][11]\n\nIn many fields, particularly artificial intelligence, Dijkstra's algorithm or a variant offers a uniform cost search and is formulated as an instance of the more general idea of best-first search.[12]\n\nDijkstra thought about the shortest path problem while working as a programmer at the Mathematical Center in Amsterdam in 1956. He wanted to demonstrate the capabilities of the new ARMAC computer.[13] His objective was to choose a problem and a computer solution that non-computing people could understand. He designed the shortest path algorithm and later implemented it for ARMAC for a slightly simplified transportation map of 64 cities in the Netherlands (he limited it to 64, so that 6 bits would be sufficient to encode the city number).[5] A year later, he came across another problem advanced by hardware engineers working on the institute's next computer: minimize the amount of wire needed to connect the pins on the machine's back panel. As a solution, he re-discovered Prim's minimal spanning tree algorithm (known earlier to Jarník, and also rediscovered by Prim).[14][15] Dijkstra published the algorithm in 1959, two years after Prim and 29 years after Jarník.[16][17]\n\nThe algorithm requires a starting node, and node N, with a distance between the starting node and N. Dijkstra's algorithm starts with infinite distances and tries to improve them step by step:\n• Create a set of all unvisited nodes: the unvisited set.\n• Assign to every node a distance from start value: for the starting node, it is zero, and for all other nodes, it is infinity, since initially no path is known to these nodes. During execution, the distance of a node N is the length of the shortest path discovered so far between the starting node and N. 18\n• From the unvisited set, select the current node to be the one with the smallest (finite) distance; initially, this is the starting node (distance zero). If the unvisited set is empty, or contains only nodes with infinite distance (which are unreachable), then the algorithm terminates by skipping to step 6. If the only concern is the path to a target node, the algorithm terminates once the current node is the target node. Otherwise, the algorithm continues.\n• For the current node, consider all of its unvisited neighbors and update their distances through the current node; compare the newly calculated distance to the one currently assigned to the neighbor and assign the smaller one to it. For example, if the current node A is marked with a distance of 6, and the edge connecting it with its neighbor B has length 2, then the distance to B through A is 6 + 2 = 8. If B was previously marked with a distance greater than 8, then update it to 8 (the path to B through A is shorter). Otherwise, keep its current distance (the path to B through A is not the shortest).\n• After considering all of the current node's unvisited neighbors, the current node is removed from the unvisited set. Thus a visited node is never rechecked, which is correct because the distance recorded on the current node is minimal (as ensured in step 3), and thus final. Repeat from step 3.\n• Once the loop exits (steps 3–5), every visited node contains its shortest distance from the starting node.\n\nNote: For ease of understanding, this discussion uses the terms intersection, road and map – however, in formal terminology these terms are vertex, edge and graph, respectively.\n\nThe shortest path between two intersections on a city map can be found by this algorithm using pencil and paper. Every intersection is listed on a separate line: one is the starting point and is labeled (given a distance of) 0. Every other intersection is initially labeled with a distance of infinity. This is done to note that no path to these intersections has yet been established. At each iteration one intersection becomes the current intersection. For the first iteration, this is the starting point.\n\nFrom the current intersection, the distance to every neighbor (directly-connected) intersection is assessed by summing the label (value) of the current intersection and the distance to the neighbor and then relabeling the neighbor with the lesser of that sum and the neighbor's existing label. I.e., the neighbor is relabeled if the path to it through the current intersection is shorter than previously assessed paths. If so, mark the road to the neighbor with an arrow pointing to it, and erase any other arrow that points to it. After the distances to each of the current intersection's neighbors have been assessed, the current intersection is marked as visited. The unvisited intersection with the smallest label becomes the current intersection and the process repeats until all nodes with labels less than the destination's label have been visited.\n\nOnce no unvisited nodes remain with a label smaller than the destination's label, the remaining arrows show the shortest path.\n\nIn the following pseudocode, dist is an array that contains the current distances from the to other vertices, i.e. dist[ ] is the current distance from the source to the vertex . The prev array contains pointers to previous-hop nodes on the shortest path from source to the given vertex (equivalently, it is the next-hop on the path from the given vertex to the source). The code u ← vertex in Q with min dist[u], searches for the vertex in the vertex set that has the least dist[ ] value. Graph.Edges( , ) returns the length of the edge joining (i.e. the distance between) the two neighbor-nodes and . The variable on line 14 is the length of the path from the node to the neighbor node if it were to go through . If this path is shorter than the current shortest path recorded for , then the distance of is updated to .[7]\n\nTo find the shortest path between vertices and , the search terminates after line 10 if = . The shortest path from to can be obtained by reverse iteration:\n\nNow sequence is the list of vertices constituting one of the shortest paths from to , or the empty sequence if no path exists.\n\nA more general problem is to find all the shortest paths between and (there might be several of the same length). Then instead of storing only a single node in each entry of prev[] all nodes satisfying the relaxation condition can be stored. For example, if both and connect to and they lie on different shortest paths through (because the edge cost is the same in both cases), then both and are added to prev[ ]. When the algorithm completes, prev[] data structure describes a graph that is a subset of the original graph with some edges removed. Its key property is that if the algorithm was run with some starting node, then every path from that node to any other node in the new graph is the shortest path between those nodes graph, and all paths of that length from the original graph are present in the new graph. Then to actually find all these shortest paths between two given nodes, a path finding algorithm on the new graph, such as depth-first search would work.\n\nA min-priority queue is an abstract data type that provides 3 basic operations: add_with_priority(), decrease_priority() and extract_min(). As mentioned earlier, using such a data structure can lead to faster computing times than using a basic queue. Notably, Fibonacci heap or Brodal queue offer optimal implementations for those 3 operations. As the algorithm is slightly different in appearance, it is mentioned here, in pseudocode as well:\n\nInstead of filling the priority queue with all nodes in the initialization phase, it is possible to initialize it to contain only source; then, inside the block, the decrease_priority() becomes an add_with_priority() operation.[7]: 198\n\nYet another alternative is to add nodes unconditionally to the priority queue and to instead check after extraction ( ) that it isn't revisiting, or that no shorter connection was found yet in the block. This can be done by additionally extracting the associated priority from the queue and only processing further inside the while Q is not empty loop.[20]\n\nThese alternatives can use entirely array-based priority queues without decrease-key functionality, which have been found to achieve even faster computing times in practice. However, the difference in performance was found to be narrower for denser graphs.[21]\n\nTo prove the correctness of Dijkstra's algorithm, mathematical induction can be used on the number of visited nodes.[22]\n\nInvariant hypothesis: For each visited node v, is the shortest distance from source to v, and for each unvisited node u, is the shortest distance from source to u when traveling via visited nodes only, or infinity if no such path exists. (Note: we do not assume is the actual shortest distance for unvisited nodes, while is the actual shortest distance)\n\nThe base case is when there is just one visited node, source. Its distance is defined to be zero, which is the shortest distance, since negative weights are not allowed. Hence, the hypothesis holds.\n\nAssuming that the hypothesis holds for visited nodes, to show it holds for nodes, let u be the next visited node, i.e. the node with minimum . The claim is that is the shortest distance from source to u.\n\nThe proof is by contradiction. If a shorter path were available, then this shorter path either contains another unvisited node or not.\n• In the former case, let be the first unvisited node on this shorter path. By induction, the shortest paths from to and through visited nodes only have costs and respectively. This means the cost of going from to via has the cost of at least + the minimal cost of going from to . As the edge costs are positive, the minimal cost of going from to is a positive number. However, is at most because otherwise w would have been picked by the priority queue instead of u. This is a contradiction, since it has already been established that + a positive number < .\n• In the latter case, let be the last but one node on the shortest path. That means . That is a contradiction because by the time is visited, it should have set to at most .\n\nFor all other visited nodes v, the is already known to be the shortest distance from source already, because of the inductive hypothesis, and these values are unchanged.\n\nAfter processing u, it is still true that for each unvisited node w, is the shortest distance from source to w using visited nodes only. Any shorter path that did not use u, would already have been found, and if a shorter path used u it would have been updated when processing u.\n\nAfter all nodes are visited, the shortest path from source to any node v consists only of visited nodes. Therefore, is the shortest distance.\n\nBounds of the running time of Dijkstra's algorithm on a graph with edges E and vertices V can be expressed as a function of the number of edges, denoted , and the number of vertices, denoted , using big-O notation. The complexity bound depends mainly on the data structure used to represent the set Q. In the following, upper bounds can be simplified because is for any simple graph, but that simplification disregards the fact that in some problems, other upper bounds on may hold.\n\nFor any data structure for the vertex set Q, the running time i s:\n\nwhere and are the complexities of the decrease-key and extract-minimum operations in Q, respectively.\n\nThe simplest version of Dijkstra's algorithm stores the vertex set Q as a linked list or array, and edges as an adjacency list or matrix. In this case, extract-minimum is simply a linear search through all vertices in Q, so the running time is .\n\nFor sparse graphs, that is, graphs with far fewer than edges, Dijkstra's algorithm can be implemented more efficiently by storing the graph in the form of adjacency lists and using a self-balancing binary search tree, binary heap, pairing heap, Fibonacci heap or a priority heap as a priority queue to implement extracting minimum efficiently. To perform decrease-key steps in a binary heap efficiently, it is necessary to use an auxiliary data structure that maps each vertex to its position in the heap, and to update this structure as the priority queue Q changes. With a self-balancing binary search tree or binary heap, the algorithm requires\n\ntime in the worst case; for connected graphs this time bound can be simplified to . The Fibonacci heap improves this to\n\nWhen using binary heaps, the average case time complexity is lower than the worst-case: assuming edge costs are drawn independently from a common probability distribution, the expected number of decrease-key operations is bounded by , giving a total running time of[7]: 199–200\n\nIn common presentations of Dijkstra's algorithm, initially all nodes are entered into the priority queue. This is, however, not necessary: the algorithm can start with a priority queue that contains only one item, and insert new items as they are discovered (instead of doing a decrease-key, check whether the key is in the queue; if it is, decrease its key, otherwise insert it).[7]: 198 This variant has the same worst-case bounds as the common variant, but maintains a smaller priority queue in practice, speeding up queue operations.[12]\n\nMoreover, not inserting all nodes in a graph makes it possible to extend the algorithm to find the shortest path from a single source to the closest of a set of target nodes on infinite graphs or those too large to represent in memory. The resulting algorithm is called uniform-cost search (UCS) in the artificial intelligence literature[12][23][24] and can be expressed in pseudocode as\n\nIts complexity can be expressed in an alternative way for very large graphs: when C* is the length of the shortest path from the start node to any node satisfying the \"goal\" predicate, each edge has cost at least ε, and the number of neighbors per node is bounded by b, then the algorithm's worst-case time and space complexity are both in O(b1+⌊C* ⁄ ε⌋).[23]\n\nFurther optimizations for the single-target case include bidirectional variants, goal-directed variants such as the A* algorithm (see § Related problems and algorithms), graph pruning to determine which nodes are likely to form the middle segment of shortest paths (reach-based routing), and hierarchical decompositions of the input graph that reduce s–t routing to connecting s and t to their respective \"transit nodes\" followed by shortest-path computation between these transit nodes using a \"highway\".[25] Combinations of such techniques may be needed for optimal practical performance on specific problems.[26]\n\nAs well as simply computing distances and paths, Dijkstra's algorithm can be used to sort vertices by their distances from a given starting vertex. In 2023, Haeupler, Rozhoň, Tětek, Hladík, and Tarjan (one of the inventors of the 1984 heap), proved that, for this sorting problem on a positively-weighted directed graph, a version of Dijkstra's algorithm with a special heap data structure has a runtime and number of comparisons that is within a constant factor of optimal among comparison-based algorithms for the same sorting problem on the same graph and starting vertex but with variable edge weights. To achieve this, they use a comparison-based heap whose cost of returning/removing the minimum element from the heap is logarithmic in the number of elements inserted after it rather than in the number of elements in the heap.[27][28]\n\nWhen arc weights are small integers (bounded by a parameter ), specialized queues can be used for increased speed. The first algorithm of this type was Dial's algorithm for graphs with positive integer edge weights, which uses a bucket queue to obtain a running time . The use of a Van Emde Boas tree as the priority queue brings the complexity to . Another interesting variant based on a combination of a new radix heap and the well-known Fibonacci heap runs in time . Finally, the best algorithms in this special case run in time and time.\n\nDijkstra's original algorithm can be extended with modifications. For example, sometimes it is desirable to present solutions which are less than mathematically optimal. To obtain a ranked list of less-than-optimal solutions, the optimal solution is first calculated. A single edge appearing in the optimal solution is removed from the graph, and the optimum solution to this new graph is calculated. Each edge of the original solution is suppressed in turn and a new shortest-path calculated. The secondary solutions are then ranked and presented after the first optimal solution.\n\nDijkstra's algorithm is usually the working principle behind link-state routing protocols. OSPF and IS-IS are the most common.\n\nUnlike Dijkstra's algorithm, the Bellman–Ford algorithm can be used on graphs with negative edge weights, as long as the graph contains no negative cycle reachable from the source vertex s. The presence of such cycles means that no shortest path can be found, since the label becomes lower each time the cycle is traversed. (This statement assumes that a \"path\" is allowed to repeat vertices. In graph theory that is normally not allowed. In theoretical computer science it often is allowed.) It is possible to adapt Dijkstra's algorithm to handle negative weights by combining it with the Bellman-Ford algorithm (to remove negative edges and detect negative cycles): Johnson's algorithm.\n\nThe A* algorithm is a generalization of Dijkstra's algorithm that reduces the size of the subgraph that must be explored, if additional information is available that provides a lower bound on the distance to the target.\n\nThe process that underlies Dijkstra's algorithm is similar to the greedy process used in Prim's algorithm. Prim's purpose is to find a minimum spanning tree that connects all nodes in the graph; Dijkstra is concerned with only two nodes. Prim's does not evaluate the total weight of the path from the starting node, only the individual edges.\n\nBreadth-first search can be viewed as a special-case of Dijkstra's algorithm on unweighted graphs, where the priority queue degenerates into a FIFO queue.\n\nThe fast marching method can be viewed as a continuous version of Dijkstra's algorithm which computes the geodesic distance on a triangle mesh.\n\nFrom a dynamic programming point of view, Dijkstra's algorithm is a successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the Reaching method.[33][34][35]\n\nIn fact, Dijkstra's explanation of the logic behind the algorithm:\n\nis a paraphrasing of Bellman's Principle of Optimality in the context of the shortest path problem.\n• None Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). \"Section 24.3: Dijkstra's algorithm\". Introduction to Algorithms (Second ed.). MIT Press and McGraw–Hill. pp. 601. ISBN .\n• None Dial, Robert B. (1969). \"Algorithm 360: Shortest-path forest with topological ordering [H]\". Communications of the ACM. 12 (11): 633. doi: . S2CID 6754003.\n• None Fredman, Michael Lawrence; Tarjan, Robert E. (1984). Fibonacci heaps and their uses in improved network optimization algorithms. 25th Annual Symposium on Foundations of Computer Science. IEEE. pp. 346. doi:10.1109/SFCS.1984.715934.\n• None Fredman, Michael Lawrence; Tarjan, Robert E. (1987). \"Fibonacci heaps and their uses in improved network optimization algorithms\". Journal of the Association for Computing Machinery. 34 (3): 615. doi: . S2CID 7904683.\n• None Leyzorek, M.; Gray, R. S.; Johnson, A. A.; Ladew, W. C.; Meaker, Jr., S. R.; Petry, R. M.; Seitz, R. N. (1957). Investigation of Model Techniques – First Annual Report – 6 June 1956 – 1 July 1957 – A Study of Model Techniques for Communication Systems. Cleveland, Ohio: Case Institute of Technology.\n• None Ahuja, Ravindra K.; Mehlhorn, Kurt; Orlin, James B.; Tarjan, Robert E. (April 1990). \"Faster Algorithms for the Shortest Path Problem\" . Journal of the ACM. 37 (2): 223. doi:10.1145/77600.77615. hdl: . S2CID 5499589.\n• None Thorup, Mikkel (1999). \"Undirected single-source shortest paths with positive integer weights in linear time\". Journal of the ACM. 46 (3): 394. doi: . S2CID 207654795.\n• Implementation of Dijkstra's algorithm using TDD, Robert Cecil Martin, The Clean Code Blog"
    },
    {
        "link": "https://dl.acm.org/doi/pdf/10.5555/545381.545503",
        "document": ""
    },
    {
        "link": "https://arxiv.org/abs/2309.05865",
        "document": ""
    },
    {
        "link": "https://geeksforgeeks.org/shortest-path-unweighted-graph",
        "document": "Given an unweighted, undirected graph of V nodes and E edges, a source node S, and a destination node D, we need to find the shortest path from node S to node D in the graph.\n\nThe idea is to use a modified version of Breadth-First Search in which we keep storing the parent of a given vertex while doing the breadth-first search. We first initialize an array dist[0, 1, …., v-1] such that dist[i] stores the distance of vertex i from the source vertex and array par[0, 1, ….., v-1] such that par[i] represents the parent of the vertex i in the breadth-first search starting from the source. Now we get the length of the path from source to any other vertex from array dist[], and for printing the path from source to any vertex we can use array par[].\n\n// Modified bfs to store the parent of nodes along with the // queue to store the nodes in the order they are // Mark the distance of the source node as 0 // Push the source node to the queue // Iterate till the queue is not empty // Pop the node at the front of the queue // Explore all the neighbours of the current node // Check if the neighbouring node is not visited // Mark the current node as the parent of // Mark the distance of the neighbouring // node as distance of the current node + 1 // Insert the neighbouring node to the queue // Function to print the shortest distance between source // function call to find the distance of all nodes and \"Source and Destination are not connected\" // vector to store the graph as adjacency list // Modified bfs to store the parent of nodes along with // the distance from the source node // Queue to store the nodes in the order they are // Mark the distance of the source node as 0 // Push the source node to the queue // Iterate until the queue is not empty // Pop the node at the front of the queue // Explore all the neighbors of the current node // Check if the neighboring node is not // Mark the current node as the parent // Mark the distance of the neighboring // node as the distance of the current // Insert the neighboring node to the // Function to print the shortest distance between the // dist[] array stores the distance of nodes from S // Function call to find the distance of all nodes \"Source and Destination are not connected\" // List to store the graph as an adjacency list # Queue to store the nodes in the order they are visited # Mark the distance of the source node as 0 # Push the source node to the queue # Iterate until the queue is not empty # Pop the node at the front of the queue # Explore all the neighbors of the current node # Check if the neighboring node is not visited # Mark the current node as the parent of the neighboring node # Mark the distance of the neighboring node as the distance of the current node + 1 # Insert the neighboring node to the queue # dist[] array stores the distance of nodes from S # Function call to find the distance of all nodes and their parent nodes \"Source and Destination are not connected\" # List to store the graph as an adjacency list // C# program for the above approach // Modified BFS to store the parent of nodes along with // the distance from the source node // Queue to store the nodes in the order they are // Mark the distance of the source node as 0 // Push the source node to the queue // Iterate until the queue is not empty // Dequeue the node at the front of the queue // Explore all the neighbors of the current node // Check if the neighboring node is not // Mark the current node as the parent // Mark the distance of the neighboring // node as the distance of the current // Insert the neighboring node to the // Function to print the shortest distance between the // dist[] array stores the distance of nodes from S // Function call to find the distance of all nodes \"Source and Destination are not connected\" // List to store the graph as an adjacency list \"Source and Destination are not connected\" // Array to store the graph as an adjacency list\n\nTime Complexity: O(V + E), where V is the number of vertices and E is the number of edges.\n\nAuxiliary Space: O(V)"
    },
    {
        "link": "https://study.com/academy/lesson/weighted-graphs-implementation-dijkstra-algorithm.html",
        "document": ""
    },
    {
        "link": "https://dilipkumar.medium.com/graph-theory-algorithms-242f5de7b0a5",
        "document": "An array of lists (or hash maps) where each index represents a vertex, and the list at that index contains its neighboring vertices (or pairs of neighbors and edge weights).\n\nIt is applicable for both directed and undirected graph.\n\nUse Case: Suitable for sparse graphs where the number of edges is much less than V^2.\n\nA 2D array of size V×V . The entry is (or the weight of the edge) if there is an edge from vertex i to vertex j, otherwise (or for no edge).\n\nIt is applicable for both directed and undirected graph.\n\nUse Case: Suitable for dense graphs where the number of edges is close to V².\n\nA list of all edges, where each edge is represented as a pair of vertices (or a triplet if weights are included).\n\nUseful for algorithms that process all edges (e.g., Kruskal’s algorithm for MST).\n\nThere are many ways and techniques to traverse the graph. Sometimes we traverse all the nodes, sometimes all the edges, sometimes from A to B, etc. Let’s go through each type of traversal algorithm.\n\nExplores one path deeply before backtracking. This makes it suitable for problems where the solution is likely to be found deep in the graph (e.g., solving mazes).\n\nFollowing is dfs recursive code for reference.\n\nNote: Most of the time we use recursive dfs.\n\nNote that this sequence of visitation depends very much on the way we order the neighbors of a vertex\n\nLet’s try to solve following coding problem.\n\nFind the minimum path sum in a 2D grid, where you can move in four directions (up, down, left, right).\n\nWe can write following dfs code with backtracking.\n\nQ. What is use of visited array?\n\nAns. The array prevents the algorithm from entering infinite loops by ensuring that the same cell is not revisited in the current path.\n\nQ. Why there is need for backtrack?\n\nAns. If you don’t backtrack (i.e., don’t unmark ), the cell will remain marked as visited for the entire recursion. This means that once a cell is visited, it cannot be part of any other path, which could lead to missing valid paths. By unmarking the cell ( ), you allow it to be part of other paths that might lead to the destination.\n\nQ. Does Undoing the Visited State Cause Cycles?\n\nAns. The array ensures that no cell is revisited in the current path. This prevents cycles within a single path. After backtracking, the cell is unmarked, but it is only reused in other paths, not in the same path. This ensures that cycles do not form.\n\nQ. What is difference between DFS vs DFS with backtrack?\n\nAns. With DFS, a node is visited only once. With backtracking, node is visited many times. This lead to backtracking with time complexity of O(V!) i.e. exponential compare to DFS with O(V+E).\n\nExplores all nodes at the current depth before moving deeper. This makes it ideal for finding the shortest path in unweighted graphs.\n\nFollowing is BFS code for reference.\n\nOne single call of dfs(u) (or bfs(u)) from source vertex u will only visit vertices that are actually connected to (or reachable by) u. Therefore to find (and to count the number of) Connected Components (CCs) in an undirected graph, we can simply launch DFS (or BFS) from one of the remaining unvisited vertices to find the next Connected Component.\n\nFollowing is code using dfs.\n\nFollowing is code using bfs.\n\nCycle detection works differently on directed vs undirected graphs. Two goals.\n\nCycle detection using UNVISITED/EXPLORED/VISITED state (both directed and undirected graph)\n• With the help of one more vertex state: (that means visited but not yet completed) on top of (visited and completed), we can use this DFS spanning tree/forest to classify graph edges into three types:\n\nThis is an edge that is part of DFS spanning tree. We can detect this when DFS moves from vertex u currently with state: \n\nto another vertex v with state: .\n\nWe can detect this when DFS moves from vertex u currently with state: to another vertex v with state: too, which implies that vertex v is an ancestor of vertex u in the DFS spanning tree. This detects the cycle.\n\nWe can detect this when DFS moves from vertex u currently with state: to another vertex v with state: .\n\nFollowing is code to implement this algorithm.\n• Every time exit from node, set the departure time.\n\nThis gives three types of edges.\n\nBack edge (after visited, departure time will not be set)\n\nCross edge (after visited, departure time will be set)\n\nForward edge (before visited, departure time will be set)\n\nFollowing is code to implement it.\n\nTo print the cycle, we need to maintain the parent relationship and once the cycle is detected, run the loop to traverse the parent until the cycle node is found and ever the result. Following is the updated code.\n\nHow to find if a node is part of the cycle?\n• One approach is to keep running loop on to collect all the nodes part of a cycle. But this will lead to very high time complexity.\n• There is an alternate approach to decrementing the starting node as and on finding back edge then increment it as .\n• After that apply the prefix sum, sum with will not be part of the cycle.\n• To apply the prefix sum, you need to calculate\n• Then apply to calculate the prefix sum for each node in .\n\nFollowing is the updated code to find if is part of a cycle or not.\n• Since & therefore there is no concept of cross edges with an undirected graph.\n• Since can be a possibility once is visited therefore need to ignore it.\n\nFollowing is the updated code for cycle detection in an undirected graph.\n\nYou are given an binary matrix . You are allowed to change at most one to be .\n\nReturn the size of the largest island in after applying this operation.\n\nAn island is a 4-directionally connected group of s.\n• Run DFS to color each connected components and track the size as well.\n• To try each edges of i.e. converting to , first we find out the list of connected components it touches then sum up to find out the new components size.\n• Use color starting with and modify the same . This will help to use the same to identify the visited node.\n\nFollowing is code for reference.\n\nGiven a list of where each element is a list of strings, where the first element is a name, and the rest of the elements are emails representing emails of the account.\n\nNow, we would like to merge these accounts. Two accounts definitely belong to the same person if there is some common email to both accounts. Note that even if two accounts have the same name, they may belong to different people as people could have the same name. A person can have any number of accounts initially, but all of their accounts definitely have the same name.\n\nAfter merging the accounts, return the accounts in the following format: the first element of each account is the name, and the rest of the elements are emails in sorted order. The accounts themselves can be returned in any order.\n• We will represent emails as nodes.\n• An edge will signify that two emails are connected and hence belong to the same person.\n\nAns. We have two options.\n\nOption#1: Build edge between every pair of account’s email.\n\nFor example ,we can build edge between every pair of these four emails. This will make edges. This will create a complete subgraph.\n\nOption#2: Connect first email to rest only\n\nAs long as two emails are connected by a path of edges, we know they belong to the same account.\n\nWe can create an acyclic graph using only K−1 edges required to connect K nodes.\n\nThen run the dfs/bfs to find out the connected components. Following is code for reference.\n\nGiven a string that contains parentheses and letters, remove the minimum number of invalid parentheses to make the input string valid.\n\nReturn a list of unique strings that are valid with the minimum number of removals. You may return the answer in any order.\n\nApproach: Use bfs to implement the minimum removal of invalid parenthesis with help of level order traversal\n• Goal is to removal minimal of invalid parenthesis to make string valid\n• Neighbor of node will be calculated by removing each char one by one.\n• If at any level, we found the valid parenthesis then include it in answer and return.\n• It means, it will not do next level processing if previous level found the answer.\n\nFollowing is code for reference.\n• Vertices can be divided into two disjoint sets: A bipartite graph can be partitioned into two sets of vertices, with no edges between vertices within each set.\n• Every edge connects vertices in different sets: Every edge in a bipartite graph connects a vertex from one set to a vertex from the other set.\n• No odd-length cycles: A bipartite graph cannot contain any odd-length cycles, as this would require vertices from the same set to be connected by an edge.\n• Maximum degree is bounded by the size of the smaller set: The maximum degree of a vertex in a bipartite graph is equal to the size of the smaller set.\n• Coloring with two colors: A bipartite graph can be colored with two colors,, such that no adjacent vertices have the same color.\n\nLet’s use the coloring property to write code to detect if a graph is a bi-partite or not.\n• It starts by coloring the source vertex (zeroth layer) with the value 0.\n• Color direct neighbors of the source vertex (first layer) with value 1.\n• Color the neighbors of direct neighbors (second layer) with value 0 again.\n• I.e. keep alternating between values 0 and value 1 as the only two valid colors.\n• If we encounter any violation(s) along the way — an edge with two endpoints having the same color, then we can conclude that the given input graph is not a Bipartite Graph.\n• In the case of disconnected components, each should be a bipartite graph.\n\nIn the case of a cycle, if a cross edge exists on the same level then an odd cycle exists.\n\nOrdering of DAG i.e. directed acyclic graph. The topological ordering of the graph is not unique.\n\nTopological sort Using DFS on an acyclic graph\n\nDAG can have multiple topological orders. DP is nothing but a topological sort. Therefore we can apply dp to solve this problem.\n\nTopological sort Using BFS on an acyclic graph (Kahn’s algo)\n• Find node with indegree=0 and add into topological order\n• Then remove the node and recalculate the indegree.\n\nQ. Removing and recalculating in degrees is an expensive operation. How to recalculate in degrees again and again?\n\nAns. During building the graph, we can also calculate the initial state of in-degree. Once a node is processed in , instead of physically removing a node from the graph, we can simply update the in-degree for all the neighbors of that node. No need to use an array to track the processed node; instead will take care of it.\n\nQ. How to check if the graph has a cycle or not?\n\nA. In the case of the cycle, a node in the cycle will never have therefore these nodes will never be added. So if the size is less than then it means the cycle does exist.\n\nSimply use Priority Queue instead of regular Queue.\n\nAll paths from source to target\n\nThe goal is to find all the possible paths from source to target. It could be on a directed or undirected graph.\n\nAll paths from source to target on directed acyclic graph\n\nFollowing is one example of traversing a path from node to on a directed acyclic path.\n• For each node we have choices to explore all the neighbors.\n• If we choose one path and keep traversing the path then if this will reach to target node then we will capture this path to answer.\n• Since this is a DAG therefore we don’t need to worry about the cycle. Therefore no need to maintain the visited node.\n• Since we need to capture the local path, therefore we need to pass to every recursion which will be updated for each neighbor.\n• Before we pick the next neighbor, we need to clean up i.e. remove the last added neighbor.\n\nFollowing is the code to implement it.\n• With just two nodes and then we only have one path.\n• If we add a third node then now we will have two paths, one the old and the second via the newly added node.\n• With the newly-added node, new paths could be created by preceding all previous paths with the newly-added node\n\n4. It means for the previous node, if we had paths then after adding one more node, we will have path i.e. .\n\n5. So total path would be .\n\n6. If is an effort to build each path then the total time complexity would be\n\nAll paths from source to target on an undirected graph\n• With an undirected graph, we might have a cycle.\n• To avoid visiting the same node, we need to also maintain array\n• Mark the node as visited at the beginning of the recursion. If the node is already visited then don’t traverse it as a neighbor.\n• At the end of the recursion, we undo the visited array for the same node.\n\nFollowing is the code to find all possible paths.\n\nYou are exploring the stunning region of Natlan! This region consists of 𝑛 cities, and each city is rated with an attractiveness 𝑎𝑖. A directed edge exists from City 𝑖 to City 𝑗 if and only if 𝑖<𝑗 and gcd(𝑎𝑖,𝑎𝑗)≠1, where gcd(𝑥,𝑦) denotes the greatest common divisor (GCD) of integers 𝑥 and 𝑦.\n\nStarting from City 1, your task is to determine the total number of distinct paths you can take to reach City 𝑛n, modulo 998244353. Two paths are different if and only if the set of cities visited is different.\n\nThe first line contains an integer 𝑛n (2≤𝑛≤2⋅105) — the number of cities.\n\nThe second line contains 𝑛n integers 𝑎1,𝑎2,…,𝑎𝑛 (2≤𝑎𝑖≤106) — the attractiveness of each city.\n\nOutput the total number of distinct paths you can take to reach City 𝑛n, modulo 998244353.\n\nApproach #1: DFS to count all possible path (DLE)\n\nWe can take following approach\n• Build graph (since there is no cycle so it will be a tree)\n\nTime complexity is O(n*n*k). Following is split on time complexity.\n• No of nodes in the graph n\n\nWhile the provided dynamic programming solution is a good approach, it can be optimized further to avoid potential time limit exceptions, especially for larger input sizes as below.\n• Precompute the prime factorization of each number as given range of 10**6.\n• We precompute the prime factorization of each number up to using a prime sieve (Prime factorization is a way of expressing a number as a product of its prime factors).\n• This allows us to quickly determine the common prime factors between two number.\n• We use a prefix sum array to efficiently calculate the sum of values for all such that .\n• This avoids the need for a nested loop, significantly reducing the time complexity.\n\nFollowing is code for reference.\n\nBreath first search is used to find out the shortest path from the source node to the rest. There are many variations of BFS implementation, we will go through each.\n\nBFS — The single source shortest path on the unweighted graph\n\nLevel order reversing (bfs) is used to find the shortest path on an unweighted graph. Following are high-level steps.\n• Keep traversing until the queue is not empty.\n• Traverse each neighbor of the node from the queue if not visited.\n• Use to track the distance of node and to track the visited node. You can also merge both and just use in this simple case to track visited nodes as well.\n\nWe can also write BFS to use to track the visited node. This works same as for the unweighted graph. If graph is weighted then we do use both. See next section on why.\n\nThe time complexity of BFS is\n\nFor a graph with edge weight as either or . Need to find out the shortest path from the given source node. We can take the following approach.\n• The intuition here is that since weights are only and therefore we can easily sort it by always keeping at the front and at the end.\n• Use Dequeue instead of Queue.\n• If the node is then add to the front otherwise to the back.\n• check is required to avoid processing the same node multiple times. Adding this check has no relationship with correctness. This check is only required for optimal time complexity. bcz, the first node which is popped out from the queue will be the smallest distance. It will never be a possibility that later in the phase we discover the shortest path for the node.\n• Now we will use the weight as the total distance from the starting node instead of a number of hops.\n• Among all the neighbors, we need to pick the neighbor with the smallest weight. To get the smallest weight quickly, we will be using min to store the node.\n• The rest of the flow will be the same as regular BFS.\n• The weight of each edge from to is\n• The cost of the path never decreases as we traverse i.e. no negative weights in the graph.\n• We can apply Dijkstra here to find out the least time to reach from to\n• Trying to reduce edges along the shortest path\n• Let's say is the list of edges with weight.\n• We find what was the path using the last edges then calculate using edges.\n\nThe following problem can be solved using this algorithm.\n• Instead of a single-source , a single destination vertex is given.\n• We are asked what are the shortest paths from more than one source\n\nvertices to t.\n• It is better to think backward.\n• Instead of running an algorithm multiple times frontally, we can transpose the graph (reverse the direction of all its edges) and run the the algorithm just once with the destination vertex as the source vertex.\n• This technique works on weighted graphs too.\n\nThis is based on reducing nodes by one.\n• To print the shortest path, we need to store parent information for each vertex.\n• In this algorithm, we need 2D parent.\n• Some Shortest Paths problems may involve more than a single source.\n• We call this variant the Multi-Sources Shortest Paths (MSSP) and can be on either unweighted or weighted graphs.\n\nApproach 1: We can simply enqueue all the sources and set for each source upfront during the initialization step before running the SSSP loop as normal.\n\nApproach 2: Another way of looking at this technique is to imagine that\n\nthere exists a (virtual) super source vertex that is (virtually) connected to all those source vertices with (virtual) cost 0 (so these additional 0-weighted edges do not actually contribute to the actual shortest paths).\n\nThese technique works on weighted graphs too.\n\nGrid of 1 and 0 find out the farthest zero from any digit one\n• For each 0, find out the minimum distance to 1.\n• Find out the overall max distance for each 0 calculated above.\n\nApproach 1: Bruteforce to get the minimum distance for each 0 and keep track of the global max\n\nThe time complexity of this code is:\n• For all 0 finding the closest 1 is a pattern to apply a multisource algorithm.\n• We can add a supernode and connect to all then run on this super node to find out the shortest path.\n• The local answer for each cell will be .\n• Store the local answer in a distance grid then scan each answer to find out the max value.\n\n5. While writing the code, we can skip adding this supernode, instead initialize all to the queue to begin the bfs.\n\nFollowing is the code to implement the multisource algorithm.\n\nFind the minimum time to tier 1 cities from every city\n• Cities are represented as nodes in the graph.\n• Edge represents the road between two cities. The weight of the edge represents the time it will take to travel on the road to reach from source city to destination city.\n• Each city is categorized as Tier 1, Tier 2, and Tier 3.\n• Another data is given to show the flights from one tier city to another tier city and the time taken by flight.\n• The goal here is to find out the minimum time it will take to reach the tier 1 city from any other city.\n\nApproach 1: Add new edges to the original graph\n• Add flight as another list of edges to the original graph for each pair of tier cities.\n• Add all tier 1 cities to the priority queue to implement a multisource single-path algorithm.\n• Run Dijkastra for each node to get the shortest distance to Tier 1 city\n• Calculate the global minimum as the answer.\n• The number of cities is in the range of . Adding a pair of each city for the flight edge will make the graph super big.\n• This will lead to a TLE error.\n\nApproach 2: Connect both graphs from Tier x city to the same Tier x with zero cost\n• Connect the city node in the graph to flight same tier node with zero cost as unidirectional.\n• One edge from the source graph to the tier graph.\n• The second edge is from the tier graph to the cities graph.\n\nNote: If we use multidirectional then you can take a cost path to reach another city with zero cost. Therefore we need to take the unidirectional edge.\n\n5. Represent tier nodes as n….n+k for in and out nodes.\n\nFollowing is the code implementation of this approach.\n\nYou are given an grid initialized with these three possible values.\n• Infinity means an empty room. We use the value to represent as you may assume that the distance to a gate is less than .\n\nFill each empty room with the distance to its nearest gate. If it is impossible to reach a gate, it should be filled with .\n\nApproach#1: Launch bfs for each gate and keep track of minimum distance\n• Each gate is not fully searched before moving on to a new gate.\n• Each gate only looks at the areas within 1 space before we check the next gate.\n• So each area within one space of the gates are checked for rooms and these rooms are marked, then added to the queue.\n• Once all gates are checked, each new space is checked, and so forth.\n• So, once a room gets hit, it has to be from the closest gate.\n\nThere is an rectangular island that borders both the Pacific Ocean and Atlantic Ocean. The Pacific Ocean touches the island's left and top edges, and the Atlantic Ocean touches the island's right and bottom edges.\n\nThe island is partitioned into a grid of square cells. You are given an integer matrix where represents the height above sea level of the cell at coordinate .\n\nThe island receives a lot of rain, and the rain water can flow to neighboring cells directly north, south, east, and west if the neighboring cell’s height is less than or equal to the current cell’s height. Water can flow from any cell adjacent to an ocean into the ocean.\n\nReturn a 2D list of grid coordinates where denotes that rain water can flow from cell to both the Pacific and Atlantic oceans.\n\nApproach#1: Launch bfs/dfs from each node to check reachability\n\nThis is one of hte bruteforce approach.\n\nApproach#2: Use multisource bfs to reverse search from pacific/atlantic to each land water\n• If we start traversing from the ocean and flip the condition (check for higher height instead of lower height), then we know that every cell we visit during the traversal can flow into that ocean.\n• Let’s start a BFS traversal from every cell that is immediately beside the Pacific ocean, and figure out what cells can flow into the Pacific.\n• Then, let’s do the exact same thing with the Atlantic ocean. At the end, the cells that end up connected to both oceans will be our answer.\n\nFollowing is code for reference.\n• A circuit that uses every edge of the graph with no repeats i.e. starting from node and come back to the same node is the Eulerian cycle/circuit.\n• A path that uses every edge of the graph with no repeats is the Eulerian path.\n• Not all graph has an Eulerian Cycle or path.\n• Eulerian circuit: A graph contains an Eulerian circuit if all vertices have even degrees.\n• Eulerian path: A graph contains an Eulerian path if it has at most two vertices of odd degree. One of the odd vertices will be starting and the second will be ending path.\n• Eulerian circuit: A directed graph contains an Eulerian circuit if each vertex has the same in-degree and out-degree.\n• Eulerian path: A directed graph contains an Eulerian path if each vertex except two has the same in-degree and out-degree.\n• In normal DFS, we visit the node only once and we do not care how many times we visit the edge.\n• If we can use a data structure that will track the visited edges then we can use DFS.\n• One option is to remove the edges once visited. So that we can be sure that the visited edge is not considered again.\n• After we visit the edge, we will remove it.\n• This helps not to revisit the same edge.\n• This also helps to avoid maintaining separate storage to track the visited edge.\n• Instead of physically deleting the edge, keep track of index of neighbor needs to be processed.\n• Increment the index of neighbor for each node.\n• Exit the loop if all neighbors are processed.\n\nYou are given a list of airline where represent the departure and the arrival airports of one flight. Reconstruct the itinerary in order and return it.\n\nAll of the tickets belong to a man who departs from , thus, the itinerary must begin with . If there are multiple valid itineraries, you should return the itinerary that has the smallest lexical order when read as a single string.\n• For example, the itinerary has a smaller lexical order than .\n\nYou may assume all tickets form at least one valid itinerary. You must use all the tickets once and only once.\n• Each ticket represent the edge of two nodes in the graph.\n• We need to use ticket only once, it mens edge needs to be traverse only once.\n• We have given starting node as .\n• Goal is to return the itinerary of user to start from to last city.\n• This is nothing but Eulerian path. We can simply apply it.\n• Note: Since we need to return one lexicographical order, therefore once neighbors is build then sort it.\n\nA Hamiltonian path is a path in a graph that visits every vertex exactly once. In other words, it’s a route that traverses every node of the graph without repeating any node.\n\nThis is an NP-hard problem i.e. exponential cost of O(2^N).\n\nThe goal here is to find out the minimum moves required for the state of the board to reach the final state.\n\nWe have following here\n• Space state: Set of possible states that the board can be in.\n• Path cost: Number of moves to get from initial state to goal state.\n\nWe can use BFS to find out the shortest path to reach the goal. Each state of the board is treated as a node of the graph. The transition function gives the neighbor of the node."
    },
    {
        "link": "https://oliviagallucci.com/graph-algorithm-basics-bfs-and-dfs",
        "document": "Graphs provide a representation of relationships between entities. I utilize graph algorithms to model and analyze relationships, as well as identify patterns, within interconnected data. Graph algorithms are commonly used to help solve problems relating to network performance, resource allocation, financial portfolio optimization, and risk management.\n\nThis blog post covers graph algorithms and their significance in solving complex computational problems.\n\nA graph is a collection of nodes (or vertices) and edges that connect these nodes. The edges represent relationships between the nodes. Note that throughout this post, I refer to vertices and nodes interchangeably.\n\nGraphs can be directed or undirected, weighted or unweighted, and cyclic or acyclic, depending on the nature of their edges.\n\nTwo fundamental types of graphs are directed and undirected. They differ in how edges between vertices are defined.\n\nEach edge has a direction in a directed graph or “digraph,” indicating a one-way relationship between two vertices. These graphs are often represented by arrows on the edges, indicating the direction of the relationship.\n\nNote that if there is an edge from vertex to vertex in a digraph, it does not imply the existence of an edge from to . However, both relationships can exist concurrently if there are edges pointing in both directions.\n\nIn an undirected graph, edges have no direction; they represent a two-way relationship between vertices. If there is an edge between vertices and , it implies that there is also an edge between and .\n\nWeighted graphs assign numerical values (weights) to edges, representing the cost or distance between nodes, whereas unweighted graphs lack assigned edge values.\n\nCyclic graphs have at least one cycle or closed loop, while acyclic graphs lack cycles or closed loops.\n\nThe choice between directed and undirected graphs can significantly impact the design and behavior of algorithms that operate on graphs.\n• Traversal algorithms\n• Undirected graphs: traversal algorithms (e.g., Depth-First Search (DFS) or Breadth-First Search (BFS)) are generally more straightforward because there is no need to consider edge directions.\n• Directed graphs: traversal algorithms need to account for the direction of edges. For example, in a directed graph, traversing an edge from to differs from traversing it from to .\n• Connectivity\n• Undirected graphs: connectivity is symmetric. If there is a path from vertex to vertex , there is also one from to .\n• Directed graphs: connectivity may not be symmetric. There could be a directed path from to without a corresponding path from to .\n• Cycles\n• Undirected graphs: cycles are straightforward to detect using standard algorithms.\n• Directed graphs: may have directed cycles (cycles where the edges have a specific direction), and detecting them may involve more complex algorithms (e.g., algorithms for detecting strongly connected components – Kosaraju, Tarjan, and Gabow).\n• Topological sorting\n• Directed graphs: directed acyclic graphs (DAGs) are essential for task scheduling. Topological sorting, which orders vertices based on directed edges, is specific to directed graphs and is used in such scenarios.\n• Degree and in/out-degree\n• Undirected graphs: the degree of a vertex is the count of edges connected to it.\n• Directed graphs: vertices have both in-degree (number of incoming edges) and out-degree (number of outgoing edges), providing information about the graph’s structure.\n• Traversal\n• Acyclic: algorithms for acyclic graphs are often more straightforward since there are no concerns about infinite loops during traversal.\n• Cyclic: algorithms that traverse graphs (e.g., DFS or BFS) may encounter infinite loops if handled improperly in cyclic graphs.\n• Connectivity and sorting\n• Acyclic: can be topologically sorted, which is helpful in scheduling or dependency resolution.\n• Cyclic: cycles can create alternate paths between nodes, influencing connectivity and the reachability of nodes.\n• Shortest path\n• Acyclic: finding the shortest paths becomes more straightforward in acyclic graphs, as there are no cycles to complicate the process.\n• Cyclic: detecting cycles or finding the shortest path becomes more complex in cyclic graphs.\n• Graph traversal\n• Connected components as subgraphs\n• Each connected component can be treated as a subgraph. When performing graph traversal algorithms, these algorithms may be applied separately to each connected component.\n• Traversal boundaries\n• Connected components serve as natural boundaries during traversal. Traversal algorithms typically do not cross between connected components, allowing for “focused” exploration within each component.\n• Connectivity and reachability\n• Isolation\n• Vertices within the same connected component are mutually reachable. However, vertices in different connected components are not reachable from each other directly.\n• Graph connectivity\n• The number of connected components influences the overall connectivity of the graph. A graph with a single connected component is fully connected, while multiple connected components indicate a fragmented structure.\n• Analysis and decomposition\n• Component size\n• The size (number of vertices) of connected components can provide insights into the graph’s structure. Small connected components may represent isolated clusters or outliers.\n• Component identification\n• Identifying connected components is helpful in community detection or identifying weakly connected regions in a network.\n• Algorithmic efficiency with component-wise processing\n• In some cases, algorithms can process connected components independently. Independent processing can lead to more efficient solutions, especially when the behavior within each connected component is relatively independent of others.\n• Graph connectivity problems with bridges and cut-edges\n• Identifying connected components is related to the analysis of bridges (edges whose removal increases the number of connected components) and cut-edges. This can help when analyzing the resilience of a graph to edge removal.\n• Graph visualization\n• Connected components can be visually represented as separate subgraphs, which can display the graph’s structure.\n\nNow that we understand how each part of a graph affects the graph’s overall behavior, let’s examine and compare graph representations.\n\nThe two primary representations are adjacency matrices and adjacency lists.\n\nIn an adjacency matrix representation, a matrix depicts the connections between nodes. If there is an edge between nodes and , the matrix entry at position will be 1; otherwise, it will be 0. This representation is efficient for dense graphs, but may be space-inefficient for sparse graphs.\n\nIn an adjacency list representation, each node maintains a list of its neighboring nodes. Adjacency lists are more space-efficient, especially for sparse graphs. However, it may take longer to determine if there is an edge between two specific nodes.\n• Breadth-First Search (BFS)\n• BFS explores a graph level by level, visiting all the neighbors of a node before moving on to the next level. It is often used to find the shortest path in unweighted graphs and can assist with network broadcasting.\n• Depth-First Search (DFS)\n• DFS explores a graph by going as deep as possible along each branch before backtracking. It is valuable for topological sorting, cycle detection, and maze solving.\n• Dijkstra’s algorithm\n• Dijkstra’s algorithm finds the shortest path in weighted graphs. It helps with routing and network optimization problems.\n• Bellman-Ford algorithm\n• Like Dijkstra’s algorithm, the Bellman-Ford algorithm finds the shortest path in weighted graphs. However, it can handle graphs with negative edge weights, making it suitable for a broader range of problems.\n\nBFS explores a graph level by level, starting from a specified source vertex.\n\nIt starts at the tree root (or some arbitrary graph node) and explores the neighbor nodes at the present depth before moving to nodes at the next depth level. Steps:\n• While the queue is not empty:\n• Enqueue all unvisited neighbors of the dequeued node.\n\nUnlike DFS, which goes as deep as possible along a branch before backtracking, BFS explores the neighbor vertices of the current level before moving on to the next level. This strategy discovers the shortest path to each reachable vertex first, making BFS useful for shortest route problems or when someone needs to explore all vertices within a certain distance from the source.\n• Queue-based structure\n• At the heart of BFS lies a queue data structure. The algorithm maintains a queue to keep track of unexplored vertices. Starting with the source vertex, the algorithm dequeues a vertex, explores its neighbors, and enqueues them for further exploration.\n• Marking visited nodes\n• To prevent revisiting already explored vertices, BFS uses a marking mechanism. Each vertex is marked as visited once dequeued, ensuring the algorithm avoids cycle entrapment.\n• Level-order traversal\n• One strength of BFS is its ability to perform level-order traversal. Level-order traversal explores all vertices at a certain distance from the source before moving to vertices at a greater distance. This property makes BFS a great choice for finding the shortest path or determining connectivity.\n\nA financial instrument is a tradable asset or contract representing a financial value, such as stocks, bonds, derivatives, or currencies. This program could theoretically help with analysis on how quickly a shock (a type of financial change) would propagate through a network.\n\nAs I stated previously, BFS excels at finding the shortest path between two vertices in an unweighted graph. Its level-order traversal ensures that the first occurrence of the destination vertex yields the shortest route.\n\nHere is how BFS accomplishes this:\n• Exploration\n• While the queue is not empty:\n• Dequeue a vertex from the front of the queue.\n• Explore all unvisited neighbors of the dequeued vertex.\n• Enqueue each unvisited neighbor and mark it as visited.\n• Recording the path\n• Maintain a data structure (e.g., an array or dictionary) to keep track of the parent of each vertex in the current traversal.\n• When exploring a neighbor, set its parent as the current dequeued vertex.\n• Termination\n• Continue until reaching the destination vertex or the entire graph is traversed.\n• Shortest path reconstruction\n• Upon reaching the destination vertex, backtrack from the destination to the source vertex using the recorded parent information to reconstruct the shortest path.\n\nThe key idea is that BFS explores the graph breadth-first, ensuring that vertices closer to the source are visited before vertices farther away. As a result, the first occurrence of the destination vertex during the BFS traversal represents the shortest path.\n\nSince the graph is unweighted, each edge is treated as having the same weight. Therefore, the first time encountering a vertex is guaranteed to be the shortest path during the BFS traversal. (Level-order exploration results in discovering longer paths later).\n\nBy modeling the maze as a graph and applying BFS, we can quickly determine the shortest path from the entrance to the exit. Here is how BFS accomplishes this:\n• Graph representation\n• Represent the maze as a graph, where each cell is a vertex, and edges represent possible movements between adjacent cells (up, down, left, right).\n• If a cell is blocked or contains an obstacle, it is not considered a valid vertex or edge in the graph.\n• Exploration\n• While the queue is not empty:\n• Dequeue a cell from the front of the queue.\n• Explore all unvisited neighboring cells accessible (not blocked) from the current cell.\n• Enqueue each unvisited and accessible neighbor and mark it as visited.\n• Recording the path\n• Maintain a data structure (e.g., an array or dictionary) to keep track of the parent of each cell in the current traversal.\n• When exploring a neighbor, set its parent as the current dequeued cell.\n• Termination\n• Keep traversing the maze until it reaches the exit cell or traverses the entire maze.\n• Shortest path reconstruction\n• Once reaching the exit cell, reconstruct the shortest path by backtracking from the exit to the entrance using the recorded parent information.\n\nBy exploring the maze using BFS, the algorithm ensures that it discovers the shortest path first. Because BFS traverses the maze level by level, it ensures the exit path is the shortest possible when it first encounters the exit cell during the traversal.\n\nBFS is crucial in discovering the optimal path between routers in network routing algorithms. Its efficiency makes it an attractive choice for scenarios where minimizing latency is essential. Here is how BFS achieves this in the context of network routing.\n• Graph representation\n• Represent the network as a graph, where routers are vertices, and connections between routers are edges.\n• Assign weights to edges based on metrics like latency, bandwidth, or any other relevant criteria.\n• Exploration\n• While the queue is not empty:\n• Dequeue a router from the front of the queue.\n• Enqueue each unvisited neighbor and mark it as visited.\n• Consider the weights on edges to determine the optimal path.\n• Recording the path and metrics\n• Maintain a data structure (e.g., an array or dictionary) to keep track of the parent of each router in the current traversal.\n• Record metrics (e.g., cumulative latency) for each router along the way.\n• Termination\n• Keep traversing the network until the algorithm reaches the destination router or traverses the entire network.\n• Optimal path reconstruction\n• Upon reaching the destination router, reconstruct the optimal path by backtracking from the destination to the source using the recorded parent information.\n• Extract relevant metrics to determine the quality of the optimal path.\n\nBy exploring the network using BFS, the algorithm discovers the optimal path with minimal latency. Since BFS traverses the network breadth-first, it naturally prioritizes shorter paths before exploring longer ones. This also explains why BFS is so helpful when minimizing latency is a priority (e.g., real-time applications and video streaming).\n\nBFS also aids in finding connected components in an undirected graph. By starting BFS from unvisited vertices, it identifies and labels different connected components. Here is how BFS accomplishes this:\n• Initialization\n• Start BFS from an unvisited vertex in the undirected graph.\n• Exploration\n• While the queue is not empty:\n• Dequeue a vertex from the front of the queue.\n• Explore all unvisited neighbors of the dequeued vertex.\n• Enqueue each unvisited neighbor and mark it as visited.\n• Labeling connected components\n• Continue the exploration until the queue is empty. Labeling ensures that the BFS process covers an entire connected component.\n• All the vertices visited during this BFS traversal belong to the same connected component.\n• Assign a label or identifier to this connected component.\n• Repeat for unvisited vertices\n• If there are still unvisited vertices in the graph, select another unvisited vertex and start BFS from that vertex.\n• Assign a new label to the vertices visited during this BFS traversal, indicating a new connected component.\n• Termination\n• Continue this process until all vertices in the graph are visited.\n\nThe BFS algorithm identifies and labels different connected components in the undirected graph. Each BFS traversal from an unvisited vertex forms a connected component, and the process repeats until all vertices are covered. The result is a decomposition of the graph into distinct connected components.\n\nConnected components help me analyze the structure of undirected graphs by showing the graph’s connectivity patterns. BFS accomplishes this task by visiting all vertices within a connected component before exploring other components. It guarantees that each connected component is identified correctly and labeled uniquely.\n\nBFS web crawling starts with the initial page, and explores its links. Then, it explores the links of those pages, and so on.\n\nBFS is used in web crawling and social network analysis to explore the connectivity of websites and social networks. Here is how BFS applies to these tasks:\n• Initialization\n• Start BFS from an initial web page or a set of initial pages.\n• Exploration\n• While the queue is not empty:\n• Dequeue a web page from the front of the queue.\n• Retrieve the links on the dequeued page.\n• Enqueue each unvisited link and mark it as visited.\n• Repeat\n• Continue the process, exploring the links of newly dequeued pages.\n• The BFS traversal makes sure to visit pages closer to the initial page before exploring deeper pages.\n• Termination\n• Keep crawling until the algorithm reaches a specified depth, crawls a specific number of pages, or traverses the entire website.\n\nBFS social network analysis illustrates the degrees of separation between individuals.\n• Initialization\n• Start BFS from an initial individual or a set of initial individuals in the social network.\n• Exploration\n• While the queue is not empty:\n• Dequeue an individual from the front of the queue.\n• Retrieve the friends or connections of the dequeued individual.\n• Enqueue each unvisited friend and mark them as visited.\n• Degree of separation\n• As BFS progresses, each level of traversal represents a degree of separation.\n• The first level consists of the initial individuals, the second level their immediate connections, and so on.\n• Repeat\n• Continue the process, exploring the connections of newly dequeued individuals.\n• The BFS traversal ensures that it visits individuals closer in terms of degrees of separation before exploring those farther away.\n• Termination\n• Keep traversing the social network until the algorithm reaches a specified depth (degree of separation) or traverses the entire network.\n\nBFS is suitable for this task because it explores pages or individuals level by level. In the context of social networks, the levels correspond to degrees of separation between individuals, showing the structure and connectivity of the network. In web crawling, BFS helps discover and index pages in a structured manner, ensuring that pages are visited early in the process.\n\nLastly, BFS can help with garbage collection algorithms to discover and free memory that is no longer reachable. Here is how BFS achieves this in garbage collection algorithms:\n• Graph representation\n• Represent the memory space as a graph, where nodes represent allocated memory blocks, and edges represent references or pointers between these blocks.\n• Each node in the graph corresponds to a memory block, and edges represent references from one block to another.\n• Initialization\n• Start BFS from a set of root nodes. These root nodes are typically the starting points for memory references, such as global variables or objects referenced by the program.\n• Exploration\n• While the queue is not empty:\n• Dequeue a memory block from the front of the queue.\n• Explore all references or pointers from the dequeued block to other blocks.\n• Mark-and-sweep\n• During the exploration, mark each visited block as reachable.\n• After the BFS traversal is complete, perform a sweep phase to identify and reclaim memory blocks marked as unreachable.\n• The unreached memory blocks are considered garbage and can be freed.\n• Repeat for unvisited roots\n• If there are additional roots or starting points for memory references, start BFS from these unvisited roots.\n• Repeat the mark-and-sweep process until all reachable memory blocks are marked.\n• Garbage collection\n• The marked memory blocks are considered live or reachable, while the unmarked blocks are considered garbage.\n• Free the memory occupied by the garbage blocks, making it available for reuse.\n\nBFS is well-suited for garbage collection because it ensures that memory blocks are visited breadth-first. Here, blocks closer to the root nodes are marked and collected first, and the algorithm gradually moves outward through the memory graph. As a result, it identifies and collects garbage efficiently, reclaiming memory that is no longer in use.\n\nGarbage collection helps manage memory in programming languages with automatic memory management, since it helps prevent leaks and ensures efficient use of available resources. Using BFS in garbage collection algorithms contributes to the systematic identification and collection of unreferenced memory, helping maintain the health and performance of a program.\n\nWhile BFS focuses on exploring neighboring nodes before moving deeper into the graph, DFS takes a different approach. DFS traverses a graph by exploring as far as possible along each branch before backtracking, delving deeply into the structure. This method is particularly valuable in topological sorting, identifying strongly connected components, and solving select puzzles.\n• Definition and concept\n• DFS is a graph traversal algorithm that explores as far as possible along each branch before backtracking. The ‘depth’ in DFS refers to how deep the algorithm explores a particular branch before considering other paths.\n• Stacks and recursion\n• DFS can utilize a stack or recursion. The stack mimics a recursion call stack, maintaining information about the nodes to be visited. Recursive DFS, on the other hand, expresses the algorithm’s “backtracking nature.”\n\nUnderstanding DFS starts with learning its pseudocode. A simple representation might look like this:\n\nHere is a step-by-step example GIF of how DFS traverses a graph. Visualizing the algorithm in action helps comprehend its depth-first nature.\n\nDFS’s deep exploration before backtracking makes it a valuable tool in many applications. Here are some of those applications.\n\nDFS can explore and visit all the vertices and edges of a graph. This application is prominent in social network analysis, network routing, and recommendation systems.\n• Start from a source node\n• Choose a starting node in the graph as the initial point for traversal.\n• Mark the node as visited\n• Mark the chosen node as visited to keep track of the explored nodes.\n• Explore adjacent nodes\n• Move to an adjacent, unvisited node from the current node. If there are multiple adjacent nodes, choose one and repeat the process.\n• Recursive exploration\n• If there are unvisited nodes adjacent to the current one, repeat step 3 by recursively applying DFS to the chosen adjacent node.\n• Backtrack\n• If there are no unvisited nodes from the current one, backtrack to the previous node (if possible) and repeat step 3.\n• Repeat until all nodes are visited\n• Continue steps 3-5 until all nodes in the graph are visited.\n\nDFS is an excellent choice for solving mazes. The algorithm can navigate through the maze, exploring paths until it finds a solution.\n• Initialization\n• Create a stack to keep track of the path.\n• Explore neighbors\n• Look at the current cell’s neighbors (e.g., up, down, left, right).\n• Choose one unvisited neighbor and move to that cell.\n• Mark the new cell as visited and push it onto the stack.\n• Backtrack if necessary\n• If there are no unvisited neighbors, backtrack by popping the last cell from the stack.\n• Repeat this process until it finds a cell with unvisited neighbors or the stack is empty.\n• Goal check\n• Repeat steps 2-3 until the algorithm reaches the exit of the maze.\n• The algorithm finds a solution when it reaches the exit.\n\nIn directed acyclic graphs (DAGs), DFS can perform topological sorting. DFS with DAGs is often used to schedule tasks with dependencies (e.g., project management), where the order of tasks matters.\n\nTopological sorting orders the vertices of a directed graph such that for every directed edge , vertex comes before in the ordering. Here are steps for DFS-based topological sorting:\n• Initialization\n• Start with an empty result list or stack to store the topological order.\n• Initialize a set or array to keep track of visited vertices.\n• DFS traversal\n• During the traversal, recursively visit the neighbors of the current vertex.\n• Mark visited\n• Mark the current vertex as visited before pushing it to the result list or stack.\n• This step ensures that a vertex is only added to the topological order after exploring all its neighbors.\n• Backtrack and record\n• After visiting all neighbors of a vertex, backtrack to the previous vertex.\n• Add the current vertex to the result list or stack.\n• Repeat for unvisited vertices\n• Repeat steps 2-4 for any unvisited vertices in the graph.\n• Result\n• The topological order is the reverse of the order in which vertices were added to the result list or stack.\n\nDFS can identify connected components in an undirected graph. This application is helpful in network analysis to learn about the structure and dynamics of a network.\n• DFS algorithm\n• Explore all adjacent nodes that are not visited, and recursively apply DFS on each.\n• Repeat until all nodes are visited.\n• Count connected components\n• After DFS completes for the first connected component, increment the counter.\n• Repeat the DFS for the next connected component if unvisited nodes remain.\n• Result\n• The counter now holds the total number of connected components in the graph.\n• You can also maintain a data structure to store nodes belonging to each connected component.\n\nDFS can be employed to detect cycles in a graph. This application helps in deadlock detection in operating systems, where cycles can lead to system instability.\n• Start DFS from any node in the graph.\n• Explore each adjacent node:\n• If the adjacent node is not visited, recursively apply DFS on it.\n• A cycle is detected if the adjacent node is visited and not the current node’s parent.\n• If unvisited nodes exist, choose one and repeat steps 2-4.\n• If no cycles are detected, the graph is acyclic.\n\nThis approach identifies cycles by checking for back edges during the DFS traversal. If an edge leads to a visited node that is not the current node’s parent, it indicates a cycle’s presence.\n\nWhile DFS might not be the primary choice for finding the shortest path in weighted graphs, its principles inspire other path-finding algorithms. Algorithms like Depth-Limited Search and Iterative Deepening Depth-First Search use DFS principles to find optimal paths in complex scenarios.\n• Initialize data structures\n• Create a stack to keep track of the nodes to be visited.\n• Start node\n• Push the starting node onto the stack.\n• Explore neighbors\n• While the stack is not empty:\n• For each unvisited neighbor, mark it as visited and push it onto the stack.\n• Termination\n• Continue until the stack is empty or the goal node is found.\n\nRemember, DFS does not guarantee the shortest path in weighted graphs, but variations like Iterative Deepening DFS can find optimal paths in some of these scenarios.\n• Initialize\n• Repeat these steps until the algorithm finds the goal or explores the entire search space.\n• Depth-first search with depth limit\n• Explore neighbors up to the current depth limit.\n• Check for goal\n• Stop and return the solution if the DFS discovers the goal node.\n• If the algorithm does not find the goal, increase the depth limit.\n• Termination\n• Continue this process until DFS finds the goal or explores the entire search space.\n\nBy incrementally increasing the depth limit in each iteration, Iterative Deepening DFS combines the benefits of DFS with the completeness of BFS. It explores the search space depth-first while gradually increasing the depth limit to find the optimal solution. This approach is instrumental in scenarios where the depth of the solution is unknown.\n\nDepth-Limited Search is a variant of DFS where the search is restricted to a fixed depth. In traditional DFS, the algorithm explores as far as possible along each branch before backtracking. However, in Depth-Limited Search, the exploration is limited to a specific depth level, and the search backtracks if the goal is not found within that depth limit.\n• Initialize\n• Set the depth limit, which restricts the depth of the Search.\n• Perform Depth-Limited Search\n• Apply the actions applicable to the current state to generate successor states.\n• Return the solution if the algorithm finds the goal state.\n• Recursion\n• If the depth limit is not reached, recursively apply the depth-limited search to the successor states with a reduced depth limit.\n• Backtrack\n• If the current depth level does not yield a solution, backtrack to the previous state and explore other branches.\n• Repeat\n• Repeat steps 2-4 until the algorithm finds a solution or searches the entire space within the specified depth limit.\n• Handle cutoffs\n• If a cutoff occurs due to reaching the depth limit, it means the solution might exist beyond the current depth. Handle this appropriately, considering the solution might be deeper in the search space.\n• Return solution\n• Once the algorithm reaches the goal state, return the solution path.\n• Termination\n• If the algorithm explores the entire search space without finding a solution, terminate the Search and conclude that no solution exists within the specified depth limit.\n\nRemember that Depth-Limited Search is a variant of DFS with a limited exploration depth to prevent infinite loops and improve efficiency. Adjusting the depth limit affects the trade-off between completeness and efficiency.\n\nCompiler design utilizes DFS for syntax analysis and code generation. It helps in traversing and analyzing the syntax tree or abstract syntax tree representing the structure of a program.\n• Construct syntax tree\n• Begin by constructing the syntax tree or abstract syntax tree (AST) from the program’s source code. Each node in the tree represents a syntactic construct in the code.\n• Initialize DFS\n• Start DFS from the root of the syntax tree. Initialize a stack to keep track of the visited nodes.\n• Visit nodes\n• Perform the syntax analysis or code generation operations for each visited node. This could involve checking for language constructs, handling variable declarations, managing control flow structures, etc.\n• Traverse children\n• Traverse the children of the current node in a depth-first manner. Push each child onto the stack for further exploration.\n• Backtrack\n• If a leaf node is reached or all children of a node have been visited, backtrack by popping the current node from the stack.\n• Continue DFS\n• Continue the DFS process until all nodes in the syntax tree have been visited. This ensures a comprehensive analysis of the program’s structure.\n• Generate code\n• During the DFS traversal, generate code based on the syntactic constructs encountered. This may involve emitting assembly or machine code instructions corresponding to the high-level language constructs.\n• Handle errors\n• Implement error-handling mechanisms within the DFS traversal. This is crucial for identifying and reporting syntax errors or other issues in the source code.\n• Output\n• Once the DFS traversal and code generation is complete, produce the final output, which could be an executable file, intermediate code, or other forms, depending on the compiler’s design.\n\nApplying DFS in compiler design ensures a systematic and efficient traversal of the syntax tree, enabling the analysis and generation of code for the given programming language.\n\nGraph algorithms are the backbone of social network analysis, route planning, network latency. Understanding the fundamentals of graph theory, like BFS and DFS, allows us to create efficient programs.\n\nRemember, BFS systematically explores and visits a graph’s vertices level by level, starting from a specified source vertex. It is particularly effective in finding the shortest path and exploring the nearest neighbors in graph-based structures or networks. Conversely, BFS’s main weakness lies in its potential inefficiency when dealing with large graphs or networks, as it explores nodes level by level without considering the specific characteristics of the problem, leading to increased time and space complexity.\n\nOn the other hand, DFS explores as far as possible along each branch before backtracking, effectively traversing the depth of a graph. DFS has a relatively low space complexity compared to its breadth-first counterpart. Its memory requirements depend on the depth of the recursion or the size of the stack. While DFS is excellent for exploration, it may not be ideal for certain tasks. For example, it might not be the most efficient algorithm for finding the shortest path in a weighted graph.\n\nIf you enjoyed this post on graph algorithm basics, consider reading Using Greedy and Dynamic Optimization Algorithms."
    },
    {
        "link": "https://geeksforgeeks.org/floyd-warshall-algorithm-dp-16",
        "document": "The Floyd Warshall Algorithm is an all-pair shortest path algorithm that uses Dynamic Programming to find the shortest distances between every pair of vertices in a graph, unlike Dijkstra and Bellman-Ford which are single source shortest path algorithms. This algorithm works for both the directed and undirected weighted graphs and can handle graphs with both positive and negative weight edges. \n\n\n\nNote: It does not work for the graphs with negative cycles (where the sum of the edges in a cycle is negative).\n\nSuppose we have a graph graph[][] with V vertices from 0to V-1. Now we have to evaluate a dist[][] where dist[i][j] represents the shortest path between vertex i to j. Let us assume that vertices i to j have intermediate nodes. The idea behind Floyd Warshall algorithm is to treat each and every vertex k from 0 to V-1 as an intermediate node one by one. When we consider the vertex k, we must have considered vertices from 0 to k-1 already. So we use the shortest paths built by previous vertices to build shorter paths with vertex k included. The following figure shows the above optimal substructure property in Floyd Warshall algorithm:\n• None Initialize the solution matrix same as the input graph matrix as a first step.\n• None Then update the solution matrix by considering all vertices as an intermediate vertex.\n• None The idea is to pick all vertices one by one and updates all shortest paths which include the picked vertex as an intermediate vertex in the shortest path.\n• k as an intermediate vertex, we already have considered vertices {0, 1, 2, .. k-1}\n• (i, j) of the source and destination vertices respectively, there are two possible cases.\n• k is not an intermediate vertex in shortest path from i j . We keep the value of dist[i][j] as it is.\n• k is an intermediate vertex in shortest path from i j . We update the value of dist[i][j] dist[i][k] + dist[k][j], dist[i][j] > dist[i][k] + dist[k][j]\n\n// Add all vertices one by one to // Pick all vertices as source one by one // for the above picked source // If vertex k is on the shortest path from // i to j, then update the value of graph[i][j] // Add all vertices one by one to // Pick all vertices as source one by one // for the above picked source // If vertex k is on the shortest path from // i to j, then update the value of graph[i][j] # Add all vertices one by one to # Pick all vertices as source one by one # for the above picked source # If vertex k is on the shortest path from # i to j, then update the value of graph[i][j] // Add all vertices one by one to // Pick all vertices as source one by one // for the above picked source // If vertex k is on the shortest path from // i to j, then update the value of graph[i,j] // Add all vertices one by one to // Pick all vertices as source one by one // for the above picked source // If vertex k is on the shortest path from // i to j, then update the value of graph[i][j]\n\nTime Complexity: O(V3), where V is the number of vertices in the graph and we run three nested loops each of size V.\n\nAuxiliary Space: O(V2), to create a 2-D matrix in order to store the shortest distance for each pair of nodes.\n\nNote: The above program only prints the shortest distances. We can modify the solution to print the shortest paths also by storing the predecessor information in a separate 2D matrix.\n\nThe algorithm relies on the principle of optimal substructure, meaning:\n• None If the shortest path from i to j passes through some vertex k, then the path from i to k and the path from k to j must also be shortest paths.\n• None The iterative approach ensures that by the time vertex k is considered, all shortest paths using only vertices 0 to k-1 have already been computed.\n\nBy the end of the algorithm, all shortest paths are computed optimally because each possible intermediate vertex has been considered.\n\nWhy Floyd-Warshall Algorithm better for Dense Graphs and not for Sparse Graphs?\n\nDense Graph: A graph in which the number of edges are significantly much higher than the number of vertices.\n\nSparse Graph: A graph in which the number of edges are very much low. No matter how many edges are there in the graph the Floyd Warshall Algorithm runs for O(V3) times therefore it is best suited for Dense graphs. In the case of sparse graphs, Johnson’s Algorithm is more suitable.\n• None In computer networking, the algorithm can be used to find the shortest path between all pairs of nodes in a network. This is termed as network routing\n• None Flight Connectivity In the aviation industry to find the shortest path between the airports.\n• GIS Geographic Information Systems ) applications often involve analyzing spatial data, such as road networks, to find the shortest paths between locations.\n• None which is a generalization of floyd warshall, can be used to find regular expression for a regular language.\n• None How to Detect Negative Cycle in a graph using Floyd Warshall Algorithm?\n• None How is Floyd-warshall algorithm different from Dijkstra’s algorithm?\n• None How is Floyd-warshall algorithm different from Bellman-Ford algorithm?\n• None Shortest path with one curved edge in an undirected Graph\n• None 1st to Kth shortest path lengths in given Graph\n• None Number of ways to reach at destination in shortest time"
    }
]