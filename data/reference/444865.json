[
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html",
        "document": "Split arrays or matrices into random train and test subsets.\n\nQuick utility that wraps input validation, , and application to input data into a single call for splitting (and optionally subsampling) data into a one-liner.\n\nRead more in the User Guide.\n\n*arrays sequence of indexables with same length / shape[0] If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If is also None, it will be set to 0.25. If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls. See Glossary. Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None. If not None, data is split in a stratified fashion, using this as the class labels. Read more in the User Guide. Added in version 0.16: If the input is sparse, the output will be a . Else, output type is the same as the input type."
    },
    {
        "link": "https://realpython.com/train-test-split-python-data",
        "document": "With from scikit-learn, you can efficiently divide your dataset into training and testing subsets to ensure unbiased model evaluation in machine learning. This process helps prevent overfitting and underfitting by keeping the test data separate from the training data, allowing you to assess the model’s predictive performance accurately.\n\nBy the end of this tutorial, you’ll understand that:\n• is a function in that divides datasets into training and testing subsets.\n• and represent the inputs and outputs of the training data subset, respectively, while and represent the input and output of the testing data subset.\n• By specifying , you use 20% of the dataset for testing, leaving 80% for training.\n• can handle imbalanced datasets using the parameter to maintain class distribution.\n\nYou’ll learn how to use and apply these concepts in real-world scenarios, ensuring your machine learning models are evaluated with precision and fairness. In addition, you’ll explore related tools from for further insights.\n\nNow that you understand the need to split a dataset in order to perform unbiased model evaluation and identify underfitting or overfitting, you’re ready to learn how to split your own datasets. You’ll use version 1.5.0 of scikit-learn, or . It has many packages for data science and machine learning, but for this tutorial, you’ll focus on the package, specifically on the function . Note: While this tutorial is tested with this specific version of scikit-learn, the features that you’ll use are core to the library and should work equivalently in other versions of scikit-learn as well. You can install with : If you use Anaconda, then you probably already have it installed. However, if you want to use a fresh environment, ensure that you have the specified version or use Miniconda. Then you can install from Anaconda Cloud with : You’ll also need NumPy, but you don’t have to install it separately. You should get it along with if you don’t already have it installed. If you want to, you can refresh your NumPy knowledge and check out NumPy Tutorial: Your First Steps Into Data Science in Python.\n\nYou need to import and NumPy before you can use them. You can work in a Jupyter notebook or start a new Python REPL session, then you can start with the statements: Now that you have both imported, you can use them to split data into training sets and test sets. You’ll split inputs and outputs at the same time, with a single function call. With , you only need to provide the arrays that you want to split. Additionally, you can also provide some optional arguments. The function usually returns a list of NumPy arrays but can also return a couple of other iterable types, such as SciPy sparse matrices, if appropriate: The parameter in the function signature of refers to the sequence of lists, NumPy arrays, pandas DataFrames, or similar array-like objects that hold the data that you want to split. All these objects together make up the dataset and must be of the same length. In supervised machine learning applications, you’ll typically work with two such arrays: The parameter indicates that you can customize the function’s behavior with optional keyword arguments:\n• is the number that defines the size of the training set. If you provide a , then it must be between and and it will define the share of the dataset used for testing. If you provide an , then it will represent the total number of the training samples. The default value is .\n• is the number that defines the size of the test set. It’s very similar to . You should provide either or . If neither is given, then the default share of the dataset that will be used for testing is , or 25 percent.\n• is the object that controls randomization during splitting. It can be either an or an instance of . Setting the random state is useful if you need reproducibility. The default value is .\n• is the Boolean object that determines whether to shuffle the dataset before applying the split. The default value is .\n• is an array-like object that, if not , determines how to use a stratified split. Now it’s time to try data splitting! You’ll start by creating a simple dataset to work with. The dataset will contain the inputs in the two-dimensional array and outputs in the one-dimensional array : To get your data, you use , which is very convenient for generating arrays based on numerical ranges. You also use to modify the shape of the array returned by and get a two-dimensional data structure. You can split both input and output datasets with a single function call: Given two arrays, like and here, performs the split and returns four arrays (in this case NumPy arrays) in this order:\n• : The training part of the first array ( )\n• : The test part of the first array ( )\n• : The training part of the second array ( )\n• : The test part of the second array ( ) You probably got different results from what you see here. This is because dataset splitting is random by default. The result differs each time you run the function. However, this often isn’t what you want. Sometimes, to make your tests reproducible, you need a random split with the same output for each function call. You can do that with the parameter . The value of isn’t important—it can be any non-negative integer. You could use an instance of instead, but that’s a more complex approach. In the previous example, you used a dataset with twelve rows, or observations, and got a training sample with nine rows and a test sample with three rows. That’s because you didn’t specify the desired size of the training and test sets. By default, 25 percent of samples are assigned to the test set. This ratio is generally fine for many applications, but it’s not always what you need. Typically, you’ll want to define the size of the test or training set explicitly, and sometimes you’ll even want to experiment with different values. You can do that with the parameters or . Modify the code so you can choose the size of the test set and get a reproducible result: With this change, you get a different result from before. Earlier, you had a training set with nine items and a test set with three items. Now, thanks to the argument , the training set has eight items and the test set has four items. You’d get the same result with because 33 percent of twelve is approximately four. There’s one more very important difference between the last two examples: You now get the same result each time you run the function. This is because you’ve fixed the random number generator with . The figure below shows what’s going on when you call : The samples of the dataset are shuffled randomly and then split into the training and test sets according to the size you defined. You can see that has six zeros and six ones. However, the test set has three zeros out of four items. If you want to (approximately) keep the proportion of values through the training and test sets, then pass . This will enable stratified splitting: Now and have the same ratio of zeros and ones as the original array. Stratified splits are desirable in some cases, like when you’re classifying an imbalanced dataset, which is a dataset with a significant difference in the number of samples that belong to distinct classes. Finally, you can turn off data shuffling and random split with : Now you have a split in which the first two-thirds of samples in the original and arrays are assigned to the training set and the last third to the test set. No shuffling. No randomness.\n\nNow it’s time to see in action when solving supervised learning problems. You’ll start with a small regression problem that can be solved with linear regression before looking at a bigger problem. You’ll also see that you can use for classification as well. In this example, you’ll apply what you’ve learned so far to solve a small regression problem. You’ll learn how to create datasets, split them into training and test subsets, and use them for linear regression. As always, you’ll start by importing the necessary packages, functions, or classes. You’ll need NumPy, , and : Now that you’ve imported everything you need, you can create two small arrays, and , to represent the observations and then split them into training and test sets just as you did before: Your dataset has twenty observations, or - pairs. You specify the argument , so the dataset is divided into a training set with twelve observations and a test set with eight observations. Now you can use the training set to fit the model: creates the object that represents the model, while trains, or fits, the model and returns it. With linear regression, fitting the model means determining the best intercept ( ) and slope ( ) values of the regression line. Although you can use and to check the goodness of fit, this isn’t a best practice. An unbiased estimation of the predictive performance of your model is based on test data: returns the coefficient of determination, or R², for the data passed. Its maximum is . The higher the R² value, the better the fit. In this case, the training data yields a slightly higher coefficient. However, the R² calculated with test data is an unbiased measure of your model’s prediction performance. This is how it looks on a graph: The green dots represent the - pairs used for training. The black line, called the estimated regression line, is defined by the results of model fitting: the intercept and the slope. So, it reflects the positions of the green dots only. The white dots represent the test set. You use them to estimate the performance of the model (regression line) with data not used for training. Now you’re ready to split a larger dataset to solve a regression problem. You’ll use the California Housing dataset, which is included in . This dataset has 20640 samples, eight input variables, and the house values as the output. You can retrieve it with . Now that you have both functions imported, you can get the data you’ll work with: As you can see, with the argument returns a tuple with two NumPy arrays: The next step is to split the data the same way as before: Now you have the training and test sets. The training data is contained in and , while the data for testing is in and . When you work with larger datasets, it’s usually more convenient to pass the training or test size as a ratio. means that approximately 40 percent of samples will be assigned to the test data, and the remaining 60 percent will be assigned to the training data. Finally, you can use the training set ( and ) to fit the model and the test set ( and ) for an unbiased evaluation of the model. In this example, you’ll apply three well-known regression algorithms to create models that fit your data: The process is pretty much the same as with the previous example:\n• Import the classes you need.\n• Fit the model instances with using the training set.\n• Evaluate the model with using the test set. Here’s the code that follows the steps described above for all three regression algorithms: You’ve used your training and test datasets to fit three models and evaluate their performance. The measure of accuracy obtained with is the coefficient of determination. It can be calculated with either the training or test set. However, as you already learned, the score obtained with the test set represents an unbiased estimation of performance. As mentioned in the documentation, you can provide optional arguments to , , and . and use the parameter for the same reason that does: to deal with randomness in the algorithms and ensure reproducibility. For some methods, you may also need feature scaling. In such cases, you should fit the scalers with training data and use them to transform test data. You can use to solve classification problems the same way you do for regression analysis. In machine learning, classification problems involve training a model to apply labels to, or classify, the input values and sort your dataset into categories. In the tutorial Logistic Regression in Python, you’ll find an example of a handwriting recognition task. The example provides another demonstration of splitting data into training and test sets to avoid bias in the evaluation process.\n\nThe package offers a lot of functionalities related to model selection and validation, including the following: Cross-validation is a set of techniques that combine the measures of prediction performance to get more accurate model estimations. One of the widely used cross-validation methods is k-fold cross-validation. In it, you divide your dataset into k (often five or ten) subsets, or folds, of equal size and then perform the training and test procedures k times. Each time, you use a different fold as the test set and all the remaining folds as the training set. This provides k measures of predictive performance, and you can then analyze their mean and standard deviation. You can implement cross-validation with , , , and a few other classes and functions from . A learning curve, sometimes called a training curve, shows how the prediction score of training and validation sets depends on the number of training samples. You can use to get this dependency, which can help you find the optimal size of the training set, choose hyperparameters, compare models, and so on. Hyperparameter tuning, also called hyperparameter optimization, is the process of determining the best set of hyperparameters to define your machine learning model. provides you with several options for this purpose, including , , , and others. Splitting your data is also important for hyperparameter tuning."
    },
    {
        "link": "https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.train_test_split.html",
        "document": "Split arrays or matrices into random train and test subsets.\n\nQuick utility that wraps input validation, , and application to input data into a single call for splitting (and optionally subsampling) data into a one-liner.\n\nRead more in the User Guide.\n\n*arrays sequence of indexables with same length / shape[0] If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If is also None, it will be set to 0.25. If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls. See Glossary. Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None. If not None, data is split in a stratified fashion, using this as the class labels. Read more in the User Guide. New in version 0.16: If the input is sparse, the output will be a . Else, output type is the same as the input type."
    },
    {
        "link": "https://quora.com/How-do-you-split-a-dataset-into-training-and-testing-sets-in-scikit-learn",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/model_selection/_split.py",
        "document": ""
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html",
        "document": "Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n\nThe transformation is given by:\n\nThis transformation is often used as an alternative to zero mean, unit variance scaling.\n\ndoesn’t reduce the effect of outliers, but it linearly scales them down into a fixed range, where the largest occurring data point corresponds to the maximum value and the smallest one corresponds to the minimum value. For an example visualization, refer to Compare MinMaxScaler with other scalers.\n\nRead more in the User Guide.\n\nNaNs are treated as missing values: disregarded in fit, and maintained in transform."
    },
    {
        "link": "https://scikit-learn.org/1.0/modules/generated/sklearn.preprocessing.MinMaxScaler.html",
        "document": "Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n\nThe transformation is given by:\n\nThis transformation is often used as an alternative to zero mean, unit variance scaling.\n\nRead more in the User Guide.\n\nSet to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array). Set to True to clip transformed values of held-out data to provided . Per feature adjustment for minimum. Equivalent to Per feature relative scaling of the data. Equivalent to Per feature minimum seen in the data Per feature maximum seen in the data Per feature range seen in the data Number of features seen during fit. The number of samples processed by the estimator. It will be reset on new calls to fit, but increments across calls. Names of features seen during fit. Defined only when has feature names that are all strings.\n\nNaNs are treated as missing values: disregarded in fit, and maintained in transform.\n\nFor a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py.\n\nCompute the minimum and maximum to be used for later scaling. Fit to data, then transform it. Get parameters for this estimator. Undo the scaling of X according to feature_range. Online computation of min and max on X for later scaling. Set the parameters of this estimator. Scale features of X according to feature_range."
    },
    {
        "link": "https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.MinMaxScaler.html",
        "document": "Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n\nThe transformation is given by:\n\nThis transformation is often used as an alternative to zero mean, unit variance scaling.\n\ndoesn’t reduce the effect of outliers, but it linearly scales them down into a fixed range, where the largest occurring data point corresponds to the maximum value and the smallest one corresponds to the minimum value. For an example visualization, refer to Compare MinMaxScaler with other scalers.\n\nRead more in the User Guide.\n\nNaNs are treated as missing values: disregarded in fit, and maintained in transform."
    },
    {
        "link": "https://github.com/scikit-learn/scikit-learn/discussions/19501",
        "document": "To see all available qualifiers, see our documentation .\n\nSaved searches Use saved searches to filter your results more quickly\n\nWe read every piece of feedback, and take your input very seriously.\n\nYou signed in with another tab or window. Reload to refresh your session.\n\nYou signed out in another tab or window. Reload to refresh your session.\n\nYou switched accounts on another tab or window. Reload to refresh your session."
    },
    {
        "link": "https://scikit-learn.ru/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html",
        "document": "Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n\nThe transformation is given by:\n\nThis transformation is often used as an alternative to zero mean, unit variance scaling.\n\ndoesn’t reduce the effect of outliers, but it linearly scales them down into a fixed range, where the largest occurring data point corresponds to the maximum value and the smallest one corresponds to the minimum value. For an example visualization, refer to Compare MinMaxScaler with other scalers.\n\nRead more in the User Guide.\n\nNaNs are treated as missing values: disregarded in fit, and maintained in transform.\n\nCompute the minimum and maximum to be used for later scaling. Fit to data, then transform it. Get metadata routing of this object. Get parameters for this estimator. Undo the scaling of X according to feature_range. Online computation of min and max on X for later scaling. Set the parameters of this estimator. Scale features of X according to feature_range."
    },
    {
        "link": "https://geeksforgeeks.org/implementing-neural-networks-using-tensorflow",
        "document": ""
    },
    {
        "link": "https://reddit.com/r/MachineLearning/comments/bcfyo2/d_pytorch_implementation_best_practices",
        "document": ""
    },
    {
        "link": "https://linkedin.com/pulse/how-implement-neural-networks-python-using-tensorflow-keras-2szpf",
        "document": ""
    },
    {
        "link": "https://medium.com/towards-data-science/lets-code-a-neural-network-in-plain-numpy-ae7e74410795",
        "document": ""
    },
    {
        "link": "https://realpython.com/python-ai-neural-network",
        "document": ""
    }
]