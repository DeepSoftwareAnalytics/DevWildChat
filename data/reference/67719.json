[
    {
        "link": "https://devblogs.microsoft.com/microsoft365dev/deploy-your-chatgpt-based-model-securely-using-microsoft-teams-power-virtual-agent-and-azure-openai",
        "document": "This article describes different options to implement the ChatGPT (gpt-35-turbo) model of Azure OpenAI in Microsoft Teams. Due to the limited availability of services – in public or gated previews – this content is meant for people that need to explore this technology, understand the use-cases and how to make it available to their users in a safe and secure way via Microsoft Teams. Even if access to those services may be limited at the time of writing, the concepts and design patterns documented here should guide the reader through different options.\n\nWhat makes generative AI different from other technologies?\n\nArtificial Intelligence (AI) and its large language models have been getting quite a lot of attention lately, and it’s easy to understand why. With the latest breakthroughs in AI, we are now seeing more accurate, adaptable, and intelligent AI systems that can solve more complex problems.\n\nChatGPT developed by OpenAI, specifically, has generated a lot of excitement and high expectation as it can generate human-like text responses based on given prompts, becoming increasingly popular in the corporate world as a tool for automating routine tasks or answering customer queries.\n\nBased on this analysis from Statista and other observers, ChatGPT on OpenAI reached their first million users after only five days compared to 2.5 months for Instagram that was the service showing the fastest adoption before that. Source: Chart: ChatGPT Sprints to One Million Users | Statista\n\nHowever, it is important to remember that with the many benefits that AI brings, there are also potential risks and challenges that must be taken into consideration, particularly potential data breaches and privacy violations. Some companies shared their concerns about employees posting sensitive information to chat-based AI services and issued guidelines on how to use these services.\n\nAs a Customer Success Manager, I’ve seen several enterprise customers exploring solutions to give their employees access to the conversational AI in a more controlled and safer environment for the company. This is how I designed this solution, called MyAssist, based on Microsoft services: Teams, Power Virtual Agent and Azure OpenAI. MyAssist is deployed to employees as an application for Microsoft Teams.\n\nIf you plan to deliver a bot service based on generative AI, I encourage the reader to review this page on responsible AI to understand how this technology works, what you can expect from this type of services, what are the limits and educate users on how to prompt practices and practice for responsible use.\n\nHow does this solution help mitigate the company’s concerns?\n\nMicrosoft Teams offers a variety of security and compliance services to protect your data and help you meet regulatory requirements. These services include data encryption, role-based access control, multi-factor authentication, and compliance with industry standards and regulations. All chat messages (questions and answers) between a user and a bot application queried from Teams are handled as any other chat messages between users in Teams. Companies that implemented Security & Compliance service like Microsoft Purview via eDiscovery, sensitive information types, retention policies, or other services like Teams Export APIs or Safe Links for Teams, and many others, can take advantage of these solutions for chat messages with bots in Teams.\n\nPower Virtual Agents lets you create powerful AI-powered chatbots for a range of requests—from providing simple answers to common questions to resolve issues requiring complex conversations. It is by far the fastest path to delivering a chat bot with built-in integration and publishing in Teams, including enabling authentication and user SSO in a click of a button. Power Virtual Agent has a large compliance offer that includes HIPAA coverage, FedRamp, SOC, MTCS, CSA and more. PVA instances are deployed in your own Power Platform environment to satisfy your security, user access, governance, data-loss prevention rules, authorized connectors, data residency requirements and more. Last, you can download and view up to seven days of conversation transcript sessions in PVA from the past 30 days (by default) and configure the transcript retention to you needs.\n\nAzure OpenAI Service gives customers advanced language AI with OpenAI GPT-4, GPT-3, Codex, and DALL-E models with the security and enterprise promise of Azure. Azure OpenAI co-develops the APIs with OpenAI, ensuring compatibility and a smooth transition from one to the other. With Azure OpenAI, customers get the security capabilities of Microsoft Azure while running the same models as OpenAI. Azure OpenAI offers private networking, regional availability, and responsible AI content filtering. With Azure OpenAI Service, customer can decide the location of the service, based on Azure regions and service availability, and review Microsoft data, privacy, and security statements on how data provided by you to the Azure OpenAI Service is processed, used, and stored.\n\nBy combining Microsoft Teams, Power Virtual Agent and Azure OpenAI Service, enterprises can provide a modern conversational AI experience, or any other generative AI model available on the Microsoft platform, that employees are so enthusiasts to use in a more controlled and secure way.\n\nAll the services used in this article are still in (limited) public preview. It’s important to remind the readers that features in preview have limitations:\n• Preview features aren’t meant for production use and may have restricted functionality.\n• These features are available before an official release so that customers can get early access and provide feedback.\n• These capabilities are in the process of rolling out and may not be available in your region yet.\n• These capabilities may be subject to usage limits or capacity throttling.\n\nFor more information, go to our preview terms.\n\nHow to access these services:\n• Microsoft Teams – You can sign-up for Microsoft Teams if you don’t have a subscription already. Please check known issues with custom apps installation if you’re using the new Teams.\n• Power Virtual Agent (preview) – If you don’t plan to deploy your bot in production, we recommend using the preview version of PVA that supports the new unified authoring canvas and Adaptive Cards format – The preview version is not mandatory but provides a better user experience to build the dialogs. Supported language is English only. You can get a 30-days trial license for Power Virtual Agent if you don’t have a license already.\n• AI Builder for Power Automate (preview) – This option provides a built-in connector for Power Automate to query ChatGPT on Azure OpenAI Service – This capability is in gated preview, and you’ll need to apply for consideration to take part in the trial. To apply, go to Limited preview request.\n• Azure OpenAI Service (preview) – You need an Azure subscription as well as an access granted to the models gpt-35-turbo or gpt-4 – These models are currently available in limited US regions. Follow this link to review the prerequisites and request access. You can get an Azure subscription for free on this link.\n\nIn this post, I document option #2 that combines PVA and Azure OpenAI Service. Option #2 provides a good trade-off between the simplicity to deploy a bot in Teams via PVA, as opposed to a full development using the Azure bot service, and better control over the AI generative models via Azure OpenAI Service, as opposed to the AI Builder connector for ChatGPT on Azure OpenAI Service.\n• If you want to explore option #1, I recommend reading this documentation – Use your Azure OpenAI Service model in Power Automate (preview) | Microsoft Learn\n• If you want to explore option #3, I recommend this GitHub repository from my colleague Freist Li – GitHub – freistli/rootbot: Bot App integrated with several LLMs services (ChatGPT, GPT-3, DALL-E) from OpenAI and Azure OpenAI.\n• If this is you first deployment of Power Virtual Agent, you’ll be asked to choose your country/region – Make sure you select a location that is compliant with your organization’s compliance and geographic boundaries – Regional settings and data locations – Power Virtual Agents | Microsoft Learn\n• Once you’ve accessed Power Virtual Agent portal, select “Create a chatbot” and then “Try the unified canvas (preview)” Note: please review the limitations of the PVA preview for more details. The preview version is recommended if you don’t intend to deploy the bot in production.\n• Give your bot a name (ex: MyAssist) and select the spoken language (ex: English)\n• (optional) Deactivate the default custom topics if you don’t intend to use them – They are named Lesson 1, 2 and 3.\n• Under Settings > Channels, select “Microsoft Teams” and then click on “Turn on Teams” – This will enable your PVA bot to run inside Microsoft Teams.\n• (optional) Once Microsoft Teams is activated, select “Edit details” if you can change the logo of your bot, its name, description or more. You can also select “Allow your users to add this bot to a team” to be able to install it into a team. Click “Save” if you make changes.\n• Under Settings > Authentication, select “Only in Teams” and click “Save”. Doing so will enforce user authentication to the bot and access via Microsoft Teams only (all other channels will be deactivated)\n• Go to “Publish”, then click on “Publish” and confirm. Your bot is now available for you only in Microsoft Teams.\n• Under “Channels > Microsoft Teams” click on “Open bot” to install the bot in Teams.\n• You’ll be redirected to Microsoft Teams where you can “Add” the bot as a personal app. Click on Add to install the Power Virtual Agent bot.\n• Test your bot in Teams – Don’t expect too much for now, just send a message and confirm that you receive a message – If you get an answer, congrats, your bot is now running in Teams!\n\nPlease check that you meet all the technical requirements and have an Azure subscription enabled to use the model gpt-35-turbo or gpt-4.\n• Search for Azure OpenAI and click on “Create” to start a new deployment.\n• Select a region that supports the models gpt-35-turbo or gpt-4 (East US and South Central US regions) and click on “Create” – More details on this documentation.\n• After the deployment, go to your instance of Azure OpenAI on the Azure portal and navigate to “Keys and Endpoint” under “Resource Management” – Copy / paste the Key value and Endpoint URL in a text file for later use.\n• Go to Model Deployments, click on “Create” and give a name to your deployment (ex: myassist-model) – Select the Model “gpt-35-turbo” and version 0301 or later (or gpt-4) and Save.\n• Go to Azure OpenAI Studio at https://oai.azure.com/ and select “Chat playground (Preview”)\n• From the Chat playground, select the Assistant setup “Shakespeare writing assistant” – this will automatically fill to “System message” and provide an example of question & answer. You can test the chat bot and configure different scenarios based on your requirements and what you expect from “MyAssist” bot. Read this documentation to know more about exploring the capabilities of the chat playground for ChatGPT.\n• Once you’re happy with the result of the conversation (you can come back on it later), click on “Clear Chat”, then “View code” and select “json” in the drop-down menu. Click on “Copy” and paste the JSON in a text file for later use.\n• Under Topics, select “System” and then click on “Fallback” – This will open the conversational flow of the Fallback topic – This topic is triggered if the bot can’t determine the user’s intent. This will be the default route in most cases as your bot has only basic custom topics configured.\n• Delete all the existing steps in this flow and add a new step – Select “Call an action > Create a flow” – This will open your Power Automate authoring canvas.\n• From Power Automate, add a “Text” input to the trigger name it “request”\n• Add an action “Initialize variable” of type JSON and paste the JSON saved from the Azure OpenAI chat playground in step 2. Name the variable “OAI_Request” like the screenshot below.\n• Update the JSON definition to include the user request from Power Virtual Agent. Pay attention to the structure and position in the JSON.\n\nHere is the JSON definition from the Shakespeare example on Azure OpenAI studio:\n\n“content”: “You are a Shakespearean writing assistant who speaks in a Shakespearean style. You help people come up with creative ideas and content like stories, poems, and songs that use Shakespearean style of writing style, including words like \\”thou\\” and \\”hath”.\n\nHere are some examples of Shakespeare’s style:\n\n – Romeo, Romeo! Wherefore art thou Romeo?\n\n – Love looks not with the eyes, but with the mind; and therefore, is winged Cupid painted blind.\n\n – Shall I compare thee to a summer’s day? Thou art more lovely and more temperate.”\n• Add an HTTP action after the trigger to query your Azure OpenAI Service endpoint. Use the following parameters in the HTTP connector:\n• URI: construct the URI following this pattern [OAI_ENDPOINT] [OAI_DEPLOYMENT_NAME] where [OAI_ENDPOINT] is the endpoint of your Azure OpenAI Service, saved from the previous configuration step 2 and [OAI_DEPLOYMENT_NAME] is the name of your OpenAI model (ex: myassist-model) example: https://myassist-bot.openai.azure.com/openai/deployments/MyAssist-model/chat/completions?api-version=2023-03-15-preview Mode info on Azure OpenAI Service REST API reference – Azure OpenAI | Microsoft Learn\n• \n• Headers:\n• api-key: [OAI_SECRET], the secret of your Azure OpenAI Service, saved from the previous configuration step 2.\n• Body: select the variable OAI_Request from previous action of the flow.\n• \n• It is highly recommended and a best security practice to store your secret in Azure Key Vault – Please read this documentation to create an Azure Key Vault to store your Azure OpenAI Service key and access it from an environment variable in Power Automate.\n• Next, we initialize the variable “OAI_Response” to store the text response from Azure OpenAI and the content filter.\n• Then, we check that ChatGPT response is OK (http code 200) – If not, set an error message in OAI_Response. The test is on the “Status code” of the previous HTTP action “HTTP query Azure OpenAI Service” and verifies that value is equal to 200.Error message value: “Sorry, I couldn't generate an answer from your prompt. Can you rephrase your request?”\n• If the condition is true (If YES path = http response code is 200), we configure the following actions:\n• Parse the JSON of the HTTP response body based on the schema below.\n• Set the value of OAI_Response with the text response from the Azure OpenAI Service. Use the following expression: Note: only the first answer generated by Azure OpenAI is extracted. Multiple responses are not supported in this pattern.\n• Configure Power Automate flow to send the response back to Power Virtual Agent. Add an output to the existing action and set the value to the variable “OAI_Response”.\n• Rename the flow as “Azure OpenAI call flow” and Save – Your Power Automate flow (wrapped) should look like this.\n• Back in Power Virtual Agent, map your Power Virtual Agent variables to Power Automate input / output.\n\nThe result should look like this:\n• Add a last action to send the response to Microsoft Teams using an Adaptive Card format. Select “+” and “Send a message” – Then “Add” > “Adaptive Card – In the editor, select “Edit formula” and paste the JSON definition below. Click on Save to validate the changes.\n\nJSON to be added in the “Edit formula” section of Power Virtual Agent in the Adaptive Card editor.\n• Publish the new version of the bot.\n\nAt this stage, you can test your bot in Power Virtual Agent and get its responses in an Adaptive card format. Once you’re happy with the tests, from Power Virtual Agent:\n• Select “Show to my teammates and shared users” (or show to everyone in my org)\n• Click on Share to make this app visible in “Built by your colleagues” in Teams app store.\n\nNow that you have your ChatGPT bot running in Teams, you can learn directly from users’ requests and better understand the questions, use-cases, and problems they are trying to solve with this new technology. All of that while keeping all these experiments in a safe and secure environment.\n\nMicrosoft Teams with Power Virtual Agent bring together an audit and analytics layer. With Azure OpenAI, you get all the flexibility of the technology combined with the playground to perfect the solution to the scenarios that are relevant at your company.\n\nAI and Copilot are infused in all our services, providing full flexibility on your design and for everyone."
    },
    {
        "link": "https://requestum.com/blog/set-up-a-private-chatgpt-instance",
        "document": "Get a free consultation on your question from our experts.\n• None How to Set Up a Private ChatGPT Instance: A 6 Steps Guide\n\nArtificial intelligence has shown its capability to upgrade many business sectors. ChatGPT is an AI tool that creates conversations that are close to human-like. Today, we will tell you more about ChatGPT's nature and capabilities. The guide provides information about how to create a private ChatGPT with your own data and keep it functional in the future. ChatGPT is an artificial intelligence generative pre-trained transformer based on natural language processing. In other words, it is a chatbot that creates human-like conversations for various purposes. According to a survey of American business leaders, ChatGPT is often involved in completing a wide range of work tasks such as code writing, content creation, customer support, meeting support, research purposes, and many others. Let's see what makes ChatGPT so useful for various tasks. Working with this chatbot, you interact with a powerful model of generative artificial intelligence. NLP allows ChatGPT to keep conversations as natural and close to human-like as possible, answer questions, and even do complex tasks. A set of available tools easily covers most of the user's needs.\n• Web browsing: The pre-trained model has an impressive amount of information, but it doesn't mean it knows everything about specialized topics. Browsing with Bing helps the program search for extra information on the web.\n• Image recognition: The integrated GPT Vision model helps the chat recognize and interpret the uploaded image. It means ChatGPT can answer questions related to the provided image or use the information from the image for required tasks.\n• Generation: You can ask ChatGPT to create various texts. It can also create images on request with the help of the DALL-E tool.\n• Translation: Language recognition and translation abilities allow it to translate answers and text into various languages depending on the user's needs.\n• Analysis: ChatGPT can extract text from uploaded documents in text formats and search for the required information. It helps GPT to create different types of summaries. Also, there is an Advanced Data Analysis tool for customers with paid plans. It can work with Excel, CSV, and JSON documents to analyze data, respond to questions, create a visualization or request, or even fix minor errors.\n• Voice recognition and generation: You can use voice during the request for ChatGPT, and it will recognize it and react to commands. On the web version, ChatGPT can also read answers aloud.\n• Advanced capabilities: GPTs are created for specific purposes and may contain specific tools. Access to private data could enhance the combination described above to improve capabilities. There are two major instances of ChatGPT: public and private generative AI. Public generation tools such as Bing, Bard, and ChatGPT are trained to work mostly with data accessible to the public. These tools are extremely helpful for content generation, providing information and recommendations on various topics and open dialogues. The main disadvantage is that all the information goes through public services, which causes privacy concerns.\n\nA private instance of ChatGPT helps to use this technology on a daily work basis, and it also gives more control of context and data privacy. A private version can improve search capability by focusing on or considering not only public data but also the company's private data. Compared to public access, private instance creates a safer AI environment for company purposes, concentrates on business privacy, and is customized to specific needs. OpenAI private instance is useful in various cases due to its ability for customization. Thanks to its customization feature, it can be used for such purposes as: What Do You Need for Setting up a ChatGPT Private Instance? The simplest thing is that you need a team of experts who can create a private instance specifically for your needs and requirements. Requestum has long-term experience working with different projects involving artificial intelligence. The most vital factors for creation are technical skills, decent hardware and software, and correct infrastructure. Let's review these key factors closer one by one. The following knowledge and skills will be especially useful during private instance setup: This set of skills combines a basic understanding of natural processing languages with deep knowledge of how to prepare your ChatGPT for work, train, and fine-tune it.\n\nHardware and software for a private instance of ChatGPT Reliable hardware and software are required to set up a private ChatGPT and ensure its proper operation. The quality of these will directly influence the level of performance and security.\n• CPU: Multi-core processors will be the best choice. For small models, a modern multi-core CPU (like Intel Core i5/i7/i9 or AMD Ryzen 5/7/9) will work well, but for larger options, consider high-performance multi-core options such as Intel Core i9, Intel Xeon, AMD Ryzen, or AMD EPYC.\n• RAM: It depends on the use scale of your future AI model. We recommend using high-performance options starting from 64GB.\n• Storage: We recommend using high-capacity SSDs, as they can provide decent speed and faster data access. For small model options, consider at least 256GB. For bigger datasets, you may need SSDs from 512 GB and more.\n• GPU: Based on our experience, we recommend high-end options starting from 16GB. Try models such as NVIDIA GTX 1660 or RTX 2060 for a small level of use. NVIDIA RTX 3080/ RTX 3090/ A100 will be better options for bigger sizes. On-premises servers require high-performance NVIDIA Tesla V100/A100 to provide the required resources for the AI model.\n• Network: For any size AI model, you will need a stable high-speed internet connection to download model weights and dependencies. On-premises options will require reliable internal networking to ensure access to the model.\n• The latest versions of the operating systems will be the best option for developing purposes.\n• We recommend to use Python 3.7 or later versions.\n• Model requires libraries and frameworks such as PyTorch 1.0 (or later), TensorFlow 2.0 (or later), and Transformers library. We divided the process of setting up a Private ChatGPT instance into several essential steps. When choosing between on-premises and cloud options (AWS, Azure, Google Cloud), you must consider costs, control possibilities, scalability, and security.\n• You have full control and customization possibilities;\n• You don't have to rely on third-party providers;\n• There is no shared infrastructure, so you can expect consistent performance;\n• It is easier to comply with company regulations and standards.\n• You will have to spend costs on hardware, infrastructure, and ongoing maintenance;\n• Scalability requires additional hardware and time for its installation;\n• You need skilled IT experts for installation, maintenance, troubleshooting, and future updates;\n• You need recovery plans and backups to reduce the risk of data loss.\n• Scaling is more cost-effective in this option;\n• There is a wide range of available services to choose from;\n• You depend on the cloud provider's service and policy, so you have less control over infrastructure;\n• The data placed outside the control zone causes additional privacy concerns;\n• Compliance with specific regulations may be more difficult as you need to verify certifications and policies with cloud providers;\n• It requires monitoring and optimization to keep services in stable operation. Once you choose your environment, you must prepare it for a future AI model. Install the required software. Make sure that you have a proper operating system with all updated and upgraded packages. You also need to have the latest possible Python version. Install Git for version control and Docker for containerization. After this, create and activate a virtual environment to manage dependencies and separate your current project from others to prevent conflicts. Now, it's time to download the ChatGPT model. You can use a pre-trained version or create it from scratch. The second option allows you to create your style of GPT, but it would require more cost and time to prepare and train it. Consider using a pre-trained model and adjusting it to your specific requirements. Once it is downloaded, you can save it in a directory of your computer and set up the runtime environment. You need to consider which data ChatGPT can process and provide the following security measures:\n• Develop authorization and authentication processes to make sure that you will give access only to authorized persons;\n\nTraining and testing your model will help detect weak points and customize the AI to be closer to expectations. For example, you must check how well the automated interaction works with CRM systems. GPT in customer support can be used to provide automated responses and recommendations for troubleshooting or forward requests to human agents. So, in this case, you will need to check the reactions and correctness of responses. Training a ChatGPT with your data ensures that ChatGPT will understand the industry-specific language and context and will be able to provide accurate and relevant information on demand. The following data is especially useful for training: Make sure that you feed relevant and accurate data with a high level of diversity to cover different scenarios. Split text into tokens and convert it to a consistent format. During the process, remove all irrelevant and low-quality data. Fine-tune the model and evaluate its performance on a valid dataset. Integrate the ChatGPT with existing systems to improve current workflows. You should still monitor the performance and provide adjustments if necessary. Regular updates and maintenance practices are the key factors for proper operation and high performance. Even after all the work is done, these procedures should stay part of the routine to ensure the model operates well. Periodically update your language model to a newer version once they are released to keep the performance accurate and responsive. Update the knowledge base with new information and reports to ensure ChatGPT stays up-to-date with your company's latest data. Monitor response time and accuracy. Gather feedback from users to see if any areas require improvements. Prioritize and troubleshot common issues reported by users and regularly fix bugs. Creating a private ChatGPT with your own data may be challenging, but it can open up new possibilities for your company. This AI tool offers a wide range of capabilities, and customization will help you adjust the model specifically to your needs and expectations. Choose the environment where you want to build it, provide the required hardware and software, and create a step for your ChatGPT tool. If you are considering adopting ChatGPT for your company to streamline operations and workflow optimization, contact us. The Requestum team will gladly offer you help and launch the proper ChatGPT model."
    },
    {
        "link": "https://medium.com/@imicknl/how-to-create-a-private-chatgpt-with-your-own-data-15754e6378a1",
        "document": "How to create a private ChatGPT with your own data\n\nWith the rise of Large Language Models (LLMs) like ChatGPT and GPT-4, many are asking if it’s possible to train a private ChatGPT with their corporate data. But is this feasible? Can such language models offer these capabilities?\n\nIn this article, I will discuss the architecture and data requirements needed to create “your private ChatGPT” that leverages your own data. We will explore the advantages of this technology and how you can overcome its current limitations.\n\n1. Disadvantages of finetuning a LLM with your own data\n\nOften people refer to finetuning (training) as a solution for adding your own data on top of a pretrained language model. However, this has drawbacks like risk of hallucinations as mentioned during the recent GPT-4 announcement. Next to that, GPT-4 has only been trained with data up to September 2021.\n• Factual correctness and traceability, where does the answer come from\n• Access control, impossible to limit certain documents to specific users or groups\n• Costs, new documents require retraining of the model and model hosting\n\nThis will make it extremely hard, close to impossible, to use fine-tuning for the purpose of Question Answering (QA). How can we overcome such limitations and still benefit from these LLMs?\n\n2. Separate your knowledge from your language model\n\nTo ensure that users receive accurate answers, we need to separate our language model from our knowledge base. This allows us to leverage the semantic understanding of our language model while also providing our users with the most relevant information. All of this happens in real-time, and no model training is required.\n\nIt might seem like a good idea to feed all documents to the model during run-time, but this isn’t feasible due to the character limit (measured in tokens) that can be processed at once. For example, GPT-3 supports up to 4K tokens, GPT-4 up to 8K or 32K tokens. Since pricing is per 1000 tokens, using fewer tokens can help to save costs as well.\n\nThe approach for this would be as follows:\n• Application finds the most relevant text that (most likely) contains the answer\n• A concise prompt with relevant document text is sent to the LLM\n• User will receive an answer or ‘No answer found’ response\n\nThis approach is often referred to as grounding the model or Retrieval Augmented Generation (RAG). The application will provide additional context to the language model, to be able to answer the question based on relevant resources."
    },
    {
        "link": "https://monovm.com/blog/chatgpt-on-vps",
        "document": "List of content you will read in this article:\n\nHave you ever been skeptical about Artificial Intelligence (AI)? Many people have been, but the truth is AI is making incredible strides, and it's becoming increasingly evident that it plays a crucial role in our everyday lives. It's a bit like a train that's picking up speed, and you don't want to be left standing at the station while it zooms by.\n\nNow, let's talk about ChatGPT. It's a shining star in the world of AI, and more and more people are realizing just how important it can be in various aspects of life. Whether you're aware of it or not, AI is becoming a part of our daily routines, and it's making things better and more efficient.\n\nSo, what can you do to stay in the loop and keep yourself updated with the latest AI developments? Well, one fantastic way is by using ChatGPT on a VPS. Think of it as your little corner of the internet where you can put ChatGPT to work, helping you with various tasks and answering your questions.\n\nThis blog post is here to guide you through the entire process, step by step. We'll walk you through setting up ChatGPT on your VPS, making it ready to assist you. By the time you finish reading, you'll have the knowledge and skills about how to deploy ChatGPT on VPS.\n\nSo, if you've ever been curious about AI or simply want to keep up with the times, you're in the right place.\n\nBefore we get into the setup process, it's essential to familiarize yourself with some important information regarding how to deploy ChatGPT on VPS or clone chatgpt.\n\nImagine you have a super-smart friend who's good with words and understands languages inside out. This friend can help you write essays, translate languages, come up with ideas, and even chat with you like a real person. Now, this super-smart friend is like the OpenAI API.\n\nThe OpenAI API is a toolbox filled with these smart friends (or, in reality, advanced AI models). One of the brightest stars in this toolbox is GPT-3.5, like the Einstein of AI. It can do lots of things with words and text, making it super useful.\n\nBut here's where it gets even more exciting. Inside this toolbox, there's a tool called the ChatGPT API. It's like giving that smart friend the ability to chat with you, answer your questions, translate languages, and even pretend to be different people in a conversation.\n\nSo, if you're a developer, this toolbox is like a treasure chest of AI possibilities. You can use it to build all sorts of clever applications. For example, you can create a chatbot that talks to customers on a website, helps with homework, or even writes stories.\n\nIn a nutshell, the OpenAI API, powered by models like GPT-3.5 and ChatGPT, is like having a team of super-smart friends ready to assist you with anything involving words and languages. It's a game-changer for developers, opening up a world of possibilities for creative and helpful AI applications. Follow the article on how to deploy ChatGPT on VPS.\n\nWhy Build and Deploy ChatGPT Clone with Open AI API\n\nCreating a ChatGPT clone with the OpenAI API has benefits. Here's why you might want to:\n\nCreating your ChatGPT clone means you're the boss. You can make it act just how you want for your unique needs, like in your industry or for your users. Customizing it is vital to make your chatbot perfect.\n\nDeploying your ChatGPT clone to work in your apps or services is smooth. It makes your software even better by adding instant, AI-powered chat features.\n\nIf your organization deals with private info, having your ChatGPT is safer. You're the boss of data security - controlling where it's stored and who gets in. This keeps your data safe from sneaky breaches.\n\nBuilding your own ChatGPT clone means you can make it fit perfectly with how much traffic you get. It's like customizing a car to drive smoothly on any road so your chatbot always works well.\n\nRead more: How to Improve the Performance of VPS?\n\nOpenAI API is good for small projects, but as they get bigger, costs can climb. Having your instance is like managing your budget. It's a smart choice when you need a chatbot a lot or for a long time.\n\nFor apps that need chatbots in places with no internet, your ChatGPT instance keeps things going without any breaks. It's like having a backup generator for electricity - reliable even in remote areas.\n\nIn some places and industries, there are rules about how data is managed. Having your ChatGPT means you can follow these rules better. You're the captain, steering clear of legal trouble.\n\nBuilding your ChatGPT clone is like having a blank canvas. You can paint any features or extras you want that the OpenAI API doesn't offer. It's like being the chef, adding your special ingredients to the recipe.\n\nHaving your ChatGPT gives you a speed boost. You can make it reply faster by tuning up the tech and connections. It's like taking the express lane on the highway of communication.\n\nYour ChatGPT clone is like your own car; you're the driver. You don't need to count on others for a ride. You can tweak it whenever you want, no need to call for help.\n\nRemember: Building and deploying your ChatGPT clone isn't all sunshine and rainbows. There are hurdles to jump, like setting up the tech, keeping things running smoothly, and having language tech and AI experts on your team. Before you dive in, think it over. Make sure you've got what it takes to tackle these challenges and meet your goals.\n\nBuilding a ChatGPT clone on a VPS (Virtual Private Server) is a complex task that involves several steps. Please note that you cannot create an exact clone of ChatGPT as it's powered by proprietary models and training data from OpenAI. However, you can create a chatbot using OpenAI's GPT-3.5 model (if available to you) or other language models that you have access to. Here's a complete guide on how to build chatgpt on VPS.\n\nMonoVM offers straightforward VPS hosting. You don't need to be tech-savvy to use our Linux VPS hosting. It's like having your piece of the internet; you don't have to share it with others, so it's dependable.\n\nWhether you have a small website or a big project, MonoVM can handle it. You can make it bigger as your project grows. And if you ever need help, our support is ready.\n\nWhen your VPS is all setup and ready, the next step is to SSH into it. SSH is like a secret handshake that lets you connect to your VPS from your computer. Once you're in, it's time to make your VPS feel like home.\n\nYou can do this by setting up the environment, which is just a fancy way of saying you're getting everything ready so your VPS can do its job. To do this on your VPS, you'll install some important stuff like Python (which is like the language your VPS speaks), pip (a tool to easily install other helpful things), and any other things your VPS might need to work its magic. It's like giving your VPS all the tools it needs to do its job well.\n\nIf you can use OpenAI's GPT-3.5 or a similar language model, make sure you get your API keys or access details. You can check OpenAI's pricing details here.\n\nYou’ll learn how to deploy a chatbot on VPS by following these steps:\n• Open a text editor like Notepad (for Windows) VSCode, Sublime Text, or PyCharm (for Windows, macOS, or Linux).\n• Write your Python script in the text editor. You can start with a simple text document and save it with a \".py\" extension, like \"my_script.py.\"\n• Install the necessary libraries. If you're using 'openai' for GPT-3.5, open your terminal or command prompt and type `pip install openai` and press Enter. This installs the library.\n• At the top of your Python script, add the line `import openai` to tell your program to use the 'openai' library. If you're using a different library, import that library instead. These steps are a necessary part of how to deploy ChatGPT on VPS.\n\nStep 3: Tell Your Program What to Do\n• Create a function or a block of code that takes user input. For example:\n• Use the library's function to send the user input to the language model API and get a response. For 'openai,' it's like this:\n\nengine=\"davinci\", # You can specify the engine you want to use.\n\nmax_tokens=50, # You can set this based on your requirements.\n\nNow, when you run your Python script, it will take what you type as input, send it to the language model using the 'openai' library, and then show you the model's response.\n\nRemember: These are simplified steps. Depending on your specific project and goals, you might need to add more complexity and error handling to your script.\n\nTo create a chatbot, you'll need to maintain the context of the conversation. Store previous messages and use them as input for the next interaction.\n\nEnsure that you handle API keys securely. Don't expose them in your code or public repositories. Implement rate limiting and other security measures to protect your VPS and the API.\n\nSet up a web server (e.g., Flask, Django) to host your chatbot application if you want to make it accessible via a web interface. Next, configure your web server to listen to incoming requests and route them to your chatbot application.\n\nThoroughly test your chatbot in various scenarios to ensure it responds correctly. Monitor the VPS for any issues, and set up error reporting if necessary.\n\nIf your chatbot experiences high traffic, consider load balancing and scaling your VPS resources accordingly.\n\nRegularly update and refine your chatbot's responses and logic to improve its performance.\n\nBe aware of the legal and ethical implications of your chatbot. Ensure it adheres to data protection regulations and doesn't engage in harmful or unethical behavior.\n\nCreate documentation or a UI if you want others to use your chatbot.\n\nTesting your ChatGPT clone is a critical step to ensure it works as intended and delivers a good user experience. Here are some steps you can follow to test your ChatGPT clone:\n• Basic Conversations: Start with basic conversations to ensure your chatbot understands and responds to simple queries correctly.\n• Edge Cases: Test edge cases, such as very short or very long messages, and see how your chatbot handles them.\n• Error Handling: Test error-handling scenarios. What happens if a user enters an unexpected or unsupported command or message?\n• Multi-turn Conversations: Test multi-turn conversations to ensure your chatbot maintains context and provides relevant responses.\n• Response Time: Measure the response time of your chatbot to ensure it responds promptly. Slow responses can frustrate users.\n• Load Testing: Simulate a high volume of concurrent users to assess how well your chatbot handles traffic. This helps identify performance bottlenecks.\n• User Interface (if applicable): If your chatbot has a web interface, test its user-friendliness, responsiveness, and design.\n• User Guidance: Check if your chatbot provides clear instructions and guidance to users, especially if it has specific commands or functions.\n• User Feedback: Gather user feedback during testing and iterate on your chatbot based on this feedback.\n• Context Handling: Ensure your chatbot remembers and appropriately references prior messages and context in the conversation.\n• Answer Quality: Evaluate the quality and accuracy of the answers provided by your chatbot. Ensure it doesn't produce incorrect or misleading information.\n• Handling Variability: Test how well your chatbot handles variations of the same question or input. It should provide consistent and relevant responses.\n• Input Validation: Test for security vulnerabilities like SQL injection or cross-site scripting (XSS) if your chatbot processes user input.\n• API Security: If your chatbot uses external APIs, make sure the communication is secure and doesn't expose sensitive information.\n\nEnsure your chatbot adheres to ethical guidelines. It should not engage in harmful, discriminatory, or unethical behavior.\n\nRegularly retest your chatbot after making updates or changes to ensure that new features or bug fixes don't introduce new issues.\n\nImplement monitoring and logging to track the performance and usage of your chatbot in production. This helps you identify and resolve issues proactively.\n\nInvite a small group of real users to test your chatbot and provide feedback. Real user feedback can be invaluable in improving your chatbot's performance and usability.\n\nEnsure that your chatbot provides help and documentation accessible to users so they can understand its capabilities and how to interact with it effectively.\n\nEnsure that your chatbot complies with relevant regulations and privacy policies. Respect user data and privacy.\n\nCheck if your chatbot is accessible to users with disabilities. Ensure it adheres to accessibility standards.\n\nTesting is an iterative process, and it's crucial to continuously refine and improve your chatbot based on\n\nOptimizing your chatbot application for better results involves fine-tuning its performance, improving user experience, and enhancing the quality of responses. Here are some strategies to optimize your chatbot application:\n\nIf you have the option to choose different language models, experiment with them to find the one that best suits your application's requirements in terms of accuracy, response quality, and speed.\n\nAdjust the maximum response length to ensure your chatbot's responses are concise and relevant. Long responses may overwhelm users.\n\nEnhance the chatbot's ability to manage context by ensuring it remembers and refers back to prior messages correctly. This improves the coherence of conversations.\n\nIf you're using a fine-tuned model, periodically retrain it with new data to keep it up to date and improve its performance.\n\nImplement data augmentation techniques to create a more diverse training dataset. This can lead to more varied and natural-sounding responses.\n\nAfter understanding how to deploy ChatGPT on VPS and building the clone, collect user feedback and use it to improve your chatbot. Analyze common user queries and refine responses to address frequently asked questions.\n\nImplement rate limiting and throttling mechanisms to prevent abuse and ensure a fair experience for all users. This also helps manage API costs.\n\nCache frequently requests responses to reduce the load on your server and improve response times.\n\nIf possible, handle multiple user requests in parallel to increase the chatbot's throughput and responsiveness.\n\nImplement robust error handling to gracefully handle unexpected user inputs or API errors. Provide clear error messages or suggestions to users.\n\nEnsure your chatbot's web interface is mobile-friendly. Mobile optimization can significantly enhance the user experience.\n\nConsider using NLP techniques to improve understanding and context management. This might include named entity recognition (NER) or sentiment analysis.\n\nEncourage user engagement by asking open-ended questions or providing suggestions for further conversation. Keeping users engaged can lead to more meaningful interactions.\n\nExperiment with different versions of your chatbot by conducting A/B tests. This allows you to measure the impact of changes on user engagement and satisfaction.\n\nContinuously monitor your chatbot's performance, response times, and user feedback. Use this data to identify areas that need improvement.\n\nKeep your chatbot up to date with the latest information and trends relevant to your application. Regularly update the knowledge base or training data.\n\nIf your chatbot serves a global audience, consider adding support for multiple languages to expand its reach.\n\nEnsure your chatbot is accessible to users with disabilities by adhering to accessibility standards.\n\nEducate users on how to interact effectively with your chatbot by providing usage instructions or hints.\n\nRegularly audit and update your chatbot's security measures to protect user data and ensure compliance with security best practices.\n\nIn summary, creating chatbots is an exciting journey with endless possibilities. Whether you're a business owner aiming to improve customer support or just curious about AI, it's a world worth exploring.\n\nWe've covered the basics, from choosing the right tools to understanding how chatbots work. Keep in mind that creating a great chatbot might involve some trial and learning, but that's all part of the adventure.\n\nAs technology keeps advancing, chatbots will play an even bigger role in our lives. So, whether you're a tech whiz or just getting started, dive in and discover how chatbots can change the way we talk to machines and make our lives easier, one conversation at a time."
    },
    {
        "link": "https://gptbots.ai/company/private-deployment",
        "document": "GPTBots works closely with enterprises, conducting joint research for specific business scenarios and providing comprehensive AI project implementation consulting and solution services.\n\nTransform Your Business with Private AI Deployment in Just Two Weeks\n\nTailored for large enterprises, financial institutions, and government sectors that demand higher levels of data compliance and security, providing advanced protection mechanisms and seamless integration with internal systems such as OA, ERP, and CRM.\n\nDifferent AI models possess varied capabilities and expertise. A unified LLM scheduling management platform is essential for effectively managing privatized deployment models, fine-tuned models, and commercial model APIs.\n\nWith rapid advancements in AI, the natural language programming capabilities of LLMs require the development of AI applications to be visual, rapidly iterative, and quickly trained. GPTBots helps enterprises focus on their core business development."
    },
    {
        "link": "https://techinformed.com/five-best-practices-to-protect-your-data-privacy-when-implementing-gen-ai",
        "document": "Gen AI is becoming increasingly popular, with many companies integrating it into their operations to enhance efficiency and innovation.\n\nFurthermore, a McKinsey & Company survey shows more companies are using AI across multiple business functions — half of respondents reported adoption in two or more areas in 2024, up from less than a third in 2023.\n\nSimilarly, according to Statista, almost 11% of employees working at global firms have tried using ChatGPT in the workplace at least once.\n\nHowever, this widespread adoption brings new security challenges, particularly regarding data privacy. For example, of those who used ChatGPT at work, almost 5% have put confidential corporate data into the AI-powered tool.\n\nIn fact, nearly one-third of employees have admitted to placing sensitive data into GenAI tools, making data leaks a top concern.\n\nAccording to a report by AI security solutions provider Hidden Layer, more than three-quarters of companies either using or exploring AI have experienced AI-related security breaches.\n\nHow are businesses using Gen AI?\n\nA study by Harmonic Security titled GenAI Unleashed found that Gen AI was used by employees to upload data to 8.25 apps on average every month.\n\nThe study found that content creation, summarising, and editing were overwhelmingly popular among workplace users, with around 47% of prompts asking apps for help in those areas.\n\nThey were followed by software engineering (15%), data interpretation, processing, and analysis (12%), business and finance (7%) and problem-solving/troubleshooting (6%).\n\nThe most popular Gen AI tool by far was ChatGPT, used by 84% of users — 6 times more popular than Google Gemini (14%), the next most popular tool.\n\nAlastair Paterson, co-founder and CEO of Harmonic Security, explains, “With a choice of over 5,000 GenAI apps and a high number of average apps used by employees, there are too many out there for IT departments to properly keep track of using existing tools. We particularly urge organisations to pay attention to apps that are training on customer data.”\n\nHow can companies ensure data privacy when using Gen AI tools?\n\nTechInformed consulted industry experts to compile a list of best practices for safeguarding data privacy in the era of Gen AI; here are our top tips.\n\nGenerative AI tools and large language models (LLMs) can store and repurpose data provided to them. To prevent unauthorised access, avoid inputting personal or proprietary information into these tools.\n\nSebastian Gierlinger, VP of Engineering at Storyblok, says, “The biggest threat we are aware of is the potential for human error when using generative AI tools to result in data breaches. Employees sharing sensitive business information while using services such as ChatGPT risk that data will be retrieved later, which could lead to leaks of confidential data and subsequent hacks.”\n\nHe says the solution could be as simple as educating employees about how to use tools like ChatGPT safely.\n\nThat said, Leanne Allen, head of AI at KPMG UK, adds that “there are security measures that can remove sensitive or personal data automatically from prompts before they are used by a generative AI model. These measures can help mitigate the risk of data leaks and breaches of legally protected information – especially since human error will likely still occur.”\n\nA comprehensive company policy on AI usage and data privacy can help mitigate many risks associated with Gen AI tools.\n\nAngus Allan, senior product manager at CreateFuture, says, “Establishing a clear AI policy from the outset can streamline this entire process by enabling businesses to tailor controls to their risk tolerance and specific use case.”\n\nAllan stresses the importance of tailoring any policy to the specific company and addressing how AI will be uniquely leveraged for that industry and use case.\n\n“An AI policy not only pre-empts data privacy risks but also sets clear expectations reduces ambiguity, and empowers teams to focus on solving the right problems,” he says.\n\n“In an era of GDPR and increased regulatory scrutiny of AI, it’s imperative for every business to get these basics right to minimise data risks and protect customers.”\n\nMost Gen AI tools have features that allow users to disable data storage. Employees should navigate to the tool’s settings and disable such features to prevent company data from being used for AI model training.\n\nPatrick Spencer, VP of corporate marketing at Kiteworks, explains, “A typical disablement feature looks something like this: navigate to Settings and, under Data Control, disable the “Improve Model for Everyone” option. Regularly review permissions to prevent unnecessary data access, ensure privacy, and thwart unauthorised access.”\n\nDeleting chat histories in AI tools can also reduce the risk of sensitive information being stored, he says.\n\n“OpenAI typically deletes chats within 30 days; however, their usage policy specifies that some chats can be retained for security or legal reasons. To delete chats, access the AI tool’s settings and find the option to manage or delete chat history.”\n\nHe adds this should be done periodically to maintain data privacy and minimise vulnerabilities.\n\nWhen using passwords, they should be long, complex, and unique for each account, including those linked to AI systems. However, CEO of cybersecurity startup Teleport, Ev Kontsevoy, advocates for moving away from passwords altogether.\n\nHe details, “Every enterprise housing modern infrastructure should cryptographically secure identities. This means basing access not on passwords but on physical-world attributes like biometric authentication and enforcing access with short-lived privileges that are only granted for individual tasks that need performing.\n\nCryptographic identities consist of three components: the device’s machine identity, the employee’s biometric marker, and a PIN. Kontsevoy says businesses can significantly reduce the attack surface threat actors can exploit with social engineering tactics by using them.\n\n“If you need a poster child for this security model, it already exists, and it’s called the iPhone. It uses facial recognition for biometric authentication, a PIN code, and a Trusted Platform Module chip inside the phone that governs its ‘machine identity.’ This is why you never hear about iPhones getting hacked.”\n• 5. Disconnect your systems from the internet\n\nTony Hasek, CEO and co-founder of cybersecurity firm Goldilock offers a unique solution: physical network segmentation, the ability to connect and disconnect networks at the press of a button.\n\n“Through a hardware-based approach, physical network segmentation enables users to segment all digital assets, from LLMs to entire networks, remotely, instantly and without using the internet,” he says.\n\nHe adds that businesses can reduce the level of sensitive data exposure by rethinking which parts of their networks they keep online and moving away from an “always-on” model.\n\n“Companies who are building their own internal large language models (LLMs) in-house are essentially creating a repository for their company’s most valuable data and intellectual property, including customer and employee data, trade secrets, and product strategies. This makes LLMs and other Gen AI models a prime target for cybercriminals.”\n\nHe concludes, “Keeping Gen AI models offline until they are needed to generate a response is a critical step in ensuring the valuable data they contain is kept safe, and physical network segmentation can ensure networks can switch from online to offline seamlessly.”\n\nNow that you’ve handled security and data privacy, you can find out how to lead the adoption of Gen AI in your enterprise (when half of all uptake is happening outside the IT department) — read more here."
    },
    {
        "link": "https://indatalabs.com/blog/data-privacy-and-ai-models",
        "document": "Companies looking to stay ahead of the curve are increasingly engaging in AI exploration. They are experimenting with various AI models, assessing their potential impact on business and people. However, this exploration comes with challenges, particularly in the realm of AI and data privacy. As AI models rely on large datasets, often containing sensitive or personal information, businesses must carefully navigate data privacy regulations and privacy-preserving techniques.\n\nCisco Data privacy benchmark study reveals that over 90% of respondents believe that generative AI demands new strategies for managing data and mitigating risks. This underscores the critical need for thoughtful governance. In this article, we explore how businesses can implement effective governance frameworks to address Generative AI data privacy.\n\nData is the core of AI applications. Without high-quality and well-managed data, AI models cannot function effectively. At this point, many businesses are facing challenges meeting these requirements. A recent Deloitte survey shows that companies have some ongoing concerns about data that hinder the implementation and scaling of GenAI solutions. The infographic below illustrates that, highlighting the most common data-related obstacles. Generative AI places specific demands on data architecture and management. Experts consistently emphasize four key principles: quality, privacy, security, and transparency. These pillars form the backbone of efficient data management and ensure that models are powerful, scalable but also ethical, and compliant with legal standards.\n• Quality: High-quality data is accurate, relevant, and representative of real-world scenarios. Effective data management helps businesses avoid errors and biases, enabling the development of reliable AI models. Continuous monitoring and updates are essential to ensure data remains relevant and maintains its effectiveness over time.\n• Privacy: Data management must comply with privacy regulations like GDPR and CCPA, which dictate how data is collected, processed, and used. Adhering to these laws fosters trust and reduces the risk of privacy breaches, ensuring data is handled in a responsible and transparent manner.\n• Security: Protecting sensitive information in AI systems is crucial. Implementing advanced security protocols, such as encryption and access control, safeguards data and limits access to authorized personnel only. AI models should be protected from potential threats that could compromise their integrity or outputs.\n• Transparency: Businesses must be able to clearly explain how their AI models process data and make decisions. It’s especially important when AI-driven decisions impact individuals or businesses. Transparency fosters trust and accountability ensuring responsible and ethical AI deployment. In addition to these core principles for AI governance, businesses can turn to AI TRiSM (Trust, Risk, and Security Management). This holistic approach integrates critical elements such as risk management, trust-building, and comprehensive security practices throughout the entire AI lifecycle. It runs like a red thread through data collection and input, model training, deployment, and continuous monitoring. All for the sake of responsible Generative AI development, where models are not only powerful but also aligned with regulatory and organizational values. AI TRiSM focuses on minimizing a variety of risks that can significantly impact the performance and ethical integrity of AI systems. Moreover, it plays a crucial role in enhancing trust by prioritizing transparency. It provides all stakeholders, such as customers, regulators, and teams, with a clear understanding of how AI models use data, make decisions, and generate outputs. In this way, businesses can navigate the complexities of AI ethics and compliance and responsibly deploy and maintain Generative AI applications.\n\nAI for business presents significant opportunities to improve operations, from optimizing customer interactions to refining decision-making processes. However, these advancements bring additional complexity when it comes to managing sensitive data. Machine learning models rely on large datasets for training, which can contain both internal and external data. Internal data may include sensitive information about employees, financial records, and proprietary business operations. External data can encompass a wide variety of sources, such as customer information, third-party data from suppliers, social media activity, or even public datasets. Meanwhile, AI and data privacy concerns arise when dealing with all sensitive information. Mitigating AI data privacy issues requires a comprehensive strategy at all stages. One of the key approaches is to anonymize data by removing or altering personal identifiers from datasets. This minimizes the risk of identifying individuals and ensures that even if a breach occurs, the data remains unusable. Another essential method is end-to-end encryption, which ensures that data is protected at every point of its lifecycle. Encryption transforms sensitive data into unreadable formats and is combined with role-based access control. Moreover, conducting regular security audits and vulnerability assessments is critical to identifying potential weaknesses in AI systems and data handling processes. Champion the development of secure AI by building AI solutions that safeguard sensitive information from breaches and misuse, ensuring privacy for users. Book a consultation When it comes to machine learning development, addressing AI data privacy is particularly critical. Businesses must ensure that sensitive data used in training machine learning models is properly anonymized and protected throughout the entire process. \n\n According to a Cisco Consumer Privacy Survey, 48% of respondents believe that AI can enhance their lives in various ways, like in shopping, streaming services, and healthcare. However, a significant 62% of consumers voiced concerns about how organizations are handling their personal data in AI applications. This highlights a growing need for transparency and trust between businesses and consumers regarding data usage. From the customer perspective, organizations must implement clear protocols for data retention and obtain explicit user consent to ensure compliance with evolving privacy regulations. By doing so, companies can not only meet legal obligations but also build stronger relationships with their customers, fostering trust in AI-driven services. Let’s look at this issue in practical cases. AI chatbots that have gained popularity, are an object of data privacy concerns. During AI chatbot development, special attention must be paid to how sensitive customer information, such as contact details, account information, and personal preferences, is handled. They interact with this data, making it critical to implement strict privacy controls. Any breach or misuse of chatbot-collected data can lead to significant security issues. Their architecture should be based on privacy-by-design principles, incorporating secure data storage, data encryption, and anonymization. AI surveillance and security play increasingly prominent roles in modern data privacy discussions due to the widespread deployment of AI systems in monitoring and protecting assets, facilities, and people. AI surveillance refers to the tools that observe, track, and analyze individuals’ activities in both physical spaces and digital environments. For example, facial recognition, automated license plate readers, or analyzing online behavior through browsing history, social media, and communication patterns. The mass collection of personal data, such as biometrics and location history, without explicit consent, is a key issue. Data breaches could result in identity theft or unauthorized profiling. Security measures like encryption and secure storage must be in place to safeguard this sensitive information. In addition, these AI models need safeguards against manipulation and bias. Ethical guidelines and regulations are necessary to prevent misuse, such as unlawful surveillance or profit-driven privacy violations.\n\nWhile Generative AI benefits are evident in enhancing efficiency and innovation, businesses must approach its implementation thoughtfully. Large language model (LLM) development presents unique challenges due to the vast amounts of data involved and the diverse range of tasks these models perform. Both input and output are subject to regulations. Input data for AI models must comply with regulations regarding privacy and consent, especially when dealing with personal or sensitive information. And as we have already stated, data quality is crucial. Security is again a concern. For instance, attackers may try to damage the input data, which will compromise the integrity of the model. On the output side, AI-generated content can unintentionally reveal sensitive or personal information, leading to privacy violations. AI outputs must be transparent and explainable to comply with accountability regulations. Outputs can also reflect biases from input data, resulting in discriminatory decisions. If not carefully monitored, AI-generated content can be harmful or misleading, posing ethical and legal challenges. The responsible use of GenAI-based applications requires companies to develop and integrate new processes to ensure safe and ethical implementation. For example, Deloitte surveyed respondents on the specific steps their organizations are taking. The top three actions identified were establishing a governance framework for the use of Generative AI tools and applications, actively monitoring regulatory requirements to ensure compliance, and conducting internal audits and testing of GenAI systems to evaluate performance, security, and reliability. A robust governance framework is essential for ensuring the responsible and ethical use of Generative AI. This framework typically includes creating clear policies around data usage, model deployment, and accountability. It should outline roles and responsibilities for AI practitioners, data scientists, and business leaders, ensuring that there is a transparent chain of accountability. In addition, the framework should establish protocols for addressing biases in the model, managing sensitive data, and aligning AI use cases with corporate ethics and compliance standards. To complement these governance efforts, securing AI models against adversarial threats is equally important. Training AI models to defend against adversarial attacks strengthens LLMs in terms of security and robustness. By incorporating adversarial examples during the development phase, organizations can enhance the model’s ability to recognize and classify malicious inputs. This helps models become more resilient to real-world adversarial threats, although regular updates and re-training are necessary to maintain these defenses. When paired with a strong governance framework, adversarial training ensures the model’s reliability and trustworthiness, protecting both the input and output data. As models evolve and are deployed in critical systems, regular internal audits become a crucial step in maintaining security and compliance. This involves stress-testing AI models for robustness, accuracy, and resilience to adversarial attacks. Auditing should cover the data pipeline, ensuring that data input and output are properly encrypted and protected from breaches. Companies should also test for bias in AI-generated content, especially if it influences decisions related to sensitive areas such as healthcare, finance, or hiring. However, securing AI systems goes beyond technical measures. Educating the practitioners is essential. For GenAI systems to be deployed responsibly, practitioners must be trained to identify and mitigate risks associated with AI, such as data privacy issues, model bias, and security vulnerabilities. Training programs should focus on equipping AI developers, data scientists, and business teams with the knowledge to detect anomalies or unethical outcomes in the model’s behavior. This may include providing education on privacy-preserving techniques, ongoing workshops, and certifications. Finally, human oversight remains essential in the responsible use of AI, especially when AI-generated content can directly impact decisions. Despite their sophistication, AI systems can still produce biased or inaccurate outputs. Ensuring that a human validator reviews all AI-generated content, particularly in high-stakes sectors like healthcare or law, is critical for maintaining accountability, accuracy, and ethical integrity. This human-in-the-loop approach ensures that AI-generated diagnostics or advice are reliable and ethically sound, reducing the risk of harmful or misleading outputs.\n\nData privacy laws such as GDPR and CCPA are designed to ensure that individuals have control over their personal data, requiring companies to obtain consent, provide transparency, and protect the data they collect. However, these laws often lag behind the technology’s rapid advancement. AI has introduced new issues such as bias, ethics, data manipulation, and transparency. Recognizing these gaps, the European Union has proposed the Artificial Intelligence Act. It categorizes AI applications into four risk levels: unacceptable risk, high risk (e.g., AI in healthcare and finance), limited risk, and minimal risk. High-risk AI systems will face stringent requirements, including mandatory transparency, human oversight, and clear accountability mechanisms to ensure ethical and safe AI use. In the U.S. there is an initiative such as the Algorithmic Accountability Act, which would require companies to assess AI models on privacy, fairness, and discrimination. According to this, companies are accountable for the outcomes of their AI models, particularly when those models affect people’s lives. AI systems can be vulnerable to hacking, data breaches, and other adversarial attacks. And AI models security is an ongoing concern. Laws like the Cybersecurity Act in the EU establish standards for AI-related cybersecurity, particularly in critical infrastructure. Ensuring that AI systems comply with cybersecurity regulations is essential for protecting sensitive data and preventing malicious use of AI technologies. Staying up-to-date with evolving regulations is a critical part of maintaining compliance in AI operations. Companies must actively monitor changes in data privacy laws, AI ethics guidelines, and security standards. Ensuring compliance also includes conducting regular assessments of how AI models handle personal data."
    },
    {
        "link": "https://sentra.io/blog/safeguarding-data-integrity-and-privacy-in-the-age-of-ai-powered-large-language-models-llms",
        "document": "In the burgeoning realm of artificial intelligence (AI), Large Language Models (LLMs) have emerged as transformative tools, enabling the development of applications that revolutionize customer experiences and streamline business operations. These sophisticated AI models, trained on massive amounts of text data, can generate human-quality text, translate languages, write different kinds of creative content, and answer questions in an informative way.\n\nUnfortunately, the extensive data consumption and rapid adoption of LLMs has also brought to light critical challenges surrounding the protection of data integrity and privacy during the training process. As organizations strive to harness the power of LLMs responsibly, it is imperative to address these vulnerabilities and ensure that sensitive information remains secure.\n\n‍The training of LLMs often involves the utilization of vast amounts of data, often containing sensitive information such as personally identifiable information (PII), intellectual property, and financial records. This wealth of data presents a tempting target for malicious actors seeking to exploit vulnerabilities and gain unauthorized access.\n\nOne of the primary challenges is preventing data leakage or public disclosure. LLMs can inadvertently disclose sensitive information if not properly configured or protected. This disclosure can occur through various means, such as unauthorized access to training data, vulnerabilities in the LLM itself, or improper handling of user inputs.\n\nAnother critical concern is avoiding overly permissive configurations. LLMs can be configured to allow users to provide inputs that may contain sensitive information. If these inputs are not adequately filtered or sanitized, they can be incorporated into the LLM's training data, potentially leading to the disclosure of sensitive information.\n\nFinally, organizations must be mindful of the potential for bias or error in LLM training data. Biased or erroneous data can lead to biased or erroneous outputs from the LLM, which can have detrimental consequences for individuals and organizations.\n\nThe OWASP Top 10 for LLM Applications identifies and prioritizes critical vulnerabilities that can arise in LLM applications. Among these, LLM03 Training Data Poisoning, LLM06 Sensitive Information Disclosure, LLM08 Excessive Agency, and LLM10 Model Theft pose significant risks that cybersecurity professionals must address. Let's dive into these:\n\nLLM03 addresses the vulnerability of LLMs to training data poisoning, a malicious attack where carefully crafted data is injected into the training dataset to manipulate the model's behavior. This can lead to biased or erroneous outputs, undermining the model's reliability and trustworthiness.\n\nThe consequences of LLM03 can be severe. Poisoned models can generate biased or discriminatory content, perpetuating societal prejudices and causing harm to individuals or groups. Moreover, erroneous outputs can lead to flawed decision-making, resulting in financial losses, operational disruptions, or even safety hazards.\n\nLLM06 highlights the vulnerability of LLMs to inadvertently disclosing sensitive information present in their training data. This can occur when the model is prompted to generate text or code that includes personally identifiable information (PII), trade secrets, or other confidential data.\n\nThe potential consequences of LLM06 are far-reaching. Data breaches can lead to financial losses, reputational damage, and regulatory penalties. Moreover, the disclosure of sensitive information can have severe implications for individuals, potentially compromising their privacy and security.\n\nLLM08 focuses on the risk of LLMs exhibiting excessive agency, meaning they may perform actions beyond their intended scope or generate outputs that cause harm or offense. This can manifest in various ways, such as the model generating discriminatory or biased content, engaging in unauthorized financial transactions, or even spreading misinformation.\n\nExcessive agency poses a significant threat to organizations and society as a whole. Supply chain compromises and excessive permissions to AI-powered apps can erode trust, damage reputations, and even lead to legal or regulatory repercussions. Moreover, the spread of harmful or offensive content can have detrimental social impacts.\n\n\n\nLLM10: Model Theft\n\nLLM10 highlights the risk of model theft, where an adversary gains unauthorized access to a trained LLM or its underlying intellectual property. This can enable the adversary to replicate the model's capabilities for malicious purposes, such as generating misleading content, impersonating legitimate users, or conducting cyberattacks.\n\nModel theft poses significant threats to organizations. The loss of intellectual property can lead to financial losses and competitive disadvantages. Moreover, stolen models can be used to spread misinformation, manipulate markets, or launch targeted attacks on individuals or organizations.\n\n\n\nTo mitigate the risks associated with LLM training data, organizations must adopt a comprehensive approach to data protection. This approach should encompass data hygiene, policy enforcement, access controls, and continuous monitoring.\n\nData hygiene is essential for ensuring the integrity and privacy of LLM training data. Organizations should implement stringent data cleaning and sanitization procedures to remove sensitive information and identify potential biases or errors.\n\nPolicy enforcement is crucial for establishing clear guidelines for the handling of LLM training data. These policies should outline acceptable data sources, permissible data types, and restrictions on data access and usage.\n\nAccess controls should be implemented to restrict access to LLM training data to authorized personnel and identities only, including third party apps that may connect. This can be achieved through role-based access control (RBAC), zero-trust IAM, and multi-factor authentication (MFA) mechanisms.\n\nContinuous monitoring is essential for detecting and responding to potential threats and vulnerabilities. Organizations should implement real-time monitoring tools to identify suspicious activity and take timely action to prevent data breaches.\n\nIn the rush to innovate, developers must remain keenly aware of the inherent risks involved with training LLMs if they wish to deliver responsible, effective AI that does not jeopardize their customer's data. Specifically, it is a foremost duty to protect the integrity and privacy of LLM training data sets, which often contain sensitive information.\n\nPreventing data leakage or public disclosure, avoiding overly permissive configurations, and negating bias or error that can contaminate such models should be top priorities.\n\n\n\nTechnological solutions play a pivotal role in safeguarding data integrity and privacy during LLM training. Data security posture management (DSPM) solutions can automate data security processes, enabling organizations to maintain a comprehensive data protection posture.\n\nDSPM solutions provide a range of capabilities, including data discovery, data classification, data access governance (DAG), and data detection and response (DDR). These capabilities help organizations identify sensitive data, enforce access controls, detect data breaches, and respond to security incidents.\n\nCloud-native DSPM solutions offer enhanced agility and scalability, enabling organizations to adapt to evolving data security needs and protect data across diverse cloud environments.\n\n\n\nSentra: Automating LLM Data Security Processes\n\n\n\nHaving to worry about securing yet another threat vector should give overburdened security teams pause. But help is available.\n\n\n\nSentra has developed a data privacy and posture management solution that can automatically secure LLM training data in support of rapid AI application development. \n\n\n\nThe solution works in tandem with AWS SageMaker, GCP Vertex AI, or other AI IDEs to support secure data usage within ML training activities. The solution combines key capabilities including DSPM, DAG, and DDR to deliver comprehensive data security and privacy. \n\n\n\nIts cloud-native design discovers all of your data and ensures good data hygiene and security posture via policy enforcement, least privilege access to sensitive data, and monitoring and near real-time alerting to suspicious identity (user/app/machine) activity, such as data exfiltration, to thwart attacks or malicious behavior early. The solution frees developers to innovate quickly and for organizations to operate with agility to best meet requirements, with confidence that their customer data and proprietary information will remain protected.\n\n\n\nLLMs are now also built into Sentra’s classification engine and data security platform to provide unprecedented classification accuracy for unstructured data. Learn more about Large Language Models (LLMs) here.\n\n\n\n\n\nConclusion: Securing the Future of AI with Data Privacy\n\nAI holds immense potential to transform our world, but its development and deployment must be accompanied by a steadfast commitment to data integrity and privacy. Protecting the integrity and privacy of data in LLMs is essential for building responsible and ethical AI applications. By implementing data protection best practices, organizations can mitigate the risks associated with data leakage, unauthorized access, and bias. Sentra's DSPM solution provides a comprehensive approach to data security and privacy, enabling organizations to develop and deploy LLMs with speed and confidence.\n\nIf you want to learn more about Sentra's Data Security Platform and how LLMs are now integrated into our classification engine to deliver unmatched accuracy for unstructured data, request a demo today"
    },
    {
        "link": "https://evertrue.com/blog/ai-data-privacy-best-practices-for-advancement",
        "document": "Consider deploying LLMs and generative AI models on your own servers or in a private cloud environment. This allows you to have more control over data access and reduces the risk of exposing sensitive information to external parties.\n\nImplement federated learning techniques, which enable model training on decentralized data sources without centralizing sensitive data. In this approach, the model is sent to the data sources, trained locally, and only model updates are shared, not raw data.\n\nApply differential privacy mechanisms to the data or the model itself. This adds noise or randomness to the data or model parameters to protect individual data while still allowing for meaningful insights.\n\nExample: Suppose you have a dataset of people’s ages and you want to calculate the average age without revealing the ages of individual people. Applying differential privacy to this scenario might involve adding random noise to each person’s age before computing the average. This ensures that even if someone tries to analyze the noisy result, they won’t be able to accurately determine the age of any individual in the dataset.\n\nEnsure that data sent to and from the AI models is encrypted using strong encryption protocols. Use Virtual Private Networks (VPNs) or secure channels to protect data during transit.\n\nImplement strict access control mechanisms and strong authentication to restrict who can interact with the AI models and access sensitive data. This can include role-based access control and multi-factor authentication.\n\nContinuously monitor access to the AI models and data. Implement audit trails and logging to detect any unauthorized or suspicious activities.\n\nSecure the deployment of the AI models by using secure containers or virtualization technologies. Regularly update and patch these environments to address security vulnerabilities.\n\nConduct regular security assessments, including penetration testing and code review, to identify and address potential security weaknesses in your AI infrastructure.\n\nEstablish data retention and deletion policies to ensure that data is only stored for as long as necessary. Implement secure data disposal procedures.\n\nTrain your employees about the importance of data privacy and security. Educate them about best practices for handling sensitive data and AI models. This is something Advancement Services can do independently or in collaboration with a central IT team.\n\nWhen working with third-party AI providers, ensure that you have robust legal agreements and contracts in place that specify data handling and privacy requirements.\n\n\n\nNote: EverTrue uses the OpenAI API to power features like mobile AI constituent summaries, available in the EverTrue mobile app. The terms of service include multiple protective clauses along the lines of those referenced above. For example, no data sent or received from the API can be used for training or improving the model, so the data remains private. EverTrue’s contract with OpenAI includes more detailed clauses and privacy stipulations surrounding the API data transfer to protect our partners’ data. Chat with us to learn more about about EverTrue’s AI-powered features.\n\nConsider conducting “red team testing”, where ethical hackers simulate attacks on your AI systems to identify vulnerabilities and weaknesses.\n\nPeriodically review and audit your AI systems to ensure they comply with data privacy regulations and your internal policies."
    },
    {
        "link": "https://hai.stanford.edu/news/privacy-ai-era-how-do-we-protect-our-personal-information",
        "document": "The AI boom, including the advent of large language models (LLMs) and their associated chatbots, poses new challenges for privacy. Is our personal information part of a model’s training data? Are our prompts being shared with law enforcement? Will chatbots connect diverse threads from our online lives and output them to anyone?\n\nTo better understand these threats and to wrestle with potential solutions, Jennifer King, privacy and data policy fellow at the Stanford University Institute for Human-Centered Artificial Intelligence (Stanford HAI), and Caroline Meinhardt, Stanford HAI’s policy research manager, published a white paper titled “Rethinking Privacy in the AI Era: Policy Provocations for a Data-Centric World.” Here, King describes their main findings.\n\nWhat kinds of risks do we face, as our data is being bought and sold and used by AI systems?\n\nFirst, AI systems pose many of the same privacy risks we’ve been facing during the past decades of internet commercialization and mostly unrestrained data collection. The difference is the scale: AI systems are so data-hungry and intransparent that we have even less control over what information about us is collected, what it is used for, and how we might correct or remove such personal information. Today, it is basically impossible for people using online products or services to escape systematic digital surveillance across most facets of life—and AI may make matters even worse.\n\nSecond, there's the risk of others using our data and AI tools for anti-social purposes. For example, generative AI tools trained with data scraped from the internet may memorize personal information about people, as well as relational data about their family and friends. This data helps enable spear-phishing—the deliberate targeting of people for purposes of identity theft or fraud. Already, bad actors are using AI voice cloning to impersonate people and then extort them over good old-fashioned phones.\n\nThird, we’re seeing data such as a resume or photograph that we’ve shared or posted for one purpose being repurposed for training AI systems, often without our knowledge or consent and sometimes with direct civil rights implications.\n\nPredictive systems are being used to help screen candidates and help employers decide whom to interview for open jobs. However, there have been instances where the AI used to help with selecting candidates has been biased. For example, Amazon famously built its own AI hiring screening tool only to discover that it was biased against female hires.\n\nAnother example involves the use of facial recognition to identify and apprehend people who have committed crimes. It’s easy to think, “It's good to have a tool like facial recognition because it'll catch the bad guys.” But instead, because of the bias inherent in the data used to train existing facial recognition algorithms, we're seeing numerous false arrests of black men. The algorithms simply misidentify them.\n\nHave we become so numb to the idea that companies are taking all our data that it’s now too late to do anything?\n\nI’m an optimist. There's certainly a lot of data that's been collected about all of us, but that doesn't mean we can't still create a much stronger regulatory system that requires users to opt in to their data being collected or forces companies to delete data when it’s being misused.\n\nCurrently, practically any place you go online, your movement across different websites is being tracked. And if you're using a mobile app and you have GPS enabled on your phone, your location data is being collected. This default is the result of the industry convincing the Federal Trade Commission about 20 years ago that if we switched from opt-out to opt-in data collection, we'd never have a commercial internet. At this point I think we've established the utility of the internet. I don't think companies need that excuse for collecting people’s data.\n\nIn my view, when I’m browsing online, my data should not be collected unless or until I make some affirmative choice, like signing up for the service or creating an account. And even then, my data shouldn’t be considered public unless I’ve agreed to share it.\n\nTen years ago, most people thought about data privacy in terms of online shopping. They thought, “I don't know if I care if these companies know what I buy and what I'm looking for, because sometimes it's helpful.” But now we've seen companies shift to this ubiquitous data collection that trains AI systems, which can have major impact across society, especially our civil rights. I don’t think it’s too late to roll things back. These default rules and practices aren’t etched in stone.\n\nAs a general approach to data privacy protection, why isn’t it enough to pass data minimization and purpose limitation regulations that say companies can only gather the data they need for a limited purpose?\n\nThese types of rules are critical and necessary. They play a key role in the European privacy law [the GDPR] and in the California equivalent [the CPPA] and are an important part of the federally proposed privacy law [the ADPPA]. But I’m concerned about the way regulators end up operationalizing these rules.\n\nFor example, how does a regulator make the assessment that a company has collected too much information for the purpose for which it wants to use it? In some instances, it could be clear that a company completely overreached by collecting data it didn’t need. But it’s a more difficult question when companies (think Amazon or Google) can realistically say that they do a lot of different things, meaning they can justify collecting a lot of data. It's not an insurmountable problem with these rules, but it’s a real issue.\n\nYour white paper identifies several possible solutions to the data privacy problems posed by AI. First, you propose a shift from opt-out to opt-in data sharing, which could be made more seamless using software. How would that work?\n\nI would argue that the default should be that our data is not collected unless we affirmatively ask for it to be collected. There have been a few movements and tech solutions in that direction.\n\nOne is Apple’s App Tracking Transparency (Apple ATT), which Apple launched in 2021 to address concerns about how much user data was being collected by third-party apps. Now, when iPhone users download a new app, Apple’s iOS system asks if they want to allow the app to track them across other apps and websites. Marketing industry reports estimate that 80% to 90% of people presented with that choice say no.\n\nAnother option is for web browsers to have a built-in opt-out signal, such as Global Privacy Control, that prevents the placement of cookies by third parties or the sale of individuals’ data without the need to check a box. Currently, the California Privacy Protection Act (CPPA) provides that browsers may include this capability, but it has not been mandatory. And while some browsers (Firefox and Brave, for example) have a built-in op-out signal, the big browser companies (such as Microsoft Edge, Apple’s Safari, and Google Chrome) do not. Interestingly though, a California legislator recently proposed a change to the CPPA that would require all browser makers to respect third-party opt-out signals. This is exactly what we need so that data is not collected by every actor possible and every place you go.\n\nYou also propose taking a supply chain approach to data privacy. What do you envision that would mean?\n\nWhen I’m talking about the data supply chain, I’m talking about the ways that AI systems raise issues on the data input side and the data output side. On the input side I’m referring to the training data piece, which is where we worry about whether an individual’s personal information is being scraped from the internet and included in a system’s training data. In turn, the presence of our personal information in the training set potentially has an influence on the output side. For example, a generative AI system might have memorized my personally identifiable information and provide it as output. Or, a generative AI system could reveal something about me that is based on an inference from multiple data points that aren’t otherwise known or connected and are unrelated to any personally identifiable information in the training dataset.\n\nAt present, we depend on the AI companies to remove personal information from their training data or to set guardrails that prevent personal information from coming out on the output side. And that’s not really an acceptable situation, because we are dependent on them choosing to do the right thing.\n\nRegulating AI requires paying specific attention to the entire supply chain for the data piece—not just to protect our privacy, but also to avoid bias and improve AI models. Unfortunately, some of the discussions that we've had about regulating AI in the United States haven't been dealing with the data at all. We’ve been focused on transparency requirements around the purpose of companies’ algorithmic systems. Even the AI Act in Europe, which already has the GDPR as a privacy baseline, didn’t take a broad look at the data ecosystem that feeds AI. It was only mentioned in the context of high-risk AI systems. So, this is an area where there is a lot of work to do if we’re going to have any sense that our personal information is protected from inclusion in AI systems, including very large systems such as foundation models.\n\nYou note in your report that the focus on individual privacy rights is too limited and we need to consider collective solutions. What do you mean?\n\nIf we want to give people more control over their data in a context where huge amounts of data are being generated and collected, it’s clear to me that doubling down on individual rights isn't sufficient.\n\nIn California where we have a data privacy law, most of us don’t even know what rights we do have, let alone the time to figure out how to exercise them. And if we did want to exercise them, we’d have to make individual requests to every company we’ve interacted with to demand that they not sell our personal information—requests that we’d have to make every two years, given that these “do not sell” opt-outs are not permanent.\n\nThis all points toward the need for a collective solution so that the public has enough leverage to negotiate for their data rights at scale. To me, the concept of a data intermediary makes the most sense. It involves delegating the negotiating power over your data rights to a collective that does the work for you, which gives consumers more leverage.\n\nWe're already seeing data intermediaries take shape in some business-to-business contexts and they can take various forms, such as a data steward, trust, cooperative, collaborative, or commons. Implementing these in the consumer space would be more challenging, but I don't think it's impossible by any means.\n\nRead the full white paper, “Rethinking Privacy in the AI Era: Policy Provocations for a Data-Centric World.”\n\nStanford HAI’s mission is to advance AI research, education, policy and practice to improve the human condition. Learn more."
    }
]