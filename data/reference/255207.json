[
    {
        "link": "https://docs.sqlalchemy.org/14/orm/query.html",
        "document": "is the source of all SELECT statements generated by the ORM, both those formulated by end-user query operations as well as by high level internal operations such as related collection loading. It features a generative interface whereby successive calls return a new object, a copy of the former with additional criteria and options associated with it.\n\nobjects are normally initially generated using the method of , and in less common cases by instantiating the directly and associating with a using the method.\n\nFor a full walk through of usage, see the Object Relational Tutorial (1.x API).\n\nreturn a Query that selects from this Query’s SELECT statement. Deprecated since version 1.4: The method is considered legacy as of the 1.x series of SQLAlchemy and will be removed in 2.0. The new approach is to use the construct in conjunction with a subquery. See the section Selecting from the query itself as a subquery in the 2.0 migration notes for an example. (Background on SQLAlchemy 2.0 at: Migrating to SQLAlchemy 2.0) essentially turns the SELECT statement into a SELECT of itself. Given a query such as: There are lots of cases where may be useful. A simple one is where above, we may want to apply a row LIMIT to the set of user objects we query against, and then apply additional joins against that row-limited set: The above query joins to the entity but only against the first five results of the query: Another key behavior of is that it applies automatic aliasing to the entities inside the subquery, when they are referenced on the outside. Above, if we continue to refer to the entity without any additional aliasing applied to it, those references will be in terms of the subquery: The ORDER BY against is aliased to be in terms of the inner subquery: The automatic aliasing feature only works in a limited way, for simple filters and orderings. More ambitious constructions such as referring to the entity in joins should prefer to use explicit subquery objects, typically making use of the method to produce an explicit subquery object. Always test the structure of queries by viewing the SQL to ensure a particular structure does what’s expected! also includes the ability to modify what columns are being queried. In our example, we want to be queried by the inner query, so that we can join to the entity on the outside, but we only wanted the outer query to return the column: Looking out for Inner / Outer Columns Keep in mind that when referring to columns that originate from inside the subquery, we need to ensure they are present in the columns clause of the subquery itself; this is an ordinary aspect of SQL. For example, if we wanted to load from a joined entity inside the subquery using , we need to add those columns. Below illustrates a join of to , then a subquery, and then we’d like to access the columns: We use above before we call so that the columns are present in the inner subquery, so that they are available to the modifier we are using on the outside, producing: If we didn’t call , but still asked to load the entity, it would be forced to add the table on the outside without the correct join criteria - note the phrase at the end: *entities¶ – optional list of entities which will replace those being selected.\n\nCreate a SQL JOIN against this object’s criterion and apply generatively, returning the newly resulting . Consider a mapping between two classes and , with a relationship representing a collection of objects associated with each . The most common usage of is to create a JOIN along this relationship, using the attribute as an indicator for how this should occur: Where above, the call to along will result in SQL approximately equivalent to: In the above example we refer to as passed to as the “on clause”, that is, it indicates how the “ON” portion of the JOIN should be constructed. To construct a chain of joins, multiple calls may be used. The relationship-bound attribute implies both the left and right side of the join at once: as seen in the above example, the order in which each call to the join() method occurs is important. Query would not, for example, know how to join correctly if we were to specify , then , then , in our chain of joins; in such a case, depending on the arguments passed, it may raise an error that it doesn’t know how to join, or it may produce invalid SQL in which case the database will raise an error. In correct practice, the method is invoked in such a way that lines up with how we would want the JOIN clauses in SQL to be rendered, and each call should represent a clear link from what precedes it. A second form of allows any mapped entity or core selectable construct as a target. In this usage, will attempt to create a JOIN along the natural foreign key relationship between two entities: In the above calling form, is called upon to create the “on clause” automatically for us. This calling form will ultimately raise an error if either there are no foreign keys between the two entities, or if there are multiple foreign key linkages between the target entity and the entity or entities already present on the left side such that creating a join requires more information. Note that when indicating a join to a target without any ON clause, ORM configured relationships are not taken into account. Joins to a Target with an ON Clause The third calling form allows both the target entity as well as the ON clause to be passed explicitly. A example that includes a SQL expression as the ON clause is as follows: The above form may also use a relationship-bound attribute as the ON clause as well: The above syntax can be useful for the case where we wish to join to an alias of a particular target entity. If we wanted to join to twice, it could be achieved using two aliases set up using the function: The relationship-bound calling form can also specify a target entity using the method; a query equivalent to the one above would be: As a substitute for providing a full custom ON condition for an existing relationship, the function may be applied to a relationship attribute to augment additional criteria into the ON clause; the additional criteria will be combined with the default criteria using AND: The target of a join may also be any table or SELECT statement, which may be related to a target entity or not. Use the appropriate method in order to make a subquery out of a query: Joining to a subquery in terms of a specific relationship and/or target entity may be achieved by linking the subquery to the entity using : Controlling what to Join From In cases where the left side of the current state of is not in line with what we want to join from, the method may be used: Which will produce SQL similar to: Deprecated since version 1.4: The following features are deprecated and will be removed in SQLAlchemy 2.0. The method currently supports several usage patterns and arguments that are considered to be legacy as of SQLAlchemy 1.3. A deprecation path will follow in the 1.4 series for the following features:\n• None Joining on relationship names rather than attributes: Why it’s legacy: the string name does not provide enough context for to always know what is desired, notably in that there is no indication of what the left side of the join should be. This gives rise to flags like as well as the ability to place several join clauses in a single call which don’t solve the problem fully while also adding new calling styles that are unnecessary and expensive to accommodate internally. Modern calling pattern: Use the actual relationship, e.g. in the above case:\n• Why it’s legacy: the automatic aliasing feature of is intensely complicated, both in its internal implementation as well as in its observed behavior, and is almost never used. It is difficult to know upon inspection where and when its aliasing of a target entity, in the above case, will be applied and when it won’t, and additionally the feature has to use very elaborate heuristics to achieve this implicit behavior.\n• # ... and several more forms actually Why it’s legacy: being able to chain multiple ON clauses in one call to is yet another attempt to solve the problem of being able to specify what entity to join from, and is the source of a large variety of potential calling patterns that are internally expensive and complicated to parse and accommodate. Modern calling pattern: Use relationship-bound attributes or SQL-oriented ON clauses within separate calls, so that each call to knows what the left side should be:\n• None *props¶ – Incoming arguments for , the props collection in modern use should be considered to be a one or two argument form, either as a single “target” entity or ORM attribute-bound relationship, or as a target entity plus an “on clause” which may be a SQL expression or ORM attribute-bound relationship.\n• None isouter=False¶ – If True, the join used will be a left outer join, just as if the method were called.\n• None When using , a setting of True here will cause the join to be from the most recent joined target, rather than starting back from the original FROM clauses of the query.\n• None If True, indicate that the JOIN target should be anonymously aliased. Subsequent calls to and similar will adapt the incoming criterion to the target alias, until is called. Querying with Joins in the ORM tutorial. Mapping Class Inheritance Hierarchies for details on how is used for inheritance relationships. - a standalone ORM-level join function, used internally by , which in previous SQLAlchemy versions was the primary ORM-level joining interface.\n\nSet the FROM clause of this to a core selectable, applying it as a replacement FROM clause for corresponding mapped entities. Deprecated since version 1.4: The method is considered legacy as of the 1.x series of SQLAlchemy and will be removed in 2.0. Use the construct instead (Background on SQLAlchemy 2.0 at: Migrating to SQLAlchemy 2.0) The method supplies an alternative approach to the use case of applying an construct explicitly throughout a query. Instead of referring to the construct explicitly, automatically adapts all occurrences of the entity to the target selectable. Given a case for such as selecting objects from a SELECT statement: Above, we apply the object explicitly throughout the query. When it’s not feasible for to be referenced explicitly in many places, may be used at the start of the query to adapt the existing entity: Above, the generated SQL will show that the entity is adapted to our statement, even in the case of the WHERE clause: The method is similar to the method, in that it sets the FROM clause of the query. The difference is that it additionally applies adaptation to the other parts of the query that refer to the primary entity. If above we had used instead, the SQL generated would have been: To supply textual SQL to the method, we can make use of the construct. However, the construct needs to be aligned with the columns of our entity, which is achieved by making use of the method: itself accepts an object, so that the special options of such as may be used within the scope of the method’s adaptation services. Suppose a view also returns rows from . If we reflect this view into a , this view has no relationship to the to which we are mapped, however we can use name matching to select from it: Changed in version 1.1.7: The method now accepts an object as an alternative to a object. from_obj¶ – a object that will replace the FROM clause of this . It also may be an instance of ."
    },
    {
        "link": "https://docs.sqlalchemy.org/14/orm/tutorial.html",
        "document": "The SQLAlchemy Object Relational Mapper presents a method of associating user-defined Python classes with database tables, and instances of those classes (objects) with rows in their corresponding tables. It includes a system that transparently synchronizes all changes in state between objects and their related rows, called a unit of work, as well as a system for expressing database queries in terms of the user defined classes and their defined relationships between each other.\n\nThe ORM is in contrast to the SQLAlchemy Expression Language, upon which the ORM is constructed. Whereas the SQL Expression Language, introduced in SQL Expression Language Tutorial (1.x API), presents a system of representing the primitive constructs of the relational database directly without opinion, the ORM presents a high level and abstracted pattern of usage, which itself is an example of applied usage of the Expression Language.\n\nWhile there is overlap among the usage patterns of the ORM and the Expression Language, the similarities are more superficial than they may at first appear. One approaches the structure and content of data from the perspective of a user-defined domain model which is transparently persisted and refreshed from its underlying storage model. The other approaches it from the perspective of literal schema and SQL expression representations which are explicitly composed into messages consumed individually by the database.\n\nA successful application may be constructed using the Object Relational Mapper exclusively. In advanced situations, an application constructed with the ORM may make occasional usage of the Expression Language directly in certain areas where specific database interactions are required.\n\nThe following tutorial is in doctest format, meaning each line represents something you can type at a Python command prompt, and the following text represents the expected return value.\n\nWhen using the ORM, the configurational process starts by describing the database tables we’ll be dealing with, and then by defining our own classes which will be mapped to those tables. In modern SQLAlchemy, these two tasks are usually performed together, using a system known as Declarative Extensions, which allows us to create classes that include directives to describe the actual database table they will be mapped to. Classes mapped using the Declarative system are defined in terms of a base class which maintains a catalog of classes and tables relative to that base - this is known as the declarative base class. Our application will usually have just one instance of this base in a commonly imported module. We create the base class using the function, as follows: Now that we have a “base”, we can define any number of mapped classes in terms of it. We will start with just a single table called , which will store records for the end-users using our application. A new class called will be the class to which we map this table. Within the class, we define details about the table to which we’ll be mapping, primarily the table name, and names and datatypes of columns: The class defines a method, but note that is optional; we only implement it in this tutorial so that our examples show nicely formatted objects. A class using Declarative at a minimum needs a attribute, and at least one which is part of a primary key . SQLAlchemy never makes any assumptions by itself about the table to which a class refers, including that it has no built-in conventions for names, datatypes, or constraints. But this doesn’t mean boilerplate is required; instead, you’re encouraged to create your own automated conventions using helper functions and mixin classes, which is described in detail at Mixin and Custom Base Classes. When our class is constructed, Declarative replaces all the objects with special Python accessors known as descriptors; this is a process known as instrumentation. The “instrumented” mapped class will provide us with the means to refer to our table in a SQL context as well as to persist and load the values of columns from the database. Outside of what the mapping process does to our class, the class remains otherwise mostly a normal Python class, to which we can define any number of ordinary attributes and methods needed by our application.\n\nWith our class constructed via the Declarative system, we have defined information about our table, known as table metadata. The object used by SQLAlchemy to represent this information for a specific table is called the object, and here Declarative has made one for us. We can see this object by inspecting the attribute: The Declarative system, though highly recommended, is not required in order to use SQLAlchemy’s ORM. Outside of Declarative, any plain Python class can be mapped to any using the function directly; this less common usage is described at Imperative Mapping. When we declared our class, Declarative used a Python metaclass in order to perform additional activities once the class declaration was complete; within this phase, it then created a object according to our specifications, and associated it with the class by constructing a object. This object is a behind-the-scenes object we normally don’t need to deal with directly (though it can provide plenty of information about our mapping when we need it). The object is a member of a larger collection known as . When using Declarative, this object is available using the attribute of our declarative base class. The is a registry which includes the ability to emit a limited set of schema generation commands to the database. As our SQLite database does not actually have a table present, we can use to issue CREATE TABLE statements to the database for all tables that don’t yet exist. Below, we call the method, passing in our as a source of database connectivity. We will see that special commands are first emitted to check for the presence of the table, and following that the actual statement: Users familiar with the syntax of CREATE TABLE may notice that the VARCHAR columns were generated without a length; on SQLite and PostgreSQL, this is a valid datatype, but on others, it’s not allowed. So if running this tutorial on one of those databases, and you wish to use SQLAlchemy to issue CREATE TABLE, a “length” may be provided to the type as below: The length field on , as well as similar precision/scale fields available on , , etc. are not referenced by SQLAlchemy other than when creating tables. Additionally, Firebird and Oracle require sequences to generate new primary key identifiers, and SQLAlchemy doesn’t generate or assume these without being instructed. For that, you use the construct: A full, foolproof generated via our declarative mapping is therefore: We include this more verbose table definition separately to highlight the difference between a minimal construct geared primarily towards in-Python usage only, versus one that will be used to emit CREATE TABLE statements on a particular set of backends with more stringent requirements.\n\nTo persist our object, we it to our : At this point, we say that the instance is pending; no SQL has yet been issued and the object is not yet represented by a row in the database. The will issue the SQL to persist as soon as is needed, using a process known as a flush. If we query the database for , all pending information will first be flushed, and the query is issued immediately thereafter. For example, below we create a new object which loads instances of . We “filter by” the attribute of , and indicate that we’d like only the first result in the full list of rows. A instance is returned which is equivalent to that which we’ve added: In fact, the has identified that the row returned is the same row as one already represented within its internal map of objects, so we actually got back the identical instance as that which we just added: The ORM concept at work here is known as an identity map and ensures that all operations upon a particular row within a operate upon the same set of data. Once an object with a particular primary key is present in the , all SQL queries on that will always return the same Python object for that particular primary key; it also will raise an error if an attempt is made to place a second, already-persisted object with the same primary key within the session. We can add more objects at once using : Also, we’ve decided Ed’s nickname isn’t that great, so lets change it: The is paying attention. It knows, for example, that has been modified: and that three new objects are pending: We tell the that we’d like to issue all remaining changes to the database and commit the transaction, which has been in progress throughout. We do this via . The emits the statement for the nickname change on “ed”, as well as statements for the three new objects we’ve added: flushes the remaining changes to the database, and commits the transaction. The connection resources referenced by the session are now returned to the connection pool. Subsequent operations with this session will occur in a new transaction, which will again re-acquire connection resources when first needed. If we look at Ed’s attribute, which earlier was , it now has a value: After the inserts new rows in the database, all newly generated identifiers and database-generated defaults become available on the instance, either immediately or via load-on-first-access. In this case, the entire row was re-loaded on access because a new transaction was begun after we issued . SQLAlchemy by default refreshes data from a previous transaction the first time it’s accessed within a new transaction, so that the most recent state is available. The level of reloading is configurable as is described in Using the Session. As our object moved from being outside the , to inside the without a primary key, to actually being inserted, it moved between three out of five available “object states” - transient, pending, and persistent. Being aware of these states and what they mean is always a good idea - be sure to read Quickie Intro to Object States for a quick overview.\n\nA object is created using the method on . This function takes a variable number of arguments, which can be any combination of classes and class-instrumented descriptors. Below, we indicate a which loads instances. When evaluated in an iterative context, the list of objects present is returned: The also accepts ORM-instrumented descriptors as arguments. Any time multiple class entities or column-based entities are expressed as arguments to the function, the return result is expressed as tuples: The tuples returned by are named tuples, supplied by the class, and can be treated much like an ordinary Python object. The names are the same as the attribute’s name for an attribute, and the class name for a class: You can control the names of individual column expressions using the construct, which is available from any -derived object, as well as any class attribute which is mapped to one (such as ): The name given to a full entity such as , assuming that multiple entities are present in the call to , can be controlled using : Basic operations with include issuing LIMIT and OFFSET, most conveniently using Python array slices and typically in conjunction with ORDER BY: and filtering results, which is accomplished either with , which uses keyword arguments: …or , which uses more flexible SQL expression language constructs. These allow you to use regular Python operators with the class-level attributes on your mapped class: The object is fully generative, meaning that most method calls return a new object upon which further criteria may be added. For example, to query for users named “ed” with a full name of “Ed Jones”, you can call twice, which joins criteria using : Here’s a rundown of some of the most common operators used in : renders the LIKE operator, which is case insensitive on some backends, and case sensitive on others. For guaranteed case-insensitive comparisons, use . most backends don’t support ILIKE directly. For those, the operator renders an expression combining LIKE with the LOWER SQL function applied to each operand. Make sure you use and not the Python operator! Make sure you use and not the Python operator! uses a database-specific or function; its behavior will vary by backend and is not available on some backends such as SQLite. A number of methods on immediately issue SQL and return a value containing loaded database results. Here’s a brief tour:\n• When the object returns lists of ORM-mapped objects such as the object above, the entries are deduplicated based on primary key, as the results are interpreted from the SQL result set. That is, if SQL query returns a row with twice, you would only get a single object back in the result list. This does not apply to the case when individual columns are queried. My Query does not return the same number of objects as query.count() tells me - why?\n• None applies a limit of one and returns the first result as a scalar:\n• None fully fetches all rows, and if not exactly one object identity or composite row is present in the result, raises an error. With multiple rows found: The method is great for systems that expect to handle “no items found” versus “multiple items found” differently; such as a RESTful web service, which may want to raise a “404 not found” when no results are found, but raise an application error when multiple results are found.\n• None is like , except that if no results are found, it doesn’t raise an error; it just returns . Like , however, it does raise an error if multiple results are found.\n• None invokes the method, and upon success returns the first column of the row: Literal strings can be used flexibly with , by specifying their use with the construct, which is accepted by most applicable methods. For example, and : Bind parameters can be specified with string-based SQL, using a colon. To specify the values, use the method: To use an entirely string-based statement, a construct representing a complete statement can be passed to . Without further specification, the ORM will match columns in the ORM mapping to the result returned by the SQL statement based on column name: For better targeting of mapped columns to a textual SELECT, as well as to match on a specific subset of columns in arbitrary order, individual mapped columns are passed in the desired order to : When selecting from a construct, the may still specify what columns and entities are to be returned; instead of we can also ask for the columns individually, as in any other case: \"SELECT name, id FROM users where name=:name\" sql Using Textual SQL - The construct explained from the perspective of Core-only queries. used to be a very complicated method when it would try to guess whether or not a subquery was needed around the existing query, and in some exotic cases it wouldn’t do the right thing. Now that it uses a simple subquery every time, it’s only two lines long and always returns the right answer. Use if a particular statement absolutely cannot tolerate the subquery being present. The method is used to determine how many rows the SQL statement would return. Looking at the generated SQL above, SQLAlchemy always places whatever it is we are querying into a subquery, then counts the rows from that. In some cases this can be reduced to a simpler , however modern versions of SQLAlchemy don’t try to guess when this is appropriate, as the exact SQL can be emitted using more explicit means. For situations where the “thing to be counted” needs to be indicated specifically, we can specify the “count” function directly using the expression , available from the construct. Below we use it to return the count of each distinct user name: To achieve our simple , we can apply it as: The usage of can be removed if we express the count in terms of the primary key directly:\n\nLet’s consider how a second table, related to , can be mapped and queried. Users in our system can store any number of email addresses associated with their username. This implies a basic one to many association from the to a new table which stores email addresses, which we will call . Using declarative, we define this table along with its mapped class, : The above class introduces the construct, which is a directive applied to that indicates that values in this column should be constrained to be values present in the named remote column. This is a core feature of relational databases, and is the “glue” that transforms an otherwise unconnected collection of tables to have rich overlapping relationships. The above expresses that values in the column should be constrained to those values in the column, i.e. its primary key. A second directive, known as , tells the ORM that the class itself should be linked to the class, using the attribute . uses the foreign key relationships between the two tables to determine the nature of this linkage, determining that will be many to one. An additional directive is placed on the mapped class under the attribute . In both directives, the parameter is assigned to refer to the complementary attribute names; by doing so, each can make intelligent decision about the same relationship as expressed in reverse; on one side, refers to a instance, and on the other side, refers to a list of instances. The parameter is a newer version of a very common SQLAlchemy feature called . The parameter hasn’t gone anywhere and will always remain available! The is the same thing, except a little more verbose and easier to manipulate. For an overview of the entire topic, see the section Using the legacy ‘backref’ relationship parameter. The reverse side of a many-to-one relationship is always one to many. A full catalog of available configurations is at Basic Relationship Patterns. The two complementing relationships and are referred to as a bidirectional relationship, and is a key feature of the SQLAlchemy ORM. The section Using the legacy ‘backref’ relationship parameter discusses the “backref” feature in detail. Arguments to which concern the remote class can be specified using strings, assuming the Declarative system is in use. Once all mappings are complete, these strings are evaluated as Python expressions in order to produce the actual argument, in the above case the class. The names which are allowed during this evaluation include, among other things, the names of all classes which have been created in terms of the declared base. See the docstring for for more detail on argument style.\n• None a FOREIGN KEY constraint in most (though not all) relational databases can only link to a primary key column, or a column that has a UNIQUE constraint.\n• None a FOREIGN KEY constraint that refers to a multiple column primary key, and itself has multiple columns, is known as a “composite foreign key”. It can also reference a subset of those columns.\n• None FOREIGN KEY columns can automatically update themselves, in response to a change in the referenced column or row. This is known as the CASCADE referential action, and is a built in function of the relational database.\n• None FOREIGN KEY can refer to its own table. This is referred to as a “self-referential” foreign key.\n• None Read more about foreign keys at Foreign Key - Wikipedia. We’ll need to create the table in the database, so we will issue another CREATE from our metadata, which will skip over tables which have already been created:\n\nNow that we have two tables, we can show some more features of , specifically how to create queries that deal with both tables at the same time. The Wikipedia page on SQL JOIN offers a good introduction to join techniques, several of which we’ll illustrate here. To construct a simple implicit join between and , we can use to equate their related columns together. Below we load the and entities at once using this method: The actual SQL JOIN syntax, on the other hand, is most easily achieved using the method: knows how to join between and because there’s only one foreign key between them. If there were no foreign keys, or several, works better when one of the following forms are used: # specify relationship from left to right As you would expect, the same idea is used for “outer” joins, using the function: The reference documentation for contains detailed information and examples of the calling styles accepted by this method; is an important method at the center of usage for any SQL-fluent application. What does select from if there’s multiple entities? The method will typically join from the leftmost item in the list of entities, when the ON clause is omitted, or if the ON clause is a plain SQL expression. To control the first entity in the list of JOINs, use the method: When querying across multiple tables, if the same table needs to be referenced more than once, SQL typically requires that the table be aliased with another name, so that it can be distinguished against other occurrences of that table. This is supported using the construct. When joining to relationships using using , the special attribute method may be used to alter the target of a relationship join to refer to a given object. Below we join to the entity twice, to locate a user who has two distinct email addresses at the same time: In addition to using the method, it is common to see the method joining to a specific target by indicating it separately: The is suitable for generating statements which can be used as subqueries. Suppose we wanted to load objects along with a count of how many records each user has. The best way to generate SQL like this is to get the count of addresses grouped by user ids, and JOIN to the parent. In this case we use a LEFT OUTER JOIN so that we get rows back for those users who don’t have any addresses, e.g.: Using the , we build a statement like this from the inside out. The accessor returns a SQL expression representing the statement generated by a particular - this is an instance of a construct, which are described in SQL Expression Language Tutorial (1.x API): The keyword generates SQL functions, and the method on produces a SQL expression construct representing a SELECT statement embedded within an alias (it’s actually shorthand for ). Once we have our statement, it behaves like a construct, such as the one we created for at the start of this tutorial. The columns on the statement are accessible through an attribute called : Above, we just selected a result that included a column from a subquery. What if we wanted our subquery to map to an entity ? For this we use to associate an “alias” of a mapped class to a subquery: The EXISTS keyword in SQL is a boolean operator which returns True if the given expression contains any rows. It may be used in many scenarios in place of joins, and is also useful for locating rows which do not have a corresponding row in a related table. There is an explicit EXISTS construct, which looks like this: The features several operators which make usage of EXISTS automatically. Above, the statement can be expressed along the relationship using : takes criterion as well, to limit the rows matched: is the same operator as for many-to-one relationships (note the operator here too, which means “NOT”): Here’s all the operators which build on relationships - each one is linked to its API documentation which includes full details on usage and behavior:\n• None IS NULL (many-to-one comparison, also uses ):\n• None (used for any relationship):\n\nRecall earlier that we illustrated a lazy loading operation, when we accessed the collection of a and SQL was emitted. If you want to reduce the number of queries (dramatically, in many cases), we can apply an eager load to the query operation. SQLAlchemy offers three types of eager loading, two of which are automatic, and a third which involves custom criterion. All three are usually invoked via functions known as query options which give additional instructions to the on how we would like various attributes to be loaded, via the method. In this case we’d like to indicate that should load eagerly. A good choice for loading a set of objects as well as their related collections is the option, which emits a second SELECT statement that fully loads the collections associated with the results just loaded. The name “selectin” originates from the fact that the SELECT statement uses an IN clause in order to locate related rows for multiple objects at once: The other automatic eager loading function is more well known and is called . This style of loading emits a JOIN, by default a LEFT OUTER JOIN, so that the lead object as well as the related object or collection is loaded in one step. We illustrate loading the same collection in this way - note that even though the collection on is actually populated right now, the query will emit the extra join regardless: Note that even though the OUTER JOIN resulted in two rows, we still only got one instance of back. This is because applies a “uniquing” strategy, based on object identity, to the returned entities. This is specifically so that joined eager loading can be applied without affecting the query results. While has been around for a long time, is a newer form of eager loading. tends to be more appropriate for loading related collections while tends to be better suited for many-to-one relationships, due to the fact that only one row is loaded for both the lead and the related object. Another form of loading, , also exists, which can be used in place of when making use of composite primary keys on certain backends. is not a replacement for The join created by is anonymously aliased such that it does not affect the query results. An or call cannot reference these aliased tables - so-called “user space” joins are constructed using . The rationale for this is that is only applied in order to affect how related objects or collections are loaded as an optimizing detail - it can be added or removed with no impact on actual results. See the section The Zen of Joined Eager Loading for a detailed description of how this is used. A third style of eager loading is when we are constructing a JOIN explicitly in order to locate the primary rows, and would like to additionally apply the extra table to a related object or collection on the primary object. This feature is supplied via the function, and is most typically useful for pre-loading the many-to-one object on a query that needs to filter on that same object. Below we illustrate loading an row as well as the related object, filtering on the named “jack” and using to apply the “user” columns to the attribute: For more information on eager loading, including how to configure various forms of loading by default, see the section Relationship Loading Techniques.\n\nBuilding a Many To Many Relationship¶ We’re moving into the bonus round here, but lets show off a many-to-many relationship. We’ll sneak in some other features too, just to take a tour. We’ll make our application a blog application, where users can write items, which have items associated with them. For a plain many-to-many, we need to create an un-mapped construct to serve as the association table. This looks like the following: Above, we can see declaring a directly is a little different than declaring a mapped class. is a constructor function, so each individual argument is separated by a comma. The object is also given its name explicitly, rather than it being taken from an assigned attribute name. Next we define and , using complementary constructs, each referring to the table as an association table: # many to many BlogPost<->Keyword The above class declarations illustrate explicit methods. Remember, when using Declarative, it’s optional! Above, the many-to-many relationship is . The defining feature of a many-to-many relationship is the keyword argument which references a object representing the association table. This table only contains columns which reference the two sides of the relationship; if it has any other columns, such as its own primary key, or foreign keys to other tables, SQLAlchemy requires a different usage pattern called the “association object”, described at Association Object. We would also like our class to have an field. We will add this as another bidirectional relationship, except one issue we’ll have is that a single user might have lots of blog posts. When we access , we’d like to be able to filter results further so as not to load the entire collection. For this we use a setting accepted by called , which configures an alternate loader strategy on the attribute: Usage is not too different from what we’ve been doing. Let’s give Wendy some blog posts: We’re storing keywords uniquely in the database, but we know that we don’t have any yet, so we can just create them: We can now look up all blog posts with the keyword ‘firstpost’. We’ll use the operator to locate “blog posts where any of its keywords has the keyword string ‘firstpost’”: If we want to look up posts owned by the user , we can tell the query to narrow down to that object as a parent: Or we can use Wendy’s own relationship, which is a “dynamic” relationship, to query straight from there:"
    },
    {
        "link": "https://stackoverflow.com/questions/41305129/sqlalchemy-dynamic-filtering",
        "document": "I'm trying to implement dynamic filtering using SQLAlchemy ORM.\n\nI was looking through StackOverflow and found very similar question:SQLALchemy dynamic filter_by\n\nIt's useful for me, but not enough.\n\nSo, here is some example of code, I'm trying to write:\n\nthen I'm trying to reuse it with something very similar:\n\nAfter the second run, there are some issues:\n\nWhen I'm trying to change my to:\n\nBut it works fine for manual querying:\n\nWhat is wrong with my filters?\n\nBTW, it looks like it works fine for case:\n\nBut following the recommendations from mentioned post I'm trying to use just .\n\nIf it's just one possible to use instead of , is there any differences between these two methods?"
    },
    {
        "link": "https://docs.sqlalchemy.org/14/orm/queryguide.html",
        "document": "This section provides an overview of emitting queries with the SQLAlchemy ORM using 2.0 style usage.\n\nReaders of this section should be familiar with the SQLAlchemy overview at SQLAlchemy 1.4 / 2.0 Tutorial, and in particular most of the content here expands upon the content at Selecting Rows with Core or ORM.\n\nSELECT statements are produced by the function which returns a object:\n\nTo invoke a with the ORM, it is passed to :\n\nThe construct accepts ORM entities, including mapped classes as well as class-level attributes representing mapped columns, which are converted into ORM-annotated and elements at construction time. A object that contains ORM-annotated entities is normally executed using a object, and not a object, so that ORM-related features may take effect, including that instances of ORM-mapped objects may be returned. When using the directly, result rows will only contain column-level data. Below we select from the entity, producing a that selects from the mapped to which is mapped: When selecting from ORM entities, the entity itself is returned in the result as a row with a single element, as opposed to a series of individual columns; for example above, the returns objects that have just a single element per row, that element holding onto a object: When selecting a list of single-element rows containing ORM entities, it is typical to skip the generation of objects and instead receive ORM entities directly, which is achieved using the method: ORM Entities are named in the result row based on their class name, such as below where we SELECT from both and at the same time: The attributes on a mapped class, such as and , have a similar behavior as that of the entity class itself such as in that they are automatically converted into ORM-annotated Core objects when passed to . They may be used in the same way as table columns are used: ORM attributes, themselves known as objects, can be used in the same way as any , and are delivered in result rows just the same way, such as below where we refer to their values by column name within each row: The construct is an extensible ORM-only construct that allows sets of column expressions to be grouped in result rows: The is potentially useful for creating lightweight views as well as custom column groupings such as mappings. As discussed in the tutorial at Using Aliases, to create a SQL alias of an ORM entity is achieved using the construct against a mapped class: As is the case when using , the SQL alias is anonymously named. For the case of selecting the entity from a row with an explicit name, the parameter may be passed as well: The construct is also central to making use of subqueries with the ORM; the sections Selecting Entities from Subqueries and Joining to Subqueries discusses this further. Getting ORM Results from Textual and Core Statements¶ The ORM supports loading of entities from SELECT statements that come from other sources. The typical use case is that of a textual SELECT statement, which in SQLAlchemy is represented using the construct. The construct, once constructed, can be augmented with information about the ORM-mapped columns that the statement would load; this can then be associated with the ORM entity itself so that ORM objects can be loaded based on this statement. Given a textual SQL statement we’d like to load from: \"SELECT id, name, fullname FROM user_account ORDER BY id\" We can add column information to the statement by using the method; when this method is invoked, the object is converted into a object, which takes on a role that is comparable to the construct. The method is typically passed objects or equivalent, and in this case we can make use of the ORM-mapped attributes on the class directly: We now have an ORM-configured SQL construct that as given, can load the “id”, “name” and “fullname” columns separately. To use this SELECT statement as a source of complete entities instead, we can link these columns to a regular ORM-enabled construct using the method: The same object can also be converted into a subquery using the method, and linked to the entity to it using the construct, in a similar manner as discussed below in Selecting Entities from Subqueries: # using aliased() to select from a subquery The difference between using the directly with versus making use of is that in the former case, no subquery is produced in the resulting SQL. This can in some scenarios be advantageous from a performance or complexity perspective. Using INSERT, UPDATE and ON CONFLICT (i.e. upsert) to return ORM Objects - The method also works with DML statements that support RETURNING.\n\nSelecting Entities from UNIONs and other set operations¶ The and functions are the most common set operations, which along with other set operations such as , and others deliver an object known as a , which is composed of multiple constructs joined by a set-operation keyword. ORM entities may be selected from simple compound selects using the method illustrated previously at Getting ORM Results from Textual and Core Statements. In this method, the UNION statement is the complete statement that will be rendered, no additional criteria can be added after is used: A construct can be more flexibly used within a query that can be further modified by organizing it into a subquery and linking it to an ORM entity using , as illustrated previously at Selecting Entities from Subqueries. In the example below, we first use to create a subquery of the UNION ALL statement, we then package that into the construct where it can be used like any other mapped entity in a construct, including that we can add filtering and order by criteria based on its exported columns: Selecting ORM Entities from Unions - in the SQLAlchemy 1.4 / 2.0 Tutorial\n\nThe and methods are used to construct SQL JOINs against a SELECT statement. This section will detail ORM use cases for these methods. For a general overview of their use from a Core perspective, see Explicit FROM clauses and JOINs in the SQLAlchemy 1.4 / 2.0 Tutorial. The usage of in an ORM context for 2.0 style queries is mostly equivalent, minus legacy use cases, to the usage of the method in 1.x style queries. Consider a mapping between two classes and , with a relationship representing a collection of objects associated with each . The most common usage of is to create a JOIN along this relationship, using the attribute as an indicator for how this should occur: Where above, the call to along will result in SQL approximately equivalent to: In the above example we refer to as passed to as the “on clause”, that is, it indicates how the “ON” portion of the JOIN should be constructed. To construct a chain of joins, multiple calls may be used. The relationship-bound attribute implies both the left and right side of the join at once. Consider additional entities and , where the relationship refers to the entity, and the relationship refers to the entity, via an association table . Two calls will result in a JOIN first from to , and a second from to . However, since is a many to many relationship, it results in two separate JOIN elements, for a total of three JOIN elements in the resulting SQL: The order in which each call to the method is significant only to the degree that the “left” side of what we would like to join from needs to be present in the list of FROMs before we indicate a new target. would not, for example, know how to join correctly if we were to specify , and would raise an error. In correct practice, the method is invoked in such a way that lines up with how we would want the JOIN clauses in SQL to be rendered, and each call should represent a clear link from what precedes it. All of the elements that we target in the FROM clause remain available as potential points to continue joining FROM. We can continue to add other elements to join FROM the entity above, for example adding on the relationship to our chain of joins: A second form of allows any mapped entity or core selectable construct as a target. In this usage, will attempt to infer the ON clause for the JOIN, using the natural foreign key relationship between two entities: In the above calling form, is called upon to infer the “on clause” automatically. This calling form will ultimately raise an error if either there are no setup between the two mapped constructs, or if there are multiple linakges between them such that the appropriate constraint to use is ambiguous. When making use of or without indicating an ON clause, ORM configured constructs are not taken into account. Only the configured relationships between the entities at the level of the mapped objects are consulted when an attempt is made to infer an ON clause for the JOIN. Joins to a Target with an ON Clause¶ The third calling form allows both the target entity as well as the ON clause to be passed explicitly. A example that includes a SQL expression as the ON clause is as follows: The expression-based ON clause may also be the relationship-bound attribute; this form in fact states the target of twice, however this is accepted: The above syntax has more functionality if we use it in terms of aliased entities. The default target for is the class, however if we pass aliased forms using , the form will be used as the target, as in the example below: When using relationship-bound attributes, the target entity can also be substituted with an aliased entity by using the method. The same example using this method would be: As a substitute for providing a full custom ON condition for an existing relationship, the function may be applied to a relationship attribute to augment additional criteria into the ON clause; the additional criteria will be combined with the default criteria using AND. Below, the ON criteria between and contains two separate elements joined by , the first one being the natural join along the foreign key, and the second being a custom limiting criteria: The method also works with loader strategies. See the section Adding Criteria to loader options for an example. The target of a join may be any “selectable” entity which usefully includes subqueries. When using the ORM, it is typical that these targets are stated in terms of an construct, but this is not strictly required particularly if the joined entity is not being returned in the results. For example, to join from the entity to the entity, where the entity is represented as a row limited subquery, we first construct a object using , which may then be used as the target of the method: The above SELECT statement when invoked via will return rows that contain entities, but not entities. In order to add entities to the set of entities that would be returned in result sets, we construct an object against the entity and the custom subquery. Note we also apply a name to the construct so that we may refer to it by name in the result row: The same subquery may be referred towards by multiple entities as well, for a subquery that represents more than one entity. The subquery itself will remain unique within the statement, while the entities that are linked to it using refer to distinct sets of columns: In cases where the left side of the current state of is not in line with what we want to join from, the method may be used: The method accepts two or three arguments, either in the form , or : To set up the initial FROM clause for a SELECT such that can be used subsequent, the method may also be used: The method does not actually have the final say on the order of tables in the FROM clause. If the statement also refers to a construct that refers to existing tables in a different order, the construct takes precedence. When we use methods like and , these methods are ultimately creating such a object. Therefore we can see the contents of being overridden in a case like this: Where above, we see that the FROM clause is , even though we stated first. Because of the method call, the statement is ultimately equivalent to the following: The construct above is added as another entry in the list which supersedes the previous entry."
    },
    {
        "link": "https://datacamp.com/tutorial/sqlalchemy-tutorial-examples",
        "document": "In this course, you'll learn the basics of relational databases and how to interact with them."
    },
    {
        "link": "http://docs.sqlalchemy.org/en/latest/orm/declarative_tables.html",
        "document": "As introduced at Declarative Mapping, the Declarative style includes the ability to generate a mapped object at the same time, or to accommodate a or other object directly.\n\nThe following examples assume a declarative base class as:\n\nAll of the examples that follow illustrate a class inheriting from the above . The decorator style introduced at Declarative Mapping using a Decorator (no declarative base) is fully supported with all the following examples as well, as are legacy forms of Declarative Base including base classes generated by .\n\nThere are several patterns available which provide for producing mapped classes against a series of objects that were introspected from the database, using the reflection process described at Reflecting Database Objects. A simple way to map a class to a table reflected from the database is to use a declarative hybrid mapping, passing the parameter to the constructor for : A variant on the above pattern that scales for many tables is to use the method to reflect a full set of objects at once, then refer to them from the : One caveat to the approach of using is that the mapped classes cannot be declared until the tables have been reflected, which requires the database connectivity source to be present while the application classes are being declared; it’s typical that classes are declared as the modules of an application are being imported, but database connectivity isn’t available until the application starts running code so that it can consume configuration information and create an engine. There are currently two approaches to working around this, described in the next two sections. To accommodate the use case of declaring mapped classes where reflection of table metadata can occur afterwards, a simple extension called the mixin is available, which alters the declarative mapping process to be delayed until a special class-level method is called, which will perform the reflection process against a target database, and will integrate the results with the declarative table mapping process, that is, classes which use the attribute: Above, we create a mixin class that will serve as a base for classes in our declarative hierarchy that should become mapped when the method is called. The above mapping is not complete until we do so, given an : The purpose of the class is to define the scope at which classes should be reflectively mapped. The plugin will search among the subclass tree of the target against which is called and reflect all tables which are named by declared classes; tables in the target database that are not part of mappings and are not related to the target tables via foreign key constraint will not be reflected. A more automated solution to mapping against an existing database where table reflection is to be used is to use the Automap extension. This extension will generate entire mapped classes from a database schema, including relationships between classes based on observed foreign key constraints. While it includes hooks for customization, such as hooks that allow custom class naming and relationship naming schemes, automap is oriented towards an expedient zero-configuration style of working. If an application wishes to have a fully explicit model that makes use of table reflection, the DeferredReflection class may be preferable for its less automated approach. When using any of the previous reflection techniques, we have the option to change the naming scheme by which columns are mapped. The object includes a parameter which is a string name that determines under what name this will be present in the collection, independently of the SQL name of the column. This key is also used by as the attribute name under which the will be mapped, if not supplied through other means such as that illustrated at Alternate Attribute Names for Mapping Table Columns. When working with table reflection, we can intercept the parameters that will be used for as they are received using the event and apply whatever changes we need, including the attribute but also things like datatypes. The event hook is most easily associated with the object that’s in use as illustrated below: With the above event, the reflection of objects will be intercepted with our event that adds a new “.key” element, such as in a mapping as below: The approach also works with both the base class as well as with the Automap extension. For automap specifically, see the section Intercepting Column Definitions for background. Mapping to an Explicit Set of Primary Key Columns¶ The construct in order to successfully map a table always requires that at least one column be identified as the “primary key” for that selectable. This is so that when an ORM object is loaded or persisted, it can be placed in the identity map with an appropriate identity key. In those cases where the a reflected table to be mapped does not include a primary key constraint, as well as in the general case for mapping against arbitrary selectables where primary key columns might not be present, the parameter is provided so that any set of columns may be configured as the “primary key” for the table, as far as ORM mapping is concerned. Given the following example of an Imperative Table mapping against an existing object where the table does not have any declared primary key (as may occur in reflection scenarios), we may map such a table as in the following example: Above, the table is an association table of some kind with string columns and , but no primary key is set up; instead, there is only a establishing that the two columns represent a unique key. The does not automatically inspect unique constraints for primary keys; instead, we make use of the parameter, passing a collection of , indicating that these two columns should be used in order to construct the identity key for instances of the class. Sometimes table reflection may provide a with many columns that are not important for our needs and may be safely ignored. For such a table that has lots of columns that don’t need to be referenced in the application, the or parameters can indicate a subset of columns to be mapped, where other columns from the target will not be considered by the ORM in any way. Example: In the above example, the class will map to the table, only including the and columns - the rest are not referenced. will map the class to the table, including all columns present except , , , and . As indicated in the two examples, columns may be referenced either by string name or by referring to the object directly. Referring to the object directly may be useful for explicitness as well as to resolve ambiguities when mapping to multi-table constructs that might have repeated names: When columns are not included in a mapping, these columns will not be referenced in any SELECT statements emitted when executing or legacy objects, nor will there be any mapped attribute on the mapped class which represents the column; assigning an attribute of that name will have no effect beyond that of a normal Python attribute assignment. However, it is important to note that schema level column defaults WILL still be in effect for those objects that include them, even though they may be excluded from the ORM mapping. “Schema level column defaults” refers to the defaults described at Column INSERT/UPDATE Defaults including those configured by the , , and parameters. These constructs continue to have normal effects because in the case of and , the object is still present on the underlying , thus allowing the default functions to take place when the ORM emits an INSERT or UPDATE, and in the case of and , the relational database itself emits these defaults as a server side behavior."
    },
    {
        "link": "https://docs.sqlalchemy.org/14/orm/declarative_tables.html",
        "document": "As introduced at Declarative Mapping, the Declarative style includes the ability to generate a mapped object at the same time, or to accommodate a or other object directly.\n\nThe following examples assume a declarative base class as:\n\nAll of the examples that follow illustrate a class inheriting from the above . The decorator style introduced at Declarative Mapping using a Decorator (no declarative base) is fully supported with all the following examples as well.\n\nDeclarative mappings may also be provided with a pre-existing object, or otherwise a or other arbitrary construct (such as a or ) that is constructed separately. This is referred to as a “hybrid declarative” mapping, as the class is mapped using the declarative style for everything involving the mapper configuration, however the mapped object is produced separately and passed to the declarative process directly: # usually a good choice for MetaData but any MetaData # collection may be used. # construct the User class using this table. Above, a object is constructed using the approach described at Describing Databases with MetaData. It can then be applied directly to a class that is declaratively mapped. The and declarative class attributes are not used in this form. The above configuration is often more readable as an inline definition: A natural effect of the above style is that the attribute is itself defined within the class definition block. As such it may be immediately referred towards within subsequent attributes, such as the example below which illustrates referring to the column in a polymorphic mapper configuration: The “imperative table” form is also used when a non- construct, such as a or object, is to be mapped. An example below: For background on mapping to non- constructs see the sections Mapping a Class against Multiple Tables and Mapping a Class against Arbitrary Subqueries. The “imperative table” form is of particular use when the class itself is using an alternative form of attribute declaration, such as Python dataclasses. See the section Applying ORM Mappings to an existing dataclass for detail.\n\nThere are several patterns available which provide for producing mapped classes against a series of objects that were introspected from the database, using the reflection process described at Reflecting Database Objects. A very simple way to map a class to a table reflected from the database is to use a declarative hybrid mapping, passing the parameter to the : A variant on the above pattern that scales much better is to use the method to reflect a full set of objects at once, then refer to them from the : Automating Column Naming Schemes from Reflected Tables - further notes on using table reflection with mapped classes A major downside to the above approach is that the mapped classes cannot be declared until the tables have been reflected, which requires the database connectivity source to be present while the application classes are being declared; it’s typical that classes are declared as the modules of an application are being imported, but database connectivity isn’t available until the application starts running code so that it can consume configuration information and create an engine. There are currently two approaches to working around this. To accommodate the use case of declaring mapped classes where reflection of table metadata can occur afterwards, a simple extension called the mixin is available, which alters the declarative mapping process to be delayed until a special class-level method is called, which will perform the reflection process against a target database, and will integrate the results with the declarative table mapping process, that is, classes which use the attribute: Above, we create a mixin class that will serve as a base for classes in our declarative hierarchy that should become mapped when the method is called. The above mapping is not complete until we do so, given an : The purpose of the class is to define the scope at which classes should be reflectively mapped. The plugin will search among the subclass tree of the target against which is called and reflect all tables which are named by declared classes; tables in the target database that are not part of mappings and are not related to the target tables via foreign key constraint will not be reflected. A more automated solution to mapping against an existing database where table reflection is to be used is to use the Automap extension. This extension will generate entire mapped classes from a database schema, including relationships between classes based on observed foreign key constraints. While it includes hooks for customization, such as hooks that allow custom class naming and relationship naming schemes, automap is oriented towards an expedient zero-configuration style of working. If an application wishes to have a fully explicit model that makes use of table reflection, the Using DeferredReflection may be preferable."
    },
    {
        "link": "https://docs.sqlalchemy.org/orm/quickstart.html",
        "document": "For new users who want to quickly see what basic ORM use looks like, here’s an abbreviated form of the mappings and examples used in the SQLAlchemy Unified Tutorial. The code here is fully runnable from a clean command line.\n\nAs the descriptions in this section are intentionally very short, please proceed to the full SQLAlchemy Unified Tutorial for a much more in-depth description of each of the concepts being illustrated here.\n\nHere, we define module-level constructs that will form the structures which we will be querying from the database. This structure, known as a Declarative Mapping, defines at once both a Python object model, as well as database metadata that describes real SQL tables that exist, or will exist, in a particular database: The mapping starts with a base class, which above is called , and is created by making a simple subclass against the class. Individual mapped classes are then created by making subclasses of . A mapped class typically refers to a single particular database table, the name of which is indicated by using the class-level attribute. Next, columns that are part of the table are declared, by adding attributes that include a special typing annotation called . The name of each attribute corresponds to the column that is to be part of the database table. The datatype of each column is taken first from the Python datatype that’s associated with each annotation; for , for , etc. Nullability derives from whether or not the type modifier is used. More specific typing information may be indicated using SQLAlchemy type objects in the right side directive, such as the datatype used above in the column. The association between Python types and SQL types can be customized using the type annotation map. The directive is used for all column-based attributes that require more specific customization. Besides typing information, this directive accepts a wide variety of arguments that indicate specific details about a database column, including server defaults and constraint information, such as membership within the primary key and foreign keys. The directive accepts a superset of arguments that are accepted by the SQLAlchemy class, which is used by SQLAlchemy Core to represent database columns. All ORM mapped classes require at least one column be declared as part of the primary key, typically by using the parameter on those objects that should be part of the key. In the above example, the and columns are marked as primary key. Taken together, the combination of a string table name as well as a list of column declarations is known in SQLAlchemy as table metadata. Setting up table metadata using both Core and ORM approaches is introduced in the SQLAlchemy Unified Tutorial at Working with Database Metadata. The above mapping is an example of what’s known as Annotated Declarative Table configuration. Other variants of are available, most commonly the construct indicated above. In contrast to the column-based attributes, denotes a linkage between two ORM classes. In the above example, links to , and links to . The construct is introduced in the SQLAlchemy Unified Tutorial at Working with ORM Related Objects. Finally, the above example classes include a method, which is not required but is useful for debugging. Mapped classes can be created with methods such as generated automatically, using dataclasses. More on dataclass mapping at Declarative Dataclass Mapping.\n\nWe are now ready to insert data in the database. We accomplish this by creating instances of and classes, which have an method already as established automatically by the declarative mapping process. We then pass them to the database using an object called a Session, which makes use of the to interact with the database. The method is used here to add multiple objects at once, and the method will be used to flush any pending changes to the database and then commit the current database transaction, which is always in progress whenever the is used: It’s recommended that the be used in context manager style as above, that is, using the Python statement. The object represents active database resources so it’s good to make sure it’s closed out when a series of operations are completed. In the next section, we’ll keep a opened just for illustration purposes. Basics on creating a are at Executing with an ORM Session and more at Basics of Using a Session. Then, some varieties of basic persistence operations are introduced at Inserting Rows using the ORM Unit of Work pattern.\n\nWith some rows in the database, here’s the simplest form of emitting a SELECT statement to load some objects. To create SELECT statements, we use the function to create a new object, which we then invoke using a . The method that is often useful when querying for ORM objects is the method, which will return a object that will iterate through the ORM objects we’ve selected: The above query also made use of the method to add WHERE criteria, and also used the method that’s part of all SQLAlchemy column-like constructs to use the SQL IN operator. More detail on how to select objects and individual columns is at Selecting ORM Entities and Columns.\n\nIt’s very common to query amongst multiple tables at once, and in SQL the JOIN keyword is the primary way this happens. The construct creates joins using the method: The above query illustrates multiple WHERE criteria which are automatically chained together using AND, as well as how to use SQLAlchemy column-like objects to create “equality” comparisons, which uses the overridden Python method to produce a SQL criteria object. Some more background on the concepts above are at The WHERE clause and Explicit FROM clauses and JOINs.\n\nThe object, in conjunction with our ORM-mapped classes and , automatically track changes to the objects as they are made, which result in SQL statements that will be emitted the next time the flushes. Below, we change one email address associated with “sandy”, and also add a new email address to “patrick”, after emitting a SELECT to retrieve the row for “patrick”: Notice when we accessed , a SELECT was emitted. This is called a lazy load. Background on different ways to access related items using more or less SQL is introduced at Loader Strategies. A detailed walkthrough on ORM data manipulation starts at Data Manipulation with the ORM.\n\nAll things must come to an end, as is the case for some of our database rows - here’s a quick demonstration of two different forms of deletion, both of which are important based on the specific use case. First we will remove one of the objects from the “sandy” user. When the next flushes, this will result in the row being deleted. This behavior is something that we configured in our mapping called the delete cascade. We can get a handle to the object by primary key using , then work with the object: The last SELECT above was the lazy load operation proceeding so that the collection could be loaded, so that we could remove the member. There are other ways to go about this series of operations that won’t emit as much SQL. We can choose to emit the DELETE SQL for what’s set to be changed so far, without committing the transaction, using the method: Next, we will delete the “patrick” user entirely. For a top-level delete of an object by itself, we use the method; this method doesn’t actually perform the deletion, but sets up the object to be deleted on the next flush. The operation will also cascade to related objects based on the cascade options that we configured, in this case, onto the related objects: The method in this particular case emitted two SELECT statements, even though it didn’t emit a DELETE, which might seem surprising. This is because when the method went to inspect the object, it turns out the object was expired, which happened when we last called upon , and the SQL emitted was to re-load the rows from the new transaction. This expiration is optional, and in normal use we will often be turning it off for situations where it doesn’t apply well. To illustrate the rows being deleted, here’s the commit: The Tutorial discusses ORM deletion at Deleting ORM Objects using the Unit of Work pattern. Background on object expiration is at Expiring / Refreshing; cascades are discussed in depth at Cascades."
    },
    {
        "link": "https://medium.com/@ramanbazhanau/mastering-sqlalchemy-a-comprehensive-guide-for-python-developers-ddb3d9f2e829",
        "document": "\n• Introduction to SQLAlchemy and its role in Python ecosystem\n• Best practices for schema changes\n• Key changes from 1.x to 2.0\n• Best practices and considerations for async usage\n\nIn the ever-evolving landscape of backend development, managing and interacting with databases efficiently is crucial for building robust and scalable applications. At the heart of many modern systems lie relational databases, which have stood the test of time due to their reliability, consistency, and powerful querying capabilities. As a Python backend developer, understanding how to leverage these databases effectively can significantly enhance your ability to create high-performance applications.\n\nRelational databases have been a cornerstone of data management for decades. They organize data into tables (relations) with rows (tuples) and columns (attributes), allowing for structured storage and retrieval of information. Some key features of relational databases include:\n• Structured Data: Data is organized in a tabular format, making it easy to understand and manage.\n• SQL Support: They use Structured Query Language (SQL) for defining, manipulating, and querying data.\n• Relationships: Tables can be linked through keys, enabling complex data relationships and joins.\n\nPopular relational database management systems (RDBMS) include PostgreSQL, MySQL, Oracle, and Microsoft SQL Server, each with its own strengths and use cases.\n\nWhile relational databases are powerful, interacting with them directly using SQL can be cumbersome and error-prone, especially in large-scale applications. This is where Object-Relational Mapping (ORM) tools come into play. ORMs bridge the gap between object-oriented programming languages and relational databases, offering several advantages:\n• Abstraction: ORMs abstract away the complexities of SQL, allowing developers to work with familiar object-oriented paradigms.\n• Productivity: By eliminating the need to write raw SQL for most operations, ORMs can significantly speed up development.\n• Portability: ORMs often support multiple database backends, making it easier to switch databases without major code changes.\n• Security: Many ORMs include features to help prevent SQL injection attacks by properly parameterizing queries.\n• Maintainability: ORM-based code is often more readable and maintainable than raw SQL embedded in application code.\n\nIntroduction to SQLAlchemy and Its Role in Python Ecosystem\n\nEnter SQLAlchemy, one of the most powerful and flexible ORMs available for Python. Created by Mike Bayer in 2005, SQLAlchemy has grown to become the de facto standard for database interaction in the Python ecosystem. Here’s why SQLAlchemy stands out:\n• Flexibility: SQLAlchemy offers both high-level ORM and low-level Core functionality, allowing developers to choose the right level of abstraction for their needs.\n• Database Agnostic: It supports a wide range of database backends, from SQLite for development to PostgreSQL and MySQL for production.\n• Performance: SQLAlchemy is designed with performance in mind, offering fine-grained control over query generation and execution.\n• Ecosystem Integration: It integrates seamlessly with popular Python web frameworks like Flask and FastAPI, as well as data analysis tools like Pandas.\n• Community and Support: With a large and active community, SQLAlchemy benefits from continuous improvements, extensive documentation, and third-party extensions.\n\nAs of SQLAlchemy 2.0, the library has evolved to embrace modern Python features, including improved type hinting and async support, making it even more powerful for contemporary Python applications.\n\nIn this comprehensive guide, we’ll dive deep into SQLAlchemy, exploring both its synchronous and asynchronous capabilities. We’ll cover everything from basic CRUD operations to advanced querying techniques, performance optimization, and best practices for real-world applications. Whether you’re new to ORMs or looking to level up your SQLAlchemy skills, this article will provide you with the knowledge and insights needed to master database interactions in your Python backend projects.\n\nLet’s embark on this journey to unlock the full potential of SQLAlchemy and revolutionize how you work with databases in Python!\n\nSQLAlchemy is more than just an ORM; it’s a comprehensive suite of tools for working with relational databases in Python. To fully leverage its power, it’s essential to understand its architecture and key components.\n\nSQLAlchemy is an open-source SQL toolkit and Object-Relational Mapping (ORM) library for Python. It provides a full suite of well-known enterprise-level persistence patterns, designed for efficient and high-performing database access, adapted into a simple and Pythonic domain language.\n\nAt its core, SQLAlchemy aims to provide a flexible set of tools for manipulating relational databases, catering to both simple and complex use cases. It’s designed to work with a wide variety of database systems, allowing developers to write database-agnostic code that can be easily ported between different database backends.\n\nOne of SQLAlchemy’s unique features is its architecture, which is split into two distinct components: Core and ORM. Understanding the difference between these two is crucial for effectively using SQLAlchemy.\n• This is the foundational layer of SQLAlchemy.\n• It provides a SQL abstraction layer that allows you to work with database commands in a database-agnostic way.\n• Core includes a SQL Expression Language, which is a Pythonic way of representing common SQL statements and expressions.\n• It’s closer to SQL and gives you more control over the generated queries.\n• Core is ideal for developers who need fine-grained control over their database operations or are working on performance-critical applications.\n• Built on top of the Core, the ORM provides a high-level abstraction that allows you to work with Python objects instead of tables and SQL.\n• It maps Python classes to database tables and instances of those classes to rows in the tables.\n• The ORM automates many common database operations, making it easier to work with databases without writing SQL.\n• It’s ideal for rapid application development and when you want to work with databases in a more Pythonic way.\n\nWhile many developers primarily use the ORM, understanding Core can be beneficial for optimizing performance and handling complex queries.\n\nSQLAlchemy offers a rich set of features that make it a powerful tool for database interaction:\n• Allows easy switching between databases with minimal code changes\n• Provides a comprehensive set of tools for constructing SQL queries\n• Allows for both ORM and SQL expression language querying\n• Provides various pooling strategies to suit different application needs\n• Provides both implicit and explicit transaction control\n• Supports various types of relationships (one-to-many, many-to-many, etc.)\n• Provides powerful tools for eager and lazy loading of related objects\n• Allows for custom data types and SQL constructs\n• Provides tools for analyzing and debugging generated SQL\n\n10. Asynchronous Support (as of SQLAlchemy 1.4 and 2.0):\n• Provides async versions of core APIs for use with async/await syntax\n\nThese features make SQLAlchemy a versatile tool suitable for a wide range of applications, from simple CRUD operations to complex data analysis tasks. Its flexibility allows developers to start with high-level abstractions and gradually move to lower-level control as needed, making it an excellent choice for both beginners and experienced developers.\n\nBy understanding these core concepts and features, you’ll be well-equipped to leverage SQLAlchemy effectively in your Python projects. In the following sections, we’ll dive deeper into how to use these features in practice, starting with setting up SQLAlchemy in your development environment.\n\nGetting started with SQLAlchemy is straightforward, but there are a few key steps to ensure a smooth setup. This section will guide you through the installation process, basic configuration, and creating your first SQLAlchemy project.\n\nSQLAlchemy can be easily installed using pip, Python’s package installer. Open your terminal or command prompt and run:\n\nFor the latest version (2.0 as of this writing), you can specify:\n\nIt’s recommended to use a virtual environment to keep your project dependencies isolated. You can create one using:\n\nAdditionally, you can use for virtual environment and dependency management.\n\nAfter installation, you can verify that SQLAlchemy is correctly installed by running Python and importing it:\n\nThis should print the version number of SQLAlchemy you’ve installed.\n\nSQLAlchemy supports various database systems. Here’s how to connect to some of the most popular ones:\n\nSQLite is included with Python and doesn’t require additional setup:\n\nLet’s create a simple project to demonstrate the basic setup and usage of SQLAlchemy.\n\nWe’ll create a small application to manage a list of books.\n\nThis script does the following:\n• Sets up a session to interact with the database.\n• Adds a new book to the database.\n• Queries the database for the added book and prints it.\n\nWhen you run this script, it will create a SQLite database file named in your current directory, create a table, add a book to it, and then retrieve and print that book.\n\nThis simple example demonstrates the basic workflow of using SQLAlchemy: defining models, creating tables, establishing a session, and performing basic CRUD (Create, Read, Update, Delete) operations.\n\nAs you become more comfortable with these basics, you can explore more advanced features of SQLAlchemy, such as relationships between tables, more complex queries, and optimization techniques, which we'll cover in the upcoming sections.\n\nSQLAlchemy Core is the foundation of SQLAlchemy, providing a SQL abstraction layer that allows you to work with databases using Python constructs. It’s a powerful tool for those who need fine-grained control over their database operations or are working on performance-critical applications.\n\nThe SQL Expression Language is a system of representing relational database structures and expressions using Python constructs. It allows you to generate SQL statements programmatically, providing a more flexible and powerful alternative to writing raw SQL strings.\n\nSQLAlchemy Core provides tools for defining and manipulating database schemas:\n\nSQLAlchemy Core provides several ways to execute queries:\n\nSQLAlchemy Core provides a powerful and flexible way to interact with databases. It offers more control over the generated SQL and can be more performant for complex queries compared to the ORM. However, it requires a deeper understanding of SQL and database concepts.\n\nIn the next sections, we’ll explore SQLAlchemy ORM, which provides a higher level of abstraction, allowing you to work with Python objects instead of SQL expressions. Understanding both Core and ORM will give you the flexibility to choose the right tool for each specific task in your applications.\n\nSQLAlchemy’s ORM provides a high-level abstraction that allows you to interact with databases using Python objects. This approach can significantly simplify database operations and make your code more Pythonic.\n\nIn SQLAlchemy ORM, database tables are represented by Python classes, and instances of these classes correspond to rows in those tables.\n\nSQLAlchemy ORM supports various types of relationships between tables:\n\nThis defines a one-to-many relationship between and .\n\nCRUD stands for Create, Read, Update, and Delete. Here’s how to perform these operations using SQLAlchemy ORM:\n\nSQLAlchemy ORM provides a powerful query API that allows you to construct complex database queries using Python methods:\n\nRemember to always close your session when you’re done:\n\nThese examples demonstrate the basics of working with SQLAlchemy ORM. The ORM provides a high-level, Pythonic interface to your database, abstracting away much of the SQL complexity. However, it’s important to understand that under the hood, SQLAlchemy is translating these operations into SQL queries.\n\nIn the next sections, we’ll dive deeper into more advanced ORM concepts, query optimization techniques, and best practices for using SQLAlchemy effectively in your applications.\n\nAs you become more comfortable with SQLAlchemy ORM basics, it’s time to explore some of its more advanced features. These concepts will help you model complex relationships, implement inheritance patterns, and create more sophisticated queries.\n\nSQLAlchemy ORM supports various types of relationships between tables, allowing you to model complex data structures effectively.\n\nWe’ve seen a basic example of this earlier, but let’s dive deeper:\n\nIn this example, a can have many objects, but each belongs to only one . The function in the class creates a virtual column , while in the class, it creates a attribute.\n\nIn this example, a can be enrolled in multiple s, and each can have multiple s.\n\nSQLAlchemy supports several inheritance mapping strategies, allowing you to represent class hierarchies in your database schema.\n\nIn this strategy, all classes in the hierarchy are stored in a single table:\n\nIn this example, all , , and objects are stored in the table, with a column to distinguish between them.\n\nIn this strategy, each class in the hierarchy gets its own table:\n\nThis creates separate tables for , , and , with foreign key relationships between them.\n\nHybrid properties allow you to define attributes that can work on both the Python object and SQL expression levels.\n\nCustom types allow you to define your own data types:\n\nThese advanced concepts allow you to create more sophisticated and flexible database models with SQLAlchemy ORM. They enable you to represent complex relationships, implement inheritance patterns, and extend SQLAlchemy’s functionality to suit your specific needs.\n\nRemember, while these features are powerful, it’s important to use them judiciously. Always consider the impact on database performance and query complexity when implementing advanced ORM features.\n\nOptimizing database queries is crucial for maintaining the performance of your application, especially as it scales. SQLAlchemy provides several tools and techniques to help you write efficient queries and manage database resources effectively.\n\nSQLAlchemy uses lazy loading by default, which means related objects are loaded from the database only when they are accessed. While this can be efficient for small datasets, it can lead to the N+1 query problem for larger datasets.\n\nSQLAlchemy provides several eager loading techniques to address this:\n\nJoined loading performs a SQL JOIN to load the related objects in a single query.\n\nSubquery loading uses a separate query to load the related objects, which can be more efficient for larger datasets.\n\nSelectin loading uses a separate SELECT IN query to load related objects, which can be more efficient for collections.\n\nProper use of joins can significantly improve query performance:\n\nSQLAlchemy doesn’t provide built-in query caching, but you can implement it using libraries like :\n\nFor larger result sets, consider caching the results in your application:\n\nUnderstanding how your database executes queries can help you optimize them. Most databases provide tools to view execution plans:\n\nFor large-scale data modifications, use bulk operations instead of individual updates:\n\nEnsure your database tables are properly indexed. While SQLAlchemy doesn’t manage indexes directly, you can define them in your models:\n\nSQLAlchemy uses connection pooling by default, but you can configure it for optimal performance:\n\nUse the event to log and profile your queries:\n\nBy implementing these optimization techniques, you can significantly improve the performance of your SQLAlchemy-based applications. Remember to profile your application and focus on optimizing the queries that have the biggest impact on performance. Also, keep in mind that different techniques may be more or less effective depending on your specific use case and database system.\n\nThe SQLAlchemy Session is the central element of ORM operations. It’s the interface for all database operations and serves as a holding zone for all the objects which you’ve loaded or associated with it during its lifespan.\n\nThe Session object is your gateway to the database. It provides several key functionalities:\n• Identity Map: Maintains a unique instance of each database object within a session.\n• Unit of Work: Tracks modifications to objects and synchronizes changes with the database.\n\nA typical session lifecycle looks like this:\n\nSQLAlchemy uses a “transaction-per-session” model. Each Session object represents a single transaction or a series of transactions.\n\nWhen you call , all changes are written to the database and a new transaction begins:\n\nIf an error occurs, you can roll back the transaction:\n\nYou can use context managers to automatically handle commits and rollbacks:\n\nSQLAlchemy allows you to control the isolation level of your transactions:\n\nThe choice of isolation level depends on your specific requirements for data consistency and concurrency.\n\nFor large operations, you can use bulk inserts and updates:\n\nYou can bind a session to multiple engines:\n\nThis allows you to work with multiple databases in a single session.\n\nUnderstanding and properly managing SQLAlchemy sessions is crucial for building efficient and reliable database-driven applications. By mastering session management, you can ensure data integrity, optimize performance, and handle complex database operations with ease.\n\nDatabase migrations are a crucial part of managing database schemas in evolving applications. Alembic, created by the author of SQLAlchemy, is a lightweight database migration tool that integrates seamlessly with SQLAlchemy.\n\nAlembic provides a way to manage incremental, reversible changes to your database schema. It allows you to:\n\nLet’s dive into how to set up and use Alembic with a SQLAlchemy project.\n\nThen, initialize Alembic in your project:\n\nThis creates an directory with a configuration file ( ) and a migration environment. Modify to point to your database:\n\nThis generates a migration script in the directory. Let's look at an example:\n\nThis script defines both and functions, allowing you to apply and revert changes.\n\nTo revert the last migration:\n\nBest Practices for Schema Changes\n\nMake small, incremental changes rather than large, sweeping changes.\n\nFor large tables, use batch operations to avoid locking the entire table.\n\nAlways commit migration scripts to version control along with your application code.\n\nCreate tests for your migrations to ensure they work as expected.\n\nBy following these practices and utilizing Alembic’s features, you can effectively manage your database schema changes, ensuring smooth upgrades and the ability to rollback when necessary. This approach provides a robust way to evolve your database schema alongside your application code.\n\nSQLAlchemy 2.0 introduces significant changes and improvements over the 1.x series. This section will explore the key changes and new features, providing detailed examples of how to leverage them in your projects.\n\nKey Changes from 1.x to 2.0\n\nSQLAlchemy 2.0 aims to streamline the API, improve performance, and provide better integration with modern Python features. Some of the most significant changes include:\n• Changes to ORM querying and Session behavior\n\nLet’s explore these changes in detail.\n\nIn SQLAlchemy 2.0, the ORM query API has been unified with the Core select() construct. This means you can use the same syntax for both ORM and Core queries.\n\nExample of the new unified API:\n\nThis unification allows for more consistent query construction and execution across the ORM and Core.\n\nSQLAlchemy 2.0 introduces improved typing support, making it easier to use with static type checkers like mypy. It also introduces a new “2.0 style” of writing SQLAlchemy code that’s more amenable to static typing.\n\nExample of 2.0 style with typing:\n\nSQLAlchemy 2.0 introduces first-class support for asynchronous database operations, allowing you to use SQLAlchemy with async frameworks like FastAPI or asyncio.\n\nChanges to ORM Querying and Session Behavior\n\nSQLAlchemy 2.0 introduces changes to how ORM queries are constructed and executed, and how the Session behaves.\n\nIn 2.0, joins must be explicitly specified in most cases.\n\nThe object is replaced by in 2.0.\n\nIn 2.0, the Session is more strict about how it’s used.\n\nThese new features and changes in SQLAlchemy 2.0 provide a more consistent, type-safe, and powerful way to interact with databases. By adopting the 2.0 style, you can write more maintainable and efficient database code, while also preparing for future improvements in the SQLAlchemy ecosystem.\n\nSQLAlchemy 1.4 and 2.0 introduce robust support for asynchronous database operations, allowing developers to build high-performance, non-blocking database applications. This feature is particularly useful when working with async frameworks like FastAPI or when building applications that need to handle a large number of concurrent database operations.\n\nAsynchronous programming allows a single thread to handle multiple I/O-bound operations concurrently. In the context of database operations, this means that while waiting for a database query to complete, the application can perform other tasks, improving overall performance and responsiveness.\n\nTo use async SQLAlchemy, you need to use an async-compatible database driver. For PostgreSQL, we’ll use asyncpg.\n\nNow, let’s set up an async engine and session:\n\nDefining models for async SQLAlchemy is similar to synchronous SQLAlchemy:\n\nLet’s go through Create, Read, Update, and Delete operations using async SQLAlchemy.\n\nBest Practices and Considerations for Async Usage\n\n1. Use for session management: This ensures proper handling of async contexts.\n\n2. Be mindful of the event loop: Avoid mixing sync and async code within the same function.\n\n3. Use connection pooling: Async SQLAlchemy uses connection pooling by default, which is crucial for managing database connections efficiently.\n\n4. Handle concurrency carefully: While async allows for concurrent operations, be cautious about race conditions and data integrity.\n\n5. Profiling and monitoring: Use tools like and to manage and monitor multiple async operations.\n\n6. Error handling: Use try/except blocks to handle database errors gracefully in an async context.\n\n7. Pagination for large result sets: When dealing with large datasets, implement pagination to avoid loading too much data into memory at once.\n\nBy leveraging these async features and following best practices, you can build highly efficient and scalable database applications with SQLAlchemy. Async SQLAlchemy is particularly powerful when combined with async web frameworks, allowing you to handle a large number of concurrent database operations without blocking the event loop.\n\nTesting is a crucial part of developing reliable and maintainable database-driven applications. SQLAlchemy provides several features that make it easier to test your database code. In this section, we’ll explore how to set up a test environment, write effective unit tests, and mock database calls when necessary.\n\nWhen testing SQLAlchemy code, it’s important to use a separate test database to avoid interfering with your production or development data.\n\nSQLite is often used for testing due to its simplicity and in-memory capabilities:\n\nFor more realistic testing, you might want to use the same database type as your production environment:\n\nSQLAlchemy’s session object can be used with database transactions to automatically roll back changes after each test:\n\nIn some cases, you might want to mock database calls to isolate your tests or improve performance. Here’s how you can do this using the library:\n\nIf you’re using asynchronous SQLAlchemy, you’ll need to use async testing tools. Here’s an example using pytest-asyncio:\n• Use a separate test database: Always use a separate database for testing to avoid interfering with development or production data.\n• Leverage fixtures: Use pytest fixtures to set up and tear down your test database and sessions.\n• Test all CRUD operations: Ensure you have tests for Create, Read, Update, and Delete operations for each model.\n• Test constraints and validations: Include tests for database constraints and any custom validations you’ve implemented.\n• Use transactions for test isolation: Wrap each test in a transaction that’s rolled back after the test to ensure a clean state.\n• Test complex queries: If you have complex queries or joins, write specific tests for them to ensure they’re working correctly.\n• Mock external dependencies: When testing services that use your SQLAlchemy models, consider mocking the database calls to isolate your tests.\n• Test error conditions: Include tests for expected error conditions, such as integrity errors or validation failures.\n• Performance testing: Consider writing performance tests for critical database operations to catch any unintended performance regressions.\n• Continuous Integration: Integrate your SQLAlchemy tests into your CI/CD pipeline to catch issues early.\n\nBy following these testing practices, you can ensure that your SQLAlchemy-based application is robust, reliable, and maintainable. Effective testing catches bugs early, provides confidence when refactoring, and serves as documentation for how your database layer should behave.\n\nSQLAlchemy’s flexibility allows it to integrate seamlessly with various Python web frameworks. In this section, we’ll explore how to use SQLAlchemy with two of the most popular Python web frameworks: Flask and FastAPI.\n\nFlask is a lightweight WSGI web application framework. It’s designed to make getting started quick and easy, with the ability to scale up to complex applications.\n\nFirst, install the necessary packages:\n\nNow, let’s set up a basic Flask application with SQLAlchemy:\n\nThis example sets up a Flask application with SQLAlchemy, defines a User model, and creates two routes for adding and retrieving users.\n\nFlask-SQLAlchemy provides some useful extensions to make working with SQLAlchemy in Flask even easier:\n\nFastAPI is a modern, fast (high-performance) web framework for building APIs with Python 3.6+ based on standard Python type hints.\n\nFirst, install the necessary packages:\n\nNow, let’s set up a basic FastAPI application with SQLAlchemy:\n\nThis example sets up a FastAPI application with SQLAlchemy, defines a User model, and creates two routes for adding and retrieving users.\n\nFastAPI provides several features that work well with SQLAlchemy:\n\n2. Pydantic Integration:\n\nFastAPI uses Pydantic for data validation. You can create Pydantic models that correspond to your SQLAlchemy models:\n\n3. Async Support:\n\nFastAPI supports asynchronous operations, which can be used with SQLAlchemy’s async features:\n\nBest Practices for Integrating SQLAlchemy with Web Frameworks\n• Use dependency injection: This allows for better testing and separation of concerns.\n• Implement proper error handling: Use try-except blocks and raise appropriate HTTP exceptions.\n• Optimize queries: Use eager loading and query optimization techniques to prevent N+1 query problems.\n• Implement proper session management: Ensure that database sessions are properly opened and closed.\n• Use environment variables: Store database connection strings and other sensitive information in environment variables.\n• Implement proper validation: Use Pydantic or Flask-WTF for input validation before interacting with the database.\n• Use connection pooling: Implement connection pooling for better performance in production environments.\n\nBy following these practices and leveraging the features of your chosen web framework, you can create robust, efficient, and scalable web applications with SQLAlchemy.\n\nWhen working with SQLAlchemy, adopting certain design patterns and best practices can significantly improve your code’s maintainability, readability, and performance. In this section, we’ll explore some of the most useful patterns and practices.\n\nThe Repository pattern provides an abstraction of data, so that your application can work with a simple abstraction that has an interface approximating that of a collection.\n\nThis pattern helps to decouple your application logic from the data access layer, making it easier to change the underlying data storage without affecting the rest of your application.\n\nThe Unit of Work pattern maintains a list of objects affected by a business transaction and coordinates the writing out of changes.\n\nThis pattern ensures that all operations within a transaction are treated as a single unit, either all succeeding or all failing together.\n\nThe Data Mapper pattern is an architectural pattern that promotes a clear separation between domain objects and database tables. SQLAlchemy’s ORM implements this pattern.\n\nWhile SQLAlchemy’s declarative system is more commonly used, understanding the Data Mapper pattern can be helpful in certain complex scenarios.\n\nThe Query Object pattern encapsulates SQL queries as objects, allowing for more modular and reusable query logic.\n\nThis pattern allows for more flexible and composable queries, improving code reusability and readability.\n\nLazy Loading is a design pattern where you delay the loading of object properties until they are specifically requested. SQLAlchemy implements this pattern by default for relationships.\n\nWhile convenient, be cautious of the N+1 query problem when using lazy loading extensively.\n• Use Sessions Wisely: Always close your sessions after use, preferably using context managers or dependency injection.\n• Optimize Queries: Use , , or to prevent N+1 query problems.\n• Use Appropriate Column Types: Choose the most appropriate SQLAlchemy column types for your data to ensure data integrity and optimal performance.\n• Implement Data Validation: Use SQLAlchemy’s built-in validation or integrate with libraries like Pydantic for robust data validation.\n• Use Migrations: Implement database migrations using tools like Alembic to manage schema changes effectively.\n• Implement Proper Indexing: Use indexes on columns that are frequently used in WHERE clauses or joins to improve query performance.\n• Use Bulk Operations: For large datasets, use bulk insert and update operations to improve performance.\n• Implement Proper Error Handling: Use try-except blocks and implement proper rollback mechanisms in case of errors.\n• Use Connection Pooling: In production environments, implement connection pooling to manage database connections efficiently.\n• Keep Your Models Separate: Maintain a clear separation between your database models and your API/serialization models.\n\nBy adopting these design patterns and best practices, you can create more maintainable, efficient, and scalable applications with SQLAlchemy. Remember that while these patterns can be very useful, it’s important to apply them judiciously based on your specific use case and requirements.\n\nOptimizing database performance is crucial for maintaining responsive and efficient applications. SQLAlchemy provides various tools and techniques to monitor and enhance the performance of your database operations. In this section, we’ll explore methods for profiling SQLAlchemy queries, identifying and resolving common performance issues, and managing database connections effectively.\n\nProfiling your SQLAlchemy queries is the first step in identifying performance bottlenecks. Here are some techniques to profile your queries:\n\nSQLAlchemy’s echo option allows you to see the SQL statements being executed:\n\nThis will print all SQL statements to the console, allowing you to see what’s being executed and how long each query takes.\n\nYou can create a custom event listener to log query execution times:\n\nSQLAlchemy offers a profiling extension that provides detailed statistics about query execution:\n\nThe N+1 query problem is a common performance issue where an application executes N additional queries to fetch related objects for each of the N results of an initial query.\n\nLook for patterns in your query logs where the same type of query is executed multiple times in a loop. This is often a sign of an N+1 problem.\n\nProper connection management is crucial for application performance and scalability.\n\nSQLAlchemy uses connection pooling by default, but you can configure it:\n• : The number of connections to keep open inside the connection pool\n• : The maximum number of connections to allow in the pool \"overflow\"\n• : The number of seconds to wait before giving up on getting a connection from the pool\n• : This setting causes the pool to recycle connections after the given number of seconds\n\n1. Proper Session Management: Always close your sessions after use, preferably using context managers:\n\n2. Use Pessimistic Connection Handling: For web applications, it’s often better to return connections to the pool as quickly as possible:\n\n1. Use Specific Queries: Instead of querying for all columns, select only the ones you need:\n\n2. Utilize Indexing: Ensure that columns frequently used in WHERE clauses or JOINs are properly indexed:\n\n3. Bulk Operations: For large datasets, use bulk insert and update operations:\n\n4. Use Hybrid Properties: For complex computations that can be done both in Python and SQL:\n• Database-Specific Tools: Most databases have their own monitoring tools (e.g., pg_stat_statements for PostgreSQL).\n• APM Tools: Application Performance Monitoring tools like New Relic or Datadog can provide insights into database performance.\n• Custom Metrics: Implement custom metrics to track important performance indicators specific to your application.\n\nBy implementing these performance monitoring and optimization techniques, you can significantly improve the efficiency and scalability of your SQLAlchemy-based applications. Remember to profile your application regularly and focus on optimizing the queries and operations that have the biggest impact on performance.\n\nWhen working with databases, security is paramount. SQLAlchemy provides several features to help developers write secure code, but it’s crucial to understand and implement these correctly. This section will cover preventing SQL injection, managing sensitive data, and implementing authentication and authorization with SQLAlchemy.\n\nSQL injection is one of the most common and dangerous security vulnerabilities in database applications. SQLAlchemy’s ORM and Core expression language provide built-in protection against SQL injection, but it’s important to use them correctly.\n\nAlways use parameterized queries instead of string concatenation:\n\nWhen using for raw SQL, always use parameter binding:\n\nAvoid dynamically generating SQL strings based on user input. If you must, use SQLAlchemy’s tools:\n\nProtecting sensitive data is crucial for maintaining user trust and complying with data protection regulations.\n\nFor sensitive data that needs to be searchable, consider using SQLAlchemy with an encryption extension:\n\nNever store passwords in plain text. Use a strong hashing algorithm like bcrypt:\n\nEnsure that sensitive data is not logged:\n\nWhile SQLAlchemy itself doesn’t provide authentication and authorization mechanisms, it can be used effectively in conjunction with other libraries to implement these security features.\n\nHere’s a basic example of how you might implement user authentication:\n\nFor more granular control, you can implement row-level security:\n\n1. Keep SQLAlchemy and Dependencies Updated: Regularly update SQLAlchemy and its dependencies to benefit from the latest security patches.\n\n2. Use HTTPS: Always use HTTPS to encrypt data in transit, especially when dealing with sensitive information.\n\n4. Use Connection Pooling Safely: Ensure that connection pools are configured with appropriate timeouts and size limits.\n\n5. Implement Rate Limiting: Protect against brute-force attacks by implementing rate limiting on authentication attempts.\n\n7. Use Principle of Least Privilege: Ensure database users have only the permissions they need.\n\n8. Sanitize Input: While SQLAlchemy provides protection against SQL injection, always sanitize and validate user input.\n\n9. Secure Configuration Management: Store sensitive configuration data (like database credentials) in secure, environment-specific configuration files or environment variables.\n\nBy implementing these security measures and best practices, you can significantly enhance the security of your SQLAlchemy-based applications. Remember that security is an ongoing process, and it’s important to stay informed about new security threats and best practices in database security.\n\nAs your application grows, you may need to scale your database operations to handle increased traffic and data volume. SQLAlchemy provides several features and can be used with various strategies to achieve scalability. This section will cover horizontal and vertical scaling strategies, sharding, read replicas, and caching layers.\n\nVertical scaling involves increasing the resources (CPU, RAM, storage) of your database server. While SQLAlchemy doesn’t directly influence vertical scaling, it can be optimized to make better use of increased resources.\n\n1. Connection Pooling Optimization:\n\n Adjust your connection pool settings based on your server’s capabilities:\n\n2. Utilize More Concurrent Connections:\n\n With more resources, you can handle more concurrent database connections. Adjust your application server settings accordingly.\n\nHorizontal scaling involves adding more database servers to distribute the load. SQLAlchemy can be configured to work with multiple databases.\n\n1. Read/Write Splitting:\n\n Use separate engines for read and write operations:\n\nSharding involves partitioning your data across multiple databases. While SQLAlchemy doesn’t provide built-in sharding support, you can implement a sharding strategy on top of SQLAlchemy.\n\n2. Querying Across Shards:\n\n For queries that need to span multiple shards, you’ll need to implement custom logic:\n\nRead replicas can significantly improve the read performance of your application. SQLAlchemy can be configured to use read replicas for select queries.\n\nImplementing a caching layer can significantly reduce database load. SQLAlchemy can be used in conjunction with caching solutions like Redis or Memcached.\n• Optimize Queries: Regularly review and optimize your SQLAlchemy queries to ensure they’re as efficient as possible.\n• Use Appropriate Indexing: Ensure your database tables are properly indexed based on your query patterns.\n• Utilize Bulk Operations: For large datasets, use SQLAlchemy’s bulk insert and update operations.\n• Consider Asynchronous Operations: For I/O-bound applications, consider using SQLAlchemy’s asynchronous features.\n• Implement Proper Error Handling: Ensure your application can handle database errors gracefully, including connection issues in a distributed setup.\n\nBy implementing these scaling strategies and best practices, you can significantly improve the performance and scalability of your SQLAlchemy-based applications. Remember that scaling is an ongoing process, and it’s important to continually monitor and adjust your strategies as your application grows and evolves.\n\nUnderstanding how SQLAlchemy is used in real-world applications can provide valuable insights into best practices and effective implementation strategies. In this section, we’ll explore three different case studies: building a RESTful API, implementing a simple blog engine, and creating a data analysis application.\n\nLet’s create a RESTful API for a library management system using Flask and SQLAlchemy.\n\nThis example demonstrates how to create a basic RESTful API using Flask and SQLAlchemy. It includes endpoints for retrieving all books, adding a new book, and getting a specific book by ID.\n\nLet’s create a simple blog engine using SQLAlchemy with features like user authentication, post creation, and comments.\n\nThis example demonstrates a basic blog engine with user authentication, post creation, and commenting functionality using SQLAlchemy.\n\nLet’s create a data analysis application that tracks stock prices and provides basic analytics.\n\nThis example demonstrates how SQLAlchemy can be used in a data analysis application. It includes functions for adding stock price data, calculating average prices, determining price changes over time, and finding the day with the highest trading volume.\n\nThese real-world examples showcase the versatility of SQLAlchemy in different application contexts. From building APIs to creating content management systems and performing data analysis, SQLAlchemy provides a powerful and flexible foundation for database operations in Python applications.\n\nEven with careful planning and implementation, developers often encounter issues when working with SQLAlchemy. This section will guide you through common problems and their solutions, helping you to efficiently debug and resolve issues in your SQLAlchemy-based applications.\n\nEffective query debugging is crucial for optimizing performance and resolving issues in SQLAlchemy applications.\n\nTo see the actual SQL queries being executed:\n\nWhen creating your engine, you can enable SQL echoing:\n\nTo see the SQL for a query without executing it:\n\nFor databases like PostgreSQL, you can use EXPLAIN to see the query execution plan:\n\nConnection errors are common, especially in distributed systems or applications with high concurrency.\n\nOften caused by connection issues or syntax errors:\n\nOccurs when accessing an unloaded attribute on a detached instance:\n\n1. Use Proper Exception Handling: Always wrap database operations in try-except blocks to catch and handle specific exceptions.\n\n2. Implement Logging: Use Python’s logging module to log exceptions and important events.\n\n3. Check Database Logs: Often, the database’s own logs can provide valuable information about errors.\n\n4. Use Database-Specific Tools: Utilize tools like pgAdmin for PostgreSQL or MySQL Workbench for MySQL to directly inspect the database.\n\n5. Verify Data Types: Ensure that the data types in your SQLAlchemy models match those in your database schema.\n\n6. Keep SQLAlchemy Updated: Regularly update SQLAlchemy to benefit from bug fixes and performance improvements.\n\n7. Use Session Wisely: Always close sessions after use, preferably using context managers.\n\n8. Profile Your Queries: Use tools like for query profiling.\n\nBy following these troubleshooting techniques and best practices, you can more effectively diagnose and resolve issues in your SQLAlchemy applications. Remember that many problems can be prevented through careful design and adherence to SQLAlchemy’s recommended usage patterns.\n\nAs database technologies and Python itself evolve, SQLAlchemy continues to adapt and improve. This section explores the future direction of SQLAlchemy, emerging trends in ORM development, and when to consider alternatives.\n\nSQLAlchemy is continuously evolving. Here are some key areas of development and improvement:\n\nSQLAlchemy 1.4 and 2.0 introduced async support, and this is expected to be further refined and expanded.\n\nFuture versions will likely include more comprehensive type hinting support, enhancing IDE integration and static type checking.\n\nOngoing efforts to improve query generation, caching mechanisms, and overall performance.\n\nMore powerful querying capabilities, improved relationship handling, and better support for complex data models.\n\nIncreased use of Python’s latest features, such as dataclasses and structural pattern matching.\n\nORMs are moving towards more expressive, graph-like query languages that can handle complex relationships more intuitively.\n\nBetter integration with data science and machine learning workflows.\n\nAdapting ORMs to work efficiently in serverless environments and at the edge.\n\nEnhanced support for real-time data updates and synchronization across distributed systems.\n• Project Size and Complexity: SQLAlchemy is well-suited for large, complex projects, while lighter ORMs might be better for smaller applications.\n• Performance Requirements: If you need maximum performance, consider using a lighter ORM or raw SQL for critical parts of your application.\n• Team Expertise: Choose an ORM that aligns with your team’s skills and the learning curve they can manage.\n• Framework Integration: If you’re using a specific web framework, consider an ORM that integrates well with it.\n• Database Support: Ensure the ORM supports all the databases and features you need.\n• Scalability: Consider how well the ORM can scale with your application’s growth.\n\nSQLAlchemy continues to be a leading ORM in the Python ecosystem, with ongoing development ensuring its relevance for future applications. However, the landscape of data persistence is evolving, with new paradigms and requirements emerging.\n\nAs a developer, it’s crucial to stay informed about these trends and alternatives. While SQLAlchemy remains an excellent choice for many applications, being aware of other options allows you to make informed decisions based on your specific project requirements.\n\nThe future of ORMs is likely to see further integration with modern development practices, improved performance, and more intuitive ways of working with complex data relationships. Regardless of the tool you choose, understanding these trends will help you build more efficient and maintainable database-driven applications.\n\nThroughout this comprehensive guide, we’ve explored the depths of SQLAlchemy, from its basic concepts to advanced features and real-world applications. As we conclude, let’s recap the key points, provide resources for continued learning, and encourage you to apply this knowledge in your projects.\n• SQLAlchemy’s Dual Nature: We’ve seen how SQLAlchemy offers both a high-level ORM and a lower-level Core, providing flexibility for various use cases.\n• Database Abstraction: SQLAlchemy’s ability to work with multiple database backends allows for portable code and easier database migrations.\n• ORM Capabilities: We’ve explored how to define models, relationships, and perform CRUD operations using the ORM.\n• Query Optimization: Techniques like eager loading and query optimization are crucial for building efficient database-driven applications.\n• Session Management: Proper session handling is vital for maintaining data integrity and managing database connections effectively.\n• Migrations: Alembic, SQLAlchemy’s migration tool, allows for version-controlled database schema changes.\n• Security Considerations: We’ve covered best practices for preventing SQL injection and managing sensitive data.\n• Scalability: Strategies for scaling SQLAlchemy applications, including sharding and read replicas, were discussed.\n• Integration with Web Frameworks: We’ve seen how SQLAlchemy can be integrated with popular frameworks like Flask and FastAPI.\n\nTo continue your journey with SQLAlchemy, consider the following resources:\n• The SQLAlchemy documentation is comprehensive and regularly updated.\n• Coursera and Udemy offer courses on SQLAlchemy and database programming in Python.\n• The SQLAlchemy mailing list is active and a great place for discussions.\n• Explore the SQLAlchemy GitHub repository for the latest developments and to contribute.\n\nNow that you’ve gained a comprehensive understanding of SQLAlchemy, it’s time to put this knowledge into practice:\n• Start Small: Begin by integrating SQLAlchemy into a small project or refactoring an existing one.\n• Experiment: Try out different features, especially those you haven’t used before, like hybrid properties or custom types.\n• Optimize Existing Code: Review your current database code and look for opportunities to apply optimization techniques you’ve learned.\n• Contribute to Open Source: Consider contributing to SQLAlchemy itself or to projects that use it. This can be through code, documentation, or helping others in the community.\n• Stay Updated: Keep an eye on SQLAlchemy’s development. New versions often bring performance improvements and new features.\n• Share Your Knowledge: Write blog posts, give presentations, or mentor others. Teaching is an excellent way to solidify your understanding.\n• Build Real-World Projects: Apply SQLAlchemy in diverse projects to gain practical experience with different scenarios and challenges.\n• Performance Tuning: Use the profiling techniques you’ve learned to optimize the performance of your database operations.\n• Explore Advanced Features: Dive deeper into advanced features like custom types, event listeners, or dialect-specific optimizations.\n• Combine with Other Technologies: Explore how SQLAlchemy works with other technologies in your stack, like caching solutions or message queues.\n\nSQLAlchemy is a powerful tool that can significantly enhance your ability to work with databases in Python. Its flexibility and depth allow it to adapt to a wide range of use cases, from simple CRUD applications to complex, high-performance systems.\n\nRemember that mastering SQLAlchemy is a journey. Each project you work on will likely teach you something new about the ORM or about database design in general. Embrace these learning opportunities and don’t hesitate to dive into the documentation or ask the community when you encounter challenges.\n\nAs you continue to work with SQLAlchemy, you’ll develop a deeper appreciation for its capabilities and the elegant solutions it provides to complex database problems. Your growing expertise will not only make you a more effective Python developer but will also give you valuable insights into database management and software architecture.\n\nThank you for embarking on this comprehensive journey through SQLAlchemy. May your future projects benefit from the power and flexibility of this outstanding ORM!"
    },
    {
        "link": "https://stackoverflow.com/questions/973481/dynamic-table-creation-and-orm-mapping-in-sqlalchemy",
        "document": "If you are looking to create dynamic classes and tables you can use the following technique based from this tutorial URL I found here (http://sparrigan.github.io/sql/sqla/2016/01/03/dynamic-tables.html), I modified how he did it a bit.\n\nFirst include all the needed dependencies and create your session and Base.\n\nThe key to creating it dynamically is this here:\n\nyou could create a table from just this by taking advantage of the 'type' function in python.\n\nNote that we are passing in attr_dict, this will give the required tablename and column information to our class, but the difference is we are defining the class name through a string! This means you could create a loop for example going through an array of strings to start creating tables dynamically!\n\nNext all you have to do is simply call\n\nBecause the dynamic class we created inherits from Base the command will simply create the tables!\n\nYou add to this table for example like this now:\n\nThis can go even further if you you don't know the column names as well. Just refer to the article to learn how to do that.\n\nYou would essentially do something like this though:\n\nThe ** operator takes the object and unpacks it so that firstColName and secondColName are added with assignment operators so it would essentially be the same thing as this:\n\nThe advantage of this technique is now you can dynamically add to the table without even having to define the column names!\n\nThese column names could be stored in a string array for example or whatever you wanted and you just take it from there."
    }
]