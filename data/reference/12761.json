[
    {
        "link": "https://stackoverflow.com/questions/40802656/persisting-a-json-object-using-hibernate-and-jpa",
        "document": "I am trying to store a JSON object in MySQL database in spring boot. I know I am doing something wrong but I a can't figure out what it is because I am fairly new to Spring.\n\nI have a rest endpoint where I get the following JSON object (via HTTP PUT) and I need to store it in database so that the user can fetch it later (via HTTP GET).\n\nNote that in the above case The number of keys in the object may vary, Due to that requirement I am using a to catch the object in the controller.\n\nAs you can see in the method, I can iterate the and persist each object in db. But I am looking for a way to persist the entire in a single record. I have did some reading and they suggest me to use a mapping.\n\nCan anyone point me in a direction to persist the in a different way? (or is using the the only and right way to do this?)"
    },
    {
        "link": "https://baeldung.com/hibernate-persist-json-object",
        "document": "Some projects may require JSON objects to be persisted in a relational database.\n\nIn this tutorial, we’ll see how to take a JSON object and persist it in a relational database.\n\nThere are several frameworks available that provide this functionality, but we will look at a few simple, generic options using Hibernate, Jackson, and the Hypersistence Utils library.\n\nWe’ll use the basic Hibernate Core dependency for this tutorial:\n\nWe’ll also be using Jackson as our JSON library:\n\nNote that these techniques are not limited to these two libraries. We can substitute our favorite JPA provider and JSON library.\n\nThe most basic way to persist a JSON object in a relational database is to convert the object into a String before persisting it. Then, we convert it back into an object when we retrieve it from the database.\n\nWe can do this in a few different ways.\n\nThe first one we’ll look at is using custom serialize and deserialize methods.\n\nWe’ll start with a simple Customer entity that stores the customer’s first and last name, as well as some attributes about that customer.\n\nA standard JSON object would represent those attributes as a HashMap, so that’s what we’ll use here:\n\nRather than saving the attributes in a separate table, we are going to store them as JSON in a column in the Customers table. This can help reduce schema complexity and improve the performance of queries.\n\nFirst, we’ll create a serialize method that will take our customerAttributes and convert it to a JSON string:\n\nWe can call this method manually before persisting, or we can call it from the setCustomerAttributes method so that each time the attributes are updated, the JSON string is also updated.\n\nNext, we’ll create a method to deserialize the JSON string back into a HashMap object when we retrieve the Customer from the database. We can do that by passing a TypeReference parameter to readValue():\n\nOnce again, there are a few different places that we can call this method from, but, in this example, we’ll call it manually.\n\nSo, persisting and retrieving our Customer object would look something like this:\n\nIf we are using JPA 2.1 or higher, we can make use of AttributeConverters to streamline this process.\n\nFirst, we’ll create an implementation of AttributeConverter. We’ll reuse our code from earlier:\n\nNext, we tell Hibernate to use our new AttributeConverter for the customerAttributes field, and we’re done:\n\nWith this approach, we no longer have to manually call the serialize and deserialize methods since Hibernate will take care of that for us. We can simply save and retrieve the Customer object normally.\n\nThe Hypersistence library allows easy mapping of JSON to database systems like H2, MySQL, Oracle Database, etc. This is achieved by defining and using custom types in an entity class.\n\nTo use the library, we need to add the hypersistence-utils-hibernate-55 dependency to the pom.xml:\n\nThis is compatible with Hibernate 5.5 and 5.6. For Hibernate 6.0 and 6.1, we need to use the hpersisitence-utils-hibernate-60 dependency instead. For Hibernate 6.2 and above, the hypersistence-utils-hibernate-62 dependency and the hypersistence-utils-hibernate-63 dependency are required respectively.\n\nIn the code above, we use the @TypeDef annotation to define a type name “json“, which is mapped to the JsonType.class from the Hypersistence Utils library. This mapping allows us to serialize and deserialize JSON in the database. We reference this type whenever we need to apply it as JSON.\n\nAlternatively, we can specify the fully qualified name of the type in Hibernate 5:\n\nIn this case, the @TypeDef annotation isn’t required because the custom type is explicitly defined.\n\nHowever, starting with Hibernate 6, @TypeDef is deprecated. We can apply the custom type directly by specifying it within the @Type annotation:\n\nThe JsonType.class is responsible for mapping JSON to the database.\n\nHibernate 6.0 and later versions provide an annotation named @JdbcTypeCode to help specify the JDBC type for column mapping.\n\nWe can serialize or deserialize a Map object as JSON by setting the SQL type to JSON using SQLTypes.JSON as the parameter of the @JdbcTypeCode annotation. This approach provides the flexibility to store structured data without the need for additional mapping configuration.\n\nLet’s define an entity class named Store and map the attributes object as JSON:\n\nIn the code above, we define the SQL type of the attributes object by explicitly using @JdbcTypeCode annotation and setting the SQL type to JSON. This allows the attributes object to be serialized and deserialized as JSON without the need for an external library or writing a custom conversion.\n\nAlso, starting from Hibernate 6.0, we can map a JSON document to an embeddable class. A class with @Embeddable annotation can be included in another class as a value type and be persisted into the database as part of the containing class.\n\nFurthermore, the @Embedded annotation is used in the containing class to indicate it contains an embedded object. To map an embedded class to a JSON object, we have to annotate it with @JdbcTypeCode annotation and set the annotation parameter to SQLTypes.JSON.\n\nNotably, this feature is only available for Oracle DB and PostgreSQL as other databases are not supported yet.\n\nNext, let’s embed the class into our entity class and map it as JSON:\n\nIn the code above, we annotate the Specification type with @Embedded to indicate it’s an embeddable class and we map the column to a JSON document by explicitly specifying the JDBC type code.\n\nIn this article, we’ve seen several examples of how to persist JSON objects using Hibernate and Jackson.\n\nIn the first example, we used a simple, compatible approach using custom serialize and deserialize methods. Then, in the second example, we introduced AttributeConverters as a powerful way to simplify our code. Also, we looked at a pre-defined type from the Hypersistence Utils library to simplify mapping JSON to a database column.\n\nFinally, we saw how to use the @JdbcTypeCode annotation introduced in Hibernate 6 to simplify mapping JSON to a database column without custom configuration or using an external library."
    },
    {
        "link": "https://medium.com/@bubu.tripathy/best-practices-entity-class-design-with-jpa-and-spring-boot-6f703339ab3d",
        "document": "In the world of modern software development, efficient design and implementation of entity classes play a crucial role in building robust and maintainable applications. JPA, coupled with the power of Spring Boot, empowers developers to streamline database operations and create highly functional applications. This guide delves into essential best practices for designing entity classes while utilizing JPA within a Spring Boot framework. By adhering to these best practices, developers can ensure the integrity, performance, and scalability of their applications.\n• Annotate your entity classes with to indicate they are JPA entities.\n• Use to specify the table name if it's different from the class name.\n• Consider using for common attributes that should be inherited by multiple entities.\n\nIn this example, the class is annotated with . It contains the common fields that you want to share across multiple entity classes. The class inherits from , effectively inheriting the field from the superclass.\n\nBy using , you're able to create a common base class for your entity hierarchy while allowing each subclass to include additional fields and annotations specific to their needs. This promotes code reusability and maintains a clean and structured entity hierarchy.\n• Use with appropriate strategy for generating primary key values (e.g., , ).\n• Use , , , and to define relationships between entities.\n• Use attribute to control loading behavior (e.g., or ).\n• Utilize to define the owning side of bidirectional relationships.\n• Use attribute to specify cascading operations (e.g., , ).\n• Be cautious with cascading to avoid unintentional data loss.\n\nIn this example, we have two entities: and . The entity has a one-to-many relationship with the entity.\n• CascadeType.ALL: This option specifies that all operations (e.g., persist, merge, remove) should be cascaded from the parent entity ( ) to the child entity ( ).\n• orphanRemoval = true: This option specifies that when an entity's reference to a entity is removed from the collection, the orphaned entity should also be removed from the database.\n\nWhen you perform a cascading operation on the entity, the corresponding operation will cascade to the associated entities. For instance:\n\nLikewise, cascading operations work for merge, remove, and other entity operations, reducing the need for explicitly managing related entities persistence.\n• Use validation annotations ( , , etc.) to enforce data integrity constraints directly in the entity class.\n• Implement entity auditing by adding fields like , , , and for tracking who created or modified an entity and when.\n\nIn this example, we’re creating an abstract class that serves as the base class for other entities that require auditing. Let's break down the annotations and their purposes:\n• @EntityListeners(AuditingEntityListener.class): This annotation specifies that this entity should be audited using the provided entity listener class. Spring Data JPA will automatically update the auditing fields before persisting or updating the entity.\n• @MappedSuperclass: This annotation indicates that this class is not an entity itself but serves as a base class for other entities. It allows attributes and behaviors to be inherited by other entities.\n• @CreatedBy: This annotation specifies the field to store the username of the user who created the entity.\n• @CreatedDate: This annotation marks the field to store the timestamp when the entity was created. The and properties are set to to ensure that this field is populated during creation and not updated afterwards.\n• @LastModifiedBy: This annotation specifies the field to store the username of the user who last modified the entity.\n• @LastModifiedDate: This annotation marks the field to store the timestamp when the entity was last modified.\n\nNow, when you create an entity that extends the class, Spring Data JPA will automatically populate the auditing fields during the relevant operations:\n\nBy implementing auditing, you can track who created or modified entities and when those actions occurred. This information can be invaluable for monitoring and maintaining your application’s data.\n• Use Java enums for fields with predefined values.\n• Annotate enum fields with to store enum values as strings in the database.\n• When retrieving data from the database, consider using DTO projections to fetch only the necessary fields, improving performance.\n• Use Spring Data JPA’s annotation or query methods to create custom projections.\n\nLet’s consider an example where we have an entity and we want to project a subset of its data into a DTO called .\n\nIn this example:\n• We have an entity with a one-to-many relationship to entities (not shown here).\n• We define a Spring Data JPA repository that extends . Within this repository, we declare a custom query method using the annotation.\n• The custom query retrieves a list of objects. The query projects a subset of data: the author's name and the count of books they have written.\n• is an interface that defines the subset of data we want to project from the entity. The getter methods in this interface correspond to the projected fields.\n\nWith this setup, when you call the method from the , it will execute the custom query and return a list of objects containing the projected data.\n• Define indexes on fields that are commonly queried for better database performance.\n• Use or annotations to specify indexes on columns or collections.\n\nIn this example, we have two entities: and . We'll define indexes on fields within the entity using the annotation from the package.\n• @Indexes: This annotation allows you to define one or more indexes on columns of a table. The annotation is used within the annotation to specify the columns that should be indexed.\n• @Index: This annotation specifies a single index on the given column(s). You can use the attribute to specify one or more columns for indexing.\n\nIn the entity, we're defining two indexes:\n• An index on the column to optimize queries that involve joining or filtering by category.\n• A composite index on the and columns to optimize queries that sort or filter products based on name and price together.\n\nIncorporating robust entity class design, validation, auditing, DTO projection, and index optimization within a Spring Boot application using JPA not only ensures efficient data management but also contributes to the foundation of a reliable and maintainable software system."
    },
    {
        "link": "https://stackoverflow.com/questions/51276703/how-to-store-postgresql-jsonb-using-springboot-jpa",
        "document": "I'm working on a migration software that will consume unknown data from REST services.\n\nI already think about use MongoDB but I decide to not use it and use PostgreSQL.\n\nAfter read this I'm trying to implement it in my SpringBoot app using Spring JPA but I don't know to map in my entity.\n\nTried this but understood nothing!\n\nHere is where I am:\n\nHow can I do this?\n\nNote: I don't want/need an Entity to work on. My JSON will always be String but I need jsonb to query the DB"
    },
    {
        "link": "https://baeldung.com/spring-boot-jpa-storing-postgresql-jsonb",
        "document": "This tutorial provides a comprehensive understanding of storing JSON data in a PostgreSQL JSONB column.\n\nUsing JPA, we’ll quickly review how we can handle a JSON value stored as a variable character (VARCHAR) database column. After that, we’ll compare the differences between the VARCHAR type and the JSONB type to understand the additional features of JSONB. Finally, we’ll address the mapping JSONB type in JPA.\n\nIn this section, we’ll explore converting a JSON value in VARCHAR type to a custom Java POJO. For this purpose, we’ll useAttributeConverter to easily convert from an entity attribute value in Java data type to its corresponding value in the database column.\n\nTo create an AttributeConverter, we have to include the latest Spring Data JPA dependency in the pom.xml:\n\nLet’s illustrate this concept with a simple example using the following database table definition:\n\nThe student table has three fields, and we’re expecting the address column to store JSON values with the following structure:\n\nTo handle this, we’ll create a corresponding POJO class to represent the address data in Java:\n\nNext, we’ll create an entity class, StudentEntity, and map it to the student table we created earlier:\n\nWe’ll annotate the address field with @Convert and apply AddressAttributeConverter to convert the Address instance into its JSON representation.\n\nEarlier, we mapped the address field in the entity class as a VARCHAR type in the database. However, JPA cannot automatically convert the custom Java type and the VARCHAR type. So, AttributeConverter comes in to bridge this gap by providing a mechanism to handle the conversion process.\n\nWe use AttributeConverter to persist a custom Java data type to a database column. It’s mandatory to define two conversion methods for every AttributeConverter implementation. One, named convertToDatabaseColumn(), converts the Java data type to its corresponding database data type, while the other, named convertToEntityAttribute(), converts the database data type to the Java data type:\n\nNow, we can verify that a Student row is persisted correctly along with its Address:\n\nMoreover, we can check the logs to see what JPA is doing when we’re inserting new data:\n\nWe can see that the first parameter has been converted successfully from our Address instance by the AddressAttributeConverter and binds as a VARCHAR type.\n\nWe learned how to persist JSON data by converting the JSON data to VARCHAR and vice-versa. Next, let’s check other solutions to handle JSON data.\n\nIn PostgreSQL, we can set the type of a column as JSONB to save JSON data:\n\nHere, we’re defining the address column as JSONB. This is a different data type than the previously used VARCHAR type, and it is important to learn why we have this data type in PostgreSQL. JSONB is a designated data type for processing JSON data in PostgreSQL.\n\nMoreover, columns using JSONB type store data in a decomposed binary format, which has a bit of overhead when storing JSON due to the additional conversion.\n\nAlso, JSONB provides additional features compared to VARCHAR. Therefore, JSONB is a more favorable choice for storing JSON data in PostgreSQL.\n\nThe JSONB type enforces data validation to make sure the column value is a valid JSON. So, attempting to insert or update a column of type JSONB with invalid JSON values fails.\n\nTo demonstrate this, we can try to insert a SQL query with an invalid JSON value for the address column where, for example, a double quote is missing at the end of the city attribute:\n\nRunning this query in PostgreSQL results throws a validation error indicating the JSON stating that we have an invalid JSON:\n\nPostgreSQL supports querying using JSON columns in SQL queries. JPA supports using native queries to search for records in the database. In Spring Data, we can define a custom query method that finds a list of Student:\n\nThis query is a native SQL query that selects all Student instances in the database where the address JSON attribute postCode equals the provided parameter.\n\nJSONB supports JSON data indexing. This gives JSONB a significant advantage when we have to query the data by keys or attributes in the JSON column.\n\nVarious types of indexes can be applied to a JSON column, including GIN, HASH, and BTREE. GIN is suitable for indexing complex data structures, including arrays and JSON. HASH is important when we only need to consider the equality operator =. BTREE allows efficient queries when we deal with range operators such as < and >=.\n\nFor example, we could create the following index if we always need to retrieve data according to the postCode attribute in the address column:\n\nWe cannot apply the same AttributeConverter when the databases column is defined as JSONB. Otherwise, our application throws the following error upon start-up if we attempt to:\n\nEven if we change the AttributeConverter class definition to use Object as the converted column value instead of String we’ll still get an error:\n\nOur application complains about the unsupported type:\n\nTherefore, we can confidently say that JPA doesn’t support JSONB type natively. However, our underlying JPA implementation, Hibernate, does support JSON custom types that allow us to map a complex type to a Java class.\n\nIn short, we need a custom type for JSONB conversion. Fortunately, we can rely on an existing library named Hypersistence Utilities.\n\nHypersistence Utilities is a general-purpose utility library for Hibernate. One of its features is the definition of JSON column type mapping for different databases such as PostgreSQL and Oracle. Thus, we can include this additional dependency in the pom.xml:\n\nHypersistence Utilities defines different custom types that are database-dependent. In PostgreSQL, we’ll use the JsonBinaryType class for the JSONB column type. In our entity class, we define the custom type using Hibernate’s @TypeDef annotation and then apply the defined type to the address field via @Type:\n\nFor this case of using @Type, we don’t need to apply the AttributeConverter to the address field anymore. The custom type from Hypersistence Utilities handles the conversion task for us, making our code more neat.\n\nNote: Keep in mind that @Type annotation is deprecated in Hibernate 6.\n\nAfter all these changes, let’s re-run the Student persistence test case again:\n\nWe’ll see that JPA triggers the same insert SQL as before, except the first parameter is binding as OTHER instead of VARCHAR. This indicates that Hibernate binds the parameter as a JSONB type this time.\n\nIn this tutorial, we learned how to manage JSON data in PostgreSQL using Spring Boot and JPA. First, we addressed the mapping of JSON value to VARCHAR type and JSONB type using a custom converter. Then, we learned about the importance of using JSONB to enforce JSON validation and query and index JSON values easily. Finally, we implemented a custom type conversion of JSONB columns using the Hypersistence library.\n\nAs always, the sample code is available on GitHub."
    },
    {
        "link": "https://medium.com/mongodb/how-to-efficiently-get-json-from-mongodb-using-spring-data-and-java-1e083f620a44",
        "document": "Or why AI may be costing your business millions\n\nLike many developers these days, if I’m not intimately familiar with how to write a piece of code to the point it’s mere muscle memory, I reach for an AI to help. Today, I realized that this practice might cost some companies millions without their realizing it.\n\nI’m a distinguished engineer at MongoDB with 11 years of tenure. I’m a language polyglot and intimately familiar with using MongoDB effectively. Using AI-driven developer tools gets the basics done very quickly, and, at least when it comes to MongoDB, I’m able to see if there are issues with the auto-generated code.\n\nOn one occasion, my task was simple: Extract a large quantity of data from the database and return it as JSON — the twist in the tale being I was doing it in Java using Spring Boot, a technology I like but have far less experience with than using Java alone.\n\nSo I fired up the AI chat and asked:\n\n“Make me a spring boot and spring data project to read all the records in a MongoDB person collection. And write them to a JSON file.”\n\nThe AI obliged with a few familiar-looking files I could have written myself, taking only about 20 times longer. After a brief skirmish working out the directory structure, it compiled and ran.\n\nReading through it, though, I realized it was failing to make some important optimizations, and that might be quite significant in terms of performance. This is the problem with large language model AIs. LLMS predict the next line (set of tokens) based on what the internet, on average, thinks they should be. This means you get the median solution, not the best one. If you point this out to an AI, it can sometimes fix the problem, but you probably asked the AI because it knows better than you, so you don’t know when to tell it to try harder.\n\nI wanted to understand the impact of this better so I built an experiment, which I have shared the code for. This generates data and then fetches it using the code from the AI, which is what 99% of developers would write. Alternatively, you can compare my code based on a deeper understanding of the MongoDB APIs.\n\nThen, I modified the code to check if my test collection was empty and if it was, to insert 300,000 records, which is approximately 1GB of JSON.\n\nThis ran and the code looked good, and I hadn’t even asked the AI for help on the last part, but it seemed exceptionally slow. As an expert, I knew what speed to expect and could therefore see a problem. A less experienced user might just incorrectly assume this is the speed MongoDB loads data at.\n\nMy code wasn’t making the rookie mistake or writing each record in its own API call and waiting for network round trips. It was using Spring Data’s saveAll() function and passing an array of documents to add. I assumed this was being mapped to a native insertMany() API call. It turned out I was wrong. My data generation code had populated the personId field that Spring Data was using to uniquely identify documents (and storing in Mongodb’s _id field used by the databases for that purpose). This meant Spring Data assumed these records already existed and it was attempting to update them individually first, rather than simply inserting them.\n\nI solved that by excluding personId from the generation. I used the easy approach of changing its type so my random generator ignores it. Now, my load has become 10x faster. The alternative is to build a custom bulk insert function using template.insert(List<Person>), but that’s quite a bit more complicated.\n\nThis is concerning because for years, we at MongoDB have advised people to make use of the _id field for your own primary key, and it seems now that advice has probably been problematic for many people. Various things recently, including this, mean I rarely advise that now — there are too many edge cases and too few benefits, typically.\n\nAnyway, that was a performance issue I was not expecting and not related to AI but likely impacted tens of thousands of systems. I moved on to what I was supposed to look into: read performance.\n\nContrary to popular belief, MongoDB doesn’t store JSON documents. Data is stored in a binary format called BSON which, whilst similar to JSON, stores non-string values like integers, doubles, and dates in their binary form rather than the string representation we see in JSON output.\n\nTo store JSON data in MongoDB, you need to parse it in your application code into objects. These objects could be: Java Maps, MongoDB’s generic document classes, Java Class instances (POJO), or Spring Data Models. Once you parse your JSON and create the objects, you tell MongoDB to persist them in the database. MongoDB will convert them to BSON byte streams in the driver library and send them to the database to store as that byte sequence.\n\nWhen retrieving data, the normal approach is to return data as whatever object type you used when storing the data. You can then convert those objects to JSON strings to return to the end user. Doing so gives you the ability to modify the objects before conversion to JSON and also apply any model-based mapping or joins — for example, putting the _id field back into the personId object member. These transforms are usually automatically performed in Spring Data. The code for this in my example uses all the standard Spring Data methods and looks like this.\n\nThe problem is this is a relatively inefficient and slow way to return JSON data as the data has to be returned (in BSON), then parsed into document objects (Java HashMaps ) — with a lot of allocation and subsequent garbage collection (GC) — which are then copied member by member using reflection into instances of model classes in Spring (with even more allocation and GC), before being converted to a JSON string (typically via an ObjectWriter object from Jackson).\n\nNone of this changes what is happening on the database server. It streams back BSON, but it can make a significant difference to the amount of CPU used in your application server and therefore the speed of a single retrieval or the number of retrievals per second you can perform.\n\nFortunately there is a more efficient way. We can retrieve data not as documents or as models but as JSONObject(s). In that case, the MongoDB driver converts the BSON documents directly into JSON strings on read without creating any intermediary objects. Doing so greatly reduces the computation, allocation, and garbage collection.\n\nOf course, it’s possible that the format in the database is not exactly the same as the JSON you want to retrieve. Perhaps you need to rename fields. Perhaps you need to calculate or transform some values, and you may even need to merge data from another collection. All things your Spring model was doing behind the scenes. Fortunately, you can make use of projections or even aggregation pipelines to perform this on the server, returning exactly the shape of data you need.\n\nI asked the AI how to add a custom function to my repository and it obliged with the sample of double inheritance needed. I sprinkled in some code I’d used before to illustrate this conversion in a talk and discovered another issue! My code, which was far from new, wouldn’t compile because some key imports could not be found.\n\nAfter some head scratching and a quick call to a human Java expert, I realised that the AI had actually created a project for me using an ancient version of Spring and tied to an even more ancient version of MongoDB. This is because, as always, the AI knows the most common way to do things on the internet even if that is now old news. I asked it to upgrade to the current version and then I got access to some post-2017 features.\n\nShortly after that, I had code running. Auto-generated data loaded and retrieved quickly, comparing the AI provided Spring method to the MongoDB expert approach. I was not surprised to see that the latter was twice as fast (in my case, a single thread fetches 1 GB in 58 seconds versus 136 seconds, and some portion of that is fetching the BSON). That code looks like this.\n\nThis may not seem important but I work with customers who run daily jobs to extract staggering amounts of data, sometimes taking 20 hours. A 50% reduction in time is very meaningful for them. For others, where the thrust of their application or service is to serve up JSON, this would mean a 50% reduction in the number of appservers they need.\n\nAnd that’s before we get started talking about that 10x data load overhead if you take the most obvious API.\n\nWhat did I take away from this? AI coding helpers save developer time, and they make development teams look good with their productivity, but are they silently costing millions in sub-optimal code? Well, no, because the team would have likely written the same average code anyway without the AI. What’s costing millions is not having expertise or not making use of it to review your database and API usage. Whether it’s MongoDB or your favourite RDBMS, most developers have limited database skills, and LLMs are no substitute for asking an expert. This is why database consultants are worth their weight in CPUs and get paid accordingly.\n\nIf you are using MongoDB, we have a range of folks who can review what you are doing in your code to see if we can cut your hosting spend. Find out more about MongoDB consulting."
    },
    {
        "link": "https://docs.spring.io/spring-data/mongodb/reference/mongodb/template-query-operations.html",
        "document": "Earlier, we saw how to retrieve a single document by using the and methods on . These methods return a single domain object right way or using a reactive API a emitting a single element. We can also query for a collection of documents to be returned as a list of domain objects. Assuming that we have a number of objects with name and age stored as documents in a collection and that each person has an embedded account document with a balance, we can now run a query using the following code: Querying for documents using the MongoTemplate All find methods take a object as a parameter. This object defines the criteria and options used to perform the query. The criteria are specified by using a object that has a static factory method named to instantiate a new object. We recommend using static imports for and to make the query more readable. The query should return a or of objects that meet the specified criteria. The rest of this section lists the methods of the and classes that correspond to the operators provided in MongoDB. Most methods return the object, to provide a fluent style for the API. The class provides the following methods, all of which correspond to operators in MongoDB:\n• all Creates a criterion using the operator\n• and Adds a chained with the specified to the current and returns the newly created one\n• andOperator Creates an and query using the operator for all of the provided criteria (requires MongoDB 2.0 or later)\n• andOperator Creates an and query using the operator for all of the provided criteria (requires MongoDB 2.0 or later)\n• in Creates a criterion using the operator for a varargs argument.\n• in Creates a criterion using the operator using a collection\n• is Creates a criterion using field matching ( ). If the specified value is a document, the order of the fields and exact equality in the document matters.\n• norOperator Creates an nor query using the operator for all of the provided criteria\n• norOperator Creates an nor query using the operator for all of the provided criteria\n• not Creates a criterion using the meta operator which affects the clause directly following\n• orOperator Creates an or query using the operator for all of the provided criteria\n• orOperator Creates an or query using the operator for all of the provided criteria\n• matchingDocumentStructure Creates a criterion using the operator for JSON schema criteria. can only be applied on the top level of a query and not property specific. Use the attribute of the schema to match against nested fields.\n• bits() is the gateway to MongoDB bitwise query operators like . The Criteria class also provides the following methods for geospatial queries.\n• nearSphere Creates a geospatial criterion using operations. This is only available for MongoDB 1.7 and higher.\n• minDistance Creates a geospatial criterion using the operation, for use with $near.\n• maxDistance Creates a geospatial criterion using the operation, for use with $near. The class has some additional methods that allow to select certain fields as well as to limit and sort the result.\n• addCriteria used to add additional criteria to the query\n• fields used to define fields to be included in the query results\n• limit used to limit the size of the returned results to the provided limit (used for paging)\n• skip used to skip the provided number of documents in the results (used for paging)\n• with used to provide sort definition for the results\n• with used to provide a scroll position (Offset- or Keyset-based pagination) to start or resume a The template API allows direct usage of result projections that enable you to map queries against a given domain type while projecting the operation result onto another one as outlined below. For more information on result projections please refer to the Projections section of the documentation.\n\nMongoDB provides an operation to obtain distinct values for a single field by using a query from the resulting documents. Resulting values are not required to have the same data type, nor is the feature limited to simple types. For retrieval, the actual result type does matter for the sake of conversion and typing. The following example shows how to query for distinct values: Select distinct values of the field. The field name is mapped according to the domain types property declaration, taking potential annotations into account. Retrieve all distinct values as a of (due to no explicit result type being specified). Retrieving distinct values into a of is the most flexible way, as it tries to determine the property value of the domain type and convert results to the desired type or mapping structures. Sometimes, when all values of the desired field are fixed to a certain type, it is more convenient to directly obtain a correctly typed , as shown in the following example: Select distinct values of the field. The fieldname is mapped according to the domain types property declaration, taking potential annotations into account. Retrieved values are converted into the desired target type — in this case, . It is also possible to map the values to a more complex type if the stored field contains a document. Retrieve all distinct values as a of . If the type cannot be converted into the desired target type, this method throws a . MongoDB supports GeoSpatial queries through the use of operators such as , , , and . Methods specific to geospatial queries are available on the class. There are also a few shape classes ( , , and ) that are used in conjunction with geospatial related methods. Using GeoSpatial queries requires attention when used within MongoDB transactions, see Special behavior inside transactions. To understand how to perform GeoSpatial queries, consider the following class (taken from the integration tests and relying on the rich ): @Document(collection=\"newyork\") public class Venue { @Id private String id; private String name; private double[] location; @PersistenceConstructor Venue(String name, double[] location) { super(); this.name = name; this.location = location; } public Venue(String name, double x, double y) { super(); this.name = name; this.location = new double[] { x, y }; } public String getName() { return name; } public double[] getLocation() { return location; } @Override public String toString() { return \"Venue [id=\" + id + \", name=\" + name + \", location=\" + Arrays.toString(location) + \"]\"; } } To find locations within a , you can use the following query: To find venues within a using spherical coordinates, you can use the following query: To find venues within a , you can use the following query: //lower-left then upper-right Box box = new Box(new Point(-73.99756, 40.73083), new Point(-73.988135, 40.741404)); List<Venue> venues = template.find(new Query(Criteria.where(\"location\").within(box)), Venue.class); To find venues near a , you can use the following queries: To find venues near a using spherical coordinates, you can use the following query:\n\nChanged in 2.2!\n\n MongoDB 4.2 removed support for the command which had been previously used to run the . Spring Data MongoDB 2.2 uses the aggregation instead of the command to run a . The calculated distance (the when using a geoNear command) previously returned within a wrapper type now is embedded into the resulting document. If the given domain type already contains a property with that name, the calculated distance is named with a potentially random postfix. Target types may contain a property named after the returned distance to (additionally) read it back directly into the domain type as shown below. Domain type used to identify the target collection and potential query mapping. MongoDB supports querying the database for geo locations and calculating the distance from a given origin at the same time. With geo-near queries, you can express queries such as \"find all restaurants in the surrounding 10 miles\". To let you do so, provides methods that take a as an argument (as well as the already familiar entity type and collection), as shown in the following example: We use the builder API to set up a query to return all instances surrounding the given out to 10 miles. The enum used here actually implements an interface so that other metrics could be plugged into a distance as well. A is backed by a multiplier to transform the distance value of the given metric into native distances. The sample shown here would consider the 10 to be miles. Using one of the built-in metrics (miles and kilometers) automatically triggers the spherical flag to be set on the query. If you want to avoid that, pass plain values into . For more information, see the Javadoc of and . The geo-near operations return a wrapper object that encapsulates instances. Wrapping allows accessing the average distance of all results. A single object carries the entity found plus its distance from the origin.\n\nThen MongoDB operator allows usage of a GeoJSON Point or legacy coordinate pairs. Though syntactically different the server is fine accepting both no matter what format the target Document within the collection is using. There is a huge difference in the distance calculation. Using the legacy format operates upon Radians on an Earth like sphere, whereas the GeoJSON format uses Meters. To avoid a serious headache make sure to set the to the desired unit of measure which ensures the distance to be calculated correctly. Assume you’ve got 5 Documents like the ones below: { \"_id\" : ObjectId(\"5c10f3735d38908db52796a5\"), \"name\" : \"Penn Station\", \"location\" : { \"type\" : \"Point\", \"coordinates\" : [ -73.99408, 40.75057 ] } } { \"_id\" : ObjectId(\"5c10f3735d38908db52796a6\"), \"name\" : \"10gen Office\", \"location\" : { \"type\" : \"Point\", \"coordinates\" : [ -73.99171, 40.738868 ] } } { \"_id\" : ObjectId(\"5c10f3735d38908db52796a9\"), \"name\" : \"City Bakery \", \"location\" : { \"type\" : \"Point\", \"coordinates\" : [ -73.992491, 40.738673 ] } } { \"_id\" : ObjectId(\"5c10f3735d38908db52796aa\"), \"name\" : \"Splash Bar\", \"location\" : { \"type\" : \"Point\", \"coordinates\" : [ -73.992491, 40.738673 ] } } { \"_id\" : ObjectId(\"5c10f3735d38908db52796ab\"), \"name\" : \"Momofuku Milk Bar\", \"location\" : { \"type\" : \"Point\", \"coordinates\" : [ -73.985839, 40.731698 ] } } Fetching all Documents within a 400 Meter radius from would look like this using GeoJSON: { \"_id\" : ObjectId(\"5c10f3735d38908db52796a6\"), \"name\" : \"10gen Office\", \"location\" : { \"type\" : \"Point\", \"coordinates\" : [ -73.99171, 40.738868 ] } \"distance\" : 0.0 (3) } { \"_id\" : ObjectId(\"5c10f3735d38908db52796a9\"), \"name\" : \"City Bakery \", \"location\" : { \"type\" : \"Point\", \"coordinates\" : [ -73.992491, 40.738673 ] } \"distance\" : 69.3582262492474 (3) } { \"_id\" : ObjectId(\"5c10f3735d38908db52796aa\"), \"name\" : \"Splash Bar\", \"location\" : { \"type\" : \"Point\", \"coordinates\" : [ -73.992491, 40.738673 ] } \"distance\" : 69.3582262492474 (3) } Now, when using legacy coordinate pairs one operates upon Radians as discussed before. So we use command. The makes sure the distance multiplier is set correctly. Returning the 3 Documents just like the GeoJSON variant: { \"_id\" : ObjectId(\"5c10f3735d38908db52796a6\"), \"name\" : \"10gen Office\", \"location\" : { \"type\" : \"Point\", \"coordinates\" : [ -73.99171, 40.738868 ] } \"distance\" : 0.0 (4) } { \"_id\" : ObjectId(\"5c10f3735d38908db52796a9\"), \"name\" : \"City Bakery \", \"location\" : { \"type\" : \"Point\", \"coordinates\" : [ -73.992491, 40.738673 ] } \"distance\" : 0.0693586286032982 (4) } { \"_id\" : ObjectId(\"5c10f3735d38908db52796aa\"), \"name\" : \"Splash Bar\", \"location\" : { \"type\" : \"Point\", \"coordinates\" : [ -73.992491, 40.738673 ] } \"distance\" : 0.0693586286032982 (4) } The distance multiplier so we get Kilometers as resulting distance. Make sure we operate on a 2d_sphere index. Distance from center point in Kilometers - take it times 1000 to match Meters of the GeoJSON variant.\n\nSince version 2.6 of MongoDB, you can run full-text queries by using the operator. Methods and operations specific to full-text queries are available in and . When doing full text search, see the MongoDB reference for its behavior and limitations. Before you can actually use full-text search, you must set up the search index correctly. See Text Index for more detail on how to create index structures. The following example shows how to set up a full-text search: A query searching for can be defined and run as follows: To sort results by relevance according to the use . Use the score property for sorting results by relevance which triggers . Use to include the calculated relevance in the resulting . You can exclude search terms by prefixing the term with or by using , as shown in the following example (note that the two lines have the same effect and are thus redundant): // search for 'coffee' and not 'cake' TextQuery.queryText(new TextCriteria().matching(\"coffee\").matching(\"-cake\")); TextQuery.queryText(new TextCriteria().matching(\"coffee\").notMatching(\"cake\")); takes the provided term as is. Therefore, you can define phrases by putting them between double quotation marks (for example, or using by The following example shows both ways of defining a phrase: You can set flags for and by using the corresponding methods on . Note that these two optional flags have been introduced in MongoDB 3.2 and are not included in the query unless explicitly set.\n\nQuery by Example can be used on the Template API level run example queries. The following snipped shows how to query by example: Person probe = new Person(); probe.lastname = \"stark\"; Example example = Example.of(probe); Query query = new Query(new Criteria().alike(example)); List<Person> result = template.find(query, Person.class); By default is strictly typed. This means that the mapped query has an included type match, restricting it to probe assignable types. For example, when sticking with the default type key ( ), the query has restrictions such as ( ). By using the , it is possible to bypass the default behavior and skip the type restriction. So, as long as field names match, nearly any domain type can be used as the probe for creating the reference, as the following example shows: class JustAnArbitraryClassWithMatchingFieldName { @Field(\"lastname\") String value; } JustAnArbitraryClassWithMatchingFieldNames probe = new JustAnArbitraryClassWithMatchingFieldNames(); probe.value = \"stark\"; Example example = Example.of(probe, UntypedExampleMatcher.matching()); Query query = new Query(new Criteria().alike(example)); List<Person> result = template.find(query, Person.class); When including values in the , Spring Data Mongo uses embedded document matching instead of dot notation property matching. Doing so forces exact document matching for all property values and the property order in the embedded document. is likely the right choice for you if you are storing different entities within a single collection or opted out of writing type hints. Also, keep in mind that using requires eager initialization of the . To do so, configure to to ensure proper alias resolution for read operations. Spring Data MongoDB provides support for different matching options:"
    },
    {
        "link": "https://stackoverflow.com/questions/48235882/how-to-query-a-array-of-json-in-a-mongodb-document-using-spring-data-mongodb",
        "document": "I am using spring data mongodb sdk to query mongo db.\n\nThe document in mongoDb looks like this:\n\nIn my api request I have a structure similar to \"suggestions\" element above. I want to create a query criteria where \"is\" clause should be the value of \"suggestions\" element in the api request.\n\nI tried the following code using spring data mongo db:\n\nThe problem with this code is that when I debug and see the query that gets created using criteria above, I goes in as $java: [{\"key\": \"take\", \"value\": 1}]\n\nTherefore, it can't match it with the mongo document and doesn't fetch me any result.\n\nIs there another way to query and array of documents in mongodb from spring data mongo ?"
    },
    {
        "link": "https://baeldung.com/queries-in-spring-data-mongodb",
        "document": "This tutorial will focus on building out different types of queries in Spring Data MongoDB.\n\nWe’re going to be looking at querying documents with Query and Criteria classes, auto-generated query methods, JSON queries, and QueryDSL.\n\nFor the Maven setup, have a look at our introductory article.\n\nOne of the more common ways to query MongoDB with Spring Data is by making use of the Query and Criteria classes, which very closely mirror native operators.\n\nThis is simply a criterion using equality. Let’s see how it works.\n\nIn the following example, we’ll look for users named Eric.\n\nLet’s look at our database:\n\nNow let’s look at the query code:\n\nA more flexible and powerful type of query is the regex. This creates a criterion using a MongoDB $regex that returns all records suitable for the regex for this field.\n\nIt works similarly to startingWith and endingWith operations.\n\nIn this example, we’ll look for all users that have names starting with A.\n\nHere’s the state of the database:\n\nHere’s another quick example, this time looking for all users that have names ending with c:\n\nSo the result will be:\n\nThese operators create a criterion using the $lt (less than) and $gt (greater than) operators.\n\nLet’s take a quick example where we’re looking for all users between the ages of 20 and 50.\n\nAnd the results for all users with an age of greater than 20 and less than 50:\n\nSort is used to specify a sort order for the results.\n\nThe example below returns all the users sorted by age in ascending order.\n\nAnd here’s the result of the query, nicely sorted by age:\n\nLet’s look at a quick example using pagination.\n\nHere’s the state of the database:\n\nNow here’s the query logic, simply asking for a page of size 2:\n\nAnd the result, the 2 documents, as expected:\n\nNow let’s explore the more common type of query that Spring Data usually provides, auto-generated queries out of method names.\n\nThe only thing we need to do to leverage these kinds of queries is to declare the method on the repository interface:\n\nWe’ll start simple, by exploring the findBy type of query. In this case, we’ll use find by name: \n\n\n\nJust like in the previous section, 2.1, the query will have the same results, finding all users with the given name:\n\nIn section 2.2, we explored a regex based query. Starts and ends with are of course less powerful, but nevertheless quite useful, especially if we don’t have to actually implement them.\n\nHere’s a quick example of what the operations would look like: \n\n\n\nThe example of actually using this would, of course, be very simple:\n\nAnd the results are exactly the same.\n\nSimilar to section 2.3, this will return all users with ages between ageGT and ageLT:\n\n\n\nCalling the method will result in exactly the same documents being found:\n\nLet’s have a look at a more advanced example this time, combining two types of modifiers for the generated query.\n\nWe’re going to be looking for all users that have names containing the letter A, and we’re also going to order the results by age, in ascending order:\n\nFor the database we used in section 2.4, the result will be:\n\nIf we can’t represent a query with the help of a method name or criteria, we can do something more low level, use the @Query annotation.\n\nWith this annotation, we can specify a raw query as a Mongo JSON query string.\n\nLet’s start simple and look at how we would represent a find by type of method first:\n\nThis method should return users by name. The placeholder ?0 references the first parameter of the method.\n\nWe can also look at a regex driven query, which of course produces the same result as in sections 2.2 and 3.2:\n\nThe usage is also exactly the same:\n\nNow let’s implement the lt and gt query:\n\nNow that the method has 2 parameters, we’re referencing each of these by index in the raw query, ?0 and ?1:\n\nMongoRepository has good support for the QueryDSL project, so we can leverage that nice, type-safe API here as well.\n\nFirst, let’s make sure we have the correct Maven dependencies defined in the pom:\n\nQueryDSL used Q-classes for creating queries, but since we don’t really want to create these by hand, we need to generate them somehow.\n\nWe’re going to use the apt-maven-plugin to do that:\n\nLet’s look at the User class, focusing specifically on the @QueryEntity annotation:\n\nAfter running the process goal of the Maven lifecycle (or any other goal after that one), the apt plugin will generate the new classes under target/generated-sources/java/{your package structure}:\n\nIt’s because of this class that we don’t need to create our queries.\n\nAs a side note, if we’re using Eclipse, introducing this plugin will generate the following warning in pom:\n\nThe Maven install works fine and the QUser class is generated, but a plugin is highlighted in the pom.\n\nA quick fix is to manually point to the JDK in eclipse.ini:\n\nNow we need to actually enable QueryDSL support in our repositories, which is done by simply extending the QueryDslPredicateExecutor interface:\n\nWith support enabled, let’s now implement the same queries as the ones we illustrated before.\n\nSimilarly, let’s implement the previous queries and find users with names that are starting with A:\n\nAs well as ending with c:\n\nThe result is the same as in sections 2.2, 3.2 and 4.2.\n\nThe next query will return users with ages between 20 and 50, similar to the previous sections:\n\nIn this article, we explored the many ways we can query using Spring Data MongoDB.\n\nIt’s interesting to take a step back and see all the powerful ways we have to query MongoDB, varying from limited control all the way to full control with raw queries."
    },
    {
        "link": "https://stackoverflow.com/questions/29656128/how-can-a-store-raw-json-in-mongo-using-spring-boot",
        "document": "I am not sure if this helps, if you have option to create a Map in your Document then create it and later populate the \"key-value\" pairs in the hash map and store it. It will be stored as you expect (which is a JSON). This will be helpful when you do not know the name of the key or value during compile time."
    },
    {
        "link": "https://stackoverflow.com/questions/75813765/spring-boot-how-to-use-jdbctemplate-to-convert-into-custom-json-object",
        "document": "I've been programming with java for years. So far I have not used any frameworks in the backend or in the frontend area. I want to change this in the future because I think a lot of the security features of Spring Boot.\n\nNow I've encountered a problem and I don't know how to solve it. I have a table in the database that contains customer and item data.\n\nMy Query for Example:\n\nIn my old Structure wihout Framework it was easy to do it. I take the Resultset -> take it in a WHILE and do my Logic to generate this JSON.\n\nBUT IN SPRING BOOT how can I do it? I use jdbcTemplate for SQL statements... The problem is: it is not possible to split the database structure. In jdbcTemplate I don't get the whole result set I can only catch the next() Element.\n\nUnfortunately I can't find the clue..."
    },
    {
        "link": "https://twilio.com/en-us/blog/java-custom-queries-jdbctemplate-springboot",
        "document": "When developing applications it is crucial for a developer to understand the abstractions behind the libraries that are used in the application. This will help the developer understand the code better and assist them in writing efficient applications. In this tutorial, you will learn how to write custom queries using JdbcTemplate which is a wrapper class for writing row queries. There are other implementations that are much more convenient than such as Hibernate and the Java Persistence API (JPA) because these API’s hide the actual implementations from the developer and only expose the necessary details such as the methods to insert, update and delete records from your database. When you decide to use as opposed to Hibernate and the Java Persistence API you are prone to have boilerplate code in your application. The benefit of using is that it provides low level access to your queries providing flexibility and the ability to work with stored procedures. To work efficiently using these abstractions it is crucial to understand how the code works behind the scenes, which is made possible by leveraging .\n• MySQL - a database management system that you will use to store, retrieve, and update your data.\n• Intellij - an integrated development environment that you will use to edit your code.\n• Postman - an application that you will use to test your API’s. Setting up your Spring Boot project with JdbcTemplate Go to Spring Initializr and generate a new Maven project on Spring Boot version 2.6.3 with the following dependencies:\n• will allow you to create different http requests such as GET, POST, and DELETE to be executed on our web services.\n• will provide connection functionality to your database to ensure transactions are complete.\n• will allow you to manage your database tables by creating, editing, and deleting them each time a new migration is added.\n• will allow us to use the to write our custom queries by leveraging the different methods it provides.\n• will allow us to add constraints to the inputs to ensure correct and consistent data is saved into the database. In the Project Metadata section, enter \"com.twilio\" as the Group id and \"jdbcTemplate'' as the Artifact ID. The Package name section shows the \"com.twilio.jdbcTemplate\" folder structure that your project will have when you import it into the development environment. Make sure that the Packaging is set to \"Jar\" and the project has Java version 11 as seen below:\n\nThe annotation indicates that this class will hold a collection of beans for our application. To configure your , we need to add a bean definition of . This definition is a method that accepts a data source containing the connection details and returns a new containing this data source passed to it through its constructor. The annotation to this method indicates that it will be managed by the spring context and we do not need to create a new instance whenever we want to use it. This denotes the entity in the database that you will use to store different records for the different objects created in our application. Create a package named model under the com/twilio/jdbcTemplate folder. Inside this package, create a class named with the following properties: Generate , , and methods that you will use to retrieve the data from our request and view the data to the console for testing purposes. You will also need to generate a constructor with all the fields as arguments because you will use it when writing the queries. Copy and paste the following code to the Employee.java package file:"
    },
    {
        "link": "https://docs.spring.io/spring-framework/reference/data-access/jdbc/core.html",
        "document": "Using the JDBC Core Classes to Control Basic JDBC Processing and Error Handling\n\nis the central class in the JDBC core package. It handles the creation and release of resources, which helps you avoid common errors, such as forgetting to close the connection. It performs the basic tasks of the core JDBC workflow (such as statement creation and execution), leaving application code to provide SQL and extract results. The class:\n• Performs iteration over instances and extraction of returned parameter values.\n• Catches JDBC exceptions and translates them to the generic, more informative, exception hierarchy defined in the package. (See Consistent Exception Hierarchy.) When you use the for your code, you need only to implement callback interfaces, giving them a clearly defined contract. Given a provided by the class, the callback interface creates a prepared statement, providing SQL and any necessary parameters. The same is true for the interface, which creates callable statements. The interface extracts values from each row of a . You can use within a DAO implementation through direct instantiation with a reference, or you can configure it in a Spring IoC container and give it to DAOs as a bean reference. The should always be configured as a bean in the Spring IoC container. In the first case the bean is given to the service directly; in the second case it is given to the prepared template. All SQL issued by this class is logged at the level under the category corresponding to the fully qualified class name of the template instance (typically , but it may be different if you use a custom subclass of the class). The following sections provide some examples of usage. These examples are not an exhaustive list of all of the functionality exposed by the . See the attendant javadoc for that. The following query gets the number of rows in a relation: The following query uses a bind variable: The following query looks for a : The following query finds and populates a single domain object: Actor actor = jdbcTemplate.queryForObject( \"select first_name, last_name from t_actor where id = ?\", (resultSet, rowNum) -> { Actor newActor = new Actor(); newActor.setFirstName(resultSet.getString(\"first_name\")); newActor.setLastName(resultSet.getString(\"last_name\")); return newActor; }, 1212L); The following query finds and populates a list of domain objects: If the last two snippets of code actually existed in the same application, it would make sense to remove the duplication present in the two lambda expressions and extract them out into a single field that could then be referenced by DAO methods as needed. For example, it may be better to write the preceding code snippet as follows: You can use the method to perform insert, update, and delete operations. Parameter values are usually provided as variable arguments or, alternatively, as an object array. The following example inserts a new entry: The following example updates an existing entry: The following example deletes an entry: You can use the method to run any arbitrary SQL. Consequently, the method is often used for DDL statements. It is heavily overloaded with variants that take callback interfaces, binding variable arrays, and so on. The following example creates a table: The following example invokes a stored procedure: More sophisticated stored procedure support is covered later. Instances of the class are thread-safe, once configured. This is important because it means that you can configure a single instance of a and then safely inject this shared reference into multiple DAOs (or repositories). The is stateful, in that it maintains a reference to a , but this state is not conversational state. A common practice when using the class (and the associated class) is to configure a in your Spring configuration file and then dependency-inject that shared bean into your DAO classes. The is created in the setter for the or in the constructor. This leads to DAOs that resemble the following: public class JdbcCorporateEventDao implements CorporateEventDao { private final JdbcTemplate jdbcTemplate; public JdbcCorporateEventDao(DataSource dataSource) { this.jdbcTemplate = new JdbcTemplate(dataSource); } // JDBC-backed implementations of the methods on the CorporateEventDao follow... } class JdbcCorporateEventDao(dataSource: DataSource): CorporateEventDao { private val jdbcTemplate = JdbcTemplate(dataSource) // JDBC-backed implementations of the methods on the CorporateEventDao follow... } The following example shows the corresponding configuration: An alternative to explicit configuration is to use component-scanning and annotation support for dependency injection. In this case, you can annotate the class with (which makes it a candidate for component-scanning). The following example shows how to do so: @Repository public class JdbcCorporateEventRepository implements CorporateEventRepository { private JdbcTemplate jdbcTemplate; // Implicitly autowire the DataSource constructor parameter public JdbcCorporateEventRepository(DataSource dataSource) { this.jdbcTemplate = new JdbcTemplate(dataSource); } // JDBC-backed implementations of the methods on the CorporateEventRepository follow... } The following example shows the corresponding configuration: <!-- Scans within the base package of the application for @Component classes to configure as beans --> <context:component-scan base-package=\"org.example.jdbc\" /> <bean id=\"dataSource\" class=\"org.apache.commons.dbcp2.BasicDataSource\" destroy-method=\"close\"> <property name=\"driverClassName\" value=\"${jdbc.driverClassName}\"/> <property name=\"url\" value=\"${jdbc.url}\"/> <property name=\"username\" value=\"${jdbc.username}\"/> <property name=\"password\" value=\"${jdbc.password}\"/> </bean> <context:property-placeholder location=\"jdbc.properties\"/> If you use Spring’s class and your various JDBC-backed DAO classes extend from it, your sub-class inherits a method from the class. You can choose whether to inherit from this class. The class is provided as a convenience only. Regardless of which of the above template initialization styles you choose to use (or not), it is seldom necessary to create a new instance of a class each time you want to run SQL. Once configured, a instance is thread-safe. If your application accesses multiple databases, you may want multiple instances, which requires multiple and, subsequently, multiple differently configured instances.\n\nThe class adds support for programming JDBC statements by using named parameters, as opposed to programming JDBC statements using only classic placeholder ( ) arguments. The class wraps a and delegates to the wrapped to do much of its work. This section describes only those areas of the class that differ from the itself — namely, programming JDBC statements by using named parameters. The following example shows how to use : // some JDBC-backed DAO class... private NamedParameterJdbcTemplate namedParameterJdbcTemplate; public void setDataSource(DataSource dataSource) { this.namedParameterJdbcTemplate = new NamedParameterJdbcTemplate(dataSource); } public int countOfActorsByFirstName(String firstName) { String sql = \"select count(*) from t_actor where first_name = :first_name\"; SqlParameterSource namedParameters = new MapSqlParameterSource(\"first_name\", firstName); return this.namedParameterJdbcTemplate.queryForObject(sql, namedParameters, Integer.class); } Notice the use of the named parameter notation in the value assigned to the variable and the corresponding value that is plugged into the variable (of type ). Alternatively, you can pass along named parameters and their corresponding values to a instance by using the -based style. The remaining methods exposed by the and implemented by the class follow a similar pattern and are not covered here. The following example shows the use of the -based style: // some JDBC-backed DAO class... private NamedParameterJdbcTemplate namedParameterJdbcTemplate; public void setDataSource(DataSource dataSource) { this.namedParameterJdbcTemplate = new NamedParameterJdbcTemplate(dataSource); } public int countOfActorsByFirstName(String firstName) { String sql = \"select count(*) from t_actor where first_name = :first_name\"; Map<String, String> namedParameters = Collections.singletonMap(\"first_name\", firstName); return this.namedParameterJdbcTemplate.queryForObject(sql, namedParameters, Integer.class); } // some JDBC-backed DAO class... private val namedParameterJdbcTemplate = NamedParameterJdbcTemplate(dataSource) fun countOfActorsByFirstName(firstName: String): Int { val sql = \"select count(*) from t_actor where first_name = :first_name\" val namedParameters = mapOf(\"first_name\" to firstName) return namedParameterJdbcTemplate.queryForObject(sql, namedParameters, Int::class.java)!! } One nice feature related to the (and existing in the same Java package) is the interface. You have already seen an example of an implementation of this interface in one of the previous code snippets (the class). An is a source of named parameter values to a . The class is a simple implementation that is an adapter around a , where the keys are the parameter names and the values are the parameter values. Another implementation is the class. This class wraps an arbitrary JavaBean (that is, an instance of a class that adheres to the JavaBean conventions) and uses the properties of the wrapped JavaBean as the source of named parameter values. The following example shows a typical JavaBean: The following example uses a to return the count of the members of the class shown in the preceding example: // some JDBC-backed DAO class... private NamedParameterJdbcTemplate namedParameterJdbcTemplate; public void setDataSource(DataSource dataSource) { this.namedParameterJdbcTemplate = new NamedParameterJdbcTemplate(dataSource); } public int countOfActors(Actor exampleActor) { // notice how the named parameters match the properties of the above 'Actor' class String sql = \"select count(*) from t_actor where first_name = :firstName and last_name = :lastName\"; SqlParameterSource namedParameters = new BeanPropertySqlParameterSource(exampleActor); return this.namedParameterJdbcTemplate.queryForObject(sql, namedParameters, Integer.class); } // some JDBC-backed DAO class... private val namedParameterJdbcTemplate = NamedParameterJdbcTemplate(dataSource) private val namedParameterJdbcTemplate = NamedParameterJdbcTemplate(dataSource) fun countOfActors(exampleActor: Actor): Int { // notice how the named parameters match the properties of the above 'Actor' class val sql = \"select count(*) from t_actor where first_name = :firstName and last_name = :lastName\" val namedParameters = BeanPropertySqlParameterSource(exampleActor) return namedParameterJdbcTemplate.queryForObject(sql, namedParameters, Int::class.java)!! } Remember that the class wraps a classic template. If you need access to the wrapped instance to access functionality that is present only in the class, you can use the method to access the wrapped through the interface. See also Best Practices for guidelines on using the class in the context of an application.\n\nAs of 6.1, the named parameter statements of and the positional parameter statements of a regular are available through a unified client API with a fluent interaction model. For example, with positional parameters: For example, with named parameters: capabilities are available as well, with flexible result resolution: Instead of a custom , you may also specify a class to map to. For example, assuming that has and properties as a record class, a custom constructor, bean properties, or plain fields: And for an update statement: Or an update statement with named parameters: Instead of individual named parameters, you may also specify a parameter source object – for example, a record class, a class with bean properties, or a plain field holder which provides and properties, such as the class from above: The automatic class mapping for parameters as well as the query results above is provided through implicit and strategies which are also available for direct use. They can serve as a common replacement for and / , also with and themselves. is a flexible but simplified facade for JDBC query/update statements. Advanced capabilities such as batch inserts and stored procedure calls typically require extra customization: consider Spring’s and classes or plain direct usage for any such capabilities not available in .\n\nis an interface to be implemented by classes that can translate between s and Spring’s own , which is agnostic in regard to data access strategy. Implementations can be generic (for example, using SQLState codes for JDBC) or proprietary (for example, using Oracle error codes) for greater precision. This exception translation mechanism is used behind the common and entry points which do not propagate but rather . As of 6.0, the default exception translator is , detecting JDBC 4 subclasses with a few extra checks, and with a fallback to introspection through . This is usually sufficient for common database access and does not require vendor-specific detection. For backwards compatibility, consider using as described below, potentially with custom error code mappings. is the implementation of that is used by default when a file named is present in the root of the classpath. This implementation uses specific vendor codes. It is more precise than or subclass translation. The error code translations are based on codes held in a JavaBean type class called . This class is created and populated by an , which (as the name suggests) is a factory for creating based on the contents of a configuration file named . This file is populated with vendor codes and based on the taken from . The codes for the actual database you are using are used. The applies matching rules in the following sequence:\n• Any custom translation implemented by a subclass. Normally, the provided concrete is used, so this rule does not apply. It applies only if you have actually provided a subclass implementation.\n• Any custom implementation of the interface that is provided as the property of the class.\n• The list of instances of the class (provided for the property of the class) are searched for a match.\n• Use the fallback translator. is the default fallback translator. If this translation is not available, the next fallback translator is the . The is used by default to define error codes and custom exception translations. They are looked up in a file named from the classpath, and the matching instance is located based on the database name from the database metadata of the database in use. You can extend , as the following example shows: In the preceding example, the specific error code ( ) is translated while other errors are left to be translated by the default translator implementation. To use this custom translator, you must pass it to the through the method , and you must use this for all of the data access processing where this translator is needed. The following example shows how you can use this custom translator: private JdbcTemplate jdbcTemplate; public void setDataSource(DataSource dataSource) { // create a JdbcTemplate and set data source this.jdbcTemplate = new JdbcTemplate(); this.jdbcTemplate.setDataSource(dataSource); // create a custom translator and set the DataSource for the default translation lookup CustomSQLErrorCodesTranslator tr = new CustomSQLErrorCodesTranslator(); tr.setDataSource(dataSource); this.jdbcTemplate.setExceptionTranslator(tr); } public void updateShippingCharge(long orderId, long pct) { // use the prepared JdbcTemplate for this update this.jdbcTemplate.update(\"update orders\" + \" set shipping_charge = shipping_charge * ? / 100\" + \" where id = ?\", pct, orderId); } // create a JdbcTemplate and set data source private val jdbcTemplate = JdbcTemplate(dataSource).apply { // create a custom translator and set the DataSource for the default translation lookup exceptionTranslator = CustomSQLErrorCodesTranslator().apply { this.dataSource = dataSource } } fun updateShippingCharge(orderId: Long, pct: Long) { // use the prepared JdbcTemplate for this update this.jdbcTemplate!!.update(\"update orders\" + \" set shipping_charge = shipping_charge * ? / 100\" + \" where id = ?\", pct, orderId) } The custom translator is passed a data source in order to look up the error codes in .\n\nSome query methods return a single value. To retrieve a count or a specific value from one row, use . The latter converts the returned JDBC to the Java class that is passed in as an argument. If the type conversion is invalid, an is thrown. The following example contains two query methods, one for an and one that queries for a : public class RunAQuery { private JdbcTemplate jdbcTemplate; public void setDataSource(DataSource dataSource) { this.jdbcTemplate = new JdbcTemplate(dataSource); } public int getCount() { return this.jdbcTemplate.queryForObject(\"select count(*) from mytable\", Integer.class); } public String getName() { return this.jdbcTemplate.queryForObject(\"select name from mytable\", String.class); } } class RunAQuery(dataSource: DataSource) { private val jdbcTemplate = JdbcTemplate(dataSource) val count: Int get() = jdbcTemplate.queryForObject(\"select count(*) from mytable\")!! val name: String? get() = jdbcTemplate.queryForObject(\"select name from mytable\") } In addition to the single result query methods, several methods return a list with an entry for each row that the query returned. The most generic method is , which returns a where each element is a containing one entry for each column, using the column name as the key. If you add a method to the preceding example to retrieve a list of all the rows, it might be as follows: The returned list would resemble the following:\n\nAn convenience method supports the retrieval of primary keys generated by the database. This support is part of the JDBC 3.0 standard. See Chapter 13.6 of the specification for details. The method takes a as its first argument, and this is the way the required insert statement is specified. The other argument is a , which contains the generated key on successful return from the update. There is no standard single way to create an appropriate (which explains why the method signature is the way it is). The following example works on Oracle but may not work on other platforms: final String INSERT_SQL = \"insert into my_test (name) values(?)\"; final String name = \"Rob\"; KeyHolder keyHolder = new GeneratedKeyHolder(); jdbcTemplate.update(connection -> { PreparedStatement ps = connection.prepareStatement(INSERT_SQL, new String[] { \"id\" }); ps.setString(1, name); return ps; }, keyHolder); // keyHolder.getKey() now contains the generated key val INSERT_SQL = \"insert into my_test (name) values(?)\" val name = \"Rob\" val keyHolder = GeneratedKeyHolder() jdbcTemplate.update({ it.prepareStatement (INSERT_SQL, arrayOf(\"id\")).apply { setString(1, name) } }, keyHolder) // keyHolder.getKey() now contains the generated key"
    },
    {
        "link": "https://baeldung.com/spring-jdbc-jdbctemplate",
        "document": "In this tutorial, we’ll go through practical use cases of the Spring JDBC module.\n\nAll the classes in Spring JDBC are divided into four separate packages:\n• core — the core functionality of JDBC. Some of the important classes under this package include JdbcTemplate, SimpleJdbcInsert, SimpleJdbcCall, and NamedParameterJdbcTemplate\n• datasource — utility classes to access a data source. It also has various data source implementations for testing JDBC code outside the Jakarta EE container\n• object — DB access in an object-oriented manner. It allows running queries and returning the results as a business object. It also maps the query results between the columns and properties of business objects\n• support — support classes for classes under core and object packages, e.g., provides the SQLException translation functionality\n\n Explore the capabilities offered by Spring to perform JDBC Authentication using an existing DataSource configuration. Introduction to Spring Data JPA with Spring 4 - the Spring config, the DAO, manual and generated queries and transaction management.\n\nLet’s add the spring-boot-starter-jdbc and mysql-connector-j to the pom.xml:\n\nAlso, let’s add the h2 dependency to the pom.xml:\n\nThe H2 database is an embedded database for fast prototyping.\n\nThere are two main approaches to configuring data sources in Spring: using properties files or using Java-based configuration.\n\nTo configure the data source, let’s modify our application.properties:\n\nHere, we configure the connection details to a MySQL database. We can now use it for database operations.\n\nNotably, we can also configure the data source as a bean:\n\nHowever, we should prefer the application.properties file configuration because it separates the configuration from the code.\n\nAlternatively, we can also make good use of an embedded database for development or testing. In that case, we can define the H2 database connection details in our application.properties file:\n\nAlternatively, here’s a quick configuration that creates an instance of H2 embedded database and pre-populates it with simple SQL scripts as a bean:\n\nWe can use this configuration if we don’t want to define it in the application.properties file. However, configuring our data source in the application.properties is generally preferred.\n\nNotably, we can use Spring profiles to manage multiple configurations in a project.\n\nLet’s explore the basic usage of the JdbcTemplate.\n\nThe JDBC template is the main API through which we’ll access most of the functionality that we’re interested in:\n• iterating over the ResultSet and returning results\n\nFirst, let’s start with a simple example to see what the JdbcTemplate can do:\n\nNotice the standard syntax of providing parameters using the? character.\n\nNext, let’s look at an alternative to this syntax.\n\nTo get support for named parameters, let’s use the other JDBC template provided by the framework — the NamedParameterJdbcTemplate.\n\nIt wraps the JbdcTemplate and provides an alternative to the traditional syntax using ? to specify parameters.\n\nUnder the hood, it substitutes the named parameters to JDBC ? placeholder and delegates to the wrapped JDCTemplate to run the queries:\n\nNotice how we are using the MapSqlParameterSource to provide the values for the named parameters.\n\nLet’s look at using properties from a bean to determine the named parameters:\n\nNote how we’re now using the BeanPropertySqlParameterSource implementations instead of manually specifying the named parameters like before.\n\nAnother very useful feature is the ability to map query results to Java objects by implementing the RowMapper interface.\n\nFor example, for every row returned by the query, Spring uses the row mapper to populate the java bean:\n\nSubsequently, we can now pass the row mapper to the query API and get fully populated Java objects:\n\nSpring comes with its own data exception hierarchy out of the box — with DataAccessException as the root exception — and it translates all underlying raw exceptions to it.\n\nSo, we keep our sanity by not handling low-level persistence exceptions. We also benefit from the fact that Spring wraps the low-level exceptions in DataAccessException or one of its sub-classes.\n\nThis also keeps the exception-handling mechanism independent of the underlying database we are using.\n\nBesides the default SQLErrorCodeSQLExceptionTranslator, we can also implement it ourselves.\n\nHere’s a quick example of a custom implementation — customizing the error message when there is a duplicate key violation, which results in error code 23505 when using H2:\n\nTo use this custom exception translator, we need to pass it to the JdbcTemplate by calling setExceptionTranslator() method:\n\nSimpleJdbc classes provide an easy way to configure and run SQL statements. These classes use database metadata to build basic queries. So, SimpleJdbcInsert and SimpleJdbcCall classes provide an easier way to run insert and stored procedure calls.\n\nLet’s take a look at running simple insert statements with minimal configuration.\n\nThe INSERT statement is generated based on the configuration of SimpleJdbcInsert. We need only provide the table, column, and value names.\n\nNext, let’s provide the column names and values and run the operation:\n\nFurther, we can use the executeAndReturnKey() API to allow the database to generate the primary key. We’ll also need to configure the actual auto-generated column:\n\nFinally, we can pass in this data using the BeanPropertySqlParameterSource and MapSqlParameterSource.\n\nLet’s also take a look at running stored procedures.\n\nWe’ll make use of the SimpleJdbcCall abstraction:\n\nAnother simple use case is batching multiple operations together.\n\nUsing JdbcTemplate, Batch Operations can be run via the batchUpdate() API.\n\nThe interesting part here is the concise but highly useful BatchPreparedStatementSetter implementation:\n\nWe also have the option of batching operations with the NamedParameterJdbcTemplate – batchUpdate() API.\n\nThis API is simpler than the previous one. So, there’s no need to implement any extra interfaces to set the parameters, as it has an internal prepared statement setter to set the parameter values.\n\nInstead, the parameter values can be passed to the batchUpdate() method as an array of SqlParameterSource.\n\nIn this article, we looked at the JDBC abstraction in the Spring Framework. We covered the various capabilities provided by Spring JDBC with practical examples.\n\nWe also looked into how we can quickly get started with Spring JDBC using a Spring Boot JDBC starter."
    },
    {
        "link": "https://stackoverflow.com/questions/11843658/update-a-row-using-spring-jdbctemplate",
        "document": "I am new to spring. I am developing a CRUD application using spring jdbc template. I am done with insert and select. but in update am facing some problem. can anybody provide me a simple example of update and delete using jdbctemplate. thnks in advance.\n\nwhen i click on update button the edited values should be updated in my DB according to the row ID , my problem is how to map the row id from jsp to controller."
    }
]