[
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.concat.html",
        "document": "Allows optional set logic along the other axes.\n\nCan also add a layer of hierarchical indexing on the concatenation axis, which may be useful if the labels are the same (or overlapping) on the passed axis number.\n\nobjs a sequence or mapping of Series or DataFrame objects If a mapping is passed, the sorted keys will be used as the argument, unless it is passed, in which case the values will be selected (see below). Any None objects will be dropped silently unless they are all None in which case a ValueError will be raised. The axis to concatenate along. How to handle indexes on other axis (or axes). If True, do not use the index values along the concatenation axis. The resulting axis will be labeled 0, …, n - 1. This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information. Note the index values on the other axes are still respected in the join. If multiple levels passed, should contain tuples. Construct hierarchical index using the passed keys as the outermost level. Specific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys. Names for the levels in the resulting hierarchical index. Check whether the new concatenated axis contains duplicates. This can be very expensive relative to the actual data concatenation. Sort non-concatenation axis if it is not already aligned. One exception to this is when the non-concatentation axis is a DatetimeIndex and join=’outer’ and the axis is not already aligned. In that case, the non-concatenation axis is always sorted lexicographically. If False, do not copy data unnecessarily. When concatenating all along the index (axis=0), a is returned. When contains at least one , a is returned. When concatenating along the columns (axis=1), a is returned.\n\nThe keys, levels, and names arguments are all optional.\n\nA walkthrough of how this method fits in with other tools for combining pandas objects can be found here.\n\nIt is not recommended to build DataFrames by adding single rows in a for loop. Build a list of rows and make a DataFrame in a single concat.\n\nClear the existing index and reset it in the result by setting the option to .\n\nAdd a hierarchical index at the outermost level of the data with the option.\n\nLabel the index keys you create with the option.\n\nCombine objects with overlapping columns and return everything. Columns outside the intersection will be filled with values.\n\nCombine objects with overlapping columns and return only those that are shared by passing to the keyword argument.\n\nCombine objects horizontally along the x axis by passing in .\n\nPrevent the result from including duplicate index values with the option.\n\nAppend a single row to the end of a object."
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/merging.html",
        "document": "pandas provides various methods for combining and comparing or .\n• None : Merge multiple or objects along a shared index or column\n• None : Update missing values with non-missing values in the same location\n• None : Combine two or objects with SQL-style joining\n• None : Combine two or objects along an ordered axis\n• None : Combine two or objects by near instead of exact matching keys\n• None and : Show differences in values between two or objects\n\nThe function concatenates an arbitrary amount of or objects along an axis while performing optional set logic (union or intersection) of the indexes on the other axes. Like , takes a list or dict of homogeneously-typed objects and concatenates them. makes a full copy of the data, and iteratively reusing can create unnecessary copies. Collect all or objects in a list before using . When concatenating with named axes, pandas will attempt to preserve these index/column names whenever possible. In the case where all inputs share a common name, this name will be assigned to the result. When the input names do not all agree, the result will be unnamed. The same is true for , but the logic is applied separately on a level-by-level basis. The keyword specifies how to handle axis values that don’t exist in the first . takes the union of all axis values takes the intersection of the axis values To perform an effective “left” join using the exact index from the original , result can be reindexed. For objects which don’t have a meaningful index, the ignores overlapping indexes. You can concatenate a mix of and objects. The will be transformed to with the column name as the name of the . will drop all name references. The argument adds another axis level to the resulting index or column (creating a ) associate specific keys with each original . The argument cane override the column names when creating a new based on existing . You can also pass a dict to in which case the dict keys will be used for the argument unless other argument is specified: The created has levels that are constructed from the passed keys and the index of the pieces: argument allows specifying resulting levels associated with the If you have a that you want to append as a single row to a , you can convert the row into a and use\n\nperforms join operations similar to relational databases like SQL. Users who are familiar with SQL but new to pandas can reference a comparison with SQL.\n• None one-to-one: joining two objects on their indexes which must contain unique values.\n• None many-to-one: joining a unique index to one or more columns in a different . When joining columns on columns, potentially a many-to-many join, any indexes on the passed objects will be discarded. For a many-to-many join, if a key combination appears more than once in both tables, the will have the Cartesian product of the associated data. The argument to specifies which keys are included in the resulting table. If a key combination does not appear in either the left or right tables, the values in the joined table will be . Here is a summary of the options and their SQL equivalent names: Use keys from left frame only Use keys from right frame only Use union of keys from both frames Use intersection of keys from both frames Create the cartesian product of rows of both frames You can and a with a if the names of the correspond to the columns from the . Transform the to a using before merging Performing an outer join with duplicate join keys in Merging on duplicate keys significantly increase the dimensions of the result and can cause a memory overflow. The argument checks whether the uniqueness of merge keys. Key uniqueness is checked before merge operations and can protect against memory overflows and unexpected key duplication. Traceback (most recent call last) in in # check if columns specified as unique in \"Merge keys are not unique in left dataset; not a one-to-one merge\" \"Merge keys are not unique in right dataset; not a one-to-one merge\" : Merge keys are not unique in right dataset; not a one-to-one merge If the user is aware of the duplicates in the right but wants to ensure there are no duplicates in the left , one can use the argument instead, which will not raise an exception. accepts the argument . If , a Categorical-type column called will be added to the output object that takes on values: A string argument to will use the value as the name for the indicator column. The merge argument takes a tuple of list of strings to append to overlapping column names in the input to disambiguate the result columns:\n\ncombines the columns of multiple, potentially differently-indexed into a single result . takes an optional argument which may be a column or multiple column names that the passed is to be aligned. To join on multiple keys, the passed must have a : The default for is to perform a left join which uses only the keys found in the calling . Other join types can be specified with . You can join a with a to a with a on a level. The of the with match the level name of the . The of the input argument must be completely used in the join and is a subset of the indices in the left argument. Merging on a combination of columns and index levels# Strings passed as the , , and parameters may refer to either column names or index level names. This enables merging instances on a combination of index levels and columns without resetting indexes. When are joined on a string that matches an index level in both arguments, the index level is preserved as an index level in the resulting . When are joined using only some of the levels of a , the extra levels will be dropped from the resulting join. To preserve those levels, use on those level names to move those levels to columns prior to the join. A list or tuple of can also be passed to to join them together on their indexes. update missing values from one with the non-missing values in another in the corresponding location.\n\nis similar to an ordered left-join except that mactches are on the nearest key rather than equal keys. For each row in the , the last row in the are selected where the key is less than the left’s key. Both must be sorted by the key. Optionally an can perform a group-wise merge by matching the key in addition to the nearest match on the key. within between the quote time and the trade time. within between the quote time and the trade time and exclude exact matches on time. Note that though we exclude the exact matches (of the quotes), prior quotes do propagate to that point in time.\n\nThe and methods allow you to compare two or , respectively, and summarize their differences. By default, if two corresponding values are equal, they will be shown as . Furthermore, if all values in an entire row / column, the row / column will be omitted from the result. The remaining differences will be aligned on columns. Keep all original rows and columns with self other self other self other Keep all the original values even if they are equal. self other self other self other"
    },
    {
        "link": "https://pandas.pydata.org/pandas-docs/version/0.20/merging.html",
        "document": "pandas provides various facilities for easily combining together Series, DataFrame, and Panel objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.\n\nThe function (in the main pandas namespace) does all of the heavy lifting of performing concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes. Note that I say “if any” because there is only a single possible axis of concatenation for Series. Before diving into all of the details of and what it can do, here is a simple example: Like its sibling function on ndarrays, , takes a list or dict of homogeneously-typed objects and concatenates them with some configurable handling of “what to do with the other axes”:\n• : a sequence or mapping of Series, DataFrame, or Panel objects. If a dict is passed, the sorted keys will be used as the argument, unless it is passed, in which case the values will be selected (see below). Any None objects will be dropped silently unless they are all None in which case a ValueError will be raised.\n• : {0, 1, ...}, default 0. The axis to concatenate along.\n• : {‘inner’, ‘outer’}, default ‘outer’. How to handle indexes on other axis(es). Outer for union and inner for intersection.\n• : boolean, default False. If True, do not use the index values on the concatenation axis. The resulting axis will be labeled 0, ..., n - 1. This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information. Note the index values on the other axes are still respected in the join.\n• : list of Index objects. Specific indexes to use for the other n - 1 axes instead of performing inner/outer set logic.\n• : sequence, default None. Construct hierarchical index using the passed keys as the outermost level. If multiple levels passed, should contain tuples.\n• : list of sequences, default None. Specific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys.\n• : list, default None. Names for the levels in the resulting hierarchical index.\n• : boolean, default False. Check whether the new concatenated axis contains duplicates. This can be very expensive relative to the actual data concatenation.\n• : boolean, default True. If False, do not copy data unnecessarily. Without a little bit of context and example many of these arguments don’t make much sense. Let’s take the above example. Suppose we wanted to associate specific keys with each of the pieces of the chopped up DataFrame. We can do this using the argument: As you can see (if you’ve read the rest of the documentation), the resulting object’s index has a hierarchical index. This means that we can now do stuff like select out each chunk by key: It’s not a stretch to see how this can be very useful. More detail on this functionality below. It is worth noting however, that (and therefore ) makes a full copy of the data, and that constantly reusing this function can create a significant performance hit. If you need to use the operation over several datasets, use a list comprehension. Set logic on the other axes¶ When gluing together multiple DataFrames (or Panels or...), for example, you have a choice of how to handle the other axes (other than the one being concatenated). This can be done in three ways:\n• Take the (sorted) union of them all, . This is the default option as it results in zero information loss.\n• Use a specific index (in the case of DataFrame) or indexes (in the case of Panel or future higher dimensional objects), i.e. the argument Here is a example of each of these methods. First, the default behavior: Note that the row indexes have been unioned and sorted. Here is the same thing with : Lastly, suppose we just wanted to reuse the exact index from the original DataFrame: A useful shortcut to are the instance methods on Series and DataFrame. These methods actually predated . They concatenate along , namely the index: In the case of DataFrame, the indexes must be disjoint but the columns do not need to be: may take multiple objects to concatenate: Unlike method, which appends to the original list and returns nothing, here does not modify and returns its copy with appended. For DataFrames which don’t have a meaningful index, you may wish to append them and ignore the fact that they may have overlapping indexes: To do this, use the argument: This is also a valid argument to : You can concatenate a mix of Series and DataFrames. The Series will be transformed to DataFrames with the column name as the name of the Series. If unnamed Series are passed they will be numbered consecutively. Passing will drop all name references. A fairly common use of the argument is to override the column names when creating a new DataFrame based on existing Series. Notice how the default behaviour consists on letting the resulting DataFrame inherits the parent Series’ name, when these existed. Through the argument we can override the existing column names. Let’s consider now a variation on the very first example presented: You can also pass a dict to in which case the dict keys will be used for the argument (unless other keys are specified): The MultiIndex created has levels that are constructed from the passed keys and the index of the DataFrame pieces: If you wish to specify other levels (as will occasionally be the case), you can do so using the argument: Yes, this is fairly esoteric, but is actually necessary for implementing things like GroupBy where the order of a categorical variable is meaningful. While not especially efficient (since a new object must be created), you can append a single row to a DataFrame by passing a Series or dict to , which returns a new DataFrame as above. You should use with this method to instruct DataFrame to discard its index. If you wish to preserve the index, you should construct an appropriately-indexed DataFrame and append or concatenate those objects. You can also pass a list of dicts or Series:\n\npandas has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL. These methods perform significantly better (in some cases well over an order of magnitude better) than other open source implementations (like in R). The reason for this is careful algorithmic design and internal layout of the data in DataFrame. See the cookbook for some advanced strategies. Users who are familiar with SQL but new to pandas might be interested in a comparison with SQL. pandas provides a single function, , as the entry point for all standard database join operations between DataFrame objects:\n• None : Columns (names) to join on. Must be found in both the left and right DataFrame objects. If not passed and and are , the intersection of the columns in the DataFrames will be inferred to be the join keys\n• None : Columns from the left DataFrame to use as keys. Can either be column names or arrays with length equal to the length of the DataFrame\n• None : Columns from the right DataFrame to use as keys. Can either be column names or arrays with length equal to the length of the DataFrame\n• None : If , use the index (row labels) from the left DataFrame as its join key(s). In the case of a DataFrame with a MultiIndex (hierarchical), the number of levels must match the number of join keys from the right DataFrame\n• None : Same usage as for the right DataFrame\n• None : One of , , , . Defaults to . See below for more detailed description of each method\n• None : Sort the result DataFrame by the join keys in lexicographical order. Defaults to , setting to will improve performance substantially in many cases\n• None : A tuple of string suffixes to apply to overlapping columns. Defaults to .\n• None : Always copy data (default ) from the passed DataFrame objects, even when reindexing is not necessary. Cannot be avoided in many cases but may improve performance / memory usage. The cases where copying can be avoided are somewhat pathological but this option is provided nonetheless.\n• None : Add a column to the output DataFrame called with information on the source of each row. is Categorical-type and takes on a value of for observations whose merge key only appears in DataFrame, for observations whose merge key only appears in DataFrame, and if the observation’s merge key is found in both. The return type will be the same as . If is a and is a subclass of DataFrame, the return type will still be . is a function in the pandas namespace, and it is also available as a DataFrame instance method, with the calling DataFrame being implicitly considered the left object in the join. The related method, uses internally for the index-on-index (by default) and column(s)-on-index join. If you are joining on index only, you may wish to use to save yourself some typing. Experienced users of relational databases like SQL will be familiar with the terminology used to describe join operations between two SQL-table like structures (DataFrame objects). There are several cases to consider which are very important to understand:\n• one-to-one joins: for example when joining two DataFrame objects on their indexes (which must contain unique values)\n• many-to-one joins: for example when joining an index (unique) to one or more columns in a DataFrame When joining columns on columns (potentially a many-to-many join), any indexes on the passed DataFrame objects will be discarded. It is worth spending some time understanding the result of the many-to-many join case. In SQL / standard relational algebra, if a key combination appears more than once in both tables, the resulting table will have the Cartesian product of the associated data. Here is a very basic example with one unique key combination: Here is a more complicated example with multiple join keys: The argument to specifies how to determine which keys are to be included in the resulting table. If a key combination does not appear in either the left or right tables, the values in the joined table will be . Here is a summary of the options and their SQL equivalent names: Use keys from left frame only Use keys from right frame only Use union of keys from both frames Use intersection of keys from both frames Here is another example with duplicate join keys in DataFrames: Joining / merging on duplicate keys can cause a returned frame that is the multiplication of the row dimensions, may result in memory overflow. It is the user’ s responsibility to manage duplicate values in keys before joining large DataFrames. now accepts the argument . If , a Categorical-type column called will be added to the output object that takes on values: The argument will also accept string arguments, in which case the indicator function will use the value of the passed string as the name for the indicator column. Merging will preserve the dtype of the join keys. We are able to preserve the join keys"
    },
    {
        "link": "https://realpython.com/pandas-merge-join-and-concat",
        "document": "The and objects in pandas are powerful tools for exploring and analyzing data. Part of their power comes from a multifaceted approach to combining separate datasets. With pandas, you can merge, join, and concatenate your datasets, allowing you to unify and better understand your data as you analyze it.\n\nIn this tutorial, you’ll learn how and when to combine your data in pandas with:\n• for combining data on common columns or indices\n• for combining data on a key column or an index\n• for combining DataFrames across rows or columns\n\nIf you have some experience using and objects in pandas and you’re ready to learn how to combine them, then this tutorial will help you do exactly that. If you’re feeling a bit rusty, then you can watch a quick refresher on DataFrames before proceeding.\n\nYou can follow along with the examples in this tutorial using the interactive Jupyter Notebook and data files available at the link below:\n\nThe first technique that you’ll learn is . You can use anytime you want functionality similar to a database’s join operations. It’s the most flexible of the three operations that you’ll learn. When you want to combine data objects based on one or more keys, similar to what you’d do in a relational database, is the tool you need. More specifically, is most useful when you want to combine rows that share data. You can achieve both many-to-one and many-to-many joins with . In a many-to-one join, one of your datasets will have many rows in the merge column that repeat the same values. For example, the values could be 1, 1, 3, 5, and 5. At the same time, the merge column in the other dataset won’t have repeated values. Take 1, 3, and 5 as an example. As you might have guessed, in a many-to-many join, both of your merge columns will have repeated values. These merges are more complex and result in the Cartesian product of the joined rows. This means that, after the merge, you’ll have every combination of rows that share the same value in the key column. You’ll see this in action in the examples below. What makes so flexible is the sheer number of options for defining the behavior of your merge. While the list can seem daunting, with practice you’ll be able to expertly merge datasets of all kinds. When you use , you’ll provide two required arguments: After that, you can provide a number of optional arguments to define how your datasets are merged:\n• defines what kind of merge to make. It defaults to , but other possible options include , , and .\n• tells which columns or indices, also called key columns or key indices, you want to join on. This is optional. If it isn’t specified, and and (covered below) are , then columns from the two DataFrames that share names will be used as join keys. If you use , then the column or index that you specify must be present in both objects.\n• and specify a column or index that’s present only in the or object that you’re merging. Both default to .\n• and both default to , but if you want to use the index of the left or right object to be merged, then you can set the relevant argument to .\n• is a tuple of strings to append to identical column names that aren’t merge keys. This allows you to keep track of the origins of columns with the same name. These are some of the most important parameters to pass to . For the full list, see the pandas documentation. Note: In this tutorial, you’ll see that examples always use to specify which column(s) to join on. This is the safest way to merge your data because you and anyone reading your code will know exactly what to expect when calling . If you don’t specify the merge column(s) with , then pandas will use any columns with the same name as the merge keys. How to Use Before getting into the details of how to use , you should first understand the various forms of joins: Note: Even though you’re learning about merging, you’ll see , , , and also referred to as join operations. For this tutorial, you can consider the terms merge and join equivalent. You’ll learn about these different joins in detail below, but first take a look at this visual representation of them: In this image, the two circles are your two datasets, and the labels point to which part or parts of the datasets you can expect to see. While this diagram doesn’t cover all the nuance, it can be a handy guide for visual learners. If you have an SQL background, then you may recognize the merge operation names from the syntax. Except for , all of these techniques are types of outer joins. With outer joins, you’ll merge your data based on all the keys in the left object, the right object, or both. For keys that only exist in one object, unmatched columns in the other object will be filled in with , which stands for Not a Number. You can also see a visual explanation of the various joins in an SQL context on Coding Horror. Now take a look at the different joins in action. Many pandas tutorials provide very simple DataFrames to illustrate the concepts that they are trying to explain. This approach can be confusing since you can’t relate the data to anything concrete. So, for this tutorial, you’ll use two real-world datasets as the DataFrames to be merged: You can explore these datasets and follow along with the examples below using the interactive Jupyter Notebook and climate data CSVs: Download the notebook and data set: Click here to get the Jupyter Notebook and CSV data set you’ll use to learn about Pandas merge(), .join(), and concat() in this tutorial. If you’d like to learn how to use Jupyter Notebooks, then check out Jupyter Notebook: An Introduction. These two datasets are from the National Oceanic and Atmospheric Administration (NOAA) and were derived from the NOAA public data repository. First, load the datasets into separate DataFrames: In the code above, you used pandas’ to conveniently load your source CSV files into objects. You can then look at the headers and first few rows of the loaded DataFrames with : Here, you used to get the first five rows of each DataFrame. Make sure to try this on your own, either with the interactive Jupyter Notebook or in your console, so that you can explore the data in greater depth. Next, take a quick look at the dimensions of the two DataFrames: Note that is a property of objects that tells you the dimensions of the DataFrame. For , the output of says that the DataFrame has 127,020 rows and 21 columns. In this example, you’ll use with its default arguments, which will result in an inner join. Remember that in an inner join, you’ll lose rows that don’t have a match in the other DataFrame’s key column. With the two datasets loaded into objects, you’ll select a small slice of the precipitation dataset and then use a plain call to do an inner join. This will result in a smaller, more focused dataset: Here you’ve created a new DataFrame called from the DataFrame, selecting only rows in which the field is . If you check the attribute, then you’ll see that it has 365 rows. When you do the merge, how many rows do you think you’ll get in the merged DataFrame? Remember that you’ll be doing an inner join: If you guessed 365 rows, then you were correct! This is because defaults to an inner join, and an inner join will discard only those rows that don’t match. Because all of your rows had a match, none were lost. You should also notice that there are many more columns now: 47 to be exact. With , you also have control over which column(s) to join on. Let’s say that you want to merge both entire datasets, but only on and since the combination of the two will yield a unique value for each row. To do so, you can use the parameter: You can specify a single key column with a string or multiple key columns with a list. This results in a DataFrame with 123,005 rows and 48 columns. Why 48 columns instead of 47? Because you specified the key columns to join on, pandas doesn’t try to merge all mergeable columns. This can result in “duplicate” column names, which may or may not have different values. “Duplicate” is in quotation marks because the column names will not be an exact match. By default, they are appended with and . You can also use the parameter to control what’s appended to the column names. To prevent surprises, all the following examples will use the parameter to specify the column or columns on which to join. Here, you’ll specify an outer join with the parameter. Remember from the diagrams above that in an outer join—also known as a full outer join—all rows from both DataFrames will be present in the new DataFrame. If a row doesn’t have a match in the other DataFrame based on the key column(s), then you won’t lose the row like you would with an inner join. Instead, the row will be in the merged DataFrame, with values filled in where appropriate. This is best illustrated in an example: If you remember from when you checked the attribute of , then you’ll see that the number of rows in is the same. With an outer join, you can expect to have the same number of rows as the larger DataFrame. That’s because no rows are lost in an outer join, even when they don’t have a match in the other DataFrame. In this example, you’ll specify a left join—also known as a left outer join—with the parameter. Using a left outer join will leave your new merged DataFrame with all rows from the left DataFrame, while discarding rows from the right DataFrame that don’t have a match in the key column of the left DataFrame. You can think of this as a half-outer, half-inner merge. The example below shows you this in action: has 127,020 rows, matching the number of rows in the left DataFrame, . To prove that this only holds for the left DataFrame, run the same code, but change the position of and : This results in a DataFrame with 365 rows, matching the number of rows in . The right join, or right outer join, is the mirror-image version of the left join. With this join, all rows from the right DataFrame will be retained, while rows in the left DataFrame without a match in the key column of the right DataFrame will be discarded. To demonstrate how right and left joins are mirror images of each other, in the example below you’ll recreate the DataFrame from above, only this time using a right join: Here, you simply flipped the positions of the input DataFrames and specified a right join. When you inspect , you might notice that it’s not exactly the same as . The only difference between the two is the order of the columns: the first input’s columns will always be the first in the newly formed DataFrame. is the most complex of the pandas data combination tools. It’s also the foundation on which the other tools are built. Its complexity is its greatest strength, allowing you to combine datasets in every which way and to generate new insights into your data. On the other hand, this complexity makes difficult to use without an intuitive grasp of set theory and database operations. In this section, you’ve learned about the various data merging techniques, as well as many-to-one and many-to-many merges, which ultimately come from set theory. For more information on set theory, check out Sets in Python. Now, you’ll look at , a simplified version of .\n\nWhile is a module function, is an instance method that lives on your DataFrame. This enables you to specify only one DataFrame, which will join the DataFrame you call on. Under the hood, uses , but it provides a more efficient way to join DataFrames than a fully specified call. Before diving into the options available to you, take a look at this short example: With the indices visible, you can see a left join happening here, with being the left DataFrame. You might notice that this example provides the parameters and . Because joins on indices and doesn’t directly merge DataFrames, all columns—even those with matching names—are retained in the resulting DataFrame. Now flip the previous example around and instead call on the larger DataFrame: Notice that the DataFrame is larger, but data that doesn’t exist in the smaller DataFrame, , is filled in with values. How to Use By default, will attempt to do a left join on indices. If you want to join on columns like you would with , then you’ll need to set the columns as indices. Like , has a few parameters that give you more flexibility in your joins. However, with , the list of parameters is relatively short:\n• is the only required parameter. It defines the other DataFrame to join. You can also specify a list of DataFrames here, allowing you to combine a number of datasets in a single call.\n• specifies an optional column or index name for the left DataFrame ( in the previous example) to join the DataFrame’s index. If it’s set to , which is the default, then you’ll get an index-on-index join.\n• has the same options as from . The difference is that it’s index-based unless you also specify columns with .\n• and are similar to in . They specify a suffix to add to any overlapping columns but have no effect when passing a list of DataFrames.\n• can be enabled to sort the resulting DataFrame by the join key. In this section, you’ll see examples showing a few different use cases for . Some will be simplifications of calls. Others will be features that set apart from the more verbose calls. Since you already saw a short call, in this first example you’ll attempt to recreate a call with . What will this require? Take a second to think about a possible solution, and then look at the proposed solution below: Because works on indices, if you want to recreate from before, then you must set indices on the join columns that you specify. In this example, you used to set your indices to the key columns within the join. Note that does a left join by default so you need to explictly use to do an inner join. With this, the connection between and should be clearer. Below you’ll see a call that’s almost bare. Because there are overlapping columns, you’ll need to specify a suffix with , , or both, but this example will demonstrate the more typical behavior of : This example should be reminiscent of what you saw in the introduction to earlier. The call is the same, resulting in a left join that produces a DataFrame with the same number of rows as . In this section, you’ve learned about and its parameters and uses. You’ve also learned about how works under the hood, and you’ve recreated a call with to better understand the connection between the two techniques.\n\nConcatenation is a bit different from the merging techniques that you saw above. With merging, you can expect the resulting dataset to have rows from the parent datasets mixed in together, often based on some commonality. Depending on the type of merge, you might also lose rows that don’t have matches in the other dataset. With concatenation, your datasets are just stitched together along an axis — either the row axis or column axis. Visually, a concatenation with no parameters along rows would look like this: To implement this in code, you’ll use and pass it a list of DataFrames that you want to concatenate. Code for this task would look like this: Note: This example assumes that your column names are the same. If your column names are different while concatenating along rows (axis 0), then by default the columns will also be added, and values will be filled in as applicable. What if you wanted to perform a concatenation along columns instead? First, take a look at a visual representation of this operation: To accomplish this, you’ll use a call like you did above, but you’ll also need to pass the parameter with a value of or : Note: This example assumes that your indices are the same between datasets. If they’re different while concatenating along columns (axis 1), then by default the extra indices (rows) will also be added, and values will be filled in as applicable. You’ll learn more about the parameters for in the section below. How to Use As you can see, concatenation is a simpler way to combine datasets. It’s often used to form a single, larger set to do additional operations on. Note: When you call , a copy of all the data that you’re concatenating is made. You should be careful with multiple calls, as the many copies that are made may negatively affect performance. Alternatively, you can set the optional parameter to When you concatenate datasets, you can specify the axis along which you’ll concatenate. But what happens with the other axis? Nothing. By default, a concatenation results in a set union, where all data is preserved. You’ve seen this with and as an outer join, and you can specify this with the parameter. If you use this parameter, then the default is , but you also have the option, which will perform an inner join, or set intersection. As with the other inner joins you saw earlier, some data loss can occur when you do an inner join with . Only where the axis labels match will you preserve rows or columns. Note: Remember, the parameter only specifies how to handle the axes that you’re not concatenating along. Since you learned about the parameter, here are some of the other parameters that takes:\n• takes any sequence—typically a list—of or objects to be concatenated. You can also provide a dictionary. In this case, the keys will be used to construct a hierarchical index.\n• represents the axis that you’ll concatenate along. The default value is , which concatenates along the index, or row axis. Alternatively, a value of will concatenate vertically, along columns. You can also use the string values or .\n• is similar to the parameter in the other techniques, but it only accepts the values or . The default value is , which preserves data, while would eliminate data that doesn’t have a match in the other dataset.\n• takes a Boolean or value. It defaults to . If , then the new combined dataset won’t preserve the original index values in the axis specified in the parameter. This lets you have entirely new index values.\n• allows you to construct a hierarchical index. One common use case is to have a new index while preserving the original indices so that you can tell which rows, for example, come from which original dataset.\n• specifies whether you want to copy the source data. The default value is . If the value is set to , then pandas won’t make copies of the source data. This list isn’t exhaustive. You can find the complete, up-to-date list of parameters in the pandas documentation. First, you’ll do a basic concatenation along the default axis using the DataFrames that you’ve been playing with throughout this tutorial: This one is very simple by design. Here, you created a DataFrame that is a double of a small DataFrame that was made earlier. One thing to notice is that the indices repeat. If you want a fresh, 0-based index, then you can use the parameter: As noted before, if you concatenate along axis 0 (rows) but have labels in axis 1 (columns) that don’t match, then those columns will be added and filled in with values. This results in an outer join: With these two DataFrames, since you’re just concatenating along rows, very few columns have the same name. That means you’ll see a lot of columns with values. To instead drop columns that have any missing data, use the parameter with the value to do an inner join: Using the inner join, you’ll be left with only those columns that the original DataFrames have in common: , , and . You can also flip this by setting the parameter: Now you have only the rows that have data for all columns in both DataFrames. It’s no coincidence that the number of rows corresponds with that of the smaller DataFrame. Another useful trick for concatenation is using the parameter to create hierarchical axis labels. This is useful if you want to preserve the indices or column names of the original datasets but also want to add new ones: If you check on the original DataFrames, then you can verify whether the higher-level axis labels and were added to the appropriate rows.\n\nYou’ve now learned the three most important techniques for combining data in pandas:\n• for combining data on common columns or indices\n• for combining data on a key column or an index\n• for combining DataFrames across rows or columns In addition to learning how to use these techniques, you also learned about set logic by experimenting with the different ways to join your datasets. Additionally, you learned about the most common parameters to each of the above techniques, and what arguments you can pass to customize their output. You saw these techniques in action on a real dataset obtained from the NOAA, which showed you not only how to combine your data but also the benefits of doing so with pandas’ built-in techniques. If you haven’t downloaded the project files yet, you can get them here: Download the notebook and data set: Click here to get the Jupyter Notebook and CSV data set you’ll use to learn about Pandas merge(), .join(), and concat() in this tutorial. Did you learn something new? Figure out a creative way to solve a problem by combining complex datasets? Let us know in the comments below!"
    },
    {
        "link": "https://pandas.pydata.org/pandas-docs/version/2.1/reference/api/pandas.concat.html",
        "document": "Allows optional set logic along the other axes.\n\nCan also add a layer of hierarchical indexing on the concatenation axis, which may be useful if the labels are the same (or overlapping) on the passed axis number.\n\nobjs a sequence or mapping of Series or DataFrame objects If a mapping is passed, the sorted keys will be used as the argument, unless it is passed, in which case the values will be selected (see below). Any None objects will be dropped silently unless they are all None in which case a ValueError will be raised. The axis to concatenate along. How to handle indexes on other axis (or axes). If True, do not use the index values along the concatenation axis. The resulting axis will be labeled 0, …, n - 1. This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information. Note the index values on the other axes are still respected in the join. If multiple levels passed, should contain tuples. Construct hierarchical index using the passed keys as the outermost level. Specific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys. Names for the levels in the resulting hierarchical index. Check whether the new concatenated axis contains duplicates. This can be very expensive relative to the actual data concatenation. Sort non-concatenation axis if it is not already aligned. If False, do not copy data unnecessarily. When concatenating all along the index (axis=0), a is returned. When contains at least one , a is returned. When concatenating along the columns (axis=1), a is returned.\n\nThe keys, levels, and names arguments are all optional.\n\nA walkthrough of how this method fits in with other tools for combining pandas objects can be found here.\n\nIt is not recommended to build DataFrames by adding single rows in a for loop. Build a list of rows and make a DataFrame in a single concat.\n\nClear the existing index and reset it in the result by setting the option to .\n\nAdd a hierarchical index at the outermost level of the data with the option.\n\nLabel the index keys you create with the option.\n\nCombine objects with overlapping columns and return everything. Columns outside the intersection will be filled with values.\n\nCombine objects with overlapping columns and return only those that are shared by passing to the keyword argument.\n\nCombine objects horizontally along the x axis by passing in .\n\nPrevent the result from including duplicate index values with the option.\n\nAppend a single row to the end of a object."
    },
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html",
        "document": "Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: . If you want to pass in a path object, pandas accepts any . By file-like object, we refer to objects with a method, such as a file handle (e.g. via builtin function) or . Deprecated since version 2.1.0: Passing byte strings is deprecated. To read from a byte string, wrap it in a object.\n\nColumn (0-indexed) to use as the row labels of the DataFrame. Pass None if there is no such column. If a list is passed, those columns will be combined into a . If a subset of data is selected with , index_col is based on the subset. Missing values will be forward filled to allow roundtripping with for . To avoid forward filling the missing values use after reading the data instead of .\n\nData type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32} Use to preserve data as stored in Excel and not interpret dtype, which will necessarily result in dtype. If converters are specified, they will be applied INSTEAD of dtype conversion. If you use , it will infer the dtype of each column based on the data.\n\nIf io is not a buffer or path, this must be set to identify io. Engine compatibility : When , the following logic will be used to determine the engine:\n• None If is an OpenDocument format (.odf, .ods, .odt), then odf will be used.\n• None Otherwise if is an xls format, will be used.\n• None Otherwise if is in xlsb format, will be used.\n• None Otherwise will be used.\n\nWhether or not to include the default NaN values when parsing the data. Depending on whether is passed in, the behavior is as follows:\n• None If is True, and are specified, is appended to the default NaN values used for parsing.\n• None If is True, and are not specified, only the default NaN values are used for parsing.\n• None If is False, and are specified, only the NaN values specified are used for parsing.\n• None If is False, and are not specified, no strings will be parsed as NaN. Note that if is passed in as False, the and parameters will be ignored.\n\nThe behavior is as follows:\n• None . If True -> try parsing the index.\n• None of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.\n• None of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column. If a column or index contains an unparsable date, the entire column or index will be returned unaltered as an object data type. If you don`t want to parse some cells as date just change their type in Excel to “Text”. For non-standard datetime parsing, use after .\n\nFunction to use for converting a sequence of string columns to an array of datetime instances. The default uses to do the conversion. Pandas will try to call in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by ) as arguments; 2) concatenate (row-wise) the string values from the columns defined by into a single array and pass that; and 3) call once for each row using one or more strings (corresponding to the columns defined by ) as arguments. Deprecated since version 2.0.0: Use instead, or read in as and then apply as-needed."
    },
    {
        "link": "https://digitalocean.com/community/tutorials/pandas-read_excel-reading-excel-file-in-python",
        "document": "We can use the pandas module read_excel() function to read the excel file data into a DataFrame object. If you look at an excel sheet, it’s a two-dimensional table. The DataFrame object also represents a two-dimensional tabular data structure.\n\nLet’s say we have an excel file with two sheets - Employees and Cars. The top row contains the header of the table.\n\nHere is the example to read the “Employees” sheet data and printing it.\n• The first parameter is the name of the excel file.\n• The sheet_name parameter defines the sheet to be read from the excel file.\n• When we print the DataFrame object, the output is a two-dimensional table. It looks similar to an excel sheet records.\n\n2. List of Columns Headers of the Excel Sheet\n\nWe can get the list of column headers using the property of the dataframe object.\n\nWe can get the column data and convert it into a list of values.\n\nWe can specify the column names to be read from the excel file. It’s useful when you are interested in only a few of the columns of the excel sheet.\n\nIf the excel sheet doesn’t have any header row, pass the header parameter value as None.\n\nIf you pass the header value as an integer, let’s say 3. Then the third row will be treated as the header row and the values will be read from the next row onwards. Any data before the header row will be discarded.\n\nThe DataFrame object has various utility methods to convert the tabular data into Dict, CSV, or JSON format."
    },
    {
        "link": "https://geeksforgeeks.org/working-with-excel-files-using-pandas",
        "document": "Excel sheets are very instinctive and user-friendly, which makes them ideal for manipulating large datasets even for less technical folks. If you are looking for places to learn to manipulate and automate stuff in Excel files using Python, look no further. You are at the right place.\n\nIn this article, you will learn how to use Pandas to work with Excel spreadsheets. In this article we will learn about:\n\nTo install Pandas in Python, we can use the following command in the command prompt:\n\nTo install Pandas in Anaconda, we can use the following command in Anaconda Terminal:\n\nFirst of all, we need to import the Pandas module which can be done by running the command:\n\nInput File: Let’s suppose the Excel file looks like this\n\nNow we can import the Excel file using the read_excel function in Pandas to read Excel file using Pandas in Python. The second statement reads the data from Excel and stores it into a pandas Data Frame which is represented by the variable newData.\n\nIf there are multiple sheets in the Excel workbook, the command will import data from the first sheet. To make a data frame with all the sheets in the workbook, the easiest method is to create different data frames separately and then concatenate them. The read_excel method takes argument sheet_name and index_col where we can specify the sheet of which the frame should be made of and index_col specifies the title column, as is shown below:\n\nThe third statement concatenates both sheets. Now to check the whole data frame, we can simply run the following command:\n\nTo view 5 columns from the top and from the bottom of the data frame, we can run the command. This head() and tail() method also take arguments as numbers for the number of columns to show.\n\nThe shape() method can be used to view the number of rows and columns in the data frame as follows:\n\nIf any column contains numerical data, we can sort that column using the sort_values() method in pandas as follows:\n\nNow, let’s suppose we want the top 5 values of the sorted column, we can use the head() method here:\n\nWe can do that with any numerical column of the data frame as shown below:\n\nNow, suppose our data is mostly numerical. We can get the statistical information like mean, max, min, etc. about the data frame using the describe() method as shown below:\n\nThis can also be done separately for all the numerical columns using the following command:\n\nOther statistical information can also be calculated using the respective methods. Like in Excel, formulas can also be applied, and calculated columns can be created as follows:\n\nAfter operating on the data in the data frame, we can export the data back to an Excel file using the method to_excel. For this, we need to specify an output Excel file where the transformed data is to be written, as shown below:\n\nHow to Use Pandas with Excel Files?\n\nPandas provides powerful tools to read from and write to Excel files, making it easy to integrate Excel data with your Python scripts. You can read Excel files using the function. It requires the or library for files or the library for files. To write a DataFrame to an Excel file, you can use the method of the DataFrame class. It requires the library to write to files. # Write the DataFrame to an Excel file \n\n\n\nHow to Extract Data from Excel Using Pandas?\n\nCan We Read XLSX File in Pandas?\n\nIs Pandas Better Than SQL?\n\nDo I Need openpyxl for Pandas?"
    },
    {
        "link": "https://stackoverflow.com/questions/38560748/python-pandas-dataframe-reading-exact-specified-range-in-an-excel-sheet",
        "document": "I have a lot of different table (and other unstructured data in an excel sheet) .. I need to create a dataframe out of range 'A3:D20' from 'Sheet2' of Excel sheet 'data'.\n\nAll examples that I come across drilldown up to sheet level, but not how to pick it from an exact range.\n\nOnce I get this, I plan to look up data in column A and find its corresponding value in column B.\n\nEdit 1: I realised that openpyxl takes too long, and so have changed that to instead, and it is much faster at that stage at least.\n\nEdit 2: For the time being, I have put my data in just one sheet and:"
    },
    {
        "link": "https://realpython.com/working-with-large-excel-files-in-pandas",
        "document": "In this tutorial you’re going to learn how to work with large Excel files in pandas, focusing on reading and analyzing an xls file and then working with a subset of the original data.\n\nThis tutorial utilizes Python (tested with 64-bit versions of v2.7.9 and v3.4.3), pandas (v0.16.1), and XlsxWriter (v0.7.3). We recommend using the Anaconda distribution to quickly get started, as it comes pre-installed with all the needed libraries.\n\nThe first file we’ll work with is a compilation of all the car accidents in England from 1979-2004, to extract all accidents that happened in London in the year 2000. Start by downloading the source ZIP file from data.gov.uk, and extract the contents. Then try to open Accidents7904.csv in Excel. Be careful. If you don’t have enough memory, this could very well crash your computer. You should see a “File Not Loaded Completely” error since Excel can only handle one million rows at a time. We tested this in LibreOffice as well and received a similar error - “The data could not be loaded completely because the maximum number of rows per sheet was exceeded.” To solve this, we can open the file in pandas. Before we start, the source code is on Github. Within a new project directory, activate a virtualenv, and then install pandas: Now let’s build the script. Create a file called pandas_accidents.py and the add the following code: # See which headers are available Here, we imported pandas, read in the file—which could take some time, depending on how much memory your system has—and outputted the total number of rows the file has as well as the available headers (e.g., column titles). When ran, you should see: So, there are over six millions rows! No wonder Excel choked. Turn your attention to the list of headers, the first one in particular: This should read . What’s with the extra at the beginning? Well, the actually means that the value is hexadecimal, which is a Byte Order Mark, indicating that the text is Unicode. Why does it matter to us? You cannot assume the files you read are clean. They might contain extra symbols like this that can throw your scripts off. This file is good, in that it is otherwise clean - but many files have missing data, data in internal inconsistent format, etc.. So any time you have a file to analyze, the first thing you must do is clean it. How much cleaning? Enough to allow you to do some analysis. Follow the KISS principle. What sort of cleanup might you require?\n• Fix date/time. The same file might have dates in different formats, like the American (mm-dd-yy) or European (dd-mm-yy) formats. These need to be brought into a common format.\n• Remove any empty values. The file might have blank columns and/or rows, and this will come up as NaN (Not a number) in pandas. pandas provides a simple way to remove these: the function. We saw an example of this in the last blog post.\n• Remove any garbage values that have made their way into the data. These are values which do not make sense (like the byte order mark we saw earlier). Sometimes, it might be possible to work around them. For example, there could be a dataset where the age was entered as a floating point number (by mistake). The function then could be used to make sure all ages are in integer format.\n\nFor those of you who know SQL, you can use the SELECT, WHERE, AND/OR statements with different keywords to refine your search. We can do the same in pandas, and in a way that is more programmer friendly. To start off, let’s find all the accidents that happened on a Sunday. Looking at the headers above, there is a field, which we will use. In the ZIP file you downloaded, there’s a file called Road-Accident-Safety-Data-Guide-1979-2004.xls, which contains extra info on the codes used. If you open it up, you will see that Sunday has the code . Here, we targeted the field and returned a DataFrame with the condition we checked for - . As you can see, there were 693,847 accidents that happened on a Sunday. Let’s make our query more complicated: Find out all accidents that happened on a Sunday and involved more than twenty cars: Run the script. Now we have 10 accidents: Open the Road-Accident-Safety-Data-Guide-1979-2004.xls, and go to the Weather sheet. You’ll see that the code means, “Raining with no heavy winds”. # Accidents which happened on a Sunday, > 20 cars, in the rain \"Accidents which happened on a Sunday involving > 20 cars in the rain: So there were four accidents that happened on a Sunday, involving more than twenty cars, while it was raining: Accidents which happened on a Sunday involving > 20 cars in the rain: 4 We could continue making this more and more complicated, as needed. For now, we’ll stop since our main interest is to look at accidents in London. If you look at Road-Accident-Safety-Data-Guide-1979-2004.xls again, there is a sheet called Police Force. The code for says, “Metropolitan Police”. This is what is more commonly known as Scotland Yard, and is the police force responsible for most (though not all) of London. For our case, this is good enough, and we can extract this info like so: Accidents in London from 1979-2004 on a Sunday: Run the script. This created a new DataFrame with the accidents handled by the “Metropolitan Police” from 1979 to 2004 on a Sunday: Accidents which happened on a Sunday involving > 20 cars in the rain: 4 Accidents in London from 1979-2004 on a Sunday: 114624 What if you wanted to create a new DataFrame that only contains accidents in the year 2000? The first thing we need to do is convert the date format to one which Python can understand using the function. This takes a date in any format and converts it to a format that we can understand (yyyy-mm-dd). Then we can create another DataFrame that only contains accidents for 2000: \"Accidents in London in the year 2000 on a Sunday: When ran, you should see: Accidents which happened on a Sunday involving > 20 cars in the rain: 4 Accidents in London from 1979-2004 on a Sunday: 114624 Accidents in London in the year 2000 on a Sunday: 3889 So, this is a bit confusing at first. Normally, to filter an array you would just use a loop with a conditional: However, you really shouldn’t define your own loop since many high-performance libraries, like pandas, have helper functions in place. In this case, the above code loops over all the elements and filters out data outside the set dates, and then returns the data points that do fall within the dates.\n\nChances are that, while using pandas, everyone else in your organization is stuck with Excel. Want to share the DataFrame with those using Excel? First, we need to do some cleanup. Remember the byte order mark we saw earlier? That causes problems when writing this data to an Excel file - pandas throws a UnicodeDecodeError. Why? Because the rest of the text is decoded as ASCII, but the hexadecimal values can’t be represented in ASCII. We could write everything as Unicode, but remember this byte order mark is an unnecessary (to us) extra we don’t want or need. So we will get rid of it by renaming the column header: This is the way to rename a column in pandas; a bit complicated, to be honest. is needed because we want to modify the existing structure, and not create a copy, which is what pandas does by default. Now we can save the data to Excel: Make sure to install XlsxWriter before running: If all went well, this should have created a file called London_Sundays_2000.xlsx, and then saved our data to Sheet1. Open this file up in Excel or LibreOffice, and confirm that the data is correct.\n\nSo, what did we accomplish? Well, we took a very large file that Excel could not open and utilized pandas to-\n• Create a new XLSX file with a subset of the original data. Keep in mind that even though this file is nearly 800MB, in the age of big data, it’s still quite small. What if you wanted to open a 4GB file? Even if you have 8GB or more of RAM, that might still not be possible since much of your RAM is reserved for the OS and other system processes. In fact, my laptop froze a few times when first reading in the 800MB file. If I opened a 4GB file, it would have a heart attack. Free Bonus: Click here to download an example Python project with source code that shows you how to read large Excel files. The trick is not to open the whole file in one go. That’s what we’ll look at in the next blog post. Until then, analyze your own data. Leave questions or comments below. You can grab the code for this tutorial from the repo."
    },
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/37787698/how-to-sort-pandas-dataframe-by-one-column",
        "document": "There are various parameters one can pass, such as (bool or list of bool):\n\nAs the default is ascending, and OP's goal is to sort ascending, one doesn't need to specify that parameter (see the last note below for the way to solve descending), so one can use one of the following ways:\n• None Performing the operation in-place, and keeping the same variable name. This requires one to pass as follows:\n• None If doing the operation in-place is not a requirement, one can assign the change (sort) to a variable:\n• None With the same name of the original dataframe, as\n• None With a different name, such as , as\n\nAll this previous operations would give the following output\n\nFinally, one can reset the index with , to get the following\n\nA one-liner that sorts ascending, and resets the index would be as follows\n• None If one is not doing the operation in-place, forgetting the steps mentioned above may lead one (as this user) to not be able to get the expected result.\n• None There are strong opinions on using . For that, one might want to read this.\n• None One is assuming that the column is not a string. If it is, one will have to convert it:\n• None If one wants in descending order, one needs to pass as df = df.sort_values(by=['2'], ascending=False) # or df.sort_values(by = '2', ascending=False, inplace=True) [Out]: 0 1 2 2 176.5 December 12.0 9 278.8 November 11.0 10 249.6 October 10.0 11 212.7 September 9.0 1 55.4 August 8.0 5 152 July 7.0 6 238.7 June 6.0 8 283.5 May 5.0 0 354.7 April 4.0 7 104.8 March 3.0 3 95.5 February 2.0 4 85.6 January 1.0"
    },
    {
        "link": "https://stackoverflow.com/questions/69658538/how-to-sort-a-column-by-a-given-list-in-pandas-dataframe",
        "document": "Given this DataFrame and a list:\n\nI would like to reorder df['A'] using the given list so the final output is:\n\nIs there any way so that I could sort a column using a list of String?"
    },
    {
        "link": "https://realpython.com/pandas-sort-python",
        "document": "Learning pandas sort methods is a great way to start with or practice doing basic data analysis using Python. Most commonly, data analysis is done with spreadsheets, SQL, or pandas. One of the great things about using pandas is that it can handle a large amount of data and offers highly performant data manipulation capabilities.\n\nIn this tutorial, you’ll learn how to use and , which will enable you to sort data efficiently in a DataFrame.\n\nBy the end of this tutorial, you’ll know how to:\n• Sort a pandas DataFrame by the values of one or more columns\n• Use the parameter to change the sort order\n• Sort a DataFrame by its using\n• Sort a DataFrame in place using set to\n\nTo follow along with this tutorial, you’ll need a basic understanding of pandas DataFrames and some familiarity with reading in data from files.\n\nAs a quick reminder, a DataFrame is a data structure with labeled axes for both rows and columns. You can sort a DataFrame by row or column value as well as by row or column index. Both rows and columns have indices, which are numerical representations of where the data is in your DataFrame. You can retrieve data from specific rows or columns using the DataFrame’s index locations. By default, index numbers start from zero. You can also manually assign your own index. In this tutorial, you’ll be working with fuel economy data compiled by the US Environmental Protection Agency (EPA) on vehicles made between 1984 and 2021. The EPA fuel economy dataset is great because it has many different types of information that you can sort on, from textual to numeric data types. The dataset contains eighty-three columns in total. To follow along, you’ll need to have the pandas Python library installed. The code in this tutorial was executed using pandas 1.2.0 and Python 3.9.1. Note: The whole fuel economy dataset is around 18 MB. Reading the entire dataset into memory could take a minute or two. Limiting the number of rows and columns will help performance, but it will still take a few seconds before the data is downloaded. For analysis purposes, you’ll be looking at MPG (miles per gallon) data on vehicles by make, model, year, and other vehicle attributes. You can specify which columns to read into your DataFrame. For this tutorial, you’ll need only a subset of the available columns. Here are the commands to read the relevant columns of the fuel economy dataset into a DataFrame and to display the first five rows: By calling with the dataset URL, you’re able to load the data into a DataFrame. Narrowing down the columns results in faster load times and lower memory use. To further limit memory consumption and to get a quick feel for the data, you can specify how many rows to load using . You use to sort values in a DataFrame along either axis (columns or rows). Typically, you want to sort the rows in a DataFrame by the values of one or more columns: The figure above shows the results of using to sort the DataFrame’s rows based on the values in the column. This is similar to how you would sort data in a spreadsheet using a column. You use to sort a DataFrame by its row index or column labels. The difference from using is that you’re sorting the DataFrame based on its row index or column names, not by the values in these rows or columns: The row index of the DataFrame is outlined in blue in the figure above. An index isn’t considered a column, and you typically have only a single row index. The row index can be thought of as the row numbers, which start from zero.\n\nTo sort the DataFrame based on the values in a single column, you’ll use . By default, this will return a new DataFrame sorted in ascending order. It does not modify the original DataFrame. To use , you pass a single argument to the method containing the name of the column you want to sort by. In this example, you sort the DataFrame by the column, which represents city MPG for fuel-only cars: This sorts your DataFrame using the column values from , showing the vehicles with the lowest MPG first. By default, sorts your data in ascending order. Although you didn’t specify a name for the argument you passed to , you actually used the parameter, which you’ll see in the next example. Another parameter of is . By default has set to . If you want the DataFrame sorted in descending order, then you can pass to this parameter: By passing to , you reverse the sort order. Now your DataFrame is sorted in descending order by the average MPG measured in city conditions. The vehicles with the highest MPG values are in the first rows. It’s good to note that pandas allows you to choose different sorting algorithms to use with both and . The available algorithms are , , and . For more information on these different sorting algorithms, check out Sorting Algorithms in Python. The algorithm used by default when sorting on a single column is . To change this to a stable sorting algorithm, use . You can do that with the parameter in or , like this: Using , you set the sorting algorithm to . The previous output used the default algorithm. Looking at the highlighted indices, you can see the rows are in a different order. This is because is not a stable sorting algorithm, but is. Note: In pandas, is ignored when you sort on more than one column or label. When you’re sorting multiple records that have the same key, a stable sorting algorithm will maintain the original order of those records after sorting. For that reason, using a stable sorting algorithm is necessary if you plan to perform multiple sorts.\n\nIn data analysis, it’s common to want to sort your data based on the values of multiple columns. Imagine you have a dataset with people’s first and last names. It would make sense to sort by last name and then first name, so that people with the same last name are arranged alphabetically according to their first names. In the first example, you sorted your DataFrame on a single column named . From an analysis standpoint, the MPG in city conditions is an important factor that could determine a car’s desirability. In addition to the MPG in city conditions, you may also want to look at MPG for highway conditions. To sort by two keys, you can pass a list of column names to : By specifying a list of the column names and , you sort the DataFrame on two columns using . The next example will explain how to specify the sort order and why it’s important to pay attention to the list of column names you use. To sort the DataFrame on multiple columns, you must provide a list of column names. For example, to sort by and , you should create the following list and then pass it to : Now your DataFrame is sorted in ascending order by . If there are two or more identical makes, then it’s sorted by . The order in which the column names are specified in your list corresponds to how your DataFrame will be sorted. Since you’re sorting using multiple columns, you can specify the order by which your columns get sorted. If you want to change the logical sort order from the previous example, then you can change the order of the column names in the list you pass to the parameter: Your DataFrame is now sorted by the column in ascending order, then sorted by if there are two or more of the same model. You can see that changing the order of columns also changes the order in which the values get sorted. Up to this point, you’ve sorted only in ascending order on multiple columns. In the next example, you’ll sort in descending order based on the and columns. To sort in descending order, set to : The values in the column are in reverse alphabetical order, and the values in the column are in descending order for any cars with the same . With textual data, the sort is case sensitive, meaning capitalized text will appear first in ascending order and last in descending order. Sorting by Multiple Columns With Different Sort Orders You might be wondering if it’s possible to sort using multiple columns and to have those columns use different arguments. With pandas, you can do this with a single method call. If you want to sort some columns in ascending order and some columns in descending order, then you can pass a list of Booleans to . In this example, you sort your DataFrame by the , , and columns, with the first two columns sorted in ascending order and sorted in descending order. To do so, you pass a list of column names to and a list of Booleans to : Now your DataFrame is sorted by and in ascending order, but with the column in descending order. This is helpful because it groups the cars in a categorical order and shows the highest MPG cars first.\n\nSorting Your DataFrame on Its Index Before sorting on the index, it’s a good idea to know what an index represents. A DataFrame has an property, which by default is a numerical representation of its rows’ locations. You can think of the index as the row numbers. It helps in quick row lookup and identification. You can sort a DataFrame based on its row index with . Sorting by column values like you did in the previous examples reorders the rows in your DataFrame, so the index becomes disorganized. This can also happen when you filter a DataFrame or when you drop or add rows. To illustrate the use of , start by creating a new sorted DataFrame using : You’ve created a DataFrame that’s sorted using multiple values. Notice how the row index is in no particular order. To get your new DataFrame back to the original order, you can use : Now the index is in ascending order. Just like , the default argument for in is , and you can change to descending order by passing . Sorting on the index has no impact on the data itself as the values are unchanged. This is particularly useful when you’ve assigned a custom index with . If you want to set a custom index using the and columns, then you can pass a list to : Using this method, you replace the default integer-based row index with two axis labels. This is considered a or a hierarchical index. Your DataFrame is now indexed by more than one key, which you can sort on with : First you assign a new index to your DataFrame using the and columns, then you sort the index using . You can read more on using in the pandas documentation. For the next example, you’ll sort your DataFrame by its index in descending order. Remember from sorting your DataFrame with that you can reverse the sort order by setting to . This parameter also works with , so you can sort your DataFrame in reverse order like this: Now your DataFrame is sorted by its index in descending order. One difference between using and is that has no parameter since it sorts a DataFrame on the row index by default. There are many cases in data analysis where you want to sort on a hierarchical index. You’ve already seen how you can use and in a . For this dataset, you could also use the column as an index. Setting the column as the index could be helpful in linking related datasets. For example, the EPA’s emissions dataset also uses to represent vehicle record IDs. This links the emissions data to the fuel economy data. Sorting the index of both datasets in DataFrames could speed up using other methods such as . To learn more about combining data in pandas, check out Combining Data in Pandas With merge(), .join(), and concat().\n\nSorting the Columns of Your DataFrame You can also use the column labels of your DataFrame to sort row values. Using with the optional parameter set to will sort the DataFrame by the column labels. The sorting algorithm is applied to the axis labels instead of to the actual data. This can be helpful for visual inspection of the DataFrame. When you use without passing any explicit arguments, it uses as a default argument. The axis of a DataFrame refers to either the index ( ) or the columns ( ). You can use both axes for indexing and selecting data in a DataFrame as well as for sorting the data. You can also use the column labels of a DataFrame as the sorting key for . Setting to sorts the columns of your DataFrame based on the column labels: The columns of your DataFrame are sorted from left to right in ascending alphabetical order. If you want to sort the columns in descending order, then you can use : Using in , you sorted the columns of your DataFrame in both ascending and descending order. This could be more useful in other datasets, such as one in which the column labels correspond to months of the year. In that case, it would make sense to arrange your data in ascending or descending order by month.\n\nWorking With Missing Data When Sorting in Pandas Oftentimes real-world data has many imperfections. While pandas has several methods you can use to clean your data before sorting, sometimes it’s nice to see which data is missing while you’re sorting. You can do that with the parameter. The subset of the fuel economy data used for this tutorial doesn’t have missing values. To illustrate the use of , first you’ll need to create some missing data. The following piece of code creates a new column based on the existing column, mapping where equals and where it doesn’t: Now you have a new column named that contains both and values. You’ll use this column to see what effect has when you use the two sort methods. To find out more about using , you can read Pandas Project: Make a Gradebook With Python & Pandas. accepts a parameter named , which helps to organize missing data in the column you’re sorting on. If you sort on a column with missing data, then the rows with the missing values will appear at the end of your DataFrame. This happens regardless of whether you’re sorting in ascending or descending order. Here’s what your DataFrame looks like when you sort on the column with missing data: To change that behavior and have the missing data appear first in your DataFrame, you can set to . The parameter only accepts the values , which is the default, and . Here’s how to use in : Now any missing data from the columns you used to sort on will be shown at the top of your DataFrame. This is most helpful when you’re first starting to analyze your data and are unsure if there are missing values. also accepts . Your DataFrame typically won’t have values as a part of its index, so this parameter is less useful in . However, it’s good to know that if your DataFrame does have in either the row index or a column name, then you can quickly identify this using and . By default, this parameter is set to , which places values at the end of the sorted result. To change that behavior and have the missing data first in your DataFrame, set to .\n\nUsing Sort Methods to Modify Your DataFrame In all the examples you’ve seen so far, both and have returned DataFrame objects when you called those methods. That’s because sorting in pandas doesn’t work in place by default. In general, this is the most common and preferred way to analyze your data with pandas since it creates a new DataFrame instead of modifying the original. This allows you to preserve the state of the data from when you read it from your file. However, you can modify the original DataFrame directly by specifying the optional parameter with the value of . The majority of pandas methods include the parameter. Below, you’ll see a few examples of using to sort your DataFrame in place. With set to , you modify the original DataFrame, so the sort methods return . Sort your DataFrame by the values of the column like the very first example, but with set to : Notice how calling doesn’t return a DataFrame. Here’s what the original looks like: In the object, the values are now sorted in ascending order based on the column. Your original DataFrame has been modified, and the changes will persist. It’s generally a good idea to avoid using for analysis because the changes to your DataFrame can’t be undone. The next example illustrates that also works with . Since the index was created in ascending order when you read your file into a DataFrame, you can modify your object again to get it back to its initial order. Use with set to to modify the DataFrame: Now your DataFrame has been modified again using . Since your DataFrame still has its default index, sorting it in ascending order puts the data back into its original order. If you’re familiar with Python’s built-in functions and , then the parameter available in the pandas sort methods might feel very similar. For more information, you can check out How to Use sorted() and .sort() in Python.\n\nYou now know how to use two core methods of the pandas library: and . With this knowledge, you can perform basic data analysis with a DataFrame. While there are a lot of similarities between these two methods, seeing the difference between them makes it clear which one to use for different analytical tasks. In this tutorial, you’ve learned how to:\n• Sort a pandas DataFrame by the values of one or more columns\n• Use the parameter to change the sort order\n• Sort a DataFrame by its using\n• Sort a DataFrame in place using set to These methods are a big part of being proficient with data analysis. They’ll help you build a strong foundation on which you can perform more advanced pandas operations. If you want to see some examples of more advanced uses of pandas sort methods, then the pandas documentation is a great resource."
    },
    {
        "link": "https://pandas.pydata.org/pandas-docs/version/1.3/reference/api/pandas.DataFrame.sort_values.html",
        "document": "Name or list of names to sort by.\n• None if is 0 or then may contain index levels and/or column labels.\n• None if is 1 or then may contain column levels and/or index labels.\n\nSort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by.\n\nChoice of sorting algorithm. See also for more information. and are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label.\n\nPuts NaNs at the beginning if ; puts NaNs at the end.\n\nIf True, the resulting axis will be labeled 0, 1, …, n - 1.\n\nApply the key function to the values before sorting. This is similar to the argument in the builtin function, with the notable difference that this function should be vectorized. It should expect a and return a Series with the same shape as the input. It will be applied to each column in independently."
    }
]