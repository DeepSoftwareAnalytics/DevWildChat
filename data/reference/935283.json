[
    {
        "link": "https://pypi.org/project/yfinance",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://interactivebrokers.com/campus/ibkr-quant-news/yfinance-library-a-complete-guide",
        "document": "The article “yfinance Library – A Complete Guide” first appeared on AlgoTrading101 Blog.\n\nyfinance is a popular open source library developed by Ran Aroussi as a means to access the financial data available on Yahoo Finance.\n\nYahoo Finance offers an excellent range of market data on stocks, bonds, currencies and cryptocurrencies. It also offers market news, reports and analysis and additionally options and fundamentals data- setting it apart from some of it’s competitors.\n\nYahoo Finance used to have their own official API, but this was decommissioned on May 15th 2017, following wide-spread misuse of data.\n\nThese days a range of unofficial APIs and libraries exist to access the same data, including of course yfinance.\n\nNote you might know of yfinance under it’s old name- fix-yahoo-finance, since it was re-named on May 26th 2019 at the same time that it went over a large overhaul to fix some usability issues.\n\nTo ensure backwards compatibility, fix-yahoo-finance now imports and uses yfinance anyway, but Ran Aroussi still recommends to install and use yfinance directly.\n\nIn this article we will focus mainly on the yfinance library, but we discuss the overall range of options and other alternative providers in more depth in our parent article, Yahoo Finance API – A Complete Guide.\n\nYes, yfinance is completely open source and free. You can find the documentation here.\n\nWhy should I use the yfinance library?\n• Quick and easy to set yourself up\n\nAs we have just mentioned yfinance is completely open source and free. There are other ways to access the Yahoo Finance data, some free and some paid, and there are certain benefits to some of the options that require paying, like being ensured a degree of maintenance to the solution, but everybody loves free!\n\nInstallation couldn’t be quicker or easier. yfinance has just 4 dependencies, all of which come with Anaconda anyway, and installs fully in a single line of code. No account creation required, or signing up for and using API keys!\n\nIts simple. yfinance is highly Pythonic in it’s design and incredibly streamlined. It’s as easy as creating a ticker object for a particular ticker/list of tickers and then just calling all the methods on this object. Like this:\n\nDon’t worry, we’ll break down that code further in a bit!\n\nFurthermore, the documentation is concise- fitting on a single page, and the method names are very self explanatory.\n\nHigh granularity of data. One cool feature of yfinance is that you can get highly refined data, all the way down to 5 minute, 3 minute and even 1 minute data! The full range of intervals available are:\n\nHowever it is important to note that the 1m data is only retrievable for the last 7 days, and anything intraday (interval <1d) only for the last 60 days.\n\nyfinance also handily returns data directly in padas dataframes or series. This is on contrast to some other options to access Yahoo Finance’s data where you will get lengthy JSONs you need parse for the specific information you want, and will have to manually convert to data-frames yourself.\n\n» Here are some alternative (mostly) free data sources and guides:\n\nWhy shouldn’t I use the yfinance library?\n• Can get yourself rate limited/blacklisted\n\nLacks specialised features. Despite the fact you can use it to get a good range of core data, including options and fundamentals data, yfinance doesn’t provide a method to scrape any of the news reports/analysis that are available on Yahoo Finance.\n\nThis obviously isn’t ideal if you want to build model that relies in part on sentiment analysis, so if you want that sort of data, you might want to check out RapidAPI (which will talk about more shortly) that does offer such data.\n\nAlso, other market data alternatives often include interesting extras. For example Alpha Vantage provides modules that calculate various technical analysis indicators for you- obviously an enormous effort save if you want to build an algorithm utilising any of them! yfinance just provides the basics.\n\nSome methods are fragile. yfinance mainly makes API calls to Yahoo Finance to gather it’s data, but it does occasionally employ HTML scraping and pandas tables scraping to unofficially gather the information off the Yahoo Finance website for some of it’s methods. As such, the functionality of some of it’s methods is at the mercy of Yahoo not changing the layout or design of some of their pages. In fact, yfinance is widely known to already have a few issues.\n\nAs a quick aside, data scraping works by simply downloading the HTML code of a web page, and searching through all the HTML tags to find the specific elements of a page you want.\n\nFor instance below is the Yahoo Finance Apple (‘AAPL’) historical data page:\n\nIf the method to get the historical data HTML scraped, it would be searching the various div, class and tr tags etc. for various IDs to pick out the data that should be returned.\n\nFor instance the class ID “Py(10px) Pstart(10px)” refers to the historical prices populating the table. If in this case Yahoo Finance was to change the class ID pointing to this value, the method might return completely incorrect data, or even nothing at all. Again, this sort of vulnerability doesn’t apply to all of yfinance’s methods- most of them do in fact make direct API calls- but it does affect a few.\n\nIt’s an unofficial solution. Again, because yfinance is simply the result of one man’s hard work and not in any way affiliated with Yahoo Finance, there’s no guarantee if it breaks it will be maintained.\n\nAs we already mentioned it did have a big update to fix issues on May 26th 2019 on the same day it was renamed, but that’s no guarantee problems will be fixed in the future. Are you sure you want to build a trading algorithm on-top of data that might one day suddenly and without warning be wrong? There are already a few known issues with yfinance, which we will highlight later on in this article.\n\nYou can get yourself rate limited/blacklisted. Again because yfinance scrapes data for a few of it’s functions, you sometimes run the risk of getting rate limited or blacklisted for too many scraping attempts.\n\nThis is a risk that’s always present when trying to scrape websites, but when you’re building applications trading real money on-top of infrastructure that might be making a lot of data requests, the risk:reward changes.\n\nOverall yfinance an incredibly beginner friendly option. You’ll be able to dive right in and test out ideas without wasting time puzzling over complex documentation whilst still having access to a good range of data!\n\nThat said, the risk of getting faulty data or being blocked from getting any data at all when employing algorithms trading real money is absolutely unacceptable.\n\nWe think yfinance is great for prototyping, or if you are beginner, or just want to download a bunch of historic data.\n\nBut if you want complete confidence that a serious trading system is going to function with total reliability, we’d absolutely recommend going with a official and alternative market data provider- preferably one claiming to provide low latency data directly from exchanges.\n\nInformation posted on IBKR Campus that is provided by third-parties does NOT constitute a recommendation that you should contract for the services of that third party. Third-party participants who contribute to IBKR Campus are independent of Interactive Brokers and Interactive Brokers does not make any representations or warranties concerning the services offered, their past or future performance, or the accuracy of the information provided by the third party. Past performance is no guarantee of future results. This material is from AlgoTrading101 and is being posted with its permission. The views expressed in this material are solely those of the author and/or AlgoTrading101 and Interactive Brokers is not endorsing or recommending any investment or trading discussed in the material. This material is not and should not be construed as an offer to buy or sell any security. It should not be construed as research or investment advice or a recommendation to buy, sell or hold any security or commodity. This material does not and is not intended to take into account the particular financial conditions, investment objectives or requirements of individual customers. Before acting on this material, you should consider whether it is suitable for your particular circumstances and, as necessary, seek professional advice."
    },
    {
        "link": "https://algotrading101.com/learn/yfinance-guide",
        "document": "\n• Why should I use the yfinance library?\n• Why shouldn’t I use the yfinance library?\n• What are some of the alternatives to the yfinance library?\n• How do I get started with the yfinance library?\n• How do I download historical data using the yfinance library?\n• How do I download fundamental data using the yfinance library?\n• Fundamentals data with multiple tickers at once\n• How do I download trading data using the yfinance library?\n• How do I download options data using the yfinance library?\n• How do I get Expiration dates?\n• How do I get Calls Data?\n• How do I get Puts Data?\n\nyfinance is a popular open source library developed by Ran Aroussi as a means to access the financial data available on Yahoo Finance.\n\nYahoo Finance offers an excellent range of market data on stocks, bonds, currencies and cryptocurrencies. It also offers market news, reports and analysis and additionally options and fundamentals data- setting it apart from some of it’s competitors.\n\nYahoo Finance used to have their own official API, but this was decommissioned on May 15th 2017, following wide-spread misuse of data.\n\nThese days a range of unofficial APIs and libraries exist to access the same data, including of course yfinance.\n\nNote you might know of yfinance under it’s old name- fix-yahoo-finance, since it was re-named on May 26th 2019 at the same time that it went over a large overhaul to fix some usability issues.\n\nTo ensure backwards compatibility, fix-yahoo-finance now imports and uses yfinance anyway, but Ran Aroussi still recommends to install and use yfinance directly.\n\nIn this article we will focus mainly on the yfinance library, but we discuss the overall range of options and other alternative providers in more depth in our parent article, Yahoo Finance API – A Complete Guide.\n\nYes, yfinance is completely open source and free. You can find the documentation here.\n\nWhy should I use the yfinance library?\n• Quick and easy to set yourself up\n\nAs we have just mentioned yfinance is completely open source and free. There are other ways to access the Yahoo Finance data, some free and some paid, and there are certain benefits to some of the options that require paying, like being ensured a degree of maintenance to the solution, but everybody loves free!\n\nInstallation couldn’t be quicker or easier. yfinance has just 4 dependencies, all of which come with Anaconda anyway, and installs fully in a single line of code. No account creation required, or signing up for and using API keys!\n\nIts simple. yfinance is highly Pythonic in it’s design and incredibly streamlined. It’s as easy as creating a ticker object for a particular ticker/list of tickers and then just calling all the methods on this object. Like this:\n\nDon’t worry, we’ll break down that code further in a bit!\n\nFurthermore, the documentation is concise- fitting on a single page, and the method names are very self explanatory.\n\nHigh granularity of data. One cool feature of yfinance is that you can get highly refined data, all the way down to 5 minute, 3 minute and even 1 minute data! The full range of intervals available are:\n\nHowever it is important to note that the 1m data is only retrievable for the last 7 days, and anything intraday (interval <1d) only for the last 60 days.\n\nyfinance also handily returns data directly in padas dataframes or series. This is on contrast to some other options to access Yahoo Finance’s data where you will get lengthy JSONs you need parse for the specific information you want, and will have to manually convert to data-frames yourself.\n\nWhy shouldn’t I use the yfinance library?\n• Can get yourself rate limited/blacklisted\n\nLacks specialised features. Despite the fact you can use it to get a good range of core data, including options and fundamentals data, yfinance doesn’t provide a method to scrape any of the news reports/analysis that are available on Yahoo Finance.\n\nThis obviously isn’t ideal if you want to build model that relies in part on sentiment analysis, so if you want that sort of data, you might want to check out RapidAPI (which will talk about more shortly) that does offer such data.\n\nAlso, other market data alternatives often include interesting extras. For example Alpha Vantage provides modules that calculate various technical analysis indicators for you- obviously an enormous effort save if you want to build an algorithm utilising any of them! yfinance just provides the basics.\n\nSome methods are fragile. yfinance mainly makes API calls to Yahoo Finance to gather it’s data, but it does occasionally employ HTML scraping and pandas tables scraping to unofficially gather the information off the Yahoo Finance website for some of it’s methods. As such, the functionality of some of it’s methods is at the mercy of Yahoo not changing the layout or design of some of their pages. In fact, yfinance is widely known to already have a few issues.\n\nAs a quick aside, data scraping works by simply downloading the HTML code of a web page, and searching through all the HTML tags to find the specific elements of a page you want.\n\nFor instance below is the Yahoo Finance Apple (‘AAPL’) historical data page:\n\nIf the method to get the historical data HTML scraped, it would be searching the various div, class and tr tags etc. for various IDs to pick out the data that should be returned.\n\nFor instance the class ID “Py(10px) Pstart(10px)” refers to the historical prices populating the table. If in this case Yahoo Finance was to change the class ID pointing to this value, the method might return completely incorrect data, or even nothing at all. Again, this sort of vulnerability doesn’t apply to all of yfinance’s methods- most of them do in fact make direct API calls- but it does affect a few.\n\nIt’s an unofficial solution. Again, because yfinance is simply the result of one man’s hard work and not in any way affiliated with Yahoo Finance, there’s no guarantee if it breaks it will be maintained.\n\nAs we already mentioned it did have a big update to fix issues on May 26th 2019 on the same day it was renamed, but that’s no guarantee problems will be fixed in the future. Are you sure you want to build a trading algorithm on-top of data that might one day suddenly and without warning be wrong? There are already a few known issues with yfinance, which we will highlight later on in this article.\n\nYou can get yourself rate limited/blacklisted. Again because yfinance scrapes data for a few of it’s functions, you sometimes run the risk of getting rate limited or blacklisted for too many scraping attempts.\n\nThis is a risk that’s always present when trying to scrape websites, but when you’re building applications trading real money on-top of infrastructure that might be making a lot of data requests, the risk:reward changes.\n\nOverall yfinance an incredibly beginner friendly option. You’ll be able to dive right in and test out ideas without wasting time puzzling over complex documentation whilst still having access to a good range of data!\n\nThat said, the risk of getting faulty data or being blocked from getting any data at all when employing algorithms trading real money is absolutely unacceptable.\n\nWe think yfinance is great for prototyping, or if you are beginner, or just want to download a bunch of historic data.\n\nBut if you want complete confidence that a serious trading system is going to function with total reliability, we’d absolutely recommend going with a official and alternative market data provider- preferably one claiming to provide low latency data directly from exchanges.\n\nWhat are some of the alternatives to the yfinance library?\n\nOf the two alternatives to yfinance we will consider, RapidAPI is the most distinct.\n\nFirstly, whilst it does still have a limited usage free tier, you will have to pay for anything over 500 requests per month:\n\nSecondly, its not quite as simple as yfinance to get started with. You will have to sign up for an account to get your own access API keys.\n\nThat said, a big plus of RapidAPI is that you can use it with 15 different languages, if for some reason Python isn’t your thing:\n\nIt also offers more range of data than our other options, specifically the option to download market news and analysis which is fantastically useful if you want to add a degree of sentiment analysis in your model!\n\nMaking snap trading decisions based on machine scanning of news far faster than a human ever could can be one way (if slightly uncertain) to gain a trading edge.\n\nThat said RapidAPI does have a few drawbacks.\n\nAs you can see requests have an average latency of 1660ms which isn’t terrible, but alternative data providers such as polygon.io offer anything from 200ms down to 1ms delays- quite the difference.\n\nMore concerning is the fact requests only have a 98% success rate. Having 1 in 50 data requests fail could be a big deal if you have a system trading real money, especially if you are making a lower frequency of calls. Definitely something to consider.\n\nResults returned can also be in quite lengthy and nested JSONs, making the data a bit trickier to get ready for use than when using yfinance:\n\nThat said a further plus of RapidAPI is that it offers a huge range of APIs for other purposes, so familiarising yourself with how to use the their API for Yahoo Finance data might carry over into easily using another of their APIs for a different project in the future.\n\nIn summary, RapidAPI offers a very limited free tier, but perhaps by using a solution where some people are paying, it is more likely that any scraping issues from Yahoo Finance structure changes are resolved more quickly.\n\nIts also fiddlier to use and harder get started with, but does provide a bigger range of data than our other two options.\n\nyahoo_fin is an open source and free library similar to yfinance.\n\nYou can find the documentation here.\n\nIt offers a similar range of data to yfinance, but notably has a few functions that generate all the tickers for certain markets for you:\n\nwhich is a useful feature yfinance lacks.\n\nWe actually focus on the yahoo_fin library in the example sections of our parent article, Yahoo Finance API – A Complete Guide, so we won’t talk about it anymore here.\n\nHow do I get started with the yfinance library?\n\nGetting started with the yfinance library is super easy.\n\nIt has the following dependencies:\n\nThese all come as standard in an installation with Anaconda, but are really easy to install manually if for some reason you don’t have them.\n\nAfter that its as easy as:\n\nThe layout itself is also really simple, there are just three modules:\n\nAlmost all the methods are in the Tickers module.\n\nThe download module is for rapidly downloading the historical data of multiple tickers at once.\n\nAnd pandas_datareader is for back compatibility with legacy code, which we will ignore as irrelevant since if you’re reading this you are probably a new user of the library!\n\nHow do I download historical data using the yfinance library?\n\nFirstly, lets import yfinance as yf and create ourselves a ticker object for a particular ticker (stock):\n\nRemember we now use this aapl ticker object for almost everything- calling various methods on it.\n\nTo get the historical data we want to use the history() method, which is the most “complicated” method in the yfinance library.\n\nIt takes the following parameters as input:\n• period: data period to download (either use period parameter or use start and end) Valid periods are:\n• interval: data interval (1m data is only for available for last 7 days, and data interval <1d for the last 60 days) Valid intervals are:\n• start: If not using period – in the format (yyyy-mm-dd) or datetime.\n• end: If not using period – in the format (yyyy-mm-dd) or datetime.\n• prepost: Include Pre and Post regular market data in results? (Default is )- no need usually to change this from False\n• auto_adjust: Adjust all OHLC (Open/High/Low/Close prices) automatically? (Default is )- just leave this always as true and don’t worry about it\n\nThat might look a little complex but mainly you will just be changing the period (or start and end) and interval parameters.\n\nSo as an example, to get 1minute historical data for Apple between 02/06/2020 and 07/06/2020 (British format) we just use the ticker object we created and run:\n\nIt’s as simple as that!\n\nTo download the historical data for multiple tickers at once you can use the download module.\n\nIt takes mostly the same arguments as the history() method on a ticker object, but additionally:\n• group_by: group by column or ticker (‘column’/’ticker’, default is ‘column’)\n• proxy: proxy URL if you want to use a proxy server for downloading the data (optional, default is None)\n\nFor example to get the data for Amazon, Apple and Google all at once we can run:\n\nNote that the default with no interval specified is daily data.\n\nThen, if we want to group by ticker instead of Open/High/Low/Close we can do:\n\nHow do I download fundamental data using the yfinance library?\n\nYou can get the price to earnings ratio with the Ticker.info() method.\n\nTicker.info() returns a dictionary with a wide range of information about a ticker, including such things as a summary description, employee count, marketcap, volume, P/E ratios, dividends etc.- we recommend taking a look at it yourself as it takes a lot of space to show, but in short if you can’t find the information you’re looking for with the other methods, try the info() method!\n\nTo get specifically the price to earnings ratio search the dictionary for ‘forwardPE’:\n\nYou can get the yearly dividend % also by using info():\n\nAnd if you want a breakdown of each dividend payout as it occurred and on what date, you can use Ticker.dividends():\n\nFundamentals data with multiple tickers at once\n\nWe might also want to grab fundamentals (or other) data for a bunch of tickers at once.\n\nLets have a go at doing that and then try comparing our tickers by a particular attribute!\n\nTo do this we can start by creating a list of the tickers we want to get data for, and an empty dictionary to store all the data.\n\nWe will need to use the pandas library to manipulate the data frames:\n\nWe then loop through the list of the tickers, in each case adding to our dictionary a key, value pair where the key is the ticker and the value the dataframe returned by the info() method for that ticker:\n\nWe then combine this dictionary of dataframes into a single dataframe:\n\nAnd then delete the unnecessary “level_1” column and clean up the column names:\n\nGreat, so we now know how to get any data we want for multiple tickers at once into the same dataframe!\n\nBut how do we easily compare by a particular attribute?\n\nIt’s quite easy actually, lets try for one of the attributes in info()– the fullTimeEmployees count:\n\nSo now we have a dataframe of just the employee counts- one entry per ticker- and we can now order by the ‘Recent’ column:\n\nBoom! Obviously not that required with only 5 tickers in our list, but a fantastically easy and powerful way to quickly compare by a particular attribute if we had the ticker list of an entire market!\n\nYou can easily use this exact same method to compare any attribute you want!\n\nHow do I download trading data using the yfinance library?\n\nYou can find the data for all three of Market Cap, Volume and Highs and Lows from the info() method.\n\nTo get the market cap, use:\n\nTo find the current volume do:\n\nIf you want the average volume over the last 24 hours do:\n\nAnd finally if you want the average volume over the last 10 days:\n\nRemember, you can find the highs and lows for any time interval:\n\nwithin a desired period by using the history() method and adjusting the interval.\n\nFor example, to get the weekly highs and lows for all the historical data that exists, use:\n\nJust filter the dataframe with:\n\nAnd so forth to get the individual columns.\n\nAlternatively, you can use info() to get the following useful high/low information:\n\nHow do I download options data using the yfinance library?\n\nBriefly, options are contracts giving a trader the right, but not the obligation, to buy (call) or sell (put) the underlying asset they represent at a specific price on or before a certain date.\n\nTo download options data we can use the option_chain() method. It takes the parameter as input:\n• date: (YYYY-MM-DD), expiry date. If None return all options data.\n\nAnd has the opt.calls and opt.puts methods.\n\nTo get the various expiry dates for options for a particular ticker it’s as easy as:\n\nHow do I get Calls Data?\n\nTo get the calls data, we can do:\n\nHow do I get Puts Data?\n\nTo get puts data, we do:\n\nFinally, opts by itself returns a ticker object containing both the calls and puts data together, if that’s useful to you!\n\nAs we highlighted near the beginning of this article, yfinance is an unofficial scraping solution to gather data from Yahoo Finance, so is subject to breaking if Yahoo Finance changes any of its layout.\n\nUnfortunately this already seems to have happened in part, with the following problems discovered when writing this guide:\n• Tickers, the multiple tickers object for interacting with multiple tickers at once, doesn’t seem to work. We have provided a more manual workaround for this in the Fundamentals data with multiple tickers at once section.\n• The financials, quarterly_financials, balance_sheet, quarterly_balance_sheet, cashflow, quarterly_cashflow, earnings, quarterly_earnings Ticker methods do not work and return empty dataframes.\n\nThis is a big problem as in many cases there is no alternative way to the data in some of these methods from other methods in yfinance.\n\nIf you are building something that requires any of this data, for example balance sheets and income and cashflow statements and still want free access to the Yahoo Finance data, check out the yahoo_fin library in the examples section of our guide https://algotrading101.com/learn/yahoo-finance-api/ which has working methods to get all of this data!\n\nSo clearly as we have just demonstrated, yfinance is NOT a safe bet to build critical infrastructure on.\n\nIf you want to build algorithms trading real money, we absolutely recommend you use an official data source/API, preferably one connected directly to exchange data and with low latency. Something like Polygon.io or IEX might suit you better.\n\nIf you absolutely HAVE to use the Yahoo Finance data specifically, we recommend at least paying for an unofficial API like RapidAPI, where you stand a good bet there is an active team of developers constantly maintaining the API. Remember RapidAPI does still have a limited usage free tier!\n\nThat said, yfinance can be good to use to build test applications as a beginner, as the sections of it that do work are fantastically easy to get started with and use.\n\nA particular forte of yfinance is that the threads parameter of yf.download does allow very rapid downloading of historical for multiple tickers when set to True!\n\nYou can find the code used in this article here."
    },
    {
        "link": "https://github.com/ranaroussi/yfinance",
        "document": "yfinance offers a Pythonic way to fetch financial & market data from Yahoo!Ⓡ finance.\n\nYahoo!, Y!Finance, and Yahoo! finance are registered trademarks of Yahoo, Inc. yfinance is not affiliated, endorsed, or vetted by Yahoo, Inc. It's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes. You should refer to Yahoo!'s terms of use (here, here, and here) **for details on your rights to use the actual data downloaded. Remember - the Yahoo! finance API is intended for personal use only.**\n\nThe list of changes can be found in the Changelog\n\nrelies on the community to investigate bugs, review code, and contribute code. Developer guide: #1084\n\nyfinance is distributed under the Apache Software License. See the LICENSE.txt file in the release for details.\n\nAGAIN - yfinance is not affiliated, endorsed, or vetted by Yahoo, Inc. It's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes. You should refer to Yahoo!'s terms of use (here, here, and here) for details on your rights to use the actual data downloaded.\n\nPlease drop me a note with any feedback you have."
    },
    {
        "link": "https://aroussi.com/post/python-yahoo-finance",
        "document": "Ever since Yahoo decommissioned their historical data API, Python developers looked for a reliable workaround. As a result, my library, yfinance, gained momentum and gets 3M+ installs per month, acording to PyPi!\n\nYahoo!, Y!Finance, and Yahoo! finance are registered trademarks of Yahoo, Inc.\n\nyfinance is not affiliated, endorsed, or vetted by Yahoo, Inc. It's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes.\n\nYou should refer to Yahoo!'s terms of use (here, here, and here) for details on your rights to use the actual data downloaded. Remember - the Yahoo! finance API is intended for personal use only.\n\naimed to offer a temporary fix to the problem by getting data from Yahoo! Finance and returning it in the same format as pandas_datareader's , thus keeping the code changes in exisiting software to minimum.\n\nThe problem was, that this hack was a bit unreliable, causing data to not being downloaded and required developers to force session re-initialization and re-fetching of cookies, by calling .\n\nyfinance is a complete re-write of the libray, offering a reliable method of downloading historical market data from Yahoo! Finance's API, up to 1 minute granularity, in a more Pythonic way.\n\nThe module allows you get market and meta data for a security, using a Pythonic way:\n\nAvailable paramaters for the method are:\n• period: data period to download (Either Use period parameter or use start and end) Valid periods are: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n• interval: data interval (intraday data cannot extend last 60 days) Valid intervals are: 1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo\n• start: If not using period - Download start date string (YYYY-MM-DD) or datetime.\n• end: If not using period - Download end date string (YYYY-MM-DD) or datetime.\n• prepost: Include Pre and Post market data in results? (Default is )\n\nYou can also download data for multiple tickers at once, like before.\n\nTo access the closing price data for SPY, you should use: .\n\nIf, however, you want to group data by Symbol, use:\n\nTo access the closing price data for SPY, you should use: .\n\nThe method accepts an additional parameter - for faster completion when downloading a lot of symbols at once.\n\n* NOTE: To keep compatibility with older versions, auto_adjust defaults to when using mass-download.\n\nIf your legacy code is using and you wand to keep the code changes to minimum, you can simply call the override method and keep your code as it was:\n\nThe Github repository has more information and issue tracking."
    },
    {
        "link": "https://pandas.pydata.org/docs/getting_started/intro_tutorials/09_timeseries.html",
        "document": "Aggregate the current hourly time series values to the monthly maximum value in each of the stations.\n\nA very powerful method on time series data with a datetime index, is the ability to time series to another frequency (e.g., converting secondly data into 5-minutely data)."
    },
    {
        "link": "https://dataquest.io/blog/tutorial-time-series-analysis-with-pandas",
        "document": "In this tutorial, we will learn about the powerful time series tools in the pandas library. And we'll learn to make cool charts like this!\n\nOriginally developed for financial time series such as daily stock market prices, the robust and flexible data structures in pandas can be applied to time series data in any domain, including business, science, engineering, public health, and many others. With these tools you can easily organize, transform, analyze, and visualize your data at any level of granularity — examining details during specific time periods of interest, and zooming out to explore variations on different time scales, such as monthly or annual aggregations, recurring patterns, and long-term trends.\n\nIn the broadest definition, a time series is any data set where the values are measured at different points in time. Many time series are uniformly spaced at a specific frequency, for example, hourly weather measurements, daily counts of web site visits, or monthly sales totals. Time series can also be irregularly spaced and sporadic, for example, timestamped data in a computer system's event log or a history of 911 emergency calls. Pandas time series tools apply equally well to either type of time series.\n\nThis tutorial will focus mainly on the data wrangling and visualization aspects of time series analysis. Working with a time series of energy data, we'll see how techniques such as time-based indexing, resampling, and rolling windows can help us explore variations in electricity demand and renewable energy supply over time. We'll be covering the following topics:\n\nWe'll be using Python 3.6, pandas, matplotlib, and seaborn. To get the most out of this tutorial, you'll want to be familiar with the basics of pandas and matplotlib.\n\nNot quite there yet? Build your foundational Python skills with our Python for Data Science: Fundamentals and Intermediate courses.\n\nIn this tutorial, we'll be working with daily time series of Open Power System Data (OPSD) for Germany, which has been rapidly expanding its renewable energy production in recent years. The data set includes country-wide totals of electricity consumption, wind power production, and solar power production for 2006-2017. You can download the data here.\n\nElectricity production and consumption are reported as daily totals in gigawatt-hours (GWh). The columns of the data file are:\n• — Sum of wind and solar power production in GWh\n\nWe will explore how electricity consumption and production in Germany have varied over time, using pandas time series tools to answer questions such as:\n• When is electricity consumption typically highest and lowest?\n• How do wind and solar power production vary with seasons of the year?\n• What are the long-term trends in electricity consumption, solar power, and wind power?\n• How do wind and solar power production compare with electricity consumption, and how has this ratio changed over time?\n\nBefore we dive into the OPSD data, let's briefly introduce the main pandas data structures for working with dates and times. In pandas, a single point in time is represented as a Timestamp. We can use the function to create Timestamps from strings in a wide variety of date/time formats. Let's import pandas and convert a few dates and times to Timestamps.\n\nAs we can see, automatically infers a date/time format based on the input. In the example above, the ambiguous date is assumed to be month/day/year and is interpreted as July 8, 1952. Alternatively, we can use the parameter to tell pandas to interpret the date as August 7, 1952.\n\nIf we supply a list or array of strings as input to , it returns a sequence of date/time values in a DatetimeIndex object, which is the core data structure that powers much of pandas time series functionality.\n\nIn the DatetimeIndex above, the data type indicates that the underlying data is stored as -bit integers, in units of nanoseconds (ns). This data structure allows pandas to compactly store large sequences of date/time values and efficiently perform vectorized operations using NumPy datetime64 arrays.\n\nIf we're dealing with a sequence of strings all in the same date/time format, we can explicitly specify it with the parameter. For very large data sets, this can greatly speed up the performance of compared to the default behavior, where the format is inferred separately for each individual string. Any of the format codes from the and functions in Python's built-in datetime module can be used. The example below uses the format codes In addition to Timestamp and DatetimeIndex objects representing individual points in time, pandas also includes data structures representing durations (e.g., 125 seconds) and periods (e.g., the month of November 2018). For more about these data structures, there is a nice summary here. In this tutorial we will use DatetimeIndexes, the most common data structure for pandas time series. To work with time series data in pandas, we use a DatetimeIndex as the index for our DataFrame (or Series). Let's see how to do this with our OPSD data set. First, we use the function to read the data into a DataFrame, and then display its shape. The DataFrame has 4383 rows, covering the period from January 1, 2006 through December 31, 2017. To see what the data looks like, let's use the and methods to display the first three and last three rows. Next, let's check out the data types of each column. Now that the column is the correct data type, let's set it as the DataFrame's index. Alternatively, we can consolidate the above steps into a single line, using the and parameters of the function. This is often a useful shortcut. Now that our DataFrame's index is a DatetimeIndex, we can use all of pandas' powerful time-based indexing to wrangle and analyze our data, as we shall see in the following sections. Another useful aspect of the DatetimeIndex is that the individual date/time components are all available as attributes such as , , , and so on. Let's add a few more columns to , containing the year, month, and weekday name. # Add columns with year, month, and weekday name opsd_daily['Year'] = opsd_daily.index.year opsd_daily['Month'] = opsd_daily.index.month opsd_daily['Weekday Name'] = opsd_daily.index.weekday_name # Display a random sampling of 5 rows opsd_daily.sample(5, random_state=0) One of the most powerful and convenient features of pandas time series is time-based indexing — using dates and times to intuitively organize and access our data. With time-based indexing, we can use date/time formatted strings to select data in our DataFrame with the accessor. The indexing works similar to standard label-based indexing with , but with a few additional features. For example, we can select data for a single day using a string such as . We can also select a slice of days, such as . As with regular label-based indexing with , the slice is inclusive of both endpoints. Another very handy feature of pandas time series is partial-string indexing, where we can select all date/times which partially match a given string. For example, we can select the entire year 2006 with , or the entire month of February 2012 with . With pandas and matplotlib, we can easily visualize our time series data. In this section, we'll cover a few examples and some useful customizations for our time series plots. First, let's import matplotlib. We'll use seaborn styling for our plots, and let's adjust the default figure size to an appropriate shape for time series plots. import seaborn as sns # Use seaborn style defaults and set the default figure size sns.set(rc={'figure.figsize':(11, 4)}) Let's create a line plot of the full time series of Germany's daily electricity consumption, using the DataFrame's method. We can see that the method has chosen pretty good tick locations (every two years) and labels (the years) for the x-axis, which is helpful. However, with so many data points, the line plot is crowded and hard to read. Let's plot the data as dots instead, and also look at the and time series. We can already see some interesting patterns emerge:\n• Electricity consumption is highest in winter, presumably due to electric heating and increased lighting usage, and lowest in summer.\n• Electricity consumption appears to split into two clusters — one with oscillations centered roughly around 1400 GWh, and another with fewer and more scattered data points, centered roughly around 1150 GWh. We might guess that these clusters correspond with weekdays and weekends, and we will investigate this further shortly.\n• Solar power production is highest in summer, when sunlight is most abundant, and lowest in winter.\n• Wind power production is highest in winter, presumably due to stronger winds and more frequent storms, and lowest in summer.\n• There appears to be a strong increasing trend in wind power production over the years. All three time series clearly exhibit periodicity—often referred to as seasonality in time series analysis—in which a pattern repeats again and again at regular time intervals. The , , and time series oscillate between high and low values on a yearly time scale, corresponding with the seasonal changes in weather over the year. However, seasonality in general does not have to correspond with the meteorological seasons. For example, retail sales data often exhibits yearly seasonality with increased sales in November and December, leading up to the holidays. Seasonality can also occur on other time scales. The plot above suggests there may be some weekly seasonality in Germany's electricity consumption, corresponding with weekdays and weekends. Let's plot the time series in a single year to investigate further. Now we can clearly see the weekly oscillations. Another interesting feature that becomes apparent at this level of granularity is the drastic decrease in electricity consumption in early January and late December, during the holidays. Let's zoom in further and look at just January and February. As we suspected, consumption is highest on weekdays and lowest on weekends. To better visualize the weekly seasonality in electricity consumption in the plot above, it would be nice to have vertical gridlines on a weekly time scale (instead of on the first day of each month). We can customize our plot with matplotlib.dates, so let's import that module. Because date/time ticks are handled a bit differently in matplotlib.dates compared with the DataFrame's method, let's create the plot directly in matplotlib. Then we use and to set the x-axis ticks to the first Monday of each week. We also use to improve the formatting of the tick labels, using the format codes we saw earlier. fig, ax = plt.subplots() ax.plot(opsd_daily.loc['2017-01':'2017-02', 'Consumption'], marker='o', linestyle='-') ax.set_ylabel('Daily Consumption (GWh)') ax.set_title('Jan-Feb 2017 Electricity Consumption') # Set x-axis major ticks to weekly interval, on Mondays ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.MONDAY)) # Format x-tick labels as 3-letter month name and day number ax.xaxis.set_major_formatter(mdates.DateFormatter(' Now we have vertical gridlines and nicely formatted tick labels on each Monday, so we can easily tell which days are weekdays and weekends. There are many other ways to visualize time series, depending on what patterns you're trying to explore — scatter plots, heatmaps, histograms, and so on. We'll see other visualization examples in the following sections, including visualizations of time series data that has been transformed in some way, such as aggregated or smoothed data. Next, let's further explore the seasonality of our data with box plots, using seaborn's function to group the data by different time periods and display the distributions for each group. We'll first group the data by month, to visualize yearly seasonality. fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True) for name, ax in zip(['Consumption', 'Solar', 'Wind'], axes): sns.boxplot(data=opsd_daily, x='Month', y=name, ax=ax) ax.set_ylabel('GWh') ax.set_title(name) # Remove the automatic x-axis label from all but the bottom subplot if ax != axes[-1]: ax.set_xlabel('') These box plots confirm the yearly seasonality that we saw in earlier plots and provide some additional insights:\n\n * Although electricity consumption is generally higher in winter and lower in summer, the median and lower two quartiles are lower in December and January compared to November and February, likely due to businesses being closed over the holidays. We saw this in the time series for the year 2017, and the box plot confirms that this is consistent pattern throughout the years.\n\n * While solar and wind power production both exhibit a yearly seasonality, the wind power distributions have many more outliers, reflecting the effects of occasional extreme wind speeds associated with storms and other transient weather conditions. Next, let's group the electricity consumption time series by day of the week, to explore weekly seasonality. As expected, electricity consumption is significantly higher on weekdays than on weekends. The low outliers on weekdays are presumably during holidays. This section has provided a brief introduction to time series seasonality. As we will see later, applying a rolling window to the data can also help to visualize seasonality on different time scales. Other techniques for analyzing seasonality include autocorrelation plots, which plot the correlation coefficients of the time series with itself at different time lags. Time series with strong seasonality can often be well represented with models that decompose the signal into seasonality and a long-term trend, and these models can be used to forecast future values of the time series. A simple example of such a model is classical seasonal decomposition, as demonstrated in this tutorial. A more sophisticated example is as Facebook's Prophet model, which uses curve fitting to decompose the time series, taking into account seasonality on multiple time scales, holiday effects, abrupt changepoints, and long-term trends, as demonstrated in this tutorial. When the data points of a time series are uniformly spaced in time (e.g., hourly, daily, monthly, etc.), the time series can be associated with a frequency in pandas. For example, let's use the function to create a sequence of uniformly spaced dates from through at daily frequency. The resulting DatetimeIndex has an attribute with a value of , indicating daily frequency. Available frequencies in pandas include hourly ( ), calendar daily ( ), business daily ( ), weekly ( ), monthly ( ), quarterly ( ), annual ( ), and many others. Frequencies can also be specified as multiples of any of the base frequencies, for example for every five days. As another example, let's create a date range at hourly frequency, specifying the start date and number of periods, instead of the start date and end date. Now let's take another look at the DatetimeIndex of our time series. We can see that it has no frequency ( ). This makes sense, since the index was created from a sequence of dates in our CSV file, without explicitly specifying any frequency for the time series. If we know that our data should be at a specific frequency, we can use the DataFrame's method to assign a frequency. If any date/times are missing in the data, new rows will be added for those date/times, which are either empty ( ), or filled according to a specified data filling method such as forward filling or interpolation. To see how this works, let's create a new DataFrame which contains only the data for Feb 3, 6, and 8, 2013. # To select an arbitrary sequence of date/time values from a pandas time series, # we need to use a DatetimeIndex, rather than simply a list of date/time strings times_sample = pd.to_datetime(['2013-02-03', '2013-02-06', '2013-02-08']) # Select the specified dates and just the Consumption column consum_sample = opsd_daily.loc[times_sample, ['Consumption']].copy() consum_sample Now we use the method to convert the DataFrame to daily frequency, with a column for unfilled data, and a column for forward filled data. # Convert the data to daily frequency, without filling any missings consum_freq = consum_sample.asfreq('D') # Create a column with missings forward filled consum_freq['Consumption - Forward Fill'] = consum_sample.asfreq('D', method='ffill') consum_freq In the column, we have the original data, with a value of for any date that was missing in our DataFrame. In the column, the missings have been forward filled, meaning that the last value repeats through the missing rows until the next non-missing value occurs. If you're doing any time series analysis which requires uniformly spaced data without any missings, you'll want to use to convert your time series to the specified frequency and fill any missings with an appropriate method. It is often useful to resample our time series data to a lower or higher frequency. Resampling to a lower frequency (downsampling) usually involves an aggregation operation — for example, computing monthly sales totals from daily data. The daily OPSD data we're working with in this tutorial was downsampled from the original hourly time series. Resampling to a higher frequency (upsampling) is less common and often involves interpolation or other data filling method — for example, interpolating hourly weather data to 10 minute intervals for input to a scientific model. We will focus here on downsampling, exploring how it can help us analyze our OPSD data on various time scales. We use the DataFrame's method, which splits the DatetimeIndex into time bins and groups the data by time bin. The method returns a Resampler object, similar to a pandas GroupBy object. We can then apply an aggregation method such as , , , etc., to the data group for each time bin. For example, let's resample the data to a weekly mean time series. # Specify the data columns we want to include (i.e. exclude Year, Month, Weekday Name) data_columns = ['Consumption', 'Wind', 'Solar', 'Wind+Solar'] # Resample to weekly frequency, aggregating with mean opsd_weekly_mean = opsd_daily[data_columns].resample('W').mean() opsd_weekly_mean.head(3) The first row above, labelled , contains the mean of all the data contained in the time bin through . The second row, labelled , contains the mean data for the through time bin, and so on. By default, each row of the downsampled time series is labelled with the right edge of the time bin. By construction, our weekly time series has 1/7 as many data points as the daily time series. We can confirm this by comparing the number of rows of the two DataFrames. Let's plot the daily and weekly time series together over a single six-month period to compare them. # Start and end of the date range to extract start, end = '2017-01', '2017-06' # Plot daily and weekly resampled time series together fig, ax = plt.subplots() ax.plot(opsd_daily.loc[start:end, 'Solar'], marker='.', linestyle='-', linewidth=0.5, label='Daily') ax.plot(opsd_weekly_mean.loc[start:end, 'Solar'], marker='o', markersize=8, linestyle='-', label='Weekly Mean Resample') ax.set_ylabel('Solar Production (GWh)') ax.legend(); We can see that the weekly mean time series is smoother than the daily time series because higher frequency variability has been averaged out in the resampling. Now let's resample the data to monthly frequency, aggregating with sum totals instead of the mean. Unlike aggregating with , which sets the output to for any period with all missing data, the default behavior of will return output of as the sum of missing data. We use the parameter to change this behavior. # Compute the monthly sums, setting the value to NaN for any month which has # fewer than 28 days of data opsd_monthly = opsd_daily[data_columns].resample('M').sum(min_count=28) opsd_monthly.head(3) You might notice that the monthly resampled data is labelled with the end of each month (the right bin edge), whereas the weekly resampled data is labelled with the left bin edge. By default, resampled data is labelled with the right bin edge for monthly, quarterly, and annual frequencies, and with the left bin edge for all other frequencies. This behavior and various other options can be adjusted using the parameters listed in the documentation. Now let's explore the monthly time series by plotting the electricity consumption as a line plot, and the wind and solar power production together as a stacked area plot. At this monthly time scale, we can clearly see the yearly seasonality in each time series, and it is also evident that electricity consumption has been fairly stable over time, while wind power production has been growing steadily, with wind + solar power comprising an increasing share of the electricity consumed. Let's explore this further by resampling to annual frequency and computing the ratio of to for each year. # Compute the annual sums, setting the value to NaN for any year which has # fewer than 360 days of data opsd_annual = opsd_daily[data_columns].resample('A').sum(min_count=360) # The default index of the resampled DataFrame is the last day of each year, # ('2006-12-31', '2007-12-31', etc.) so to make life easier, set the index # to the year component opsd_annual = opsd_annual.set_index(opsd_annual.index.year) opsd_annual.index.name = 'Year' # Compute the ratio of Wind+Solar to Consumption opsd_annual['Wind+Solar/Consumption'] = opsd_annual['Wind+Solar'] / opsd_annual['Consumption'] opsd_annual.tail(3) Finally, let's plot the wind + solar share of annual electricity consumption as a bar chart. # Plot from 2012 onwards, because there is no solar production data in earlier years ax = opsd_annual.loc[2012:, 'Wind+Solar/Consumption'].plot.bar(color='C0') ax.set_ylabel('Fraction') ax.set_ylim(0, 0.3) ax.set_title('Wind + Solar Share of Annual Electricity Consumption') plt.xticks(rotation=0); We can see that wind + solar production as a share of annual electricity consumption has been increasing from about 15 Rolling window operations are another important transformation for time series data. Similar to downsampling, rolling windows split the data into time windows and and the data in each window is aggregated with a function such as , , , etc. However, unlike downsampling, where the time bins do not overlap and the output is at a lower frequency than the input, rolling windows overlap and \"roll\" along at the same frequency as the data, so the transformed time series is at the same frequency as the original time series. By default, all data points within a window are equally weighted in the aggregation, but this can be changed by specifying window types such as Gaussian, triangular, and others. We'll stick with the standard equally weighted window here. Let's use the method to compute the 7-day rolling mean of our daily data. We use the argument to label each window at its midpoint, so the rolling windows are:\n• and so on... We can see that the first non-missing rolling mean value is on , because this is the midpoint of the first rolling window. To visualize the differences between rolling mean and resampling, let's update our earlier plot of January-June 2017 solar power production to include the 7-day rolling mean along with the weekly mean resampled time series and the original daily data. # Start and end of the date range to extract start, end = '2017-01', '2017-06' # Plot daily, weekly resampled, and 7-day rolling mean time series together fig, ax = plt.subplots() ax.plot(opsd_daily.loc[start:end, 'Solar'], marker='.', linestyle='-', linewidth=0.5, label='Daily') ax.plot(opsd_weekly_mean.loc[start:end, 'Solar'], marker='o', markersize=8, linestyle='-', label='Weekly Mean Resample') ax.plot(opsd_7d.loc[start:end, 'Solar'], marker='.', linestyle='-', label='7-d Rolling Mean') ax.set_ylabel('Solar Production (GWh)') ax.legend(); We can see that data points in the rolling mean time series have the same spacing as the daily data, but the curve is smoother because higher frequency variability has been averaged out. In the rolling mean time series, the peaks and troughs tend to align closely with the peaks and troughs of the daily time series. In contrast, the peaks and troughs in the weekly resampled time series are less closely aligned with the daily time series, since the resampled time series is at a coarser granularity. Time series data often exhibit some slow, gradual variability in addition to higher frequency variability such as seasonality and noise. An easy way to visualize these trends is with rolling means at different time scales. A rolling mean tends to smooth a time series by averaging out variations at frequencies much higher than the window size and averaging out any seasonality on a time scale equal to the window size. This allows lower-frequency variations in the data to be explored. Since our electricity consumption time series has weekly and yearly seasonality, let's look at rolling means on those two time scales. We've already computed 7-day rolling means, so now let's compute the 365-day rolling mean of our OPSD data. # The min_periods=360 argument accounts for a few isolated missing days in the # wind and solar production time series opsd_365d = opsd_daily[data_columns].rolling(window=365, center=True, min_periods=360).mean() Let's plot the 7-day and 365-day rolling mean electricity consumption, along with the daily time series. # Plot daily, 7-day rolling mean, and 365-day rolling mean time series fig, ax = plt.subplots() ax.plot(opsd_daily['Consumption'], marker='.', markersize=2, color='0.6', linestyle='None', label='Daily') ax.plot(opsd_7d['Consumption'], linewidth=2, label='7-d Rolling Mean') ax.plot(opsd_365d['Consumption'], color='0.2', linewidth=3, label='Trend (365-d Rolling Mean)') # Set x-ticks to yearly interval and add legend and labels ax.xaxis.set_major_locator(mdates.YearLocator()) ax.legend() ax.set_xlabel('Year') ax.set_ylabel('Consumption (GWh)') ax.set_title('Trends in Electricity Consumption'); We can see that the 7-day rolling mean has smoothed out all the weekly seasonality, while preserving the yearly seasonality. The 7-day rolling mean reveals that while electricity consumption is typically higher in winter and lower in summer, there is a dramatic decrease for a few weeks every winter at the end of December and beginning of January, during the holidays. Looking at the 365-day rolling mean time series, we can see that the long-term trend in electricity consumption is pretty flat, with a couple of periods of anomalously low consumption around 2009 and 2012-2013. Now let's look at trends in wind and solar production. # Plot 365-day rolling mean time series of wind and solar power fig, ax = plt.subplots() for nm in ['Wind', 'Solar', 'Wind+Solar']: ax.plot(opsd_365d[nm], label=nm) # Set x-ticks to yearly interval, adjust y-axis limits, add legend and labels ax.xaxis.set_major_locator(mdates.YearLocator()) ax.set_ylim(0, 400) ax.legend() ax.set_ylabel('Production (GWh)') ax.set_title('Trends in Electricity Production (365-d Rolling Means)'); We can see a small increasing trend in solar power production and a large increasing trend in wind power production, as Germany continues to expand its capacity in those sectors. We've learned how to wrangle, analyze, and visualize our time series data in pandas using techniques such as time-based indexing, resampling, and rolling windows. Applying these techniques to our OPSD data set, we've gained insights on seasonality, trends, and other interesting features of electricity consumption and production in Germany. Other potentially useful topics we haven't covered include time zone handling and time shifts. If you'd like to learn more about working with time series data in pandas, you can check out this section of the Python Data Science Handbook, this blog post, and of course the official documentation. If you're interested in forecasting and machine learning with time series data, we'll be covering those topics in a future blog post, so stay tuned! If you’d like to learn more about this topic, check out Dataquest's interactive Pandas and NumPy Fundamentals course, and our Data Analyst in Python, and Data Scientist in Python paths that will help you become job-ready in around 6 months."
    },
    {
        "link": "https://datasource.ai/en/data-science-articles/4-must-know-python-pandas-functions-for-time-series-analysis",
        "document": "Time series data consists of data points attached to sequential time stamps. Daily sales, hourly temperature values, and second-level measurements in a chemical process are some examples of time series data.\n\n\n\nTime series data has different characteristics than ordinary tabular data. Thus, time series analysis has its own dynamics and can be considered as a separate field. There are books over 500 pages to cover time series analysis concepts and techniques in depth.\n\n\n\nPandas was created by Wes Mckinney to provide an efficient and flexible tool to work with financial data which is kind of a time series. In this article, we will go over 4 Pandas functions that can be used for time series analysis.\n\n\n\nWe need data for the examples. Let’s start with creating our own time series data.\n\n\n\n\n\n\n\nWe have created a data frame that contains temperature measurements during a period of 100 days. The date_range function of Pandas can be used for generating a date range with customized frequency. The temperature values are generated randomly using Numpy functions.\n\n\n\nWe can now start on the functions.\n\n\n\nIt is a common operation to shift time series data. We may need to make a comparison between lagged or lead features. In our data frame, we can create a new feature that contains the temperature of the previous day.\n\n\n\n\n\n\n\nThe scalar value passed to the shift function indicates the number of periods to shift. The first row of the new column is filled with NaN because there is no previous value for the first row.\n\n\n\nThe fill_value parameter can be used for filling the missing values with a scalar. Let’s replace the NaN with the average value of the temperature column.\n\n\n\n\n\n\n\nIf you are interested in the future values, you can shift backwards by passing negative values to the shift function. For instance, “-1” brings the temperature in the next day.\n\n\n\nAnother common operation performed on time series data is resampling. It involves in changing the frequency of the periods. For instance, we may be interested in the weekly temperature data rather than daily measurements.\n\n\n\nThe resample function creates groups (or bins) of a specified internal. Then, we can apply aggregation functions to the groups to calculate the value based on resampled frequency.\n\n\n\nLet’s calculate the average weekly temperatures. The first step is to resample the data to week level. Then, we will apply the mean function to calculate the average.\n\n\n\n\n\n\n\nThe first parameter specifies the frequency for resampling. “W” stands for week, surprisingly. If the data frame does not have a datetime index, the column that contains the date or time related information needs to be passed to the on parameter.\n\n\n\nThe asfreq function provides a different technique for resampling. It returns the value at the end of the specified interval. For instance, asfreq(“W”)returns the value on the last day of each week.\n\n\n\nIn order to use the asfreq function, we should set the date column as the index of the data frame.\n\n\n\n\n\n\n\nSince we are getting a value at a specific day, it is not necessary to apply an aggregation function.\n\n\n\nThe rolling function can be used for calculating moving average which is a highly common operation for time series data. It creates a window of a particular size. Then, we can use this window to make calculations as it rolls through the data points.\n\n\n\nThe figure below explains the concept of rolling.\n\n\n\n\n\n\n\nLet’s create a rolling window of 3 and use it to calculate the moving average.\n\n\n\n\n\n\n\nFor any day, the values show the average of the day and the previous 2 days. The values of the first 3 days are 18.9, 23.8, and 19.9. Thus, the moving average on the third day is the average of these values which is 20.7.\n\n\n\nThe first 2 values are NaN because they do not have previous 2 values. We can also use this rolling window to cover the previous and next day for any given day. It can be done by setting the center parameter as true.\n\n\n\n\n\n\n\nThe values of the first 3 days are 18.9, 23.8, and 19.9. Thus, the moving average in the second day is the average of these 3 values. In this setting, only the first value is NaN because we only need 1 previous value.\n\n\n\nWe have covered 4 Pandas functions that are commonly used in time series analysis. Predictive analytics is an essential part of data science. Time series analysis is at the core of many problems that predictive analytics aims to solve. Hence, if you plan to work on predictive analytics, you should definitely learn how to handle time series data.\n\n\n\nThank you for reading. Please let me know if you have any feedback."
    },
    {
        "link": "https://medium.com/@mubarakdaha/time-series-analysis-with-python-pandas-9e92981f54a8",
        "document": "Time series analysis is a technique used to analyze time-dependent data, with the goal of identifying patterns, trends, and relationships that can be used to make forecasts and predictions. Time series data can be found in various fields, including finance, economics, marketing, and more.\n\nIn this article, we will explore time series analysis using Python’s Pandas library. Pandas is a popular library for data manipulation and analysis, and it provides powerful tools for working with time series data.\n\nWe will cover the following topics:\n\nBefore we can start analyzing time series data, we need to import it into Pandas. Pandas provides a few built-in functions for reading time series data from different file formats.\n\nWe will start by reading time series data from a CSV file. The following code reads a CSV file containing stock prices and creates a Pandas DataFrame:\n\nHere, we use the function to read the CSV file. The parameter specifies the column to use as the index and the parameter specifies that Pandas should try to parse the index column as a date. Finally, we print the first few rows of the DataFrame using the method.\n\nPandas also provides a function for reading data from Excel files. The following code reads time series data from an Excel file:\n\nHere, we use the function to read the Excel file. The and parameters work the same way as with the function. The parameter specifies the sheet to read data from.\n\nAfter importing time series data into Pandas, we often need to resample and aggregate the data to a lower or higher frequency. Resampling refers to the process of changing the frequency of the data, while aggregation refers to the process of summarizing the data at a specific frequency.\n\nThe following code resamples the stock price data at a monthly frequency:\n\nHere, we use the method to resample the data to a monthly frequency. The method is used to select the last observation in each resampled period.\n\nWe can also aggregate the data by calculating summary statistics for each period. The following code calculates the mean price for each month:\n\nHere, we use the method to calculate the mean price for each month.\n\nTime series data often contain missing values, which can cause problems when analyzing the data. Pandas provides several functions for handling missing values in time series data.\n\nThe following code fills in the missing values in the stock price data with the previous observation:\n\nHere, we use the method to fill in missing values in the DataFrame. The parameter specifies the method to use for filling in missing values and the means to fill in the previous observation.\n\nAlternatively, we can interpolate missing values using the method. The following code interpolates missing values in the stock prices data:\n\nHere, we use the method to interpolate missing values in the DataFrame. By default, Pandas uses linear interpolation to fill in missing values.\n\nPandas provides several functions for visualizing time series data. These functions allow us to explore the data and identify patterns and trends.\n\nThe following code creates a line plot of the monthly mean stock prices:\n\nHere, we use the method to create a line plot of the data. We then use it to display the plot.\n\nWe can also create bar plots to compare different periods. The following code creates a bar plot of the total monthly stock prices:\n\nHere, we use the method to resample the data to a monthly frequency and calculate the total monthly stock prices using the method. We then create a bar plot of the data using the method with .\n\nBox plots are useful for visualizing the distribution of the data. The following code creates a box plot of the monthly stock prices:\n\nHere, we use the method to create a box plot of the data.\n\nTime series forecasting is the process of using historical data to make predictions about future values. Pandas provides several functions for time series forecasting, including the method, which shifts the data by a specified number of periods.\n\nThe following code creates a lag plot of the stock prices data:\n\nHere, we use the function from the module to create a lag plot of the data. A lag plot is a scatter plot of the data against itself with a time lag. If the data has a strong auto-correlation, we should see a linear relationship in the plot.\n\nWe can also use the method to create lagged features for time series forecasting. The following code creates a DataFrame with lagged features:\n\nHere, we use the method to concatenate the original data with a shifted copy of the data. We then rename the columns and drop any rows with missing values.\n\nWe can use this DataFrame to train a time series model using machine learning algorithms such as linear regression or ARIMA.\n\nTo conclude, Pandas provides powerful tools for working with time series data, including importing, resampling, aggregating, handling missing values, visualizing, and forecasting. By mastering these tools, you can analyze time series data and make accurate predictions about future values."
    },
    {
        "link": "https://linkedin.com/advice/0/how-do-you-use-pandas-manipulate-analyze-time-series-jjg8c",
        "document": "Shifting or lagging time series data is useful when comparing a series with a past version of itself. Use the shift() method to move the data forwards or backwards in time. This is often used in financial analysis to calculate changes over time, such as year-over-year growth rates or to create features for machine learning models. Help others by sharing more (125 characters min.)\n• To shift a time series data, you use the shift() method on a pandas DataFrame or Series. By default, shift() moves the data down by one row (creating a lag) Applications in Financial Analysis 1. Year-Over-Year Growth Rates: To calculate year-over-year growth rates, you can shift the data by 365 days : df['value_yoy'] = df['value'].shift(365) 2. Creating Features for Machine Learning: Shifting data can help create lagged features for machine learning models, capturing the effect of past values on current values: df['lag_1'] = df['value'].shift(1) # 1-day lag df['lag_7'] = df['value'].shift(7) # 1-week lag Key Parameters: periods: Number of periods to shift. Can be positive or negative. freq: Used to shift by a specific frequency.\n\nFinally, visualizing time series data can help you spot patterns, trends, and anomalies. Pandas integrates with Matplotlib, a Python plotting library, allowing you to create line plots of your time series data with a simple df.plot() . Customize your plots with titles, labels, and legends to make them more informative. Visual analysis is an integral part of the exploratory data analysis process in data science. Help others by sharing more (125 characters min.)\n• Quant Risk Analyst at Evalueserve | Quantitative Finance | Financial Mathematics | Research in Quantitative Finance You can also leverage advanced visualization libraries like Seaborn for richer plots or Plotly for interactive visualizations. Additionally, techniques like rolling averages and decomposition can help uncover underlying seasonal patterns and trends, offering deeper insights during exploratory data analysis."
    }
]