[
    {
        "link": "https://docs.unity3d.com/2023.2/Documentation/Manual/best-practice-guides.html",
        "document": "Use this page to discover and learn production-tested best practices from Unity experts. The Technical Content Marketing team at Unity created the guides listed on this page together with industry experts, and engineers and technical artists from Unity R&D and the Accelerate Games Solutions teams."
    },
    {
        "link": "https://unity.com/how-to",
        "document": ""
    },
    {
        "link": "https://reddit.com/r/Unity3D/comments/1221rm9/whats_the_best_way_to_learn_unity_in_2023",
        "document": "I have basically zero coding knowledge. I downloaded unity and started watching tutorials to understand the interface. I learned how to import assets Ive made in Blender etc, but the code is where I get stuck.\n\nI tried following specific tutorials like for character controllers. After 3 new attemps and about 4 retries for each tutorial, it just isnt working.\n\nI asked chatgpt to write the code and explain to me what each line is doing. Im getting a better understanding that way and even learning to troubleshoot the errors.\n\nThis is still a slow process so I asked it to write me a study guide. The problem is Im not too sure if the things its telling me to learn is the proper workflow to learning Unity.\n\nIm patient and eager to start learning and creating. Any advice is very appreciated."
    },
    {
        "link": "https://docs.unity3d.com/2023.1/Documentation/Manual/best-practice-guides.html",
        "document": "Use this page to discover and learn production-tested best practices from Unity experts. The Technical Content Marketing team at Unity created the guides listed on this page together with industry experts, and engineers and technical artists from Unity R&D and the Accelerate Games Solutions teams.\n\nLearn the fundamentals of version controlA system for managing file changes. You can use Unity in conjunction with most common version control tools, including Perforce, Git, Mercurial and PlasticSCM. More info\n\nSee in Glossary and project organization in Unity. Key topics covered include setting up a Unity project with different version control solutions, organizing a Unity project, proper folder structure, and naming standards."
    },
    {
        "link": "https://unity.com/resources/level-up-your-code-with-game-programming-patterns",
        "document": ""
    },
    {
        "link": "https://dev.epicgames.com/community/learning/tutorials/63wP/how-to-make-a-behavior-tree-in-unreal-engine-5-tutorial",
        "document": ""
    },
    {
        "link": "https://medium.com/@7019727855a/advanced-ai-behaviors-in-unreal-engine-using-c-and-behavior-trees-cb58ce8540ff",
        "document": "In game development, creating lifelike NPCs is crucial for immersive gameplay. Our AdvancedAIBehavior system for Unreal Engine 5 offers a flexible solution for crafting diverse AI entities. Built on Unreal’s Behavior Tree and Blackboard components, this system provides granular control over AI states, from idle behaviors to complex attack patterns.\n\nThe core of our AI system utilizes Behavior Trees for decision-making, Blackboards for memory management, and custom Tasks, Services, and Decorators for specific actions and conditions. This modular approach allows for easy extension and modification of AI behaviors\n\nthe AdvancedAIBehavior class represents the main AI NPC (Non-Player Character) class, managing the AI’s behavior like movement, state changes, and actions.\n\nThe UnifiedAIComponent is a reusable component that can be added to the AdvancedAIBehavior class or other actors. It centralizes logic (e.g., state changes, actions like parkour) that can be reused across Behavior Tree Tasks, Services, or Decorators. This component makes it easier to share common AI functionality, ensuring consistency and reducing code duplication in various parts of the AI’s behavior system.\n• Keep character-specific logic (like parkour, animation updates, patrol, and AI perception).\n• Move general-purpose logic (e.g., , , ) that can be shared across Behavior Tree tasks, services, and decorators.\n• Use UnifiedAIComponent to perform shared actions instead of duplicating logic within each.\n• In the Content Browser, right-click and select New C++ Class.\n• Select AIController as the parent class and name it (e.g., ).\n• Open your new AIController class and ensure it runs the Behavior Tree when the AI character is spawned.\n\n// AdvancedAIBehavior.h\n\n#pragma once\n\n\n\n#include \"CoreMinimal.h\"\n\n#include \"GameFramework/Actor.h\"\n\n#include \"BehaviorTree/BehaviorTree.h\"\n\n#include \"BehaviorTree/BlackboardComponent.h\"\n\n#include \"Components/SphereComponent.h\"\n\n \n\n#include \"AdvancedAIBehavior.h\"\n\n#include \"AIController.h\" \n\n#include \"Kismet/GameplayStatics.h\"\n\n#include \"GameFramework/CharacterMovementComponent.h\"\n\n#include \"Components/AudioComponent.h\"\n\n#include \"Animation/AnimInstance.h\"\n\n\n\n#include \"MyAIController.h\"\n\n\n\n#include \"AdvancedAIBehavior.generated.h\"\n\n\n\n \n\n\n\nUENUM(BlueprintType)\n\nenum class EAIBehaviorState : uint8\n\n{\n\n Idle,\n\n Follow,\n\n Flee,\n\n Patrol,\n\n Seek,\n\n AttackMelee,\n\n AttackRanged,\n\n Defend,\n\n Hit,\n\n Investigate,\n\n Parkour\n\n};\n\n\n\nUCLASS()\n\nclass YOURPROJECTNAME_API AAdvancedAIBehavior : public AActor\n\n{\n\n GENERATED_BODY()\n\n\n\npublic:\n\n AAdvancedAIBehavior();\n\n\n\nprotected:\n\n virtual void BeginPlay() override;\n\n\n\npublic:\n\n virtual void Tick(float DeltaTime) override;\n\n\n\n UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"AI\")\n\n UBehaviorTree* BehaviorTree;\n\n\n\n UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"AI\")\n\n UBlackboardComponent* BlackboardComponent;\n\n\n\n UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"AI\")\n\n USphereComponent* PerceptionSphere;\n\n\n\n UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"AI\")\n\n EAIBehaviorState CurrentState;\n\n\n\n UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"AI\")\n\n float Health;\n\n\n\n UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"AI\")\n\n TArray<AActor*> PatrolPoints;\n\n\n\n UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"AI\")\n\n TArray<AActor*> ParkourObstacles;\n\n\n\n UFUNCTION(BlueprintCallable, Category = \"AI\")\n\n void PerformParkour();\n\n\n\n UFUNCTION(BlueprintCallable, Category = \"AI\")\n\n void StartPatrol();\n\n\n\n UFUNCTION(BlueprintCallable, Category = \"AI\")\n\n void UpdateAnimationState();\n\n\n\n UFUNCTION(BlueprintCallable, Category = \"AI\")\n\n void PlayAudioForState();\n\n\n\n UFUNCTION(BlueprintCallable, Category = \"AI\")\n\n void InitializeBehaviorTree();\n\n \n\n // AI Controller------\n\n public:\n\n UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"AI\")\n\n TSubclassOf<AAIController> AIControllerClass;\n\n};\n\n\n\n#include \"AdvancedAIBehavior.h\"\n\n#include \"AIController.h\"\n\n#include \"BehaviorTree/BehaviorTreeComponent.h\"\n\n#include \"Kismet/GameplayStatics.h\"\n\n#include \"Components/AudioComponent.h\"\n\n#include \"GameFramework/CharacterMovementComponent.h\"\n\n#include \"UnifiedAIComponent.h\"\n\n\n\nAAdvancedAIBehavior::AAdvancedAIBehavior()\n\n{\n\n PrimaryActorTick.bCanEverTick = true;\n\n \n\n // Set the AIController class to the custom MyAIController class\n\n AIControllerClass = AMyAIController::StaticClass();\n\n\n\n PerceptionSphere = CreateDefaultSubobject<USphereComponent>(TEXT(\"PerceptionSphere\"));\n\n RootComponent = PerceptionSphere;\n\n\n\n BlackboardComponent = CreateDefaultSubobject<UBlackboardComponent>(TEXT(\"BlackboardComponent\"));\n\n\n\n CurrentState = EAIBehaviorState::Idle;\n\n Health = 100.0f;\n\n}\n\n\n\nvoid AAdvancedAIBehavior::BeginPlay()\n\n{\n\n Super::BeginPlay();\n\n InitializeBehaviorTree();\n\n}\n\n\n\nvoid AAdvancedAIBehavior::Tick(float DeltaTime)\n\n{\n\n Super::Tick(DeltaTime);\n\n UpdateAnimationState();\n\n}\n\n\n\nvoid AAdvancedAIBehavior::PerformParkour()\n\n{\n\n // Parkour logic, specific to this AI character\n\n // Keep this here because it deals with character movement\n\n AActor* NearestObstacle = nullptr;\n\n float NearestDistance = MAX_FLT;\n\n\n\n for (AActor* Obstacle : ParkourObstacles)\n\n {\n\n float Distance = FVector::Dist(GetActorLocation(), Obstacle->GetActorLocation());\n\n if (Distance < NearestDistance)\n\n {\n\n NearestDistance = Distance;\n\n NearestObstacle = Obstacle;\n\n }\n\n }\n\n\n\n if (NearestObstacle)\n\n {\n\n AAIController* AIController = Cast<AAIController>(GetOwner());\n\n if (AIController)\n\n {\n\n AIController->MoveToActor(NearestObstacle, 5.0f); // Acceptance radius\n\n }\n\n\n\n FVector JumpTarget = NearestObstacle->GetActorLocation() + FVector(0, 0, 100); // Jump above obstacle\n\n ACharacter* AICharacter = Cast<ACharacter>(GetOwner());\n\n if (AICharacter)\n\n {\n\n AICharacter->LaunchCharacter((JumpTarget - AICharacter->GetActorLocation()).GetSafeNormal() * 500, false, true);\n\n }\n\n }\n\n}\n\n\n\nvoid AAdvancedAIBehavior::StartPatrol()\n\n{\n\n // Patrol logic stays in the AI character class\n\n SetBehaviorState(EAIBehaviorState::Patrol);\n\n\n\n if (PatrolPoints.Num() > 0)\n\n {\n\n CurrentPatrolIndex = 0;\n\n AAIController* AIController = Cast<AAIController>(GetOwner());\n\n if (AIController)\n\n {\n\n AIController->MoveToActor(PatrolPoints[CurrentPatrolIndex]);\n\n }\n\n }\n\n}\n\n\n\nvoid AAdvancedAIBehavior::UpdateAnimationState()\n\n{\n\n ACharacter* AICharacter = Cast<ACharacter>(GetOwner());\n\n if (AICharacter)\n\n {\n\n UAnimInstance* AnimInstance = AICharacter->GetMesh()->GetAnimInstance();\n\n if (AnimInstance)\n\n {\n\n AnimInstance->SetVariableEnum(FName(\"AIState\"), (uint8)CurrentState);\n\n \n\n UCharacterMovementComponent* MovementComponent = AICharacter->GetCharacterMovement();\n\n if (MovementComponent)\n\n {\n\n AnimInstance->SetVariableFloat(FName(\"Speed\"), MovementComponent->Velocity.Size());\n\n }\n\n }\n\n }\n\n}\n\n\n\nvoid AAdvancedAIBehavior::PlayAudioForState()\n\n{\n\n UAudioComponent* AudioComponent = FindComponentByClass<UAudioComponent>();\n\n if (AudioComponent)\n\n {\n\n USoundBase* SoundToPlay = nullptr;\n\n switch (CurrentState)\n\n {\n\n case EAIBehaviorState::Idle:\n\n SoundToPlay = IdleSound;\n\n break;\n\n case EAIBehaviorState::AttackMelee:\n\n SoundToPlay = MeleeAttackSound;\n\n break;\n\n case EAIBehaviorState::Hit:\n\n SoundToPlay = HitSound;\n\n break;\n\n }\n\n\n\n if (SoundToPlay)\n\n {\n\n AudioComponent->SetSound(SoundToPlay);\n\n AudioComponent->Play();\n\n }\n\n }\n\n}\n• Centralized Logic: All AI-related functions are in one place, making it easier to manage and update.\n• Reusability: The same component can be used for tasks, services, and decorators.\n• Flexibility: You can easily add new functions without creating new classes each time.\n• Blueprint Compatibility: The macros make these functions callable from Blueprints.\n\nTo use this unified component:\n• Add the UnifiedAIComponent to your AI character.\n• Create custom Tasks, Services, and Decorators that use the UnifiedAIComponent.\n• Set up your Behavior Tree using these custom nodes.\n\nWhile this approach doesn’t completely eliminate the need for separate Task, Service, and Decorator classes (due to Unreal’s Behavior Tree structure), it does centralize the core logic and make it more reusable.\n\nNow, let me describe the Behavior Tree structure and Blackboard variables we’ll use with these components:\n• TargetActor (Object): The current target of the AI (e.g., player or object of interest).\n• TargetLocation (Vector): The current location of the target.\n• IsAlerted (Bool): Whether the AI is in an alerted state.\n\nThe reason for different UClass base classes:\n• UBTTaskNode for tasks: Tasks are single-execution nodes that perform a specific action and then finish.\n• UBTService for services: Services run continuously in the background, updating at regular intervals.\n• UBTDecorator for decorators: Decorators are conditional nodes that determine whether their child nodes should execute.\n\nEach of these base classes provides specific functionality and lifecycle methods tailored to their role in the Behavior Tree.\n\nwhat you should keep in mind:\n• Behavior Tree Asset: You’ll need to create and configure a Behavior Tree asset in Unreal Engine (e.g., ). Inside this tree, create nodes for tasks, services, and decorators based on the provided graph structure.\n• Blackboard Asset: Create a Blackboard asset (e.g., ) to define and manage variables like , , , and .\n• AIController: Ensure that your AI class ( ) is assigned an appropriate . The should be linked to the behavior tree and blackboard components.\n• In the AIController’s blueprint or C++ class, make sure the behavior tree runs ( ) on game start.\n• Task, Service, and Decorator Classes: You will need to implement the corresponding task ( ), service ( ), and decorator ( ) nodes in the Behavior Tree graph. These custom nodes will make use of the UnifiedAIComponent to manage the logic centrally.\n• Ensure that these nodes are properly linked and execute the expected behavior in the Behavior Tree.\n• UnifiedAIComponent: Add the UnifiedAIComponent to your AdvancedAIBehavior class. This component should be initialized at the start of the game or when the AI character is spawned.\n• AI States and Behavior: Make sure the SetBehaviorState function correctly sets the current behavior state of the AI. The state changes will be critical for behavior transitions (e.g., from idle to chase or attack).\n• Custom Nodes in Behavior Tree: Make sure the custom nodes ( , , ) are properly added to the Behavior Tree asset. Unreal Engine needs to recognize these as valid nodes.\n• To expose them in the Behavior Tree Editor, the macro must be correctly configured with or similar attributes.\n• AI Movement and Decision-Making: Test AI movement behavior (e.g., patrol, chase) to ensure that the behavior tree is correctly making decisions based on distance, patrol points, and other conditions.\n• Animation and Audio: If you use animations or audio cues based on AI states, ensure they are correctly linked to the AI character’s behavior.\n• Behavior Tree Debugging: Unreal Engine’s Behavior Tree debugger can be used during Play Mode to verify the decision-making process of the AI and see which tasks are running, failing, or succeeding.\n• Perception Sphere: Make sure your (PerceptionSphere) in the AdvancedAIBehavior class is set up correctly for detecting nearby actors.\n• NavMesh: Ensure your AI uses Unreal Engine’s NavMesh for pathfinding, especially if the AI needs to move between patrol points or chase targets.\n• Animation Blueprints: The function in AdvancedAIBehavior updates animation parameters based on movement and AI states. Ensure that your animation blueprint is correctly set up to handle state changes (like speed and AIState).\n• You’ll need an animation blueprint linked to your AI character’s mesh to drive the animations.\n• Compiler and Runtime Errors: If you encounter issues like missing components or nodes, you may need to verify the logic in your C++ code and the behavior tree nodes.\n• Ensure the custom C++ classes (tasks, services, and decorators) are correctly inherited from their respective Unreal base classes ( , , ).\n• Create and set up Behavior Tree and Blackboard assets.\n• Add and configure the UnifiedAIComponent in the AI character.\n• Implement custom tasks, services, and decorators, and add them to the behavior tree."
    },
    {
        "link": "https://forums.unrealengine.com/t/ue5-unreal-engine-support-for-machine-learning/514171",
        "document": "Is this experimental machine learning plugin the only option at the moment for creating ML-based AI? I would very much love to have some way to utilize ML models for AI behavior, is that even something that currently exists inside Unreal Engine? I just started using Unreal Engine a few months ago, and I am still learning the ins and outs. I have experience using ML models and I would really like to combine ML models with AI behavior (like having ML models inside the behavior tree). Is that even possible at the moment? Thank you!\n\n“Unreal Support for Machine Learning” has become “ML Adapter” (MLAdapter | Unreal Engine Documentation) in UE5.1. You should be able to find it in the plugins browser by searching for “ML”. ML Adapter has support for NNI which will allow you to use previously trained models at game-time for inference. ML Adapter is still a nascent plugin. It likely contains a myriad of bugs and its API could drastically change in future versions. That aside, ML Adapter works and I have been able to create some cool projects using it. You can define agents/sensors/actuators and train them via the Python API. Once you are happy with the results, you can export the model via ONNX and import them into the editor and run them via NNI. If you have more questions about the code, feel free to reach out in more threads here or at ml-adapter@epicgames.com and I will try to get back to you soon.\n\nThanks for reporting the issue and awesome work figuring out how to use the plugin from the minimal documentation. That error is very strange as I have not seen anything like it and the call stack isn’t particularly enlightening. I have always been using PyTorch in my testing, which shouldn’t make a difference AFAIK but maybe there is something strange in the ONNX file (?). Weird that it would work once though… hmm. FWIW a lot of things are in-flight right now and it is increasingly likely ML Adapter will be sunset later this year in favor of a new plugin we are intending to release. I intend to do a blog post or something once that new plugin is ready.\n\nHey @AAbdelkader92,\n\n This is the C++ code: MLAgentDebug.rar (7.2 KB) You need to enable the MLAdapter and the NeuralNetworkInference plugins and add them to public dependency modules:\n\n In the .rar, I added the GameMode, Agent, Sensor and Actuator classes. The basic idea is that an Agent has Sensors and it will use them to get information about the environment and it also has Actuators that it will use to apply actions to the avatar. In the GameMode, the function we need is:\n\n \n\n The python environment will use it to reset the simulation. Python calls to unreal will instantiate the Agent, Sensor and Actuator classes, but it will not spawn the avatar. I spawn the avatar inside that function. In my case, a blueprint actor with 2 static meshes, one kinematic, one dynamic, connected by a UPhysicsConstraintComponent, a.k.a a pendulum. This is the python project (I used pyCharm): RLLecture.rar (11.8 MB) The MLAdapter plugin comes with a python class called ActionRPG, located here (in my case):\n\n \n\n I used that as a basis, duplicated it, changed some variables for my test, created this:\n\n ue_debug.py (1.5 KB)\n\n Notice the function inside this class.\n\n Then, you need to edit to add the new class. Mine looks like this:\n\n init.py|attachment (2.3 KB) To start training, hit play in editor, and after that, run main_ue.py. After training, import the .onnx file to unreal, update the path in function and remember the bug I explained in my previous post. Set the AddAgents bool in GameMode and hit Play. Agent should behave as it was trained. I didn’t spend too much time training it, this is what i got:\n\n \n\n For some reason, when it reached the top, it would swing right and repeat. My conclusion is that, this is fine for small agents, but after the pendulum test I wanted to create a more complex convolutional neural net, to use as a car driver. The problem is that python, has got to be, the slowest of them all, and 2 FPS while training something like that is just not practical. @Deathcalibur mentioned, I think, something about moving the whole thing to C++ in the next iteration. No more python, that will obviously improve training time. It also sounds like a lot of work making the same functionality available, because there are a ton of third party libraries available for python right now.\n\nThanks for your patience! I am happy to share that we have pushed out the very first version of a new plugin, Learning Agents! This plugin is similar to ML Adapter but flips the design on its head a bit: instead of the python training process “controlling” Unreal, the Unreal process is in charge and controls the python one. You can learn a little more about it here: Learning Agents Introduction We believe this design is much better on several axes including runtime performance, flexibility for future growth, and fits many more use cases. For example, it’s much easier to replace a module in a traditional AI behavior tree with a model created with Learning Agents, where ML Adapter would encourage you to replace your whole AI altogether. The current design’s foundations are hopefully relatively stable, but the plugin is still experimental so some breaking changes may be needed. We mostly intend to add new observations and actions to hit common use-cases, but the AddFloatObservation and AddFloatAction can be used for almost anything if you’re willing to do more preprocessing work yourself. We are also currently working on adding comments to all the codebase as well as a developer course which will get you familiar with the plugin and walk you through an example of building and training your first agent. One other note is that the current training and neural network support is relatively limited. We currently are supporting “vanilla” feed forward models and a PPO RL implementation, as well as a basic behavior cloning imitation model. We intend to expand these options greatly while the plugin is in the experimental state. Please check it out if you are so inclined and feel free to contact us here on the forums or you can reach out to learning-agents@epicgames.com. Any feedback is welcome and you can have a huge impact on the future of ML in Unreal."
    },
    {
        "link": "https://reddit.com/r/gamedev/comments/zie3o5/learn_unreal_engine_ai_in_30_minutes_beginners",
        "document": "The subreddit covers various game development aspects, including programming, design, writing, art, game jams, postmortems, and marketing. It serves as a hub for game creators to discuss and share their insights, experiences, and expertise in the industry."
    },
    {
        "link": "https://reddit.com/r/UnrealEngine5/comments/180f5cc/can_i_create_this_level_of_ai_in_unreal_5",
        "document": "First I should mention I'm a retired backend developer. So I have lots of experience with programming complex code. Second I am new to Unreal Engine 5 (I think the last time I touched Unreal it was at version 2)\n\nFor my survival game there will be a number of \"tribes\" that number between 6 and 10 members.\n\nOne member is the Captain and gives orders when a stranger is encountered. His response may be either cautious and not attack or hostile and always attack. Everyone else on the team has a tool, a weapon, possibly armor.\n\nThe teams will spawn and despawn based on how far they are from the player. If the player begins on a large enough island he/she may encounter them on \"his/her\" island. When they spawn in (team size amount - number killed) they will do every day activities. Hunt, gather food, gather resources, and at night they will sleep. They will do this as long as they do not \"see\" player. Once they see him they run back to the commander to alert the Commander and he puts everyone on alert. Thus there is a lag between when one member goes on alert and when the entire team is on alert. When the Commander puts everyone on alert, everyone runs to their base.\n\nThere will be three or four levels of fighting. Level 1 they have little in the way of weapons and armor and will just run blindly to attack. Each level up they will have better weapons and armor and attack in a more coordinated fashion. I'm thinking that instead of all fighting styles in one block of code, the fighting is 4-5 separate sets of code independent of the normal behavior thread.\n\nAll neutral teams will start cautious with the player and the more time that goes by without violence, the more they move to peaceful. But if the player attacks one member the Commander makes everyone hostile. Once a team is hostile they will forever remain hostile.\n\nI think this pretty much covers the AI I want to create. Looking for feedback and answer to the question is this possible?"
    }
]