[
    {
        "link": "https://docs.python.org/3/library/shutil.html",
        "document": "The module offers a number of high-level operations on files and collections of files. In particular, functions are provided which support file copying and removal. For operations on individual files, see also the module.\n\nChanged in version 3.5: Added support for the xztar format. High-level utilities to create and read compressed and archived files are also provided. They rely on the and modules. Create an archive file (such as zip or tar) and return its name. base_name is the name of the file to create, including the path, minus any format-specific extension. format is the archive format: one of “zip” (if the module is available), “tar”, “gztar” (if the module is available), “bztar” (if the module is available), or “xztar” (if the module is available). root_dir is a directory that will be the root directory of the archive, all paths in the archive will be relative to it; for example, we typically chdir into root_dir before creating the archive. base_dir is the directory where we start archiving from; i.e. base_dir will be the common prefix of all files and directories in the archive. base_dir must be given relative to root_dir. See Archiving example with base_dir for how to use base_dir and root_dir together. root_dir and base_dir both default to the current directory. If dry_run is true, no archive is created, but the operations that would be executed are logged to logger. owner and group are used when creating a tar archive. By default, uses the current owner and group. logger must be an object compatible with PEP 282, usually an instance of . The verbose argument is unused and deprecated. This function is not thread-safe when custom archivers registered with do not support the root_dir argument. In this case it temporarily changes the current working directory of the process to root_dir to perform archiving. Changed in version 3.8: The modern pax (POSIX.1-2001) format is now used instead of the legacy GNU format for archives created with . Changed in version 3.10.6: This function is now made thread-safe during creation of standard and tar archives. Return a list of supported formats for archiving. Each element of the returned sequence is a tuple . By default provides these formats:\n• None zip: ZIP file (if the module is available).\n• None tar: Uncompressed tar file. Uses POSIX.1-2001 pax format for new archives.\n• None gztar: gzip’ed tar-file (if the module is available).\n• None bztar: bzip2’ed tar-file (if the module is available).\n• None xztar: xz’ed tar-file (if the module is available). You can register new formats or provide your own archiver for any existing formats, by using . Register an archiver for the format name. function is the callable that will be used to unpack archives. The callable will receive the base_name of the file to create, followed by the base_dir (which defaults to ) to start archiving from. Further arguments are passed as keyword arguments: owner, group, dry_run and logger (as passed in ). If function has the custom attribute set to , the root_dir argument is passed as a keyword argument. Otherwise the current working directory of the process is temporarily changed to root_dir before calling function. In this case is not thread-safe. If given, extra_args is a sequence of pairs that will be used as extra keywords arguments when the archiver callable is used. description is used by which returns the list of archivers. Defaults to an empty string. Changed in version 3.12: Added support for functions supporting the root_dir argument. Remove the archive format name from the list of supported formats. Unpack an archive. filename is the full path of the archive. extract_dir is the name of the target directory where the archive is unpacked. If not provided, the current working directory is used. format is the archive format: one of “zip”, “tar”, “gztar”, “bztar”, or “xztar”. Or any other format registered with . If not provided, will use the archive file name extension and see if an unpacker was registered for that extension. In case none is found, a is raised. The keyword-only filter argument is passed to the underlying unpacking function. For zip files, filter is not accepted. For tar files, it is recommended to set it to , unless using features specific to tar and UNIX-like filesystems. (See Extraction filters for details.) The filter will become the default for tar files in Python 3.14. Never extract archives from untrusted sources without prior inspection. It is possible that files are created outside of the path specified in the extract_dir argument, e.g. members that have absolute filenames starting with “/” or filenames with two dots “..”. Changed in version 3.7: Accepts a path-like object for filename and extract_dir. Registers an unpack format. name is the name of the format and extensions is a list of extensions corresponding to the format, like for Zip files. function is the callable that will be used to unpack archives. The callable will receive:\n• None the path of the archive, as a positional argument;\n• None the directory the archive must be extracted to, as a positional argument;\n• None possibly a filter keyword argument, if it was given to ;\n• None additional keyword arguments, specified by extra_args as a sequence of tuples. description can be provided to describe the format, and will be returned by the function. Unregister an unpack format. name is the name of the format. Return a list of all registered formats for unpacking. Each element of the returned sequence is a tuple . By default provides these formats:\n• None zip: ZIP file (unpacking compressed files works only if the corresponding module is available).\n• None gztar: gzip’ed tar-file (if the module is available).\n• None bztar: bzip2’ed tar-file (if the module is available).\n• None xztar: xz’ed tar-file (if the module is available). You can register new formats or provide your own unpacker for any existing formats, by using . In this example, we create a gzip’ed tar-file archive containing all files found in the directory of the user: In this example, similar to the one above, we show how to use , but this time with the usage of base_dir. We now have the following directory structure: In the final archive, should be included, but should not. Therefore we use the following: Listing the files in the resulting archive gives us:"
    },
    {
        "link": "https://geeksforgeeks.org/shutil-module-in-python",
        "document": "Shutil module offers high-level operation on a file like a copy, create, and remote operation on the file. It comes under Python’s standard utility modules. This module helps in automating the process of copying and removal of files and directories. In this article, we will learn this module.\n\nshutil.copy() method in Python is used to copy the content of the source file to the destination file or directory. It also preserves the file’s permission mode but other metadata of the file like the file’s creation and modification times is not preserved.\n\nThe source must represent a file but the destination can be a file or a directory. If the destination is a directory then the file will be copied into the destination using the base filename from the source. Also, the destination must be writable. If the destination is a file and already exists then it will be replaced with the source file otherwise a new file will be created.\n\nExample 2: If the destination is a directory.\n\nCopying the Metadata along with File\n\nshutil.copy2() method in Python is used to copy the content of the source file to the destination file or directory. This method is identical to shutil.copy() method but it also tries to preserve the file’s metadata.\n\nExample 2: If the destination is a directory\n\nCopying the content of one file to another\n\nshutil.copyfile() method in Python is used to copy the content of the source file to the destination file. The metadata of the file is not copied. Source and destination must represent a file and destination must be writable. If the destination already exists then it will be replaced with the source file otherwise a new file will be created.\n\nIf source and destination represent the same file then SameFileError exception will be raised.\n\nshutil.copytree() method recursively copies an entire directory tree rooted at source (src) to the destination directory. The destination directory, named by (dst) must not already exist. It will be created during copying.\n\nParameters:\n\nsrc: A string representing the path of the source directory.\n\ndest: A string representing the path of the destination.\n\nsymlinks (optional) : This parameter accepts True or False, depending on which the metadata of the original links or linked links will be copied to the new tree.\n\nignore (optional) : If ignore is given, it must be a callable that will receive as its arguments the directory being visited by copytree(), and a list of its contents, as returned by os.listdir().\n\ncopy_function (optional): The default value of this parameter is copy2. We can use other copy function like copy() for this parameter.\n\nignore_dangling_symlinks (optional) : This parameter value when set to True is used to put a silence on the exception raised if the file pointed by the symlink doesn’t exist. Return Value: This method returns a string which represents the path of newly created directory.\n\nshutil.rmtree() is used to delete an entire directory tree, the path must point to a directory (but not a symbolic link to a directory).\n\nshutil.which() method tells the path to an executable application that would be run if the given cmd was called. This method can be used to find a file on a computer which is present on the PATH.\n\nSyntax: shutil.which(cmd, mode = os.F_OK | os.X_OK, path = None)\n\nParameters:\n\ncmd: A string representing the file.\n\nmode: This parameter specifies mode by which method should execute. os.F_OK tests existence of the path and os.X_OK Checks if path can be executed or we can say mode determines if the file exists and executable.\n\npath: This parameter specifies the path to be used, if no path is specified then the results of os.environ() are used\n\nReturn Value: This method returns the path to an executable application"
    },
    {
        "link": "https://docs.python.org/3/library/filesys.html",
        "document": "The modules described in this chapter deal with disk files and directories. For example, there are modules for reading the properties of files, manipulating paths in a portable way, and creating temporary files. The full list of modules in this chapter is:\n\nOperating system interfaces, including functions to work with files at a lower level than Python file objects. Python’s built-in I/O library, including both abstract classes and some concrete classes such as file I/O. The standard way to open files for reading and writing with Python."
    },
    {
        "link": "https://pydoc-zh.readthedocs.io/en/latest/library/shutil.html",
        "document": "The module offers a number of high-level operations on files and collections of files. In particular, functions are provided which support file copying and removal. For operations on individual files, see also the module.\n\nOn POSIX platforms, this means that file owner and group are lost as well as ACLs. On Mac OS, the resource fork and other metadata are not used. This means that resources will be lost and file type and creator codes will not be correct. On Windows, file owners, ACLs and alternate data streams are not copied.\n\nCopy the contents of the file-like object fsrc to the file-like object fdst. The integer length, if given, is the buffer size. In particular, a negative length value means to copy the data without looping over the source data in chunks; by default the data is read in chunks to avoid uncontrolled memory consumption. Note that if the current file position of the fsrc object is not 0, only the contents from the current file position to the end of the file will be copied.\n\nCopy the contents (no metadata) of the file named src to a file named dst. dst must be the complete target file name; look at for a copy that accepts a target directory path. If src and dst are the same files, is raised. The destination location must be writable; otherwise, an exception will be raised. If dst already exists, it will be replaced. Special files such as character or block devices and pipes cannot be copied with this function. src and dst are path names given as strings.\n\nRecursively copy an entire directory tree rooted at src. The destination directory, named by dst, must not already exist; it will be created as well as missing parent directories. Permissions and times of directories are copied with , individual files are copied using . If symlinks is true, symbolic links in the source tree are represented as symbolic links in the new tree, but the metadata of the original links is NOT copied; if false or omitted, the contents and metadata of the linked files are copied to the new tree. If ignore is given, it must be a callable that will receive as its arguments the directory being visited by , and a list of its contents, as returned by . Since is called recursively, the ignore callable will be called once for each directory that is copied. The callable must return a sequence of directory and file names relative to the current directory (i.e. a subset of the items in its second argument); these names will then be ignored in the copy process. can be used to create such a callable that ignores names based on glob-style patterns. If exception(s) occur, an is raised with a list of reasons. The source code for this should be considered an example rather than the ultimate tool. Changed in version 2.3: is raised if any exceptions occur during copying, rather than printing a message. Changed in version 2.5: Create intermediate directories needed to create dst, rather than raising an error. Copy permissions and times of directories using . Changed in version 2.6: Added the ignore argument to be able to influence what is being copied.\n\nDelete an entire directory tree; path must point to a directory (but not a symbolic link to a directory). If ignore_errors is true, errors resulting from failed removals will be ignored; if false or omitted, such errors are handled by calling a handler specified by onerror or, if that is omitted, they raise an exception. If onerror is provided, it must be a callable that accepts three parameters: function, path, and excinfo. The first parameter, function, is the function which raised the exception; it will be , , or . The second parameter, path, will be the path name passed to function. The third parameter, excinfo, will be the exception information return by . Exceptions raised by onerror will not be caught. Changed in version 2.6: Explicitly check for path being a symbolic link and raise in that case.\n\nRecursively move a file or directory (src) to another location (dst). If the destination is a directory or a symlink to a directory, then src is moved inside that directory. The destination directory must not already exist. If the destination already exists but is not a directory, it may be overwritten depending on semantics. If the destination is on the current filesystem, then is used. Otherwise, src is copied (using ) to dst and then removed.\n\nThis example is the implementation of the function, described above, with the docstring omitted. It demonstrates many of the other functions provided by this module. # XXX What about devices, sockets etc.? # catch the Error from the recursive copytree so that we can Another example that uses the helper: This will copy everything except files and files or directories whose name starts with . Another example that uses the ignore argument to add a logging call: # nothing will be ignored"
    },
    {
        "link": "https://tutorialspoint.com/shutil-module-in-python",
        "document": "Python, being a versatile and powerful programming language, offers a wide range of modules and libraries to simplify various tasks. One such module is Shutil, which stands for \"shell utilities,\" providing a comprehensive set of functions for file and directory operations. Whether you need to copy, move, rename, or delete files and directories, the Shutil module in Python comes to the rescue with its user−friendly and efficient functionalities.\n\nIn this tutorial, we will delve into the world of the Shutil module and explore its capabilities for managing files and directories in Python. We will walk you through the key features and functionalities of Shutil, providing you with practical examples and code snippets along the way. In the next section of the article, we will begin by understanding. Let's jump right in and explore the powerful capabilities of the Shutil module in Python!\n\nWhen it comes to working with files and directories in Python, the Shutil module is a powerful ally. It provides a collection of functions that allow us to perform various operations such as copying, moving, renaming, and deleting files and directories, as well as working with file permissions. By utilizing the Shutil module, we can simplify complex file operations and handle common tasks with ease.\n\nIn this tutorial, we will explore the key functionalities of the Shutil module, starting with copying files and directories. Using the `shutil.copy()` function, we can create copies of individual files. For instance, let's say we have a file named \"file.txt\" in our current working directory, and we want to make a copy of it with the name \"file_copy.txt\". We can achieve this as follows:\n\nBy executing this code, the Shutil module will create a duplicate of the original \"file.txt\" with the name \"file_copy.txt\".\n\nAdvantages of using the Shutil module for file and directory operations\n\nThe Shutil module offers several advantages when it comes to file and directory operations in Python. Firstly, it provides a straightforward and intuitive interface, making it accessible to developers of all skill levels. Whether you're a beginner or an experienced Python programmer, you can quickly grasp the functionalities of the Shutil module and start utilizing them in your projects.\n\nSecondly, the Shutil module is cross−platform compatible, meaning it works seamlessly across different operating systems such as Windows, macOS, and Linux. This ensures that your code can be executed on various platforms without the need for platform−specific modifications. Whether you're developing on Windows and deploying on Linux, or vice versa, the Shutil module ensures consistent and reliable file operations.\n\nIn the next section of the article, we will continue exploring the capabilities of the Shutil module.\n\nCopying files and directories is a common task in file operations, and the Shutil module provides convenient functions to accomplish this.\n\nTo copy a single file, we can use the `shutil.copy()` function. It takes two arguments: the path of the source file and the destination where the copy will be placed. For instance, suppose we have a file named \"source.txt\" in the current working directory, and we want to create a copy of it named \"destination.txt\". We can achieve this with the following code:\n\nRunning this code will duplicate \"source.txt\" and create a new file named \"destination.txt\" in the same directory.\n\nWhen it comes to copying entire directories, the `shutil.copytree()` function is the way to go. It allows us to recursively copy the contents of a source directory to a target directory. For example, let's say we have a directory named \"source_dir\" and we want to duplicate it into a new directory called \"target_dir\". We can do so using the following code:\n\nExecuting this code will create a new directory named \"target_dir\" and copy all the files and subdirectories from \"source_dir\" into it.\n\nMoving files and directories involves both copying and deleting operations. The Shutil module simplifies this process with dedicated functions.\n\nTo move a single file, we can use the `shutil.move()` function. It works similarly to the `shutil.copy()` function but also removes the original file after successfully moving it. For example, let's move a file named \"source_file.txt\" to a different location:\n\nAfter executing this code, \"source_file.txt\" will be moved to the specified destination directory, and it will no longer exist in its original location.\n\nMoving directories with the Shutil module follows the same principles as moving files. The `shutil.move()` function can be used to move entire directories, including their contents. For instance, to move a directory named \"source_directory\" to a new location, we can use the following code:\n\nExecuting this code will move the entire \"source_directory\" and its contents to the specified destination directory.\n\nThe Shutil module allows us to rename both files and directories using the `shutil.move()` function. To rename a file or directory, we specify the current path as the source and provide the desired new name as the destination. Here's an example of renaming a file:\n\nBy executing this code, the file \"old_name.txt\" will be renamed to \"new_name.txt\".\n\nDeleting files and directories is a common operation, and the Shutil module provides a simple function to accomplish this task.\n\nTo delete a file, we can use the `os.remove()` function from the built−in `os` module. For example:\n\nThis code will delete the file named \"file.txt\".\n\nTo delete an empty directory, we can use the `os.rmdir()` function. However, if the directory is not empty, we need to use the `shutil.rmtree()` function, which recursively removes the directory and all its contents. Here's an example:\n\nExecuting this code will delete the \"directory\" and all its files and subdirectories.\n\nThe Shutil module also provides functions to work with file permissions and attributes. For example, we can use the `shutil.copystat()` function to copy the permission bits, last access time, last modification time, and flags from one file to another:\n\nRunning this code will copy the file attributes from \"source_file.txt\" to \"destination_file.txt\".\n\nIn this tutorial, we explored the powerful capabilities of the Shutil module in Python for managing files and directories. We discussed key features and functionalities, including copying files and directories, moving them, renaming them, and deleting them. We provided example codes for each method to help you understand their usage and see them in action. Additionally, we touched upon working with file permissions and attributes using the Shutil module."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html",
        "document": "Split arrays or matrices into random train and test subsets.\n\nQuick utility that wraps input validation, , and application to input data into a single call for splitting (and optionally subsampling) data into a one-liner.\n\nRead more in the User Guide.\n\n*arrays sequence of indexables with same length / shape[0] If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If is also None, it will be set to 0.25. If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls. See Glossary. Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None. If not None, data is split in a stratified fashion, using this as the class labels. Read more in the User Guide. Added in version 0.16: If the input is sparse, the output will be a . Else, output type is the same as the input type."
    },
    {
        "link": "https://realpython.com/train-test-split-python-data",
        "document": "With from scikit-learn, you can efficiently divide your dataset into training and testing subsets to ensure unbiased model evaluation in machine learning. This process helps prevent overfitting and underfitting by keeping the test data separate from the training data, allowing you to assess the model’s predictive performance accurately.\n\nBy the end of this tutorial, you’ll understand that:\n• is a function in that divides datasets into training and testing subsets.\n• and represent the inputs and outputs of the training data subset, respectively, while and represent the input and output of the testing data subset.\n• By specifying , you use 20% of the dataset for testing, leaving 80% for training.\n• can handle imbalanced datasets using the parameter to maintain class distribution.\n\nYou’ll learn how to use and apply these concepts in real-world scenarios, ensuring your machine learning models are evaluated with precision and fairness. In addition, you’ll explore related tools from for further insights.\n\nNow that you understand the need to split a dataset in order to perform unbiased model evaluation and identify underfitting or overfitting, you’re ready to learn how to split your own datasets. You’ll use version 1.5.0 of scikit-learn, or . It has many packages for data science and machine learning, but for this tutorial, you’ll focus on the package, specifically on the function . Note: While this tutorial is tested with this specific version of scikit-learn, the features that you’ll use are core to the library and should work equivalently in other versions of scikit-learn as well. You can install with : If you use Anaconda, then you probably already have it installed. However, if you want to use a fresh environment, ensure that you have the specified version or use Miniconda. Then you can install from Anaconda Cloud with : You’ll also need NumPy, but you don’t have to install it separately. You should get it along with if you don’t already have it installed. If you want to, you can refresh your NumPy knowledge and check out NumPy Tutorial: Your First Steps Into Data Science in Python.\n\nYou need to import and NumPy before you can use them. You can work in a Jupyter notebook or start a new Python REPL session, then you can start with the statements: Now that you have both imported, you can use them to split data into training sets and test sets. You’ll split inputs and outputs at the same time, with a single function call. With , you only need to provide the arrays that you want to split. Additionally, you can also provide some optional arguments. The function usually returns a list of NumPy arrays but can also return a couple of other iterable types, such as SciPy sparse matrices, if appropriate: The parameter in the function signature of refers to the sequence of lists, NumPy arrays, pandas DataFrames, or similar array-like objects that hold the data that you want to split. All these objects together make up the dataset and must be of the same length. In supervised machine learning applications, you’ll typically work with two such arrays: The parameter indicates that you can customize the function’s behavior with optional keyword arguments:\n• is the number that defines the size of the training set. If you provide a , then it must be between and and it will define the share of the dataset used for testing. If you provide an , then it will represent the total number of the training samples. The default value is .\n• is the number that defines the size of the test set. It’s very similar to . You should provide either or . If neither is given, then the default share of the dataset that will be used for testing is , or 25 percent.\n• is the object that controls randomization during splitting. It can be either an or an instance of . Setting the random state is useful if you need reproducibility. The default value is .\n• is the Boolean object that determines whether to shuffle the dataset before applying the split. The default value is .\n• is an array-like object that, if not , determines how to use a stratified split. Now it’s time to try data splitting! You’ll start by creating a simple dataset to work with. The dataset will contain the inputs in the two-dimensional array and outputs in the one-dimensional array : To get your data, you use , which is very convenient for generating arrays based on numerical ranges. You also use to modify the shape of the array returned by and get a two-dimensional data structure. You can split both input and output datasets with a single function call: Given two arrays, like and here, performs the split and returns four arrays (in this case NumPy arrays) in this order:\n• : The training part of the first array ( )\n• : The test part of the first array ( )\n• : The training part of the second array ( )\n• : The test part of the second array ( ) You probably got different results from what you see here. This is because dataset splitting is random by default. The result differs each time you run the function. However, this often isn’t what you want. Sometimes, to make your tests reproducible, you need a random split with the same output for each function call. You can do that with the parameter . The value of isn’t important—it can be any non-negative integer. You could use an instance of instead, but that’s a more complex approach. In the previous example, you used a dataset with twelve rows, or observations, and got a training sample with nine rows and a test sample with three rows. That’s because you didn’t specify the desired size of the training and test sets. By default, 25 percent of samples are assigned to the test set. This ratio is generally fine for many applications, but it’s not always what you need. Typically, you’ll want to define the size of the test or training set explicitly, and sometimes you’ll even want to experiment with different values. You can do that with the parameters or . Modify the code so you can choose the size of the test set and get a reproducible result: With this change, you get a different result from before. Earlier, you had a training set with nine items and a test set with three items. Now, thanks to the argument , the training set has eight items and the test set has four items. You’d get the same result with because 33 percent of twelve is approximately four. There’s one more very important difference between the last two examples: You now get the same result each time you run the function. This is because you’ve fixed the random number generator with . The figure below shows what’s going on when you call : The samples of the dataset are shuffled randomly and then split into the training and test sets according to the size you defined. You can see that has six zeros and six ones. However, the test set has three zeros out of four items. If you want to (approximately) keep the proportion of values through the training and test sets, then pass . This will enable stratified splitting: Now and have the same ratio of zeros and ones as the original array. Stratified splits are desirable in some cases, like when you’re classifying an imbalanced dataset, which is a dataset with a significant difference in the number of samples that belong to distinct classes. Finally, you can turn off data shuffling and random split with : Now you have a split in which the first two-thirds of samples in the original and arrays are assigned to the training set and the last third to the test set. No shuffling. No randomness.\n\nNow it’s time to see in action when solving supervised learning problems. You’ll start with a small regression problem that can be solved with linear regression before looking at a bigger problem. You’ll also see that you can use for classification as well. In this example, you’ll apply what you’ve learned so far to solve a small regression problem. You’ll learn how to create datasets, split them into training and test subsets, and use them for linear regression. As always, you’ll start by importing the necessary packages, functions, or classes. You’ll need NumPy, , and : Now that you’ve imported everything you need, you can create two small arrays, and , to represent the observations and then split them into training and test sets just as you did before: Your dataset has twenty observations, or - pairs. You specify the argument , so the dataset is divided into a training set with twelve observations and a test set with eight observations. Now you can use the training set to fit the model: creates the object that represents the model, while trains, or fits, the model and returns it. With linear regression, fitting the model means determining the best intercept ( ) and slope ( ) values of the regression line. Although you can use and to check the goodness of fit, this isn’t a best practice. An unbiased estimation of the predictive performance of your model is based on test data: returns the coefficient of determination, or R², for the data passed. Its maximum is . The higher the R² value, the better the fit. In this case, the training data yields a slightly higher coefficient. However, the R² calculated with test data is an unbiased measure of your model’s prediction performance. This is how it looks on a graph: The green dots represent the - pairs used for training. The black line, called the estimated regression line, is defined by the results of model fitting: the intercept and the slope. So, it reflects the positions of the green dots only. The white dots represent the test set. You use them to estimate the performance of the model (regression line) with data not used for training. Now you’re ready to split a larger dataset to solve a regression problem. You’ll use the California Housing dataset, which is included in . This dataset has 20640 samples, eight input variables, and the house values as the output. You can retrieve it with . Now that you have both functions imported, you can get the data you’ll work with: As you can see, with the argument returns a tuple with two NumPy arrays: The next step is to split the data the same way as before: Now you have the training and test sets. The training data is contained in and , while the data for testing is in and . When you work with larger datasets, it’s usually more convenient to pass the training or test size as a ratio. means that approximately 40 percent of samples will be assigned to the test data, and the remaining 60 percent will be assigned to the training data. Finally, you can use the training set ( and ) to fit the model and the test set ( and ) for an unbiased evaluation of the model. In this example, you’ll apply three well-known regression algorithms to create models that fit your data: The process is pretty much the same as with the previous example:\n• Import the classes you need.\n• Fit the model instances with using the training set.\n• Evaluate the model with using the test set. Here’s the code that follows the steps described above for all three regression algorithms: You’ve used your training and test datasets to fit three models and evaluate their performance. The measure of accuracy obtained with is the coefficient of determination. It can be calculated with either the training or test set. However, as you already learned, the score obtained with the test set represents an unbiased estimation of performance. As mentioned in the documentation, you can provide optional arguments to , , and . and use the parameter for the same reason that does: to deal with randomness in the algorithms and ensure reproducibility. For some methods, you may also need feature scaling. In such cases, you should fit the scalers with training data and use them to transform test data. You can use to solve classification problems the same way you do for regression analysis. In machine learning, classification problems involve training a model to apply labels to, or classify, the input values and sort your dataset into categories. In the tutorial Logistic Regression in Python, you’ll find an example of a handwriting recognition task. The example provides another demonstration of splitting data into training and test sets to avoid bias in the evaluation process.\n\nThe package offers a lot of functionalities related to model selection and validation, including the following: Cross-validation is a set of techniques that combine the measures of prediction performance to get more accurate model estimations. One of the widely used cross-validation methods is k-fold cross-validation. In it, you divide your dataset into k (often five or ten) subsets, or folds, of equal size and then perform the training and test procedures k times. Each time, you use a different fold as the test set and all the remaining folds as the training set. This provides k measures of predictive performance, and you can then analyze their mean and standard deviation. You can implement cross-validation with , , , and a few other classes and functions from . A learning curve, sometimes called a training curve, shows how the prediction score of training and validation sets depends on the number of training samples. You can use to get this dependency, which can help you find the optimal size of the training set, choose hyperparameters, compare models, and so on. Hyperparameter tuning, also called hyperparameter optimization, is the process of determining the best set of hyperparameters to define your machine learning model. provides you with several options for this purpose, including , , , and others. Splitting your data is also important for hyperparameter tuning."
    },
    {
        "link": "https://geeksforgeeks.org/how-to-split-the-dataset-with-scikit-learns-train_test_split-function",
        "document": "In this article, we will discuss how to split a dataset using scikit-learns’ train_test_split().\n\nThe train_test_split() method is used to split our data into train and test sets. First, we need to divide our data into features (X) and labels (y). The dataframe gets divided into X_train, X_test, y_train, and y_test. X_train and y_train sets are used for training and fitting the model. The X_test and y_test sets are used for testing the model if it’s predicting the right outputs/labels. we can explicitly test the size of the train and test sets. It is suggested to keep our train sets larger than the test sets.\n• Train set: The training dataset is a set of data that was utilized to fit the model. The dataset on which the model is trained. This data is seen and learned by the model.\n• Test set: The test dataset is a subset of the training dataset that is utilized to give an accurate evaluation of a final model fit.\n• validation set: A validation dataset is a sample of data from your model’s training set that is used to estimate model performance while tuning the model’s hyperparameters.\n• underfitting: A data model that is under-fitted has a high error rate on both the training set and unobserved data because it is unable to effectively represent the relationship between the input and output variables.\n• overfitting: when a statistical model matches its training data exactly but the algorithm’s goal is lost because it is unable to accurately execute against unseen data is called overfitting\n\nStep 1: Import the necessary packages or modules:\n\nIn this step, we are importing the necessary packages or modules into the working python environment.\n\nHere, we load the CSV using pd.read_csv() method from pandas and get the shape of the data set using the shape() function.\n\nHere, we are assigning the X and the Y variable in which the X feature variable has independent variables and the y feature variable has a dependent variable.\n\nStep 4: Use the train test split class to split data into train and test sets:\n\nHere, the train_test_split() class from sklearn.model_selection is used to split our data into train and test sets where feature variables are given as input in the method. test_size determines the portion of the data which will go into test sets and a random state is used for data reproducibility.\n\nIn this example, ‘predictions.csv’ file is imported. df.shape attribute is used to retrieve the shape of the data frame. The shape of the dataframe is (13,3). The features columns are taken in the X variable and the outcome column is taken in the y variable. X and y variables are passed in the train_test_split() method to split the data frame into train and test sets. The random state parameter is used for data reproducibility. test_size is given as 0.25 which means 25% of the data goes into the test sets. 4 out of 13 rows in the dataframe go into the test sets. 75% of data goes into the train sets, which is 9 rows out of 13 rows. The train sets are used to fit and train the machine learning model. The test sets are used for evaluation.\n\nIn this example the following steps are executed :\n• The necessary packages are imported.\n• Advertising.csv data set is loaded and cleaned, and null values are dropped.\n• The arrays created are split into train and test sets. 30% of the dataset goes into the test set, which means 70% data is a train set.\n• X_train is fit into the scaler.\n• X_train and X_test are transformed using the transform() method.\n• the predict() method is used to carry out predictions on the X_test set.\n• mean_squared_error() metric is used to evaluate the model.\n\nTo view and download the CSV file used in this example, click here.\n\nIn this example, we’re gonna use the K-nearest neighbors classifier model.\n\nIn this example the following steps are executed :\n• The necessary packages are imported.\n• The arrays created are split into train and test sets. 30% of the dataset goes into the test set, which means 70% data is a train set.\n• A basic Knn model is created using the KNeighborsClassifier class.\n• the predict() method is used to carry out predictions on the X_test set."
    },
    {
        "link": "https://builtin.com/data-science/train-test-split",
        "document": "A goal of supervised learning is to build a machine learning model that performs well on new data. If you have new data, it’s a good idea to see how your model performs on it. The problem is that you may not have new data, but you can simulate this experience with a procedure like train test split.\n• What is the train test split procedure?\n• How to use train test split to tune models in Python.\n\nIf you would like to follow along, the code and images used in this tutorial is available on GitHub. With that, let’s get started.\n\nWhat Is the Train Test Split Procedure?\n\nTrain test split is a model validation procedure that allows you to simulate how a model would perform on new/unseen data by splitting a dataset into a training set and a testing set. The training set is data used to train the model, and the testing set data (which is new to the model) is used to test the model’s performance and accuracy. A train test split can also involve splitting data into a validation set, which is data used to fine-tune hyperparameters and optimize the model during the training process.\n\nHere is how the train test split procedure works:\n\nMake sure your data is arranged into a format acceptable for train test split. In scikit-learn, this consists of separating your full dataset into “Features” and “Target.”\n\nSplit the dataset into two pieces — a training set and a testing set. This consists of random sampling without replacement about 75 percent of the rows (you can vary this) and putting them into your training set. The remaining 25 percent is put into your test set. Note that the colors in “Features” and “Target” indicate where their data will go (“X_train,” “X_test,” “y_train,” “y_test”) for a particular train test split.\n\nTrain the model on the training set. This is “X_train” and “y_train” in the image.\n\nTest the model on the testing set (“X_test” and “y_test” in the image) and evaluate the performance.\n\nDifferent kinds of datasets require different methods for splitting the data into training and testing sets. Here’s some common methods of splitting data in a train test split:\n\nRandom splitting involves randomly shuffling data and splitting it into training and testing sets based on given percentages (like 75% training and 25% testing). This is one of the most popular methods for splitting data in train test splits due to it being simple and easy to implement, and is used by default in the scikit-learn method. Random splitting is most effective for large, diverse datasets where categories are generally represented equally in the data.\n\nStratified splitting divides a dataset in a way that preserves its proportion of classes or categories. This creates training and testing sets with class proportions representative of the original dataset. Using stratified splitting can prevent model bias, and is most effective for imbalanced datasets or datasets where categories aren’t represented equally. In a scikit-learn train test split, stratified splitting can be used by specifying the parameter in the method.\n\nTime-based splitting involves organizing data in a set by points in time, ensuring past data is in the training set and future or later data is in the testing set. Splitting data based on time works to simulate real-world scenarios (for example, predicting future financial or market trends) and allows for time series analysis on time series datasets. However, one drawback to time-based splitting is that it may not fully capture trends for non-stationary data (data that continually changes over time). In scikit-learn, time series data can be split into training and testing sets by using the method.\n\nConsequences of Not Using Train Test Split\n\nYou could try not using train test split and instead train and test the model on the same data. However, I don’t recommend this approach as it doesn’t simulate how a model would perform on new data. It also tends to reward overly complex models that overfit on the dataset, which means a model doesn’t generalize and attempts to fit data too closely to training data.\n\nThe steps below go over how this inadvisable process works.\n\nMake sure your data is arranged into a format acceptable for train test split. In scikit-learn, this consists of separating your full dataset into “Features” and “Target.”\n\nTrain the model on “Features” and “Target.”\n\nTest the model on “Features” and “Target” and evaluate the performance.\n\nI want to emphasize again that training on an entire dataset and then testing on that same dataset can lead to overfitting. Overfitting is illustrated in the image below. The green squiggly line best follows the training data. The problem is that it is likely overfitting on the training data, meaning it is likely to perform worse on new data.\n\nRelatedHow to Use the Z-Table\n\nThis section is about the practical application of train test split as a way to predict home prices. It spans everything from importing a dataset to performing a train test split to hyperparameter tuning a decision tree regressor to predicting home prices and more.\n\nPython has a lot of libraries that help you accomplish your data science goals including scikit-learn, pandas, and NumPy, which the code below imports.\n\nKaggle hosts a dataset which contains the price at which houses were sold for King County, which includes Seattle between May 2014 and May 2015. You can download the dataset from Kaggle or load it from my GitHub. The code below loads the dataset.\n\nScikit-learn’s expects data in the form of features and target. In scikit-learn, a features matrix is a two-dimensional grid of data where rows represent samples and columns represent features. A target is what you want to predict from the data. This tutorial uses price as a target.\n\nIn the code below, splits the data and returns a list which contains four NumPy arrays, while puts 75 percent of the data into a training set and the remaining 25 percent into a testing set.\n\nThe image below shows the number of rows and columns the variables contain using the shape attribute before and after the .\n\nShape before and after percent of the rows went to the training set and 25 percent went to the test set .\n\n4. What Is ‘random_state’ in Train Test Split?\n\nThe image above shows that if you select a different value for , different information would go to “X_train,” “X_test,” “y_train” and “y_test”. The is a pseudo-random number parameter that allows you to reproduce the same train test split each time you run the code.\n\nThere are a number of reasons why people use , including software testing, tutorials like this one and talks. However, it is recommended you remove it if you are trying to see how well a model generalizes to new data.\n\nHere’s what you need to know about using train test split in scikit-learn:\n\n1. Import the Model You Want to Use\n\nIn scikit-learn, all machine learning models are implemented as Python classes.\n\n2. Make An Instance of the Model\n\nIn the code below, I set the hyperparameter to pre-prune my tree to make sure it doesn’t have a depth greater than two. The next section of the tutorial will go over how to choose an optimal for your tree. Also note that I made so that you can get the same results as me.\n\n3. Train the Model on the Data\n\nTrain the model on the data, storing the information learned from the data.\n\nFor the multiple predictions above, notice how many times some of the predictions are repeated. If you are wondering why, I encourage you to check out the code below, which will start by looking at a single observation/house and then proceed to examine how the model makes its prediction.\n\nThe code below shows how to make a prediction for that single observation.\n\nThe image below shows how the trained model makes a prediction for the one observation.\n\nIf you are curious how these sorts of diagrams are made, consider checking out my tutorial Visualizing Decision Trees using Graphviz and Matplotlib.\n\nWhile there are other ways of measuring model performance such as root-mean-square error, and mean absolute error, we are going to keep this simple and use R² — known as the coefficient of determination — as our metric.\n\nThe best possible score is 1.0. A constant model that would always predict the mean value of price would get a R² score of 0.0. However, it is possible to get a negative R² on the test set. The code below uses the trained model’s score method to return the R² of the model that we evaluated on the test set.\n\nYou might be wondering if our R² above is good for our model. In general, the higher the R², the better the model fits the data. Determining whether a model is performing well can also depend on your field of study. Something harder to predict will generally have a lower R². My argument is that for housing data, we should have a higher R² based solely on our data.\n\nDomain experts generally agree that one of the most important factors in housing prices is location. After all, if you are looking for a home, you’ll most likely care where it’s located. As you can see in the trained model below, the decision tree only incorporates .\n\nEven if the model was performing very well, it is unlikely that it would get buy-in from stakeholders or coworkers since there is more to a home than .\n\nNote that the original dataset has location information like “lat” and “long.” The image below visualizes the price percentile of all the houses in the dataset based on “lat” and “long,” neither were included in the data the model trained on. As you can see, there is a relationship between home price and location.\n\nYou can incorporate location information like “lat” and “long” as a way to improve the model. It’s likely places like Zillow found a way to incorporate that into their models.\n\nHow to Tune the ‘max_depth’ of a Tree\n\nThe R² for the model trained earlier in the tutorial was about .438. However, suppose we want to improve the performance so that we can better make predictions on unseen data. While we could add more features like “lat” and “long” to the model or increase the number of rows in the dataset (i.e. find more houses), we could also improve performance through hyperparameter tuning.\n\nThis involves selecting the optimal values of tuning parameters for a machine learning problem, which are often called hyperparameters. But first, we need to briefly go over the difference between parameters and hyperparameters. Parameters vs. Hyperparameters\n\nA machine learning algorithm estimates model parameters for a given dataset and updates these values as it continues to learn. You can think of a model parameter as a learned value from applying the fitting process. For example, in logistic regression you have model coefficients. In a neural network, you can think of neural network weights as a parameter. Hyperparameters or tuning parameters are metaparameters that influence the fitting process itself.\n\nFor logistic regression, there are many hyperparameters like regularization strength C. For a neural network, there are many hyperparameters like the number of hidden layers. If all of this sounds confusing, Jason Brownlee, founder of Machine Learning Mastery, offers a good rule of thumb in his guide on parameters and hyperparameters which is: “If you have to specify a model parameter manually, then it is probably a model hyperparameter.”\n\nThere are a lot of diff erent ways to hyperparameter tune a decision tree for regression. One way is to tune the hyperparameter. The (hyperparameter) is not the same thing as depth (parameter of a decision tree), but is a way to pre-prune a decision tree. In other words, if a tree is already as pure as possible at a depth, it will not continue to split. If this isn’t clear, check out my Understanding Decision Trees for Classification (Python) tutorial to see the difference between and depth.\n\nThe code below outputs the accuracy for decision trees with different values for .\n\nThe graph below shows that the best model R² is when the hyperparameter is equal to 5. This process of selecting the best model ( in this case) among many other candidate models (with different values in this case) is called model selection.\n\nNote that the model above could have still been overfitted on the test set since the code changed repeatedly to achieve the best model. In other words, knowledge of the test set could have leaked into the model as the code iterated through 24 different values for (the length of is 24). This would lessen the power of our evaluation metric R², as it would no longer be as strong an indicator of generalization performance. This is why in real life, we often have training, test and validation sets when hyperparameter tuning.\n\nIn order to understand why of 5 was the “Best Model” for our data, take a look at the graph below, which shows the model performance when tested on the training and test set.\n\nNaturally, the training R² is always better than the test R² for every point on this graph because the models are making predictions on data they have seen before.\n\nTo the left side of the “Best Model” on the graph (anything less than ), we have models that underfit the data and are considered high bias because they do not have enough complexity to learn enough about the data.\n\nTo the right side of the “Best Model” on the graph (anything more than ), we have models that overfit the data and are considered high variance because they are overly-complex models. These models perform well on the training data but badly on testing data.\n\nThe “Best Model” is formed by minimizing bias error — or bad assumptions in the model — and variance error — or oversensitivity to small fluctuations/noise in the training set.\n\nA goal of supervised learning is to build a model that performs well on new data, which train test split helps you simulate. With any model validation procedure it’s important to keep in mind the advantages and disadvantages.\n\nThe advantages of using a train test split include:\n• A relatively fast way to evaluate model performance with new data.\n• Simplicity and ease of understanding compared to other methods, such as K-fold cross validation.\n• Avoidance of overly complex models that don’t generalize well to new data.\n\nThe disadvantages of using a train test split include:\n• High variance, as results may vary depending on the specific train test split ( ).\n• Unreliable model performance results when using a dataset that is too small.\n• Loss of data that could have been used for training a machine learning model, since testing data isn’t used for training.\n• Risk of data leakage during hyperparameter tuning, where knowledge of the test set influences the model. This can be partially solved by using separate training, test, and validation sets."
    },
    {
        "link": "https://medium.com/@luo.li086/its-important-to-understand-train-test-split-when-splitting-data-to-train-ml-model-5fa843c82b3b",
        "document": "It’s important to understand train_test_split() when splitting data to train ML model\n\nIn machine learning, one important step before training the model is to split the dataset into train data and test data, which are used to train the model and evaluate the performance of the trained model. Here, train_test_split in sklearn package is used to randomly split the dataset.\n\n*arrays: variable number of arguments. Allowed arguments are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n\nSince most of the time, in a machine learning project or data analysis project, the dataset is first imported into a dataframe, so it’s comfortable to use dataframe to hold the input dataset, which includes the prediction target y and Features X.\n• If it is a float number, it should be between 0.0 and 1.0 to represent the proportion of dataset for the test- dataset.\n• If it is int, it represents the absolute number of samples for test-dataset.\n• if test_size is None, it will be set defaultly with 0.25.\n• If it is a float number, it should be between 0.0 and 1.0 to represent the proportion of dataset for the train dataset.\n• If it is int, it represents the absolute number of samples for the train dataset.\n• if train_size is None, it will be automatically set to the complement of test_size.\n\nIt controls the shuffling process of data before splitting. Without a concrete value of random_state, each time we execute the splitting function, we get a different test dataset and train dataset, since the shuffling process is differently executed each time. With a setting value of random_state, it ensures that each time after execution of our code, we get the same test dataset and train dataset, which is helpful to make the results consistent.\n\nWhether or not to shuffle the data before splitting.\n\nIf it is not None, data is split in a stratified fashion, which is usually used for classification problems to ensure that the data samples of each class in test dataset and train dataset have the same proportion as the original dataset."
    }
]