[
    {
        "link": "https://pandas.pydata.org",
        "document": ""
    },
    {
        "link": "https://pandas.pydata.org/docs",
        "document": "The reference guide contains a detailed description of the pandas API. The reference describes how the methods work and which parameters can be used. It assumes that you have an understanding of the key concepts."
    },
    {
        "link": "https://geeksforgeeks.org/pandas-tutorial",
        "document": "Pandas, which is styled as pandas is an open-source software library designed for the Python programming language, focusing on data manipulation and analysis. It provides data structures like series and DataFrames to effectively easily clean, transform, and analyze large datasets and integrates seamlessly with other Python libraries, such as NumPy and Matplotlib.\n\nIt offers powerful functions for data transformation, aggregation, and visualization, which are important for credible analysis. Created by Wes McKinney in 2008, Pandas has grown to become a cornerstone of data analysis in Python, widely used by data scientists, analysts and researchers worldwide. Pandas revolves around two primary Data structures: Series (1D) for single columns and DataFrame (2D) for tabular data enabling efficient data manipulation.\n\nWhat is Pandas Used for?\n\nWith pandas, you can perform a wide range of data operations, including\n• None Reading and writing data from various file formats like CSV, Excel, and SQL databases.\n• None Cleaning and preparing data by handling missing values and filtering entries.\n\nNow that we know what pandas are and their uses, let’s move towards the tutorial part. In the section below, you will find 8 sections, from basic to advanced, that will help you learn more about pandas.\n\nIn this section, we will explore the fundamentals of Pandas. We will start with an introduction to Pandas, learn how to install it, and get familiar with its core functionalities. Additionally, we will cover how to use Jupyter Notebook, a popular tool for interactive coding. By the end of this section, we will have a solid understanding of how to set up and start working with Pandas for data analysis.\n• None How To Use Jupyter Notebook\n\nA DataFrame is a two-dimensional, size-mutable and potentially heterogeneous tabular data structure with labeled axes (rows and columns)., think of it as a table or a spreadsheet.\n\nA Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating-point numbers, Python objects, etc.). It’s similar to a column in a spreadsheet or a database table.\n\nPandas offers a variety of functions to read data from and write data to different file formats as given below:\n\nData cleaning is an essential step in data preprocessing to ensure accuracy and consistency. Here are some articles to know more about it:\n\nWe will cover data processing, normalization, manipulation, and analysis, along with techniques for grouping and aggregating data. These concepts will help you efficiently clean, transform, and analyze datasets. By the end of this section, you’ll be equipped with essential Pandas operations to handle real-world data effectively.\n• None Different Types of Joins in Pandas\n\nIn this section, we will explore advanced Pandas functionalities for deeper data analysis and visualization. We will cover techniques for finding correlations, working with time series data, and using Pandas’ built-in plotting functions for effective data visualization. By the end of this section, you’ll have a strong grasp of advanced Pandas operations and how to apply them to real-world datasets.\n\nTest your knowledge of Python’s pandas library with this quiz. It’s designed to help you check your knowledge of key topics like handling data, working with DataFrames, and creating visualizations.\n\nIn this section, we will work on real-world data analysis projects using Pandas and other data science tools. These projects will cover various domains, including food delivery, sports, travel, healthcare, real estate, and retail. By analyzing datasets like Zomato, IPL, Airbnb, COVID-19, and Titanic, we will apply data processing, visualization, and predictive modeling techniques. By the end of this section, you will gain hands-on experience in data analysis and machine learning applications.\n\nWhat is pandas used for in Python?\n\nWhat are the Benifits of Pandas?\n\nWhat are Pandas best used for?\n\nWhat are the basic data structures in Pandas?\n\nWhen to use pandas?\n\nWhat are the disadvantages of Pandas?"
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/index.html",
        "document": "The User Guide covers all of pandas by topic area. Each of the subsections introduces a topic (such as “working with missing data”), and discusses how pandas approaches the problem, with many examples throughout.\n\nUsers brand-new to pandas should start with 10 minutes to pandas.\n\nFor a high level summary of the pandas fundamentals, see Intro to data structures and Essential basic functionality.\n\nFurther information on any specific method can be obtained in the API reference.\n\nHow to read these guides# In these guides you will see input code inside code blocks such as: The first block is a standard python input, while in the second the indicates the input is inside a notebook. In Jupyter Notebooks the last line is printed and plots are shown inline."
    },
    {
        "link": "https://github.com/pandas-dev/pandas",
        "document": "pandas is a Python package that provides fast, flexible, and expressive data structures designed to make working with \"relational\" or \"labeled\" data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language. It is already well on its way towards this goal.\n• Where to get it\n\nHere are just a few of the things that pandas does well:\n• Easy handling of missing data (represented as , , or ) in floating point as well as non-floating point data\n• Size mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects\n• Automatic and explicit data alignment: objects can be explicitly aligned to a set of labels, or the user can simply ignore the labels and let , , etc. automatically align the data for you in computations\n• Powerful, flexible group by functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data\n• Make it easy to convert ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects\n• Hierarchical labeling of axes (possible to have multiple labels per tick)\n• Robust IO tools for loading data from flat files (CSV and delimited), Excel files, databases, and saving/loading data from the ultrafast HDF5 format\n\nThe source code is currently hosted on GitHub at: https://github.com/pandas-dev/pandas\n\nBinary installers for the latest released version are available at the Python Package Index (PyPI) and on Conda.\n\nThe list of changes to pandas between each release can be found here. For full details, see the commit logs at https://github.com/pandas-dev/pandas.\n• NumPy - Adds support for large, multi-dimensional arrays, matrices and high-level mathematical functions to operate on these arrays\n• python-dateutil - Provides powerful extensions to the standard datetime module\n• pytz - Brings the Olson tz database into Python which allows accurate and cross platform timezone calculations\n\nSee the full installation instructions for minimum supported versions of required, recommended and optional dependencies.\n\nTo install pandas from source you need Cython in addition to the normal dependencies above. Cython can be installed from PyPI:\n\nIn the directory (same one where you found this file after cloning the git repo), execute:\n\nor for installing in development mode:\n\nSee the full instructions for installing from source.\n\nThe official documentation is hosted on PyData.org.\n\nWork on started at AQR (a quantitative hedge fund) in 2008 and has been under active development since then.\n\nFor usage questions, the best place to go to is StackOverflow. Further, general questions and discussions can also take place on the pydata mailing list.\n\nMost development discussions take place on GitHub in this repo, via the GitHub issue tracker.\n\nFurther, the pandas-dev mailing list can also be used for specialized discussions or design issues, and a Slack channel is available for quick development related questions.\n\nThere are also frequent community meetings for project maintainers open to the community as well as monthly new contributor meetings to help support new contributors.\n\nAdditional information on the communication channels can be found on the contributor community page.\n\nAll contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.\n\nA detailed overview on how to contribute can be found in the contributing guide.\n\nIf you are simply looking to start working with the pandas codebase, navigate to the GitHub \"issues\" tab and start looking through interesting issues. There are a number of issues listed under Docs and good first issue where you could start out.\n\nYou can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to subscribe to pandas on CodeTriage.\n\nOr maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking ‘this can be improved’...you can do something about it!\n\nFeel free to ask questions on the mailing list or on Slack.\n\nAs contributors and maintainers to this project, you are expected to abide by pandas' code of conduct. More information can be found at: Contributor Code of Conduct"
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html",
        "document": "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Trees in the forest use the best split strategy, i.e. equivalent to passing to the underlying . The sub-sample size is controlled with the parameter if (default), otherwise the whole dataset is used to build each tree.\n\nFor a comparison between tree-based ensemble models see the example Comparing Random Forests and Histogram Gradient Boosting models.\n\nRead more in the User Guide.\n\nThe number of trees in the forest. Changed in version 0.22: The default value of changed from 10 to 100 in 0.22. The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain, see Mathematical formulation. Note: This parameter is tree-specific. The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. The minimum number of samples required to split an internal node:\n• None If int, then consider as the minimum number.\n• None If float, then is a fraction and are the minimum number of samples for each split. The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n• None If int, then consider as the minimum number.\n• None If float, then is a fraction and are the minimum number of samples for each node. The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. The number of features to consider when looking for the best split:\n• None If int, then consider features at each split.\n• None If float, then is a fraction and features are considered at each split.\n• None If None, then . Changed in version 1.1: The default of changed from to . Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than features. Grow trees with in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following: where is the total number of samples, is the number of samples at the current node, is the number of samples in the left child, and is the number of samples in the right child. , , and all refer to the weighted sum, if is passed. Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree. Whether to use out-of-bag samples to estimate the generalization score. By default, is used. Provide a callable with signature to use a custom metric. Only available if . The number of jobs to run in parallel. , , and are all parallelized over the trees. means 1 unless in a context. means using all processors. See Glossary for more details. Controls both the randomness of the bootstrapping of the samples used when building trees (if ) and the sampling of the features to consider when looking for the best split at each node (if ). See Glossary for details. Controls the verbosity when fitting and predicting. When set to , reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See Glossary and Fitting additional trees for details. Weights associated with classes in the form . If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as The “balanced_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown. For multi-output, the weights of each column of y will be multiplied. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than will be chosen. By default, no pruning is performed. See Minimal Cost-Complexity Pruning for details. See Post pruning decision trees with cost complexity pruning for an example of such pruning. If bootstrap is True, the number of samples to draw from X to train each base estimator.\n• None If None (default), then draw samples.\n• None If float, then draw samples. Thus, should be in the interval . Indicates the monotonicity constraint to enforce on each feature. If monotonic_cst is None, no constraints are applied. Monotonicity constraints are not supported for: The constraints hold over the probability of the positive class. Read more in the User Guide. The child estimator template used to create the collection of fitted sub-estimators. Added in version 1.2: was renamed to . classes_ ndarray of shape (n_classes,) or a list of such arrays The classes labels (single output problem), or a list of arrays of class labels (multi-output problem). The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem). Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings. The number of outputs when is performed. Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when is True. Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, might contain NaN. This attribute exists only when is True. The subset of drawn samples for each base estimator.\n\nThe default values for the parameters controlling the size of the trees (e.g. , , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.\n\nThe features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, and , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, has to be fixed.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://datacamp.com/tutorial/random-forests-classifier-python",
        "document": "Master the basics of data analysis with Python in just four hours. This online course will introduce the Python interface and explore popular packages."
    },
    {
        "link": "https://geeksforgeeks.org/random-forest-classifier-using-scikit-learn",
        "document": "Random Forest is a method that combines the predictions of multiple decision trees to produce a more accurate and stable result. It can be used for both classification and regression tasks.\n\nIn classification tasks, Random Forest Classification predicts categorical outcomes based on the input data. It uses multiple decision trees and outputs the label that has the maximum votes among all the individual tree predictions and in this article we will learn more about it.\n\nRandom Forest Classification works by creating multiple decision trees each trained on a random subset of data. The process begins with Bootstrap Sampling where random rows of data are selected with replacement to form different training datasets for each tree.\n\nThen where only a random subset of features is used to build each tree ensuring diversity across the models.\n\nDuring the training phase Feature Sampling is applied to each tree built by recursively partitioning the data based on the features. At each split the algorithm selects the best feature from the random subset optimizing for information gain or Gini impurity. The process continues until a predefined stopping criterion is met such as reaching maximum depth or having a minimum number of samples in each leaf node. After the trees are trained each tree makes a prediction. The final prediction for classification tasks is determined by majority voting.\n• None By combining predictions from many decision trees it reduces the risk of overfitting compared to a single decision tree.\n• None It is robust to noisy data and works well with categorical data.\n\nBefore implementing random forest classifier in Python let’s first understand it’s parameters.\n• n_estimators: Number of trees in the forest.\n• max_features: Number of features considered for splitting at each node.\n• criterion: Function used to measure split quality (‘gini’ or ‘entropy’).\n• min_samples_leaf: Minimum samples required to be at a leaf node.\n• bootstrap: Whether to use bootstrap sampling when building trees (True or False).\n\nNow that we know it’s parameters we can start building it in python.\n\nWe will be importing Pandas, matplotlib, seaborn and sklearn to build the model.\n\nFor this we’ll use the Iris Dataset which is available within . This dataset contains information about three types of Iris flowers and their respective features (sepal length, sepal width, petal length and petal width).\n\nHere we will separate the features (X) and the target variable (y).\n\nWe’ll split the dataset into training and testing sets so we can train the model on one part and evaluate it on another.\n\nFeature scaling ensures that all the features are on a similar scale which is important for some machine learning models. However Random Forest is not highly sensitive to feature scaling. But it is a good practice to scale when combining models.\n\nWe will create the Random Forest Classifier model, train it on the training data and make predictions on the test data.\n\nWe will evaluate the model using the accuracy score and confusion matrix.\n\nIts perfect accuracy along with the confusion matrix shown by Random Forest Classifier has learned to classify all the instances correctly. However it’s essential to note that the Iris dataset used here is relatively simple and well-known in the machine learning\n\nRandom Forest Classifiers also provide insight into which features were the most important in making predictions. We can plot the feature importance.\n\nFrom the graph we can see that petal width (cm) is the most important feature followed closely by petal length (cm). The sepal width (cm) and sepal length (cm) have lower importance in determining the model’s predictions. This indicates that the classifier relies more on the petal measurements to make predictions about the flower species.\n\nRandom Forest Classifiers are useful for classification tasks offering high accuracy and robustness. They are easy to use, provide insights into feature importance and can handle complex datasets.\n\nWhat is the random forest classifier?\n\nCan random forest be used for regression ?\n\nWhat is the principle of random forest?"
    },
    {
        "link": "https://scikit-learn.org/stable/api/index.html",
        "document": ""
    },
    {
        "link": "https://medium.com/codenx/machine-learning-using-scikit-learn-sklearn-evaluating-classification-model-using-metrics-018857ea4b69",
        "document": "Scikit-learn, commonly known as sklearn, stands out as one of the most influential and extensively utilized machine learning libraries in Python. It offers a comprehensive array of tools and pre-trained models, encompassing pre-processing utilities, model training, and model evaluation functionalities.\n\nThere are three distinct APIs for assessing the quality of a model’s predictions:\n• Estimator score method: Estimators come equipped with a score method that offers a default evaluation criterion tailored to the problem they are designed to address.\n• Scoring parameter: Tools for model evaluation that utilize cross-validation, such as model_selection.cross_val_score and model_selection.GridSearchCV, rely on an internal scoring strategy.\n• Metric functions: The sklearn.metrics module incorporates functions designed to assess prediction errors for specific purposes.\n\nIn this blog we will see how to evaluate a classification problem/model.\n\nFirst let us score a classification problem, in this scenario taking new data set from Kaggle on if water is safe to drink or not based on different parameters. You can download the data set from here.\n\nWe know scoring parameter uses cross-validation. Let us understand what cross validation is, in normal train and test split data is split into training and test and wont change. But in cross validation we split the data multiple times by picking different versions of test data based on types of cross-validation. For understanding different types of cross-validation you can read here. Let us see how K-Fold cross validation works using below image. Basically, in cross validation out model will be fitted and evaluated on complete data set. That is the reason we will get score array when we use cross validation.\n\nLet us see an example.\n\nIf you notice as the number of folds increase the accuracy also varies since it depends on the data set it got fitted. Also, as we increase the number of folds time for evaluation also increases. If you calculate the mean on the cross-validation array it will be lower than regular score function.\n\nOne more thing we can change with cross validation is changing the scoring parameter. As shown in above code default is None, which will be set to ‘mean accuracy’ scoring since this is classification problem. But there are other scoring options available for the scoring parameter, below screen shot shows some of them.\n\nA Machine Learning Metric, or ML Metric, serves as a performance measure for an ML model in each task, aiding in the assessment of its effectiveness or the need for improvement. This task may encompass various ML objectives such as classification, regression, clustering, and more. ML Metrics typically yield numerical values, guiding decisions on whether to retain the model, explore alternative algorithms, or engage in hyperparameter tuning. In classification tasks, a common metric is `accuracy` indicating the proportion of correctly predicted labels. For regression tasks, it might involve the mean absolute error, providing an average measure of the prediction’s deviation from actual values. There can be multiple metrics offering insights into the model’s performance from different perspectives, including accuracy, ROC AUC curve, confusion matrix, etc., particularly in classification tasks.\n\nIn scikit-learn, the default metric for classification is ‘accuracy,’ representing the number of correctly classified labels. For regression tasks, the default metric is ‘r2,’ denoting the coefficient of determination. This can be achieved by using `score()` function or scoring parameter. But in the section, we will learn more about different functions for assessing prediction error for specific purposes.\n\nThere are different classification model metrics available, we will go through each of them.\n\nBefore we dig deep into metric functions, we need to understand some key terms in classification model evaluation. Let us take heart disease as example and explain them.\n• True Positives (TP): These are instances where the model correctly predicts the positive class (e.g., presence of a disease) when it is indeed present in the actual data.\n• True Negatives (TN): True Negatives occur when the model correctly predicts the negative class (e.g., absence of a disease) when it is indeed not present in the actual data.\n• False Positives (FP): These occur when the model incorrectly predicts the positive class when it is not present in the actual data.\n• False Negatives (FN): False Negatives happen when the model incorrectly predicts the negative class when it is, in fact, the positive class.\n\nSensitivity tells us what percentage of patients with heart disease are correctly Identified. Sensitivity is also known as True Positive rate and some cases called recall.\n\nSpecificity tells us what percentage of patients without heart disease were correctly identified. Specificity is also known as True Negative Rate\n\nPrecision tells us what percentage of patients with heart disease are correctly predicted.\n\nAccuracy tells us what percentage of patients with or without heart disease are correctly predicted.\n\nAccuracy is the ratio of true predictions to the total number of samples. It indicates the percentage or portion of examples that the model accurately predicted. For this we can use `accuracy_score()` function. Also, all classification models by default calculate accuracy when we call their `score()` methods to evaluate model performance. Accuracy might be deceptive when handling imbalanced datasets, characterized by a significant disparity in the size of one class compared to the other.\n\nThe Receiver Operating Characteristic (ROC) curve serves as a graphical representation of a classification model’s efficacy in distinguishing between positive and negative classes across various classification thresholds. It showcases the True Positive Rate (TPR), also known as recall or sensitivity, plotted against the False Positive Rate (FPR), calculated as 1−Specificity.\n\nThe ROC curve visually demonstrates how the model’s performance varies as the threshold for classifying an instance as positive changes. Key points about the ROC curve:\n• The x-axis represents the False Positive Rate (FPR), indicating the proportion of negative instances incorrectly classified as positive.\n• The y-axis represents the True Positive Rate (TPR), denoting the proportion of positive instances correctly classified as positive.\n• A typical ROC curve follows an ascending trajectory, moving from the bottom-left corner to the top-right corner of the plot. The ideal ROC curve would be a right-angle (90-degree) curve from the bottom-left corner to the top-left corner, signifying perfect discrimination between positive and negative instances at all thresholds.\n\nThe Area Under the ROC Curve (AUC) serves as a quantitative measure of a classification model’s overall performance. It quantifies the area under the ROC curve, ranging from 0 to 1, where:\n• An AUC of 0.5 indicates the model’s performance is equivalent to random guessing.\n• An AUC of 1.0 signifies perfect discrimination, where the model can perfectly distinguish between positive and negative instances at all thresholds.\n• The AUC provides a unified scalar value summarizing the model’s ability to rank positive instances higher than negative instances, irrespective of the specific threshold chosen for classification. Higher AUC values indicate superior model performance.\n\nLet us do an example.\n\nFor confusion matrix we can use seaborn heatmap for visualizing or need sklearn 1.0+ we will try doing both, but first lets us see how we can add a package from Jupyter notebook itself.\n\nIn general, if we get this error when adding new package, it means package is not installed and you can add it in Jupyter notebook as shown above.\n\nNow let us do confusion matrix using sklearn itself.\n\nWe know what precision, recall and accuracy are. Let us look at rest of the terms.\n\nf1-score — A combination of precision and recall. A perfect model achieves an F1 score of 1.0.\n\nsupport — The number of samples each metric was calculated on.\n\nmacro avg — Short of macro average, the average precision, recall and F1 score between classes (0 and 1). Macro avg doesn’t class imbalance into effort, so if you do have class imbalances, pay attention to this metric.\n\nweighted avg — Short for weighted average, the weighted average precision, recall and f1-score between classes. Weighted means each metric is calculated with respect to how many samples there are in each class. This metric will favor the majority class (ex: will give a high value when one class outperforms another due to having more samples).\n\nYou might be wondering where will be classification report useful? It is useful to determine what metric we should really consider. Let us take an example with dataset of 10000 patients with just 1 positive and see the classification report.\n\nWith this we covered all the important metrics used in classification problem, there are other metrics as well like Precision-Recall Curve, Balanced Accuracy Score, Hamming Loss etc which should be used as required in scenarios. In the next blog we will see how to evaluate a Regression model using metrics."
    }
]