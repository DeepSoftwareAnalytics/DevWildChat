[
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/api/winuser/nf-winuser-peekmessagea",
        "document": "Dispatches incoming nonqueued messages, checks the thread message queue for a posted message, and retrieves the message (if any exist).\n\nA pointer to an MSG structure that receives message information.\n\nA handle to the window whose messages are to be retrieved. The window must belong to the current thread.\n\nIf hWnd is NULL, PeekMessage retrieves messages for any window that belongs to the current thread, and any messages on the current thread's message queue whose hwnd value is NULL (see the MSG structure). Therefore if hWnd is NULL, both window messages and thread messages are processed.\n\nIf hWnd is -1, PeekMessage retrieves only messages on the current thread's message queue whose hwnd value is NULL, that is, thread messages as posted by PostMessage (when the hWnd parameter is NULL) or PostThreadMessage.\n\nThe value of the first message in the range of messages to be examined. Use WM_KEYFIRST (0x0100) to specify the first keyboard message or WM_MOUSEFIRST (0x0200) to specify the first mouse message.\n\nIf wMsgFilterMin and wMsgFilterMax are both zero, PeekMessage returns all available messages (that is, no range filtering is performed).\n\nThe value of the last message in the range of messages to be examined. Use WM_KEYLAST to specify the last keyboard message or WM_MOUSELAST to specify the last mouse message.\n\nIf wMsgFilterMin and wMsgFilterMax are both zero, PeekMessage returns all available messages (that is, no range filtering is performed).\n\nSpecifies how messages are to be handled. This parameter can be one or more of the following values.\n\nBy default, all message types are processed. To specify that only certain message should be processed, specify one or more of the following values.\n\nFor more info on listed flags and types of messages see GetQueueStatus documentation.\n\nIf a message is available, the return value is nonzero.\n\nIf no messages are available, the return value is zero.\n\nPeekMessage retrieves messages associated with the window identified by the hWnd parameter or any of its children as specified by the IsChild function, and within the range of message values given by the wMsgFilterMin and wMsgFilterMax parameters. Note that an application can only use the low word in the wMsgFilterMin and wMsgFilterMax parameters; the high word is reserved for the system.\n\nNote that PeekMessage always retrieves WM_QUIT messages, no matter which values you specify for wMsgFilterMin and wMsgFilterMax.\n\nDuring this call, the system dispatches (DispatchMessage) pending, nonqueued messages, that is, messages sent to windows owned by the calling thread using the SendMessage, SendMessageCallback, SendMessageTimeout, or SendNotifyMessage function. Then the first queued message that matches the specified filter is retrieved. The system may also process internal events. If no filter is specified, messages are processed in the following order:\n\nTo retrieve input messages before posted messages, use theandparameters.\n\nThe PeekMessage function normally does not remove WM_PAINT messages from the queue. WM_PAINT messages remain in the queue until they are processed. However, if a WM_PAINT message has a NULL update region, PeekMessage does remove it from the queue.\n\nIf a top-level window stops responding to messages for more than several seconds, the system considers the window to be not responding and replaces it with a ghost window that has the same z-order, location, size, and visual attributes. This allows the user to move it, resize it, or even close the application. However, these are the only actions available because the application is actually not responding. When an application is being debugged, the system does not generate a ghost window.\n\nThis API does not participate in DPI virtualization. The output is in the mode of the window that the message is targeting. The calling thread is not taken into consideration.\n\nFor an example, see Examining a Message Queue."
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/api/winuser/nf-winuser-peekmessagew",
        "document": "Dispatches incoming nonqueued messages, checks the thread message queue for a posted message, and retrieves the message (if any exist).\n\nA pointer to an MSG structure that receives message information.\n\nA handle to the window whose messages are to be retrieved. The window must belong to the current thread.\n\nIf hWnd is NULL, PeekMessage retrieves messages for any window that belongs to the current thread, and any messages on the current thread's message queue whose hwnd value is NULL (see the MSG structure). Therefore if hWnd is NULL, both window messages and thread messages are processed.\n\nIf hWnd is -1, PeekMessage retrieves only messages on the current thread's message queue whose hwnd value is NULL, that is, thread messages as posted by PostMessage (when the hWnd parameter is NULL) or PostThreadMessage.\n\nThe value of the first message in the range of messages to be examined. Use WM_KEYFIRST (0x0100) to specify the first keyboard message or WM_MOUSEFIRST (0x0200) to specify the first mouse message.\n\nIf wMsgFilterMin and wMsgFilterMax are both zero, PeekMessage returns all available messages (that is, no range filtering is performed).\n\nThe value of the last message in the range of messages to be examined. Use WM_KEYLAST to specify the last keyboard message or WM_MOUSELAST to specify the last mouse message.\n\nIf wMsgFilterMin and wMsgFilterMax are both zero, PeekMessage returns all available messages (that is, no range filtering is performed).\n\nSpecifies how messages are to be handled. This parameter can be one or more of the following values.\n\nBy default, all message types are processed. To specify that only certain message should be processed, specify one or more of the following values.\n\nFor more info on listed flags and types of messages see GetQueueStatus documentation.\n\nIf a message is available, the return value is nonzero.\n\nIf no messages are available, the return value is zero.\n\nPeekMessage retrieves messages associated with the window identified by the hWnd parameter or any of its children as specified by the IsChild function, and within the range of message values given by the wMsgFilterMin and wMsgFilterMax parameters. Note that an application can only use the low word in the wMsgFilterMin and wMsgFilterMax parameters; the high word is reserved for the system.\n\nNote that PeekMessage always retrieves WM_QUIT messages, no matter which values you specify for wMsgFilterMin and wMsgFilterMax.\n\nDuring this call, the system dispatches (DispatchMessage) pending, nonqueued messages, that is, messages sent to windows owned by the calling thread using the SendMessage, SendMessageCallback, SendMessageTimeout, or SendNotifyMessage function. Then the first queued message that matches the specified filter is retrieved. The system may also process internal events. If no filter is specified, messages are processed in the following order:\n\nTo retrieve input messages before posted messages, use theandparameters.\n\nThe PeekMessage function normally does not remove WM_PAINT messages from the queue. WM_PAINT messages remain in the queue until they are processed. However, if a WM_PAINT message has a NULL update region, PeekMessage does remove it from the queue.\n\nIf a top-level window stops responding to messages for more than several seconds, the system considers the window to be not responding and replaces it with a ghost window that has the same z-order, location, size, and visual attributes. This allows the user to move it, resize it, or even close the application. However, these are the only actions available because the application is actually not responding. When an application is being debugged, the system does not generate a ghost window.\n\nThis API does not participate in DPI virtualization. The output is in the mode of the window that the message is targeting. The calling thread is not taken into consideration.\n\nFor an example, see Examining a Message Queue."
    },
    {
        "link": "https://stackoverflow.com/questions/78171825/how-does-win32-api-peekmessage-function-work",
        "document": "I was trying to program with Win32 API, but I came across an unusual behaviour that I wasn't expecting which left me kinda confused. In my while loop:\n\nand only when I am not moving the window everything works as expected, but when I try to move the window by move dragging the title bar the loop doesn't continue for some reason. I thought that is happening because I am trying to it with loop but the same behaviour shows even with an statement.\n\nI would like to know what this happens and how can I make it so the rest of the loop gets executed. I don't know how the message queuing and dispatching works so I need some help."
    },
    {
        "link": "https://stackoverflow.com/questions/18317210/winapi-peekmessage-always-returns-false",
        "document": "When you pass (i.e. 0) for the parameter, the function retrieves thread messages, and messages for any window that belongs to the current thread. This is called out explicitly in the documentation:\n\nA handle to the window whose messages are to be retrieved. The window must belong to the current thread. If hWnd is , retrieves messages for any window that belongs to the current thread, and any messages on the current thread's message queue whose hwnd value is (see the structure). Therefore if hWnd is , both window messages and thread messages are processed.\n\nSince you are calling this function on a new thread in a ThreadPool, there are no messages for it to retrieve. That thread is not associated with any windows and has no messages.\n\nThe function returns (i.e. 0) when there are no messages available.\n\nIf you were calling on the main UI thread (the one associated with your form), you would be getting a peek at all messages destined for your form window."
    },
    {
        "link": "https://pearsonhighered.com/assets/samplechapter/0/3/2/1/0321262506.pdf",
        "document": ""
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/apps/design/input/touch-developer-guide",
        "document": "Design your app with the expectation that touch will be the primary input method of your users. If you use UWP controls, support for touchpad, mouse, and pen/stylus requires no additional programming, because UWP apps provide this for free.\n\nHowever, keep in mind that a UI optimized for touch is not always superior to a traditional UI. Both provide advantages and disadvantages that are unique to a technology and application. In the move to a touch-first UI, it is important to understand the core differences between touch, touchpad, pen/stylus, mouse, and keyboard input.\n\nMany devices have multi-touch screens that support using one or more fingers (or touch contacts) as input. The touch contacts, and their movement, are interpreted as touch gestures and manipulations to support various user interactions.\n\nThe Windows app includes a number of different mechanisms for handling touch input, enabling you to create an immersive experience that your users can explore with confidence. Here, we cover the basics of using touch input in a Windows app.\n• The direct contact (or proximity to, if the display has proximity sensors and supports hover detection) of one or more fingers on that display.\n• Movement of the touch contacts (or lack thereof, based on a time threshold).\n\nThe input data provided by the touch sensor can be:\n• Interpreted as a physical gesture for direct manipulation of one or more UI elements, such as panning, rotating, resizing, or moving. (In contrast, interacting with an element through its properties window, dialog box, or other UI affordance is considered indirect manipulation.)\n• Recognized as an alternative input method, such as mouse or pen.\n• Used to complement or modify aspects of other input methods, such as smudging an ink stroke drawn with a pen.\n\nTouch input typically involves the direct manipulation of an element on the screen. The element responds immediately to any touch contact within its hit test area, and reacts appropriately to any subsequent movement of the touch contacts, including removal.\n\nCustom touch gestures and interactions should be designed carefully. They should be intuitive, responsive, and discoverable, and they should let users explore your app with confidence.\n\nEnsure that app functionality is exposed consistently across every supported input device type. If necessary, use some form of indirect input mode, such as text input for keyboard interactions, or UI affordances for mouse and pen.\n\nRemember that traditional input devices (such as mouse and keyboard) are familiar and appealing to many users. They can offer speed, accuracy, and tactile feedback that touch cannot.\n\nProviding unique and distinctive interaction experiences for all input devices will support the widest range of capabilities and preferences, appeal to the broadest possible audience, and attract more customers to your app.\n\nThe following table shows some of the differences between input devices that you should consider when you design touch-optimized Windows apps.\n\nAppropriate visual feedback during interactions with your app helps users recognize, learn, and adapt to how their interactions are interpreted by both the app and the Windows platform. Visual feedback can indicate successful interactions, relay system status, improve the sense of control, reduce errors, help users understand the system and input device, and encourage interaction.\n\nVisual feedback is critical when the user relies on touch input for activities that require accuracy and precision based on location. Display feedback whenever and wherever touch input is detected, to help the user understand any custom targeting rules that are defined by your app and its controls.\n• Clear size guidelines ensure that applications provide a comfortable UI that contains objects and controls that are easy and safe to target.\n• The entire contact area of the finger determines the most likely target object.\n• Items within a group are easily re-targeted by dragging the finger between them (for example, radio buttons). The current item is activated when the touch is released.\n• Densely packed items (for example, hyperlinks) are easily re-targeted by pressing the finger down and, without sliding, rocking it back and forth over the items. Due to occlusion, the current item is identified through a tooltip or the status bar and is activated when the touch is released.\n\nDesign for sloppy interactions by using:\n• Snap-points that can make it easier to stop at desired locations when users interact with content.\n• Directional \"rails\" that can assist with vertical or horizontal panning, even when the hand moves in a slight arc. For more information, see Guidelines for panning.\n\nFinger and hand occlusion is avoided through:\n• Make UI elements big enough so that they cannot be completely covered by a fingertip contact area. Position menus and pop-ups above the contact area whenever possible.\n• Show tooltips when a user maintains finger contact on an object. This is useful for describing object functionality. The user can drag the fingertip off the object to avoid invoking the tooltip. For small objects, offset tooltips so they are not covered by the fingertip contact area. This is helpful for targeting.\n• Where precision is required (for example, text selection), provide selection handles that are offset to improve accuracy. For more information, see Guidelines for selecting text and images (Windows Runtime apps).\n\nAvoid timed mode changes in favor of direct manipulation. Direct manipulation simulates the direct, real-time physical handling of an object. The object responds as the fingers are moved.\n\nA timed interaction, on the other hand, occurs after a touch interaction. Timed interactions typically depend on invisible thresholds like time, distance, or speed to determine what command to perform. Timed interactions have no visual feedback until the system performs the action.\n\nDirect manipulation provides a number of benefits over timed interactions:\n• Instant visual feedback during interactions make users feel more engaged, confident, and in control.\n• Direct manipulations make it safer to explore a system because they are reversible—users can easily step back through their actions in a logical and intuitive manner.\n• Interactions that directly affect objects and mimic real world interactions are more intuitive, discoverable, and memorable. They don't rely on obscure or abstract interactions.\n• Timed interactions can be difficult to perform, as users must reach arbitrary and invisible thresholds.\n\nIn addition, the following are strongly recommended:\n• Manipulations should not be distinguished by the number of fingers used.\n• Interactions should support compound manipulations. For example, pinch to zoom while dragging the fingers to pan.\n• Interactions should not be distinguished by time. The same interaction should have the same outcome regardless of the time taken to perform it. Time-based activations introduce mandatory delays for users and detract from both the immersive nature of direct manipulation and the perception of system responsiveness. An exception to this is where you use specific timed interactions to assist in learning and exploration (for example, press and hold).\n• Appropriate descriptions and visual cues have a great effect on the use of advanced interactions.\n\nTweak the user interaction experience through the pan/scroll and zoom settings of your app views. An app view dictates how a user accesses and manipulates your app and its content. Views also provide behaviors such as inertia, content boundary bounce, and snap points.\n\nPan and scroll settings of the ScrollViewer control dictate how users navigate within a single view, when the content of the view doesn't fit within the viewport. A single view, for example, can be a page of a magazine or book, the folder structure of a computer, a library of documents, or a photo album.\n\nZoom settings apply to both optical zoom (supported by the ScrollViewer control) and the Semantic Zoom control. Semantic Zoom is a touch-optimized technique for presenting and navigating large sets of related data or content within a single view. It works by using two distinct modes of classification, or zoom levels. This is analogous to panning and scrolling within a single view. Panning and scrolling can be used in conjunction with Semantic Zoom.\n\nUse app views and events to modify the pan/scroll and zoom behaviors. This can provide a smoother interaction experience than is possible through the handling of pointer and gesture events.\n\nFor more info about app views, see Controls, layouts, and text.\n\nIf you implement your own interaction support, keep in mind that users expect an intuitive experience involving direct interaction with the UI elements in your app. We recommend that you model your custom interactions on the platform control libraries to keep things consistent and discoverable. The controls in these libraries provide the full user interaction experience, including standard interactions, animated physics effects, visual feedback, and accessibility. Create custom interactions only if there is a clear, well-defined requirement and basic interactions don't support your scenario.\n\nTo provide customized touch support, you can handle various UIElement events. These events are grouped into three levels of abstraction.\n• None Static gesture events are triggered after an interaction is complete. Gesture events include Tapped, DoubleTapped, RightTapped, and Holding. You can disable gesture events on specific elements by setting IsTapEnabled, IsDoubleTapEnabled, IsRightTapEnabled, and IsHoldingEnabled to false.\n• None Pointer events such as PointerPressed and PointerMoved provide low-level details for each touch contact, including pointer motion and the ability to distinguish press and release events. A pointer is a generic input type with a unified event mechanism. It exposes basic info, such as screen position, on the active input source, which can be touch, touchpad, mouse, or pen.\n• None Manipulation gesture events, such as ManipulationStarted, indicate an ongoing interaction. They start firing when the user touches an element and continue until the user lifts their finger(s), or the manipulation is canceled. Manipulation events include multi-touch interactions such as zooming, panning, or rotating, and interactions that use inertia and velocity data such as dragging. The information provided by the manipulation events doesn't identify the form of the interaction that was performed, but rather includes data such as position, translation delta, and velocity. You can use this touch data to determine the type of interaction that should be performed.\n\nHere is the basic set of touch gestures supported by UWP.\n\nFor details about individual controls, see Controls list.\n\nPointer events are raised by a variety of active input sources, including touch, touchpad, pen, and mouse (they replace traditional mouse events.)\n\nPointer events are based on a single input point (finger, pen tip, mouse cursor) and do not support velocity-based interactions.\n\nHere is a list of pointer events and their related event argument.\n\nThe following example shows how to use the PointerPressed, PointerReleased, and PointerExited events to handle a tap interaction on a Rectangle object.\n\nFirst, a Rectangle named is created in Extensible Application Markup Language (XAML).\n\nNext, listeners for the PointerPressed, PointerReleased, and PointerExited events are specified.\n\nFinally, the PointerPressed event handler increases the Height and Width of the Rectangle, while the PointerReleased and PointerExited event handlers set the Height and Width back to their starting values.\n\nUse manipulation events in your app if you need to support multiple finger interactions or interactions that require velocity data.\n\nYou can use manipulation events to detect interactions such as drag, zoom, and hold.\n\nHere is a list of manipulation events and related event arguments.\n\nA gesture consists of a series of manipulation events. Each gesture starts with a ManipulationStarted event, such as when a user touches the screen.\n\nNext, one or more ManipulationDelta events are fired. For example, if you touch the screen and then drag your finger across the screen. Finally, a ManipulationCompleted event is raised when the interaction finishes.\n\nThe following example shows how to use the ManipulationDelta events to handle a slide interaction on a Rectangle and move it across the screen.\n\nFirst, a Rectangle named is created in XAML with a Height and Width of 200.\n\nNext, a global TranslateTransform named is created for translating the Rectangle. A ManipulationDelta event listener is specified on the Rectangle, and is added to the RenderTransform of the Rectangle.\n\nFinally, in the ManipulationDelta event handler, the position of the Rectangle is updated by using the TranslateTransform on the Delta property.\n\nAll of the pointer events, gesture events and manipulation events mentioned here are implemented as routed events. This means that the event can potentially be handled by objects other than the one that originally raised the event. Successive parents in an object tree, such as the parent containers of a UIElement or the root Page of your app, can choose to handle these events even if the original element does not. Conversely, any object that does handle the event can mark the event handled so that it no longer reaches any parent element. For more info about the routed event concept and how it affects how you write handlers for routed events, see Events and routed events overview.\n• Design applications with touch interaction as the primary expected input method.\n• Provide visual feedback for interactions of all types (touch, pen, stylus, mouse, etc.)\n• Optimize accuracy through the use of snap points and directional \"rails\".\n• Provide tooltips and handles to help improve touch accuracy for tightly packed UI items.\n• Don't use timed interactions whenever possible (example of appropriate use: touch and hold).\n• Don't use the number of fingers used to distinguish the manipulation whenever possible."
    },
    {
        "link": "https://stackoverflow.com/questions/10817860/c-how-to-catch-mouse-clicks-wherever-they-happen",
        "document": ", as it name suggests is a (specific) window (message) processor where you can analyze and respond to messages on the queue that were deferred by the system to your custom definition of the callback for further processing.\n\nSince you want to detect and act on mouse clicks anywhere on the screen, as chris suggested in the comments, one way is to hook yourself into the system by calling SetWindowsHookEx() which is quite verbose in its very definition - it allows you to track stuff happening on the system and relay that information back to your application.\n\nThis is the syntax which you need to employ in order to get yourself\n\nIt takes in a specific hook id, which are basically little #defines which tell the function what kind of messages you wish to receive from all over the system, you pass it a callback just like the WndProc, but this time it's meant to process the incoming messages regarding across the system. hMod simply refers to the handle to the application or the DLL in which the just mentioned proc function callback is located in. The last one relates to threads currently running on the system, setting this to 0 or NULL retrieves messages for all existing threads.\n\nDo note that Aurus' example call to the SetWindowsHookEx is process-specific which a fancy word relating it to an actual application, instead of a DLL which can be appended to multiple processes across the system ( a global one ) and return information to your application. It would be prudent to take the time and effort to investigate Eugene's recommended method instead of this forceful approach useful only for experiments. It's a bit \"harder\", but the reward is worthwhile.\n\nLess work is not always better or preferable."
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/apps/design/input/mouse-interactions",
        "document": "Optimize your Windows app design for touch input and get basic mouse support by default.\n\nMouse input is best suited for user interactions that require precision when pointing and clicking. This inherent precision is naturally supported by the UI of Windows, which is optimized for the imprecise nature of touch.\n\nWhere mouse and touch input diverge is the ability for touch to more closely emulate the direct manipulation of UI elements through physical gestures performed directly on those objects (such as swiping, sliding, dragging, rotating, and so on). Manipulations with a mouse typically require some other UI affordance, such as the use of handles to resize or rotate an object.\n\nA concise set of mouse interactions are used consistently throughout the system.\n\nHover over an element to display more detailed info or teaching visuals (such as a tooltip) without a commitment to an action. Left-click an element to invoke its primary action (such as launching an app or executing a command). Display scroll bars to move up, down, left, and right within a content area. Users can scroll by clicking scroll bars or rotating the mouse wheel. Scroll bars can indicate the location of the current view within the content area (panning with touch displays a similar UI). Right-click to display the navigation bar (if available) and the app bar with global commands. Right-click an element to select it and display the app bar with contextual commands for the selected element. Note Right-click to display a context menu if selection or app bar commands are not appropriate UI behaviors. But we strongly recommend that you use the app bar for all command behaviors. Display UI commands in the app bar (such as + and -), or press Ctrl and rotate mouse wheel, to emulate pinch and stretch gestures for zooming. Display UI commands in the app bar, or press Ctrl+Shift and rotate mouse wheel, to emulate the turn gesture for rotating. Rotate the device itself to rotate the entire screen. Left-click and drag an element to move it. Left-click within selectable text and drag to select it. Double-click to select a word.\n\nMost mouse input can be handled through the common routed input events supported by all UIElement objects. These include:\n\nHowever, you can take advantage of the specific capabilities of each device (such as mouse wheel events) using the pointer, gesture, and manipulation events in Windows.UI.Input.\n\nSamples: See our BasicInput sample, for .\n• When a mouse is detected (through move or hover events), show mouse-specific UI to indicate functionality exposed by the element. If the mouse doesn't move for a certain amount of time, or if the user initiates a touch interaction, make the mouse UI gradually fade away. This keeps the UI clean and uncluttered.\n• Don't use the cursor for hover feedback, the feedback provided by the element is sufficient (see Cursors below).\n• Don't display visual feedback if an element doesn't support interaction (such as static text).\n• Don't use focus rectangles with mouse interactions. Reserve these for keyboard interactions.\n• Display visual feedback concurrently for all elements that represent the same input target.\n• Provide buttons (such as + and -) for emulating touch-based manipulations such as panning, rotating, zooming, and so on.\n\nFor more general guidance on visual feedback, see Guidelines for visual feedback.\n\nA set of standard cursors is available for a mouse pointer. These are used to indicate the primary action of an element.\n\nEach standard cursor has a corresponding default image associated with it. The user or an app can replace the default image associated with any standard cursor at any time. Specify a cursor image through the PointerCursor function.\n\nIf you need to customize the mouse cursor:\n• Always use the arrow cursor ( ) for clickable elements. don't use the pointing hand cursor ( ) for links or other interactive elements. Instead, use hover effects (described earlier).\n• Use the text cursor ( ) for selectable text.\n• Use the move cursor ( ) when moving is the primary action (such as dragging or cropping). Don't use the move cursor for elements where the primary action is navigation (such as Start tiles).\n• Use the horizontal, vertical and diagonal resize cursors ( , , , ), when an object is resizable.\n• Use the grasping hand cursors ( , ) when panning content within a fixed canvas (such as a map)."
    },
    {
        "link": "https://zserge.com/posts/fenster",
        "document": "I grew up with BASIC and Turbo Pascal. My first programs were very simple graphical games. I still miss the era of BGI and BASIC’s painting commands. Those graphics were neither beautiful nor fast, but they were simple. Simple enough for a kid with only elemental geometry knowledge to build something exciting.\n\nEven today, whenever I want to prototype or visualise something, to experiment with generative art or to write a toy graphical app – I feel nostalgic for the old days.\n\nSure, there is SDL2 and Qt and JavaScript Canvas and Löve and many others. But how hard could it be to build our own cross-platform graphical library from scratch?\n\nOur goal here is to have a framebuffer for a single window that would run and look the same on Linux, Windows and macOS. Good old C would the language of choice to keep the retro-computing atmosphere of the story.\n\nTLDR: the end result is available as a single-header library on Github: https://github.com/zserge/fenster.\n\nFrom my past experience with webview and lorca, Linux is the easiest platform to develop for. Or maybe I’m just biased. Of all the graphical options that Linux provides we’ll choose X11 as the foundation to our window, since it’s the lowest common denominator, so to say.\n\nIt all starts with opening the display connection. X server is an actual network server, so all the APIs are special packets sent over the X server connection. Once the connection is established – we should choose the screen and create a window on that screen. Fortunately, X11 comes with very simple and convenient APIs for all of it. Then we enter the event loop where we poll for incoming events and can handle them. Once we are about to exit the app – we close the display connection.\n\nIn only 15 lines of code we got a working window with the given size 320x240 and a title!\n\nWhile X11 apps can run on macOS using XQuartz, it’s still better to support native Cocoa apps. We won’t be using Swift or XCode and will try to only rely on bare bones technologies to remain portable. Let’s start with Objective-C, something like this might work:\n\nSlighly more verbose but still very compact and follows the same logic: start an app, open a window, run event loop, close the window.\n\nBut it’s Objective-C and I promised you just C…\n\nFortunately, macOS comes with Objective-C runtime library that enables calling Objective-C constructs from C using something like a reflection layer:\n\nBut now it’s barely readable. Imagine rewriting like this. Also, if you experiment more with it you’ll discover that should be type-casted to a correct signature before each call to avoid crashes.\n\nBut what if we come up with some clever preprocessor macros to achieve a terser syntax?\n\nHere is a message (think: method call) with no arguments. The return type should be specified, the receiver of the message and the message name itself. Similarly , , and are helpers for messages with one and more parameters. For each parameter a type should be provided as well as its value. Finally, is just a helper for .\n\nUsing these macros our minimal Cocoa window can be rewritten in standard C as follows:\n\nNow it’s a perfectly valid C code that does exactly the same as the Objective-C code above. Moreover, using this technique we can create new classes dynamically, attach methods to them, create delegates and do much more. But for now our goal is achieved – we’ve made an empty macOS window with a title.\n\nWinAPI was traditionally designed to be used from C/C++, and Windows is a graphical desktop OS, so making a window on Windows should be pretty straightforward (although tautological).\n\nWinAPI version requires a separate callback to handle all window events, unlike the previous two platforms, where events could be handled inside the loop body. Also it requires to register a window class before creating a window. But other than that it’s rather trivial: open a window, run event loop, handle events, exit when the window is closed.\n\nNow that we’re done with window initialisation and the main loop for all three common desktop platforms – we can move on to actually drawing something inside those windows.\n\nWe won’t do any fancy graphics, we won’t bother with GPUs and OpenGL. We’ll aim for a very simple framebuffer view. Typically, a framebuffer is a section of RAM that contains a bitmap and changing that bitmap affects the video display.\n\nIn our case we will create an array of 32-bit integers, where every integer controls a single pixel of the window. Changing the value of an integer would change the color of a single pixel. More advanced algorithms could be used to draw spires, lines, rectangles, circles, arc etc. But we’ll come back to it later. Now let’s draw some pixels.\n\nIn our X11 window a framebuffer can be implemented using a graphical context (GC) and an image:\n\nImage is backed by an array, every time we change the array – we should call and flush the connection to redraw the window. Writing random integers to the array would create colourful white noise. Writing 0xff0000 would paint it red. Now let’s try reproduce it on macOS.\n\nWe can keep using ObjC runtime to create a custom class and override its method. Then in a loop we would need to invalidate the view, it should trigger and we’re done.\n\nIf needed we can override more methods and create a custom delegate class for the window to handle the close button properly and do other nice things.\n\nOn Windows we don’t need a separate child view, but we have to handle message instead. Inside we would do things exactly like we did on macOS and Linux – we create an , configure information about its pixel format, and send framebuffer data to the canvas. Something like this:\n\nHere in the BITMAPINFO initialiser is not a typo. According to MSDN negative height values for uncompressed bitmaps mean that the bitmap is filled up from the top to bottom (unlike BMP format that goes from the bottom to the top).\n\nNow that we got an app window and a framebuffer working – we can attempt to handle some user input, such as keyboard or mouse. The good news is that we already have an event loop/callback, so all we need is to extend that with a few more “case” statements within a switch.\n\nFor mouse events we’ll only focus on simple things, such as movement (X/Y coordinate of the mouse should be reported) and the left button click. With the presence of touch screens and laptops it’s hard to find a middle mouse button or a horizontal scrolling wheel, anyway.\n\nFor keyboard we will be interested in key presses and releases, as well as some modifier key status. At least we should handle Ctrl and Shift.\n\nOn Linux we first need to adjust our window initialiser to support all the required event types:\n\nIn the event loop we can handle them and extract information from the event structure:\n\nThis is where the ugly part starts. Mouse handling is simple. But for keyboard every platform has its own definitions of “key codes”. To remain cross-platform we would have to map platform-specific keycodes into some common layout.\n\nUsually, there are several approaches how to pick that common layout. Some choose the “primary” platform and convert key codes from the others, some choose HID standard (which is really well defined and elegant). I decided to restrict key codes to a small minimal subset, and only use 7-bit ASCII for alphanumeric keys or symbols such as Enter, Backspace, Tab etc. I completely ignored CapsLock and other not-so-common keys. But at least we shall have a key code that can be just printed to see which key it belongs to. For arrows I made an exception and mapped codes 17..20 to them (DC1..DC4 in ASCII set).\n\nI wasn’t sure if X11 keysym definitions are stable constants on all the systems that support X11, so I put them in an array where ASCII key code follows the X11 keysym, so that a simple for-loop could map one to the other:\n\nMacOS comes with a bit less of a surprise for key codes, but is somewhat peculiar in terms of mouse handling – the Y coordinate is inverted. Like in maths, where X axis goes right and Y axis goes up. To keep things compatible with other platforms we have to subtract Y from the window height to get a proper “computer graphics” coordinate system (where Y does down and (0,0) is a top-left corner of the window).\n\nNo changes are required in the initialisation of an app, but the event loop should be changed to handle more event types:\n\nOn macOS the key codes are known constants, so we can define a lookup-table to convert them to the ASCII symbols. We have to handle two separate move events – one for the left mouse button being pressed (“drag”) and another for a regular move with a mouse button released. The rest should be self-explanatory.\n\nFinally, WinAPI comes with very convenient message types that we can handle in the most straightforward way inside our WndProc:\n\nWe’re done here. Inside the main event loop on all three platforms users can now check for / coordinates, flag, a bitmask and status array.\n\nSince different computers have different performance it’s a common practice to limit the rate at which the frames are refreshed. Usually, an FPS=60 (i.e. 60 frames per second, or 16.6ms per frame) is used. To restrict our event loop from rendering and polling things faster than that we would need two functions: one to return the current time (at least in milliseconds) and another to sleep for the given amount of time.\n\nSince both macOS and Linux have some POSIX compatibility we can reuse the same code:\n\nWindows, on the other hand, is special, but nevertheless very convenient:\n\nA typical application structure for our library would look like this (you can use an up-to-date “fenster.h” from https://github.com/zserge/fenster):\n\nSo far so good. Clear, cross-platform app loop. But how can we actually draw anything?\n\nHaving a cross-platform framebuffer is convenient, but how we can turn it into a canvas?\n\nLet’s start with drawing pixels. Since a framebuffer is essentially an array of pixels – we can introduce a macro to access an individual pixel:\n\nNext simple task would be to fill the complete framebuffer with a solid colour:\n\nTo draw a circle we can use a simple algorithm that checks for every pixel within the square where the circle is inscribed. If a circle radius squared is less than the sum of dx/dy squared (basic school math) – the pixel belongs to the circle and should be painted:\n\nTo draw a line we can use Bresenham’s algorithm, a well-known classic:\n\nHaving this we can now draw complex polygons and would probably need a “flood fill” algorithm. Typically it is implemented using a queue of pixels to check and paint, but we can use recursion, as long as the filled area remains small enough to not overflow the stack:\n\nLast but not least, let’s try to print some text on screen! To go truly old-school we will be using a bitmap font. In other words, our font is an array where each element corresponds to an ASCII character and describes which pixels in a glyph grid to colourise to make it look like a letter.\n\nI’ve picked the smallest possible readable bitmap font where each letter is 5 pixels tall and 3 pixels wide (even though I’ve previously tried to create much smaller fonts that are not so readable at all).\n\nTo encode the 5x3 font we can use one per glyph. For example, here’s letters “A” and “B”:\n\nIf we treat every glyph as a sequence of bits counting from the top left corner we get for “A”. But bits in numbers are usually numbered right-to-left, so we should reverse it and get . Splitting this into octets and converting them to hexadecimal form would give us . Now, how to draw such a glyph?\n\nAnd we are finally ready to create some visual masterpieces:\n\nWe carry on. So far we’ve conquered main app loop, framebuffer/canvas and user input. But why do it all in silence? Can we add some audio playback?\n\nAudio can be very complicated, and low-level audio even more so. But if we only focus on the most basic use cases – like, play a mono stream of floating-point audio samples – we can make it work rather easily on most of the computers.\n\nIt’s quite a safe bet to choose ALSA for Linux (since it’s part of the kernel), WinMM for WinAPI (as it’s been there for ages) and CoreAudio for macOS.\n\nMany audio libraries choose a callback API model to give user more control over the latency and buffering. In our case, to keep the simplicity of the polling loop from the above let’s choose the synchronous “streaming” approach. We would need to implement 4 functions:\n• One to open the default audio device.\n• Another to close it.\n• One to get the number of available samples to be written into the audio stream.\n• Another to actually write them.\n\nThe implementation should guarantee that any attempt to write a buffer of samples not longer than the available number – it will not block. Then the caller should check the available count, take care of rendering/mixing sound and writing that part of the buffer.\n\nALSA implementation would be the simplest of them all:\n\nOn macOS and WinAPI we would have to use double-buffering technique, where multiple audio buffer objects are created and chained for the playback, and as soon as the first one is done – the next one continues the playback, while the first one is being fulfilled with the following audio samples.\n\nI wouldn’t post all of the code here, but you can check it out on Github.\n\nC is a nice small language, but for quick prototyping maybe some other, less archaic languages could fit better.\n\nThere are Go bindings using CGo and the library can be imported like a normal Go module. @dim13 has offerred a few adjustments to the API so that our framebuffer is available as a standard interface. This enables compatibility with many other libraries, such as https://github.com/fogleman/gg or https://github.com/llgcode/draw2d.\n\nOther bindings that were trivial to write were in Zig. In fact, Zig so well integrates with C code that it can use Fenster library directly:\n\nOf course, some Zig-friendly idiomatic wrappers are always welcome!\n\nBut can it run Doom?\n\nYes, it can! Porting Doom has never been easier these days, thanks to the wonderful https://github.com/ozkl/doomgeneric. All you have to do is to implement several functions: for opening a window, for updating the framebuffer, for handling time and for user input. We’re lucky – Fenster literally has an API call for each of these cases.\n\nIf you only override these functions you already get a familiar starting picture and some demo animation:\n\nWhat’s left is user input. Doom uses key events instead of key statuses, and it peeks them one by one. So we should keep another array of last know key statuses, compare it to the current keys and return an event if there is a mismatch. Additionally, Doom has its own custom key mapping that might not match our key codes, so we need to do some conversion:\n\nUnbelievable, but in less than 50 lines of code we got a fully working Doom game running on top of a tiny cross platform graphics library we’ve just created!\n\nWell, if it can run Doom – it can do anything! I hope this little toy library would be helpful to those who miss the simplicity of the old school graphics. Pull requests, bug fixes and contributions are appreciated, as long as they don’t bloat the library and keep it simple.\n\nI hope you’ve enjoyed this article. You can follow – and contribute to – on Github, Mastodon, Twitter or subscribe via rss."
    },
    {
        "link": "https://reddit.com/r/C_Programming/comments/119z0bt/graphic_library_for_c",
        "document": "I have a project in college where I need to make a function grapher in C. I won't get into details, but it basically works like a graphing calculator : you write down your function, it graphs it and then you're able to zoom in, zoom out, etc. Now the analytical and syntax parts are not hard to implement. However, when it comes to the graphic output, I have no clue about what to use.\n\nCan someone please give me an insight on what library would be best for me?"
    }
]