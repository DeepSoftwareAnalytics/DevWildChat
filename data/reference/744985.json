[
    {
        "link": "https://stackoverflow.com/questions/57729427/how-do-you-properly-handle-database-connections-across-several-modules-and-funct",
        "document": "I've been trying to read and understand how to handle a database connection in Python but remain confused. I'm not a professionally trained developer so a lot of the jargon and language is lost on me.\n\nIn one example, we do a lot of calculations on our players. So I created a player class.\n\nIn a separate module, if I have a series of functions using the data in the PlayerData class and potentially several others, ultimately yielding a new set of data we want to save to our database, do I have to then create another instance of the Database class to do so? Like this...\n\nTo my untrained mind, this feels like a lot of database connecting and each time one is established it seems to slow down the whole program. When I first started learning Python I would have a database module that created the connection immediately (not wrapped in a class) and would just import it at the top of each module where I needed to run queries. It seemed as though the same connection was being used everywhere. I was told, however, that's not the correct to handle it.\n\nPlease tell me if I'm thinking about this all wrong as well but what is the correct way to handle this? Thank you!"
    },
    {
        "link": "https://pynative.com/python-mysql-transaction-management-using-commit-rollback",
        "document": "This lesson mainly focuses on how to manage Database transactions while working with the MySQL database in Python. Learn Python MySQL transaction management using commit and rollback using ‘Mysql connector python’ module.\n\nThe database transaction represents a single unit of work. Any operation which modifies the state of the MySQL database is a transaction. Let see in detail what is database transaction. For example, take a sample of a Bank amount transfer, which involves two significant transactions.\n\nIf the first Transaction is executed successfully but the second failed, in this case, we need to re-deposit money back to account A. To manage such instances, we need transaction management.\n\nUsing ACID properties, we can study transaction management well. ACID stands for Atomicity, Consistency, isolation, and durability.\n• Atomicity: means all or nothing. Either all transactions are successful or none. You can group SQL statements as one logical unit, and if any query fails, the whole transaction fails.\n• Consistency: It ensures that the database remains in a consistent state after performing a transaction.\n• Isolation: It ensures that the transaction is isolated from other transactions.\n• Durability: It means once a transaction has been committed, it persists in the database irrespective of power loss, error, or restart system.\n\nPlease follow the below steps to manage MySQL transactions in Python: –\n• Prepare the SQL queries that you want to run as a part of a transaction. For example, we can combine two SQL queries(withdrawal money and deposit money query) in a single transaction.\n• Set an auto-commit property of MySQL connection to false.\n• Execute all queries one by one using the cursor.execute()\n• If all queries execute successfully, commit the changes to the database\n• If one of the queries failed to execute, then rollback all the changes.\n• Catch any SQL exceptions that may occur during this process\n\nPython MySQL Connector provides the following method to manage database transactions.\n• : method sends a COMMIT statement to the MySQL server, committing the current transaction. After the successful execution of a query make changes persistent into a database using the commit() of a connection class.\n• : revert the changes made by the current transaction. When one of the transactions fails to execute, and you want to revert or undo all your changes, call a rollback method of MySQL connection object.\n• : value can be as True or False to enable or disable the auto-commit feature of MySQL. By default its value is False.\n\nPython example to manage MySQL transactions using commit and rollback\n\nYou should get the following output if a query fails to execute.\n• We imported the MySQL connector python module so we can use its API to communicate with MySQL Database.\n• After a successful MySQL connection, we set to , i.e., we need to commit the transaction only when both the transactions complete successfully.\n• We prepared two update SQL queries as a part of a single transaction to deposit money to account B from account A.\n• We executed both the queries one by one using a method.\n• After successful execution of both the queries, we committed our changes to the database using a .\n• In case of an exception or failure of one of the queries, we can revert our changes using a .\n• We placed all our code in the block to catch the database exceptions that may occur during the process.\n\nTo practice what you learned in this article, Please solve a Python Database Exercise project to Practice and master the Python Database operations."
    },
    {
        "link": "https://geeksforgeeks.org/how-to-connect-python-with-sql-database",
        "document": "Python is a high-level, general-purpose, and very popular programming language. Basically, it was designed with an emphasis on code readability, and programmers can express their concepts in fewer lines of code. We can also use Python with SQL. In this article, we will learn how to connect SQL with Python using the ‘MySQL Connector Python module. The diagram given below illustrates how a connection request is sent to MySQL connector Python, how it gets accepted from the database and how the cursor is executed with result data.\n\nTo create a connection between the MySQL database and Python, the connect() method of mysql.connector module is used. We pass the database details like HostName, username, and the password in the method call, and then the method returns the connection object.\n\nThe following steps are required to connect SQL with Python:\n\nStep 1: Download and Install the free MySQL database from here.\n\nStep 2: After installing the MySQL database, open your Command prompt.\n\nStep 3: Navigate your Command prompt to the location of PIP. Click here to see, How to install PIP?\n\nStep 4: Now run the commands given below to download and install “MySQL Connector”. Here, mysql.connector statement will help you to communicate with the MySQL database.\n\nTo check if the installation was successful, or if you already installed “MySQL Connector”, go to your IDE and run the given below code :\n\nIf the above code gets executed with no errors, “MySQL Connector” is ready to be used.\n\nNow to connect SQL with Python, run the code given below in your IDE.\n\nHere, in the above code:\n\nTo create a database, we will use CREATE DATABASE database_name statement and we will execute this statement by creating an instance of the ‘cursor’ class.\n\nIf the database with the name ‘geeksforgeeks’ already exists then you will get an error, otherwise no error. So make sure that the new database that you are creating does not have the same name as the database already you created or exists previously. Now to check the databases that you created, use “SHOW DATABASES” – SQL statement i.e. cursor.execute(“SHOW DATABASES”)\n\nNow to create tables in a database, first, we have to select a database and for that, we will pass database = “NameofDatabase” as your fourth parameter in connect() function. Since we have created a database with the name ‘geekforgeeks’ above, so we will use that and create our tables. We will use CREATE TABLE gfg (variableName1 datatype, variableName2 datatype) statement to create our table with the name ‘gfg’.\n\nIf the table with the name ‘gfg’ already exists, you will get an error, otherwise no error. So make sure that the new table that you are creating does not have the same name as the table already you created or exists previously. Now to check tables that you created, use “SHOW TABLES” – SQL statement i.e. cursor.execute(“SHOW TABLES”).\n• connect() method of the MySQL Connector class with the arguments will connect to MySQL and would return a MySQLConnection object if the connection is established successfully.\n• user = “yourusername” , here “yourusername” should be the same username as you set during MySQL installation.\n• password = “your_password” , here “your_password” should be the same password as you set during MySQL installation.\n• cursor() is used to execute the SQL statements in Python.\n• execute() method is used to compile a SQL statement.\n\nHow to Connect Python with SQL Database? – FAQs\n\nHow to Connect Python with MySQL Database\n\nWhich SQL Database to Use with Python\n\nCan I Use Python and SQL Together?\n\nWhich Library is Used to Connect Python with MySQL?\n\nHow to Execute a SQL Query in Python"
    },
    {
        "link": "https://stackoverflow.com/questions/14883346/how-should-i-establish-and-manage-database-connections-in-a-multi-module-python",
        "document": "We have a Python application with over twenty modules, most of which are shared by several web and console applications.\n\nI've never had a clear understanding of the best practice for establishing and managing database connection in multi module Python apps. Consider this example:\n\nI have a module defining an object class for Users. It has many defs for creating/deleting/updating users in the database. The users.py module is imported into a) a console based utility, 2) a web.py based web application and 3) a constantly running daemon process.\n\nEach of these three application have different life cycles. The daemon can open a connection and keep it open. The console utility connects, does work, then dies. Of course the http requests are atomic, however the web server is a daemon.\n\nI am currently opening, using then closing a connection inside each function in the Users class. This seems the most inefficient, but it works in all examples. An alternative used as a test is to declare and open a global connection for the entire module. Another option would be to create the connection at the top application layer and pass references when instantiating classes, but this seems the worst idea to me.\n\nI know every application architecture is different. I'm just wondering if there's a best practice, and what it would be?"
    },
    {
        "link": "http://docs.sqlalchemy.org/en/latest/orm/session_transaction.html",
        "document": "The tracks the state of a single “virtual” transaction at a time, using an object called . This object then makes use of the underlying or engines to which the object is bound in order to start real connection-level transactions using the object as needed.\n\nThis “virtual” transaction is created automatically when needed, or can alternatively be started using the method. To as great a degree as possible, Python context manager use is supported both at the level of creating objects as well as to maintain the scope of the .\n\nBelow, assume we start with a :\n\nWe can now run operations within a demarcated transaction using a context manager:\n\nAt the end of the above context, assuming no exceptions were raised, any pending objects will be flushed to the database and the database transaction will be committed. If an exception was raised within the above block, then the transaction would be rolled back. In both cases, the above subsequent to exiting the block is ready to be used in subsequent transactions.\n\nThe method is optional, and the may also be used in a commit-as-you-go approach, where it will begin transactions automatically as needed; these only need be committed or rolled back:\n\nThe itself features a method. If the is begun within a transaction that has not yet been committed or rolled back, this method will cancel (i.e. rollback) that transaction, and also expunge all objects contained within the object’s state. If the is being used in such a way that a call to or is not guaranteed (e.g. not within a context manager or similar), the method may be used to ensure all resources are released:\n\nFinally, the session construction / close process can itself be run via context manager. This is the best way to ensure that the scope of a object’s use is scoped within a fixed block. Illustrated via the constructor first:\n\nSimilarly, the can be used in the same way:\n\nitself includes a method to allow both operations to take place at once:\n\nSAVEPOINT transactions, if supported by the underlying engine, may be delineated using the method: Each time is called, a new “BEGIN SAVEPOINT” command is emitted to the database within the scope of the current database transaction (starting one if not already in progress), and an object of type is returned, which represents a handle to this SAVEPOINT. When the method on this object is called, “RELEASE SAVEPOINT” is emitted to the database, and if instead the method is called, “ROLLBACK TO SAVEPOINT” is emitted. The enclosing database transaction remains in progress. is typically used as a context manager where specific per-instance errors may be caught, in conjunction with a rollback emitted for that portion of the transaction’s state, without rolling back the whole transaction, as in the example below: When the context manager yielded by completes, it “commits” the savepoint, which includes the usual behavior of flushing all pending state. When an error is raised, the savepoint is rolled back and the state of the local to the objects that were changed is expired. This pattern is ideal for situations such as using PostgreSQL and catching to detect duplicate rows; PostgreSQL normally aborts the entire transaction when such an error is raised, however when using SAVEPOINT, the outer transaction is maintained. In the example below a list of data is persisted into the database, with the occasional “duplicate primary key” record skipped, without rolling back the entire operation: When is called, the first flushes all currently pending state to the database; this occurs unconditionally, regardless of the value of the parameter which normally may be used to disable automatic flush. The rationale for this behavior is so that when a rollback on this nested transaction occurs, the may expire any in-memory state that was created within the scope of the SAVEPOINT, while ensuring that when those expired objects are refreshed, the state of the object graph prior to the beginning of the SAVEPOINT will be available to re-load from the database. In modern versions of SQLAlchemy, when a SAVEPOINT initiated by is rolled back, in-memory object state that was modified since the SAVEPOINT was created is expired, however other object state that was not altered since the SAVEPOINT began is maintained. This is so that subsequent operations can continue to make use of the otherwise unaffected data without the need for refreshing it from the database.\n\nThe in Core and in ORM feature equivalent transactional semantics, both at the level of the vs. the , as well as the vs. the . The following sections detail these scenarios based on the following scheme: ORM Core ----------------------------------------- ----------------------------------- sessionmaker Engine Session Connection sessionmaker.begin() Engine.begin() some_session.commit() some_connection.commit() with some_sessionmaker() as session: with some_engine.connect() as conn: with some_sessionmaker.begin() as session: with some_engine.begin() as conn: with some_session.begin_nested() as sp: with some_connection.begin_nested() as sp: Both and feature and methods. Using SQLAlchemy 2.0-style operation, these methods affect the outermost transaction in all cases. For the , it is assumed that is left at its default value of . Both and feature a method that will both procure a new object with which to execute SQL statements (the and , respectively) and then return a context manager that will maintain a begin/commit/rollback context for that object. When using a SAVEPOINT via the or methods, the transaction object returned must be used to commit or rollback the SAVEPOINT. Calling the or methods will always commit the outermost transaction; this is a SQLAlchemy 2.0 specific behavior that is reversed from the 1.x series.\n\nThe features “autobegin” behavior, meaning that as soon as operations begin to take place, it ensures a is present to track ongoing operations. This transaction is completed when is called. It is often desirable, particularly in framework integrations, to control the point at which the “begin” operation occurs. To suit this, the uses an “autobegin” strategy, such that the method may be called directly for a that has not already had a transaction begun: The above pattern is more idiomatically invoked using a context manager: The method and the session’s “autobegin” process use the same sequence of steps to begin the transaction. This includes that the event is invoked when it occurs; this hook is used by frameworks in order to integrate their own transactional processes with that of the ORM .\n\nMost DBAPIs support the concept of configurable transaction isolation levels. These are traditionally the four levels “READ UNCOMMITTED”, “READ COMMITTED”, “REPEATABLE READ” and “SERIALIZABLE”. These are usually applied to a DBAPI connection before it begins a new transaction, noting that most DBAPIs will begin this transaction implicitly when SQL statements are first emitted. DBAPIs that support isolation levels also usually support the concept of true “autocommit”, which means that the DBAPI connection itself will be placed into a non-transactional autocommit mode. This usually means that the typical DBAPI behavior of emitting “BEGIN” to the database automatically no longer occurs, but it may also include other directives. When using this mode, the DBAPI does not use a transaction under any circumstances. SQLAlchemy methods like , and pass silently. SQLAlchemy’s dialects support settable isolation modes on a per- or per- basis, using flags at both the level as well as at the level. When using the ORM , it acts as a facade for engines and connections, but does not expose transaction isolation directly. So in order to affect transaction isolation level, we need to act upon the or as appropriate. Setting Transaction Isolation Levels including DBAPI Autocommit - be sure to review how isolation levels work at the level of the SQLAlchemy object as well. To set up a or with a specific isolation level globally, the first technique is that an can be constructed against a specific isolation level in all cases, which is then used as the source of connectivity for a and/or : Another option, useful if there are to be two engines with different isolation levels at once, is to use the method, which will produce a shallow copy of the original which shares the same connection pool as the parent engine. This is often preferable when operations will be separated into “transactional” and “autocommit” operations: Above, both “ ” and share the same dialect and connection pool. However the “AUTOCOMMIT” mode will be set upon connections when they are acquired from the . The two objects “ ” and “ then inherit these characteristics when they work with database connections. The “ ” continues to have transactional semantics, including that and still consider themselves to be “committing” and “rolling back” objects, however the transaction will be silently absent. For this reason, it is typical, though not strictly required, that a Session with AUTOCOMMIT isolation be used in a read-only fashion, that is: When we make a new , either using the constructor directly or when we call upon the callable produced by a , we can pass the argument directly, overriding the pre-existing bind. We can for example create our from a default and pass an engine set for autocommit: # will normally use plain_engine # make a specific Session that will use the \"autocommit\" engine For the case where the or is configured with multiple “binds”, we can either re-specify the argument fully, or if we want to only replace specific binds, we can use the or methods: A key caveat regarding isolation level is that the setting cannot be safely modified on a where a transaction has already started. Databases cannot change the isolation level of a transaction in progress, and some DBAPIs and SQLAlchemy dialects have inconsistent behaviors in this area. Therefore it is preferable to use a that is up front bound to an engine with the desired isolation level. However, the isolation level on a per-connection basis can be affected by using the method at the start of a transaction: # call connection() with options before any other operations proceed. # this will procure a new connection from the bound engine and begin a real # and reverted to its previous isolation level. # subsequent to commit() above, a new transaction may be begun if desired, # which will proceed with the previous default isolation level unless Above, we first produce a using either the constructor or a . Then we explicitly set up the start of a database-level transaction by calling upon , which provides for execution options that will be passed to the connection before the database-level transaction is begun. The transaction proceeds with this selected isolation level. When the transaction completes, the isolation level is reset on the connection to its default before the connection is returned to the connection pool. The method may also be used to begin the level transaction; calling upon subsequent to that call may be used to set up the per-connection-transaction isolation level: # call connection() with options before any other operations proceed. # this will procure a new connection from the bound engine and begin a # outside the block, the transaction has been committed. the connection is # released and reverted to its previous isolation level."
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/config",
        "document": ""
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/table/config",
        "document": ""
    },
    {
        "link": "https://docs.confluent.io/cloud/current/flink/reference/sql-examples.html",
        "document": "The following code examples show common Flink SQL use cases with Confluent Cloud for Apache Flink®.\n\nThe following examples show how to create Flink tables with various options.\n• The column and system watermark are added implicitly.\n• is declared as being unique, meaning no duplicate rows.\n• must not contain NULLs, so an implicit NOT NULL is added.\n• The column and system watermark are added implicitly.\n• is declared as being unique, meaning no duplicate rows.\n• must not contain NULLs, meaning implicit NOT NULL.\n• The column and system watermark are added implicitly.\n• The column and system watermark are added implicitly.\n• is treated as a value column and is stored in the value part of Schema Registry.\n• and are declared as being unique, meaning no duplicates.\n• and must not contain NULLs, meaning implicit NOT NULL.\n• The column and system watermark are added implicitly. Table with overlapping names in key/value of Schema Registry but disjoint data¶\n• The key prefix is defined and is stripped before storing the schema in Schema Registry.\n• Therefore, is the Schema Registry key of type INT.\n• Also, is the Schema Registry value of type STRING.\n• Both key and value store disjoint data, so they can have different data types Create with overlapping names in key/value of Schema Registry but joint data¶\n• By default, the key is never included in the value in Schema Registry.\n• By setting , the value contains the full table schema\n• Therefore, is the Schema Registry key.\n• Also, is the Schema Registry value.\n• The payload of is stored twice in the Kafka message, because key and value store joint data and they have the same data type for . Table with string key and value in Schema Registry¶\n• Schema Registry is filled with a value subject containing .\n• The key columns are determined by the DISTRIBUTED BY clause.\n• By default, Avro in Schema Registry would be used for the key, but the WITH clause overrides this to the format.\n• None Create two Kafka clusters in different regions, for example, and .\n• None Create two Flink compute pools in different regions, for example, and .\n• None In the first region, run the following statement.\n• None In the second region, run the same statement.\n• The SQL metastore, Flink compute pools, and Kafka clusters are regional.\n• Both tables in either region share the Schema Registry subjects and . There are three ways of storing events in a table’s log, this is, in the underlying Kafka topic.\n• Every insertion event is an immutable fact.\n• Events can be distributed in a round-robin fashion across workers/shards because they are unrelated.\n• Every event is either an upsert or delete event for a primary key.\n• Events for the same primary key should land at the same worker/shard.\n• None Every upsert event is a fact that can be “undone”.\n• None This means that every event is either an insertion or its retraction.\n• None So, two events are related by all columns. In other words, the entire row is the key. For example, is related to and is related to .\n• The retract mode is intermediate between the append and upsert modes.\n• The append and upsert modes are natural to existing Kafka consumers and producers. Start with a table created by the following statement.\n• Confluent Cloud for Apache Flink always derives an appropriate changelog mode for the preceding declaration.\n• If there is no primary key, append is the safest option, because it prevents users from pushing updates into a topic accidentally, and it has the best support of downstream consumers. -- works because the query is non-updating -- does not work because the query is updating, causing an error If you need updates, and if downstream consumers support it, for example, when the consumer is another Flink job, you can set the changelog mode to retract.\n• The table starts accepting retractions during INSERT INTO.\n• Already existing records in the Kafka topic are treated as insertions. Going back to append mode is possible, but retractions (-U, -D) appear as insertions, and the Kafka header metadata column reveals the changeflag.\n• By default, the retention time is 7 days, as in all other APIs.\n• Durations in Flink support or syntax, so it doesn’t need to be in milliseconds.\n• If no unit is specified, the unit is milliseconds.\n• The following units are supported:\n\nInferred tables are tables that have not been created by using a CREATE TABLE statement, but instead are automatically detected from information about existing Kafka topics and Schema Registry entries. You can use the ALTER TABLE statement to evolve schemas for inferred tables. The following examples show output from the SHOW CREATE TABLE statement called on the resulting table. No key or value in Schema Registry¶ For an inferred table with no registered key or value schemas, SHOW CREATE TABLE returns the following output:\n• None Key and value formats are raw (binary format) with BYTES.\n• None Following Kafka message semantics, both key and value support NULL as well, so the following code is valid: No key and but record value in Schema Registry¶ For the following value schema in Schema Registry:\n• None The key format is raw (binary format) with BYTES.\n• None Following Kafka message semantics, the key supports NULL as well, so the following code is valid: Atomic key and record value in Schema Registry¶ For the following key schema in Schema Registry: And for the following value schema in Schema Registry:\n• Schema Registry defines the column data type as INT NOT NULL.\n• The column name, , is used as the default, because Schema Registry doesn’t provide a column name. Overlapping names in key/value, no key in Schema Registry¶ For the following value schema in Schema Registry:\n• The Schema Registry value schema defines columns and .\n• The column name is used as the default if no key is in Schema Registry.\n• Because would collide with value schema column, the prefix is added. Record key and record value in Schema Registry¶ For the following key schema in Schema Registry: And for the following value schema in Schema Registry:\n• Schema Registry defines columns for both key and value.\n• The column names of key and value are disjoint sets and don’t overlap. Record key and record value with overlap in Schema Registry¶ For the following key schema in Schema Registry: And for the following value schema in Schema Registry:\n• Schema Registry defines columns for both key and value.\n• The column names of key and value overlap on .\n• is set to exclude the key, because it is fully contained in the value.\n• Detecting that key is fully contained in the value requires that both field name and data type match completely, including nullability, and all fields of the key are included in the value. For the following value schema in Schema Registry: For the following value schema in Schema Registry:\n• NULL and NOT NULL are inferred depending on whether a union contains NULL.\n• Elements of a union are always NULL, because they need to be set to NULL when a different element is set.\n• If a record defines a , the field is prefixed with it, for example, . For the following value schema in Schema Registry: For the following value schema in Schema Registry: For the following value schema in Schema Registry:\n\nThe following examples show frequently used scenarios for ALTER TABLE. Flink guarantees that rows are always emitted before the watermark is generated. The following statements ensure that for perfectly ordered events, meaning events without time-skew, a watermark can be equal to the timestamp or 1 ms less than the timestamp. -- If multiple events can have the same timestamp. -- If a single event can have the timestamp. Remove the custom watermark strategy to restore the default watermark strategy.\n• +-------------+------------------------+----------+-------------------+ | Column Name | Data Type | Nullable | Extras | +-------------+------------------------+----------+-------------------+ | user | BIGINT | NOT NULL | PRIMARY KEY | | product | STRING | NULL | | | amount | INT | NULL | | | ts | TIMESTAMP(3) *ROWTIME* | NULL | WATERMARK AS `ts` | +-------------+------------------------+----------+-------------------+\n• None Remove the watermark strategy of the table.\n• None Check the new table schema and metadata. For tables that have been inferred with regular formats but contain Debezium CDC (Change Data Capture) data: -- and configure the appropriate Flink changelog interpretation mode: -- * append: Treats each record as an INSERT operation with no relationship between records -- * retract: Handles paired operations (INSERT/UPDATE/DELETE) where changes to the same row -- are represented as a retraction of the old value followed by an addition of the new value -- * upsert: Groups all operations for the primary key (derived from the Kafka message key), -- with each operation effectively merging with or replacing previous state -- and configure the appropriate Flink changelog interpretation mode: -- * append: Treats each record as an INSERT operation with no relationship between records -- * retract: Handles paired operations (INSERT/UPDATE/DELETE) where changes to the same row -- are represented as a retraction of the old value followed by an addition of the new value -- * upsert: Groups all operations for the primary key (derived from the Kafka message key), -- with each operation effectively merging with or replacing previous state -- and configure the appropriate Flink changelog interpretation mode: -- * append: Treats each record as an INSERT operation with no relationship between records -- * retract: Handles paired operations (INSERT/UPDATE/DELETE) where changes to the same row -- are represented as a retraction of the old value followed by an addition of the new value -- * upsert: Groups all operations for the primary key (derived from the Kafka message key), -- with each operation effectively merging with or replacing previous state For tables with any type of data that need a different processing mode for handling changes: -- Best for event streams where each record is independent -- Useful when changes to the same row are represented as paired operations -- Best when tracking state changes using a primary key (derived from Kafka message key) -- For read and write (persisted). Column becomes mandatory in INSERT INTO. -- Use implicit casting (origin is always MAP<BYTES, BYTES>)\n• The metadata key is . If you don’t want to name the column this way, use: .\n• Keys of headers must be unique. Multi-key headers are not supported. You can get the headers of a Kafka record as a map of raw bytes by adding a virtual metadata column.\n• None Run the following statement to add the Kafka partition as a metadata column: -- Create example topic with 1 partition filled with values\n• and control which range in the changelog (Kafka topic) to read.\n• In the example, only 1 partition is used. For multiple partitions, use the following syntax: The root cause for most “no output” cases is that a time-based operation, for example, TUMBLE, MATCH_RECOGNIZE, and FOR SYSTEM_TIME AS OF, did not receive recent enough watermarks. The current time of an operator is calculated by the minimum watermark of all inputs, meaning across all tables/topics and their partitions. If one partition does not emit a watermark, it can affect the entire pipeline. The following statements may be helpful for debugging issues related to watermarks. -- Each value lands in a separate Kafka partition (out of 4). -- Leave out values to see missing watermarks. -- If ROW_NUMBER doesn't show results, it's clearly a watermark issue. -- Use the CURRENT_WATERMARK() function to check which watermark is calculated -- Due to the table declaration (with 4 buckets), this query should show 4 rows. -- If not, the missing partitions might be the cause for watermark issues. -- A workaround could be to not use the system watermark: -- Add \"fresh\" data while the above statements with The debugging examples above won’t solve everything but may help in finding the root cause. The system watermark strategy is smart and excludes idle Kafka partitions from the watermark calculation after some time, but at least one partition must produce new data for the “logical clock” with watermarks.\n• Not enough data in Kafka partitions\n• No fresh data after warm up with historical data for progressing the logical clock Idle partitions often cause missing watermarks. Also, no data in a partition or infrequent data can be a root cause. -- Avoid the \"not enough data\" problem by using a custom watermark. -- The watermark strategy is still coarse-grained enough for this example. -- Each value lands in a separate Kafka partition, and partition 1 is empty. -- Thread 2: Insert some data immediately -> Thread 1 still without results. 'Another Bob in partition 0 shortly after' -- Thread 2: Insert some data after 15s -> Thread 1 should show results. 'Another Bob in partition 0 after 15s' Within the first 15 seconds, all partitions contribute to the watermark calculation, so the first INSERT INTO has no effect because partition 1 is still empty. After 15 seconds, all partitions are marked as idle. No partition contributes to the watermark calculation. But when the second INSERT INTO is executed, it becomes the main driving partition for the logical clock. The global watermark jumps to “second INSERT INTO - 2 seconds”. In the following code, the configuration overrides the default idle-detection algorithm, so even an immediate INSERT INTO can be the main driving partition for the logical clock, because all other partitions are marked as idle after 1 second. 'Another Bob in partition 0 shortly after' You can set the schema context for key and value formats to control the namespace for your schema resolution in Schema Registry.\n• None Set the schema context for the value format\n• +----------------------------------------------------------------------+ | SHOW CREATE TABLE | +----------------------------------------------------------------------+ | CREATE TABLE `catalog`.`database`.`orders` ( | | `user` BIGINT NOT NULL, | | `product` VARCHAR(2147483647), | | `amount` INT, | | `ts` TIMESTAMP(3) | | ) | | DISTRIBUTED BY HASH(`user`) INTO 6 BUCKETS | | WITH ( | | 'changelog.mode' = 'upsert', | | 'connector' = 'confluent', | | 'kafka.cleanup-policy' = 'delete', | | 'kafka.max-message-size' = '2097164 bytes', | | 'kafka.retention.size' = '0 bytes', | | 'kafka.retention.time' = '604800000 ms', | | 'key.format' = 'avro-registry', | | 'scan.bounded.mode' = 'unbounded', | | 'scan.startup.mode' = 'latest-offset', | | 'value.format' = 'avro-registry', | | ) | | | +----------------------------------------------------------------------+\n\nYou can use the ALTER TABLE statement to evolve schemas for inferred tables. The following examples show output from the SHOW CREATE TABLE statement called on the resulting table. For the following value schema in Schema Registry:\n• None Schema Registry says there is a timestamp physical column, but Flink says there is timestamp metadata column.\n• None In this case, metadata columns and computed columns have precedence, and Confluent Cloud for Apache Flink removes the physical column from the schema.\n• None Because Confluent Cloud for Apache Flink advertises FULL_TRANSITIVE mode, queries still work, and the physical column is set to NULL in the payload:\n• Now, both physical and metadata columns appear and can be accessed for reading and writing. Enrich a column that has no Schema Registry information¶ For the following value schema in Schema Registry:\n• Schema Registry provides only information for the value part.\n• Because the part is not backed by Schema Registry, the is .\n• The default data type of is BYTES, but you can change this by using the ALTER TABLE statement.\n• Only changes to simple, atomic types, like INT, BYTES, and STRING are supported, where the binary representation is clear.\n• For more complex modifications, use Schema Registry.\n• In multi-cluster scenarios, the ALTER TABLE statement must be executed for every cluster, because the data type for is stored in the Flink regional metastore. When working with topics that use RecordNameStrategy or TopicRecordNameStrategy, you can configure the subject names for the schema resolution in Schema Registry. This is particularly useful when handling multiple event types in a single topic. For topics using these strategies, Flink initially infers a raw binary table: Configure value schema subject names for each format: If your topic uses keyed messages, you can also configure the key format: You can configure both key and value schema subject names in a single statement:\n• Subject names must match exactly with the names registered in Schema Registry\n• The format prefix ( , , or ) must match the schema format in Schema Registry"
    },
    {
        "link": "https://docs.confluent.io/cloud/current/flink/reference/statements/create-table.html",
        "document": "Confluent Cloud for Apache Flink® enables creating tables backed by Apache Kafka® topics by using the CREATE TABLE statement. With Flink tables, you can run SQL queries on streaming data in Kafka topics.\n\nA primary key constraint is a hint for Flink SQL to leverage for optimizations which specifies that a column or a set of columns in a table or a view are unique and they do not contain null. A primary key uniquely identifies a row in a table. No columns in a primary key can be nullable. You can declare a primary key constraint together with a column definition (a column constraint) or as a single line (a table constraint). In both cases, it must be declared as a singleton. If you define more than one primary key constraint in the same statement, Flink SQL throws an exception. The SQL standard specifies that a constraint can be or , which controls whether the constraint checks are performed on the incoming/outgoing data. Flink SQL doesn’t own the data, so the only mode it supports is . It’s your responsibility to ensure that the query enforces key integrity. Flink SQL assumes correctness of the primary key by assuming that the column’s nullability is aligned with the columns in primary key. Connectors must ensure that these are aligned. The constraint distributes the table implicitly by the key column. A Kafka message key is defined either by an implicit DISTRIBUTED BY clause clause from a PRIMARY KEY constraint or an explicit . In a CREATE TABLE statement, a primary key constraint alters the column’s nullability, which means that a column with a primary key constraint isn’t nullable. The following SQL statement creates a table named with a primary key defined on . This statement creates a Kafka topic, a value-schema, and a key-schema. The value-schema contains the definitions for and , while the key-schema contains the definition for .\n\nThe clause defines the event-time attributes of a table. A watermark in Flink is used to track the progress of event time and provide a way to trigger time-based operations. Confluent Cloud for Apache Flink provides a default watermark strategy for all tables, whether created automatically from a Kafka topic or from a CREATE TABLE statement. The default watermark strategy is applied on the system column. Watermarks are calculated per Kafka partition, and at least 250 events are required per partition. If a delay of longer than 7 days can occur, choose a custom watermark strategy. Because the concrete implementation is provided by Confluent, you see only in the declaration. You can replace the default strategy with a custom strategy at any time by using ALTER TABLE. The defines an existing column that is marked as the event-time attribute of the table. The column must be of type , and it must be a top-level column in the schema. The defines the watermark generation strategy. It allows arbitrary non-query expressions, including computed columns, to calculate the watermark. The expression return type must be , which represents the timestamp since the Unix Epoch. The returned watermark is emitted only if it’s non-null and its value is larger than the previously emitted local watermark, to respect the contract of ascending watermarks. The watermark generation expression is evaluated by Flink SQL for every record. The framework emits the largest generated watermark periodically. No new watermark is emitted if any of the following conditions apply.\n• The current watermark is identical to the previous watermark.\n• The value of the returned watermark is smaller than the value of the last emitted watermark. When you use event-time semantics, your tables must contain an event-time attribute and watermarking strategy.\n• None Strictly ascending timestamps: Emit a watermark of the maximum observed timestamp so far. Rows that have a timestamp larger than the max timestamp are not late.\n• None Ascending timestamps: Emit a watermark of the maximum observed timestamp so far, minus 1. Rows that have a timestamp larger than or equal to the max timestamp are not late.\n• None Bounded out-of-orderness timestamps: Emit watermarks which are the maximum observed timestamp minus the specified delay. The following example shows a “5-seconds delayed” watermark strategy. The following CREATE TABLE statement defines an table that has a rowtime column named and a watermark strategy with a 5-second delay. When a source does not receive any elements for a timeout time, which is specified by the property, the source is marked as temporarily idle. This enables each downstream task to advance its watermark without the need to wait for watermarks from this source while it’s idle. By default, Confluent Cloud for Apache Flink has progressive idleness detection that starts at and increases to a maximum of over time. You can turn off progressive idleness by setting the property to , or you can can set a fixed idleness timeout with your desired value. For more information, see the video, How to Set Idle Timeouts.\n\nTables can also be created and populated by the results of a query in one create-table-as-select (CTAS) statement. CTAS is the simplest and fastest way to create and insert data into a table with a single command. The CTAS statement consists of two parts:\n• The SELECT part can be any SELECT query supported by Flink SQL.\n• The CREATE part takes the resulting schema from the SELECT part and creates the target table. The following two code examples are equivalent. -- Equivalent to the following CREATE TABLE and INSERT INTO statements. -- These two statements are equivalent to the preceding CREATE TABLE AS statement. Similar to CREATE TABLE, CTAS requires all options of the target table to be specified in the WITH clause. The syntax is , for example: The CREATE part enables you to specify explicit columns. The resulting table schema contains the columns defined in the CREATE part first, followed by the columns from the SELECT part. Columns named in both parts retain the same column position as defined in the SELECT part. You can also override the data type of SELECT columns if you specify it in the CREATE part. The CREATE part enable you to specify primary keys and distribution strategies. Primary keys work only on NOT NULL columns. Currently, primary keys only allow you to define columns from the SELECT part, which may be NOT NULL. The following two code examples are equivalent. -- Equivalent to the following CREATE TABLE and INSERT INTO statements. -- These two statements are equivalent to the preceding CREATE TABLE AS statement.\n\nThe CREATE TABLE LIKE clause enables creating a new table with the same schema as an existing table. It is a combination of SQL features and can be used to extend or exclude certain parts of the original table. The clause must be defined at the top-level of a CREATE statement and applies to multiple parts of the table definition. Use the LIKE options to control the merging logic of table features. You can control the merging behavior of:\n• CONSTRAINTS - Constraints such as and unique keys. with three different merging strategies:\n• INCLUDING - Includes the feature of the source table and fails on duplicate entries, for example, if an option with the same key exists in both tables.\n• EXCLUDING - Does not include the given feature of the source table.\n• OVERWRITING - Includes the feature of the source table, overwrites duplicate entries of the source table with properties of the new table. For example, if an option with the same key exists in both tables, the option from the current statement is used. Additionally, you can use the INCLUDING/EXCLUDING ALL option to specify what should be the strategy if no specific strategy is defined. For example, if you use EXCLUDING ALL INCLUDING WATERMARKS, only the watermarks are included from the source table. If you provide no LIKE options, INCLUDING ALL OVERWRITING OPTIONS is used as a default. The following CREATE TABLE statement defines a table named that has 5 physical columns and three metadata columns. You can run the following CREATE TABLE LIKE statement to define table , which contains the physical and computed columns of , drops the metadata and default watermark strategy, and applies a custom watermark strategy on .\n\nTable properties used to create a table source or sink. Both the key and value of the expression are string literals. You can change an existing table’s property values by using the ALTER TABLE Statement in Confluent Cloud for Apache Flink. You can set the following properties when you create a table. Set the changelog mode of the connector. For more information on changelog modes, see dynamic tables. These are the changelog modes for an inferred table: These are the changelog modes for a manually created table: With a primary key declared, the changelog modes have these properties:\n• means that every row can be treated as an independent fact.\n• means that the combination of and are related and must be partitioned together.\n• means that all rows with same primary key are related and must be partitioned together To build indices, primary keys must be partitioned together. Each value is an insertion (+I). A special header represents the change (+I, -U, +U, -D). The header is omitted for insertions. Append queries encoding is the same for all modes. If value is , it represents a deletion (-D). Other values are +U and the engine will normalize the changelog internally. Changes for an updating table have the change type encoded in the Kafka record as a special header that represents the change (+I, -U, +U, -D). The value of the header, if present, represents the kind of change that a row can describe in a changelog:\n• : represents UPDATE_BEFORE (-U), an update operation with the previous content of the updated row.\n• : represents UPDATE_AFTER (+U), an update operation with new content for the updated row. For more information, see Changelog entries. Set the default cleanup policy for Kafka topic log segments beyond the retention window. Translates to the Kafka property. For more information, see Log Compaction.\n• : topic log is compacted periodically in the background by the log cleaner.\n• : old log segments are discarded when their retention time or size limit is reached.\n• : compact the log and follow the retention time or size limit settings.\n• : Only return messages from committed transactions. Any transactional messages from aborted or in-progress transactions are filtered out.\n• : Return all messages, including those from transactional messages that were aborted or are still in progress. For more information, see delivery guarantees and latency. Specify a custom prefix for all fields of the key format. The property defines a custom prefix for all fields of the key format, which avoids name clashes with fields of the value format. By default, the prefix is empty. If a custom prefix is defined, the table schema property works with prefixed names. When constructing the data type of the key format, the prefix is removed, and the non-prefixed names are used within the key format. This option requires that the value.fields-include property is set to . The prefix for an inferred table is , for non-atomic Schema Registry types and fields that have a name. Specify the serialization format of the table’s key fields. These are the key formats for an inferred table: These are the key formats for a manually created table: If no format is specified, Avro Schema Registry is used by default. This applies only if a primary or distribution key is defined. Currently, the Schema Registry subject compatibility mode must be FULL or FULL_TRANSITIVE. For more information, see Schema Evolution and Compatibility for Schema Registry on Confluent Cloud. Specify the Confluent Schema Registry Schema Context for the key format. Similar to , this option allows you to specify a schema context for the key format. It provides an independent scope in Schema Registry for key schemas. Specify the bounded mode for the Kafka consumer. The following list shows the valid bounded mode values.\n• : bounded by committed offsets in Kafka brokers of a specific consumer group. This is evaluated at the start of consumption from a given partition.\n• : bounded by latest offsets. This is evaluated at the start of consumption from a given partition. If isn’t set, the default is an unbounded table. If is specified, the scan.bounded.timestamp-millis config option is required to specify a specific bounded timestamp in milliseconds since the Unix epoch, . The following list shows the valid startup mode values.\n• : start from the earliest offset possible.\n• : start from user-supplied specific offsets for each partition.\n• : start from the user-supplied timestamp for each partition. The default is . This differs from the default in Apache Flink, which is . If is specified, the scan.startup.specific-offsets config option is required to specify specific startup offsets for each partition. For example, a value of indicates offset for partition and offset for partition . If is specified, the scan.startup.timestamp-millis config option is required, to define a specific startup timestamp in milliseconds since the Unix epoch, January 1, 1970 00:00:00.000 GMT. Specify offsets for each partition when the mode is set in the scan.startup.mode property. Specify a strategy for handling key columns in the data type of the value format. If is specified, all physical columns of the table schema are included in the value format, which means that key columns appear in the data type for both the key and value format. Specify the format for serializing and deserializing the value part of Kafka messages. These are the value formats for an inferred table: These are the value formats for a manually created table: If no format is specified, Avro Schema Registry is used by default. Specify the Confluent Schema Registry Schema Context for the value format. A schema context represents an independent scope in Schema Registry and can be used to create separate “sub-registries” within one Schema Registry. Each schema context is an independent grouping of schema IDs and subject names, allowing the same schema ID in different contexts to represent completely different schemas.\n\nInferred tables are tables that have not been created by using a CREATE TABLE statement, but instead are automatically detected from information about existing Kafka topics and Schema Registry entries. You can use the ALTER TABLE statement to evolve schemas for inferred tables. The following examples show output from the SHOW CREATE TABLE statement called on the resulting table. No key or value in Schema Registry¶ For an inferred table with no registered key or value schemas, SHOW CREATE TABLE returns the following output:\n• None Key and value formats are raw (binary format) with BYTES.\n• None Following Kafka message semantics, both key and value support NULL as well, so the following code is valid: No key and but record value in Schema Registry¶ For the following value schema in Schema Registry:\n• None The key format is raw (binary format) with BYTES.\n• None Following Kafka message semantics, the key supports NULL as well, so the following code is valid: Atomic key and record value in Schema Registry¶ For the following key schema in Schema Registry: And for the following value schema in Schema Registry:\n• Schema Registry defines the column data type as INT NOT NULL.\n• The column name, , is used as the default, because Schema Registry doesn’t provide a column name. Overlapping names in key/value, no key in Schema Registry¶ For the following value schema in Schema Registry:\n• The Schema Registry value schema defines columns and .\n• The column name is used as the default if no key is in Schema Registry.\n• Because would collide with value schema column, the prefix is added. Record key and record value in Schema Registry¶ For the following key schema in Schema Registry: And for the following value schema in Schema Registry:\n• Schema Registry defines columns for both key and value.\n• The column names of key and value are disjoint sets and don’t overlap. Record key and record value with overlap in Schema Registry¶ For the following key schema in Schema Registry: And for the following value schema in Schema Registry:\n• Schema Registry defines columns for both key and value.\n• The column names of key and value overlap on .\n• is set to exclude the key, because it is fully contained in the value.\n• Detecting that key is fully contained in the value requires that both field name and data type match completely, including nullability, and all fields of the key are included in the value. For the following value schema in Schema Registry: For the following value schema in Schema Registry:\n• NULL and NOT NULL are inferred depending on whether a union contains NULL.\n• Elements of a union are always NULL, because they need to be set to NULL when a different element is set.\n• If a record defines a , the field is prefixed with it, for example, . For the following value schema in Schema Registry: For the following value schema in Schema Registry: For the following value schema in Schema Registry:"
    },
    {
        "link": "https://stackoverflow.com/questions/79293375/unable-to-insert-data-to-kafka-topic-in-apache-flink-using-upsert-kafka-connecto",
        "document": "I am working on building Apache Flink pipeline where source is SQL server CDC, and sink is Upsert Kafka, everything looks fine until when executing insert I got error of\n\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.kafka.clients.producer.ProducerConfig\n\nI am using Apache flink's Table API in Python.\n\nThis following way I am adding required jar files into my pipeline\n\nfollowing is my kafka Sink\n\nAnd this is how I am inserting data into my kafka topic"
    }
]