[
    {
        "link": "https://face-recognition.readthedocs.io/en/latest/face_recognition.html",
        "document": "Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector If you are using a GPU, this can give you much faster results since the GPU can process batches of images at once. If you aren’t using a GPU, you don’t need this function.\n• images – A list of images (each as a numpy array)\n• number_of_times_to_upsample – How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n• batch_size – How many images to include in each GPU processing batch. A list of tuples of found face locations in css (top, right, bottom, left) order\n\nCompare a list of face encodings against a candidate encoding to see if they match.\n• face_encoding_to_check – A single face encoding to compare against the list\n• tolerance – How much distance between faces to consider it a match. Lower is more strict. 0.6 is typical best performance. A list of True/False values indicating which known_face_encodings match the face encoding to check\n\nGiven a list of face encodings, compare them to a known face encoding and get a euclidean distance for each comparison face. The distance tells you how similar the faces are. A numpy ndarray with the distance for each face in the same order as the ‘faces’ array\n\nGiven an image, return the 128-dimension face encoding for each face in the image.\n• face_image – The image that contains one or more faces\n• known_face_locations – Optional - the bounding boxes of each face if you already know them.\n• num_jitters – How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n• model – Optional - which model to use. “large” or “small” (default) which only returns 5 points but is faster. A list of 128-dimensional face encodings (one for each face in the image)\n\nGiven an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n• model – Optional - which model to use. “large” (default) or “small” which only returns 5 points but is faster. A list of dicts of face feature locations (eyes, nose, etc)"
    },
    {
        "link": "https://face-recognition.readthedocs.io/en/latest/readme.html",
        "document": "Find all the faces that appear in a picture: Get the locations and outlines of each person’s eyes, nose, mouth and chin. Finding facial features is super useful for lots of important stuff. But you can also use for really stupid stuff Recognize who appears in each photo. You can even use this library with other Python libraries to do real-time face recognition: See this example for the code.\n\nWhen you install , you get a simple command-line program called that you can use to recognize faces in a First, you need to provide a folder with one picture of each person you already know. There should be one image file for each person with the files named according to who is in the picture: Next, you need a second folder with the files you want to identify: Then in you simply run the command , passing in the folder of known people and the folder (or single image) with unknown people and it tells you who is in each image: There’s one line in the output for each face. The data is comma-separated with the filename and the name of the person found. An is a face in the image that didn’t match anyone in your folder of known people. If you are getting multiple matches for the same person, it might be that the people in your photos look very similar and a lower tolerance value is needed to make face comparisons more strict. You can do that with the parameter. The default tolerance value is 0.6 and lower numbers make face comparisons more strict: If you want to see the face distance calculated for each match in order to adjust the tolerance setting, you can use : If you simply want to know the names of the people in each photograph but don’t care about file names, you could do this: Face recognition can be done in parallel if you have a computer with multiple CPU cores. For example if your system has 4 CPU cores, you can process about 4 times as many images in the same amount of time by using all your CPU cores in parallel. If you are using Python 3.4 or newer, pass in a parameter: You can also pass in to use all CPU cores in your system. You can import the module and then easily manipulate faces with just a couple of lines of code. It’s super easy! Automatically find all the faces in an image¶ # face_locations is now an array listing the co-ordinates of each face! You can also opt-in to a somewhat more accurate deep-learning-based face detection model. Note: GPU acceleration (via nvidia’s CUDA library) is required for good performance with this model. You’ll also want to enable CUDA support # face_locations is now an array listing the co-ordinates of each face! If you have a lot of images and a GPU, you can also Automatically locate the facial features of a person in an image¶ # face_landmarks_list is now an array with the locations of each facial feature in each face. # face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye. Recognize faces in images and identify who they are¶ # my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face! # Now we can see the two face encodings are of the same person with `compare_faces`! \"It's not a picture of me!\"\n\nIssue: when using face_recognition or running examples. Solution: is compiled with SSE4 or AVX support, but your CPU is too old and doesn’t support that. after You’ll need to recompileafter making the code change outlined here Issue: RuntimeError: Unsupported image type, must be 8bit gray or RGB image. when running the webcam examples. Solution: Your webcam probably isn’t set up correctly with OpenCV. Look here for more. Solution: The face_recognition_models file is too big for your available pip cache memory. Instead, try to avoid the issue. Solution: The version of you have installed is too old. You need version 19.7 or newer. Upgrade . Solution: The version of you have installed is too old. You need version 19.7 or newer. Upgrade . Solution: The version of you have installed is too old. You need version 0.17 or newer. Upgrade ."
    },
    {
        "link": "https://pypi.org/project/face-recognition",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://github.com/ageitgey/face_recognition",
        "document": "You can also read a translated version of this file in Chinese 简体中文版 or in Korean 한국어 or in Japanese 日本語.\n\nRecognize and manipulate faces from Python or from the command line with the world's simplest face recognition library.\n\nBuilt using dlib's state-of-the-art face recognition built with deep learning. The model has an accuracy of 99.38% on the Labeled Faces in the Wild benchmark.\n\nThis also provides a simple command line tool that lets you do face recognition on a folder of images from the command line!\n\nFind all the faces that appear in a picture:\n\nGet the locations and outlines of each person's eyes, nose, mouth and chin.\n\nFinding facial features is super useful for lots of important stuff. But you can also use it for really stupid stuff like applying digital make-up (think 'Meitu'):\n\nRecognize who appears in each photo.\n\nYou can even use this library with other Python libraries to do real-time face recognition:\n\nSee this example for the code.\n• macOS or Linux (Windows not officially supported, but might work)\n\nFirst, make sure you have dlib already installed with Python bindings:\n• How to install dlib from source on macOS or Ubuntu\n\nThen, make sure you have cmake installed:\n\nFinally, install this module from pypi using (or for Python 2):\n\nAlternatively, you can try this library with Docker, see this section.\n\nIf you are having trouble with installation, you can also try out a pre-configured VM.\n• Jetson Nano installation instructions\n• Please follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don't follow the instructions in the article to comment out a line in dlib and recompile it.\n\nWhile Windows isn't officially supported, helpful users have posted instructions on how to install this library:\n• Download the pre-configured VM image (for VMware Player or VirtualBox).\n\nWhen you install , you get two simple command-line programs:\n• - Recognize faces in a photograph or folder full for photographs.\n• - Find faces in a photograph or folder full for photographs.\n\nThe command lets you recognize faces in a photograph or folder full for photographs.\n\nFirst, you need to provide a folder with one picture of each person you already know. There should be one image file for each person with the files named according to who is in the picture:\n\nNext, you need a second folder with the files you want to identify:\n\nThen in you simply run the command , passing in the folder of known people and the folder (or single image) with unknown people and it tells you who is in each image:\n\nThere's one line in the output for each face. The data is comma-separated with the filename and the name of the person found.\n\nAn is a face in the image that didn't match anyone in your folder of known people.\n\nThe command lets you find the location (pixel coordinatates) of any faces in an image.\n\nJust run the command , passing in a folder of images to check (or a single image):\n\nIt prints one line for each face that was detected. The coordinates reported are the top, right, bottom and left coordinates of the face (in pixels).\n\nIf you are getting multiple matches for the same person, it might be that the people in your photos look very similar and a lower tolerance value is needed to make face comparisons more strict.\n\nYou can do that with the parameter. The default tolerance value is 0.6 and lower numbers make face comparisons more strict:\n\nIf you want to see the face distance calculated for each match in order to adjust the tolerance setting, you can use :\n\nIf you simply want to know the names of the people in each photograph but don't care about file names, you could do this:\n\nFace recognition can be done in parallel if you have a computer with multiple CPU cores. For example, if your system has 4 CPU cores, you can process about 4 times as many images in the same amount of time by using all your CPU cores in parallel.\n\nIf you are using Python 3.4 or newer, pass in a parameter:\n\nYou can also pass in to use all CPU cores in your system.\n\nYou can import the module and then easily manipulate faces with just a couple of lines of code. It's super easy!\n\nSee this example to try it out.\n\nYou can also opt-in to a somewhat more accurate deep-learning-based face detection model.\n\nNote: GPU acceleration (via NVidia's CUDA library) is required for good performance with this model. You'll also want to enable CUDA support when compliling .\n\nSee this example to try it out.\n\nIf you have a lot of images and a GPU, you can also find faces in batches.\n\nSee this example to try it out.\n\nSee this example to try it out.\n\nAll the examples are available here.\n• Find faces in batches of images w/ GPU (using deep learning)\n• Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)\n• Find and recognize unknown faces in a photograph based on photographs of known people\n• Identify and draw boxes around each person in a photo\n• Compare faces by numeric face distance instead of only True/False matches\n• Recognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)\n• Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)\n• Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)\n• Run a web service to recognize faces via HTTP (Requires Flask to be installed)\n• Train multiple images per person then recognize faces using a SVM\n\nIf you want to create a standalone executable that can run without the need to install or , you can use PyInstaller. However, it requires some custom configuration to work with this library. See this issue for how to do it.\n• My article on how Face Recognition works: Modern Face Recognition with Deep Learning\n• Covers the algorithms and how they generally work\n• Face recognition with OpenCV, Python, and deep learning by Adrian Rosebrock\n• Covers how to use face recognition in practice\n• Raspberry Pi Face Recognition by Adrian Rosebrock\n• Covers how to use this on a Raspberry Pi\n• Face clustering with Python by Adrian Rosebrock\n• Covers how to automatically cluster photos based on who appears in each photo using unsupervised learning\n\nIf you want to learn how face location and recognition work instead of depending on a black box library, read my article.\n• The face recognition model is trained on adults and does not work very well on children. It tends to mix up children quite easy using the default comparison threshold of 0.6.\n• Accuracy may vary between ethnic groups. Please see this wiki page for more details.\n\nSince depends on which is written in C++, it can be tricky to deploy an app using it to a cloud hosting provider like Heroku or AWS.\n\nTo make things easier, there's an example Dockerfile in this repo that shows how to run an app built with in a Docker container. With that, you should be able to deploy to any service that supports Docker images.\n\nYou can try the Docker image locally by running:\n\nThere are also several prebuilt Docker images.\n\nLinux users with a GPU (drivers >= 384.81) and Nvidia-Docker installed can run the example on the GPU: Open the docker-compose.yml file and uncomment the and lines.\n\nIf you run into problems, please read the Common Errors section of the wiki before filing a github issue.\n• Many, many thanks to Davis King (@nulhom) for creating dlib and for providing the trained facial feature detection and face encoding models used in this library. For more information on the ResNet that powers the face encodings, check out his blog post.\n• Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image, pillow, etc, etc that makes this kind of stuff so easy and fun in Python.\n• Thanks to Cookiecutter and the audreyr/cookiecutter-pypackage project template for making Python project packaging way more tolerable."
    },
    {
        "link": "https://github.com/ageitgey/face_recognition/blob/master/README.md",
        "document": "You can also read a translated version of this file in Chinese 简体中文版 or in Korean 한국어 or in Japanese 日本語.\n\nRecognize and manipulate faces from Python or from the command line with the world's simplest face recognition library.\n\nBuilt using dlib's state-of-the-art face recognition built with deep learning. The model has an accuracy of 99.38% on the Labeled Faces in the Wild benchmark.\n\nThis also provides a simple command line tool that lets you do face recognition on a folder of images from the command line!\n\nFind all the faces that appear in a picture:\n\nGet the locations and outlines of each person's eyes, nose, mouth and chin.\n\nFinding facial features is super useful for lots of important stuff. But you can also use it for really stupid stuff like applying digital make-up (think 'Meitu'):\n\nRecognize who appears in each photo.\n\nYou can even use this library with other Python libraries to do real-time face recognition:\n\nSee this example for the code.\n• macOS or Linux (Windows not officially supported, but might work)\n\nFirst, make sure you have dlib already installed with Python bindings:\n• How to install dlib from source on macOS or Ubuntu\n\nThen, make sure you have cmake installed:\n\nFinally, install this module from pypi using (or for Python 2):\n\nAlternatively, you can try this library with Docker, see this section.\n\nIf you are having trouble with installation, you can also try out a pre-configured VM.\n• Jetson Nano installation instructions\n• Please follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don't follow the instructions in the article to comment out a line in dlib and recompile it.\n\nWhile Windows isn't officially supported, helpful users have posted instructions on how to install this library:\n• Download the pre-configured VM image (for VMware Player or VirtualBox).\n\nWhen you install , you get two simple command-line programs:\n• - Recognize faces in a photograph or folder full for photographs.\n• - Find faces in a photograph or folder full for photographs.\n\nThe command lets you recognize faces in a photograph or folder full for photographs.\n\nFirst, you need to provide a folder with one picture of each person you already know. There should be one image file for each person with the files named according to who is in the picture:\n\nNext, you need a second folder with the files you want to identify:\n\nThen in you simply run the command , passing in the folder of known people and the folder (or single image) with unknown people and it tells you who is in each image:\n\nThere's one line in the output for each face. The data is comma-separated with the filename and the name of the person found.\n\nAn is a face in the image that didn't match anyone in your folder of known people.\n\nThe command lets you find the location (pixel coordinatates) of any faces in an image.\n\nJust run the command , passing in a folder of images to check (or a single image):\n\nIt prints one line for each face that was detected. The coordinates reported are the top, right, bottom and left coordinates of the face (in pixels).\n\nIf you are getting multiple matches for the same person, it might be that the people in your photos look very similar and a lower tolerance value is needed to make face comparisons more strict.\n\nYou can do that with the parameter. The default tolerance value is 0.6 and lower numbers make face comparisons more strict:\n\nIf you want to see the face distance calculated for each match in order to adjust the tolerance setting, you can use :\n\nIf you simply want to know the names of the people in each photograph but don't care about file names, you could do this:\n\nFace recognition can be done in parallel if you have a computer with multiple CPU cores. For example, if your system has 4 CPU cores, you can process about 4 times as many images in the same amount of time by using all your CPU cores in parallel.\n\nIf you are using Python 3.4 or newer, pass in a parameter:\n\nYou can also pass in to use all CPU cores in your system.\n\nYou can import the module and then easily manipulate faces with just a couple of lines of code. It's super easy!\n\nSee this example to try it out.\n\nYou can also opt-in to a somewhat more accurate deep-learning-based face detection model.\n\nNote: GPU acceleration (via NVidia's CUDA library) is required for good performance with this model. You'll also want to enable CUDA support when compliling .\n\nSee this example to try it out.\n\nIf you have a lot of images and a GPU, you can also find faces in batches.\n\nSee this example to try it out.\n\nSee this example to try it out.\n\nAll the examples are available here.\n• Find faces in batches of images w/ GPU (using deep learning)\n• Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)\n• Find and recognize unknown faces in a photograph based on photographs of known people\n• Identify and draw boxes around each person in a photo\n• Compare faces by numeric face distance instead of only True/False matches\n• Recognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)\n• Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)\n• Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)\n• Run a web service to recognize faces via HTTP (Requires Flask to be installed)\n• Train multiple images per person then recognize faces using a SVM\n\nIf you want to create a standalone executable that can run without the need to install or , you can use PyInstaller. However, it requires some custom configuration to work with this library. See this issue for how to do it.\n• My article on how Face Recognition works: Modern Face Recognition with Deep Learning\n• Covers the algorithms and how they generally work\n• Face recognition with OpenCV, Python, and deep learning by Adrian Rosebrock\n• Covers how to use face recognition in practice\n• Raspberry Pi Face Recognition by Adrian Rosebrock\n• Covers how to use this on a Raspberry Pi\n• Face clustering with Python by Adrian Rosebrock\n• Covers how to automatically cluster photos based on who appears in each photo using unsupervised learning\n\nIf you want to learn how face location and recognition work instead of depending on a black box library, read my article.\n• The face recognition model is trained on adults and does not work very well on children. It tends to mix up children quite easy using the default comparison threshold of 0.6.\n• Accuracy may vary between ethnic groups. Please see this wiki page for more details.\n\nSince depends on which is written in C++, it can be tricky to deploy an app using it to a cloud hosting provider like Heroku or AWS.\n\nTo make things easier, there's an example Dockerfile in this repo that shows how to run an app built with in a Docker container. With that, you should be able to deploy to any service that supports Docker images.\n\nYou can try the Docker image locally by running:\n\nThere are also several prebuilt Docker images.\n\nLinux users with a GPU (drivers >= 384.81) and Nvidia-Docker installed can run the example on the GPU: Open the docker-compose.yml file and uncomment the and lines.\n\nIf you run into problems, please read the Common Errors section of the wiki before filing a github issue.\n• Many, many thanks to Davis King (@nulhom) for creating dlib and for providing the trained facial feature detection and face encoding models used in this library. For more information on the ResNet that powers the face encodings, check out his blog post.\n• Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image, pillow, etc, etc that makes this kind of stuff so easy and fun in Python.\n• Thanks to Cookiecutter and the audreyr/cookiecutter-pypackage project template for making Python project packaging way more tolerable."
    },
    {
        "link": "https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html",
        "document": ""
    },
    {
        "link": "https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html",
        "document": "\n• Learn to capture video from a camera and display it.\n• You will learn these functions : cv.VideoCapture(), cv.VideoWriter()\n\nOften, we have to capture live stream with a camera. OpenCV provides a very simple interface to do this. Let's capture a video from the camera (I am using the built-in webcam on my laptop), convert it into grayscale video and display it. Just a simple task to get started.\n\nTo capture a video, you need to create a VideoCapture object. Its argument can be either the device index or the name of a video file. A device index is just the number to specify which camera. Normally one camera will be connected (as in my case). So I simply pass 0 (or -1). You can select the second camera by passing 1 and so on. After that, you can capture frame-by-frame. But at the end, don't forget to release the capture.\n\nreturns a bool ( / ). If the frame is read correctly, it will be . So you can check for the end of the video by checking this returned value.\n\nSometimes, cap may not have initialized the capture. In that case, this code shows an error. You can check whether it is initialized or not by the method cap.isOpened(). If it is , OK. Otherwise open it using cap.open().\n\nYou can also access some of the features of this video using cap.get(propId) method where propId is a number from 0 to 18. Each number denotes a property of the video (if it is applicable to that video). Full details can be seen here: cv::VideoCapture::get(). Some of these values can be modified using cap.set(propId, value). Value is the new value you want.\n\nFor example, I can check the frame width and height by and . It gives me 640x480 by default. But I want to modify it to 320x240. Just use and .\n\nPlaying video from file is the same as capturing it from camera, just change the camera index to a video file name. Also while displaying the frame, use appropriate time for . If it is too less, video will be very fast and if it is too high, video will be slow (Well, that is how you can display videos in slow motion). 25 milliseconds will be OK in normal cases.\n\nSo we capture a video and process it frame-by-frame, and we want to save that video. For images, it is very simple: just use . Here, a little more work is required.\n\nThis time we create a VideoWriter object. We should specify the output file name (eg: output.avi). Then we should specify the FourCC code (details in next paragraph). Then number of frames per second (fps) and frame size should be passed. And the last one is the isColor flag. If it is , the encoder expect color frame, otherwise it works with grayscale frame.\n\nFourCC is a 4-byte code used to specify the video codec. The list of available codes can be found in fourcc.org. It is platform dependent. The following codecs work fine for me.\n• In Fedora: DIVX, XVID, MJPG, X264, WMV1, WMV2. (XVID is more preferable. MJPG results in high size video. X264 gives very small size video)\n• In Windows: DIVX (More to be tested and added)\n\nFourCC code is passed as ‘cv.VideoWriter_fourcc('M’,'J','P','G') cv.VideoWriter_fourcc(*'MJPG')` for MJPG.\n\nThe below code captures from a camera, flips every frame in the vertical direction, and saves the video."
    },
    {
        "link": "https://docs.opencv.org/3.4/dd/d43/tutorial_py_video_display.html",
        "document": "\n• Learn to capture video from a camera and display it.\n• You will learn these functions : cv.VideoCapture(), cv.VideoWriter()\n\nOften, we have to capture live stream with a camera. OpenCV provides a very simple interface to do this. Let's capture a video from the camera (I am using the built-in webcam on my laptop), convert it into grayscale video and display it. Just a simple task to get started.\n\nTo capture a video, you need to create a VideoCapture object. Its argument can be either the device index or the name of a video file. A device index is just the number to specify which camera. Normally one camera will be connected (as in my case). So I simply pass 0 (or -1). You can select the second camera by passing 1 and so on. After that, you can capture frame-by-frame. But at the end, don't forget to release the capture.\n\nreturns a bool ( / ). If the frame is read correctly, it will be . So you can check for the end of the video by checking this returned value.\n\nSometimes, cap may not have initialized the capture. In that case, this code shows an error. You can check whether it is initialized or not by the method cap.isOpened(). If it is , OK. Otherwise open it using cap.open().\n\nYou can also access some of the features of this video using cap.get(propId) method where propId is a number from 0 to 18. Each number denotes a property of the video (if it is applicable to that video). Full details can be seen here: cv::VideoCapture::get(). Some of these values can be modified using cap.set(propId, value). Value is the new value you want.\n\nFor example, I can check the frame width and height by and . It gives me 640x480 by default. But I want to modify it to 320x240. Just use and .\n\nPlaying video from file is the same as capturing it from camera, just change the camera index to a video file name. Also while displaying the frame, use appropriate time for . If it is too less, video will be very fast and if it is too high, video will be slow (Well, that is how you can display videos in slow motion). 25 milliseconds will be OK in normal cases.\n\nSo we capture a video and process it frame-by-frame, and we want to save that video. For images, it is very simple: just use . Here, a little more work is required.\n\nThis time we create a VideoWriter object. We should specify the output file name (eg: output.avi). Then we should specify the FourCC code (details in next paragraph). Then number of frames per second (fps) and frame size should be passed. And the last one is the isColor flag. If it is , the encoder expect color frame, otherwise it works with grayscale frame.\n\nFourCC is a 4-byte code used to specify the video codec. The list of available codes can be found in fourcc.org. It is platform dependent. The following codecs work fine for me.\n• In Fedora: DIVX, XVID, MJPG, X264, WMV1, WMV2. (XVID is more preferable. MJPG results in high size video. X264 gives very small size video)\n• In Windows: DIVX (More to be tested and added)\n\nFourCC code is passed as `cv.VideoWriter_fourcc('M','J','P','G') cv.VideoWriter_fourcc(*'MJPG')` for MJPG.\n\nThe below code captures from a camera, flips every frame in the vertical direction, and saves the video."
    },
    {
        "link": "https://geeksforgeeks.org/python-opencv-capture-video-from-camera",
        "document": "Python provides various libraries for image and video processing. One of them is OpenCV. OpenCV is a vast library that helps in providing various functions for image and video operations. With OpenCV, we can capture a video from the camera. It lets you create a video capture object which is helpful to capture videos through webcam and then you may perform desired operations on that video.\n• None ) to create a video capture object for the camera.\n• None Create a VideoWriter object to save captured frames as a video in the computer.\n• None Set up an infinite while loop and use the method to read the frames using the above created object.\n• None method to show the frames in the video.\n• None Breaks the loop when the user clicks a specific key.\n\nBelow is the implementation.\n\nFirst, we import the OpenCV library for python i.e. cv2. We use the VideoCapture() class of OpenCV to create the camera object which is used to capture photos or videos. The constructor of this class takes an integer argument which denotes the hardware camera index. Suppose the devices has two cameras, then we can capture photos from first camera by using VideoCapture(0) and if we want to use second camera, we should use VideoCapture(1).\n\nThe dimensions of the frame and the recorded video must be specified. For this purpose, we use the camera object method which is cam.get() to get cv2.CAP_PROP_FRAME_WIDTH and cv2.CAP_PROP_FRAME_HEIGHT which are the fields of cv2 package. The default values of these fields depend upon the actual hardware camera and drivers which are used.\n\nWe are creating a VideoWriter object to write the captured frames to a video file which is stored in the local machine. The constructor of this class takes the following parameters:\n• Filepath/Filename: The file would be saved by specified name and at the specified path. If we mention only the file name, then the video file would be saved at the same folder where the python script for capturing video is running.\n• Four Character Code: A four-letter word to denote the specific encoding and decoding technique or format which must be used in order to create the video. Some popular codecs include ‘MJPG’ = Motion JPEG codec, ‘X264’ = H.264 codec, ‘MP4V’ =\n• Frame rate: Frame rate determines the number of frames or individual images which are shown in a second. Higher frame rates produce smoother video, but it will also take more storage space as compared to lower frame rates.\n• Video Dimension: The height and width of the video which are passed as a tuple.\n\nInside an infinite while loop, we use the read() method of the camera object which returns two values, the first value is boolean and describes whether the frame was successfully captured or not. The second value is a 3D numpy array which represents the actual photo capture. In order to represent an image, we need two-dimensional data structure and for representing color information, we must add another dimension to it making it three dimensional.\n\nThen, we are writing the individual frames to the video file using the VideoWriter object which was previously created using the write() method which takes the captured frame as the argument. To display the captured frame, we are using cv2.imshow() function, which takes window name and the captured frame as arguments. This process is repeated infinitely to produce continuous video stream which consists of these frames.\n\nIn order to exit the application, we have break out of this infinite while loop. Hence we are assigning a key which can be used to quit the application. Here the cv2.waitKey() function takes and integer argument which denotes the number of milliseconds the program waits for to check whether the user has pressed any button. We can use this function to adjust the frame rate of the captured video by providing certain delay. We have assigned “q” as the quit button and we are checking every millisecond whether it is pressed in order to break out of the while loop. The ord() function returns the ASCII value of the character which is passed to it as an argument."
    },
    {
        "link": "https://docs.opencv.org/4.x/d8/dfe/classcv_1_1VideoCapture.html",
        "document": "Class for video capturing from video files, image sequences or cameras. More...\n\nClass for video capturing from video files, image sequences or cameras. The class provides C++ API for capturing video from cameras or for reading video files and image sequences. Here is how the class can be used: Returns true if the array has no elements. Class for video capturing from video files, image sequences or cameras. Grabs, decodes and returns the next video frame. Opens a video file or a capturing device or an IP video stream for video capturing. Returns true if video capturing has been initialized already. In C API the black-box structure is used instead of VideoCapture.\n• (C++) A basic sample on using the VideoCapture interface can be found at\n• (Python) A basic sample on using the VideoCapture interface can be found at\n• (Python) A multi threaded video processing sample can be found at\n• (Python) VideoCapture sample showcasing some features of the Video4Linux2 backend\n\nOpens a video file or a capturing device or an IP video stream for video capturing with API Preference and parameters. This is an overloaded member function, provided for convenience. It differs from the above function only in what argument(s) it accepts. The parameter allows to specify extra parameters encoded as pairs . See cv::VideoCaptureProperties\n\nGrabs the next frame from video file or capturing device. (non-zero) in the case of success. The method/function grabs the next frame from video file or camera and returns true (non-zero) in the case of success. The primary use of the function is in multi-camera environments, especially when the cameras do not have hardware synchronization. That is, you call VideoCapture::grab() for each camera and after that call the slower method VideoCapture::retrieve() to decode and get frame from each camera. This way the overhead on demosaicing or motion jpeg decompression etc. is eliminated and the retrieved frames from different cameras will be closer in time. Also, when a connected camera is multi-head (for example, a stereo camera or a Kinect device), the correct way of retrieving data from it is to call VideoCapture::grab() first and then call VideoCapture::retrieve() one or more times with different values of the channel parameter. Using Kinect and other OpenNI compatible depth sensors\n\nOpens a video file or a capturing device or an IP video stream for video capturing with API Preference and parameters. This is an overloaded member function, provided for convenience. It differs from the above function only in what argument(s) it accepts. The parameter allows to specify extra parameters encoded as pairs . See cv::VideoCaptureProperties if the file has been successfully opened The method first calls VideoCapture::release to close the already opened file or camera.\n\nGrabs, decodes and returns the next video frame. the video frame is returned here. If no frames has been grabbed the image will be empty. if no frames has been grabbed The method/function combines VideoCapture::grab() and VideoCapture::retrieve() in one call. This is the most convenient method for reading video files or capturing data from decode and returns the just grabbed frame. If no frames has been grabbed (camera has been disconnected, or there are no more frames in video file), the method returns false and the function returns empty image (with cv::Mat, test it with Mat::empty()). In C API, functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video capturing structure. It is not allowed to modify or release the image! You can copy the frame using cvCloneImage and then do whatever you want with the copy.\n\nThe documentation for this class was generated from the following file:"
    }
]