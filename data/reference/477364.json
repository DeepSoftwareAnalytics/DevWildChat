[
    {
        "link": "https://flink.apache.org/2020/08/04/pyflink-the-integration-of-pandas-into-pyflink",
        "document": "PyFlink: The integration of Pandas into PyFlink\n\nPython has evolved into one of the most important programming languages for many fields of data processing. So big has been Python’s popularity, that it has pretty much become the default data processing language for data scientists. On top of that, there is a plethora of Python-based data processing tools such as NumPy, Pandas, and Scikit-learn that have gained additional popularity due to their flexibility or powerful functionalities.\n\nIn an effort to meet the user needs and demands, the Flink community hopes to leverage and make better use of these tools. Along this direction, the Flink community put some great effort in integrating Pandas into PyFlink with the latest Flink version 1.11. Some of the added features include support for Pandas UDF and the conversion between Pandas DataFrame and Table. Pandas UDF not only greatly improve the execution performance of Python UDF, but also make it more convenient for users to leverage libraries such as Pandas and NumPy in Python UDF. Additionally, providing support for the conversion between Pandas DataFrame and Table enables users to switch processing engines seamlessly without the need for an intermediate connector. In the remainder of this article, we will introduce how these functionalities work and how to use them with a step-by-step example.\n\nCurrently, only Scalar Pandas UDFs are supported in PyFlink.\n\nUsing scalar Python UDF was already possible in Flink 1.10 as described in a previous article on the Flink blog. Scalar Python UDFs work based on three primary steps:\n• the Java operator serializes one input row to bytes and sends them to the Python worker;\n• the Python worker deserializes the input row and evaluates the Python UDF with it;\n• the resulting row is serialized and sent back to the Java operator\n\nWhile providing support for Python UDFs in PyFlink greatly improved the user experience, it had some drawbacks, namely resulting in:\n• Difficulty when leveraging popular Python libraries used by data scientists — such as Pandas or NumPy — that provide high-performance data structure and functions.\n\nThe introduction of Pandas UDF is used to address these drawbacks. For Pandas UDF, a batch of rows is transferred between the JVM and PVM in a columnar format (Arrow memory format). The batch of rows will be converted into a collection of Pandas Series and will be transferred to the Pandas UDF to then leverage popular Python libraries (such as Pandas, or NumPy) for the Python UDF implementation.\n\nThe performance of vectorized UDFs is usually much higher when compared to the normal Python UDF, as the serialization/deserialization overhead is minimized by falling back to Apache Arrow, while handling as input/output allows us to take full advantage of the Pandas and NumPy libraries, making it a popular solution to parallelize Machine Learning and other large-scale, distributed data science workloads (e.g. feature engineering, distributed model application).\n\nPandas DataFrame is the de-facto standard for working with tabular data in the Python community while PyFlink Table is Flink’s representation of the tabular data in Python. Enabling the conversion between PyFlink Table and Pandas DataFrame allows switching between PyFlink and Pandas seamlessly when processing data in Python. Users can process data by utilizing one execution engine and switch to a different one effortlessly. For example, in case users already have a Pandas DataFrame at hand and want to perform some expensive transformation, they can easily convert it to a PyFlink Table and leverage the power of the Flink engine. On the other hand, users can also convert a PyFlink Table to a Pandas DataFrame and perform the same transformation with the rich functionalities provided by the Pandas ecosystem.\n\nUsing Python in Apache Flink requires installing PyFlink, which is available on PyPI and can be easily installed using . Before installing PyFlink, check the working version of Python running in your system using:\n\nPlease note that Python 3.5 or higher is required to install and run PyFlink\n\nPandas UDFs take as the input and return a of the same length as the output. Pandas UDFs can be used at the exact same place where non-Pandas functions are currently being utilized. To mark a UDF as a Pandas UDF, you only need to add an extra parameter udf_type=“pandas” in the udf decorator:\n\nThe Pandas UDF above uses the Pandas function to interpolate the missing temperature data for each equipment id. This is a common IoT scenario whereby each equipment/device reports it’s id and temperature to be analyzed, but the temperature field may be null due to various reasons. With the function, you can register and use it in the same way as the normal Python UDF. Below is a complete example of how to use the Pandas UDF in PyFlink.\n• Firstly, you need to prepare the input data in the “/tmp/input” file. For example,\n• Next, you can run this example on the command line,\n\nThe command builds and runs the Python Table API program in a local mini-cluster. You can also submit the Python Table API program to a remote cluster using different command lines, see more details here.\n• Finally, you can see the execution result on the command line. As you can see, all the temperature data with an empty value has been interpolated:\n\nYou can use the method to create a PyFlink Table from a Pandas DataFrame or use the method to convert a PyFlink Table to a Pandas DataFrame.\n\nIn this article, we introduce the integration of Pandas in Flink 1.11, including Pandas UDF and the conversion between Table and Pandas. In fact, in the latest Apache Flink release, there are many excellent features added to PyFlink, such as support of User-defined Table functions and User-defined Metrics for Python UDFs. What’s more, from Flink 1.11, you can build PyFlink with Cython support and “Cythonize” your Python UDFs to substantially improve code execution speed (up to 30x faster, compared to Python UDFs in Flink 1.10).\n\nFuture work by the community will focus on adding more features and bringing additional optimizations with follow up releases. Such optimizations and additions include a Python DataStream API and more integration with the Python ecosystem, such as support for distributed Pandas in Flink. Stay tuned for more information and updates with the upcoming releases!"
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-master/api/python/examples/table/pandas.html",
        "document": ""
    },
    {
        "link": "https://quix.io/blog/pyflink-deep-dive",
        "document": "Due to its versatility, ease of use, and rich ecosystem, Python has become one of the most popular programming languages in the world. It has numerous applications, especially across data science, data analysis, and machine learning. Such use cases typically require collecting and processing high volumes of data, often in real time.\n\nTraditionally, Java has been the go-to language for data processing frameworks. However, since Python is the lingua franca in the data science world, it’s no wonder we’re witnessing the rise of Python-based data processing tech. This article explores one of these technologies: PyFlink.\n\nPyFlink is a Python-based interface for Apache Flink. It was first introduced in 2019 as part of Apache Flink version 1.9. PyFlink is particularly useful for development and data teams looking to harness Flink’s data processing features using Python rather than Java or Scala.\n\nPyFlink offers compatibility with many of Flink’s core capabilities. Additionally, it allows you to integrate Python-specific libraries such as NumPy and Pandas. However, leveraging these libraries within PyFlink workflows is not as straightforward as you’d think because it involves the use of User-Defined Functions (UDFs), which add extra complexity.\n• PyFlink Table API. Unified, relational API that can handle both batch and stream processing. It allows you to write queries in a way that is similar to SQL or working with tabular data in Python.\n• PyFlink DataStream API. Suitable for building stateful stream processing applications. It gives you fine-grained control over state and time and allows you to implement complex transformations.\n• Exploratory data analysis (although alternatives like Apache Spark are arguably more commonly used for this use case)\n\nWe’ll now look at what it takes to build a PyFlink pipeline that ingests data from a source, processes it, and then sends it to a destination. Flink natively supports various connectors for seamless integration with source and sink systems such as filesystems, Apache Kafka, Apache Cassandra, DynamoDB, Amazon Kinesis, RabbitMQ, and MongoDB. In addition to using pre-existing connectors, you can write your own custom ones.\n\nWe will go through a couple of basic examples, showing how to use PyFlink’s Table and DataStream APIs to read data from a Kafka topic, process it, and then send the output to another Kafka topic.\n\nWe’ll also review other aspects, such as logging and debugging your PyFlink programs, handling dependencies, and deploying PyFlink jobs to production.\n\nNote: If you’ve used Flink previously or if you have a good theoretical understanding of its features and capabilities, you will come across plenty of familiar concepts (since Flink is the underlying foundation for PyFlink). If you’re entirely new to Flink, it might be worth acquainting yourself with its basics (e.g., features, capabilities, architecture, key concepts) before diving into the PyFlink content below.\n\nBefore diving into hands-on examples and code, I think it’s worth discussing the workflow, roles, and responsibilities involved in bringing a PyFlink program from concept to production.\n\nThe high-level PyFlink workflow generally looks like this:\n• Create a job (set up a sink and source, and implement your processing logic via PyFlink’s built-in capabilities or through custom UDFs).\n• Debug, iterate, and optimize the job until you’re satisfied.\n• Deploy the job to a remote cluster for further testing and debugging.\n• Debug and optimize the job (again).\n• Manage the job in production (continuously monitor the job and debug it if necessary).\n\nGetting a PyFlink pipeline from ideation to production involves a collaboration between different types of stakeholders. Ideally, all of the following roles are involved in creating and deploying PyFlink programs:\n\nExact responsibilities may differ from organization to organization. Large-scale enterprises may have all these roles, with clearly defined responsibilities. However, in medium and early-stage companies, these roles and responsibilities are often conflated. For instance, in a start-up, a data engineer might also take on ML engineering tasks, while a software developer might additionally manage DevOps responsibilities like job deployment and infrastructure management.\n\nIn any case, the takeaway is that working with PyFlink is not a one-person job. Quite the contrary. It’s a multi-step process that introduces operational complexity and requires cross-functional collaboration. You might even need a dedicated team (or teams!) to handle large-scale PyFLink deployments.\n\nAt the time of writing (March 2024), using PyFlink requires a Python version between 3.8 and 3.11.\n\nPyFlink is available on PyPi. Installing it is as simple as:\n\nYou can also build your own custom PyFlink package for pip installation. To do so, follow these instructions.\n\nThe PyFlink Table API uses a Domain-Specific Language (DSL) for defining table queries and transformations. While DSLs can offer benefits like higher abstraction levels and a more intuitive syntax for domain-specific tasks, they also have downsides:\n• There’s a learning curve associated with any DSL.\n• Integrating DSLs with other parts of a software system or with other tools and languages can be challenging.\n• Users will likely encounter limitations when trying to implement functionality that goes beyond the scope of DSLs.\n\nBefore using the PyFflink Table API to implement a processing job, you must first create an execution environment ( ). This is the main component within PyFlink's Table API for setting up, configuring, and executing table-based data transformations. Here’s a snippet demonstrating how to declare an execution environment:\n\nAfter that’s done, you can create sink and source tables. As mentioned before, for our example, we will be consuming from and sending data to Kafka topics:\n\nThe code snippet above registers two tables, and . The former consumes data from a Kafka topic named , while the latter writes data to another Kafka topic named .\n\nAfter setting up the sink and source tables, you can implement your data processing logic. PyFlink’s Table API operators allow you to transform and manipulate tables. Numerous operators are available; for brevity, I won’t go through all of them here. Instead, I will list some of the main types of operations you can perform with them:\n• Aggregations such as grouping by clause and aggregating group rows.\n• Joins (for example, inner, outer, and interval joins).\n\nRefer to the documentation to see the entire list of operators available for the Table API, together with code snippets for each.\n\nUnlike operators, which are mainly used to shape the overall structure of your tables, functions generally allow you to transform the data in tables. PyFlink’s Table API offers numerous built-in functions. These enable a wide range of transformations, including string manipulations, mathematical calculations, date and time manipulation, type conversion, pattern matching, and the implementation of conditional logic.\n\nYou can even write custom UDFs to extend PyFlink’s Table API capabilities beyond built-in functions. These can be either standard UDFs, which process data one row at a time, or vectorized UDFs, which process data one batch at a time. A UDF can be further classified into one of the following categories:\n• Scalar function. It maps zero, one, or multiple scalar values to a new scalar value.\n• Table function. It takes zero, one, or multiple scalar values as input and returns an arbitrary number of rows as output.\n• Aggregate function. It maps scalar values of multiple rows to a new scalar value.\n• Table aggregate function. It maps scalar values of multiple rows to zero, one, or multiple rows.\n\nAnyway, going back to our example, let’s assume we want to drop a couple of columns ( and ) from the table (which reads from in Kafka). We can do that by using the operator:\n\nThe final line of code in the snippet above writes the result to the table. The use of triggers the execution of the job, where the transformed data is inserted into the table (and from there, to the in Kafka).\n\nTo recap, here’s our end-to-end example:\n\nYou can execute jobs locally for testing and debugging purposes. For instance, if you name your job “sales_report_job.py”, this is how you do it:\n\nThis will launch a local mini-cluster that runs in a single process to execute the PyFlink job.\n\nMore information about deploying PyFlink jobs to remote clusters and production environments is available later in this article.\n\nThe PyFlink DataStream API provides a lower-level abstraction for building real-time, event-driven applications. It enables detailed control over data streams and stateful computations. However, it has a steep learning curve; to make the most of it, users must understand concepts like checkpoints, savepoints and state management, which can be complex for newcomers. Learn more about:\n\nAdditionally, deploying and managing stateful processing applications in production brings many challenges.\n\nSimilar to the Table API, building a processing job with PyFlink’s DataStream API requires you to first declare an execution environment ( ):\n\nThe is the central component responsible for creating, configuring, and executing streaming data applications, similar to the role played by the in the Table API.\n\nOnce you have an execution environment, you can configure your source and sink. For our example, the source and sink point to Kafka topics:\n\nThe code snippet above initializes a Kafka source to ingest data from a topic named and a Kafka sink to output data to a topic named .\n\nUp next, let’s see what kind of processing we can do. PyFlink’s DataStream API operators enable us to implement transformations on data streams. Similar to the Table API, PyFlink’s DataStream API offers numerous operators. While I won't cover all of them, I'll highlight some key types of transformations you can perform:\n• Map. Transform an element of the stream to another element.\n• FlatMap. Similar to Map, but each input item can be transformed into zero, one, or more output items.\n• Filter. Evaluate a boolean function for each element and retain those for which the function returns true.\n• KeyBy. Logically partition the stream based on certain attributes; necessary for aggregations and keyed process functions.\n• Reduce. Apply a reduce function to consecutive elements in a keyed stream to produce a rolling aggregation.\n• Window Join. Join two streams on a key and a common window.\n\nCheck out this page to see the entire list of transformations available for the DataStream API, together with code examples for each.\n\nIt’s important to note that you can also use UDFs to customize and define the functionality of transformations to fit the specific needs and requirements of your application. There are several different ways to do so:\n• Implement function interfaces (e.g., is provided for the transformation, is provided for the transformation, etc.).\n• Define the functionality of the transformation through a Lambda function.\n• Use a Python function to implement the logic of the transformation.\n\nFor our example, let’s assume we want to perform sentiment analysis on data coming from the Kafka source (the topic). To do so, we can use a UDF that customizes and implements the transformation:\n\nTo submit the job for execution, you simply have to call .\n\nHere’s what our end-to-end job looks like:\n\nThe job will continuously consume social media posts from the source Kafka topic ( ) and perform sentiment analysis via the class. categorizes each post as \"Positive,\" \"Negative,\" or \"Neutral\" based on the presence of certain keywords. The output is a string that includes the original post text along with its determined sentiment. This output is sent to the sync Kafka topic ( ). This entire process happens in real time.\n\nBut let’s take it one step further. Let’s say you not only want to classify each post, but you additionally want to apply 5-minute tumbling windows to count the number of posts per sentiment category within each window. This aggregation is useful for monitoring and analyzing evolving trends over time (and it’s a classical example of stateful stream processing).\n\nHere’s how we could adjust our existing job to incorporate these additions:\n\nOnce you’ve configured the job, you’re ready to test it. Similar to Table API jobs, you can execute DataStream API jobs locally for testing and debugging purposes:\n\nThis will launch a local mini-cluster that runs in a single process to execute the PyFlink job.\n\nYour PyFlink jobs may depend on external components (e.g., other Python libraries, ML frameworks, data storage solutions) that aren’t part of the Flink distribution. You must manage these dependencies to ensure your PyFlink job runs smoothly.\n\nPyFlink provides several ways to handle dependencies, which I will briefly present.\n\nThird-party JARs can be specified in the Table and DataStream APIs. Here’s an example showing how you can do that:\n\nSimilar to JARs, Python libraries can also be configured via the Table and DataStream APIs as dependencies:\n\nAlternatively, third-party Python dependencies can be specified via a file.\n\nNote that using Python libraries within PyFlink involves writing UDFs that allow you to extend the capabilities of PyFlink by incorporating custom logic. This is a powerful feature, as it opens up the vast ecosystem of Python libraries for tasks such as data manipulation, machine learning, and statistical analysis directly within your PyFlink data pipelines. However, this also brings some challenges:\n• Invoking Python UDFs adds a performance overhead that can affect the latency and throughput of your data processing tasks, especially for high-volume or low-latency requirements.\n• Debugging and testing UDFs is more complex than using Flink’s built-in operators and functions.\n\nGoing beyond Python libraries, PyFlink allows you to specify archive files and Python interpreters (for executing Python workers and parsing Python UDFs on the client side) as dependencies in your code. Read the PyFlink Dependency Management documentation page to find out more.\n\nNote: If you are using both the DataStream API and the Table API in a single job, it’s enough to specify the dependencies via the DataStream API to ensure they work for both APIs.\n\nAll the dependencies I’ve presented above can also be passed through command line arguments in the Flink CLI when submitting the job.\n\nPyFlink supports several types of conversions. First of all, you can convert Pandas DataFrames to PyFlink tables and the other way around. This offers a way to port data and leverage the strengths of both tools — Pandas for complex data manipulation and analysis tasks and PyFlink for scalable data processing. However, bear in mind that converting between Pandas DataFrames and PyFlink tables can introduce a performance overhead (due to serialization and deserialization of data). Additionally, you might encounter data compatibility issues, and you’ll have a more complex workflow.\n\nThe following example shows how to create a PyFlink table from a Pandas DataFrame:\n\nAnd this is how you convert a PyFlink table to a Pandas DataFrame:\n\nSecondly, PyFlink allows you to convert a table into a DataStream and vice versa. This is useful for scenarios where you might use the Table API alongside the DataStream API as part of your data processing pipeline. For example, you could rely on the Table API for some initial stateless data normalization and cleansing before implementing the main stateful data processing pipeline with the DataStream API. Similar to porting data between PyFlink tables and Pandas DataFrames, the ability to convert between tables and DataStreams can impact performance and lead to increased complexity.\n\nTo help with debugging, PyFlink applications support both client-side and server-side logging. You can use print statements and standard Python logging modules to log contextual and debug information (outside of UDF code).\n\nHere’s an example of how to add logging to the client side:\n\nClient logs will appear in the client's log files during job submission.\n\nAnd here’s a snippet demonstrating how to implement logging on the server side:\n\nThese server logs will appear in the log files of the during job execution.\n\nIf the environment variable is set, logs will be written in the log directory under . Otherwise, logs will be written to the PyFlink module directory.\n\nYou can debug Python UDFs either locally or remotely:\n• Local debugging implies debugging your functions directly in an IDE (such as PyCharm, PyDev, Wing Python IDE, or VS Code, to name just a few).\n• Remote debugging can be done by using the pydevd_pycharm debugger. See these instructions for details.\n\nIn some scenarios, Java knowledge might be required in addition to Python to perform debugging. That’s because Flink is fundamentally a Java technology, while PyFlink is just a wrapper around it. For example, if your PyFlink application experiences performance slowdowns or memory leaks, the issue might stem from the way Python processes interact with Java processes. Debugging the Java side could help you understand how resources are being managed and how data is being passed between the Python and Java layers, potentially uncovering inefficiencies or bugs in the JVM or the Flink Java codebase.\n\nIn practice, if you’re a data scientist, developer, or ML engineer who knows Python but not Java, you may need to collaborate with Java developers to debug your PyFlink programs.\n\nAfter you’ve configured and tested your PyFlink job locally, it’s time to move it to a remote environment for further testing and, eventually, production.\n\nYou can submit PyFlink jobs to remote clusters (standalone, YARN, or Kubernetes) for execution via the Flink CLI. Here’s an example demonstrating how to run a job on a remote Kubernetes cluster:\n\nMore examples are available here.\n\nWhen working with UDFs, there are two different runtime execution modes to choose from:\n• process. UDFs are executed in separate Python processes. This mode offers better resource isolation.\n• thread. UDFs are executed in threads within the Flink JVM process. This option ensures higher throughput and lower latencies.\n\nThe following snippet shows how to specify what execution mode you want to use:\n\nPyFlink leverages Flink’s scalability and fault tolerance capabilities. It supports elastic scaling, which allows the system to dynamically adjust computational resources in response to workload changes. PyFlink can scale to process up to petabytes of data per day, across hundreds or even thousands of nodes.\n\nFor fault tolerance, PyFlink employs checkpoints and savepoints. Checkpoints periodically and automatically capture the state of processing tasks. This allows for recovery from failures by restoring to the most recent checkpoint. Meanwhile, savepoints are triggered manually to create snapshots of the application's state. They’re mostly used for planned operations (e.g., updating the Flink version).\n\nPyFlink also inherits Flink’s monitoring capabilities. Internally, Flink provides metrics accessible via its web UI, including job throughput, latency, operator backpressure, and memory usage, which are essential for identifying bottlenecks and optimizing PyFlink applications. Additionally, (Py)Flink’s metrics system can be extended to external monitoring tools such as Prometheus and Grafana.\n\nEnsuring robust monitoring, effective scaling, and fault tolerance for PyFlink applications in production are some of the greatest operational challenges of working with PyFlink. Dealing with these challenges requires a significant time investment and ongoing financial costs.\n\nLet’s now take a look at how PyFlink works under the hood. Specifically, at how jobs are compiled and then executed. First, though, I suggest you familiarize yourself with Flink’s architecture (in case you aren’t already). It’s important to understand Flink’s internals before delving into PyFlink, as PyFlink builds upon them.\n\nThe key thing to bear in mind is that PyFlink is not a rewrite of the Flink engine in Python. Instead, it’s a wrapper around Flink’s Java computing engine.\n\nPyFlink reuses the existing job compiling stack of Flink’s Java API. PyFlink and the Flink Java API expose equivalent classes and objects (e.g., ). When a Python program makes a PyFlink API call, a corresponding Java object is created in the JVM and the method is called on it. Communication between the Python Virtual Machine (PVM) and Java Virtual Machine (JVM) layers is powered by Py4J.\n\nA Flink job is typically composed of a sequence of data transformations, where each transformation is represented by an operator. Once you submit the job to a Flink cluster, assigns the job’s operators to for execution.\n\nAs I mentioned earlier in this article, when working with Python UDFs, there are two execution modes to choose from: and .\n\nIn mode, special Python operators are used to handle Python UDFs. This is needed because Python UDFs are executed by processes running on Python workers. The communication between JVM and PVM is achieved through various gRPC services (e.g., DataService manages the transfer of business data) that are built using Apache Beam's Fn APIs.\n\nExecuting Python UDFs in separate Python processes adds serialization/deserialization and communication overhead, leading to extra latency. That’s why starting with Apache Flink version 1.15, the execution mode was introduced. This new mode allows Python UDFs to be executed in the JVM as a thread instead of a separate Python process, which leads to lower latency and better throughput. However, it has its drawbacks too. For example, it only supports the CPython interpreter, and it offers reduced isolation compared to the mode. To learn more about and mode and when it’s best to use each, check out this article.\n\nPyFlink: the good and the bad\n\nI’ve already mentioned (or implied) some of PyFlink’s advantages and disadvantages throughout this article, but I think it’s worth collating them here to refresh our memories. They are key aspects you will have to consider if you are pondering whether or not to adopt PyFlink.\n\nLet’s start with the pros of using (Py)Flink:\n• Makes Flink’s features accessible to Python users (developers, engineers and to a lesser extent, data scientists).\n• Rich stream processing capabilities, such as complex event-time processing, windowing, and state management.\n• Supports Python UDFs, which allows you to implement custom processing logic.\n• Scalable (up to thousands of nodes running in parallel) and able to handle large volumes of high-velocity data with low latency (in the millisecond range).\n• Can be used in conjunction with Python libraries like Pandas and NumPy.\n• Integrates with a variety of tools and systems through sink and source connectors, including streaming platforms like Apache Kafka, message queues (e.g., RabbitMQ), and databases (for instance, MongoDB).\n\nAnd here are the cons of using PyFlink:\n• The PyFlink Table API is effectively a query language that executes standard operations by compiling them to Java at runtime. This is very limiting to Python developers who won’t be free to use their own classes, methods, or external libraries.\n• The PyFlink DataStream API allows you to execute dependencies such as third-party Python libraries and ML models, however the documentation is limited and it’s very difficult to understand how to make use of the capability. Basically, you can create a UDF to execute your ML model in a SQL-style query, but it’s not intuitive.\n• PyFlink is essentially a wrapper around Flink’s Java APIs. While it’s great that you can write your code in Python, you will sometimes need Java expertise for debugging. It would be much more convenient and efficient if you could code and debug in the same language.\n• PyFlink bridges Python's runtime with the Java Virtual Machine (JVM) where Flink operates. This requires data serialization and deserialization between Python and Java, leading to additional computational costs and potential bottlenecks, especially in high-throughput scenarios.\n• When new Flink features or improvements are launched, these updates are generally first available in the Java API, and only later added to the Python API. So it’s safe to expect a feature parity gap between Flink’s Java API and PyFlink.\n• The learning curve for PyFlink can be steep. Depending on the team's prior experience with stream processing and distributed systems, this can take anywhere from a few weeks to a few months.\n• Setting up and configuring a (Py)Flink pipeline can be difficult, time-consuming, and expensive, often requiring a dedicated team. For instance, the Contentsquare engineering team worked for a year (!) to migrate from Spark to Flink and ensure everything worked as intended; it was a road riddled with challenges and gotchas they wished they had known earlier.\n• Considering its steep learning curve and complexity, it’s probably not cost-efficient to use (Py)Flink for small and medium workloads.\n\nDue to Flink’s proven capabilities and growing popularity, PyFlink is certainly worth considering if you are specifically interested in big data processing with Python. But other similar solutions are worth investigating too. Noteworthy Python alternatives to PyFlink include Apache Spark (PySpark), Faust, and Bytewax. I encourage you to assess them all before deciding which technology best suits your needs.\n\nAnother alternative to PyFlink is Quix Streams, an open-source library for building containerized stream processing applications with Python and Apache Kafka. In a nutshell, Quix Streams brings the following benefits:\n• No JVM, no orchestrator, no server-side engine.\n• Easily integrates with the entire Python ecosystem (pandas, scikit-learn, TensorFlow, PyTorch, etc).\n• Support for many serialization formats, including JSON (and Quix-specific).\n• A simple framework with a Pandas-like interface to ease newcomers to streaming.\n• Designed to run and scale resiliently via container orchestration (e.g., Kubernetes).\n• Easily runs locally and in Jupyter Notebook for convenient development and debugging.\n\nIn addition, Quix Streams seamlessly integrates with Quix Cloud, which provides fully managed containers, Kafka, and observability tools. By pairing Quix Streams with Quix Cloud, you can build, deploy, and monitor Python stream processing apps without the headache of managing the underlying infrastructure. To learn more about Quix and what it can do for you, check out:\n• The gallery of Quix sample projects. These are fully functioning event-driven applications that you can use to figure out what is possible with Quix, how Quix works, and how to start building your own projects.‍"
    },
    {
        "link": "https://flink.apache.org/2020/04/09/pyflink-introducing-python-support-for-udfs-in-flinks-table-api",
        "document": "Flink 1.9 introduced the Python Table API, allowing developers and data engineers to write Python Table API jobs for Table transformations and analysis, such as Python ETL or aggregate jobs. However, Python users faced some limitations when it came to support for Python UDFs in Flink 1.9, preventing them from extending the system’s built-in functionality.\n\nIn Flink 1.10, the community further extended the support for Python by adding Python UDFs in PyFlink. Additionally, both the Python UDF environment and dependency management are now supported, allowing users to import third-party libraries in the UDFs, leveraging Python’s rich set of third-party libraries.\n\nBefore diving into how you can define and use Python UDFs, we explain the motivation and background behind how UDFs work in PyFlink and provide some additional context about the implementation of our approach. Below we give a brief introduction on the PyFlink architecture from job submission, all the way to executing the Python UDF.\n\nThe PyFlink architecture mainly includes two parts — local and cluster — as shown in the architecture visual below. The local phase is the compilation of the job, and the cluster is the execution of the job.\n\nFor the local part, the Python API is a mapping of the Java API: each time Python executes a method in the figure above, it will synchronously call the method corresponding to Java through Py4J, and finally generate a Java JobGraph, before submitting it to the cluster.\n\nFor the cluster part, just like ordinary Java jobs, the JobMaster schedules tasks to TaskManagers. The tasks that include Python UDF in a TaskManager involve the execution of Java and Python operators. In the Python UDF operator, various gRPC services are used to provide different communications between the Java VM and the Python VM, such as DataService for data transmissions, StateService for state requirements, and Logging and Metrics Services. These services are built on Beam’s Fn API. While currently only Process mode is supported for Python workers, support for Docker mode and External service mode is also considered for future Flink releases.\n\nHow to use PyFlink with UDFs in Flink 1.10 #\n\nThis section provides some Python user defined function (UDF) examples, including how to install PyFlink, how to define/register/invoke UDFs in PyFlink and how to execute the job.\n\nUsing Python in Apache Flink requires installing PyFlink. PyFlink is available through PyPI and can be easily installed using pip:\n\nPlease note that Python 3.5 or higher is required to install and run PyFlink\n\nThere are many ways to define a Python scalar function, besides extending the base class . The following example shows the different ways of defining a Python scalar function that takes two columns of as input parameters and returns the sum of them as the result.\n\nBelow, you can find a complete example of using Python UDF.\n\nFirstly, you need to prepare the input data in the “/tmp/input” file. For example,\n\nNext, you can run this example on the command line,\n\nThe command builds and runs the Python Table API program in a local mini-cluster. You can also submit the Python Table API program to a remote cluster using different command lines, (see more details here).\n\nFinally, you can see the execution result on the command line:\n\nIn many cases, you would like to import third-party dependencies in the Python UDF. The example below provides detailed guidance on how to manage such dependencies.\n\nSuppose you want to use the to perform the sum of the example above. The Python UDF may look like:\n\nTo make it available on the worker node that does not contain the dependency, you can specify the dependencies with the following commands and API:\n\nA file that defines the third-party dependencies is used. If the dependencies cannot be accessed in the cluster, then you can specify a directory containing the installation packages of these dependencies by using the parameter “ ”, as illustrated in the example above. The dependencies will be uploaded to the cluster and installed offline.\n\nIn this blog post, we introduced the architecture of Python UDFs in PyFlink and provided some examples on how to define, register and invoke UDFs. Flink 1.10 brings Python support in the framework to new levels, allowing Python users to write even more magic with their preferred language. The community is actively working towards continuously improving the functionality and performance of PyFlink. Future work in upcoming releases will introduce support for Pandas UDFs in scalar and aggregate functions, add support to use Python UDFs through the SQL client to further expand the usage scope of Python UDFs, provide support for a Python ML Pipeline API and finally work towards even more performance improvements. The picture below provides more details on the roadmap for succeeding releases."
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-release-2.0-preview1/docs/dev/python/table/udfs/python_udfs",
        "document": ""
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-release-1.3/api/java/org/apache/flink/table/functions/ScalarFunction.html",
        "document": ""
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-release-2.0/api/java/org/apache/flink/table/functions/ScalarFunction.html",
        "document": ""
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/functions/udfs",
        "document": ""
    },
    {
        "link": "https://ververica.com/blog/announcing-the-release-of-apache-flink-1.19",
        "document": "The Apache Flink PMC is pleased to announce the release of Apache Flink 1.19.0. As usual, we are looking at a packed release with a wide variety of improvements and new features. Overall, 162 people contributed to this release, including our very own Release Manager, Jing Ge (Head of Engineering). In this release a total of 33 FLIPs were completed along with 600+ issues. Thank you!\n\nNow in Flink 1.19, you can set a custom parallelism for performance tuning via the option. The first available connector is DataGen (Kafka connector is on the way). Here is an example using SQL Client:\n\nA new option for specifying the Java options is introduced in Flink 1.19, so you can fine-tune the memory settings, garbage collection behavior, and other relevant Java parameters for SQL Gateway.\n\nStarting from Flink 1.18, Table API and SQL users can set state time-to-live (TTL) individually for stateful operators via the SQL compiled plan. In Flink 1.19, users have a more flexible way to specify custom TTL values for regular joins and group aggregations directly within their queries by utilizing the STATE_TTL hint. This improvement means that you no longer need to alter your compiled plan to set specific TTLs for these frequently used operators. With the introduction of hints, you can streamline your workflow and dynamically adjust the TTL based on your operational requirements.\n\nHere is an example:\n\nNamed parameters now can be used when calling a function or stored procedure. With named parameters, users do not need to strictly specify the parameter position, just specify the parameter name and its corresponding value. At the same time, if non-essential parameters are not specified, they will default to being filled with null.\n\nHere’s an example of defining a function with one mandatory parameter and two optional parameters using named parameters:\n\nWhen calling the function in SQL, parameters can be specified by name, for example:\n\nAlso the optional parameters can be omitted:\n• Supports SESSION Window TVF in Streaming Mode\n\nNow users can use SESSION Window TVF in streaming mode. A simple example is as follows:\n• Supports Changelog Inputs for Window TVF Aggregation\n\nWindow aggregation operators (generated based on Window TVF Function) can now handle changelog streams (e.g., CDC data sources, etc.). Users are recommended to migrate from legacy window aggregation to the new syntax for more complete feature support.\n\nThe common UDF type works well for CPU-intensive operations, but less well for IO bound or otherwise long-running computations. In Flink 1.19, we have a new which is a user-defined asynchronous allows for issuing concurrent function calls asynchronously.\n\nThe record amplification is a pain point when performing cascading joins in Flink, now in Flink 1.19, the new mini-batch optimization can be used for regular join to reduce intermediate result in such cascading join scenarios.\n\nIn Flink 1.19, we have supported dynamic source parallelism inference for batch jobs, which allows source connectors to dynamically infer the parallelism based on the actual amount of data to consume. This feature is a significant improvement over previous versions, which only assigned a fixed default parallelism to source vertices. Source connectors need to implement the inference interface to enable dynamic parallelism inference. Currently, the FileSource connector has already been developed with this functionality in place. Additionally, the configuration will be used as the upper bound of source parallelism inference. And now it will not default to 1. Instead, if it is not set, the upper bound of allowed parallelism set via will be used. If that configuration is also not set, the default parallelism set via or will be used instead.\n\nStarting with Flink 1.19, Flink has officially introduced full support for the standard YAML 1.2 syntax. The default configuration file has been changed to and placed in the directory. Users should directly modify this file to configure Flink. If users want to use the legacy configuration file , users just need to copy this file into the directory. Once the legacy configuration file is detected, Flink will prioritize using it as the configuration file. And in the upcoming Flink 2.0, the configuration file will no longer work.\n\nIn Flink 1.19, we support triggering profiling at the JobManager/TaskManager level, allowing users to create a profiling instance with arbitrary intervals and event modes (supported by async-profiler). Users can easily submit profiles and export results in the Flink Web UI.\n\nFor example, users can simply submit a profiling instance with a specified period and mode by “Creating a Profiling Instance” after identifying a candidate TaskManager/JobManager with a performance bottleneck:\n\nthen easily download the interactive HTML file after the profiling instance is complete:\n\nA set of administrator JVM options are available, which prepend the user-set extra JVM options for platform-wide JVM tuning.\n\nApache Flink was made ready to compile and run with Java 21. This feature is still in beta mode. Issues should be reported in Flink’s bug tracker.\n\nUsing Larger Checkpointing Interval When Source is Processing Backlog\n\nis introduced to indicate whether a record should be processed with low latency or high throughput. Connector developers can update the Source implementation to utilize the method to report whether the records are backlog records. Users can set the to use a larger checkpoint interval to enhance the throughput while the job is processing backlog if the source is backlog-aware.\n• FLIP-309: Support using larger checkpointing interval when source is processing backlog\n\nNow when disposing of no longer needed checkpoints, every state handle/state file will be disposed in parallel by the ioExecutor, vastly improving the disposing speed of a single checkpoint (for large checkpoints the disposal time can be improved from 10 minutes to < 1 minute) . The old behavior can be restored by setting to false.\n\nBy specifying the ‘-full’ option, a full checkpoint is triggered. Otherwise an incremental checkpoint is triggered if the job is configured to take incremental ones periodically.\n\nNew Interfaces to SinkV2 That Are Consistent with Source API\n\nIn Flink 1.19, the SinkV2 API made some changes to align with Source API.\n\nThe following interfaces are deprecated: , , , , .\n\nThe following new interfaces have been introduced: , , , .\n\nThe following interface method’s parameter has been changed: .\n\nThe original interfaces will remain available during the 1.19 release line, but they will be removed in consecutive releases.\n• FLIP-372: Enhance and synchronize Sink API to match the Source API\n\nNew Committer Metrics to Track the Status of Committables\n\nThe method parameterization has been changed, a new parameter has been added. The original method will remain available during the 1.19 release line, but they will be removed in consecutive releases.\n\nIn preparation for the release of Flink 2.0 later this year, the community has decided to officially deprecate multiple APIs that were approaching end of life for a while.\n• Flink’s is now officially deprecated and will be dropped in Flink 2.0 Please migrate it to Java’s own class. Methods supporting the class that replace the deprecated -based methods were introduced.\n• is deprecated. Please use or mode instead to get a clear state file ownership when restoring.\n• The old method of resolving schema compatibility has been deprecated, please migrate to the new method following Migrating from deprecated before Flink 1.19\n• Configuring serialization behavior through hard codes is deprecated, e.g., . Please use the options , , , and . Registration of instance-level serializers is deprecated, using class-level serializers instead.\n• We have deprecated all and methods except and , such as: , , and etc. Users and developers are recommend to use get and set methods with instead of string as key.\n• The non- objects in the , , and and their corresponding getter/setter interfaces are now be deprecated. These objects and methods are planned to be removed in Flink 2.0. The deprecated interfaces include the getter and setter methods of , , and .\n• is now officially deprecated and will be dropped in Flink 2.0. Please migrate all related usages to the new getter method:\n\nMigrate to \n\nMigrate to \n\nMigrate to\n• method has been deprecated and will be removed in future versions. Users are encouraged to migrate to the new .\n• is deprecated and replaced with .\n\nThe Flink community tries to ensure that upgrades are as seamless as possible. However, certain changes may require users to make adjustments to certain parts of the program when upgrading to version 1.19. Please refer to the release notes for a comprehensive list of adjustments to make and issues to check during the upgrading process.\n\nThe Apache Flink community would like to express gratitude to all the contributors who made this release possible:\n\nThis blog post was originally published by Lincoln Lee on The Apache Flink Blog."
    },
    {
        "link": "https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/functions/udfs",
        "document": ""
    }
]