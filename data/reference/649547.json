[
    {
        "link": "https://kernel.org/doc/html/next/core-api/mm-api.html",
        "document": "These flags provide hints about how mobile the page is. Pages with similar mobility are placed within the same pageblocks to minimise problems due to external fragmentation. (also a zone modifier) indicates that the page can be moved by page migration during memory compaction or can be reclaimed. is used for slab allocations that specify SLAB_RECLAIM_ACCOUNT and whose pages can be freed via shrinkers. indicates the caller intends to dirty the page. Where possible, these pages will be spread between local zones to avoid all the dirty pages being in one zone (fair zone allocation policy). forces the allocation to be satisfied from the requested node with no fallbacks or placement policy enforcements. causes the allocation to be accounted to kmemcg. causes slab allocation to have no object extension. indicates that the caller is high-priority and that granting the request is necessary before the system can make forward progress. For example creating an IO context to clean pages and requests from atomic context. allows access to all memory. This should only be used when the caller guarantees the allocation will allow more memory to be freed very shortly e.g. process exiting or swapping. Users either should be the MM or co-ordinating closely with the VM (e.g. swap over NFS). Users of this flag have to be extremely careful to not deplete the reserve completely and implement a throttling mechanism which controls the consumption of the reserve based on the amount of freed memory. Usage of a pre-allocated pool (e.g. mempool) should be always considered before using this flag. is used to explicitly forbid access to emergency reserves. This takes precedence over the flag if both are set. Please note that all the following flags are only applicable to sleepable allocations (e.g. and will ignore them). can call down to the low-level FS. Clearing the flag avoids the allocator recursing into the filesystem which might already be holding locks. indicates that the caller may enter direct reclaim. This flag can be cleared to avoid unnecessary delays when a fallback option is available. indicates that the caller wants to wake kswapd when the low watermark is reached and have it reclaim pages until the high watermark is reached. A caller may wish to clear this flag when fallback options are available and the reclaim is likely to disrupt the system. The canonical example is THP allocation where a fallback is cheap but reclaim/compaction may cause indirect stalls. is shorthand to allow/forbid both direct and kswapd reclaim. The default allocator behavior depends on the request size. We have a concept of so-called costly allocations (with order > ). !costly allocations are too essential to fail so they are implicitly non-failing by default (with some exceptions like OOM victims might fail so the caller still has to check for failures) while costly requests try to be not disruptive and back off even without invoking the OOM killer. The following three modifiers might be used to override some of these implicit rules. Please note that all of them must be used along with flag. : The VM implementation will try only very lightweight memory direct reclaim to get some memory under memory pressure (thus it can sleep). It will avoid disruptive actions like OOM killer. The caller must handle the failure which is quite likely to happen under heavy memory pressure. The flag is suitable when failure can easily be handled at small cost, such as reduced throughput. : The VM implementation will retry memory reclaim procedures that have previously failed if there is some indication that progress has been made elsewhere. It can wait for other tasks to attempt high-level approaches to freeing memory such as compaction (which removes fragmentation) and page-out. There is still a definite limit to the number of retries, but it is a larger limit than with . Allocations with this flag may fail, but only when there is genuinely little unused memory. While these allocations do not directly trigger the OOM killer, their failure indicates that the system is likely to need to use the OOM killer soon. The caller must handle failure, but can reasonably do so by failing a higher-level request, or completing it only in a much less efficient manner. If the allocation does fail, and the caller is in a position to free some non-essential memory, doing so could benefit the system as a whole. : The VM implementation _must_ retry infinitely: the caller cannot handle allocation failures. The allocation could block indefinitely but will never return with failure. Testing for failure is pointless. It _must_ be blockable and used together with __GFP_DIRECT_RECLAIM. It should _never_ be used in non-sleepable contexts. New users should be evaluated carefully (and the flag should be used only when there is no reasonable failure policy) but it is definitely preferable to use the flag rather than opencode endless loop around allocator. Allocating pages from the buddy with __GFP_NOFAIL and order > 1 is not supported. Please consider using kvmalloc() instead. Useful GFP flag combinations that are commonly used. It is recommended that subsystems start with one of these combinations and then set/clear flags as necessary. users can not sleep and need the allocation to succeed. A lower watermark is applied to allow access to “atomic reserves”. The current implementation doesn’t support NMI and few other strict non-preemptive contexts (e.g. raw_spin_lock). The same applies to . is typical for kernel-internal allocations. The caller requires or a lower zone for direct access but can direct reclaim. is the same as GFP_KERNEL, except the allocation is accounted to kmemcg. is for kernel allocations that should not stall for direct reclaim, start physical IO or use any filesystem callback. It is very likely to fail to allocate memory, even for very small allocations. will use direct reclaim to discard clean pages or slab pages that do not require the starting of any physical IO. Please try to avoid using this flag directly and instead use memalloc_noio_{save,restore} to mark the whole scope which cannot perform any IO with a short explanation why. All allocation requests will inherit GFP_NOIO implicitly. will use direct reclaim but will not use any filesystem interfaces. Please try to avoid using this flag directly and instead use memalloc_nofs_{save,restore} to mark the whole scope which cannot/shouldn’t recurse into the FS layer with a short explanation why. All allocation requests will inherit GFP_NOFS implicitly. is for userspace allocations that also need to be directly accessibly by the kernel or hardware. It is typically used by hardware for buffers that are mapped to userspace (e.g. graphics) that hardware still must DMA to. cpuset limits are enforced for these allocations. exists for historical reasons and should be avoided where possible. The flags indicates that the caller requires that the lowest zone be used ( or 16M on x86-64). Ideally, this would be removed but it would require careful auditing as some users really require it and others use the flag to avoid lowmem reserves in and treat the lowest zone as a type of emergency reserve. is similar to except that the caller requires a 32-bit address. Note that kmalloc(..., GFP_DMA32) does not return DMA32 memory because the DMA32 kmalloc cache array is not implemented. (Reason: there is no such user in kernel). is for userspace allocations that may be mapped to userspace, do not need to be directly accessible by the kernel but that cannot move once in use. An example may be a hardware allocation that maps data directly into userspace but has no addressing limitations. is for userspace allocations that the kernel does not need direct access to but can use when access is required. They are expected to be movable via page reclaim or page migration. Typically, pages on the LRU would also be allocated with . and are used for THP allocations. They are compound allocations that will generally fail quickly if memory is not available and will not wake kswapd/kcompactd on failure. The _LIGHT version does not attempt reclaim/compaction at all and is by default used in page fault path, while the non-light is used by khugepaged.\n\nSufficiently large objects are aligned on cache line boundary. For object size smaller than a half of cache line size, the alignment is on the half of cache line size. In general, if object size is smaller than 1/2^n of cache line size, the alignment is adjusted to 1/2^n. If explicit alignment is also requested by the respective field, the greater of both is alignments is applied. This delays freeing the SLAB page by a grace period, it does _NOT_ delay object freeing. This means that if you do that memory location is free to be reused at any time. Thus it may be possible to see another object there in the same RCU grace period. This feature only ensures the memory location backing the object stays valid, the trick to using this is relying on an independent object validation pass. Something like: begin: rcu_read_lock(); obj = lockless_lookup(key); if (obj) { if (!try_get_ref(obj)) // might fail for free objects rcu_read_unlock(); goto begin; if (obj->key != key) { // not the object we expected put_ref(obj); rcu_read_unlock(); goto begin; } } rcu_read_unlock(); This is useful if we need to approach a kernel structure obliquely, from its address obtained without the usual locking. We can lock the structure to stabilize it and check it’s still at the given address, only if we can be sure that the memory has not been meanwhile reused for some other kind of object (which our subsystem’s lock might corrupt). rcu_read_lock before reading the address, then rcu_read_unlock after taking the spinlock within the structure expected at that address. Note that object identity check has to be done after acquiring a reference, therefore user has to ensure proper ordering for loads. Similarly, when initializing objects allocated with SLAB_TYPESAFE_BY_RCU, the newly allocated object has to be fully initialized before its refcount gets initialized and proper ordering for stores is required. refcount_{add|inc}_not_zero_acquire() and are designed with the proper fences required for reference counting objects allocated with SLAB_TYPESAFE_BY_RCU. Note that it is not possible to acquire a lock within a structure allocated with SLAB_TYPESAFE_BY_RCU without first acquiring a reference as described above. The reason is that SLAB_TYPESAFE_BY_RCU pages are not zeroed before being given to the slab, which means that any locks must be initialized after each and every kmem_struct_alloc(). Alternatively, make the ctor passed to initialize the locks at page-allocation time, as is done in __i915_request_ctor(), sighand_ctor(), and anon_vma_ctor(). Such a ctor permits readers to safely acquire those ctor-initialized locks under protection. All object allocations from this cache will be memcg accounted, regardless of __GFP_ACCOUNT being or not being passed to individual allocations. Use this flag for caches that have an associated shrinker. As a result, slab pages are allocated with __GFP_RECLAIMABLE, which affects grouping pages by mobility, and are accounted in SReclaimable counter in /proc/meminfo The required alignment for the objects. is a valid offset, when usersize is non- means no usercopy region is specified. Custom offset for the free pointer in caches By default caches place the free pointer outside of the object. This might cause the object to grow in size. Cache creators that have a reason to avoid this can specify a custom free pointer offset in their struct where the free pointer will be placed. Note that placing the free pointer inside the object requires the caller to ensure that no fields are invalidated that are required to guard against object recycling (See for details). Using as a value for freeptr_offset is valid. If freeptr_offset is specified, must be set . Note that ctor currently isn’t supported with custom free pointers as a ctor requires an external free pointer. Whether a freeptr_offset is used. The constructor is invoked for each object in a newly allocated slab page. It is the cache user’s responsibility to free object in the same state as after calling the constructor, or deal appropriately with any differences between a freshly constructed and a reallocated object. Any uninitialized fields of the structure are interpreted as unused. The exception is freeptr_offset where is a valid value, so use_freeptr_offset must be also set to in order to interpret the field as used. For useroffset is also valid, but only with non- usersize. When args is passed to , it is equivalent to all fields unused. Create a kmem cache with a region suitable for copying to userspace. A string which is used in /proc/slabinfo to identify this cache. The size of objects to be created in this cache. The required alignment for the objects. A constructor for the objects, or . This is a legacy wrapper, new code should use either KMEM_CACHE_USERCOPY() if whitelisting a single field is sufficient, or with the necessary parameters passed via the args parameter (see ) a pointer to the cache on success, NULL on failure. A string which is used in /proc/slabinfo to identify this cache. The size of objects to be created in this cache. Optional arguments, see . Passing means defaults will be used for all the arguments. This is currently implemented as a macro using to call either the new variant of the function, or a legacy one. The new variant has 4 parameters: See which implements this. The align and ctor parameters map to the respective fields of Cannot be called within a interrupt, but can be interrupted. a pointer to the cache on success, NULL on failure. This should not be used for writing beyond the originally requested allocation size. Either use krealloc() or round up the allocation size with prior to allocation. If this is used to access beyond the originally requested allocation size, UBSAN_BOUNDS and/or FORTIFY_SOURCE may trip, since they only know about the originally allocated size via the __alloc_size attribute. The cache to allocate from. Allocate an object from this cache. See kmem_cache_zalloc() for a shortcut of adding __GFP_ZERO to flags. pointer to the new object or in case of error address of the slab object to memcg charge kmem_cache_charge allows charging a slab object to the current memcg, primarily in cases where charging at allocation time might not be possible because the target memcg is not known (i.e. softirq context) The objp should be pointer returned by the slab allocator functions like kmalloc (with __GFP_ACCOUNT in flags) or kmem_cache_alloc. The memcg charge behavior can be controlled through gfpflags parameter, which affects how the necessary internal metadata can be allocated. Including __GFP_NOFAIL denotes that overcharging is requested instead of failure, but is not applied for the internal metadata allocation. There are several cases where it will return true even if the charging was not done: More specifically:\n• None For slab objects from KMALLOC_NORMAL caches - allocated by without __GFP_ACCOUNT true if charge was successful otherwise false. how many bytes of memory are required. kmalloc is the normal method of allocating memory for objects smaller than page size in the kernel. The allocated object address is aligned to at least ARCH_KMALLOC_MINALIGN bytes. For size of power of two bytes, the alignment is also guaranteed to be at least to the size. For other sizes, the alignment is guaranteed to be at least the largest power-of-two divisor of size. The flags argument may be one of the GFP flags defined at include/linux/gfp_types.h and described at Documentation/core-api/mm-api.rst The recommended usage of the flags is described at Documentation/core-api/memory-allocation.rst Below is a brief outline of the most useful GFP flags Allocation will not sleep. May use emergency pools. Also it is possible to set different flags by OR’ing in one or more of the following additional flags: Zero the allocated memory before returning. Also see . This allocation has high priority and may use emergency pools. Indicate that this allocation is in no way allowed to fail (think twice before using). If memory is not immediately available, then give up at once. Try really hard to succeed the allocation but fail eventually. the type of memory to allocate (see kmalloc). pointer to the memory chunk to reallocate new number of elements to alloc new size of a single member of the array the type of memory to allocate (see kmalloc) If __GFP_ZERO logic is requested, callers must ensure that, starting with the initial memory allocation, every subsequent call to this API for the same memory allocation is flagged with __GFP_ZERO. Otherwise, it is possible that __GFP_ZERO is not fully honored by this API. See krealloc_noprof() for further details. In any case, the contents of the object pointed to are preserved up to the lesser of the new and old sizes. allocate memory for an array. The memory is set to zero. the type of memory to allocate (see kmalloc). allocate memory. The memory is set to zero. how many bytes of memory are required. the type of memory to allocate (see kmalloc). Report allocation bucket size for the given size Number of bytes to round up from. This returns the number of bytes that would be available in a allocation of size bytes. For example, a 126 byte request would be rounded up to the next sized kmalloc bucket, 128 bytes. (This is strictly for the general-purpose -based allocations, and is not for the pre-sized -based allocations.) Use this to the full bucket size ahead of time instead of using to query the size after an allocation. The cache the allocation was from. Free an object which was previously allocated from this cache. If object is NULL, no operation is performed. kvfree frees memory allocated by any of vmalloc(), or kvmalloc(). It is slightly more efficient to use or if you are certain that you know which one to use. address of the data object to be freed. Use the special function to clear the content of a kvmalloc’ed object containing sensitive data to make sure that the compiler won’t optimize out the data clearing. A string which is used in /proc/slabinfo to identify this cache. The size of objects to be created in this cache. Additional arguments for the cache creation (see ). See the desriptions of individual flags. The common ones are listed in the description below. Not to be called directly, use the wrapper with the same parameters. - Slab page (not individual objects) freeing delayed by a grace period - see the full description before using. Cannot be called within a interrupt, but can be interrupted. a pointer to the cache on success, NULL on failure. Create a set of caches that handle dynamic sized allocations via kmem_buckets_alloc() A prefix string which is used in /proc/slabinfo to identify this cache. The individual caches with have their sizes as the suffix. Starting offset within an allocation that may be copied to/from userspace. How many bytes, starting at useroffset, may be copied to/from userspace. A constructor for the objects, run when new allocations are made. Cannot be called within an interrupt, but can be interrupted. a pointer to the cache on success, NULL on failure. When CONFIG_SLAB_BUCKETS is not enabled, ZERO_SIZE_PTR is returned, and subsequent calls to kmem_buckets_alloc() will fall back to . (i.e. callers only need to check for NULL on failure.) Releases as many slabs as possible for a cache. To help debugging, a zero exit status indicates all slabs were released. if all slabs were released, non-zero otherwise slab object for which to find provenance information. This function uses , so that the caller is expected to have printed out whatever preamble is appropriate. The provenance information depends on the type of object and on how much debugging is enabled. For a slab-cache object, the fact that it is a slab object is printed, and, if available, the slab name, return address, and stack trace from the allocation and last free path of that object. if the pointer is to a not-yet-freed object from or , either or if the pointer is to an already-freed object, and otherwise. The memory of the object p points to is zeroed before freed. If p is , does nothing. this function zeroes the whole allocated buffer which can be a good deal bigger than the requested buffer size passed to . So be careful when using this function in performance sensitive code. Note that a single argument of kvfree_rcu() call has a slow path that triggers following by freeing a pointer. It is done before the return from the function. Therefore for any single-argument call that will result in a to a cache that is to be destroyed during module exit, it is developer’s responsibility to ensure that all such calls have returned before the call to kmem_cache_destroy(). Function calls kfree only if x is not in .rodata section.\n\nCall writepages on the mapping using the provided wbc to control the writeout. This is a non-integrity writeback helper, to start writing back folios for the indicated range. This is a mostly non-blocking flush. Not suitable for data-integrity purposes - I/O may not be started against all dirty pages. address space within which to check offset in bytes where the range starts offset in bytes where the range ends (inclusive) Find at least one page in the range supplied, usually used to check if direct writing in this range will trigger a writeback. if at least one page exists in the specified range, otherwise. offset in bytes where the range starts offset in bytes where the range ends (inclusive) Walk the list of under-writeback pages of the given address space in the given range and wait for all of them. Check error status of the address space and return it. Since the error status of the address space is cleared by this function, callers are responsible for checking the return value and handling and/or reporting the error. offset in bytes where the range starts offset in bytes where the range ends (inclusive) Walk the list of under-writeback pages of the given address space in the given range and wait for all of them. Unlike , this function does not clear error status of the address space. Use this function if callers don’t handle errors themselves. Expected call sites are system-wide / filesystem-wide data flushers: e.g. sync(2), fsfreeze(8) file pointing to address space structure to wait for offset in bytes where the range starts offset in bytes where the range ends (inclusive) Walk the list of under-writeback pages of the address space that file refers to, in the given range and wait for all of them. Check error status of the address space vs. the file->f_wb_err cursor and return it. Since the error status of the file is advanced by this function, callers are responsible for checking the return value and handling and/or reporting the error. error status of the address space vs. the file->f_wb_err cursor. Walk the list of under-writeback pages of the given address space and wait for all of them. Unlike filemap_fdatawait(), this function does not clear error status of the address space. Use this function if callers don’t handle errors themselves. Expected call sites are system-wide / filesystem-wide data flushers: e.g. sync(2), fsfreeze(8) the address_space for the pages offset in bytes where the range starts offset in bytes where the range ends (inclusive) Write out and wait upon file offsets lstart->lend, inclusive. Note that lend is inclusive (describes the last byte to be written) so that this function can be used to write to the very end-of-file (end = -1). report wb error (if any) that was previously and advance wb_err to current one on which the error is being reported When userland calls fsync (or something like nfsd does the equivalent), we want to report any writeback errors that occurred since the last fsync (or since the file was opened if there haven’t been any). Grab the wb_err from the mapping. If it matches what we have in the file, then just quickly return 0. The file is all caught up. If it doesn’t match, then take the mapping value, set the “seen” flag in it and try to swap it into place. If it works, or another task beat us to it with the new value, then update the f_wb_err and return the error portion. The error at this point must be reported via proper channels (a’la fsync, or NFS COMMIT operation, etc.). While we handle mapping->wb_err with atomic operations, the f_wb_err value is protected by the f_lock since we must ensure that it reflects the latest value swapped in for this file descriptor. offset in bytes where the range starts offset in bytes where the range ends (inclusive) Write out and wait upon file offsets lstart->lend, inclusive. Note that lend is inclusive (describes the last byte to be written) so that this function can be used to write to the very end-of-file (end = -1). After writing out and waiting on the data, we check and advance the f_wb_err cursor to the latest value, and return any errors detected there. replace a pagecache folio with a new one This function replaces a folio in the pagecache with a new one. On success it acquires the pagecache reference for the new folio and drops it for the old folio. Both the old and new folios must be locked. This function does not add the new folio to the LRU, the caller must do that. The remove + add is atomic. This function cannot fail. Unlocks the folio and wakes up any thread sleeping on the page lock. May be called from interrupt or process context. May not be called from NMI context. When all reads against a folio have completed, filesystems should call this function to let the pagecache know that no more reads are outstanding. This will unlock the folio and wake up any thread sleeping on the lock. The folio will also be marked uptodate if all reads succeeded. May be called from interrupt or process context. May not be called from NMI context. Clear the PG_private_2 bit on a folio and wake up any sleepers waiting for it. The folio reference held for PG_private_2 being set is released. This is, for example, used when a netfs folio is being written to a local disk cache, thereby allowing writes to the cache for the same folio to be serialised. Wait for PG_private_2 to be cleared on a folio. Wait for PG_private_2 to be cleared on a folio. Wait for PG_private_2 to be cleared on a folio. Wait for PG_private_2 to be cleared on a folio or until a fatal signal is received by the calling task. The folio must actually be under writeback. May be called from process or interrupt context. Get a lock on the folio, assuming we need to sleep to get it. Find the next gap in the page cache. Search the range [index, min(index + max_scan - 1, ULONG_MAX)] for the gap with the lowest index. This function may be called under the rcu_read_lock. However, this will not atomically search a snapshot of the cache at a single point in time. For example, if a gap is created at index 5, then subsequently a gap is created at index 10, page_cache_next_miss covering both indices may return 10 if called under the rcu_read_lock. The index of the gap if found, otherwise an index outside the range specified (in which case ‘return - index >= max_scan’ will be true). In the rare case of index wrap-around, 0 will be returned. Find the previous gap in the page cache. Search the range [max(index - max_scan + 1, 0), index] for the gap with the highest index. This function may be called under the rcu_read_lock. However, this will not atomically search a snapshot of the cache at a single point in time. For example, if a gap is created at index 10, then subsequently a gap is created at index 5, covering both indices may return 5 if called under the rcu_read_lock. The index of the gap if found, otherwise an index outside the range specified (in which case ‘index - return >= max_scan’ will be true). In the rare case of wrap-around, ULONG_MAX will be returned. Find and get a reference to a folio. flags modify how the folio is returned. Memory allocation flags to use if is specified. Looks up the page cache entry at mapping & index. If or are specified then the function may sleep even if the flags specified for are atomic. If this function returns a folio, it is returned with an increased refcount. The found folio or an otherwise. Search for and return a batch of folios in the mapping starting at index start and up to index end (inclusive). The folios are returned in fbatch with an elevated reference count. The number of folios which were found. We also update start to index the next folio for the traversal. works exactly like , except the returned folios are guaranteed to be contiguous. This may not return all contiguous folios if the batch gets filled up. The number of folios found. Also update start to be positioned for traversal of the next folio. The first folio may start before start; if it does, it will contain start. The final folio may extend beyond end; if it does, it will contain end. The folios have ascending indices. There may be gaps between the folios if there are indices which have no folio in the page cache. If folios are added to or removed from the page cache while this is running, they may or may not be found by this call. Only returns folios that are tagged with tag. The number of folios found. Also update start to index the next folio for traversal. Number of bytes already read by the caller. Copies data from the page cache. If the data is not currently present, uses the readahead and read_folio address_space operations to fetch it. Total number of bytes copied, including those already read by the caller. If an error happens before any bytes are copied, returns a negative error number. This is the “read_iter()” routine for all filesystems that can use the page cache directly. The IOCB_NOWAIT flag in iocb->ki_flags indicates that -EAGAIN shall be returned when no data can be read without waiting for I/O requests to complete; it doesn’t prevent readahead. The IOCB_NOIO flag in iocb->ki_flags indicates that no new I/O requests shall be made for the read or for readahead. When no data can be read, -EAGAIN shall be returned. When readahead would be triggered, a partial, possibly empty read shall be returned.\n• None number of bytes copied, even for partial reads\n• None negative error code (or 0 if IOCB_NOIO) if nothing was read Pointer to the file position to read from This function gets folios from a file’s pagecache and splices them into the pipe. Readahead will be called as necessary to fill more folios. This may be used for blockdevs also. On success, the number of bytes read will be returned and *ppos will be updated if appropriate; 0 will be returned if there is no more data to be read; -EAGAIN will be returned if the pipe had no space, and some other negative error code will be returned on error. A short read may occur if the pipe has insufficient space, we reach the end of the data or we hit a hole. struct vm_fault containing details of the fault is invoked via the vma operations vector for a mapped memory region to read in file data during a page fault. The goto’s are kind of ugly, but this streamlines the normal case of having it in the page cache, and handles the special cases reasonably without having a lot of duplicated code. vma->vm_mm->mmap_lock must be held on entry. If our return value has VM_FAULT_RETRY set, it’s because the mmap_lock may be dropped before doing I/O or by lock_folio_maybe_drop_mmap(). If our return value does not have VM_FAULT_RETRY set, the mmap_lock has not been released. We never return with VM_FAULT_RETRY and a bit from VM_FAULT_ERROR set. Read into page cache, fill it if needed. The address_space to read from. Function to perform the read, or NULL to use aops->read_folio(). Passed to filler function, may be NULL if not required. Read one page into the page cache. If it succeeds, the folio returned will contain index, but it may not be the first page of the folio. If the filler function returns an error, it will be returned to the caller. May sleep. Expects mapping->invalidate_lock to be held. An uptodate folio on success, on failure. Read into page cache, using specified allocation flags. The address_space for the folio. The index that the allocated folio will contain. The page allocator flags to use if allocating. This is the same as “read_cache_folio(mapping, index, NULL, NULL)”, but with any new memory allocations done using the specified allocation flags. The most likely error from this function is EIO, but ENOMEM is possible and so is EINTR. If ->read_folio returns another error, that will be returned to the caller. The function expects mapping->invalidate_lock to be already held. read into page cache, using specified page allocation flags. the page allocator flags to use if allocating This is the same as “read_mapping_page(mapping, index, NULL)”, but with any new page allocations done using the specified allocation flags. If the page does not get brought uptodate, return -EIO. The function expects mapping->invalidate_lock to be already held. up to date page on success, on failure. This function does all the work needed for actually writing data to a file. It does all basic checks, removes SUID from the file, updates modification times and calls proper subroutines depending on whether we do direct IO or a standard buffered write. It expects i_rwsem to be grabbed unless we work on a block device or similar object which does not need locking at all. This function does not take care of syncing data in case of O_SYNC write. A caller has to handle it. This is mainly due to the fact that we want to avoid syncing under i_rwsem.\n• None number of bytes written, even for truncated writes\n• None negative error code if no data has been written at all This is a wrapper around to be used by most filesystems. It takes care of syncing the file in case of O_SYNC file and acquires i_rwsem as needed.\n• None negative error code if no data has been written at all of failed for a synchronous write\n• None number of bytes written, even for truncated writes The folio which the kernel is trying to free. The address_space is trying to release any data attached to a folio (presumably at folio->private). This will also be called if the private_2 flag is set on a page, indicating that the folio has other metadata associated with it. The gfp argument specifies whether I/O may be performed to release this page (__GFP_IO), and whether the call may block (__GFP_RECLAIM & __GFP_FS). if the release was successful, otherwise . Set to write back rather than simply invalidate. Last byte in range (inclusive), or LLONG_MAX for everything from start onwards. Invalidate all the folios on an inode that contribute to the specified range, possibly writing them back first. Whilst the operation is undertaken, the invalidate lock is held to prevent new folios from being installed. Readahead is used to read content into the page cache before it is explicitly requested by the application. Readahead only ever attempts to read folios that are not yet in the page cache. If a folio is present but not up-to-date, readahead will not try to read it. In that case a simple ->read_folio() will be requested. Readahead is triggered when an application read request (whether a system call or a page fault) finds that the requested folio is not in the page cache, or that it is in the page cache and has the readahead flag set. This flag indicates that the folio was read as part of a previous readahead request and now that it has been accessed, it is time for the next readahead. Each readahead request is partly synchronous read, and partly async readahead. This is reflected in the which contains ->size being the total number of pages, and ->async_size which is the number of pages in the async section. The readahead flag will be set on the first folio in this async section to trigger a subsequent readahead. Once a series of sequential reads has been established, there should be no need for a synchronous component and all readahead request will be fully asynchronous. When either of the triggers causes a readahead, three numbers need to be determined: the start of the region to read, the size of the region, and the size of the async tail. The start of the region is simply the first page address at or after the accessed address, which is not currently populated in the page cache. This is found with a simple search in the page cache. The size of the async tail is determined by subtracting the size that was explicitly requested from the determined request size, unless this would be less than zero - then zero is used. NOTE THIS CALCULATION IS WRONG WHEN THE START OF THE REGION IS NOT THE ACCESSED PAGE. ALSO THIS CALCULATION IS NOT USED CONSISTENTLY. The size of the region is normally determined from the size of the previous readahead which loaded the preceding pages. This may be discovered from the for simple sequential reads, or from examining the state of the page cache when multiple sequential reads are interleaved. Specifically: where the readahead was triggered by the readahead flag, the size of the previous readahead is assumed to be the number of pages from the triggering page to the start of the new readahead. In these cases, the size of the previous readahead is scaled, often doubled, for the new readahead, though see get_next_ra_size() for details. If the size of the previous read cannot be determined, the number of preceding pages in the page cache is used to estimate the size of a previous read. This estimate could easily be misled by random reads being coincidentally adjacent, so it is ignored unless it is larger than the current request, and it is not scaled up, unless it is at the start of file. In general readahead is accelerated at the start of the file, as reads from there are often sequential. There are other minor adjustments to the readahead size in various special cases and these are best discovered by reading the code. The above calculation, based on the previous readahead size, determines the size of the readahead, to which any requested read size may be added. Readahead requests are sent to the filesystem using the ->readahead() address space operation, for which is a canonical implementation. ->readahead() should normally initiate reads on all folios, but may fail to read any or all folios without causing an I/O error. The page cache reading code will issue a ->read_folio() request for any folio which ->readahead() did not read, and only an error from this will be final. ->readahead() will generally call repeatedly to get each folio from those prepared for readahead. It may fail to read a folio by:\n• None not calling sufficiently many times, effectively ignoring some folios, as might be appropriate if the path to storage is congested.\n• None failing to actually submit a read request for a given folio, possibly due to insufficient resources, or\n• None getting an error during subsequent processing of a request. In the last two cases, the folio should be unlocked by the filesystem to indicate that the read attempt has failed. In the first case the folio will be unlocked by the VFS. Those folios not in the final of the request should be considered to be important and ->readahead() should not fail them due to congestion or temporary resource unavailability, but should wait for necessary resources (e.g. memory or indexing information) to become available. Folios in the final may be considered less urgent and failure to read them is more acceptable. In this case it is best to use filemap_remove_folio() to remove the folios from the page cache as is automatically done for folios that were not fetched with . This will allow a subsequent synchronous readahead request to try them again. If they are left in the page cache, then they will be read individually using ->read_folio() which may be less efficient. The number of pages to read. Where to start the next readahead. This function is for filesystems to call when they want to start readahead beyond a file’s stated i_size. This is almost certainly not the function you want to call. Use or instead. File is referenced by caller. Mutexes may be held by caller. May sleep, but will not reenter filesystem to reclaim memory. The request to be expanded The revised size of the request Attempt to expand a readahead request outwards from the current size to the specified size by inserting locked pages before and after the current window to increase the size to the new window. This may involve the insertion of THPs, in which case the window may get expanded even beyond what was requested. The algorithm will stop if it encounters a conflicting page already in the pagecache and leave a smaller expansion than requested. The caller must check for this by examining the revised ractl object for a different expansion than was requested. Processes which are dirtying memory should call in here once for each page which was newly dirtied. The function will periodically check the system’s dirty state and will initiate writeback if needed. If flags contains BDP_ASYNC, it may return -EAGAIN to indicate that memory is out of balance and the caller must wait for I/O to complete. Otherwise, it will return 0 to indicate that either memory was already in balance, or it was able to sleep until the amount of dirty memory returned to balance. Processes which are dirtying memory should call in here once for each page which was newly dirtied. The function will periodically check the system’s dirty state and will initiate writeback if needed. Once we’re over the dirty memory limit we decrease the ratelimiting by a lot, to prevent individual processes from overshooting the limit by (ratelimit_pages) each. tag pages to be written by writeback This function scans the page range from start to end (inclusive) and tags all pages that have DIRTY tag set with a special TOWRITE tag. The caller can then use the TOWRITE tag to identify pages eligible for writeback. This mechanism is used to avoid livelocking of writeback by a process steadily creating new dirty pages in the file (thus it is important for this function to be quick so that it can tag pages faster than a dirtying process can create them). in-out pointer for writeback errors (see below) This function returns the next folio for the writeback operation described by wbc on mapping and should be called in a while loop in the ->writepages implementation. To start the writeback operation, is passed in the folio argument, and for every subsequent iteration the folio returned previously should be passed back in. If there was an error in the per-folio writeback inside the loop, error should be set to the error value. Once the writeback described in wbc has finished, this function will return and if there was an error in any iteration restore it to error. callers should not manually break out of the loop using break or goto but must keep calling until it returns . the folio to write or if the loop is done. walk the list of dirty pages of the given address space and write all of them. subtract the number of written pages from *wbc->nr_to_write please use instead. Mark a folio dirty for filesystems which do not use buffer_heads. Folio to be marked as dirty. Filesystems which do not use buffer heads should call this function from their dirty_folio address space operation. It ignores the contents of folio_get_private(), so if the filesystem marks individual blocks as dirty, the filesystem should handle that itself. This is also sometimes used by filesystems which use buffer_heads when a single buffer is being dirtied: we want to set the folio dirty in that case, but not all the buffers. This is a “bottom-up” dirtying, whereas is a “top-down” dirtying. The caller must ensure this doesn’t race with truncation. Most will simply hold the folio lock, but e.g. zap_pte_range() calls with the folio mapped and the pte lock held, which also locks out truncation. When a writepage implementation decides that it doesn’t want to write folio for some reason, it should call this function, unlock folio and return 0. True if we redirtied the folio. False if someone else dirtied it first. The folio may not be truncated while this function is running. Holding the folio lock is sufficient to prevent truncation, but some callers cannot acquire a sleeping lock. These callers instead hold the page table lock for a page table which contains at least one page in this folio. Truncation will block on the page table lock as it unmaps pages before removing the folio from its mapping. True if the folio was newly dirtied, false if it was already dirty. If the folio is currently being written back to storage, wait for the I/O to complete. Sleeps. Must be called in process context and with no spinlocks held. Caller should hold a reference on the folio. If the folio is not locked, writeback may start again after writeback has finished. If the folio is currently being written back to storage, wait for the I/O to complete or a fatal signal to arrive. Sleeps. Must be called in process context and with no spinlocks held. Caller should hold a reference on the folio. If the folio is not locked, writeback may start again after writeback has finished. 0 on success, -EINTR if we get a fatal signal while waiting. wait for writeback to finish, if necessary. This function determines if the given folio is related to a backing device that requires folio contents to be held stable during writeback. If so, then it will wait for any pending writeback to complete. Sleeps. Must be called in process context and with no spinlocks held. Caller should hold a reference on the folio. If the folio is not locked, writeback may start again after writeback has finished. Invalidate part or all of a folio. The folio which is affected. start of the range to invalidate length of the range to invalidate is called when all or part of the folio has become invalidated by a truncate operation. does not have to release all buffers, but it must ensure that no dirty buffer is left outside offset and that no I/O is underway against any of the blocks which are outside the truncation point. Because the caller is about to free (and possibly reuse) those blocks on-disk. truncate range of pages specified by start & end byte offsets offset from which to truncate offset to which to truncate (inclusive) Truncate the page cache, removing the pages that are between specified offsets (and zeroing out partial pages if lstart or lend + 1 is not page aligned). Truncate takes two passes - the first pass is nonblocking. It will not block on page locks and it will not block on writeback. The second pass will wait. This is to prevent as much IO as possible in the affected region. The first pass will remove most pages, so the search cost of the second pass is low. We pass down the cache-hot hint to the page freeing code. Even if the mapping is large, it is probably the case that the final pages are the most recently touched, and freeing happens in ascending file offset order. Note that since ->invalidate_folio() accepts range to invalidate truncate_inode_pages_range is able to handle cases where lend + 1 is not page aligned properly. truncate all the pages from an offset offset from which to truncate Called under (and serialised by) inode->i_rwsem and mapping->invalidate_lock. When this function returns, there can be a page in the process of deletion (inside __filemap_remove_folio()) in the specified range. Thus mapping->nrpages can be non-zero when this function returns even after truncation of the whole mapping. Called under (and serialized by) inode->i_rwsem. Filesystems have to use this in the .evict_inode path to inform the VM that this is the final truncate and the inode is going away. Invalidate all clean, unlocked cache of one inode the address_space which holds the cache to invalidate the offset ‘from’ which to invalidate the offset ‘to’ which to invalidate (inclusive) This function removes pages that are clean, unmapped and unlocked, as well as shadow entries. It will not block on IO activity. If you want to remove all the pages of one inode, regardless of their use and writeback state, use . The number of indices that had their contents invalidated remove range of pages from an address_space the page offset ‘from’ which to invalidate the page offset ‘to’ which to invalidate (inclusive) Any pages which are found to be mapped into pagetables are unmapped prior to invalidation. -EBUSY if any pages could not be invalidated. remove all pages from an address_space Any pages which are found to be mapped into pagetables are unmapped prior to invalidation. -EBUSY if any pages could not be invalidated. unmap and remove pagecache that has been truncated inode’s new i_size must already be written before truncate_pagecache is called. This function should typically be called before the filesystem releases resources associated with the freed range (eg. deallocates blocks). This way, pagecache will always stay logically coherent with on-disk format, and the filesystem would not have to deal with situations such as writepage being called for a page that has already had its underlying blocks deallocated. update inode and pagecache for a new file size truncate_setsize updates i_size and performs pagecache truncation (if necessary) to newsize. It will be typically be called from the filesystem’s setattr function when ATTR_SIZE is passed in. Must be called with a lock serializing truncates and writes (generally i_rwsem but e.g. xfs uses a different lock) and before all filesystem specific block truncation has been performed. inode for which i_size was extended Handle extension of inode size either caused by extending truncate or by write starting after current i_size. We mark the page straddling current i_size RO so that page_mkwrite() is called on the first write access to the page. The filesystem will update its per-block information before user writes to the page via mmap after the i_size has been changed. The function must be called after i_size is updated so that page fault coming after we unlock the folio will already see the new i_size. The function must be called while we still hold i_rwsem - this not only makes sure i_size is stable but also that userspace cannot observe new i_size value before we are prepared to store mmap writes at new inode size. unmap and remove pagecache that is hole-punched offset of last byte of hole This function should typically be called before the filesystem releases resources associated with the freed range (eg. deallocates blocks). This way, pagecache will always stay logically coherent with on-disk format, and the filesystem would not have to deal with situations such as writepage being called for a page that has already had its underlying blocks deallocated. mapping in which to set writeback error error to be set in mapping When writeback fails in some way, we must record that error so that userspace can be informed when fsync and the like are called. We endeavor to report errors on any file that was open at the time of the error. Some internal callers also need to know when writeback errors have occurred. When a writeback error occurs, most filesystems will want to call filemap_set_wb_err to record the error in the mapping so that it will be automatically reported whenever fsync is called on the file. has an error occurred since the mark was sampled? Grab the errseq_t value from the mapping, and see if it has changed “since” the given value was sampled. If it has then report the latest error set, otherwise return 0. sample the current errseq_t to test for later errors Writeback errors are always reported relative to a particular sample point in the past. This function provides those sample points. sample the current errseq_t to test for later errors Grab the most current superblock-level errseq_t value for the given . the mapping in which an error should be set the error to set in the mapping When writeback fails in some way, we must record that error so that userspace can be informed when fsync and the like are called. We endeavor to report errors on any file that was open at the time of the error. Some internal callers also need to know when writeback errors have occurred. When a writeback error occurs, most filesystems will want to call mapping_set_error to record the error in the mapping so that it can be reported when the application calls fsync(2). The address space of the file. The filesystem should call this function in its inode constructor to indicate that the VFS can use large folios to cache the contents of the file. This should not be called while the inode is active as it is non-atomic. The index of a folio must be naturally aligned. If you are adding a new folio to the page cache and need to know what index to give it, call this function. Find the file mapping this folio belongs to. For folios which are in the page cache, return the mapping that this page belongs to. Anonymous folios return NULL, even if they’re in the swap cache. Other kinds of folio also return NULL. This is ONLY used by architecture cache flushing code. If you aren’t writing cache flushing code, you want either or folio_file_mapping(). Get the host inode for this folio. For folios which are in the page cache, return the inode that this folio belongs to. Do not call this for folios which aren’t in the page cache. Attaching private data to a folio increments the page’s reference count. The data must be detached before the folio will be freed. Folio to change the data on. Data to set on the folio. Change the private data attached to a folio and return the old data. The page must previously have had data attached and the data must be detached before the folio will be freed. Data that was previously attached to the folio. Removes the data that was previously attached to the folio and decrements the refcount on the page. Data that was attached to the folio. Flags for getting folios from the page cache. Most users of the page cache will not need to use these flags; there are convenience functions such as and . For users which need more control over exactly what is done with the folios, these flags to are available.\n• None - The folio will be marked accessed.\n• None - If no folio is present then a new folio is allocated, added to the page cache and the VM’s LRU list. The folio is returned locked.\n• None - The caller wants to do its own locking dance if the folio is already in cache. If the folio was allocated, unlock it before returning so the caller can do the same dance.\n• None - The folio will be written to by the caller.\n• None - __GFP_FS will get cleared in gfp.\n• None - Wait for the folio to be stable (finished writeback)\n• None - The flags to use in a filesystem write_begin() implementation. The suggested size of the folio to create. The caller of can use this to suggest a preferred size for the folio that is created. If there is already a folio at the index, it will be returned, no matter what its size. If a folio is freshly created, it may be of a different size than requested due to alignment constraints, memory pressure, or the presence of other folios at nearby indices. Looks up the page cache entry at mapping & index. If a folio is present, it is returned with an increased refcount. A folio or ERR_PTR(-ENOENT) if there is no folio in the cache for this index. Will not return a shadow, swap or DAX entry. Looks up the page cache entry at mapping & index. If a folio is present, it is returned locked with an increased refcount. A folio or ERR_PTR(-ENOENT) if there is no folio in the cache for this index. Will not return a shadow, swap or DAX entry. Looks up the page cache entry at mapping & index. If no folio is found, a new folio is created. The folio is locked, marked as accessed, and returned. A found or created folio. ERR_PTR(-ENOMEM) if no folio is found and failed to create a folio. Looks up the page cache slot at mapping & offset. If there is a page cache page, it is returned with an increased refcount. Looks up the page cache entry at mapping & index. If there is a page cache page, it is returned locked and with an increased refcount. A struct page or if there is no page in the cache for this index. the page’s index into the mapping Looks up the page cache slot at mapping & offset. If there is a page cache page, it is returned locked and with an increased refcount. If the page is not present, a new page is allocated using gfp_mask and added to the page cache and the VM’s LRU list. The page is returned locked and with an increased refcount. may sleep, even if gfp_flags specifies an atomic allocation! returns locked page at given index in given cache Same as grab_cache_page(), but do not wait if the page is unavailable. This is intended for speculative data generators, where the data can be regenerated if the page couldn’t be grabbed. This routine should be safe to call while holding the lock for another page. Clear __GFP_FS when allocating the page to avoid recursion into the fs and deadlock against the caller’s locked page. For a folio which is either in the page cache or the swap cache, return its index within the address_space it belongs to. If you know the page is definitely in the page cache, you can look at the folio’s index directly. The index (offset in units of pages) of a folio in its file. Get the index of the next folio. The index of the folio which follows this folio in the file. The page for a particular index. The folio which contains this index. The index we want to look up. Sometimes after looking up a folio in the page cache, we need to obtain the specific page for an index (eg a page fault). The page containing the file data for this index. Does this folio contain this index? The page index within the file. The caller should have the page locked in order to prevent (eg) shmem from moving the page between the page cache and swap cache and changing its index in the middle of the operation. Calculate the logical page offset of this page. The folio containing this page. The page which we need the offset of. For file pages, this is the offset from the beginning of the file in units of PAGE_SIZE. For anonymous pages, this is the offset from the beginning of the anon_vma in units of PAGE_SIZE. This will return nonsense for KSM pages. Caller must have a reference on the folio or otherwise prevent it from being split or freed. The offset in units of PAGE_SIZE. Returns the byte position of this folio in its file. The folio to attempt to lock. Sometimes it is undesirable to wait for a folio to be unlocked (eg when the locks are being taken in the wrong order, or if making progress through a batch of folios is more important than processing them in order). Usually is the correct function to call. Whether the lock was successfully acquired. The folio lock protects against many things, probably more than it should. It is primarily held while a folio is being brought uptodate, either from its backing file or from swap. It is also held while a folio is being truncated from its address_space, so holding the lock is sufficient to keep folio->mapping stable. The folio lock is also held while write() is modifying the page to provide POSIX atomicity guarantees (as long as the write does not cross a page boundary). Other modifications to the data in the folio do not hold the folio lock and can race with writes, eg DMA and stores to mapped pages. May sleep. If you need to acquire the locks of two or more folios, they must be in order of ascending index, if they are in the same address_space. If they are in different address_spaces, acquire the lock of the folio which belongs to the address_space which has the lowest address in memory first. Lock the folio containing this page. See for a description of what the lock protects. This is a legacy function and new code should probably use instead. May sleep. Pages in the same folio share a lock, so do not attempt to lock two pages which share a folio. Attempts to lock the folio, like , except that the sleep to acquire the lock is interruptible by a fatal signal. 0 if the lock was acquired; -EINTR if a fatal signal was received. address space within which to check offset in bytes where the range starts offset in bytes where the range ends (inclusive) Find at least one page in the range supplied, usually used to check if direct writing in this range will trigger a writeback. Used by O_DIRECT read/write with IOCB_NOWAIT, to see if the caller needs to do before proceeding. if the caller should do before doing O_DIRECT to a page in this range, otherwise. The file, used primarily by network filesystems for authentication. May be NULL if invoked internally by the filesystem. A readahead request is for consecutive pages. Filesystems which implement the ->readahead method should call or in a loop and attempt to start I/O against each page in the request. Most of the fields in this struct are private and should be accessed by the functions below. address_space which holds the pagecache and I/O vectors Used by the filesystem for authentication. Index of first page to be read. Total number of pages being read by the caller. should be called when a cache miss happened: it will submit the read. The readahead logic may decide to piggyback more pages onto the read request if access patterns suggest it will improve performance. address_space which holds the pagecache and I/O vectors Used by the filesystem for authentication. The folio which triggered the readahead call. Total number of pages being read by the caller. should be called when a page is used which is marked as PageReadahead; this is a marker to suggest that the application has used up enough of the readahead window that we should start pulling in more pages. Get the next page to read. The page is locked and has an elevated refcount. The caller should decreases the refcount once the page has been submitted for I/O and unlock the page once all I/O to that page has completed. A pointer to the next page, or if we are done. Get the next folio to read. The folio is locked. The caller should unlock the folio once all I/O to that folio has completed. A pointer to the next folio, or if we are done. Get a batch of pages to read. An array of pointers to struct page. The pages are locked and have an elevated refcount. The caller should decreases the refcount once the page has been submitted for I/O and unlock the page once all I/O to that page has completed. The number of pages placed in the array. 0 indicates the request is complete. The byte offset into the file of this readahead request. The number of bytes in this readahead request. The index of the first page in this readahead request. The number of pages in this readahead request. The number of bytes in the current batch. the inode to check the folio against the number of bytes in the folio up to EOF, or -EFAULT if the folio was truncated. How many blocks fit in this folio. The inode which contains the blocks. If the block size is larger than the size of this folio, return zero. The caller should hold a refcount on the folio to prevent it from being split. The number of filesystem blocks covered by this folio.\n\nThis function only unmaps ptes assigned to VM_PFNMAP vmas. The entire address range must be fully contained within the vma. in: number of pages to map. out: number of pages that were not mapped. (0 means all pages were successfully mapped). In case of error, we may have mapped a subset of the provided pages. It is the caller’s responsibility to account for this case. The same restrictions apply as in . This allows drivers to insert individual pages they’ve allocated into a user vma. The zeropage is supported in some VMAs, see vm_mixed_zeropage_allowed(). The page has to be a nice clean _individual_ kernel allocation. If you allocate a compound page, you need to have marked it as such (__GFP_COMP), or manually just split the page up yourself (see split_page()). NOTE! Traditionally this was done with “ ” which took an arbitrary page protection parameter. This doesn’t allow that. Your vma protection will have to be set up correctly, which means that if you want a shared writable mapping, you’d better ask for a shared writable mapping! The page does not need to be reserved. Usually this function is called from f_op->mmap() handler under mm->mmap_lock write-lock, so it can change vma->vm_flags. Caller must set VM_MIXEDMAP on vma if it wants to call this function from other places, for example from page-fault handler. maps range of kernel pages starts with non zero offset Maps an object consisting of num pages, catering for the user’s requested vm_pgoff If we fail to insert any page into the vma, the function will return immediately leaving any previously inserted pages present. Callers from the mmap handler may immediately return the error as their caller will destroy the vma, removing any successfully inserted pages. Other callers should make their own arrangements for calling unmap_region(). 0 on success and error code otherwise. map range of kernel pages starts with zero offset Similar to , except that it explicitly sets the offset to 0. This function is intended for the drivers that did not consider vm_pgoff. 0 on success and error code otherwise. insert single pfn into user vma with specified pgprot This is exactly like , except that it allows drivers to override pgprot on a per-page basis. This only makes sense for IO mappings, and it makes no sense for COW mappings. In general, using multiple vmas is preferable; vmf_insert_pfn_prot should only be used if using multiple VMAs is impractical. pgprot typically only differs from vma->vm_page_prot when drivers set caching- and encryption bits different than those of vma->vm_page_prot, because the caching- or encryption mode may not be known at mmap() time. This is ok as long as vma->vm_page_prot is not used by the core vm to set caching and encryption bits for those vmas (except for COW pages). This is ensured by core vm only modifying these page table entries using functions that don’t touch caching- or encryption bits, using pte_modify() if needed. (See for example mprotect()). Also when new page-table entries are created, this is only done using the fault() callback, and never using the value of vma->vm_page_prot, except for page-table entries that point to anonymous pages as the result of COW. Similar to vm_insert_page, this allows drivers to insert individual pages they’ve allocated into a user vma. Same comments apply. This function should only be called from a vm_ops->fault handler, and in that case the handler should return the result of this function. As this is called only for pages that do not currently exist, we do not need to flush old virtual caches or the TLB. this is only safe if the mm semaphore is held when called. start of the physical memory to be mapped This is a simplified io_remap_pfn_range() for common driver use. The driver just needs to give us the physical memory range to be mapped, we’ll figure out the rest from the vma information. NOTE! Some drivers might want to tweak vma->vm_page_prot first to get whatever write-combining details or similar. The address space containing pages to be unmapped. Index of first page to be unmapped. Number of pages to be unmapped. 0 to unmap to end of file. Whether to unmap even private COWed pages. Unmap the pages in this address space from any userspace process which has them mmaped. Generally, you want to remove COWed pages as well when a file is being truncated, but not when invalidating pages from the page cache. unmap the portion of all mmaps in the specified address_space corresponding to the specified byte range in the underlying file. the address space containing mmaps to be unmapped. byte in first page to unmap, relative to the start of the underlying file. This will be rounded down to a PAGE_SIZE boundary. Note that this is different from , which must keep the partial page. In contrast, we must get rid of partial pages. size of prospective hole in bytes. This will be rounded up to a PAGE_SIZE boundary. A holelen of zero truncates to the end of the file. 1 when truncating a file, unmap even private COWed pages; but 0 when invalidating pagecache, don’t throw away private data. Look up a pfn mapping at a user virtual address The caller needs to setup args->vma and args->address to point to the virtual address as the target of such lookup. On a successful return, the results will be put into other output fields. After the caller finished using the fields, the caller must invoke another to proper releases the locks and resources of such look up request. During the and end() calls, the results in args will be valid as proper locks will be held. After the end() is called, all the fields in follow_pfnmap_args will be invalid to be further accessed. Further use of such information after end() may require proper synchronizations by the caller with page table updates, otherwise it can create a security bug. If the PTE maps a refcounted page, callers are responsible to protect against invalidation with MMU notifiers; otherwise access to the PFN at a later point in time can trigger use-after-free. Only IO mappings and raw PFN mappings are allowed. The mmap semaphore should be taken for read, and the mmap semaphore cannot be released before the end() is invoked. This function must not be used to modify PTE content. zero on success, negative otherwise. Must be used in pair of . See the function above for more information. set to FOLL_WRITE when writing, otherwise reading This is a generic implementation for for an iomem mapping. This callback is used by access_process_vm() when the vma is not page based. the task of the target address space The caller must hold a reference on mm. number of bytes copied from addr (source) to buf (destination); not including the trailing NUL. Always guaranteed to leave NUL-terminated buffer. On any error, return -EFAULT. Return the requested group of flags for the pageblock_nr_pages block of pages The page within the block of interest mask of bits that the caller is interested in Set the requested group of flags for a pageblock_nr_pages block of pages The page within the block of interest mask of bits that the caller is interested in migratetype to set on the pageblock This is similar to move_freepages_block(), but handles the special case encountered in page isolation, where the block of interest might be part of a larger buddy spanning multiple pageblocks. Unlike the regular page allocator path, which moves pages while stealing buddies off the freelist, page isolation is interested in arbitrary pfn ranges that may have overlapping buddies on both ends. This function handles that. Straddling buddies are split into individual pageblocks. Only the block of interest is moved. Returns if pages could be moved, otherwise. Return a now-isolated page back where we got it This function is meant to return a page pulled from the free lists via __isolate_free_page back to the free lists they were pulled from. The order of the allocation. This function can free multi-page allocations that are not compound pages. It does not check that the order passed in matches that of the allocation, so it is easy to leak memory. Freeing more memory than was allocated will probably emit a warning. If the last reference to this page is speculative, it will be released by put_page() which only frees the first page of a non-compound allocation. To prevent the remaining pages from being leaked, we free the subsequent pages here. If you want to use the page’s reference count to decide when to free the allocation, you should allocate a compound page, and use put_page() instead of __free_pages(). May be called in interrupt context or while holding a normal spinlock, but not in NMI context or while holding a raw spinlock. the number of bytes to allocate GFP flags for the allocation, must not contain __GFP_COMP This function is similar to , except that it allocates the minimum number of pages to satisfy the request. can only allocate memory in power-of-two pages. This function is also limited by MAX_PAGE_ORDER. Memory allocated by this function must be released by . pointer to the allocated area or in case of error. allocate an exact number of physically-contiguous pages on a node. the preferred node ID where memory should be allocated the number of bytes to allocate GFP flags for the allocation, must not contain __GFP_COMP Like , but try to allocate on node nid first before falling back. pointer to the allocated area or in case of error. the value returned by alloc_pages_exact. size of allocation, same value as passed to . Release the memory allocated by a previous call to alloc_pages_exact. The zone index of the highest zone counts the number of pages which are beyond the high watermark within all zones at or below a given zone index. For each zone, the number of pages is calculated as: counts the number of pages which are beyond the high watermark within ZONE_DMA and ZONE_NORMAL. number of pages beyond high watermark within ZONE_DMA and ZONE_NORMAL. find the next node that should appear in a given node’s fallback list nodemask_t of already used nodes We use a number of factors to determine which is the next node that should appear on a given node’s fallback list. The node should not have appeared already in node’s fallback list, and it should be the next closest node according to the distance array (which contains arbitrary distance values from each node to each node in the system), and should also prefer nodes with no CPUs, since presumably they’ll have very little allocation pressure on them otherwise. node id of the found node or if no node is found. called when min_free_kbytes changes or when memory is hot-{added|removed} Ensures that the watermark[min,low,high] values for each zone are set correctly with respect to min_free_kbytes.\n• None tries to allocate given range of pages migratetype of the underlying pageblocks (either #MIGRATE_MOVABLE or #MIGRATE_CMA). All pageblocks in range must have the same migratetype and it must be either of the two. GFP mask. Node/zone/placement hints are ignored; only some action and reclaim modifiers are supported. Reclaim modifiers control allocation behavior during compaction/migration/reclaim. The PFN range does not have to be pageblock aligned. The PFN range must belong to a single zone. The first thing this routine does is attempt to MIGRATE_ISOLATE all pageblocks in the range. Once isolated, the pageblocks should not be modified by others. zero on success or negative error code. On success all pages which PFN is in [start, end) are allocated for the caller and need to be freed with free_contig_range().\n• None tries to find and allocate contiguous range of pages GFP mask. Node/zone/placement hints limit the search; only some action and reclaim modifiers are supported. Reclaim modifiers control allocation behavior during compaction/migration/reclaim. Mask for other possible nodes This routine is a wrapper around . It scans over zones on an applicable zonelist to find a contiguous pfn range which can then be tried for allocation with . This routine is intended for allocation requests which can not be fulfilled with the buddy allocator. The allocated memory is always aligned to a page boundary. If nr_pages is a power of two, then allocated range is also guaranteed to be aligned to same nr_pages (e.g. 1GB request would be aligned to 1GB). Allocated pages can be freed with free_contig_range() or by manually calling __free_page() on each allocated page. pointer to contiguous pages on success, or NULL if not successful. Allocates pages of a given order from the given node. This is safe to call from any context (from atomic, NMI, and also reentrant allocator -> tracepoint -> try_alloc_pages_noprof). Allocation is best effort and to be expected to fail easily so nobody should rely on the success. Failures are not reported via warn_alloc(). See always fail conditions below. Lookup the closest node by distance if nid is not in state. this node if it is in state, otherwise the closest node by distance Find the node in mask at the nearest distance from node. a valid node ID to start the search from. This function iterates over all nodes in mask and calculates the distance from the starting node, then it returns the node ID that is the closest to node, or MAX_NUMNODES if no node is found. Note that node must be a valid node ID usable with node_distance(), providing an invalid node ID (e.g., NUMA_NO_NODE) may result in crashes or unexpected behavior. Preferred node (usually numa_node_id() but mpol may override it). The page on success or NULL if allocation fails. Virtual address of the allocation. Must be inside vma. Allocate a folio for a specific address in vma, using the appropriate NUMA policy. The caller must hold the mmap_lock of the mm_struct of the VMA to prevent it from going away. Should be used for all allocations for folios that will be mapped into user space, excepting hugetlbfs, and excepting where direct use of folio_alloc_mpol() is more appropriate. The folio on success or NULL if allocation fails. Power of two of number of pages to allocate. Allocate 1 << order contiguous pages. The physical address of the first page is naturally aligned (eg an order-3 allocation will be aligned to a multiple of 8 * PAGE_SIZE bytes). The NUMA policy of the current process is honoured when in process context. Can be called from any context, providing the appropriate GFP flags are used. The page on success or NULL if allocation fails. check whether current folio node is valid in policy virtual address in vma for shared policy lookup and interleave policy Lookup current policy node id for vma,addr and “compare to” folio’s node id. Policy determination “mimics” alloc_page_vma(). Called from fault path where we know the vma and faulting address. NUMA_NO_NODE if the page is in a node that is valid for this policy, or a suitable node ID to allocate a replacement folio from. Install non-NULL mpol in inode’s shared policy rb-tree. On entry, the current task has a reference on a non-NULL mpol. This must be released on exit. This is called at get_inode() calls and we can use GFP_KERNEL. pointer to mempolicy to be formatted Convert pol into a string. If buffer is too short, truncate the string. Recommend a maxlen of at least 51 for the longest mode, “weighted interleave”, plus the longest flag flags, “relative|balancing”, and to display at least a few node ids. Least Recently Used list; tracks how recently this folio was used. Number of times this folio has been pinned by mlock(). The file this page belongs to, or refers to the anon_vma for anonymous memory. Offset within the file, in units of pages. For anonymous memory, this is the index from the beginning of the mmap. number of DAX mappings that reference this folio. See dax_associate_entry. Used for swp_entry_t if folio_test_swapcache(). Do not access this member directly. Use to find out how many times this folio is mapped by userspace. Do not access this member directly. Use to find how many references there are to this folio. IDs of last CPU and last process that accessed the folio. Do not use directly, call . Do not use outside of rmap and debug code. Do not use directly, call folio_entire_mapcount(). Do not use directly, call . Do not use outside of rmap code. Do not use outside of rmap code. Do not use outside of rmap code. Do not use directly, call . Folios to be split under memory pressure. Do not use directly, call folio_entire_mapcount(). Do not use directly, call . Do not use directly, use accessor in hugetlb.h. Do not use directly, use accessor in hugetlb_cgroup.h. Do not use directly, use accessor in hugetlb_cgroup.h. Do not use directly, call raw_hwp_list_head(). A folio is a physically, virtually and logically contiguous set of bytes. It is a power-of-two in size, and it is aligned to that same power-of-two. It is at least as large as . If it is in the page cache, it is at a file offset which is a multiple of that power-of-two. It may be mapped into userspace at an address which is at an arbitrary page offset, but its kernel virtual address is aligned to its size. Same as page flags. Powerpc only. List of used page tables. Used for s390 gmap shadow pages (which are not linked into the user page tables) and x86 pgds. Protected by ptdesc->ptl, used for THPs. Same as page->page_type. Unused for page tables. This struct overlays struct page for now. Do not modify without a good understanding of the issues. Page fault handlers return a bitmask of these values to tell the core VM what happened when handling the fault. Used to decide whether a process gets delivered SIGBUS or just gets major/minor fault counters bumped up. ->fault did not modify page tables and needs fsync() to complete (for synchronous page faults in DAX) Allow to retry the fault if blocked. The fault task is in SIGKILL killable region. The fault has been tried once. The fault is not for current task/mm. The fault was during an instruction fetch. The fault can be interrupted by non-fatal signals. The fault is an unsharing request to break COW in a COW mapping, making sure that an exclusive anon page is mapped after the fault. whether the fault has vmf->orig_pte cached. We should only access orig_pte if this flag set. The fault is handled under VMA lock. About FAULT_FLAG_ALLOW_RETRY and FAULT_FLAG_TRIED: we can specify whether we would allow page faults to retry by specifying these two fault flags correctly. Currently there can be three legal combinations:\n• None ALLOW_RETRY and !TRIED: this means the page fault allows retry, and\n• None ALLOW_RETRY and TRIED: this means the page fault allows retry, and we’ve already tried at least once\n• None !ALLOW_RETRY and !TRIED: this means the page fault does not allow retry The unlisted combination (!ALLOW_RETRY && TRIED) is illegal and should never be used. Note that page faults can be allowed to retry for multiple times, in which case we’ll have an initial fault with flags (a) then later on continuous faults with flags (b). We should always try to detect pending signals before a retry to make sure the continuous page faults can still be interrupted if necessary. The combination FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE is illegal. FAULT_FLAG_UNSHARE is ignored and treated like an ordinary read fault when applied to mappings that are not COW mappings. Should the folio be on a file LRU or anon LRU? We would like to get this info without a page flag, but the state needs to survive until the folio is last deleted from the LRU, which could be as far down as __page_cache_release. An integer (not a boolean!) used to sort a folio onto the right LRU list and to account folios correctly. 1 if folio is a regular filesystem backed page cache folio or a lazily freed anonymous folio (e.g. via MADV_FREE). 0 if folio is a normal anonymous folio, a tmpfs folio or otherwise ram or swap backed folio. The folio that was on lru and now has a zero reference. Which LRU list should a folio be on? The LRU list a folio should be on, as an index into the array of LRU lists. Every page is part of a folio. This function cannot be called on a NULL pointer. No reference, nor lock is required on page. If the caller does not hold a reference, this call may race with a folio split, so it should re-check the folio still contains this page after gaining a reference on the folio. The folio which contains this page. n is relative to the start of the folio. This function does not check that the page number lies within folio; the caller is presumed to have a reference to the page. Bits set in this word will be changed. This must only be used for flags which are changed with the folio lock held. For example, it is unsafe to use for PG_dirty as that can be set without the folio lock held. It can also only be used on flags which are in the range 0-6 as some of the implementations only affect those bits. Whether there are tasks waiting on the folio. Is this folio up to date? The uptodate flag is set on a folio when every byte in the folio is at least as new as the corresponding bytes on storage. Anonymous and CoW folios are always uptodate. If the folio is not uptodate, some of the bytes in it may be; see the is_partially_uptodate() address_space operation. Does this folio contain more than one page? True if the folio is larger than one page. Determine if the page belongs to the slab allocator True for slab pages, false for any other kind of page. Determine if the page belongs to hugetlbfs True for hugetlbfs pages, false for anon pages or pages belonging to other filesystems. Determine if a folio has private stuff, indicating that release routines should be invoked upon it. This is mostly used for places where we want to try to avoid taking the mmap_lock for too long a time when waiting for another condition to change, in which case we can try to be polite to release the mmap_lock in the first round to avoid potential starvation of other processes that would also want the mmap_lock. true if the page fault allows retry and this is the first attempt of the fault handling; false otherwise. A folio is composed of 2^order pages. See get_order() for the definition of order. Number of mappings of this folio. The folio mapcount corresponds to the number of present user page table entries that reference any part of a folio. Each such present user page table entry must be paired with exactly on folio reference. For ordindary folios, each user page table entry (PTE/PMD/PUD/...) counts exactly once. For hugetlb folios, each abstracted “hugetlb” user page table entry that references the entire folio counts exactly once, even when such special page table entries are comprised of multiple ordinary page table entries. Will report 0 for pages which cannot be mapped into userspace, such as slab, page tables and similar. The number of times this folio is mapped. Is this folio mapped into userspace? True if any page in this folio is referenced by user page tables. Number of bytes in this page. May be called in any context, as long as you know that you have a refcount on the folio. If you do not already have one, may be the right interface for you to use. If the folio’s reference count reaches zero, the memory will be released back to the page allocator and may be used by another allocation immediately. Do not access the memory or the after calling unless you can be sure that it wasn’t the last reference. May be called in process or interrupt context, but not in NMI context. May be called while holding a spinlock. The amount to subtract from the folio’s reference count. If the folio’s reference count reaches zero, the memory will be released back to the page allocator and may be used by another allocation immediately. Do not access the memory or the after calling unless you can be sure that these weren’t the last references. May be called in process or interrupt context, but not in NMI context. May be called while holding a spinlock. Decrement the reference count on an array of folios. Like , but for a batch of folios. This is more efficient than writing the loop yourself as it will optimise the locks which need to be taken if the folios are freed. The folios batch is returned empty and ready to be reused for another batch; there is no need to reinitialise it. May be called in process or interrupt context, but not in NMI context. May be called while holding a spinlock. A folio may contain multiple pages. The pages have consecutive Page Frame Numbers. The Page Frame Number of the first page in the folio. Report if a folio may be pinned for DMA. This function checks if a folio has been pinned via a call to a function in the pin_user_pages() family. For small folios, the return value is partially fuzzy: false is not fuzzy, because it means “definitely not pinned for DMA”, but true means “probably pinned for DMA, but possibly a false positive due to having at least GUP_PIN_COUNTING_BIAS worth of normal folio references”. False positives are OK, because: a) it’s unlikely for a folio to get that many refcounts, and b) all the callers of this routine are expected to be able to deal gracefully with a false positive. For most large folios, the result will be exactly correct. That’s because we have more tracking data available: the _pincount field is used instead of the GUP_PIN_COUNTING_BIAS scheme. For more information, please see pin_user_pages() and related calls. True, if it is likely that the folio has been “dma-pinned”. False, if the folio is definitely not dma-pinned. Query if a page is a zero page This returns true if page is one of the permanent zero pages. Query if a folio is a zero page This returns true if folio is one of the permanent zero pages. The number of pages in the folio. The number of regular pages in this huge page. Move to the next physical folio. The folio we’re currently operating on. If you have physically contiguous memory which may span more than one folio (eg a ), use this function to move from one folio to the next. Do not use it if the memory is only virtually contiguous as the folios are almost certainly not adjacent to each other. This is the folio equivalent to writing . We assume that the folios are refcounted and/or locked at a higher level and do not adjust the reference counts. The size of the memory described by this folio. A folio represents a number of bytes which is a power-of-two in size. This function tells you which power-of-two the folio is. See also and . The caller should have a reference on the folio to prevent it from being split. It is not necessary for the folio to be locked. The base-2 logarithm of the size of this folio. The number of bytes in a folio. The caller should have a reference on the folio to prevent it from being split. It is not necessary for the folio to be locked. The number of bytes in this folio. Whether the folio is mapped into the page tables of more than one MM This function checks if the folio maybe currently mapped into more than one MM (“maybe mapped shared”), or if the folio is certainly mapped into a single MM (“mapped exclusively”). For KSM folios, this function also returns “mapped shared” when a folio is mapped multiple times into the same MM, because the individual page mappings are independent. For small anonymous folios and anonymous hugetlb folios, the return value will be exactly correct: non-KSM folios can only be mapped at most once into an MM, and they cannot be partially mapped. KSM folios are considered shared even if mapped multiple times into the same MM. For other folios, the result can be fuzzy:\n• None For partially-mappable large folios (THP), the return value can wrongly indicate “mapped shared” (false positive) if a folio was mapped by more than two MMs at one point in time.\n• None For pagecache folios (including hugetlb), the return value can wrongly indicate “mapped shared” (false positive) when two VMAs in the same MM cover the same file range. Further, this function only considers current page table mappings that are tracked using the folio mapcount(s). This function does not consider:\n• None If the folio might get mapped in the (near) future (e.g., swapcache, pagecache, temporary unmapping for migration).\n• None If the folio is mapped differently (VM_PFNMAP).\n• None If hugetlb page table sharing applies. Callers might want to check hugetlb_pmd_shared(). Whether the folio is estimated to be mapped into more than one MM. pagetable_alloc allocates memory for page tables as well as a page table descriptor to describe that memory. pagetable_free frees the memory of all page tables described by a page table descriptor and the memory for the descriptor itself. The vm_area_struct at the given address, otherwise. Pointer to the struct vm_area_struct to consider Whether transhuge page-table entries are considered “special” following the definition in vm_normal_page(). true if transhuge page-table entries should be considered special, false otherwise. The reference count on this folio. The refcount is usually incremented by calls to and decremented by calls to . Some typical users of the folio refcount:\n• None Direct IO which references this page in the process address space The number of references to this folio. Attempt to increase the refcount on a folio. If you do not already have a reference to a folio, you can attempt to get one using this function. It may fail if, for example, the folio has been freed since you found a pointer to it, or it is frozen for the purposes of splitting or migration. True if the reference count was successfully incremented. helper function to quickly check if a struct zone is a highmem zone or not. This is an attempt to keep references to ZONE_{DMA/NORMAL/HIGHMEM/etc} in general code to a minimum. helper macro to iterate over all online nodes helper macro to iterate over all memory zones The user only needs to declare the zone variable, for_each_zone fills it in. Returns the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point The cursor used as a starting point for the search The zone index of the highest zone to return An optional nodemask to filter the zonelist with This function returns the next zone at or below a given zone index that is within the allowed nodemask using a cursor as the starting point for the search. The zoneref returned is a cursor that represents the current zone being examined. It should be advanced by one before calling next_zones_zonelist again. the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist The zonelist to search for a suitable zone The zone index of the highest zone to return An optional nodemask to filter the zonelist with This function returns the first zone at or below a given zone index that is within the allowed nodemask. The zoneref returned is a cursor that can be used to iterate the zonelist with next_zones_zonelist by advancing it by one before calling. When no eligible zone is found, zoneref->zone is NULL (zoneref itself is never NULL). This may happen either genuinely, or due to concurrent nodemask update due to cpuset modification. Zoneref pointer for the first suitable zone found helper macro to iterate over valid zones in a zonelist at or below a given zone index and within a nodemask The current zone in the iterator The current pointer within zonelist->_zonerefs being iterated The zone index of the highest zone to return This iterator iterates though all zones at or below a given zone index and within a given nodemask helper macro to iterate over valid zones in a zonelist at or below a given zone index The current zone in the iterator The current pointer within zonelist->zones being iterated The zone index of the highest zone to return This iterator iterates though all zones at or below a given zone index. check if there is a valid memory map entry for a PFN Check if there is a valid memory map entry aka struct page for the pfn. Note, that availability of the memory map entry does not imply that there is actual usable memory at that pfn. The struct page may represent a hole or an unusable page frame. 1 for PFNs that have memory map entries and 0 otherwise Find the mapping where this folio is stored. For folios which are in the page cache, return the mapping that this page belongs to. Folios in the swap cache return the swap mapping this page is stored in (which is different from the mapping for the swap file or swap device where the data is stored). You can call this for folios which aren’t in the swap cache or page cache and it will return NULL. This makes sure the memory mapping described by ‘vma’ has an ‘anon_vma’ attached to it, so that we can associate the anonymous pages mapped into it with that anon_vma. The common case will be that we already have one, which is handled inline by anon_vma_prepare(). But if not we either need to find an adjacent mapping that we can re-use the anon_vma from (very common when the only reason for splitting a vma has been mprotect()), or we allocate a new one. Anon-vma allocations are very subtle, because we may have optimistically looked up an anon_vma in folio_lock_anon_vma_read() and that may actually touch the rwsem even in the newly allocated vma (it depends on RCU to make sure that the anon_vma isn’t actually destroyed). As a result, we need to do proper anon_vma locking even for the new allocation. At the same time, we do not want to do any locking for the common case of already having an anon_vma. The virtual address of a page in this VMA. The folio containing the page. The page within the folio. The VMA we need to know the address in. Calculates the user virtual address of this page in the specified VMA. It is the caller’s responsibililty to check the page is actually within the VMA. There may not currently be a PTE pointing at this page, but if a page fault occurs at this address, this is the page which will be accessed. Caller should hold a reference to the folio. Caller should hold a lock (eg the i_mmap_lock or the mmap_lock) which keeps the VMA from being altered. The virtual address corresponding to this page in the VMA. Test if the folio was referenced. A combination of all the vma->vm_flags which referenced the folio. Quick test_and_clear_referenced for all mappings of a folio, The number of mappings which referenced the folio. Return -1 if the function bailed out due to rmap lock contention. Write-protect all mappings in a specified range. The mapping whose reverse mapping should be traversed. The page offset at which pfn is mapped within mapping. The PFN of the page mapped in mapping at pgoff. Traverses the reverse mapping, finding all VMAs which contain a shared mapping of the pages in the specified range in mapping, and write-protects them (that is, updates the page tables to mark the mappings read-only such that a write protection fault arises when the mappings are written to). The pfn value need not refer to a folio, but rather can reference a kernel allocation which is mapped into userland. We therefore do not require that the page maps to a folio with a valid mapping or index field, rather the caller specifies these in mapping and pgoff. the number of write-protected PTEs, or an error. Cleans the PTEs (including PMDs) mapped with range of [pfn, pfn + nr_pages) at the specific offset (pgoff) within the vma of shared mappings. And since clean PTEs should also be readonly, write protects them too. page offset that the pfn mapped with. The folio to move to our anon_vma The vma the folio belongs to When a folio belongs exclusively to one process after a COW event, that folio can be moved into the anon_vma that belongs to just that process, so the rmap code will not search the parent or sibling processes. set up a new anonymous rmap for a folio The folio to set up the new anonymous rmap for. VM area to add the folio to. Whether the folio is exclusive to the process. the page to check the mapping of the vm area in which the mapping is added add PTE mappings to a page range of an anon folio The folio to add the mappings to The number of pages which will be mapped The vm area in which the mappings are added The user virtual address of the first page to map The page range of folio is defined by [first_page, first_page + nr_pages) The caller needs to hold the page table lock, and the page must be locked in the anon_vma case: to serialize mapping,index checking after setting, and to ensure that an anon folio is not being upgraded racily to a KSM folio (but KSM folios are never downgraded). add a PMD mapping to a page range of an anon folio The folio to add the mapping to The vm area in which the mapping is added The user virtual address of the first page to map The page range of folio is defined by [first_page, first_page + HPAGE_PMD_NR) The caller needs to hold the page table lock, and the page must be locked in the anon_vma case: to serialize mapping,index checking after setting. The folio to add the mapping to. the vm area in which the mapping is added Like folio_add_anon_rmap_*() but must only be called on new folios. This means the inc-and-test can be bypassed. The folio doesn’t necessarily need to be locked while it’s exclusive unless two threads map it concurrently. However, the folio must be locked if it’s shared. If the folio is pmd-mappable, it is accounted as a THP. The folio to add the mappings to The number of pages that will be mapped using PTEs The vm area in which the mappings are added The page range of the folio is defined by [page, page + nr_pages) The caller needs to hold the page table lock. The folio to add the mapping to The vm area in which the mapping is added The page range of the folio is defined by [page, page + HPAGE_PMD_NR) The caller needs to hold the page table lock. The folio to add the mapping to The vm area in which the mapping is added The page range of the folio is defined by [page, page + HPAGE_PUD_NR) The caller needs to hold the page table lock. The folio to remove the mappings from The number of pages that will be removed from the mapping The vm area from which the mappings are removed The page range of the folio is defined by [page, page + nr_pages) The caller needs to hold the page table lock. The folio to remove the mapping from The vm area from which the mapping is removed The page range of the folio is defined by [page, page + HPAGE_PMD_NR) The caller needs to hold the page table lock. The folio to remove the mapping from The vm area from which the mapping is removed The page range of the folio is defined by [page, page + HPAGE_PUD_NR) The caller needs to hold the page table lock. Try to remove all page table mappings to a folio. Tries to remove all the page table entries which are mapping this folio. It is the caller’s responsibility to check if the folio is still mapped if needed (use TTU_SYNC to prevent accounting races). try to replace all page table mappings with swap entries the folio to replace page table entries for Tries to remove all the page table entries which are mapping this folio and replace them with special swap entries. Caller must hold the folio lock. Mark a page for exclusive use by a device the virtual address to mark for exclusive device access passed to MMU_NOTIFY_EXCLUSIVE range notifier to allow filtering folio pointer will be stored here on success. This function looks up the page mapped at the given address, grabs a folio reference, locks the folio and replaces the PTE with special device-exclusive PFN swap entry, preventing access through the process page tables. The function will return with the folio locked and referenced. On fault, the device-exclusive entries are replaced with the original PTE under folio lock, after calling MMU notifiers. Only anonymous non-hugetlb folios are supported and the VMA must have write permissions such that we can fault in the anonymous page writable in order to mark it exclusive. The caller must hold the mmap_lock in read mode. A driver using this to program access from a device must use a mmu notifier critical section to hold a device specific lock during programming. Once programming is complete it should drop the folio lock and reference after which point CPU access to the page will revoke the exclusive access.\n• None This function always operates on individual PTEs mapping individual pages. PMD-sized THPs are first remapped to be mapped by PTEs before the conversion happens on a single PTE corresponding to addr.\n• None While concurrent access through the process page tables is prevented, concurrent access through other page references (e.g., earlier GUP invocation) is not handled and not supported.\n• None device-exclusive entries are considered “clean” and “old” by core-mm. Device drivers must update the folio state when informed by MMU notifiers. pointer to mapped page on success, otherwise a negative error. Traverse the reverse mapping for a file-backed mapping of a page mapped within a specified page cache object at a specified offset. Either the folio whose mappings to traverse, or if NULL, the callbacks specified in rwc will be configured such as to be able to look up mappings correctly. The page cache object whose mapping VMAs we intend to traverse. If folio is non-NULL, this should be equal to folio_mapping(folio). The offset within mapping of the page which we are looking up. If folio is non-NULL, this should be equal to folio_pgoff(folio). The number of pages mapped by the mapping. If folio is non-NULL, this should be equal to folio_nr_pages(folio). The reverse mapping walk control object describing how the traversal should proceed. Is the mapping already locked? If not, we acquire the lock. The address_space containing the folio. The folio to migrate the data to. The folio containing the current data. Common logic to directly migrate a single LRU folio suitable for folios that do not have private data. Folios are locked upon entry and exit. The folio to migrate from. How to migrate the folio. This function can only be used if the underlying filesystem guarantees that no other references to src exist. For example attached buffer heads are accessed only under the folio lock. If your filesystem cannot provide this guarantee, may be more appropriate. 0 on success or a negative errno on failure. The folio to migrate from. How to migrate the folio. Like except that this variant is more careful and checks that there are also no buffer head references. This function is the right one for mappings where buffer heads are directly looked up and referenced (such as block device mappings). 0 on success or a negative errno on failure. Perform a userland memory mapping into the current process address space of length len with protection bits prot, mmap flags flags (from which VMA flags will be inferred), and any additional VMA flags to apply vm_flags. If this is a file-backed mapping then the file is specified in file and page offset into the file via pgoff. An optional pointer describing the file which is to be mapped, if a file-backed mapping. If non-zero, hints at (or if flags has MAP_FIXED set, specifies) the address at which to perform this mapping. See mmap (2) for details. Must be page-aligned. The length of the mapping. Will be page-aligned and must be at least 1 page in size. Protection bits describing access required to the mapping. See mmap (2) for details. Flags specifying how the mapping should be performed, see mmap (2) for details. VMA flags which should be set by default, or 0 otherwise. Page offset into the file if file-backed, should be 0 otherwise. A pointer to a value which will be set to 0 if no population of the range is required, or the number of bytes to populate if it is. Must be non-NULL. See mmap (2) for details as to under what circumstances population of the range occurs. An optional pointer to a list head to track userfaultfd unmap events should unmapping events arise. If provided, it is up to the caller to manage this. This function does not perform security checks on the file and assumes, if uf is non-NULL, the caller has provided a list head to track unmap events for userfaultfd uf. It also simply indicates whether memory population is required by setting populate, which must be non-NULL, expecting the caller to actually perform this task itself if appropriate. This function will invoke architecture-specific (and if provided and relevant, file system-specific) logic to determine the most appropriate unmapped area in which to place the mapping if not MAP_FIXED. Callers which require userland mmap() behaviour should invoke vm_mmap(), which is also exported for module use. Those which require this behaviour less security checks, userfaultfd and populate behaviour, and who handle the mmap write lock themselves, should call this function. Note that the returned address may reside within a merged VMA if an appropriate merge were to take place, so it doesn’t necessarily specify the start of a VMA, rather only the start of a valid mapped range of length len bytes, rounded down to the nearest page size. Either an error, or the address at which the requested mapping has been performed. Look up the first VMA which intersects the interval The first VMA within the provided range, otherwise. Assumes start_addr < end_addr. Find the VMA for a given address, or the next VMA. The VMA associated with addr, or the next VMA. May return in the case of no VMA at addr or above. Find the VMA for a given address, or the next vma and set to the previous VMA, if any. The pointer to set to the previous VMA Note that RCU lock is missing here since the external mmap_lock() is used instead. The VMA associated with addr, or the next vma. May return in the case of no vma at addr or above. pointer to beginning of the object minimum number of references to this object. If during memory scanning a number of references less than min_count is found, the object is reported as a memory leak. If min_count is 0, the object is never reported as a leak. If min_count is -1, the object is ignored (not scanned and not reported as a leak) This function is called from the kernel allocators when a new object (memory block) is allocated (kmem_cache_alloc, kmalloc etc.). __percpu pointer to beginning of the object This function is called from the kernel percpu allocator when a new object (memory block) is allocated (alloc_percpu). This function is called from the vmalloc() kernel allocator when a new object (memory block) is allocated. pointer to beginning of the object This function is called from the kernel allocators when an object (memory block) is freed (kmem_cache_free, kfree, vfree etc.). pointer to the beginning or inside the object. This also represents the start of the range to be freed This function is called when only a part of a memory block is freed (usually from the bootmem allocator). __percpu pointer to beginning of the object This function is called from the kernel percpu allocator when an object (memory block) is freed (free_percpu). pointer to beginning of the object Override the object allocation stack trace for cases where the actual allocation place is not always useful. pointer to beginning of the object Calling this function on an object will cause the memory block to no longer be reported as leak and always be scanned. pointer to beginning of the object Calling this function on an object will cause the memory block to not be reported as a leak temporarily. This may happen, for example, if the object is part of a singly linked list and the ->next reference to it is changed. pointer to beginning of the object Calling this function on an object will cause the memory block to be ignored (not scanned and not reported as a leak). This is usually done when it is known that the corresponding block is not a leak and does not contain any references to other allocated memory blocks. limit the range to be scanned in an allocated object pointer to beginning or inside the object. This also represents the start of the scan area This function is used when it is known that only certain parts of an object contain references to other objects. Kmemleak will only scan these areas reducing the number false negatives. do not scan an allocated object pointer to beginning of the object This function notifies kmemleak not to scan the given memory block. Useful in situations where it is known that the given object does not contain any references to other objects. Kmemleak will not scan such objects reducing the number of false negatives. physical address if the beginning or inside an object. This also represents the start of the range to be freed remap and provide memmap backing for the given resource 1/ At a minimum the range and type members of pgmap must be initialized by the caller before passing it to this function 2/ The altmap field may optionally be initialized, in which case PGMAP_ALTMAP_VALID must be set in pgmap->flags. 3/ The ref field may optionally be provided, in which pgmap->ref must be ‘live’ on entry and will be killed and reaped at devm_memremap_pages_release() time, or if this routine fails. 4/ range is expected to be a host memory range that could feasibly be treated as a “System RAM” range, i.e. not a device mmio range, but this is not enforced. take a new live reference on the dev_pagemap for pfn optional known pgmap that already has a reference If pgmap is non-NULL and covers pfn it will be returned as-is. If pgmap is non-NULL but does not cover pfn the reference to it will be released. Folios in this VMA will be aligned to, and at least the size of the number of bytes returned by this function. The default size of the folios allocated when backing a VMA. try to isolate an allocated hugetlb folio the list to add the folio to on success Isolate an allocated (refcount > 0) hugetlb folio, marking it as isolated/non-migratable, and moving it from the active list to the given list. Isolation will fail if folio is not an allocated hugetlb folio, or if it is already isolated/non-migratable. On success, an additional folio reference is taken that must be dropped using to undo the isolation. Putback/un-isolate the hugetlb folio that was previous isolated using : marking it non-isolated/migratable and putting it back onto the active list. Will drop the additional folio reference obtained through . Mark a folio as having seen activity. This function will perform one of the following transitions: When a newly allocated folio is not yet visible, so safe for non-atomic ops, __folio_set_referenced() may be substituted for . The folio to be added to the LRU. Queue the folio for addition to the LRU. The decision on whether to add the page to the [in]active [file|anon] list is deferred until the folio_batch is drained. This gives a chance for the caller of have the folio added to the active list using . Add a folio to the appropate LRU list for this VMA. The folio to be added to the LRU. VMA in which the folio is mapped. If the VMA is mlocked, folio is added to the unevictable list. Otherwise, it is treated the same way as . This function hints to the VM that folio is a good reclaim candidate, for example if its invalidation fails due to the folio being dirty or under writeback. moves folio to the inactive file list. This is done to accelerate the reclaim of folio. Reduce the reference count on a batch of folios. The number of refs to subtract from each folio. Like , but for a batch of folios. This is more efficient than writing the loop yourself as it will optimise the locks which need to be taken if the folios are freed. The folios batch is returned empty and ready to be reused for another batch; there is no need to reinitialise it. If refs is NULL, we subtract one from each folio refcount. May be called in process or interrupt context, but not in NMI context. May be called while holding a spinlock. Decrement the reference count on all the pages in arg. If it fell to zero, remove the page from the LRU and free it. Note that the argument can be an array of pages, encoded pages, or folio pointers. We ignore any encoded bits, and turn any of them into just a folio that gets free’d. find_get_entries() fills a batch with both folios and shadow/swap/DAX entries. This function prunes all the non-folio entries from fbatch without leaving holes, so that it can be passed on to folio-only batch operations. Module usage counting is used to prevent using a driver while/after unloading, so if this is called from module exit function, this should never fail; if called from other than the module exit function, and this returns failure, the driver is in use and must remain available. Check if the pool driver is available The type of the zpool to check (e.g. zsmalloc) This checks if the type pool driver is available. This will try to load the requested module, if needed, but there is no guarantee the module will still be loaded and available immediately after calling. If this returns true, the caller should assume the pool is available, but must be prepared to handle the returning failure. However if this returns false, the caller should assume the requested pool type is not available; either the requested pool type module does not exist, or could not be loaded, and calling with the pool type will fail. The type string must be null-terminated. true if type pool is available, false if not The type of the zpool to create (e.g. zsmalloc) The name of the zpool (e.g. zram0, zswap) The GFP flags to use when allocating the pool. This creates a new zpool of the specified type. The gfp flags will be used when allocating memory, if the implementation supports it. If the ops param is NULL, then the created zpool will not be evictable. Implementations must guarantee this to be thread-safe. The type and name strings must be null-terminated. New zpool on success, NULL on failure. Implementations must guarantee this to be thread-safe, however only when destroying different pools. The same pool should only be destroyed once, and should not be used after it is destroyed. This destroys an existing zpool. The zpool should not be in use. Get the type of the zpool This returns the type of the pool. Implementations must guarantee this to be thread-safe. The zpool to allocate from. The amount of memory to allocate. The GFP flags to use when allocating memory. Pointer to the handle to set This allocates the requested amount of memory from the pool. The gfp flags will be used when allocating memory, if the implementation supports it. The provided handle will be set to the allocated object handle. Implementations must guarantee this to be thread-safe. 0 on success, negative value on error. The zpool that allocated the memory. The handle to the memory to free. This frees previously allocated memory. This does not guarantee that the pool will actually free memory, only that the memory in the pool will become available for use by the pool. Implementations must guarantee this to be thread-safe, however only when freeing different handles. The same handle should only be freed once, and should not be used after freeing. The zpool that the handle was allocated from A local buffer to use if needed. This starts a read operation of a previously allocated handle. The passed local_copy buffer may be used if needed by copying the memory into. MUST be called after the read is completed to undo any actions taken (e.g. release locks). A pointer to the handle memory to be read, if local_copy is used, the returned pointer is local_copy. The zpool that the handle was allocated from The zpool that the handle was allocated from The memory to copy from into the handle. The length of memory to be written. The total size of the pool This returns the total size in pages of the pool. Total size of the zpool in pages. css of the memcg associated with a folio If memcg is bound to the default hierarchy, css of the memcg associated with folio is returned. The returned css remains associated with folio until it is released. If memcg is bound to a traditional hierarchy, the css of root_mem_cgroup is returned. return inode number of the memcg a page is charged to Look up the closest online ancestor of the memory cgroup page is charged to and return its inode number or 0 if page is not charged to any cgroup. It is safe to call this function without holding a reference to page. Note, this function is inherently racy, because there is nothing to prevent the cgroup inode from getting torn down and potentially reallocated a moment after returns, so it only should be used by callers that do not care (such as procfs interfaces). the stat item - can be enum memcg_stat_item or enum node_stat_item delta to add to the counter, can be negative delta to add to the counter, can be negative The lruvec is the intersection of the NUMA node and a cgroup. This function updates the all three counters that are affected by a change of state at this level: per-node, per-cgroup, per-lruvec. the number of events that occurred mm from which memcg should be extracted. It can be NULL. Obtain a reference on mm->memcg and returns it if successful. If mm is NULL, then the memcg is chosen as follows: 1) The active memcg, if set. 2) current->mm->memcg, if available 3) root memcg If mem_cgroup is disabled, NULL is returned. folio from which memcg should be extracted. Returns references to children of the hierarchy below root, or root itself, or after a full round-trip. Caller must pass the return value in prev on subsequent invocations for reference counting, or use to cancel a hierarchy walk before the round-trip is complete. Reclaimers can specify a node in reclaim to divide up the memcgs in the hierarchy among all concurrent reclaimers operating on the same node. last visited hierarchy member as returned by function to call for each task This function iterates over tasks attached to memcg or to any of its descendants and calls fn for each task. If fn returns a non-zero value, the function breaks the iteration loop. Otherwise, it will iterate over all tasks and return 0. This function must not be called for the root memory cgroup. These functions are safe to use under any of the following conditions: - folio locked - folio_test_lru false - folio frozen (refcount of 0) The lruvec this folio is on with its lock held. These functions are safe to use under any of the following conditions: - folio locked - folio_test_lru false - folio frozen (refcount of 0) The lruvec this folio is on with its lock held and interrupts disabled. These functions are safe to use under any of the following conditions: - folio locked - folio_test_lru false - folio frozen (refcount of 0) The lruvec this folio is on with its lock held and interrupts disabled. account for adding or removing an lru page index of lru list the page is sitting on positive when adding or negative when removing This function must be called under lru_lock, just before a page is added to or just after a page is removed from an lru list. Returns the maximum amount of memory mem can be charged with, in pages. The memory cgroup that went over limit Task that is going to be killed memcg and p’s mem_cgroup can be different when hierarchy is enabled The memory cgroup that went over limit get a memory cgroup to clean up after OOM task to be killed by the OOM killer memcg in case of memcg OOM, NULL in case of system-wide OOM Returns a pointer to a memory cgroup, which has to be cleaned up by killing all belonging OOM-killable tasks. Caller has to call mem_cgroup_put() on the returned non-NULL memcg. Try to consume stocked charge on this cpu. how many pages to charge. The charges will only happen if memcg matches the current cpu’s memcg stock, and at least nr_pages are available in that stock. Failure to service an allocation will refill the stock. Returns 0 on success, an error code on failure. out parameter for number of file pages out parameter for number of allocatable pages according to memcg out parameter for number of dirty pages out parameter for number of pages under writeback Determine the numbers of file, headroom, dirty, and writeback pages in wb’s memcg. File, dirty and writeback are self-explanatory. Headroom is a bit more involved. A memcg’s headroom is “min(max, high) - used”. In the hierarchy, the headroom is calculated as the lowest headroom of itself and the ancestors. Note that this doesn’t consider the actual amount of available memory in the system. The caller should further cap *pheadroom accordingly. look up a memcg from a memcg id Reset the states of the mem_cgroup associated with css. This is invoked when the userland requests disabling on the default hierarchy but the memcg is pinned through dependency. The memcg should stop applying policies and should revert to the vanilla state as it may be made visible again. The current implementation only resets the essential configurations. This needs to be expanded to cover all the visible parts. check if memory consumption is in the normal range the top ancestor of the sub-tree being checked WARNING: This function is not stateless! It can only be used as part of a top-down tree iteration, not for isolated queries. This function is called when allocating a huge page folio, after the page has already been obtained and charged to the appropriate hugetlb cgroup controller (if it is enabled). Returns ENOMEM if the memcg is already full. Returns 0 if either the charge was successful, or if we skip the charging. swap entry for which the folio is allocated This function charges a folio allocated for swapin. Please call this before adding the folio to the swapcache. Returns 0 on success. Otherwise, an error code is returned. Charge new as a replacement folio for old. old will be uncharged upon free. Both folios must be locked, new->mapping must be set up. Transfer the memcg data from the old to the new folio. Transfer the memcg data from the old folio to the new folio for migration. The old folio’s data info will be cleared. Note that the memory counters will remain unchanged throughout the process. Both folios must be locked, new->mapping must be set up. Charges nr_pages to memcg. Returns if the charge fit within memcg’s configured limit, if it doesn’t. Try to charge folio’s memcg for the swap space at entry. the amount of swap space to uncharge check if this cgroup can zswap Check if the hierarchical zswap limit has been reached. This doesn’t check for specific headroom, and it is not atomic either. But with zswap, the size of the allocation is only known once compression has occurred, and this optimistic pre-check avoids spending cycles on compression when there is already no room left or zswap is disabled altogether somewhere in the hierarchy. This forces the charge after allowed compression and storage in zwap for this cgroup to go ahead. recalculate the block usage of an inode the change in number of pages allocated to inode the change in number of pages swapped from inode We have to calculate the free blocks since the mm can drop undirtied hole pages behind our back. But normally info->alloced == inode->i_mapping->nrpages + info->swapped So mm freed is info->alloced - (inode->i_mapping->nrpages + info->swapped) Get allowable folio orders for the given file size. This returns huge orders for folios (when supported) based on the file size which the mapping currently allows at the given index. The index is relevant due to alignment considerations the mapping might have. The returned order may be less than the size passed. pointer to the folio if found Looks up the page cache entry at inode & index. If a folio is present, it is returned locked with an increased refcount. If the caller modifies data in the folio, it must call before unlocking the folio to ensure that the folio is not reclaimed. There is no need to reserve space before calling . When no folio is found, the behavior depends on sgp:\n• None for SGP_READ, *foliop is and 0 is returned\n• None for SGP_NOALLOC, *foliop is and -ENOENT is returned\n• None for all other flags a new folio is allocated, inserted into the page cache and returned locked in foliop. get an unlinked file living in tmpfs which must be kernel internal. There will be NO LSM permission checks against the underlying inode. So users of this interface must do LSM checks at a higher layer. The users are the big_key and shm implementations. LSM checks are provided at the key or shm level rather than the inode. name for dentry (to be seen in /proc/<pid>/maps) size to be set for the file get an unlinked file living in tmpfs name for dentry (to be seen in /proc/<pid>/maps) size to be set for the file get an unlinked file living in tmpfs the tmpfs mount where the file will be created name for dentry (to be seen in /proc/<pid>/maps) size to be set for the file the vma to be mmapped is prepared by do_mmap read into page cache, using specified page allocation flags. the page allocator flags to use if allocating This behaves as a tmpfs “read_cache_page_gfp(mapping, index, gfp)”, with any new page allocations done using the specified allocation flags. But uses the ->read_folio() method: which does not suit tmpfs, since it may have pages in swapcache, and needs to find those for itself; although drivers/gpu/drm i915 and ttm rely upon this support. i915_gem_object_get_pages_gtt() mixes __GFP_NORETRY | __GFP_NOWARN in with the mapping_gfp_mask(), to avoid OOMing the machine unnecessarily. contains the vma, start, and pfns arrays for the migration negative errno on failures, 0 when 0 or more pages were migrated without an error. Prepare to migrate a range of memory virtual address range by collecting all the pages backing each virtual address in the range, saving them inside the src array. Then lock those pages and unmap them. Once the pages are locked and unmapped, check whether each page is pinned or not. Pages that aren’t pinned have the MIGRATE_PFN_MIGRATE flag set (by this function) in the corresponding src array entry. Then restores any pages that are pinned, by remapping and unlocking those pages. The caller should then allocate destination memory and copy source memory to it for all those entries (ie with MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE flag set). Once these are allocated and copied, the caller must update each corresponding entry in the dst array with the pfn value of the destination page and with MIGRATE_PFN_VALID. Destination pages must be locked via . Note that the caller does not have to migrate all the pages that are marked with MIGRATE_PFN_MIGRATE flag in src array unless this is a migration from device memory to system memory. If the caller cannot migrate a device page back to system memory, then it must return VM_FAULT_SIGBUS, which has severe consequences for the userspace process, so it must be avoided if at all possible. For empty entries inside CPU page table (pte_none() or pmd_none() is true) we do set MIGRATE_PFN_MIGRATE flag inside the corresponding source array thus allowing the caller to allocate device memory for those unbacked virtual addresses. For this the caller simply has to allocate device memory and properly set the destination entry like for regular migration. Note that this can still fail, and thus inside the device driver you must check if the migration was successful for those entries after calling , just like for regular migration. After that, the callers must call to go over each entry in the src array that has the MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE flag set. If the corresponding entry in dst array has MIGRATE_PFN_VALID flag set, then to migrate struct page information from the source struct page to the destination struct page. If it fails to migrate the struct page information, then it clears the MIGRATE_PFN_MIGRATE flag in the src array. At this point all successfully migrated pages have an entry in the src array with MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE flag set and the dst array entry with MIGRATE_PFN_VALID flag set. Once returns the caller may inspect which pages were successfully migrated, and which were not. Successfully migrated pages will have the MIGRATE_PFN_MIGRATE flag set for their src array entry. It is safe to update device page table after because both destination and source page are still locked, and the mmap_lock is held in read mode (hence no one can unmap the range being migrated). Once the caller is done cleaning up things and updating its page table (if it chose to do so, this is not an obligation) it finally calls to update the CPU page table to point to new pages for successfully migrated pages or otherwise restore the CPU page table to point to the original source pages. array of pfns allocated by the driver to migrate memory to number of pages in the range Equivalent to . This is called to migrate struct page meta-data from source struct page to destination. This migrates struct page meta-data from source struct page to destination struct page. This effectively finishes the migration from source page to the destination page. This replaces the special migration pte entry with either a mapping to the new page if migration was successful for that page, or to the original page otherwise. This also unlocks the pages and puts them back on the lru, or drops the extra refcount, for device pages. starting pfn in the range to migrate. is similar in concept to except that instead of looking up pages based on virtual address mappings a range of device pfns that should be migrated to system memory is used instead. This is useful when a driver needs to free device memory but doesn’t know the virtual mappings of every page that may be in device memory. For example this is often the case when a driver is being unloaded or unbound from a device. Like this function will take a reference and lock any migrating pages that aren’t free before unmapping them. Drivers may then allocate destination pages and start copying data from the device to CPU memory before calling . Similar to but supports non-contiguous pre-popluated array of device pages to migrate. The function write-protects a pte and records the range in virtual address space of touched ptes for efficient range TLB flushes. Address_space Page offset of the first bit in bitmap Bitmap with one bit for each page offset in the address_space range covered. Address_space page offset of first modified pte relative to bitmap_pgoff Address_space page offset of last modified pte relative to bitmap_pgoff Clean a pte and record its address space offset in a bitmap The start of virtual address to be clean The end of virtual address to be clean The function cleans a pte and records the range in virtual address space of touched ptes for efficient TLB flushes. It also records dirty ptes in a bitmap representing page offsets in the address_space, as well as the first and last of the bits touched. Write-protect all ptes in an address space range The address_space we want to write protect The first page offset in the range This function currently skips transhuge page-table entries, since it’s intended for dirty-tracking on the PTE level. It will warn on encountering transhuge write-enabled entries, though, and can easily be extended to handle them as well. The number of ptes actually write-protected. Note that already write-protected ptes are not counted. Clean and record all ptes in an address space range The address_space we want to clean The first page offset in the range The page offset of the first bit in bitmap Pointer to a bitmap of at least nr bits. The bitmap needs to cover the whole range first_index..**first_index** + nr. Pointer to number of the first set bit in bitmap. is modified as new bits are set by the function. Pointer to the number of the last set bit in bitmap. none set. The value is modified as new bits are set by the function. When this function returns there is no guarantee that a CPU has not already dirtied new ptes. However it will not clean any ptes not reported in the bitmap. The guarantees are as follows:\n• None All ptes dirty when the function starts executing will end up recorded in the bitmap.\n• None All ptes dirtied after that will either remain dirty, be recorded in the bitmap or both. If a caller needs to make sure all dirty ptes are picked up and none additional are added, it first needs to write-protect the address-space range and make sure new writers are blocked in page_mkwrite() or pfn_mkwrite(). And then after a TLB flush following the write-protection pick up all dirty bits. This function currently skips transhuge page-table entries, since it’s intended for dirty-tracking on the PTE level. It will warn on encountering transhuge dirty entries, though, and can easily be extended to handle them as well. The number of dirty ptes actually cleaned. check if the address is served from this chunk True if the address is served from this chunk. Check to see if the allocation can fit in the block’s contig hint. Note, a chunk uses the same hints as a block so this can also check against the chunk’s contig hint. Helper function for pcpu_for_each_md_free_region. It checks block->contig_hint and performs aggregation across blocks to find the next hint. It modifies bit_off and bits in-place to be consumed in the loop. Finds the next free region that is viable for use with a given size and alignment. This only returns if there is a valid area to be used for this allocation. block->first_free is returned if the allocation request fits within the block to see if the request can be fulfilled prior to the contig hint. Allocate size bytes. If size is smaller than PAGE_SIZE, is used; otherwise, the equivalent of vzalloc() is used. This is to facilitate passing through whitelisted flags. The returned memory is always zeroed. Pointer to the allocated area on success, NULL on failure. Free ptr. ptr should have been allocated using . put chunk in the appropriate chunk slot the previous slot it was on This function is called after an allocation or free changed chunk. New slot according to the changed state is determined and chunk is moved to the slot. Note that the reserved chunk is never put on chunk slots. Updates a block given a known free area. The region [start, end) is expected to be the entirety of the free area within a block. Chooses the best starting offset if the contig hints are equal. if we should scan from the beginning Iterates over the metadata blocks to find the largest contig area. A full scan can be avoided on the allocation path as this is triggered if we broke the contig_hint. In doing so, the scan_hint will be before the contig_hint or after if the scan_hint == contig_hint. This cannot be prevented on freeing as we want to find the largest area possibly spanning blocks. Scans over the block beginning at first_free and updates the block metadata accordingly. Updates metadata for the allocation path. The metadata only has to be refreshed by a full scan iff the chunk’s contig hint is broken. Block level scans are required if the block’s contig hint is broken. updates the block hints on the free path Updates metadata for the allocation path. This avoids a blind block refresh by making use of the block contig hints. If this fails, it scans forward and backward to determine the extent of the free area. This is capped at the boundary of blocks. A chunk update is triggered if a page becomes free, a block becomes free, or the free spans across blocks. This tradeoff is to minimize iterating over the block metadata to update chunk_md->contig_hint. chunk_md->contig_hint may be off by up to a page, but it will never be more than the available space. If the contig hint is contained in one block, it will be accurate. determines if the region is populated return value for the next offset to start searching For atomic allocations, check if the backing pages are populated. Bool if the backing pages are populated. next_index is to skip over unpopulated blocks in pcpu_find_block_fit. Given a chunk and an allocation spec, find the offset to begin searching for a free region. This iterates over the bitmap metadata blocks to find an offset that will be guaranteed to fit the requirements. It is not quite first fit as if the allocation does not fit in the contig hint of a block or chunk, it is skipped. This errs on the side of caution to prevent excess iteration. Poor alignment can cause the allocator to skip over blocks and chunks that have valid free areas. The offset in the bitmap to begin searching. -1 if no offset is found. This function takes in a start offset to begin searching to fit an allocation of alloc_bits with alignment align. It needs to scan the allocation map because if it fits within the block’s contig hint, start will be block->first_free. This is an attempt to fill the allocation prior to breaking the contig hint. The allocation and boundary maps are updated accordingly if it confirms a valid free area. Allocated addr offset in chunk on success. -1 if no matching area is found. This function determines the size of an allocation to free using the boundary bitmap and clears the allocation map. creates chunks that serve the first chunk the start of the region served This is responsible for creating the chunks that serve the first chunk. The base_addr is page aligned down of tmp_addr while the region end is page aligned up. Offsets are kept track of to determine the region served. All this is done to appease the bitmap allocator in avoiding partial blocks. Chunk serving the region at tmp_addr of map_size. Pages in [page_start,**page_end**) have been populated to chunk. Update the bookkeeping information accordingly. Must be called after each successful population. Pages in [page_start,**page_end**) have been depopulated from chunk. Update the bookkeeping information accordingly. Must be called after each successful depopulation. address for which the chunk needs to be determined. This is an internal function that handles all but static allocations. Static percpu address values should never be passed into the allocator. The address of the found chunk. size of area to allocate in bytes allocate from the reserved chunk if available Allocate percpu area of size bytes aligned at align. If gfp doesn’t contain , the allocation is atomic. If gfp has __GFP_NOWARN then no warning will be triggered on invalid or failed allocation requests. Percpu pointer to the allocated area on success, NULL on failure. free chunks only if there are no populated pages If empty_only is , reclaim all fully free chunks regardless of the number of populated pages. Otherwise, only reclaim chunks that have no populated pages. Maintain a certain amount of populated pages to satisfy atomic allocations. It is possible that this is called when physical memory is scarce causing OOM killer to be triggered. We should avoid doing so until an actual allocation causes the failure as it is possible that requests can be serviced from already backed regions. Scan over chunks in the depopulate list and try to release unused populated pages back to the system. Depopulated chunks are sidelined to prevent repopulating these pages unless required. Fully free chunks are reintegrated and freed accordingly (1 is kept around). If we drop below the empty populated pages threshold, reintegrate the chunk if it has empty free pages. Each chunk is scanned in the reverse order to keep populated pages close to the beginning of the chunk. manage the amount of free chunks and populated pages For each chunk type, manage the number of fully free chunks and the number of populated pages. An important thing to consider is when pages are freed and how they contribute to the global counts. Can be called from atomic context. test whether address is from static percpu area Test whether addr belongs to in-kernel static percpu area. Module static percpu areas are not considered. For those, use is_module_percpu_address(). if addr is from in-kernel static percpu area, otherwise. the address to be converted to physical address Given addr which is dereferenceable address obtained via one of percpu access macros, this function translates it into its physical address. The caller is responsible for ensuring addr stays valid until this function finishes. percpu allocator has special setup for the first chunk, which currently supports either embedding in linear address space or vmalloc mapping, and, from the second one, the backing allocator (currently either vm or km) provides translation. The addr can be translated simply without checking if it falls into the first chunk. But the current code reflects better how percpu allocator actually works, and the verification can discover both bugs in percpu allocator itself and callers. So we keep current code. Allocate ai which is large enough for nr_groups groups containing nr_units units. The returned ai’s groups[0].cpu_map points to the cpu_map array which is long enough for nr_units and filled with NR_CPUS. It’s the caller’s responsibility to initialize cpu_map pointer of other groups. Pointer to the allocated pcpu_alloc_info on success, NULL on failure. Free ai which was allocated by . Print out information about ai using loglevel lvl. pcpu_alloc_info describing how to percpu area is shaped Initialize the first percpu chunk which contains the kernel static percpu area. This function is to be called from arch percpu area setup path. ai contains all information necessary to initialize the first chunk and prime the dynamic percpu allocator. ai->static_size is the size of static percpu area. ai->reserved_size, if non-zero, specifies the amount of bytes to reserve after the static area in the first chunk. This reserves the first chunk such that it’s available only through reserved percpu allocation. This is primarily used to serve module percpu static areas on architectures where the addressing model has limited offset range for symbol relocations to guarantee module percpu symbols fall inside the relocatable range. ai->dyn_size determines the number of bytes available for dynamic allocation in the first chunk. The area between ai->static_size + ai->reserved_size + ai->dyn_size and ai->unit_size is unused. ai->unit_size specifies unit size and must be aligned to PAGE_SIZE and equal to or larger than ai->static_size + ai->reserved_size + ai->dyn_size. ai->atom_size is the allocation atom size and used as alignment for vm areas. ai->alloc_size is the allocation size and always multiple of ai->atom_size. This is larger than ai->atom_size if ai->unit_size is larger than ai->atom_size. ai->nr_groups and ai->groups describe virtual memory layout of percpu areas. Units which should be colocated are put into the same group. Dynamic VM areas will be allocated according to these groupings. If ai->nr_groups is zero, a single group containing all units is assumed. The caller should have mapped the first chunk at base_addr and copied static data to each unit. The first chunk will always contain a static and a dynamic region. However, the static region is not managed by any chunk. If the first chunk also contains a reserved region, it is served by two chunks - one for the reserved region and one for the dynamic region. They share the same vm, but use offset regions in the area allocation map. The chunk serving the dynamic region is circulated in the chunk slots and available for dynamic allocation like any other chunk. the size of reserved percpu area in bytes This function determines grouping of units, their mappings to cpus and other parameters considering needed percpu size, allocation atom size and distances between CPUs. Groups are always multiples of atom size and CPUs which are of LOCAL_DISTANCE both ways are grouped together and share space for units in the same group. The returned configuration is guaranteed to have CPUs on different nodes on different groups and >=75% usage of allocated virtual address space. On success, pointer to the new allocation_info is returned. On failure, ERR_PTR value is returned. embed the first percpu chunk into bootmem the size of reserved percpu area in bytes This is a helper to ease setting up embedded first percpu chunk and can be called where is expected. If this function is used to setup the first chunk, it is allocated by calling pcpu_fc_alloc and used as-is without being mapped into vmalloc area. Allocations are always whole multiples of atom_size aligned to atom_size. This enables the first chunk to piggy back on the linear physical mapping which often uses larger page size. Please note that this can result in very sparse cpu->unit mapping on NUMA machines thus requiring large vmalloc address space. Don’t use this allocator if vmalloc space is not orders of magnitude larger than distances between node memory addresses (ie. 32bit NUMA machines). If the needed size is smaller than the minimum or specified unit size, the leftover is returned using pcpu_fc_free. map the first chunk using PAGE_SIZE pages the size of reserved percpu area in bytes This is a helper to ease setting up page-remapped first percpu chunk and can be called where is expected. This is the basic allocator. Static percpu area is allocated page-by-page into vmalloc area. pointer to the buffer that shall take the data address to read from. This must be a user address. Safely read from user address src to the buffer at dst. If a kernel fault happens, handle that and return -EFAULT. pointer to the data that shall be written Safely write to address dst from the buffer at src. If a kernel fault happens, handle that and return -EFAULT. Destination address, in kernel space. This buffer must be at least count bytes long. Maximum number of bytes to copy, including the trailing NUL. On success, returns the length of the string INCLUDING the trailing NUL. If access fails, returns -EFAULT (some data may have been copied and the trailing NUL added). If count is smaller than the length of the string, copies count-1 bytes, sets the last byte of dst buffer to NUL and returns count.\n• None Get the size of a user string INCLUDING final NUL. Get the size of a NUL-terminated string in user space without pagefault. Returns the size of the string INCLUDING the terminating NUL. If the string is too long, returns a number larger than count. User has to check the return value against “> count”. On exception (or invalid count), returns 0. Unlike strnlen_user, this can be used from IRQ handler etc. because it disables pagefaults. is the usual dirty throttling mechanism available? The normal page dirty throttling mechanism in balance_dirty_pages() is completely broken with the legacy memcg and direct stalling in shrink_folio_list() is used for throttling instead, which lacks all the niceties such as fairness, adaptive pausing, bandwidth proportional allocation and configurability. This function tests whether the vmscan currently in progress can assume that the normal dirty throttling mechanism is operational. Returns the number of pages on the given LRU list. zones to consider (use MAX_NR_ZONES - 1 for the whole LRU list) Attempt to remove a folio from its mapping. If the folio is dirty, under writeback or if someone else has a ref on it, removal will fail. The number of pages removed from the mapping. 0 if the folio could not be removed. The caller should have a single refcount on the folio and hold its lock. Folio to be returned to an LRU list. Add previously isolated folio to appropriate LRU list. The folio may still be unevictable for other reasons. lru_lock must not be held, interrupts must be enabled. Try to isolate a folio from its LRU list. Folio to isolate from its LRU list. Isolate a folio from an LRU list and adjust the vmstat statistic corresponding to whatever LRU list the folio was on. The folio will have its LRU flag cleared. If it was found on the active list, it will have the Active flag set. If it was found on the unevictable list, it will have the Unevictable flag set. These flags may need to be cleared by the caller before letting the page go.\n• None Must be called with an elevated refcount on the folio. This is a fundamental difference from isolate_lru_folios() (which is called without a stable reference).\n• None The lru_lock must not be held. true if the folio was removed from an LRU list. false if the folio was not on an LRU list. Checks folios for evictability, if an evictable folio is in the unevictable lru list, moves it to the appropriate evictable lru list. This function should be only used for lru folios. starting pageframe (must be aligned to start of a section) number of pages to remove (must be multiple of section size) alternative device page map or if default memmap is used Generic helper function to remove section mappings and sysfs entries for the section of the memory we are removing. Caller needs to make sure that pages are marked reserved and zones are adjust properly by calling offline_pages(). Offline a node if all memory sections and cpus of the node are removed. The caller must call lock_device_hotplug() to serialize hotplug and online/offline operations before this call. Remove memory if every memory block is offline physical address of the region to remove size of the region to remove The caller must call lock_device_hotplug() to serialize hotplug and online/offline operations before this call, as required by . mmu_iterval_read_begin()/mmu_iterval_read_retry() implement a collision-retry scheme similar to seqcount for the VA range under subscription. If the mm invokes invalidation during the critical section then mmu_interval_read_retry() will return true. This is useful to obtain shadow PTEs where teardown or setup of the SPTEs require a blocking context. The critical region formed by this can sleep, and the required ‘user_lock’ can also be a sleeping lock. The caller is required to provide a ‘user_lock’ to serialize both teardown and setup. The return value should be passed to mmu_interval_read_retry(). The mm to attach the notifier to Must not hold mmap_lock nor any other VM related lock when calling this registration function. Must also ensure mm_users can’t go down to zero while this runs to avoid races with mmu_notifier_release, so mm has to be current->mm or the mm should be pinned safely such as with get_task_mm(). If the mm is not current->mm, the mm_users pin should be released by calling mmput after mmu_notifier_register returns. mmu_notifier_unregister() or must be always called to unregister the notifier. While the caller has a mmu_notifier get the subscription->mm pointer will remain valid, and can be converted to an active mm pointer via mmget_not_zero(). Return the single struct mmu_notifier for the mm & ops The operations struct being subscribe with The mm to attach notifiers too This function either allocates a new mmu_notifier via ops->alloc_notifier(), or returns an already existing notifier on the list. The value of the ops pointer is used to determine when two notifiers are the same. Each call to mmu_notifier_get() must be paired with a call to . The caller must hold the write side of mm->mmap_lock. While the caller has a mmu_notifier get the mm pointer will remain valid, and can be converted to an active mm pointer via mmget_not_zero(). Release the reference on the notifier This function must be paired with each mmu_notifier_get(), it releases the reference obtained by the get. If this is the last reference then process to free the notifier will be run asynchronously. Unlike mmu_notifier_unregister() the get/put flow only calls ops->release when the mm_struct is destroyed. Instead free_notifier is always called to release any resources held by the user. As ops->release is not guaranteed to be called, the user must ensure that all sptes are dropped, and no new sptes can be established before is called. This function can be called from the ops->release callback, however the caller must still ensure it is called pairwise with mmu_notifier_get(). Modules calling this function must call in their __exit functions to ensure the async work is completed. Length of the range to monitor Interval notifier operations to be called on matching events This function subscribes the interval notifier for notifications from the mm. Upon return the ops related to mmu_interval_notifier will be called whenever an event that intersects with the given range occurs. Upon return the range_notifier may not be present in the interval tree yet. The caller must use the normal interval notifier read flow via to establish SPTEs for this range. This function must be paired with . It cannot be called from any ops callback. Once this returns ops callbacks are no longer running on other CPUs and will not be called in future. This function ensures that all outstanding async SRU work from is completed. After it returns any mmu_notifier_ops associated with an unused mmu_notifier will no longer be called. Before using the caller must ensure that all of its mmu_notifiers have been fully released via . Modules using the API should call this in their __exit function to avoid module unloading races. inserts a list of pages into the balloon page list. balloon device descriptor where we will insert a new page to Driver must call this function to properly enqueue balloon pages before definitively removing them from the guest system. number of pages that were enqueued. removes pages from balloon’s page list and returns a list of the pages. balloon device descriptor where we will grab a page from. pointer to the list of pages that would be returned to the caller. Driver must call this function to properly de-allocate a previous enlisted balloon pages before definitively releasing it back to the guest system. This function tries to remove n_req_pages from the ballooned pages and return them to the caller in the pages list. Note that this function may fail to dequeue some pages even if the balloon isn’t empty - since the page list can be temporarily empty due to compaction of isolated pages. number of pages that were added to the pages list. this is only safe if the mm semaphore is held when called."
    },
    {
        "link": "https://docs.kernel.org/core-api/mm-api.html",
        "document": "These flags provide hints about how mobile the page is. Pages with similar mobility are placed within the same pageblocks to minimise problems due to external fragmentation. (also a zone modifier) indicates that the page can be moved by page migration during memory compaction or can be reclaimed. is used for slab allocations that specify SLAB_RECLAIM_ACCOUNT and whose pages can be freed via shrinkers. indicates the caller intends to dirty the page. Where possible, these pages will be spread between local zones to avoid all the dirty pages being in one zone (fair zone allocation policy). forces the allocation to be satisfied from the requested node with no fallbacks or placement policy enforcements. causes the allocation to be accounted to kmemcg. causes slab allocation to have no object extension. indicates that the caller is high-priority and that granting the request is necessary before the system can make forward progress. For example creating an IO context to clean pages and requests from atomic context. allows access to all memory. This should only be used when the caller guarantees the allocation will allow more memory to be freed very shortly e.g. process exiting or swapping. Users either should be the MM or co-ordinating closely with the VM (e.g. swap over NFS). Users of this flag have to be extremely careful to not deplete the reserve completely and implement a throttling mechanism which controls the consumption of the reserve based on the amount of freed memory. Usage of a pre-allocated pool (e.g. mempool) should be always considered before using this flag. is used to explicitly forbid access to emergency reserves. This takes precedence over the flag if both are set. Please note that all the following flags are only applicable to sleepable allocations (e.g. and will ignore them). can call down to the low-level FS. Clearing the flag avoids the allocator recursing into the filesystem which might already be holding locks. indicates that the caller may enter direct reclaim. This flag can be cleared to avoid unnecessary delays when a fallback option is available. indicates that the caller wants to wake kswapd when the low watermark is reached and have it reclaim pages until the high watermark is reached. A caller may wish to clear this flag when fallback options are available and the reclaim is likely to disrupt the system. The canonical example is THP allocation where a fallback is cheap but reclaim/compaction may cause indirect stalls. is shorthand to allow/forbid both direct and kswapd reclaim. The default allocator behavior depends on the request size. We have a concept of so-called costly allocations (with order > ). !costly allocations are too essential to fail so they are implicitly non-failing by default (with some exceptions like OOM victims might fail so the caller still has to check for failures) while costly requests try to be not disruptive and back off even without invoking the OOM killer. The following three modifiers might be used to override some of these implicit rules. Please note that all of them must be used along with flag. : The VM implementation will try only very lightweight memory direct reclaim to get some memory under memory pressure (thus it can sleep). It will avoid disruptive actions like OOM killer. The caller must handle the failure which is quite likely to happen under heavy memory pressure. The flag is suitable when failure can easily be handled at small cost, such as reduced throughput. : The VM implementation will retry memory reclaim procedures that have previously failed if there is some indication that progress has been made elsewhere. It can wait for other tasks to attempt high-level approaches to freeing memory such as compaction (which removes fragmentation) and page-out. There is still a definite limit to the number of retries, but it is a larger limit than with . Allocations with this flag may fail, but only when there is genuinely little unused memory. While these allocations do not directly trigger the OOM killer, their failure indicates that the system is likely to need to use the OOM killer soon. The caller must handle failure, but can reasonably do so by failing a higher-level request, or completing it only in a much less efficient manner. If the allocation does fail, and the caller is in a position to free some non-essential memory, doing so could benefit the system as a whole. : The VM implementation _must_ retry infinitely: the caller cannot handle allocation failures. The allocation could block indefinitely but will never return with failure. Testing for failure is pointless. It _must_ be blockable and used together with __GFP_DIRECT_RECLAIM. It should _never_ be used in non-sleepable contexts. New users should be evaluated carefully (and the flag should be used only when there is no reasonable failure policy) but it is definitely preferable to use the flag rather than opencode endless loop around allocator. Allocating pages from the buddy with __GFP_NOFAIL and order > 1 is not supported. Please consider using kvmalloc() instead. Useful GFP flag combinations that are commonly used. It is recommended that subsystems start with one of these combinations and then set/clear flags as necessary. users can not sleep and need the allocation to succeed. A lower watermark is applied to allow access to “atomic reserves”. The current implementation doesn’t support NMI and few other strict non-preemptive contexts (e.g. raw_spin_lock). The same applies to . is typical for kernel-internal allocations. The caller requires or a lower zone for direct access but can direct reclaim. is the same as GFP_KERNEL, except the allocation is accounted to kmemcg. is for kernel allocations that should not stall for direct reclaim, start physical IO or use any filesystem callback. It is very likely to fail to allocate memory, even for very small allocations. will use direct reclaim to discard clean pages or slab pages that do not require the starting of any physical IO. Please try to avoid using this flag directly and instead use memalloc_noio_{save,restore} to mark the whole scope which cannot perform any IO with a short explanation why. All allocation requests will inherit GFP_NOIO implicitly. will use direct reclaim but will not use any filesystem interfaces. Please try to avoid using this flag directly and instead use memalloc_nofs_{save,restore} to mark the whole scope which cannot/shouldn’t recurse into the FS layer with a short explanation why. All allocation requests will inherit GFP_NOFS implicitly. is for userspace allocations that also need to be directly accessibly by the kernel or hardware. It is typically used by hardware for buffers that are mapped to userspace (e.g. graphics) that hardware still must DMA to. cpuset limits are enforced for these allocations. exists for historical reasons and should be avoided where possible. The flags indicates that the caller requires that the lowest zone be used ( or 16M on x86-64). Ideally, this would be removed but it would require careful auditing as some users really require it and others use the flag to avoid lowmem reserves in and treat the lowest zone as a type of emergency reserve. is similar to except that the caller requires a 32-bit address. Note that kmalloc(..., GFP_DMA32) does not return DMA32 memory because the DMA32 kmalloc cache array is not implemented. (Reason: there is no such user in kernel). is for userspace allocations that may be mapped to userspace, do not need to be directly accessible by the kernel but that cannot move once in use. An example may be a hardware allocation that maps data directly into userspace but has no addressing limitations. is for userspace allocations that the kernel does not need direct access to but can use when access is required. They are expected to be movable via page reclaim or page migration. Typically, pages on the LRU would also be allocated with . and are used for THP allocations. They are compound allocations that will generally fail quickly if memory is not available and will not wake kswapd/kcompactd on failure. The _LIGHT version does not attempt reclaim/compaction at all and is by default used in page fault path, while the non-light is used by khugepaged.\n\nSufficiently large objects are aligned on cache line boundary. For object size smaller than a half of cache line size, the alignment is on the half of cache line size. In general, if object size is smaller than 1/2^n of cache line size, the alignment is adjusted to 1/2^n. If explicit alignment is also requested by the respective field, the greater of both is alignments is applied. This delays freeing the SLAB page by a grace period, it does _NOT_ delay object freeing. This means that if you do that memory location is free to be reused at any time. Thus it may be possible to see another object there in the same RCU grace period. This feature only ensures the memory location backing the object stays valid, the trick to using this is relying on an independent object validation pass. Something like: begin: rcu_read_lock(); obj = lockless_lookup(key); if (obj) { if (!try_get_ref(obj)) // might fail for free objects rcu_read_unlock(); goto begin; if (obj->key != key) { // not the object we expected put_ref(obj); rcu_read_unlock(); goto begin; } } rcu_read_unlock(); This is useful if we need to approach a kernel structure obliquely, from its address obtained without the usual locking. We can lock the structure to stabilize it and check it’s still at the given address, only if we can be sure that the memory has not been meanwhile reused for some other kind of object (which our subsystem’s lock might corrupt). rcu_read_lock before reading the address, then rcu_read_unlock after taking the spinlock within the structure expected at that address. Note that it is not possible to acquire a lock within a structure allocated with SLAB_TYPESAFE_BY_RCU without first acquiring a reference as described above. The reason is that SLAB_TYPESAFE_BY_RCU pages are not zeroed before being given to the slab, which means that any locks must be initialized after each and every kmem_struct_alloc(). Alternatively, make the ctor passed to initialize the locks at page-allocation time, as is done in __i915_request_ctor(), sighand_ctor(), and anon_vma_ctor(). Such a ctor permits readers to safely acquire those ctor-initialized locks under protection. All object allocations from this cache will be memcg accounted, regardless of __GFP_ACCOUNT being or not being passed to individual allocations. Use this flag for caches that have an associated shrinker. As a result, slab pages are allocated with __GFP_RECLAIMABLE, which affects grouping pages by mobility, and are accounted in SReclaimable counter in /proc/meminfo The required alignment for the objects. is a valid offset, when usersize is non- means no usercopy region is specified. Custom offset for the free pointer in caches By default caches place the free pointer outside of the object. This might cause the object to grow in size. Cache creators that have a reason to avoid this can specify a custom free pointer offset in their struct where the free pointer will be placed. Note that placing the free pointer inside the object requires the caller to ensure that no fields are invalidated that are required to guard against object recycling (See for details). Using as a value for freeptr_offset is valid. If freeptr_offset is specified, must be set . Note that ctor currently isn’t supported with custom free pointers as a ctor requires an external free pointer. Whether a freeptr_offset is used. The constructor is invoked for each object in a newly allocated slab page. It is the cache user’s responsibility to free object in the same state as after calling the constructor, or deal appropriately with any differences between a freshly constructed and a reallocated object. Any uninitialized fields of the structure are interpreted as unused. The exception is freeptr_offset where is a valid value, so use_freeptr_offset must be also set to in order to interpret the field as used. For useroffset is also valid, but only with non- usersize. When args is passed to , it is equivalent to all fields unused. Create a kmem cache with a region suitable for copying to userspace. A string which is used in /proc/slabinfo to identify this cache. The size of objects to be created in this cache. The required alignment for the objects. A constructor for the objects, or . This is a legacy wrapper, new code should use either KMEM_CACHE_USERCOPY() if whitelisting a single field is sufficient, or with the necessary parameters passed via the args parameter (see ) a pointer to the cache on success, NULL on failure. A string which is used in /proc/slabinfo to identify this cache. The size of objects to be created in this cache. Optional arguments, see . Passing means defaults will be used for all the arguments. This is currently implemented as a macro using to call either the new variant of the function, or a legacy one. The new variant has 4 parameters: See which implements this. The align and ctor parameters map to the respective fields of Cannot be called within a interrupt, but can be interrupted. a pointer to the cache on success, NULL on failure. This should not be used for writing beyond the originally requested allocation size. Either use krealloc() or round up the allocation size with prior to allocation. If this is used to access beyond the originally requested allocation size, UBSAN_BOUNDS and/or FORTIFY_SOURCE may trip, since they only know about the originally allocated size via the __alloc_size attribute. The cache to allocate from. Allocate an object from this cache. See kmem_cache_zalloc() for a shortcut of adding __GFP_ZERO to flags. pointer to the new object or in case of error address of the slab object to memcg charge kmem_cache_charge allows charging a slab object to the current memcg, primarily in cases where charging at allocation time might not be possible because the target memcg is not known (i.e. softirq context) The objp should be pointer returned by the slab allocator functions like kmalloc (with __GFP_ACCOUNT in flags) or kmem_cache_alloc. The memcg charge behavior can be controlled through gfpflags parameter, which affects how the necessary internal metadata can be allocated. Including __GFP_NOFAIL denotes that overcharging is requested instead of failure, but is not applied for the internal metadata allocation. There are several cases where it will return true even if the charging was not done: More specifically:\n• None For slab objects from KMALLOC_NORMAL caches - allocated by without __GFP_ACCOUNT true if charge was successful otherwise false. how many bytes of memory are required. kmalloc is the normal method of allocating memory for objects smaller than page size in the kernel. The allocated object address is aligned to at least ARCH_KMALLOC_MINALIGN bytes. For size of power of two bytes, the alignment is also guaranteed to be at least to the size. For other sizes, the alignment is guaranteed to be at least the largest power-of-two divisor of size. The flags argument may be one of the GFP flags defined at include/linux/gfp_types.h and described at Documentation/core-api/mm-api.rst The recommended usage of the flags is described at Documentation/core-api/memory-allocation.rst Below is a brief outline of the most useful GFP flags Allocation will not sleep. May use emergency pools. Also it is possible to set different flags by OR’ing in one or more of the following additional flags: Zero the allocated memory before returning. Also see . This allocation has high priority and may use emergency pools. Indicate that this allocation is in no way allowed to fail (think twice before using). If memory is not immediately available, then give up at once. Try really hard to succeed the allocation but fail eventually. the type of memory to allocate (see kmalloc). pointer to the memory chunk to reallocate new number of elements to alloc new size of a single member of the array the type of memory to allocate (see kmalloc) If __GFP_ZERO logic is requested, callers must ensure that, starting with the initial memory allocation, every subsequent call to this API for the same memory allocation is flagged with __GFP_ZERO. Otherwise, it is possible that __GFP_ZERO is not fully honored by this API. See krealloc_noprof() for further details. In any case, the contents of the object pointed to are preserved up to the lesser of the new and old sizes. allocate memory for an array. The memory is set to zero. the type of memory to allocate (see kmalloc). allocate memory. The memory is set to zero. how many bytes of memory are required. the type of memory to allocate (see kmalloc). Report allocation bucket size for the given size Number of bytes to round up from. This returns the number of bytes that would be available in a allocation of size bytes. For example, a 126 byte request would be rounded up to the next sized kmalloc bucket, 128 bytes. (This is strictly for the general-purpose -based allocations, and is not for the pre-sized -based allocations.) Use this to the full bucket size ahead of time instead of using to query the size after an allocation. The cache the allocation was from. Free an object which was previously allocated from this cache. If object is NULL, no operation is performed. A string which is used in /proc/slabinfo to identify this cache. The size of objects to be created in this cache. Additional arguments for the cache creation (see ). See the desriptions of individual flags. The common ones are listed in the description below. Not to be called directly, use the wrapper with the same parameters. - Slab page (not individual objects) freeing delayed by a grace period - see the full description before using. Cannot be called within a interrupt, but can be interrupted. a pointer to the cache on success, NULL on failure. Create a set of caches that handle dynamic sized allocations via kmem_buckets_alloc() A prefix string which is used in /proc/slabinfo to identify this cache. The individual caches with have their sizes as the suffix. Starting offset within an allocation that may be copied to/from userspace. How many bytes, starting at useroffset, may be copied to/from userspace. A constructor for the objects, run when new allocations are made. Cannot be called within an interrupt, but can be interrupted. a pointer to the cache on success, NULL on failure. When CONFIG_SLAB_BUCKETS is not enabled, ZERO_SIZE_PTR is returned, and subsequent calls to kmem_buckets_alloc() will fall back to . (i.e. callers only need to check for NULL on failure.) Releases as many slabs as possible for a cache. To help debugging, a zero exit status indicates all slabs were released. if all slabs were released, non-zero otherwise slab object for which to find provenance information. This function uses , so that the caller is expected to have printed out whatever preamble is appropriate. The provenance information depends on the type of object and on how much debugging is enabled. For a slab-cache object, the fact that it is a slab object is printed, and, if available, the slab name, return address, and stack trace from the allocation and last free path of that object. if the pointer is to a not-yet-freed object from or , either or if the pointer is to an already-freed object, and otherwise. The memory of the object p points to is zeroed before freed. If p is , does nothing. this function zeroes the whole allocated buffer which can be a good deal bigger than the requested buffer size passed to . So be careful when using this function in performance sensitive code. Note that a single argument of kvfree_rcu() call has a slow path that triggers following by freeing a pointer. It is done before the return from the function. Therefore for any single-argument call that will result in a to a cache that is to be destroyed during module exit, it is developer’s responsibility to ensure that all such calls have returned before the call to kmem_cache_destroy(). Function calls kfree only if x is not in .rodata section. kvfree frees memory allocated by any of vmalloc(), or kvmalloc(). It is slightly more efficient to use or if you are certain that you know which one to use.\n\nCall writepages on the mapping using the provided wbc to control the writeout. This is a non-integrity writeback helper, to start writing back folios for the indicated range. This is a mostly non-blocking flush. Not suitable for data-integrity purposes - I/O may not be started against all dirty pages. address space within which to check offset in bytes where the range starts offset in bytes where the range ends (inclusive) Find at least one page in the range supplied, usually used to check if direct writing in this range will trigger a writeback. if at least one page exists in the specified range, otherwise. offset in bytes where the range starts offset in bytes where the range ends (inclusive) Walk the list of under-writeback pages of the given address space in the given range and wait for all of them. Check error status of the address space and return it. Since the error status of the address space is cleared by this function, callers are responsible for checking the return value and handling and/or reporting the error. offset in bytes where the range starts offset in bytes where the range ends (inclusive) Walk the list of under-writeback pages of the given address space in the given range and wait for all of them. Unlike , this function does not clear error status of the address space. Use this function if callers don’t handle errors themselves. Expected call sites are system-wide / filesystem-wide data flushers: e.g. sync(2), fsfreeze(8) file pointing to address space structure to wait for offset in bytes where the range starts offset in bytes where the range ends (inclusive) Walk the list of under-writeback pages of the address space that file refers to, in the given range and wait for all of them. Check error status of the address space vs. the file->f_wb_err cursor and return it. Since the error status of the file is advanced by this function, callers are responsible for checking the return value and handling and/or reporting the error. error status of the address space vs. the file->f_wb_err cursor. Walk the list of under-writeback pages of the given address space and wait for all of them. Unlike filemap_fdatawait(), this function does not clear error status of the address space. Use this function if callers don’t handle errors themselves. Expected call sites are system-wide / filesystem-wide data flushers: e.g. sync(2), fsfreeze(8) the address_space for the pages offset in bytes where the range starts offset in bytes where the range ends (inclusive) Write out and wait upon file offsets lstart->lend, inclusive. Note that lend is inclusive (describes the last byte to be written) so that this function can be used to write to the very end-of-file (end = -1). report wb error (if any) that was previously and advance wb_err to current one on which the error is being reported When userland calls fsync (or something like nfsd does the equivalent), we want to report any writeback errors that occurred since the last fsync (or since the file was opened if there haven’t been any). Grab the wb_err from the mapping. If it matches what we have in the file, then just quickly return 0. The file is all caught up. If it doesn’t match, then take the mapping value, set the “seen” flag in it and try to swap it into place. If it works, or another task beat us to it with the new value, then update the f_wb_err and return the error portion. The error at this point must be reported via proper channels (a’la fsync, or NFS COMMIT operation, etc.). While we handle mapping->wb_err with atomic operations, the f_wb_err value is protected by the f_lock since we must ensure that it reflects the latest value swapped in for this file descriptor. offset in bytes where the range starts offset in bytes where the range ends (inclusive) Write out and wait upon file offsets lstart->lend, inclusive. Note that lend is inclusive (describes the last byte to be written) so that this function can be used to write to the very end-of-file (end = -1). After writing out and waiting on the data, we check and advance the f_wb_err cursor to the latest value, and return any errors detected there. replace a pagecache folio with a new one This function replaces a folio in the pagecache with a new one. On success it acquires the pagecache reference for the new folio and drops it for the old folio. Both the old and new folios must be locked. This function does not add the new folio to the LRU, the caller must do that. The remove + add is atomic. This function cannot fail. Unlocks the folio and wakes up any thread sleeping on the page lock. May be called from interrupt or process context. May not be called from NMI context. When all reads against a folio have completed, filesystems should call this function to let the pagecache know that no more reads are outstanding. This will unlock the folio and wake up any thread sleeping on the lock. The folio will also be marked uptodate if all reads succeeded. May be called from interrupt or process context. May not be called from NMI context. Clear the PG_private_2 bit on a folio and wake up any sleepers waiting for it. The folio reference held for PG_private_2 being set is released. This is, for example, used when a netfs folio is being written to a local disk cache, thereby allowing writes to the cache for the same folio to be serialised. Wait for PG_private_2 to be cleared on a folio. Wait for PG_private_2 to be cleared on a folio. Wait for PG_private_2 to be cleared on a folio. Wait for PG_private_2 to be cleared on a folio or until a fatal signal is received by the calling task. The folio must actually be under writeback. May be called from process or interrupt context. Get a lock on the folio, assuming we need to sleep to get it. Find the next gap in the page cache. Search the range [index, min(index + max_scan - 1, ULONG_MAX)] for the gap with the lowest index. This function may be called under the rcu_read_lock. However, this will not atomically search a snapshot of the cache at a single point in time. For example, if a gap is created at index 5, then subsequently a gap is created at index 10, page_cache_next_miss covering both indices may return 10 if called under the rcu_read_lock. The index of the gap if found, otherwise an index outside the range specified (in which case ‘return - index >= max_scan’ will be true). In the rare case of index wrap-around, 0 will be returned. Find the previous gap in the page cache. Search the range [max(index - max_scan + 1, 0), index] for the gap with the highest index. This function may be called under the rcu_read_lock. However, this will not atomically search a snapshot of the cache at a single point in time. For example, if a gap is created at index 10, then subsequently a gap is created at index 5, covering both indices may return 5 if called under the rcu_read_lock. The index of the gap if found, otherwise an index outside the range specified (in which case ‘index - return >= max_scan’ will be true). In the rare case of wrap-around, ULONG_MAX will be returned. Find and get a reference to a folio. flags modify how the folio is returned. Memory allocation flags to use if is specified. Looks up the page cache entry at mapping & index. If or are specified then the function may sleep even if the flags specified for are atomic. If this function returns a folio, it is returned with an increased refcount. The found folio or an otherwise. Search for and return a batch of folios in the mapping starting at index start and up to index end (inclusive). The folios are returned in fbatch with an elevated reference count. The number of folios which were found. We also update start to index the next folio for the traversal. works exactly like , except the returned folios are guaranteed to be contiguous. This may not return all contiguous folios if the batch gets filled up. The number of folios found. Also update start to be positioned for traversal of the next folio. The first folio may start before start; if it does, it will contain start. The final folio may extend beyond end; if it does, it will contain end. The folios have ascending indices. There may be gaps between the folios if there are indices which have no folio in the page cache. If folios are added to or removed from the page cache while this is running, they may or may not be found by this call. Only returns folios that are tagged with tag. The number of folios found. Also update start to index the next folio for traversal. Number of bytes already read by the caller. Copies data from the page cache. If the data is not currently present, uses the readahead and read_folio address_space operations to fetch it. Total number of bytes copied, including those already read by the caller. If an error happens before any bytes are copied, returns a negative error number. This is the “read_iter()” routine for all filesystems that can use the page cache directly. The IOCB_NOWAIT flag in iocb->ki_flags indicates that -EAGAIN shall be returned when no data can be read without waiting for I/O requests to complete; it doesn’t prevent readahead. The IOCB_NOIO flag in iocb->ki_flags indicates that no new I/O requests shall be made for the read or for readahead. When no data can be read, -EAGAIN shall be returned. When readahead would be triggered, a partial, possibly empty read shall be returned.\n• None number of bytes copied, even for partial reads\n• None negative error code (or 0 if IOCB_NOIO) if nothing was read Pointer to the file position to read from This function gets folios from a file’s pagecache and splices them into the pipe. Readahead will be called as necessary to fill more folios. This may be used for blockdevs also. On success, the number of bytes read will be returned and *ppos will be updated if appropriate; 0 will be returned if there is no more data to be read; -EAGAIN will be returned if the pipe had no space, and some other negative error code will be returned on error. A short read may occur if the pipe has insufficient space, we reach the end of the data or we hit a hole. struct vm_fault containing details of the fault is invoked via the vma operations vector for a mapped memory region to read in file data during a page fault. The goto’s are kind of ugly, but this streamlines the normal case of having it in the page cache, and handles the special cases reasonably without having a lot of duplicated code. vma->vm_mm->mmap_lock must be held on entry. If our return value has VM_FAULT_RETRY set, it’s because the mmap_lock may be dropped before doing I/O or by lock_folio_maybe_drop_mmap(). If our return value does not have VM_FAULT_RETRY set, the mmap_lock has not been released. We never return with VM_FAULT_RETRY and a bit from VM_FAULT_ERROR set. Read into page cache, fill it if needed. The address_space to read from. Function to perform the read, or NULL to use aops->read_folio(). Passed to filler function, may be NULL if not required. Read one page into the page cache. If it succeeds, the folio returned will contain index, but it may not be the first page of the folio. If the filler function returns an error, it will be returned to the caller. May sleep. Expects mapping->invalidate_lock to be held. An uptodate folio on success, on failure. Read into page cache, using specified allocation flags. The address_space for the folio. The index that the allocated folio will contain. The page allocator flags to use if allocating. This is the same as “read_cache_folio(mapping, index, NULL, NULL)”, but with any new memory allocations done using the specified allocation flags. The most likely error from this function is EIO, but ENOMEM is possible and so is EINTR. If ->read_folio returns another error, that will be returned to the caller. The function expects mapping->invalidate_lock to be already held. read into page cache, using specified page allocation flags. the page allocator flags to use if allocating This is the same as “read_mapping_page(mapping, index, NULL)”, but with any new page allocations done using the specified allocation flags. If the page does not get brought uptodate, return -EIO. The function expects mapping->invalidate_lock to be already held. up to date page on success, on failure. This function does all the work needed for actually writing data to a file. It does all basic checks, removes SUID from the file, updates modification times and calls proper subroutines depending on whether we do direct IO or a standard buffered write. It expects i_rwsem to be grabbed unless we work on a block device or similar object which does not need locking at all. This function does not take care of syncing data in case of O_SYNC write. A caller has to handle it. This is mainly due to the fact that we want to avoid syncing under i_rwsem.\n• None number of bytes written, even for truncated writes\n• None negative error code if no data has been written at all This is a wrapper around to be used by most filesystems. It takes care of syncing the file in case of O_SYNC file and acquires i_rwsem as needed.\n• None negative error code if no data has been written at all of failed for a synchronous write\n• None number of bytes written, even for truncated writes The folio which the kernel is trying to free. The address_space is trying to release any data attached to a folio (presumably at folio->private). This will also be called if the private_2 flag is set on a page, indicating that the folio has other metadata associated with it. The gfp argument specifies whether I/O may be performed to release this page (__GFP_IO), and whether the call may block (__GFP_RECLAIM & __GFP_FS). if the release was successful, otherwise . Set to write back rather than simply invalidate. Last byte in range (inclusive), or LLONG_MAX for everything from start onwards. Invalidate all the folios on an inode that contribute to the specified range, possibly writing them back first. Whilst the operation is undertaken, the invalidate lock is held to prevent new folios from being installed. Readahead is used to read content into the page cache before it is explicitly requested by the application. Readahead only ever attempts to read folios that are not yet in the page cache. If a folio is present but not up-to-date, readahead will not try to read it. In that case a simple ->read_folio() will be requested. Readahead is triggered when an application read request (whether a system call or a page fault) finds that the requested folio is not in the page cache, or that it is in the page cache and has the readahead flag set. This flag indicates that the folio was read as part of a previous readahead request and now that it has been accessed, it is time for the next readahead. Each readahead request is partly synchronous read, and partly async readahead. This is reflected in the which contains ->size being the total number of pages, and ->async_size which is the number of pages in the async section. The readahead flag will be set on the first folio in this async section to trigger a subsequent readahead. Once a series of sequential reads has been established, there should be no need for a synchronous component and all readahead request will be fully asynchronous. When either of the triggers causes a readahead, three numbers need to be determined: the start of the region to read, the size of the region, and the size of the async tail. The start of the region is simply the first page address at or after the accessed address, which is not currently populated in the page cache. This is found with a simple search in the page cache. The size of the async tail is determined by subtracting the size that was explicitly requested from the determined request size, unless this would be less than zero - then zero is used. NOTE THIS CALCULATION IS WRONG WHEN THE START OF THE REGION IS NOT THE ACCESSED PAGE. ALSO THIS CALCULATION IS NOT USED CONSISTENTLY. The size of the region is normally determined from the size of the previous readahead which loaded the preceding pages. This may be discovered from the for simple sequential reads, or from examining the state of the page cache when multiple sequential reads are interleaved. Specifically: where the readahead was triggered by the readahead flag, the size of the previous readahead is assumed to be the number of pages from the triggering page to the start of the new readahead. In these cases, the size of the previous readahead is scaled, often doubled, for the new readahead, though see get_next_ra_size() for details. If the size of the previous read cannot be determined, the number of preceding pages in the page cache is used to estimate the size of a previous read. This estimate could easily be misled by random reads being coincidentally adjacent, so it is ignored unless it is larger than the current request, and it is not scaled up, unless it is at the start of file. In general readahead is accelerated at the start of the file, as reads from there are often sequential. There are other minor adjustments to the readahead size in various special cases and these are best discovered by reading the code. The above calculation, based on the previous readahead size, determines the size of the readahead, to which any requested read size may be added. Readahead requests are sent to the filesystem using the ->readahead() address space operation, for which is a canonical implementation. ->readahead() should normally initiate reads on all folios, but may fail to read any or all folios without causing an I/O error. The page cache reading code will issue a ->read_folio() request for any folio which ->readahead() did not read, and only an error from this will be final. ->readahead() will generally call repeatedly to get each folio from those prepared for readahead. It may fail to read a folio by:\n• None not calling sufficiently many times, effectively ignoring some folios, as might be appropriate if the path to storage is congested.\n• None failing to actually submit a read request for a given folio, possibly due to insufficient resources, or\n• None getting an error during subsequent processing of a request. In the last two cases, the folio should be unlocked by the filesystem to indicate that the read attempt has failed. In the first case the folio will be unlocked by the VFS. Those folios not in the final of the request should be considered to be important and ->readahead() should not fail them due to congestion or temporary resource unavailability, but should wait for necessary resources (e.g. memory or indexing information) to become available. Folios in the final may be considered less urgent and failure to read them is more acceptable. In this case it is best to use filemap_remove_folio() to remove the folios from the page cache as is automatically done for folios that were not fetched with . This will allow a subsequent synchronous readahead request to try them again. If they are left in the page cache, then they will be read individually using ->read_folio() which may be less efficient. The number of pages to read. Where to start the next readahead. This function is for filesystems to call when they want to start readahead beyond a file’s stated i_size. This is almost certainly not the function you want to call. Use or instead. File is referenced by caller. Mutexes may be held by caller. May sleep, but will not reenter filesystem to reclaim memory. The request to be expanded The revised size of the request Attempt to expand a readahead request outwards from the current size to the specified size by inserting locked pages before and after the current window to increase the size to the new window. This may involve the insertion of THPs, in which case the window may get expanded even beyond what was requested. The algorithm will stop if it encounters a conflicting page already in the pagecache and leave a smaller expansion than requested. The caller must check for this by examining the revised ractl object for a different expansion than was requested. Processes which are dirtying memory should call in here once for each page which was newly dirtied. The function will periodically check the system’s dirty state and will initiate writeback if needed. If flags contains BDP_ASYNC, it may return -EAGAIN to indicate that memory is out of balance and the caller must wait for I/O to complete. Otherwise, it will return 0 to indicate that either memory was already in balance, or it was able to sleep until the amount of dirty memory returned to balance. Processes which are dirtying memory should call in here once for each page which was newly dirtied. The function will periodically check the system’s dirty state and will initiate writeback if needed. Once we’re over the dirty memory limit we decrease the ratelimiting by a lot, to prevent individual processes from overshooting the limit by (ratelimit_pages) each. tag pages to be written by writeback This function scans the page range from start to end (inclusive) and tags all pages that have DIRTY tag set with a special TOWRITE tag. The caller can then use the TOWRITE tag to identify pages eligible for writeback. This mechanism is used to avoid livelocking of writeback by a process steadily creating new dirty pages in the file (thus it is important for this function to be quick so that it can tag pages faster than a dirtying process can create them). in-out pointer for writeback errors (see below) This function returns the next folio for the writeback operation described by wbc on mapping and should be called in a while loop in the ->writepages implementation. To start the writeback operation, is passed in the folio argument, and for every subsequent iteration the folio returned previously should be passed back in. If there was an error in the per-folio writeback inside the loop, error should be set to the error value. Once the writeback described in wbc has finished, this function will return and if there was an error in any iteration restore it to error. callers should not manually break out of the loop using break or goto but must keep calling until it returns . the folio to write or if the loop is done. walk the list of dirty pages of the given address space and write all of them. subtract the number of written pages from *wbc->nr_to_write please use instead. Mark a folio dirty for filesystems which do not use buffer_heads. Folio to be marked as dirty. Filesystems which do not use buffer heads should call this function from their dirty_folio address space operation. It ignores the contents of folio_get_private(), so if the filesystem marks individual blocks as dirty, the filesystem should handle that itself. This is also sometimes used by filesystems which use buffer_heads when a single buffer is being dirtied: we want to set the folio dirty in that case, but not all the buffers. This is a “bottom-up” dirtying, whereas is a “top-down” dirtying. The caller must ensure this doesn’t race with truncation. Most will simply hold the folio lock, but e.g. zap_pte_range() calls with the folio mapped and the pte lock held, which also locks out truncation. When a writepage implementation decides that it doesn’t want to write folio for some reason, it should call this function, unlock folio and return 0. True if we redirtied the folio. False if someone else dirtied it first. The folio may not be truncated while this function is running. Holding the folio lock is sufficient to prevent truncation, but some callers cannot acquire a sleeping lock. These callers instead hold the page table lock for a page table which contains at least one page in this folio. Truncation will block on the page table lock as it unmaps pages before removing the folio from its mapping. True if the folio was newly dirtied, false if it was already dirty. If the folio is currently being written back to storage, wait for the I/O to complete. Sleeps. Must be called in process context and with no spinlocks held. Caller should hold a reference on the folio. If the folio is not locked, writeback may start again after writeback has finished. If the folio is currently being written back to storage, wait for the I/O to complete or a fatal signal to arrive. Sleeps. Must be called in process context and with no spinlocks held. Caller should hold a reference on the folio. If the folio is not locked, writeback may start again after writeback has finished. 0 on success, -EINTR if we get a fatal signal while waiting. wait for writeback to finish, if necessary. This function determines if the given folio is related to a backing device that requires folio contents to be held stable during writeback. If so, then it will wait for any pending writeback to complete. Sleeps. Must be called in process context and with no spinlocks held. Caller should hold a reference on the folio. If the folio is not locked, writeback may start again after writeback has finished. Invalidate part or all of a folio. The folio which is affected. start of the range to invalidate length of the range to invalidate is called when all or part of the folio has become invalidated by a truncate operation. does not have to release all buffers, but it must ensure that no dirty buffer is left outside offset and that no I/O is underway against any of the blocks which are outside the truncation point. Because the caller is about to free (and possibly reuse) those blocks on-disk. truncate range of pages specified by start & end byte offsets offset from which to truncate offset to which to truncate (inclusive) Truncate the page cache, removing the pages that are between specified offsets (and zeroing out partial pages if lstart or lend + 1 is not page aligned). Truncate takes two passes - the first pass is nonblocking. It will not block on page locks and it will not block on writeback. The second pass will wait. This is to prevent as much IO as possible in the affected region. The first pass will remove most pages, so the search cost of the second pass is low. We pass down the cache-hot hint to the page freeing code. Even if the mapping is large, it is probably the case that the final pages are the most recently touched, and freeing happens in ascending file offset order. Note that since ->invalidate_folio() accepts range to invalidate truncate_inode_pages_range is able to handle cases where lend + 1 is not page aligned properly. truncate all the pages from an offset offset from which to truncate Called under (and serialised by) inode->i_rwsem and mapping->invalidate_lock. When this function returns, there can be a page in the process of deletion (inside __filemap_remove_folio()) in the specified range. Thus mapping->nrpages can be non-zero when this function returns even after truncation of the whole mapping. Called under (and serialized by) inode->i_rwsem. Filesystems have to use this in the .evict_inode path to inform the VM that this is the final truncate and the inode is going away. Invalidate all clean, unlocked cache of one inode the address_space which holds the cache to invalidate the offset ‘from’ which to invalidate the offset ‘to’ which to invalidate (inclusive) This function removes pages that are clean, unmapped and unlocked, as well as shadow entries. It will not block on IO activity. If you want to remove all the pages of one inode, regardless of their use and writeback state, use . The number of indices that had their contents invalidated remove range of pages from an address_space the page offset ‘from’ which to invalidate the page offset ‘to’ which to invalidate (inclusive) Any pages which are found to be mapped into pagetables are unmapped prior to invalidation. -EBUSY if any pages could not be invalidated. remove all pages from an address_space Any pages which are found to be mapped into pagetables are unmapped prior to invalidation. -EBUSY if any pages could not be invalidated. unmap and remove pagecache that has been truncated inode’s new i_size must already be written before truncate_pagecache is called. This function should typically be called before the filesystem releases resources associated with the freed range (eg. deallocates blocks). This way, pagecache will always stay logically coherent with on-disk format, and the filesystem would not have to deal with situations such as writepage being called for a page that has already had its underlying blocks deallocated. update inode and pagecache for a new file size truncate_setsize updates i_size and performs pagecache truncation (if necessary) to newsize. It will be typically be called from the filesystem’s setattr function when ATTR_SIZE is passed in. Must be called with a lock serializing truncates and writes (generally i_rwsem but e.g. xfs uses a different lock) and before all filesystem specific block truncation has been performed. inode for which i_size was extended Handle extension of inode size either caused by extending truncate or by write starting after current i_size. We mark the page straddling current i_size RO so that page_mkwrite() is called on the first write access to the page. The filesystem will update its per-block information before user writes to the page via mmap after the i_size has been changed. The function must be called after i_size is updated so that page fault coming after we unlock the folio will already see the new i_size. The function must be called while we still hold i_rwsem - this not only makes sure i_size is stable but also that userspace cannot observe new i_size value before we are prepared to store mmap writes at new inode size. unmap and remove pagecache that is hole-punched offset of last byte of hole This function should typically be called before the filesystem releases resources associated with the freed range (eg. deallocates blocks). This way, pagecache will always stay logically coherent with on-disk format, and the filesystem would not have to deal with situations such as writepage being called for a page that has already had its underlying blocks deallocated. mapping in which to set writeback error error to be set in mapping When writeback fails in some way, we must record that error so that userspace can be informed when fsync and the like are called. We endeavor to report errors on any file that was open at the time of the error. Some internal callers also need to know when writeback errors have occurred. When a writeback error occurs, most filesystems will want to call filemap_set_wb_err to record the error in the mapping so that it will be automatically reported whenever fsync is called on the file. has an error occurred since the mark was sampled? Grab the errseq_t value from the mapping, and see if it has changed “since” the given value was sampled. If it has then report the latest error set, otherwise return 0. sample the current errseq_t to test for later errors Writeback errors are always reported relative to a particular sample point in the past. This function provides those sample points. sample the current errseq_t to test for later errors Grab the most current superblock-level errseq_t value for the given . the mapping in which an error should be set the error to set in the mapping When writeback fails in some way, we must record that error so that userspace can be informed when fsync and the like are called. We endeavor to report errors on any file that was open at the time of the error. Some internal callers also need to know when writeback errors have occurred. When a writeback error occurs, most filesystems will want to call mapping_set_error to record the error in the mapping so that it can be reported when the application calls fsync(2). The address space of the file. The filesystem should call this function in its inode constructor to indicate that the VFS can use large folios to cache the contents of the file. This should not be called while the inode is active as it is non-atomic. The index of a folio must be naturally aligned. If you are adding a new folio to the page cache and need to know what index to give it, call this function. Find the mapping this folio belongs to. For folios which are in the page cache, return the mapping that this page belongs to. Folios in the swap cache return the mapping of the swap file or swap device where the data is stored. This is different from the mapping returned by . The only reason to use it is if, like NFS, you return 0 from ->activate_swapfile. Do not call this for folios which aren’t in the page cache or swap cache. Find the file mapping this folio belongs to. For folios which are in the page cache, return the mapping that this page belongs to. Anonymous folios return NULL, even if they’re in the swap cache. Other kinds of folio also return NULL. This is ONLY used by architecture cache flushing code. If you aren’t writing cache flushing code, you want either or . Get the host inode for this folio. For folios which are in the page cache, return the inode that this folio belongs to. Do not call this for folios which aren’t in the page cache. Attaching private data to a folio increments the page’s reference count. The data must be detached before the folio will be freed. Folio to change the data on. Data to set on the folio. Change the private data attached to a folio and return the old data. The page must previously have had data attached and the data must be detached before the folio will be freed. Data that was previously attached to the folio. Removes the data that was previously attached to the folio and decrements the refcount on the page. Data that was attached to the folio. Flags for getting folios from the page cache. Most users of the page cache will not need to use these flags; there are convenience functions such as and . For users which need more control over exactly what is done with the folios, these flags to are available.\n• None - The folio will be marked accessed.\n• None - If no folio is present then a new folio is allocated, added to the page cache and the VM’s LRU list. The folio is returned locked.\n• None - The caller wants to do its own locking dance if the folio is already in cache. If the folio was allocated, unlock it before returning so the caller can do the same dance.\n• None - The folio will be written to by the caller.\n• None - __GFP_FS will get cleared in gfp.\n• None - Wait for the folio to be stable (finished writeback)\n• None - The flags to use in a filesystem write_begin() implementation. The suggested size of the folio to create. The caller of can use this to suggest a preferred size for the folio that is created. If there is already a folio at the index, it will be returned, no matter what its size. If a folio is freshly created, it may be of a different size than requested due to alignment constraints, memory pressure, or the presence of other folios at nearby indices. Looks up the page cache entry at mapping & index. If a folio is present, it is returned with an increased refcount. A folio or ERR_PTR(-ENOENT) if there is no folio in the cache for this index. Will not return a shadow, swap or DAX entry. Looks up the page cache entry at mapping & index. If a folio is present, it is returned locked with an increased refcount. A folio or ERR_PTR(-ENOENT) if there is no folio in the cache for this index. Will not return a shadow, swap or DAX entry. Looks up the page cache entry at mapping & index. If no folio is found, a new folio is created. The folio is locked, marked as accessed, and returned. A found or created folio. ERR_PTR(-ENOMEM) if no folio is found and failed to create a folio. Looks up the page cache slot at mapping & offset. If there is a page cache page, it is returned with an increased refcount. Looks up the page cache entry at mapping & index. If there is a page cache page, it is returned locked and with an increased refcount. A struct page or if there is no page in the cache for this index. the page’s index into the mapping Looks up the page cache slot at mapping & offset. If there is a page cache page, it is returned locked and with an increased refcount. If the page is not present, a new page is allocated using gfp_mask and added to the page cache and the VM’s LRU list. The page is returned locked and with an increased refcount. may sleep, even if gfp_flags specifies an atomic allocation! returns locked page at given index in given cache Same as grab_cache_page(), but do not wait if the page is unavailable. This is intended for speculative data generators, where the data can be regenerated if the page couldn’t be grabbed. This routine should be safe to call while holding the lock for another page. Clear __GFP_FS when allocating the page to avoid recursion into the fs and deadlock against the caller’s locked page. For a folio which is either in the page cache or the swap cache, return its index within the address_space it belongs to. If you know the page is definitely in the page cache, you can look at the folio’s index directly. The index (offset in units of pages) of a folio in its file. Get the index of the next folio. The index of the folio which follows this folio in the file. The page for a particular index. The folio which contains this index. The index we want to look up. Sometimes after looking up a folio in the page cache, we need to obtain the specific page for an index (eg a page fault). The page containing the file data for this index. Does this folio contain this index? The page index within the file. The caller should have the page locked in order to prevent (eg) shmem from moving the page between the page cache and swap cache and changing its index in the middle of the operation. Calculate the logical page offset of this page. The folio containing this page. The page which we need the offset of. For file pages, this is the offset from the beginning of the file in units of PAGE_SIZE. For anonymous pages, this is the offset from the beginning of the anon_vma in units of PAGE_SIZE. This will return nonsense for KSM pages. Caller must have a reference on the folio or otherwise prevent it from being split or freed. The offset in units of PAGE_SIZE. Returns the byte position of this folio in its file. The folio to attempt to lock. Sometimes it is undesirable to wait for a folio to be unlocked (eg when the locks are being taken in the wrong order, or if making progress through a batch of folios is more important than processing them in order). Usually is the correct function to call. Whether the lock was successfully acquired. The folio lock protects against many things, probably more than it should. It is primarily held while a folio is being brought uptodate, either from its backing file or from swap. It is also held while a folio is being truncated from its address_space, so holding the lock is sufficient to keep folio->mapping stable. The folio lock is also held while write() is modifying the page to provide POSIX atomicity guarantees (as long as the write does not cross a page boundary). Other modifications to the data in the folio do not hold the folio lock and can race with writes, eg DMA and stores to mapped pages. May sleep. If you need to acquire the locks of two or more folios, they must be in order of ascending index, if they are in the same address_space. If they are in different address_spaces, acquire the lock of the folio which belongs to the address_space which has the lowest address in memory first. Lock the folio containing this page. See for a description of what the lock protects. This is a legacy function and new code should probably use instead. May sleep. Pages in the same folio share a lock, so do not attempt to lock two pages which share a folio. Attempts to lock the folio, like , except that the sleep to acquire the lock is interruptible by a fatal signal. 0 if the lock was acquired; -EINTR if a fatal signal was received. address space within which to check offset in bytes where the range starts offset in bytes where the range ends (inclusive) Find at least one page in the range supplied, usually used to check if direct writing in this range will trigger a writeback. Used by O_DIRECT read/write with IOCB_NOWAIT, to see if the caller needs to do before proceeding. if the caller should do before doing O_DIRECT to a page in this range, otherwise. The file, used primarily by network filesystems for authentication. May be NULL if invoked internally by the filesystem. A readahead request is for consecutive pages. Filesystems which implement the ->readahead method should call or in a loop and attempt to start I/O against each page in the request. Most of the fields in this struct are private and should be accessed by the functions below. address_space which holds the pagecache and I/O vectors Used by the filesystem for authentication. Index of first page to be read. Total number of pages being read by the caller. should be called when a cache miss happened: it will submit the read. The readahead logic may decide to piggyback more pages onto the read request if access patterns suggest it will improve performance. address_space which holds the pagecache and I/O vectors Used by the filesystem for authentication. The folio which triggered the readahead call. Total number of pages being read by the caller. should be called when a page is used which is marked as PageReadahead; this is a marker to suggest that the application has used up enough of the readahead window that we should start pulling in more pages. Get the next page to read. The page is locked and has an elevated refcount. The caller should decreases the refcount once the page has been submitted for I/O and unlock the page once all I/O to that page has completed. A pointer to the next page, or if we are done. Get the next folio to read. The folio is locked. The caller should unlock the folio once all I/O to that folio has completed. A pointer to the next folio, or if we are done. Get a batch of pages to read. An array of pointers to struct page. The pages are locked and have an elevated refcount. The caller should decreases the refcount once the page has been submitted for I/O and unlock the page once all I/O to that page has completed. The number of pages placed in the array. 0 indicates the request is complete. The byte offset into the file of this readahead request. The number of bytes in this readahead request. The index of the first page in this readahead request. The number of pages in this readahead request. The number of bytes in the current batch. the inode to check the folio against the number of bytes in the folio up to EOF, or -EFAULT if the folio was truncated. the inode to check the page against Returns the number of bytes in the page up to EOF, or -EFAULT if the page was truncated. How many blocks fit in this folio. The inode which contains the blocks. If the block size is larger than the size of this folio, return zero. The caller should hold a refcount on the folio to prevent it from being split. The number of filesystem blocks covered by this folio.\n\nThis function only unmaps ptes assigned to VM_PFNMAP vmas. The entire address range must be fully contained within the vma. in: number of pages to map. out: number of pages that were not mapped. (0 means all pages were successfully mapped). In case of error, we may have mapped a subset of the provided pages. It is the caller’s responsibility to account for this case. The same restrictions apply as in . This allows drivers to insert individual pages they’ve allocated into a user vma. The zeropage is supported in some VMAs, see vm_mixed_zeropage_allowed(). The page has to be a nice clean _individual_ kernel allocation. If you allocate a compound page, you need to have marked it as such (__GFP_COMP), or manually just split the page up yourself (see split_page()). NOTE! Traditionally this was done with “ ” which took an arbitrary page protection parameter. This doesn’t allow that. Your vma protection will have to be set up correctly, which means that if you want a shared writable mapping, you’d better ask for a shared writable mapping! The page does not need to be reserved. Usually this function is called from f_op->mmap() handler under mm->mmap_lock write-lock, so it can change vma->vm_flags. Caller must set VM_MIXEDMAP on vma if it wants to call this function from other places, for example from page-fault handler. maps range of kernel pages starts with non zero offset Maps an object consisting of num pages, catering for the user’s requested vm_pgoff If we fail to insert any page into the vma, the function will return immediately leaving any previously inserted pages present. Callers from the mmap handler may immediately return the error as their caller will destroy the vma, removing any successfully inserted pages. Other callers should make their own arrangements for calling unmap_region(). 0 on success and error code otherwise. map range of kernel pages starts with zero offset Similar to , except that it explicitly sets the offset to 0. This function is intended for the drivers that did not consider vm_pgoff. 0 on success and error code otherwise. insert single pfn into user vma with specified pgprot This is exactly like , except that it allows drivers to override pgprot on a per-page basis. This only makes sense for IO mappings, and it makes no sense for COW mappings. In general, using multiple vmas is preferable; vmf_insert_pfn_prot should only be used if using multiple VMAs is impractical. pgprot typically only differs from vma->vm_page_prot when drivers set caching- and encryption bits different than those of vma->vm_page_prot, because the caching- or encryption mode may not be known at mmap() time. This is ok as long as vma->vm_page_prot is not used by the core vm to set caching and encryption bits for those vmas (except for COW pages). This is ensured by core vm only modifying these page table entries using functions that don’t touch caching- or encryption bits, using pte_modify() if needed. (See for example mprotect()). Also when new page-table entries are created, this is only done using the fault() callback, and never using the value of vma->vm_page_prot, except for page-table entries that point to anonymous pages as the result of COW. Similar to vm_insert_page, this allows drivers to insert individual pages they’ve allocated into a user vma. Same comments apply. This function should only be called from a vm_ops->fault handler, and in that case the handler should return the result of this function. As this is called only for pages that do not currently exist, we do not need to flush old virtual caches or the TLB. this is only safe if the mm semaphore is held when called. start of the physical memory to be mapped This is a simplified io_remap_pfn_range() for common driver use. The driver just needs to give us the physical memory range to be mapped, we’ll figure out the rest from the vma information. NOTE! Some drivers might want to tweak vma->vm_page_prot first to get whatever write-combining details or similar. The address space containing pages to be unmapped. Index of first page to be unmapped. Number of pages to be unmapped. 0 to unmap to end of file. Whether to unmap even private COWed pages. Unmap the pages in this address space from any userspace process which has them mmaped. Generally, you want to remove COWed pages as well when a file is being truncated, but not when invalidating pages from the page cache. unmap the portion of all mmaps in the specified address_space corresponding to the specified byte range in the underlying file. the address space containing mmaps to be unmapped. byte in first page to unmap, relative to the start of the underlying file. This will be rounded down to a PAGE_SIZE boundary. Note that this is different from , which must keep the partial page. In contrast, we must get rid of partial pages. size of prospective hole in bytes. This will be rounded up to a PAGE_SIZE boundary. A holelen of zero truncates to the end of the file. 1 when truncating a file, unmap even private COWed pages; but 0 when invalidating pagecache, don’t throw away private data. Look up a pfn mapping at a user virtual address The caller needs to setup args->vma and args->address to point to the virtual address as the target of such lookup. On a successful return, the results will be put into other output fields. After the caller finished using the fields, the caller must invoke another to proper releases the locks and resources of such look up request. During the and end() calls, the results in args will be valid as proper locks will be held. After the end() is called, all the fields in follow_pfnmap_args will be invalid to be further accessed. Further use of such information after end() may require proper synchronizations by the caller with page table updates, otherwise it can create a security bug. If the PTE maps a refcounted page, callers are responsible to protect against invalidation with MMU notifiers; otherwise access to the PFN at a later point in time can trigger use-after-free. Only IO mappings and raw PFN mappings are allowed. The mmap semaphore should be taken for read, and the mmap semaphore cannot be released before the end() is invoked. This function must not be used to modify PTE content. zero on success, negative otherwise. Must be used in pair of . See the function above for more information. set to FOLL_WRITE when writing, otherwise reading This is a generic implementation for for an iomem mapping. This callback is used by access_process_vm() when the vma is not page based. Return the requested group of flags for the pageblock_nr_pages block of pages The page within the block of interest mask of bits that the caller is interested in Set the requested group of flags for a pageblock_nr_pages block of pages The page within the block of interest mask of bits that the caller is interested in migratetype to set on the pageblock This is similar to move_freepages_block(), but handles the special case encountered in page isolation, where the block of interest might be part of a larger buddy spanning multiple pageblocks. Unlike the regular page allocator path, which moves pages while stealing buddies off the freelist, page isolation is interested in arbitrary pfn ranges that may have overlapping buddies on both ends. This function handles that. Straddling buddies are split into individual pageblocks. Only the block of interest is moved. Returns if pages could be moved, otherwise. Return a now-isolated page back where we got it This function is meant to return a page pulled from the free lists via __isolate_free_page back to the free lists they were pulled from. The order of the allocation. This function can free multi-page allocations that are not compound pages. It does not check that the order passed in matches that of the allocation, so it is easy to leak memory. Freeing more memory than was allocated will probably emit a warning. If the last reference to this page is speculative, it will be released by put_page() which only frees the first page of a non-compound allocation. To prevent the remaining pages from being leaked, we free the subsequent pages here. If you want to use the page’s reference count to decide when to free the allocation, you should allocate a compound page, and use put_page() instead of . May be called in interrupt context or while holding a normal spinlock, but not in NMI context or while holding a raw spinlock. the number of bytes to allocate GFP flags for the allocation, must not contain __GFP_COMP This function is similar to , except that it allocates the minimum number of pages to satisfy the request. can only allocate memory in power-of-two pages. This function is also limited by MAX_PAGE_ORDER. Memory allocated by this function must be released by . pointer to the allocated area or in case of error. allocate an exact number of physically-contiguous pages on a node. the preferred node ID where memory should be allocated the number of bytes to allocate GFP flags for the allocation, must not contain __GFP_COMP Like , but try to allocate on node nid first before falling back. pointer to the allocated area or in case of error. the value returned by alloc_pages_exact. size of allocation, same value as passed to . Release the memory allocated by a previous call to alloc_pages_exact. The zone index of the highest zone counts the number of pages which are beyond the high watermark within all zones at or below a given zone index. For each zone, the number of pages is calculated as: counts the number of pages which are beyond the high watermark within ZONE_DMA and ZONE_NORMAL. number of pages beyond high watermark within ZONE_DMA and ZONE_NORMAL. find the next node that should appear in a given node’s fallback list nodemask_t of already used nodes We use a number of factors to determine which is the next node that should appear on a given node’s fallback list. The node should not have appeared already in node’s fallback list, and it should be the next closest node according to the distance array (which contains arbitrary distance values from each node to each node in the system), and should also prefer nodes with no CPUs, since presumably they’ll have very little allocation pressure on them otherwise. node id of the found node or if no node is found. called when min_free_kbytes changes or when memory is hot-{added|removed} Ensures that the watermark[min,low,high] values for each zone are set correctly with respect to min_free_kbytes.\n• None tries to allocate given range of pages migratetype of the underlying pageblocks (either #MIGRATE_MOVABLE or #MIGRATE_CMA). All pageblocks in range must have the same migratetype and it must be either of the two. GFP mask. Node/zone/placement hints are ignored; only some action and reclaim modifiers are supported. Reclaim modifiers control allocation behavior during compaction/migration/reclaim. The PFN range does not have to be pageblock aligned. The PFN range must belong to a single zone. The first thing this routine does is attempt to MIGRATE_ISOLATE all pageblocks in the range. Once isolated, the pageblocks should not be modified by others. zero on success or negative error code. On success all pages which PFN is in [start, end) are allocated for the caller and need to be freed with free_contig_range().\n• None tries to find and allocate contiguous range of pages GFP mask. Node/zone/placement hints limit the search; only some action and reclaim modifiers are supported. Reclaim modifiers control allocation behavior during compaction/migration/reclaim. Mask for other possible nodes This routine is a wrapper around . It scans over zones on an applicable zonelist to find a contiguous pfn range which can then be tried for allocation with . This routine is intended for allocation requests which can not be fulfilled with the buddy allocator. The allocated memory is always aligned to a page boundary. If nr_pages is a power of two, then allocated range is also guaranteed to be aligned to same nr_pages (e.g. 1GB request would be aligned to 1GB). Allocated pages can be freed with free_contig_range() or by manually calling __free_page() on each allocated page. pointer to contiguous pages on success, or NULL if not successful. Lookup the closest node by distance if nid is not in state. this node if it is in state, otherwise the closest node by distance Preferred node (usually numa_node_id() but mpol may override it). The page on success or NULL if allocation fails. Virtual address of the allocation. Must be inside vma. Allocate a folio for a specific address in vma, using the appropriate NUMA policy. The caller must hold the mmap_lock of the mm_struct of the VMA to prevent it from going away. Should be used for all allocations for folios that will be mapped into user space, excepting hugetlbfs, and excepting where direct use of folio_alloc_mpol() is more appropriate. The folio on success or NULL if allocation fails. Power of two of number of pages to allocate. Allocate 1 << order contiguous pages. The physical address of the first page is naturally aligned (eg an order-3 allocation will be aligned to a multiple of 8 * PAGE_SIZE bytes). The NUMA policy of the current process is honoured when in process context. Can be called from any context, providing the appropriate GFP flags are used. The page on success or NULL if allocation fails. check whether current folio node is valid in policy virtual address in vma for shared policy lookup and interleave policy Lookup current policy node id for vma,addr and “compare to” folio’s node id. Policy determination “mimics” alloc_page_vma(). Called from fault path where we know the vma and faulting address. NUMA_NO_NODE if the page is in a node that is valid for this policy, or a suitable node ID to allocate a replacement folio from. Install non-NULL mpol in inode’s shared policy rb-tree. On entry, the current task has a reference on a non-NULL mpol. This must be released on exit. This is called at get_inode() calls and we can use GFP_KERNEL. pointer to mempolicy to be formatted Convert pol into a string. If buffer is too short, truncate the string. Recommend a maxlen of at least 51 for the longest mode, “weighted interleave”, plus the longest flag flags, “relative|balancing”, and to display at least a few node ids. Least Recently Used list; tracks how recently this folio was used. Number of times this folio has been pinned by mlock(). The file this page belongs to, or refers to the anon_vma for anonymous memory. Offset within the file, in units of pages. For anonymous memory, this is the index from the beginning of the mmap. Used for swp_entry_t if folio_test_swapcache(). Do not access this member directly. Use to find out how many times this folio is mapped by userspace. Do not access this member directly. Use to find how many references there are to this folio. IDs of last CPU and last process that accessed the folio. Do not use directly, call . Do not use directly, call folio_entire_mapcount(). Do not use outside of rmap and debug code. Do not use directly, call . Do not use directly, call . Do not use directly, use accessor in hugetlb.h. Do not use directly, use accessor in hugetlb_cgroup.h. Do not use directly, use accessor in hugetlb_cgroup.h. Do not use directly, call raw_hwp_list_head(). Folios to be split under memory pressure. A folio is a physically, virtually and logically contiguous set of bytes. It is a power-of-two in size, and it is aligned to that same power-of-two. It is at least as large as . If it is in the page cache, it is at a file offset which is a multiple of that power-of-two. It may be mapped into userspace at an address which is at an arbitrary page offset, but its kernel virtual address is aligned to its size. Same as page flags. Powerpc only. List of used page tables. Used for s390 gmap shadow pages (which are not linked into the user page tables) and x86 pgds. Protected by ptdesc->ptl, used for THPs. Same as page->page_type. Unused for page tables. This struct overlays struct page for now. Do not modify without a good understanding of the issues. Page fault handlers return a bitmask of these values to tell the core VM what happened when handling the fault. Used to decide whether a process gets delivered SIGBUS or just gets major/minor fault counters bumped up. ->fault did not modify page tables and needs fsync() to complete (for synchronous page faults in DAX) Allow to retry the fault if blocked. The fault task is in SIGKILL killable region. The fault has been tried once. The fault is not for current task/mm. The fault was during an instruction fetch. The fault can be interrupted by non-fatal signals. The fault is an unsharing request to break COW in a COW mapping, making sure that an exclusive anon page is mapped after the fault. whether the fault has vmf->orig_pte cached. We should only access orig_pte if this flag set. The fault is handled under VMA lock. About FAULT_FLAG_ALLOW_RETRY and FAULT_FLAG_TRIED: we can specify whether we would allow page faults to retry by specifying these two fault flags correctly. Currently there can be three legal combinations:\n• None ALLOW_RETRY and !TRIED: this means the page fault allows retry, and\n• None ALLOW_RETRY and TRIED: this means the page fault allows retry, and we’ve already tried at least once\n• None !ALLOW_RETRY and !TRIED: this means the page fault does not allow retry The unlisted combination (!ALLOW_RETRY && TRIED) is illegal and should never be used. Note that page faults can be allowed to retry for multiple times, in which case we’ll have an initial fault with flags (a) then later on continuous faults with flags (b). We should always try to detect pending signals before a retry to make sure the continuous page faults can still be interrupted if necessary. The combination FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE is illegal. FAULT_FLAG_UNSHARE is ignored and treated like an ordinary read fault when applied to mappings that are not COW mappings. Should the folio be on a file LRU or anon LRU? We would like to get this info without a page flag, but the state needs to survive until the folio is last deleted from the LRU, which could be as far down as __page_cache_release. An integer (not a boolean!) used to sort a folio onto the right LRU list and to account folios correctly. 1 if folio is a regular filesystem backed page cache folio or a lazily freed anonymous folio (e.g. via MADV_FREE). 0 if folio is a normal anonymous folio, a tmpfs folio or otherwise ram or swap backed folio. The folio that was on lru and now has a zero reference. Which LRU list should a folio be on? The LRU list a folio should be on, as an index into the array of LRU lists. Every page is part of a folio. This function cannot be called on a NULL pointer. No reference, nor lock is required on page. If the caller does not hold a reference, this call may race with a folio split, so it should re-check the folio still contains this page after gaining a reference on the folio. The folio which contains this page. n is relative to the start of the folio. This function does not check that the page number lies within folio; the caller is presumed to have a reference to the page. Bits set in this word will be changed. This must only be used for flags which are changed with the folio lock held. For example, it is unsafe to use for PG_dirty as that can be set without the folio lock held. It can also only be used on flags which are in the range 0-6 as some of the implementations only affect those bits. Whether there are tasks waiting on the folio. Is this folio up to date? The uptodate flag is set on a folio when every byte in the folio is at least as new as the corresponding bytes on storage. Anonymous and CoW folios are always uptodate. If the folio is not uptodate, some of the bytes in it may be; see the is_partially_uptodate() address_space operation. Does this folio contain more than one page? True if the folio is larger than one page. Determine if the page belongs to the slab allocator True for slab pages, false for any other kind of page. Determine if the page belongs to hugetlbfs True for hugetlbfs pages, false for anon pages or pages belonging to other filesystems. Determine if a folio has private stuff, indicating that release routines should be invoked upon it. This is mostly used for places where we want to try to avoid taking the mmap_lock for too long a time when waiting for another condition to change, in which case we can try to be polite to release the mmap_lock in the first round to avoid potential starvation of other processes that would also want the mmap_lock. true if the page fault allows retry and this is the first attempt of the fault handling; false otherwise. A folio is composed of 2^order pages. See get_order() for the definition of order. Number of mappings of this folio. The folio mapcount corresponds to the number of present user page table entries that reference any part of a folio. Each such present user page table entry must be paired with exactly on folio reference. For ordindary folios, each user page table entry (PTE/PMD/PUD/...) counts exactly once. For hugetlb folios, each abstracted “hugetlb” user page table entry that references the entire folio counts exactly once, even when such special page table entries are comprised of multiple ordinary page table entries. Will report 0 for pages which cannot be mapped into userspace, such as slab, page tables and similar. The number of times this folio is mapped. Is this folio mapped into userspace? True if any page in this folio is referenced by user page tables. Number of bytes in this page. May be called in any context, as long as you know that you have a refcount on the folio. If you do not already have one, may be the right interface for you to use. If the folio’s reference count reaches zero, the memory will be released back to the page allocator and may be used by another allocation immediately. Do not access the memory or the after calling unless you can be sure that it wasn’t the last reference. May be called in process or interrupt context, but not in NMI context. May be called while holding a spinlock. The amount to subtract from the folio’s reference count. If the folio’s reference count reaches zero, the memory will be released back to the page allocator and may be used by another allocation immediately. Do not access the memory or the after calling unless you can be sure that these weren’t the last references. May be called in process or interrupt context, but not in NMI context. May be called while holding a spinlock. Decrement the reference count on an array of folios. Like , but for a batch of folios. This is more efficient than writing the loop yourself as it will optimise the locks which need to be taken if the folios are freed. The folios batch is returned empty and ready to be reused for another batch; there is no need to reinitialise it. May be called in process or interrupt context, but not in NMI context. May be called while holding a spinlock. A folio may contain multiple pages. The pages have consecutive Page Frame Numbers. The Page Frame Number of the first page in the folio. Report if a folio may be pinned for DMA. This function checks if a folio has been pinned via a call to a function in the pin_user_pages() family. For small folios, the return value is partially fuzzy: false is not fuzzy, because it means “definitely not pinned for DMA”, but true means “probably pinned for DMA, but possibly a false positive due to having at least GUP_PIN_COUNTING_BIAS worth of normal folio references”. False positives are OK, because: a) it’s unlikely for a folio to get that many refcounts, and b) all the callers of this routine are expected to be able to deal gracefully with a false positive. For large folios, the result will be exactly correct. That’s because we have more tracking data available: the _pincount field is used instead of the GUP_PIN_COUNTING_BIAS scheme. For more information, please see pin_user_pages() and related calls. True, if it is likely that the folio has been “dma-pinned”. False, if the folio is definitely not dma-pinned. Query if a page is a zero page This returns true if page is one of the permanent zero pages. Query if a folio is a zero page This returns true if folio is one of the permanent zero pages. The number of pages in the folio. The number of regular pages in this huge page. Move to the next physical folio. The folio we’re currently operating on. If you have physically contiguous memory which may span more than one folio (eg a ), use this function to move from one folio to the next. Do not use it if the memory is only virtually contiguous as the folios are almost certainly not adjacent to each other. This is the folio equivalent to writing . We assume that the folios are refcounted and/or locked at a higher level and do not adjust the reference counts. The size of the memory described by this folio. A folio represents a number of bytes which is a power-of-two in size. This function tells you which power-of-two the folio is. See also and . The caller should have a reference on the folio to prevent it from being split. It is not necessary for the folio to be locked. The base-2 logarithm of the size of this folio. The number of bytes in a folio. The caller should have a reference on the folio to prevent it from being split. It is not necessary for the folio to be locked. The number of bytes in this folio. Estimate if the folio is mapped into the page tables of more than one MM This function checks if the folio is currently mapped into more than one MM (“mapped shared”), or if the folio is only mapped into a single MM (“mapped exclusively”). For KSM folios, this function also returns “mapped shared” when a folio is mapped multiple times into the same MM, because the individual page mappings are independent. As precise information is not easily available for all folios, this function estimates the number of MMs (“sharers”) that are currently mapping a folio using the number of times the first page of the folio is currently mapped into page tables. For small anonymous folios and anonymous hugetlb folios, the return value will be exactly correct: non-KSM folios can only be mapped at most once into an MM, and they cannot be partially mapped. KSM folios are considered shared even if mapped multiple times into the same MM. For other folios, the result can be fuzzy:\n• None For partially-mappable large folios (THP), the return value can wrongly indicate “mapped exclusively” (false negative) when the folio is only partially mapped into at least one MM.\n• None For pagecache folios (including hugetlb), the return value can wrongly indicate “mapped shared” (false positive) when two VMAs in the same MM cover the same file range. Further, this function only considers current page table mappings that are tracked using the folio mapcount(s). This function does not consider:\n• None If the folio might get mapped in the (near) future (e.g., swapcache, pagecache, temporary unmapping for migration).\n• None If the folio is mapped differently (VM_PFNMAP).\n• None If hugetlb page table sharing applies. Callers might want to check hugetlb_pmd_shared(). Whether the folio is estimated to be mapped into more than one MM. pagetable_alloc allocates memory for page tables as well as a page table descriptor to describe that memory. pagetable_free frees the memory of all page tables described by a page table descriptor and the memory for the descriptor itself. The vm_area_struct at the given address, otherwise. Pointer to the struct vm_area_struct to consider Whether transhuge page-table entries are considered “special” following the definition in vm_normal_page(). true if transhuge page-table entries should be considered special, false otherwise. The reference count on this folio. The refcount is usually incremented by calls to and decremented by calls to . Some typical users of the folio refcount:\n• None Direct IO which references this page in the process address space The number of references to this folio. Attempt to increase the refcount on a folio. If you do not already have a reference to a folio, you can attempt to get one using this function. It may fail if, for example, the folio has been freed since you found a pointer to it, or it is frozen for the purposes of splitting or migration. True if the reference count was successfully incremented. helper function to quickly check if a struct zone is a highmem zone or not. This is an attempt to keep references to ZONE_{DMA/NORMAL/HIGHMEM/etc} in general code to a minimum. helper macro to iterate over all online nodes helper macro to iterate over all memory zones The user only needs to declare the zone variable, for_each_zone fills it in. Returns the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point The cursor used as a starting point for the search The zone index of the highest zone to return An optional nodemask to filter the zonelist with This function returns the next zone at or below a given zone index that is within the allowed nodemask using a cursor as the starting point for the search. The zoneref returned is a cursor that represents the current zone being examined. It should be advanced by one before calling next_zones_zonelist again. the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist The zonelist to search for a suitable zone The zone index of the highest zone to return An optional nodemask to filter the zonelist with This function returns the first zone at or below a given zone index that is within the allowed nodemask. The zoneref returned is a cursor that can be used to iterate the zonelist with next_zones_zonelist by advancing it by one before calling. When no eligible zone is found, zoneref->zone is NULL (zoneref itself is never NULL). This may happen either genuinely, or due to concurrent nodemask update due to cpuset modification. Zoneref pointer for the first suitable zone found helper macro to iterate over valid zones in a zonelist at or below a given zone index and within a nodemask The current zone in the iterator The current pointer within zonelist->_zonerefs being iterated The zone index of the highest zone to return This iterator iterates though all zones at or below a given zone index and within a given nodemask helper macro to iterate over valid zones in a zonelist at or below a given zone index The current zone in the iterator The current pointer within zonelist->zones being iterated The zone index of the highest zone to return This iterator iterates though all zones at or below a given zone index. check if there is a valid memory map entry for a PFN Check if there is a valid memory map entry aka struct page for the pfn. Note, that availability of the memory map entry does not imply that there is actual usable memory at that pfn. The struct page may represent a hole or an unusable page frame. 1 for PFNs that have memory map entries and 0 otherwise Find the mapping where this folio is stored. For folios which are in the page cache, return the mapping that this page belongs to. Folios in the swap cache return the swap mapping this page is stored in (which is different from the mapping for the swap file or swap device where the data is stored). You can call this for folios which aren’t in the swap cache or page cache and it will return NULL. This makes sure the memory mapping described by ‘vma’ has an ‘anon_vma’ attached to it, so that we can associate the anonymous pages mapped into it with that anon_vma. The common case will be that we already have one, which is handled inline by anon_vma_prepare(). But if not we either need to find an adjacent mapping that we can re-use the anon_vma from (very common when the only reason for splitting a vma has been mprotect()), or we allocate a new one. Anon-vma allocations are very subtle, because we may have optimistically looked up an anon_vma in folio_lock_anon_vma_read() and that may actually touch the rwsem even in the newly allocated vma (it depends on RCU to make sure that the anon_vma isn’t actually destroyed). As a result, we need to do proper anon_vma locking even for the new allocation. At the same time, we do not want to do any locking for the common case of already having an anon_vma. The virtual address of a page in this VMA. The folio containing the page. The page within the folio. The VMA we need to know the address in. Calculates the user virtual address of this page in the specified VMA. It is the caller’s responsibililty to check the page is actually within the VMA. There may not currently be a PTE pointing at this page, but if a page fault occurs at this address, this is the page which will be accessed. Caller should hold a reference to the folio. Caller should hold a lock (eg the i_mmap_lock or the mmap_lock) which keeps the VMA from being altered. The virtual address corresponding to this page in the VMA. Test if the folio was referenced. A combination of all the vma->vm_flags which referenced the folio. Quick test_and_clear_referenced for all mappings of a folio, The number of mappings which referenced the folio. Return -1 if the function bailed out due to rmap lock contention. Cleans the PTEs (including PMDs) mapped with range of [pfn, pfn + nr_pages) at the specific offset (pgoff) within the vma of shared mappings. And since clean PTEs should also be readonly, write protects them too. page offset that the pfn mapped with. The folio to move to our anon_vma The vma the folio belongs to When a folio belongs exclusively to one process after a COW event, that folio can be moved into the anon_vma that belongs to just that process, so the rmap code will not search the parent or sibling processes. set up a new anonymous rmap for a folio The folio to set up the new anonymous rmap for. VM area to add the folio to. Whether the folio is exclusive to the process. the page to check the mapping of the vm area in which the mapping is added add PTE mappings to a page range of an anon folio The folio to add the mappings to The number of pages which will be mapped The vm area in which the mappings are added The user virtual address of the first page to map The page range of folio is defined by [first_page, first_page + nr_pages) The caller needs to hold the page table lock, and the page must be locked in the anon_vma case: to serialize mapping,index checking after setting, and to ensure that an anon folio is not being upgraded racily to a KSM folio (but KSM folios are never downgraded). add a PMD mapping to a page range of an anon folio The folio to add the mapping to The vm area in which the mapping is added The user virtual address of the first page to map The page range of folio is defined by [first_page, first_page + HPAGE_PMD_NR) The caller needs to hold the page table lock, and the page must be locked in the anon_vma case: to serialize mapping,index checking after setting. The folio to add the mapping to. the vm area in which the mapping is added Like folio_add_anon_rmap_*() but must only be called on new folios. This means the inc-and-test can be bypassed. The folio doesn’t necessarily need to be locked while it’s exclusive unless two threads map it concurrently. However, the folio must be locked if it’s shared. If the folio is pmd-mappable, it is accounted as a THP. The folio to add the mappings to The number of pages that will be mapped using PTEs The vm area in which the mappings are added The page range of the folio is defined by [page, page + nr_pages) The caller needs to hold the page table lock. The folio to add the mapping to The vm area in which the mapping is added The page range of the folio is defined by [page, page + HPAGE_PMD_NR) The caller needs to hold the page table lock. The folio to remove the mappings from The number of pages that will be removed from the mapping The vm area from which the mappings are removed The page range of the folio is defined by [page, page + nr_pages) The caller needs to hold the page table lock. The folio to remove the mapping from The vm area from which the mapping is removed The page range of the folio is defined by [page, page + HPAGE_PMD_NR) The caller needs to hold the page table lock. Try to remove all page table mappings to a folio. Tries to remove all the page table entries which are mapping this folio. It is the caller’s responsibility to check if the folio is still mapped if needed (use TTU_SYNC to prevent accounting races). try to replace all page table mappings with swap entries the folio to replace page table entries for Tries to remove all the page table entries which are mapping this folio and replace them with special swap entries. Caller must hold the folio lock. The folio to replace page table entries for. The mm_struct where the folio is expected to be mapped. Address where the folio is expected to be mapped. Tries to remove all the page table entries which are mapping this folio and replace them with special device exclusive swap entries to grant a device exclusive access to the folio. false if the page is still mapped, or if it could not be unmapped from the expected address. Otherwise returns true (success). Mark a range for exclusive use by a device start of the region to mark for exclusive device access returns the pages which were successfully marked for exclusive access passed to MMU_NOTIFY_EXCLUSIVE range notifier to allow filtering number of pages found in the range by GUP. A page is marked for exclusive access only if the page pointer is non-NULL. This function finds ptes mapping page(s) to the given address range, locks them and replaces mappings with special swap entries preventing userspace CPU access. On fault these entries are replaced with the original mapping after calling MMU notifiers. A driver using this to program access from a device must use a mmu notifier critical section to hold a device specific lock during programming. Once programming is complete it should drop the page lock and reference after which point CPU access to the page will revoke the exclusive access. The address_space containing the folio. The folio to migrate the data to. The folio containing the current data. Common logic to directly migrate a single LRU folio suitable for folios that do not have private data. Folios are locked upon entry and exit. The folio to migrate from. How to migrate the folio. This function can only be used if the underlying filesystem guarantees that no other references to src exist. For example attached buffer heads are accessed only under the folio lock. If your filesystem cannot provide this guarantee, may be more appropriate. 0 on success or a negative errno on failure. The folio to migrate from. How to migrate the folio. Like except that this variant is more careful and checks that there are also no buffer head references. This function is the right one for mappings where buffer heads are directly looked up and referenced (such as block device mappings). 0 on success or a negative errno on failure. Perform a userland memory mapping into the current process address space of length len with protection bits prot, mmap flags flags (from which VMA flags will be inferred), and any additional VMA flags to apply vm_flags. If this is a file-backed mapping then the file is specified in file and page offset into the file via pgoff. An optional pointer describing the file which is to be mapped, if a file-backed mapping. If non-zero, hints at (or if flags has MAP_FIXED set, specifies) the address at which to perform this mapping. See mmap (2) for details. Must be page-aligned. The length of the mapping. Will be page-aligned and must be at least 1 page in size. Protection bits describing access required to the mapping. See mmap (2) for details. Flags specifying how the mapping should be performed, see mmap (2) for details. VMA flags which should be set by default, or 0 otherwise. Page offset into the file if file-backed, should be 0 otherwise. A pointer to a value which will be set to 0 if no population of the range is required, or the number of bytes to populate if it is. Must be non-NULL. See mmap (2) for details as to under what circumstances population of the range occurs. An optional pointer to a list head to track userfaultfd unmap events should unmapping events arise. If provided, it is up to the caller to manage this. This function does not perform security checks on the file and assumes, if uf is non-NULL, the caller has provided a list head to track unmap events for userfaultfd uf. It also simply indicates whether memory population is required by setting populate, which must be non-NULL, expecting the caller to actually perform this task itself if appropriate. This function will invoke architecture-specific (and if provided and relevant, file system-specific) logic to determine the most appropriate unmapped area in which to place the mapping if not MAP_FIXED. Callers which require userland mmap() behaviour should invoke vm_mmap(), which is also exported for module use. Those which require this behaviour less security checks, userfaultfd and populate behaviour, and who handle the mmap write lock themselves, should call this function. Note that the returned address may reside within a merged VMA if an appropriate merge were to take place, so it doesn’t necessarily specify the start of a VMA, rather only the start of a valid mapped range of length len bytes, rounded down to the nearest page size. Either an error, or the address at which the requested mapping has been performed. Look up the first VMA which intersects the interval The first VMA within the provided range, otherwise. Assumes start_addr < end_addr. Find the VMA for a given address, or the next VMA. The VMA associated with addr, or the next VMA. May return in the case of no VMA at addr or above. Find the VMA for a given address, or the next vma and set to the previous VMA, if any. The pointer to set to the previous VMA Note that RCU lock is missing here since the external mmap_lock() is used instead. The VMA associated with addr, or the next vma. May return in the case of no vma at addr or above. pointer to beginning of the object minimum number of references to this object. If during memory scanning a number of references less than min_count is found, the object is reported as a memory leak. If min_count is 0, the object is never reported as a leak. If min_count is -1, the object is ignored (not scanned and not reported as a leak) This function is called from the kernel allocators when a new object (memory block) is allocated (kmem_cache_alloc, kmalloc etc.). __percpu pointer to beginning of the object This function is called from the kernel percpu allocator when a new object (memory block) is allocated (alloc_percpu). This function is called from the vmalloc() kernel allocator when a new object (memory block) is allocated. pointer to beginning of the object This function is called from the kernel allocators when an object (memory block) is freed (kmem_cache_free, kfree, vfree etc.). pointer to the beginning or inside the object. This also represents the start of the range to be freed This function is called when only a part of a memory block is freed (usually from the bootmem allocator). __percpu pointer to beginning of the object This function is called from the kernel percpu allocator when an object (memory block) is freed (free_percpu). pointer to beginning of the object Override the object allocation stack trace for cases where the actual allocation place is not always useful. pointer to beginning of the object Calling this function on an object will cause the memory block to no longer be reported as leak and always be scanned. pointer to beginning of the object Calling this function on an object will cause the memory block to not be reported as a leak temporarily. This may happen, for example, if the object is part of a singly linked list and the ->next reference to it is changed. pointer to beginning of the object Calling this function on an object will cause the memory block to be ignored (not scanned and not reported as a leak). This is usually done when it is known that the corresponding block is not a leak and does not contain any references to other allocated memory blocks. limit the range to be scanned in an allocated object pointer to beginning or inside the object. This also represents the start of the scan area This function is used when it is known that only certain parts of an object contain references to other objects. Kmemleak will only scan these areas reducing the number false negatives. do not scan an allocated object pointer to beginning of the object This function notifies kmemleak not to scan the given memory block. Useful in situations where it is known that the given object does not contain any references to other objects. Kmemleak will not scan such objects reducing the number of false negatives. physical address if the beginning or inside an object. This also represents the start of the range to be freed remap and provide memmap backing for the given resource 1/ At a minimum the range and type members of pgmap must be initialized by the caller before passing it to this function 2/ The altmap field may optionally be initialized, in which case PGMAP_ALTMAP_VALID must be set in pgmap->flags. 3/ The ref field may optionally be provided, in which pgmap->ref must be ‘live’ on entry and will be killed and reaped at devm_memremap_pages_release() time, or if this routine fails. 4/ range is expected to be a host memory range that could feasibly be treated as a “System RAM” range, i.e. not a device mmio range, but this is not enforced. take a new live reference on the dev_pagemap for pfn optional known pgmap that already has a reference If pgmap is non-NULL and covers pfn it will be returned as-is. If pgmap is non-NULL but does not cover pfn the reference to it will be released. Folios in this VMA will be aligned to, and at least the size of the number of bytes returned by this function. The default size of the folios allocated when backing a VMA. try to isolate an allocated hugetlb folio the list to add the folio to on success Isolate an allocated (refcount > 0) hugetlb folio, marking it as isolated/non-migratable, and moving it from the active list to the given list. Isolation will fail if folio is not an allocated hugetlb folio, or if it is already isolated/non-migratable. On success, an additional folio reference is taken that must be dropped using to undo the isolation. Putback/un-isolate the hugetlb folio that was previous isolated using : marking it non-isolated/migratable and putting it back onto the active list. Will drop the additional folio reference obtained through . Mark a folio as having seen activity. This function will perform one of the following transitions: When a newly allocated folio is not yet visible, so safe for non-atomic ops, __folio_set_referenced() may be substituted for . The folio to be added to the LRU. Queue the folio for addition to the LRU. The decision on whether to add the page to the [in]active [file|anon] list is deferred until the folio_batch is drained. This gives a chance for the caller of have the folio added to the active list using . Add a folio to the appropate LRU list for this VMA. The folio to be added to the LRU. VMA in which the folio is mapped. If the VMA is mlocked, folio is added to the unevictable list. Otherwise, it is treated the same way as . This function hints to the VM that folio is a good reclaim candidate, for example if its invalidation fails due to the folio being dirty or under writeback. moves folio to the inactive file list. This is done to accelerate the reclaim of folio. Reduce the reference count on a batch of folios. The number of refs to subtract from each folio. Like , but for a batch of folios. This is more efficient than writing the loop yourself as it will optimise the locks which need to be taken if the folios are freed. The folios batch is returned empty and ready to be reused for another batch; there is no need to reinitialise it. If refs is NULL, we subtract one from each folio refcount. May be called in process or interrupt context, but not in NMI context. May be called while holding a spinlock. Decrement the reference count on all the pages in arg. If it fell to zero, remove the page from the LRU and free it. Note that the argument can be an array of pages, encoded pages, or folio pointers. We ignore any encoded bits, and turn any of them into just a folio that gets free’d. find_get_entries() fills a batch with both folios and shadow/swap/DAX entries. This function prunes all the non-folio entries from fbatch without leaving holes, so that it can be passed on to folio-only batch operations. Module usage counting is used to prevent using a driver while/after unloading, so if this is called from module exit function, this should never fail; if called from other than the module exit function, and this returns failure, the driver is in use and must remain available. Check if the pool driver is available The type of the zpool to check (e.g. zbud, zsmalloc) This checks if the type pool driver is available. This will try to load the requested module, if needed, but there is no guarantee the module will still be loaded and available immediately after calling. If this returns true, the caller should assume the pool is available, but must be prepared to handle the returning failure. However if this returns false, the caller should assume the requested pool type is not available; either the requested pool type module does not exist, or could not be loaded, and calling with the pool type will fail. The type string must be null-terminated. true if type pool is available, false if not The type of the zpool to create (e.g. zbud, zsmalloc) The name of the zpool (e.g. zram0, zswap) The GFP flags to use when allocating the pool. This creates a new zpool of the specified type. The gfp flags will be used when allocating memory, if the implementation supports it. If the ops param is NULL, then the created zpool will not be evictable. Implementations must guarantee this to be thread-safe. The type and name strings must be null-terminated. New zpool on success, NULL on failure. Implementations must guarantee this to be thread-safe, however only when destroying different pools. The same pool should only be destroyed once, and should not be used after it is destroyed. This destroys an existing zpool. The zpool should not be in use. Get the type of the zpool This returns the type of the pool. Implementations must guarantee this to be thread-safe. This returns if the zpool supports allocating movable memory. Implementations must guarantee this to be thread-safe. true if the zpool supports allocating movable memory, false if not The zpool to allocate from. The amount of memory to allocate. The GFP flags to use when allocating memory. Pointer to the handle to set This allocates the requested amount of memory from the pool. The gfp flags will be used when allocating memory, if the implementation supports it. The provided handle will be set to the allocated object handle. Implementations must guarantee this to be thread-safe. 0 on success, negative value on error. The zpool that allocated the memory. The handle to the memory to free. This frees previously allocated memory. This does not guarantee that the pool will actually free memory, only that the memory in the pool will become available for use by the pool. Implementations must guarantee this to be thread-safe, however only when freeing different handles. The same handle should only be freed once, and should not be used after freeing. The zpool that the handle was allocated from How the memory should be mapped This maps a previously allocated handle into memory. The mapmode param indicates to the implementation how the memory will be used, i.e. read-only, write-only, read-write. If the implementation does not support it, the memory will be treated as read-write. This may hold locks, disable interrupts, and/or preemption, and the must be called to undo those actions. The code that uses the mapped handle should complete its operations on the mapped handle memory quickly and unmap as soon as possible. As the implementation may use per-cpu data, multiple handles should not be mapped concurrently on any cpu. The zpool that the handle was allocated from This unmaps a previously mapped handle. Any locks or other actions that the implementation took in will be undone here. The memory area returned from should no longer be used after this. The total size of the pool This returns the total size in pages of the pool. Total size of the zpool in pages. Test if zpool can sleep when do mapped. Some allocators enter non-preemptible context in ->map() callback (e.g. disable pagefaults) and exit that context in ->unmap(), which limits what we can do with the mapped object. For instance, we cannot wait for asynchronous crypto API to decompress such an object or take mutexes since those will call into the scheduler. This function tells us whether we use such an allocator. true if zpool can sleep; false otherwise. css of the memcg associated with a folio If memcg is bound to the default hierarchy, css of the memcg associated with folio is returned. The returned css remains associated with folio until it is released. If memcg is bound to a traditional hierarchy, the css of root_mem_cgroup is returned. return inode number of the memcg a page is charged to Look up the closest online ancestor of the memory cgroup page is charged to and return its inode number or 0 if page is not charged to any cgroup. It is safe to call this function without holding a reference to page. Note, this function is inherently racy, because there is nothing to prevent the cgroup inode from getting torn down and potentially reallocated a moment after returns, so it only should be used by callers that do not care (such as procfs interfaces). the stat item - can be enum memcg_stat_item or enum node_stat_item delta to add to the counter, can be negative delta to add to the counter, can be negative The lruvec is the intersection of the NUMA node and a cgroup. This function updates the all three counters that are affected by a change of state at this level: per-node, per-cgroup, per-lruvec. the number of events that occurred mm from which memcg should be extracted. It can be NULL. Obtain a reference on mm->memcg and returns it if successful. If mm is NULL, then the memcg is chosen as follows: 1) The active memcg, if set. 2) current->mm->memcg, if available 3) root memcg If mem_cgroup is disabled, NULL is returned. folio from which memcg should be extracted. Returns references to children of the hierarchy below root, or root itself, or after a full round-trip. Caller must pass the return value in prev on subsequent invocations for reference counting, or use to cancel a hierarchy walk before the round-trip is complete. Reclaimers can specify a node in reclaim to divide up the memcgs in the hierarchy among all concurrent reclaimers operating on the same node. last visited hierarchy member as returned by function to call for each task This function iterates over tasks attached to memcg or to any of its descendants and calls fn for each task. If fn returns a non-zero value, the function breaks the iteration loop. Otherwise, it will iterate over all tasks and return 0. This function must not be called for the root memory cgroup. These functions are safe to use under any of the following conditions: - folio locked - folio_test_lru false - folio frozen (refcount of 0) The lruvec this folio is on with its lock held. These functions are safe to use under any of the following conditions: - folio locked - folio_test_lru false - folio frozen (refcount of 0) The lruvec this folio is on with its lock held and interrupts disabled. These functions are safe to use under any of the following conditions: - folio locked - folio_test_lru false - folio frozen (refcount of 0) The lruvec this folio is on with its lock held and interrupts disabled. account for adding or removing an lru page index of lru list the page is sitting on positive when adding or negative when removing This function must be called under lru_lock, just before a page is added to or just after a page is removed from an lru list. Returns the maximum amount of memory mem can be charged with, in pages. The memory cgroup that went over limit Task that is going to be killed memcg and p’s mem_cgroup can be different when hierarchy is enabled The memory cgroup that went over limit get a memory cgroup to clean up after OOM task to be killed by the OOM killer memcg in case of memcg OOM, NULL in case of system-wide OOM Returns a pointer to a memory cgroup, which has to be cleaned up by killing all belonging OOM-killable tasks. Caller has to call mem_cgroup_put() on the returned non-NULL memcg. Try to consume stocked charge on this cpu. how many pages to charge. The charges will only happen if memcg matches the current cpu’s memcg stock, and at least nr_pages are available in that stock. Failure to service an allocation will refill the stock. Returns 0 on success, an error code on failure. out parameter for number of file pages out parameter for number of allocatable pages according to memcg out parameter for number of dirty pages out parameter for number of pages under writeback Determine the numbers of file, headroom, dirty, and writeback pages in wb’s memcg. File, dirty and writeback are self-explanatory. Headroom is a bit more involved. A memcg’s headroom is “min(max, high) - used”. In the hierarchy, the headroom is calculated as the lowest headroom of itself and the ancestors. Note that this doesn’t consider the actual amount of available memory in the system. The caller should further cap *pheadroom accordingly. look up a memcg from a memcg id Reset the states of the mem_cgroup associated with css. This is invoked when the userland requests disabling on the default hierarchy but the memcg is pinned through dependency. The memcg should stop applying policies and should revert to the vanilla state as it may be made visible again. The current implementation only resets the essential configurations. This needs to be expanded to cover all the visible parts. check if memory consumption is in the normal range the top ancestor of the sub-tree being checked WARNING: This function is not stateless! It can only be used as part of a top-down tree iteration, not for isolated queries. This function is called when allocating a huge page folio, after the page has already been obtained and charged to the appropriate hugetlb cgroup controller (if it is enabled). Returns ENOMEM if the memcg is already full. Returns 0 if either the charge was successful, or if we skip the charging. swap entry for which the folio is allocated This function charges a folio allocated for swapin. Please call this before adding the folio to the swapcache. Returns 0 on success. Otherwise, an error code is returned. Charge new as a replacement folio for old. old will be uncharged upon free. Both folios must be locked, new->mapping must be set up. Transfer the memcg data from the old to the new folio. Transfer the memcg data from the old folio to the new folio for migration. The old folio’s data info will be cleared. Note that the memory counters will remain unchanged throughout the process. Both folios must be locked, new->mapping must be set up. Charges nr_pages to memcg. Returns if the charge fit within memcg’s configured limit, if it doesn’t. swap entry to move the charge to Transfer the memsw charge of folio to entry. Try to charge folio’s memcg for the swap space at entry. the amount of swap space to uncharge check if this cgroup can zswap Check if the hierarchical zswap limit has been reached. This doesn’t check for specific headroom, and it is not atomic either. But with zswap, the size of the allocation is only known once compression has occurred, and this optimistic pre-check avoids spending cycles on compression when there is already no room left or zswap is disabled altogether somewhere in the hierarchy. This forces the charge after allowed compression and storage in zwap for this cgroup to go ahead. recalculate the block usage of an inode the change in number of pages allocated to inode the change in number of pages swapped from inode We have to calculate the free blocks since the mm can drop undirtied hole pages behind our back. But normally info->alloced == inode->i_mapping->nrpages + info->swapped So mm freed is info->alloced - (inode->i_mapping->nrpages + info->swapped) Get allowable folio orders for the given file size. This returns huge orders for folios (when supported) based on the file size which the mapping currently allows at the given index. The index is relevant due to alignment considerations the mapping might have. The returned order may be less than the size passed. pointer to the folio if found Looks up the page cache entry at inode & index. If a folio is present, it is returned locked with an increased refcount. If the caller modifies data in the folio, it must call before unlocking the folio to ensure that the folio is not reclaimed. There is no need to reserve space before calling . When no folio is found, the behavior depends on sgp:\n• None for SGP_READ, *foliop is and 0 is returned\n• None for SGP_NOALLOC, *foliop is and -ENOENT is returned\n• None for all other flags a new folio is allocated, inserted into the page cache and returned locked in foliop. get an unlinked file living in tmpfs which must be kernel internal. There will be NO LSM permission checks against the underlying inode. So users of this interface must do LSM checks at a higher layer. The users are the big_key and shm implementations. LSM checks are provided at the key or shm level rather than the inode. name for dentry (to be seen in /proc/<pid>/maps size to be set for the file get an unlinked file living in tmpfs name for dentry (to be seen in /proc/<pid>/maps size to be set for the file get an unlinked file living in tmpfs the tmpfs mount where the file will be created name for dentry (to be seen in /proc/<pid>/maps size to be set for the file the vma to be mmapped is prepared by do_mmap read into page cache, using specified page allocation flags. the page allocator flags to use if allocating This behaves as a tmpfs “read_cache_page_gfp(mapping, index, gfp)”, with any new page allocations done using the specified allocation flags. But uses the ->read_folio() method: which does not suit tmpfs, since it may have pages in swapcache, and needs to find those for itself; although drivers/gpu/drm i915 and ttm rely upon this support. i915_gem_object_get_pages_gtt() mixes __GFP_NORETRY | __GFP_NOWARN in with the mapping_gfp_mask(), to avoid OOMing the machine unnecessarily. contains the vma, start, and pfns arrays for the migration negative errno on failures, 0 when 0 or more pages were migrated without an error. Prepare to migrate a range of memory virtual address range by collecting all the pages backing each virtual address in the range, saving them inside the src array. Then lock those pages and unmap them. Once the pages are locked and unmapped, check whether each page is pinned or not. Pages that aren’t pinned have the MIGRATE_PFN_MIGRATE flag set (by this function) in the corresponding src array entry. Then restores any pages that are pinned, by remapping and unlocking those pages. The caller should then allocate destination memory and copy source memory to it for all those entries (ie with MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE flag set). Once these are allocated and copied, the caller must update each corresponding entry in the dst array with the pfn value of the destination page and with MIGRATE_PFN_VALID. Destination pages must be locked via . Note that the caller does not have to migrate all the pages that are marked with MIGRATE_PFN_MIGRATE flag in src array unless this is a migration from device memory to system memory. If the caller cannot migrate a device page back to system memory, then it must return VM_FAULT_SIGBUS, which has severe consequences for the userspace process, so it must be avoided if at all possible. For empty entries inside CPU page table (pte_none() or pmd_none() is true) we do set MIGRATE_PFN_MIGRATE flag inside the corresponding source array thus allowing the caller to allocate device memory for those unbacked virtual addresses. For this the caller simply has to allocate device memory and properly set the destination entry like for regular migration. Note that this can still fail, and thus inside the device driver you must check if the migration was successful for those entries after calling , just like for regular migration. After that, the callers must call to go over each entry in the src array that has the MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE flag set. If the corresponding entry in dst array has MIGRATE_PFN_VALID flag set, then to migrate struct page information from the source struct page to the destination struct page. If it fails to migrate the struct page information, then it clears the MIGRATE_PFN_MIGRATE flag in the src array. At this point all successfully migrated pages have an entry in the src array with MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE flag set and the dst array entry with MIGRATE_PFN_VALID flag set. Once returns the caller may inspect which pages were successfully migrated, and which were not. Successfully migrated pages will have the MIGRATE_PFN_MIGRATE flag set for their src array entry. It is safe to update device page table after because both destination and source page are still locked, and the mmap_lock is held in read mode (hence no one can unmap the range being migrated). Once the caller is done cleaning up things and updating its page table (if it chose to do so, this is not an obligation) it finally calls to update the CPU page table to point to new pages for successfully migrated pages or otherwise restore the CPU page table to point to the original source pages. array of pfns allocated by the driver to migrate memory to number of pages in the range Equivalent to . This is called to migrate struct page meta-data from source struct page to destination. This migrates struct page meta-data from source struct page to destination struct page. This effectively finishes the migration from source page to the destination page. This replaces the special migration pte entry with either a mapping to the new page if migration was successful for that page, or to the original page otherwise. This also unlocks the pages and puts them back on the lru, or drops the extra refcount, for device pages. starting pfn in the range to migrate. is similar in concept to except that instead of looking up pages based on virtual address mappings a range of device pfns that should be migrated to system memory is used instead. This is useful when a driver needs to free device memory but doesn’t know the virtual mappings of every page that may be in device memory. For example this is often the case when a driver is being unloaded or unbound from a device. Like this function will take a reference and lock any migrating pages that aren’t free before unmapping them. Drivers may then allocate destination pages and start copying data from the device to CPU memory before calling . The function write-protects a pte and records the range in virtual address space of touched ptes for efficient range TLB flushes. Address_space Page offset of the first bit in bitmap Bitmap with one bit for each page offset in the address_space range covered. Address_space page offset of first modified pte relative to bitmap_pgoff Address_space page offset of last modified pte relative to bitmap_pgoff Clean a pte and record its address space offset in a bitmap The start of virtual address to be clean The end of virtual address to be clean The function cleans a pte and records the range in virtual address space of touched ptes for efficient TLB flushes. It also records dirty ptes in a bitmap representing page offsets in the address_space, as well as the first and last of the bits touched. Write-protect all ptes in an address space range The address_space we want to write protect The first page offset in the range This function currently skips transhuge page-table entries, since it’s intended for dirty-tracking on the PTE level. It will warn on encountering transhuge write-enabled entries, though, and can easily be extended to handle them as well. The number of ptes actually write-protected. Note that already write-protected ptes are not counted. Clean and record all ptes in an address space range The address_space we want to clean The first page offset in the range The page offset of the first bit in bitmap Pointer to a bitmap of at least nr bits. The bitmap needs to cover the whole range first_index..**first_index** + nr. Pointer to number of the first set bit in bitmap. is modified as new bits are set by the function. Pointer to the number of the last set bit in bitmap. none set. The value is modified as new bits are set by the function. When this function returns there is no guarantee that a CPU has not already dirtied new ptes. However it will not clean any ptes not reported in the bitmap. The guarantees are as follows:\n• None All ptes dirty when the function starts executing will end up recorded in the bitmap.\n• None All ptes dirtied after that will either remain dirty, be recorded in the bitmap or both. If a caller needs to make sure all dirty ptes are picked up and none additional are added, it first needs to write-protect the address-space range and make sure new writers are blocked in page_mkwrite() or pfn_mkwrite(). And then after a TLB flush following the write-protection pick up all dirty bits. This function currently skips transhuge page-table entries, since it’s intended for dirty-tracking on the PTE level. It will warn on encountering transhuge dirty entries, though, and can easily be extended to handle them as well. The number of dirty ptes actually cleaned. check if the address is served from this chunk True if the address is served from this chunk. Check to see if the allocation can fit in the block’s contig hint. Note, a chunk uses the same hints as a block so this can also check against the chunk’s contig hint. Helper function for pcpu_for_each_md_free_region. It checks block->contig_hint and performs aggregation across blocks to find the next hint. It modifies bit_off and bits in-place to be consumed in the loop. Finds the next free region that is viable for use with a given size and alignment. This only returns if there is a valid area to be used for this allocation. block->first_free is returned if the allocation request fits within the block to see if the request can be fulfilled prior to the contig hint. Allocate size bytes. If size is smaller than PAGE_SIZE, is used; otherwise, the equivalent of vzalloc() is used. This is to facilitate passing through whitelisted flags. The returned memory is always zeroed. Pointer to the allocated area on success, NULL on failure. Free ptr. ptr should have been allocated using . put chunk in the appropriate chunk slot the previous slot it was on This function is called after an allocation or free changed chunk. New slot according to the changed state is determined and chunk is moved to the slot. Note that the reserved chunk is never put on chunk slots. Updates a block given a known free area. The region [start, end) is expected to be the entirety of the free area within a block. Chooses the best starting offset if the contig hints are equal. if we should scan from the beginning Iterates over the metadata blocks to find the largest contig area. A full scan can be avoided on the allocation path as this is triggered if we broke the contig_hint. In doing so, the scan_hint will be before the contig_hint or after if the scan_hint == contig_hint. This cannot be prevented on freeing as we want to find the largest area possibly spanning blocks. Scans over the block beginning at first_free and updates the block metadata accordingly. Updates metadata for the allocation path. The metadata only has to be refreshed by a full scan iff the chunk’s contig hint is broken. Block level scans are required if the block’s contig hint is broken. updates the block hints on the free path Updates metadata for the allocation path. This avoids a blind block refresh by making use of the block contig hints. If this fails, it scans forward and backward to determine the extent of the free area. This is capped at the boundary of blocks. A chunk update is triggered if a page becomes free, a block becomes free, or the free spans across blocks. This tradeoff is to minimize iterating over the block metadata to update chunk_md->contig_hint. chunk_md->contig_hint may be off by up to a page, but it will never be more than the available space. If the contig hint is contained in one block, it will be accurate. determines if the region is populated return value for the next offset to start searching For atomic allocations, check if the backing pages are populated. Bool if the backing pages are populated. next_index is to skip over unpopulated blocks in pcpu_find_block_fit. Given a chunk and an allocation spec, find the offset to begin searching for a free region. This iterates over the bitmap metadata blocks to find an offset that will be guaranteed to fit the requirements. It is not quite first fit as if the allocation does not fit in the contig hint of a block or chunk, it is skipped. This errs on the side of caution to prevent excess iteration. Poor alignment can cause the allocator to skip over blocks and chunks that have valid free areas. The offset in the bitmap to begin searching. -1 if no offset is found. This function takes in a start offset to begin searching to fit an allocation of alloc_bits with alignment align. It needs to scan the allocation map because if it fits within the block’s contig hint, start will be block->first_free. This is an attempt to fill the allocation prior to breaking the contig hint. The allocation and boundary maps are updated accordingly if it confirms a valid free area. Allocated addr offset in chunk on success. -1 if no matching area is found. This function determines the size of an allocation to free using the boundary bitmap and clears the allocation map. creates chunks that serve the first chunk the start of the region served This is responsible for creating the chunks that serve the first chunk. The base_addr is page aligned down of tmp_addr while the region end is page aligned up. Offsets are kept track of to determine the region served. All this is done to appease the bitmap allocator in avoiding partial blocks. Chunk serving the region at tmp_addr of map_size. Pages in [page_start,**page_end**) have been populated to chunk. Update the bookkeeping information accordingly. Must be called after each successful population. Pages in [page_start,**page_end**) have been depopulated from chunk. Update the bookkeeping information accordingly. Must be called after each successful depopulation. address for which the chunk needs to be determined. This is an internal function that handles all but static allocations. Static percpu address values should never be passed into the allocator. The address of the found chunk. size of area to allocate in bytes allocate from the reserved chunk if available Allocate percpu area of size bytes aligned at align. If gfp doesn’t contain , the allocation is atomic. If gfp has __GFP_NOWARN then no warning will be triggered on invalid or failed allocation requests. Percpu pointer to the allocated area on success, NULL on failure. free chunks only if there are no populated pages If empty_only is , reclaim all fully free chunks regardless of the number of populated pages. Otherwise, only reclaim chunks that have no populated pages. Maintain a certain amount of populated pages to satisfy atomic allocations. It is possible that this is called when physical memory is scarce causing OOM killer to be triggered. We should avoid doing so until an actual allocation causes the failure as it is possible that requests can be serviced from already backed regions. Scan over chunks in the depopulate list and try to release unused populated pages back to the system. Depopulated chunks are sidelined to prevent repopulating these pages unless required. Fully free chunks are reintegrated and freed accordingly (1 is kept around). If we drop below the empty populated pages threshold, reintegrate the chunk if it has empty free pages. Each chunk is scanned in the reverse order to keep populated pages close to the beginning of the chunk. manage the amount of free chunks and populated pages For each chunk type, manage the number of fully free chunks and the number of populated pages. An important thing to consider is when pages are freed and how they contribute to the global counts. Can be called from atomic context. test whether address is from static percpu area Test whether addr belongs to in-kernel static percpu area. Module static percpu areas are not considered. For those, use is_module_percpu_address(). if addr is from in-kernel static percpu area, otherwise. the address to be converted to physical address Given addr which is dereferenceable address obtained via one of percpu access macros, this function translates it into its physical address. The caller is responsible for ensuring addr stays valid until this function finishes. percpu allocator has special setup for the first chunk, which currently supports either embedding in linear address space or vmalloc mapping, and, from the second one, the backing allocator (currently either vm or km) provides translation. The addr can be translated simply without checking if it falls into the first chunk. But the current code reflects better how percpu allocator actually works, and the verification can discover both bugs in percpu allocator itself and callers. So we keep current code. Allocate ai which is large enough for nr_groups groups containing nr_units units. The returned ai’s groups[0].cpu_map points to the cpu_map array which is long enough for nr_units and filled with NR_CPUS. It’s the caller’s responsibility to initialize cpu_map pointer of other groups. Pointer to the allocated pcpu_alloc_info on success, NULL on failure. Free ai which was allocated by . Print out information about ai using loglevel lvl. pcpu_alloc_info describing how to percpu area is shaped Initialize the first percpu chunk which contains the kernel static percpu area. This function is to be called from arch percpu area setup path. ai contains all information necessary to initialize the first chunk and prime the dynamic percpu allocator. ai->static_size is the size of static percpu area. ai->reserved_size, if non-zero, specifies the amount of bytes to reserve after the static area in the first chunk. This reserves the first chunk such that it’s available only through reserved percpu allocation. This is primarily used to serve module percpu static areas on architectures where the addressing model has limited offset range for symbol relocations to guarantee module percpu symbols fall inside the relocatable range. ai->dyn_size determines the number of bytes available for dynamic allocation in the first chunk. The area between ai->static_size + ai->reserved_size + ai->dyn_size and ai->unit_size is unused. ai->unit_size specifies unit size and must be aligned to PAGE_SIZE and equal to or larger than ai->static_size + ai->reserved_size + ai->dyn_size. ai->atom_size is the allocation atom size and used as alignment for vm areas. ai->alloc_size is the allocation size and always multiple of ai->atom_size. This is larger than ai->atom_size if ai->unit_size is larger than ai->atom_size. ai->nr_groups and ai->groups describe virtual memory layout of percpu areas. Units which should be colocated are put into the same group. Dynamic VM areas will be allocated according to these groupings. If ai->nr_groups is zero, a single group containing all units is assumed. The caller should have mapped the first chunk at base_addr and copied static data to each unit. The first chunk will always contain a static and a dynamic region. However, the static region is not managed by any chunk. If the first chunk also contains a reserved region, it is served by two chunks - one for the reserved region and one for the dynamic region. They share the same vm, but use offset regions in the area allocation map. The chunk serving the dynamic region is circulated in the chunk slots and available for dynamic allocation like any other chunk. the size of reserved percpu area in bytes This function determines grouping of units, their mappings to cpus and other parameters considering needed percpu size, allocation atom size and distances between CPUs. Groups are always multiples of atom size and CPUs which are of LOCAL_DISTANCE both ways are grouped together and share space for units in the same group. The returned configuration is guaranteed to have CPUs on different nodes on different groups and >=75% usage of allocated virtual address space. On success, pointer to the new allocation_info is returned. On failure, ERR_PTR value is returned. embed the first percpu chunk into bootmem the size of reserved percpu area in bytes This is a helper to ease setting up embedded first percpu chunk and can be called where is expected. If this function is used to setup the first chunk, it is allocated by calling pcpu_fc_alloc and used as-is without being mapped into vmalloc area. Allocations are always whole multiples of atom_size aligned to atom_size. This enables the first chunk to piggy back on the linear physical mapping which often uses larger page size. Please note that this can result in very sparse cpu->unit mapping on NUMA machines thus requiring large vmalloc address space. Don’t use this allocator if vmalloc space is not orders of magnitude larger than distances between node memory addresses (ie. 32bit NUMA machines). If the needed size is smaller than the minimum or specified unit size, the leftover is returned using pcpu_fc_free. map the first chunk using PAGE_SIZE pages the size of reserved percpu area in bytes This is a helper to ease setting up page-remapped first percpu chunk and can be called where is expected. This is the basic allocator. Static percpu area is allocated page-by-page into vmalloc area. pointer to the buffer that shall take the data address to read from. This must be a user address. Safely read from user address src to the buffer at dst. If a kernel fault happens, handle that and return -EFAULT. pointer to the data that shall be written Safely write to address dst from the buffer at src. If a kernel fault happens, handle that and return -EFAULT. Destination address, in kernel space. This buffer must be at least count bytes long. Maximum number of bytes to copy, including the trailing NUL. On success, returns the length of the string INCLUDING the trailing NUL. If access fails, returns -EFAULT (some data may have been copied and the trailing NUL added). If count is smaller than the length of the string, copies count-1 bytes, sets the last byte of dst buffer to NUL and returns count.\n• None Get the size of a user string INCLUDING final NUL. Get the size of a NUL-terminated string in user space without pagefault. Returns the size of the string INCLUDING the terminating NUL. If the string is too long, returns a number larger than count. User has to check the return value against “> count”. On exception (or invalid count), returns 0. Unlike strnlen_user, this can be used from IRQ handler etc. because it disables pagefaults. is the usual dirty throttling mechanism available? The normal page dirty throttling mechanism in balance_dirty_pages() is completely broken with the legacy memcg and direct stalling in shrink_folio_list() is used for throttling instead, which lacks all the niceties such as fairness, adaptive pausing, bandwidth proportional allocation and configurability. This function tests whether the vmscan currently in progress can assume that the normal dirty throttling mechanism is operational. Returns the number of pages on the given LRU list. zones to consider (use MAX_NR_ZONES - 1 for the whole LRU list) Attempt to remove a folio from its mapping. If the folio is dirty, under writeback or if someone else has a ref on it, removal will fail. The number of pages removed from the mapping. 0 if the folio could not be removed. The caller should have a single refcount on the folio and hold its lock. Folio to be returned to an LRU list. Add previously isolated folio to appropriate LRU list. The folio may still be unevictable for other reasons. lru_lock must not be held, interrupts must be enabled. Try to isolate a folio from its LRU list. Folio to isolate from its LRU list. Isolate a folio from an LRU list and adjust the vmstat statistic corresponding to whatever LRU list the folio was on. The folio will have its LRU flag cleared. If it was found on the active list, it will have the Active flag set. If it was found on the unevictable list, it will have the Unevictable flag set. These flags may need to be cleared by the caller before letting the page go.\n• None Must be called with an elevated refcount on the folio. This is a fundamental difference from isolate_lru_folios() (which is called without a stable reference).\n• None The lru_lock must not be held. true if the folio was removed from an LRU list. false if the folio was not on an LRU list. Checks folios for evictability, if an evictable folio is in the unevictable lru list, moves it to the appropriate evictable lru list. This function should be only used for lru folios. starting pageframe (must be aligned to start of a section) number of pages to remove (must be multiple of section size) alternative device page map or if default memmap is used Generic helper function to remove section mappings and sysfs entries for the section of the memory we are removing. Caller needs to make sure that pages are marked reserved and zones are adjust properly by calling offline_pages(). Offline a node if all memory sections and cpus of the node are removed. The caller must call lock_device_hotplug() to serialize hotplug and online/offline operations before this call. Remove memory if every memory block is offline physical address of the region to remove size of the region to remove The caller must call lock_device_hotplug() to serialize hotplug and online/offline operations before this call, as required by . mmu_iterval_read_begin()/mmu_iterval_read_retry() implement a collision-retry scheme similar to seqcount for the VA range under subscription. If the mm invokes invalidation during the critical section then mmu_interval_read_retry() will return true. This is useful to obtain shadow PTEs where teardown or setup of the SPTEs require a blocking context. The critical region formed by this can sleep, and the required ‘user_lock’ can also be a sleeping lock. The caller is required to provide a ‘user_lock’ to serialize both teardown and setup. The return value should be passed to mmu_interval_read_retry(). The mm to attach the notifier to Must not hold mmap_lock nor any other VM related lock when calling this registration function. Must also ensure mm_users can’t go down to zero while this runs to avoid races with mmu_notifier_release, so mm has to be current->mm or the mm should be pinned safely such as with get_task_mm(). If the mm is not current->mm, the mm_users pin should be released by calling mmput after mmu_notifier_register returns. mmu_notifier_unregister() or must be always called to unregister the notifier. While the caller has a mmu_notifier get the subscription->mm pointer will remain valid, and can be converted to an active mm pointer via mmget_not_zero(). Return the single struct mmu_notifier for the mm & ops The operations struct being subscribe with The mm to attach notifiers too This function either allocates a new mmu_notifier via ops->alloc_notifier(), or returns an already existing notifier on the list. The value of the ops pointer is used to determine when two notifiers are the same. Each call to mmu_notifier_get() must be paired with a call to . The caller must hold the write side of mm->mmap_lock. While the caller has a mmu_notifier get the mm pointer will remain valid, and can be converted to an active mm pointer via mmget_not_zero(). Release the reference on the notifier This function must be paired with each mmu_notifier_get(), it releases the reference obtained by the get. If this is the last reference then process to free the notifier will be run asynchronously. Unlike mmu_notifier_unregister() the get/put flow only calls ops->release when the mm_struct is destroyed. Instead free_notifier is always called to release any resources held by the user. As ops->release is not guaranteed to be called, the user must ensure that all sptes are dropped, and no new sptes can be established before is called. This function can be called from the ops->release callback, however the caller must still ensure it is called pairwise with mmu_notifier_get(). Modules calling this function must call in their __exit functions to ensure the async work is completed. Length of the range to monitor Interval notifier operations to be called on matching events This function subscribes the interval notifier for notifications from the mm. Upon return the ops related to mmu_interval_notifier will be called whenever an event that intersects with the given range occurs. Upon return the range_notifier may not be present in the interval tree yet. The caller must use the normal interval notifier read flow via to establish SPTEs for this range. This function must be paired with . It cannot be called from any ops callback. Once this returns ops callbacks are no longer running on other CPUs and will not be called in future. This function ensures that all outstanding async SRU work from is completed. After it returns any mmu_notifier_ops associated with an unused mmu_notifier will no longer be called. Before using the caller must ensure that all of its mmu_notifiers have been fully released via . Modules using the API should call this in their __exit function to avoid module unloading races. inserts a list of pages into the balloon page list. balloon device descriptor where we will insert a new page to Driver must call this function to properly enqueue balloon pages before definitively removing them from the guest system. number of pages that were enqueued. removes pages from balloon’s page list and returns a list of the pages. balloon device descriptor where we will grab a page from. pointer to the list of pages that would be returned to the caller. Driver must call this function to properly de-allocate a previous enlisted balloon pages before definitively releasing it back to the guest system. This function tries to remove n_req_pages from the ballooned pages and return them to the caller in the pages list. Note that this function may fail to dequeue some pages even if the balloon isn’t empty - since the page list can be temporarily empty due to compaction of isolated pages. number of pages that were added to the pages list. this is only safe if the mm semaphore is held when called."
    },
    {
        "link": "https://stackoverflow.com/questions/6401262/available-memory-in-kernel",
        "document": "First, let me say that if you're going to make any policy decisions (should I proceed with this operation?) based on this information, STOP. As WGW pointed out, there are unavoidable races here; memory can be used up between when you check and when you use it. Just test for errors on your memory allocations and have an appropriate failure path. Moreover, if you request memory when there isn't enough free memory, often the kernel can obtain more free memory by cleaning up various cache memory, swapping to disk, freeing slabs, etc. And kernel memory fragmentation can fail large (multiple page) allocations when not made through vmalloc even with plenty of memory free.\n\nThat said, there are APIs for querying kernel memory availability. You should note that the kernel has multiple memory pools, so even if one of these API says you have no free RAM, it could be that it's available in the memory pool you are interested in.\n\nFirst, we have si_meminfo. This is the call that provides availability data for , among other things, and reports on the current state of the buddy page allocator. Note that cached and buffer ram can be converted to free ram very quickly.\n\ncan also be used to get counts of how much slab memory can be quickly reclaimed. If you request an allocation, this memory can and will be freed on demand.\n\nThe SLUB allocator (used for and the like, among others) also provides statistics for its internal memory pools that can also reflect free memory within each memory pool. This may not be available with the same API depending on which allocator is selected in your configuration - please do not use this data except for debugging. The relevant code (implementing ) can be found in mm/slub.c"
    },
    {
        "link": "https://stackoverflow.com/questions/48550984/where-are-enum-for-meminfo-proc-show-and-variables-for-si-meminfo-set",
        "document": "I am not a developer but I understand some C concepts. However, I'm having a hard time finding where the enums (e.g , etc) in meminfo.c/meminfo_proc_show() and the variables (e.g. totalram_pages, etc) in page_alloc.c/si_meminfo() are set.\n\nWhat I meant by set is for example for instance. What I understood there is that equals , but there's no operator in front of , so it must be set somewhere else.\n\nI've clicked on the enums/variables to see where they may be called, but there's either too much unrelevant or either non-defining references.\n\nThe last thing would be me not being aware of something, but what ?\n\nTo be honest, my goal here is to determine how /proc/meminfo 's values are calculated.\n\nBut, here my question is: Where do these enums and variables are set ?\n\nThe enums part is now solved, and equals .\n\nBut the part seems to be harder to find out..."
    },
    {
        "link": "https://dri.freedesktop.org/docs/drm/core-api/mm-api.html",
        "document": "These flags provide hints about how mobile the page is. Pages with similar mobility are placed within the same pageblocks to minimise problems due to external fragmentation. (also a zone modifier) indicates that the page can be moved by page migration during memory compaction or can be reclaimed. is used for slab allocations that specify SLAB_RECLAIM_ACCOUNT and whose pages can be freed via shrinkers. indicates the caller intends to dirty the page. Where possible, these pages will be spread between local zones to avoid all the dirty pages being in one zone (fair zone allocation policy). forces the allocation to be satisfied from the requested node with no fallbacks or placement policy enforcements. causes the allocation to be accounted to kmemcg. causes slab allocation to have no object extension. indicates that the caller is high-priority and that granting the request is necessary before the system can make forward progress. For example creating an IO context to clean pages and requests from atomic context. allows access to all memory. This should only be used when the caller guarantees the allocation will allow more memory to be freed very shortly e.g. process exiting or swapping. Users either should be the MM or co-ordinating closely with the VM (e.g. swap over NFS). Users of this flag have to be extremely careful to not deplete the reserve completely and implement a throttling mechanism which controls the consumption of the reserve based on the amount of freed memory. Usage of a pre-allocated pool (e.g. mempool) should be always considered before using this flag. is used to explicitly forbid access to emergency reserves. This takes precedence over the flag if both are set. Please note that all the following flags are only applicable to sleepable allocations (e.g. and will ignore them). can call down to the low-level FS. Clearing the flag avoids the allocator recursing into the filesystem which might already be holding locks. indicates that the caller may enter direct reclaim. This flag can be cleared to avoid unnecessary delays when a fallback option is available. indicates that the caller wants to wake kswapd when the low watermark is reached and have it reclaim pages until the high watermark is reached. A caller may wish to clear this flag when fallback options are available and the reclaim is likely to disrupt the system. The canonical example is THP allocation where a fallback is cheap but reclaim/compaction may cause indirect stalls. is shorthand to allow/forbid both direct and kswapd reclaim. The default allocator behavior depends on the request size. We have a concept of so-called costly allocations (with order > ). !costly allocations are too essential to fail so they are implicitly non-failing by default (with some exceptions like OOM victims might fail so the caller still has to check for failures) while costly requests try to be not disruptive and back off even without invoking the OOM killer. The following three modifiers might be used to override some of these implicit rules. Please note that all of them must be used along with flag. : The VM implementation will try only very lightweight memory direct reclaim to get some memory under memory pressure (thus it can sleep). It will avoid disruptive actions like OOM killer. The caller must handle the failure which is quite likely to happen under heavy memory pressure. The flag is suitable when failure can easily be handled at small cost, such as reduced throughput. : The VM implementation will retry memory reclaim procedures that have previously failed if there is some indication that progress has been made elsewhere. It can wait for other tasks to attempt high-level approaches to freeing memory such as compaction (which removes fragmentation) and page-out. There is still a definite limit to the number of retries, but it is a larger limit than with . Allocations with this flag may fail, but only when there is genuinely little unused memory. While these allocations do not directly trigger the OOM killer, their failure indicates that the system is likely to need to use the OOM killer soon. The caller must handle failure, but can reasonably do so by failing a higher-level request, or completing it only in a much less efficient manner. If the allocation does fail, and the caller is in a position to free some non-essential memory, doing so could benefit the system as a whole. : The VM implementation _must_ retry infinitely: the caller cannot handle allocation failures. The allocation could block indefinitely but will never return with failure. Testing for failure is pointless. It _must_ be blockable and used together with __GFP_DIRECT_RECLAIM. It should _never_ be used in non-sleepable contexts. New users should be evaluated carefully (and the flag should be used only when there is no reasonable failure policy) but it is definitely preferable to use the flag rather than opencode endless loop around allocator. Allocating pages from the buddy with __GFP_NOFAIL and order > 1 is not supported. Please consider using kvmalloc() instead. Useful GFP flag combinations that are commonly used. It is recommended that subsystems start with one of these combinations and then set/clear flags as necessary. users can not sleep and need the allocation to succeed. A lower watermark is applied to allow access to “atomic reserves”. The current implementation doesn’t support NMI and few other strict non-preemptive contexts (e.g. raw_spin_lock). The same applies to . is typical for kernel-internal allocations. The caller requires or a lower zone for direct access but can direct reclaim. is the same as GFP_KERNEL, except the allocation is accounted to kmemcg. is for kernel allocations that should not stall for direct reclaim, start physical IO or use any filesystem callback. It is very likely to fail to allocate memory, even for very small allocations. will use direct reclaim to discard clean pages or slab pages that do not require the starting of any physical IO. Please try to avoid using this flag directly and instead use memalloc_noio_{save,restore} to mark the whole scope which cannot perform any IO with a short explanation why. All allocation requests will inherit GFP_NOIO implicitly. will use direct reclaim but will not use any filesystem interfaces. Please try to avoid using this flag directly and instead use memalloc_nofs_{save,restore} to mark the whole scope which cannot/shouldn’t recurse into the FS layer with a short explanation why. All allocation requests will inherit GFP_NOFS implicitly. is for userspace allocations that also need to be directly accessibly by the kernel or hardware. It is typically used by hardware for buffers that are mapped to userspace (e.g. graphics) that hardware still must DMA to. cpuset limits are enforced for these allocations. exists for historical reasons and should be avoided where possible. The flags indicates that the caller requires that the lowest zone be used ( or 16M on x86-64). Ideally, this would be removed but it would require careful auditing as some users really require it and others use the flag to avoid lowmem reserves in and treat the lowest zone as a type of emergency reserve. is similar to except that the caller requires a 32-bit address. Note that kmalloc(..., GFP_DMA32) does not return DMA32 memory because the DMA32 kmalloc cache array is not implemented. (Reason: there is no such user in kernel). is for userspace allocations that may be mapped to userspace, do not need to be directly accessible by the kernel but that cannot move once in use. An example may be a hardware allocation that maps data directly into userspace but has no addressing limitations. is for userspace allocations that the kernel does not need direct access to but can use when access is required. They are expected to be movable via page reclaim or page migration. Typically, pages on the LRU would also be allocated with . and are used for THP allocations. They are compound allocations that will generally fail quickly if memory is not available and will not wake kswapd/kcompactd on failure. The _LIGHT version does not attempt reclaim/compaction at all and is by default used in page fault path, while the non-light is used by khugepaged.\n\nSufficiently large objects are aligned on cache line boundary. For object size smaller than a half of cache line size, the alignment is on the half of cache line size. In general, if object size is smaller than 1/2^n of cache line size, the alignment is adjusted to 1/2^n. If explicit alignment is also requested by the respective field, the greater of both is alignments is applied. This delays freeing the SLAB page by a grace period, it does _NOT_ delay object freeing. This means that if you do that memory location is free to be reused at any time. Thus it may be possible to see another object there in the same RCU grace period. This feature only ensures the memory location backing the object stays valid, the trick to using this is relying on an independent object validation pass. Something like: begin: rcu_read_lock(); obj = lockless_lookup(key); if (obj) { if (!try_get_ref(obj)) // might fail for free objects rcu_read_unlock(); goto begin; if (obj->key != key) { // not the object we expected put_ref(obj); rcu_read_unlock(); goto begin; } } rcu_read_unlock(); This is useful if we need to approach a kernel structure obliquely, from its address obtained without the usual locking. We can lock the structure to stabilize it and check it’s still at the given address, only if we can be sure that the memory has not been meanwhile reused for some other kind of object (which our subsystem’s lock might corrupt). rcu_read_lock before reading the address, then rcu_read_unlock after taking the spinlock within the structure expected at that address. Note that it is not possible to acquire a lock within a structure allocated with SLAB_TYPESAFE_BY_RCU without first acquiring a reference as described above. The reason is that SLAB_TYPESAFE_BY_RCU pages are not zeroed before being given to the slab, which means that any locks must be initialized after each and every kmem_struct_alloc(). Alternatively, make the ctor passed to initialize the locks at page-allocation time, as is done in __i915_request_ctor(), sighand_ctor(), and anon_vma_ctor(). Such a ctor permits readers to safely acquire those ctor-initialized locks under protection. All object allocations from this cache will be memcg accounted, regardless of __GFP_ACCOUNT being or not being passed to individual allocations. Use this flag for caches that have an associated shrinker. As a result, slab pages are allocated with __GFP_RECLAIMABLE, which affects grouping pages by mobility, and are accounted in SReclaimable counter in /proc/meminfo The required alignment for the objects. is a valid offset, when usersize is non- means no usercopy region is specified. Custom offset for the free pointer in caches By default caches place the free pointer outside of the object. This might cause the object to grow in size. Cache creators that have a reason to avoid this can specify a custom free pointer offset in their struct where the free pointer will be placed. Note that placing the free pointer inside the object requires the caller to ensure that no fields are invalidated that are required to guard against object recycling (See for details). Using as a value for freeptr_offset is valid. If freeptr_offset is specified, must be set . Note that ctor currently isn’t supported with custom free pointers as a ctor requires an external free pointer. Whether a freeptr_offset is used. The constructor is invoked for each object in a newly allocated slab page. It is the cache user’s responsibility to free object in the same state as after calling the constructor, or deal appropriately with any differences between a freshly constructed and a reallocated object. Any uninitialized fields of the structure are interpreted as unused. The exception is freeptr_offset where is a valid value, so use_freeptr_offset must be also set to in order to interpret the field as used. For useroffset is also valid, but only with non- usersize. When args is passed to , it is equivalent to all fields unused. Create a kmem cache with a region suitable for copying to userspace. A string which is used in /proc/slabinfo to identify this cache. The size of objects to be created in this cache. The required alignment for the objects. A constructor for the objects, or . This is a legacy wrapper, new code should use either KMEM_CACHE_USERCOPY() if whitelisting a single field is sufficient, or with the necessary parameters passed via the args parameter (see ) a pointer to the cache on success, NULL on failure. A string which is used in /proc/slabinfo to identify this cache. The size of objects to be created in this cache. Optional arguments, see . Passing means defaults will be used for all the arguments. This is currently implemented as a macro using to call either the new variant of the function, or a legacy one. The new variant has 4 parameters: See which implements this. The align and ctor parameters map to the respective fields of Cannot be called within a interrupt, but can be interrupted. a pointer to the cache on success, NULL on failure. This should not be used for writing beyond the originally requested allocation size. Either use krealloc() or round up the allocation size with prior to allocation. If this is used to access beyond the originally requested allocation size, UBSAN_BOUNDS and/or FORTIFY_SOURCE may trip, since they only know about the originally allocated size via the __alloc_size attribute. The cache to allocate from. Allocate an object from this cache. See kmem_cache_zalloc() for a shortcut of adding __GFP_ZERO to flags. pointer to the new object or in case of error address of the slab object to memcg charge kmem_cache_charge allows charging a slab object to the current memcg, primarily in cases where charging at allocation time might not be possible because the target memcg is not known (i.e. softirq context) The objp should be pointer returned by the slab allocator functions like kmalloc (with __GFP_ACCOUNT in flags) or kmem_cache_alloc. The memcg charge behavior can be controlled through gfpflags parameter, which affects how the necessary internal metadata can be allocated. Including __GFP_NOFAIL denotes that overcharging is requested instead of failure, but is not applied for the internal metadata allocation. There are several cases where it will return true even if the charging was not done: More specifically:\n• None For slab objects from KMALLOC_NORMAL caches - allocated by without __GFP_ACCOUNT true if charge was successful otherwise false. how many bytes of memory are required. kmalloc is the normal method of allocating memory for objects smaller than page size in the kernel. The allocated object address is aligned to at least ARCH_KMALLOC_MINALIGN bytes. For size of power of two bytes, the alignment is also guaranteed to be at least to the size. For other sizes, the alignment is guaranteed to be at least the largest power-of-two divisor of size. The flags argument may be one of the GFP flags defined at include/linux/gfp_types.h and described at Documentation/core-api/mm-api.rst The recommended usage of the flags is described at Documentation/core-api/memory-allocation.rst Below is a brief outline of the most useful GFP flags Allocation will not sleep. May use emergency pools. Also it is possible to set different flags by OR’ing in one or more of the following additional flags: Zero the allocated memory before returning. Also see . This allocation has high priority and may use emergency pools. Indicate that this allocation is in no way allowed to fail (think twice before using). If memory is not immediately available, then give up at once. Try really hard to succeed the allocation but fail eventually. the type of memory to allocate (see kmalloc). pointer to the memory chunk to reallocate new number of elements to alloc new size of a single member of the array the type of memory to allocate (see kmalloc) If __GFP_ZERO logic is requested, callers must ensure that, starting with the initial memory allocation, every subsequent call to this API for the same memory allocation is flagged with __GFP_ZERO. Otherwise, it is possible that __GFP_ZERO is not fully honored by this API. See krealloc_noprof() for further details. In any case, the contents of the object pointed to are preserved up to the lesser of the new and old sizes. allocate memory for an array. The memory is set to zero. the type of memory to allocate (see kmalloc). allocate memory. The memory is set to zero. how many bytes of memory are required. the type of memory to allocate (see kmalloc). Report allocation bucket size for the given size Number of bytes to round up from. This returns the number of bytes that would be available in a allocation of size bytes. For example, a 126 byte request would be rounded up to the next sized kmalloc bucket, 128 bytes. (This is strictly for the general-purpose -based allocations, and is not for the pre-sized -based allocations.) Use this to the full bucket size ahead of time instead of using to query the size after an allocation. The cache the allocation was from. Free an object which was previously allocated from this cache. If object is NULL, no operation is performed. A string which is used in /proc/slabinfo to identify this cache. The size of objects to be created in this cache. Additional arguments for the cache creation (see ). See the desriptions of individual flags. The common ones are listed in the description below. Not to be called directly, use the wrapper with the same parameters. - Slab page (not individual objects) freeing delayed by a grace period - see the full description before using. Cannot be called within a interrupt, but can be interrupted. a pointer to the cache on success, NULL on failure. Create a set of caches that handle dynamic sized allocations via kmem_buckets_alloc() A prefix string which is used in /proc/slabinfo to identify this cache. The individual caches with have their sizes as the suffix. Starting offset within an allocation that may be copied to/from userspace. How many bytes, starting at useroffset, may be copied to/from userspace. A constructor for the objects, run when new allocations are made. Cannot be called within an interrupt, but can be interrupted. a pointer to the cache on success, NULL on failure. When CONFIG_SLAB_BUCKETS is not enabled, ZERO_SIZE_PTR is returned, and subsequent calls to kmem_buckets_alloc() will fall back to . (i.e. callers only need to check for NULL on failure.) Releases as many slabs as possible for a cache. To help debugging, a zero exit status indicates all slabs were released. if all slabs were released, non-zero otherwise slab object for which to find provenance information. This function uses , so that the caller is expected to have printed out whatever preamble is appropriate. The provenance information depends on the type of object and on how much debugging is enabled. For a slab-cache object, the fact that it is a slab object is printed, and, if available, the slab name, return address, and stack trace from the allocation and last free path of that object. if the pointer is to a not-yet-freed object from or , either or if the pointer is to an already-freed object, and otherwise. The memory of the object p points to is zeroed before freed. If p is , does nothing. this function zeroes the whole allocated buffer which can be a good deal bigger than the requested buffer size passed to . So be careful when using this function in performance sensitive code. Note that a single argument of kvfree_rcu() call has a slow path that triggers following by freeing a pointer. It is done before the return from the function. Therefore for any single-argument call that will result in a to a cache that is to be destroyed during module exit, it is developer’s responsibility to ensure that all such calls have returned before the call to kmem_cache_destroy(). Function calls kfree only if x is not in .rodata section. kvfree frees memory allocated by any of vmalloc(), or kvmalloc(). It is slightly more efficient to use or if you are certain that you know which one to use.\n\nCall writepages on the mapping using the provided wbc to control the writeout. This is a non-integrity writeback helper, to start writing back folios for the indicated range. This is a mostly non-blocking flush. Not suitable for data-integrity purposes - I/O may not be started against all dirty pages. address space within which to check offset in bytes where the range starts offset in bytes where the range ends (inclusive) Find at least one page in the range supplied, usually used to check if direct writing in this range will trigger a writeback. if at least one page exists in the specified range, otherwise. offset in bytes where the range starts offset in bytes where the range ends (inclusive) Walk the list of under-writeback pages of the given address space in the given range and wait for all of them. Check error status of the address space and return it. Since the error status of the address space is cleared by this function, callers are responsible for checking the return value and handling and/or reporting the error. offset in bytes where the range starts offset in bytes where the range ends (inclusive) Walk the list of under-writeback pages of the given address space in the given range and wait for all of them. Unlike , this function does not clear error status of the address space. Use this function if callers don’t handle errors themselves. Expected call sites are system-wide / filesystem-wide data flushers: e.g. sync(2), fsfreeze(8) file pointing to address space structure to wait for offset in bytes where the range starts offset in bytes where the range ends (inclusive) Walk the list of under-writeback pages of the address space that file refers to, in the given range and wait for all of them. Check error status of the address space vs. the file->f_wb_err cursor and return it. Since the error status of the file is advanced by this function, callers are responsible for checking the return value and handling and/or reporting the error. error status of the address space vs. the file->f_wb_err cursor. Walk the list of under-writeback pages of the given address space and wait for all of them. Unlike filemap_fdatawait(), this function does not clear error status of the address space. Use this function if callers don’t handle errors themselves. Expected call sites are system-wide / filesystem-wide data flushers: e.g. sync(2), fsfreeze(8) the address_space for the pages offset in bytes where the range starts offset in bytes where the range ends (inclusive) Write out and wait upon file offsets lstart->lend, inclusive. Note that lend is inclusive (describes the last byte to be written) so that this function can be used to write to the very end-of-file (end = -1). report wb error (if any) that was previously and advance wb_err to current one on which the error is being reported When userland calls fsync (or something like nfsd does the equivalent), we want to report any writeback errors that occurred since the last fsync (or since the file was opened if there haven’t been any). Grab the wb_err from the mapping. If it matches what we have in the file, then just quickly return 0. The file is all caught up. If it doesn’t match, then take the mapping value, set the “seen” flag in it and try to swap it into place. If it works, or another task beat us to it with the new value, then update the f_wb_err and return the error portion. The error at this point must be reported via proper channels (a’la fsync, or NFS COMMIT operation, etc.). While we handle mapping->wb_err with atomic operations, the f_wb_err value is protected by the f_lock since we must ensure that it reflects the latest value swapped in for this file descriptor. offset in bytes where the range starts offset in bytes where the range ends (inclusive) Write out and wait upon file offsets lstart->lend, inclusive. Note that lend is inclusive (describes the last byte to be written) so that this function can be used to write to the very end-of-file (end = -1). After writing out and waiting on the data, we check and advance the f_wb_err cursor to the latest value, and return any errors detected there. replace a pagecache folio with a new one This function replaces a folio in the pagecache with a new one. On success it acquires the pagecache reference for the new folio and drops it for the old folio. Both the old and new folios must be locked. This function does not add the new folio to the LRU, the caller must do that. The remove + add is atomic. This function cannot fail. Unlocks the folio and wakes up any thread sleeping on the page lock. May be called from interrupt or process context. May not be called from NMI context. When all reads against a folio have completed, filesystems should call this function to let the pagecache know that no more reads are outstanding. This will unlock the folio and wake up any thread sleeping on the lock. The folio will also be marked uptodate if all reads succeeded. May be called from interrupt or process context. May not be called from NMI context. Clear the PG_private_2 bit on a folio and wake up any sleepers waiting for it. The folio reference held for PG_private_2 being set is released. This is, for example, used when a netfs folio is being written to a local disk cache, thereby allowing writes to the cache for the same folio to be serialised. Wait for PG_private_2 to be cleared on a folio. Wait for PG_private_2 to be cleared on a folio. Wait for PG_private_2 to be cleared on a folio. Wait for PG_private_2 to be cleared on a folio or until a fatal signal is received by the calling task. The folio must actually be under writeback. May be called from process or interrupt context. Get a lock on the folio, assuming we need to sleep to get it. Find the next gap in the page cache. Search the range [index, min(index + max_scan - 1, ULONG_MAX)] for the gap with the lowest index. This function may be called under the rcu_read_lock. However, this will not atomically search a snapshot of the cache at a single point in time. For example, if a gap is created at index 5, then subsequently a gap is created at index 10, page_cache_next_miss covering both indices may return 10 if called under the rcu_read_lock. The index of the gap if found, otherwise an index outside the range specified (in which case ‘return - index >= max_scan’ will be true). In the rare case of index wrap-around, 0 will be returned. Find the previous gap in the page cache. Search the range [max(index - max_scan + 1, 0), index] for the gap with the highest index. This function may be called under the rcu_read_lock. However, this will not atomically search a snapshot of the cache at a single point in time. For example, if a gap is created at index 10, then subsequently a gap is created at index 5, covering both indices may return 5 if called under the rcu_read_lock. The index of the gap if found, otherwise an index outside the range specified (in which case ‘index - return >= max_scan’ will be true). In the rare case of wrap-around, ULONG_MAX will be returned. Find and get a reference to a folio. flags modify how the folio is returned. Memory allocation flags to use if is specified. Looks up the page cache entry at mapping & index. If or are specified then the function may sleep even if the flags specified for are atomic. If this function returns a folio, it is returned with an increased refcount. The found folio or an otherwise. Search for and return a batch of folios in the mapping starting at index start and up to index end (inclusive). The folios are returned in fbatch with an elevated reference count. The number of folios which were found. We also update start to index the next folio for the traversal. works exactly like , except the returned folios are guaranteed to be contiguous. This may not return all contiguous folios if the batch gets filled up. The number of folios found. Also update start to be positioned for traversal of the next folio. The first folio may start before start; if it does, it will contain start. The final folio may extend beyond end; if it does, it will contain end. The folios have ascending indices. There may be gaps between the folios if there are indices which have no folio in the page cache. If folios are added to or removed from the page cache while this is running, they may or may not be found by this call. Only returns folios that are tagged with tag. The number of folios found. Also update start to index the next folio for traversal. Number of bytes already read by the caller. Copies data from the page cache. If the data is not currently present, uses the readahead and read_folio address_space operations to fetch it. Total number of bytes copied, including those already read by the caller. If an error happens before any bytes are copied, returns a negative error number. This is the “read_iter()” routine for all filesystems that can use the page cache directly. The IOCB_NOWAIT flag in iocb->ki_flags indicates that -EAGAIN shall be returned when no data can be read without waiting for I/O requests to complete; it doesn’t prevent readahead. The IOCB_NOIO flag in iocb->ki_flags indicates that no new I/O requests shall be made for the read or for readahead. When no data can be read, -EAGAIN shall be returned. When readahead would be triggered, a partial, possibly empty read shall be returned.\n• None number of bytes copied, even for partial reads\n• None negative error code (or 0 if IOCB_NOIO) if nothing was read Pointer to the file position to read from This function gets folios from a file’s pagecache and splices them into the pipe. Readahead will be called as necessary to fill more folios. This may be used for blockdevs also. On success, the number of bytes read will be returned and *ppos will be updated if appropriate; 0 will be returned if there is no more data to be read; -EAGAIN will be returned if the pipe had no space, and some other negative error code will be returned on error. A short read may occur if the pipe has insufficient space, we reach the end of the data or we hit a hole. struct vm_fault containing details of the fault is invoked via the vma operations vector for a mapped memory region to read in file data during a page fault. The goto’s are kind of ugly, but this streamlines the normal case of having it in the page cache, and handles the special cases reasonably without having a lot of duplicated code. vma->vm_mm->mmap_lock must be held on entry. If our return value has VM_FAULT_RETRY set, it’s because the mmap_lock may be dropped before doing I/O or by lock_folio_maybe_drop_mmap(). If our return value does not have VM_FAULT_RETRY set, the mmap_lock has not been released. We never return with VM_FAULT_RETRY and a bit from VM_FAULT_ERROR set. Read into page cache, fill it if needed. The address_space to read from. Function to perform the read, or NULL to use aops->read_folio(). Passed to filler function, may be NULL if not required. Read one page into the page cache. If it succeeds, the folio returned will contain index, but it may not be the first page of the folio. If the filler function returns an error, it will be returned to the caller. May sleep. Expects mapping->invalidate_lock to be held. An uptodate folio on success, on failure. Read into page cache, using specified allocation flags. The address_space for the folio. The index that the allocated folio will contain. The page allocator flags to use if allocating. This is the same as “read_cache_folio(mapping, index, NULL, NULL)”, but with any new memory allocations done using the specified allocation flags. The most likely error from this function is EIO, but ENOMEM is possible and so is EINTR. If ->read_folio returns another error, that will be returned to the caller. The function expects mapping->invalidate_lock to be already held. read into page cache, using specified page allocation flags. the page allocator flags to use if allocating This is the same as “read_mapping_page(mapping, index, NULL)”, but with any new page allocations done using the specified allocation flags. If the page does not get brought uptodate, return -EIO. The function expects mapping->invalidate_lock to be already held. up to date page on success, on failure. This function does all the work needed for actually writing data to a file. It does all basic checks, removes SUID from the file, updates modification times and calls proper subroutines depending on whether we do direct IO or a standard buffered write. It expects i_rwsem to be grabbed unless we work on a block device or similar object which does not need locking at all. This function does not take care of syncing data in case of O_SYNC write. A caller has to handle it. This is mainly due to the fact that we want to avoid syncing under i_rwsem.\n• None number of bytes written, even for truncated writes\n• None negative error code if no data has been written at all This is a wrapper around to be used by most filesystems. It takes care of syncing the file in case of O_SYNC file and acquires i_rwsem as needed.\n• None negative error code if no data has been written at all of failed for a synchronous write\n• None number of bytes written, even for truncated writes The folio which the kernel is trying to free. The address_space is trying to release any data attached to a folio (presumably at folio->private). This will also be called if the private_2 flag is set on a page, indicating that the folio has other metadata associated with it. The gfp argument specifies whether I/O may be performed to release this page (__GFP_IO), and whether the call may block (__GFP_RECLAIM & __GFP_FS). if the release was successful, otherwise . Set to write back rather than simply invalidate. Last byte in range (inclusive), or LLONG_MAX for everything from start onwards. Invalidate all the folios on an inode that contribute to the specified range, possibly writing them back first. Whilst the operation is undertaken, the invalidate lock is held to prevent new folios from being installed. Readahead is used to read content into the page cache before it is explicitly requested by the application. Readahead only ever attempts to read folios that are not yet in the page cache. If a folio is present but not up-to-date, readahead will not try to read it. In that case a simple ->read_folio() will be requested. Readahead is triggered when an application read request (whether a system call or a page fault) finds that the requested folio is not in the page cache, or that it is in the page cache and has the readahead flag set. This flag indicates that the folio was read as part of a previous readahead request and now that it has been accessed, it is time for the next readahead. Each readahead request is partly synchronous read, and partly async readahead. This is reflected in the which contains ->size being the total number of pages, and ->async_size which is the number of pages in the async section. The readahead flag will be set on the first folio in this async section to trigger a subsequent readahead. Once a series of sequential reads has been established, there should be no need for a synchronous component and all readahead request will be fully asynchronous. When either of the triggers causes a readahead, three numbers need to be determined: the start of the region to read, the size of the region, and the size of the async tail. The start of the region is simply the first page address at or after the accessed address, which is not currently populated in the page cache. This is found with a simple search in the page cache. The size of the async tail is determined by subtracting the size that was explicitly requested from the determined request size, unless this would be less than zero - then zero is used. NOTE THIS CALCULATION IS WRONG WHEN THE START OF THE REGION IS NOT THE ACCESSED PAGE. ALSO THIS CALCULATION IS NOT USED CONSISTENTLY. The size of the region is normally determined from the size of the previous readahead which loaded the preceding pages. This may be discovered from the for simple sequential reads, or from examining the state of the page cache when multiple sequential reads are interleaved. Specifically: where the readahead was triggered by the readahead flag, the size of the previous readahead is assumed to be the number of pages from the triggering page to the start of the new readahead. In these cases, the size of the previous readahead is scaled, often doubled, for the new readahead, though see get_next_ra_size() for details. If the size of the previous read cannot be determined, the number of preceding pages in the page cache is used to estimate the size of a previous read. This estimate could easily be misled by random reads being coincidentally adjacent, so it is ignored unless it is larger than the current request, and it is not scaled up, unless it is at the start of file. In general readahead is accelerated at the start of the file, as reads from there are often sequential. There are other minor adjustments to the readahead size in various special cases and these are best discovered by reading the code. The above calculation, based on the previous readahead size, determines the size of the readahead, to which any requested read size may be added. Readahead requests are sent to the filesystem using the ->readahead() address space operation, for which is a canonical implementation. ->readahead() should normally initiate reads on all folios, but may fail to read any or all folios without causing an I/O error. The page cache reading code will issue a ->read_folio() request for any folio which ->readahead() did not read, and only an error from this will be final. ->readahead() will generally call repeatedly to get each folio from those prepared for readahead. It may fail to read a folio by:\n• None not calling sufficiently many times, effectively ignoring some folios, as might be appropriate if the path to storage is congested.\n• None failing to actually submit a read request for a given folio, possibly due to insufficient resources, or\n• None getting an error during subsequent processing of a request. In the last two cases, the folio should be unlocked by the filesystem to indicate that the read attempt has failed. In the first case the folio will be unlocked by the VFS. Those folios not in the final of the request should be considered to be important and ->readahead() should not fail them due to congestion or temporary resource unavailability, but should wait for necessary resources (e.g. memory or indexing information) to become available. Folios in the final may be considered less urgent and failure to read them is more acceptable. In this case it is best to use filemap_remove_folio() to remove the folios from the page cache as is automatically done for folios that were not fetched with . This will allow a subsequent synchronous readahead request to try them again. If they are left in the page cache, then they will be read individually using ->read_folio() which may be less efficient. The number of pages to read. Where to start the next readahead. This function is for filesystems to call when they want to start readahead beyond a file’s stated i_size. This is almost certainly not the function you want to call. Use or instead. File is referenced by caller. Mutexes may be held by caller. May sleep, but will not reenter filesystem to reclaim memory. The request to be expanded The revised size of the request Attempt to expand a readahead request outwards from the current size to the specified size by inserting locked pages before and after the current window to increase the size to the new window. This may involve the insertion of THPs, in which case the window may get expanded even beyond what was requested. The algorithm will stop if it encounters a conflicting page already in the pagecache and leave a smaller expansion than requested. The caller must check for this by examining the revised ractl object for a different expansion than was requested. Processes which are dirtying memory should call in here once for each page which was newly dirtied. The function will periodically check the system’s dirty state and will initiate writeback if needed. If flags contains BDP_ASYNC, it may return -EAGAIN to indicate that memory is out of balance and the caller must wait for I/O to complete. Otherwise, it will return 0 to indicate that either memory was already in balance, or it was able to sleep until the amount of dirty memory returned to balance. Processes which are dirtying memory should call in here once for each page which was newly dirtied. The function will periodically check the system’s dirty state and will initiate writeback if needed. Once we’re over the dirty memory limit we decrease the ratelimiting by a lot, to prevent individual processes from overshooting the limit by (ratelimit_pages) each. tag pages to be written by writeback This function scans the page range from start to end (inclusive) and tags all pages that have DIRTY tag set with a special TOWRITE tag. The caller can then use the TOWRITE tag to identify pages eligible for writeback. This mechanism is used to avoid livelocking of writeback by a process steadily creating new dirty pages in the file (thus it is important for this function to be quick so that it can tag pages faster than a dirtying process can create them). in-out pointer for writeback errors (see below) This function returns the next folio for the writeback operation described by wbc on mapping and should be called in a while loop in the ->writepages implementation. To start the writeback operation, is passed in the folio argument, and for every subsequent iteration the folio returned previously should be passed back in. If there was an error in the per-folio writeback inside the loop, error should be set to the error value. Once the writeback described in wbc has finished, this function will return and if there was an error in any iteration restore it to error. callers should not manually break out of the loop using break or goto but must keep calling until it returns . the folio to write or if the loop is done. walk the list of dirty pages of the given address space and write all of them. subtract the number of written pages from *wbc->nr_to_write please use instead. Mark a folio dirty for filesystems which do not use buffer_heads. Folio to be marked as dirty. Filesystems which do not use buffer heads should call this function from their dirty_folio address space operation. It ignores the contents of folio_get_private(), so if the filesystem marks individual blocks as dirty, the filesystem should handle that itself. This is also sometimes used by filesystems which use buffer_heads when a single buffer is being dirtied: we want to set the folio dirty in that case, but not all the buffers. This is a “bottom-up” dirtying, whereas is a “top-down” dirtying. The caller must ensure this doesn’t race with truncation. Most will simply hold the folio lock, but e.g. zap_pte_range() calls with the folio mapped and the pte lock held, which also locks out truncation. When a writepage implementation decides that it doesn’t want to write folio for some reason, it should call this function, unlock folio and return 0. True if we redirtied the folio. False if someone else dirtied it first. The folio may not be truncated while this function is running. Holding the folio lock is sufficient to prevent truncation, but some callers cannot acquire a sleeping lock. These callers instead hold the page table lock for a page table which contains at least one page in this folio. Truncation will block on the page table lock as it unmaps pages before removing the folio from its mapping. True if the folio was newly dirtied, false if it was already dirty. If the folio is currently being written back to storage, wait for the I/O to complete. Sleeps. Must be called in process context and with no spinlocks held. Caller should hold a reference on the folio. If the folio is not locked, writeback may start again after writeback has finished. If the folio is currently being written back to storage, wait for the I/O to complete or a fatal signal to arrive. Sleeps. Must be called in process context and with no spinlocks held. Caller should hold a reference on the folio. If the folio is not locked, writeback may start again after writeback has finished. 0 on success, -EINTR if we get a fatal signal while waiting. wait for writeback to finish, if necessary. This function determines if the given folio is related to a backing device that requires folio contents to be held stable during writeback. If so, then it will wait for any pending writeback to complete. Sleeps. Must be called in process context and with no spinlocks held. Caller should hold a reference on the folio. If the folio is not locked, writeback may start again after writeback has finished. Invalidate part or all of a folio. The folio which is affected. start of the range to invalidate length of the range to invalidate is called when all or part of the folio has become invalidated by a truncate operation. does not have to release all buffers, but it must ensure that no dirty buffer is left outside offset and that no I/O is underway against any of the blocks which are outside the truncation point. Because the caller is about to free (and possibly reuse) those blocks on-disk. truncate range of pages specified by start & end byte offsets offset from which to truncate offset to which to truncate (inclusive) Truncate the page cache, removing the pages that are between specified offsets (and zeroing out partial pages if lstart or lend + 1 is not page aligned). Truncate takes two passes - the first pass is nonblocking. It will not block on page locks and it will not block on writeback. The second pass will wait. This is to prevent as much IO as possible in the affected region. The first pass will remove most pages, so the search cost of the second pass is low. We pass down the cache-hot hint to the page freeing code. Even if the mapping is large, it is probably the case that the final pages are the most recently touched, and freeing happens in ascending file offset order. Note that since ->invalidate_folio() accepts range to invalidate truncate_inode_pages_range is able to handle cases where lend + 1 is not page aligned properly. truncate all the pages from an offset offset from which to truncate Called under (and serialised by) inode->i_rwsem and mapping->invalidate_lock. When this function returns, there can be a page in the process of deletion (inside __filemap_remove_folio()) in the specified range. Thus mapping->nrpages can be non-zero when this function returns even after truncation of the whole mapping. Called under (and serialized by) inode->i_rwsem. Filesystems have to use this in the .evict_inode path to inform the VM that this is the final truncate and the inode is going away. Invalidate all clean, unlocked cache of one inode the address_space which holds the cache to invalidate the offset ‘from’ which to invalidate the offset ‘to’ which to invalidate (inclusive) This function removes pages that are clean, unmapped and unlocked, as well as shadow entries. It will not block on IO activity. If you want to remove all the pages of one inode, regardless of their use and writeback state, use . The number of indices that had their contents invalidated remove range of pages from an address_space the page offset ‘from’ which to invalidate the page offset ‘to’ which to invalidate (inclusive) Any pages which are found to be mapped into pagetables are unmapped prior to invalidation. -EBUSY if any pages could not be invalidated. remove all pages from an address_space Any pages which are found to be mapped into pagetables are unmapped prior to invalidation. -EBUSY if any pages could not be invalidated. unmap and remove pagecache that has been truncated inode’s new i_size must already be written before truncate_pagecache is called. This function should typically be called before the filesystem releases resources associated with the freed range (eg. deallocates blocks). This way, pagecache will always stay logically coherent with on-disk format, and the filesystem would not have to deal with situations such as writepage being called for a page that has already had its underlying blocks deallocated. update inode and pagecache for a new file size truncate_setsize updates i_size and performs pagecache truncation (if necessary) to newsize. It will be typically be called from the filesystem’s setattr function when ATTR_SIZE is passed in. Must be called with a lock serializing truncates and writes (generally i_rwsem but e.g. xfs uses a different lock) and before all filesystem specific block truncation has been performed. inode for which i_size was extended Handle extension of inode size either caused by extending truncate or by write starting after current i_size. We mark the page straddling current i_size RO so that page_mkwrite() is called on the first write access to the page. The filesystem will update its per-block information before user writes to the page via mmap after the i_size has been changed. The function must be called after i_size is updated so that page fault coming after we unlock the folio will already see the new i_size. The function must be called while we still hold i_rwsem - this not only makes sure i_size is stable but also that userspace cannot observe new i_size value before we are prepared to store mmap writes at new inode size. unmap and remove pagecache that is hole-punched offset of last byte of hole This function should typically be called before the filesystem releases resources associated with the freed range (eg. deallocates blocks). This way, pagecache will always stay logically coherent with on-disk format, and the filesystem would not have to deal with situations such as writepage being called for a page that has already had its underlying blocks deallocated. mapping in which to set writeback error error to be set in mapping When writeback fails in some way, we must record that error so that userspace can be informed when fsync and the like are called. We endeavor to report errors on any file that was open at the time of the error. Some internal callers also need to know when writeback errors have occurred. When a writeback error occurs, most filesystems will want to call filemap_set_wb_err to record the error in the mapping so that it will be automatically reported whenever fsync is called on the file. has an error occurred since the mark was sampled? Grab the errseq_t value from the mapping, and see if it has changed “since” the given value was sampled. If it has then report the latest error set, otherwise return 0. sample the current errseq_t to test for later errors Writeback errors are always reported relative to a particular sample point in the past. This function provides those sample points. sample the current errseq_t to test for later errors Grab the most current superblock-level errseq_t value for the given . the mapping in which an error should be set the error to set in the mapping When writeback fails in some way, we must record that error so that userspace can be informed when fsync and the like are called. We endeavor to report errors on any file that was open at the time of the error. Some internal callers also need to know when writeback errors have occurred. When a writeback error occurs, most filesystems will want to call mapping_set_error to record the error in the mapping so that it can be reported when the application calls fsync(2). The address space of the file. The filesystem should call this function in its inode constructor to indicate that the VFS can use large folios to cache the contents of the file. This should not be called while the inode is active as it is non-atomic. The index of a folio must be naturally aligned. If you are adding a new folio to the page cache and need to know what index to give it, call this function. Find the mapping this folio belongs to. For folios which are in the page cache, return the mapping that this page belongs to. Folios in the swap cache return the mapping of the swap file or swap device where the data is stored. This is different from the mapping returned by . The only reason to use it is if, like NFS, you return 0 from ->activate_swapfile. Do not call this for folios which aren’t in the page cache or swap cache. Find the file mapping this folio belongs to. For folios which are in the page cache, return the mapping that this page belongs to. Anonymous folios return NULL, even if they’re in the swap cache. Other kinds of folio also return NULL. This is ONLY used by architecture cache flushing code. If you aren’t writing cache flushing code, you want either or . Get the host inode for this folio. For folios which are in the page cache, return the inode that this folio belongs to. Do not call this for folios which aren’t in the page cache. Attaching private data to a folio increments the page’s reference count. The data must be detached before the folio will be freed. Folio to change the data on. Data to set on the folio. Change the private data attached to a folio and return the old data. The page must previously have had data attached and the data must be detached before the folio will be freed. Data that was previously attached to the folio. Removes the data that was previously attached to the folio and decrements the refcount on the page. Data that was attached to the folio. Flags for getting folios from the page cache. Most users of the page cache will not need to use these flags; there are convenience functions such as and . For users which need more control over exactly what is done with the folios, these flags to are available.\n• None - The folio will be marked accessed.\n• None - If no folio is present then a new folio is allocated, added to the page cache and the VM’s LRU list. The folio is returned locked.\n• None - The caller wants to do its own locking dance if the folio is already in cache. If the folio was allocated, unlock it before returning so the caller can do the same dance.\n• None - The folio will be written to by the caller.\n• None - __GFP_FS will get cleared in gfp.\n• None - Wait for the folio to be stable (finished writeback)\n• None - The flags to use in a filesystem write_begin() implementation. The suggested size of the folio to create. The caller of can use this to suggest a preferred size for the folio that is created. If there is already a folio at the index, it will be returned, no matter what its size. If a folio is freshly created, it may be of a different size than requested due to alignment constraints, memory pressure, or the presence of other folios at nearby indices. Looks up the page cache entry at mapping & index. If a folio is present, it is returned with an increased refcount. A folio or ERR_PTR(-ENOENT) if there is no folio in the cache for this index. Will not return a shadow, swap or DAX entry. Looks up the page cache entry at mapping & index. If a folio is present, it is returned locked with an increased refcount. A folio or ERR_PTR(-ENOENT) if there is no folio in the cache for this index. Will not return a shadow, swap or DAX entry. Looks up the page cache entry at mapping & index. If no folio is found, a new folio is created. The folio is locked, marked as accessed, and returned. A found or created folio. ERR_PTR(-ENOMEM) if no folio is found and failed to create a folio. Looks up the page cache slot at mapping & offset. If there is a page cache page, it is returned with an increased refcount. Looks up the page cache entry at mapping & index. If there is a page cache page, it is returned locked and with an increased refcount. A struct page or if there is no page in the cache for this index. the page’s index into the mapping Looks up the page cache slot at mapping & offset. If there is a page cache page, it is returned locked and with an increased refcount. If the page is not present, a new page is allocated using gfp_mask and added to the page cache and the VM’s LRU list. The page is returned locked and with an increased refcount. may sleep, even if gfp_flags specifies an atomic allocation! returns locked page at given index in given cache Same as grab_cache_page(), but do not wait if the page is unavailable. This is intended for speculative data generators, where the data can be regenerated if the page couldn’t be grabbed. This routine should be safe to call while holding the lock for another page. Clear __GFP_FS when allocating the page to avoid recursion into the fs and deadlock against the caller’s locked page. For a folio which is either in the page cache or the swap cache, return its index within the address_space it belongs to. If you know the page is definitely in the page cache, you can look at the folio’s index directly. The index (offset in units of pages) of a folio in its file. Get the index of the next folio. The index of the folio which follows this folio in the file. The page for a particular index. The folio which contains this index. The index we want to look up. Sometimes after looking up a folio in the page cache, we need to obtain the specific page for an index (eg a page fault). The page containing the file data for this index. Does this folio contain this index? The page index within the file. The caller should have the page locked in order to prevent (eg) shmem from moving the page between the page cache and swap cache and changing its index in the middle of the operation. Calculate the logical page offset of this page. The folio containing this page. The page which we need the offset of. For file pages, this is the offset from the beginning of the file in units of PAGE_SIZE. For anonymous pages, this is the offset from the beginning of the anon_vma in units of PAGE_SIZE. This will return nonsense for KSM pages. Caller must have a reference on the folio or otherwise prevent it from being split or freed. The offset in units of PAGE_SIZE. Returns the byte position of this folio in its file. The folio to attempt to lock. Sometimes it is undesirable to wait for a folio to be unlocked (eg when the locks are being taken in the wrong order, or if making progress through a batch of folios is more important than processing them in order). Usually is the correct function to call. Whether the lock was successfully acquired. The folio lock protects against many things, probably more than it should. It is primarily held while a folio is being brought uptodate, either from its backing file or from swap. It is also held while a folio is being truncated from its address_space, so holding the lock is sufficient to keep folio->mapping stable. The folio lock is also held while write() is modifying the page to provide POSIX atomicity guarantees (as long as the write does not cross a page boundary). Other modifications to the data in the folio do not hold the folio lock and can race with writes, eg DMA and stores to mapped pages. May sleep. If you need to acquire the locks of two or more folios, they must be in order of ascending index, if they are in the same address_space. If they are in different address_spaces, acquire the lock of the folio which belongs to the address_space which has the lowest address in memory first. Lock the folio containing this page. See for a description of what the lock protects. This is a legacy function and new code should probably use instead. May sleep. Pages in the same folio share a lock, so do not attempt to lock two pages which share a folio. Attempts to lock the folio, like , except that the sleep to acquire the lock is interruptible by a fatal signal. 0 if the lock was acquired; -EINTR if a fatal signal was received. address space within which to check offset in bytes where the range starts offset in bytes where the range ends (inclusive) Find at least one page in the range supplied, usually used to check if direct writing in this range will trigger a writeback. Used by O_DIRECT read/write with IOCB_NOWAIT, to see if the caller needs to do before proceeding. if the caller should do before doing O_DIRECT to a page in this range, otherwise. The file, used primarily by network filesystems for authentication. May be NULL if invoked internally by the filesystem. A readahead request is for consecutive pages. Filesystems which implement the ->readahead method should call or in a loop and attempt to start I/O against each page in the request. Most of the fields in this struct are private and should be accessed by the functions below. address_space which holds the pagecache and I/O vectors Used by the filesystem for authentication. Index of first page to be read. Total number of pages being read by the caller. should be called when a cache miss happened: it will submit the read. The readahead logic may decide to piggyback more pages onto the read request if access patterns suggest it will improve performance. address_space which holds the pagecache and I/O vectors Used by the filesystem for authentication. The folio which triggered the readahead call. Total number of pages being read by the caller. should be called when a page is used which is marked as PageReadahead; this is a marker to suggest that the application has used up enough of the readahead window that we should start pulling in more pages. Get the next page to read. The page is locked and has an elevated refcount. The caller should decreases the refcount once the page has been submitted for I/O and unlock the page once all I/O to that page has completed. A pointer to the next page, or if we are done. Get the next folio to read. The folio is locked. The caller should unlock the folio once all I/O to that folio has completed. A pointer to the next folio, or if we are done. Get a batch of pages to read. An array of pointers to struct page. The pages are locked and have an elevated refcount. The caller should decreases the refcount once the page has been submitted for I/O and unlock the page once all I/O to that page has completed. The number of pages placed in the array. 0 indicates the request is complete. The byte offset into the file of this readahead request. The number of bytes in this readahead request. The index of the first page in this readahead request. The number of pages in this readahead request. The number of bytes in the current batch. the inode to check the folio against the number of bytes in the folio up to EOF, or -EFAULT if the folio was truncated. the inode to check the page against Returns the number of bytes in the page up to EOF, or -EFAULT if the page was truncated. How many blocks fit in this folio. The inode which contains the blocks. If the block size is larger than the size of this folio, return zero. The caller should hold a refcount on the folio to prevent it from being split. The number of filesystem blocks covered by this folio.\n\nThis function only unmaps ptes assigned to VM_PFNMAP vmas. The entire address range must be fully contained within the vma. in: number of pages to map. out: number of pages that were not mapped. (0 means all pages were successfully mapped). In case of error, we may have mapped a subset of the provided pages. It is the caller’s responsibility to account for this case. The same restrictions apply as in . This allows drivers to insert individual pages they’ve allocated into a user vma. The zeropage is supported in some VMAs, see vm_mixed_zeropage_allowed(). The page has to be a nice clean _individual_ kernel allocation. If you allocate a compound page, you need to have marked it as such (__GFP_COMP), or manually just split the page up yourself (see split_page()). NOTE! Traditionally this was done with “ ” which took an arbitrary page protection parameter. This doesn’t allow that. Your vma protection will have to be set up correctly, which means that if you want a shared writable mapping, you’d better ask for a shared writable mapping! The page does not need to be reserved. Usually this function is called from f_op->mmap() handler under mm->mmap_lock write-lock, so it can change vma->vm_flags. Caller must set VM_MIXEDMAP on vma if it wants to call this function from other places, for example from page-fault handler. maps range of kernel pages starts with non zero offset Maps an object consisting of num pages, catering for the user’s requested vm_pgoff If we fail to insert any page into the vma, the function will return immediately leaving any previously inserted pages present. Callers from the mmap handler may immediately return the error as their caller will destroy the vma, removing any successfully inserted pages. Other callers should make their own arrangements for calling unmap_region(). 0 on success and error code otherwise. map range of kernel pages starts with zero offset Similar to , except that it explicitly sets the offset to 0. This function is intended for the drivers that did not consider vm_pgoff. 0 on success and error code otherwise. insert single pfn into user vma with specified pgprot This is exactly like , except that it allows drivers to override pgprot on a per-page basis. This only makes sense for IO mappings, and it makes no sense for COW mappings. In general, using multiple vmas is preferable; vmf_insert_pfn_prot should only be used if using multiple VMAs is impractical. pgprot typically only differs from vma->vm_page_prot when drivers set caching- and encryption bits different than those of vma->vm_page_prot, because the caching- or encryption mode may not be known at mmap() time. This is ok as long as vma->vm_page_prot is not used by the core vm to set caching and encryption bits for those vmas (except for COW pages). This is ensured by core vm only modifying these page table entries using functions that don’t touch caching- or encryption bits, using pte_modify() if needed. (See for example mprotect()). Also when new page-table entries are created, this is only done using the fault() callback, and never using the value of vma->vm_page_prot, except for page-table entries that point to anonymous pages as the result of COW. Similar to vm_insert_page, this allows drivers to insert individual pages they’ve allocated into a user vma. Same comments apply. This function should only be called from a vm_ops->fault handler, and in that case the handler should return the result of this function. As this is called only for pages that do not currently exist, we do not need to flush old virtual caches or the TLB. this is only safe if the mm semaphore is held when called. start of the physical memory to be mapped This is a simplified io_remap_pfn_range() for common driver use. The driver just needs to give us the physical memory range to be mapped, we’ll figure out the rest from the vma information. NOTE! Some drivers might want to tweak vma->vm_page_prot first to get whatever write-combining details or similar. The address space containing pages to be unmapped. Index of first page to be unmapped. Number of pages to be unmapped. 0 to unmap to end of file. Whether to unmap even private COWed pages. Unmap the pages in this address space from any userspace process which has them mmaped. Generally, you want to remove COWed pages as well when a file is being truncated, but not when invalidating pages from the page cache. unmap the portion of all mmaps in the specified address_space corresponding to the specified byte range in the underlying file. the address space containing mmaps to be unmapped. byte in first page to unmap, relative to the start of the underlying file. This will be rounded down to a PAGE_SIZE boundary. Note that this is different from , which must keep the partial page. In contrast, we must get rid of partial pages. size of prospective hole in bytes. This will be rounded up to a PAGE_SIZE boundary. A holelen of zero truncates to the end of the file. 1 when truncating a file, unmap even private COWed pages; but 0 when invalidating pagecache, don’t throw away private data. Look up a pfn mapping at a user virtual address The caller needs to setup args->vma and args->address to point to the virtual address as the target of such lookup. On a successful return, the results will be put into other output fields. After the caller finished using the fields, the caller must invoke another to proper releases the locks and resources of such look up request. During the and end() calls, the results in args will be valid as proper locks will be held. After the end() is called, all the fields in follow_pfnmap_args will be invalid to be further accessed. Further use of such information after end() may require proper synchronizations by the caller with page table updates, otherwise it can create a security bug. If the PTE maps a refcounted page, callers are responsible to protect against invalidation with MMU notifiers; otherwise access to the PFN at a later point in time can trigger use-after-free. Only IO mappings and raw PFN mappings are allowed. The mmap semaphore should be taken for read, and the mmap semaphore cannot be released before the end() is invoked. This function must not be used to modify PTE content. zero on success, negative otherwise. Must be used in pair of . See the function above for more information. set to FOLL_WRITE when writing, otherwise reading This is a generic implementation for for an iomem mapping. This callback is used by access_process_vm() when the vma is not page based. Return the requested group of flags for the pageblock_nr_pages block of pages The page within the block of interest mask of bits that the caller is interested in Set the requested group of flags for a pageblock_nr_pages block of pages The page within the block of interest mask of bits that the caller is interested in migratetype to set on the pageblock This is similar to move_freepages_block(), but handles the special case encountered in page isolation, where the block of interest might be part of a larger buddy spanning multiple pageblocks. Unlike the regular page allocator path, which moves pages while stealing buddies off the freelist, page isolation is interested in arbitrary pfn ranges that may have overlapping buddies on both ends. This function handles that. Straddling buddies are split into individual pageblocks. Only the block of interest is moved. Returns if pages could be moved, otherwise. Return a now-isolated page back where we got it This function is meant to return a page pulled from the free lists via __isolate_free_page back to the free lists they were pulled from. The order of the allocation. This function can free multi-page allocations that are not compound pages. It does not check that the order passed in matches that of the allocation, so it is easy to leak memory. Freeing more memory than was allocated will probably emit a warning. If the last reference to this page is speculative, it will be released by put_page() which only frees the first page of a non-compound allocation. To prevent the remaining pages from being leaked, we free the subsequent pages here. If you want to use the page’s reference count to decide when to free the allocation, you should allocate a compound page, and use put_page() instead of . May be called in interrupt context or while holding a normal spinlock, but not in NMI context or while holding a raw spinlock. the number of bytes to allocate GFP flags for the allocation, must not contain __GFP_COMP This function is similar to , except that it allocates the minimum number of pages to satisfy the request. can only allocate memory in power-of-two pages. This function is also limited by MAX_PAGE_ORDER. Memory allocated by this function must be released by . pointer to the allocated area or in case of error. allocate an exact number of physically-contiguous pages on a node. the preferred node ID where memory should be allocated the number of bytes to allocate GFP flags for the allocation, must not contain __GFP_COMP Like , but try to allocate on node nid first before falling back. pointer to the allocated area or in case of error. the value returned by alloc_pages_exact. size of allocation, same value as passed to . Release the memory allocated by a previous call to alloc_pages_exact. The zone index of the highest zone counts the number of pages which are beyond the high watermark within all zones at or below a given zone index. For each zone, the number of pages is calculated as: counts the number of pages which are beyond the high watermark within ZONE_DMA and ZONE_NORMAL. number of pages beyond high watermark within ZONE_DMA and ZONE_NORMAL. find the next node that should appear in a given node’s fallback list nodemask_t of already used nodes We use a number of factors to determine which is the next node that should appear on a given node’s fallback list. The node should not have appeared already in node’s fallback list, and it should be the next closest node according to the distance array (which contains arbitrary distance values from each node to each node in the system), and should also prefer nodes with no CPUs, since presumably they’ll have very little allocation pressure on them otherwise. node id of the found node or if no node is found. called when min_free_kbytes changes or when memory is hot-{added|removed} Ensures that the watermark[min,low,high] values for each zone are set correctly with respect to min_free_kbytes.\n• None tries to allocate given range of pages migratetype of the underlying pageblocks (either #MIGRATE_MOVABLE or #MIGRATE_CMA). All pageblocks in range must have the same migratetype and it must be either of the two. GFP mask. Node/zone/placement hints are ignored; only some action and reclaim modifiers are supported. Reclaim modifiers control allocation behavior during compaction/migration/reclaim. The PFN range does not have to be pageblock aligned. The PFN range must belong to a single zone. The first thing this routine does is attempt to MIGRATE_ISOLATE all pageblocks in the range. Once isolated, the pageblocks should not be modified by others. zero on success or negative error code. On success all pages which PFN is in [start, end) are allocated for the caller and need to be freed with free_contig_range().\n• None tries to find and allocate contiguous range of pages GFP mask. Node/zone/placement hints limit the search; only some action and reclaim modifiers are supported. Reclaim modifiers control allocation behavior during compaction/migration/reclaim. Mask for other possible nodes This routine is a wrapper around . It scans over zones on an applicable zonelist to find a contiguous pfn range which can then be tried for allocation with . This routine is intended for allocation requests which can not be fulfilled with the buddy allocator. The allocated memory is always aligned to a page boundary. If nr_pages is a power of two, then allocated range is also guaranteed to be aligned to same nr_pages (e.g. 1GB request would be aligned to 1GB). Allocated pages can be freed with free_contig_range() or by manually calling __free_page() on each allocated page. pointer to contiguous pages on success, or NULL if not successful. Lookup the closest node by distance if nid is not in state. this node if it is in state, otherwise the closest node by distance Preferred node (usually numa_node_id() but mpol may override it). The page on success or NULL if allocation fails. Virtual address of the allocation. Must be inside vma. Allocate a folio for a specific address in vma, using the appropriate NUMA policy. The caller must hold the mmap_lock of the mm_struct of the VMA to prevent it from going away. Should be used for all allocations for folios that will be mapped into user space, excepting hugetlbfs, and excepting where direct use of folio_alloc_mpol() is more appropriate. The folio on success or NULL if allocation fails. Power of two of number of pages to allocate. Allocate 1 << order contiguous pages. The physical address of the first page is naturally aligned (eg an order-3 allocation will be aligned to a multiple of 8 * PAGE_SIZE bytes). The NUMA policy of the current process is honoured when in process context. Can be called from any context, providing the appropriate GFP flags are used. The page on success or NULL if allocation fails. check whether current folio node is valid in policy virtual address in vma for shared policy lookup and interleave policy Lookup current policy node id for vma,addr and “compare to” folio’s node id. Policy determination “mimics” alloc_page_vma(). Called from fault path where we know the vma and faulting address. NUMA_NO_NODE if the page is in a node that is valid for this policy, or a suitable node ID to allocate a replacement folio from. Install non-NULL mpol in inode’s shared policy rb-tree. On entry, the current task has a reference on a non-NULL mpol. This must be released on exit. This is called at get_inode() calls and we can use GFP_KERNEL. pointer to mempolicy to be formatted Convert pol into a string. If buffer is too short, truncate the string. Recommend a maxlen of at least 51 for the longest mode, “weighted interleave”, plus the longest flag flags, “relative|balancing”, and to display at least a few node ids. Least Recently Used list; tracks how recently this folio was used. Number of times this folio has been pinned by mlock(). The file this page belongs to, or refers to the anon_vma for anonymous memory. Offset within the file, in units of pages. For anonymous memory, this is the index from the beginning of the mmap. Used for swp_entry_t if folio_test_swapcache(). Do not access this member directly. Use to find out how many times this folio is mapped by userspace. Do not access this member directly. Use to find how many references there are to this folio. IDs of last CPU and last process that accessed the folio. Do not use directly, call . Do not use directly, call folio_entire_mapcount(). Do not use outside of rmap and debug code. Do not use directly, call . Do not use directly, call . Do not use directly, use accessor in hugetlb.h. Do not use directly, use accessor in hugetlb_cgroup.h. Do not use directly, use accessor in hugetlb_cgroup.h. Do not use directly, call raw_hwp_list_head(). Folios to be split under memory pressure. A folio is a physically, virtually and logically contiguous set of bytes. It is a power-of-two in size, and it is aligned to that same power-of-two. It is at least as large as . If it is in the page cache, it is at a file offset which is a multiple of that power-of-two. It may be mapped into userspace at an address which is at an arbitrary page offset, but its kernel virtual address is aligned to its size. Same as page flags. Powerpc only. List of used page tables. Used for s390 gmap shadow pages (which are not linked into the user page tables) and x86 pgds. Protected by ptdesc->ptl, used for THPs. Same as page->page_type. Unused for page tables. This struct overlays struct page for now. Do not modify without a good understanding of the issues. Page fault handlers return a bitmask of these values to tell the core VM what happened when handling the fault. Used to decide whether a process gets delivered SIGBUS or just gets major/minor fault counters bumped up. ->fault did not modify page tables and needs fsync() to complete (for synchronous page faults in DAX) Allow to retry the fault if blocked. The fault task is in SIGKILL killable region. The fault has been tried once. The fault is not for current task/mm. The fault was during an instruction fetch. The fault can be interrupted by non-fatal signals. The fault is an unsharing request to break COW in a COW mapping, making sure that an exclusive anon page is mapped after the fault. whether the fault has vmf->orig_pte cached. We should only access orig_pte if this flag set. The fault is handled under VMA lock. About FAULT_FLAG_ALLOW_RETRY and FAULT_FLAG_TRIED: we can specify whether we would allow page faults to retry by specifying these two fault flags correctly. Currently there can be three legal combinations:\n• None ALLOW_RETRY and !TRIED: this means the page fault allows retry, and\n• None ALLOW_RETRY and TRIED: this means the page fault allows retry, and we’ve already tried at least once\n• None !ALLOW_RETRY and !TRIED: this means the page fault does not allow retry The unlisted combination (!ALLOW_RETRY && TRIED) is illegal and should never be used. Note that page faults can be allowed to retry for multiple times, in which case we’ll have an initial fault with flags (a) then later on continuous faults with flags (b). We should always try to detect pending signals before a retry to make sure the continuous page faults can still be interrupted if necessary. The combination FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE is illegal. FAULT_FLAG_UNSHARE is ignored and treated like an ordinary read fault when applied to mappings that are not COW mappings. Should the folio be on a file LRU or anon LRU? We would like to get this info without a page flag, but the state needs to survive until the folio is last deleted from the LRU, which could be as far down as __page_cache_release. An integer (not a boolean!) used to sort a folio onto the right LRU list and to account folios correctly. 1 if folio is a regular filesystem backed page cache folio or a lazily freed anonymous folio (e.g. via MADV_FREE). 0 if folio is a normal anonymous folio, a tmpfs folio or otherwise ram or swap backed folio. The folio that was on lru and now has a zero reference. Which LRU list should a folio be on? The LRU list a folio should be on, as an index into the array of LRU lists. Every page is part of a folio. This function cannot be called on a NULL pointer. No reference, nor lock is required on page. If the caller does not hold a reference, this call may race with a folio split, so it should re-check the folio still contains this page after gaining a reference on the folio. The folio which contains this page. n is relative to the start of the folio. This function does not check that the page number lies within folio; the caller is presumed to have a reference to the page. Bits set in this word will be changed. This must only be used for flags which are changed with the folio lock held. For example, it is unsafe to use for PG_dirty as that can be set without the folio lock held. It can also only be used on flags which are in the range 0-6 as some of the implementations only affect those bits. Whether there are tasks waiting on the folio. Is this folio up to date? The uptodate flag is set on a folio when every byte in the folio is at least as new as the corresponding bytes on storage. Anonymous and CoW folios are always uptodate. If the folio is not uptodate, some of the bytes in it may be; see the is_partially_uptodate() address_space operation. Does this folio contain more than one page? True if the folio is larger than one page. Determine if the page belongs to the slab allocator True for slab pages, false for any other kind of page. Determine if the page belongs to hugetlbfs True for hugetlbfs pages, false for anon pages or pages belonging to other filesystems. Determine if a folio has private stuff, indicating that release routines should be invoked upon it. This is mostly used for places where we want to try to avoid taking the mmap_lock for too long a time when waiting for another condition to change, in which case we can try to be polite to release the mmap_lock in the first round to avoid potential starvation of other processes that would also want the mmap_lock. true if the page fault allows retry and this is the first attempt of the fault handling; false otherwise. A folio is composed of 2^order pages. See get_order() for the definition of order. Number of mappings of this folio. The folio mapcount corresponds to the number of present user page table entries that reference any part of a folio. Each such present user page table entry must be paired with exactly on folio reference. For ordindary folios, each user page table entry (PTE/PMD/PUD/...) counts exactly once. For hugetlb folios, each abstracted “hugetlb” user page table entry that references the entire folio counts exactly once, even when such special page table entries are comprised of multiple ordinary page table entries. Will report 0 for pages which cannot be mapped into userspace, such as slab, page tables and similar. The number of times this folio is mapped. Is this folio mapped into userspace? True if any page in this folio is referenced by user page tables. Number of bytes in this page. May be called in any context, as long as you know that you have a refcount on the folio. If you do not already have one, may be the right interface for you to use. If the folio’s reference count reaches zero, the memory will be released back to the page allocator and may be used by another allocation immediately. Do not access the memory or the after calling unless you can be sure that it wasn’t the last reference. May be called in process or interrupt context, but not in NMI context. May be called while holding a spinlock. The amount to subtract from the folio’s reference count. If the folio’s reference count reaches zero, the memory will be released back to the page allocator and may be used by another allocation immediately. Do not access the memory or the after calling unless you can be sure that these weren’t the last references. May be called in process or interrupt context, but not in NMI context. May be called while holding a spinlock. Decrement the reference count on an array of folios. Like , but for a batch of folios. This is more efficient than writing the loop yourself as it will optimise the locks which need to be taken if the folios are freed. The folios batch is returned empty and ready to be reused for another batch; there is no need to reinitialise it. May be called in process or interrupt context, but not in NMI context. May be called while holding a spinlock. A folio may contain multiple pages. The pages have consecutive Page Frame Numbers. The Page Frame Number of the first page in the folio. Report if a folio may be pinned for DMA. This function checks if a folio has been pinned via a call to a function in the pin_user_pages() family. For small folios, the return value is partially fuzzy: false is not fuzzy, because it means “definitely not pinned for DMA”, but true means “probably pinned for DMA, but possibly a false positive due to having at least GUP_PIN_COUNTING_BIAS worth of normal folio references”. False positives are OK, because: a) it’s unlikely for a folio to get that many refcounts, and b) all the callers of this routine are expected to be able to deal gracefully with a false positive. For large folios, the result will be exactly correct. That’s because we have more tracking data available: the _pincount field is used instead of the GUP_PIN_COUNTING_BIAS scheme. For more information, please see pin_user_pages() and related calls. True, if it is likely that the folio has been “dma-pinned”. False, if the folio is definitely not dma-pinned. Query if a page is a zero page This returns true if page is one of the permanent zero pages. Query if a folio is a zero page This returns true if folio is one of the permanent zero pages. The number of pages in the folio. The number of regular pages in this huge page. Move to the next physical folio. The folio we’re currently operating on. If you have physically contiguous memory which may span more than one folio (eg a ), use this function to move from one folio to the next. Do not use it if the memory is only virtually contiguous as the folios are almost certainly not adjacent to each other. This is the folio equivalent to writing . We assume that the folios are refcounted and/or locked at a higher level and do not adjust the reference counts. The size of the memory described by this folio. A folio represents a number of bytes which is a power-of-two in size. This function tells you which power-of-two the folio is. See also and . The caller should have a reference on the folio to prevent it from being split. It is not necessary for the folio to be locked. The base-2 logarithm of the size of this folio. The number of bytes in a folio. The caller should have a reference on the folio to prevent it from being split. It is not necessary for the folio to be locked. The number of bytes in this folio. Estimate if the folio is mapped into the page tables of more than one MM This function checks if the folio is currently mapped into more than one MM (“mapped shared”), or if the folio is only mapped into a single MM (“mapped exclusively”). For KSM folios, this function also returns “mapped shared” when a folio is mapped multiple times into the same MM, because the individual page mappings are independent. As precise information is not easily available for all folios, this function estimates the number of MMs (“sharers”) that are currently mapping a folio using the number of times the first page of the folio is currently mapped into page tables. For small anonymous folios and anonymous hugetlb folios, the return value will be exactly correct: non-KSM folios can only be mapped at most once into an MM, and they cannot be partially mapped. KSM folios are considered shared even if mapped multiple times into the same MM. For other folios, the result can be fuzzy:\n• None For partially-mappable large folios (THP), the return value can wrongly indicate “mapped exclusively” (false negative) when the folio is only partially mapped into at least one MM.\n• None For pagecache folios (including hugetlb), the return value can wrongly indicate “mapped shared” (false positive) when two VMAs in the same MM cover the same file range. Further, this function only considers current page table mappings that are tracked using the folio mapcount(s). This function does not consider:\n• None If the folio might get mapped in the (near) future (e.g., swapcache, pagecache, temporary unmapping for migration).\n• None If the folio is mapped differently (VM_PFNMAP).\n• None If hugetlb page table sharing applies. Callers might want to check hugetlb_pmd_shared(). Whether the folio is estimated to be mapped into more than one MM. pagetable_alloc allocates memory for page tables as well as a page table descriptor to describe that memory. pagetable_free frees the memory of all page tables described by a page table descriptor and the memory for the descriptor itself. The vm_area_struct at the given address, otherwise. Pointer to the struct vm_area_struct to consider Whether transhuge page-table entries are considered “special” following the definition in vm_normal_page(). true if transhuge page-table entries should be considered special, false otherwise. The reference count on this folio. The refcount is usually incremented by calls to and decremented by calls to . Some typical users of the folio refcount:\n• None Direct IO which references this page in the process address space The number of references to this folio. Attempt to increase the refcount on a folio. If you do not already have a reference to a folio, you can attempt to get one using this function. It may fail if, for example, the folio has been freed since you found a pointer to it, or it is frozen for the purposes of splitting or migration. True if the reference count was successfully incremented. helper function to quickly check if a struct zone is a highmem zone or not. This is an attempt to keep references to ZONE_{DMA/NORMAL/HIGHMEM/etc} in general code to a minimum. helper macro to iterate over all online nodes helper macro to iterate over all memory zones The user only needs to declare the zone variable, for_each_zone fills it in. Returns the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point The cursor used as a starting point for the search The zone index of the highest zone to return An optional nodemask to filter the zonelist with This function returns the next zone at or below a given zone index that is within the allowed nodemask using a cursor as the starting point for the search. The zoneref returned is a cursor that represents the current zone being examined. It should be advanced by one before calling next_zones_zonelist again. the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist The zonelist to search for a suitable zone The zone index of the highest zone to return An optional nodemask to filter the zonelist with This function returns the first zone at or below a given zone index that is within the allowed nodemask. The zoneref returned is a cursor that can be used to iterate the zonelist with next_zones_zonelist by advancing it by one before calling. When no eligible zone is found, zoneref->zone is NULL (zoneref itself is never NULL). This may happen either genuinely, or due to concurrent nodemask update due to cpuset modification. Zoneref pointer for the first suitable zone found helper macro to iterate over valid zones in a zonelist at or below a given zone index and within a nodemask The current zone in the iterator The current pointer within zonelist->_zonerefs being iterated The zone index of the highest zone to return This iterator iterates though all zones at or below a given zone index and within a given nodemask helper macro to iterate over valid zones in a zonelist at or below a given zone index The current zone in the iterator The current pointer within zonelist->zones being iterated The zone index of the highest zone to return This iterator iterates though all zones at or below a given zone index. check if there is a valid memory map entry for a PFN Check if there is a valid memory map entry aka struct page for the pfn. Note, that availability of the memory map entry does not imply that there is actual usable memory at that pfn. The struct page may represent a hole or an unusable page frame. 1 for PFNs that have memory map entries and 0 otherwise Find the mapping where this folio is stored. For folios which are in the page cache, return the mapping that this page belongs to. Folios in the swap cache return the swap mapping this page is stored in (which is different from the mapping for the swap file or swap device where the data is stored). You can call this for folios which aren’t in the swap cache or page cache and it will return NULL. This makes sure the memory mapping described by ‘vma’ has an ‘anon_vma’ attached to it, so that we can associate the anonymous pages mapped into it with that anon_vma. The common case will be that we already have one, which is handled inline by anon_vma_prepare(). But if not we either need to find an adjacent mapping that we can re-use the anon_vma from (very common when the only reason for splitting a vma has been mprotect()), or we allocate a new one. Anon-vma allocations are very subtle, because we may have optimistically looked up an anon_vma in folio_lock_anon_vma_read() and that may actually touch the rwsem even in the newly allocated vma (it depends on RCU to make sure that the anon_vma isn’t actually destroyed). As a result, we need to do proper anon_vma locking even for the new allocation. At the same time, we do not want to do any locking for the common case of already having an anon_vma. The virtual address of a page in this VMA. The folio containing the page. The page within the folio. The VMA we need to know the address in. Calculates the user virtual address of this page in the specified VMA. It is the caller’s responsibililty to check the page is actually within the VMA. There may not currently be a PTE pointing at this page, but if a page fault occurs at this address, this is the page which will be accessed. Caller should hold a reference to the folio. Caller should hold a lock (eg the i_mmap_lock or the mmap_lock) which keeps the VMA from being altered. The virtual address corresponding to this page in the VMA. Test if the folio was referenced. A combination of all the vma->vm_flags which referenced the folio. Quick test_and_clear_referenced for all mappings of a folio, The number of mappings which referenced the folio. Return -1 if the function bailed out due to rmap lock contention. Cleans the PTEs (including PMDs) mapped with range of [pfn, pfn + nr_pages) at the specific offset (pgoff) within the vma of shared mappings. And since clean PTEs should also be readonly, write protects them too. page offset that the pfn mapped with. The folio to move to our anon_vma The vma the folio belongs to When a folio belongs exclusively to one process after a COW event, that folio can be moved into the anon_vma that belongs to just that process, so the rmap code will not search the parent or sibling processes. set up a new anonymous rmap for a folio The folio to set up the new anonymous rmap for. VM area to add the folio to. Whether the folio is exclusive to the process. the page to check the mapping of the vm area in which the mapping is added add PTE mappings to a page range of an anon folio The folio to add the mappings to The number of pages which will be mapped The vm area in which the mappings are added The user virtual address of the first page to map The page range of folio is defined by [first_page, first_page + nr_pages) The caller needs to hold the page table lock, and the page must be locked in the anon_vma case: to serialize mapping,index checking after setting, and to ensure that an anon folio is not being upgraded racily to a KSM folio (but KSM folios are never downgraded). add a PMD mapping to a page range of an anon folio The folio to add the mapping to The vm area in which the mapping is added The user virtual address of the first page to map The page range of folio is defined by [first_page, first_page + HPAGE_PMD_NR) The caller needs to hold the page table lock, and the page must be locked in the anon_vma case: to serialize mapping,index checking after setting. The folio to add the mapping to. the vm area in which the mapping is added Like folio_add_anon_rmap_*() but must only be called on new folios. This means the inc-and-test can be bypassed. The folio doesn’t necessarily need to be locked while it’s exclusive unless two threads map it concurrently. However, the folio must be locked if it’s shared. If the folio is pmd-mappable, it is accounted as a THP. The folio to add the mappings to The number of pages that will be mapped using PTEs The vm area in which the mappings are added The page range of the folio is defined by [page, page + nr_pages) The caller needs to hold the page table lock. The folio to add the mapping to The vm area in which the mapping is added The page range of the folio is defined by [page, page + HPAGE_PMD_NR) The caller needs to hold the page table lock. The folio to remove the mappings from The number of pages that will be removed from the mapping The vm area from which the mappings are removed The page range of the folio is defined by [page, page + nr_pages) The caller needs to hold the page table lock. The folio to remove the mapping from The vm area from which the mapping is removed The page range of the folio is defined by [page, page + HPAGE_PMD_NR) The caller needs to hold the page table lock. Try to remove all page table mappings to a folio. Tries to remove all the page table entries which are mapping this folio. It is the caller’s responsibility to check if the folio is still mapped if needed (use TTU_SYNC to prevent accounting races). try to replace all page table mappings with swap entries the folio to replace page table entries for Tries to remove all the page table entries which are mapping this folio and replace them with special swap entries. Caller must hold the folio lock. The folio to replace page table entries for. The mm_struct where the folio is expected to be mapped. Address where the folio is expected to be mapped. Tries to remove all the page table entries which are mapping this folio and replace them with special device exclusive swap entries to grant a device exclusive access to the folio. false if the page is still mapped, or if it could not be unmapped from the expected address. Otherwise returns true (success). Mark a range for exclusive use by a device start of the region to mark for exclusive device access returns the pages which were successfully marked for exclusive access passed to MMU_NOTIFY_EXCLUSIVE range notifier to allow filtering number of pages found in the range by GUP. A page is marked for exclusive access only if the page pointer is non-NULL. This function finds ptes mapping page(s) to the given address range, locks them and replaces mappings with special swap entries preventing userspace CPU access. On fault these entries are replaced with the original mapping after calling MMU notifiers. A driver using this to program access from a device must use a mmu notifier critical section to hold a device specific lock during programming. Once programming is complete it should drop the page lock and reference after which point CPU access to the page will revoke the exclusive access. The address_space containing the folio. The folio to migrate the data to. The folio containing the current data. Common logic to directly migrate a single LRU folio suitable for folios that do not have private data. Folios are locked upon entry and exit. The folio to migrate from. How to migrate the folio. This function can only be used if the underlying filesystem guarantees that no other references to src exist. For example attached buffer heads are accessed only under the folio lock. If your filesystem cannot provide this guarantee, may be more appropriate. 0 on success or a negative errno on failure. The folio to migrate from. How to migrate the folio. Like except that this variant is more careful and checks that there are also no buffer head references. This function is the right one for mappings where buffer heads are directly looked up and referenced (such as block device mappings). 0 on success or a negative errno on failure. Perform a userland memory mapping into the current process address space of length len with protection bits prot, mmap flags flags (from which VMA flags will be inferred), and any additional VMA flags to apply vm_flags. If this is a file-backed mapping then the file is specified in file and page offset into the file via pgoff. An optional pointer describing the file which is to be mapped, if a file-backed mapping. If non-zero, hints at (or if flags has MAP_FIXED set, specifies) the address at which to perform this mapping. See mmap (2) for details. Must be page-aligned. The length of the mapping. Will be page-aligned and must be at least 1 page in size. Protection bits describing access required to the mapping. See mmap (2) for details. Flags specifying how the mapping should be performed, see mmap (2) for details. VMA flags which should be set by default, or 0 otherwise. Page offset into the file if file-backed, should be 0 otherwise. A pointer to a value which will be set to 0 if no population of the range is required, or the number of bytes to populate if it is. Must be non-NULL. See mmap (2) for details as to under what circumstances population of the range occurs. An optional pointer to a list head to track userfaultfd unmap events should unmapping events arise. If provided, it is up to the caller to manage this. This function does not perform security checks on the file and assumes, if uf is non-NULL, the caller has provided a list head to track unmap events for userfaultfd uf. It also simply indicates whether memory population is required by setting populate, which must be non-NULL, expecting the caller to actually perform this task itself if appropriate. This function will invoke architecture-specific (and if provided and relevant, file system-specific) logic to determine the most appropriate unmapped area in which to place the mapping if not MAP_FIXED. Callers which require userland mmap() behaviour should invoke vm_mmap(), which is also exported for module use. Those which require this behaviour less security checks, userfaultfd and populate behaviour, and who handle the mmap write lock themselves, should call this function. Note that the returned address may reside within a merged VMA if an appropriate merge were to take place, so it doesn’t necessarily specify the start of a VMA, rather only the start of a valid mapped range of length len bytes, rounded down to the nearest page size. Either an error, or the address at which the requested mapping has been performed. Look up the first VMA which intersects the interval The first VMA within the provided range, otherwise. Assumes start_addr < end_addr. Find the VMA for a given address, or the next VMA. The VMA associated with addr, or the next VMA. May return in the case of no VMA at addr or above. Find the VMA for a given address, or the next vma and set to the previous VMA, if any. The pointer to set to the previous VMA Note that RCU lock is missing here since the external mmap_lock() is used instead. The VMA associated with addr, or the next vma. May return in the case of no vma at addr or above. pointer to beginning of the object minimum number of references to this object. If during memory scanning a number of references less than min_count is found, the object is reported as a memory leak. If min_count is 0, the object is never reported as a leak. If min_count is -1, the object is ignored (not scanned and not reported as a leak) This function is called from the kernel allocators when a new object (memory block) is allocated (kmem_cache_alloc, kmalloc etc.). __percpu pointer to beginning of the object This function is called from the kernel percpu allocator when a new object (memory block) is allocated (alloc_percpu). This function is called from the vmalloc() kernel allocator when a new object (memory block) is allocated. pointer to beginning of the object This function is called from the kernel allocators when an object (memory block) is freed (kmem_cache_free, kfree, vfree etc.). pointer to the beginning or inside the object. This also represents the start of the range to be freed This function is called when only a part of a memory block is freed (usually from the bootmem allocator). __percpu pointer to beginning of the object This function is called from the kernel percpu allocator when an object (memory block) is freed (free_percpu). pointer to beginning of the object Override the object allocation stack trace for cases where the actual allocation place is not always useful. pointer to beginning of the object Calling this function on an object will cause the memory block to no longer be reported as leak and always be scanned. pointer to beginning of the object Calling this function on an object will cause the memory block to not be reported as a leak temporarily. This may happen, for example, if the object is part of a singly linked list and the ->next reference to it is changed. pointer to beginning of the object Calling this function on an object will cause the memory block to be ignored (not scanned and not reported as a leak). This is usually done when it is known that the corresponding block is not a leak and does not contain any references to other allocated memory blocks. limit the range to be scanned in an allocated object pointer to beginning or inside the object. This also represents the start of the scan area This function is used when it is known that only certain parts of an object contain references to other objects. Kmemleak will only scan these areas reducing the number false negatives. do not scan an allocated object pointer to beginning of the object This function notifies kmemleak not to scan the given memory block. Useful in situations where it is known that the given object does not contain any references to other objects. Kmemleak will not scan such objects reducing the number of false negatives. physical address if the beginning or inside an object. This also represents the start of the range to be freed remap and provide memmap backing for the given resource 1/ At a minimum the range and type members of pgmap must be initialized by the caller before passing it to this function 2/ The altmap field may optionally be initialized, in which case PGMAP_ALTMAP_VALID must be set in pgmap->flags. 3/ The ref field may optionally be provided, in which pgmap->ref must be ‘live’ on entry and will be killed and reaped at devm_memremap_pages_release() time, or if this routine fails. 4/ range is expected to be a host memory range that could feasibly be treated as a “System RAM” range, i.e. not a device mmio range, but this is not enforced. take a new live reference on the dev_pagemap for pfn optional known pgmap that already has a reference If pgmap is non-NULL and covers pfn it will be returned as-is. If pgmap is non-NULL but does not cover pfn the reference to it will be released. Folios in this VMA will be aligned to, and at least the size of the number of bytes returned by this function. The default size of the folios allocated when backing a VMA. try to isolate an allocated hugetlb folio the list to add the folio to on success Isolate an allocated (refcount > 0) hugetlb folio, marking it as isolated/non-migratable, and moving it from the active list to the given list. Isolation will fail if folio is not an allocated hugetlb folio, or if it is already isolated/non-migratable. On success, an additional folio reference is taken that must be dropped using to undo the isolation. Putback/un-isolate the hugetlb folio that was previous isolated using : marking it non-isolated/migratable and putting it back onto the active list. Will drop the additional folio reference obtained through . Mark a folio as having seen activity. This function will perform one of the following transitions: When a newly allocated folio is not yet visible, so safe for non-atomic ops, __folio_set_referenced() may be substituted for . The folio to be added to the LRU. Queue the folio for addition to the LRU. The decision on whether to add the page to the [in]active [file|anon] list is deferred until the folio_batch is drained. This gives a chance for the caller of have the folio added to the active list using . Add a folio to the appropate LRU list for this VMA. The folio to be added to the LRU. VMA in which the folio is mapped. If the VMA is mlocked, folio is added to the unevictable list. Otherwise, it is treated the same way as . This function hints to the VM that folio is a good reclaim candidate, for example if its invalidation fails due to the folio being dirty or under writeback. moves folio to the inactive file list. This is done to accelerate the reclaim of folio. Reduce the reference count on a batch of folios. The number of refs to subtract from each folio. Like , but for a batch of folios. This is more efficient than writing the loop yourself as it will optimise the locks which need to be taken if the folios are freed. The folios batch is returned empty and ready to be reused for another batch; there is no need to reinitialise it. If refs is NULL, we subtract one from each folio refcount. May be called in process or interrupt context, but not in NMI context. May be called while holding a spinlock. Decrement the reference count on all the pages in arg. If it fell to zero, remove the page from the LRU and free it. Note that the argument can be an array of pages, encoded pages, or folio pointers. We ignore any encoded bits, and turn any of them into just a folio that gets free’d. find_get_entries() fills a batch with both folios and shadow/swap/DAX entries. This function prunes all the non-folio entries from fbatch without leaving holes, so that it can be passed on to folio-only batch operations. Module usage counting is used to prevent using a driver while/after unloading, so if this is called from module exit function, this should never fail; if called from other than the module exit function, and this returns failure, the driver is in use and must remain available. Check if the pool driver is available The type of the zpool to check (e.g. zbud, zsmalloc) This checks if the type pool driver is available. This will try to load the requested module, if needed, but there is no guarantee the module will still be loaded and available immediately after calling. If this returns true, the caller should assume the pool is available, but must be prepared to handle the returning failure. However if this returns false, the caller should assume the requested pool type is not available; either the requested pool type module does not exist, or could not be loaded, and calling with the pool type will fail. The type string must be null-terminated. true if type pool is available, false if not The type of the zpool to create (e.g. zbud, zsmalloc) The name of the zpool (e.g. zram0, zswap) The GFP flags to use when allocating the pool. This creates a new zpool of the specified type. The gfp flags will be used when allocating memory, if the implementation supports it. If the ops param is NULL, then the created zpool will not be evictable. Implementations must guarantee this to be thread-safe. The type and name strings must be null-terminated. New zpool on success, NULL on failure. Implementations must guarantee this to be thread-safe, however only when destroying different pools. The same pool should only be destroyed once, and should not be used after it is destroyed. This destroys an existing zpool. The zpool should not be in use. Get the type of the zpool This returns the type of the pool. Implementations must guarantee this to be thread-safe. This returns if the zpool supports allocating movable memory. Implementations must guarantee this to be thread-safe. true if the zpool supports allocating movable memory, false if not The zpool to allocate from. The amount of memory to allocate. The GFP flags to use when allocating memory. Pointer to the handle to set This allocates the requested amount of memory from the pool. The gfp flags will be used when allocating memory, if the implementation supports it. The provided handle will be set to the allocated object handle. Implementations must guarantee this to be thread-safe. 0 on success, negative value on error. The zpool that allocated the memory. The handle to the memory to free. This frees previously allocated memory. This does not guarantee that the pool will actually free memory, only that the memory in the pool will become available for use by the pool. Implementations must guarantee this to be thread-safe, however only when freeing different handles. The same handle should only be freed once, and should not be used after freeing. The zpool that the handle was allocated from How the memory should be mapped This maps a previously allocated handle into memory. The mapmode param indicates to the implementation how the memory will be used, i.e. read-only, write-only, read-write. If the implementation does not support it, the memory will be treated as read-write. This may hold locks, disable interrupts, and/or preemption, and the must be called to undo those actions. The code that uses the mapped handle should complete its operations on the mapped handle memory quickly and unmap as soon as possible. As the implementation may use per-cpu data, multiple handles should not be mapped concurrently on any cpu. The zpool that the handle was allocated from This unmaps a previously mapped handle. Any locks or other actions that the implementation took in will be undone here. The memory area returned from should no longer be used after this. The total size of the pool This returns the total size in pages of the pool. Total size of the zpool in pages. Test if zpool can sleep when do mapped. Some allocators enter non-preemptible context in ->map() callback (e.g. disable pagefaults) and exit that context in ->unmap(), which limits what we can do with the mapped object. For instance, we cannot wait for asynchronous crypto API to decompress such an object or take mutexes since those will call into the scheduler. This function tells us whether we use such an allocator. true if zpool can sleep; false otherwise. css of the memcg associated with a folio If memcg is bound to the default hierarchy, css of the memcg associated with folio is returned. The returned css remains associated with folio until it is released. If memcg is bound to a traditional hierarchy, the css of root_mem_cgroup is returned. return inode number of the memcg a page is charged to Look up the closest online ancestor of the memory cgroup page is charged to and return its inode number or 0 if page is not charged to any cgroup. It is safe to call this function without holding a reference to page. Note, this function is inherently racy, because there is nothing to prevent the cgroup inode from getting torn down and potentially reallocated a moment after returns, so it only should be used by callers that do not care (such as procfs interfaces). the stat item - can be enum memcg_stat_item or enum node_stat_item delta to add to the counter, can be negative delta to add to the counter, can be negative The lruvec is the intersection of the NUMA node and a cgroup. This function updates the all three counters that are affected by a change of state at this level: per-node, per-cgroup, per-lruvec. the number of events that occurred mm from which memcg should be extracted. It can be NULL. Obtain a reference on mm->memcg and returns it if successful. If mm is NULL, then the memcg is chosen as follows: 1) The active memcg, if set. 2) current->mm->memcg, if available 3) root memcg If mem_cgroup is disabled, NULL is returned. folio from which memcg should be extracted. Returns references to children of the hierarchy below root, or root itself, or after a full round-trip. Caller must pass the return value in prev on subsequent invocations for reference counting, or use to cancel a hierarchy walk before the round-trip is complete. Reclaimers can specify a node in reclaim to divide up the memcgs in the hierarchy among all concurrent reclaimers operating on the same node. last visited hierarchy member as returned by function to call for each task This function iterates over tasks attached to memcg or to any of its descendants and calls fn for each task. If fn returns a non-zero value, the function breaks the iteration loop. Otherwise, it will iterate over all tasks and return 0. This function must not be called for the root memory cgroup. These functions are safe to use under any of the following conditions: - folio locked - folio_test_lru false - folio frozen (refcount of 0) The lruvec this folio is on with its lock held. These functions are safe to use under any of the following conditions: - folio locked - folio_test_lru false - folio frozen (refcount of 0) The lruvec this folio is on with its lock held and interrupts disabled. These functions are safe to use under any of the following conditions: - folio locked - folio_test_lru false - folio frozen (refcount of 0) The lruvec this folio is on with its lock held and interrupts disabled. account for adding or removing an lru page index of lru list the page is sitting on positive when adding or negative when removing This function must be called under lru_lock, just before a page is added to or just after a page is removed from an lru list. Returns the maximum amount of memory mem can be charged with, in pages. The memory cgroup that went over limit Task that is going to be killed memcg and p’s mem_cgroup can be different when hierarchy is enabled The memory cgroup that went over limit get a memory cgroup to clean up after OOM task to be killed by the OOM killer memcg in case of memcg OOM, NULL in case of system-wide OOM Returns a pointer to a memory cgroup, which has to be cleaned up by killing all belonging OOM-killable tasks. Caller has to call mem_cgroup_put() on the returned non-NULL memcg. Try to consume stocked charge on this cpu. how many pages to charge. The charges will only happen if memcg matches the current cpu’s memcg stock, and at least nr_pages are available in that stock. Failure to service an allocation will refill the stock. Returns 0 on success, an error code on failure. out parameter for number of file pages out parameter for number of allocatable pages according to memcg out parameter for number of dirty pages out parameter for number of pages under writeback Determine the numbers of file, headroom, dirty, and writeback pages in wb’s memcg. File, dirty and writeback are self-explanatory. Headroom is a bit more involved. A memcg’s headroom is “min(max, high) - used”. In the hierarchy, the headroom is calculated as the lowest headroom of itself and the ancestors. Note that this doesn’t consider the actual amount of available memory in the system. The caller should further cap *pheadroom accordingly. look up a memcg from a memcg id Reset the states of the mem_cgroup associated with css. This is invoked when the userland requests disabling on the default hierarchy but the memcg is pinned through dependency. The memcg should stop applying policies and should revert to the vanilla state as it may be made visible again. The current implementation only resets the essential configurations. This needs to be expanded to cover all the visible parts. check if memory consumption is in the normal range the top ancestor of the sub-tree being checked WARNING: This function is not stateless! It can only be used as part of a top-down tree iteration, not for isolated queries. This function is called when allocating a huge page folio, after the page has already been obtained and charged to the appropriate hugetlb cgroup controller (if it is enabled). Returns ENOMEM if the memcg is already full. Returns 0 if either the charge was successful, or if we skip the charging. swap entry for which the folio is allocated This function charges a folio allocated for swapin. Please call this before adding the folio to the swapcache. Returns 0 on success. Otherwise, an error code is returned. Charge new as a replacement folio for old. old will be uncharged upon free. Both folios must be locked, new->mapping must be set up. Transfer the memcg data from the old to the new folio. Transfer the memcg data from the old folio to the new folio for migration. The old folio’s data info will be cleared. Note that the memory counters will remain unchanged throughout the process. Both folios must be locked, new->mapping must be set up. Charges nr_pages to memcg. Returns if the charge fit within memcg’s configured limit, if it doesn’t. swap entry to move the charge to Transfer the memsw charge of folio to entry. Try to charge folio’s memcg for the swap space at entry. the amount of swap space to uncharge check if this cgroup can zswap Check if the hierarchical zswap limit has been reached. This doesn’t check for specific headroom, and it is not atomic either. But with zswap, the size of the allocation is only known once compression has occurred, and this optimistic pre-check avoids spending cycles on compression when there is already no room left or zswap is disabled altogether somewhere in the hierarchy. This forces the charge after allowed compression and storage in zwap for this cgroup to go ahead. recalculate the block usage of an inode the change in number of pages allocated to inode the change in number of pages swapped from inode We have to calculate the free blocks since the mm can drop undirtied hole pages behind our back. But normally info->alloced == inode->i_mapping->nrpages + info->swapped So mm freed is info->alloced - (inode->i_mapping->nrpages + info->swapped) Get allowable folio orders for the given file size. This returns huge orders for folios (when supported) based on the file size which the mapping currently allows at the given index. The index is relevant due to alignment considerations the mapping might have. The returned order may be less than the size passed. pointer to the folio if found Looks up the page cache entry at inode & index. If a folio is present, it is returned locked with an increased refcount. If the caller modifies data in the folio, it must call before unlocking the folio to ensure that the folio is not reclaimed. There is no need to reserve space before calling . When no folio is found, the behavior depends on sgp:\n• None for SGP_READ, *foliop is and 0 is returned\n• None for SGP_NOALLOC, *foliop is and -ENOENT is returned\n• None for all other flags a new folio is allocated, inserted into the page cache and returned locked in foliop. get an unlinked file living in tmpfs which must be kernel internal. There will be NO LSM permission checks against the underlying inode. So users of this interface must do LSM checks at a higher layer. The users are the big_key and shm implementations. LSM checks are provided at the key or shm level rather than the inode. name for dentry (to be seen in /proc/<pid>/maps size to be set for the file get an unlinked file living in tmpfs name for dentry (to be seen in /proc/<pid>/maps size to be set for the file get an unlinked file living in tmpfs the tmpfs mount where the file will be created name for dentry (to be seen in /proc/<pid>/maps size to be set for the file the vma to be mmapped is prepared by do_mmap read into page cache, using specified page allocation flags. the page allocator flags to use if allocating This behaves as a tmpfs “read_cache_page_gfp(mapping, index, gfp)”, with any new page allocations done using the specified allocation flags. But uses the ->read_folio() method: which does not suit tmpfs, since it may have pages in swapcache, and needs to find those for itself; although drivers/gpu/drm i915 and ttm rely upon this support. i915_gem_object_get_pages_gtt() mixes __GFP_NORETRY | __GFP_NOWARN in with the mapping_gfp_mask(), to avoid OOMing the machine unnecessarily. contains the vma, start, and pfns arrays for the migration negative errno on failures, 0 when 0 or more pages were migrated without an error. Prepare to migrate a range of memory virtual address range by collecting all the pages backing each virtual address in the range, saving them inside the src array. Then lock those pages and unmap them. Once the pages are locked and unmapped, check whether each page is pinned or not. Pages that aren’t pinned have the MIGRATE_PFN_MIGRATE flag set (by this function) in the corresponding src array entry. Then restores any pages that are pinned, by remapping and unlocking those pages. The caller should then allocate destination memory and copy source memory to it for all those entries (ie with MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE flag set). Once these are allocated and copied, the caller must update each corresponding entry in the dst array with the pfn value of the destination page and with MIGRATE_PFN_VALID. Destination pages must be locked via . Note that the caller does not have to migrate all the pages that are marked with MIGRATE_PFN_MIGRATE flag in src array unless this is a migration from device memory to system memory. If the caller cannot migrate a device page back to system memory, then it must return VM_FAULT_SIGBUS, which has severe consequences for the userspace process, so it must be avoided if at all possible. For empty entries inside CPU page table (pte_none() or pmd_none() is true) we do set MIGRATE_PFN_MIGRATE flag inside the corresponding source array thus allowing the caller to allocate device memory for those unbacked virtual addresses. For this the caller simply has to allocate device memory and properly set the destination entry like for regular migration. Note that this can still fail, and thus inside the device driver you must check if the migration was successful for those entries after calling , just like for regular migration. After that, the callers must call to go over each entry in the src array that has the MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE flag set. If the corresponding entry in dst array has MIGRATE_PFN_VALID flag set, then to migrate struct page information from the source struct page to the destination struct page. If it fails to migrate the struct page information, then it clears the MIGRATE_PFN_MIGRATE flag in the src array. At this point all successfully migrated pages have an entry in the src array with MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE flag set and the dst array entry with MIGRATE_PFN_VALID flag set. Once returns the caller may inspect which pages were successfully migrated, and which were not. Successfully migrated pages will have the MIGRATE_PFN_MIGRATE flag set for their src array entry. It is safe to update device page table after because both destination and source page are still locked, and the mmap_lock is held in read mode (hence no one can unmap the range being migrated). Once the caller is done cleaning up things and updating its page table (if it chose to do so, this is not an obligation) it finally calls to update the CPU page table to point to new pages for successfully migrated pages or otherwise restore the CPU page table to point to the original source pages. array of pfns allocated by the driver to migrate memory to number of pages in the range Equivalent to . This is called to migrate struct page meta-data from source struct page to destination. This migrates struct page meta-data from source struct page to destination struct page. This effectively finishes the migration from source page to the destination page. This replaces the special migration pte entry with either a mapping to the new page if migration was successful for that page, or to the original page otherwise. This also unlocks the pages and puts them back on the lru, or drops the extra refcount, for device pages. starting pfn in the range to migrate. is similar in concept to except that instead of looking up pages based on virtual address mappings a range of device pfns that should be migrated to system memory is used instead. This is useful when a driver needs to free device memory but doesn’t know the virtual mappings of every page that may be in device memory. For example this is often the case when a driver is being unloaded or unbound from a device. Like this function will take a reference and lock any migrating pages that aren’t free before unmapping them. Drivers may then allocate destination pages and start copying data from the device to CPU memory before calling . Similar to but supports non-contiguous pre-popluated array of device pages to migrate. The function write-protects a pte and records the range in virtual address space of touched ptes for efficient range TLB flushes. Address_space Page offset of the first bit in bitmap Bitmap with one bit for each page offset in the address_space range covered. Address_space page offset of first modified pte relative to bitmap_pgoff Address_space page offset of last modified pte relative to bitmap_pgoff Clean a pte and record its address space offset in a bitmap The start of virtual address to be clean The end of virtual address to be clean The function cleans a pte and records the range in virtual address space of touched ptes for efficient TLB flushes. It also records dirty ptes in a bitmap representing page offsets in the address_space, as well as the first and last of the bits touched. Write-protect all ptes in an address space range The address_space we want to write protect The first page offset in the range This function currently skips transhuge page-table entries, since it’s intended for dirty-tracking on the PTE level. It will warn on encountering transhuge write-enabled entries, though, and can easily be extended to handle them as well. The number of ptes actually write-protected. Note that already write-protected ptes are not counted. Clean and record all ptes in an address space range The address_space we want to clean The first page offset in the range The page offset of the first bit in bitmap Pointer to a bitmap of at least nr bits. The bitmap needs to cover the whole range first_index..**first_index** + nr. Pointer to number of the first set bit in bitmap. is modified as new bits are set by the function. Pointer to the number of the last set bit in bitmap. none set. The value is modified as new bits are set by the function. When this function returns there is no guarantee that a CPU has not already dirtied new ptes. However it will not clean any ptes not reported in the bitmap. The guarantees are as follows:\n• None All ptes dirty when the function starts executing will end up recorded in the bitmap.\n• None All ptes dirtied after that will either remain dirty, be recorded in the bitmap or both. If a caller needs to make sure all dirty ptes are picked up and none additional are added, it first needs to write-protect the address-space range and make sure new writers are blocked in page_mkwrite() or pfn_mkwrite(). And then after a TLB flush following the write-protection pick up all dirty bits. This function currently skips transhuge page-table entries, since it’s intended for dirty-tracking on the PTE level. It will warn on encountering transhuge dirty entries, though, and can easily be extended to handle them as well. The number of dirty ptes actually cleaned. check if the address is served from this chunk True if the address is served from this chunk. Check to see if the allocation can fit in the block’s contig hint. Note, a chunk uses the same hints as a block so this can also check against the chunk’s contig hint. Helper function for pcpu_for_each_md_free_region. It checks block->contig_hint and performs aggregation across blocks to find the next hint. It modifies bit_off and bits in-place to be consumed in the loop. Finds the next free region that is viable for use with a given size and alignment. This only returns if there is a valid area to be used for this allocation. block->first_free is returned if the allocation request fits within the block to see if the request can be fulfilled prior to the contig hint. Allocate size bytes. If size is smaller than PAGE_SIZE, is used; otherwise, the equivalent of vzalloc() is used. This is to facilitate passing through whitelisted flags. The returned memory is always zeroed. Pointer to the allocated area on success, NULL on failure. Free ptr. ptr should have been allocated using . put chunk in the appropriate chunk slot the previous slot it was on This function is called after an allocation or free changed chunk. New slot according to the changed state is determined and chunk is moved to the slot. Note that the reserved chunk is never put on chunk slots. Updates a block given a known free area. The region [start, end) is expected to be the entirety of the free area within a block. Chooses the best starting offset if the contig hints are equal. if we should scan from the beginning Iterates over the metadata blocks to find the largest contig area. A full scan can be avoided on the allocation path as this is triggered if we broke the contig_hint. In doing so, the scan_hint will be before the contig_hint or after if the scan_hint == contig_hint. This cannot be prevented on freeing as we want to find the largest area possibly spanning blocks. Scans over the block beginning at first_free and updates the block metadata accordingly. Updates metadata for the allocation path. The metadata only has to be refreshed by a full scan iff the chunk’s contig hint is broken. Block level scans are required if the block’s contig hint is broken. updates the block hints on the free path Updates metadata for the allocation path. This avoids a blind block refresh by making use of the block contig hints. If this fails, it scans forward and backward to determine the extent of the free area. This is capped at the boundary of blocks. A chunk update is triggered if a page becomes free, a block becomes free, or the free spans across blocks. This tradeoff is to minimize iterating over the block metadata to update chunk_md->contig_hint. chunk_md->contig_hint may be off by up to a page, but it will never be more than the available space. If the contig hint is contained in one block, it will be accurate. determines if the region is populated return value for the next offset to start searching For atomic allocations, check if the backing pages are populated. Bool if the backing pages are populated. next_index is to skip over unpopulated blocks in pcpu_find_block_fit. Given a chunk and an allocation spec, find the offset to begin searching for a free region. This iterates over the bitmap metadata blocks to find an offset that will be guaranteed to fit the requirements. It is not quite first fit as if the allocation does not fit in the contig hint of a block or chunk, it is skipped. This errs on the side of caution to prevent excess iteration. Poor alignment can cause the allocator to skip over blocks and chunks that have valid free areas. The offset in the bitmap to begin searching. -1 if no offset is found. This function takes in a start offset to begin searching to fit an allocation of alloc_bits with alignment align. It needs to scan the allocation map because if it fits within the block’s contig hint, start will be block->first_free. This is an attempt to fill the allocation prior to breaking the contig hint. The allocation and boundary maps are updated accordingly if it confirms a valid free area. Allocated addr offset in chunk on success. -1 if no matching area is found. This function determines the size of an allocation to free using the boundary bitmap and clears the allocation map. creates chunks that serve the first chunk the start of the region served This is responsible for creating the chunks that serve the first chunk. The base_addr is page aligned down of tmp_addr while the region end is page aligned up. Offsets are kept track of to determine the region served. All this is done to appease the bitmap allocator in avoiding partial blocks. Chunk serving the region at tmp_addr of map_size. Pages in [page_start,**page_end**) have been populated to chunk. Update the bookkeeping information accordingly. Must be called after each successful population. Pages in [page_start,**page_end**) have been depopulated from chunk. Update the bookkeeping information accordingly. Must be called after each successful depopulation. address for which the chunk needs to be determined. This is an internal function that handles all but static allocations. Static percpu address values should never be passed into the allocator. The address of the found chunk. size of area to allocate in bytes allocate from the reserved chunk if available Allocate percpu area of size bytes aligned at align. If gfp doesn’t contain , the allocation is atomic. If gfp has __GFP_NOWARN then no warning will be triggered on invalid or failed allocation requests. Percpu pointer to the allocated area on success, NULL on failure. free chunks only if there are no populated pages If empty_only is , reclaim all fully free chunks regardless of the number of populated pages. Otherwise, only reclaim chunks that have no populated pages. Maintain a certain amount of populated pages to satisfy atomic allocations. It is possible that this is called when physical memory is scarce causing OOM killer to be triggered. We should avoid doing so until an actual allocation causes the failure as it is possible that requests can be serviced from already backed regions. Scan over chunks in the depopulate list and try to release unused populated pages back to the system. Depopulated chunks are sidelined to prevent repopulating these pages unless required. Fully free chunks are reintegrated and freed accordingly (1 is kept around). If we drop below the empty populated pages threshold, reintegrate the chunk if it has empty free pages. Each chunk is scanned in the reverse order to keep populated pages close to the beginning of the chunk. manage the amount of free chunks and populated pages For each chunk type, manage the number of fully free chunks and the number of populated pages. An important thing to consider is when pages are freed and how they contribute to the global counts. Can be called from atomic context. test whether address is from static percpu area Test whether addr belongs to in-kernel static percpu area. Module static percpu areas are not considered. For those, use is_module_percpu_address(). if addr is from in-kernel static percpu area, otherwise. the address to be converted to physical address Given addr which is dereferenceable address obtained via one of percpu access macros, this function translates it into its physical address. The caller is responsible for ensuring addr stays valid until this function finishes. percpu allocator has special setup for the first chunk, which currently supports either embedding in linear address space or vmalloc mapping, and, from the second one, the backing allocator (currently either vm or km) provides translation. The addr can be translated simply without checking if it falls into the first chunk. But the current code reflects better how percpu allocator actually works, and the verification can discover both bugs in percpu allocator itself and callers. So we keep current code. Allocate ai which is large enough for nr_groups groups containing nr_units units. The returned ai’s groups[0].cpu_map points to the cpu_map array which is long enough for nr_units and filled with NR_CPUS. It’s the caller’s responsibility to initialize cpu_map pointer of other groups. Pointer to the allocated pcpu_alloc_info on success, NULL on failure. Free ai which was allocated by . Print out information about ai using loglevel lvl. pcpu_alloc_info describing how to percpu area is shaped Initialize the first percpu chunk which contains the kernel static percpu area. This function is to be called from arch percpu area setup path. ai contains all information necessary to initialize the first chunk and prime the dynamic percpu allocator. ai->static_size is the size of static percpu area. ai->reserved_size, if non-zero, specifies the amount of bytes to reserve after the static area in the first chunk. This reserves the first chunk such that it’s available only through reserved percpu allocation. This is primarily used to serve module percpu static areas on architectures where the addressing model has limited offset range for symbol relocations to guarantee module percpu symbols fall inside the relocatable range. ai->dyn_size determines the number of bytes available for dynamic allocation in the first chunk. The area between ai->static_size + ai->reserved_size + ai->dyn_size and ai->unit_size is unused. ai->unit_size specifies unit size and must be aligned to PAGE_SIZE and equal to or larger than ai->static_size + ai->reserved_size + ai->dyn_size. ai->atom_size is the allocation atom size and used as alignment for vm areas. ai->alloc_size is the allocation size and always multiple of ai->atom_size. This is larger than ai->atom_size if ai->unit_size is larger than ai->atom_size. ai->nr_groups and ai->groups describe virtual memory layout of percpu areas. Units which should be colocated are put into the same group. Dynamic VM areas will be allocated according to these groupings. If ai->nr_groups is zero, a single group containing all units is assumed. The caller should have mapped the first chunk at base_addr and copied static data to each unit. The first chunk will always contain a static and a dynamic region. However, the static region is not managed by any chunk. If the first chunk also contains a reserved region, it is served by two chunks - one for the reserved region and one for the dynamic region. They share the same vm, but use offset regions in the area allocation map. The chunk serving the dynamic region is circulated in the chunk slots and available for dynamic allocation like any other chunk. the size of reserved percpu area in bytes This function determines grouping of units, their mappings to cpus and other parameters considering needed percpu size, allocation atom size and distances between CPUs. Groups are always multiples of atom size and CPUs which are of LOCAL_DISTANCE both ways are grouped together and share space for units in the same group. The returned configuration is guaranteed to have CPUs on different nodes on different groups and >=75% usage of allocated virtual address space. On success, pointer to the new allocation_info is returned. On failure, ERR_PTR value is returned. embed the first percpu chunk into bootmem the size of reserved percpu area in bytes This is a helper to ease setting up embedded first percpu chunk and can be called where is expected. If this function is used to setup the first chunk, it is allocated by calling pcpu_fc_alloc and used as-is without being mapped into vmalloc area. Allocations are always whole multiples of atom_size aligned to atom_size. This enables the first chunk to piggy back on the linear physical mapping which often uses larger page size. Please note that this can result in very sparse cpu->unit mapping on NUMA machines thus requiring large vmalloc address space. Don’t use this allocator if vmalloc space is not orders of magnitude larger than distances between node memory addresses (ie. 32bit NUMA machines). If the needed size is smaller than the minimum or specified unit size, the leftover is returned using pcpu_fc_free. map the first chunk using PAGE_SIZE pages the size of reserved percpu area in bytes This is a helper to ease setting up page-remapped first percpu chunk and can be called where is expected. This is the basic allocator. Static percpu area is allocated page-by-page into vmalloc area. pointer to the buffer that shall take the data address to read from. This must be a user address. Safely read from user address src to the buffer at dst. If a kernel fault happens, handle that and return -EFAULT. pointer to the data that shall be written Safely write to address dst from the buffer at src. If a kernel fault happens, handle that and return -EFAULT. Destination address, in kernel space. This buffer must be at least count bytes long. Maximum number of bytes to copy, including the trailing NUL. On success, returns the length of the string INCLUDING the trailing NUL. If access fails, returns -EFAULT (some data may have been copied and the trailing NUL added). If count is smaller than the length of the string, copies count-1 bytes, sets the last byte of dst buffer to NUL and returns count.\n• None Get the size of a user string INCLUDING final NUL. Get the size of a NUL-terminated string in user space without pagefault. Returns the size of the string INCLUDING the terminating NUL. If the string is too long, returns a number larger than count. User has to check the return value against “> count”. On exception (or invalid count), returns 0. Unlike strnlen_user, this can be used from IRQ handler etc. because it disables pagefaults. is the usual dirty throttling mechanism available? The normal page dirty throttling mechanism in balance_dirty_pages() is completely broken with the legacy memcg and direct stalling in shrink_folio_list() is used for throttling instead, which lacks all the niceties such as fairness, adaptive pausing, bandwidth proportional allocation and configurability. This function tests whether the vmscan currently in progress can assume that the normal dirty throttling mechanism is operational. Returns the number of pages on the given LRU list. zones to consider (use MAX_NR_ZONES - 1 for the whole LRU list) Attempt to remove a folio from its mapping. If the folio is dirty, under writeback or if someone else has a ref on it, removal will fail. The number of pages removed from the mapping. 0 if the folio could not be removed. The caller should have a single refcount on the folio and hold its lock. Folio to be returned to an LRU list. Add previously isolated folio to appropriate LRU list. The folio may still be unevictable for other reasons. lru_lock must not be held, interrupts must be enabled. Try to isolate a folio from its LRU list. Folio to isolate from its LRU list. Isolate a folio from an LRU list and adjust the vmstat statistic corresponding to whatever LRU list the folio was on. The folio will have its LRU flag cleared. If it was found on the active list, it will have the Active flag set. If it was found on the unevictable list, it will have the Unevictable flag set. These flags may need to be cleared by the caller before letting the page go.\n• None Must be called with an elevated refcount on the folio. This is a fundamental difference from isolate_lru_folios() (which is called without a stable reference).\n• None The lru_lock must not be held. true if the folio was removed from an LRU list. false if the folio was not on an LRU list. Checks folios for evictability, if an evictable folio is in the unevictable lru list, moves it to the appropriate evictable lru list. This function should be only used for lru folios. starting pageframe (must be aligned to start of a section) number of pages to remove (must be multiple of section size) alternative device page map or if default memmap is used Generic helper function to remove section mappings and sysfs entries for the section of the memory we are removing. Caller needs to make sure that pages are marked reserved and zones are adjust properly by calling offline_pages(). Offline a node if all memory sections and cpus of the node are removed. The caller must call lock_device_hotplug() to serialize hotplug and online/offline operations before this call. Remove memory if every memory block is offline physical address of the region to remove size of the region to remove The caller must call lock_device_hotplug() to serialize hotplug and online/offline operations before this call, as required by . mmu_iterval_read_begin()/mmu_iterval_read_retry() implement a collision-retry scheme similar to seqcount for the VA range under subscription. If the mm invokes invalidation during the critical section then mmu_interval_read_retry() will return true. This is useful to obtain shadow PTEs where teardown or setup of the SPTEs require a blocking context. The critical region formed by this can sleep, and the required ‘user_lock’ can also be a sleeping lock. The caller is required to provide a ‘user_lock’ to serialize both teardown and setup. The return value should be passed to mmu_interval_read_retry(). The mm to attach the notifier to Must not hold mmap_lock nor any other VM related lock when calling this registration function. Must also ensure mm_users can’t go down to zero while this runs to avoid races with mmu_notifier_release, so mm has to be current->mm or the mm should be pinned safely such as with get_task_mm(). If the mm is not current->mm, the mm_users pin should be released by calling mmput after mmu_notifier_register returns. mmu_notifier_unregister() or must be always called to unregister the notifier. While the caller has a mmu_notifier get the subscription->mm pointer will remain valid, and can be converted to an active mm pointer via mmget_not_zero(). Return the single struct mmu_notifier for the mm & ops The operations struct being subscribe with The mm to attach notifiers too This function either allocates a new mmu_notifier via ops->alloc_notifier(), or returns an already existing notifier on the list. The value of the ops pointer is used to determine when two notifiers are the same. Each call to mmu_notifier_get() must be paired with a call to . The caller must hold the write side of mm->mmap_lock. While the caller has a mmu_notifier get the mm pointer will remain valid, and can be converted to an active mm pointer via mmget_not_zero(). Release the reference on the notifier This function must be paired with each mmu_notifier_get(), it releases the reference obtained by the get. If this is the last reference then process to free the notifier will be run asynchronously. Unlike mmu_notifier_unregister() the get/put flow only calls ops->release when the mm_struct is destroyed. Instead free_notifier is always called to release any resources held by the user. As ops->release is not guaranteed to be called, the user must ensure that all sptes are dropped, and no new sptes can be established before is called. This function can be called from the ops->release callback, however the caller must still ensure it is called pairwise with mmu_notifier_get(). Modules calling this function must call in their __exit functions to ensure the async work is completed. Length of the range to monitor Interval notifier operations to be called on matching events This function subscribes the interval notifier for notifications from the mm. Upon return the ops related to mmu_interval_notifier will be called whenever an event that intersects with the given range occurs. Upon return the range_notifier may not be present in the interval tree yet. The caller must use the normal interval notifier read flow via to establish SPTEs for this range. This function must be paired with . It cannot be called from any ops callback. Once this returns ops callbacks are no longer running on other CPUs and will not be called in future. This function ensures that all outstanding async SRU work from is completed. After it returns any mmu_notifier_ops associated with an unused mmu_notifier will no longer be called. Before using the caller must ensure that all of its mmu_notifiers have been fully released via . Modules using the API should call this in their __exit function to avoid module unloading races. inserts a list of pages into the balloon page list. balloon device descriptor where we will insert a new page to Driver must call this function to properly enqueue balloon pages before definitively removing them from the guest system. number of pages that were enqueued. removes pages from balloon’s page list and returns a list of the pages. balloon device descriptor where we will grab a page from. pointer to the list of pages that would be returned to the caller. Driver must call this function to properly de-allocate a previous enlisted balloon pages before definitively releasing it back to the guest system. This function tries to remove n_req_pages from the ballooned pages and return them to the caller in the pages list. Note that this function may fail to dequeue some pages even if the balloon isn’t empty - since the page list can be temporarily empty due to compaction of isolated pages. number of pages that were added to the pages list. this is only safe if the mm semaphore is held when called."
    },
    {
        "link": "https://docs.kernel.org/core-api/printk-basics.html",
        "document": "is one of the most widely known functions in the Linux kernel. It’s the standard tool we have for printing messages and usually the most basic way of tracing and debugging. If you’re familiar with printf(3) you can tell is based on it, although it has some functional differences:\n\nAll messages are printed to the kernel log buffer, which is a ring buffer exported to userspace through /dev/kmsg. The usual way to read it is using .\n\nis typically used like this:\n\nwhere is the log level (note that it’s concatenated to the format string, the log level is not a separate argument). The available log levels are:\n\nThe log level specifies the importance of a message. The kernel decides whether to show the message immediately (printing it to the current console) depending on its log level and the current console_loglevel (a kernel variable). If the message priority is higher (lower log level value) than the console_loglevel the message will be printed to the console.\n\nIf the log level is omitted, the message is printed with level.\n\nYou can check the current console_loglevel with:\n\nThe result shows the current, default, minimum and boot-time-default log levels.\n\nTo change the current console_loglevel simply write the desired level to . For example, to print all messages to the console:\n\nAnother way, using :\n\nsets the console_loglevel to print KERN_WARNING (4) or more severe messages to console. See for more information.\n\nAs an alternative to you can use the aliases for logging. This family of macros embed the log level in the macro names. For example:\n\nBesides being more concise than the equivalent calls, they can use a common definition for the format string through the macro. For instance, defining this at the top of a source file (before any directive):\n\nwould prefix every pr_*() message in that file with the module and function name that originated the message.\n\nFor debugging purposes there are also two conditionally-compiled macros: and , which are compiled-out unless (or also in the case of ) is defined.\n\nused by the pr_*() macros to generate the printk format string This macro can be used to generate a unified format string for pr_*() macros. A common use is to prefix all pr_*() messages in a file with a common string. For example, defining this at the top of a source file: would prefix all pr_info, pr_emerg... messages in the file with the module name. This is . It can be called from any context. We want it to work. If printk indexing is enabled, _printk() is called from printk_index_wrap. Otherwise, printk is simply #defined to _printk. We try to grab the console_lock. If we succeed, it’s easy - we log the output and call the console drivers. If we fail to get the semaphore, we place the output into the log buffer and return. The current holder of the console_sem will notice the new output in ; and will send it to the consoles before releasing the lock. One effect of this deferred printing is that code which calls and then changes console_loglevel may break. This is because console_loglevel is inspected when the actual printing occurs. See the documentation for format string extensions over C99. This macro expands to a printk with KERN_EMERG loglevel. It uses to generate the format string. This macro expands to a printk with KERN_ALERT loglevel. It uses to generate the format string. This macro expands to a printk with KERN_CRIT loglevel. It uses to generate the format string. This macro expands to a printk with KERN_ERR loglevel. It uses to generate the format string. This macro expands to a printk with KERN_WARNING loglevel. It uses to generate the format string. This macro expands to a printk with KERN_NOTICE loglevel. It uses to generate the format string. This macro expands to a printk with KERN_INFO loglevel. It uses to generate the format string. Continues a previous log message in the same line. This macro expands to a printk with KERN_CONT loglevel. It should only be used when continuing a log message with no newline (’n’) enclosed. Otherwise it defaults back to KERN_DEFAULT loglevel. This macro expands to a printk with KERN_DEBUG loglevel if DEBUG is defined. Otherwise it does nothing. It uses to generate the format string. This macro expands to dynamic_pr_debug() if CONFIG_DYNAMIC_DEBUG is set. Otherwise, if DEBUG is defined, it’s equivalent to a printk with KERN_DEBUG loglevel. If DEBUG is not defined it does nothing. It uses to generate the format string (dynamic_pr_debug() uses internally)."
    },
    {
        "link": "https://stackoverflow.com/questions/61734205/how-to-use-printk",
        "document": "it is the first time working in a Linux environment. i need a lot of help from you. i want to add prink() in shced_setattr https://elixir.bootlin.com/linux/v4.18/source/kernel/sched/core.c#L4578\n\nbut i don't know how to edit the procedure. If someone could explain me edit the linux function for the printk() function it would be great!"
    },
    {
        "link": "https://linux-kernel-labs.github.io/refs/heads/master/labs/kernel_modules.html",
        "document": "A monolithic kernel, though faster than a microkernel, has the disadvantage of lack of modularity and extensibility. On modern monolithic kernels, this has been solved by using kernel modules. A kernel module (or loadable kernel mode) is an object file that contains code that can extend the kernel functionality at runtime (it is loaded as needed); When a kernel module is no longer needed, it can be unloaded. Most of the device drivers are used in the form of kernel modules. For the development of Linux device drivers, it is recommended to download the kernel sources, configure and compile them and then install the compiled version on the test /development tool machine.\n\nCompiling a kernel module differs from compiling an user program. First, other headers should be used. Also, the module should not be linked to libraries. And, last but not least, the module must be compiled with the same options as the kernel in which we load the module. For these reasons, there is a standard compilation method ( ). This method requires the use of two files: a and a file. Below is an example of a : And the example of a file used to compile a module: As you can see, calling make on the file in the example shown will result in the make invocation in the kernel source directory ( ) and referring to the current directory ( ). This process ultimately leads to reading the file from the current directory and compiling the module as instructed in this file. For labs we will configure different KDIR, according to the virtual machine specifications: A file contains one or more directives for compiling a kernel module. The easiest example of such a directive is . Following this directive, a kernel module ( - kernel object) will be created, starting from the file. will be created starting from or . All of these files can be found in the 's directory. An example of a file that uses several sub-modules is shown below: For the example above, the steps to compile are:\n• compile the and sources, resulting in module-a.o and module-b.o objects\n• and will then be linked in\n• from will be created module The suffix of targets in determines how they are used, as follows:\n• Y (yes) represents a target for object files to be compiled and then linked to a module ( ) or within the kernel ( )\n• any other target suffix will be ignored by and will not be compiled These suffixes are used to easily configure the kernel by running the make menuconfig command or directly editing the file. This file sets a series of variables that are used to determine which features are added to the kernel at build time. For example, when adding BTRFS support with make menuconfig, add the line to the file. The BTRFS kbuild contains the line , which becomes . This will compile the object and will be linked to the kernel. Before the variable was set, the line became and so it was ignored, and the kernel was build without BTRFS support. For more details, see the and files within the kernel sources.\n\nTroubleshooting a kernel module is much more complicated than debugging a regular program. First, a mistake in a kernel module can lead to blocking the entire system. Troubleshooting is therefore much slowed down. To avoid reboot, it is recommended to use a virtual machine (qemu, virtualbox, vmware). When a module containing bugs is inserted into the kernel, it will eventually generate a kernel oops. A kernel oops is an invalid operation detected by the kernel and can only be generated by the kernel. For a stable kernel version, it almost certainly means that the module contains a bug. After the oops appears, the kernel will continue to work. Very important to the appearance of a kernel oops is saving the generated message. As noted above, messages generated by the kernel are saved in logs and can be displayed with the dmesg command. To make sure that no kernel message is lost, it is recommended to insert/test the kernel directly from the console, or periodically check the kernel messages. Noteworthy is that an oops can occur because of a programming error, but also a because of hardware error. If a fatal error occurs, after which the system can not return to a stable state, a kernel panic is generated. Look at the kernel module below that contains a bug that generates an oops: Inserting this module into the kernel will generate an oops: faust:~/lab-01/modul-oops# insmod oops.ko ... faust:~/lab-01/modul-oops# dmesg tail -32 BUG: unable to handle kernel paging request at IP: <c89d4005> my_oops_init+0x5/0x20 oops *de Oops: last sysfs file: /sys/devices/virtual/net/lo/operstate Modules linked in: oops + netconsole ide_cd_mod pcnet32 crc32 cdrom last unloaded: modul Pid: , comm: insmod Not tainted .6.28.4 EIP: : <c89d4005> EFLAGS: CPU: EIP is at my_oops_init+0x5/0x20 oops EAX: EBX: fffffffc ECX: c89d4300 EDX: ESI: c89d4000 EDI: EBP: c5799e24 ESP: c5799e24 DS: 007b ES: 007b FS: GS: SS: Process insmod pid: , c5799000 c665c780 task.ti c5799000 Stack: c5799f8c c010102d c72b51d8 0000000c c5799e58 c01708e4 c89d4300 c5799e58 c724f448 c89d4300 c5799e60 c0170981 c5799f8c c014b698 c5799f78 c5799f20 c665cb00 c89d4300 Call Trace: <c010102d> ? _stext+0x2d/0x170 <c01708e4> ? __vunmap+0xa4/0xf0 <c0170981> ? vfree+0x21/0x30 <c014b698> ? load_module+0x19b8/0x1a40 <c035e965> ? __mutex_unlock_slowpath+0xd5/0x140 <c0140da6> ? trace_hardirqs_on_caller+0x106/0x150 <c014b7aa> ? sys_init_module+0x8a/0x1b0 <c0140da6> ? trace_hardirqs_on_caller+0x106/0x150 <c0240a08> ? trace_hardirqs_on_thunk+0xc/0x10 <c0103407> ? sysenter_do_call+0x12/0x43 Code: <c7> 5d c3 eb 0d EIP: <c89d4005> my_oops_init+0x5/0x20 oops SS:ESP :c5799e24 --- end trace 2981ce73ae801363 --- Although relatively cryptic, the message provided by the kernel to the appearance of an oops provides valuable information about the error. First line: Tells us the cause and the address of the instruction that generated the error. In our case this is an invalid access to memory. Tells us that it's the first oops (#1). This is important in the context that an oops can lead to other oopses. Usually only the first oops is relevant. Furthermore, the oops code ( ) provides information about the error type (see ): In this case, we have a write access that generated the oops (bit 1 is 1). Below is a dump of the registers. It decodes the instruction pointer ( ) value and notes that the bug appeared in the function with a 5-byte offset ( ). The message also shows the stack content and a backtrace of calls until then. If an invalid read call is generated ( ), the message will be the same, but the oops code will differ, which would now be : faust:~/lab-01/modul-oops# dmesg tail -33 BUG: unable to handle kernel paging request at IP: <c89c3016> my_oops_init+0x6/0x20 oops *de Oops: last sysfs file: /sys/devices/virtual/net/lo/operstate Modules linked in: oops + netconsole pcnet32 crc32 ide_cd_mod cdrom Pid: , comm: insmod Not tainted .6.28.4 EIP: : <c89c3016> EFLAGS: CPU: EIP is at my_oops_init+0x6/0x20 oops EAX: EBX: fffffffc ECX: c89c3380 EDX: ESI: c89c3010 EDI: EBP: c57cbe24 ESP: c57cbe1c DS: 007b ES: 007b FS: GS: SS: Process insmod pid: , c57cb000 c66ec780 task.ti c57cb000 Stack: c57cbe34 c57cbf8c c010102d c57b9280 0000000c c57cbe58 c01708e4 c89c3380 c57cbe58 c5db1d38 c89c3380 c57cbe60 c0170981 c57cbf8c c014b698 c57cbf78 c57cbf20 Call Trace: <c010102d> ? _stext+0x2d/0x170 <c01708e4> ? __vunmap+0xa4/0xf0 <c0170981> ? vfree+0x21/0x30 <c014b698> ? load_module+0x19b8/0x1a40 <c035d083> ? printk+0x0/0x1a <c035e965> ? __mutex_unlock_slowpath+0xd5/0x140 <c0140da6> ? trace_hardirqs_on_caller+0x106/0x150 <c014b7aa> ? sys_init_module+0x8a/0x1b0 <c0140da6> ? trace_hardirqs_on_caller+0x106/0x150 <c0240a08> ? trace_hardirqs_on_thunk+0xc/0x10 <c0103407> ? sysenter_do_call+0x12/0x43 Code: <a1> c7 9c c8 e8 a0 f7 EIP: <c89c3016> my_oops_init+0x6/0x20 oops SS:ESP :c57cbe1c --- end trace 45eeb3d6ea8ff1ed --- Detailed information about the instruction that generated the oops can be found using the objdump utility. Useful options to use are -d to disassemble the code and -S for interleaving C code in assembly language code. For efficient decoding, however, we need the address where the kernel module was loaded. This can be found in . Here's an example of using objdump on the above module to identify the instruction that generated the oops: Note that the instruction that generated the oops ( identified earlier) is: That is exactly what was expected - storing value 3 at 0x0001234. The is used to find the address where a kernel module is loaded. The --adjust-vma option allows you to display instructions relative to . The -l option displays the number of each line in the source code interleaved with the assembly language code. A more simplistic way to find the code that generated an oops is to use the addr2line utility: Where is the value of the program counter ( ) that generated the oops, minus the base address of the module ( ) according to Minicom (or other equivalent utilities, eg picocom, screen) is a utility that can be used to connect and interact with a serial port. The serial port is the basic method for analyzing kernel messages or interacting with an embedded system in the development phase. There are two more common ways to connect:\n• a serial port where the device we are going to use is\n• a serial USB port (FTDI) in which case the device we are going to use is . For the virtual machine used in the lab, the device that we need to use is displayed after the virtual machine starts: #for connecting via COM1 and using a speed of 115,200 characters per second minicom -b -D /dev/ttyS0 minicom -D /dev/ttyUSB0 #To connect to the serial port of the virtual machine minicom -D /dev/pts/20 Netconsole is a utility that allows logging of kernel debugging messages over the network. This is useful when the disk logging system does not work or when serial ports are not available or when the terminal does not respond to commands. Netconsole comes in the form of a kernel module. To work, it needs the following parameters:\n• port, IP address, and the source interface name of the debug station\n• port, MAC address, and IP address of the machine to which the debug messages will be sent These parameters can be configured when the module is inserted into the kernel, or even while the module is inserted if it has been compiled with the option. An example configuration when inserting netconsole kernel module is as follows: Thus, the debug messages on the station that has the address will be sent to the interface, having source port . The messages will be sent to with the MAC address , on port . Messages can be played on the destination station using netcat: Alternatively, the destination station can configure syslogd to intercept these messages. More information can be found in . The two oldest and most useful debugging aids are Your Brain and Printf . For debugging, a primitive way is often used, but it is quite effective: debugging. Although a debugger can also be used, it is generally not very useful: simple bugs (uninitialized variables, memory management problems, etc.) can be easily localized by control messages and the kernel-decoded oop message. For more complex bugs, even a debugger can not help us too much unless the operating system structure is very well understood. When debugging a kernel module, there are a lot of unknowns in the equation: multiple contexts (we have multiple processes and threads running at a time), interruptions, virtual memory, etc. You can use to display kernel messages to user space. It is similar to 's functionality; the only difference is that the transmitted message can be prefixed with a string of , where indicates the error level (loglevel) and has values between and . Instead of , the levels can also be coded by symbolic constants: The definitions of all log levels are found in . Basically, these log levels are used by the system to route messages sent to various outputs: console, log files in etc. To display messages in user space, the log level must be of higher priority than variable. The default console log level can be configured from . will enable all the kernel log messages to be displayed in the console. That is, the logging level has to be strictly less than the variable. For example, if the has a value of (specific to ), only messages with loglevel stricter than (i.e , , , , ) will be shown. Console-redirected messages can be useful for quickly viewing the effect of executing the kernel code, but they are no longer so useful if the kernel encounters an irreparable error and the system freezes. In this case, the logs of the system must be consulted, as they keep the information between system restarts. These are found in and are text files, populated by and during the kernel run. and take the information from the virtual file system mounted in . In principle, with and turned on, all messages coming from the kernel will go to . A simpler version for debugging is using the file. It is populated only with the messages from the kernel with the log level. Given that a production kernel (similar to the one we're probably running with) contains only release code, our module is among the few that send messages prefixed with KERN_DEBUG . In this way, we can easily navigate through the information by finding the messages corresponding to a debugging session for our module. Such an example would be the following: # Clear the debug file of previous information (or possibly a backup) $ > /var/log/debug # If there is no critical error causing a panic kernel, check the output # if a critical error occurs and the machine only responds to a restart, restart the system and check /var/log/debug. The format of the messages must obviously contain all the information of interest in order to detect the error, but inserting in the code to provide detailed information can be as time-consuming as writing the code to solve the problem. This is usually a trade-off between the completeness of the debugging messages displayed using and the time it takes to insert these messages into the text. A very simple way, less time-consuming for inserting and providing the possibility to analyze the flow of instructions for tests is the use of the predefined constants , and :\n• is replaced by the compiler with the name of the source file it is currently being compiled.\n• is replaced by the compiler with the line number on which the current instruction is found in the current source file.\n• / is replaced by the compiler with the name of the function in which the current instruction is found. and are part of the ANSI C specifications: is part of specification C99; is a GNU extension and is not portable; However, since we write code for the kernel, we can use it without any problems. The following macro definition can be used in this case: Then, at each point where we want to see if it is \"reached\" in execution, insert PRINT_DEBUG; This is a simple and quick way, and can yield by carefully analyzing the output. The dmesg command is used to view the messages printed with but not appearing on the console. To delete all previous messages from a log file, run: To delete messages displayed by the dmesg command, run: Dynamic dyndbg debugging enables dynamic debugging activation/deactivation. Unlike , it offers more advanced options for the messages we want to display; it is very useful for complex modules or troubleshooting subsystems. This significantly reduces the amount of messages displayed, leaving only those relevant for the debug context. To enable , the kernel must be compiled with the option. Once configured, , and , can be dynamically enabled per call. The file from the debugfs (where is the path to which debugfs was mounted) is used to filter messages or to view existing filters. Debugfs is a simple file system, used as a kernel-space interface and user-space interface to configure different debug options. Any debug utility can create and use its own files /folders in debugfs. For example, to display existing filters in , you will use: And to enable the debug message from line in the file: The file is not a regular file. It shows the settings on the filters. Writing in it with an echo will change these settings (it will not actually make a write). Be aware that the file contains settings for debugging messages. Do not log in this file.\n• None - just the debug messages from the functions that have the same name as the one defined in the filter.\n• None - the name of the file(s) for which we want to display the debug messages. It can be just the source name, but also the absolute path or kernel-tree path.\n• None - only messages whose display format contains the specified string.\n• None - the line or lines for which we want to enable debug calls. # Triggers debug messages between lines 1603 and 1605 in the svcsock.c file $ > /sys/kernel/debug/dynamic_debug/control # Enables debug messages from the beginning of the file to line 1605 $ > /sys/kernel/debug/dynamic_debug/control In addition to the above options, a series of flags can be added, removed, or set with operators , or :\n• includes the name of the function in the printed message.\n• includes the line number in the printed message.\n• includes the module name in the printed message.\n• includes the thread id if it is not called from interrupt context The kernel debugger has proven to be very useful to facilitate the development and debugging process. One of its main advantages is the possibility to perform live debugging. This allows us to monitor, in real time, the accesses to memory or even modify the memory while debugging. The debugger has been integrated in the mainline kernel starting with version 2.6.26-rci. KDB is not a source debugger, but for a complete analysis it can be used in parallel with gdb and symbol files -- see the GDB debugging section To use KDB, you have the following options: For the lab, we will use a serial interface connected to the host. The following command will activate GDB over the serial port: KDB is a stop mode debugger, which means that, while it is active, all the other processes are stopped. The kernel can be forced to enter KDB during execution using the following SysRq command or by using the key combination in a terminal connected to the serial port (for example using minicom). KDB has various commands to control and define the context of the debugged system: For a better description of the available commands you can use the command in the KDB shell. In the next example, you can notice a simple KDB usage example which sets a hardware breakpoint to monitor the changes of the variable. g > /proc/sysrq-trigger # or if we are connected to the serial port issue Ctrl-O g # breakpoint on write access to the mVar variable kdb> bph mVar dataw kdb> go\n\nWe strongly encourage you to use the setup from this repository. To solve exercises, you need to perform these steps:\n• start the VM and test the module in the VM. The current lab name is kernel_modules. See the exercises for the task name. The skeleton code is generated from full source examples located in . To solve the tasks, start by generating the skeleton code for a complete lab: You can also generate the skeleton for a single task, using Once the skeleton drivers are generated, build the source: The modules are placed in /home/root/skels/kernel_modules/<task_name>. You DO NOT need to STOP the VM when rebuilding modules! The local directory is shared with the VM. Review the Exercises section for more detailed information. Before starting the exercises or generating the skeletons, please run git pull inside the Linux repo, to make sure you have the latest version of the exercises. If you have local changes, the pull command will fail. Check for local changes using . If you want to keep them, run before and after. To discard the changes, run . If you already generated the skeleton before you will need to generate it again. Using cscope or LXR find the definitions of the following symbols in the Linux kernel source code:\n• and\n• what do the two macros do? What is and ?\n• None\n• What is this variable used for? If you have problems using cscope, it is possible that the database is not generated. To generate it, use the following command in the kernel directory: When searching for a structure using cscope, use only the structure name (without ). So, to search for the structure , you will use the command For more info on using cscope, read the cscope section in the previous lab. To work with the kernel modules, we will follow the steps described above. Generate the skeleton for the task named 1-2-test-mod then build the module, by running the following command in . These command will build all the modules in the current lab skeleton. Until after solving exercise 3, you will get a compilation error for . To avoid this issue, remove the directory and remove the corresponding line from . Start the VM using make console, and perform the following tasks:\n• list the kernel modules and check if current module is present\n• view the messages displayed at loading/unloading the kernel module using dmesg command Read Loading/unloading a kernel module section. When unloading a kernel module, you can specify only the module name (without extension). Watch the virtual machine console. Why were the messages displayed directly to the virtual machine console? Configure the system such that the messages are not displayed directly on the serial console, and they can only be inspected using . One option is to set the console log level by writting the desired level to . Use a value smaller than the level used for the prints in the source code of the module. Load/unload the module again. The messages should not be printed to the virtual machine console, but they should be visible when running . Generate the skeleton for the task named 3-error-mod. Compile the sources and get the corresponding kernel module. Why have compilation errors occurred? Hint: How does this module differ from the previous module? Modify the module to solve the cause of those errors, then compile and test the module. Inspect the C source files and in . Module 2 contains only the definition of a function used by module 1. Change the file to create the module from the two C source files. Read the Compiling kernel modules section of the lab. Compile, copy, boot the VM, load and unload the kernel module. Make sure messages are properly displayed on the console. Enter the directory for the task 5-oops-mod and inspect the C source file. Notice where the problem will occur. Add the compilation flag in the Kbuild file. Compile the corresponding module and load it into the kernel. Identify the memory address at which the oops appeared. Read `Debugging`_ section of the lab. To identify the address, follow the oops message and extract the value of the instructions pointer ( ) register. Determine which instruction has triggered the oops. Use the information to get the load address of the kernel module. Use, on the physical machine, objdump and/or addr2line . Objdump needs debugging support for compilation! Read the lab's objdump and addr2line sections. Try to unload the kernel module. Notice that the operation does not work because there are references from the kernel module within the kernel since the oops; Until the release of those references (which is almost impossible in the case of an oops), the module can not be unloaded. Enter the directory for the task 6-cmd-mod and inspect the C source file. Compile and copy the associated module and load the kernel module to see the printk message. Then unload the module from the kernel. Without modifying the sources, load the kernel module so that the message shown is . The str variable can be changed by passing a parameter to the module. Find more information here. Check the skeleton for the task named 7-list-proc. Add code to display the Process ID ( ) and the executable name for the current process. Follow the commands marked with . The information must be displayed both when loading and unloading the module.\n• In the Linux kernel, a process is described by the . Use LXR or to find the definition of .\n• To find the structure field that contains the name of the executable, look for the \"executable\" comment.\n• The pointer to the structure of the current process running at a given time in the kernel is given by the variable (of the type ). To use you'll need to include the header in which the is defined, i.e . Compile, copy, boot the VM and load the module. Unload the kernel module. Repeat the loading/unloading operation. Note that the PIDs of the displayed processes differ. This is because a process is created from the executable when the module is loaded and when the module is unloaded a process is created from the executable .\n\nGo to the 8-kdb directory. Activate KDB over the serial port and enter KDB mode using SysRq. Connect to the pseudo-terminal linked to virtiocon0 using minicom, configure KDB to use the hvc0 serial port: and enable it using SysRq (Ctrl + O g). Review the current system status (help to see the available KDB commands). Continue the kernel execution using the go command. Load the module. The module will simulate a bug when writing to the file. To simulate a bug, use the below command: After running the above command, at every oops/panic the kernel stops the execution and enters debug mode. Analyze the stacktrace and determine the code that generated the bug. How can we find out from KDB the address where the module was loaded? In parallel, use GDB in a new window to view the code based on KDB information. When writing to , the module will increment the variable. Enter KDB and set a breakpoint for each write access of the variable. Return to kernel to trigger a write using: Update the created kernel module at proc-info in order to display information about all the processes in the system, when inserting the kernel module, not just about the current process. Afterwards, compare the obtained result with the output of the ps command.\n• Processes in the system are structured in a circular list.\n• macros (such as ) are useful when you want to navigate the items in a list.\n• To understand how to use a feature or a macro, use LXR or Vim and cscope and search for usage scenarios. Create a kernel module that displays the virtual memory areas of the current process; for each memory area it will display the start address and the end address.\n• Investigate the structures , and . A memory area is indicated by a structure of type .\n• Don't forget to include the headers where the necessary structures are defined. Go to the 9-dyndbg directory and compile the module. Familiarize yourself with the file system mounted in and analyze the contents of the file . Insert the module and notice the new content of the file. What appears extra in the respective file? Run the following command: Configure dyndbg so that only messages marked as \"Important\" in function are displayed when the module is unloaded. The exercise will only filter out the calls; calls being always displayed. Specify two ways to filter. Read the Dynamic debugging section and look at the dyndbg options (for example, line, format). Perform the filtering and revise the file. What has changed? How do you know which calls are activated? Check the dyndbg flags. Unload the kernel module and observe the log messages. As you have noticed, calls can only be activated /filtered after module insertion. In some situations, it might be helpful to view the messages from the initialization of the module. This can be done by using a default (fake) parameter called dyndbg that can be passed as an argument to initialize the module. With this parameter you can add /delete dyndbg flags. Read the last part of the Dynamic debugging section and see the available flags (e.g.: +/- p). Read the Debug Messages section at Module Initialization Time and insert the module so that the messages in (called ) are also displayed during initialization. In the VM from the lab, you will need to use insmod instead of modprobe. You can delete the set flags. Unload the kernel module."
    },
    {
        "link": "https://kernel.org/doc/html/v5.8/core-api/printk-basics.html",
        "document": "is one of the most widely known functions in the Linux kernel. It’s the standard tool we have for printing messages and usually the most basic way of tracing and debugging. If you’re familiar with printf(3) you can tell is based on it, although it has some functional differences:\n\nAll messages are printed to the kernel log buffer, which is a ring buffer exported to userspace through /dev/kmsg. The usual way to read it is using .\n\nis typically used like this:\n\nwhere is the log level (note that it’s concatenated to the format string, the log level is not a separate argument). The available log levels are:\n\nThe log level specifies the importance of a message. The kernel decides whether to show the message immediately (printing it to the current console) depending on its log level and the current console_loglevel (a kernel variable). If the message priority is higher (lower log level value) than the console_loglevel the message will be printed to the console.\n\nIf the log level is omitted, the message is printed with level.\n\nYou can check the current console_loglevel with:\n\nThe result shows the current, default, minimum and boot-time-default log levels.\n\nTo change the current console_loglevel simply write the the desired level to . For example, to print all messages to the console:\n\nAnother way, using :\n\nsets the console_loglevel to print KERN_WARNING (4) or more severe messages to console. See for more information.\n\nAs an alternative to you can use the aliases for logging. This family of macros embed the log level in the macro names. For example:\n\nBesides being more concise than the equivalent calls, they can use a common definition for the format string through the macro. For instance, defining this at the top of a source file (before any directive):\n\nwould prefix every pr_*() message in that file with the module and function name that originated the message.\n\nFor debugging purposes there are also two conditionally-compiled macros: and , which are compiled-out unless (or also in the case of ) is defined.\n\nThis is . It can be called from any context. We want it to work. We try to grab the console_lock. If we succeed, it’s easy - we log the output and call the console drivers. If we fail to get the semaphore, we place the output into the log buffer and return. The current holder of the console_sem will notice the new output in ; and will send it to the consoles before releasing the lock. One effect of this deferred printing is that code which calls and then changes console_loglevel may break. This is because console_loglevel is inspected when the actual printing occurs. See the documentation for format string extensions over C99. used by the pr_*() macros to generate the printk format string This macro can be used to generate a unified format string for pr_*() macros. A common use is to prefix all pr_*() messages in a file with a common string. For example, defining this at the top of a source file: would prefix all pr_info, pr_emerg… messages in the file with the module name. This macro expands to a printk with KERN_EMERG loglevel. It uses to generate the format string. This macro expands to a printk with KERN_ALERT loglevel. It uses to generate the format string. This macro expands to a printk with KERN_CRIT loglevel. It uses to generate the format string. This macro expands to a printk with KERN_ERR loglevel. It uses to generate the format string. This macro expands to a printk with KERN_WARNING loglevel. It uses to generate the format string. This macro expands to a printk with KERN_NOTICE loglevel. It uses to generate the format string. This macro expands to a printk with KERN_INFO loglevel. It uses to generate the format string. Continues a previous log message in the same line. This macro expands to a printk with KERN_CONT loglevel. It should only be used when continuing a log message with no newline (‘n’) enclosed. Otherwise it defaults back to KERN_DEFAULT loglevel. This macro expands to a printk with KERN_DEBUG loglevel if DEBUG is defined. Otherwise it does nothing. It uses to generate the format string. This macro expands to dynamic_pr_debug() if CONFIG_DYNAMIC_DEBUG is set. Otherwise, if DEBUG is defined, it’s equivalent to a printk with KERN_DEBUG loglevel. If DEBUG is not defined it does nothing. It uses to generate the format string (dynamic_pr_debug() uses internally)."
    },
    {
        "link": "https://stackoverflow.com/questions/57754930/how-to-find-which-function-prints-printk-statement",
        "document": "Since we are talking about Linux kernel programming, there are several ways to achieve this. The worst one is to define custom macro where you add something additional to print. Now, let's consider better approaches.\n\nApproach 1. If you are interesting to put function name to only subset of the messages, assuming for debug purposes, the best option is to enable Dynamic Debug option and use special macros instead of direct calls to , i.e. , , and so on.\n\nIt will allow you to turn on and off any single message at run time along with enabling __func__ to be printed or not.\n\nApproach 2. Another approach if you want to enable additional arguments to the families of macros, such as and , you may define special macros at the very beginning of each module or, if you want, include from the header, though it must be the very fist one in each C-file. See example from ipmi_msghandler.c:\n\nIt can be easily transformed to\n\nThe benefit of this method is a possibility to set up different prefixes to different modules, like using their filenames, and it will be applied to all messages of the same family of macros.\n\nDisadvantage is that only and families do have such facilities."
    }
]