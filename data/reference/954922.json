[
    {
        "link": "https://sycured-aiokafka.readthedocs.io/en/latest/api.html",
        "document": "Security is not an easy thing, at least when you want to do it right. Before diving in on how to setup to work with SSL, make sure there is a need for SSL Authentication and go through the official documentation for SSL support in Kafka itself.\n\nprovides only as a parameter for Consumer and Producer classes. This is done intentionally, as it is recommended that you read through the python ssl documentation to have some understanding on the topic. Although if you know what you are doing, there is a simple helper function `aiokafka.helpers.create_ssl_context`_, that will create an based on similar params to .\n\nA few notes on Kafka’s SSL store types. Java uses JKS store type, that contains normal certificates, same as ones OpenSSL (and Python, as it’s based on OpenSSL) uses, but encodes them into a single, encrypted file, protected by another password. Just look the internet on how to extract , and from JKS store.\n\nSee also the Using SSL with aiokafka example."
    },
    {
        "link": "https://aiokafka.readthedocs.io/en/stable/consumer.html",
        "document": "is a client that consumes records from a Kafka cluster. Most simple usage would be:\n\ntransparently handles the failure of Kafka brokers and transparently adapts as topic partitions it fetches migrate within the cluster. It also interacts with the broker to allow groups of consumers to load balance consumption using Consumer Groups.\n\nKafka maintains a numerical for each record in a partition. This acts as a unique identifier of a record within that partition and also denotes the position of the consumer in the partition. For example: # Position is the next fetched offset To use the and APIs you need to set to something other than . See consumer-groups below. Here if the consumer is at position , it has consumed records with offsets through and will next receive the record with offset . There are actually two notions of position:\n• None The position gives the of the next record that should be given out. It will be one larger than the highest the consumer has seen in that partition. It automatically increases every time the consumer yields messages in either or calls.\n• None The committed position is the last that has been stored securely. Should the process restart, this is the offset that the consumer will start from. The consumer can either automatically commit offsets periodically, or it can choose to control this committed position manually by calling . This distinction gives the consumer control over when a record is considered consumed. It is discussed in further detail below. For most simple use cases auto committing is probably the best choice: # Consumer must be in a group to commit # Is True by default anyway This example can have “At least once” delivery semantics, but only if we process messages one at a time. If you want “At least once” semantics for batch operations you should use manual commit: # Consumer must be in a group to commit When using manual commit it is recommended to provide a which will process pending messages in the batch and commit before allowing rejoin. If your group will rebalance during processing commit will fail with , as partitions may have been processed by other consumer already. This example will hold on to messages until we have enough to process in bulk. The algorithm can be enhanced by taking advantage of:\n• None to avoid multiple calls to get a batch of messages.\n• None to understand if we have more unconsumed messages or this one is the last one in the partition. If you want to have more control over which partition and message is committed, you can specify offset manually: # Commit progress only for this partition The committed offset should always be the offset of the next message that your application will read. Thus, when calling you should add one to the offset of the last message processed. Here we process a batch of messages per partition and commit not all consumed offsets, but only for the partition, we processed. In most use cases the consumer will simply consume records from beginning to end, periodically committing its position (either automatically or manually). If you only want your consumer to process newest messages, you can ask it to start from offset: If you have a valid committed position consumer will use that. will only be used when the position is invalid. Kafka also allows the consumer to manually control its position, moving forward or backwards in a partition at will using . For example, you can re-consume records: Also you can combine it with API to query to specific offsets based on timestamp. There are several use cases where manually controlling the consumer’s position can be useful. One case is for time-sensitive record processing it may make sense for a consumer that falls far enough behind to not attempt to catch up processing all records, but rather just skip to the most recent records. Or you can use API to get the offsets after certain timestamp. Another use case is for a system that maintains local state. In such a system the consumer will want to initialize its position on startup to whatever is contained in the local store. Likewise, if the local state is destroyed (say because the disk is lost) the state may be recreated on a new machine by re-consuming all the data and recreating the state (assuming that Kafka is retaining sufficient history). See also related configuration params and API docs:\n• None config option to set behaviour in case the position is either undefined or incorrect.\n• None , , API’s to query offsets for partitions even if they are not assigned to this consumer. Storing offsets in Kafka is optional, you can store offsets in another place and use API to start from saved position. The primary use case for this is allowing the application to store both the offset and the results of the consumption in the same system in a way that both the results and offsets are stored atomically. For example, if we save aggregated by counts in Redis: # Load initial state of aggregation and last processed offset # Same as with manual commit, you need to fetch next message, so +1 So to save results outside of Kafka you need to:\n• None Use the offset provided with each to save your position\n• None On restart or rebalance restore the position of the consumer using This is not always possible, but when it is it will make the consumption fully atomic and give exactly once semantics that are stronger than the default at-least once semantics you get with Kafka’s offset commit functionality. This type of usage is simplest when the partition assignment is also done manually (like we did above). If the partition assignment is done automatically special care is needed to handle the case where partition assignments change. See Local state and storing offsets outside of Kafka example for more details.\n\nKafka uses the concept of Consumer Groups to allow a pool of processes to divide the work of consuming and processing records. These processes can either be running on the same machine or they can be distributed over many machines to provide scalability and fault tolerance for processing. All instances sharing the same will be part of the same Consumer Group: Each consumer in a group can dynamically set the list of topics it wants to subscribe to through call. Kafka will deliver each message in the subscribed topics to only one of the processes in each consumer group. This is achieved by balancing the partitions between all members in the consumer group so that each partition is assigned to exactly one consumer in the group. So if there is a topic with four partitions and a consumer group with two processes, each process would consume from two partitions. Membership in a consumer group is maintained dynamically: if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group. Similarly, if a new consumer joins the group, partitions will be moved from existing consumers to the new one. This is known as rebalancing the group. Conceptually you can think of a Consumer Group as being a single logical subscriber that happens to be made up of multiple processes. In addition, when group reassignment happens automatically, consumers can be notified through a , which allows them to finish necessary application-level logic such as state cleanup, manual offset commits, etc. See docs for more details. Be careful with to avoid deadlocks. The Consumer will await the defined handlers and will block subsequent calls to and . For example this code will deadlock: You need to put call outside of the lock. For more information on how Consumer Groups are organized see Official Kafka Docs. performs periodic metadata refreshes in the background and will notice when new partitions are added to one of the subscribed topics or when a new topic matching a subscribed regex is created. For example: Here, the consumer will automatically detect new topics like or and start consuming them. If you use Consumer Groups the group’s Leader will trigger a group rebalance when it notices metadata changes. It’s because only the Leader has full knowledge of which topics are assigned to the group. It is also possible for the consumer to manually assign specific partitions using . In this case, dynamic partition assignment and consumer group coordination will be disabled. For example: can still be used for committing position, but be careful to avoid collisions with multiple instances sharing the same group. It is not possible to mix manual partition assignment and topic subscription . An attempt to do so will result in an . By default Consumer will fetch from all partitions, effectively giving these partitions the same priority. However in some cases, you would want for some partitions to have higher priority (say they have more lag and you want to catch up). For example: # Fetch all partitions on first request This interface differs from / interface of kafka-python and Java clients. Here we will consume all partitions if they do not lag behind, but if some go above a certain threshold, we will consume them to catch up. This can very well be used in a case where some consumer died and this consumer took over its partitions, that are now lagging behind. Some things to note about it:\n• None There may be a slight pause in consumption if you change the partitions you are fetching. This can happen when Consumer requests a fetch for partitions that have no data available. Consider setting a relatively low to avoid this.\n• None The interface can not be used with explicit partition filtering, just use instead. Transactions were introduced in Kafka 0.11.0 wherein applications can write to multiple topics and partitions atomically. In order for this to work, consumers reading from these partitions should be configured to only read committed data. This can be achieved by by setting the in the consumer’s configuration: In mode, the consumer will read only those transactional messages which have been successfully committed. It will continue to read non-transactional messages as before. There is no client-side buffering in mode. Instead, the end offset of a partition for a consumer would be the offset of the first message in the partition belonging to an open transaction. This offset is known as the Last Stable Offset (LSO). A consumer will only read up to the LSO and filter out any transactional messages which have been aborted. The LSO also affects the behavior of and for consumers, details of which are in each method’s documentation. Finally, API was added similarly to API to query the lSO on a currently assigned transaction: Partitions with transactional messages will include commit or abort markers which indicate the result of a transaction. There markers are not returned to applications, yet have an offset in the log. As a result, applications reading from topics with transactional messages will see gaps in the consumed offsets. These missing messages would be the transaction markers, and they are filtered out for consumers in both isolation levels. Additionally, applications using consumers may also see gaps due to aborted transactions, since those messages would not be returned by the consumer and yet would have valid offsets."
    },
    {
        "link": "https://sycured-aiokafka.readthedocs.io",
        "document": "aiokafka is a client for the Apache Kafka distributed stream processing system using asyncio. It is based on the kafka-python library and reuses its internals for protocol parsing, errors, etc. The client is designed to function much like the official Java client, with a sprinkling of Pythonic interfaces.\n\naiokafka can be used with 0.9+ Kafka brokers and supports fully coordinated consumer groups – i.e., dynamic partition assignment to multiple consumers in the same group.\n\nis a high-level message consumer, intended to operate as similarly as possible to the official Java client. # Wait for all pending messages to be delivered or expire.\n\nNote, that on Windows you will need Visual Studio build tools, available for download from http://landinghub.visualstudio.com/visual-cpp-build-tools For Windows the easiest way is to fetch a precompiled wheel from http://www.lfd.uci.edu/~gohlke/pythonlibs/#python-snappy To enable SASL authentication with GSSAPI you need to install :\n\nThe project is hosted on GitHub Please feel free to file an issue on bug tracker if you have found a bug or have some suggestion for library improvement. The library uses Travis for Continious Integration."
    },
    {
        "link": "https://aiokafka.readthedocs.io/en/stable/api.html",
        "document": "A Kafka client that publishes records to the Kafka cluster. The producer consists of a pool of buffer space that holds records that haven’t yet been transmitted to the server as well as a background task that is responsible for turning these records into requests and transmitting them to the cluster. The method is asynchronous. When called it adds the record to a buffer of pending record sends and immediately returns. This allows the producer to batch together individual records for efficiency. The config controls the criteria under which requests are considered complete. The setting will result in waiting for all replicas to respond, the slowest but most durable setting. The and instruct how to turn the key and value objects the user provides into .\n• None bootstrap_servers (str, list(str)) – a string or list of strings that the producer should contact to bootstrap initial cluster metadata. This does not have to be the full node list. It just needs to have at least one broker that will respond to a Metadata API Request. Default port is 9092. If no servers are specified, will default to .\n• None client_id (str) – a name for this client. This string is passed in each request to servers and can be used to identify specific server-side log entries that correspond to this client. Default: (appended with a unique number per instance)\n• None key_serializer (Callable) – used to convert user-supplied keys to bytes If not , called as should return . Default: .\n• None value_serializer (Callable) – used to convert user-supplied message values to . If not , called as , should return . Default: .\n• None one of , , . The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent. The following settings are common:\n• None : Producer will not wait for any acknowledgment from the server at all. The message will immediately be added to the socket buffer and considered sent. No guarantee can be made that the server has received the record in this case, and the retries configuration will not take effect (as the client won’t generally know of any failures). The offset given back for each record will always be set to -1.\n• None : The broker leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers. In this case should the leader fail immediately after acknowledging the record but before the followers have replicated it then the record will be lost.\n• None : The broker leader will wait for the full set of in-sync replicas to acknowledge the record. This guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This is the strongest available guarantee. If unset, defaults to . If is defaults to\n• None compression_type (str) – The compression type for all data generated by the producer. Valid values are , , , or . Compression is of full batches of data, so the efficacy of batching will also impact the compression ratio (more batching means better compression). Default: .\n• None max_batch_size (int) – Maximum size of buffered data per partition. After this amount coroutine will block until batch is drained. Default: 16384\n• None linger_ms (int) – The producer groups together any records that arrive in between request transmissions into a single batched request. Normally this occurs only under load when records arrive faster than they can be sent out. However in some circumstances the client may want to reduce the number of requests even under moderate load. This setting accomplishes this by adding a small amount of artificial delay; that is, if first request is processed faster, than , producer will wait . Default: 0 (i.e. no delay).\n• None partitioner (Callable) – Callable used to determine which partition each message is assigned to. Called (after key serialization): . The default partitioner implementation hashes each non-None key using the same murmur2 algorithm as the Java client so that messages with the same key are assigned to the same partition. When a key is , the message is delivered to a random partition (filtered to partitions with available leaders only, if possible).\n• None max_request_size (int) – The maximum size of a request. This is also effectively a cap on the maximum record size. Note that the server has its own cap on record size which may be different from this. This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests. Default: 1048576.\n• None metadata_max_age_ms (int) – The period of time in milliseconds after which we force a refresh of metadata even if we haven’t seen any partition leadership changes to proactively discover any new brokers or partitions. Default: 300000\n• None request_timeout_ms (int) – Produce request timeout in milliseconds. As it’s sent as part of (it’s a blocking call), maximum waiting time can be up to . Default: 40000.\n• None retry_backoff_ms (int) – Milliseconds to backoff when retrying on errors. Default: 100.\n• None api_version (str) – specify which kafka API version to use. If set to , will attempt to infer the broker version by probing various APIs. Default:\n• None security_protocol (str) – Protocol used to communicate with brokers. Valid values are: , , , . Default: .\n• None ssl_context (ssl.SSLContext) – pre-configured for wrapping socket connections. Directly passed into asyncio’s . For more information see SSL Authentication. Default:\n• None connections_max_idle_ms (int) – Close idle connections after the number of milliseconds specified by this config. Specifying will disable idle checks. Default: 540000 (9 minutes).\n• None enable_idempotence (bool) – When set to , the producer will ensure that exactly one copy of each message is written in the stream. If , producer retries due to broker failures, etc., may write duplicates of the retried message in the stream. Note that enabling idempotence acks to set to . If it is not explicitly set by the user it will be chosen. If incompatible values are set, a will be thrown. New in version 0.5.0.\n• None sasl_mechanism (str) – Authentication mechanism when security_protocol is configured for or . Valid values are: , , , , . Default: Many configuration parameters are taken from the Java client: https://kafka.apache.org/documentation.html#producerconfigs The batch is not queued for send until submission to . empty batch to be filled and submitted by the caller. Wait until all batches are Delivered and futures resolved Returns set of all known partitions for the topic.\n• None topic (str) – topic where the message will be published\n• None message value. Must be type , or be serializable to via configured . If value is , key is required and message acts as a . See Kafka compaction documentation for more details. (compaction requires kafka >= 0.8.1)\n• None partition (int, Optional) – optionally specify a partition. If not set, the partition will be selected using the configured .\n• None key (Optional) – a key to associate with the message. Can be used to determine which partition to send the message to. If partition is (and producer’s partitioner config is left as default), then messages with the same key will be delivered to the same partition (but if key is , partition is chosen randomly). Must be type , or be serializable to bytes via configured .\n• None timestamp_ms (int, Optional) – epoch milliseconds (from Jan 1 1970 UTC) to use as the message timestamp. Defaults to current time.\n• None headers (Optional) – Kafka headers to be included in the message using the format . Iterable of tuples where key is a normal string and value is a byte string. object that will be set when message is processed KafkaTimeoutError – if we can’t schedule this record (pending buffer is full) in up to milliseconds. The returned future will wait based on setting. Cancelling the returned future will not stop event from being sent, but cancelling the coroutine itself will. Publish a message to a topic and wait the result\n• None topic (str) – topic where the batch will be published.\n• None partition (int) – partition where this batch will be published. object that will be set when the batch is Flush all pending data and close all connections to kafka cluster\n\nThe consumer will transparently handle the failure of servers in the Kafka cluster, and adapt as topic-partitions are created or migrate between brokers. It also interacts with the assigned Kafka Group Coordinator node to allow multiple consumers to load balance consumption of topics (feature of Kafka >= 0.9.0.0).\n• None *topics (list(str)) – optional list of topics to subscribe to. If not set, call or before consuming records. Passing topics directly is same as calling API.\n• None a string (or list of strings) that the consumer should contact to bootstrap initial cluster metadata. This does not have to be the full node list. It just needs to have at least one broker that will respond to a Metadata API Request. Default port is 9092. If no servers are specified, will default to .\n• None client_id (str) – a name for this client. This string is passed in each request to servers and can be used to identify specific server-side log entries that correspond to this client. Also submitted to for logging with respect to consumer group administration. Default:\n• None group_id (str or None) – name of the consumer group to join for dynamic partition assignment (if enabled), and to use for fetching and committing offsets. If None, auto-partition assignment (via group coordinator) and offset commits are disabled. Default: None\n• None group_instance_id (str or None) – name of the group instance ID used for static membership (KIP-345)\n• None key_deserializer (Callable) – Any callable that takes a raw message key and returns a deserialized key.\n• None value_deserializer (Callable, Optional) – Any callable that takes a raw message value and returns a deserialized value.\n• None fetch_min_bytes (int) – Minimum amount of data the server should return for a fetch request, otherwise wait up to for more data to accumulate. Default: 1.\n• None fetch_max_bytes (int) – The maximum amount of data the server should return for a fetch request. This is not an absolute maximum, if the first message in the first non-empty partition of the fetch is larger than this value, the message will still be returned to ensure that the consumer can make progress. NOTE: consumer performs fetches to multiple brokers in parallel so memory usage will depend on the number of brokers containing partitions for the topic. Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).\n• None fetch_max_wait_ms (int) – The maximum amount of time in milliseconds the server will block before answering the fetch request if there isn’t sufficient data to immediately satisfy the requirement given by fetch_min_bytes. Default: 500.\n• None max_partition_fetch_bytes (int) – The maximum amount of data per-partition the server will return. The maximum total memory used for a request . This size must be at least as large as the maximum message size the server allows or else it is possible for the producer to send messages larger than the consumer can fetch. If that happens, the consumer can get stuck trying to fetch a large message on a certain partition. Default: 1048576.\n• None max_poll_records (int) – The maximum number of records returned in a single call to . Defaults , no limit.\n• None retry_backoff_ms (int) – Milliseconds to backoff when retrying on errors. Default: 100.\n• None auto_offset_reset (str) – A policy for resetting offsets on errors: will move to the oldest available message, will move to the most recent, and will raise an exception so you can handle this case. Default: .\n• None enable_auto_commit (bool) – If true the consumer’s offset will be periodically committed in the background. Default: True.\n• None auto_commit_interval_ms (int) – milliseconds between automatic offset commits, if enable_auto_commit is True. Default: 5000.\n• None check_crcs (bool) – Automatically check the CRC32 of the records consumed. This ensures no on-the-wire or on-disk corruption to the messages occurred. This check adds some overhead, so it may be disabled in cases seeking extreme performance. Default: True\n• None metadata_max_age_ms (int) – The period of time in milliseconds after which we force a refresh of metadata even if we haven’t seen any partition leadership changes to proactively discover any new brokers or partitions. Default: 300000\n• None partition_assignment_strategy (list) – List of objects to use to distribute partition ownership amongst consumer instances when group management is used. This preference is implicit in the order of the strategies in the list. When assignment strategy changes: to support a change to the assignment strategy, new versions must enable support both for the old assignment strategy and the new one. The coordinator will choose the old assignment strategy until all members have been updated. Then it will choose the new strategy. Default: [ ]\n• None max_poll_interval_ms (int) – Maximum allowed time between calls to consume messages (e.g., ). If this interval is exceeded the consumer is considered failed and the group will rebalance in order to reassign the partitions to another consumer group member. If API methods block waiting for messages, that time does not count against this timeout. See KIP-62 for more information. Default 300000\n• None rebalance_timeout_ms (int) – The maximum time server will wait for this consumer to rejoin the group in a case of rebalance. In Java client this behaviour is bound to configuration, but as will rejoin the group in the background, we decouple this setting to allow finer tuning by users that use to delay rebalacing. Defaults to\n• None session_timeout_ms (int) – Client group session and failure detection timeout. The consumer sends periodic heartbeats ( ) to indicate its liveness to the broker. If no hearts are received by the broker for a group member within the session timeout, the broker will remove the consumer from the group and trigger a rebalance. The allowed range is configured with the broker configuration properties and . Default: 10000\n• None heartbeat_interval_ms (int) – The expected time in milliseconds between heartbeats to the consumer coordinator when using Kafka’s group management feature. Heartbeats are used to ensure that the consumer’s session stays active and to facilitate rebalancing when new consumers join or leave the group. The value must be set lower than , but typically should be set no higher than 1/3 of that value. It can be adjusted even lower to control the expected time for normal rebalances. Default: 3000\n• None consumer_timeout_ms (int) – maximum wait timeout for background fetching routine. Mostly defines how fast the system will see rebalance and request new data for new partitions. Default: 200\n• None api_version (str) – specify which kafka API version to use. supports Kafka API versions >=0.9 only. If set to , will attempt to infer the broker version by probing various APIs. Default:\n• None security_protocol (str) – Protocol used to communicate with brokers. Valid values are: , , , . Default: .\n• None ssl_context (ssl.SSLContext) – pre-configured for wrapping socket connections. Directly passed into asyncio’s . For more information see SSL Authentication. Default: None.\n• None exclude_internal_topics (bool) – Whether records from internal topics (such as offsets) should be exposed to the consumer. If set to True the only way to receive records from an internal topic is subscribing to it. Requires 0.10+ Default: True\n• None connections_max_idle_ms (int) – Close idle connections after the number of milliseconds specified by this config. Specifying will disable idle checks. Default: 540000 (9 minutes).\n• If set to , will only return transactional messages which have been committed. If set to (the default), will return all messages, even transactional messages which have been aborted. Non-transactional messages will be returned unconditionally in either mode. Messages will always be returned in offset order. Hence, in mode, will only return messages up to the last stable offset (LSO), which is the one less than the offset of the first open transaction. In particular any messages appearing after messages belonging to ongoing transactions will be withheld until the relevant transaction has been completed. As a result, consumers will not be able to read up to the high watermark when there are in flight transactions. Further, when in the seek_to_end method will return the LSO. See method docs below. Default:\n• None sasl_mechanism (str) – Authentication mechanism when security_protocol is configured for or . Valid values are: , , , , . Default: Many configuration parameters are taken from Java Client: https://kafka.apache.org/documentation.html#newconsumerconfigs Manually assign a list of to this consumer. This interface does not support incremental assignment and will replace the previous assignment (if there was one). IllegalStateError – if consumer has already called It is not possible to use both manual partition assignment with and group assignment with . Manual topic assignment through this method does not use the consumer’s group management functionality. As such, there will be no rebalance operation triggered when group membership or cluster and topic metadata change. Get the set of partitions currently assigned to this consumer. If partitions were directly assigned using , then this will simply return the same partitions that were previously assigned. If topics were subscribed using , then this will give the set of topic partitions currently assigned to the consumer (which may be empty if the assignment hasn’t happened yet or if the partitions are in the process of being reassigned). the set of partitions currently assigned to this consumer Get the first offset for the given partitions. This method does not change the current consumer position of the partitions. This method may block indefinitely if the partition does not exist. partitions (list[TopicPartition]) – List of instances to fetch offsets for. mapping of partition to earliest available offset.\n• None UnsupportedVersionError – If the broker does not support looking up the offsets by timestamp. This commits offsets only to Kafka. The offsets committed using this API will be used on the first fetch after every rebalance and also on startup. As such, if you need to store offsets in anything other than Kafka, this API should not be used. Currently only supports kafka-topic offset storage (not Zookeeper) When explicitly passing use either offset of next record, or tuple of offset and metadata: If you want fire and forget commit, like in kafka-python, just run it in a task. Something like: offsets (dict, Optional) – A mapping from to to commit with the configured . Defaults to current consumed offsets for all subscribed partitions.\n• None CommitFailedError – If membership already changed on broker.\n• None IllegalOperation – If used with .\n• None KafkaError – If commit failed on broker side. This could be due to invalid offset, too long metadata, authorization failure, etc.\n• None ValueError – If offsets is of wrong format. Changed in version 0.4.0: Changed to in case of unassigned partition. Changed in version 0.4.0: Will now raise in case membership changed, as (possibly) this partition is handled by another consumer. Get the last committed offset for the given partition. (whether the commit happened by this process or another). This offset will be used as the position for the consumer in the event of a failure. This call will block to do a remote call to get the latest offset, as those are not cached by consumer (Transactional Producer can change them without Consumer knowledge as of Kafka 0.11.0) The last committed offset, or None if there was no prior commit. IllegalOperation – If used with Get the last offset for the given partitions. The last offset of a partition is the offset of the upcoming message, i.e. the offset of the last available message + 1. This method does not change the current consumer position of the partitions. This method may block indefinitely if the partition does not exist. partitions (list[TopicPartition]) – List of instances to fetch offsets for. mapping of partition to last available offset + 1.\n• None UnsupportedVersionError – If the broker does not support looking up the offsets by timestamp. Prefetched messages are returned in batches by topic-partition. If messages is not available in the prefetched buffer this method waits milliseconds.\n• None partitions (list[TopicPartition]) – The partitions that need fetching message. If no one partition specified then all subscribed partitions will be used\n• None timeout_ms (int, Optional) – milliseconds spent waiting if data is not available in the buffer. If 0, returns immediately with any records that are available currently in the buffer, else returns empty. Must not be negative. Default: 0 topic to list of records since the last fetch for the subscribed list of topics and partitions Get one message from Kafka. If no new messages prefetched, this method will wait for it. partitions (list(TopicPartition)) – Optional list of partitions to return from. If no partitions specified then returned message will be from any partition, which consumer is subscribed to. Last known highwater offset for a partition. A highwater offset is the offset that will be assigned to the next message that is produced. It may be useful for calculating lag, by comparing with the reported position. Note that both position and highwater refer to the next offset - i.e., highwater offset is one greater than the newest available message. Highwater offsets are returned as part of , so will not be available if messages for this partition were not requested yet. Returns the timestamp of the last poll of this partition (in ms). It is the last time and were updated. However it does not mean that new messages were received. As with will not be available until some messages are consumed. Returns the Last Stable Offset of a topic. It will be the last offset up to which point all transactions were completed. Only available in with isolation_level , in will always return -1. Will return None for older Brokers. As with will not be available until some messages are consumed. Look up the offsets for the given partitions by timestamp. The returned offset for each partition is the earliest offset whose timestamp is greater than or equal to the given timestamp in the corresponding partition. The consumer does not have to be assigned the partitions. If the message format version in a partition is before 0.10.0, i.e. the messages do not have timestamps, will be returned for that partition. This method may block indefinitely if the partition does not exist. timestamps (dict(TopicPartition, int)) – mapping from partition to the timestamp to look up. Unit should be milliseconds since beginning of the epoch (midnight Jan 1, 1970 (UTC)) mapping from partition to the timestamp and offset of the first message with timestamp greater than or equal to the target timestamp.\n• None ValueError – If the target timestamp is negative\n• None UnsupportedVersionError – If the broker does not support looking up the offsets by timestamp. Get metadata about the partitions for a given topic. This method will return if Consumer does not already have metadata for this topic. Future calls to will not return any records from these partitions until they have been resumed using . Note: This method does not affect partition subscription. In particular, it does not cause a group rebalance when automatic assignment is used. Get the partitions that were previously paused using . Get the offset of the next record that will be fetched (if a record with that offset exists on broker). Changed in version 0.4.0: Changed to in case of unassigned partition Resume fetching from the specified (paused) partitions. Manually specify the fetch offset for a . Overrides the fetch offsets that the consumer will use on the next / call. If this API is invoked for the same partition more than once, the latest offset will be used on the next fetch. You may lose data if this API is arbitrarily used in the middle of consumption to reset the fetch offsets. Use it either on rebalance listeners or after all pending messages are processed.\n• None ValueError – if offset is not a positive integer\n• None IllegalStateError – partition is not currently assigned Changed in version 0.4.0: Changed to and in respective cases. Seek to the oldest available offset for partitions. *partitions – Optionally provide specific , otherwise default to all assigned partitions.\n• None IllegalStateError – If any partition is not currently assigned\n• None TypeError – If partitions are not instances of Seek to the committed offset for partitions. *partitions – Optionally provide specific , otherwise default to all assigned partitions. mapping of the currently committed offsets.\n• None IllegalStateError – If any partition is not currently assigned\n• None IllegalOperation – If used with Changed in version 0.3.0: Changed to in case of unassigned partition Seek to the most recent available offset for partitions. *partitions – Optionally provide specific , otherwise default to all assigned partitions.\n• None IllegalStateError – If any partition is not currently assigned\n• None TypeError – If partitions are not instances of Connect to Kafka cluster. This will:\n• None Load metadata for all cluster nodes and partition allocation Close the consumer, while waiting for finalizers: Subscribe to a list of topics, or a topic regex pattern. Partitions will be dynamically assigned via a group coordinator. Topic subscriptions are not incremental: this list will replace the current assignment (if there is one). This method is incompatible with .\n• None pattern (str) – Pattern to match available topics. You must provide either topics or pattern, but not both.\n• None Optionally include listener callback, which will be called before and after each rebalance operation. As part of group management, the consumer will keep track of the list of consumers that belong to a particular group and will trigger a rebalance operation if one of the following events trigger:\n• None Number of partitions change for any of the subscribed topics\n• None An existing member of the consumer group dies\n• None A new member is added to the consumer group When any of these events are triggered, the provided listener will be invoked first to indicate that the consumer’s assignment has been revoked, and then again when the new assignment has been received. Note that this listener will immediately override any listener set in a previous call to subscribe. It is guaranteed, however, that the partitions revoked/assigned through this interface are from topics subscribed in this call.\n• None ValueError – if neither topics or pattern is provided or both are provided\n• None TypeError – if listener is not a Get all topics the user is authorized to view. Unsubscribe from all topics and clear all assigned partitions.\n\nA Token Provider must be used for the SASL OAuthBearer protocol. The implementation should ensure token reuse so that multiple calls at connect time do not create multiple tokens. The implementation should also periodically refresh the token in order to guarantee that each call returns an unexpired token. A timeout error should be returned after a short period of inactivity so that the broker can log debugging info and retry. An async callback returning a ID/Access Token to be sent to the Kafka client. In case where a synchronous callback is needed, implementations like following can be used: This is an OPTIONAL method that may be implemented. Returns a map of key-value pairs that can be sent with the SASL/OAUTHBEARER initial client request. If not implemented, the values are ignored This feature is only available in Kafka >= 2.1.0. A callback interface that the user can implement to trigger custom actions when the set of partitions assigned to the consumer changes. This is applicable when the consumer is having Kafka auto-manage group membership. If the consumer’s directly assign partitions, those partitions will never be reassigned and this callback is not applicable. When Kafka is managing the group membership, a partition re-assignment will be triggered any time the members of the group changes or the subscription of the members changes. This can occur when processes die, new process instances are added or old instances come back to life after failure. Rebalances can also be triggered by changes affecting the subscribed topics (e.g. when then number of partitions is administratively adjusted). There are many uses for this functionality. One common use is saving offsets in a custom store. By saving offsets in the , call we can ensure that any time partition assignment changes the offset gets saved. Another use is flushing out any kind of cache of intermediate results the consumer may be keeping. For example, consider a case where the consumer is subscribed to a topic containing user page views, and the goal is to count the number of page views per users for each five minute window. Let’s say the topic is partitioned by the user id so that all events for a particular user will go to a single consumer instance. The consumer can keep in memory a running tally of actions per user and only flush these out to a remote data store when its cache gets too big. However if a partition is reassigned it may want to automatically trigger a flush of this cache, before the new owner takes over consumption. This callback will execute during the rebalance process, and Consumer will wait for callbacks to finish before proceeding with group join. It is guaranteed that all consumer processes will invoke prior to any process invoking . So if offsets or other state is saved in the call, it should be saved by the time the process taking over that partition has their callback called to load the state. A coroutine or function the user can implement to provide cleanup or custom state save on the start of a rebalance operation. This method will be called before a rebalance operation starts and after the consumer stops fetching data. If you are using manual commit you have to commit all consumed offsets here, to avoid duplicate message delivery after rebalance is finished. This method is only called before rebalances. It is not called prior to revoked (list(TopicPartition)) – the partitions that were assigned to the consumer on the last rebalance A coroutine or function the user can implement to provide load of custom consumer state or cache warmup on completion of a successful partition re-assignment. This method will be called after partition re-assignment completes and before the consumer starts fetching data again. It is guaranteed that all the processes in a consumer group will execute their callback before any instance executes its callback. assigned (list(TopicPartition)) – the partitions assigned to the consumer (may include partitions that were previously assigned)\n\nSecurity is not an easy thing, at least when you want to do it right. Before diving in on how to setup to work with SSL, make sure there is a need for SSL Authentication and go through the official documentation for SSL support in Kafka itself. provides only as a parameter for Consumer and Producer classes. This is done intentionally, as it is recommended that you read through the Python ssl documentation to have some understanding on the topic. Although if you know what you are doing, there is a simple helper function , that will create an based on similar params to kafka-python. A few notes on Kafka’s SSL store types. Java uses JKS store type, that contains normal certificates, same as ones OpenSSL (and Python, as it’s based on OpenSSL) uses, but encodes them into a single, encrypted file, protected by another password. Just look the internet on how to extract , and from JKS store. See also the Using SSL with aiokafka example.\n\nGroupCoordinator implements group management for single group member by interacting with a designated Kafka broker (the coordinator). Group semantics are provided by extending this class. From a high level, Kafka’s group management protocol consists of the following sequence of actions:\n• None Group Registration: Group members register with the coordinator providing their own metadata (such as the set of topics they are interested in).\n• None Group/Leader Selection: The coordinator (one of Kafka nodes) select the members of the group and chooses one member (one of client’s) as the leader.\n• None State Assignment: The leader receives metadata for all members and assigns partitions to them.\n• None Group Stabilization: Each member receives the state assigned by the leader and begins processing. Between each phase coordinator awaits all clients to respond. If some do not respond in time - it will revoke their membership NOTE: Try to maintain same log messages and behaviour as Java and The roundrobin assignor lays out all the available partitions and all the available consumers. It then proceeds to do a roundrobin assignment from partition to consumer. If the subscriptions of all consumer instances are identical, then the partitions will be uniformly distributed. (i.e., the partition ownership counts will be within a delta of exactly one across all consumers.) For example, suppose there are two consumers C0 and C1, two topics t0 and t1, and each topic has 3 partitions, resulting in partitions t0p0, t0p1, t0p2, t1p0, t1p1, and t1p2. The assignment will be: When subscriptions differ across consumer instances, the assignment process still considers each consumer instance in round robin fashion but skips over an instance if it is not subscribed to the topic. Unlike the case when subscriptions are identical, this can result in imbalanced assignments. For example, suppose we have three consumers C0, C1, C2, and three topics t0, t1, t2, with unbalanced partitions t0p0, t1p0, t1p1, t2p0, t2p1, t2p2, where C0 is subscribed to t0; C1 is subscribed to t0, t1; and C2 is subscribed to t0, t1, t2. The assignment will be: Raised on methods of Consumer if it’s cancelled, even pending ones. Raised if you try to execute an operation, that is not available with current configuration. For example trying to commit if no group_id was given. Another producer with the same transactional ID went online. NOTE: As it seems this will be raised by Broker if transaction timeout occurred also. The unique offset of the message in this partition. See Offsets and Consumer Position for more details on offsets. Timestamp in millis, None for older Brokers The timestamp type of this record. If the broker respected the timestamp passed to , will be returned ( ). If the broker set it’s own timestamp, will be returned ( ). The key (or if no key is specified) The position of this record in the corresponding Kafka partition. The partition from which this record is received The size of the serialized, uncompressed key in bytes. The size of the serialized, uncompressed value in bytes. The timestamp of this record The timestamp type of this record The topic this record is received from"
    },
    {
        "link": "https://github.com/aio-libs/aiokafka/blob/master/aiokafka/consumer/consumer.py",
        "document": ""
    },
    {
        "link": "https://pypi.org/project/aiokafka",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://stackoverflow.com/questions/73556681/python-kafka-getting-consumer-messages-in-an-async-manner",
        "document": "If you must use , rather than the async analogue ( ), then you could use a combination of asyncio's method and an asynchronous generator, as follows:\n\nWith the above, you will fetch messages, if any, from the Kafka brokers in a separate thread after waiting for 0.1 milliseconds. Hence, you will yield control to the event loop and allow other tasks to run in between polling."
    },
    {
        "link": "https://github.com/aio-libs/aiokafka",
        "document": "AIOKafkaConsumer is a high-level, asynchronous message consumer. It interacts with the assigned Kafka Group Coordinator node to allow multiple consumers to load balance consumption of topics (requires kafka >= 0.9.0.0).\n\nDocker is required to run tests. See https://docs.docker.com/engine/installation for installation notes. Also note, that lz4 compression libraries for python will require python-dev package, or python source header files for compilation on Linux. NOTE: You will also need a valid java installation. It's required for the utility, used to generate ssh keys for some tests.\n\nSetting up tests requirements (assuming you're within virtualenv on ubuntu 14.04+):\n\nTo run tests with a specific version of Kafka (default one is 2.8.1) use KAFKA_VERSION variable:"
    },
    {
        "link": "https://pypi.org/project/aiokafka/0.8.0",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://stackoverflow.com/questions/55495959/aiokafka-library-does-not-consume-messages-asynchronously",
        "document": "I'm trying to implement Python aiokafka async library and for some reason I can't process the messages asynchronously.\n\nI created async consumer, producer and use the asyncio python library."
    }
]