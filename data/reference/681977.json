[
    {
        "link": "https://docs.python.org/3/library/multiprocessing.html",
        "document": "is a package that supports spawning processes using an API similar to the module. The package offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Due to this, the module allows the programmer to fully leverage multiple processors on a given machine. It runs on both POSIX and Windows. The module also introduces APIs which do not have analogs in the module. A prime example of this is the object which offers a convenient means of parallelizing the execution of a function across multiple input values, distributing the input data across processes (data parallelism). The following example demonstrates the common practice of defining such functions in a module so that child processes can successfully import that module. This basic example of data parallelism using , offers a higher level interface to push tasks to a background process without blocking execution of the calling process. Compared to using the interface directly, the API more readily allows the submission of work to the underlying process pool to be separated from waiting for the results. In , processes are spawned by creating a object and then calling its method. follows the API of . A trivial example of a multiprocess program is To show the individual process IDs involved, here is an expanded example: For an explanation of why the part is necessary, see Programming guidelines. Depending on the platform, supports three ways to start a process. These start methods are The parent process starts a fresh Python interpreter process. The child process will only inherit those resources necessary to run the process object’s method. In particular, unnecessary file descriptors and handles from the parent process will not be inherited. Starting a process using this method is rather slow compared to using fork or forkserver. Available on POSIX and Windows platforms. The default on Windows and macOS. The parent process uses to fork the Python interpreter. The child process, when it begins, is effectively identical to the parent process. All resources of the parent are inherited by the child process. Note that safely forking a multithreaded process is problematic. Available on POSIX systems. Currently the default on POSIX except macOS. The default start method will change away from fork in Python 3.14. Code that requires fork should explicitly specify that via or . Changed in version 3.12: If Python is able to detect that your process has multiple threads, the function that this start method calls internally will raise a . Use a different start method. See the documentation for further explanation. When the program starts and selects the forkserver start method, a server process is spawned. From then on, whenever a new process is needed, the parent process connects to the server and requests that it fork a new process. The fork server process is single threaded unless system libraries or preloaded imports spawn threads as a side-effect so it is generally safe for it to use . No unnecessary resources are inherited. Available on POSIX platforms which support passing file descriptors over Unix pipes such as Linux. Changed in version 3.4: spawn added on all POSIX platforms, and forkserver added for some POSIX platforms. Child processes no longer inherit all of the parents inheritable handles on Windows. Changed in version 3.8: On macOS, the spawn start method is now the default. The fork start method should be considered unsafe as it can lead to crashes of the subprocess as macOS system libraries may start threads. See bpo-33725. On POSIX using the spawn or forkserver start methods will also start a resource tracker process which tracks the unlinked named system resources (such as named semaphores or objects) created by processes of the program. When all processes have exited the resource tracker unlinks any remaining tracked object. Usually there should be none, but if a process was killed by a signal there may be some “leaked” resources. (Neither leaked semaphores nor shared memory segments will be automatically unlinked until the next reboot. This is problematic for both objects because the system allows only a limited number of named semaphores, and shared memory segments occupy some space in the main memory.) To select a start method you use the in the clause of the main module. For example: should not be used more than once in the program. Alternatively, you can use to obtain a context object. Context objects have the same API as the multiprocessing module, and allow one to use multiple start methods in the same program. Note that objects related to one context may not be compatible with processes for a different context. In particular, locks created using the fork context cannot be passed to processes started using the spawn or forkserver start methods. A library which wants to use a particular start method should probably use to avoid interfering with the choice of the library user. The and start methods generally cannot be used with “frozen” executables (i.e., binaries produced by packages like PyInstaller and cx_Freeze) on POSIX systems. The start method may work if code does not use threads. supports two types of communication channel between processes: The class is a near clone of . For example: Queues are thread and process safe. Any object put into a queue will be serialized. The function returns a pair of connection objects connected by a pipe which by default is duplex (two-way). For example: The two connection objects returned by represent the two ends of the pipe. Each connection object has and methods (among others). Note that data in a pipe may become corrupted if two processes (or threads) try to read from or write to the same end of the pipe at the same time. Of course there is no risk of corruption from processes using different ends of the pipe at the same time. The method serializes the object and re-creates the object. contains equivalents of all the synchronization primitives from . For instance one can use a lock to ensure that only one process prints to standard output at a time: Without using the lock output from the different processes is liable to get all mixed up. As mentioned above, when doing concurrent programming it is usually best to avoid using shared state as far as possible. This is particularly true when using multiple processes. However, if you really do need to use some shared data then provides a couple of ways of doing so. Data can be stored in a shared memory map using or . For example, the following code The and arguments used when creating and are typecodes of the kind used by the module: indicates a double precision float and indicates a signed integer. These shared objects will be process and thread-safe. For more flexibility in using shared memory one can use the module which supports the creation of arbitrary ctypes objects allocated from shared memory. A manager object returned by controls a server process which holds Python objects and allows other processes to manipulate them using proxies. A manager returned by will support types , , , , , , , , , , , and . For example, Server process managers are more flexible than using shared memory objects because they can be made to support arbitrary object types. Also, a single manager can be shared by processes on different computers over a network. They are, however, slower than using shared memory. The class represents a pool of worker processes. It has methods which allows tasks to be offloaded to the worker processes in a few different ways. # runs in *only* one process # runs in *only* one process # prints the PID of that process # launching multiple evaluations asynchronously *may* use more processes \"We lacked patience and got a multiprocessing.TimeoutError\" \"For the moment, the pool remains available for more work\" # exiting the 'with'-block has stopped the pool \"Now the pool is closed and no longer available\" Note that the methods of a pool should only ever be used by the process which created it. Functionality within this package requires that the module be importable by the children. This is covered in Programming guidelines however it is worth pointing out here. This means that some examples, such as the examples will not work in the interactive interpreter. For example: : Can't get attribute 'f' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)> AttributeError: Can't get attribute 'f' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)> AttributeError: Can't get attribute 'f' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n\nThere are certain guidelines and idioms which should be adhered to when using . The following applies to all start methods. As far as possible one should try to avoid shifting large amounts of data between processes. It is probably best to stick to using queues or pipes for communication between processes rather than using the lower level synchronization primitives. Ensure that the arguments to the methods of proxies are picklable. Do not use a proxy object from more than one thread unless you protect it with a lock. On POSIX when a process finishes but has not been joined it becomes a zombie. There should never be very many because each time a new process starts (or is called) all completed processes which have not yet been joined will be joined. Also calling a finished process’s will join the process. Even so it is probably good practice to explicitly join all the processes that you start. Better to inherit than pickle/unpickle When using the spawn or forkserver start methods many types from need to be picklable so that child processes can use them. However, one should generally avoid sending shared objects to other processes using pipes or queues. Instead you should arrange the program so that a process which needs access to a shared resource created elsewhere can inherit it from an ancestor process. Using the method to stop a process is liable to cause any shared resources (such as locks, semaphores, pipes and queues) currently being used by the process to become broken or unavailable to other processes. Therefore it is probably best to only consider using on processes which never use any shared resources. Bear in mind that a process that has put items in a queue will wait before terminating until all the buffered items are fed by the “feeder” thread to the underlying pipe. (The child process can call the method of the queue to avoid this behaviour.) This means that whenever you use a queue you need to make sure that all items which have been put on the queue will eventually be removed before the process is joined. Otherwise you cannot be sure that processes which have put items on the queue will terminate. Remember also that non-daemonic processes will be joined automatically. An example which will deadlock is the following: A fix here would be to swap the last two lines (or simply remove the line). On POSIX using the fork start method, a child process can make use of a shared resource created in a parent process using a global resource. However, it is better to pass the object as an argument to the constructor for the child process. Apart from making the code (potentially) compatible with Windows and the other start methods this also ensures that as long as the child process is still alive the object will not be garbage collected in the parent process. This might be important if some resource is freed when the object is garbage collected in the parent process. Beware of replacing with a “file like object” in the method — this resulted in issues with processes-in-processes. This has been changed to: Which solves the fundamental issue of processes colliding with each other resulting in a bad file descriptor error, but introduces a potential danger to applications which replace with a “file-like object” with output buffering. This danger is that if multiple processes call on this file-like object, it could result in the same data being flushed to the object multiple times, resulting in corruption. If you write a file-like object and implement your own caching, you can make it fork-safe by storing the pid whenever you append to the cache, and discarding the cache when the pid changes. For example: For more information, see bpo-5155, bpo-5313 and bpo-5331 There are a few extra restrictions which don’t apply to the fork start method. Ensure that all arguments to are picklable. Also, if you subclass then make sure that instances will be picklable when the method is called. Bear in mind that if code run in a child process tries to access a global variable, then the value it sees (if any) may not be the same as the value in the parent process at the time that was called. However, global variables which are just module level constants cause no problems. Make sure that the main module can be safely imported by a new Python interpreter without causing unintended side effects (such as starting a new process). For example, using the spawn or forkserver start method running the following module would fail with a : Instead one should protect the “entry point” of the program by using as follows: This allows the newly spawned Python interpreter to safely import the module and then run the module’s function. Similar restrictions apply if a pool or manager is created in the main module."
    },
    {
        "link": "https://medium.com/@AlexanderObregon/understanding-pythons-multiprocessing-module-744dba8d4be4",
        "document": "Python’s multiprocessing module allows you to create processes, manage concurrent tasks, and facilitate inter-process communication. It’s an important tool for taking advantage of multiple CPU cores and improving the performance of your Python programs. In this article, we’ll take a look at the workings of the multiprocessing module, exploring how to create processes, manage inter-process communication, and handle concurrent tasks, with a heavy focus on practical code examples to illustrate each concept.\n\nIt’s important to understand before going further, the difference between concurrency and parallelism. Concurrency involves dealing with multiple tasks at the same time, but not necessarily executing them simultaneously. It focuses on managing the tasks efficiently. Parallelism, on the other hand, refers to executing multiple tasks simultaneously, which is possible on multi-core processors. The multiprocessing module primarily enables parallelism, allowing you to leverage multiple CPU cores to run tasks concurrently.\n\nCreating and managing processes is a fundamental feature of the multiprocessing module. It allows you to run multiple instances of a function or task in separate processes, thus enabling parallel execution. This section will cover the basics of process creation, managing multiple processes, and handling process synchronization.\n\nCreating a process in Python using the multiprocessing module is straightforward. The class is used to instantiate a new process. You define a target function that the process will execute and start the process using the method. Here’s an example:\n\nIn this example, we define a function that simulates some work by sleeping for two seconds. We then create a object, passing the function as the target. The method begins the execution of the worker process. The method makes sure that the main process waits for the worker process to complete before continuing.\n\nOften, you need to create and manage multiple processes to perform concurrent tasks. You can achieve this by instantiating several objects and starting them sequentially. Here’s how you can do it:\n\nIn this example, we create and start five worker processes, each with a unique number. The parameter is used to pass arguments to the target function. We use a loop to create and start each process and then join all processes to make sure the main process waits for all of them to complete.\n\nDaemon processes are background processes that run alongside the main program and terminate automatically when the main program exits. They are useful for tasks that should not block the main program from exiting. You can create a daemon process by setting the attribute to :\n\nIn this example, the function runs indefinitely, printing a message every second. The main process sleeps for five seconds and then exits, causing the daemon process to terminate automatically.\n\nA practical, real world use case for daemon processes is a background process monitoring system resources while your main program performs other tasks. This makes sure that resource monitoring continues as long as the main program runs, but terminates automatically when the main program exits.\n\nWhen multiple processes need to share resources or coordinate their actions, synchronization mechanisms are essential. The multiprocessing module provides several synchronization primitives, such as , , , and .\n\nExample: Using Lock for Synchronization\n\nA makes sure that only one process can access a shared resource at a time. This is useful for preventing race conditions:\n\nIn this example, each worker process acquires a lock before printing its messages and releases it after completing its task. The statement make sure that the lock is properly acquired and released, preventing race conditions.\n\nExample: Using Event for Synchronization\n\nAn is a simple synchronization primitive that can be set or cleared to control the execution of processes:\n\nIn this example, each worker process waits for the event to be set before proceeding. The main process sleeps for three seconds and then triggers the event, allowing all waiting processes to continue.\n\nInter-process communication (IPC) is crucial for enabling processes to exchange data and synchronize their actions. The multiprocessing module provides several mechanisms for IPC, including , , , and . Here we will cover the basics of these IPC mechanisms and provide examples of their usage.\n\nA is a thread- and process-safe FIFO data structure that allows multiple producers and consumers. It's one of the simplest ways to pass messages between processes.\n\nIn this example, the worker process sends a message to the main process using a . The main process retrieves the message using the method.\n\nThis example demonstrates using a with multiple worker processes. Each worker sends a unique message to the queue, which the main process retrieves and prints after all workers have completed.\n\nA provides a two-way communication channel between two processes. It is simpler and more direct than a but is limited to two processes.\n\nIn this example, the worker process sends a message through the pipe, and the main process receives it using the method.\n\nHere, the communication is bidirectional. The parent process sends a message to the worker, which the worker receives and acknowledges with a reply. The parent then receives and prints the worker’s response.\n\nUsing Value and Array for Shared Memory\n\nShared memory allows multiple processes to share data without copying. The and classes provide ways to share data between processes.\n\nExample: Using Value for Shared Memory\n\nIn this example, a shared integer value is incremented by each worker process. The class is used to create a shared integer initialized to 0. Each worker increments this value by 1, and the final value is printed after all processes have completed.\n\nExample: Using Array for Shared Memory\n\nHere, a shared array is created and manipulated by multiple worker processes. Each worker squares the element at its designated index. The final array is printed after all processes have completed.\n\nCaution note: When multiple processes modify shared data concurrently, there is a potential for race conditions. To prevent these issues, use synchronization mechanisms like to ensure data integrity.\n\nThe class provides a way to create shared objects that can be used by multiple processes. It supports a variety of shared data types, including lists, dictionaries, and more.\n\nExample: Using Manager for Shared List\n\nIn this example, a is used to create a shared list. Each worker appends a unique number to the list. The final list is printed after all processes have completed.\n\nManaging concurrent tasks efficiently is a key aspect of leveraging Python’s multiprocessing module. It provides the class to facilitate the parallel execution of a function across multiple input values, thus enabling you to distribute the workload across multiple processes. Pool is particularly well-suited for CPU-bound tasks, where parallel execution can significantly improve performance. This section covers the basics of using the class, including synchronous and asynchronous task execution, and demonstrates how to handle large datasets efficiently.\n\nhe class allows you to create a pool of worker processes and distribute tasks among them. The simplest method is , which applies a function to every item of a given iterable and waits for all tasks to complete before returning the results.\n\nExample: Using Pool.map for Parallel Execution\n\nIn this example, a with four worker processes is created. The method applies the function to each value in the range from 0 to 9 in parallel. The method waits for all tasks to complete before returning the results, which are then printed.\n\nExample: Using Pool.starmap for Multiple Arguments\n\nThe method is similar to , but it supports functions that take multiple arguments. The order of results in matches the order of the input tuples.\n\nHere, is used to apply the function, which multiplies two numbers, to each tuple of arguments. The results are collected and printed after all tasks are completed, and they match the order of the input tuples.\n\nThe multiprocessing module also supports asynchronous execution using the method. This method allows you to submit a function to the pool and retrieve the result at a later time.\n\nExample: Using Pool.apply_async for Asynchronous Execution\n\nIn this example, is used to submit the function for each value in the range from 0 to 9. Each call to returns a result object, which can be used to retrieve the result once the task is complete. The results are collected and printed after all tasks are completed.\n\nYou can also handle callbacks using to process the results as soon as they are available. Callbacks can be particularly useful for processing results as they become available, especially in long-running tasks.\n\nHere, a function is defined to handle the results. The parameter of is used to specify this function, which is called with the result of each task as soon as it is available. The pool is then closed and joined to make sure all tasks are completed before printing the results.\n\nWhen dealing with large datasets, it is important to manage memory efficiently and avoid overloading the system. Excessive memory usage can lead to performance issues or crashes. The and methods provide a way to process large datasets in chunks, controlling the number of tasks sent to each worker process at a time.\n\nExample: Using Pool.imap for Efficient Processing\n\nIn this example, is used to apply the function to each value in the range from 0 to 999,999. The parameter specifies the size of the chunks the data is divided into for processing, which helps manage memory usage efficiently.\n\nExample: Using Pool.imap_unordered for Unordered Results\n\nThe method is similar to , but it returns results as soon as they are available, without preserving the order of the input.\n\nIn this example, is used to process the data. The results are printed as soon as they are available, which can be faster than waiting for the ordered results, especially when processing times vary.\n\nProper management of the process pool is crucial for efficient and safe execution. Always make sure that the pool is properly closed and joined to release resources. It’s worth noting that prevents new tasks from being submitted, but the existing tasks in the pool will continue to execute.\n\nIn this example, the pool is closed after all tasks are submitted, preventing any new tasks from being added. The method make sure that the main process waits for all worker processes to complete before continuing. This proper management makes sure that resources are released correctly and prevents potential issues.\n\nNote: It’s worth mentioning that the method can also be used to abruptly terminate all worker processes. This is particularly useful for error handling or resource cleanup when you need to stop the worker processes immediately.\n\nPython’s multiprocessing module is a powerful tool for managing concurrent tasks and leveraging multiple CPU cores to improve performance. By understanding process creation, inter-process communication, and efficient task management, you can write more efficient and scalable Python programs. Whether you are dealing with CPU-bound tasks or handling large datasets, the multiprocessing module provides the necessary tools to maximize your program’s performance and resource utilization.\n\nIt’s also worth clarifying that there are other ways to achieve concurrency in Python, such as threading and asyncio. Threading is suitable for I/O-bound tasks where tasks spend a lot of time waiting for external events, while asyncio is ideal for asynchronous I/O operations. Multiprocessing, however, is preferred for CPU-bound tasks where you need to execute multiple operations simultaneously to fully utilize the CPU cores.\n\nThank you for reading! If you find this article helpful, please consider highlighting, clapping, responding or connecting with me on Twitter/X as it’s very appreciated and helps keeps content like this free!"
    },
    {
        "link": "https://mimo.org/glossary/python/multiprocessing",
        "document": "Python multiprocessing allows you to run multiple processes in parallel, leveraging multiple CPU cores for improved performance. Unlike multithreading, which is limited by Python’s Global Interpreter Lock (GIL), multiprocessing lets you execute CPU-bound tasks concurrently. The multiprocessing module provides tools to create, manage, and synchronize processes, making it ideal for heavy computations, parallel data processing, and task automation.\n\nHow to Use Python Multiprocessing\n\nThe multiprocessing module provides the class, which you can use to create and run independent processes. Here’s how you can create a new process:\n• : Creates a new process that runs the specified function.\n• : Ensures the main process waits for the child process to finish.\n\nWhen to Use Multiprocessing in Python\n\nMultiprocessing is ideal when your code needs to handle CPU-intensive tasks. Here are three common situations where multiprocessing improves performance:\n\nWhen processing large datasets, using multiple CPU cores can significantly speed up execution.\n\nWeb scraping involves sending multiple requests, which can be parallelized to speed up data retrieval.\n\nIf you need to process large files, multiprocessing lets you split the work across multiple CPU cores.\n\nWhen multiple processes need to share data, you can use or from the multiprocessing module.\n\nIf you have many small tasks, a process pool distributes the workload efficiently.\n\nThreadPool is best for I/O-bound tasks like web scraping or file I/O, while multiprocessing excels in CPU-bound tasks like numerical computations or image processing. The choice depends on whether your bottleneck is CPU or waiting time.\n\nWhen making multiple HTTP requests, you can prevent overloading a server by implementing a rate limit.\n\nMultiprocessing allows processes to share memory using .\n\nIf you run into segmentation faults due to , use instead:\n\nWhen multiple processes modify shared resources, use to prevent conflicts.\n\nLogging across multiple processes can be tricky, but helps.\n• High Overhead: Creating and managing processes requires more memory than threads.\n• Not Always Faster: If a task involves a lot of data transfer between processes, the overhead can slow things down.\n• Not Suitable for Short Tasks: The time taken to start and terminate processes can outweigh the benefits.\n\nIf your program is mostly I/O-bound or requires frequent inter-process communication, consider using multithreading instead.\n\nPython multiprocessing is a powerful tool for optimizing performance when working with CPU-bound tasks. By leveraging multiple CPU cores, you can significantly speed up data processing, image transformations, and complex calculations.\n\nHowever, multiprocessing comes with trade-offs, including higher memory usage and potential synchronization challenges."
    },
    {
        "link": "https://marwahaha.github.io/2015-07-09-berkeley/intermediate-python/04-multiprocessing.html",
        "document": "In this lesson, you will learn how to write programs that perform several tasks in parallel using Python's built-in multiprocessing library. You are encouraged to consult the documentation to learn more, or to answer any detailed questions as we will only cover a small subset of the library's functionality. This lesson assumes you have completed the novice python lessons or have some familiarity with using functions, loops, conditionals, command line argument processing, NumPy, and matplotlib (though don't worry if you don't know NumPy and matplotlib well). Parallel programming has been important to scientific computing for decades as a way to decrease program run times, making more complex analyses possible (e.g. climate modeling, gene sequencing, pharmaceutical development, aircraft design). One of the motivations for parallel programming has been the diminishing marginal increases in single CPU performance with each new generation of CPU (see The Free Lunch is over). In response, computer makers have introduced multi-core processors that contain more than one processing core. It's not uncommon for desktop, laptop, and even tablets and smart phones to have two or more CPU cores. In addition to multi-core CPUs, Graphics Processing Units (GPU) have become more powerful recently (often having hundreds of parallel execution units). GPUs are increasingly being use not just for drawing graphics to the screen, but for general purpose computation. GPUs can even be used in conjunction with CPUs to boost parallel computing performance (this is known as heterogeneous computing). GPUs are best suited to applying the same computation over arrays of data, while CPUs are better suited to algorithms that include conditional branches of execution (e.g. different paths through the code based on if statements). Emerging tools, such as OpenCL help coordinate parallel execution across heterogeneous computer platforms that contain differing CPU and GPU resources. Unfortunately, most computer programs cannot take advantage of performance increases offered by GPUs or multi-core CPUs unless we modify these programs. In this lesson we will develop an example program that uses the Python multiprocessing library to simultaneously execute tasks on a multi-core CPU, decreasing the overall program run time. Multi-processing is one way to execute tasks in parallel on a multi-core CPU, or across multiple computers in a computing cluster. In multi-processing, each task runs in its own process; each program running on your computer is represented by one or more processes. For example, if you are running a web browser, a text editor, and an e-mail client, you are running at least three processes (and likely many more background processes). On modern operating systems, each process gets its own portion of your computer's memory, ensuring that no process can interfere with the execution of another (though tools like MPI and even Python's multiprocessing library can be used to share data between processes running locally or in distributed computing environments). Multi-processing is not to be confused with multi-threading, or shared-memory parallelism. In modern operating systems, each process contains one or more threads of execution. These threads share the same portion of memory assigned to their parent process; each thread can run in parallel if the computer has more than one CPU core. For certain algorithms, multi-threading can be more efficient than multi-processing (though multi-processing solutions such as MPI often scale better to larger problem sizes). However, multi-threading is more error-prone to program and is generally only done directly by expert systems programmers. Tools such as OpenMP should in general be used for multi-threading in scientific computing. In our example application, we'll show how to parallelize the plotting of randomly generated data using multiple processors. You can find the complete solution here. Key portions of the code will be discussed below. Due to the design of the multiprocessing library, the code portions generally will not work in interactive interpreters such as IPython. Consult the documentation for details. To run the example application, download it here, and then, from your command line interface, type: for usage instructions. We suggest you create an output directory called \"temp\" to store the plots in (to make deletion easier). Defining the work to be done¶ Before running code in parallel, we need to define the work to be done. In multiprocessing, this work is defined as a callable object, usually a Python function. Here is the function we'll use:\n\nThis function takes as input the path of the output directory to which plots should be saved, and the unique identifier of the plot we are creating; the function returns the path of the PDF file to which the plotted data were written, as well as the plot identifier. The reason for returning the plot identifier will be described below. Don't worry if you are not familiar with NumPy or matplotlib. All we're doing here is making a scatter plot of random data. As is, this is not a very useful task, however it could easily be extended to plot actual data. **The key thing to note about this function is that when executing code in parallel environments, weird behavior can result unless special care is taken in your code** (even when doing multi-processing programming where each process has its own memory space). In this case, there are two special things we must do. First, we must force NumPy's random number generator to re-initialize for each call of the parallel function, this is accomplished by: Where we use the current system time and the plot identifier as a seed for the random number generator. If we don't do this, each worker process will use the same random number generator as it makes the plots it is responsible for. Thus, all plots made by Worker 1 would contain the same sequence of \"random\" data. Second, we need to tell matplotlib to clear the current figure context after each plot is generated:\n\nWe get the results for each task by calling the method on that task's result object. Calls to will wait until there are results available (this is called \"blocking\"). You can optionally set a timeout after which an exception will be raised if a result is not returned (this is useful if your task involves network activity). Remember that our task function returns a tuple of the plot identifier as well as the filename of the plot generated. You might be asking yourself why would we need to return the plot identifier when the plot identifier was passed into the worker function, and did not change in the worker function. This is necessary because the worker pool makes no guarantee that workers will complete their tasks in any particular order. Indeed, in a more realistic application, each tasks' workload may vary, so there is no way to know how long each task will take to complete. In our example we simply print out the plot identifier and the name of file produced. If the worker function were performing a numerical computation, we could instead do something with that result, such as adding the result to the results obtained from other tasks. Here is the output that our example program should produce: $ ./plot_rand_mp.py -o temp -n 4 Making 4 plots of random data using 8 processors... Making plot 1 Making plot 2 Making plot 3 Making plot 4 Result: plot 1 written to temp/plot_1.pdf Result: plot 2 written to temp/plot_2.pdf Result: plot 3 written to temp/plot_3.pdf Result: plot 4 written to temp/plot_4.pdf\n\nWhenever we decide to parallelize a task, it is important to evaluate the runtime savings and efficiency of our parallel program. Speedup and efficiency are common ways of doing this. Before decided parallelize or otherwise optimize a program, you should first use a profiler to identify what proportion of runtime your program is spending in each function or component. This will help you to prioritize optimization or parallelization to maximize runtime reductions. Speedup (Sp) is defined as the ratio of runtime for a sequential algorithm (T1) to runtime for a parallel algorithm with p processors (Tp). That is, Sp = T1 / Tp. Ideal speedup results when Sp = p. Speedup is formally derived from Amdahl's law, which considers the portion of a program that is serial vs. the portion that is parallel when calculating speedup. Efficiency (Ep) is defined as the ratio of speedup for p processors (Sp) to the number of processors p, or Ep = Sp / p. Below is a graph comparing the speedup and efficiency that resulted when running the example program to create 64 plots using a range of processors on computer with a 4-core processor (each data point represents the average of three runs). Note that the comparison here is not quite fair because a sequential version of the program was not written. The runtime for the sequential version were approximated by running the parallel version using a single processor. The single processor runtime was likely longer than that of a sequential version of the same program due to the overhead needed to create the worker pool and dispatch a single task to a worker. Thus these results slightly overstate the parallel speedup and efficiency. First note that the slope of the speedup curve is low, and grows farther away from the ideal speedup line as the number of processors increases. Some divergence between actual and ideal speedup is typical. However our example program isn't strictly computational and involves input/output (I/O) to the filesystem (i.e. writing the plot). I/O-bound tasks do not typically parallelize well because I/O resources (e.g. disks, network, memory) are shared across processors, and because I/O operations, especially to disk, usually take orders of magnitude more time to complete than computational operations. However, even I/O-bound tasks can see moderate speedup due to the effects of \"pipelining\" (see here). Efficiency, which ranges from 0 to 1, is a bit easier to interpret than speedup. With two processors, efficiency was over 90% (i.e. our two workers were 90% utilized, doing useful work far more than they were waiting for I/O). As processors were added to the worker pool, the efficiency dropped, which is expected for a task dependent on I/O. However, the speedup continued increasing up to and including eight processors; that is, runtimes continued to drop until we added more than eight workers. Thus, even though efficiency was decreasing, we were still able to save time by adding more workers, up to a point. Efficiency is especially important in enterprise computing environments, where concerns like providing equitable access shared resources and reducing energy consumption may dictate the use of fewer processors to maintain higher processor utilization. Despite our computer having only 4-cores, why did runtime continue to decrease between four and eight processors? This was in part because the computer we ran the tests on had a feature call \"HyperThreading.\" HyperThreading is the marketing name of a technology that enables each physical core of a CPU to appear as two virtual cores to your operating system. These virtual cores can improve performance, but are rarely as effective as additional physical cores. Running a computation in multiple processes requires some communication between these processes. One of the nice aspects of multiprocessing in Python is that most of the time you do not need to know how this communication is handled: it just works. However, it is useful to understand the basics of this mechanism in order to figure out how to solve two kinds of problems: unexpected errors, and bad performance. Communication between processes takes the form of streams of bytes that travel through specific communication channels. To send an object from one process to another, Python has to convert it to a stream of bytes, and assemble the object back at the receiving end. Python's mechanism for doing these conversions was originally designed for storing objects in files and is implemented in the pickle module. Every argument that is passed to a Python function running in another process is pickled and then unpickled. The result of the function undergoes the same process on its way back. There are two things you need to know about pickle in the context of multiprocessing. First, most objects can be pickled but some cannot. Second, pickling and unpickling take time and can sometimes add considerable overhead to your multiprocessing. The objects that cannot be pickled come in two varieties: those for which pickling does not make sense, and those for which it has simply not been implemented. A good example for the first category is file objects. The second category contains mainly object types defined in extension modules whose authors didn't implement pickling. If you use an old release of NumPy, you may discover that its array-aware functions are not picklable, making it impossible to use such a function directly as a task in multiprocessing. For Python's built-in objects, there is one important restriction that is due to the implementation details of pickle: functions and classes can only be pickled if they are defined at the top level of a module. This means, for example, that if you define a function inside another function, you cannot pickle it and thus not pass it to a multiprocessing task. The performance implications of pickling are rather obvious: you should try to pass as few arguments as possible to your tasks, and make sure you pass no more data than you really need to. For example, rather than passing a huge list and the index of the item that your taks is supposed to process, you should pass only that item. Run the example application on your computer several times. Each time, vary the number of processors to use and note how the computation efficiency varies. You can use the Unix program to measure execution times."
    },
    {
        "link": "https://realpython.com/python-concurrency",
        "document": "Concurrency refers to the ability of a program to manage multiple tasks at once, improving performance and responsiveness. It encompasses different models like threading, asynchronous tasks, and multiprocessing, each offering unique benefits and trade-offs. In Python, threads and asynchronous tasks facilitate concurrency on a single processor, while multiprocessing allows for true parallelism by utilizing multiple CPU cores.\n\nUnderstanding concurrency is crucial for optimizing programs, especially those that are I/O-bound or CPU-bound. Efficient concurrency management can significantly enhance a program’s performance by reducing wait times and better utilizing system resources.\n\nIn this tutorial, you’ll learn how to:\n• Understand the different forms of concurrency in Python\n• Choose the appropriate concurrency model based on your program’s needs\n\nTo get the most out of this tutorial, you should be familiar with Python basics, including functions and loops. A rudimentary understanding of system processes and CPU operations will also be helpful. You can download the sample code for this tutorial by clicking the link below:\n\nIn this section, you’ll get familiar with the terminology surrounding concurrency. You’ll also learn that concurrency can take different forms depending on the problem it aims to solve. Finally, you’ll discover how the different concurrency models translate to Python. The dictionary definition of concurrency is simultaneous occurrence. In Python, the things that are occurring simultaneously are called by different names, including these: At a high level, they all refer to a sequence of instructions that run in order. You can think of them as different trains of thought. Each one can be stopped at certain points, and the CPU or brain that’s processing them can switch to a different one. The state of each train of thought is saved so it can be restored right where it was interrupted. You might wonder why Python uses different words for the same concept. It turns out that threads, tasks, and processes are only the same if you view them from a high-level perspective. Once you start digging into the details, you’ll find that they all represent slightly different things. You’ll see more of how they’re different as you progress through the examples. Now, you’ll consider the simultaneous part of that definition. You have to be a little careful because, when you get down to the details, you’ll discover that only multiple system processes can enable Python to run these trains of thought at literally the same time. In contrast, threads and asynchronous tasks always run on a single processor, which means they can only run one at a time. They just cleverly find ways to take turns to speed up the overall process. Even though they don’t run different trains of thought simultaneously, they still fall under the concept of concurrency. Note: Threads in most other programming languages often run in parallel. To learn why Python threads can’t, check out What Is the Python Global Interpreter Lock (GIL)? If you’re curious about even more details, then you can also read about Bypassing the GIL for Parallel Processing in Python or check out the experimental free threading introduced in Python 3.13. The way the threads, tasks, or processes take turns differs. In a multi-threaded approach, the operating system actually knows about each thread and can interrupt it at any time to start running a different thread. This mechanism is also true for processes. It’s called preemptive multitasking since the operating system can preempt your thread or process to make the switch. Preemptive multitasking is handy in that the code in the thread doesn’t need to do anything special to make the switch. It can also be difficult because of that at any time phrase. The context switch can happen in the middle of a single Python statement, even a trivial one like . This is because Python statements typically consist of several low-level bytecode instructions. On the other hand, asynchronous tasks use cooperative multitasking. The tasks must cooperate with each other by announcing when they’re ready to be switched out without the operating system’s involvement. This means that the code in the task has to change slightly to make it happen. The benefit of doing this extra work upfront is that you always know where your task will be swapped out, making it easier to reason about the flow of execution. A task won’t be swapped out in the middle of a Python statement unless that statement is appropriately marked. You’ll see later how this can simplify parts of your design. So far, you’ve looked at concurrency that happens on a single processor. What about all of those CPU cores your cool, new laptop has? How can you make use of them in Python? The answer is to execute separate processes! A process can be thought of as almost a completely different program, though technically, it’s usually defined as a collection of resources including memory, file handles, and things like that. One way to think about it is that each process runs in its own Python interpreter. Because they’re different processes, each of your trains of thought in a program leveraging multiprocessing can run on a different CPU core. Running on a different core means that they can actually run at the same time, which is fabulous. There are some complications that arise from doing this, but Python does a pretty good job of smoothing them over most of the time. Now that you have an idea of what concurrency and parallelism are, you can review their differences and then determine which Python modules support them: The tasks decide when to give up control. The operating system decides when to switch tasks external to Python. The processes all run at the same time on different processors. You’ll explore these modules as you make your way through the tutorial. Note: Both and represent fairly low-level building blocks in concurrent programs. In practice, you can often replace them with , which provides a higher-level interface for both modules. On the other hand, offers a bit of a different approach to concurrency, which you’ll dive into later. Each of the corresponding types of concurrency can be useful in its own way. You’ll now take a look at what types of programs they can help you speed up. When Is Concurrency Useful? Concurrency can make a big difference for two types of problems: I/O-bound problems cause your program to slow down because it frequently must wait for input or output (I/O) from some external resource. They arise when your program is working with things that are much slower than your CPU. Examples of things that are slower than your CPU are legion, but your program thankfully doesn’t interact with most of them. The slow things your program will interact with the most are the file system and network connections. The blue boxes show the time when your program is doing work, and the red boxes are time spent waiting for an I/O operation to complete. This diagram is not to scale because requests on the internet can take several orders of magnitude longer than CPU instructions, so your program can end up spending most of its time waiting. That’s what your web browser is doing most of the time. On the flip side, there are classes of programs that do significant computation without talking to the network or accessing a file. These are CPU-bound programs because the resource limiting the speed of your program is the CPU, not the network or the file system. As you work through the examples in the following section, you’ll see that different forms of concurrency work better or worse with I/O-bound and CPU-bound programs. Adding concurrency to your program introduces extra code and complications, so you’ll need to decide if the potential speedup is worth the additional effort. By the end of this tutorial, you should have enough information to start making that decision. Your program spends most of its time talking to a slow device, like a network adapter, a hard drive, or a printer. Your program spends most of its time doing CPU operations. Speeding it up involves overlapping the times spent waiting for these devices. Speeding it up involves finding ways to do more computations in the same amount of time. You’ll look at I/O-bound programs first. Then, you’ll get to see some code dealing with CPU-bound programs.\n\nIn this section, you’ll focus on I/O-bound programs and a common problem: downloading content over the network. For this example, you’ll be downloading web pages from a few sites, but it really could be any network traffic. It’s just more convenient to visualize and set up with web pages. You’ll start with a non-concurrent version of this task. Note that this program requires the third-party Requests library. So, you should first run the following command in an activated virtual environment: This version of your program doesn’t use concurrency at all: As you can see, this is a fairly short program. It just downloads the site contents from a list of addresses and prints their sizes. One small thing to point out is that you’re using a session object from . It’s possible to call directly, but creating a object allows the library to retain state across requests and reuse the connection to speed things up. You create the session in and then walk through the list of sites, downloading each one in turn. Finally, you print out how long this process took so you can have the satisfaction of seeing how much concurrency has helped you in the following examples. The processing diagram for this program will look much like the I/O-bound diagram in the last section. Note: Network traffic is dependent on many factors that can vary from second to second. You may see the times of these tests double from one run to another due to network issues. The great thing about this version of code is that, well, it’s simple. It was comparatively quick to write and debug. It’s also more straightforward to think about. There’s only one train of thought running through it, so you can predict what the next step is and how it’ll behave. The big problem here is that it’s relatively slow compared to the other solutions that you’re about to see. Here’s an example of what the final output might look like: Note that these results may vary significantly depending on the speed of your internet connection, network congestion, and other factors. To account for them, you should repeat each benchmark a few times and take the fastest of the runs. That way, the differences between your program’s versions will still be clear. Being slower isn’t always a big issue. If the program you’re running takes only two seconds with a synchronous version and is only run rarely, then it’s probably not worth adding concurrency. You can stop here. What if your program is run frequently? What if it takes hours to run? You’ll move on to concurrency by rewriting this program using Python threads. As you probably guessed, writing a program leveraging multithreading takes more effort. However, you might be surprised at how little extra effort it takes for basic cases. Here’s what the same program looks like when you take advantage of the and modules mentioned earlier: The overall structure of your program is the same, but the highlighted lines indicate the changes you needed to make. On line 20, you created an instance of the to manage the threads for you. In this case, you explicitly requested five workers or threads. Note: How do you pick the number of threads in your pool? The difficult answer here is that the correct number of threads is not a constant from one task to another. In general, with IO-bound problems, you’re not limited to the number of CPU cores. In fact, it’s not uncommon to create hundreds or even thousands of threads as long as they wait for data instead of doing real work. But, at some point, you’ll eventually start experiencing diminishing returns due to the extra overhead of switching threads. Some experimentation is always recommended. Feel free to play around with this number to see how it affects the overall execution time. Creating a seems like a complicated thing. But, when you break it down, you’ll end up with these three components: You already know about the thread part. That’s just the train of thought mentioned earlier. The pool portion is where it starts to get interesting. This object is going to create a pool of threads, each of which can run concurrently. Finally, the executor is the part that’s going to control how and when each of the threads in the pool will run. It’ll execute the request in the pool. Note: Using a thread pool can be beneficial when you have limited system resources but still want to handle many tasks. By creating the threads upfront and reusing them for the subsequent tasks, a pool reduces the overhead of repeatedly creating and destroying threads. The standard library implements as a context manager, so you can use the syntax to manage creating and freeing the pool of instances. In this multi-threaded version of the program, you let the executor call on your behalf instead of doing it manually in a loop. The method on line 21 takes care of distributing the workload across the available threads, allowing each one to handle a different site concurrently. This method takes two arguments:\n• A function to be executed on each data item, like a site address\n• A collection of data items to be processed by that function Since the function that you passed to the executor’s method must take exactly one argument, you modified on line 23 to only accept a URL. But how do you obtain the session object now? This is one of the interesting and difficult issues with threading. Because the operating system controls when your task gets interrupted and another task starts, any data shared between the threads needs to be protected or thread-safe to avoid unexpected behavior or potential data corruption. Unfortunately, isn’t thread-safe, meaning that one thread may interfere with the session while another thread is still using it. There are several strategies for making data access thread-safe. One of them is to use a thread-safe data structure, such as a , , or an . These objects use low-level primitives like lock objects to ensure that only one thread can access a block of code or a bit of memory at the same time. You’re using this strategy indirectly by way of the object. Another strategy to use here is something called thread-local storage. When you call on line 7, you create an object that resembles a global variable but is specific to each individual thread. It looks a little odd, but you only want to create one of these objects, not one for each thread. The object itself takes care of separating accesses from different threads to its attributes. When is called, the session it looks up is specific to the particular thread on which it’s running. So each thread will create a single session the first time it calls and then will use that session on each subsequent call throughout its lifetime. Okay. It’s time to put your multi-threaded program to the ultimate test: It’s fast! Remember that the non-concurrent version took more than fourteen seconds in the best case. Here’s what its execution timing diagram looks like: The program uses multiple threads to have many open requests out to web sites at the same time. This allows your program to overlap the waiting times and get the final result faster. Yippee! That was the goal. Are there any problems with the multi-threaded version? Well, as you can see from the example, it takes a little more code to make this happen, and you really have to give some thought to what data is shared between threads. Threads can interact in ways that are subtle and hard to detect. These interactions can cause race conditions that frequently result in random, intermittent bugs that can be quite difficult to find. If you’re unfamiliar with this concept, then you might want to check out a section on race conditions in another tutorial on thread safety. Running threads concurrently allowed you to cut down the total execution time of your original synchronous code by an order of magnitude. That’s already pretty remarkable, but you can do even better than that by taking advantage of Python’s module, which enables asynchronous I/O. Asynchronous processing is a concurrency model that’s well-suited for I/O-bound tasks—hence the name, . It avoids the overhead of context switching between threads by employing the event loop, non-blocking operations, and coroutines, among other things. Perhaps somewhat surprisingly, the asynchronous code needs only one thread of execution to run concurrently. Note: If these concepts sound unfamiliar to you, or you need a quick refresher, then check out Getting Started With Async Features in Python and Async IO in Python: A Complete Walkthrough to learn more. In a nutshell, the event loop controls how and when each asynchronous task gets to execute. As the name suggests, it continuously loops through your tasks while monitoring their state. As soon as the current task starts waiting for an I/O operation to finish, the loop suspends it and immediately switches to another task. Conversely, once the expected event occurs, the loop will eventually resume the suspended task in the next iteration. A coroutine is similar to a thread but much more lightweight and cheaper to suspend or resume. That’s what makes it possible to spawn many more coroutines than threads without a significant memory or performance overhead. This capability helps address the C10k problem, which involves handling ten thousand concurrent connections efficiently. But there’s a catch. You can’t have blocking function calls in your coroutines if you want to reap the full benefits of asynchronous programming. A blocking call is a synchronous one, meaning that it prevents other code from running while it’s waiting for data to arrive. In contrast, a non-blocking call can voluntarily give up control and wait to be notified when the data is ready. In Python, you create a coroutine object by calling an asynchronous function, also known as a coroutine function. Those are defined with the statement instead of the usual . Only within the body of an asynchronous function are you allowed to use the keyword, which pauses the execution of the coroutine until the awaited task is completed: In this case, you defined as an asynchronous function that implicitly returns a coroutine object when called. Thanks to the keyword, your coroutine makes a non-blocking call to , simulating a delay of three and a half seconds. While your function awaits the wake-up event, other tasks could potentially run concurrently. Note: To run the sample code above, you’ll need to either wrap the call to in or await in Python’s asyncio REPL. Now that you’ve got a basic understanding of what asynchronous I/O is, you can walk through the asynchronous version of the example code and figure out how it works. However, because the Requests library that you’ve been using in this tutorial is blocking, you must now switch to a non-blocking counterpart, such as , which was designed for Python’s : After installing this library in your virtual environment, you can use it in the asynchronous version of the code: This version looks strikingly similar to the synchronous one, which is yet another advantage of . It’s a double-edged sword, though. While it arguably makes your concurrent code easier to reason about than the multi-threaded version, is far from easy when you get into more complex scenarios. Here are the most important differences when compared to the non-concurrent version:\n• Line 1 imports from Python’s standard library. This is necessary to run your asynchronous function on line 26.\n• Line 4 imports the third-party library, which you’ve installed into the virtual environment. This library replaces Requests from earlier examples.\n• Lines 6, 16, and 21 redefine your regular functions as asynchronous ones by qualifying their signatures with the keyword.\n• Line 12 prepends the keyword to so that the returned coroutine object can be awaited. This effectively suspends your function until all sites have been downloaded.\n• Lines 17 and 22 leverage the statement to create asynchronous context managers for the session object and the response, respectively.\n• Line 18 creates a list of tasks using a list comprehension, where each task is a coroutine object returned by . Notice that you don’t await the individual coroutine objects, as doing so would lead to executing them sequentially.\n• Line 19 uses to run all the tasks concurrently, allowing for efficient downloading of multiple sites at the same time.\n• Line 23 awaits the completion of the session’s HTTP GET request before printing the number of bytes read. You can share the session across all tasks, so the session is created here as a context manager. The tasks can share the session because they’re all running on the same thread. There’s no way one task could interrupt another while the session is in a bad state. There’s one small but important change buried in the details here. Remember the mention about the optimal number of threads to create? It wasn’t obvious in the multi-threaded example what the optimal number of threads was. One of the cool advantages of is that it scales far better than or . Each task takes far fewer resources and less time to create than a thread, so creating and running more of them works well. This example just creates a separate task for each site to download, which works out quite well. And, it’s really fast. The asynchronous version is the fastest of them all by a good margin: It took less than a half a second to complete, making this code seven times quicker than the multi-threaded version and over thirty times faster than the non-concurrent version! Note: In the synchronous version, you cycled through a list of sites and kept downloading their content in a deterministic order. With the multi-threaded version, you ceded control over task scheduling to the operating system, so the final order seemed random. While the asynchronous version may show some clustering of completions, it’s generally non-deterministic due to changing network conditions. The execution timing diagram looks quite similar to what’s happening in the multi-threaded example. It’s just that the I/O requests are all done by the same thread: There’s a common argument that having to add and in the proper locations is an extra complication. To a small extent, that’s true. The flip side of this argument is that it forces you to think about when a given task will get swapped out, which can help you create a better design. The scaling issue also looms large here. Running the multi-threaded example with a thread for each site is noticeably slower than running it with a handful of threads. Running the example with hundreds of tasks doesn’t slow it down at all. There are a couple of issues with at this point. You need special asynchronous versions of libraries to gain the full advantage of . Had you just used Requests for downloading the sites, it would’ve been much slower because Requests isn’t designed to notify the event loop that it’s blocked. This issue is becoming less significant as time goes on and more libraries embrace . Another more subtle issue is that all the advantages of cooperative multitasking get thrown away if one of the tasks doesn’t cooperate. A minor mistake in code can cause a task to run off and hold the processor for a long time, starving other tasks that need running. There’s no way for the event loop to break in if a task doesn’t hand control back to it. With that in mind, you can step up to a radically different approach to concurrency using multiple processes. Up to this point, all of the examples of concurrency in this tutorial ran only on a single CPU or core in your computer. The reasons for this have to do with the current design of CPython and something called the Global Interpreter Lock, or GIL. This tutorial won’t dive into the hows and whys of the GIL. It’s enough for now to know that the synchronous, multi-threaded, and asynchronous versions of this example all run on a single CPU. The module, along with the corresponding wrappers in , was designed to break down that barrier and run your code across multiple CPUs. At a high level, it does this by creating a new instance of the Python interpreter to run on each CPU and then farming out part of your program to run on it. As you can imagine, bringing up a separate Python interpreter is not as fast as starting a new thread in the current Python interpreter. It’s a heavyweight operation and comes with some restrictions and difficulties, but for the correct problem, it can make a huge difference. Unlike the previous approaches, using multiprocessing allows you to take full advantage of the all CPUs that your cool, new computer has. Here’s the sample code: This actually looks quite similar to the multi-threaded example, as you leverage the familiar abstraction instead of relying on directly. Go ahead and take a quick tour of what this code does for you:\n• Line 8 uses type hints to declare a global variable that will hold the session object. Note that this doesn’t actually define the value of the variable.\n• Line 21 replaces with from and passes , which is defined further down.\n• Lines 29 to 32 define a custom initializer function that each process will call shortly after starting. It ensures that each process initializes its own session.\n• Line 32 registers a cleanup function with , which ensures that the session is properly closed when the process stops. This helps prevent potential memory leaks. What happens here is that the pool creates a number of separate Python interpreter processes and has each one run the specified function on some of the items in the iterable, which in your case is the list of sites. The communication between the main process and the other processes is handled for you. The line that creates a pool instance is worth your attention. First off, it doesn’t specify how many processes to create in the pool, although that’s an optional parameter. By default, it’ll determine the number of CPUs in your computer and match that. This is frequently the best answer, and it is in your case. For an I/O-bound problem, increasing the number of processes won’t make things faster. It’ll actually slow things down because the cost of setting up and tearing down all those processes is larger than the benefit of doing the I/O requests in parallel. Note: If you need to exchange data between your processes, then it’ll require expensive inter-process communication (IPC) and data serialization, which increases the overall cost even further. Besides this, serialization isn’t always possible because Python uses the module under the surface, which supports only a few data types. Next, you have the initializer part of that call. Remember that each process in our pool has its own memory space. That means they can’t easily share things like a session object. You don’t want to create a new instance each time the function is called—you want to create one for each process. The function parameter is built for just this case. There’s no way to pass a return value back from the to , but you can initialize a global variable to hold the single session for each process. Because each process has its own memory space, the global for each one will be different. That’s really all there is to it. The rest of the code is quite similar to what you’ve seen before. The process-based version does require some extra setup, and the global session object is strange. You have to spend some time thinking about which variables will be accessed in each process. While this version takes full advantage of the CPU power in your computer, the resulting performance is surprisingly underwhelming: On a computer equipped with four CPU cores, it runs about four times faster than the synchronous version. Still, it’s a bit slower than the multi-threaded version and much slower than the asynchronous version. The execution timing diagram for this code looks like this: There are a few separate processes executing in parallel. The corresponding diagrams of each one of them resemble the non-concurrent version you saw at the beginning of this tutorial. I/O-bound problems aren’t really why multiprocessing exists. You’ll see more as you step into the next section and look at CPU-bound examples.\n\nIt’s time to shift gears here a little bit. The examples so far have all dealt with an I/O-bound problem. Now, you’ll look into a CPU-bound problem. As you learned earlier, an I/O-bound problem spends most of its time waiting for external operations to complete, such as network calls. In contrast, a CPU-bound problem performs fewer I/O operations, and its total execution time depends on how quickly it can process the required data. For the purposes of this example, you’ll use a somewhat silly function to create a piece of code that takes a long time to run on the CPU. This function computes the n-th Fibonacci number using the recursive approach: Notice how quickly the resulting values grow as the function computes higher Fibonacci numbers. The recursive nature of this implementation leads to many repeated calculations of the same numbers, which requires substantial processing time. That’s what makes this such a convenient example of a CPU-bound task. Remember, this is just a placeholder for your code that actually does something useful and requires lengthy processing, like computing the roots of equations or sorting a large data structure. First off, you can look at the non-concurrent version of the example: This code calls twenty times in a loop. Due to the recursive nature of its implementation, the function calls itself hundreds of millions of times! It does all of this on a single thread in a single process on a single CPU. The execution timing diagram looks like this: Unlike the I/O-bound examples, the CPU-bound examples are usually fairly consistent in their run times. This one takes about thirty-five seconds on the same machine as before: Clearly, you can do better than this. After all, it’s all running on a single CPU with no concurrency. Next, you’ll see what you can do to improve it. How much do you think rewriting this code using threads—or asynchronous tasks—will speed this up? If you answered “Not at all,” then give yourself a cookie. If you answered, “It will slow it down,” then give yourself two cookies. Here’s why: In your earlier I/O-bound example, much of the overall time was spent waiting for slow operations to finish. Threads and asynchronous tasks sped this up by allowing you to overlap the waiting times instead of performing them sequentially. With a CPU-bound problem, there’s no waiting. The CPU is cranking away as fast as it can to finish the problem. In Python, both threads and asynchronous tasks run on the same CPU in the same process. This means that the one CPU is doing all of the work of the non-concurrent code plus the extra work of setting up threads or tasks. Here’s the code of the multi-threaded version of your CPU-bound problem: Little of this code had to change from the non-concurrent version. After importing , you just changed from looping through the numbers to creating a thread pool and using its method to send individual numbers to worker threads as they become free. This was just what you did for the I/O-bound multi-threaded code, but here, you didn’t need to worry about the object. Below is the output you might see when running this code: Unsurprisingly, it takes a few seconds longer than the synchronous version. Okay. At this point, you should know what to expect from the asynchronous version of a CPU-bound problem. But for completeness, you’ll now test how it stacks up against the others. Implementing the asynchronous version of this CPU-bound problem involves rewriting your functions into coroutine functions with and awaiting their return values: You create twenty tasks and pass them to to let the corresponding coroutines run concurrently. However, they actually run in sequence, as each blocks execution until the previous one is finished. When run, this code takes over twice as long to execute as your original synchronous version and also takes longer than the multi-threaded version: Ironically, the asynchronous approach is the slowest for a CPU-bound problem, yet it was the fastest for an I/O-bound one. Because there are no I/O operations involved here, there’s nothing to wait for. The overhead of the event loop and context switching at every single statement slows down the total execution substantially. In Python, to improve the performance of a CPU-bound task like this one, you must use an alternative concurrency model. You’ll take a closer look at that now. You’ve finally reached the part where multiprocessing really shines. Unlike the other concurrency models, process-based parallelism is explicitly designed to share heavy CPU workloads across multiple CPUs. Here’s what the corresponding code looks like: It’s almost identical to the multi-threaded version of the Fibonacci problem. You literally changed just two lines of code! Instead of using , you replaced it with . As mentioned before, the optional parameter to the pool’s constructor deserves some attention. You can use it to specify how many processes you want to be created and managed in the pool. By default, it’ll determine how many CPUs are in your machine and create a process for each one. While this works great for your simple example, you might want to have a little more control in a production environment. This version takes about ten seconds, which is less than one-third of the non-concurrent implementation you started with: This is much better than what you saw with the other options, making it by far the best choice for this kind of task. Here’s what the execution timing diagram looks like: The individual tasks run alongside each other on separate CPU cores, making parallel execution possible. There are some drawbacks to using multiprocessing that don’t really show up in a simple example like this one. For example, dividing your problem into segments so each processor can operate independently can sometimes be difficult. Also, many solutions require more communication between the processes. This can add some complexity to your solution that a non-concurrent program just wouldn’t need to deal with."
    },
    {
        "link": "https://chardet.readthedocs.io/en/latest/usage.html",
        "document": "The easiest way to use the Universal Encoding Detector library is with the detect function.\n\nThe detect function takes one argument, a non-Unicode string. It returns a dictionary containing the auto-detected character encoding and a confidence level from 0 to 1 .\n\nIf you’re dealing with a large amount of text, you can call the Universal Encoding Detector library incrementally, and it will stop as soon as it is confident enough to report its results.\n\nCreate a object, then call its method repeatedly with each block of text. If the detector reaches a minimum threshold of confidence, it will set to .\n\nOnce you’ve exhausted the source text, call , which will do some final calculations in case the detector didn’t hit its minimum confidence threshold earlier. Then will be a dictionary containing the auto-detected character encoding and confidence level (the same as the function returns)."
    },
    {
        "link": "https://geeksforgeeks.org/character-encoding-detection-with-chardet-in-python",
        "document": "We are given some characters in the form of text files, unknown encoded text, and website content and our task is to detect the character encoding with Chardet in Python. In this article, we will see how we can perform character encoding detection with Chardet in Python.\n\nBelow are some of the examples by which we can understand how to detect the character encoding with Chardet in Python:\n\nFirst of all, we will install chardet in Python by using the following command and then we will perform other operations to detect character encoding in Python:\n\nIn this example, the Python script uses the library to detect the character encoding of a given byte sequence ( ). The detected encoding and its confidence level are printed, revealing information about the encoding scheme of the provided binary data.\n\nIn this example, the Python script utilizes the library to fetch the HTML content of the GeeksforGeeks webpage. The library is then employed to detect the character encoding of the retrieved content. The detected encoding and its confidence level are printed, providing insights into the encoding scheme used by the webpage.\n\nIn this example, the Python script reads the content of a text file ('utf-8.txt') in binary mode using and . The library is then used to detect the character encoding of the file's content. The detected encoding and its confidence level are printed, offering information about the encoding scheme used in the specified text file."
    },
    {
        "link": "https://chardet.readthedocs.io",
        "document": "Character encoding auto-detection in Python. As smart as your browser. Open source."
    },
    {
        "link": "https://github.com/chardet/chardet",
        "document": "Our ISO-8859-2 and windows-1250 (Hungarian) probers have been temporarily disabled until we can retrain the models.\n\nFor users, docs are now available at https://chardet.readthedocs.io/.\n\nchardet comes with a command-line script which reports on the encodings of one or more files:\n\nThis is a continuation of Mark Pilgrim's excellent original chardet port from C, and Ian Cordasco's charade Python 3-compatible fork."
    },
    {
        "link": "https://stackoverflow.com/questions/13591926/encoding-detection-in-python-use-the-chardet-library-or-not",
        "document": "I'm writing an app that takes some massive amounts of texts as input which could be in any character encoding, and I want to save it all in UTF-8. I won't receive, or can't trust, the character encoding that comes defined with the data (if any).\n\nI have for a while used Pythons library chardet to detect the original character encoding, http://pypi.python.org/pypi/chardet, but ran into some problems lately where I noticed that it doesn't support Scandinavian encodings (for example iso-8859-1). And apart from that, it takes a huge amount of time/CPU/mem to get results. ~40s for a 2MB text file.\n\nI tried just using the standard Linux file\n\nAnd with all my files so far it provides me with a 100% result. And this with ~0.1s for a 2MB file. And it supports Scandinavian character encodings as well.\n\nSo, I guess the advantages with using file is clear. What are the downsides? Am I missing something?"
    }
]