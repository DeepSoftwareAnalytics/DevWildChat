[
    {
        "link": "https://stackoverflow.com/questions/10113532/how-do-i-fix-the-visual-studio-compile-error-mismatch-between-processor-archit",
        "document": "I'm new to project configuration in Visual Studio 2010, but I've done some research and still can't quite figure this issue out. I have a Visual Studio solution with a C++ DLL referencing the C# DLL. The C# DLL references a few other DLLs, some within my project and some external. When I try to compile the C++ DLL, I get this warning: warning MSB3270: There was a mismatch between the processor architecture of the project being build \"MSIL\" and the processor architecture of the reference \"[internal C# dll]\", \"x86\". It tells me to go to Configuration Manager to align my architectures. The C# DLL is set up with platform target x86. If I try to change this to something else, like Any CPU, it complains because one of the external DLLs it depends on has platform target x86. When I look at Configuration Manager it shows the Platform for my C# DLL as x86 and for my C++ project as Win32. This seems like the right setup; surely I don't want the project for my C++ project to have platform set to x64, which is the only other option presented. What am I doing wrong here?"
    },
    {
        "link": "https://stackoverflow.com/questions/25063268/how-do-i-fix-a-mismatch-between-processor-architectures",
        "document": "I have a number of projects that I have combined in my project. The error message I get is this:\n\nThere was a mismatch between the processor architecture of the project being built \"MSIL\" and the processor architecture of the reference \"Interop.Domino, Version=1.2.0.0, Culture=neutral, processorArchitecture=x86\", \"x86\". This mismatch may cause runtime failures. Please consider changing the targeted processor architecture of your project through the Configuration Manager so as to align the processor architectures between your project and references, or take a dependency on references with a processor architecture that matches the targeted processor architecture of your project.\n\nI have found this link which gives some interesting information.\n\nHowever when I open up the Configuration Manager, everything looks fine:\n\nAny idea of what I can do to get rid of this compiler message?"
    },
    {
        "link": "https://askubuntu.com/questions/713624/how-do-i-get-rid-of-ffmpeg-warning-library-configuration-mismatch-message",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://github.com/tanersener/mobile-ffmpeg/issues/613",
        "document": "Description\n\n I'm getting error on x86-64 architecture while building aar. Other arhitecures are working."
    },
    {
        "link": "https://reddit.com/r/learnprogramming/comments/2j3zrh/c_compilations_errors_with_ffmpeg_libraries",
        "document": "I seem to have coded myself into a knot i cannot seem to untie.\n\nI am currently compiling ffmpeg examples for testing purposes, but i seem to have some problems during compilation.\n\nFfmpeg has been compiled from latest stable sources (FFmpeg 2.4.2 \"Fresnel\").\n\nAll lib headers have been included:"
    },
    {
        "link": "https://github.com/leandromoreira/ffmpeg-libav-tutorial",
        "document": "I was looking for a tutorial/book that would teach me how to start to use FFmpeg as a library (a.k.a. libav) and then I found the \"How to write a video player in less than 1k lines\" tutorial. Unfortunately it was deprecated, so I decided to write this one.\n\nMost of the code in here will be in C but don't worry: you can easily understand and apply it to your preferred language. FFmpeg libav has lots of bindings for many languages like python, go and even if your language doesn't have it, you can still support it through the (here's an example with Lua).\n\nWe'll start with a quick lesson about what is video, audio, codec and container and then we'll go to a crash course on how to use command line and finally we'll write code, feel free to skip directly to the section Learn FFmpeg libav the Hard Way.\n\nSome people used to say that the Internet video streaming is the future of the traditional TV, in any case, the FFmpeg is something that is worth studying.\n• Intro\n• video - what you see!\n\nIf you have a sequence series of images and change them at a given frequency (let's say 24 images per second), you will create an illusion of movement. In summary this is the very basic idea behind a video: a series of pictures / frames running at a given rate.\n\nAlthough a muted video can express a variety of feelings, adding sound to it brings more pleasure to the experience.\n\nSound is the vibration that propagates as a wave of pressure, through the air or any other transmission medium, such as a gas, liquid or solid.\n\nBut if we chose to pack millions of images in a single file and called it a movie, we might end up with a huge file. Let's do the math:\n\nSuppose we are creating a video with a resolution of (height x width) and that we'll spend per pixel (the minimal point at a screen) to encode the color (or 24 bit color, what gives us 16,777,216 different colors) and this video runs at and it is long.\n\nThis video would require approximately of storage or of bandwidth! That's why we need to use a CODEC.\n\nA single file that contains all the streams (mostly the audio and video) and it also provides synchronization and general metadata, such as title, resolution and etc.\n\nUsually we can infer the format of a file by looking at its extension: for instance a is probably a video using the container .\n\nTo work with multimedia we can use the AMAZING tool/library called FFmpeg. Chances are you already know/use it directly or indirectly (do you use Chrome?).\n\nIt has a command line program called , a very simple yet powerful binary. For instance, you can convert from to the container just by typing the follow command:\n\nWe just made a remuxing here, which is converting from one container to another one. Technically FFmpeg could also be doing a transcoding but we'll talk about that later.\n\nFFmpeg does have a documentation that does a great job of explaining how it works.\n\nTo make things short, the FFmpeg command line program expects the following argument format to perform its actions , where:\n\nThe parts 2, 3, 4 and 5 can be as many as you need. It's easier to understand this argument format in action:\n\nThis command takes an input file containing two streams (an audio encoded with CODEC and a video encoded using CODEC) and convert it to , changing its audio and video CODECs too.\n\nWe could simplify the command above but then be aware that FFmpeg will adopt or guess the default values for you. For instance when you just type what audio/video CODEC does it use to produce the ?\n\nWerner Robitza wrote a must read/execute tutorial about encoding and editing with FFmpeg.\n\nWhile working with audio/video we usually do a set of tasks with the media.\n\nWhat? the act of converting one of the streams (audio or video) from one CODEC to another one.\n\nWhy? sometimes some devices (TVs, smartphones, console and etc) doesn't support X but Y and newer CODECs provide better compression rate.\n\nHow? converting an (AVC) video to an (HEVC).\n\nWhat? the act of converting from one format (container) to another one.\n\nWhy? sometimes some devices (TVs, smartphones, console and etc) doesn't support X but Y and sometimes newer containers provide modern required features.\n\nWhat? the act of changing the bit rate, or producing other renditions.\n\nWhy? people will try to watch your video in a (edge) connection using a less powerful smartphone or in a Internet connection on their 4K TVs therefore you should offer more than one rendition of the same video with different bit rate.\n\nHow? producing a rendition with bit rate between 964K and 3856K.\n\nUsually we'll be using transrating with transsizing. Werner Robitza wrote another must read/execute series of posts about FFmpeg rate control.\n\nWhat? the act of converting from one resolution to another one. As said before transsizing is often used with transrating.\n\nWhy? reasons are about the same as for the transrating.\n\nWhat? the act of producing many resolutions (bit rates) and split the media into chunks and serve them via http.\n\nWhy? to provide a flexible media that can be watched on a low end smartphone or on a 4K TV, it's also easy to scale and deploy but it can add latency.\n\nHow? creating an adaptive WebM using DASH.\n\nPS: I stole this example from the Instructions to playback Adaptive WebM using DASH\n\nThere are many and many other usages for FFmpeg. I use it in conjunction with iMovie to produce/edit some videos for YouTube and you can certainly use it professionally.\n\nSince the FFmpeg is so useful as a command line tool to do essential tasks over the media files, how can we use it in our programs?\n\nFFmpeg is composed by several libraries that can be integrated into our own programs. Usually, when you install FFmpeg, it installs automatically all these libraries. I'll be referring to the set of these libraries as FFmpeg libav.\n\nThis hello world actually won't show the message in the terminal 👅 Instead we're going to print out information about the video, things like its format (container), duration, resolution, audio channels and, in the end, we'll decode some frames and save them as image files.\n\nBut before we start to code, let's learn how FFmpeg libav architecture works and how its components communicate with others.\n\nHere's a diagram of the process of decoding a video:\n\nYou'll first need to load your media file into a component called (the video container is also known as format). It actually doesn't fully load the whole file: it often only reads the header.\n\nOnce we loaded the minimal header of our container, we can access its streams (think of them as a rudimentary audio and video data). Each stream will be available in a component called .\n\nSuppose our video has two streams: an audio encoded with AAC CODEC and a video encoded with H264 (AVC) CODEC. From each stream we can extract pieces (slices) of data called packets that will be loaded into components named .\n\nThe data inside the packets are still coded (compressed) and in order to decode the packets, we need to pass them to a specific .\n\nThe will decode them into and finally, this component gives us the uncompressed frame. Noticed that the same terminology/process is used either by audio and video stream.\n\nSince some people were facing issues while compiling or running the examples we're going to use as our development/runner environment, we'll also use the big buck bunny video so if you don't have it locally just run the command .\n\nWe'll skip some details, but don't worry: the source code is available at github.\n\nWe're going to allocate memory to the component that will hold information about the format (container).\n\nNow we're going to open the file and read its header and fill the with minimal information about the format (notice that usually the codecs are not opened). The function used to do this is . It expects an , a and two optional arguments: the (if you pass , FFmpeg will guess the format) and the (which are the options to the demuxer).\n\nWe can print the format name and the media duration:\n\nTo access the , we need to read data from the media. The function does that. Now, the will hold the amount of streams and the will give us the stream (an ).\n\nNow we'll loop through all the streams.\n\nFor each stream, we're going to keep the , which describes the properties of a codec used by the stream .\n\nWith the codec properties we can look up the proper CODEC querying the function and find the registered decoder for the codec id and return an , the component that knows how to enCOde and DECode the stream.\n\nNow we can print information about the codecs.\n\nWith the codec, we can allocate memory for the , which will hold the context for our decode/encode process, but then we need to fill this codec context with CODEC parameters; we do that with .\n\nOnce we filled the codec context, we need to open the codec. We call the function and then we can use it.\n\nNow we're going to read the packets from the stream and decode them into frames but first, we need to allocate memory for both components, the and .\n\nLet's feed our packets from the streams with the function while it has packets.\n\nLet's send the raw data packet (compressed frame) to the decoder, through the codec context, using the function .\n\nAnd let's receive the raw data frame (uncompressed frame) from the decoder, through the same codec context, using the function .\n\nWe can print the frame number, the PTS, DTS, frame type and etc.\n\nFinally we can save our decoded frame into a simple gray image. The process is very simple, we'll use the where the index is related to the planes Y, Cb and Cr, we just picked (Y) to save our gray image.\n\nAnd voilà! Now we have a gray scale image with 2MB:\n\nBefore we move to code a transcoding example let's talk about timing, or how a video player knows the right time to play a frame.\n\nIn the last example, we saved some frames that can be seen here:\n\nWhen we're designing a video player we need to play each frame at a given pace, otherwise it would be hard to pleasantly see the video either because it's playing so fast or so slow.\n\nTherefore we need to introduce some logic to play each frame smoothly. For that matter, each frame has a presentation timestamp (PTS) which is an increasing number factored in a timebase that is a rational number (where the denominator is known as timescale) divisible by the frame rate (fps).\n\nIt's easier to understand when we look at some examples, let's simulate some scenarios.\n\nFor a and each PTS will increase therefore the PTS real time for each frame could be (supposing it started at 0):\n\nFor almost the same scenario but with a timebase equal to .\n\nFor a and each PTS will increase and the PTS time could be:\n\nNow with the we can find a way to render this synched with audio or with a system clock. The FFmpeg libav provides these info through its API:\n\nJust out of curiosity, the frames we saved were sent in a DTS order (frames: 1,6,4,2,3,5) but played at a PTS order (frames: 1,2,3,4,5). Also, notice how cheap are B-Frames in comparison to P or I-Frames.\n\nRemuxing is the act of changing from one format (container) to another, for instance, we can change a MPEG-4 video to a MPEG-TS one without much pain using FFmpeg:\n\nIt'll demux the mp4 but it won't decode or encode it ( ) and in the end, it'll mux it into a file. If you don't provide the format the ffmpeg will try to guess it based on the file's extension.\n\nThe general usage of FFmpeg or the libav follows a pattern/architecture or workflow:\n• protocol layer - it accepts an (a for instance but it could be a or input as well)\n• format layer - it its content, revealing mostly metadata and its streams\n• pixel layer - it can also apply some to the raw frames (like resizing)optional\n• and then it does the reverse path\n• codec layer - it (or or even ) the raw framesoptional\n• format layer - it (or ) the raw streams (the compressed data)\n• protocol layer - and finally the muxed data is sent to an (another file or maybe a network remote server)\n\nNow let's code an example using libav to provide the same effect as in .\n\nWe're going to read from an input ( ) and change it to another output ( ).\n\nWe start doing the usually allocate memory and open the input format. For this specific case, we're going to open an input file and allocate memory for an output file.\n\nWe're going to remux only the video, audio and subtitle types of streams so we're holding what streams we'll be using into an array of indexes.\n\nJust after we allocated the required memory, we're going to loop throughout all the streams and for each one we need to create new out stream into our output format context, using the avformat_new_stream function. Notice that we're marking all the streams that aren't video, audio or subtitle so we can skip them after.\n\nNow we can create the output file.\n\nAfter that, we can copy the streams, packet by packet, from our input to our output streams. We'll loop while it has packets ( ), for each packet we need to re-calculate the PTS and DTS to finally write it ( ) to our output format context.\n\nTo finalize we need to write the stream trailer to an output media file with av_write_trailer function.\n\nNow we're ready to test it and the first test will be a format (video container) conversion from a MP4 to a MPEG-TS video file. We're basically making the command line with libav.\n\nIt's working!!! don't you trust me?! you shouldn't, we can check it with :\n\nTo sum up what we did here in a graph, we can revisit our initial idea about how libav works but showing that we skipped the codec part.\n\nBefore we end this chapter I'd like to show an important part of the remuxing process, you can pass options to the muxer. Let's say we want to delivery MPEG-DASH format for that matter we need to use fragmented mp4 (sometimes referred as ) instead of MPEG-TS or plain MPEG-4.\n\nWith the command line we can do that easily.\n\nAlmost equally easy as the command line is the libav version of it, we just need to pass the options when write the output header, just before the packets copy.\n\nWe now can generate this fragmented mp4 file:\n\nBut to make sure that I'm not lying to you. You can use the amazing site/tool gpac/mp4box.js or the site http://mp4parser.com/ to see the differences, first load up the \"common\" mp4.\n\nAs you can see it has a single atom/box, this is place where the video and audio frames are. Now load the fragmented mp4 to see which how it spreads the boxes.\n\nIn this chapter, we're going to create a minimalist transcoder, written in C, that can convert videos coded in H264 to H265 using FFmpeg/libav library specifically libavcodec, libavformat, and libavutil.\n\nLet's start with the simple transmuxing operation and then we can build upon this code, the first step is to load the input file.\n\nNow we're going to set up the decoder, the will give us access to all the components and for each one of them, we can get their and create the particular and finally we can open the given codec so we can proceed to the decoding process.\n\nWe need to prepare the output media file for transmuxing as well, we first allocate memory for the output . We create each stream in the output format. In order to pack the stream properly, we copy the codec parameters from the decoder.\n\nWe set the flag which tells the encoder that it can use the global headers and finally we open the output file for write and persist the headers.\n\nWe're getting the 's from the decoder, adjusting the timestamps, and write the packet properly to the output file. Even though the function says \"write frame\" we are storing the packet. We finish the transmuxing process by writing the stream trailer to the file.\n\nThe previous section showed a simple transmuxer program, now we're going to add the capability to encode files, specifically we're going to enable it to transcode videos from to .\n\nAfter we prepared the decoder but before we arrange the output media file we're going to set up the encoder.\n• Create the video in the encoder,\n• Create the based in the created codec,\n• Set up basic attributes for the transcoding session, and\n• Open the codec and copy parameters from the context to the stream. and\n\nWe need to expand our decoding loop for the video stream transcoding:\n• Send the empty to the decoder,\n• Receive the compressed, based on our codec, ,\n• Set up the timestamp, and\n• Write it to the output file.\n\nWe converted the media stream from to , as expected the version of the media file is smaller than the however the created program is capable of:"
    },
    {
        "link": "https://ffmpeg.org/libavutil.html",
        "document": "The libavutil library is a utility library to aid portable multimedia programming. It contains safe portable string functions, random number generators, data structures, additional mathematics functions, cryptography and multimedia related functionality (like enumerations for pixel and sample formats). It is not a library for code needed by both libavcodec and libavformat.\n\nThe goals for this library is to be:\n\nIt should have few interdependencies and the possibility of disabling individual parts during . Both sources and objects should be small. It should have low CPU and memory usage. It should avoid useless features that almost no one needs.\n\nFor details about the authorship, see the Git history of the project (https://git.ffmpeg.org/ffmpeg), e.g. by typing the command in the FFmpeg source directory, or browsing the online repository at https://git.ffmpeg.org/ffmpeg.\n\nMaintainers for the specific components are listed in the file in the source code tree.\n\nThis document was generated on March 23, 2025 using makeinfo."
    },
    {
        "link": "https://github.com/leandromoreira/ffmpeg-libav-tutorial/blob/master/README.md",
        "document": "I was looking for a tutorial/book that would teach me how to start to use FFmpeg as a library (a.k.a. libav) and then I found the \"How to write a video player in less than 1k lines\" tutorial. Unfortunately it was deprecated, so I decided to write this one.\n\nMost of the code in here will be in C but don't worry: you can easily understand and apply it to your preferred language. FFmpeg libav has lots of bindings for many languages like python, go and even if your language doesn't have it, you can still support it through the (here's an example with Lua).\n\nWe'll start with a quick lesson about what is video, audio, codec and container and then we'll go to a crash course on how to use command line and finally we'll write code, feel free to skip directly to the section Learn FFmpeg libav the Hard Way.\n\nSome people used to say that the Internet video streaming is the future of the traditional TV, in any case, the FFmpeg is something that is worth studying.\n• Intro\n• video - what you see!\n\nIf you have a sequence series of images and change them at a given frequency (let's say 24 images per second), you will create an illusion of movement. In summary this is the very basic idea behind a video: a series of pictures / frames running at a given rate.\n\nAlthough a muted video can express a variety of feelings, adding sound to it brings more pleasure to the experience.\n\nSound is the vibration that propagates as a wave of pressure, through the air or any other transmission medium, such as a gas, liquid or solid.\n\nBut if we chose to pack millions of images in a single file and called it a movie, we might end up with a huge file. Let's do the math:\n\nSuppose we are creating a video with a resolution of (height x width) and that we'll spend per pixel (the minimal point at a screen) to encode the color (or 24 bit color, what gives us 16,777,216 different colors) and this video runs at and it is long.\n\nThis video would require approximately of storage or of bandwidth! That's why we need to use a CODEC.\n\nA single file that contains all the streams (mostly the audio and video) and it also provides synchronization and general metadata, such as title, resolution and etc.\n\nUsually we can infer the format of a file by looking at its extension: for instance a is probably a video using the container .\n\nTo work with multimedia we can use the AMAZING tool/library called FFmpeg. Chances are you already know/use it directly or indirectly (do you use Chrome?).\n\nIt has a command line program called , a very simple yet powerful binary. For instance, you can convert from to the container just by typing the follow command:\n\nWe just made a remuxing here, which is converting from one container to another one. Technically FFmpeg could also be doing a transcoding but we'll talk about that later.\n\nFFmpeg does have a documentation that does a great job of explaining how it works.\n\nTo make things short, the FFmpeg command line program expects the following argument format to perform its actions , where:\n\nThe parts 2, 3, 4 and 5 can be as many as you need. It's easier to understand this argument format in action:\n\nThis command takes an input file containing two streams (an audio encoded with CODEC and a video encoded using CODEC) and convert it to , changing its audio and video CODECs too.\n\nWe could simplify the command above but then be aware that FFmpeg will adopt or guess the default values for you. For instance when you just type what audio/video CODEC does it use to produce the ?\n\nWerner Robitza wrote a must read/execute tutorial about encoding and editing with FFmpeg.\n\nWhile working with audio/video we usually do a set of tasks with the media.\n\nWhat? the act of converting one of the streams (audio or video) from one CODEC to another one.\n\nWhy? sometimes some devices (TVs, smartphones, console and etc) doesn't support X but Y and newer CODECs provide better compression rate.\n\nHow? converting an (AVC) video to an (HEVC).\n\nWhat? the act of converting from one format (container) to another one.\n\nWhy? sometimes some devices (TVs, smartphones, console and etc) doesn't support X but Y and sometimes newer containers provide modern required features.\n\nWhat? the act of changing the bit rate, or producing other renditions.\n\nWhy? people will try to watch your video in a (edge) connection using a less powerful smartphone or in a Internet connection on their 4K TVs therefore you should offer more than one rendition of the same video with different bit rate.\n\nHow? producing a rendition with bit rate between 964K and 3856K.\n\nUsually we'll be using transrating with transsizing. Werner Robitza wrote another must read/execute series of posts about FFmpeg rate control.\n\nWhat? the act of converting from one resolution to another one. As said before transsizing is often used with transrating.\n\nWhy? reasons are about the same as for the transrating.\n\nWhat? the act of producing many resolutions (bit rates) and split the media into chunks and serve them via http.\n\nWhy? to provide a flexible media that can be watched on a low end smartphone or on a 4K TV, it's also easy to scale and deploy but it can add latency.\n\nHow? creating an adaptive WebM using DASH.\n\nPS: I stole this example from the Instructions to playback Adaptive WebM using DASH\n\nThere are many and many other usages for FFmpeg. I use it in conjunction with iMovie to produce/edit some videos for YouTube and you can certainly use it professionally.\n\nSince the FFmpeg is so useful as a command line tool to do essential tasks over the media files, how can we use it in our programs?\n\nFFmpeg is composed by several libraries that can be integrated into our own programs. Usually, when you install FFmpeg, it installs automatically all these libraries. I'll be referring to the set of these libraries as FFmpeg libav.\n\nThis hello world actually won't show the message in the terminal 👅 Instead we're going to print out information about the video, things like its format (container), duration, resolution, audio channels and, in the end, we'll decode some frames and save them as image files.\n\nBut before we start to code, let's learn how FFmpeg libav architecture works and how its components communicate with others.\n\nHere's a diagram of the process of decoding a video:\n\nYou'll first need to load your media file into a component called (the video container is also known as format). It actually doesn't fully load the whole file: it often only reads the header.\n\nOnce we loaded the minimal header of our container, we can access its streams (think of them as a rudimentary audio and video data). Each stream will be available in a component called .\n\nSuppose our video has two streams: an audio encoded with AAC CODEC and a video encoded with H264 (AVC) CODEC. From each stream we can extract pieces (slices) of data called packets that will be loaded into components named .\n\nThe data inside the packets are still coded (compressed) and in order to decode the packets, we need to pass them to a specific .\n\nThe will decode them into and finally, this component gives us the uncompressed frame. Noticed that the same terminology/process is used either by audio and video stream.\n\nSince some people were facing issues while compiling or running the examples we're going to use as our development/runner environment, we'll also use the big buck bunny video so if you don't have it locally just run the command .\n\nWe'll skip some details, but don't worry: the source code is available at github.\n\nWe're going to allocate memory to the component that will hold information about the format (container).\n\nNow we're going to open the file and read its header and fill the with minimal information about the format (notice that usually the codecs are not opened). The function used to do this is . It expects an , a and two optional arguments: the (if you pass , FFmpeg will guess the format) and the (which are the options to the demuxer).\n\nWe can print the format name and the media duration:\n\nTo access the , we need to read data from the media. The function does that. Now, the will hold the amount of streams and the will give us the stream (an ).\n\nNow we'll loop through all the streams.\n\nFor each stream, we're going to keep the , which describes the properties of a codec used by the stream .\n\nWith the codec properties we can look up the proper CODEC querying the function and find the registered decoder for the codec id and return an , the component that knows how to enCOde and DECode the stream.\n\nNow we can print information about the codecs.\n\nWith the codec, we can allocate memory for the , which will hold the context for our decode/encode process, but then we need to fill this codec context with CODEC parameters; we do that with .\n\nOnce we filled the codec context, we need to open the codec. We call the function and then we can use it.\n\nNow we're going to read the packets from the stream and decode them into frames but first, we need to allocate memory for both components, the and .\n\nLet's feed our packets from the streams with the function while it has packets.\n\nLet's send the raw data packet (compressed frame) to the decoder, through the codec context, using the function .\n\nAnd let's receive the raw data frame (uncompressed frame) from the decoder, through the same codec context, using the function .\n\nWe can print the frame number, the PTS, DTS, frame type and etc.\n\nFinally we can save our decoded frame into a simple gray image. The process is very simple, we'll use the where the index is related to the planes Y, Cb and Cr, we just picked (Y) to save our gray image.\n\nAnd voilà! Now we have a gray scale image with 2MB:\n\nBefore we move to code a transcoding example let's talk about timing, or how a video player knows the right time to play a frame.\n\nIn the last example, we saved some frames that can be seen here:\n\nWhen we're designing a video player we need to play each frame at a given pace, otherwise it would be hard to pleasantly see the video either because it's playing so fast or so slow.\n\nTherefore we need to introduce some logic to play each frame smoothly. For that matter, each frame has a presentation timestamp (PTS) which is an increasing number factored in a timebase that is a rational number (where the denominator is known as timescale) divisible by the frame rate (fps).\n\nIt's easier to understand when we look at some examples, let's simulate some scenarios.\n\nFor a and each PTS will increase therefore the PTS real time for each frame could be (supposing it started at 0):\n\nFor almost the same scenario but with a timebase equal to .\n\nFor a and each PTS will increase and the PTS time could be:\n\nNow with the we can find a way to render this synched with audio or with a system clock. The FFmpeg libav provides these info through its API:\n\nJust out of curiosity, the frames we saved were sent in a DTS order (frames: 1,6,4,2,3,5) but played at a PTS order (frames: 1,2,3,4,5). Also, notice how cheap are B-Frames in comparison to P or I-Frames.\n\nRemuxing is the act of changing from one format (container) to another, for instance, we can change a MPEG-4 video to a MPEG-TS one without much pain using FFmpeg:\n\nIt'll demux the mp4 but it won't decode or encode it ( ) and in the end, it'll mux it into a file. If you don't provide the format the ffmpeg will try to guess it based on the file's extension.\n\nThe general usage of FFmpeg or the libav follows a pattern/architecture or workflow:\n• protocol layer - it accepts an (a for instance but it could be a or input as well)\n• format layer - it its content, revealing mostly metadata and its streams\n• pixel layer - it can also apply some to the raw frames (like resizing)optional\n• and then it does the reverse path\n• codec layer - it (or or even ) the raw framesoptional\n• format layer - it (or ) the raw streams (the compressed data)\n• protocol layer - and finally the muxed data is sent to an (another file or maybe a network remote server)\n\nNow let's code an example using libav to provide the same effect as in .\n\nWe're going to read from an input ( ) and change it to another output ( ).\n\nWe start doing the usually allocate memory and open the input format. For this specific case, we're going to open an input file and allocate memory for an output file.\n\nWe're going to remux only the video, audio and subtitle types of streams so we're holding what streams we'll be using into an array of indexes.\n\nJust after we allocated the required memory, we're going to loop throughout all the streams and for each one we need to create new out stream into our output format context, using the avformat_new_stream function. Notice that we're marking all the streams that aren't video, audio or subtitle so we can skip them after.\n\nNow we can create the output file.\n\nAfter that, we can copy the streams, packet by packet, from our input to our output streams. We'll loop while it has packets ( ), for each packet we need to re-calculate the PTS and DTS to finally write it ( ) to our output format context.\n\nTo finalize we need to write the stream trailer to an output media file with av_write_trailer function.\n\nNow we're ready to test it and the first test will be a format (video container) conversion from a MP4 to a MPEG-TS video file. We're basically making the command line with libav.\n\nIt's working!!! don't you trust me?! you shouldn't, we can check it with :\n\nTo sum up what we did here in a graph, we can revisit our initial idea about how libav works but showing that we skipped the codec part.\n\nBefore we end this chapter I'd like to show an important part of the remuxing process, you can pass options to the muxer. Let's say we want to delivery MPEG-DASH format for that matter we need to use fragmented mp4 (sometimes referred as ) instead of MPEG-TS or plain MPEG-4.\n\nWith the command line we can do that easily.\n\nAlmost equally easy as the command line is the libav version of it, we just need to pass the options when write the output header, just before the packets copy.\n\nWe now can generate this fragmented mp4 file:\n\nBut to make sure that I'm not lying to you. You can use the amazing site/tool gpac/mp4box.js or the site http://mp4parser.com/ to see the differences, first load up the \"common\" mp4.\n\nAs you can see it has a single atom/box, this is place where the video and audio frames are. Now load the fragmented mp4 to see which how it spreads the boxes.\n\nIn this chapter, we're going to create a minimalist transcoder, written in C, that can convert videos coded in H264 to H265 using FFmpeg/libav library specifically libavcodec, libavformat, and libavutil.\n\nLet's start with the simple transmuxing operation and then we can build upon this code, the first step is to load the input file.\n\nNow we're going to set up the decoder, the will give us access to all the components and for each one of them, we can get their and create the particular and finally we can open the given codec so we can proceed to the decoding process.\n\nWe need to prepare the output media file for transmuxing as well, we first allocate memory for the output . We create each stream in the output format. In order to pack the stream properly, we copy the codec parameters from the decoder.\n\nWe set the flag which tells the encoder that it can use the global headers and finally we open the output file for write and persist the headers.\n\nWe're getting the 's from the decoder, adjusting the timestamps, and write the packet properly to the output file. Even though the function says \"write frame\" we are storing the packet. We finish the transmuxing process by writing the stream trailer to the file.\n\nThe previous section showed a simple transmuxer program, now we're going to add the capability to encode files, specifically we're going to enable it to transcode videos from to .\n\nAfter we prepared the decoder but before we arrange the output media file we're going to set up the encoder.\n• Create the video in the encoder,\n• Create the based in the created codec,\n• Set up basic attributes for the transcoding session, and\n• Open the codec and copy parameters from the context to the stream. and\n\nWe need to expand our decoding loop for the video stream transcoding:\n• Send the empty to the decoder,\n• Receive the compressed, based on our codec, ,\n• Set up the timestamp, and\n• Write it to the output file.\n\nWe converted the media stream from to , as expected the version of the media file is smaller than the however the created program is capable of:"
    },
    {
        "link": "https://ffmpeg.org/ffmpeg-all.html",
        "document": "is a universal media converter. It can read a wide variety of inputs - including live grabbing/recording devices - filter, and transcode them into a plethora of output formats.\n\nreads from an arbitrary number of inputs (which can be regular files, pipes, network streams, grabbing devices, etc.), specified by the option, and writes to an arbitrary number of outputs, which are specified by a plain output url. Anything found on the command line which cannot be interpreted as an option is considered to be an output url.\n\nEach input or output can, in principle, contain any number of elementary streams of different types (video/audio/subtitle/attachment/data), though the allowed stream counts and/or types may be limited by the container format. Selecting which streams from which inputs will go into which output is either done automatically or with the option (see the Stream selection chapter).\n\nTo refer to inputs/outputs in options, you must use their indices (0-based). E.g. the first input is , the second is , etc. Similarly, streams within an input/output are referred to by their indices. E.g. refers to the fourth stream in the third input or output. Also see the Stream specifiers chapter.\n\nAs a general rule, options are applied to the next specified file. Therefore, order is important, and you can have the same option on the command line multiple times. Each occurrence is then applied to the next input or output file. Exceptions from this rule are the global options (e.g. verbosity level), which should be specified first.\n\nDo not mix input and output files – first specify all input files, then all output files. Also do not mix options which belong to different files. All options apply ONLY to the next input or output file and are reset between files.\n• Convert an input media file to a different format, by re-encoding media streams:\n• Set the video bitrate of the output file to 64 kbit/s:\n• Force the frame rate of the output file to 24 fps:\n• Force the frame rate of the input file (valid for raw formats only) to 1 fps and the frame rate of the output file to 24 fps:\n\nThe format option may be needed for raw input files.\n\nbuilds a transcoding pipeline out of the components listed below. The program’s operation then consists of input data chunks flowing from the sources down the pipes towards the sinks, while being transformed by the components they encounter along the way.\n\nThe following kinds of components are available:\n• Demuxers (short for \"demultiplexers\") read an input source in order to extract\n• global properties such as metadata or chapters;\n• list of input elementary streams and their properties One demuxer instance is created for each option, and sends encoded packets to decoders or muxers. In other literature, demuxers are sometimes called splitters, because their main function is splitting a file into elementary streams (though some files only contain one elementary stream). A schematic representation of a demuxer looks like this: ┌──────────┬───────────────────────┐ │ demuxer │ │ packets for stream 0 ╞══════════╡ elementary stream 0 ├──────────────────────► │ │ │ │ global ├───────────────────────┤ │properties│ │ packets for stream 1 │ and │ elementary stream 1 ├──────────────────────► │ metadata │ │ │ ├───────────────────────┤ │ │ │ │ │ ........... │ │ │ │ │ ├───────────────────────┤ │ │ │ packets for stream N │ │ elementary stream N ├──────────────────────► │ │ │ └──────────┴───────────────────────┘ ▲ │ │ read from file, network stream, │ grabbing device, etc. │\n• Decoders receive encoded (compressed) packets for an audio, video, or subtitle elementary stream, and decode them into raw frames (arrays of pixels for video, PCM for audio). A decoder is typically associated with (and receives its input from) an elementary stream in a demuxer, but sometimes may also exist on its own (see Loopback decoders). A schematic representation of a decoder looks like this:\n• Filtergraphs process and transform raw audio or video frames. A filtergraph consists of one or more individual filters linked into a graph. Filtergraphs come in two flavors - simple and complex, configured with the and options, respectively. A simple filtergraph is associated with an output elementary stream; it receives the input to be filtered from a decoder and sends filtered output to that output stream’s encoder. A simple video filtergraph that performs deinterlacing (using the deinterlacer) followed by resizing (using the filter) can look like this: ┌────────────────────────┐ │ simple filtergraph │ frames from ╞════════════════════════╡ frames for a decoder │ ┌───────┐ ┌───────┐ │ an encoder ────────────►├─►│ yadif ├─►│ scale ├─►│────────────► │ └───────┘ └───────┘ │ └────────────────────────┘ A complex filtergraph is standalone and not associated with any specific stream. It may have multiple (or zero) inputs, potentially of different types (audio or video), each of which receiving data either from a decoder or another complex filtergraph’s output. It also has one or more outputs that feed either an encoder or another complex filtergraph’s input. The following example diagram represents a complex filtergraph with 3 inputs and 2 outputs (all video): Frames from second input are overlaid over those from the first. Frames from the third input are rescaled, then the duplicated into two identical streams. One of them is overlaid over the combined first two inputs, with the result exposed as the filtergraph’s first output. The other duplicate ends up being the filtergraph’s second output.\n• Encoders receive raw audio, video, or subtitle frames and encode them into encoded packets. The encoding (compression) process is typically lossy - it degrades stream quality to make the output smaller; some encoders are lossless, but at the cost of much higher output size. A video or audio encoder receives its input from some filtergraph’s output, subtitle encoders receive input from a decoder (since subtitle filtering is not supported yet). Every encoder is associated with some muxer’s output elementary stream and sends its output to that muxer. A schematic representation of an encoder looks like this:\n• Muxers (short for \"multiplexers\") receive encoded packets for their elementary streams from encoders (the transcoding path) or directly from demuxers (the streamcopy path), interleave them (when there is more than one elementary stream), and write the resulting bytes into the output file (or pipe, network stream, etc.). A schematic representation of a muxer looks like this: ┌──────────────────────┬───────────┐ packets for stream 0 │ │ muxer │ ──────────────────────►│ elementary stream 0 ╞═══════════╡ │ │ │ ├──────────────────────┤ global │ packets for stream 1 │ │properties │ ──────────────────────►│ elementary stream 1 │ and │ │ │ metadata │ ├──────────────────────┤ │ │ │ │ │ ........... │ │ │ │ │ ├──────────────────────┤ │ packets for stream N │ │ │ ──────────────────────►│ elementary stream N │ │ │ │ │ └──────────────────────┴─────┬─────┘ │ write to file, network stream, │ grabbing device, etc. │ │ ▼\n\nThe simplest pipeline in is single-stream streamcopy, that is copying one input elementary stream’s packets without decoding, filtering, or encoding them. As an example, consider an input file called with 3 elementary streams, from which we take the second and write it to file . A schematic representation of such a pipeline looks like this:\n\nThe above pipeline can be constructed with the following commandline:\n• there are no input options for this input;\n• there are two output options for this output:\n• selects the input stream to be used - from input with index 0 (i.e. the first one) the stream with index 1 (i.e. the second one);\n• selects the encoder, i.e. streamcopy with no decoding or encoding.\n\nStreamcopy is useful for changing the elementary stream count, container format, or modifying container-level metadata. Since there is no decoding or encoding, it is very fast and there is no quality loss. However, it might not work in some cases because of a variety of factors (e.g. certain information required by the target container is not available in the source). Applying filters is obviously also impossible, since filters work on decoded frames.\n\nMore complex streamcopy scenarios can be constructed - e.g. combining streams from two input files into a single output:\n\nthat can be built by the commandline\n\nThe output option is used twice here, creating two streams in the output file - one fed by the first input and one by the second. The single instance of the option selects streamcopy for both of those streams. You could also use multiple instances of this option together with Stream specifiers to apply different values to each stream, as will be demonstrated in following sections.\n\nA converse scenario is splitting multiple streams from a single input into multiple outputs:\n\nNote how a separate instance of the option is needed for every output file even though their values are the same. This is because non-global options (which is most of them) only apply in the context of the file before which they are placed.\n\nThese examples can of course be further generalized into arbitrary remappings of any number of inputs into any number of outputs.\n\nTranscoding is the process of decoding a stream and then encoding it again. Since encoding tends to be computationally expensive and in most cases degrades the stream quality (i.e. it is lossy), you should only transcode when you need to and perform streamcopy otherwise. Typical reasons to transcode are:\n• you want to feed the stream to something that cannot decode the original codec.\n\nNote that will transcode all audio, video, and subtitle streams unless you specify for them.\n\nConsider an example pipeline that reads an input file with one audio and one video stream, transcodes the video and copies the audio into a single output file. This can be schematically represented as follows\n\nand implemented with the following commandline:\n\nNote how it uses stream specifiers and to select input streams and apply different values of the option to them; see the Stream specifiers section for more details.\n\nWhen transcoding, audio and video streams can be filtered before encoding, with either a simple or complex filtergraph.\n\nSimple filtergraphs are those that have exactly one input and output, both of the same type (audio or video). They are configured with the per-stream option (with and aliases for (video) and (audio) respectively). Note that simple filtergraphs are tied to their output stream, so e.g. if you have multiple audio streams, will create a separate filtergraph for each one.\n\nTaking the trancoding example from above, adding filtering (and omitting audio, for clarity) makes it look like this:\n\nComplex filtergraphs are those which cannot be described as simply a linear processing chain applied to one stream. This is the case, for example, when the graph has more than one input and/or output, or when output stream type is different from input. Complex filtergraphs are configured with the option. Note that this option is global, since a complex filtergraph, by its nature, cannot be unambiguously associated with a single stream or file. Each instance of creates a new complex filtergraph, and there can be any number of them.\n\nA trivial example of a complex filtergraph is the filter, which has two video inputs and one video output, containing one video overlaid on top of the other. Its audio counterpart is the filter.\n\nWhile decoders are normally associated with demuxer streams, it is also possible to create \"loopback\" decoders that decode the output from some encoder and allow it to be fed back to complex filtergraphs. This is done with the directive, which takes as a parameter the index of the output stream that should be decoded. Every such directive creates a new loopback decoder, indexed with successive integers starting at zero. These indices should then be used to refer to loopback decoders in complex filtergraph link labels, as described in the documentation for .\n\nDecoding AVOptions can be passed to loopback decoders by placing them before , analogously to input/output options.\n\nE.g. the following example:\n• (line 2) encodes it with at low quality;\n• (line 4) places decoded video side by side with the original input video;\n• (line 5) combined video is then losslessly encoded and written into .\n\nSuch a transcoding pipeline can be represented with the following diagram:\n\nprovides the option for manual control of stream selection in each output file. Users can skip and let ffmpeg perform automatic stream selection as described below. The options can be used to skip inclusion of video, audio, subtitle and data streams respectively, whether manually mapped or automatically selected, except for those streams which are outputs of complex filtergraphs.\n\nThe sub-sections that follow describe the various rules that are involved in stream selection. The examples that follow next show how these rules are applied in practice.\n\nWhile every effort is made to accurately reflect the behavior of the program, FFmpeg is under continuous development and the code may have changed since the time of this writing.\n\nIn the absence of any map options for a particular output file, ffmpeg inspects the output format to check which type of streams can be included in it, viz. video, audio and/or subtitles. For each acceptable stream type, ffmpeg will pick one stream, when available, from among all the inputs.\n\nIt will select that stream based upon the following criteria:\n• for video, it is the stream with the highest resolution,\n• for audio, it is the stream with the most channels,\n• for subtitles, it is the first subtitle stream found but there’s a caveat. The output format’s default subtitle encoder can be either text-based or image-based, and only a subtitle stream of the same type will be chosen.\n\nIn the case where several streams of the same type rate equally, the stream with the lowest index is chosen.\n\nData or attachment streams are not automatically selected and can only be included using .\n\nWhen is used, only user-mapped streams are included in that output file, with one possible exception for filtergraph outputs described below.\n\nIf there are any complex filtergraph output streams with unlabeled pads, they will be added to the first output file. This will lead to a fatal error if the stream type is not supported by the output format. In the absence of the map option, the inclusion of these streams leads to the automatic stream selection of their types being skipped. If map options are present, these filtergraph streams are included in addition to the mapped streams.\n\nComplex filtergraph output streams with labeled pads must be mapped once and exactly once.\n\nStream handling is independent of stream selection, with an exception for subtitles described below. Stream handling is set via the option addressed to streams within a specific output file. In particular, codec options are applied by ffmpeg after the stream selection process and thus do not influence the latter. If no option is specified for a stream type, ffmpeg will select the default encoder registered by the output file muxer.\n\nAn exception exists for subtitles. If a subtitle encoder is specified for an output file, the first subtitle stream found of any type, text or image, will be included. ffmpeg does not validate if the specified encoder can convert the selected stream or if the converted stream is acceptable within the output format. This applies generally as well: when the user sets an encoder manually, the stream selection process cannot check if the encoded stream can be muxed into the output file. If it cannot, ffmpeg will abort and all output files will fail to be processed.\n\nThe following examples illustrate the behavior, quirks and limitations of ffmpeg’s stream selection methods.\n\nThey assume the following three input files.\n\nThere are three output files specified, and for the first two, no options are set, so ffmpeg will select streams for these two files automatically.\n\nis a Matroska container file and accepts video, audio and subtitle streams, so ffmpeg will try to select one of each type.\n\n For video, it will select from , which has the highest resolution among all the input video streams.\n\n For audio, it will select from , since it has the greatest number of channels.\n\n For subtitles, it will select from , which is the first subtitle stream from among and .\n\naccepts only audio streams, so only from is selected.\n\nFor , since a option is set, no automatic stream selection will occur. The option will select all audio streams from the second input . No other streams will be included in this output file.\n\nFor the first two outputs, all included streams will be transcoded. The encoders chosen will be the default ones registered by each output format, which may not match the codec of the selected input streams.\n\nFor the third output, codec option for audio streams has been set to , so no decoding-filtering-encoding operations will occur, or can occur. Packets of selected streams shall be conveyed from the input file and muxed within the output file.\n\nAlthough is a Matroska container file which accepts subtitle streams, only a video and audio stream shall be selected. The subtitle stream of is image-based and the default subtitle encoder of the Matroska muxer is text-based, so a transcode operation for the subtitles is expected to fail and hence the stream isn’t selected. However, in , a subtitle encoder is specified in the command and so, the subtitle stream is selected, in addition to the video stream. The presence of disables audio stream selection for .\n\nA filtergraph is setup here using the option and consists of a single video filter. The filter requires exactly two video inputs, but none are specified, so the first two available video streams are used, those of and . The output pad of the filter has no label and so is sent to the first output file . Due to this, automatic selection of the video stream is skipped, which would have selected the stream in . The audio stream with most channels viz. in , is chosen automatically. No subtitle stream is chosen however, since the MP4 format has no default subtitle encoder registered, and the user hasn’t specified a subtitle encoder.\n\nThe 2nd output file, , only accepts text-based subtitle streams. So, even though the first subtitle stream available belongs to , it is image-based and hence skipped. The selected stream, in , is the first text-based subtitle stream.\n\nThe above command will fail, as the output pad labelled has been mapped twice. None of the output files shall be processed.\n\nThis command above will also fail as the hue filter output has a label, , and hasn’t been mapped anywhere.\n\nThe command should be modified as follows,\n\nThe video stream from is sent to the hue filter, whose output is cloned once using the split filter, and both outputs labelled. Then a copy each is mapped to the first and third output files.\n\nThe overlay filter, requiring two video inputs, uses the first two unused video streams. Those are the streams from and . The overlay output isn’t labelled, so it is sent to the first output file , regardless of the presence of the option.\n\nThe aresample filter is sent the first unused audio stream, that of . Since this filter output is also unlabelled, it too is mapped to the first output file. The presence of only suppresses automatic or manual stream selection of audio streams, not outputs sent from filtergraphs. Both these mapped streams shall be ordered before the mapped stream in .\n\nThe video, audio and subtitle streams mapped to are entirely determined by automatic stream selection.\n\nconsists of the cloned video output from the hue filter and the first audio stream from . \n\n\n\nAll the numerical options, if not specified otherwise, accept a string representing a number as input, which may be followed by one of the SI unit prefixes, for example: ’K’, ’M’, or ’G’.\n\nIf ’i’ is appended to the SI unit prefix, the complete prefix will be interpreted as a unit prefix for binary multiples, which are based on powers of 1024 instead of powers of 1000. Appending ’B’ to the SI unit prefix multiplies the value by 8. This allows using, for example: ’KB’, ’MiB’, ’G’ and ’B’ as number suffixes.\n\nOptions which do not take arguments are boolean options, and set the corresponding value to true. They can be set to false by prefixing the option name with \"no\". For example using \"-nofoo\" will set the boolean option with name \"foo\" to false.\n\nOptions that take arguments support a special syntax where the argument given on the command line is interpreted as a path to the file from which the actual argument value is loaded. To use this feature, add a forward slash ’/’ immediately before the option name (after the leading dash). E.g.\n\nwill load a filtergraph description from the file named .\n\nSome options are applied per-stream, e.g. bitrate or codec. Stream specifiers are used to precisely specify which stream(s) a given option belongs to.\n\nA stream specifier is a string generally appended to the option name and separated from it by a colon. E.g. contains the stream specifier, which matches the second audio stream. Therefore, it would select the ac3 codec for the second audio stream.\n\nA stream specifier can match several streams, so that the option is applied to all of them. E.g. the stream specifier in matches all audio streams.\n\nAn empty stream specifier matches all streams. For example, or would copy all the streams without reencoding.\n\nPossible forms of stream specifiers are:\n\nThese options are shared amongst the ff* tools.\n\nThese options are provided directly by the libavformat, libavdevice and libavcodec libraries. To see the list of available AVOptions, use the option. They are separated into two categories:\n\nFor example to write an ID3v2.3 header instead of a default ID3v2.4 to an MP3 file, use the private option of the MP3 muxer:\n\nAll codec AVOptions are per-stream, and thus a stream specifier should be attached to them:\n\nIn the above example, a multichannel audio stream is mapped twice for output. The first instance is encoded with codec ac3 and bitrate 640k. The second instance is downmixed to 2 channels and encoded with codec aac. A bitrate of 128k is specified for it using absolute index of the output stream.\n\nNote: the syntax cannot be used for boolean AVOptions, use / .\n\nNote: the old undocumented way of specifying per-stream AVOptions by prepending v/a/s to the options name is now obsolete and will be removed soon.\n\nA preset file contains a sequence of = pairs, one for each line, specifying a sequence of options which would be awkward to specify on the command line. Lines starting with the hash (’#’) character are ignored and are used to provide comments. Check the directory in the FFmpeg source tree for examples.\n\nThere are two types of preset files: ffpreset and avpreset files.\n\nffpreset files are specified with the , , , and options. The option takes the filename of the preset instead of a preset name as input and can be used for any kind of codec. For the , , and options, the options specified in a preset file are applied to the currently selected codec of the same type as the preset option.\n\nThe argument passed to the , , and preset options identifies the preset file to use according to the following rules:\n\nFirst ffmpeg searches for a file named .ffpreset in the directories (if set), and , and in the datadir defined at configuration time (usually ) or in a folder along the executable on win32, in that order. For example, if the argument is , it will search for the file .\n\nIf no such file is found, then ffmpeg will search for a file named - .ffpreset in the above-mentioned directories, where is the name of the codec to which the preset file options will be applied. For example, if you select the video codec with and use , then it will search for the file .\n\navpreset files are specified with the option. They work similar to ffpreset files, but they only allow encoder- specific options. Therefore, an = pair specifying an encoder cannot be used.\n\nWhen the option is specified, ffmpeg will look for files with the suffix .avpreset in the directories (if set), and , and in the datadir defined at configuration time (usually ), in that order.\n\nFirst ffmpeg searches for a file named - .avpreset in the above-mentioned directories, where is the name of the codec to which the preset file options will be applied. For example, if you select the video codec with and use , then it will search for the file .\n\nIf no such file is found, then ffmpeg will search for a file named .avpreset in the same directories.\n\nThe and options enable generation of a file containing statistics about the generated video outputs.\n\nThe option controls the format version of the generated file.\n\nWith version the format is:\n\nWith version the format is:\n\nThe value corresponding to each key is described below:\n\nSee also the -stats_enc options for an alternative way to show encoding statistics.\n\nIf you specify the input format and device then ffmpeg can grab video and audio directly.\n\nOr with an ALSA audio source (mono input, card id 1) instead of OSS:\n\nNote that you must activate the right video source and channel before launching ffmpeg with any TV viewer such as xawtv by Gerd Knorr. You also have to set the audio recording levels correctly with a standard mixer.\n\nGrab the X11 display with ffmpeg via\n\n0.0 is display.screen number of your X11 server, same as the DISPLAY environment variable.\n\n0.0 is display.screen number of your X11 server, same as the DISPLAY environment variable. 10 is the x-offset and 20 the y-offset for the grabbing.\n\nAny supported file format and protocol can serve as input to ffmpeg:\n• You can use YUV files as input: It will use the files: The Y files use twice the resolution of the U and V files. They are raw files, without header. They can be generated by all decent video decoders. You must specify the size of the image with the option if ffmpeg cannot guess it.\n• You can input from a raw YUV420P file: test.yuv is a file containing raw YUV planar data. Each frame is composed of the Y plane followed by the U and V planes at half vertical and horizontal resolution.\n• You can output to a raw YUV420P file:\n• You can set several input files and output files: Converts the audio file a.wav and the raw YUV video file a.yuv to MPEG file a.mpg.\n• You can also do audio and video conversions at the same time:\n• You can encode to several formats at the same time and define a mapping from input stream to output streams: Converts a.wav to a.mp2 at 64 kbits and to b.mp2 at 128 kbits. ’-map file:index’ specifies which input stream is used for each output stream, in the order of the definition of output streams.\n• You can transcode decrypted VOBs: This is a typical DVD ripping example; the input is a VOB file, the output an AVI file with MPEG-4 video and MP3 audio. Note that in this command we use B-frames so the MPEG-4 stream is DivX5 compatible, and GOP size is 300 which means one intra frame every 10 seconds for 29.97fps input video. Furthermore, the audio stream is MP3-encoded so you need to enable LAME support by passing to configure. The mapping is particularly useful for DVD transcoding to get the desired audio language. NOTE: To see the supported input formats, use .\n• You can extract images from a video, or create a video from many images: This will extract one video frame per second from the video and will output them in files named , , etc. Images will be rescaled to fit the new WxH values. If you want to extract just a limited number of frames, you can use the above command in combination with the or option, or in combination with -ss to start extracting from a certain point in time. For creating a video from many images: The syntax specifies to use a decimal number composed of three digits padded with zeroes to express the sequence number. It is the same syntax supported by the C printf function, but only formats accepting a normal integer are suitable. When importing an image sequence, -i also supports expanding shell-like wildcard patterns (globbing) internally, by selecting the image2-specific option. For example, for creating a video from filenames matching the glob pattern :\n• You can put many streams of the same type in the output: The resulting output file will contain the first four streams from the input files in reverse order.\n• The four options lmin, lmax, mblmin and mblmax use ’lambda’ units, but you may use the QP2LAMBDA constant to easily convert from ’q’ units:\n\nThis section documents the syntax and formats employed by the FFmpeg libraries and tools.\n\nFFmpeg adopts the following quoting and escaping mechanism, unless explicitly specified. The following rules are applied:\n• ‘ ’ and ‘ ’ are special characters (respectively used for quoting and escaping). In addition to them, there might be other special characters depending on the specific syntax where the escaping and quoting are employed.\n• A special character is escaped by prefixing it with a ‘ ’.\n• All characters enclosed between ‘ ’ are included literally in the parsed string. The quote character ‘ ’ itself cannot be quoted, so you may need to close the quote and escape it.\n• Leading and trailing whitespaces, unless escaped or quoted, are removed from the parsed string.\n\nNote that you may need to add a second level of escaping when using the command line or a script, which depends on the syntax of the adopted shell language.\n\nThe function defined in can be used to parse a token quoted or escaped according to the rules defined above.\n\nThe tool in the FFmpeg source tree can be used to automatically quote or escape a string in a script.\n• Escape the string containing the special character:\n• The string above contains a quote, so the needs to be escaped when quoting it:\n• Include leading or trailing whitespaces using quoting: ' this string starts and ends with whitespaces '\n• Escaping and quoting can be mixed together:\n• To include a literal ‘ ’ you can use either escaping or quoting: 'c:\\foo' can be written as c:\\\\foo\n\nIf the value is \"now\" it takes the current time.\n\nTime is local time unless Z is appended, in which case it is interpreted as UTC. If the year-month-day part is not specified it takes the current year-month-day.\n\nThere are two accepted syntaxes for expressing time duration.\n\nexpresses the number of hours, the number of minutes for a maximum of 2 digits, and the number of seconds for a maximum of 2 digits. The at the end expresses decimal value for .\n\nexpresses the number of seconds, with the optional decimal part . The optional literal suffixes ‘ ’, ‘ ’ or ‘ ’ indicate to interpret the value as seconds, milliseconds or microseconds, respectively.\n\nIn both expressions, the optional ‘ ’ indicates negative duration.\n\nThe following examples are all valid time duration:\n\nSpecify the size of the sourced video, it may be a string of the form x , or the name of a size abbreviation.\n\nThe following abbreviations are recognized:\n\nSpecify the frame rate of a video, expressed as the number of frames generated per second. It has to be a string in the format / , an integer number, a float number or a valid video frame rate abbreviation.\n\nThe following abbreviations are recognized:\n\nA ratio can be expressed as an expression, or in the form : .\n\nNote that a ratio with infinite (1/0) or negative value is considered valid, so you should check on the returned value if you want to exclude those values.\n\nThe undefined value can be expressed using the \"0:0\" string.\n\nIt can be the name of a color as defined below (case insensitive match) or a sequence, possibly followed by @ and a string representing the alpha component.\n\nThe alpha component may be a string composed by \"0x\" followed by an hexadecimal number or a decimal number between 0.0 and 1.0, which represents the opacity value (‘ ’ or ‘ ’ means completely transparent, ‘ ’ or ‘ ’ completely opaque). If the alpha component is not specified then ‘ ’ is assumed.\n\nThe string ‘ ’ will result in a random color.\n\nThe following names of colors are recognized:\n\nA channel layout specifies the spatial disposition of the channels in a multi-channel audio stream. To specify a channel layout, FFmpeg makes use of a special syntax.\n\nIndividual channels are identified by an id, as given by the table below:\n\nStandard channel layout compositions can be specified by using the following identifiers:\n\nA custom channel layout can be specified as a sequence of terms, separated by ’+’. Each term can be:\n• the name of a single channel (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.), each optionally containing a custom name after a ’@’, (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.)\n\nA standard channel layout can be specified by the following:\n• the name of a single channel (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.)\n• the name of a standard channel layout (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.)\n• a number of channels, in decimal, followed by ’c’, yielding the default channel layout for that number of channels (see the function ). Note that not all channel counts have a default layout.\n• a number of channels, in decimal, followed by ’C’, yielding an unknown channel layout with the specified number of channels. Note that not all channel layout specification strings support unknown channel layouts.\n• a channel layout mask, in hexadecimal starting with \"0x\" (see the macros in .\n\nBefore libavutil version 53 the trailing character \"c\" to specify a number of channels was optional, but now it is required, while a channel layout mask can also be specified as a decimal number (if and only if not followed by \"c\" or \"C\").\n\nSee also the function defined in .\n\nWhen evaluating an arithmetic expression, FFmpeg uses an internal formula evaluator, implemented through the interface.\n\nAn expression may contain unary, binary operators, constants, and functions.\n\nTwo expressions and can be combined to form another expression \" ; \". and are evaluated in turn, and the new expression evaluates to the value of .\n\nThe following binary operators are available: , , , , .\n\nThe following unary operators are available: , .\n\nSome internal variables can be used to store and load intermediary results. They can be accessed using the and functions with an index argument varying from 0 to 9 to specify which internal variable to access.\n\nThe following functions are available:\n\nThe following constants are available:\n\nAssuming that an expression is considered \"true\" if it has a non-zero value, note that:\n\nFor example the construct:\n\nIn your C code, you can extend the list of unary and binary functions, and define recognized constants, so that they are available for your expressions.\n\nThe evaluator also recognizes the International System unit prefixes. If ’i’ is appended after the prefix, binary prefixes are used, which are based on powers of 1024 instead of powers of 1000. The ’B’ postfix multiplies the value by 8, and can be appended after a unit prefix or used alone. This allows using for example ’KB’, ’MiB’, ’G’ and ’B’ as number postfix.\n\nThe list of available International System prefixes follows, with indication of the corresponding powers of 10 and of 2.\n\nlibavcodec provides some generic global options, which can be set on all the encoders and decoders. In addition, each codec may support so-called private options, which are specific for a given codec.\n\nSometimes, a global option may only affect a specific kind of codec, and may be nonsensical or ignored by another, so you need to be aware of the meaning of the specified options. Also some options are meant only for decoding or encoding.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the options or using the API for programmatic use.\n\nDecoders are configured elements in FFmpeg which allow the decoding of multimedia streams.\n\nWhen you configure your FFmpeg build, all the supported native decoders are enabled by default. Decoders requiring an external library must be enabled manually via the corresponding option. You can list all available decoders using the configure option .\n\nYou can disable all the decoders with the configure option and selectively enable / disable single decoders with the options / .\n\nThe option of the ff* tools will display the list of enabled decoders.\n\nA description of some of the currently available video decoders follows.\n\nThe decoder supports MV-HEVC multiview streams with at most two views. Views to be output are selected by supplying a list of view IDs to the decoder (the option). This option may be set either statically before decoder init, or from the callback - useful for the case when the view count or IDs change dynamically during decoding.\n\nOnly the base layer is decoded by default.\n\nNote that if you are using the CLI tool, you should be using view specifiers as documented in its manual, rather than the options documented here.\n\nlibdav1d allows libavcodec to decode the AOMedia Video 1 (AV1) codec. Requires the presence of the libdav1d headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libdav1d wrapper.\n\nThis decoder allows libavcodec to decode AVS2 streams with davs2 library.\n\nlibuavs3d allows libavcodec to decode AVS3 streams. Requires the presence of the libuavs3d headers and library during configuration. You need to explicitly configure the build with .\n\nThe following option is supported by the libuavs3d wrapper.\n\nThis decoder requires the presence of the libxevd headers and library during configuration. You need to explicitly configure the build with .\n\nThe xevd project website is at https://github.com/mpeg5/xevd.\n\nThe following options are supported by the libxevd wrapper. The xevd-equivalent options or values are listed in parentheses for easy migration.\n\nTo get a more accurate and extensive documentation of the libxevd options, invoke the command or consult the libxevd documentation.\n\nThe following options are supported by all qsv decoders.\n\nA description of some of the currently available audio decoders follows.\n\nThis decoder implements part of ATSC A/52:2010 and ETSI TS 102 366, as well as the undocumented RealAudio 3 (a.k.a. dnet).\n\nThis decoder aims to implement the complete FLAC specification from Xiph.\n\nThis decoder generates wave patterns according to predefined sequences. Its use is purely internal and the format of the data it accepts is not publicly documented.\n\nlibcelt allows libavcodec to decode the Xiph CELT ultra-low delay audio codec. Requires the presence of the libcelt headers and library during configuration. You need to explicitly configure the build with .\n\nlibgsm allows libavcodec to decode the GSM full rate audio codec. Requires the presence of the libgsm headers and library during configuration. You need to explicitly configure the build with .\n\nThis decoder supports both the ordinary GSM and the Microsoft variant.\n\nlibilbc allows libavcodec to decode the Internet Low Bitrate Codec (iLBC) audio codec. Requires the presence of the libilbc headers and library during configuration. You need to explicitly configure the build with .\n\nThe following option is supported by the libilbc wrapper.\n\nlibopencore-amrnb allows libavcodec to decode the Adaptive Multi-Rate Narrowband audio codec. Using it requires the presence of the libopencore-amrnb headers and library during configuration. You need to explicitly configure the build with .\n\nAn FFmpeg native decoder for AMR-NB exists, so users can decode AMR-NB without this library.\n\nlibopencore-amrwb allows libavcodec to decode the Adaptive Multi-Rate Wideband audio codec. Using it requires the presence of the libopencore-amrwb headers and library during configuration. You need to explicitly configure the build with .\n\nAn FFmpeg native decoder for AMR-WB exists, so users can decode AMR-WB without this library.\n\nlibopus allows libavcodec to decode the Opus Interactive Audio Codec. Requires the presence of the libopus headers and library during configuration. You need to explicitly configure the build with .\n\nAn FFmpeg native decoder for Opus exists, so users can decode Opus without this library.\n\nImplements profiles A and C of the ARIB STD-B24 standard.\n\nYet another ARIB STD-B24 caption decoder using external libaribcaption library.\n\nImplements profiles A and C of the Japanse ARIB STD-B24 standard, Brazilian ABNT NBR 15606-1, and Philippines version of ISDB-T.\n\nRequires the presence of the libaribcaption headers and library (https://github.com/xqq/libaribcaption) during configuration. You need to explicitly configure the build with . If both libaribb24 and libaribcaption are enabled, libaribcaption decoder precedes.\n\nThis codec decodes the bitmap subtitles used in DVDs; the same subtitles can also be found in VobSub file pairs and in some Matroska files.\n\nLibzvbi allows libavcodec to decode DVB teletext pages and DVB teletext subtitles. Requires the presence of the libzvbi headers and library during configuration. You need to explicitly configure the build with .\n\nEncoders are configured elements in FFmpeg which allow the encoding of multimedia streams.\n\nWhen you configure your FFmpeg build, all the supported native encoders are enabled by default. Encoders requiring an external library must be enabled manually via the corresponding option. You can list all available encoders using the configure option .\n\nYou can disable all the encoders with the configure option and selectively enable / disable single encoders with the options / .\n\nThe option of the ff* tools will display the list of enabled encoders.\n\nA description of some of the currently available audio encoders follows.\n\nThis encoder is the default AAC encoder, natively implemented into FFmpeg.\n\nThese encoders implement part of ATSC A/52:2010 and ETSI TS 102 366.\n\nThe encoder uses floating-point math, while the encoder only uses fixed-point integer math. This does not mean that one is always faster, just that one or the other may be better suited to a particular system. The encoder is not the default codec for any of the output formats, so it must be specified explicitly using the option in order to use it.\n\nThe AC-3 metadata options are used to set parameters that describe the audio, but in most cases do not affect the audio encoding itself. Some of the options do directly affect or influence the decoding and playback of the resulting bitstream, while others are just for informational purposes. A few of the options will add bits to the output stream that could otherwise be used for audio data, and will thus affect the quality of the output. Those will be indicated accordingly with a note in the option list below.\n\nThese parameters are described in detail in several publicly-available documents.\n• A/54 - Guide to the Use of the ATSC Digital Television Standard\n\nAudio Production Information is optional information describing the mixing environment. Either none or both of the fields are written to the bitstream.\n\nThe extended bitstream options are part of the Alternate Bit Stream Syntax as specified in Annex D of the A/52:2010 standard. It is grouped into 2 parts. If any one parameter in a group is specified, all values in that group will be written to the bitstream. Default values are used for those that are written but have not been specified. If the mixing levels are written, the decoder will use these values instead of the ones specified in the and options if it supports the Alternate Bit Stream Syntax.\n\nThese options are only valid for the floating-point encoder and do not exist for the fixed-point encoder due to the corresponding features not being implemented in fixed-point.\n\nThe following options are supported by FFmpeg’s FFv1 encoder.\n\nThe following options are supported by FFmpeg’s flac encoder.\n\nThis is a native FFmpeg encoder for the Opus format. Currently, it’s in development and only implements the CELT part of the codec. Its quality is usually worse and at best is equal to the libopus encoder.\n\nThe libfdk-aac library is based on the Fraunhofer FDK AAC code from the Android project.\n\nRequires the presence of the libfdk-aac headers and library during configuration. You need to explicitly configure the build with . The library is also incompatible with GPL, so if you allow the use of GPL, you should configure with .\n\nThis encoder has support for the AAC-HE profiles.\n\nVBR encoding, enabled through the or options, is experimental and only works with some combinations of parameters.\n\nSupport for encoding 7.1 audio is only available with libfdk-aac 0.1.3 or higher.\n\nFor more information see the fdk-aac project at http://sourceforge.net/p/opencore-amr/fdk-aac/.\n\nThe following options are mapped on the shared FFmpeg codec options.\n\nThe following are private options of the libfdk_aac encoder.\n• Use to convert an audio file to VBR AAC in an M4A (MP4) container:\n• Use to convert an audio file to CBR 64k kbps AAC, using the High-Efficiency AAC profile:\n\nRequires the presence of the liblc3 headers and library during configuration. You need to explicitly configure the build with .\n\nThis encoder has support for the Bluetooth SIG LC3 codec for the LE Audio protocol, and the following features of LC3plus:\n\nFor more information see the liblc3 project at https://github.com/google/liblc3.\n\nThe following options are mapped on the shared FFmpeg codec options.\n\nRequires the presence of the libmp3lame headers and library during configuration. You need to explicitly configure the build with .\n\nSee libshine for a fixed-point MP3 encoder, although with a lower quality.\n\nThe following options are supported by the libmp3lame wrapper. The -equivalent of the options are listed in parentheses.\n\nRequires the presence of the libopencore-amrnb headers and library during configuration. You need to explicitly configure the build with .\n\nThis is a mono-only encoder. Officially it only supports 8000Hz sample rate, but you can override it by setting to ‘ ’ or lower.\n\nRequires the presence of the libopus headers and library during configuration. You need to explicitly configure the build with .\n\nMost libopus options are modelled after the utility from opus-tools. The following is an option mapping chart describing options supported by the libopus wrapper, and their -equivalent in parentheses.\n\nShine is a fixed-point MP3 encoder. It has a far better performance on platforms without an FPU, e.g. armel CPUs, and some phones and tablets. However, as it is more targeted on performance than quality, it is not on par with LAME and other production-grade encoders quality-wise. Also, according to the project’s homepage, this encoder may not be free of bugs as the code was written a long time ago and the project was dead for at least 5 years.\n\nThis encoder only supports stereo and mono input. This is also CBR-only.\n\nThe original project (last updated in early 2007) is at http://sourceforge.net/projects/libshine-fxp/. We only support the updated fork by the Savonet/Liquidsoap project at https://github.com/savonet/shine.\n\nRequires the presence of the libshine headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libshine wrapper. The -equivalent of the options are listed in parentheses.\n\nRequires the presence of the libtwolame headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libtwolame wrapper. The -equivalent options follow the FFmpeg ones and are in parentheses.\n\nRequires the presence of the libvo-amrwbenc headers and library during configuration. You need to explicitly configure the build with .\n\nThis is a mono-only encoder. Officially it only supports 16000Hz sample rate, but you can override it by setting to ‘ ’ or lower.\n\nRequires the presence of the libvorbisenc headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libvorbis wrapper. The -equivalent of the options are listed in parentheses.\n\nTo get a more accurate and extensive documentation of the libvorbis options, consult the libvorbisenc’s and ’s documentations. See http://xiph.org/vorbis/, http://wiki.xiph.org/Vorbis-tools, and oggenc(1).\n\nThe equivalent options for command line utility are listed in parentheses.\n\nThe following shared options are effective for this encoder. Only special notes about this particular encoder will be documented here. For the general meaning of the options, see the Codec Options chapter.\n\nA description of some of the currently available video encoders follows.\n\nThe native jpeg 2000 encoder is lossy by default, the option can be used to set the encoding quality. Lossless encoding can be selected with .\n\nRequires the presence of the rav1e headers and library during configuration. You need to explicitly configure the build with .\n\nRequires the presence of the libaom headers and library during configuration. You need to explicitly configure the build with .\n\nThe wrapper supports the following standard libavcodec options:\n\nThe wrapper also has some specific options:\n\nRequires the presence of the SVT-AV1 headers and library during configuration. You need to explicitly configure the build with .\n\nRequires the presence of the libjxl headers and library during configuration. You need to explicitly configure the build with .\n\nThe libjxl wrapper supports the following options:\n\nRequires the presence of the libkvazaar headers and library during configuration. You need to explicitly configure the build with .\n\nThis encoder requires the presence of the libopenh264 headers and library during configuration. You need to explicitly configure the build with . The library is detected using .\n\nFor more information about the library see http://www.openh264.org.\n\nThe following FFmpeg global options affect the configurations of the libopenh264 encoder.\n\nRequires the presence of the libtheora headers and library during configuration. You need to explicitly configure the build with .\n\nFor more information about the libtheora project see http://www.theora.org/.\n\nThe following global options are mapped to internal libtheora options which affect the quality and the bitrate of the encoded stream.\n\nRequires the presence of the libvpx headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libvpx wrapper. The -equivalent options or values are listed in parentheses for easy migration.\n\nTo reduce the duplication of documentation, only the private options and some others requiring special attention are documented here. For the documentation of the undocumented generic options, see the Codec Options chapter.\n\nTo get more documentation of the libvpx options, invoke the command , or . Further information is available in the libvpx API documentation.\n\nFor more information about libvpx see: http://www.webmproject.org/\n\nThis encoder requires the presence of the libvvenc headers and library during configuration. You need to explicitly configure the build with .\n\nThe VVenC project website is at https://github.com/fraunhoferhhi/vvenc.\n\nVVenC supports only 10-bit color spaces as input. But the internal (encoded) bit depth can be set to 8-bit or 10-bit at runtime.\n\nlibwebp is Google’s official encoder for WebP images. It can encode in either lossy or lossless mode. Lossy images are essentially a wrapper around a VP8 frame. Lossless images are a separate codec developed by Google.\n\nCurrently, libwebp only supports YUV420 for lossy and RGB for lossless due to limitations of the format and libwebp. Alpha is supported for either mode. Because of API limitations, if RGB is passed in when encoding lossy or YUV is passed in for encoding lossless, the pixel format will automatically be converted using functions from libwebp. This is not ideal and is done only for convenience.\n\nThis encoder requires the presence of the libx264 headers and library during configuration. You need to explicitly configure the build with .\n\nlibx264 supports an impressive number of features, including 8x8 and 4x4 adaptive spatial transform, adaptive B-frame placement, CAVLC/CABAC entropy coding, interlacing (MBAFF), lossless mode, psy optimizations for detail retention (adaptive quantization, psy-RD, psy-trellis).\n\nMany libx264 encoder options are mapped to FFmpeg global codec options, while unique encoder options are provided through private options. Additionally the and private options allows one to pass a list of key=value tuples as accepted by the libx264 function.\n\nThe x264 project website is at http://www.videolan.org/developers/x264.html.\n\nThe libx264rgb encoder is the same as libx264, except it accepts packed RGB pixel formats as input instead of YUV.\n\nx264 supports 8- to 10-bit color spaces. The exact bit depth is controlled at x264’s configure time.\n\nThe following options are supported by the libx264 wrapper. The -equivalent options or values are listed in parentheses for easy migration.\n\nTo reduce the duplication of documentation, only the private options and some others requiring special attention are documented here. For the documentation of the undocumented generic options, see the Codec Options chapter.\n\nTo get a more accurate and extensive documentation of the libx264 options, invoke the command or consult the libx264 documentation.\n\nIn the list below, note that the option name is shown in parentheses after the libavcodec corresponding name, in case there is a direct mapping.\n\nEncoding ffpresets for common usages are provided so they can be used with the general presets system (e.g. passing the option).\n\nThis encoder requires the presence of the libx265 headers and library during configuration. You need to explicitly configure the build with .\n\nThis encoder requires the presence of the libxavs2 headers and library during configuration. You need to explicitly configure the build with .\n\nThe following standard libavcodec options are used:\n\nThe encoder also has its own specific options:\n\neXtra-fast Essential Video Encoder (XEVE) MPEG-5 EVC encoder wrapper. The xeve-equivalent options or values are listed in parentheses for easy migration.\n\nThis encoder requires the presence of the libxeve headers and library during configuration. You need to explicitly configure the build with .\n\nThe xeve project website is at https://github.com/mpeg5/xeve.\n\nThe following options are supported by the libxeve wrapper. The xeve-equivalent options or values are listed in parentheses for easy migration.\n\nThis encoder requires the presence of the libxvidcore headers and library during configuration. You need to explicitly configure the build with .\n\nThe native encoder supports the MPEG-4 Part 2 format, so users can encode to this format without this library.\n\nThe following options are supported by the libxvid wrapper. Some of the following options are listed but are not documented, and correspond to shared codec options. See the Codec Options chapter for their documentation. The other shared options which are not listed have no effect for the libxvid encoder.\n\nThis provides wrappers to encoders (both audio and video) in the MediaFoundation framework. It can access both SW and HW encoders. Video encoders can take input in either of nv12 or yuv420p form (some encoders support both, some support only either - in practice, nv12 is the safer choice, especially among HW encoders).\n\nMicrosoft RLE aka MSRLE encoder. Only 8-bit palette mode supported. Compatible with Windows 3.1 and Windows 95.\n\nFFmpeg contains 2 ProRes encoders, the prores-aw and prores-ks encoder. The used encoder can be chosen with the option.\n\nIn the default mode of operation the encoder has to honor frame constraints (i.e. not produce frames with size bigger than requested) while still making output picture as good as possible. A frame containing a lot of small details is harder to compress and the encoder would spend more time searching for appropriate quantizers for each slice.\n\nFor the fastest encoding speed set the parameter (4 is the recommended value) and do not set a size constraint.\n\nThe ratecontrol method is selected as follows:\n• When is specified, a quality-based mode is used. Specifically this means either\n• - - constant quantizer scale, when the codec flag is also set (the ffmpeg option).\n• - - intelligent constant quality with lookahead, when the option is also set.\n• - – intelligent constant quality otherwise. For the ICQ modes, global quality range is 1 to 51, with 1 being the best quality.\n• Otherwise when the desired average bitrate is specified with the option, a bitrate-based mode is used.\n• - - VBR with lookahead, when the option is specified.\n• - - video conferencing mode, when the option is set.\n• - - constant bitrate, when is specified and equal to the average bitrate.\n• - - variable bitrate, when is specified, but is higher than the average bitrate.\n• - - average VBR mode, when is not specified, both and are set to non-zero. This mode is available for H264 and HEVC on Windows.\n• Otherwise the default ratecontrol method is used.\n\nNote that depending on your system, a different mode than the one you specified may be selected by the encoder. Set the verbosity level to or higher to see the actual settings used by the QSV runtime.\n\nAdditional libavcodec global options are mapped to MSDK options as follows:\n• For the mode, the and set the difference between and , and and respectively.\n• Setting the option to the value will make the H.264 encoder use CAVLC instead of CABAC.\n\nFollowing options are used by all qsv encoders.\n\nFollowing options can be used durning qsv encoding.\n\nThese options are used by h264_qsv\n\nThese options are used by hevc_qsv\n\nThese options are used by mpeg2_qsv\n\nThese options are used by vp9_qsv\n\nThese options are used by av1_qsv (requires libvpl).\n\nThese encoders only accept input in VAAPI hardware surfaces. If you have input in software frames, use the filter to upload them to the GPU.\n\nThe following standard libavcodec options are used:\n• If not set, this will be determined automatically from the format of the input frames and the profiles supported by the driver.\n\nAll encoders support the following options:\n\nEach encoder also has its own specific options:\n\nThis format is used by the broadcast vendor Vizrt for quick texture streaming. Advanced features of the format such as LZW compression of texture data or generation of mipmaps are not supported.\n\nSMPTE VC-2 (previously BBC Dirac Pro). This codec was primarily aimed at professional broadcasting but since it supports yuv420, yuv422 and yuv444 at 8 (limited range or full range), 10 or 12 bits, this makes it suitable for other tasks which require low overhead and low compression (like screen recording).\n\nThis codec encodes the bitmap subtitle format that is used in DVDs. Typically they are stored in VOBSUB file pairs (*.idx + *.sub), and they can also be used in Matroska files.\n\nWhen you configure your FFmpeg build, all the supported bitstream filters are enabled by default. You can list all available ones using the configure option .\n\nYou can disable all the bitstream filters using the configure option , and selectively enable any bitstream filter using the option , or you can disable a particular bitstream filter using the option .\n\nThe option of the ff* tools will display the list of all the supported bitstream filters included in your build.\n\nThe ff* tools have a -bsf option applied per stream, taking a comma-separated list of filters, whose parameters follow the filter name after a ’=’.\n\nBelow is a description of the currently available bitstream filters, with their parameters, if any.\n\nThis filter creates an MPEG-4 AudioSpecificConfig from an MPEG-2/4 ADTS header and removes the ADTS header.\n\nThis filter is required for example when copying an AAC stream from a raw ADTS AAC or an MPEG-TS container to MP4A-LATM, to an FLV file, or to MOV/MP4 files and related formats such as 3GP or M4A. Please note that it is auto-inserted for MP4A-LATM and MOV/MP4 and related formats.\n\nRemove zero padding at the end of a packet.\n\nExtract the core from a DCA/DTS stream, dropping extensions such as DTS-HD.\n\nAdd extradata to the beginning of the filtered packets except when said packets already exactly begin with the extradata that is intended to be added.\n\nIf not specified it is assumed ‘ ’.\n\nFor example the following command forces a global header (thus disabling individual packet headers) in the H.264 packets generated by the encoder, but corrects them by adding the header stored in extradata to the key packets:\n\nBlocks in DV which are marked as damaged are replaced by blocks of the specified color.\n\nCertain codecs allow the long-term headers (e.g. MPEG-2 sequence headers, or H.264/HEVC (VPS/)SPS/PPS) to be transmitted either \"in-band\" (i.e. as a part of the bitstream containing the coded frames) or \"out of band\" (e.g. on the container level). This latter form is called \"extradata\" in FFmpeg terminology.\n\nThis bitstream filter detects the in-band headers and makes them available as extradata.\n\nRemove units with types in or not in a given set from the stream.\n\nThe types used by pass_types and remove_types correspond to NAL unit types (nal_unit_type) in H.264, HEVC and H.266 (see Table 7-1 in the H.264 and HEVC specifications or Table 5 in the H.266 specification), to marker values for JPEG (without 0xFF prefix) and to start codes without start code prefix (i.e. the byte following the 0x000001) for MPEG-2. For VP8 and VP9, every unit has type zero.\n\nExtradata is unchanged by this transformation, but note that if the stream contains inline parameter sets then the output may be unusable if they are removed.\n\nFor example, to remove all non-VCL NAL units from an H.264 stream:\n\nTo remove all AUDs, SEI and filler from an H.265 stream:\n\nTo remove all user data from a MPEG-2 stream, including Closed Captions:\n\nTo remove all SEI from a H264 stream, including Closed Captions:\n\nTo remove all prefix and suffix SEI from a HEVC stream, including Closed Captions and dynamic HDR:\n\nExtract Rgb or Alpha part of an HAPQA file, without recompression, in order to create an HAPQ or an HAPAlphaOnly file.\n\nConvert an H.264 bitstream from length prefixed mode to start code prefixed mode (as defined in the Annex B of the ITU-T H.264 specification).\n\nThis is required by some streaming formats, typically the MPEG-2 transport stream format (muxer ).\n\nFor example to remux an MP4 file containing an H.264 stream to mpegts format with , you can use the command:\n\nPlease note that this filter is auto-inserted for MPEG-TS (muxer ) and raw H.264 (muxer ) output formats.\n\nThis applies a specific fixup to some Blu-ray BDMV H264 streams which contain redundant PPSs. The PPSs modify irrelevant parameters of the stream, confusing other transformations which require the correct extradata.\n\nThe encoder used on these impacted streams adds extra PPSs throughout the stream, varying the initial QP and whether weighted prediction was enabled. This causes issues after copying the stream into a global header container, as the starting PPS is not suitable for the rest of the stream. One side effect, for example, is seeking will return garbled output until a new PPS appears.\n\nThis BSF removes the extra PPSs and rewrites the slice headers such that the stream uses a single leading PPS in the global header, which resolves the issue.\n\nConvert an HEVC/H.265 bitstream from length prefixed mode to start code prefixed mode (as defined in the Annex B of the ITU-T H.265 specification).\n\nThis is required by some streaming formats, typically the MPEG-2 transport stream format (muxer ).\n\nFor example to remux an MP4 file containing an HEVC stream to mpegts format with , you can use the command:\n\nPlease note that this filter is auto-inserted for MPEG-TS (muxer ) and raw HEVC/H.265 (muxer or ) output formats.\n\nModifies the bitstream to fit in MOV and to be usable by the Final Cut Pro decoder. This filter only applies to the mpeg2video codec, and is likely not needed for Final Cut Pro 7 and newer with the appropriate .\n\nFor example, to remux 30 MB/sec NTSC IMX to MOV:\n\nMJPEG is a video codec wherein each video frame is essentially a JPEG image. The individual frames can be extracted without loss, e.g. by\n\nUnfortunately, these chunks are incomplete JPEG images, because they lack the DHT segment required for decoding. Quoting from http://www.digitalpreservation.gov/formats/fdd/fdd000063.shtml:\n\nAvery Lee, writing in the rec.video.desktop newsgroup in 2001, commented that \"MJPEG, or at least the MJPEG in AVIs having the MJPG fourcc, is restricted JPEG with a fixed – and *omitted* – Huffman table. The JPEG must be YCbCr colorspace, it must be 4:2:2, and it must use basic Huffman encoding, not arithmetic or progressive. . . . You can indeed extract the MJPEG frames and decode them with a regular JPEG decoder, but you have to prepend the DHT segment to them, or else the decoder won’t have any idea how to decompress the data. The exact table necessary is given in the OpenDML spec.\"\n\nThis bitstream filter patches the header of frames extracted from an MJPEG stream (carrying the AVI1 header ID and lacking a DHT segment) to produce fully qualified JPEG images.\n\nAdd an MJPEG A header to the bitstream, to enable decoding by Quicktime.\n\nExtract a representable text file from MOV subtitles, stripping the metadata header from each subtitle packet.\n\nSee also the text2movsub filter.\n\nDivX-style packed B-frames are not valid MPEG-4 and were only a workaround for the broken Video for Windows subsystem. They use more space, can cause minor AV sync issues, require more CPU power to decode (unless the player has some decoded picture queue to compensate the 2,0,2,0 frame per packet style) and cause trouble if copied into a standard container like mp4 or mpeg-ps/ts, because MPEG-4 decoders may not be able to decode them, since they are not valid MPEG-4.\n\nFor example to fix an AVI file containing an MPEG-4 stream with DivX-style packed B-frames using , you can use the command:\n\nDamages the contents of packets or simply drops them without damaging the container. Can be used for fuzzing or testing error resilience/concealment.\n\nBoth and accept expressions containing the following variables:\n\nApply modification to every byte but don’t drop any packets.\n\nDrop every video packet not marked as a keyframe after timestamp 30s but do not modify any of the remaining packets.\n\nDrop one second of audio every 10 seconds and add some random noise to the rest.\n\nThis bitstream filter passes the packets through unchanged.\n\nRepacketize PCM audio to a fixed number of samples per packet or a fixed packet rate per second. This is similar to the (ffmpeg-filters)asetnsamples audio filter but works on audio packets instead of audio frames.\n\nYou can generate the well known 1602-1601-1602-1601-1602 pattern of 48kHz audio for NTSC frame rate using the option.\n\nMerge a sequence of PGS Subtitle segments ending with an \"end of display set\" segment into a single packet.\n\nThis is required by some containers that support PGS subtitles (muxer ).\n\nSet Rec709 colorspace for each frame of the file\n\nSet Hybrid Log-Gamma parameters for each frame of the file\n\nIt accepts the following parameter:\n\nIt accepts the following parameters:\n\nThe expressions are evaluated through the eval API and can contain the following constants:\n\nFor example, to set PTS equal to DTS (not recommended if B-frames are involved):\n\nLog basic packet information. Mainly useful for testing, debugging, and development.\n\nConvert text subtitles to MOV subtitles (as used by the codec) with metadata headers.\n\nSee also the mov2textsub filter.\n\nLog trace output containing all syntax elements in the coded stream headers (everything above the level of individual coded blocks). This can be useful for debugging low-level stream issues.\n\nSupports AV1, H.264, H.265, (M)JPEG, MPEG-2 and VP9, but depending on the build only a subset of these may be available.\n\nMerge VP9 invisible (alt-ref) frames back into VP9 superframes. This fixes merging of split/segmented VP9 streams where the alt-ref frame was split from its visible counterpart.\n\nGiven a VP9 stream with correct timestamps but possibly out of order, insert additional show-existing-frame packets to correct the ordering.\n\nThe libavformat library provides some generic global options, which can be set on all the muxers and demuxers. In addition each muxer or demuxer may support so-called private options, which are specific for that component.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the options or using the API for programmatic use.\n\nThe list of supported options follows:\n\nFormat stream specifiers allow selection of one or more streams that match specific properties.\n\nThe exact semantics of stream specifiers is defined by the function declared in the header and documented in the (ffmpeg)Stream specifiers section in the ffmpeg(1) manual.\n\nDemuxers are configured elements in FFmpeg that can read the multimedia streams from a particular type of file.\n\nWhen you configure your FFmpeg build, all the supported demuxers are enabled by default. You can list all available ones using the configure option .\n\nYou can disable all the demuxers using the configure option , and selectively enable a single demuxer with the option , or disable it with the option .\n\nThe option of the ff* tools will display the list of enabled demuxers. Use to view a combined list of enabled demuxers and muxers.\n\nThe description of some of the currently available demuxers follows.\n\nThis demuxer is used to demux Audible Format 2, 3, and 4 (.aa) files.\n\nThis demuxer is used to demux an ADTS input containing a single AAC stream alongwith any ID3v1/2 or APE tags in it.\n\nThis demuxer is used to demux APNG files. All headers, but the PNG signature, up to (but not including) the first fcTL chunk are transmitted as extradata. Frames are then split as being all the chunks between two fcTL ones, or between the last fcTL and IEND chunks.\n\nThis demuxer is used to demux ASF files and MMS network streams.\n\nThis demuxer reads a list of files and other directives from a text file and demuxes them one after the other, as if all their packets had been muxed together.\n\nThe timestamps in the files are adjusted so that the first file starts at 0 and each next file starts where the previous one finishes. Note that it is done globally and may cause gaps if all streams do not have exactly the same length.\n\nAll files must have the same streams (same codecs, same time base, etc.).\n\nThe duration of each file is used to adjust the timestamps of the next file: if the duration is incorrect (because it was computed using the bit-rate or because the file is truncated, for example), it can cause artifacts. The directive can be used to override the duration stored in each file.\n\nThe script is a text file in extended-ASCII, with one directive per line. Empty lines, leading spaces and lines starting with ’#’ are ignored. The following directive is recognized:\n\nThis demuxer accepts the following option:\n• Use absolute filenames and include some comments: # my first filename file /mnt/share/file-1.wav # my second filename including whitespace file '/mnt/share/file 2.wav' # my third filename including whitespace plus single quote file '/mnt/share/file 3'\\''.wav'\n• Allow for input format auto-probing, use safe filenames and set the duration of the first file:\n\nThis demuxer presents all AVStreams found in the manifest. By setting the discard flags on AVStreams the caller can decide which streams to actually receive. Each stream mirrors the and properties from the as metadata keys named \"id\" and \"variant_bitrate\" respectively.\n\nThis demuxer accepts the following option:\n\nCan directly ingest DVD titles, specifically sequential PGCs, into a conversion pipeline. Menu assets, such as background video or audio, can also be demuxed given the menu’s coordinates (at best effort).\n\nBlock devices (DVD drives), ISO files, and directory structures are accepted. Activate with in front of one of these inputs.\n\nThis demuxer does NOT have decryption code of any kind. You are on your own working with encrypted DVDs, and should not expect support on the matter.\n\nUnderlying playback is handled by libdvdnav, and structure parsing by libdvdread. FFmpeg must be built with GPL library support available as well as the configure switches and .\n\nYou will need to provide either the desired \"title number\" or exact PGC/PG coordinates. Many open-source DVD players and tools can aid in providing this information. If not specified, the demuxer will default to title 1 which works for many discs. However, due to the flexibility of the format, it is recommended to check manually. There are many discs that are authored strangely or with invalid headers.\n\nIf the input is a real DVD drive, please note that there are some drives which may silently fail on reading bad sectors from the disc, returning random bits instead which is effectively corrupt data. This is especially prominent on aging or rotting discs. A second pass and integrity checks would be needed to detect the corruption. This is not an FFmpeg issue.\n\nDVD-Video is not a directly accessible, linear container format in the traditional sense. Instead, it allows for complex and programmatic playback of carefully muxed MPEG-PS streams that are stored in headerless VOB files. To the end-user, these streams are known simply as \"titles\", but the actual logical playback sequence is defined by one or more \"PGCs\", or Program Group Chains, within the title. The PGC is in turn comprised of multiple \"PGs\", or Programs\", which are the actual video segments (and for a typical video feature, sequentially ordered). The PGC structure, along with stream layout and metadata, are stored in IFO files that need to be parsed. PGCs can be thought of as playlists in easier terms.\n\nAn actual DVD player relies on user GUI interaction via menus and an internal VM to drive the direction of demuxing. Generally, the user would either navigate (via menus) or automatically be redirected to the PGC of their choice. During this process and the subsequent playback, the DVD player’s internal VM also maintains a state and executes instructions that can create jumps to different sectors during playback. This is why libdvdnav is involved, as a linear read of the MPEG-PS blobs on the disc (VOBs) is not enough to produce the right sequence in many cases.\n\nThere are many other DVD structures (a long subject) that will not be discussed here. NAV packets, in particular, are handled by this demuxer to build accurate timing but not emitted as a stream. For a good high-level understanding, refer to: https://code.videolan.org/videolan/libdvdnav/-/blob/master/doc/dvd_structures\n\nThis demuxer accepts the following options:\n• Open chapters 3-6 from title 1 from a given DVD structure:\n• Open only chapter 5 from title 1 from a given DVD structure:\n• Demux menu with language 1 from VTS 1, PGC 1, starting at PG 1:\n\nThis format is used by various Electronic Arts games.\n\nThis demuxer presents audio and video streams found in an IMF Composition, as specified in SMPTE ST 2067-2.\n\nIf is not specified, the demuxer looks for a file called in the same directory as the CPL.\n\nThis demuxer is used to demux FLV files and RTMP network streams. In case of live network streams, if you force format, you may use live_flv option instead of flv to survive timestamp discontinuities. KUX is a flv variant used on the Youku platform.\n\nIt accepts the following options:\n\nFor example, with the overlay filter, place an infinitely looping GIF over another video:\n\nNote that in the above example the shortest option for overlay filter is used to end the output video at the length of the shortest input file, which in this case is as the GIF in this example loops infinitely.\n\nThis demuxer presents all AVStreams from all variant streams. The id field is set to the bitrate variant index number. By setting the discard flags on AVStreams (by pressing ’a’ or ’v’ in ffplay), the caller can decide which variant streams to actually receive. The total bitrate of the variant that the stream belongs to is available in a metadata key named \"variant_bitrate\".\n\nIt accepts the following options:\n\nThis demuxer reads from a list of image files specified by a pattern. The syntax and meaning of the pattern is specified by the option .\n\nThe pattern may contain a suffix which is used to automatically determine the format of the images contained in the files.\n\nThe size, the pixel format, and the format of each image must be the same for all the files in the sequence.\n\nThis demuxer accepts the following options:\n• Use for creating a video from the images in the file sequence , , ..., assuming an input frame rate of 10 frames per second:\n• As above, but start by reading from a file with index 100 in the sequence:\n• Read images matching the \"*.png\" glob pattern , that is all the files terminating with the \".png\" suffix:\n\nThe Game Music Emu library is a collection of video game music file emulators.\n\nSee https://bitbucket.org/mpyne/game-music-emu/overview for more information.\n\nIt accepts the following options:\n\nIt will export one 2-channel 16-bit 44.1 kHz audio stream. Optionally, a 16-color video stream can be exported with or without printed metadata.\n\nIt accepts the following options:\n\nSee https://lib.openmpt.org/libopenmpt/ for more information.\n\nSome files have multiple subsongs (tracks) this can be set with the option.\n\nIt accepts the following options:\n\nDemuxer for Quicktime File Format & ISO/IEC Base Media File Format (ISO/IEC 14496-12 or MPEG-4 Part 12, ISO/IEC 15444-12 or JPEG 2000 Part 12).\n\nThis demuxer accepts the following options:\n\nAudible AAX files are encrypted M4B files, and they can be decrypted by specifying a 4 byte activation secret.\n\nThis demuxer accepts the following options:\n\nThis demuxer allows reading of MJPEG, where each frame is represented as a part of multipart/x-mixed-replace stream.\n\nThis demuxer allows one to read raw video data. Since there is no header specifying the assumed video parameters, the user must specify them in order to be able to decode the data correctly.\n\nThis demuxer accepts the following options:\n\nFor example to read a rawvideo file with , assuming a pixel format of , a video size of , and a frame rate of 10 images per second, use the command:\n\nRCWT (Raw Captions With Time) is a format native to ccextractor, a commonly used open source tool for processing 608/708 Closed Captions (CC) sources. For more information on the format, see (ffmpeg-formats)rcwtenc.\n\nThis demuxer implements the specification as of March 2024, which has been stable and unchanged since April 2014.\n• Render CC to ASS using the built-in decoder: Note that if your output appears to be empty, you may have to manually set the decoder’s option to pick the desired CC substream.\n• Convert an RCWT backup to Scenarist (SCC) format: Note that the SCC format does not support all of the possible CC extensions that can be stored in RCWT (such as EIA-708).\n\nThis demuxer reads the script language used by SBaGen http://uazu.net/sbagen/ to generate binaural beats sessions. A SBG script looks like that:\n\nA SBG script can mix absolute and relative timestamps. If the script uses either only absolute timestamps (including the script start time) or only relative ones, then its layout is fixed, and the conversion is straightforward. On the other hand, if the script mixes both kind of timestamps, then the reference for relative timestamps will be taken from the current time of day at the time the script is read, and the script layout will be frozen according to that reference. That means that if the script is directly played, the actual times will match the absolute timestamps up to the sound controller’s clock accuracy, but if the user somehow pauses the playback or seeks, all times will be shifted accordingly.\n\nTED does not provide links to the captions, but they can be guessed from the page. The file from the FFmpeg source tree contains a bookmarklet to expose them.\n\nThis demuxer accepts the following option:\n\nExample: convert the captions to a format most players understand:\n\nDue to security concerns, Vapoursynth scripts will not be autodetected so the input format has to be forced. For ff* CLI tools, add before the input .\n\nThis demuxer accepts the following option:\n\nThis demuxer accepts the following options:\n\nThis demuxer accepts the following options:\n\nMuxers are configured elements in FFmpeg which allow writing multimedia streams to a particular type of file.\n\nWhen you configure your FFmpeg build, all the supported muxers are enabled by default. You can list all available muxers using the configure option .\n\nYou can disable all the muxers with the configure option and selectively enable / disable single muxers with the options / .\n\nThe option of the ff* tools will display the list of enabled muxers. Use to view a combined list of enabled demuxers and muxers.\n\nA description of some of the currently available muxers follows.\n\nThis section covers raw muxers. They accept a single stream matching the designated codec. They do not store timestamps or metadata. The recognized extension is the same as the muxer name unless indicated otherwise.\n\nIt comprises the following muxers. The media type and the eventual extensions used to automatically selects the muxer from the output extensions are also shown.\n• Store raw video frames with the ‘ ’ muxer using : Since the rawvideo muxer do not store the information related to size and format, this information must be provided when demuxing the file:\n\nThey accept a single stream matching the designated codec. They do not store timestamps or metadata. The recognized extension is the same as the muxer name.\n\nIt comprises the following muxers. The optional additional extension used to automatically select the muxer from the output extension is also shown in parentheses.\n\nThis section covers formats belonging to the MPEG-1 and MPEG-2 Systems family.\n\nThe MPEG-1 Systems format (also known as ISO/IEEC 11172-1 or MPEG-1 program stream) has been adopted for the format of media track stored in VCD (Video Compact Disc).\n\nThe MPEG-2 Systems standard (also known as ISO/IEEC 13818-1) covers two containers formats, one known as transport stream and one known as program stream; only the latter is covered here.\n\nThe MPEG-2 program stream format (also known as VOB due to the corresponding file extension) is an extension of MPEG-1 program stream: in addition to support different codecs for the audio and video streams, it also stores subtitles and navigation metadata. MPEG-2 program stream has been adopted for storing media streams in SVCD and DVD storage devices.\n\nThis section comprises the following muxers.\n\nThis section covers formats belonging to the QuickTime / MOV family, including the MPEG-4 Part 14 format and ISO base media file format (ISOBMFF). These formats share a common structure based on the ISO base media file format (ISOBMFF).\n\nThe MOV format was originally developed for use with Apple QuickTime. It was later used as the basis for the MPEG-4 Part 1 (later Part 14) format, also known as ISO/IEC 14496-1. That format was then generalized into ISOBMFF, also named MPEG-4 Part 12 format, ISO/IEC 14496-12, or ISO/IEC 15444-12.\n\nIt comprises the following muxers.\n\nThe ‘ ’, ‘ ’, and ‘ ’ muxers support fragmentation. Normally, a MOV/MP4 file has all the metadata about all packets stored in one location.\n\nThis data is usually written at the end of the file, but it can be moved to the start for better playback by adding to the , or using the tool).\n\nA fragmented file consists of a number of fragments, where packets and metadata about these packets are stored together. Writing a fragmented file has the advantage that the file is decodable even if the writing is interrupted (while a normal MOV/MP4 is undecodable if it is not properly finished), and it requires less memory when writing very long files (since writing normal MOV/MP4 files stores info about every single packet in memory until the file is closed). The downside is that it is less compatible with other applications.\n\nFragmentation is enabled by setting one of the options that define how to cut the file into fragments:\n\nIf more than one condition is specified, fragments are cut when one of the specified conditions is fulfilled. The exception to this is the option , which has to be fulfilled for any of the other conditions to apply.\n• Push Smooth Streaming content in real time to a publishing point on IIS with the ‘ ’ muxer using :\n\nThis muxer accepts a single ATRAC1 audio stream with either one or two channels and a sample rate of 44100Hz.\n\nAs AEA supports storing the track title, this muxer will also write the title from stream’s metadata to the container.\n\nIt accepts a single ADPCM_IMA_ALP stream with no more than 2 channels and a sample rate not greater than 44100 Hz.\n\nIt accepts a single audio stream containing an AMR NB stream.\n• Use to generate an APNG output with 2 repetitions, and with a delay of half a second after the first repetition:\n\nThe and options set the corresponding flags in the header which can be later retrieved to process the audio stream accordingly.\n\nThe ‘ ’ variant should be selected for streaming.\n\nNote that Windows Media Audio (wma) and Windows Media Video (wmv) use this muxer too.\n\nThis format is used to play audio on some Nintendo Wii games.\n\nThe and options can be used to define a section of the file to loop for players honoring such options.\n\nAVI is a proprietary format developed by Microsoft, and later formally specified through the Open DML specification.\n\nBecause of differences in players implementations, it might be required to set some options to make sure that the generated output can be correctly played by the target player.\n\nThis muxers stores images encoded using the AV1 codec.\n\nIt accepts one or two video streams. In case two video streams are provided, the second one shall contain a single plane storing the alpha mask.\n\nIn case more than one image is provided, the generated output is considered an animated AVIF and the number of loops can be specified with the option.\n\nThis is based on the specification by Alliance for Open Media at url https://aomediacodec.github.io/av1-avif.\n\nIt accepts one audio stream, one video stream, or both.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThis muxer feeds audio data to the Chromaprint library, which generates a fingerprint for the provided audio data. See: https://acoustid.org/chromaprint\n\nIt takes a single signed native-endian 16-bit raw audio stream of at most 2 channels.\n\nThis muxer computes and prints the Adler-32 CRC of all the input audio and video frames. By default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the CRC.\n\nThe output of the muxer consists of a single line of the form: CRC=0x , where is a hexadecimal number 0-padded to 8 digits containing the CRC for all the decoded input frames.\n\nSee also the framecrc muxer.\n• Use to compute the CRC of the input, and store it in the file :\n• Use to print the CRC to stdout with the command:\n• You can select the output format of each frame with by specifying the audio and video codec and format. For example, to compute the CRC of the input audio converted to PCM unsigned 8-bit and the input video converted to MPEG-2 video, use the command:\n\nThis muxer creates segments and manifest files according to the MPEG-DASH standard ISO/IEC 23009-1:2014 and following standard updates.\n\nFor more information see:\n\nThis muxer creates an MPD (Media Presentation Description) manifest file and segment files for each stream. Segment files are placed in the same directory of the MPD manifest file.\n\nThe segment filename might contain pre-defined identifiers used in the manifest section as defined in section 5.3.9.4.4 of the standard.\n\nAvailable identifiers are , , , and . In addition to the standard identifiers, an ffmpeg-specific identifier is also supported. When specified, will replace in the file name with muxing format’s extensions such as , etc.\n\nGenerate a DASH output reading from an input source in realtime using .\n\nTwo multimedia streams are generated from the input file, both containing a video stream encoded through ‘ ’, and an audio stream encoded with ‘ ’. The first multimedia stream contains video with a bitrate of 800k and audio at the default rate, the second with video scaled to 320x170 pixels at 300k and audio resampled at 22005 Hz.\n\nThe option keeps only the latest 5 segments with the default duration of 5 seconds.\n\nIt accepts a single 6-channels audio stream resampled at 96000 Hz encoded with the ‘ ’ codec.\n\nUse to mux input audio to a ‘ ’ channel layout resampled at 96000Hz:\n\nFor ffmpeg versions before 7.0 you might have to use the ‘ ’ filter to limit the muxed packet size, because this format does not support muxing packets larger than 65535 bytes (3640 samples). For newer ffmpeg versions audio is automatically packetized to 36000 byte (2000 sample) packets.\n\nIt accepts exactly one ‘ ’ video stream and at most two ‘ ’ audio streams. More constraints are defined by the property of the video, which must correspond to a DV video supported profile, and on the framerate.\n\nUse to convert the input:\n\nThis muxer writes the streams metadata in the ‘ ’ format.\n\nSee (ffmpeg-formats)the Metadata chapter for information about the format.\n\nUse to extract metadata from an input file to a file in ‘ ’ format:\n\nThe ‘ ’ pseudo-muxer allows the separation of encoding and muxing by using a first-in-first-out queue and running the actual muxer in a separate thread.\n\nThis is especially useful in combination with the tee muxer and can be used to send data to several destinations with different reliability/writing speed/latency.\n\nThe target muxer is either selected from the output name or specified through the option.\n\nThe behavior of the ‘ ’ muxer if the queue fills up or if the output fails (e.g. if a packet cannot be written to the output) is selectable:\n• Output can be transparently restarted with configurable delay between retries based on real time or time of the processed stream.\n• Encoding can be blocked during temporary failure, or continue transparently dropping packets in case the FIFO queue fills up.\n\nAPI users should be aware that callback functions ( , and ) used within its must be thread-safe.\n\nUse to stream to an RTMP server, continue processing the stream at real-time rate even in case of temporary failure (network outage) and attempt to recover streaming every second indefinitely:\n\nThis format was used as internal format for several Sega games.\n\nFor more information regarding the Sega film file format, visit http://wiki.multimedia.cx/index.php?title=Sega_FILM.\n\nIt accepts at maximum one ‘ ’ or raw video stream, and at maximum one audio stream.\n\nThis format is used by several Adobe tools to store a generated filmstrip export. It accepts a single raw video stream.\n\nThis image format is used to store astronomical data.\n\nFor more information regarding the format, visit https://fits.gsfc.nasa.gov.\n\nThis muxer accepts exactly one FLAC audio stream. Additionally, it is possible to add images with disposition ‘ ’.\n\nUse to store the audio stream from an input file, together with several pictures used with ‘ ’ disposition:\n\nThis muxer computes and prints the Adler-32 CRC for each audio and video packet. By default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the CRC.\n\nThe output of the muxer consists of a line for each audio and video packet of the form:\n\nis a hexadecimal number 0-padded to 8 digits containing the CRC of the packet.\n\nFor example to compute the CRC of the audio and video frames in , converted to raw audio and video packets, and store it in the file :\n\nTo print the information to stdout, use the command:\n\nWith , you can select the output format to which the audio and video frames are encoded before computing the CRC for each packet by specifying the audio and video codec. For example, to compute the CRC of each decoded input audio frame converted to PCM unsigned 8-bit and of each decoded input video frame converted to MPEG-2 video, use the command:\n\nSee also the crc muxer.\n\nThis muxer computes and prints a cryptographic hash for each audio and video packet. This can be used for packet-by-packet equality checks without having to individually do a binary comparison on each.\n\nBy default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the hash, but the output of explicit conversions to other codecs can also be used. It uses the SHA-256 cryptographic hash function by default, but supports several other algorithms.\n\nThe output of the muxer consists of a line for each audio and video packet of the form:\n\nis a hexadecimal number representing the computed hash for the packet.\n\nTo compute the SHA-256 hash of the audio and video frames in , converted to raw audio and video packets, and store it in the file :\n\nTo print the information to stdout, using the MD5 hash function, use the command:\n\nSee also the hash muxer.\n\nThis is a variant of the framehash muxer. Unlike that muxer, it defaults to using the MD5 hash function.\n\nTo compute the MD5 hash of the audio and video frames in , converted to raw audio and video packets, and store it in the file :\n\nTo print the information to stdout, use the command:\n\nSee also the framehash and md5 muxers.\n\nNote that the GIF format has a very large time base: the delay between two frames can therefore not be smaller than one centi second.\n\nEncode a gif looping 10 times, with a 5 seconds delay between the loops:\n\nNote 1: if you wish to extract the frames into separate GIF files, you need to force the image2 muxer:\n\nGXF was developed by Grass Valley Group, then standardized by SMPTE as SMPTE 360M and was extended in SMPTE RDD 14-2007 to include high-definition video resolutions.\n\nIt accepts at most one video stream with codec ‘ ’, or ‘ ’, or ‘ ’, or ‘ ’ with resolution ‘ ’ or ‘ ’, and several audio streams with rate 48000Hz and codec ‘ ’.\n\nThis muxer computes and prints a cryptographic hash of all the input audio and video frames. This can be used for equality checks without having to do a complete binary comparison.\n\nBy default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the hash, but the output of explicit conversions to other codecs can also be used. Timestamps are ignored. It uses the SHA-256 cryptographic hash function by default, but supports several other algorithms.\n\nThe output of the muxer consists of a single line of the form: = , where is a short string representing the hash function used, and is a hexadecimal number representing the computed hash.\n\nTo compute the SHA-256 hash of the input converted to raw audio and video, and store it in the file :\n\nTo print an MD5 hash to stdout use the command:\n\nSee also the framehash muxer.\n\nHTTP dynamic streaming, or HDS, is an adaptive bitrate streaming method developed by Adobe. HDS delivers MP4 video content over HTTP connections. HDS can be used for on-demand streaming or live streaming.\n\nThis muxer creates an .f4m (Adobe Flash Media Manifest File) manifest, an .abst (Adobe Bootstrap File) for each stream, and segment files in a directory specified as the output.\n\nThese needs to be accessed by an HDS player throuhg HTTPS for it to be able to perform playback on the generated stream.\n\nUse to generate HDS files to the directory in real-time rate:\n\nApple HTTP Live Streaming muxer that segments MPEG-TS according to the HTTP Live Streaming (HLS) specification.\n\nIt creates a playlist file, and one or more segment files. The output filename specifies the playlist filename.\n\nBy default, the muxer creates a file for each segment produced. These files have the same name as the playlist, followed by a sequential number and a .ts extension.\n\nMake sure to require a closed GOP when encoding and to set the GOP size to fit your segment time constraint.\n\nFor example, to convert an input file with :\n\nThis example will produce the playlist, , and segment files: , , , etc.\n\nSee also the segment muxer, which provides a more generic and flexible implementation of a segmenter, and can be used to perform HLS segmentation.\n\nIAMF is used to provide immersive audio content for presentation on a wide range of devices in both streaming and offline applications. These applications include internet audio streaming, multicasting/broadcasting services, file download, gaming, communication, virtual and augmented reality, and others. In these applications, audio may be played back on a wide range of devices, e.g., headphones, mobile phones, tablets, TVs, sound bars, home theater systems, and big screens.\n\nThis format was promoted and desgined by Alliance for Open Media.\n\nFor more information about this format, see https://aomedia.org/iamf/.\n\nMicrosoft’s icon file format (ICO) has some strict limitations that should be noted:\n• Size cannot exceed 256 pixels in any dimension\n• Only BMP and PNG images can be stored\n• If a BMP image is used, it must be one of the following pixel formats:\n• If a BMP image is used, it must use the BITMAPINFOHEADER DIB header\n• If a PNG image is used, it must use the rgba pixel format\n\nThe output filenames are specified by a pattern, which can be used to produce sequentially numbered series of files. The pattern may contain the string \"%d\" or \"%0 d\", this string specifies the position of the characters representing a numbering in the filenames. If the form \"%0 d\" is used, the string representing the number in each filename is 0-padded to digits. The literal character ’%’ can be specified in the pattern with the string \"%%\".\n\nIf the pattern contains \"%d\" or \"%0 d\", the first filename of the file list specified will contain the number 1, all the following numbers will be sequential.\n\nThe pattern may contain a suffix which is used to automatically determine the format of the image files to write.\n\nFor example the pattern \"img-%03d.bmp\" will specify a sequence of filenames of the form , , ..., , etc. The pattern \"img%%-%d.jpg\" will specify a sequence of filenames of the form , , ..., , etc.\n\nThe image muxer supports the .Y.U.V image file format. This format is special in that each image frame consists of three files, for each of the YUV420P components. To read or write this image file format, specify the name of the ’.Y’ file. The muxer will automatically open the ’.U’ and ’.V’ files as required.\n\nThe ‘ ’ muxer accepts the same options as the ‘ ’ muxer, but ignores the pattern verification and expansion, as it is supposed to write to the command output rather than to an actual stored file.\n• Use for creating a sequence of files , , ..., taking one image every second from the input video: Note that with , if the format is not specified with the option and the output filename specifies an image file format, the image2 muxer is automatically selected, so the previous command can be written as: Note also that the pattern must not necessarily contain \"%d\" or \"%0 d\", for example to create a single image file from the start of the input video you can employ the command:\n• The option allows you to expand the filename with date and time information. Check the documentation of the function for the syntax. To generate image files from the \"%Y-%m-%d_%H-%M-%S\" pattern, the following command can be used:\n• Set the file name with current frame’s PTS:\n• Publish contents of your desktop directly to a WebDAV server every second:\n\nThe Berkeley/IRCAM/CARL Sound Format, developed in the 1980s, is a result of the merging of several different earlier sound file formats and systems including the csound system developed by Dr Gareth Loy at the Computer Audio Research Lab (CARL) at UC San Diego, the IRCAM sound file system developed by Rob Gross and Dan Timis at the Institut de Recherche et Coordination Acoustique / Musique in Paris and the Berkeley Fast Filesystem.\n\nIt was developed initially as part of the Berkeley/IRCAM/CARL Sound Filesystem, a suite of programs designed to implement a filesystem for audio applications running under Berkeley UNIX. It was particularly popular in academic music research centres, and was used a number of times in the creation of early computer-generated compositions.\n\nIVF was developed by On2 Technologies (formerly known as Duck Corporation), to store internally developed codecs.\n\nFor more information about the format, see http://unicorn.us.com/jacosub/jscripts.html.\n\nThis custom VAG container is used by some Simon & Schuster Interactive games such as \"Real War\", and \"Real War: Rogue States\".\n\nLRC (short for LyRiCs) is a computer file format that synchronizes song lyrics with an audio file, such as MP3, Vorbis, or MIDI.\n\nThe following metadata tags are converted to the format corresponding metadata:\n\nIf ‘ ’ is not explicitly set, it is automatically set to the libavformat version.\n\nThis muxer implements the matroska and webm container specs.\n\nThe recognized metadata settings in this muxer are:\n\nFor example a 3D WebM clip can be created using the following command line:\n\nThis is a variant of the hash muxer. Unlike that muxer, it defaults to using the MD5 hash function.\n\nSee also the hash and framemd5 muxers.\n• To compute the MD5 hash of the input converted to raw audio and video, and store it in the file :\n• To print the MD5 hash to stdout:\n\nSMAF is a music data format specified by Yamaha for portable electronic devices, such as mobile phones and personal digital assistants.\n\nThe MP3 muxer writes a raw MP3 stream with the following optional features:\n• An ID3v2 metadata header at the beginning (enabled by default). Versions 2.3 and 2.4 are supported, the private option controls which one is used (3 or 4). Setting to 0 disables the ID3v2 header completely. The muxer supports writing attached pictures (APIC frames) to the ID3v2 header. The pictures are supplied to the muxer in form of a video stream with a single packet. There can be any number of those streams, each will correspond to a single APIC frame. The stream metadata tags and map to APIC and respectively. See http://id3.org/id3v2.4.0-frames for allowed picture types. Note that the APIC frames must be written at the beginning, so the muxer will buffer the audio frames until it gets all the pictures. It is therefore advised to provide the pictures as soon as possible to avoid excessive buffering.\n• A Xing/LAME frame right after the ID3v2 header (if present). It is enabled by default, but will be written only if the output is seekable. The private option can be used to disable it. The frame contains various information that may be useful to the decoder, like the audio duration or encoder delay.\n• A legacy ID3v1 tag at the end of the file (disabled by default). It may be enabled with the private option, but as its capabilities are very limited, its usage is not recommended.\n\nWrite an mp3 with an ID3v2.3 header and an ID3v1 footer:\n\nTo attach a picture to an mp3 file select both the audio and the picture stream with :\n\nThis muxer implements ISO 13818-1 and part of ETSI EN 300 468.\n\nThe recognized metadata settings in mpegts muxer are and . If they are not set the default for is ‘ ’ and the default for is ‘ ’.\n\nThis muxer does not generate any output file, it is mainly useful for testing or benchmarking purposes.\n\nFor example to benchmark decoding with you can use the command:\n\nNote that the above command does not read or write the file, but specifying the output file is required by the syntax.\n\nAlternatively you can write the command as:\n\nRCWT (Raw Captions With Time) is a format native to ccextractor, a commonly used open source tool for processing 608/708 Closed Captions (CC) sources. It can be used to archive the original extracted CC bitstream and to produce a source file for later processing or conversion. The format allows for interoperability between ccextractor and FFmpeg, is simple to parse, and can be used to create a backup of the CC presentation.\n\nThis muxer implements the specification as of March 2024, which has been stable and unchanged since April 2014.\n\nThis muxer will have some nuances from the way that ccextractor muxes RCWT. No compatibility issues when processing the output with ccextractor have been observed as a result of this so far, but mileage may vary and outputs will not be a bit-exact match.\n\nA free specification of RCWT can be found here: https://github.com/CCExtractor/ccextractor/blob/master/docs/BINARY_FILE_FORMAT.TXT\n\nThis muxer outputs streams to a number of separate files of nearly fixed duration. Output filename pattern can be set in a fashion similar to image2, or by using a template if the option is enabled.\n\nis a variant of the muxer used to write to streaming output formats, i.e. which do not require global headers, and is recommended for outputting e.g. to MPEG transport stream segments. is a shorter alias for .\n\nEvery segment starts with a keyframe of the selected reference stream, which is set through the option.\n\nNote that if you want accurate splitting for a video file, you need to make the input key frames correspond to the exact splitting times expected by the segmenter, or the segment muxer will start the new segment with the key frame found next after the specified start time.\n\nThe segment muxer works best with a single constant frame rate video.\n\nOptionally it can generate a list of the created segments, by setting the option . The list type is specified by the option. The entry filenames in the segment list are set by default to the basename of the corresponding segment files.\n\nSee also the hls muxer, which provides a more specific implementation for HLS segmentation.\n\nThe segment muxer supports the following options:\n\nMake sure to require a closed GOP when encoding and to set the GOP size to fit your segment time constraint.\n• Remux the content of file to a list of segments , , etc., and write the list of generated segments to :\n• Segment input and set output format options for the output segments:\n• Segment the input file according to the split points specified by the option:\n• Use the option to force key frames in the input at the specified location, together with the segment option to account for possible roundings operated when setting key frame times. In order to force key frames on the input file, transcoding is required.\n• Segment the input file by splitting the input file according to the frame numbers sequence specified with the option:\n• Convert the to TS segments using the and encoders:\n• Segment the input file, and create an M3U8 live playlist (can be used as live HLS source):\n\nSmooth Streaming muxer generates a set of files (Manifest, chunks) suitable for serving with conventional web server.\n\nThis muxer computes and prints a cryptographic hash of all the input frames, on a per-stream basis. This can be used for equality checks without having to do a complete binary comparison.\n\nBy default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the hash, but the output of explicit conversions to other codecs can also be used. Timestamps are ignored. It uses the SHA-256 cryptographic hash function by default, but supports several other algorithms.\n\nThe output of the muxer consists of one line per stream of the form: , , = , where is the index of the mapped stream, is a single character indicating the type of stream, is a short string representing the hash function used, and is a hexadecimal number representing the computed hash.\n\nTo compute the SHA-256 hash of the input converted to raw audio and video, and store it in the file :\n\nTo print an MD5 hash to stdout use the command:\n\nSee also the hash and framehash muxers.\n\nThe tee muxer can be used to write the same data to several outputs, such as files or streams. It can be used, for example, to stream a video over a network and save it to disk at the same time.\n\nIt is different from specifying several outputs to the command-line tool. With the tee muxer, the audio and video data will be encoded only once. With conventional multiple outputs, multiple encoding operations in parallel are initiated, which can be a very expensive process. The tee muxer is not useful when using the libavformat API directly because it is then possible to feed the same packets to several muxers directly.\n\nSince the tee muxer does not represent any particular output format, ffmpeg cannot auto-select output streams. So all streams intended for output must be specified using . See the examples below.\n\nSome encoders may need different options depending on the output format; the auto-detection of this can not work with the tee muxer, so they need to be explicitly specified. The main example is the flag.\n\nThe slave outputs are specified in the file name given to the muxer, separated by ’|’. If any of the slave name contains the ’|’ separator, leading or trailing spaces or any special character, those must be escaped (see (ffmpeg-utils)the \"Quoting and escaping\" section in the ffmpeg-utils(1) manual).\n\nMuxer options can be specified for each slave by prepending them as a list of = pairs separated by ’:’, between square brackets. If the options values contain a special character or the ’:’ separator, they must be escaped; note that this is a second level escaping.\n\nThe following special options are also recognized:\n• Encode something and both archive it in a WebM file and stream it as MPEG-TS over UDP:\n• As above, but continue streaming even if output to local file fails (for example local drive fills up):\n• Use to encode the input, and send the output to three different destinations. The bitstream filter is used to add extradata information to all the output video keyframes packets, as requested by the MPEG-TS format. The select option is applied to in order to make it contain only audio packets.\n• As above, but select only stream for the audio output. Note that a second level escaping must be performed, as \":\" is a special character used to separate options.\n\nThis muxer writes out WebM headers and chunks as separate files which can be consumed by clients that support WebM Live streams via DASH.\n\nThis muxer supports the following options:\n\nThis muxer implements the WebM DASH Manifest specification to generate the DASH manifest XML. It also supports manifest generation for DASH live streams.\n\nFor more information see:\n\nThis muxer supports the following options:\n\nFFmpeg is able to dump metadata from media files into a simple UTF-8-encoded INI-like text file and then load it back using the metadata muxer/demuxer.\n\nThe file format is as follows:\n• A file consists of a header and a number of metadata tags divided into sections, each on its own line.\n• The header is a ‘ ’ string, followed by a version number (now 1).\n• Metadata tags are of the form ‘ ’\n• After global metadata there may be sections with per-stream/per-chapter metadata.\n• A section starts with the section name in uppercase (i.e. STREAM or CHAPTER) in brackets (‘ ’, ‘ ’) and ends with next section or end of file.\n• At the beginning of a chapter section there may be an optional timebase to be used for start/end values. It must be in form ‘ ’, where and are integers. If the timebase is missing then start/end times are assumed to be in nanoseconds. Next a chapter section must contain chapter start and end times in form ‘ ’, ‘ ’, where is a positive integer.\n• Empty lines and lines starting with ‘ ’ or ‘ ’ are ignored.\n• Metadata keys or values containing special characters (‘ ’, ‘ ’, ‘ ’, ‘ ’ and a newline) must be escaped with a backslash ‘ ’.\n• Note that whitespace in metadata (e.g. ‘ ’) is considered to be a part of the tag (in the example above key is ‘ ’, value is ‘ ’).\n\nA ffmetadata file might look like this:\n\nBy using the ffmetadata muxer and demuxer it is possible to extract metadata from an input file to an ffmetadata file, and then transcode the file into an output file with the edited ffmetadata file.\n\nExtracting an ffmetadata file with goes as follows:\n\nReinserting edited metadata information from the FFMETADATAFILE file can be done as:\n\nThe libavformat library provides some generic global options, which can be set on all the protocols. In addition each protocol may support so-called private options, which are specific for that component.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the options or using the API for programmatic use.\n\nThe list of supported options follows:\n\nProtocols are configured elements in FFmpeg that enable access to resources that require specific protocols.\n\nWhen you configure your FFmpeg build, all the supported protocols are enabled by default. You can list all available ones using the configure option \"–list-protocols\".\n\nYou can disable all the protocols using the configure option \"–disable-protocols\", and selectively enable a protocol using the option \"–enable-protocol= \", or you can disable a particular protocol using the option \"–disable-protocol= \".\n\nThe option \"-protocols\" of the ff* tools will display the list of supported protocols.\n\nAll protocols accept the following options:\n\nA description of the currently available protocols follows.\n\nFFmpeg must be compiled with –enable-librabbitmq to support AMQP. A separate AMQP broker must also be run. An example open-source AMQP broker is RabbitMQ.\n\nAfter starting the broker, an FFmpeg client may stream data to the broker using the command:\n\nWhere hostname and port (default is 5672) is the address of the broker. The client may also set a user/password for authentication. The default for both fields is \"guest\". Name of virtual host on broker can be set with vhost. The default value is \"/\".\n\nMuliple subscribers may stream from the broker using the command:\n\nIn RabbitMQ all data published to the broker flows through a specific exchange, and each subscribing client has an assigned queue/buffer. When a packet arrives at an exchange, it may be copied to a client’s queue depending on the exchange and routing_key fields.\n\nThe following options are supported:\n\nFill data in a background thread, to decouple I/O operation from demux thread.\n\nRead angle 2 of playlist 4 from BluRay mounted to /mnt/bluray, start from chapter 2:\n\nCache the input stream to temporary file. It brings seeking capability to live streams.\n\nRead and seek from many resources in sequence as if they were a unique resource.\n\nA URL accepted by this protocol has the syntax:\n\nwhere , , ..., are the urls of the resource to be concatenated, each one possibly specifying a distinct protocol.\n\nFor example to read a sequence of files , , with use the command:\n\nNote that you may need to escape the character \"|\" which is special for many shells.\n\nRead and seek from many resources in sequence as if they were a unique resource.\n\nA URL accepted by this protocol has the syntax:\n\nwhere is the url containing a line break delimited list of resources to be concatenated, each one possibly specifying a distinct protocol. Special characters must be escaped with backslash or single quotes. See (ffmpeg-utils)the \"Quoting and escaping\" section in the ffmpeg-utils(1) manual.\n\nFor example to read a sequence of files , , listed in separate lines within a file with use the command:\n\nWhere contains the lines:\n\nData in-line in the URI. See http://en.wikipedia.org/wiki/Data_URI_scheme.\n\nFor example, to convert a GIF file given inline with :\n\nIf is not specified, by default the stdout file descriptor will be used for writing, stdin for reading. Unlike the pipe protocol, fd protocol has seek support if it corresponding to a regular file. fd protocol doesn’t support pass file descriptor via URL for security.\n\nThis protocol accepts the following options:\n\nRead from or write to a file.\n\nA file URL can have the form:\n\nwhere is the path of the file to read.\n\nAn URL that does not have a protocol prefix will be assumed to be a file URL. Depending on the build, an URL that looks like a Windows path with the drive letter at the beginning will also be assumed to be a file URL (usually not the case in builds for unix-like systems).\n\nFor example to read from a file with use the command:\n\nThis protocol accepts the following options:\n\nRead from or write to remote resources using FTP protocol.\n\nThis protocol accepts the following options.\n\nNOTE: Protocol can be used as output, but it is recommended to not do it, unless special care is taken (tests, customized server configuration etc.). Different FTP servers behave in different way during seek operation. ff* tools may produce incomplete content due to server limitations.\n\nRead Apple HTTP Live Streaming compliant segmented stream as a uniform one. The M3U8 playlists describing the segments can be remote HTTP resources or local files, accessed using the standard file protocol. The nested protocol is declared by specifying \"+ \" after the hls URI scheme name, where is either \"file\" or \"http\".\n\nUsing this protocol is discouraged - the hls demuxer should work just as well (if not, please report the issues) and is more complete. To use the hls demuxer instead, simply use the direct URLs to the m3u8 files.\n\nThis protocol accepts the following options:\n\nSome HTTP requests will be denied unless cookie values are passed in with the request. The option allows these cookies to be specified. At the very least, each cookie must specify a value along with a path and domain. HTTP requests that match both the domain and path will automatically include the cookie value in the HTTP Cookie header field. Multiple cookies can be delimited by a newline.\n\nThe required syntax to play a stream specifying a cookie is:\n\nThis protocol accepts the following options:\n\nInterPlanetary File System (IPFS) protocol support. One can access files stored on the IPFS network through so-called gateways. These are http(s) endpoints. This protocol wraps the IPFS native protocols (ipfs:// and ipns://) to be sent to such a gateway. Users can (and should) host their own node which means this protocol will use one’s local gateway to access files on the IPFS network.\n\nThis protocol accepts the following options:\n\nOne can use this protocol in 2 ways. Using IPFS:\n\nOr the IPNS protocol (IPNS is mutable IPFS):\n\nComputes the MD5 hash of the data to be written, and on close writes this to the designated output or stdout if none is specified. It can be used to test muxers without writing an actual file.\n\nNote that some formats (typically MOV) require the output protocol to be seekable, so they will fail with the MD5 output protocol.\n\nIf isn’t specified, is the number corresponding to the file descriptor of the pipe (e.g. 0 for stdin, 1 for stdout, 2 for stderr). If is not specified, by default the stdout file descriptor will be used for writing, stdin for reading.\n\nFor example to read from stdin with :\n\nFor writing to stdout with :\n\nThis protocol accepts the following options:\n\nNote that some formats (typically MOV), require the output protocol to be seekable, so they will fail with the pipe output protocol.\n\nThe Pro-MPEG CoP#3 FEC is a 2D parity-check forward error correction mechanism for MPEG-2 Transport Streams sent over RTP.\n\nThis protocol must be used in conjunction with the muxer and the protocol.\n\nThe destination UDP ports are for the column FEC stream and for the row FEC stream.\n\nThis protocol accepts the following options:\n\nThe Real-Time Messaging Protocol (RTMP) is used for streaming multimedia content across a TCP/IP network.\n\nAdditionally, the following parameters can be set via command line options (or in code via s):\n\nFor example to read with a multimedia resource named \"sample\" from the application \"vod\" from an RTMP server \"myserver\":\n\nTo publish to a password protected server, passing the playpath and app names separately:\n\nThe Encrypted Real-Time Messaging Protocol (RTMPE) is used for streaming multimedia content within standard cryptographic primitives, consisting of Diffie-Hellman key exchange and HMACSHA256, generating a pair of RC4 keys.\n\nThe Real-Time Messaging Protocol (RTMPS) is used for streaming multimedia content across an encrypted connection.\n\nThe Real-Time Messaging Protocol tunneled through HTTP (RTMPT) is used for streaming multimedia content within HTTP requests to traverse firewalls.\n\nThe Encrypted Real-Time Messaging Protocol tunneled through HTTP (RTMPTE) is used for streaming multimedia content within HTTP requests to traverse firewalls.\n\nThe Real-Time Messaging Protocol tunneled through HTTPS (RTMPTS) is used for streaming multimedia content within HTTPS requests to traverse firewalls.\n\nThis protocol accepts the following options.\n\nFor more information see: http://www.samba.org/.\n\nRead from or write to remote resources using SFTP protocol.\n\nThis protocol accepts the following options.\n\nReal-Time Messaging Protocol and its variants supported through librtmp.\n\nRequires the presence of the librtmp headers and library during configuration. You need to explicitly configure the build with \"–enable-librtmp\". If enabled this will replace the native RTMP protocol.\n\nThis protocol provides most client functions and a few server functions needed to support RTMP, RTMP tunneled in HTTP (RTMPT), encrypted RTMP (RTMPE), RTMP over SSL/TLS (RTMPS) and tunneled variants of these encrypted types (RTMPTE, RTMPTS).\n\nwhere is one of the strings \"rtmp\", \"rtmpt\", \"rtmpe\", \"rtmps\", \"rtmpte\", \"rtmpts\" corresponding to each RTMP variant, and , , and have the same meaning as specified for the RTMP native protocol. contains a list of space-separated options of the form = .\n\nSee the librtmp manual page (man 3 librtmp) for more information.\n\nFor example, to stream a file in real-time to an RTMP server using :\n\nTo play the same stream using :\n\nThe required syntax for an RTP URL is:\n\nspecifies the RTP port to use.\n\ncontains a list of &-separated options of the form = .\n\nThe following URL options are supported:\n• If is not set the RTCP port will be set to the RTP port value plus 1.\n• If (the local RTP port) is not set any available port will be used for the local RTP and RTCP ports.\n• If (the local RTCP port) is not set it will be set to the local RTP port value plus 1.\n\nRTSP is not technically a protocol handler in libavformat, it is a demuxer and muxer. The demuxer supports both normal RTSP (with data transferred over RTP; this is used by e.g. Apple and Microsoft) and Real-RTSP (with data transferred over RDT).\n\nThe muxer can be used to send a stream using RTSP ANNOUNCE to a server supporting it (currently Darwin Streaming Server and Mischa Spiegelmock’s RTSP server).\n\nThe required syntax for a RTSP url is:\n\nOptions can be set on the / command line, or set in code via s or in .\n\nThe following options are supported.\n\nThe following options are supported.\n\nWhen receiving data over UDP, the demuxer tries to reorder received packets (since they may arrive out of order, or packets may get lost totally). This can be disabled by setting the maximum demuxing delay to zero (via the field of AVFormatContext).\n\nWhen watching multi-bitrate Real-RTSP streams with , the streams to display can be chosen with and for video and audio respectively, and can be switched on the fly by pressing and .\n\nThe following examples all make use of the and tools.\n• Watch a stream over UDP, with a max reordering delay of 0.5 seconds:\n• Send a stream in realtime to a RTSP server, for others to watch:\n\nSession Announcement Protocol (RFC 2974). This is not technically a protocol handler in libavformat, it is a muxer and demuxer. It is used for signalling of RTP streams, by announcing the SDP for the streams regularly on a separate port.\n\nThe syntax for a SAP url given to the muxer is:\n\nThe RTP packets are sent to on port , or to port 5004 if no port is specified. is a -separated list. The following options are supported:\n\nTo broadcast a stream on the local subnet, for watching in VLC:\n\nAnd for watching in , over IPv6:\n\nThe syntax for a SAP url given to the demuxer is:\n\nis the multicast address to listen for announcements on, if omitted, the default 224.2.127.254 (sap.mcast.net) is used. is the port that is listened on, 9875 if omitted.\n\nThe demuxers listens for announcements on the given address and port. Once an announcement is received, it tries to receive that particular stream.\n\nTo play back the first stream announced on the normal SAP multicast address:\n\nTo play back the first stream announced on one the default IPv6 SAP multicast address:\n\nThe protocol accepts the following options:\n\nThe supported syntax for a SRT URL is:\n\ncontains a list of &-separated options of the form = .\n\nThis protocol accepts the following options.\n\nFor more information see: https://github.com/Haivision/srt.\n\nVirtually extract a segment of a file or another stream. The underlying stream must be seekable.\n\nExtract a chapter from a DVD VOB file (start and end sectors obtained externally and multiplied by 2048):\n\nWrites the output to multiple protocols. The individual outputs are separated by |\n\nThe required syntax for a TCP url is:\n\ncontains a list of &-separated options of the form = .\n\nThe list of supported options follows.\n\nThe following example shows how to setup a listening TCP connection with , which is then accessed with :\n\nThe required syntax for a TLS/SSL url is:\n\nThe following parameters can be set via command line options (or in code via s):\n\nTo create a TLS/SSL server that serves an input stream.\n\nTo play back a stream from the TLS/SSL server using :\n\nThe required syntax for an UDP URL is:\n\ncontains a list of &-separated options of the form = .\n\nIn case threading is enabled on the system, a circular buffer is used to store the incoming data, which allows one to reduce loss of data due to UDP socket buffer overruns. The and options are related to this buffer.\n\nThe list of supported options follows.\n• Use to stream over UDP to a remote endpoint:\n• Use to stream in mpegts format over UDP using 188 sized UDP packets, using a large input buffer:\n• Use to receive over UDP from a remote endpoint:\n\nThe required syntax for a Unix socket URL is:\n\nThe following parameters can be set via command line options (or in code via s):\n\nThis library supports unicast streaming to multiple clients without relying on an external server.\n\nThe required syntax for streaming or connecting to a stream is:\n\nMultiple clients may connect to the stream using:\n\nStreaming to multiple clients is implemented using a ZeroMQ Pub-Sub pattern. The server side binds to a port and publishes data. Clients connect to the server (via IP address/port) and subscribe to the stream. The order in which the server and client start generally does not matter.\n\nffmpeg must be compiled with the –enable-libzmq option to support this protocol.\n\nOptions can be set on the / command line. The following options are supported:\n\nThe libavdevice library provides the same interface as libavformat. Namely, an input device is considered like a demuxer, and an output device like a muxer, and the interface and generic device options are the same provided by libavformat (see the ffmpeg-formats manual).\n\nIn addition each input or output device may support so-called private options, which are specific for that component.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the device options or using the API for programmatic use.\n\nInput devices are configured elements in FFmpeg which enable accessing the data coming from a multimedia device attached to your system.\n\nWhen you configure your FFmpeg build, all the supported input devices are enabled by default. You can list all available ones using the configure option \"–list-indevs\".\n\nYou can disable all the input devices using the configure option \"–disable-indevs\", and selectively enable an input device using the option \"–enable-indev= \", or you can disable a particular input device using the option \"–disable-indev= \".\n\nThe option \"-devices\" of the ff* tools will display the list of supported input devices.\n\nA description of the currently available input devices follows.\n\nTo enable this input device during configuration you need libasound installed on your system.\n\nThis device allows capturing from an ALSA device. The name of the device to capture has to be an ALSA card identifier.\n\nAn ALSA identifier has the syntax:\n\nwhere the and components are optional.\n\nThe three arguments (in order: , , ) specify card number or identifier, device number and subdevice number (-1 means any).\n\nTo see the list of cards currently recognized by your system check the files and .\n\nFor example to capture with from an ALSA device with card id 0, you may run the command:\n\nFor more information see: http://www.alsa-project.org/alsa-doc/alsa-lib/pcm.html\n\nThis input devices uses the Android Camera2 NDK API which is available on devices with API level 24+. The availability of android_camera is autodetected during configuration.\n\nThis device allows capturing from all cameras on an Android device, which are integrated into the Camera2 NDK API.\n\nThe available cameras are enumerated internally and can be selected with the parameter. The input file string is discarded.\n\nGenerally the back facing camera has index 0 while the front facing camera has index 1.\n\nAVFoundation is the currently recommended framework by Apple for streamgrabbing on OSX >= 10.7 as well as on iOS.\n\nThe input filename has to be given in the following syntax:\n\nThe first entry selects the video input while the latter selects the audio input. The stream has to be specified by the device name or the device index as shown by the device list. Alternatively, the video and/or audio input device can be chosen by index using the and/or , overriding any device name or index given in the input filename.\n\nAll available devices can be enumerated by using , listing all device names and corresponding indices.\n\nThere are two device name aliases:\n• Print the list of AVFoundation supported devices and exit:\n• Record video from video device 0 and audio from audio device 0 into out.avi:\n• Record video from video device 2 and audio from audio device 1 into out.avi:\n• Record video from the system default video device using the pixel format bgr0 and do not record any audio into out.avi:\n• Record raw DV data from a suitable input device and write the output into out.dv:\n\nBSD video input device. Deprecated and will be removed - please contact the developers if you are interested in maintaining it.\n\nThe decklink input device provides capture capabilities for Blackmagic DeckLink devices.\n\nTo enable this input device, you need the Blackmagic DeckLink SDK and you need to configure with the appropriate and . On Windows, you need to run the IDL files through .\n\nDeckLink is very picky about the formats it supports. Pixel format of the input can be set with . Framerate and video size must be determined for your device with . Audio sample rate is always 48 kHz and the number of channels can be 2, 8 or 16. Note that all audio channels are bundled in one single audio track.\n\nDirectShow support is enabled when FFmpeg is built with the mingw-w64 project. Currently only audio and video devices are supported.\n\nMultiple devices may be opened as separate inputs, but they may also be opened on the same input, which should improve synchronism between them.\n\nThe input name should be in the format:\n\nwhere can be either or , and is the device’s name or alternative name..\n\nIf no options are specified, the device’s defaults are used. If the device does not support the requested options, it will fail to open.\n• Print the list of DirectShow supported devices and exit:\n• Open second video device with name :\n• Print the list of supported options in selected device and exit:\n• Specify pin names to capture by name or alternative name, specify alternative device name:\n• Configure a crossbar device, specifying crossbar pins, allow user to adjust video capture properties at startup:\n\nThe Linux framebuffer is a graphic hardware-independent abstraction layer to show graphics on a computer monitor, typically on the console. It is accessed through a file device node, usually .\n\nFor more detailed information read the file Documentation/fb/framebuffer.txt included in the Linux source tree.\n\nSee also http://linux-fbdev.sourceforge.net/, and fbset(1).\n\nTo record from the framebuffer device with :\n\nYou can take a single screenshot image with the command:\n\nThis device allows you to capture a region of the display on Windows.\n\nAmongst options for the imput filenames are such elements as:\n\nThe first option will capture the entire desktop, or a fixed region of the desktop. The second and third options will instead capture the contents of a single window, regardless of its position on the screen.\n\nFor example, to grab the entire desktop using :\n\nGrab the contents of the window named \"Calculator\"\n\nTo enable this input device, you need libiec61883, libraw1394 and libavc1394 installed on your system. Use the configure option to compile with the device enabled.\n\nThe iec61883 capture device supports capturing from a video device connected via IEEE1394 (FireWire), using libiec61883 and the new Linux FireWire stack (juju). This is the default DV/HDV input method in Linux Kernel 2.6.37 and later, since the old FireWire stack was removed.\n\nSpecify the FireWire port to be used as input file, or \"auto\" to choose the first port connected.\n• Grab and show the input of a FireWire DV/HDV device.\n• Grab and record the input of a FireWire DV/HDV device, using a packet buffer of 100000 packets if the source is HDV.\n\nTo enable this input device during configuration you need libjack installed on your system.\n\nA JACK input device creates one or more JACK writable clients, one for each audio channel, with name :input_ , where is the name provided by the application, and is a number which identifies the channel. Each writable client will send the acquired data to the FFmpeg input device.\n\nOnce you have created one or more JACK readable clients, you need to connect them to one or more JACK writable clients.\n\nTo connect or disconnect JACK clients you can use the and programs, or do it through a graphical interface, for example with .\n\nTo list the JACK clients and their properties you can invoke the command .\n\nFollows an example which shows how to capture a JACK readable client with .\n\nCaptures the KMS scanout framebuffer associated with a specified CRTC or plane as a DRM object that can be passed to other hardware functions.\n\nRequires either DRM master or CAP_SYS_ADMIN to run.\n\nIf you don’t understand what all of that means, you probably don’t want this. Look at instead.\n• Capture from the first active plane, download the result to normal frames and encode. This will only work if the framebuffer is both linear and mappable - if not, the result may be scrambled or fail to download.\n• Capture from CRTC ID 42 at 60fps, map the result to VAAPI, convert to NV12 and encode as H.264.\n• To capture only part of a plane the output can be cropped - this can be used to capture a single window, as long as it has a known absolute position and size. For example, to capture and encode the middle quarter of a 1920x1080 plane:\n\nThis input device reads data from the open output pads of a libavfilter filtergraph.\n\nFor each filtergraph open output, the input device will create a corresponding stream which is mapped to the generated output. The filtergraph is specified through the option .\n• Create a color video stream and play it back with :\n• As the previous example, but use filename for specifying the graph description, and omit the \"out0\" label:\n• Create three different video test filtered sources and play them:\n• Read an audio stream from a file using the amovie source and play it back with :\n• Read an audio stream and a video stream and play it back with :\n• Dump decoded frames to images and Closed Captions to an RCWT backup:\n\nTo enable this input device during configuration you need libcdio installed on your system. It requires the configure option .\n\nThis device allows playing and grabbing from an Audio-CD.\n\nFor example to copy with the entire Audio-CD in , you may run the command:\n\nThe OpenAL input device provides audio capture on all systems with a working OpenAL 1.1 implementation.\n\nTo enable this input device during configuration, you need OpenAL headers and libraries installed on your system, and need to configure FFmpeg with .\n\nOpenAL headers and libraries should be provided as part of your OpenAL implementation, or as an additional download (an SDK). Depending on your installation you may need to specify additional flags via the and for allowing the build system to locate the OpenAL headers and libraries.\n\nAn incomplete list of OpenAL implementations follows:\n\nThis device allows one to capture from an audio input device handled through OpenAL.\n\nYou need to specify the name of the device to capture in the provided filename. If the empty string is provided, the device will automatically select the default device. You can get the list of the supported devices by using the option .\n\nPrint the list of OpenAL supported devices and exit:\n\nCapture from the default device (note the empty string ” as filename):\n\nCapture from two devices simultaneously, writing to two different files, within the same command:\n\nNote: not all OpenAL implementations support multiple simultaneous capture - try the latest OpenAL Soft if the above does not work.\n\nThe filename to provide to the input device is the device node representing the OSS input device, and is usually set to .\n\nFor example to grab from using use the command:\n\nFor more information about OSS see: http://manuals.opensound.com/usersguide/dsp.html\n\nTo enable this output device you need to configure FFmpeg with .\n\nThe filename to provide to the input device is a source device or the string \"default\"\n\nTo list the PulseAudio source devices and their properties you can invoke the command .\n\nMore information about PulseAudio can be found on http://www.pulseaudio.org.\n\nTo enable this input device during configuration you need libsndio installed on your system.\n\nThe filename to provide to the input device is the device node representing the sndio input device, and is usually set to .\n\nFor example to grab from using use the command:\n\n\"v4l2\" can be used as alias for \"video4linux2\".\n\nIf FFmpeg is built with v4l-utils support (by using the configure option), it is possible to use it with the input device option.\n\nThe name of the device to grab is a file device node, usually Linux systems tend to automatically create such nodes when the device (e.g. an USB webcam) is plugged into the system, and has a name of the kind , where is a number associated to the device.\n\nVideo4Linux2 devices usually support a limited set of x sizes and frame rates. You can check which are supported using for Video4Linux2 devices. Some devices, like TV cards, support one or more standards. It is possible to list all the supported standards using .\n\nThe time base for the timestamps is 1 microsecond. Depending on the kernel version and configuration, the timestamps may be derived from the real time clock (origin at the Unix Epoch) or the monotonic clock (origin usually at boot time, unaffected by NTP or manual changes to the clock). The or option can be used to force conversion into the real time clock.\n\nSome usage examples of the video4linux2 device with and :\n• Grab and show the input of a video4linux2 device:\n• Grab and record the input of a video4linux2 device, leave the frame rate and size as previously set:\n\nFor more information about Video4Linux, check http://linuxtv.org/.\n\nThe filename passed as input is the capture driver number, ranging from 0 to 9. You may use \"list\" as filename to print a list of drivers. Any other filename will be interpreted as device number 0.\n\nTo enable this input device during configuration you need libxcb installed on your system. It will be automatically detected during configuration.\n\nThis device allows one to capture a region of an X11 display.\n\nThe filename passed as input has the syntax:\n\n: . specifies the X11 display name of the screen to grab from. can be omitted, and defaults to \"localhost\". The environment variable contains the default display name.\n\nand specify the offsets of the grabbed area with respect to the top-left border of the X11 screen. They default to 0.\n\nCheck the X11 documentation (e.g. ) for more detailed information.\n\nUse the program for getting basic information about the properties of your X11 display (e.g. grep for \"name\" or \"dimensions\").\n\nFor example to grab from using :\n\nOutput devices are configured elements in FFmpeg that can write multimedia data to an output device attached to your system.\n\nWhen you configure your FFmpeg build, all the supported output devices are enabled by default. You can list all available ones using the configure option \"–list-outdevs\".\n\nYou can disable all the output devices using the configure option \"–disable-outdevs\", and selectively enable an output device using the option \"–enable-outdev= \", or you can disable a particular input device using the option \"–disable-outdev= \".\n\nThe option \"-devices\" of the ff* tools will display the list of enabled output devices.\n\nA description of the currently available output devices follows.\n\nAllows native output to CoreAudio devices on OSX.\n\nThe output filename can be empty (or ) to refer to the default system output device or a number that refers to the device index as shown using: .\n\nAlternatively, the audio input device can be chosen by index using the , overriding any device name or index given in the input filename.\n\nAll available devices can be enumerated by using , listing all device names, UIDs and corresponding indices.\n• Print the list of supported devices and output a sine wave to the default device:\n• Output a sine wave to the device with the index 2, overriding any output filename:\n\nThis output device allows one to show a video stream in CACA window. Only one CACA window is allowed per application, so you can have only one instance of this output device in an application.\n\nTo enable this output device you need to configure FFmpeg with . libcaca is a graphics library that outputs text instead of pixels.\n\nFor more information about libcaca, check: http://caca.zoy.org/wiki/libcaca\n• The following command shows the output is an CACA window, forcing its size to 80x25:\n• Show the list of available drivers and exit:\n• Show the list of available dither colors and exit:\n\nThe decklink output device provides playback capabilities for Blackmagic DeckLink devices.\n\nTo enable this output device, you need the Blackmagic DeckLink SDK and you need to configure with the appropriate and . On Windows, you need to run the IDL files through .\n\nDeckLink is very picky about the formats it supports. Pixel format is always uyvy422, framerate, field order and video size must be determined for your device with . Audio sample rate is always 48 kHz.\n\nThe Linux framebuffer is a graphic hardware-independent abstraction layer to show graphics on a computer monitor, typically on the console. It is accessed through a file device node, usually .\n\nFor more detailed information read the file included in the Linux source tree.\n\nSee also http://linux-fbdev.sourceforge.net/, and fbset(1).\n\nOpenGL output device. Deprecated and will be removed.\n\nTo enable this output device you need to configure FFmpeg with .\n\nThis output device allows one to render to OpenGL context. Context may be provided by application or default SDL window is created.\n\nWhen device renders to external context, application must implement handlers for following messages: - create OpenGL context on current thread. - make OpenGL context current. - swap buffers. - destroy OpenGL context. Application is also required to inform a device about current resolution by sending message.\n\nTo enable this output device you need to configure FFmpeg with .\n\nMore information about PulseAudio can be found on http://www.pulseaudio.org\n\nSDL (Simple DirectMedia Layer) output device. Deprecated and will be removed.\n\nFor monitoring purposes in FFmpeg, pipes and a video player such as ffplay can be used:\n\n\"sdl2\" can be used as alias for \"sdl\".\n\nThis output device allows one to show a video stream in an SDL window. Only one SDL window is allowed per application, so you can have only one instance of this output device in an application.\n\nTo enable this output device you need libsdl installed on your system when configuring your build.\n\nFor more information about SDL, check: http://www.libsdl.org/\n\nThe window created by the device can be controlled through the following interactive commands.\n\nThe following command shows the output is an SDL window, forcing its size to the qcif format:\n\nThis output device allows one to show a video stream in a X Window System window.\n\nFor more information about XVideo see http://www.x.org/.\n• Decode, display and encode video input with at the same time:\n• Decode and display the input video to multiple X11 windows:\n\nThe audio resampler supports the following named options.\n\nOptions may be set by specifying - in the FFmpeg tools, = for the aresample filter, by setting the value explicitly in the options or using the API for programmatic use.\n\nThe video scaler supports the following named options.\n\nOptions may be set by specifying - in the FFmpeg tools, with a few API-only exceptions noted below. For programmatic use, they can be set explicitly in the options or through the API.\n\nFiltering in FFmpeg is enabled through the libavfilter library.\n\nIn libavfilter, a filter can have multiple inputs and multiple outputs. To illustrate the sorts of things that are possible, we consider the following filtergraph.\n\nThis filtergraph splits the input stream in two streams, then sends one stream through the crop filter and the vflip filter, before merging it back with the other stream by overlaying it on top. You can use the following command to achieve this:\n\nThe result will be that the top half of the video is mirrored onto the bottom half of the output video.\n\nFilters in the same linear chain are separated by commas, and distinct linear chains of filters are separated by semicolons. In our example, are in one linear chain, and are separately in another. The points where the linear chains join are labelled by names enclosed in square brackets. In the example, the split filter generates two outputs that are associated to the labels and .\n\nThe stream sent to the second output of , labelled as , is processed through the filter, which crops away the lower half part of the video, and then vertically flipped. The filter takes in input the first unchanged output of the split filter (which was labelled as ), and overlay on its lower half the output generated by the filterchain.\n\nSome filters take in input a list of parameters: they are specified after the filter name and an equal sign, and are separated from each other by a colon.\n\nThere exist so-called that do not have an audio/video input, and that will not have audio/video output.\n\nThe program included in the FFmpeg directory can be used to parse a filtergraph description and issue a corresponding textual representation in the dot language.\n\nto see how to use .\n\nYou can then pass the dot description to the program (from the graphviz suite of programs) and obtain a graphical representation of the filtergraph.\n\nFor example the sequence of commands:\n\ncan be used to create and display an image representing the graph described by the string. Note that this string must be a complete self-contained graph, with its inputs and outputs explicitly defined. For example if your command line is of the form:\n\nyour string will need to be of the form:\n\nyou may also need to set the parameters and add a filter in order to simulate a specific input file.\n\nA filtergraph is a directed graph of connected filters. It can contain cycles, and there can be multiple links between a pair of filters. Each link has one input pad on one side connecting it to one filter from which it takes its input, and one output pad on the other side connecting it to one filter accepting its output.\n\nEach filter in a filtergraph is an instance of a filter class registered in the application, which defines the features and the number of input and output pads of the filter.\n\nA filter with no input pads is called a \"source\", and a filter with no output pads is called a \"sink\".\n\nA filtergraph has a textual representation, which is recognized by the / / and options in and / in , and by the function defined in .\n\nA filterchain consists of a sequence of connected filters, each one connected to the previous one in the sequence. A filterchain is represented by a list of \",\"-separated filter descriptions.\n\nA filtergraph consists of a sequence of filterchains. A sequence of filterchains is represented by a list of \";\"-separated filterchain descriptions.\n\nA filter is represented by a string of the form: [ ]...[ ] @ = [ ]...[ ]\n\nis the name of the filter class of which the described filter is an instance of, and has to be the name of one of the filter classes registered in the program optionally followed by \"@ \". The name of the filter class is optionally followed by a string \"= \".\n\nis a string which contains the parameters used to initialize the filter instance. It may have one of two forms:\n• A ’:’-separated list of . In this case, the keys are assumed to be the option names in the order they are declared. E.g. the filter declares three options in this order – , and . Then the parameter list means that the value is assigned to the option , to and to .\n• A ’:’-separated list of mixed direct and long pairs. The direct must precede the pairs, and follow the same constraints order of the previous point. The following pairs can be set in any preferred order.\n\nIf the option value itself is a list of items (e.g. the filter takes a list of pixel formats), the items in the list are usually separated by ‘ ’.\n\nThe list of arguments can be quoted using the character ‘ ’ as initial and ending mark, and the character ‘ ’ for escaping the characters within the quoted text; otherwise the argument string is considered terminated when the next special character (belonging to the set ‘ ’) is encountered.\n\nA special syntax implemented in the CLI tool allows loading option values from files. This is done be prepending a slash ’/’ to the option name, then the supplied value is interpreted as a path from which the actual value is loaded. E.g.\n\nwill load the text to be drawn from . API users wishing to implement a similar feature should use the functions together with custom IO code.\n\nThe name and arguments of the filter are optionally preceded and followed by a list of link labels. A link label allows one to name a link and associate it to a filter output or input pad. The preceding labels ... , are associated to the filter input pads, the following labels ... , are associated to the output pads.\n\nWhen two link labels with the same name are found in the filtergraph, a link between the corresponding input and output pad is created.\n\nIf an output pad is not labelled, it is linked by default to the first unlabelled input pad of the next filter in the filterchain. For example in the filterchain\n\nthe split filter instance has two output pads, and the overlay filter instance two input pads. The first output pad of split is labelled \"L1\", the first input pad of overlay is labelled \"L2\", and the second output pad of split is linked to the second input pad of overlay, which are both unlabelled.\n\nIn a filter description, if the input label of the first filter is not specified, \"in\" is assumed; if the output label of the last filter is not specified, \"out\" is assumed.\n\nIn a complete filterchain all the unlabelled filter input and output pads must be connected. A filtergraph is considered valid if all the filter input and output pads of all the filterchains are connected.\n\nLeading and trailing whitespaces (space, tabs, or line feeds) separating tokens in the filtergraph specification are ignored. This means that the filtergraph can be expressed using empty lines and spaces to improve redability.\n\nFor example, the filtergraph:\n\ncan be represented as:\n\nLibavfilter will automatically insert scale filters where format conversion is required. It is possible to specify swscale flags for those automatically inserted scalers by prepending to the filtergraph description.\n\nHere is a BNF description of the filtergraph syntax:\n\nFiltergraph description composition entails several levels of escaping. See (ffmpeg-utils)the \"Quoting and escaping\" section in the ffmpeg-utils(1) manual for more information about the employed escaping procedure.\n\nA first level escaping affects the content of each filter option value, which may contain the special character used to separate values, or one of the escaping characters .\n\nA second level escaping affects the whole filter description, which may contain the escaping characters or the special characters used by the filtergraph description.\n\nFinally, when you specify a filtergraph on a shell commandline, you need to perform a third level escaping for the shell special characters contained within it.\n\nFor example, consider the following string to be embedded in the drawtext filter description value:\n\nThis string contains the special escaping character, and the special character, so it needs to be escaped in this way:\n\nA second level of escaping is required when embedding the filter description in a filtergraph description, in order to escape all the filtergraph special characters. Thus the example above becomes:\n\n(note that in addition to the escaping special characters, also needs to be escaped).\n\nFinally an additional level of escaping is needed when writing the filtergraph description in a shell command, which depends on the escaping rules of the adopted shell. For example, assuming that is special and needs to be escaped with another , the previous string will finally result in:\n\nIn order to avoid cumbersome escaping when using a commandline tool accepting a filter specification as input, it is advisable to avoid direct inclusion of the filter or options specification in the shell.\n\nFor example, in case of the drawtext filter, you might prefer to use the option in place of to specify the text to render.\n\nSome filters support a generic option. For the filters supporting timeline editing, this option can be set to an expression which is evaluated before sending a frame to the filter. If the evaluation is non-zero, the filter will be enabled, otherwise the frame will be sent unchanged to the next filter in the filtergraph.\n\nThe expression accepts the following values:\n\nAdditionally, these filters support an command that can be used to re-define the expression.\n\nLike any other filtering option, the option follows the same rules.\n\nFor example, to enable a blur filter (smartblur) from 10 seconds to 3 minutes, and a curves filter starting at 3 seconds:\n\nSee to view which filters have timeline support.\n\nSome options can be changed during the operation of the filter using a command. These options are marked ’T’ on the output of . The name of the command is the name of the option and the argument is the new value.\n\n35 Options for filters with several inputs (framesync)\n\nSome filters with several inputs support a common set of options. These options can only be set by name, not with the short notation.\n\nWhen you configure your FFmpeg build, you can disable any of the existing filters using . The configure output will show the audio filters included in your build.\n\nBelow is a description of the currently available audio filters.\n\nApply Affine Projection algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to estimate unknown audio based on multiple input audio samples. Affine projection algorithm can make trade-offs between computation complexity with convergence speed.\n\nA description of the accepted options follows.\n\nA compressor is mainly used to reduce the dynamic range of a signal. Especially modern music is mostly compressed at a high ratio to improve the overall loudness. It’s done to get the highest attention of a listener, \"fatten\" the sound and bring more \"power\" to the track. If a signal is compressed too much it may sound dull or \"dead\" afterwards or it may start to \"pump\" (which could be a powerful effect but can also destroy a track completely). The right compression is the key to reach a professional sound and is the high art of mixing and mastering. Because of its complex settings it may take a long time to get the right feeling for this kind of effect.\n\nCompression is done by detecting the volume above a chosen level and dividing it by the factor set with . So if you set the threshold to -12dB and your signal reaches -6dB a ratio of 2:1 will result in a signal at -9dB. Because an exact manipulation of the signal would cause distortion of the waveform the reduction can be levelled over the time. This is done by setting \"Attack\" and \"Release\". determines how long the signal has to rise above the threshold before any reduction will occur and sets the time the signal has to fall below the threshold to reduce the reduction again. Shorter signals than the chosen attack time will be left untouched. The overall reduction of the signal can be made up afterwards with the setting. So compressing the peaks of a signal about 6dB and raising the makeup to this level results in a signal twice as loud than the source. To gain a softer entry in the compression the flattens the hard edge at the threshold in the range of the chosen decibels.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nCopy the input audio source unchanged to the output. This is mainly useful for testing purposes.\n\nApply cross fade from one input audio stream to another input audio stream. The cross fade is applied for specified duration near the end of first stream.\n\nThe filter accepts the following options:\n• Cross fade from one input to another:\n• Cross fade from one input to another but without overlapping:\n\nThis filter splits audio stream into two or more frequency ranges. Summing all streams back will give flat output.\n\nThe filter accepts the following options:\n• Split input audio stream into two bands (low and high) with split frequency of 1500 Hz, each band will be in separate stream:\n• Same as above, but with higher filter order:\n• Same as above, but also with additional middle band (frequencies between 1500 and 8000):\n\nThis filter is bit crusher with enhanced functionality. A bit crusher is used to audibly reduce number of bits an audio signal is sampled with. This doesn’t change the bit depth at all, it just produces the effect. Material reduced in bit depth sounds more harsh and \"digital\". This filter is able to even round to continuous values instead of discrete bit depths. Additionally it has a D/C offset which results in different crushing of the lower and the upper half of the signal. An Anti-Aliasing setting is able to produce \"softer\" crushing sounds.\n\nAnother feature of this filter is the logarithmic mode. This setting switches from linear distances between bits to logarithmic ones. The result is a much more \"natural\" sounding crusher which doesn’t gate low signals for example. The human ear has a logarithmic perception, so this kind of crushing is much more pleasant. Logarithmic crushing is also able to get anti-aliased.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDelay audio filtering until a given wallclock timestamp. See the cue filter.\n\nSamples detected as impulsive noise are replaced by interpolated samples using autoregressive modelling.\n\nSamples detected as clipped are replaced by interpolated samples using autoregressive modelling.\n\nThe filter accepts the following options:\n\nDelay one or more audio channels.\n\nSamples in delayed channel are filled with silence.\n\nThe filter accepts the following option:\n• Delay first channel by 1.5 seconds, the third channel by 0.5 seconds and leave the second channel (and any other channels that may be present) unchanged.\n• Delay second channel by 500 samples, the third channel by 700 samples and leave the first channel (and any other channels that may be present) unchanged.\n• Delay all channels by same number of samples:\n\nThis filter shall be placed before any filter that can produce denormals.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n\nApplying both filters one after another produces original audio.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n• Apply spectral compression to all frequencies with threshold of -50 dB and 1:6 ratio:\n• Similar to above but with 1:2 ratio and filtering only front center channel:\n• Apply spectral noise gate to all frequencies with threshold of -85 dB and with short attack time and short release time:\n• Apply spectral expansion to all frequencies with threshold of -10 dB and 1:2 ratio:\n• Apply limiter to max -60 dB to all frequencies, with attack of 2 ms and release of 10 ms:\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nEchoes are reflected sound and can occur naturally amongst mountains (and sometimes large buildings) when talking or shouting; digital echo effects emulate this behaviour and are often used to help fill out the sound of a single instrument or vocal. The time difference between the original signal and the reflection is the , and the loudness of the reflected signal is the . Multiple echoes can have different delays and decays.\n\nA description of the accepted parameters follows.\n• Make it sound as if there are twice as many instruments as are actually playing:\n• If delay is very short, then it sounds like a (metallic) robot playing music:\n• A longer delay will sound like an open air concert in the mountains:\n• Same as above but with one more mountain:\n\nAudio emphasis filter creates or restores material directly taken from LPs or emphased CDs with different filter curves. E.g. to store music on vinyl the signal has to be altered by a filter first to even out the disadvantages of this recording medium. Once the material is played back the inverse filter has to be applied to restore the distortion of the frequency response.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nModify an audio signal according to the specified expressions.\n\nThis filter accepts one or more expressions (one for each channel), which are evaluated and used to modify a corresponding audio signal.\n\nIt accepts the following parameters:\n\nEach expression in can contain the following constants and functions:\n\nNote: this filter is slow. For faster processing you should use a dedicated filter.\n• Invert phase of the second channel:\n\nAn exciter is used to produce high sound that is not present in the original signal. This is done by creating harmonic distortions of the signal which are restricted in range and added to the original signal. An Exciter raises the upper end of an audio signal without simply raising the higher frequencies like an equalizer would do to create a more \"crisp\" or \"brilliant\" sound.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n• Fade in first 15 seconds of audio:\n• Fade out last 25 seconds of a 900 seconds audio:\n\nA description of the accepted parameters follows.\n\nThis filter supports the some above mentioned options as commands.\n• Reduce white noise by 10dB, and use previously measured noise floor of -40dB:\n• Reduce white noise by 10dB, also set initial noise floor to -80dB and enable automatic tracking of noise floor so noise floor will gradually change during processing:\n• Reduce noise by 20dB, using noise floor of -40dB and using commands to take noise profile of first 0.4 seconds of input audio:\n• Leave almost only low frequencies in audio:\n\nThis filter is designed for applying long FIR filters, up to 60 seconds long.\n\nIt can be used as component for digital crossover filters, room equalization, cross talk cancellation, wavefield synthesis, auralization, ambiophonics, ambisonics and spatialization.\n\nThis filter uses the streams higher than first one as FIR coefficients. If the non-first stream holds a single channel, it will be used for all input channels in the first stream, otherwise the number of channels in the non-first stream must be same as the number of channels in the first stream.\n\nIt accepts the following parameters:\n• Apply reverb to stream using mono IR file as second input, complete command using ffmpeg:\n• Apply true stereo processing given input stereo stream, and two stereo impulse responses for left and right channel, the impulse response files are files with names l_ir.wav and r_ir.wav, and setting irnorm option value:\n• Similar to above example, but with explicitly set to estimated value and with disabled:\n\nSet output format constraints for the input audio. The framework will negotiate the most appropriate format to minimize conversions.\n\nIt accepts the following parameters:\n\nIf a parameter is omitted, all values are allowed.\n\nForce the output to either unsigned 8-bit or signed 16-bit stereo\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nA gate is mainly used to reduce lower parts of a signal. This kind of signal processing reduces disturbing noise between useful signals.\n\nGating is done by detecting the volume below a chosen level and dividing it by the factor set with . The bottom of the noise floor is set via . Because an exact manipulation of the signal would cause distortion of the waveform the reduction can be levelled over time. This is done by setting and .\n\ndetermines how long the signal has to fall below the threshold before any reduction will occur and sets the time the signal has to rise above the threshold to reduce the reduction again. Shorter signals than the chosen attack time will be left untouched.\n\nThis filter supports the all above options as commands.\n\nIt accepts the following parameters:\n\nCoefficients in and format are separated by spaces and are in ascending order.\n\nCoefficients in format are separated by spaces and order of coefficients doesn’t matter. Coefficients in format are complex numbers with imaginary unit.\n\nDifferent coefficients and gains can be provided for every channel, in such case use ’|’ to separate coefficients or gains. Last provided coefficients will be used for all remaining channels.\n• Apply 2 pole elliptic notch at around 5000Hz for 48000 Hz sample rate:\n• Same as above but in format:\n\nThe limiter prevents an input signal from rising over a desired threshold. This limiter uses lookahead technology to prevent your signal from distorting. It means that there is a small delay after the signal is processed. Keep in mind that the delay it produces is the attack time you set.\n\nThe filter accepts the following options:\n\nDepending on picked setting it is recommended to upsample input 2x or 4x times with aresample before applying this filter.\n\nApply a two-pole all-pass filter with central frequency (in Hz) , and filter-width . An all-pass filter changes the audio’s frequency to phase relationship without changing its frequency to amplitude relationship.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThe filter accepts the following options:\n\nMerge two or more audio streams into a single multi-channel stream.\n\nThe filter accepts the following options:\n\nIf the channel layouts of the inputs are disjoint, and therefore compatible, the channel layout of the output will be set accordingly and the channels will be reordered as necessary. If the channel layouts of the inputs are not disjoint, the output will have all the channels of the first input then all the channels of the second input, in that order, and the channel layout of the output will be the default value corresponding to the total number of channels.\n\nFor example, if the first input is in 2.1 (FL+FR+LF) and the second input is FC+BL+BR, then the output will be in 5.1, with the channels in the following order: a1, a2, b1, a3, b2, b3 (a1 is the first channel of the first input, b1 is the first channel of the second input).\n\nOn the other hand, if both input are in stereo, the output channels will be in the default order: a1, a2, b1, b2, and the channel layout will be arbitrarily set to 4.0, which may or may not be the expected value.\n\nAll inputs must have the same sample rate, and format.\n\nIf inputs do not have the same duration, the output will stop with the shortest.\n\nNote that this filter only supports float samples (the and audio filters support many formats). If the input has integer samples then aresample will be automatically inserted to perform the conversion to float samples.\n\nIt accepts the following parameters:\n• This will mix 3 input audio streams to a single output with the same duration as the first input and a dropout transition time of 3 seconds:\n• This will mix one vocal and one music input audio stream to a single output with the same duration as the longest input. The music will have quarter the weight as the vocals, and the inputs are not normalized:\n\nThis filter supports the following commands:\n\nMultiply first audio stream with second audio stream and store result in output audio stream. Multiplication is done by multiplying each sample from first stream with sample at same position from second stream.\n\nWith this element-wise multiplication one can create amplitude fades and amplitude modulations.\n\nIt accepts the following parameters:\n• Lower gain by 10 of central frequency 200Hz and width 100 Hz for first 2 channels using Chebyshev type 1 filter:\n\nThis filter supports the following commands:\n\nEach sample is adjusted by looking for other samples with similar contexts. This context similarity is defined by comparing their surrounding patches of size . Patches are searched in an area of around the sample.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply Normalized Least-Mean-(Squares|Fourth) algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean square of the error signal (difference between the desired, 2nd input audio stream and the actual signal, the 1st input audio stream).\n\nA description of the accepted options follows.\n• One of many usages of this filter is noise reduction, input audio is filtered with same samples that are delayed by fixed amount, one such example for stereo audio is:\n\nThis filter supports the same commands as options, excluding option .\n\nPass the audio source unchanged to the output.\n\nPad the end of an audio stream with silence.\n\nThis can be used together with to extend audio streams to the same length as the video stream.\n\nA description of the accepted options follows.\n\nIf neither the nor the nor nor option is set, the filter will add silence to the end of the input stream indefinitely.\n\nNote that for ffmpeg 4.4 and earlier a zero or also caused the filter to add silence indefinitely.\n• Add 1024 samples of silence to the end of the input:\n• Make sure the audio output will contain at least 10000 samples, pad the input with silence if required:\n• Use to pad the audio input with silence, so that the video stream will always result the shortest and will be converted until the end in the output file when using the option:\n\nA phaser filter creates series of peaks and troughs in the frequency spectrum. The position of the peaks and troughs are modulated so that they vary over time, creating a sweeping effect.\n\nA description of the accepted parameters follows.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nAudio pulsator is something between an autopanner and a tremolo. But it can produce funny stereo effects as well. Pulsator changes the volume of the left and right channel based on a LFO (low frequency oscillator) with different waveforms and shifted phases. This filter have the ability to define an offset between left and right channel. An offset of 0 means that both LFO shapes match each other. The left and right channel are altered equally - a conventional tremolo. An offset of 50% means that the shape of the right channel is exactly shifted in phase (or moved backwards about half of the frequency) - pulsator acts as an autopanner. At 1 both curves match again. Every setting in between moves the phase shift gapless between all stages and produces some \"bypassing\" sounds with sine and triangle waveforms. The more you set the offset near 1 (starting from the 0.5) the faster the signal passes from the left to the right speaker.\n\nThe filter accepts the following options:\n\nResample the input audio to the specified parameters, using the libswresample library. If none are specified then the filter will automatically convert between its input and output.\n\nThis filter is also able to stretch/squeeze the audio data to make it match the timestamps or to inject silence / cut out audio to make it match the timestamps, do a combination of both or do neither.\n\nThe filter accepts the syntax [ :] , where expresses a sample rate and is a list of = pairs, separated by \":\". See the (ffmpeg-resampler)\"Resampler Options\" section in the ffmpeg-resampler(1) manual for the complete list of supported options.\n• Stretch/squeeze samples to the given timestamps, with a maximum of 1000 samples per second compensation:\n\nWarning: This filter requires memory to buffer the entire clip, so trimming is suggested.\n• Take the first 5 seconds of a clip, and reverse it.\n\nApply Recursive Least Squares algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to mimic a desired filter by recursively finding the filter coefficients that relate to producing the minimal weighted linear least squares cost function of the error signal (difference between the desired, 2nd input audio stream and the actual signal, the 1st input audio stream).\n\nA description of the accepted options follows.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nSet the number of samples per each output audio frame.\n\nThe last output packet may contain a different number of samples, as the filter will flush all the remaining samples when the input audio signals its end.\n\nThe filter accepts the following options:\n\nFor example, to set the number of per-frame samples to 1234 and disable padding for the last frame, use:\n\nSet the sample rate without altering the PCM data. This will result in a change of speed and pitch.\n\nThe filter accepts the following options:\n\nShow a line containing various information for each input audio frame. The input audio is not modified.\n\nThe shown line contains a sequence of key/value pairs of the form : .\n\nThe following values are shown in the output:\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nSoft clipping is a type of distortion effect where the amplitude of a signal is saturated along a smooth curve, rather than the abrupt shape of hard-clipping.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplay frequency domain statistical information about the audio channels. Statistics are calculated and stored as metadata for each audio channel and for each audio frame.\n\nIt accepts the following option:\n\nA list of each metadata key follows:\n\nThis filter uses PocketSphinx for speech recognition. To enable compilation of this filter, you need to configure FFmpeg with .\n\nIt accepts the following options:\n\nThe filter exports recognized speech as the frame metadata .\n\nDisplay time domain statistical information about the audio channels. Statistics are calculated and displayed for each audio channel and, where applicable, an overall figure is also given.\n\nIt accepts the following option:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter allows to set custom, steeper roll off than highpass filter, and thus is able to more attenuate frequency content in stop-band.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts exactly one parameter, the audio tempo. If not specified then the filter will assume nominal 1.0 tempo. Tempo must be in the [0.5, 100.0] range.\n\nNote that tempo greater than 2 will skip some samples rather than blend them in. If for any reason this is a concern it is always possible to daisy-chain several instances of atempo to achieve the desired product tempo.\n• To speed up audio to 300% tempo:\n• To speed up audio to 300% tempo by daisy-chaining two atempo instances:\n\nThis filter supports the following commands:\n\nThis filter apply any spectral roll-off slope over any specified frequency band.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nTrim the input so that the output contains one continuous subpart of the input.\n\nIt accepts the following parameters:\n\n, , and are expressed as time duration specifications; see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual.\n\nNote that the first two sets of the start/end options and the option look at the frame timestamp, while the _sample options simply count the samples that pass through the filter. So start/end_pts and start/end_sample will give different results when the timestamps are wrong, inexact or do not start at zero. Also note that this filter does not modify the timestamps. If you wish to have the output timestamps start at zero, insert the asetpts filter after the atrim filter.\n\nIf multiple start or end options are set, this filter tries to be greedy and keep all samples that match at least one of the specified constraints. To keep only the part that matches all the constraints at once, chain multiple atrim filters.\n\nThe defaults are such that all the input is kept. So it is possible to set e.g. just the end values to keep everything before the specified time.\n• Drop everything except the second minute of input:\n• Keep only the first 1000 samples:\n\nResulted samples are always between -1 and 1 inclusive. If result is 1 it means two input samples are highly correlated in that selected segment. Result 0 means they are not correlated at all. If result is -1 it means two input samples are out of phase, which means they cancel each other.\n\nThe filter accepts the following options:\n\nApply a two-pole Butterworth band-pass filter with central frequency , and (3dB-point) band-width width. The option selects a constant skirt gain (peak gain = Q) instead of the default: constant 0dB peak gain. The filter roll off at 6dB per octave (20dB per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nApply a two-pole Butterworth band-reject filter with central frequency , and (3dB-point) band-width . The filter roll off at 6dB per octave (20dB per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nBoost or cut the bass (lower) frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nApply a biquad IIR filter with the given coefficients. Where , , and , , are the numerator and denominator coefficients respectively. and , specify which channels to filter, by default all available are filtered.\n\nThis filter supports the following commands:\n\nBauer stereo to binaural transformation, which improves headphone listening of stereo audio records.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n\nIf no mapping is present, the filter will implicitly map input channels to output channels, preserving indices.\n• For example, assuming a 5.1+downmix input MOV file, will create an output WAV file tagged as stereo from the downmix channels of the input.\n\nSplit each channel from an input audio stream into a separate output stream.\n\nIt accepts the following parameters:\n• For example, assuming a stereo input MP3 file, will create an output Matroska file with two audio streams, one containing only the left channel and the other the right channel.\n\nCan make a single vocal sound like a chorus, but can also be applied to instrumentation.\n\nChorus resembles an echo effect with a short delay, but whereas with echo the delay is constant, with chorus, it is varied using using sinusoidal or triangular modulation. The modulation depth defines the range the modulated delay is played before or after the delay. Hence the delayed sound will sound slower or faster, that is the delayed sound tuned around the original one, like in a chorus where some vocals are slightly off key.\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n• Make music with both quiet and loud passages suitable for listening to in a noisy environment: Another example for audio with whisper and explosion parts:\n• A noise gate for when the noise is at a lower level than the signal:\n• Here is another noise gate, this time for when the noise is at a higher level than the signal (making it, in some ways, similar to squelch):\n\nCompensation Delay Line is a metric based delay to compensate differing positions of microphones or speakers.\n\nFor example, you have recorded guitar with two microphones placed in different locations. Because the front of sound wave has fixed speed in normal conditions, the phasing of microphones can vary and depends on their location and interposition. The best sound mix can be achieved when these microphones are in phase (synchronized). Note that a distance of ~30 cm between microphones makes one microphone capture the signal in antiphase to the other microphone. That makes the final mix sound moody. This filter helps to solve phasing problems by adding different delays to each microphone track and make them synchronized.\n\nThe best result can be reached when you take one track as base and synchronize other tracks one by one with it. Remember that synchronization/delay tolerance depends on sample rate, too. Higher sample rates will give more tolerance.\n\nThe filter accepts the following parameters:\n\nThis filter supports the all above options as commands.\n\nCrossfeed is the process of blending the left and right channels of stereo audio recording. It is mainly used to reduce extreme stereo separation of low frequencies.\n\nThe intent is to produce more speaker like sound to the listener.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter linearly increases differences between each audio sample.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis can be useful to remove a DC offset (caused perhaps by a hardware problem in the recording chain) from the audio. The effect of a DC offset is reduced headroom and hence volume. The astats filter can be used to determine if a signal has a DC offset.\n\nThis filter accepts stereo input and produce surround (3.0) channels output. The newly produced front center channel have enhanced speech dialogue originally available in both stereo channels. This filter outputs front left and front right channels same as available in stereo input.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDR values of 14 and higher is found in very dynamic material. DR of 8 to 13 is found in transition material. And anything less that 8 have very poor dynamics and is very compressed.\n\nThe filter accepts the following options:\n\nThis filter applies a certain amount of gain to the input audio in order to bring its peak magnitude to a target level (e.g. 0 dBFS). However, in contrast to more \"simple\" normalization algorithms, the Dynamic Audio Normalizer *dynamically* re-adjusts the gain factor to the input audio. This allows for applying extra gain to the \"quiet\" sections of the audio while avoiding distortions or clipping the \"loud\" sections. In other words: The Dynamic Audio Normalizer will \"even out\" the volume of quiet and loud sections, in the sense that the volume of each section is brought to the same target level. Note, however, that the Dynamic Audio Normalizer achieves this goal *without* applying \"dynamic range compressing\". It will retain 100% of the dynamic range *within* each section of the audio file.\n\nThis filter supports the all above options as commands.\n\nMake audio easier to listen to on headphones.\n\nThis filter adds ‘cues’ to 44.1kHz stereo (i.e. audio CD format) audio so that when listened to on headphones the stereo image is moved from inside your head (standard for headphones) to outside and in front of the listener (standard for speakers).\n\nApply a two-pole peaking equalisation (EQ) filter. With this filter, the signal-level at and around a selected frequency can be increased or decreased, whilst (unlike bandpass and bandreject filters) that at all other frequencies is unchanged.\n\nIn order to produce complex equalisation curves, this filter can be given several times, each with a different central frequency.\n\nThe filter accepts the following options:\n• Attenuate 10 dB at 1000 Hz, with a bandwidth of 200 Hz:\n• Apply 2 dB gain at 1000 Hz with Q 1 and attenuate 5 dB at 100 Hz with Q 2:\n\nThis filter supports the following commands:\n\nLinearly increases the difference between left and right channels which adds some sort of \"live\" effect to playback.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following option:\n• higher delay with zero phase to compensate delay:\n• lowpass on left channel, highpass on right channel:\n\nThe filter accepts the following options:\n\nNote that this makes most sense to apply on mono signals. With this filter applied to mono signals it give some directionality and stretches its stereo image.\n\nThe filter accepts the following options:\n\nDecodes High Definition Compatible Digital (HDCD) data. A 16-bit PCM stream with embedded HDCD codes is expanded into a 20-bit PCM stream.\n\nThe filter supports the Peak Extend and Low-level Gain Adjustment features of HDCD, and detects the Transient Filter flag.\n\nWhen using the filter with wav, note the default encoding for wav is 16-bit, so the resulting 20-bit stream will be truncated back to 16-bit. Use something like after the filter to get 24-bit PCM output.\n\nThe filter accepts the following options:\n\nApply head-related transfer functions (HRTFs) to create virtual loudspeakers around the user for binaural listening via headphones. The HRIRs are provided via additional streams, for each channel one stereo input stream is needed.\n\nThe filter accepts the following options:\n• Full example using wav files as coefficients with amovie filters for 7.1 downmix, each amovie filter use stereo file with IR coefficients as input. The files give coefficients for each position of virtual loudspeaker:\n• Full example using wav files as coefficients with amovie filters for 7.1 downmix, but now in format.\n\nApply a high-pass filter with 3dB point frequency. The filter can be either single-pole, or double-pole (the default). The filter roll off at 6dB per pole per octave (20dB per pole per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nIt accepts the following parameters:\n\nThe filter will attempt to guess the mappings when they are not specified explicitly. It does so by first trying to find an unused matching input channel and if that fails it picks the first unused input channel.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n• List all available plugins within amp (LADSPA example plugin) library:\n• List all available controls and their valid ranges for plugin from library:\n• Add reverberation to the audio using TAP-plugins (Tom’s Audio Processing plugins):\n• Generate 20 bpm clicks using plugin from the (CAPS) library:\n• Increase volume by 20dB using fast lookahead limiter from Steve Harris collection:\n• Reduce stereo image using from the (CAPS) library:\n• Another white noise, now using (CAPS) library:\n\nThis filter supports the following commands:\n\nEBU R128 loudness normalization. Includes both dynamic and linear normalization modes. Support for both single pass (livestreams, files) and double pass (files) modes. This algorithm can target IL, LRA, and maximum true peak. In dynamic mode, to accurately detect true peaks, the audio stream will be upsampled to 192 kHz. Use the option or filter to explicitly set an output sample rate.\n\nThe filter accepts the following options:\n\nApply a low-pass filter with 3dB point frequency. The filter can be either single-pole or double-pole (the default). The filter roll off at 6dB per pole per octave (20dB per pole per decade).\n\nThe filter accepts the following options:\n• Lowpass only LFE channel, it LFE is not present it does nothing:\n\nThis filter supports the following commands:\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThis filter supports all options that are exported by plugin as commands.\n\nThe input audio is divided into bands using 4th order Linkwitz-Riley IIRs. This is akin to the crossover of a loudspeaker, and results in flat frequency response when absent compander action.\n\nIt accepts the following parameters:\n\nMix channels with specific gain levels. The filter accepts the output channel layout followed by a set of channels definitions.\n\nThis filter is also designed to efficiently remap the channels of an audio stream.\n\nThe filter accepts parameters of the form: \" | | |...\"\n\nIf the ‘=’ in a channel specification is replaced by ‘<’, then the gains for that specification will be renormalized so that the total is 1, thus avoiding clipping noise.\n\nFor example, if you want to down-mix from stereo to mono, but with a bigger factor for the left channel:\n\nA customized down-mix to stereo that works automatically for 3-, 4-, 5- and 7-channels surround:\n\nNote that integrates a default down-mix (and up-mix) system that should be preferred (see \"-ac\" option) unless you have very specific needs.\n\nThe channel remapping will be effective if, and only if:\n• gain coefficients are zeroes or ones,\n• only one input per channel output,\n\nIf all these conditions are satisfied, the filter will notify the user (\"Pure channel mapping detected\"), and use an optimized and lossless method to do the remapping.\n\nFor example, if you have a 5.1 source and want a stereo audio stream by dropping the extra channels:\n\nGiven the same source, you can also switch front left and front right channels and keep the input channel layout:\n\nIf the input is a stereo audio stream, you can mute the front left channel (and still keep the stereo channel layout) with:\n\nStill with a stereo audio stream input, you can copy the right channel in both front left and right:\n\nReplayGain scanner filter. This filter takes an audio stream as an input and outputs it unchanged. At end of filtering it displays and .\n\nThe filter accepts the following exported read-only options:\n\nConvert the audio sample format, sample rate and channel layout. It is not meant to be used directly.\n\nTo enable compilation of this filter, you need to configure FFmpeg with .\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThis filter acts like normal compressor but has the ability to compress detected signal using second input signal. It needs two input streams and returns one output stream. First input stream will be processed depending on second stream signal. The filtered signal then can be filtered with other filters in later stages of processing. See pan and amerge filter.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Full ffmpeg example taking 2 audio inputs, 1st input to be compressed depending on the signal of 2nd input and later compressed signal to be merged with 2nd input:\n\nA sidechain gate acts like a normal (wideband) gate but has the ability to filter the detected signal before sending it to the gain reduction stage. Normally a gate uses the full range signal to detect a level above the threshold. For example: If you cut all lower frequencies from your sidechain signal the gate will decrease the volume of your track only if not enough highs appear. With this technique you are able to reduce the resonation of a natural drum or remove \"rumbling\" of muted strokes from a heavily distorted guitar. It needs two input streams and returns one output stream. First input stream will be processed depending on second stream signal.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter logs a message when it detects that the input audio volume is less or equal to a noise tolerance value for a duration greater or equal to the minimum detected noise duration.\n\nThe printed times and duration are expressed in seconds. The or metadata key is set on the first frame whose timestamp equals or exceeds the detection duration and it contains the timestamp of the first frame of the silence.\n\nThe or and or metadata keys are set on the first frame after the silence. If is enabled, and each channel is evaluated separately, the suffixed keys are used, and corresponds to the channel number.\n\nThe filter accepts the following options:\n• Complete example with to detect silence with 0.0001 noise tolerance in :\n\nRemove silence from the beginning, middle or end of the audio.\n\nThe filter accepts the following options:\n• The following example shows how this filter can be used to start a recording that does not contain the delay at the start which usually occurs between pressing the record button and the start of the performance:\n• Trim all silence encountered from beginning to end where there is more than 1 second of silence in audio:\n• Trim all digital silence samples, using peak detection, from beginning to end where there is more than 0 samples of digital silence in audio and digital silence is detected in all channels at same positions in stream:\n• Trim every 2nd encountered silence period from beginning to end where there is more than 1 second of silence per silence period in audio:\n• Similar as above, but keep maximum of 0.5 seconds of silence from each trimmed period:\n• Similar as above, but keep maximum of 1.5 seconds of silence from start of audio:\n\nThis filter supports some above options as commands.\n\nSOFAlizer uses head-related transfer functions (HRTFs) to create virtual loudspeakers around the user for binaural listening via headphones (audio formats up to 9 channels supported). The HRTFs are stored in SOFA files (see http://www.sofacoustics.org/ for a database). SOFAlizer is developed at the Acoustics Research Institute (ARI) of the Austrian Academy of Sciences.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThe filter accepts the following options:\n• Using ClubFritz12 sofa file and bigger radius with small rotation:\n• Similar as above but with custom speaker positions for front left, front right, back left and back right and also with custom gain:\n\nThis filter expands or compresses each half-cycle of audio samples (local set of samples all above or all below zero and between two nearest zero crossings) depending on threshold value, so audio reaches target peak value under conditions controlled by below options.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter has some handy utilities to manage stereo signals, for converting M/S stereo recordings to L/R signal while having control over the parameters or spreading the stereo image of master track.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter enhance the stereo effect by suppressing signal common to both channels and by delaying the signal of left into right and vice versa, thereby widening the stereo effect.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options except as commands.\n\nThe filter accepts the following options:\n\nThis filter allows to produce multichannel output from audio stream.\n\nThe filter accepts the following options:\n\nBoost or cut the lower frequencies and cut or boost higher frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports some options as commands.\n\nBoost or cut treble (upper) frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter accepts stereo input and produce stereo with LFE (2.1) channels output. The newly produced LFE channel have enhanced virtual bass originally obtained from both stereo channels. This filter outputs front left and front right channels unchanged as available in stereo input.\n\nThe filter accepts the following options:\n\nIt accepts the following parameters:\n\nThe volume expression can contain the following parameters.\n\nNote that when is set to ‘ ’ only the and variables are available, all other variables will evaluate to NAN.\n\nThis filter supports the following commands:\n• Halve the input audio volume: In all the above example the named key for can be omitted, for example like in:\n• Fade volume after time 10 with an annihilation period of 5 seconds:\n\nDetect the volume of the input video.\n\nThe filter has no parameters. It supports only 16-bit signed integer samples, so the input will be converted when needed. Statistics about the volume will be printed in the log when the input stream end is reached.\n\nIn particular it will show the mean volume (root mean square), maximum volume (on a per-sample basis), and the beginning of a histogram of the registered volume values (from the maximum value to a cumulated 1/1000 of the samples).\n\nAll volumes are in decibels relative to the maximum PCM value.\n\nHere is an excerpt of the output:\n• The mean square energy is approximately -27 dB, or 10^-2.7.\n• The largest sample is at -4 dB, or more precisely between -4 dB and -5 dB.\n• There are 6 samples at -4 dB, 62 at -5 dB, 286 at -6 dB, etc.\n\nIn other words, raising the volume by +4 dB does not cause any clipping, raising it by +5 dB causes clipping for 6 samples, etc.\n\nBelow is a description of the currently available audio sources.\n\nBuffer audio frames, and make them available to the filter chain.\n\nThis source is mainly intended for a programmatic use, in particular through the interface defined in .\n\nIt accepts the following parameters:\n\nwill instruct the source to accept planar 16bit signed stereo at 44100Hz. Since the sample format with name \"s16p\" corresponds to the number 6 and the \"stereo\" channel layout corresponds to the value 0x3, this is equivalent to:\n\nGenerate an audio signal specified by an expression.\n\nThis source accepts in input one or more expressions (one for each channel), which are evaluated and used to generate a corresponding audio signal.\n\nThis source accepts the following options:\n\nEach expression in can contain the following constants:\n• Generate a sin signal with frequency of 440 Hz, set sample rate to 8000 Hz:\n• Generate a two channels signal, specify the channel layout (Front Center + Back Center) explicitly:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe null audio source, return unprocessed audio frames. It is mainly useful as a template and to be employed in analysis / debugging tools, or as the source for filters which ignore the input data (for example the sox synth filter).\n\nThis source accepts the following options:\n• Set the sample rate to 48000 Hz and the channel layout to AV_CH_LAYOUT_MONO.\n• Do the same operation with a more obvious syntax:\n\nAll the parameters need to be explicitly defined.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nNote that versions of the flite library prior to 2.0 are not thread-safe.\n\nThe filter accepts the following options:\n• Read from file , and synthesize the text using the standard flite voice:\n• Read the specified text selecting the voice: flite=text='So fare thee well, poor devil of a Sub-Sub, whose commentator I am':voice=slt\n• Input text to ffmpeg: ffmpeg -f lavfi -i flite=text='So fare thee well, poor devil of a Sub-Sub, whose commentator I am':voice=slt\n• Make speak the specified text, using and the device: ffplay -f lavfi flite=text='No more be grieved for which that thou hast done.'\n\nFor more information about libflite, check: http://www.festvox.org/flite/\n\nThe filter accepts the following options:\n• Generate 60 seconds of pink noise, with a 44.1 kHz sampling rate and an amplitude of 0.5:\n\nThe resulting stream can be used with afir filter for phase-shifting the signal by 90 degrees.\n\nThis is used in many matrix coding schemes and for analytic signal generation. The process is often written as a multiplication by i (or j), the imaginary unit.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nGenerate an audio signal made of a sine wave with amplitude 1/8.\n\nThe filter accepts the following options:\n• Generate a 220 Hz sine wave with a 880 Hz beep each second, for 5 seconds:\n\nBelow is a description of the currently available audio sinks.\n\nBuffer audio frames, and make them available to the end of filter chain.\n\nThis sink is mainly intended for programmatic use, in particular through the interface defined in or the options system.\n\nIt accepts a pointer to an AVABufferSinkContext structure, which defines the incoming buffers’ formats, to be passed as the opaque parameter to for initialization.\n\nNull audio sink; do absolutely nothing with the input audio. It is mainly useful as a template and for use in analysis / debugging tools.\n\nWhen you configure your FFmpeg build, you can disable any of the existing filters using . The configure output will show the video filters included in your build.\n\nBelow is a description of the currently available video filters.\n\nThe frame data is passed through unchanged, but metadata is attached to the frame indicating regions of interest which can affect the behaviour of later encoding. Multiple regions can be marked by applying the filter multiple times.\n• Mark the centre quarter of the frame as interesting.\n• Mark the 100-pixel-wide region on the left edge of the frame as very uninteresting (to be encoded at much lower quality than the rest of the frame).\n\nExtract the alpha component from the input as a grayscale video. This is especially useful with the filter.\n\nAdd or replace the alpha component of the primary input with the grayscale value of a second input. This is intended for use with to allow the transmission or storage of frame sequences that have alpha in a format that doesn’t support an alpha channel.\n\nFor example, to reconstruct full frames from a normal YUV-encoded video and a separate video created with , you might use:\n\nAmplify differences between current pixel and pixels of adjacent frames in same pixel location.\n\nThis filter accepts the following options:\n\nThis filter supports the following commands that corresponds to option of same name:\n\nSame as the subtitles filter, except that it doesn’t require libavcodec and libavformat to work. On the other hand, it is limited to ASS (Advanced Substation Alpha) subtitles files.\n\nThis filter accepts the following option in addition to the common options from the subtitles filter:\n\nApply an Adaptive Temporal Averaging Denoiser to the video input.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options except option . The command accepts the same syntax of the corresponding option.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nCompute the bounding box for the non-black pixels in the input frame luma plane.\n\nThis filter computes the bounding box containing all the pixels with a luma value greater than the minimum allowed value. The parameters describing the bounding box are printed on the filter log.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nDetect video intervals that are (almost) completely black. Can be useful to detect chapter transitions, commercials, or invalid recordings.\n\nThe filter outputs its detection analysis to both the log as well as frame metadata. If a black segment of at least the specified minimum duration is found, a line with the start and end timestamps as well as duration is printed to the log with level . In addition, a log line with level is printed per frame showing the black amount detected for that frame.\n\nThe filter also attaches metadata to the first frame of a black segment with key and to the first frame after the black segment ends with key . The value is the frame’s timestamp. This metadata is added regardless of the minimum duration specified.\n\nThe filter accepts the following options:\n\nThe following example sets the maximum pixel threshold to the minimum value, and detects only black intervals of 2 or more seconds:\n\nDetect frames that are (almost) completely black. Can be useful to detect chapter transitions or commercials. Output lines consist of the frame number of the detected frame, the percentage of blackness, the position in the file if known or -1 and the timestamp in seconds.\n\nIn order to display the output lines, you need to set the loglevel at least to the AV_LOG_INFO value.\n\nThis filter exports frame metadata . The value represents the percentage of pixels in the picture that are below the threshold value.\n\nIt accepts the following parameters:\n\nBlend two video frames into each other.\n\nThe filter takes two input streams and outputs one stream, the first input is the \"top\" layer and second input is \"bottom\" layer. By default, the output terminates when the longest input terminates.\n\nThe (time blend) filter takes two consecutive frames from one single stream, and outputs the result obtained by blending the new frame on top of the old frame.\n\nA description of the accepted options follows.\n\nThe filter also supports the framesync options.\n• Apply transition from bottom layer to top layer in first 10 seconds:\n• Split diagonally video and shows top and bottom layer on each side:\n• Display differences between the current and the previous frame:\n\nThis filter supports same commands as options.\n\nDetermines blockiness of frames without altering the input frames.\n\nBased on Remco Muijs and Ihor Kirenko: \"A no-reference blocking artifact measure for adaptive video processing.\" 2005 13th European signal processing conference.\n\nThe filter accepts the following options:\n• Determine blockiness for the first plane and search for periods within [8,32]:\n\nDetermines blurriness of frames without altering the input frames.\n\nBased on Marziliano, Pina, et al. \"A no-reference perceptual blur metric.\" Allows for a block-based abbreviation.\n\nThe filter accepts the following options:\n• Determine blur for 80% of most significant 32x32 blocks:\n\nThe filter accepts the following options.\n• Same as above, but filtering only luma:\n• Same as above, but with both estimation modes:\n• Same as above, but prefilter with nlmeans filter instead:\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n• Apply a boxblur filter with the luma, chroma, and alpha radii set to 2:\n• Set the luma radius to 2, and alpha and chroma radius to 0:\n• Set the luma and chroma radii to a fraction of the video dimension:\n\nMotion adaptive deinterlacing based on yadif with the use of w3fdif and cubic interpolation algorithms. It accepts the following parameters:\n\nThis filter fixes various issues seen with commerical encoders related to upstream malformed CEA-708 payloads, specifically incorrect number of tuples (wrong cc_count for the target FPS), and incorrect ordering of tuples (i.e. the CEA-608 tuples are not at the first entries in the payload).\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nRemove all color information for all colors except for certain one.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n• Make every green pixel in the input image transparent:\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplay CIE color diagram with pixels overlaid onto it.\n\nThe filter accepts the following options:\n\nSome codecs can export information through frames using side-data or other means. For example, some MPEG based codecs export motion vectors through the flag in the codec option.\n\nThe filter accepts the following option:\n• Visualize forward predicted MVs of all frames using :\n• Visualize multi-directionals MVs of P and B-Frames using :\n\nModify intensity of primary colors (red, green and blue) of input frames.\n\nThe filter allows an input frame to be adjusted in the shadows, midtones or highlights regions for the red-cyan, green-magenta or blue-yellow balance.\n\nA positive adjustment value shifts the balance towards the primary color, a negative value towards the complementary color.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nAdjust color white balance selectively for blacks and whites. This filter operates in YUV colorspace.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter modifies a color channel by adding the values associated to the other channels of the same pixels. For example if the value to modify is red, the output value will be:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nRGB colorspace color keying. This filter operates on 8-bit RGB format frames by setting the alpha component of each pixel which falls within the similarity radius of the key color to 0. The alpha value for pixels outside the similarity radius depends on the value of the blend option.\n\nThe filter accepts the following options:\n• Make every green pixel in the input image transparent:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nRemove all color information for all RGB colors except for certain one.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter needs three input video streams. First stream is video stream that is going to be filtered out. Second and third video stream specify color patches for source color to target color mapping.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nFor example to convert from BT.601 to SMPTE-240M, use the command:\n\nConvert colorspace, transfer characteristics or color primaries. Input video needs to have an even size.\n\nThe filter accepts the following options:\n\nThe filter converts the transfer characteristics, color space and color primaries to the specified user values. The output value, if not specified, is set to a default value based on the \"all\" property. If that property is also not specified, the filter will log an error. The output color range and format default to the same value as the input color range and format. The input transfer characteristics, color space, color primaries and color range should be set on the input data. If any of these are missing, the filter will log an error and no conversion will take place.\n\nFor example to convert the input to SMPTE-240M, use the command:\n\nAdjust color temperature in video to simulate variations in ambient color temperature.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nApply convolution of 3x3, 5x5, 7x7 or horizontal/vertical up to 49 elements.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply 2D convolution of video stream in frequency domain using second stream as impulse.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nCopy the input video source unchanged to the output. This is mainly useful for testing purposes.\n\nVideo filtering on GPU using Apple’s CoreImage API on OSX.\n\nHardware acceleration is based on an OpenGL context. Usually, this means it is processed by video hardware. However, software-based OpenGL implementations exist which means there is no guarantee for hardware processing. It depends on the respective OSX.\n\nThere are many filters and image generators provided by Apple that come with a large variety of options. The filter has to be referenced by its name along with its options.\n\nThe coreimage filter accepts the following options:\n\nSeveral filters can be chained for successive processing without GPU-HOST transfers allowing for fast processing of complex filter chains. Currently, only filters with zero (generators) or exactly one (filters) input image and one output image are supported. Also, transition filters are not yet usable as intended.\n\nSome filters generate output images with additional padding depending on the respective filter kernel. The padding is automatically removed to ensure the filter output has the same size as the input image.\n\nFor image generators, the size of the output image is determined by the previous output image of the filter chain or the input image of the whole filterchain, respectively. The generators do not use the pixel information of this image to generate their output. However, the generated output is blended onto this image, resulting in partial or complete coverage of the output image.\n\nThe coreimagesrc video source can be used for generating input images which are directly fed into the filter chain. By using it, providing input images by another video source or an input video is not required.\n• Use the CIBoxBlur filter with default options to blur an image:\n• Use a filter chain with CISepiaTone at default values and CIVignetteEffect with its center at 100x100 and a radius of 50 pixels:\n• Use nullsrc and CIQRCodeGenerator to create a QR code for the FFmpeg homepage, given as complete and escaped command-line for Apple’s standard bash shell:\n\nObtain the correlation between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max correlation is printed through the logging system.\n\nThe filter stores the calculated correlation of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nIt accepts the following options:\n• Cover a rectangular object by the supplied image of a given video using :\n\nCrop the input video to given dimensions.\n\nIt accepts the following parameters:\n\nThe , , , parameters are expressions containing the following constants:\n\nThe expression for may depend on the value of , and the expression for may depend on , but they cannot depend on and , as and are evaluated after and .\n\nThe and parameters specify the expressions for the position of the top-left corner of the output (non-cropped) area. They are evaluated for each frame. If the evaluated value is not valid, it is approximated to the nearest valid value.\n\nThe expression for may depend on , and the expression for may depend on .\n• Crop area with size 100x100 at position (12,34). Using named options, the example above becomes:\n• Crop the central input area with size 2/3 of the input video:\n• Delimit the rectangle with the top-left corner placed at position 100:100 and the right-bottom corner corresponding to the right-bottom corner of the input image.\n• Crop 10 pixels from the left and right borders, and 20 pixels from the top and bottom borders\n• Keep only the bottom right quarter of the input image:\n• Set x depending on the value of y:\n\nThis filter supports the following commands:\n\nIt calculates the necessary cropping parameters and prints the recommended parameters via the logging system. The detected dimensions correspond to the non-black or video area of the input video according to .\n\nIt accepts the following parameters:\n• Find an embedded video area, use motion vectors from decoder:\n\nThis filter supports the following commands:\n\nDelay video filtering until a given wallclock timestamp. The filter first passes on amount of frames, then it buffers at most amount of frames and waits for the cue. After reaching the cue it forwards the buffered frames and also any subsequent frames coming in its input.\n\nThe filter can be used synchronize the output of multiple ffmpeg processes for realtime output devices like decklink. By putting the delay in the filtering chain and pre-buffering frames the process can pass on data to output almost immediately after the target wallclock timestamp is reached.\n\nPerfect frame accuracy cannot be guaranteed, but the result is good enough for some use cases.\n\nThis filter is similar to the Adobe Photoshop and GIMP curves tools. Each component (red, green and blue) has its values defined by key points tied from each other using a smooth curve. The x-axis represents the pixel values from the input frame, and the y-axis the new pixel values to be set for the output frame.\n\nBy default, a component curve is defined by the two points and . This creates a straight line where each original pixel value is \"adjusted\" to its own value, which means no change to the image.\n\nThe filter allows you to redefine these two points and add some more. A new curve will be defined to pass smoothly through all these new coordinates. The new defined points need to be strictly increasing over the x-axis, and their and values must be in the interval. The curve is formed by using a natural or monotonic cubic spline interpolation, depending on the option (default: ). The spline produces a smoother curve in general while the monotonic ( ) spline guarantees the transitions between the specified points to be monotonic. If the computed curves happened to go outside the vector spaces, the values will be clipped accordingly.\n\nThe filter accepts the following options:\n\nTo avoid some filtergraph syntax conflicts, each key points list need to be defined using the following syntax: .\n\nThis filter supports same commands as options.\n• Vintage effect: Here we obtain the following coordinates for each components:\n• The previous example can also be achieved with the associated built-in preset:\n• Use a Photoshop preset and redefine the points of the green component:\n• Check out the curves of the profile using and :\n\nThis filter shows hexadecimal pixel values of part of video.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options excluding option.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThis filter is not designed for real time.\n\nThe filter accepts the following options:\n\nThe same operation can be achieved using the expression system:\n\nRemove banding artifacts from input video. It works by replacing banded pixels with average value of referenced pixels.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n• Deblock using weak filter and block size of 4 pixels.\n• Deblock using strong filter, block size of 4 pixels and custom thresholds for deblocking more edges.\n• Similar as above, but filter only first plane.\n• Similar as above, but filter only second and third plane.\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nApply 2D deconvolution of video stream in frequency domain using second stream as impulse.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nIt accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) average by taking into account only values lower than the pixel.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nIt accepts the following options:\n\nJudder can be introduced, for instance, by pullup filter. If the original source was partially telecined content then the output of will have a variable frame rate. May change the recorded frame rate of the container. Aside from that change, this filter will not affect constant frame rate video.\n\nThe option available in this filter is:\n\nSuppress a TV station logo by a simple interpolation of the surrounding pixels. Just set a rectangle covering the logo and watch it disappear (and sometimes something even uglier appear - your mileage may vary).\n\nIt accepts the following parameters:\n• Set a rectangle covering the area with top left corner coordinates 0,0 and size 100x77:\n\nRemove the rain in the input image/video by applying the derain methods based on convolutional neural networks. Supported models:\n\nTraining as well as model generation scripts are provided in the repository at https://github.com/XueweiMeng/derain_filter.git.\n\nThe filter accepts the following options:\n\nTo get full functionality (such as async execution), please use the dnn_processing filter.\n\nAttempt to fix small changes in horizontal and/or vertical shift. This filter helps remove camera shake from hand-holding a camera, bumping a tripod, moving on a vehicle, etc.\n\nThe filter accepts the following options:\n\nRemove unwanted contamination of foreground colors, caused by reflected color of greenscreen or bluescreen.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply an exact inverse of the telecine operation. It requires a predefined pattern specified using the pattern option which must be the same as that passed to the telecine filter.\n\nThis filter accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) maximum.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplace pixels as indicated by second and third input stream.\n\nIt takes three input streams and outputs one stream, the first input is the source, and second and third input are displacement maps.\n\nThe second input specifies how much to displace pixels along the x-axis, while the third input specifies how much to displace pixels along the y-axis. If one of displacement map streams terminates, last frame from that displacement map will be used.\n\nNote that once generated, displacements maps can be reused over and over again.\n\nA description of the accepted options follows.\n\nDo classification with deep neural networks based on bounding boxes.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nDo image processing with deep neural networks. It works together with another filter which converts the pixel format of the Frame to what the dnn network requires.\n\nThe filter accepts the following options:\n• Remove rain in rgb24 frame with can.pb (see derain filter):\n• Handle the Y channel with srcnn.pb (see sr filter) for frame with yuv420p (planar YUV formats supported):\n• Handle the Y channel with espcn.pb (see sr filter), which changes frame size, for format yuv420p (planar YUV formats supported), please use tools/python/tf_sess_config.py to get the configs of TensorFlow backend for your system.\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a black box around the edge of the input image:\n• Draw a box with color red and an opacity of 50%: The previous example can be specified as:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nIt accepts the following parameters:\n\nExample using metadata from signalstats filter:\n\nExample using metadata from ebur128 filter:\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a grid with cell 100x100 pixels, thickness 2 pixels, with color red and an opacity of 50%:\n• Draw a white 3x3 grid with an opacity of 50%:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nDraw a text string or text from a specified file on top of a video, using the libfreetype library.\n\nTo enable compilation of this filter, you need to configure FFmpeg with and . To enable default font fallback and the option you need to configure FFmpeg with . To enable the option, you need to configure FFmpeg with .\n\nIt accepts the following parameters:\n\nThe parameters for and are expressions containing the following constants and functions:\n\nIf is set to , the filter recognizes sequences accepted by the C function in the provided text and expands them accordingly. Check the documentation of . This feature is deprecated in favor of expansion with the or expansion functions.\n\nIf is set to , the text is printed verbatim.\n\nIf is set to (which is the default), the following expansion mechanism is used.\n\nThe backslash character ‘ ’, followed by any character, always expands to the second character.\n\nSequences of the form are expanded. The text between the braces is a function name, possibly followed by arguments separated by ’:’. If the arguments contain special characters or delimiters (’:’ or ’}’), they should be escaped.\n\nNote that they probably must also be escaped as the value for the option in the filter argument string and as the filter argument in the filtergraph description, and possibly also for the shell, that makes up to four levels of escaping; using a text file with the option avoids these problems.\n\nThe following functions are available:\n\nThe following options are also supported as commands:\n• Draw \"Test Text\" with font FreeSerif, using the default values for the optional parameters.\n• Draw ’Test Text’ with font FreeSerif of size 24 at position x=100 and y=50 (counting from the top-left corner of the screen), text is yellow with a red box around it. Both the text and the box have an opacity of 20%. Note that the double quotes are not necessary if spaces are not used within the parameter list.\n• Show the text at the center of the video frame:\n• Show the text at a random position, switching to a new position every 30 seconds:\n• Show a text line sliding from right to left in the last row of the video frame. The file is assumed to contain a single line with no newlines.\n• Show the content of file off the bottom of the frame and scroll up.\n• Draw a single green letter \"g\", at the center of the input video. The glyph baseline is placed at half screen height.\n• Show text for 1 second every 3 seconds:\n• Use fontconfig to set the font. Note that the colons need to be escaped.\n• Draw \"Test Text\" with font size dependent on height of the video.\n• Print the date of a real-time encoding (see documentation for the C function):\n• Show text fading in and out (appearing/disappearing):\n• Horizontally align multiple separate texts. Note that and the value are included in the offset.\n• Plot special metadata onto each frame if such metadata exists. Otherwise, plot the string \"NA\". Note that image2 demuxer must have option for the special metadata fields to be available for filters.\n\nFor more information about libfreetype, check: http://www.freetype.org/.\n\nFor more information about fontconfig, check: http://freedesktop.org/software/fontconfig/fontconfig-user.html.\n\nFor more information about libfribidi, check: http://fribidi.org/.\n\nFor more information about libharfbuzz, check: https://github.com/harfbuzz/harfbuzz.\n\nDetect and draw edges. The filter uses the Canny Edge Detection algorithm.\n\nThe filter accepts the following options:\n• Standard edge detection with custom values for the hysteresis thresholding:\n\nFor each input image, the filter will compute the optimal mapping from the input to the output given the codebook length, that is the number of distinct output colors.\n\nThis filter accepts the following options.\n\nMeasure graylevel entropy in histogram of color channels of video frames.\n\nIt accepts the following parameters:\n\nApply the EPX magnification filter which is designed for pixel art.\n\nIt accepts the following option:\n\nThe filter accepts the following options:\n\nThe expressions accept the following parameters:\n\nThe filter supports the following commands:\n\nThis filter replaces the pixel by the local(3x3) minimum.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nSpatial only filter that uses edge slope tracing algorithm to interpolate missing lines. It accepts the following parameters:\n\nThis filter supports same commands as options.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nThe filter accepts the following option:\n• Extract luma, u and v color channel component from input video frame into 3 grayscale outputs:\n\nIt accepts the following parameters:\n• Fade in the first 30 frames of video: The command above is equivalent to:\n• Fade out the last 45 frames of a 200-frame video:\n• Fade in the first 25 frames and fade out the last 25 frames of a 1000-frame video:\n• Make the first 5 frames yellow, then fade in from frame 5-24:\n• Fade in alpha over first 25 frames of video:\n• Make the first 5.5 seconds black, then fade in for 0.5 seconds:\n\nThis filter pass cropped input frames to 2nd output. From there it can be filtered with other video filters. After filter receives frame from 2nd input, that frame is combined on top of original frame from 1st input and passed to 1st output.\n\nThe typical usage is filter only part of frame.\n\nThe filter accepts the following options:\n• Blur only top left rectangular part of video frame size 100x100 with gblur filter.\n• Draw black box on top left part of video frame of size 100x100 with drawbox filter.\n• Pixelize rectangular part of video frame of size 100x100 with pixelize filter.\n\nThe filter accepts the following options:\n\nExtract a single field from an interlaced image using stride arithmetic to avoid wasting CPU time. The output frames are marked as non-interlaced.\n\nThe filter accepts the following options:\n\nCreate new frames by copying the top and bottom fields from surrounding frames supplied as numbers by the hint file.\n\nExample of first several lines of file for mode:\n\nField matching filter for inverse telecine. It is meant to reconstruct the progressive frames from a telecined stream. The filter does not drop duplicated frames, so to achieve a complete inverse telecine needs to be followed by a decimation filter such as decimate in the filtergraph.\n\nThe separation of the field matching and the decimation is notably motivated by the possibility of inserting a de-interlacing filter fallback between the two. If the source has mixed telecined and real interlaced content, will not be able to match fields for the interlaced parts. But these remaining combed frames will be marked as interlaced, and thus can be de-interlaced by a later filter such as yadif before decimation.\n\nIn addition to the various configuration options, can take an optional second stream, activated through the option. If enabled, the frames reconstruction will be based on the fields and frames from this second stream. This allows the first input to be pre-processed in order to help the various algorithms of the filter, while keeping the output lossless (assuming the fields are matched properly). Typically, a field-aware denoiser, or brightness/contrast adjustments can help.\n\nNote that this filter uses the same algorithms as TIVTC/TFM (AviSynth project) and VIVTC/VFM (VapourSynth project). The later is a light clone of TFM from which is based on. While the semantic and usage are very close, some behaviour and options names can differ.\n\nThe decimate filter currently only works for constant frame rate input. If your input has mixed telecined (30fps) and progressive content with a lower framerate like 24fps use the following filterchain to produce the necessary cfr stream: .\n\nThe filter accepts the following options:\n\nWe assume the following telecined stream:\n\nThe numbers correspond to the progressive frame the fields relate to. Here, the first two frames are progressive, the 3rd and 4th are combed, and so on.\n\nWhen is configured to run a matching from bottom ( = ) this is how this input stream get transformed:\n\nAs a result of the field matching, we can see that some frames get duplicated. To perform a complete inverse telecine, you need to rely on a decimation filter after this operation. See for instance the decimate filter.\n\nThe same operation now matching from top fields ( = ) looks like this:\n\nIn these examples, we can see what , and mean; basically, they refer to the frame and field of the opposite parity:\n• matches the field of the opposite parity in the previous frame\n• matches the field of the opposite parity in the current frame\n• matches the field of the opposite parity in the next frame\n\nThe and matching are a bit special in the sense that they match from the opposite parity flag. In the following examples, we assume that we are currently matching the 2nd frame (Top:2, bottom:2). According to the match, a ’x’ is placed above and below each matched fields.\n\nAdvanced IVTC, with fallback on yadif for still combed frames:\n\nTransform the field order of the input video.\n\nIt accepts the following parameters:\n\nThe default value is ‘ ’.\n\nThe transformation is done by shifting the picture content up or down by one line, and filling the remaining line with appropriate picture content. This method is consistent with most broadcast field order converters.\n\nIf the input video is not flagged as being interlaced, or it is already flagged as being of the required output field order, then this filter does not alter the incoming video.\n\nIt is very useful when converting to or from PAL DV material, which is bottom field first.\n\nFill borders of the input video, without changing video stream dimensions. Sometimes video can have garbage at the four edges and you may not want to crop video input to keep size multiple of some number.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe object to search for must be specified as a gray8 image specified with the option.\n\nFor each possible match, a score is computed. If the score reaches the specified threshold, the object is considered found.\n\nIf the input video contains multiple instances of the object, the filter will find only one of them.\n\nWhen an object is found, the following metadata entries are set in the matching frame:\n\nIt accepts the following options:\n• Cover a rectangular object by the supplied image of a given video using :\n• Find the position of an object in each frame using and write it to a log file:\n\nFlood area with values of same pixel components with another values.\n\nIt accepts the following options:\n\nConvert the input video to one of the specified pixel formats. Libavfilter will try to pick one that is suitable as input to the next filter.\n\nIt accepts the following parameters:\n• Convert the input video to the format Convert the input video to any of the formats in the list\n\nConvert the video to specified constant frame rate by duplicating or dropping frames as necessary.\n\nIt accepts the following parameters:\n\nAlternatively, the options can be specified as a flat string: [: [: ]].\n\nSee also the setpts filter.\n• A typical usage in order to set the fps to 25:\n• Sets the fps to 24, using abbreviation and rounding method to round to nearest:\n\nPack two different video streams into a stereoscopic video, setting proper metadata on supported codecs. The two views should have the same size and framerate and processing will stop when the shorter video ends. Please note that you may conveniently adjust view properties with the scale and fps filters.\n\nIt accepts the following parameters:\n\nChange the frame rate by interpolating new video output frames from the source frames.\n\nThis filter is not designed to function correctly with interlaced media. If you wish to change the frame rate of interlaced media then you are required to deinterlace before this filter and re-interlace after this filter.\n\nA description of the accepted options follows.\n\nThis filter accepts the following option:\n\nThis filter logs a message and sets frame metadata when it detects that the input video has no significant change in content during a specified duration. Video freeze detection calculates the mean average absolute difference of all the components of video frames and compares it to a noise floor.\n\nThe printed times and duration are expressed in seconds. The metadata key is set on the first frame whose timestamp equals or exceeds the detection duration and it contains the timestamp of the first frame of the freeze. The and metadata keys are set on the first frame after the freeze.\n\nThe filter accepts the following options:\n\nThis filter freezes video frames using frame from 2nd input.\n\nThe filter accepts the following options:\n\nTo enable the compilation of this filter, you need to install the frei0r header and configure FFmpeg with .\n\nIt accepts the following parameters:\n\nA frei0r effect parameter can be a boolean (its value is either \"y\" or \"n\"), a double, a color (specified as / / , where , , and are floating point numbers between 0.0 and 1.0, inclusive) or a color description as specified in the (ffmpeg-utils)\"Color\" section in the ffmpeg-utils manual, a position (specified as / , where and are floating point numbers) and/or a string.\n\nThe number and types of parameters depend on the loaded effect. If an effect parameter is not specified, the default value is set.\n• Apply the distort0r effect, setting the first two double parameters:\n• Apply the colordistance effect, taking a color as the first parameter:\n• Apply the perspective effect, specifying the top left and top right image positions:\n\nFor more information, see http://frei0r.dyne.org\n\nThis filter supports the option as commands.\n\nApply fast and simple postprocessing. It is a faster version of spp.\n\nIt splits (I)DCT into horizontal/vertical passes. Unlike the simple post- processing filter, one of them is performed once per block, not per pixel. This allows for much higher speed.\n\nThe filter accepts the following options:\n\nSynchronize video frames with an external mapping from a file.\n\nFor each input PTS given in the map file it either drops or creates as many frames as necessary to recreate the sequence of output frames given in the map file.\n\nThis filter is useful to recreate the output frames of a framerate conversion by the fps filter, recorded into a map file using the ffmpeg option , and do further processing to the corresponding frames e.g. quality comparison.\n\nEach line of the map file must contain three items per input frame, the input PTS (decimal), the output PTS (decimal) and the output TIMEBASE (decimal/decimal), seperated by a space. This file format corresponds to the output of .\n\nThe filter assumes the map file is sorted by increasing input PTS.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThe colorspace is selected according to the specified options. If one of the , , or options is specified, the filter will automatically select a YCbCr colorspace. If one of the , , or options is specified, it will select an RGB colorspace.\n\nIf one of the chrominance expression is not defined, it falls back on the other one. If no alpha expression is specified it will evaluate to opaque value. If none of chrominance expressions are specified, they will evaluate to the luma expression.\n\nThe expressions can use the following variables and functions:\n\nFor functions, if and are outside the area, the value will be automatically clipped to the closer edge.\n\nPlease note that this filter can use multiple threads in which case each slice will have its own expression state. If you want to use only a single expression state because your expressions depend on previous state then you should limit the number of filter threads to 1.\n• Generate a bidimensional sine wave, with angle and a wavelength of 100 pixels:\n• Create a radial gradient that is the same size as the input (also see the vignette filter):\n\nFix the banding artifacts that are sometimes introduced into nearly flat regions by truncation to 8-bit color depth. Interpolate the gradients that should go where the bands are, and dither them.\n\nIt is designed for playback only. Do not use it prior to lossy compression, because compression tends to lose the dither and bring back the bands.\n\nIt accepts the following parameters:\n\nAlternatively, the options can be specified as a flat string: [: ]\n• Apply the filter with a strength and radius of :\n• Specify radius, omitting the strength (which will fall-back to the default value):\n\nWith this filter one can debug complete filtergraph. Especially issues with links filling with queued frames.\n\nThe filter accepts the following options:\n\nA color constancy filter that applies color correction based on the grayworld assumption\n\nThe algorithm uses linear light, so input data should be linearized beforehand (and possibly correctly tagged).\n\nA color constancy variation filter which estimates scene illumination via grey edge algorithm and corrects the scene colors accordingly.\n\nThe filter accepts the following options:\n\nApply guided filter for edge-preserving smoothing, dehazing and so on.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Dehazing, structure-transferring filtering, detail enhancement with guided filter. For the generation of guidance image, refer to paper \"Guided Image Filtering\". See: http://kaiminghe.com/publications/pami12guidedfilter.pdf.\n\nFirst input is the video stream to process, and second one is the Hald CLUT. The Hald CLUT input can be a simple picture or a complete video stream.\n\nThe filter accepts the following options:\n\nalso has the same interpolation options as lut3d (both filters share the same internals).\n\nThis filter also supports the framesync options.\n\nMore information about the Hald CLUT can be found on Eskil Steenberg’s website (Hald CLUT author) at http://www.quelsolaar.com/technology/clut.html.\n\nThis filter supports the option as commands.\n\nGenerate an identity Hald CLUT stream altered with various effects:\n\nNote: make sure you use a lossless codec.\n\nThen use it with to apply it on some random stream:\n\nThe Hald CLUT will be applied to the 10 first seconds (duration of ), then the latest picture of that CLUT stream will be applied to the remaining frames of the stream.\n\nA Hald CLUT is supposed to be a squared image of by pixels. For a given Hald CLUT, FFmpeg will select the biggest possible square starting at the top left of the picture. The remaining padding pixels (bottom or right) will be ignored. This area can be used to add a preview of the Hald CLUT.\n\nTypically, the following generated Hald CLUT will be supported by the filter:\n\nIt contains the original and a preview of the effect of the CLUT: SMPTE color bars are displayed on the right-top, and below the same color bars processed by the color changes.\n\nThen, the effect of this Hald CLUT can be visualized with:\n\nFor example, to horizontally flip the input video with :\n\nIt can be used to correct video that has a compressed range of pixel intensities. The filter redistributes the pixel intensities to equalize their distribution across the intensity range. It may be viewed as an \"automatically adjusting contrast filter\". This filter is useful only for correcting degraded or poorly captured source video.\n\nThe filter accepts the following options:\n\nCompute and draw a color distribution histogram for the input video.\n\nThe computed histogram is a representation of the color component distribution in an image.\n\nStandard histogram displays the color components distribution in an image. Displays color graph for each color component. Shows distribution of the Y, U, V, A or R, G, B components, depending on input format, in the current frame. Below each graph a color component scale meter is shown.\n\nThe filter accepts the following options:\n\nThis is a high precision/quality 3d denoise filter. It aims to reduce image noise, producing smooth images and making still images really still. It should enhance compressibility.\n\nIt accepts the following optional parameters:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe input must be in hardware frames, and the output a non-hardware format. Not all formats will be supported on the output - it may be necessary to insert an additional filter immediately following in the graph to get the output in a supported format.\n\nMap hardware frames to system memory or to another device.\n\nThis filter has several different modes of operation; which one is used depends on the input and output formats:\n• Hardware frame input, normal frame output Map the input frames to system memory and pass them to the output. If the original hardware frame is later required (for example, after overlaying something else on part of it), the filter can be used again in the next mode to retrieve it.\n• Normal frame input, hardware frame output If the input is actually a software-mapped hardware frame, then unmap it - that is, return the original hardware frame. Otherwise, a device must be provided. Create new hardware surfaces on that device for the output, then map them back to the software format at the input and give those frames to the preceding filter. This will then act like the filter, but may be able to avoid an additional copy when the input is already in a compatible format.\n• Hardware frame input and output A device must be supplied for the output, either directly or with the option. The input and output devices must be of different types and compatible - the exact meaning of this is system-dependent, but typically it means that they must refer to the same underlying hardware context (for example, refer to the same graphics card). If the input frames were originally created on the output device, then unmap to retrieve the original frames. Otherwise, map the frames to the output device - create new hardware frames on the output corresponding to the frames on the input.\n\nThe following additional parameters are accepted:\n\nThe device to upload to must be supplied when the filter is initialised. If using ffmpeg, select the appropriate device with the option or with the option. The input and output devices must be of different types and compatible - the exact meaning of this is system-dependent, but typically it means that they must refer to the same underlying hardware context (for example, refer to the same graphics card).\n\nThe following additional parameters are accepted:\n\nIt accepts the following optional parameters:\n\nApply a high-quality magnification filter designed for pixel art. This filter was originally created by Maxim Stepin.\n\nIt accepts the following option:\n\nAll streams must be of same pixel format and of same height.\n\nNote that this filter is faster than using overlay and pad filter to create same output.\n\nThe filter accepts the following option:\n\nThis filter measures color difference between set HSV color in options and ones measured in video stream. Depending on options, output colors can be changed to be gray or not.\n\nThe filter accepts the following options:\n\nThis filter measures color difference between set HSV color in options and ones measured in video stream. Depending on options, output colors can be changed to transparent by adding alpha channel.\n\nThe filter accepts the following options:\n\nModify the hue and/or the saturation of the input.\n\nIt accepts the following parameters:\n\nand are mutually exclusive, and can’t be specified at the same time.\n\nThe , , and option values are expressions containing the following constants:\n• Set the hue to 90 degrees and the saturation to 1.0:\n• Same command but expressing the hue in radians:\n• Rotate hue and make the saturation swing between 0 and 2 over a period of 1 second:\n• Apply a 3 seconds saturation fade-in effect starting at 0: The general fade-in expression can be written as:\n• Apply a 3 seconds saturation fade-out effect starting at 5 seconds: The general fade-out expression can be written as:\n\nThis filter supports the following commands:\n\nThis filter accepts the following options:\n\nGrow first stream into second stream by connecting components. This makes it possible to build more robust edge masks.\n\nThis filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nDetect the colorspace from an embedded ICC profile (if present), and update the frame’s tags accordingly.\n\nThis filter accepts the following options:\n\nGenerate ICC profiles and attach them to frames.\n\nThis filter accepts the following options:\n\nObtain the identity score between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max identity score is printed through the logging system.\n\nThe filter stores the calculated identity scores of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nThis filter tries to detect if the input frames are interlaced, progressive, top or bottom field first. It will also try to detect fields that are repeated between adjacent frames (a sign of telecine).\n\nSingle frame detection considers only immediately adjacent frames when classifying each frame. Multiple frame detection incorporates the classification history of previous frames.\n\nThe filter will log these metadata values:\n\nThe filter accepts the following options:\n\nInspect the field order of the first 360 frames in a video, in verbose detail:\n\nThe idet filter will add analysis metadata to each frame, which will then be discarded. At the end, the filter will also print a final report with statistics.\n\nThis filter allows one to process interlaced images fields without deinterlacing them. Deinterleaving splits the input frame into 2 fields (so called half pictures). Odd lines are moved to the top half of the output image, even lines to the bottom half. You can process (filter) them independently and then re-interleave them.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter replaces the pixel by the local(3x3) average by taking into account only values higher than the pixel.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nSimple interlacing filter from progressive contents. This interleaves upper (or lower) lines from odd frames with lower (or upper) lines from even frames, halving the frame rate and preserving image height.\n\nIt accepts the following optional parameters:\n\nDeinterlace input video by applying Donald Graft’s adaptive kernel deinterling. Work on interlaced parts of a video to produce progressive frames.\n\nThe description of the accepted parameters follows.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThis filter makes short flashes of light appear longer. This filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter can be used to correct for radial distortion as can result from the use of wide angle lenses, and thereby re-rectify the image. To find the right parameters one can use tools available for example as part of opencv or simply trial-and-error. To use opencv use the calibration sample (under samples/cpp) from the opencv sources and extract the k1 and k2 coefficients from the resulting matrix.\n\nNote that effectively the same filter is available in the open-source tools Krita and Digikam from the KDE project.\n\nIn contrast to the vignette filter, which can also be used to compensate lens errors, this filter corrects the distortion of the image, whereas vignette corrects the brightness distribution, so you may want to use both filters together in certain cases, though you will have to take care of ordering, i.e. whether vignetting should be applied before or after lens correction.\n\nThe filter accepts the following options:\n\nThe formula that generates the correction is:\n\nwhere is halve of the image diagonal and and are the distances from the focal point in the source and target images, respectively.\n\nThis filter supports the all above options as commands.\n\nThe filter requires the camera make, camera model, and lens model to apply the lens correction. The filter will load the lensfun database and query it to find the corresponding camera and lens entries in the database. As long as these entries can be found with the given options, the filter can perform corrections on frames. Note that incomplete strings will result in the filter choosing the best match with the given options, and the filter will output the chosen camera and lens models (logged with level \"info\"). You must provide the make, camera model, and lens model as they are required.\n\nTo obtain a list of available makes and models, leave out one or both of and options. The filter will send the full list to the log with level . The first column is the make and the second column is the model. To obtain a list of available lenses, set any values for make and model and leave out the option. The filter will send the full list of lenses in the log with level . The ffmpeg tool will exit after the list is printed.\n\nThe filter accepts the following options:\n• Apply lens correction with make \"Canon\", camera model \"Canon EOS 100D\", and lens model \"Canon EF-S 18-55mm f/3.5-5.6 IS STM\" with focal length of \"18\" and aperture of \"8.0\".\n• Apply the same as before, but only for the first 5 seconds of video.\n\nThe options for this filter are divided into the following sections:\n\nThese options control the overall output mode. By default, libplacebo will try to preserve the source colorimetry and size as best as it can, but it will apply any embedded film grain, dolby vision metadata or anamorphic SAR present in source frames.\n\nIn addition to the expression constants documented for the scale filter, the , , , , , , and options can also contain the following constants:\n\nThe options in this section control how libplacebo performs upscaling and (if necessary) downscaling. Note that libplacebo will always internally operate on 4:4:4 content, so any sub-sampled chroma formats such as will necessarily be upsampled and downsampled as part of the rendering process. That means scaling might be in effect even if the source and destination resolution are the same.\n\nDeinterlacing is automatically supported when frames are tagged as interlaced, however frames are not deinterlaced unless a deinterlacing algorithm is chosen.\n\nLibplacebo comes with a built-in debanding filter that is good at counteracting many common sources of banding and blocking. Turning this on is highly recommended whenever quality is desired.\n\nA collection of subjective color controls. Not very rigorous, so the exact effect will vary somewhat depending on the input primaries and colorspace.\n\nTo help deal with sources that only have static HDR10 metadata (or no tagging whatsoever), libplacebo uses its own internal frame analysis compute shader to analyze source frames and adapt the tone mapping function in realtime. If this is too slow, or if exactly reproducible frame-perfect results are needed, it’s recommended to turn this feature off.\n\nThe options in this section control how libplacebo performs tone-mapping and gamut-mapping when dealing with mismatches between wide-gamut or HDR content. In general, libplacebo relies on accurate source tagging and mastering display gamut information to produce the best results.\n\nBy default, libplacebo will dither whenever necessary, which includes rendering to any integer format below 16-bit precision. It’s recommended to always leave this on, since not doing so may result in visible banding in the output, even if the filter is enabled. If maximum performance is needed, use instead of disabling dithering.\n\nlibplacebo supports a number of custom shaders based on the mpv .hook GLSL syntax. A collection of such shaders can be found here: https://github.com/mpv-player/mpv/wiki/User-Scripts#user-shaders\n\nA full description of the mpv shader format is beyond the scope of this section, but a summary can be found here: https://mpv.io/manual/master/#options-glsl-shader\n\nAll of the options in this section default off. They may be of assistance when attempting to squeeze the maximum performance at the cost of quality.\n\nThis filter supports almost all of the above options as commands.\n• Rescale input to fit into standard 1080p, with high quality scaling:\n• Run this filter on the CPU, on systems with Mesa installed (and with the most expensive options disabled):\n• Suppress CPU-based AV1/H.274 film grain application in the decoder, in favor of doing it with this filter. Note that this is only a gain if the frames are either already on the GPU, or if you’re using libplacebo for other purposes, since otherwise the VRAM roundtrip will more than offset any expected speedup.\n• Interop with VAAPI hwdec to avoid round-tripping through RAM:\n\nCalculate the VMAF (Video Multi-Method Assessment Fusion) score for a reference/distorted pair of input videos.\n\nThe first input is the distorted video, and the second input is the reference video.\n\nThe obtained VMAF score is printed through the logging system.\n\nIt requires Netflix’s vmaf library (libvmaf) as a pre-requisite. After installing the library it can be enabled using: .\n\nThe filter has following options:\n\nThis filter also supports the framesync options.\n• In the examples below, a distorted video is compared with a reference file .\n• Example with options and different containers:\n\nThis is the CUDA variant of the libvmaf filter. It only accepts CUDA frames.\n\nIt requires Netflix’s vmaf library (libvmaf) as a pre-requisite. After installing the library it can be enabled using: .\n\nApply limited difference filter using second and optionally third video stream.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands except option ‘ ’.\n\nLimits the pixel components values to the specified range [min, max].\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the option as commands.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nCompute a look-up table for binding each pixel component input value to an output value, and apply it to the input video.\n\napplies a lookup table to a YUV input video, to an RGB input video.\n\nThese filters accept the following parameters:\n\nEach of them specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe exact component associated to each of the options depends on the format in input.\n\nThe filter requires either YUV or RGB pixel formats in input, requires RGB pixel formats in input, and requires YUV.\n\nThe expressions can contain the following constants and functions:\n\nThis filter supports same commands as options.\n• Negate input video: The above is the same as:\n\nThe filter takes two input streams and outputs one stream.\n\nThe (time lut2) filter takes two consecutive frames from one single stream.\n\nThis filter accepts the following parameters:\n\nThe filter also supports the framesync options.\n\nEach of them specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe exact component associated to each of the options depends on the format in inputs.\n\nThe expressions can contain the following constants:\n\nThis filter supports the all above options as commands except option .\n\nClamp the first input stream with the second input and third input stream.\n\nReturns the value of first stream to be between second input stream - and third input stream + .\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the second and third input stream into output stream using absolute differences between second input stream and first input stream and absolute difference between third input stream and first input stream. The picked value will be from second input stream if second absolute difference is greater than first one or from third input stream otherwise.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the first input stream with the second input stream using per pixel weights in the third input stream.\n\nA value of 0 in the third stream pixel component means that pixel component from first stream is returned unchanged, while maximum value (eg. 255 for 8-bit videos) means that pixel component from second stream is returned unchanged. Intermediate values define the amount of merging between both input stream’s pixel components.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the second and third input stream into output stream using absolute differences between second input stream and first input stream and absolute difference between third input stream and first input stream. The picked value will be from second input stream if second absolute difference is less than first one or from third input stream otherwise.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nPick pixels comparing absolute difference of two video streams with fixed threshold.\n\nIf absolute difference between pixel component of first and second video stream is equal or lower than user supplied threshold than pixel component from first video stream is picked, otherwise pixel component from second video stream is picked.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nFor example it is useful to create motion masks after filter.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nIt needs one field per frame as input and must thus be used together with yadif=1/3 or equivalent.\n\nThis filter accepts the following options:\n\nPick median pixel from certain rectangle defined by radius.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts up to 4 input streams, and merge selected input planes to the output video.\n\nThis filter accepts the following options:\n• Merge three gray video streams of same width and height into single video stream:\n\nEstimate and export motion vectors using block matching algorithms. Motion vectors are stored in frame side data to be used by other filters.\n\nThis filter accepts the following options:\n\nMidway Image Equalization adjusts a pair of images to have the same histogram, while maintaining their dynamics as much as possible. It’s useful for e.g. matching exposures from a pair of stereo cameras.\n\nThis filter has two inputs and one output, which must be of same pixel format, but may be of different sizes. The output of filter is first input adjusted with midway histogram of both inputs.\n\nThis filter accepts the following option:\n\nConvert the video to specified frame rate using motion interpolation.\n\nThis filter accepts the following options:\n\nMix several video input streams into one video stream.\n\nA description of the accepted options follows.\n\nThis filter supports the following commands:\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nThis filter allows to apply main morphological grayscale transforms, erode and dilate with arbitrary structures set in second input stream.\n\nUnlike naive implementation and much slower performance in erosion and dilation filters, when speed is critical filter should be used instead.\n\nThe filter also supports the framesync options.\n\nThis filter supports same commands as options.\n\nDrop frames that do not differ greatly from the previous frame in order to reduce frame rate.\n\nThe main use of this filter is for very-low-bitrate encoding (e.g. streaming over dialup modem), but it could in theory be used for fixing movies that were inverse-telecined incorrectly.\n\nA description of the accepted options follows.\n\nObtain the MSAD (Mean Sum of Absolute Differences) between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max MSAD is printed through the logging system.\n\nThe filter stores the calculated MSAD of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nMultiply first video stream pixels values with second video stream pixels values.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nIt accepts the following option:\n\nThis filter supports same commands as options.\n\nEach pixel is adjusted by looking for other pixels with similar contexts. This context similarity is defined by comparing their surrounding patches of size x . Patches are searched in an area of x around the pixel.\n\nNote that the research area defines centers for patches, which means some patches will be made of pixels outside that research area.\n\nThe filter accepts the following options.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options, excluding option.\n\nForce libavfilter not to use any of the specified pixel formats for the input to the next filter.\n\nIt accepts the following parameters:\n• Force libavfilter to use a format different from for the input to the vflip filter:\n• Convert the input video to any of the formats not contained in the list:\n\nThe filter accepts the following options:\n\nFor each channel of each frame, the filter computes the input range and maps it linearly to the user-specified output range. The output range defaults to the full dynamic range from pure black to pure white.\n\nTemporal smoothing can be used on the input range to reduce flickering (rapid changes in brightness) caused when small dark or bright objects enter or leave the scene. This is similar to the auto-exposure (automatic gain control) on a video camera, and, like a video camera, it may cause a period of over- or under-exposure of the video.\n\nThe R,G,B channels can be normalized independently, which may cause some color shifting, or linked together as a single channel, which prevents color shifting. Linked normalization preserves hue. Independent normalization does not, so it can be used to remove some color casts. Independent and linked normalization can be combined in any ratio.\n\nThe normalize filter accepts the following options:\n\nThis filter supports same commands as options, excluding option. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nStretch video contrast to use the full dynamic range, with no temporal smoothing; may flicker depending on the source content:\n\nAs above, but with 50 frames of temporal smoothing; flicker should be reduced, depending on the source content:\n\nAs above, but with hue-preserving linked channel normalization:\n\nAs above, but with half strength:\n\nMap the darkest input color to red, the brightest input color to cyan:\n\nPass the video source unchanged to the output.\n\nThis filter uses Tesseract for optical character recognition. To enable compilation of this filter, you need to configure FFmpeg with .\n\nIt accepts the following options:\n\nThe filter exports recognized text as the frame metadata . The filter exports confidence of recognized words as the frame metadata .\n\nTo enable this filter, install the libopencv library and headers and configure FFmpeg with .\n\nIt accepts the following parameters:\n\nRefer to the official libopencv documentation for more precise information: http://docs.opencv.org/master/modules/imgproc/doc/filtering.html\n\nSeveral libopencv filters are supported; see the following subsections.\n\nDilate an image by using a specific structuring element. It corresponds to the libopencv function .\n\nrepresents a structuring element, and has the syntax: x + x /\n\nand represent the number of columns and rows of the structuring element, and the anchor point, and the shape for the structuring element. must be \"rect\", \"cross\", \"ellipse\", or \"custom\".\n\nIf the value for is \"custom\", it must be followed by a string of the form \"= \". The file with name is assumed to represent a binary image, with each printable character corresponding to a bright pixel. When a custom is used, and are ignored, the number or columns and rows of the read file are assumed instead.\n\nThe default value for is \"3x3+0x0/rect\".\n\nspecifies the number of times the transform is applied to the image, and defaults to 1.\n\nErode an image by using a specific structuring element. It corresponds to the libopencv function .\n\nIt accepts the parameters: : , with the same syntax and semantics as the dilate filter.\n\nThe filter takes the following parameters: | | | | .\n\nis the type of smooth filter to apply, and must be one of the following values: \"blur\", \"blur_no_scale\", \"median\", \"gaussian\", or \"bilateral\". The default value is \"gaussian\".\n\nThe meaning of , , , and depends on the smooth type. and accept integer positive values or 0. and accept floating point values.\n\nThe default value for is 3. The default value for the other parameters is 0.\n\nThese parameters correspond to the parameters assigned to the libopencv function .\n\nUseful to measure spatial impulse, step responses, chroma delays, etc.\n\nIt accepts the following parameters:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n\nThe , and expressions can contain the following parameters.\n\nThis filter also supports the framesync options.\n\nNote that the , variables are available only when evaluation is done per frame, and will evaluate to NAN when is set to ‘ ’.\n\nBe aware that frames are taken from each input video in timestamp order, hence, if their initial timestamps differ, it is a good idea to pass the two inputs through a filter to have them begin in the same zero timestamp, as the example for the filter does.\n\nYou can chain together more overlays but you should test the efficiency of such approach.\n\nThis filter supports the following commands:\n• Draw the overlay at 10 pixels from the bottom right corner of the main video: Using named options the example above becomes:\n• Insert a transparent PNG logo in the bottom left corner of the input, using the tool with the option:\n• Insert 2 different transparent PNG logos (second logo on bottom right corner) using the tool:\n• Add a transparent color layer on top of the main video; must specify the size of the main input to the overlay filter:\n• Play an original video and a filtered version (here with the deshake filter) side by side using the tool: The above command is the same as:\n• Make a sliding overlay appearing from the left to the right top part of the screen starting since time 2:\n• Compose output by putting two input videos side to side:\n• Mask 10-20 seconds of a video by applying the delogo filter to a section\n\nThe filter accepts the following options:\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following parameters:\n\nThe value for the , , , and options are expressions containing the following constants:\n• Add paddings with the color \"violet\" to the input video. The output video size is 640x480, and the top-left corner of the input video is placed at column 0, row 40 The example above is equivalent to the following command:\n• Pad the input to get an output with dimensions increased by 3/2, and put the input video at the center of the padded area:\n• Pad the input to get a squared output with size equal to the maximum value between the input width and height, and put the input video at the center of the padded area:\n• Pad the input to get a final w/h ratio of 16:9:\n• In case of anamorphic video, in order to set the output display aspect correctly, it is necessary to use in the expression, according to the relation: Thus the previous example needs to be modified to:\n• Double the output size and put the input video in the bottom-right corner of the output padded area:\n\nGenerate one palette for a whole video stream.\n\nIt accepts the following options:\n\nThe filter also exports the frame metadata ( ) which you can use to evaluate the degree of color quantization of the palette. This information is also visible at logging level.\n• Generate a representative palette of a given video using :\n\nUse a palette to downsample an input video stream.\n\nThe filter takes two inputs: one video stream and a palette. The palette must be a 256 pixels image.\n\nIt accepts the following options:\n• Use a palette (generated for example with palettegen) to encode a GIF using :\n\nCorrect perspective of video not recorded perpendicular to the screen.\n\nA description of the accepted parameters follows.\n\nDelay interlaced video by one field time so that the field order changes.\n\nThe intended use is to fix PAL movies that have been captured with the opposite field order to the film-to-video transfer.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n\nReduce various flashes in video, so to help users with epilepsy.\n\nIt accepts the following options:\n\nPixel format descriptor test filter, mainly useful for internal testing. The output video should be equal to the input video.\n\ncan be used to test the monowhite pixel format descriptor definition.\n\nThe filter accepts the following options:\n\nThis filter supports all options as commands.\n\nDisplay sample values of color channels. Mainly useful for checking color and levels. Minimum supported resolution is 640x480.\n\nThe filters accept the following options:\n\nThis filter supports same commands as options.\n\nEnable the specified chain of postprocessing subfilters using libpostproc. This library should be automatically selected with a GPL build ( ). Subfilters must be separated by ’/’ and can be disabled by prepending a ’-’. Each subfilter and some options have a short and a long name that can be used interchangeably, i.e. dr/dering are the same.\n\nThe filters accept the following options:\n\nAll subfilters share common options to determine their scope:\n\nThese options can be appended after the subfilter name, separated by a ’|’.\n\nThe horizontal and vertical deblocking filters share the difference and flatness values so you cannot set different horizontal and vertical thresholds.\n• Apply deblocking on luma only, and switch vertical deblocking on or off automatically depending on available CPU time:\n\nApply Postprocessing filter 7. It is variant of the spp filter, similar to spp = 6 with 7 point DCT, where only the center sample is used after IDCT.\n\nThe filter accepts the following options:\n\nApply alpha premultiply effect to input video stream using first plane of second stream as alpha.\n\nBoth streams must have same dimensions and same pixel format.\n\nThe filter accepts the following option:\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThis filter accepts the following options:\n\nEach of the expression options specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe expressions can contain the following constants and functions:\n\nThis filter supports the all above options as commands.\n\nObtain the average, maximum and minimum PSNR (Peak Signal to Noise Ratio) between two input videos.\n\nThis filter takes in input two input videos, the first input is considered the \"main\" source and is passed unchanged to the output. The second input is used as a \"reference\" video for computing the PSNR.\n\nBoth video inputs must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained average PSNR is printed through the logging system.\n\nThe filter stores the accumulated MSE (mean squared error) of each frame, and at the end of the processing it is averaged across all frames equally, and the following formula is applied to obtain the PSNR:\n\nWhere MAX is the average of the maximum values of each component of the image.\n\nThe description of the accepted parameters follows.\n\nThis filter also supports the framesync options.\n\nThe file printed if is selected, contains a sequence of key/value pairs of the form : for each compared couple of frames.\n\nIf a greater than 1 is specified, a header line precedes the list of per-frame-pair stats, with key value pairs following the frame format with the following parameters:\n\nA description of each shown per-frame-pair parameter follows:\n• For example: On this example the input file being processed is compared with the reference file . The PSNR of each individual frame is stored in .\n• Another example with different containers:\n\nThe pullup filter is designed to take advantage of future context in making its decisions. This filter is stateless in the sense that it does not lock onto a pattern to follow, but it instead looks forward to the following fields in order to identify matches and rebuild progressive frames.\n\nTo produce content with an even framerate, insert the fps filter after pullup, use if the input frame rate is 29.97fps, for 30fps and the (rare) telecined 25fps input.\n\nThe filter accepts the following options:\n\nFor best results (without duplicated frames in the output file) it is necessary to change the output frame rate. For example, to inverse telecine NTSC input:\n\nThe filter accepts the following option:\n\nThe expression is evaluated through the eval API and can contain, among others, the following constants:\n\nGenerate a QR code using the libqrencode library (see https://fukuchi.org/works/qrencode/), and overlay it on top of the current frame.\n\nTo enable the compilation of this filter, you need to configure FFmpeg with .\n\nThe QR code is generated from the provided text or text pattern. The corresponding QR code is scaled and overlayed into the video output according to the specified options.\n\nIn case no text is specified, no QR code is overlaied.\n\nThis filter accepts the following options:\n\nThe expressions set by the options contain the following constants and functions.\n\nIf is set to , the text is printed verbatim.\n\nIf is set to (which is the default), the following expansion mechanism is used.\n\nThe backslash character ‘ ’, followed by any character, always expands to the second character.\n\nSequences of the form are expanded. The text between the braces is a function name, possibly followed by arguments separated by ’:’. If the arguments contain special characters or delimiters (’:’ or ’}’), they should be escaped.\n\nNote that they probably must also be escaped as the value for the option in the filter argument string and as the filter argument in the filtergraph description, and possibly also for the shell, that makes up to four levels of escaping; using a text file with the option avoids these problems.\n\nThe following functions are available:\n• Generate a QR code encoding the specified text with the default size, overalaid in the top left corner of the input video, with the default size:\n• Same as below, but select blue on pink colors:\n• Place the QR code in the bottom right corner of the input video:\n• Generate a QR code with width of 200 pixels and padding, making the padded width 4/3 of the QR code width:\n• Generate a QR code with padded width of 200 pixels and padding, making the QR code width 3/4 of the padded width:\n• Make the QR code a fraction of the input video width:\n\nIdentify and decode a QR code using the libquirc library (see https://github.com/dlbeer/quirc/), and print the identified QR codes positions and payload as metadata.\n\nTo enable the compilation of this filter, you need to configure FFmpeg with .\n\nFor each found QR code in the input video, some metadata entries are added with the prefix , where is the index, starting from 0, associated to the QR code.\n\nA description of each metadata value follows:\n\nFlush video frames from internal cache of frames into a random order. No frame is discarded. Inspired by frei0r nervous filter.\n\nRead closed captioning (EIA-608) information from the top lines of a video frame.\n\nThis filter adds frame metadata for and , where is the number of the identified line with EIA-608 data (starting from 0). A description of each metadata value follows:\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Output a csv with presentation time and the first two lines of identified EIA-608 captioning data.\n\nRead vertical interval timecode (VITC) information from the top lines of a video frame.\n\nThe filter adds frame metadata key with the timecode value, if a valid timecode has been detected. Further metadata key is set to 0/1 depending on whether timecode data has been found or not.\n\nThis filter accepts the following options:\n• Detect and draw VITC data onto the video frame; if no valid VITC is detected, draw as a placeholder:\n\nDestination pixel at position (X, Y) will be picked from source (x, y) position where x = Xmap(X, Y) and y = Ymap(X, Y). If mapping values are out of range, zero value for pixel will be used for destination pixel.\n\nXmap and Ymap input video streams must be of same dimensions. Output video stream will have Xmap/Ymap video stream dimensions. Xmap and Ymap input video streams are 16bit depth, single channel.\n\nThe removegrain filter is a spatial denoiser for progressive video.\n\nRange of mode is from 0 to 24. Description of each mode follows:\n\nSuppress a TV station logo, using an image file to determine which pixels comprise the logo. It works by filling in the pixels that comprise the logo with neighboring pixels.\n\nThe filter accepts the following options:\n\nPixels in the provided bitmap image with a value of zero are not considered part of the logo, non-zero pixels are considered part of the logo. If you use white (255) for the logo and black (0) for the rest, you will be safe. For making the filter bitmap, it is recommended to take a screen capture of a black frame with the logo visible, and then using a threshold filter followed by the erode filter once or twice.\n\nIf needed, little splotches can be fixed manually. Remember that if logo pixels are not covered, the filter quality will be much reduced. Marking too many pixels as part of the logo does not hurt as much, but it will increase the amount of blurring needed to cover over the image and will destroy more information than necessary, and extra pixels will slow things down on a large logo.\n\nThis filter uses the repeat_field flag from the Video ES headers and hard repeats fields based on its value.\n\nWarning: This filter requires memory to buffer the entire clip, so trimming is suggested.\n• Take the first 5 seconds of a clip, and reverse it.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nRotate video by an arbitrary angle expressed in radians.\n\nThe filter accepts the following options:\n\nA description of the optional parameters follows.\n\nThe expressions for the angle and the output size can contain the following constants and functions:\n• Apply a constant rotation with period T, starting from an angle of PI/3:\n• Make the input video rotation oscillating with a period of T seconds and an amplitude of A radians:\n• Rotate the video, output size is chosen so that the whole rotating input video is always completely contained in the output:\n• Rotate the video, reduce the output size so that no background is ever shown:\n\nThe filter supports the following commands:\n\nThe filter accepts the following options:\n\nEach chroma option value, if not explicitly specified, is set to the corresponding luma option value.\n\nScale (resize) the input video, using the libswscale library.\n\nThe scale filter forces the output display aspect ratio to be the same of the input, by changing the output sample aspect ratio.\n\nIf the input image format is different from the format requested by the next filter, the scale filter will convert the input to the requested format.\n\nThe filter accepts the following options, any of the options supported by the libswscale scaler, as well as any of the framesync options.\n\nSee (ffmpeg-scaler)the ffmpeg-scaler manual for the complete list of scaler options.\n\nThe values of the and options are expressions containing the following constants:\n• Scale the input video to a size of 200x100 This is equivalent to:\n• Specify a size abbreviation for the output size: which can also be written as:\n• The above is the same as:\n• Scale the input to 2x with forced interlaced scaling:\n• Increase the width, and set the height to the same size:\n• Increase the height, and set the width to 3/2 of the height:\n• Increase the size, making the size a multiple of the chroma subsample values:\n• Increase the width to a maximum of 500 pixels, keeping the same aspect ratio as the input:\n• Make pixels square using reset_sar, making sure the resulting resolution is even (required by some codecs):\n• Scale to target exactly, however reset SAR to 1:\n• Scale to even dimensions that fit within 400x300, preserving input SAR:\n• Scale to produce square pixels with even dimensions that fit within 400x300:\n• Scale a subtitle stream (sub) to match the main video (main) in size before overlaying. (\"scale2ref\")\n• Scale a logo to 1/10th the height of a video, while preserving its display aspect ratio.\n\nThis filter supports the following commands:\n\nScale and convert the color parameters using VTPixelTransferSession.\n\nThe filter accepts the following options:\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThis filter sets frame metadata with mafd between frame, the scene score, and forward the frame to the next filter, so they can use these metadata to detect scene change or others.\n\nIn addition, this filter logs a message and sets frame metadata when it detects a scene change by .\n\nmetadata keys are set with mafd for every frame.\n\nmetadata keys are set with scene change score for every frame to detect scene change.\n\nmetadata keys are set with current filtered frame time which detect scene change with .\n\nThe filter accepts the following options:\n\nAdjust cyan, magenta, yellow and black (CMYK) to certain ranges of colors (such as \"reds\", \"yellows\", \"greens\", \"cyans\", ...). The adjustment range is defined by the \"purity\" of the color (that is, how saturated it already is).\n\nThis filter is similar to the Adobe Photoshop Selective Color tool.\n\nThe filter accepts the following options:\n\nAll the adjustment settings ( , , ...) accept up to 4 space separated floating point adjustment values in the [-1,1] range, respectively to adjust the amount of cyan, magenta, yellow and black for the pixels of its range.\n• Increase cyan by 50% and reduce yellow by 33% in every green areas, and increase magenta by 27% in blue areas:\n\nThe takes a frame-based video input and splits each frame into its components fields, producing a new half height clip with twice the frame rate and twice the frame count.\n\nThis filter use field-dominance information in frame to decide which of each pair of fields to place first in the output. If it gets it wrong use setfield filter before filter.\n\nThe filter sets the Display Aspect Ratio for the filter output video.\n\nThis is done by changing the specified Sample (aka Pixel) Aspect Ratio, according to the following equation:\n\nKeep in mind that the filter does not modify the pixel dimensions of the video frame. Also, the display aspect ratio set by this filter may be changed by later filters in the filterchain, e.g. in case of scaling or if another \"setdar\" or a \"setsar\" filter is applied.\n\nThe filter sets the Sample (aka Pixel) Aspect Ratio for the filter output video.\n\nNote that as a consequence of the application of this filter, the output display aspect ratio will change according to the equation above.\n\nKeep in mind that the sample aspect ratio set by the filter may be changed by later filters in the filterchain, e.g. if another \"setsar\" or a \"setdar\" filter is applied.\n\nIt accepts the following parameters:\n\nThe parameter is an expression containing the following constants:\n• To change the display aspect ratio to 16:9, specify one of the following:\n• To change the sample aspect ratio to 10:11, specify:\n• To set a display aspect ratio of 16:9, and specify a maximum integer value of 1000 in the aspect ratio reduction, use the command:\n\nThe filter marks the interlace type field for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by following filters (e.g. or ).\n\nThe filter accepts the following options:\n\nThe filter marks interlace and color range for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by filters/encoders.\n\nThis filter supports the following options:\n\nThis filter supports the all above options as commands.\n\nShow a line containing various information for each input video frame. The input video is not modified.\n\nThis filter supports the following options:\n\nThe shown line contains a sequence of key/value pairs of the form : .\n\nThe following values are shown in the output:\n\nDisplays the 256 colors palette of each frame. This filter is only relevant for pixel format frames.\n\nIt accepts the following option:\n\nIt accepts the following parameters:\n\nThe first frame has the index 0. The default is to keep the input unchanged.\n• Swap second and third frame of every three frames of the input:\n• Swap 10th and 1st frame of every ten frames of the input:\n\nThis filter accepts the following options:\n\nIt accepts the following parameters:\n\nThe first plane has the index 0. The default is to keep the input unchanged.\n• Swap the second and third planes of the input:\n\nEvaluate various visual metrics that assist in determining issues associated with the digitization of analog video media.\n\nBy default the filter will log these metadata values:\n\nThe filter accepts the following options:\n• Output specific data about the minimum and maximum values of the Y plane per frame:\n• Playback video while highlighting pixels that are outside of broadcast range in red.\n• Playback video with signalstats metadata drawn over the frame. The contents of signalstat_drawtext.txt used in the command are:\n\nCalculates the MPEG-7 Video Signature. The filter can handle more than one input. In this case the matching between the inputs can be calculated additionally. The filter always passes through the first input. The signature of each stream can be written into a file.\n\nIt accepts the following options:\n• To calculate the signature of an input video and store it in signature.bin:\n• To detect whether two videos match and store the signatures in XML format in signature0.xml and signature1.xml:\n\nCalculate Spatial Information (SI) and Temporal Information (TI) scores for a video, as defined in ITU-T Rec. P.910 (11/21): Subjective video quality assessment methods for multimedia applications. Available PDF at https://www.itu.int/rec/T-REC-P.910-202111-S/en. Note that this is a legacy implementation that corresponds to a superseded recommendation. Refer to ITU-T Rec. P.910 (07/22) for the latest version: https://www.itu.int/rec/T-REC-P.910-202207-I/en\n\nIt accepts the following option:\n\nBlur the input video without impacting the outlines.\n\nIt accepts the following options:\n\nIf a chroma or alpha option is not explicitly set, the corresponding luma value is set.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nApply a simple postprocessing filter that compresses and decompresses the image at several (or - in the case of level - all) shifts and average the results.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nScale the input by applying one of the super-resolution methods based on convolutional neural networks. Supported models:\n\nTraining scripts as well as scripts for model file (.pb) saving can be found at https://github.com/XueweiMeng/sr/tree/sr_dnn_native. Original repository is at https://github.com/HighVoltageRocknRoll/sr.git.\n\nThe filter accepts the following options:\n\nTo get full functionality (such as async execution), please use the dnn_processing filter.\n\nUpscale (size increasing) for the input video using AMD Advanced Media Framework library for hardware acceleration. Use advanced algorithms for upscaling with higher output quality. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n\nObtain the SSIM (Structural SImilarity Metric) between two input videos.\n\nThis filter takes in input two input videos, the first input is considered the \"main\" source and is passed unchanged to the output. The second input is used as a \"reference\" video for computing the SSIM.\n\nBoth video inputs must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe filter stores the calculated SSIM of each frame.\n\nThe description of the accepted parameters follows.\n\nThe file printed if is selected, contains a sequence of key/value pairs of the form : for each compared couple of frames.\n\nA description of each shown parameter follows:\n\nThis filter also supports the framesync options.\n• For example: On this example the input file being processed is compared with the reference file . The SSIM of each individual frame is stored in .\n• Another example with both psnr and ssim at same time:\n• Another example with different containers:\n\nThe filters accept the following options:\n• Convert input video from side by side parallel to anaglyph yellow/blue dubois:\n• Convert input video from above below (left eye above, right eye below) to side by side crosseye.\n\nThe filter accepts the following options:\n\nThe and filter supports the following commands:\n• Select first 5 seconds 1st stream and rest of time 2nd stream:\n• Same as above, but for audio:\n\nDraw subtitles on top of input video using the libass library.\n\nTo enable compilation of this filter you need to configure FFmpeg with . This filter also requires a build with libavcodec and libavformat to convert the passed subtitles file to ASS (Advanced Substation Alpha) subtitles format.\n\nThe filter accepts the following options:\n\nIf the first key is not specified, it is assumed that the first value specifies the .\n\nFor example, to render the file on top of the input video, use the command:\n\nwhich is equivalent to:\n\nTo render the default subtitles stream from file , use:\n\nTo render the second subtitles stream from that file, use:\n\nTo make the subtitles stream from appear in 80% transparent blue , use:\n\nScale the input by 2x and smooth using the Super2xSaI (Scale and Interpolate) pixel art scaling algorithm.\n\nUseful for enlarging pixel art images without reducing sharpness.\n\nThis filter accepts the following options:\n\nThe all options are expressions containing the following constants:\n\nThis filter supports the all above options as commands.\n\nThis filter accepts the following options:\n\nCompute and draw a color distribution histogram for the input video across time.\n\nUnlike histogram video filter which only shows histogram of single input frame at certain time, this filter shows also past histograms of number of frames defined by option.\n\nThe computed histogram is a representation of the color component distribution in an image.\n\nThe filter accepts the following options:\n\nThis filter needs four video streams to perform thresholding. First stream is stream we are filtering. Second stream is holding threshold values, third stream is holding min values, and last, fourth stream is holding max values.\n\nThe filter accepts the following option:\n\nFor example if first stream pixel’s component value is less then threshold value of pixel component from 2nd threshold stream, third stream value will picked, otherwise fourth stream pixel component value will be picked.\n\nUsing color source filter one can perform various types of thresholding:\n\nThis filter supports the all options as commands.\n• Threshold to zero, using gray color as threshold:\n• Inverted threshold to zero, using gray color as threshold:\n\nSelect the most representative frame in a given sequence of consecutive frames.\n\nThe filter accepts the following options:\n\nSince the filter keeps track of the whole frames sequence, a bigger value will result in a higher memory usage, so a high value is not recommended.\n• Complete example of a thumbnail creation with :\n\nThe untile filter can do the reverse.\n\nThe filter accepts the following options:\n• Produce 8x8 PNG tiles of all keyframes ( ) in a movie: The is necessary to prevent from duplicating each output frame to accommodate the originally detected frame rate.\n• Display pictures in an area of frames, with pixels between them, and pixels of initial margin, using mixed flat and named options:\n\nWhat happens when you invert time and space?\n\nNormally a video is composed of several frames that represent a different instant of time and shows a scene that evolves in the space captured by the frame. This filter is the antipode of that concept, taking inspiration from tilt and shift photography.\n\nA filtered frame contains the whole timeline of events composing the sequence, and this is obtained by placing a slice of pixels from each frame into a single one. However, since there are no infinite-width frames, this is done up the width of the input frame, and a video is recomposed by shifting away one column for each subsequent frame. In order to map space to time, the filter tilts each input frame as well, so that motion is preserved. This is accomplished by progressively selecting a different column from each input frame.\n\nThe end result is a sort of inverted parallax, so that far away objects move much faster that the ones in the front. The ideal conditions for this video effect are when there is either very little motion and the backgroud is static, or when there is a lot of motion and a very wide depth of field (e.g. wide panorama, while moving on a train).\n\nThe filter accepts the following parameters:\n\nNormally the filter shifts and tilts from the very first frame, and stops when the last one is received. However, before filtering starts, normal video may be preseved, so that the effect is slowly shifted in its place. Similarly, the last video frame may be reconstructed at the end. Alternatively it is possible to just start and end with black.\n\nFrames are counted starting from 1, so the first input frame is considered odd.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports all above options as commands, excluding option .\n\nMidway Video Equalization adjusts a sequence of video frames to have the same histograms, while maintaining their dynamics as much as possible. It’s useful for e.g. matching exposures from a video frames sequence.\n\nThis filter accepts the following option:\n\nA description of the accepted options follows.\n• Similar as above but only showing temporal differences:\n\nThis filter supports the following commands:\n\nThis filter expects data in single precision floating point, as it needs to operate on (and can output) out-of-range values. Another filter, such as zscale, is needed to convert the resulting frame to a usable format.\n\nThe tonemapping algorithms implemented only work on linear light, so input data should be linearized beforehand (and possibly correctly tagged).\n\nThe filter accepts the following options.\n\nThe filter accepts the following options:\n\nTranspose rows with columns in the input video and optionally flip it.\n\nIt accepts the following parameters:\n\nFor example to rotate by 90 degrees clockwise and preserve portrait layout:\n\nThe command above can also be specified as:\n\nTrim the input so that the output contains one continuous subpart of the input.\n\nIt accepts the following parameters:\n\n, , and are expressed as time duration specifications; see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual for the accepted syntax.\n\nNote that the first two sets of the start/end options and the option look at the frame timestamp, while the _frame variants simply count the frames that pass through the filter. Also note that this filter does not modify the timestamps. If you wish for the output timestamps to start at zero, insert a setpts filter after the trim filter.\n\nIf multiple start or end options are set, this filter tries to be greedy and keep all the frames that match at least one of the specified constraints. To keep only the part that matches all the constraints at once, chain multiple trim filters.\n\nThe defaults are such that all the input is kept. So it is possible to set e.g. just the end values to keep everything before the specified time.\n• Drop everything except the second minute of input:\n• Keep only the first second:\n\nApply alpha unpremultiply effect to input video stream using first plane of second stream as alpha.\n\nBoth streams must have same dimensions and same pixel format.\n\nThe filter accepts the following option:\n\nIt accepts the following parameters:\n\nAll parameters are optional and default to the equivalent of the string ’5:5:1.0:5:5:0.0’.\n• Apply a strong blur of both luma and chroma parameters:\n\nDecompose a video made of tiled images into the individual images.\n\nThe frame rate of the output video is the frame rate of the input video multiplied by the number of tiles.\n\nThis filter does the reverse of tile.\n\nThe filter accepts the following options:\n• Produce a 1-second video from a still image file made of 25 frames stacked vertically, like an analogic film reel:\n\nApply ultra slow/simple postprocessing filter that compresses and decompresses the image at several (or - in the case of level - all) shifts and average the results.\n\nThe way this differs from the behavior of spp is that uspp actually encodes & decodes each case with libavcodec Snow, whereas spp uses a simplified intra only 8x8 DCT similar to MJPEG.\n\nThis filter is not available in ffmpeg versions between 5.0 and 6.0.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n• Convert equirectangular video to cubemap with 3x2 layout and 1% padding using bicubic interpolation:\n• Convert transposed and horizontally flipped Equi-Angular Cubemap in side-by-side stereo format to equirectangular top-bottom stereo format:\n\nThis filter supports subset of above options as commands.\n\nIt transforms each frame from the video input into the wavelet domain, using Cohen-Daubechies-Feauveau 9/7. Then it applies some filtering to the obtained coefficients. It does an inverse wavelet transform after. Due to wavelet properties, it should give a nice smoothed result, and reduced noise, without blurring picture features.\n\nThis filter accepts the following options:\n\nApply variable blur filter by using 2nd video stream to set blur radius. The 2nd stream must have the same dimensions.\n\nThis filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nThis filter supports all the above options as commands.\n\nDisplay 2 color component values in the two dimensional graph (which is called a vectorscope).\n\nThis filter accepts the following options:\n\nAnalyze video stabilization/deshaking. Perform pass 1 of 2, see vidstabtransform for pass 2.\n\nThis filter generates a file with relative translation and rotation transform information about subsequent frames, which is then used by the vidstabtransform filter.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThis filter accepts the following options:\n• Analyze strongly shaky movie and put the results in file :\n• Visualize the result of internal transformations in the resulting video:\n\nVideo stabilization/deshaking: pass 2 of 2, see vidstabdetect for pass 1.\n\nRead a file with transform information for each frame and apply/compensate them. Together with the vidstabdetect filter this can be used to deshake videos. See also http://public.hronopik.de/vid.stab. It is important to also use the unsharp filter, see below.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n• Use for a typical stabilization with default values: Note the use of the unsharp filter which is always recommended.\n• Zoom in a bit more and load transform data from a given file:\n• Smoothen the video even more:\n\nFor example, to vertically flip a video with :\n\nThis filter tries to detect if the input is variable or constant frame rate.\n\nAt end it will output number of frames detected as having variable delta pts, and ones with constant delta pts. If there was frames with variable delta, than it will also show min, max and average delta encountered.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nObtain the average VIF (Visual Information Fidelity) between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained average VIF score is printed through the logging system.\n\nThe filter stores the calculated VIF score of each frame.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nThe filter accepts the following options:\n\nThe , and expressions can contain the following parameters.\n\nObtain the average VMAF motion score of a video. It is one of the component metrics of VMAF.\n\nThe obtained average motion score is printed through the logging system.\n\nThe filter accepts the following options:\n\nScale (resize) and convert colorspace, transfer characteristics or color primaries for the input video, using AMD Advanced Media Framework library for hardware acceleration. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n• Upscale to 4K and change color profile to bt2020.\n\nAll streams must be of same pixel format and of same width.\n\nNote that this filter is faster than using overlay and pad filter to create same output.\n\nThe filter accepts the following options:\n\nBased on the process described by Martin Weston for BBC R&D, and implemented based on the de-interlace algorithm written by Jim Easterbrook for BBC R&D, the Weston 3 field deinterlacing filter uses filter coefficients calculated by BBC R&D.\n\nThis filter uses field-dominance information in frame to decide which of each pair of fields to place first in the output. If it gets it wrong use setfield filter before filter.\n\nThere are two sets of filter coefficients, so called \"simple\" and \"complex\". Which set of filter coefficients is used can be set by passing an optional parameter:\n\nThis filter supports same commands as options.\n\nThe waveform monitor plots color component intensity. By default luma only. Each column of the waveform corresponds to a column of pixels in the source video.\n\nIt accepts the following options:\n\nThe takes a field-based video input and join each two sequential fields into single frame, producing a new double height clip with half the frame rate and half the frame count.\n\nThe works same as but without halving frame rate and frame count.\n\nIt accepts the following option:\n\nApply the xBR high-quality magnification filter which is designed for pixel art. It follows a set of edge-detection rules, see https://forums.libretro.com/t/xbr-algorithm-tutorial/123.\n\nIt accepts the following option:\n\nApply normalized cross-correlation between first and second input video stream.\n\nSecond input video stream dimensions must be lower than first input video stream.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nApply cross fade from one input video stream to another input video stream. The cross fade is applied for specified duration.\n\nBoth inputs must be constant frame-rate and have the same resolution, pixel format, frame rate and timebase.\n\nThe filter accepts the following options:\n• Cross fade from one input video to another input video, with fade transition and duration of transition of 2 seconds starting at offset of 5 seconds:\n\nThe filter accepts the following options:\n\nThis filter supports all above options as commands, excluding option .\n\nObtain the average (across all input frames) and minimum (across all color plane averages) eXtended Perceptually weighted peak Signal-to-Noise Ratio (XPSNR) between two input videos.\n\nThe XPSNR is a low-complexity psychovisually motivated distortion measurement algorithm for assessing the difference between two video streams or images. This is especially useful for objectively quantifying the distortions caused by video and image codecs, as an alternative to a formal subjective test. The logarithmic XPSNR output values are in a similar range as those of traditional psnr assessments but better reflect human impressions of visual coding quality. More details on the XPSNR measure, which essentially represents a blockwise weighted variant of the PSNR measure, can be found in the following freely available papers:\n• C. R. Helmrich, M. Siekmann, S. Becker, S. Bosse, D. Marpe, and T. Wiegand, \"XPSNR: A Low-Complexity Extension of the Perceptually Weighted Peak Signal-to-Noise Ratio for High-Resolution Video Quality Assessment,\" in Proc. IEEE Int. Conf. Acoustics, Speech, Sig. Process. (ICASSP), virt./online, May 2020. www.ecodis.de/xpsnr.htm\n• C. R. Helmrich, S. Bosse, H. Schwarz, D. Marpe, and T. Wiegand, \"A Study of the Extended Perceptually Weighted Peak Signal-to-Noise Ratio (XPSNR) for Video Compression with Different Resolutions and Bit Depths,\" ITU Journal: ICT Discoveries, vol. 3, no. 1, pp. 65 - 72, May 2020. http://handle.itu.int/11.1002/pub/8153d78b-en\n\nWhen publishing the results of XPSNR assessments obtained using, e.g., this FFmpeg filter, a reference to the above papers as a means of documentation is strongly encouraged. The filter requires two input videos. The first input is considered a (usually not distorted) reference source and is passed unchanged to the output, whereas the second input is a (distorted) test signal. Except for the bit depth, these two video inputs must have the same pixel format. In addition, for best performance, both compared input videos should be in YCbCr color format.\n\nThe obtained overall XPSNR values mentioned above are printed through the logging system. In case of input with multiple color planes, we suggest reporting of the minimum XPSNR average.\n\nThe following parameter, which behaves like the one for the psnr filter, is accepted:\n\nThis filter also supports the framesync options.\n• XPSNR analysis of two 1080p HD videos, ref_source.yuv and test_video.yuv, both at 24 frames per second, with color format 4:2:0, bit depth 8, and output of a logfile named \"xpsnr.log\":\n• XPSNR analysis of two 2160p UHD videos, ref_source.yuv with bit depth 8 and test_video.yuv with bit depth 10, both at 60 frames per second with color format 4:2:0, no logfile output:\n\nAll streams must be of same pixel format.\n\nThe filter accepts the following options:\n• Display 4 inputs into 2x2 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n• Display 4 inputs into 1x4 grid. Note that if inputs are of different widths, unused space will appear.\n• Display 9 inputs into 3x3 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n• Display 16 inputs into 4x4 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n\nDeinterlace the input video (\"yadif\" means \"yet another deinterlacing filter\").\n\nIt accepts the following parameters:\n\nApply blur filter while preserving edges (\"yaepblur\" means \"yet another edge preserving blur filter\"). The algorithm is described in \"J. S. Lee, Digital image enhancement and noise filtering by use of local statistics, IEEE Trans. Pattern Anal. Mach. Intell. PAMI-2, 1980.\"\n\nIt accepts the following parameters:\n\nThis filter supports same commands as options.\n\nThis filter accepts the following options:\n\nEach expression can contain the following constants:\n• Zoom in up to 1.5x and pan at same time to some spot near center of picture:\n• Zoom in up to 1.5x and pan always at center of picture:\n• Same as above but without pausing:\n• Zoom in 2x into center of picture only for the first second of the input video:\n\nScale (resize) the input video, using the z.lib library: https://github.com/sekrit-twc/zimg. To enable compilation of this filter, you need to configure FFmpeg with .\n\nThe zscale filter forces the output display aspect ratio to be the same as the input, by changing the output sample aspect ratio.\n\nIf the input image format is different from the format requested by the next filter, the zscale filter will convert the input to the requested format.\n\nThe filter accepts the following options.\n\nThe values of the and options are expressions containing the following constants:\n\nThis filter supports the following commands:\n\nTo enable CUDA and/or NPP filters please refer to configuration guidelines for CUDA and for CUDA NPP filters.\n\nRunning CUDA filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of initializing second CUDA device on the system and running scale_cuda and bilateral_cuda filters.\n\nSince CUDA filters operate exclusively on GPU memory, frame data must sometimes be uploaded (hwupload) to hardware surfaces associated with the appropriate CUDA device before processing, and downloaded (hwdownload) back to normal memory afterward, if required. Whether hwupload or hwdownload is necessary depends on the specific workflow:\n• If the input frames are already in GPU memory (e.g., when using or ), explicit use of hwupload is not needed, as the data is already in the appropriate memory space.\n• If the input frames are in CPU memory (e.g., software-decoded frames or frames processed by CPU-based filters), it is necessary to use hwupload to transfer the data to GPU memory for CUDA processing.\n• If the output of the CUDA filters needs to be further processed by software-based filters or saved in a format not supported by GPU-based encoders, hwdownload is required to transfer the data back to CPU memory.\n\nNote that hwupload uploads data to a surface with the same layout as the software frame, so it may be necessary to add a format filter immediately before hwupload to ensure the input is in the correct format. Similarly, hwdownload may not support all output formats, so an additional format filter may need to be inserted immediately after hwdownload in the filter graph to ensure compatibility.\n\nBelow is a description of the currently available Nvidia CUDA video filters.\n\nNote: If FFmpeg detects the Nvidia CUDA Toolkit during configuration, it will enable CUDA filters automatically without requiring any additional flags. If you want to explicitly enable them, use the following options:\n• Configure FFmpeg with . Additional requirement: lib must be installed.\n\nCUDA accelerated bilateral filter, an edge preserving filter. This filter is mathematically accurate thanks to the use of GPU acceleration. For best output quality, use one to one chroma subsampling, i.e. yuv444p format.\n\nThe filter accepts the following options:\n\nDeinterlace the input video using the bwdif algorithm, but implemented in CUDA so that it can work as part of a GPU accelerated pipeline with nvdec and/or nvenc.\n\nIt accepts the following parameters:\n\nThis filter works like normal chromakey filter but operates on CUDA frames. for more details and parameters see chromakey.\n• Make all the green pixels in the input video transparent and use it as an overlay for another video:\n\nIt is by no means feature complete compared to the software colorspace filter, and at the current time only supports color range conversion between jpeg/full and mpeg/limited range.\n\nThe filter accepts the following options:\n\nOverlay one video on top of another.\n\nThis is the CUDA variant of the overlay filter. It only accepts CUDA frames. The underlying input pixel formats have to match.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nIt accepts the following parameters:\n\nThis filter also supports the framesync options.\n\nScale (resize) and convert (pixel format) the input video, using accelerated CUDA kernels. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n• Don’t do any conversion or scaling, but copy all input frames into newly allocated ones. This can be useful to deal with a filter and encode chain that otherwise exhausts the decoders frame pool.\n\nDeinterlace the input video using the yadif algorithm, but implemented in CUDA so that it can work as part of a GPU accelerated pipeline with nvdec and/or nvenc.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available NVIDIA Performance Primitives (libnpp) video filters.\n\nUse the NVIDIA Performance Primitives (libnpp) to perform scaling and/or pixel format conversion on CUDA video frames. Setting the output width and height works in the same way as for the filter.\n\nThe following additional options are accepted:\n\nThe values of the and options are expressions containing the following constants:\n\nUse the NVIDIA Performance Primitives (libnpp) to scale (resize) the input video, based on a reference video.\n\nSee the scale_npp filter for available options, scale2ref_npp supports the same but uses the reference video instead of the main input as basis. scale2ref_npp also supports the following additional constants for the and options:\n• Scale a subtitle stream (b) to match the main video (a) in size before overlaying\n• Scale a logo to 1/10th the height of a video, while preserving its display aspect ratio.\n\nUse the NVIDIA Performance Primitives (libnpp) to perform image sharpening with border control.\n\nThe following additional options are accepted:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available OpenCL video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with .\n\nRunning OpenCL filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of choosing the first device on the second platform and running avgblur_opencl filter with default parameters on it.\n\nSince OpenCL filters are not able to access frame data in normal memory, all frame data needs to be uploaded(hwupload) to hardware surfaces connected to the appropriate device before being used and then downloaded(hwdownload) back to normal memory. Note that hwupload will upload to a surface with the same layout as the software frame, so it may be necessary to add a format filter immediately before to get the input into the right format and hwdownload does not support all formats on the output - it may be necessary to insert an additional format filter immediately following in the graph to get the output in a supported format.\n\nThe filter accepts the following options:\n• Apply average blur filter with horizontal and vertical size of 3, setting each pixel of the output to the average value of the 7x7 region centered on it in the input. For pixels on the edges of the image, the region does not extend beyond the image boundaries, and so out-of-range coordinates are not used in the calculations.\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n\nApply boxblur filter, setting each pixel of the output to the average value of box-radiuses , , for each plane respectively. The filter will apply , , times onto the corresponding plane. For pixels on the edges of the image, the radius does not extend beyond the image boundaries, and so out-of-range coordinates are not used in the calculations.\n• Apply a boxblur filter with the luma, chroma, and alpha radius set to 2 and luma, chroma, and alpha power set to 3. The filter will run 3 times with box-radius set to 2 for every plane of the image.\n• Apply a boxblur filter with luma radius set to 2, luma_power to 1, chroma_radius to 4, chroma_power to 5, alpha_radius to 3 and alpha_power to 7. For the luma plane, a 2x2 box radius will be run once. For the chroma plane, a 4x4 box radius will be run 5 times. For the alpha plane, a 3x3 box radius will be run 7 times.\n\nThe filter accepts the following options:\n• Make every semi-green pixel in the input transparent with some slight blending:\n\nThe filter accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) minimum.\n\nIt accepts the following options:\n• Apply erosion filter with threshold0 set to 30, threshold1 set 40, threshold2 set to 50 and coordinates set to 231, setting each pixel of the output to the local minimum between pixels: 1, 2, 3, 6, 7, 8 of the 3x3 region centered on it in the input. If the difference between input pixel and local minimum is more then threshold of the corresponding plane, output pixel will be set to input pixel - threshold of corresponding plane.\n\nThe filter accepts the following options:\n• Stabilize a video with debugging (both in console and in rendered video):\n\nThis filter replaces the pixel by the local(3x3) maximum.\n\nIt accepts the following options:\n• Apply dilation filter with threshold0 set to 30, threshold1 set 40, threshold2 set to 50 and coordinates set to 231, setting each pixel of the output to the local maximum between pixels: 1, 2, 3, 6, 7, 8 of the 3x3 region centered on it in the input. If the difference between input pixel and local maximum is more then threshold of the corresponding plane, output pixel will be set to input pixel + threshold of corresponding plane.\n\nNon-local Means denoise filter through OpenCL, this filter accepts same options as nlmeans.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid. This filter requires same memory layout for all the inputs. So, format conversion may be needed.\n\nThe filter accepts the following options:\n• Overlay an image LOGO at the top-left corner of the INPUT video. Both inputs are yuv420p format.\n• The inputs have same memory layout for color channels , the overlay has additional alpha plane, like INPUT is yuv420p, and the LOGO is yuva420p.\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following options:\n\nThe value for the , , , and options are expressions containing the following constants:\n\nThe filter accepts the following option:\n• Apply the Prewitt operator with scale set to 2 and delta set to 10.\n\nThe filter also supports the framesync options.\n\nThe program source file must contain a kernel function with the given name, which will be run once for each plane of the output. Each run on a plane gets enqueued as a separate 2D global NDRange with one work-item for each pixel to be generated. The global ID offset for each work-item is therefore the coordinates of a pixel in the destination image.\n\nThe kernel function needs to take the following arguments:\n• Destination image, . This image will become the output; the kernel should write all of it.\n• Frame index, . This is a counter starting from zero and increasing by one for each frame.\n• Source images, . These are the most recent images on each input. The kernel may read from them to generate the output, but they can’t be written to.\n• Copy the input to the output (output must be the same size as the input).\n• Apply a simple transformation, rotating the input by an amount increasing with the index counter. Pixel values are linearly interpolated by the sampler, and the output need not have the same dimensions as the input.\n• Blend two inputs together, with the amount of each input used varying with the index counter.\n\nDestination pixel at position (X, Y) will be picked from source (x, y) position where x = Xmap(X, Y) and y = Ymap(X, Y). If mapping values are out of range, zero value for pixel will be used for destination pixel.\n\nXmap and Ymap input video streams must be of same dimensions. Output video stream will have Xmap/Ymap video stream dimensions. Xmap and Ymap input video streams are 32bit float pixel format, single channel.\n\nThe filter accepts the following option:\n• Apply the Roberts cross operator with scale set to 2 and delta set to 10\n\nThe filter accepts the following option:\n• Apply sobel operator with scale set to 2 and delta set to 10\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n\nAll parameters are optional and default to the equivalent of the string ’5:5:1.0:5:5:0.0’.\n• Apply a strong blur of both luma and chroma parameters:\n\nCross fade two videos with custom transition effect by using OpenCL.\n\nIt accepts the following options:\n\nThe program source file must contain a kernel function with the given name, which will be run once for each plane of the output. Each run on a plane gets enqueued as a separate 2D global NDRange with one work-item for each pixel to be generated. The global ID offset for each work-item is therefore the coordinates of a pixel in the destination image.\n\nThe kernel function needs to take the following arguments:\n• Destination image, . This image will become the output; the kernel should write all of it.\n• First Source image, . Second Source image, . These are the most recent images on each input. The kernel may read from them to generate the output, but they can’t be written to.\n• Transition progress, . This value is always between 0 and 1 inclusive.\n\nVAAPI Video filters are usually used with VAAPI decoder and VAAPI encoder. Below is a description of VAAPI video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with .\n\nTo use vaapi filters, you need to setup the vaapi device correctly. For more information, please read https://trac.ffmpeg.org/wiki/Hardware/VAAPI\n\nOverlay one video on the top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nThe filter accepts the following options:\n\nThis filter also supports the framesync options.\n• Overlay an image LOGO at the top-left corner of the INPUT video. Both inputs for this filter are yuv420p format.\n• Overlay an image LOGO at the offset (200, 100) from the top-left corner of the INPUT video. The inputs have same memory layout for color channels, the overlay has additional alpha plane, like INPUT is yuv420p, and the LOGO is yuva420p.\n\nPerform HDR-to-SDR or HDR-to-HDR tone-mapping. It currently only accepts HDR10 as input.\n\nIt accepts the following parameters:\n\nThis is the VA-API variant of the hstack filter, each input stream may have different height, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the VA-API variant of the vstack filter, each input stream may have different width, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the VA-API variant of the xstack filter, each input stream may have different size, this filter will scale down/up each input stream to the given output size, or the size of the first input stream.\n\nIt accepts the following options:\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following options:\n\nThe value for the , , , and options are expressions containing the following constants:\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a black box around the edge of the input image:\n• Draw a box with color red and an opacity of 50%: The previous example can be specified as:\n\nBelow is a description of the currently available Vulkan video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with and either or .\n\nRunning Vulkan filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of choosing the first device and running nlmeans_vulkan filter with default parameters on it.\n\nAs Vulkan filters are not able to access frame data in normal memory, all frame data needs to be uploaded (hwupload) to hardware surfaces connected to the appropriate device before being used and then downloaded (hwdownload) back to normal memory. Note that hwupload will upload to a frame with the same layout as the software frame, so it may be necessary to add a format filter immediately before to get the input into the right format and hwdownload does not support all formats on the output - it is usually necessary to insert an additional format filter immediately following in the graph to get the output in a supported format.\n\nApply an average blur filter, implemented on the GPU using Vulkan.\n\nThe filter accepts the following options:\n\nBlend two Vulkan frames into each other.\n\nThe filter takes two input streams and outputs one stream, the first input is the \"top\" layer and second input is \"bottom\" layer. By default, the output terminates when the longest input terminates.\n\nA description of the accepted options follows.\n\nDeinterlacer using bwdif, the \"Bob Weaver Deinterlacing Filter\" algorithm, implemented on the GPU using Vulkan.\n\nIt accepts the following parameters:\n\nApply an effect that emulates chromatic aberration. Works best with RGB inputs, but provides a similar effect with YCbCr inputs too.\n\nVideo source that creates a Vulkan frame of a solid color. Useful for benchmarking, or overlaying.\n\nIt accepts the following parameters:\n\nFlips an image along both the vertical and horizontal axis.\n\nThe filter accepts the following options:\n\nDenoise frames using Non-Local Means algorithm, implemented on the GPU using Vulkan. Supports more pixel formats than nlmeans or nlmeans_opencl, including alpha channel support.\n\nThe filter accepts the following options.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid. This filter requires all inputs to use the same pixel format. So, format conversion may be needed.\n\nThe filter accepts the following options:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available QSV video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with or .\n\nTo use QSV filters, you need to setup the QSV device correctly. For more information, please read https://trac.ffmpeg.org/wiki/Hardware/QuickSync\n\nThis is the QSV variant of the hstack filter, each input stream may have different height, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the QSV variant of the vstack filter, each input stream may have different width, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the QSV variant of the xstack filter.\n\nIt accepts the following options:\n\nBelow is a description of the currently available video sources.\n\nBuffer video frames, and make them available to the filter chain.\n\nThis source is mainly intended for a programmatic use, in particular through the interface defined in .\n\nIt accepts the following parameters:\n\nwill instruct the source to accept video frames with size 320x240 and with format \"yuv410p\", assuming 1/24 as the timestamps timebase and square pixels (1:1 sample aspect ratio). Since the pixel format with name \"yuv410p\" corresponds to the number 6 (check the enum AVPixelFormat definition in ), this example corresponds to:\n\nAlternatively, the options can be specified as a flat string, but this syntax is deprecated:\n\nThe initial state of the cellular automaton can be defined through the and options. If such options are not specified an initial state is created randomly.\n\nAt each new frame a new row in the video is filled with the result of the cellular automaton next generation. The behavior when the whole frame is filled is defined by the option.\n\nThis source accepts the following options:\n• Read the initial state from , and specify an output of size 200x400.\n• Generate a random initial row with a width of 200 cells, with a fill ratio of 2/3:\n• Create a pattern generated by rule 18 starting by a single alive cell centered on an initial row with width 100:\n\nVideo source generated on GPU using Apple’s CoreImage API on OSX.\n\nThis video source is a specialized version of the coreimage video filter. Use a core image generator at the beginning of the applied filterchain to generate the content.\n\nThe coreimagesrc video source accepts the following options:\n\nAdditionally, all options of the coreimage video filter are accepted. A complete filterchain can be used for further processing of the generated input without CPU-HOST transfer. See coreimage documentation and examples for details.\n• Use CIQRCodeGenerator to create a QR code for the FFmpeg homepage, given as complete and escaped command-line for Apple’s standard bash shell: This example is equivalent to the QRCode example of coreimage without the need for a nullsrc video source.\n\nThe filter exclusively returns D3D11 Hardware Frames, for on-gpu encoding or processing. So an explicit hwdownload is needed for any kind of software processing.\n\nIt accepts the following options:\n\nYou can also skip the lavfi device and directly use the filter. Also demonstrates downloading the frame and encoding with libx264. Explicit output format specification is required in this case:\n\nIf you want to capture only a subsection of the desktop, this can be achieved by specifying a smaller size and its offsets into the screen:\n\nThis source supports the some above options as commands.\n\nGenerate a Mandelbrot set fractal, and progressively zoom towards the point specified with and .\n\nThis source accepts the following options:\n\nGenerate various test patterns, as generated by the MPlayer test filter.\n\nThe size of the generated video is fixed, and is 256x256. This source is useful in particular for testing encoding features.\n\nThis source accepts the following options:\n\nTo enable compilation of this filter you need to install the frei0r header and configure FFmpeg with .\n\nThis source accepts the following parameters:\n\nFor example, to generate a frei0r partik0l source with size 200x200 and frame rate 10 which is overlaid on the overlay filter main input:\n\nThis source is based on a generalization of John Conway’s life game.\n\nThe sourced input represents a life grid, each pixel represents a cell which can be in one of two possible states, alive or dead. Every cell interacts with its eight neighbours, which are the cells that are horizontally, vertically, or diagonally adjacent.\n\nAt each interaction the grid evolves according to the adopted rule, which specifies the number of neighbor alive cells which will make a cell stay alive or born. The option allows one to specify the rule to adopt.\n\nThis source accepts the following options:\n• Read a grid from , and center it on a grid of size 300x300 pixels:\n• Generate a random grid of size 200x200, with a fill ratio of 2/3:\n• Full example with slow death effect (mold) using :\n\nPerlin noise is a kind of noise with local continuity in space. This can be used to generate patterns with continuity in space and time, e.g. to simulate smoke, fluids, or terrain.\n\nIn case more than one octave is specified through the option, Perlin noise is generated as a sum of components, each one with doubled frequency. In this case the option specify the ratio of the amplitude with respect to the previous component. More octave components enable to specify more high frequency details in the generated noise (e.g. small size variations due to boulders in a generated terrain).\n• Use Perlin noise with 7 components, each one with a halved contribution to total amplitude:\n• Chain Perlin noise with the lutyuv to generate a black&white effect:\n• Stretch noise along the y axis, and convert gray level to red-only signal:\n\nGenerate a QR code using the libqrencode library (see https://fukuchi.org/works/qrencode/).\n\nTo enable the compilation of this source, you need to configure FFmpeg with .\n\nThe QR code is generated from the provided text or text pattern. The corresponding QR code is scaled and put in the video output according to the specified output size options.\n\nIn case no text is specified, the QR code is not generated, but an empty colored output is returned instead.\n\nThis source accepts the following options:\n• Generate a QR code encoding the specified text with the default size:\n• Same as below, but select blue on pink colors:\n• Generate a QR code with width of 200 pixels and padding, making the padded width 4/3 of the QR code width:\n• Generate a QR code with padded width of 200 pixels and padding, making the QR code width 3/4 of the padded width:\n\nThe source returns frames of size 4096x4096 of all rgb colors.\n\nThe source returns frames of size 4096x4096 of all yuv colors.\n\nThe source provides an uniformly colored input.\n\nThe source provides an identity Hald CLUT. See also haldclut filter.\n\nThe source returns unprocessed video frames. It is mainly useful to be employed in analysis / debugging tools, or as the source for filters which ignore the input data.\n\nThe source generates a color bars pattern, based on EBU PAL recommendations with 75% color levels.\n\nThe source generates a color bars pattern, based on EBU PAL recommendations with 100% color levels.\n\nThe source generates an RGB test pattern useful for detecting RGB vs BGR issues. You should see a red, green and blue stripe from top to bottom.\n\nThe source generates a color bars pattern, based on the SMPTE Engineering Guideline EG 1-1990.\n\nThe source generates a color bars pattern, based on the SMPTE RP 219-2002.\n\nThe source generates a test video pattern, showing a color pattern, a scrolling gradient and a timestamp. This is mainly intended for testing purposes.\n\nThe source is similar to testsrc, but supports more pixel formats instead of just . This allows using it as an input for other tests without requiring a format conversion.\n\nThe source generates an YUV test pattern. You should see a y, cb and cr stripe from top to bottom.\n\nThe sources accept the following parameters:\n• Generate a video with a duration of 5.3 seconds, with size 176x144 and a frame rate of 10 frames per second:\n• The following graph description will generate a red source with an opacity of 0.2, with size \"qcif\" and a frame rate of 10 frames per second:\n• If the input content is to be ignored, can be used. The following command generates noise in the luma plane by employing the filter:\n\nThe source supports the following commands:\n\nFor details of how the program loading works, see the program_opencl filter.\n• Generate a colour ramp by setting pixel values from the position of the pixel in the output image. (Note that this will work with all pixel formats, but the generated output will not be the same.)\n• Generate a Sierpinski carpet pattern, panning by a single pixel each frame. __kernel void sierpinski_carpet(__write_only image2d_t dst, unsigned int index) { int2 loc = (int2)(get_global_id(0), get_global_id(1)); float4 value = 0.0f; int x = loc.x + index; int y = loc.y + index; while (x > 0 || y > 0) { if (x % 3 == 1 && y % 3 == 1) { value = 1.0f; break; } x /= 3; y /= 3; } write_imagef(dst, loc, value); }\n\nThis source accepts the following options:\n\nThis source accepts the following options:\n\nThis source supports the some above options as commands.\n\nBelow is a description of the currently available video sinks.\n\nBuffer video frames, and make them available to the end of the filter graph.\n\nThis sink is mainly intended for programmatic use, in particular through the interface defined in or the options system.\n\nIt accepts a pointer to an AVBufferSinkContext structure, which defines the incoming buffers’ formats, to be passed as the opaque parameter to for initialization.\n\nNull video sink: do absolutely nothing with the input video. It is mainly useful as a template and for use in analysis / debugging tools.\n\nBelow is a description of the currently available multimedia filters.\n\nThe filter accepts the following options:\n\nFilter supports the some above options as commands.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nMeasures phase of input audio, which is exported as metadata , representing mean phase of current audio frame. A video output can also be produced and is enabled by default. The audio is passed through as first output.\n\nAudio will be rematrixed to stereo if it has a different channel layout. Phase value is in range where means left and right channels are completely out of phase and means channels are in phase.\n\nThe filter accepts the following options, all related to its video output:\n\nThe filter also detects out of phase and mono sequences in stereo streams. It logs the sequence start, end and duration when it lasts longer or as long as the minimum set.\n\nThe filter accepts the following options for this detection:\n• Complete example with to detect 1 second of mono with 0.001 phase tolerance:\n\nThe filter is used to measure the difference between channels of stereo audio stream. A monaural signal, consisting of identical left and right signal, results in straight vertical line. Any stereo separation is visible as a deviation from this line, creating a Lissajous figure. If the straight (or deviation from it) but horizontal line appears this indicates that the left and right channels are out of phase.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands except options and .\n\nThe filter accepts the following options:\n\nConcatenate audio and video streams, joining them together one after the other.\n\nThe filter works on segments of synchronized video and audio streams. All segments must have the same number of streams of each type, and that will also be the number of streams at output.\n\nThe filter accepts the following options:\n\nThe filter has + outputs: first video outputs, then audio outputs.\n\nThere are x( + ) inputs: first the inputs for the first segment, in the same order as the outputs, then the inputs for the second segment, etc.\n\nRelated streams do not always have exactly the same duration, for various reasons including codec frame size or sloppy authoring. For that reason, related synchronized streams (e.g. a video and its audio track) should be concatenated at once. The concat filter will use the duration of the longest stream in each segment (except the last one), and if necessary pad shorter audio streams with silence.\n\nFor this filter to work correctly, all segments must start at timestamp 0.\n\nAll corresponding streams must have the same parameters in all segments; the filtering system will automatically select a common pixel format for video streams, and a common sample format, sample rate and channel layout for audio streams, but other settings, such as resolution, must be converted explicitly by the user.\n\nDifferent frame rates are acceptable but will result in variable frame rate at output; be sure to configure the output file to handle it.\n• Concatenate an opening, an episode and an ending, all in bilingual version (video in stream 0, audio in streams 1 and 2):\n• Concatenate two parts, handling audio and video separately, using the (a)movie sources, and adjusting the resolution: Note that a desync will happen at the stitch if the audio and video streams do not have exactly the same duration in the first file.\n\nThis filter supports the following commands:\n\nEBU R128 scanner filter. This filter takes an audio stream and analyzes its loudness level. By default, it logs a message at a frequency of 10Hz with the Momentary loudness (identified by ), Short-term loudness ( ), Integrated loudness ( ) and Loudness Range ( ).\n\nThe filter can only analyze streams which have sample format is double-precision floating point. The input stream will be converted to this specification, if needed. Users may need to insert aformat and/or aresample filters after this filter to obtain the original parameters.\n\nThe filter also has a video output (see the option) with a real time graph to observe the loudness evolution. The graphic contains the logged message mentioned above, so it is not printed anymore when this option is set, unless the verbose logging is set. The main graphing area contains the short-term loudness (3 seconds of analysis), and the gauge on the right is for the momentary loudness (400 milliseconds), but can optionally be configured to instead display short-term loudness (see ).\n\nThe green area marks a +/- 1LU target range around the target loudness (-23LUFS by default, unless modified through ).\n\nMore information about the Loudness Recommendation EBU R128 on http://tech.ebu.ch/loudness.\n\nThe filter accepts the following options:\n\nThese filters read frames from several inputs and send the oldest queued frame to the output.\n\nInput streams must have well defined, monotonically increasing frame timestamp values.\n\nIn order to submit one frame to output, these filters need to enqueue at least one frame for each input, so they cannot work in case one input is not yet terminated and will not receive incoming frames.\n\nFor example consider the case when one input is a filter which always drops input frames. The filter will keep reading from that input, but it will never be able to send new frames to output until the input sends an end-of-stream signal.\n\nAlso, depending on inputs synchronization, the filters will drop frames in case one input receives more frames than the other ones, and the queue is already filled.\n\nThese filters accept the following options:\n• Interleave frames belonging to different streams using :\n\nReport previous filter filtering latency, delay in number of audio samples for audio filters or number of video frames for video filters.\n\nOn end of input stream, filter will report min and max measured latency for previous running filter in filtergraph.\n\nThis filter accepts the following options:\n• Print all metadata values for frames with key with values between 0 and 1.\n• Direct all metadata to a pipe with file descriptor 4.\n\nThese filters are mainly aimed at developers to test direct path in the following filter in the filtergraph.\n\nThe filters accept the following options:\n\nNote: in case of auto-inserted filter between the permission filter and the following one, the permission might not be received as expected in that following filter. Inserting a format or aformat filter before the perms/aperms filter can avoid this problem.\n\nThese filters will pause the filtering for a variable amount of time to match the output rate with the input timestamps. They are similar to the option to .\n\nThey accept the following options:\n\nBoth filters supports the all above options as commands.\n\nThis filter does opposite of concat filters.\n\nThis filter accepts the following options:\n\nIn all cases, prefixing an each segment with ’+’ will make it relative to the previous segment.\n• Split input audio stream into three output audio streams, starting at start of input audio stream and storing that in 1st output audio stream, then following at 60th second and storing than in 2nd output audio stream, and last after 150th second of input audio stream store in 3rd output audio stream:\n\nThis filter accepts the following options:\n\nThe expression can contain the following constants:\n\nThe default value of the select expression is \"1\".\n• Select all frames in input: The example above is the same as:\n• Select only frames contained in the 10-20 time interval:\n• Select only I-frames contained in the 10-20 time interval:\n• Use aselect to select only audio frames with samples number > 100:\n• Create a mosaic of the first scenes: Comparing against a value between 0.3 and 0.5 is generally a sane choice.\n• Send even and odd frames to separate outputs, and compose them:\n• Select useful frames from an ffconcat file which is using inpoints and outpoints but where the source files are not intra frame only.\n\nSend commands to filters in the filtergraph.\n\nThese filters read commands to be sent to other filters in the filtergraph.\n\nmust be inserted between two video filters, must be inserted between two audio filters, but apart from that they act the same way.\n\nThe specification of commands can be provided in the filter arguments with the option, or in a file specified by the option.\n\nThese filters accept the following options:\n\nA commands description consists of a sequence of interval specifications, comprising a list of commands to be executed when a particular event related to that interval occurs. The occurring event is typically the current frame time entering or leaving a given time interval.\n\nAn interval is specified by the following syntax:\n\nThe time interval is specified by the and times. is optional and defaults to the maximum time.\n\nThe current frame time is considered within the specified interval if it is included in the interval [ , ), that is when the time is greater or equal to and is lesser than .\n\nconsists of a sequence of one or more command specifications, separated by \",\", relating to that interval. The syntax of a command specification is given by:\n\nis optional and specifies the type of events relating to the time interval which enable sending the specified command, and must be a non-null sequence of identifier flags separated by \"+\" or \"|\" and enclosed between \"[\" and \"]\".\n\nThe following flags are recognized:\n\nIf is not specified, a default value of is assumed.\n\nspecifies the target of the command, usually the name of the filter class or a specific filter instance name.\n\nspecifies the name of the command for the target filter.\n\nis optional and specifies the optional list of argument for the given .\n\nBetween one interval specification and another, whitespaces, or sequences of characters starting with until the end of line, are ignored and can be used to annotate comments.\n\nA simplified BNF description of the commands specification syntax follows:\n• Specify audio tempo change at second 4:\n• Specify a list of drawtext and hue commands in a file. # show text in the interval 5-10 5.0-10.0 [enter] drawtext reinit 'fontfile=FreeSerif.ttf:text=hello world', [leave] drawtext reinit 'fontfile=FreeSerif.ttf:text='; # desaturate the image in the interval 15-20 15.0-20.0 [enter] hue s 0, [enter] drawtext reinit 'fontfile=FreeSerif.ttf:text=nocolor', [leave] hue s 1, [leave] drawtext reinit 'fontfile=FreeSerif.ttf:text=color'; # apply an exponential saturation fade-out effect, starting from time 25 25 [enter] hue s exp(25-t) A filtergraph allowing to read and process the above command list stored in a file , can be specified with:\n\nChange the PTS (presentation timestamp) of the input frames.\n\nThis filter accepts the following options:\n\nThe expression is evaluated through the eval API and can contain the following constants:\n• Set fixed rate of 25 frames per second:\n• Apply an offset of 10 seconds to the input PTS:\n• Generate timestamps from a \"live source\" and rebase onto the current timebase:\n\nBoth filters support all above options as commands.\n\nThe filter marks the color range property for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by following filters.\n\nThe filter accepts the following options:\n\nSet the timebase to use for the output frames timestamps. It is mainly useful for testing timebase configuration.\n\nIt accepts the following parameters:\n\nThe value for is an arithmetic expression representing a rational. The expression can contain the constants \"AVTB\" (the default timebase), \"intb\" (the input timebase) and \"sr\" (the sample rate, audio only). Default value is \"intb\".\n\nConvert input audio to a video output representing frequency spectrum logarithmically using Brown-Puckette constant Q transform algorithm with direct frequency domain coefficient calculation (but the transform itself is not really constant Q, instead the Q factor is actually variable/clamped), with musical tone scale, from E0 to D#10.\n\nThe filter accepts the following options:\n• Same as above, but with frame rate 30 fps:\n• Same as above, but with more accuracy in frequency domain:\n• Custom gamma, now spectrum is linear to the amplitude.\n• Custom fontcolor and fontfile, C-note is colored green, others are colored blue:\n\nConvert input audio to video output representing frequency spectrum using Continuous Wavelet Transform and Morlet wavelet.\n\nThe filter accepts the following options:\n\nConvert input audio to video output representing the audio power spectrum. Audio amplitude is on Y-axis while frequency is on X-axis.\n\nThe filter accepts the following options:\n\nConvert stereo input audio to a video output, representing the spatial relationship between two channels.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThe usage is very similar to the showwaves filter; see the examples in that section.\n• Complete example for a colored and sliding spectrum per channel using :\n\nThe filter accepts the following options:\n• Extract an audio spectrogram of a whole audio track in a 1024x1024 picture using :\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n• Output the input file audio and the corresponding video representation at the same time:\n• Create a synthetic signal and show it with showwaves, forcing a frame rate of 30 frames per second:\n\nThe filter accepts the following options:\n• Extract a channel split representation of the wave form of a whole audio track in a 1024x800 picture using :\n\nDelete frame side data, or select frames based on it.\n\nThis filter accepts the following options:\n\nSynthesize audio from 2 input video spectrums, first input stream represents magnitude across time and second represents phase across time. The filter will transform from frequency domain as displayed in videos back to time domain as presented in audio output.\n\nThis filter is primarily created for reversing processed showspectrum filter outputs, but can synthesize sound from other spectrograms too. But in such case results are going to be poor if the phase data is not available, because in such cases phase data need to be recreated, usually it’s just recreated from random noise. For best results use gray only output ( color mode in showspectrum filter) and scale for magnitude video and scale for phase video. To produce phase, for 2nd video, use option. Inputs videos should generally use slide mode as that saves resources needed for decoding video.\n\nThe filter accepts the following options:\n• First create magnitude and phase videos from audio, assuming audio is stereo with 44100 sample rate, then resynthesize videos back to audio with spectrumsynth:\n\nThe filter accepts a single parameter which specifies the number of outputs. If unspecified, it defaults to 2.\n• Create two separate outputs from the same input:\n• To create 3 or more outputs, you need to specify the number of outputs, like in:\n• Create two separate outputs from the same input, one cropped and one padded:\n• Create 5 copies of the input audio with :\n\nReceive commands sent through a libzmq client, and forward them to filters in the filtergraph.\n\nand work as a pass-through filters. must be inserted between two video filters, between two audio filters. Both are capable to send messages to any filter type.\n\nTo enable these filters you need to install the libzmq library and headers and configure FFmpeg with .\n\nFor more information about libzmq see: http://www.zeromq.org/\n\nThe and filters work as a libzmq server, which receives messages sent through a network interface defined by the (or the abbreviation \" \") option. Default value of this option is . You may want to alter this value to your needs, but do not forget to escape any ’:’ signs (see filtergraph escaping).\n\nThe received message must be in the form:\n\nspecifies the target of the command, usually the name of the filter class or a specific filter instance name. The default filter instance name uses the pattern ‘ ’, but you can override this by using the ‘ ’ syntax (see Filtergraph syntax).\n\nspecifies the name of the command for the target filter.\n\nis optional and specifies the optional argument list for the given .\n\nUpon reception, the message is processed and the corresponding command is injected into the filtergraph. Depending on the result, the filter will send a reply to the client, adopting the format:\n\nLook at for an example of a zmq client which can be used to send commands processed by these filters.\n\nConsider the following filtergraph generated by . In this example the last overlay filter has an instance name. All other filters will have default instance names.\n\nTo change the color of the left side of the video, the following command can be used:\n\nTo change the right side:\n\nTo change the position of the right side:\n\nBelow is a description of the currently available multimedia sources.\n\nThis is the same as movie source, except it selects an audio stream by default.\n\nGenerated stream periodically shows flash video frame and emits beep in audio. Useful to inspect A/V sync issues.\n\nIt accepts the following options:\n\nThis source supports the some above options as commands.\n\nIt accepts the following parameters:\n\nIt allows overlaying a second video on top of the main input of a filtergraph, as shown in this graph:\n• Skip 3.2 seconds from the start of the AVI file in.avi, and overlay it on top of the input labelled \"in\": movie=in.avi:seek_point=3.2, scale=180:-1, setpts=PTS-STARTPTS [over]; [in] setpts=PTS-STARTPTS [main]; [main][over] overlay=16:16 [out]\n• Read from a video4linux2 device, and overlay it on top of the input labelled \"in\": movie=/dev/video0:f=video4linux2, scale=180:-1, setpts=PTS-STARTPTS [over]; [in] setpts=PTS-STARTPTS [main]; [main][over] overlay=16:16 [out]\n• Read the first video stream and the audio stream with id 0x81 from dvd.vob; the video is connected to the pad named \"video\" and the audio is connected to the pad named \"audio\":\n\nBoth movie and amovie support the following commands:\n\nFFmpeg can be hooked up with a number of external libraries to add support for more formats. None of them are used by default, their use has to be explicitly requested by passing the appropriate flags to .\n\nFFmpeg can make use of the AOM library for AV1 decoding and encoding.\n\nGo to http://aomedia.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can use the AMD Advanced Media Framework library for accelerated H.264 and HEVC(only windows) encoding on hardware with Video Coding Engine (VCE).\n\nTo enable support you must obtain the AMF framework header files(version 1.4.9+) from https://github.com/GPUOpen-LibrariesAndSDKs/AMF.git.\n\nCreate an directory in the system include path. Copy the contents of into that directory. Then configure FFmpeg with .\n\nInitialization of amf encoder occurs in this order: 1) trying to initialize through dx11(only windows) 2) trying to initialize through dx9(only windows) 3) trying to initialize through vulkan\n\nTo use h.264(AMD VCE) encoder on linux amdgru-pro version 19.20+ and amf-amdgpu-pro package(amdgru-pro contains, but does not install automatically) are required.\n\nThis driver can be installed using amdgpu-pro-install script in official amd driver archive.\n\nFFmpeg can read AviSynth scripts as input. To enable support, pass to configure after installing the headers provided by AviSynth+. AviSynth+ can be configured to install only the headers by either passing to the normal CMake-based build system, or by using the supplied .\n\nFor Windows, supported AviSynth variants are AviSynth 2.6 RC1 or higher for 32-bit builds and AviSynth+ r1718 or higher for 32-bit and 64-bit builds.\n\nFor Linux, macOS, and BSD, the only supported AviSynth variant is AviSynth+, starting with version 3.5.\n\nFFmpeg can make use of the Chromaprint library for generating audio fingerprints. Pass to configure to enable it. See https://acoustid.org/chromaprint.\n\nFFmpeg can make use of the codec2 library for codec2 decoding and encoding. There is currently no native decoder, so libcodec2 must be used for decoding.\n\nGo to http://freedv.org/, download \"Codec 2 source archive\". Build and install using CMake. Debian users can install the libcodec2-dev package instead. Once libcodec2 is installed you can pass to configure to enable it.\n\nThe easiest way to use codec2 is with .c2 files, since they contain the mode information required for decoding. To encode such a file, use a .c2 file extension and give the libcodec2 encoder the -mode option: . Playback is as simple as . For a list of supported modes, run . Raw codec2 files are also supported. To make sense of them the mode in use needs to be specified as a format option: .\n\nFFmpeg can make use of the dav1d library for AV1 video decoding.\n\nGo to https://code.videolan.org/videolan/dav1d and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the davs2 library for AVS2-P2/IEEE1857.4 video decoding.\n\nGo to https://github.com/pkuvcl/davs2 and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the uavs3d library for AVS3-P2/IEEE1857.10 video decoding.\n\nGo to https://github.com/uavs3/uavs3d and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the Game Music Emu library to read audio from supported video game music file formats. Pass to configure to enable it. See https://bitbucket.org/mpyne/game-music-emu/overview.\n\nFFmpeg can use Intel QuickSync Video (QSV) for accelerated decoding and encoding of multiple codecs. To use QSV, FFmpeg must be linked against the dispatcher, which loads the actual decoding libraries.\n\nThe dispatcher is open source and can be downloaded from https://github.com/lu-zero/mfx_dispatch.git. FFmpeg needs to be configured with the option and needs to be able to locate the dispatcher’s files.\n\nFFmpeg can make use of the Kvazaar library for HEVC encoding.\n\nGo to https://github.com/ultravideo/kvazaar and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the LAME library for MP3 encoding.\n\nGo to http://lame.sourceforge.net/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the liblcevc_dec library for LCEVC enhacement layer decoding on supported bitstreams.\n\nGo to https://github.com/v-novaltd/LCEVCdec and follow the instructions for installing the library. Then pass to configure to enable it.\n\niLBC is a narrowband speech codec that has been made freely available by Google as part of the WebRTC project. libilbc is a packaging friendly copy of the iLBC codec. FFmpeg can make use of the libilbc library for iLBC decoding and encoding.\n\nGo to https://github.com/TimothyGu/libilbc and follow the instructions for installing the library. Then pass to configure to enable it.\n\nJPEG XL is an image format intended to fully replace legacy JPEG for an extended period of life. See https://jpegxl.info/ for more information, and see https://github.com/libjxl/libjxl for the library source. You can pass to configure in order enable the libjxl wrapper.\n\nFFmpeg can make use of the libvpx library for VP8/VP9 decoding and encoding.\n\nGo to http://www.webmproject.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of this library, originating in Modplug-XMMS, to read from MOD-like music files. See https://github.com/Konstanty/libmodplug. Pass to configure to enable it.\n\nSpun off Google Android sources, OpenCore, VisualOn and Fraunhofer libraries provide encoders for a number of audio codecs.\n\nFFmpeg can make use of the OpenCORE libraries for AMR-NB decoding/encoding and AMR-WB decoding.\n\nGo to http://sourceforge.net/projects/opencore-amr/ and follow the instructions for installing the libraries. Then pass and/or to configure to enable them.\n\nFFmpeg can make use of the VisualOn AMR-WBenc library for AMR-WB encoding.\n\nGo to http://sourceforge.net/projects/opencore-amr/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the Fraunhofer AAC library for AAC decoding & encoding.\n\nGo to http://sourceforge.net/projects/opencore-amr/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the Google LC3 library for LC3 decoding & encoding.\n\nGo to https://github.com/google/liblc3/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the OpenH264 library for H.264 decoding and encoding.\n\nGo to http://www.openh264.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFor decoding, this library is much more limited than the built-in decoder in libavcodec; currently, this library lacks support for decoding B-frames and some other main/high profile features. (It currently only supports constrained baseline profile and CABAC.) Using it is mostly useful for testing and for taking advantage of Cisco’s patent portfolio license (http://www.openh264.org/BINARY_LICENSE.txt).\n\nFFmpeg can use the OpenJPEG libraries for decoding/encoding J2K videos. Go to http://www.openjpeg.org/ to get the libraries and follow the installation instructions. To enable using OpenJPEG in FFmpeg, pass to .\n\nFFmpeg can make use of rav1e (Rust AV1 Encoder) via its C bindings to encode videos. Go to https://github.com/xiph/rav1e/ and follow the instructions to build the C library. To enable using rav1e in FFmpeg, pass to .\n\nFFmpeg can make use of the Scalable Video Technology for AV1 library for AV1 encoding.\n\nGo to https://gitlab.com/AOMediaCodec/SVT-AV1/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the TwoLAME library for MP2 encoding.\n\nGo to http://www.twolame.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can read VapourSynth scripts as input. To enable support, pass to configure. Vapoursynth is detected via . Versions 42 or greater supported. See http://www.vapoursynth.com/.\n\nDue to security concerns, Vapoursynth scripts will not be autodetected so the input format has to be forced. For ff* CLI tools, add before the input .\n\nFFmpeg can make use of the x264 library for H.264 encoding.\n\nGo to http://www.videolan.org/developers/x264.html and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the x265 library for HEVC encoding.\n\nGo to http://x265.org/developers.html and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the xavs library for AVS encoding.\n\nGo to http://xavs.sf.net/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the xavs2 library for AVS2-P2/IEEE1857.4 video encoding.\n\nGo to https://github.com/pkuvcl/xavs2 and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the XEVE library for EVC video encoding.\n\nGo to https://github.com/mpeg5/xeve and follow the instructions for installing the XEVE library. Then pass to configure to enable it.\n\nFFmpeg can make use of the XEVD library for EVC video decoding.\n\nGo to https://github.com/mpeg5/xevd and follow the instructions for installing the XEVD library. Then pass to configure to enable it.\n\nZVBI is a VBI decoding library which can be used by FFmpeg to decode DVB teletext pages and DVB teletext subtitles.\n\nGo to http://sourceforge.net/projects/zapping/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nYou can use the and options to have an exhaustive list.\n\nFFmpeg supports the following file formats through the library:\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nFFmpeg can read and write images for each frame of a video sequence. The following image formats are supported:\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nmeans that support is provided through an external library.\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nmeans that support is provided through an external library.\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nmeans that support is provided through an external library.\n\nmeans that an integer-only version is available, too (ensures high performance on systems without hardware floating point support).\n\nmeans that the feature is supported.\n\nmeans that support is provided through an external library.\n\nmeans that the protocol is supported.\n\nmeans that support is provided through an external library.\n\nFor details about the authorship, see the Git history of the project (https://git.ffmpeg.org/ffmpeg), e.g. by typing the command in the FFmpeg source directory, or browsing the online repository at https://git.ffmpeg.org/ffmpeg.\n\nMaintainers for the specific components are listed in the file in the source code tree.\n\nThis document was generated on March 23, 2025 using makeinfo."
    },
    {
        "link": "https://ffmpeg.org/developer.html",
        "document": "This text is concerned with the development of FFmpeg itself. Information on using the FFmpeg libraries in other programs can be found elsewhere, e.g. in:\n• the Doxygen documentation generated from the headers\n\nFor more detailed legal information about the use of FFmpeg in external programs read the file in the source tree and consult https://ffmpeg.org/legal.html.\n\nIf you modify FFmpeg code for your own use case, you are highly encouraged to submit your changes back to us, using this document as a guide. There are both pragmatic and ideological reasons to do so:\n• Maintaining external changes to keep up with upstream development is time-consuming and error-prone. With your code in the main tree, it will be maintained by FFmpeg developers.\n• FFmpeg developers include leading experts in the field who can find bugs or design flaws in your code.\n• By supporting the project you find useful you ensure it continues to be maintained and developed.\n\nAll proposed code changes should be submitted for review to the development mailing list, as described in more detail in the Submitting patches chapter. The code should comply with the Development Policy and follow the Coding Rules. The developer making the commit and the author are responsible for their changes and should try to fix issues their commit causes.\n\nFFmpeg is mainly programmed in the ISO C11 language, except for the public headers which must stay C99 compatible.\n\nCompiler-specific extensions may be used with good reason, but must not be depended on, i.e. the code must still compile and work with compilers lacking the extension.\n\nThe following C99 features must not be used anywhere in the codebase:\n\nAs modern compilers are unable to generate efficient SIMD or other performance-critical DSP code from plain C, handwritten assembly is used. Usually such code is isolated in a separate function. Then the standard approach is writing multiple versions of this function – a plain C one that works everywhere and may also be useful for debugging, and potentially multiple architecture-specific optimized implementations. Initialization code then chooses the best available version at runtime and loads it into a function pointer; the function in question is then always called through this pointer.\n\nThe specific syntax used for writing assembly is:\n\nA unit testing framework for assembly called lives under . All new assembly should come with tests; adding tests for existing assembly that lacks them is also strongly encouraged.\n\nOther languages than C may be used in special cases:\n• Compiler intrinsics or inline assembly when the code in question cannot be written in the standard way described in the SIMD/DSP section. This typically applies to code that needs to be inlined.\n• Objective-C where required for interacting with macOS-specific interfaces.\n\nThere are the following guidelines regarding the code style in files:\n• The TAB character is forbidden outside of Makefiles as is any form of trailing whitespace. Commits containing either will be rejected by the git repository.\n• You should try to limit your code lines to 80 characters; however, do so if and only if this improves readability.\n\nThe presentation is one inspired by ’indent -i4 -kr -nut’.\n\nSome notable examples to illustrate common code style in FFmpeg:\n• Space around assignments and after / / / keywords: However no spaces between the parentheses and condition, unless it helps readability of complex conditions, so the following should not be done:\n• No unnecessary parentheses, unless it helps readability:\n• Don’t wrap single-line blocks in braces. Use braces only if there is an accompanying else statement. This keeps future code changes easier to keep track of. // Good if (bits_pixel == 24) { avctx->pix_fmt = AV_PIX_FMT_BGR24; } else if (bits_pixel == 8) { avctx->pix_fmt = AV_PIX_FMT_GRAY8; } else return AVERROR_INVALIDDATA;\n• Avoid assignments in conditions where it makes sense:\n• When declaring a pointer variable, the goes with the variable not the type:\n\nIf you work on a file that does not follow these guidelines consistently, change the parts that you are editing to follow these guidelines but do not make unrelated changes in the file to make it conform to these.\n\nIn order to configure Vim to follow FFmpeg formatting conventions, paste the following snippet into your :\n\nFor Emacs, add these roughly equivalent lines to your :\n\nUse the JavaDoc/Doxygen format (see examples below) so that code documentation can be generated automatically. All nontrivial functions should have a comment above them explaining what the function does, even if it is just one sentence. All structures and their member variables should be documented, too.\n\nAvoid Qt-style and similar Doxygen syntax with in it, i.e. replace with and similar. Also @ syntax should be employed for markup commands, i.e. use and not .\n\nNames of functions, variables, and struct members must be lowercase, using underscores (_) to separate words. For example, ‘ ’ is an acceptable function name and ‘ ’ is not.\n\nStruct, union, enum, and typedeffed type names must use CamelCase. All structs and unions should be typedeffed to the same name as the struct/union tag, e.g. . Enums are typically not typedeffed.\n\nEnumeration constants and macros must be UPPERCASE, except for macros masquerading as functions, which should use the function naming convention.\n\nAll identifiers in the libraries should be namespaced as follows:\n• No namespacing for identifiers with file and lower scope (e.g. local variables, static functions), and struct and union members,\n• The prefix must be used for variables and functions visible outside of file scope, but only used internally within a single library, e.g. ‘ ’. This prevents name collisions when FFmpeg is statically linked.\n• For variables and functions visible outside of file scope, used internally across multiple libraries, use as prefix, for example, ‘ ’.\n• All other internal identifiers, like private type or macro names, should be namespaced only to avoid possible internal conflicts. E.g. vs. .\n• Each library has its own prefix for public symbols, in addition to the commonly used ( for libavformat, for libavcodec, for libswresample, etc). Check the existing code and choose names accordingly.\n• Other public identifiers (struct, union, enum, macro, type names) must use their library’s public prefix ( , , or ).\n\nFurthermore, name space reserved for the system should not be invaded. Identifiers ending in are reserved by POSIX. Also avoid names starting with or followed by an uppercase letter as they are reserved by the C standard. Names starting with are reserved at the file level and may not be used for externally visible symbols. If in doubt, just avoid names starting with altogether.\n• Casts should be used only when necessary. Unneeded parentheses should also be avoided if they don’t make the code easier to understand.\n\nThe code must be valid. It must not crash, abort, access invalid pointers, leak memory, cause data races or signed integer overflow, or otherwise cause undefined behaviour. Error codes should be checked and, when applicable, forwarded to the caller.\n\nOur libraries may be called by multiple independent callers in the same process. These calls may happen from any number of threads and the different call sites may not be aware of each other - e.g. a user program may be calling our libraries directly, and use one or more libraries that also call our libraries. The code must behave correctly under such conditions.\n\nThe code must treat as untrusted any bytestream received from a caller or read from a file, network, etc. It must not misbehave when arbitrary data is sent to it - typically it should print an error message and return on encountering invalid input data.\n\nThe code must use the family of functions from to perform all memory allocation, except in special cases (e.g. when interacting with an external library that requires a specific allocator to be used).\n\nAll allocations should be checked and returned on failure. A common mistake is that error paths leak memory - make sure that does not happen.\n\nOur libraries must not access the stdio streams stdin/stdout/stderr directly (e.g. via family of functions), as that is not library-safe. For logging, use .\n\nLicenses for patches must be compatible with FFmpeg.\n\nContributions should be licensed under the LGPL 2.1, including an \"or any later version\" clause, or, if you prefer a gift-style license, the ISC or MIT license. GPL 2 including an \"or any later version\" clause is also acceptable, but LGPL is preferred. If you add a new file, give it a proper license header. Do not copy and paste it from a random place, use an existing file as template.\n\nYou must not commit code which breaks FFmpeg!\n\nThis means unfinished code which is enabled and breaks compilation, or compiles but does not work/breaks the regression tests. Code which is unfinished but disabled may be permitted under-circumstances, like missing samples or an implementation with a small subset of features. Always check the mailing list for any reviewers with issues and test FATE before you push.\n\nCommit messages are highly important tools for informing other developers on what a given change does and why. Every commit must always have a properly filled out commit message with the following format:\n\nIf the commit addresses a known bug on our bug tracker or other external issue (e.g. CVE), the commit message should include the relevant bug ID(s) or other external identifiers. Note that this should be done in addition to a proper explanation and not instead of it. Comments such as \"fixed!\" or \"Changed it.\" are not acceptable.\n\nWhen applying patches that have been discussed at length on the mailing list, reference the thread in the commit message.\n\nTesting must be adequate but not excessive.\n\nIf it works for you, others, and passes FATE then it should be OK to commit it, provided it fits the other committing criteria. You should not worry about over-testing things. If your code has problems (portability, triggers compiler bugs, unusual environment etc) they will be reported and eventually fixed.\n\nDo not commit unrelated changes together.\n\nThey should be split them into self-contained pieces. Also do not forget that if part B depends on part A, but A does not depend on B, then A can and should be committed first and separate from B. Keeping changes well split into self-contained parts makes reviewing and understanding them on the commit log mailing list easier. This also helps in case of debugging later on. Also if you have doubts about splitting or not splitting, do not hesitate to ask/discuss it on the developer mailing list.\n\nCosmetic changes should be kept in separate patches.\n\nWe refuse source indentation and other cosmetic changes if they are mixed with functional changes, such commits will be rejected and removed. Every developer has his own indentation style, you should not change it. Of course if you (re)write something, you can use your own style, even though we would prefer if the indentation throughout FFmpeg was consistent (Many projects force a given indentation style - we do not.). If you really need to make indentation changes (try to avoid this), separate them strictly from real changes.\n\nNOTE: If you had to put if(){ .. } over a large (> 5 lines) chunk of code, then either do NOT change the indentation of the inner part within (do not move it to the right)! or do so in a separate commit\n\nCredit the author of the patch.\n\nMake sure the author of the commit is set correctly. (see git commit –author) If you apply a patch, send an answer to ffmpeg-devel (or wherever you got the patch from) saying that you applied the patch.\n\nIf a commit/patch fixes an issues found by some researcher, always credit the researcher in the commit message for finding/reporting the issue.\n\nAlways wait long enough before pushing changes\n\nDo NOT commit to code actively maintained by others without permission. Send a patch to ffmpeg-devel. If no one answers within a reasonable time-frame (12h for build failures and security fixes, 3 days small changes, 1 week for big patches) then commit your patch if you think it is OK. Also note, the maintainer can simply ask for more time to review!\n\nWarnings for correct code may be disabled if there is no other option.\n\nCompiler warnings indicate potential bugs or code with bad style. If a type of warning always points to correct and clean code, that warning should be disabled, not the code changed. Thus the remaining warnings can either be bugs or correct code. If it is a bug, the bug has to be fixed. If it is not, the code should be changed to not generate a warning unless that causes a slowdown or obfuscates the code.\n\nEvery library in FFmpeg provides a set of public APIs in its installed headers, which are those listed in the variable in that library’s . All identifiers defined in those headers (except for those explicitly documented otherwise), and corresponding symbols exported from compiled shared or static libraries are considered public interfaces and must comply with the API and ABI compatibility rules described in this section.\n\nPublic APIs must be backward compatible within a given major version. I.e. any valid user code that compiles and works with a given library version must still compile and work with any later version, as long as the major version number is unchanged. \"Valid user code\" here means code that is calling our APIs in a documented and/or intended manner and is not relying on any undefined behavior. Incrementing the major version may break backward compatibility, but only to the extent described in Major version bumps.\n\nWe also guarantee backward ABI compatibility for shared and static libraries. I.e. it should be possible to replace a shared or static build of our library with a build of any later version (re-linking the user binary in the static case) without breaking any valid user binaries, as long as the major version number remains unchanged.\n\nAny new public identifiers in installed headers are considered new API - this includes new functions, structs, macros, enum values, typedefs, new fields in existing structs, new installed headers, etc. Consider the following guidelines when adding new APIs.\n\nWhile new APIs can be added relatively easily, changing or removing them is much harder due to abovementioned compatibility requirements. You should then consider carefully whether the functionality you are adding really needs to be exposed to our callers as new public API.\n\nYour new API should have at least one well-established use case outside of the library that cannot be easily achieved with existing APIs. Every library in FFmpeg also has a defined scope - your new API must fit within it.\n\nIf your new API is replacing an existing one, it should be strictly superior to it, so that the advantages of using the new API outweight the cost to the callers of changing their code. After adding the new API you should then deprecate the old one and schedule it for removal, as described in Removing interfaces.\n\nIf you deem an existing API deficient and want to fix it, the preferred approach in most cases is to add a differently-named replacement and deprecate the existing API rather than modify it. It is important to make the changes visible to our callers (e.g. through compile- or run-time deprecation warnings) and make it clear how to transition to the new API (e.g. in the Doxygen documentation or on the wiki).\n\nThe FFmpeg libraries are used by a variety of callers to perform a wide range of multimedia-related processing tasks. You should therefore - within reason - try to design your new API for the broadest feasible set of use cases and avoid unnecessarily limiting it to a specific type of callers (e.g. just media playback or just transcoding).\n\nCheck whether similar APIs already exist in FFmpeg. If they do, try to model your new addition on them to achieve better overall consistency.\n\nThe naming of your new identifiers should follow the Naming conventions and be aligned with other similar APIs, if applicable.\n\nYou should also consider how your API might be extended in the future in a backward-compatible way. If you are adding a new struct , the standard approach is requiring the caller to always allocate it through a constructor function, typically named . This way new fields may be added to the end of the struct without breaking ABI compatibility. Typically you will also want a destructor - that frees the indirectly supplied object (and its contents, if applicable) and writes to the supplied pointer, thus eliminating the potential dangling pointer in the caller’s memory.\n\nIf you are adding new functions, consider whether it might be desirable to tweak their behavior in the future - you may want to add a flags argument, even though it would be unused initially.\n\nAll new APIs must be documented as Doxygen-formatted comments above the identifiers you add to the public headers. You should also briefly mention the change in .\n\nBackward-incompatible API or ABI changes require incrementing (bumping) the major version number, as described in Major version bumps. Major bumps are significant events that happen on a schedule - so if your change strictly requires one you should add it under preprocesor guards that disable it until the next major bump happens.\n\nNew APIs that can be added without breaking API or ABI compatibility require bumping the minor version number.\n\nIncrementing the third (micro) version component means a noteworthy binary compatible change (e.g. encoder bug fix that matters for the decoder). The third component always starts at 100 to distinguish FFmpeg from Libav.\n\nDue to abovementioned compatibility guarantees, removing APIs is an involved process that should only be undertaken with good reason. Typically a deficient, restrictive, or otherwise inadequate API is replaced by a superior one, though it does at times happen that we remove an API without any replacement (e.g. when the feature it provides is deemed not worth the maintenance effort, out of scope of the project, fundamentally flawed, etc.).\n\nThe removal has two steps - first the API is deprecated and scheduled for removal, but remains present and functional. The second step is actually removing the API - this is described in Major version bumps.\n\nTo deprecate an API you should signal to our users that they should stop using it. E.g. if you intend to remove struct members or functions, you should mark them with . When this cannot be done, it may be possible to detect the use of the deprecated API at runtime and print a warning (though take care not to print it too often). You should also document the deprecation (and the replacement, if applicable) in the relevant Doxygen documentation block.\n\nFinally, you should define a deprecation guard along the lines of (where XX is the major version in which the API will be removed) in ( in case of ). Then wrap all uses of the deprecated API in , so that the code will automatically get disabled once the major version reaches XX. You can also use and to suppress compiler deprecation warnings inside these guards. You should test that the code compiles and works with the guard macro evaluating to both true and false.\n\nA major version bump signifies an API and/or ABI compatibility break. To reduce the negative effects on our callers, who are required to adapt their code, backward-incompatible changes during a major bump should be limited to:\n• Performing ABI- but not API-breaking changes, like reordering struct contents.\n\nIt is important to be subscribed to the ffmpeg-devel mailing list. Almost any non-trivial patch is to be sent there for review. Other developers may have comments about your contribution. We expect you see those comments, and to improve it if requested. (N.B. Experienced committers have other channels, and may sometimes skip review for trivial fixes.) Also, discussion here about bug fixes and FFmpeg improvements by other developers may be helpful information for you. Finally, by being a list subscriber, your contribution will be posted immediately to the list, without the moderation hold which messages from non-subscribers experience.\n\nHowever, it is more important to the project that we receive your patch than that you be subscribed to the ffmpeg-devel list. If you have a patch, and don’t want to subscribe and discuss the patch, then please do send it to the list anyway.\n\nDiffs of all commits are sent to the ffmpeg-cvslog mailing list. Some developers read this list to review all code base changes from all sources. Subscribing to this list is not mandatory.\n\nKeep the documentation up to date.\n\nUpdate the documentation if you change behavior or add features. If you are unsure how best to do this, send a patch to ffmpeg-devel, the documentation maintainer(s) will review and commit your stuff.\n\nImportant discussions should be accessible to all.\n\nTry to keep important discussions and requests (also) on the public developer mailing list, so that all developers can benefit from them.\n\nMake sure that no parts of the codebase that you maintain are missing from the file. If something that you want to maintain is missing add it with your name after it. If at some point you no longer want to maintain some code, then please help in finding a new maintainer and also don’t forget to update the file.\n\nWe think our rules are not too hard. If you have comments, contact us.\n\nFirst, read the Coding Rules above if you did not yet, in particular the rules regarding patch submission.\n\nWhen you submit your patch, please use or . We cannot read other diffs :-).\n\nAlso please do not submit a patch which contains several unrelated changes. Split it into separate, self-contained pieces. This does not mean splitting file by file. Instead, make the patch as small as possible while still keeping it as a logical unit that contains an individual change, even if it spans multiple files. This makes reviewing your patches much easier for us and greatly increases your chances of getting your patch applied.\n\nUse the patcheck tool of FFmpeg to check your patch. The tool is located in the tools directory.\n\nRun the Regression tests before submitting a patch in order to verify it does not cause unexpected problems.\n\nIt also helps quite a bit if you tell us what the patch does (for example ’replaces lrint by lrintf’), and why (for example ’*BSD isn’t C99 compliant and has no lrint()’)\n\nAlso please if you send several patches, send each patch as a separate mail, do not attach several unrelated patches to the same mail.\n\nPatches should be posted to the ffmpeg-devel mailing list. Use when possible since it will properly send patches without requiring extra care. If you cannot, then send patches as base64-encoded attachments, so your patch is not trashed during transmission. Also ensure the correct mime type is used (text/x-diff or text/x-patch or at least text/plain) and that only one patch is inline or attached per mail. You can check https://patchwork.ffmpeg.org, if your patch does not show up, its mime type likely was wrong.\n\nPlease see https://git-send-email.io/. For gmail additionally see https://shallowsky.com/blog/tech/email/gmail-app-passwds.html.\n\nUsing might not be desirable for everyone. The following trick allows to send patches via email clients in a safe way. It has been tested with Outlook and Thunderbird (with X-Unsent extension) and might work with other applications.\n\nCreate your patch like this:\n\nNow you’ll just need to open the eml file with the email application and execute ’Send’.\n\nYour patch will be reviewed on the mailing list. You will likely be asked to make some changes and are expected to send in an improved version that incorporates the requests from the review. This process may go through several iterations. Once your patch is deemed good enough, some developer will pick it up and commit it to the official FFmpeg tree.\n\nGive us a few days to react. But if some time passes without reaction, send a reminder by email. Your patch should eventually be dealt with.\n• Did you use av_cold for codec initialization and close functions?\n• Did you add a long_name under NULL_IF_CONFIG_SMALL to the AVCodec or AVInputFormat/AVOutputFormat struct?\n• Did you bump the minor version number (and reset the micro version number) in or ?\n• Did you register it in or ?\n• Did you add the AVCodecID to ? When adding new codec IDs, also add an entry to the codec descriptor list in .\n• If it has a FourCC, did you add it to , even if it is only a decoder?\n• Did you add a rule to compile the appropriate files in the Makefile? Remember to do this even if you’re just adding a format to a file that is already being compiled by some other rule, like a raw demuxer.\n• Did you add an entry to the table of supported formats or codecs in ?\n• Did you add an entry in the Changelog?\n• If it depends on a parser or a library, did you add that dependency in configure?\n• Did you the appropriate files before committing?\n• Did you make sure it compiles standalone, i.e. with (or or whatever your component is)?\n• Does pass with the patch applied?\n• Was the patch generated with git format-patch or send-email?\n• Did you sign-off your patch? ( ) See Sign your work for the meaning of sign-off.\n• Is the patch against latest FFmpeg git master branch?\n• Are you subscribed to ffmpeg-devel? (the list is subscribers only due to spam)\n• Have you checked that the changes are minimal, so that the same cannot be achieved with a smaller patch and/or simpler final code?\n• If the change is to speed critical code, did you benchmark it?\n• If you did any benchmarks, did you provide them in the mail?\n• Have you checked that the patch does not introduce buffer overflows or other security issues?\n• Did you test your decoder or demuxer against damaged data? If no, see tools/trasher, the noise bitstream filter, and zzuf. Your decoder or demuxer should not crash, end in a (near) infinite loop, or allocate ridiculous amounts of memory when fed damaged data.\n• Did you test your decoder or demuxer against sample files? Samples may be obtained at https://samples.ffmpeg.org.\n• Does the patch not mix functional and cosmetic changes?\n• Did you add tabs or trailing whitespace to the code? Both are forbidden.\n• Is the patch attached to the email you send?\n• Is the mime type of the patch correct? It should be text/x-diff or text/x-patch or at least text/plain and not application/octet-stream.\n• If the patch fixes a bug, did you provide a verbose analysis of the bug?\n• If the patch fixes a bug, did you provide enough information, including a sample, so the bug can be reproduced and the fix can be verified? Note please do not attach samples >100k to mails but rather provide a URL, you can upload to https://streams.videolan.org/upload/.\n• Did you provide a verbose summary about what the patch does change?\n• Did you provide a verbose explanation why it changes things like it does?\n• Did you provide a verbose summary of the user visible advantages and disadvantages if the patch is applied?\n• Did you provide an example so we can verify the new feature added by the patch easily?\n• If you added a new file, did you insert a license header? It should be taken from FFmpeg, not randomly copied and pasted from somewhere else.\n• You should maintain alphabetical order in alphabetically ordered lists as long as doing so does not break API/ABI compatibility.\n• Lines with similar content should be aligned vertically when doing so improves readability.\n• Consider adding a regression test for your code. All new modules should be covered by tests. That includes demuxers, muxers, decoders, encoders filters, bitstream filters, parsers. If its not possible to do that, add an explanation why to your patchset, its ok to not test if theres a reason.\n• If you added NASM code please check that things still work with –disable-x86asm.\n• Test your code with valgrind and or Address Sanitizer to ensure it’s free of leaks, out of array accesses, etc.\n\nAll patches posted to ffmpeg-devel will be reviewed, unless they contain a clear note that the patch is not for the git master branch. Reviews and comments will be posted as replies to the patch on the mailing list. The patch submitter then has to take care of every comment, that can be by resubmitting a changed patch or by discussion. Resubmitted patches will themselves be reviewed like any other patch. If at some point a patch passes review with no comments then it is approved, that can for simple and small patches happen immediately while large patches will generally have to be changed and reviewed many times before they are approved. After a patch is approved it will be committed to the repository.\n\nWe will review all submitted patches, but sometimes we are quite busy so especially for large patches this can take several weeks.\n\nIf you feel that the review process is too slow and you are willing to try to take over maintainership of the area of code you change then just clone git master and maintain the area of code there. We will merge each area from where its best maintained.\n\nWhen resubmitting patches, please do not make any significant changes not related to the comments received during review. Such patches will be rejected. Instead, submit significant changes or new features as separate patches.\n\nEveryone is welcome to review patches. Also if you are waiting for your patch to be reviewed, please consider helping to review other patches, that is a great way to get everyone’s patches reviewed sooner.\n\nBefore submitting a patch (or committing to the repository), you should at least test that you did not break anything.\n\nRunning ’make fate’ accomplishes this, please see fate.html for details.\n\n[Of course, some patches may change the results of the regression tests. In this case, the reference results of the regression tests shall be modified accordingly].\n\nIf you need a sample uploaded send a mail to samples-request.\n\nWhen there is no muxer or encoder available to generate test media for a specific test then the media has to be included in the fate-suite. First please make sure that the sample file is as small as possible to test the respective decoder or demuxer sufficiently. Large files increase network bandwidth and disk space requirements. Once you have a working fate test and fate sample, provide in the commit message or introductory message for the patch series that you post to the ffmpeg-devel mailing list, a direct link to download the sample media.\n\nThe FFmpeg build system allows visualizing the test coverage in an easy manner with the coverage tools / . This involves the following steps:\n• Run your test case, either manually or via FATE. This can be either the full FATE regression suite, or any arbitrary invocation of any front-end tool provided by FFmpeg, in any combination.\n\nYou can use the command to reset the coverage measurements. You will need to rerun after running a new test.\n\nThe configure script provides a shortcut for using valgrind to spot bugs related to memory handling. Just add the option or to your configure line, and reasonable defaults will be set for running FATE under the supervision of either the memcheck or the massif tool of the valgrind suite.\n\nIn case you need finer control over how valgrind is invoked, use the option in your configure line instead.\n\nThe developers maintaining each part of the codebase are listed in . Being listed in , gives one the right to have git write access to the specific repository.\n\nPeople add themselves to by sending a patch like any other code change. These get reviewed by the community like any other patch. It is expected that, if someone has an objection to a new maintainer, she is willing to object in public with her full name and is willing to take over maintainership for the area.\n\nFFmpeg maintains a set of release branches, which are the recommended deliverable for system integrators and distributors (such as Linux distributions, etc.). At regular times, a release manager prepares, tests and publishes tarballs on the https://ffmpeg.org website.\n\nThere are two kinds of releases:\n• Major releases always include the latest and greatest features and functionality.\n• Point releases are cut from release branches, which are named , with being the release version number.\n\nNote that we promise to our users that shared libraries from any FFmpeg release never break programs that have been compiled against previous versions of the same release series in any case!\n\nHowever, from time to time, we do make API changes that require adaptations in applications. Such changes are only allowed in (new) major releases and require further steps such as bumping library version numbers and/or adjustments to the symbol versioning file. Please discuss such changes on the ffmpeg-devel mailing list in time to allow forward planning.\n\nChanges that match the following criteria are valid candidates for inclusion into a point release:\n• Retains both source code and binary compatibility with previous point releases of the same release branch.\n\nThe order for checking the rules is (1 OR 2 OR 3) AND 4.\n\nThe release process involves the following steps:\n• Ensure that the file contains the version number for the upcoming release.\n• Announce the intent to do a release to the mailing list.\n• Make sure all relevant security fixes have been backported. See https://ffmpeg.org/security.html.\n• Ensure that the FATE regression suite still passes in the release branch on at least i386 and amd64 (cf. Regression tests).\n• Prepare the release tarballs in and formats, and supplementing files that contain signatures\n• Publish the tarballs at https://ffmpeg.org/releases. Create and push an annotated tag in the form , with containing the version number.\n• Propose and send a patch to the ffmpeg-devel mailing list with a news entry for the website.\n• Send an announcement to the mailing list.\n\nThis document was generated on March 23, 2025 using makeinfo."
    }
]