[
    {
        "link": "https://pytorch.org/tutorials",
        "document": ""
    },
    {
        "link": "https://discuss.pytorch.org/t/transformer-positional-encoding-class/159168",
        "document": "I’m learning a transformer implementation through this Kaggle tutorial Transformer from scratch using pytorch | Kaggle . I don’t understand several of the lines of code in the PositionalEmbedding class:\n• What is unsqueeze() doing? I’m guessing it adds a batch dimensions but I’m not sure…\n• Why are we using “self.register_buffer(‘pe’, pe)” ? What is this doing? Why can’t we just do “self.pe = pe”?\n• I don’t understand these 2 lines of code at all:"
    },
    {
        "link": "https://stackoverflow.com/questions/77444485/using-positional-encoding-in-pytorch",
        "document": "There isn't, as far as I'm aware.\n\nHowever, you can use an implementation from PyTorch's documentation:\n\nYou can find it here:"
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html",
        "document": "User is able to modify the attributes as needed. The architecture is based on the paper “Attention Is All You Need”. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010.\n• None d_model (int) – the number of expected features in the encoder/decoder inputs (default=512).\n• None nhead (int) – the number of heads in the multiheadattention models (default=8).\n• None num_encoder_layers (int) – the number of sub-encoder-layers in the encoder (default=6).\n• None num_decoder_layers (int) – the number of sub-decoder-layers in the decoder (default=6).\n• None dim_feedforward (int) – the dimension of the feedforward network model (default=2048).\n• None activation (Union[str, Callable[[Tensor], Tensor]]) – the activation function of encoder/decoder intermediate layer, can be a string (“relu” or “gelu”) or a unary callable. Default: relu\n• None layer_norm_eps (float) – the eps value in layer normalization components (default=1e-5).\n• None batch_first (bool) – If , then the input and output tensors are provided as (batch, seq, feature). Default: (seq, batch, feature).\n• None norm_first (bool) – if , encoder and decoder layers will perform LayerNorms before other attention and feedforward operations, otherwise after. Default: (after).\n• None bias (bool) – If set to , and layers will not learn an additive bias. Default: .\n\nNote: A full example to apply nn.Transformer module for the word language model is available in https://github.com/pytorch/examples/tree/master/word_language_model\n\nTake in and process masked source/target sequences. If a boolean tensor is provided for any of the [src/tgt/memory]_mask arguments, positions with a value are not allowed to participate in the attention, which is the opposite of the definition for in .\n• None src (Tensor) – the sequence to the encoder (required).\n• None tgt (Tensor) – the sequence to the decoder (required).\n• None src_mask (Optional[Tensor]) – the additive mask for the src sequence (optional).\n• None tgt_mask (Optional[Tensor]) – the additive mask for the tgt sequence (optional).\n• None memory_mask (Optional[Tensor]) – the additive mask for the encoder output (optional).\n• None src_key_padding_mask (Optional[Tensor]) – the Tensor mask for src keys per batch (optional).\n• None tgt_key_padding_mask (Optional[Tensor]) – the Tensor mask for tgt keys per batch (optional).\n• None memory_key_padding_mask (Optional[Tensor]) – the Tensor mask for memory keys per batch (optional).\n• None src_is_causal (Optional[bool]) – If specified, applies a causal mask as . Default: ; try to detect a causal mask. Warning: provides a hint that is the causal mask. Providing incorrect hints can result in incorrect execution, including forward and backward compatibility.\n• None tgt_is_causal (Optional[bool]) – If specified, applies a causal mask as . Default: ; try to detect a causal mask. Warning: provides a hint that is the causal mask. Providing incorrect hints can result in incorrect execution, including forward and backward compatibility.\n• None memory_is_causal (bool) – If specified, applies a causal mask as . Default: . Warning: provides a hint that is the causal mask. Providing incorrect hints can result in incorrect execution, including forward and backward compatibility.\n• None src: (S,E) for unbatched input, (S,N,E) if or if .\n• None tgt: (T,E) for unbatched input, (T,N,E) if or if . Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a BoolTensor is provided, positions with are not allowed to attend while values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a BoolTensor is provided, the positions with the value of will be ignored while the position with the value of will be unchanged.\n• None output: (T,E) for unbatched input, (T,N,E) if or if . Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decoder. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number"
    },
    {
        "link": "https://github.com/hyunwoongko/transformer",
        "document": "This code was written in 2019, and I was not very familiar with transformer model in that time. So don't trust this code too much. Currently I am not managing this code well, so please open pull requests if you find bugs in the code and want to fix.\n\nMy own implementation Transformer model (Attention is All You Need - Google Brain, 2017) \n\n\n\n \n\n\n\n\n\n( . ): ( , , , ): ( , ). () # same size with input matrix (for adding with input matrix) . . ( , , ) . . # we don't need to compute gradient . ( , , ) . (). ( ) . ( , , , ). () # \"step=2\" means 'i' multiplied with two (same with 2 * i) . [:, :: ] . ( ( ( ))) . [:, :: ] . ( ( ( ))) # compute positional encoding to consider positional information of words ( , ): , . () . [: , :] # it will add with tok_emb : [128, 30, 512]\n\n( . ): Query : given sentence that we focused on (decoder) Key : every sentence to check relationship with Qeury(encoder) Value : every sentence same with Key (encoder) ( ): ( , ). () . . ( ) ( , , , , , ): , , , . () . ( , ) ( @ ) . ( ) : . ( , ) . ( ) @ ,\n\nI use Multi30K Dataset to train and evaluate model \n\n You can check detail of dataset here \n\n I follow original paper's parameter settings. (below)\n• Attention is All You Need, 2017 - Google"
    },
    {
        "link": "https://pytorch.org/tutorials",
        "document": ""
    },
    {
        "link": "https://discuss.pytorch.org/t/how-to-modify-the-positional-encoding-in-torch-nn-transformer/104308",
        "document": "I am doing some experiments on positional encoding, and would like to use for my experiments.\n\nBut it seems there is no argument for me to change the positional encoding. I also cannot seem to find in the source code where the is handling tthe positional encoding.\n\nHow to change the default sin cos encoding to some of my custom-made encoding?"
    },
    {
        "link": "https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1",
        "document": "I have recently been getting more involved in the world of machine learning. When I’ve had a problem understanding a complex issue or coding a neural network, the internet has seemed to have all the answers: from a simple linear regression to complex convolutional networks. At least that is what I thought…\n\nOnce I began getting better at this Deep Learning thing, I stumbled upon the all-glorious transformer. The original paper: \"Attention is all you need\", proposed an innovative way to construct neural networks. No more convolutions! The paper proposes an encoder-decoder neural network made up of repeated encoder and decoder blocks. The structure is the following:\n\nThe left block is the encoder, and the right block is the decoder. If you don’t understand the parts of this model yet, I highly recommend going over Harvard’s \"The Annotated Transformer\" guide where they code the transformer model in PyTorch from scratch. I will not be covering important concepts like \"multi-head attention\" or \"feed-forward layers\" in this tutorial, so you should know them before you continue reading. If you have already taken a look at the code from scratch, you are probably wondering if you are going to have to copy-paste that code all over the place for every project you make. Thankfully, no. Modern python libraries like PyTorch and Tensorflow already include easily accessible transformer models through an import. However, there is more to it than just importing the model and plugging it in. Today I will explain how to use and tune PyTorch nn.Transformer() module. I personally struggled trying to find information about how to implement, train, and infer from it, so I decided to create my own guide for all of you.\n\nTo start, we need to import PyTorch and some other libraries we are going to be using:\n\nNow, let’s take a closer look at the transformer module. I recommend starting by reading over PyTorch’s documentation about it. As they explain, there are no mandatory parameters. The module comes with the \"Attention is all you need\" model hyperparameters. To use it, let’s begin by creating a simple PyTorch model. I will only change some of the default parameters so our model doesn’t take unnecessarily long to train. I made those parameters part of our class:\n\nThe transformer blocks don’t care about the order of the input sequence. This, of course, is a problem. Saying \"I ate a pizza with pineapple\" is not the same as saying \"a pineapple ate I with pizza\". Thankfully, we have a solution: positional encoding. This is a way to \"give importance\" to elements depending on their position. A detailed explanation of how it works can be found here, but a quick explanation is that we create a vector for each element representing its position with regard to every other element in the sequence. Positional encoding follows this very complicated-looking formula which, in practice, we won’t really need to understand:\n\nFor the sake of organization and reusability, let’s create a separate class for the positional encoding layer (it looks hard but it is really just the formula, dropout, and a residual connection):\n\nNow that we have the only layer not included in PyTorch, we are ready to finish our model. Before adding the positional encoding, we need an embedding layer so that each element in our sequences is converted into a vector we can manipulate (instead of a fixed integer). We will also need a final linear layer so that we can convert the model’s output into the dimensions of our desired output. The final model should look something like this:\n\nI know… It looks very intimidating, but if you understand what each part does, it is actually a pretty simple model to implement.\n\nYou may recall there was a special block in the model structure called \"masked multi-head attention\":\n\nSo… what is masking? Before I can explain it to you, let’s quickly recapitulate what is going on with our tensors when we feed them into our model. First, we embed and encode (positional encoding) our source tensor. Then, our source tensor is encoded into an unintelligible encoded tensor that we feed into our decoder with our embedded and encoded (positionally) target vector. For our model to learn, we can’t just show it the whole target tensor! This would just give him the answer straight up.\n\nThe solution to this is a masking tensor. This tensor is made up of size (sequence length x sequence length) since for every element in the sequence, we show the model one more element. This matrix will be added to our target vector, so the matrix will be made up of zeros in the positions where the transformer can have access to the elements, and minus infinity where it can’t. An illustrated explanation might help you a bit more:\n\nIn case you didn’t know, tensors are matrices that can be stored in a GPU, and since they are matrices, all dimensions must have elements of the same size. Of course, this won’t happen when treating with tasks like NLP or different-sized images. Therefore, we use the so-called \"special tokens\". These tokens allow our model to know where the start of the sentence is (), where the end of the sentence is () and what elements are just there to fill up the remaining space so that our matrices have the sam sequence size (). These tokens must also be converted into their corresponding integer id (In our example they will be 2, 3, and 4 respectively). Padding a sequence looks something like this:\n\nTo tell our model that these tokens should be irrelevant, we use a binary matrix where there is a True value on the positions where the padding token is and False where it isn’t:\n\nTo create the two masking matrices we talked about, we need to extend our transformer model. If you know a bit of NumPy, you will have no problem understanding what these methods do. If you can’t understand it, I recommend opening a Jupyter notebook and going step by step to understand what they do.\n\nThe full extended model looks like this (note the change in the forward method as well):\n\nFor the sake of this project, I am going to create a set of fake data we can use to train our model. This data will be made up of sequences like:\n\nFeel free to skip to the next section if you aren’t interested in the data creation part.\n\nI won’t bother explaining what these functions do since they are pretty easy to understand with basic NumPy knowledge. I’ll create all the sentences of size 8 so I don’t need padding, and I’ll organize them randomly into batches of size 16:\n\nNow that we have data to work with, we can get to training our model. Let’s begin by creating an instance of our model, loss function, and optimizer. We will use the Stochastic Gradient Descent optimizer, the Cross-Entropy Loss function, and a learning rate of 0.01. I will also use my graphics card for this training since it will take less time, but it is not necessary.\n\nAn important concept we need to understand before continuing is that the target tensor we give as an input to the transformer must be shifted by one to the right (compared to the target output tensor). In other words, the tensor we want to give the model for training must have one extra element at the beginning and one less element at the end, and the tensor we compute the loss function with must be shifted in the other direction. This is so that if we give the model an element during inference, it gives us the next one.\n\nNow that we have grasped this concept, let’s get to coding! The training loop is a standard training loop except:\n• The target tensor is passed to the model during the prediction\n• A target mask is generated to hide the next words\n• A padding mask might be generated and passed to the model as well\n\nThe validation loop is exactly the same as our training loop except we don’t read or update gradients:\n\nIn this example, I am training the model for 10 epochs. To simplify the training I created a fit function that calls the train and validation loop every epoch and prints the loss:\n\nThis produces the following out\n\nAfter training we obtain the following losses per epoch:\n\nAs we can see, our model seems to have learned something. It is time to check if it has, but… how do we check it? We don’t have target tensors for data we have never seen. Here is where shifting our input target and output target tensor has an effect. As we saw before, our model learned to predict the next token when given an element. Therefore, we should be able to give our model the input tensor and the start token, and it should give us back the next element. If when the model predicts a token, we concatenate it with our previous input, we should slowly be able to add words to our output until our model predicts the token.\n\nHere is the code for that process:\n\nThe output of running this code is:\n\nSo the model has indeed gotten the gist of our sequences, but it still makes some mistakes when trying to predict the continuation. For example, in \"Example 4\", the model should predict a 1 as the first token, since the ending of the input is a 0. We can also see how during inference our sentences don’t need to have the same length, and the outputs will also not have the same length (see \"Example 5\").\n\nI believe this article can help a lot of beginner/intermediate Machine Learning developers learn how to work with transformer models in PyTorch, and, since the structure is the same in other languages, this tutorial is probably also useful for other frameworks like Tensorflow (hopefully).\n\nIf you have any suggestions or find any bugs feel free to leave a comment and I will fix it ASAP."
    },
    {
        "link": "https://datacamp.com/tutorial/building-a-transformer-with-py-torch",
        "document": "First introduced in the paper Attention is All You Need by Vaswani et al., Transformers have since become a cornerstone of many NLP tasks due to their unique design and effectiveness.\n\nAt the heart of Transformers is the attention mechanism, specifically the concept of 'self-attention,' which allows the model to weigh and prioritize different parts of the input data. This mechanism is what enables Transformers to manage long-range dependencies in data. It is fundamentally a weighting scheme that allows a model to focus on different parts of the input when producing an output.\n\nThis mechanism allows the model to consider different words or features in the input sequence, assigning each one a 'weight' that signifies its importance for producing a given output.\n\nFor instance, in a sentence translation task, while translating a particular word, the model might assign higher attention weights to words that are grammatically or semantically related to the target word. This process allows the Transformer to capture dependencies between words or features, regardless of their distance from each other in the sequence.\n\nTransformers' impact in the field of NLP cannot be overstated. They have outperformed traditional models in many tasks, demonstrating superior capacity to comprehend and generate human language in a more nuanced way.\n\nFor a deeper understanding of NLP, DataCamp's Introduction to Natural Language Processing in Python course is a recommended resource.\n\nBefore diving into building a Transformer, it is essential to set up the working environment correctly. First and foremost, PyTorch needs to be installed. PyTorch (current stable version - 2.0.1) can be easily installed through pip or conda package managers.\n\nFor pip, use the command:\n\nFor conda, use the command:\n\nFor using pytorch with a cpu kindly visit the pytorch documentation.\n\nAdditionally, it is beneficial to have a basic understanding of deep learning concepts, as these will be fundamental to understanding the operation of Transformers. For those who need a refresher, the DataCamp course Deep Learning in Python is a valuable resource that covers key concepts in deep learning.\n\nTo build the Transformer model the following steps are necessary:\n• Combining the Encoder and Decoder layers to create the complete Transformer network\n\n1. Importing the necessary libraries and modules\n\nWe’ll start with importing the PyTorch library for core functionality, the neural network module for creating neural networks, the optimization module for training networks, and the data utility functions for handling data. Additionally, we’ll import the standard Python math module for mathematical operations and the copy module for creating copies of complex objects.\n\nThese tools set the foundation for defining the model's architecture, managing data, and establishing the training process.\n\nThe Multi-Head Attention mechanism computes the attention between each pair of positions in a sequence. It consists of multiple “attention heads” that capture different aspects of the input sequence.\n\nTo know more about Multi-Head Attention, check out this Attention mechanisms section of the Large Language Models (LLMs) Concepts course.\n\nThe class is defined as a subclass of PyTorch's nn.Module.\n• num_heads: The number of attention heads to split the input into.\n\nThe initialization checks if d_model is divisible by num_heads, and then defines the transformation weights for query, key, value, and output.\n• Calculating Attention Scores: attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k). Here, the attention scores are calculated by taking the dot product of queries (Q) and keys (K), and then scaling by the square root of the key dimension (d_k).\n• Applying Mask: If a mask is provided, it is applied to the attention scores to mask out specific values.\n• Calculating Attention Weights: The attention scores are passed through a softmax function to convert them into probabilities that sum to 1.\n• Calculating Output: The final output of the attention is calculated by multiplying the attention weights by the values (V).\n\nThis method reshapes the input x into the shape (batch_size, num_heads, seq_length, d_k). It enables the model to process multiple attention heads concurrently, allowing for parallel computation.\n\nAfter applying attention to each head separately, this method combines the results back into a single tensor of shape (batch_size, seq_length, d_model). This prepares the result for further processing.\n\nThe forward method is where the actual computation happens:\n• Apply Linear Transformations: The queries (Q), keys (K), and values (V) are first passed through linear transformations using the weights defined in the initialization.\n• Split Heads: The transformed Q, K, V are split into multiple heads using the split_heads method.\n• Apply Scaled Dot-Product Attention: The scaled_dot_product_attention method is called on the split heads.\n• Combine Heads: The results from each head are combined back into a single tensor using the combine_heads method.\n• Apply Output Transformation: Finally, the combined tensor is passed through an output linear transformation.\n\nIn summary, the MultiHeadAttention class encapsulates the multi-head attention mechanism commonly used in transformer models. It takes care of splitting the input into multiple attention heads, applying attention to each head, and then combining the results. By doing so, the model can capture various relationships in the input data at different scales, improving the expressive ability of the model.\n\nThe class is a subclass of PyTorch's nn.Module, which means it will inherit all functionalities required to work with neural network layers.\n• d_model: Dimensionality of the model's input and output.\n• d_ff: Dimensionality of the inner layer in the feed-forward network.\n• self.fc1 and self.fc2: Two fully connected (linear) layers with input and output dimensions as defined by d_model and d_ff.\n• self.relu: ReLU (Rectified Linear Unit) activation function, which introduces non-linearity between the two linear layers.\n• x: The input to the feed-forward network.\n• self.fc1(x): The input is first passed through the first linear layer (fc1).\n• self.relu(...): The output of fc1 is then passed through a ReLU activation function. ReLU replaces all negative values with zeros, introducing non-linearity into the model.\n• self.fc2(...): The activated output is then passed through the second linear layer (fc2), producing the final output.\n\nIn summary, the PositionWiseFeedForward class defines a position-wise feed-forward neural network that consists of two linear layers with a ReLU activation function in between. In the context of transformer models, this feed-forward network is applied to each position separately and identically. It helps in transforming the features learned by the attention mechanisms within the transformer, acting as an additional processing step for the attention outputs.\n\nPositional Encoding is used to inject the position information of each token in the input sequence. It uses sine and cosine functions of different frequencies to generate the positional encoding.\n\nThe class is defined as a subclass of PyTorch's nn.Module, allowing it to be used as a standard PyTorch layer.\n• d_model: The dimension of the model's input.\n• max_seq_length: The maximum length of the sequence for which positional encodings are pre-computed.\n• pe: A tensor filled with zeros, which will be populated with positional encodings.\n• position: A tensor containing the position indices for each position in the sequence.\n• div_term: A term used to scale the position indices in a specific way.\n• The sine function is applied to the even indices and the cosine function to the odd indices of pe.\n• Finally, pe is registered as a buffer, which means it will be part of the module's state but will not be considered a trainable parameter.\n\nThe forward method simply adds the positional encodings to the input x.\n\nIt uses the first x.size(1) elements of pe to ensure that the positional encodings match the actual sequence length of x.\n\nThe PositionalEncoding class adds information about the position of tokens within the sequence. Since the transformer model lacks inherent knowledge of the order of tokens (due to its self-attention mechanism), this class helps the model to consider the position of tokens in the sequence. The sinusoidal functions used are chosen to allow the model to easily learn to attend to relative positions, as they produce a unique and smooth encoding for each position in the sequence.\n\nFigure 2. The Encoder part of the transformer network (Source: image from the original paper)\n\nThe class is defined as a subclass of PyTorch's nn.Module, which means it can be used as a building block for neural networks in PyTorch.\n• d_model: The dimensionality of the input.\n• num_heads: The number of attention heads in the multi-head attention.\n• d_ff: The dimensionality of the inner layer in the position-wise feed-forward network.\n• dropout: The dropout rate used for regularization.\n• self.norm1 and self.norm2: Layer normalization, applied to smooth the layer's input.\n• self.dropout: Dropout layer, used to prevent overfitting by randomly setting some activations to zero during training.\n• x: The input to the encoder layer.\n• mask: Optional mask to ignore certain parts of the input.\n• Self-Attention: The input x is passed through the multi-head self-attention mechanism.\n• Add & Normalize (after Attention): The attention output is added to the original input (residual connection), followed by dropout and normalization using norm1.\n• Feed-Forward Network: The output from the previous step is passed through the position-wise feed-forward network.\n• Add & Normalize (after Feed-Forward): Similar to step 2, the feed-forward output is added to the input of this stage (residual connection), followed by dropout and normalization using norm2.\n• Output: The processed tensor is returned as the output of the encoder layer.\n\nThe EncoderLayer class defines a single layer of the transformer's encoder. It encapsulates a multi-head self-attention mechanism followed by position-wise feed-forward neural network, with residual connections, layer normalization, and dropout applied as appropriate. These components together allow the encoder to capture complex relationships in the input data and transform them into a useful representation for downstream tasks. Typically, multiple such encoder layers are stacked to form the complete encoder part of a transformer model.\n• d_model: The dimensionality of the input.\n• num_heads: The number of attention heads in the multi-head attention.\n• d_ff: The dimensionality of the inner layer in the feed-forward network.\n• self.cross_attn: Multi-head attention mechanism that attends to the encoder's output.\n• x: The input to the decoder layer.\n• enc_output: The output from the corresponding encoder (used in the cross-attention step).\n• src_mask: Source mask to ignore certain parts of the encoder's output.\n• tgt_mask: Target mask to ignore certain parts of the decoder's input.\n• Self-Attention on Target Sequence: The input x is processed through a self-attention mechanism.\n• Add & Normalize (after Self-Attention): The output from self-attention is added to the original x, followed by dropout and normalization using norm1.\n• Cross-Attention with Encoder Output: The normalized output from the previous step is processed through a cross-attention mechanism that attends to the encoder's output enc_output.\n• Add & Normalize (after Cross-Attention): The output from cross-attention is added to the input of this stage, followed by dropout and normalization using norm2.\n• Feed-Forward Network: The output from the previous step is passed through the feed-forward network.\n• Add & Normalize (after Feed-Forward): The feed-forward output is added to the input of this stage, followed by dropout and normalization using norm3.\n• Output: The processed tensor is returned as the output of the decoder layer.\n\nThe DecoderLayer class defines a single layer of the transformer's decoder. It consists of a multi-head self-attention mechanism, a multi-head cross-attention mechanism (that attends to the encoder's output), a position-wise feed-forward neural network, and the corresponding residual connections, layer normalization, and dropout layers. This combination enables the decoder to generate meaningful outputs based on the encoder's representations, taking into account both the target sequence and the source sequence. As with the encoder, multiple decoder layers are typically stacked to form the complete decoder part of a transformer model.\n\nNext, the Encoder and Decoder blocks are brought together to construct the comprehensive Transformer model.\n\n5. Combining the Encoder and Decoder layers to create the complete Transformer network\n\nFigure 4. The Transformer Network (Source: Image from the original paper)\n\nThe constructor takes the following parameters:\n• d_model: The dimensionality of the model's embeddings.\n• num_heads: Number of attention heads in the multi-head attention mechanism.\n• num_layers: Number of layers for both the encoder and the decoder.\n• d_ff: Dimensionality of the inner layer in the feed-forward network.\n\nAnd it defines the following components:\n\nThis method is used to create masks for the source and target sequences, ensuring that padding tokens are ignored and that future tokens are not visible during training for the target sequence.\n\nThis method defines the forward pass for the Transformer, taking source and target sequences and producing the output predictions.\n• Input Embedding and Positional Encoding: The source and target sequences are first embedded using their respective embedding layers and then added to their positional encodings.\n• Encoder Layers: The source sequence is passed through the encoder layers, with the final encoder output representing the processed source sequence.\n• Decoder Layers: The target sequence and the encoder's output are passed through the decoder layers, resulting in the decoder's output.\n• Final Linear Layer: The decoder's output is mapped to the target vocabulary size using a fully connected (linear) layer.\n\nThe final output is a tensor representing the model's predictions for the target sequence.\n\nThe Transformer class brings together the various components of a Transformer model, including the embeddings, positional encoding, encoder layers, and decoder layers. It provides a convenient interface for training and inference, encapsulating the complexities of multi-head attention, feed-forward networks, and layer normalization.\n\nThis implementation follows the standard Transformer architecture, making it suitable for sequence-to-sequence tasks like machine translation, text summarization, etc. The inclusion of masking ensures that the model adheres to the causal dependencies within sequences, ignoring padding tokens and preventing information leakage from future tokens.\n\nThese sequential steps empower the Transformer model to efficiently process input sequences and produce corresponding output sequences.\n\nFor illustrative purposes, a dummy dataset will be crafted in this example. However, in a practical scenario, a more substantial dataset would be employed, and the process would involve text preprocessing along with the creation of vocabulary mappings for both the source and target languages.\n\nThese values define the architecture and behavior of the transformer model:\n• src_vocab_size, tgt_vocab_size: Vocabulary sizes for source and target sequences, both set to 5000.\n• d_model: Dimensionality of the model's embeddings, set to 512.\n• num_heads: Number of attention heads in the multi-head attention mechanism, set to 8.\n• num_layers: Number of layers for both the encoder and the decoder, set to 6.\n• d_ff: Dimensionality of the inner layer in the feed-forward network, set to 2048.\n\nThis line creates an instance of the Transformer class, initializing it with the given hyperparameters. The instance will have the architecture and behavior defined by these hyperparameters.\n\nThe following lines generate random source and target sequences:\n• src_data: Random integers between 1 and src_vocab_size, representing a batch of source sequences with shape (64, max_seq_length).\n• tgt_data: Random integers between 1 and tgt_vocab_size, representing a batch of target sequences with shape (64, max_seq_length).\n• These random sequences can be used as inputs to the transformer model, simulating a batch of data with 64 examples and sequences of length 100.\n\nThe code snippet demonstrates how to initialize a transformer model and generate random source and target sequences that can be fed into the model. The chosen hyperparameters determine the specific structure and properties of the transformer. This setup could be part of a larger script where the model is trained and evaluated on actual sequence-to-sequence tasks, such as machine translation or text summarization.\n\nNext, the model will be trained utilizing the aforementioned sample data. However, in a real-world scenario, a significantly larger dataset would be employed, which would typically be partitioned into distinct sets for training and validation purposes.\n• criterion = nn.CrossEntropyLoss(ignore_index=0): Defines the loss function as cross-entropy loss. The ignore_index argument is set to 0, meaning the loss will not consider targets with an index of 0 (typically reserved for padding tokens).\n• optimizer = optim.Adam(...): Defines the optimizer as Adam with a learning rate of 0.0001 and specific beta values.\n• transformer.train(): Sets the transformer model to training mode, enabling behaviors like dropout that only apply during training.\n\nThe code snippet trains the model for 100 epochs using a typical training loop:\n• for epoch in range(100): Iterates over 100 training epochs.\n• optimizer.zero_grad(): Clears the gradients from the previous iteration.\n• output = transformer(src_data, tgt_data[:, :-1]): Passes the source data and the target data (excluding the last token in each sequence) through the transformer. This is common in sequence-to-sequence tasks where the target is shifted by one token.\n• loss = criterion(...): Computes the loss between the model's predictions and the target data (excluding the first token in each sequence). The loss is calculated by reshaping the data into one-dimensional tensors and using the cross-entropy loss function.\n• loss.backward(): Computes the gradients of the loss with respect to the model's parameters.\n• optimizer.step(): Updates the model's parameters using the computed gradients.\n• print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\"): Prints the current epoch number and the loss value for that epoch.\n\nThis code snippet trains the transformer model on randomly generated source and target sequences for 100 epochs. It uses the Adam optimizer and the cross-entropy loss function. The loss is printed for each epoch, allowing you to monitor the training progress. In a real-world scenario, you would replace the random source and target sequences with actual data from your task, such as machine translation.\n\nAfter training the model, its performance can be evaluated on a validation dataset or test dataset. The following is an example of how this could be done:\n• transformer.eval(): Puts the transformer model in evaluation mode. This is important because it turns off certain behaviors like dropout that are only used during training.\n• val_src_data: Random integers between 1 and src_vocab_size, representing a batch of validation source sequences with shape (64, max_seq_length).\n• val_tgt_data: Random integers between 1 and tgt_vocab_size, representing a batch of validation target sequences with shape (64, max_seq_length).\n• with torch.no_grad(): Disables gradient computation, as we don't need to compute gradients during validation. This can reduce memory consumption and speed up computations.\n• val_output = transformer(val_src_data, val_tgt_data[:, :-1]): Passes the validation source data and the validation target data (excluding the last token in each sequence) through the transformer.\n• val_loss = criterion(...): Computes the loss between the model's predictions and the validation target data (excluding the first token in each sequence). The loss is calculated by reshaping the data into one-dimensional tensors and using the previously defined cross-entropy loss function.\n\nThis code snippet evaluates the transformer model on a randomly generated validation dataset, computes the validation loss, and prints it. In a real-world scenario, the random validation data should be replaced with actual validation data from the task you are working on. The validation loss can give you an indication of how well your model is performing on unseen data, which is a critical measure of the model's generalization ability.\n\nFor further details about Transformers and Hugging Face, our tutorial, An Introduction to Using Transformers and Hugging Face, is useful.\n\nIn conclusion, this tutorial demonstrated how to construct a Transformer model using PyTorch, one of the most versatile tools for deep learning. With their capacity for parallelization and the ability to capture long-term dependencies in data, Transformers have immense potential in various fields, especially NLP tasks like translation, summarization, and sentiment analysis.\n\nFor those eager to deepen their understanding of advanced deep learning concepts and techniques, consider exploring the course Advanced Deep Learning with Keras on DataCamp. You can also read about building a simple neural network with PyTorch in a separate tutorial."
    },
    {
        "link": "https://github.com/hyunwoongko/transformer",
        "document": "This code was written in 2019, and I was not very familiar with transformer model in that time. So don't trust this code too much. Currently I am not managing this code well, so please open pull requests if you find bugs in the code and want to fix.\n\nMy own implementation Transformer model (Attention is All You Need - Google Brain, 2017) \n\n\n\n \n\n\n\n\n\n( . ): ( , , , ): ( , ). () # same size with input matrix (for adding with input matrix) . . ( , , ) . . # we don't need to compute gradient . ( , , ) . (). ( ) . ( , , , ). () # \"step=2\" means 'i' multiplied with two (same with 2 * i) . [:, :: ] . ( ( ( ))) . [:, :: ] . ( ( ( ))) # compute positional encoding to consider positional information of words ( , ): , . () . [: , :] # it will add with tok_emb : [128, 30, 512]\n\n( . ): Query : given sentence that we focused on (decoder) Key : every sentence to check relationship with Qeury(encoder) Value : every sentence same with Key (encoder) ( ): ( , ). () . . ( ) ( , , , , , ): , , , . () . ( , ) ( @ ) . ( ) : . ( , ) . ( ) @ ,\n\nI use Multi30K Dataset to train and evaluate model \n\n You can check detail of dataset here \n\n I follow original paper's parameter settings. (below)\n• Attention is All You Need, 2017 - Google"
    }
]