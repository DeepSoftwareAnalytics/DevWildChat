[
    {
        "link": "https://docs.libuv.org",
        "document": "libuv is a multi-platform support library with a focus on asynchronous I/O. It was primarily developed for use by Node.js, but it’s also used by Luvit, Julia, uvloop, and others.\n\nIn case you find errors in this documentation you can help by sending pull requests!"
    },
    {
        "link": "https://github.com/libuv/libuv",
        "document": "libuv is a multi-platform support library with a focus on asynchronous I/O. It was primarily developed for use by Node.js, but it's also used by Luvit, Julia, uvloop, and others.\n• IPC with socket sharing, using Unix domain sockets or named pipes (Windows)\n\nStarting with version 1.0.0 libuv follows the semantic versioning scheme. The API change and backwards compatibility rules are those indicated by SemVer. libuv will keep a stable ABI across major releases.\n\nThe ABI/API changes can be tracked here.\n\nlibuv is licensed under the MIT license. Check the LICENSE and LICENSE-extra files.\n\nThe documentation is licensed under the CC BY 4.0 license. Check the LICENSE-docs file.\n\nLocated in the docs/ subdirectory. It uses the Sphinx framework, which makes it possible to build the documentation in multiple formats.\n\nBuild documentation as HTML and live reload it when it changes (this requires sphinx-autobuild to be installed and is only supported on Unix):\n\nNOTE: Windows users need to use make.bat instead of plain 'make'.\n\nDocumentation can be browsed online here.\n\nThe tests and benchmarks also serve as API specification and usage examples.\n• libuv-dox — Documenting types and methods of libuv, mostly by reading uv.h.\n• learnuv — Learn uv for fun and profit, a self guided workshop to libuv.\n\nThese resources are not handled by libuv maintainers and might be out of date. Please verify it before opening new issues.\n\nlibuv can be downloaded either from the GitHub repository or from the downloads site.\n\nBefore verifying the git tags or signature files, importing the relevant keys is necessary. Key IDs are listed in the MAINTAINERS file, but are also available as git blob objects for easier use.\n\nGit tags are signed with the developer's key, they can be verified as follows:\n\nStarting with libuv 1.7.0, the tarballs stored in the downloads site are signed and an accompanying signature file sit alongside each. Once both the release tarball and the signature file are downloaded, the file can be verified as follows:\n\nFor UNIX-like platforms, including macOS, there are two build methods: autotools or CMake.\n\nFor Windows, CMake is the only supported build method and has the following prerequisites:\n\nTo cross-compile with CMake (unsupported but generally works):\n\nMake sure that you specify the architecture you wish to build for in the \"ARCHS\" flag. You can specify more than one by delimiting with a space (e.g. \"x86_64 i386\").\n\nYou can install pre-built binaries for libuv or build it from source using Conan. Use the following command:\n\nThe libuv Conan recipe is kept up to date by Conan maintainers and community contributors. If the version is out of date, please create an issue or pull request on the ConanCenterIndex repository.\n\nSome tests are timing sensitive. Relaxing test timeouts may be necessary on slow or overloaded machines:\n\nThe list of all tests is in .\n\nThis invocation will cause the test driver to fork and execute in a child process:\n\nThis invocation will cause the test driver to execute the test in the same process:\n\nWhen running the test from within the test driver process ( ), tools like gdb and valgrind work normally.\n\nWhen running the test from a child of the test driver process ( ), use these tools in a fork-aware manner.\n\nSee the section on running tests. The benchmark driver is and the benchmarks are listed in .\n\nIt is recommended to turn on the compiler flag in projects that use libuv. The use of ad hoc \"inheritance\" in the libuv API may not be safe in the presence of compiler optimizations that depend on strict aliasing.\n\nMSVC does not have an equivalent flag but it also does not appear to need it at the time of writing (December 2019.)\n\nAIX support for filesystem events requires the non-default IBM package to be installed. This package provides the AIX Event Infrastructure that is detected by . IBM documentation describes the package in more detail.\n\nz/OS compilation requires ZOSLIB to be installed. When building with CMake, use the flag to specify the path to ZOSLIB:\n\nz/OS creates System V semaphores and message queues. These persist on the system after the process terminates unless the event loop is closed.\n\nUse the command to manually clear up System V resources.\n\nSee the guidelines for contributing."
    },
    {
        "link": "https://docs.libuv.org/en/v1.x/fs.html",
        "document": "libuv provides a wide variety of cross-platform sync and async file system operations. All functions defined in this document take a callback, which is allowed to be NULL. If the callback is NULL the request is completed synchronously, otherwise it will be performed asynchronously.\n\nAll file operations are run on the threadpool. See Thread pool work scheduling for information on the threadpool size.\n\nStarting with libuv v1.45.0, some file operations on Linux are handed off to when possible. Apart from a (sometimes significant) increase in throughput there should be no change in observable behavior. Libuv reverts to using its threadpool when the necessary kernel features are unavailable or unsuitable. Starting with libuv v1.49.0 this behavior was reverted and Libuv on Linux by default will be using the threadpool again. In order to enable io_uring the instance must be configured with the option.\n\nCleanup request. Must be called after a request is finished to deallocate any memory libuv might have allocated. On Windows libuv uses and thus the file is always opened in binary mode. Because of this the O_BINARY and O_TEXT flags are not supported. Equivalent to preadv(2). If the argument is , then the current file offset is used and updated. On Windows, under non-MSVC environments (e.g. when GCC or Clang is used to build libuv), files opened using may cause a fatal crash if the memory mapped read operation fails. Equivalent to pwritev(2). If the argument is , then the current file offset is used and updated. On Windows, under non-MSVC environments (e.g. when GCC or Clang is used to build libuv), files opened using may cause a fatal crash if the memory mapped write operation fails. is currently not implemented on Windows. Equivalent to mkdtemp(3). The result can be found as a null terminated string at . Equivalent to mkstemp(3). The created file path can be found as a null terminated string at . The file descriptor can be found as an integer at . Opens as a directory stream. On success, a is allocated and returned via . This memory is not freed by , although is set to . The allocated memory must be freed by calling . On failure, no memory is allocated. The contents of the directory can be iterated over by passing the resulting to . Closes the directory stream represented by and frees the memory allocated by . Iterates over the directory stream, , returned by a successful call. Prior to invoking , the caller must set and , representing the array of elements used to hold the read directory entries and its size. On success, the result is an integer >= 0 representing the number of entries read from the stream. This function does not return the “.” and “..” entries. On success this function allocates memory that must be freed using . must be called before closing the directory with . Equivalent to scandir(3), with a slightly different API. Once the callback for the request is called, the user can use to get populated with the next directory entry data. When there are no more entries will be returned. Unlike , this function does not return the “.” and “..” entries. On Linux, getting the type of an entry is only supported by some file systems (btrfs, ext2, ext3 and ext4 at the time of this writing), check the getdents(2) man page. Equivalent to stat(2), fstat(2) and lstat(2) respectively. Equivalent to statfs(2). On success, a is allocated and returned via . This memory is freed by . Any fields in the resulting that are not supported by the underlying operating system are set to zero. For AIX, returns on file descriptors referencing non regular files. Copies a file from to . Supported are described below.\n• None : If present, will fail with if the destination path already exists. The default behavior is to overwrite the destination if it exists.\n• None : If present, will attempt to create a copy-on-write reflink. If the underlying platform does not support copy-on-write, or an error occurs while attempting to use copy-on-write, a fallback copy mechanism based on is used.\n• None : If present, will attempt to create a copy-on-write reflink. If the underlying platform does not support copy-on-write, or an error occurs while attempting to use copy-on-write, then an error is returned. If the destination path is created, but an error occurs while copying the data, then the destination path is removed. There is a brief window of time between closing and removing the file where another process could access the file. Changed in version 1.20.0: and are supported. Changed in version 1.33.0: If an error occurs while using , that error is returned. Previously, all errors were mapped to . Equivalent to access(2) on Unix. Windows uses . Equivalent to chmod(2) and fchmod(2) respectively. Equivalent to utime(2), futimes(3) and lutimes(3) respectively. Passing as the atime or mtime sets the timestamp to the current time. Passing as the atime or mtime leaves the timestamp untouched. z/OS: is not implemented for z/OS. It can still be called but will return . AIX: and functions only work for AIX 7.1 and newer. They can still be called on older versions but will return . Changed in version 1.10.0: sub-second precission is supported on Windows On Windows the parameter can be specified to control how the symlink will be created:\n• None : indicates that points to a directory.\n• None : request that the symlink is created using junction points. Equivalent to readlink(2). The resulting string is stored in . Equivalent to realpath(3) on Unix. Windows uses GetFinalPathNameByHandleW. The resulting string is stored in . This function has certain platform-specific caveats that were discovered when used in Node.\n• None macOS and other BSDs: this function will fail with UV_ELOOP if more than 32 symlinks are found while resolving the given path. This limit is hardcoded and cannot be sidestepped.\n• None Windows: while this function works in the common case, there are a number of corner cases where it doesn’t:\n• None Paths in ramdisk volumes created by tools which sidestep the Volume Manager (such as ImDisk) cannot be resolved. While this function can still be used, it’s not recommended if scenarios such as the above need to be supported. The background story and some more details on these issues can be checked here. Equivalent to chown(2), fchown(2) and lchown(2) respectively. These functions are not implemented on Windows. Returns the platform specific error code - value on Windows and on other platforms.\n\nThe file is opened in append mode. Before each write, the file offset is positioned at the end of the file. The file is created if it does not already exist. File I/O is done directly to and from user-space buffers, which must be aligned. Buffer size and address should be a multiple of the physical sector size of the block device. is supported on Linux, and on Windows via FILE_FLAG_NO_BUFFERING. is not supported on macOS. If the path is not a directory, fail the open. is not supported on Windows. The file is opened for synchronous I/O. Write operations will complete once all data and a minimum of metadata are flushed to disk. is supported on Windows via FILE_FLAG_WRITE_THROUGH. If the flag is set and the file already exists, fail the open. In general, the behavior of is undefined if it is used without . There is one exception: on Linux 2.6 and later, can be used without if pathname refers to a block device. If the block device is in use by the system (e.g., mounted), the open will fail with the error . is only supported on macOS and Windows. Changed in version 1.17.0: support is added for Windows. Use a memory file mapping to access the file. When using this flag, the file cannot be open multiple times concurrently. is only supported on Windows. Do not update the file access time when the file is read. is not supported on Windows. If the path identifies a terminal device, opening the path will not cause that terminal to become the controlling terminal for the process (if the process does not already have one). is not supported on Windows. If the path is a symbolic link, fail the open. is not supported on Windows. Open the file in nonblocking mode if possible. is not supported on Windows. Access is intended to be random. The system can use this as a hint to optimize file caching. is only supported on Windows via FILE_FLAG_RANDOM_ACCESS. Access is intended to be sequential from beginning to end. The system can use this as a hint to optimize file caching. is only supported on Windows via FILE_FLAG_SEQUENTIAL_SCAN. The file is temporary and should not be flushed to disk if possible. is only supported on Windows via FILE_ATTRIBUTE_TEMPORARY. Open the symbolic link itself rather than the resource it points to. The file is opened for synchronous I/O. Write operations will complete once all data and all metadata are flushed to disk. is supported on Windows via FILE_FLAG_WRITE_THROUGH. The file is temporary and should not be flushed to disk if possible. is only supported on Windows via FILE_ATTRIBUTE_TEMPORARY. If the file exists and is a regular file, and the file is opened successfully for write access, its length shall be truncated to zero."
    },
    {
        "link": "https://docs.oracle.com/cd/E88353_01/html/E37839/libuv-1.html",
        "document": ""
    },
    {
        "link": "https://libuv.org",
        "document": "libuv is a multi-platform support library with a focus on asynchronous I/O."
    },
    {
        "link": "https://protobuf.dev/getting-started/cpptutorial",
        "document": "A basic C++ programmers introduction to working with protocol buffers.\n\nThis tutorial provides a basic C++ programmers introduction to working with protocol buffers. By walking through creating a simple example application, it shows you how to\n• Use the C++ protocol buffer API to write and read messages.\n\nThis isn’t a comprehensive guide to using protocol buffers in C++. For more detailed reference information, see the Protocol Buffer Language Guide (proto2), the Protocol Buffer Language Guide (proto3), the C++ API Reference, the C++ Generated Code Guide, and the Encoding Reference.\n\nThe example we’re going to use is a very simple “address book” application that can read and write people’s contact details to and from a file. Each person in the address book has a name, an ID, an email address, and a contact phone number.\n\nHow do you serialize and retrieve structured data like this? There are a few ways to solve this problem:\n• The raw in-memory data structures can be sent/saved in binary form. Over time, this is a fragile approach, as the receiving/reading code must be compiled with exactly the same memory layout, endianness, etc. Also, as files accumulate data in the raw format and copies of software that are wired for that format are spread around, it’s very hard to extend the format.\n• You can invent an ad-hoc way to encode the data items into a single string – such as encoding 4 ints as “12:3:-23:67”. This is a simple and flexible approach, although it does require writing one-off encoding and parsing code, and the parsing imposes a small run-time cost. This works best for encoding very simple data.\n• Serialize the data to XML. This approach can be very attractive since XML is (sort of) human readable and there are binding libraries for lots of languages. This can be a good choice if you want to share data with other applications/projects. However, XML is notoriously space intensive, and encoding/decoding it can impose a huge performance penalty on applications. Also, navigating an XML DOM tree is considerably more complicated than navigating simple fields in a class normally would be.\n\nInstead of these options, you can use protocol buffers. Protocol buffers are the flexible, efficient, automated solution to solve exactly this problem. With protocol buffers, you write a description of the data structure you wish to store. From that, the protocol buffer compiler creates a class that implements automatic encoding and parsing of the protocol buffer data with an efficient binary format. The generated class provides getters and setters for the fields that make up a protocol buffer and takes care of the details of reading and writing the protocol buffer as a unit. Importantly, the protocol buffer format supports the idea of extending the format over time in such a way that the code can still read data encoded with the old format.\n\nWhere to Find the Example Code\n\nThe example code is included in the source code package, under the “examples” directory.\n\nTo create your address book application, you’ll need to start with a file. The definitions in a file are simple: you add a message for each data structure you want to serialize, then specify a name and a type for each field in the message. Here is the file that defines your messages, .\n\nAs you can see, the syntax is similar to C++ or Java. Let’s go through each part of the file and see what it does.\n\nThe file starts with a package declaration, which helps to prevent naming conflicts between different projects. In C++, your generated classes will be placed in a namespace matching the package name.\n\nNext, you have your message definitions. A message is just an aggregate containing a set of typed fields. Many standard simple data types are available as field types, including , , , , and . You can also add further structure to your messages by using other message types as field types – in the above example the message contains messages, while the message contains messages. You can even define message types nested inside other messages – as you can see, the type is defined inside . You can also define types if you want one of your fields to have one of a predefined list of values – here you want to specify that a phone number can be one of the following phone types: , , or .\n\nThe \" = 1\", \" = 2\" markers on each element identify the unique field number that field uses in the binary encoding. Field numbers 1-15 require one less byte to encode than higher numbers, so as an optimization you can decide to use those numbers for the commonly used or repeated elements, leaving field numbers 16 and higher for less-commonly used optional elements. Each element in a repeated field requires re-encoding the field number, so repeated fields are particularly good candidates for this optimization.\n\nEach field must be annotated with one of the following modifiers:\n• : the field may or may not be set. If an optional field value isn’t set, a default value is used. For simple types, you can specify your own default value, as we’ve done for the phone number in the example. Otherwise, a system default is used: zero for numeric types, the empty string for strings, false for bools. For embedded messages, the default value is always the “default instance” or “prototype” of the message, which has none of its fields set. Calling the accessor to get the value of an optional (or required) field which has not been explicitly set always returns that field’s default value.\n• : the field may be repeated any number of times (including zero). The order of the repeated values will be preserved in the protocol buffer. Think of repeated fields as dynamically sized arrays.\n• : a value for the field must be provided, otherwise the message will be considered “uninitialized”. If is compiled in debug mode, serializing an uninitialized message will cause an assertion failure. In optimized builds, the check is skipped and the message will be written anyway. However, parsing an uninitialized message will always fail (by returning from the parse method). Other than this, a required field behaves exactly like an optional field.\n\nRequired Is Forever You should be very careful about marking fields as . If at some point you wish to stop writing or sending a required field, it will be problematic to change the field to an optional field – old readers will consider messages without this field to be incomplete and may reject or drop them unintentionally. You should consider writing application-specific custom validation routines for your buffers instead. Within Google, fields are strongly disfavored; most messages defined in proto2 syntax use and only. (Proto3 does not support fields at all.)\n\nYou’ll find a complete guide to writing files – including all the possible field types – in the Protocol Buffer Language Guide. Don’t go looking for facilities similar to class inheritance, though – protocol buffers don’t do that.\n\nNow that you have a , the next thing you need to do is generate the classes you’ll need to read and write (and hence and ) messages. To do this, you need to run the protocol buffer compiler on your :\n• None If you haven’t installed the compiler, download the package and follow the instructions in the README.\n• None Now run the compiler, specifying the source directory (where your application’s source code lives – the current directory is used if you don’t provide a value), the destination directory (where you want the generated code to go; often the same as ), and the path to your . In this case, you…: Because you want C++ classes, you use the option – similar options are provided for other supported languages.\n\nThis generates the following files in your specified destination directory:\n• , the header which declares your generated classes.\n• , which contains the implementation of your classes.\n\nLet’s look at some of the generated code and see what classes and functions the compiler has created for you. If you look in , you can see that you have a class for each message you specified in . Looking closer at the class, you can see that the compiler has generated accessors for each field. For example, for the , , , and fields, you have these methods:\n\nAs you can see, the getters have exactly the name as the field in lowercase, and the setter methods begin with . There are also methods for each singular (required or optional) field which return true if that field has been set. Finally, each field has a method that un-sets the field back to its empty state.\n\nWhile the numeric field just has the basic accessor set described above, the and fields have a couple of extra methods because they’re strings – a getter that lets you get a direct pointer to the string, and an extra setter. Note that you can call even if is not already set; it will be initialized to an empty string automatically. If you had a repeated message field in this example, it would also have a method but not a method.\n\nRepeated fields also have some special methods – if you look at the methods for the repeated field, you’ll see that you can\n• check the repeated field’s (in other words, how many phone numbers are associated with this ).\n• get a specified phone number using its index.\n• update an existing phone number at the specified index.\n• add another phone number to the message which you can then edit (repeated scalar types have an that just lets you pass in the new value).\n\nFor more information on exactly what members the protocol compiler generates for any particular field definition, see the C++ generated code reference.\n\nThe generated code includes a enum that corresponds to your enum. You can refer to this type as and its values as , , and (the implementation details are a little more complicated, but you don’t need to understand them to use the enum).\n\nThe compiler has also generated a nested class for you called . If you look at the code, you can see that the “real” class is actually called , but a typedef defined inside allows you to treat it as if it were a nested class. The only case where this makes a difference is if you want to forward-declare the class in another file – you cannot forward-declare nested types in C++, but you can forward-declare .\n\nEach message class also contains a number of other methods that let you check or manipulate the entire message, including:\n• : checks if all the required fields have been set.\n• : returns a human-readable representation of the message, particularly useful for debugging.\n• : overwrites the message with the given message’s values.\n• : clears all the elements back to the empty state.\n\nThese and the I/O methods described in the following section implement the interface shared by all C++ protocol buffer classes. For more info, see the complete API documentation for .\n\nFinally, each protocol buffer class has methods for writing and reading messages of your chosen type using the protocol buffer binary format. These include:\n• : serializes the message and stores the bytes in the given string. Note that the bytes are binary, not text; we only use the class as a convenient container.\n• : parses a message from the given string.\n• : writes the message to the given C++ .\n• : parses a message from the given C++ .\n\nThese are just a couple of the options provided for parsing and serialization. Again, see the API reference for a complete list.\n\nProtocol Buffers and Object Oriented Design Protocol buffer classes are basically data holders (like structs in C) that don’t provide additional functionality; they don’t make good first class citizens in an object model. If you want to add richer behavior to a generated class, the best way to do this is to wrap the generated protocol buffer class in an application-specific class. Wrapping protocol buffers is also a good idea if you don’t have control over the design of the file (if, say, you’re reusing one from another project). In that case, you can use the wrapper class to craft an interface better suited to the unique environment of your application: hiding some data and methods, exposing convenience functions, etc. You should never add behavior to the generated classes by inheriting from them. This will break internal mechanisms and is not good object-oriented practice anyway.\n\nNow let’s try using your protocol buffer classes. The first thing you want your address book application to be able to do is write personal details to your address book file. To do this, you need to create and populate instances of your protocol buffer classes and then write them to an output stream.\n\nHere is a program which reads an from a file, adds one new to it based on user input, and writes the new back out to the file again. The parts which directly call or reference code generated by the protocol compiler are highlighted.\n\nNotice the macro. It is good practice – though not strictly necessary – to execute this macro before using the C++ Protocol Buffer library. It verifies that you have not accidentally linked against a version of the library which is incompatible with the version of the headers you compiled with. If a version mismatch is detected, the program will abort. Note that every file automatically invokes this macro on startup.\n\nAlso notice the call to at the end of the program. All this does is delete any global objects that were allocated by the Protocol Buffer library. This is unnecessary for most programs, since the process is just going to exit anyway and the OS will take care of reclaiming all of its memory. However, if you use a memory leak checker that requires that every last object be freed, or if you are writing a library which may be loaded and unloaded multiple times by a single process, then you may want to force Protocol Buffers to clean up everything.\n\nOf course, an address book wouldn’t be much use if you couldn’t get any information out of it! This example reads the file created by the above example and prints all the information in it.\n\nSooner or later after you release the code that uses your protocol buffer, you will undoubtedly want to “improve” the protocol buffer’s definition. If you want your new buffers to be backwards-compatible, and your old buffers to be forward-compatible – and you almost certainly do want this – then there are some rules you need to follow. In the new version of the protocol buffer:\n• you must not change the field numbers of any existing fields.\n• you must not add or delete any required fields.\n• you may delete optional or repeated fields.\n• you may add new optional or repeated fields but you must use fresh field numbers (that is, field numbers that were never used in this protocol buffer, not even by deleted fields).\n\nIf you follow these rules, old code will happily read new messages and simply ignore any new fields. To the old code, optional fields that were deleted will simply have their default value, and deleted repeated fields will be empty. New code will also transparently read old messages. However, keep in mind that new optional fields will not be present in old messages, so you will need to either check explicitly whether they’re set with , or provide a reasonable default value in your file with after the field number. If the default value is not specified for an optional element, a type-specific default value is used instead: for strings, the default value is the empty string. For booleans, the default value is false. For numeric types, the default value is zero. Note also that if you added a new repeated field, your new code will not be able to tell whether it was left empty (by new code) or never set at all (by old code) since there is no flag for it.\n\nThe C++ Protocol Buffers library is extremely heavily optimized. However, proper usage can improve performance even more. Here are some tips for squeezing every last drop of speed out of the library:\n• Reuse message objects when possible. Messages try to keep around any memory they allocate for reuse, even when they are cleared. Thus, if you are handling many messages with the same type and similar structure in succession, it is a good idea to reuse the same message object each time to take load off the memory allocator. However, objects can become bloated over time, especially if your messages vary in “shape” or if you occasionally construct a message that is much larger than usual. You should monitor the sizes of your message objects by calling the method and delete them once they get too big.\n• Your system’s memory allocator may not be well-optimized for allocating lots of small objects from multiple threads. Try using Google’s TCMalloc instead.\n\nProtocol buffers have uses that go beyond simple accessors and serialization. Be sure to explore the C++ API reference to see what else you can do with them.\n\nOne key feature provided by protocol message classes is reflection. You can iterate over the fields of a message and manipulate their values without writing your code against any specific message type. One very useful way to use reflection is for converting protocol messages to and from other encodings, such as XML or JSON. A more advanced use of reflection might be to find differences between two messages of the same type, or to develop a sort of “regular expressions for protocol messages” in which you can write expressions that match certain message contents. If you use your imagination, it’s possible to apply Protocol Buffers to a much wider range of problems than you might initially expect!\n\nReflection is provided by the interface."
    },
    {
        "link": "https://protobuf.dev/reference/cpp/api-docs",
        "document": "This section contains reference documentation for working with protocol buffer classes in C++.\n\nThe Protocol Buffer library uses the classes in this package to deal with I/O and encoding/decoding raw bytes. Most users will not need to deal with this package. However, users who want to adapt the system to work with their own I/O abstractions – e.g., to allow Protocol Buffers to be read from a different kind of input stream without the need for a temporary buffer – should take a closer look.\n\nThis file contains the CodedInputStream and CodedOutputStream classes, which wrap a ZeroCopyInputStream or ZeroCopyOutputStream , respectively, and allow you to read or write individual pieces of data in various formats. This file contains the ZeroCopyInputStream and ZeroCopyOutputStream interfaces, which represent abstract I/O streams to and from which protocol buffers can be read and written. This file contains common implementations of the interfaces defined in zero_copy_stream.h which are only included in the full (non-lite) protobuf library. This file contains common implementations of the interfaces defined in zero_copy_stream.h which are included in the \"lite\" protobuf library.\n\nThis package contains various utilities for message comparison, JSON conversion, well known types, etc.\n\nDefines utilities for the FieldMask well known type. Utility functions to convert between protobuf binary format and proto3 JSON format. This file defines static methods and classes for comparing Protocol Messages. Defines utilities for the Timestamp and Duration well known types. Defines a TypeResolver for the Any message.\n\nThis package contains code for parsing .proto files and generating code based on them. There are two reasons you might be interested in this package:\n• You want to parse .proto files at runtime. In this case, you should look at importer.h. Since this functionality is widely useful, it is included in the libprotobuf base library; you do not have to link against libprotoc.\n• You want to write a custom protocol compiler which generates different kinds of code, e.g. code in a different language which is not supported by the official compiler. For this purpose, command_line_interface.h provides you with a complete compiler front-end, so all you need to do is write a custom implementation of CodeGenerator and a trivial main() function. You can even make your compiler support the official languages in addition to your own. Since this functionality is only useful to those writing custom compilers, it is in a separate library called \"libprotoc\" which you will have to link against.\n\nDefines the abstract interface implemented by each of the language-specific code generators. Implements the Protocol Compiler front-end such that it may be reused by custom compilers written to support other languages. This file is the public interface to the .proto file parser. Front-end for protoc code generator plugins written in C++. Generates C++ code for a given .proto file. Generates C# code for a given .proto file. Provides a mechanism for mapping a descriptor to the fully-qualified name of the corresponding C# class. Provides a mechanism for mapping a descriptor to the fully-qualified name of the corresponding Java class.\n\nThis section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++. This section contains reference documentation for working with protocol buffer classes in C++."
    },
    {
        "link": "https://stackoverflow.com/questions/22881876/protocol-buffers-how-to-serialize-and-deserialize-multiple-messages-into-a-file",
        "document": "I am new to Protocol Buffers and c++ but my task requires me to use the two. I want to write a structure of data ( message) into a single file multiple times and be able to read the data. i can read and write a single message but multiple messages is proving harder. I have looked for answers for hours but i can't seem to be able to read the data as a structure. Any example code or pointers will be very helpful.\n\nThis is the format of my structure:\n\nThis is what i've been using to write into a file:\n\nI have tried to change the file editing options to append instead of truncate but that does not let me read the data in the format that I want.\n\nthis is what I use to read:\n\nThese are the contents of my .proto file:\n\nFrom all the searching i've done it seems like CodedInputStream/CodedOutputStream are my best options but I haven't been able to find any detailed examples or explanation to help me understand. I understand that Protocol Buffers are not self delimiting and this is probably the reason why I can't read my messages back in the original format. Any help would be appreciated. Thanks\n\nEDIT: I have tried to use CodedOutputStream based on the messages I've recieved but it doesn't seem to write anything into the file.\n\nAfter using this code the file comes out blank. Where am I going wrong ?"
    },
    {
        "link": "https://stackoverflow.com/questions/28060457/serialize-and-deserialize-the-message-using-google-protobuf-in-socket-programmin",
        "document": "You should send more than just the protobuf message to be able to decode it on the client side.\n\nA simple solution would be to send the value of over the socket as a 4-byte integer using network byte order, and the send the buffer itself.\n\nThe client should first read the buffer's size from the socket and convert it from network to host byte order. Let's denote the resulting value . The client must then preallocate a buffer of size and read bytes from the socket into it. After that, just use MessageLite::ParseFromString to reconstruct your protobuf.\n\nSee here for more info on protobuf message methods.\n\nAlso, this document discourages the usage of :\n\nYou should be very careful about marking fields as required. If at some point you wish to stop writing or sending a required field, it will be problematic to change the field to an optional field – old readers will consider messages without this field to be incomplete and may reject or drop them unintentionally. You should consider writing application-specific custom validation routines for your buffers instead. Some engineers at Google have come to the conclusion that using required does more harm than good; they prefer to use only optional and repeated. However, this view is not universal."
    },
    {
        "link": "https://protobuf.dev/best-practices/api",
        "document": "A future-proof API is surprisingly hard to get right. The suggestions in this document make trade-offs to favor long-term, bug-free evolution.\n\nThis doc is a complement to Proto Best Practices. It’s not a prescription for Java/C++/Go and other APIs.\n\nIf you see a proto straying from these guidelines in a code review, point the author to this topic and help spread the word.\n\nThese guidelines are just that and many have documented exceptions. For example, if you’re writing a performance-critical backend, you might want to sacrifice flexibility or safety for speed. This topic will help you better understand the trade-offs and make a decision that works for your situation.\n\nChances are good your proto will be inherited and used by people who don’t know what you were thinking when you wrote or modified it. Document each field in terms that will be useful to a new team-member or client with little knowledge of your system.\n\nDocument the constraints, expectations and interpretation of each field in as few words as possible.\n\nYou can use custom proto annotations. See Custom Options to define cross-language constants like in the example above. Supported in proto2 and proto3.\n\nOver time, documentation of an interface can get longer and longer. The length takes away from the clarity. When the documentation is genuinely unclear, fix it, but look at it holistically and aim for brevity.\n\nUse Different Messages for Wire and Storage\n\nIf a top-level proto you expose to clients is the same one you store on disk, you’re headed for trouble. More and more binaries will depend on your API over time, making it harder to change. You’ll want the freedom to change your storage format without impacting your clients. Layer your code so that modules deal either with client protos, storage protos, or translation.\n\nWhy? You might want to swap your underlying storage system. You might want to normalize—or denormalize—data differently. You might realize that parts of your client-exposed proto make sense to store in RAM while other parts make sense to go on disk.\n\nWhen it comes to protos nested one or more levels within a top-level request or response, the case for separating storage and wire protos isn’t as strong, and depends on how closely you’re willing to couple your clients to those protos.\n\nThere’s a cost in maintaining the translation layer, but it quickly pays off once you have clients and have to do your first storage changes.\n\nYou might be tempted to share protos and diverge “when you need to.” With a perceived high cost to diverge and no clear place to put internal fields, your API will accrue fields clients either don’t understand or begin to depend on without your knowledge.\n\nBy starting with separate proto files, your team will know where to add internal fields without polluting your API. In the early days, the wire proto can be tag-for-tag identical with an automatic translation layer (think: byte copying or proto reflection). Proto annotations can also power an automatic translation layer.\n\nThe following are exceptions to the rule:\n• None If the proto field is one of a common type, such as or , then using that type both as storage and API is acceptable.\n• None If your service is extremely performance-sensitive, it may be worth trading flexibility for execution speed. If your service doesn’t have millions of QPS with millisecond latency, you’re probably not the exception.\n• None If all of the following are true:\n• your service is the storage system\n• your system doesn’t make decisions based on your clients’ structured data\n• your system simply stores, loads, and perhaps provides queries at your client’s request Note that if you are implementing something like a logging system or a proto-based wrapper around a generic storages system, then you probably want to aim to have your clients’ messages transit into your storage backend as opaquely as possible so that you don’t create a dependency nexus. Consider using extensions or Encode Opaque Data in Strings by Web-safe Encoding Binary Proto Serialization.\n\nDon’t make an that only takes a .\n\nIf a client doesn’t preserve unknown fields, they will not have the newest fields of leading to data loss on a round-trip. Some systems don’t preserve unknown fields. Proto2 and proto3 implementations do preserve unknown fields unless the application drops the unknown fields explicitly. In general, public APIs should drop unknown fields on server-side to prevent security attack via unknown fields. For example, garbage unknown fields may cause a server to fail when it starts to use them as new fields in the future.\n\nAbsent documentation, handling of optional fields is ambiguous. Will clear the field? That leaves you open to data loss when the client doesn’t know about the field. Does it not touch a field? Then how can clients clear the field? Neither are good.\n\nHave your client pass which fields it wants to modify and include only those fields in the update request. Your server leaves other fields alone and updates only those specified by the mask. In general, the structure of your mask should mirror the structure of the response proto; that is, if contains , contains .\n\nFor example, instead of , you might have: , , , etc.\n\nCustom update methods are easier to monitor, audit, and secure than a very flexible update method. They’re also easier to implement and call. A large number of them can increase the cognitive load of an API.\n\nMany of the pitfalls described elsewhere in this doc are solved with this rule. For example:\n\nTelling clients that a repeated field is unset in storage versus not-populated in this particular call can be done by wrapping the repeated field in a message.\n\nCommon request options that are shared between requests naturally fall out of following this rule. Read and write field masks fall out of this.\n\nYour top-level proto should almost always be a container for other messages that can grow independently.\n\nEven when you only need a single primitive type today, having it wrapped in a message gives you a clear path to expand that type and share the type among other methods that return the similar values. For example:\n\nOne exception to top-level primitives: Opaque strings (or bytes) that encode a proto but are only built and parsed on the server. Continuation tokens, version info tokens and IDs can all be returned as strings if the string is actually an encoding of a structured proto.\n\nNever Use Booleans for Something That Has Two States Now, but Might Have More Later\n\nIf you are using boolean for a field, make sure that the field is indeed describing just two possible states (for all time, not just now and the near future). Often, the flexibility of an enum, int, or message turns out to be worth it.\n\nFor example, in returning a stream of posts a developer may need to indicate whether a post should be rendered in two-columns or not based on the current mocks from UX. Even though a boolean is all that’s needed today, nothing prevents UX from introducing two-row posts, three-column posts or four-square posts in a future version.\n\nBe cautious about adding states to an enum that conflates concepts.\n\nIf a state introduces a new dimension to the enum or implies multiple application behaviors, you almost certainly want another field.\n\nRarely Use an Integer Field for an ID\n\nIt’s tempting to use an int64 as an identifier for an object. Opt instead for a string.\n\nThis lets you change your ID space if you need to and reduces the chance of collisions. 2^64 isn’t as big as it used to be.\n\nYou can also encode a structured identifier as a string which encourages clients to treat it as an opaque blob. You still must have a proto backing the string, but you can serialize the proto to a string field (encoded as web-safe Base64) which removes any of the internal details from the client-exposed API. In this case follow the guidelines below.\n\nIf you start off with your own serialization scheme to represent your IDs as strings, things can get weird quickly. That’s why it’s often best to start with an internal proto that backs your string field.\n\nDon’t Encode Data in a String That You Expect a Client to Construct or Parse\n\nIt’s less efficient over the wire, more work for the consumer of the proto, and confusing for someone reading your documentation. Your clients also have to wonder about the encoding: Are lists comma-separated? Did I escape this untrusted data correctly? Are numbers base-10? Better to have clients send an actual message or primitive type. It’s more compact over the wire and clearer for your clients.\n\nThis gets especially bad when your service acquires clients in several languages. Now each will have to choose the right parser or builder—or worse—write one.\n\nMore generally, choose the right primitive type. See the Scalar Value Types table in the Protocol Buffer Language Guide.\n\nWith a JavaScript client, it’s tempting to return HTML or JSON in a field of your API. This is a slippery slope towards tying your API to a specific UI. Here are three concrete dangers:\n• A “scrappy” non-web client will end up parsing your HTML or JSON to get the data they want leading to fragility if you change formats and vulnerabilities if their parsing is bad.\n• Your web-client is now vulnerable to an XSS exploit if that HTML is ever returned unsanitized.\n• The tags and classes you’re returning expect a particular style-sheet and DOM structure. From release to release, that structure will change, and you risk a version-skew problem where the JavaScript client is older than the server and the HTML the server returns no longer renders properly on old clients. For projects that release often, this is not an edge case.\n\nOther than the initial page load, it’s usually better to return data and use client-side templating to construct HTML on the client .\n\nIf you do encode opaque data in a client-visible field (continuation tokens, serialized IDs, version infos, and so on), document that clients should treat it as an opaque blob. Always use binary proto serialization, never text-format or something of your own devising for these fields. When you need to expand the data encoded in an opaque field, you’ll find yourself reinventing protocol buffer serialization if you’re not already using it.\n\nDefine an internal proto to hold the fields that will go in the opaque field (even if you only need one field), serialize this internal proto to bytes then web-safe base-64 encode the result into your string field .\n\nOne rare exception to using proto serialization: Very occasionally, the compactness wins from a carefully constructed alternative format are worth it.\n\nDon’t Include Fields that Your Clients Can’t Possibly Have a Use for\n\nThe API you expose to clients should only be for describing how to interact with your system. Including anything else in it adds cognitive overhead to someone trying to understand it.\n\nReturning debug data in response protos used to be a common practice, but we have a better way. RPC response extensions (also called “side channels”) let you describe your client interface with one proto and your debugging surface with another.\n\nSimilarly, returning experiment names in response protos used to be a logging convenience–the unwritten contract was the client would send those experiments back on subsequent actions. The accepted way of accomplishing the same is to do log joining in the analysis pipeline.\n\nIf you need continuous, real-time analytics and are on a small machine budget, running log joins might be prohibitive. In cases where cost is a deciding factor, denormalizing log data ahead of time can be a win. If you need log data round-tripped to you, send it to clients as an opaque blob and document the request and response fields.\n\nCaution: If you need to return or round-trip hidden data on every request , you’re hiding the true cost of using your service and that’s not good either.\n\nThe best practice for a pagination API is to use an opaque continuation token (called next_page_token ) backed by an internal proto that you serialize and then (C++) or (Java). That internal proto could include many fields. The important thing is it buys you flexibility and–if you choose–it can buy your clients stability in the results.\n\nDo not forget to validate the fields of this proto as untrustworthy inputs (see note in Encode opaque data in strings).\n\nGroup Related Fields into a new . Nest Only Fields with High Cohesion\n\nOnly fields with high cohesion should be nested. If the fields are genuinely related, you’ll often want to pass them around together inside a server. That’s easier if they’re defined together in a message. Think:\n\nIf your CL introduces one field, but that field might have related fields later, preemptively put it in its own message to avoid this:\n\nThe problem with a nested message is that while might be a popular candidate for reuse in other places of your API, might not. In the worst case, is reused, but -specific fields leak into it.\n\nWhile loose coupling is generally accepted as a best practice when developing systems, that practice may not always apply when designing files. There may be cases in which tightly coupling two units of information (by nesting one unit inside of the other) may make sense. For example, if you are creating a set of fields that appear fairly generic right now but which you anticipate adding specialized fields into at a later time, nesting the message would dissuade others from referencing that message from elsewhere in this or other files.\n\nIf you use the recommended , you can use the (Java/C++) libraries to automatically filter a proto.\n\nRead masks set clear expectations on the client side, give them control of how much data they want back and allow the backend to only fetch data the client needs.\n\nThe acceptable alternative is to always populate every field; that is, treat the request as if there were an implicit read mask with all fields set to true. This can get costly as your proto grows.\n\nThe worst failure mode is to have an implicit (undeclared) read mask that varies depending on which method populated the message. This anti-pattern leads to apparent data loss on clients that build a local cache from response protos.\n\nInclude a Version Field to Allow for Consistent Reads\n\nWhen a client does a write followed by a read of the same object, they expect to get back what they wrote–even when the expectation isn’t reasonable for the underlying storage system.\n\nYour server will read the local value and if the local version_info is less than the expected version_info, it will read from remote replicas to find the latest value. Typically version_info is a proto encoded as a string that includes the datacenter the mutation went to and the timestamp at which it was committed.\n\nEven systems backed by consistent storage often want a token to trigger the more expensive read-consistent path rather than incurring the cost on every read.\n\nUse Consistent Request Options for RPCs that Return the Same Data Type\n\nAn example failure pattern is the request options for a service in which each RPC returns the same data type, but has separate request options for specifying things like maximum comments, embeds supported types list, and so on.\n\nThe cost of approaching this ad hoc is increased complexity on the client from figuring out how to fill out each request and increased complexity on the server transforming the N request options into a common internal one. A not-small number of real-life bugs are traceable to this example.\n\nInstead, create a single, separate message to hold request options and include that in each of the top-level request messages. Here’s a better-practices example:\n\nWhere possible, make mutations atomic. Even more important, make mutations idempotent. A full retry of a partial failure shouldn’t corrupt/duplicate data.\n\nOccasionally, you’ll need a single RPC that encapsulates multiple operations for performance reasons. What to do on a partial failure? If some succeeded and some failed, it’s best to let clients know.\n\nConsider setting the RPC as failed and return details of both the successes and failures in an RPC status proto.\n\nIn general, you want clients who are unaware of your handling of partial failures to still behave correctly and clients who are aware to get extra value.\n\nCreate Methods that Return or Manipulate Small Bits of Data and Expect Clients to Compose UIs from Batching Multiple Such Requests\n\nThe ability to query many narrowly specified bits of data in a single round-trip allows a wider range of UX options without server changes by letting the client compose what they need.\n\nThis is most relevant for front-end and middle-tier servers.\n\nMany services expose their own batching API.\n\nMake a One-off RPC when the Alternative is Serial Round-trips on Mobile or Web\n\nIn cases where a web or mobile client needs to make two queries with a data dependency between them, the current best practice is to create a new RPC that protects the client from the round trip.\n\nIn the case of mobile, it’s almost always worth saving your client the cost of an extra round-trip by bundling the two service methods together in one new one. For server-to-server calls, the case may not be as clear; it depends on how performance-sensitive your service is and how much cognitive overhead the new method introduces.\n\nA common evolution is that a single repeated field needs to become multiple related repeated fields. If you start with a repeated primitive your options are limited–you either create parallel repeated fields, or define a new repeated field with a new message that holds the values and migrate clients to it.\n\nIf you start with a repeated message, evolution becomes trivial.\n\nImagine the following feature request: “We need to know which enhancements were performed by the user and which enhancements were automatically applied by the system.”\n\nIf the enhancement field in were a scalar or enum, this would be much harder to support.\n\nThis applies equally to maps. It is much easier to add additional fields to a map value if it’s already a message rather than having to migrate from to .\n\nLatency-critical applications will find parallel arrays of primitive types are faster to construct and delete than a single array of messages; they can also be smaller over the wire if you use [packed=true] (eliding field tags). Allocating a fixed number of arrays is less work than allocating N messages. Bonus: in Proto3, packing is automatic; you don’t need to explicitly specify it.\n\nPrior to the introduction in Proto3 of Proto3 maps, services would sometimes expose data as pairs using an ad-hoc KVPair message with scalar fields. Eventually clients would need a deeper structure and would end up devising keys or values that need to be parsed in some way. See Don’t encode data in a string.\n\nSo, using a (extensible) message type for the value is an immediate improvement over the naive design.\n\nMaps were back-ported to proto2 in all languages, so using is better than inventing your own KVPair for the same purpose.\n\nIf you want to represent arbitrary data whose structure you don’t know ahead of time, use .\n\nSomewhere in the stack above you, a client may have retry logic. If the retry is a mutation, the user could be in for a surprise. Duplicate comments, build requests, edits, and so on aren’t good for anyone.\n\nA simple way to avoid duplicate writes is to allow clients to specify a client-created request ID that your server dedupes on (for example, hash of content or UUID).\n\nBe Mindful of Your Service Name, and Make it Globally Unique\n\nA service name (that is, the part after the keyword in your file) is used in surprisingly many places, not just to generate the service class name. This makes this name more important than one might think.\n\nWhat’s tricky is that these tools make the implicit assumption that your service name is unique across a network . Worse, the service name they use is the unqualified service name (for example, ), not the qualified service name (for example, ).\n\nFor this reason, it makes sense to take steps to prevent naming conflicts on your service name, even if it is defined inside a specific package. For example, a service named is likely to cause problems; something like would be better.\n\nRequest and response sizes should be bounded. We recommend a bound in the ballpark of 8 MiB, and 2 GiB is a hard limit at which many proto implementations break . Many storage systems have a limit on message sizes .\n• decrease resiliency by relying on a long-lived connection between a single client and a single server.\n\nHere are a few ways to bound all messages in an API:\n• Define RPCs that return bounded messages, where each RPC call is logically independent from the others.\n• Define RPCs that operate on a single object, instead of on an unbounded, client-specified list of objects.\n• Define a long-running operation . Store the result in a storage system designed for scalable, concurrent reads .\n• Use a pagination API (see Rarely define a pagination API without a continuation token).\n\nIf you are working on a UI, see also Create methods that return or manipulate small bits of data.\n\nRPC services should take care at RPC boundaries to interrogate errors, and return meaningful status errors to their callers.\n\nLet’s examine a toy example to illustrate the point:\n\nConsider a client that calls , which takes no arguments. As part of , might get all the products, and call for each product.\n\nIf is incorrectly implemented, it might send the wrong arguments to , resulting in an .\n\nIf carelessly returns errors to its callers, the client will receive , since status codes propagate across RPC boundaries. But, the client didn’t pass any arguments to . So, the error is worse than useless: it will cause a great deal of confusion!\n\nInstead, should interrogate errors it receives at the RPC boundary; that is, the RPC handler it implements. It should return meaningful errors to users: if it received invalid arguments from the caller, it should return . If something downstream received invalid arguments, it should convert the to before returning the error to the caller.\n\nCarelessly propagating status errors leads to confusion, which can be very expensive to debug. Worse, it can lead to an invisible outage where every service forwards a client error without causing any alerts to happen .\n\nThe general rule is: at RPC boundaries, take care to interrogate errors, and return meaningful status errors to callers, with appropriate status codes. To convey meaning, each RPC method should document what error codes it returns in which circumstances. The implementation of each method should conform to the documented API contract.\n\nCreate a unique request and response proto for each RPC method. Discovering later that you need to diverge the top-level request or response can be expensive. This includes “empty” responses; create a unique empty response proto rather than reusing the well-known Empty message type.\n\nTo reuse messages, create shared “domain” message types to include in multiple Request and Response protos. Write your application logic in terms of those types rather than the request and response types.\n\nThis gives you the flexibility to evolve your method request/response types independently, but share code for logical sub-units.\n\nWhen a repeated field is empty, the client can’t tell if the field just wasn’t populated by the server or if the backing data for the field is genuinely empty. In other words, there’s no method for repeated fields.\n\nWrapping a repeated field in a message is an easy way to get a hasFoo method.\n\nThe more holistic way to solve it is with a field read mask. If the field was requested, an empty list means there’s no data. If the field wasn’t requested the client should ignore the field in the response.\n\nThe worst way to update a repeated field is to force the client to supply a replacement list. The dangers with forcing the client to supply the entire array are manyfold. Clients that don’t preserve unknown fields cause data loss. Concurrent writes cause data loss. Even if those problems don’t apply, your clients will need to carefully read your documentation to know how the field is interpreted on the server side. Does an empty field mean the server won’t update it, or that the server will clear it?\n\nFix #1: Use a repeated update mask that permits the client to replace, delete, or insert elements into the array without supplying the entire array on a write.\n\nFix #3: Allow only appending or clearing. You can do this by wrapping the repeated field in a message. A present, but empty, message means clear, otherwise, any repeated elements mean append.\n\nTry to avoid order dependence in general. It’s an extra layer of fragility. An especially bad type of order dependence is parallel arrays. Parallel arrays make it more difficult for clients to interpret the results and make it unnatural to pass the two related fields around inside your own service.\n\nLeaking Features Because Your Proto is in a Mobile Build\n\nAndroid and iOS runtimes both support reflection. To do that, the unfiltered names of fields and messages are embedded in the application binary (APK, IPA) as strings.\n• ProGuard obfuscation on Android. As of Q3 2014. iOS has no obfuscation option: once you have the IPA on a desktop, piping it through will reveal field names of included protos. iOS Chrome tear-down\n• Curate precisely which fields are sent to mobile clients .\n• If plugging the leak isn’t feasible on an acceptable timescale, get buy-in from the feature owner to risk it.\n\nNever use this as an excuse to obfuscate the meaning of a field with a code-name. Either plug the leak or get buy-in to risk it.\n\nYou can trade type safety or clarity for performance wins in some cases. For example, a proto with hundreds of fields–particularly message-type fields–is going to be slower to parse than one with fewer fields. A very deeply-nested message can be slow to deserialize just from the memory management. A handful of techniques teams have used to speed deserialization:\n• Create a parallel, trimmed proto that mirrors the larger proto but has only some of the tags declared. Use this for parsing when you don’t need all the fields. Add tests to enforce that tag numbers continue to match as the trimmed proto accumulates numbering “holes.”\n• Annotate the fields as “lazily parsed” with [lazy=true].\n• Declare a field as bytes and document its type. Clients who care to parse the field can do so manually. The danger with this approach is there’s nothing preventing someone from putting a message of the wrong type in the bytes field. You should never do this with a proto that’s written to any logs, as it prevents the proto from being vetted for PII or scrubbed for policy or privacy reasons."
    },
    {
        "link": "https://en.cppreference.com/w/cpp/filesystem",
        "document": "The Filesystem library provides facilities for performing operations on file systems and their components, such as paths, regular files, and directories.\n\nThe filesystem library was originally developed as boost.filesystem, was published as the technical specification ISO/IEC TS 18822:2015, and finally merged to ISO C++ as of C++17. The boost implementation is currently available on more compilers and platforms than the C++17 library.\n\nThe filesystem library facilities may be unavailable if a hierarchical file system is not accessible to the implementation, or if it does not provide the necessary capabilities. Some features may not be available if they are not supported by the underlying file system (e.g. the FAT filesystem lacks symbolic links and forbids multiple hardlinks). In those cases, errors must be reported.\n\nThe behavior is undefined if the calls to functions in this library introduce a file system race, that is, when multiple threads, processes, or computers interleave access and modification to the same object in a file system.\n• file: a file system object that holds data, can be written to, read from, or both. Files have names, attributes, one of which is file type:\n• file name: a string of characters that names a file. Permissible characters, case sensitivity, maximum length, and the disallowed names are implementation-defined. Names (dot) and (dot-dot) have special meaning at library level.\n• path: sequence of elements that identifies a file. It begins with an optional (e.g. or on Windows), followed by an optional (e.g. on Unix), followed by a sequence of zero or more file names (all but last of which have to be directories or links to directories). The native format (e.g. which characters are used as separators) and character encoding of the string representation of a path (the pathname) is implementation-defined, this library provides portable representation of paths.\n\nUsing this library may require additional compiler/linker options. GNU implementation prior to 9.1 requires linking with and LLVM implementation prior to LLVM 9.0 requires linking with ."
    },
    {
        "link": "https://geeksforgeeks.org/file-system-library-in-cpp-17",
        "document": "In this article, we will learn about the File System library in C++17 and with examples. <filesystem> header was added in C++17 and introduces a set of classes, functions, and types that simplify file system operations. In simple words, we can say that the filesystem library provides tools that help us to simplify working with files and directories.\n\nIn earlier versions, performing file and directory operations was often a bulky and mistake-susceptible task as it required the use of platform-specific functions and libraries. The file system library was added to cope with these troubles, offering a portable and standardized way to paint with the file system.\n\nTo use the features of the file system library, we have to import <filesystem> header using #include preprocessor.\n\nAll the identifiers of <filesystem> headers are defined inside the std::filesystem namespace.\n\nThe following are some commonly used classes of file system libraries.\n\nIn this example, we will create a new file in a newly created directory. The parent directory looks like this before execution:\n\nIn this example, a directory is created named “mydirectory”. It checks if the directory exists and creates it if no longer. Sooner or later, a file named “my_file.Txt” is defined within this directory. The code then opens this file for writing using std::ofstream. If a success, it writes the text “Hello, FileSystem!” to the file and closes it. If any errors occur in the directory of the listing or file advent procedure, suitable error messages are displayed.\n\nIn this example, first, we define the directorypath to indicate the target directory. Then we wrote the condition to check if the directory exists or not using fs::exists() and fs::is_directory(). If it exists, it iterates through its contents using a range-based for loop with fs::directory_iterator(), and prints each item’s path to the standard output.\n\nIn this example, first, we define the path for the old file and the new file. Then, wrote a condition to check if the old file exists in the directory or not with fs::exists(), and if it is discovered, then rename it using fs::rename(). A success message will be displayed with the new file path. and if the old file isn’t found, it prints an error message.\n\nLet’s explore some of the key features provided by the <filesystem> library:\n• Path Manipulation: The file system library introduces the std::filesystem::path class to represent file system paths. This class encapsulates the platform-specific path representation and provides an easy way to manipulate and inspect paths.\n• File and Directory Operations: The file system library includes functions to perform common file and directory operations such as creating, removing, renaming, and checking for the existence of files and directories.\n• Error Handling: The <filesystem> library provides exceptions to handle errors during file system operations. You can catch exceptions like std::filesystem::filesystem_error to gracefully handle failures.\n• Portable Code: One of the main advantages of using <filesystem> is the portability it brings to your code. Since it abstracts platform-specific details, you can write code that works consistently across different operating systems."
    },
    {
        "link": "https://stackoverflow.com/questions/67273/how-do-you-iterate-through-every-file-directory-recursively-in-standard-c",
        "document": "How do you iterate through every file/directory recursively in standard C++?\n\nFrom C++17 onward, the header, and range- , you can simply do this: As of C++17, is part of the standard library and can be found in the header (no longer \"experimental\").\n\nIf using the Win32 API you can use the FindFirstFile and FindNextFile functions. For recursive traversal of directories you must inspect each WIN32_FIND_DATA.dwFileAttributes to check if the FILE_ATTRIBUTE_DIRECTORY bit is set. If the bit is set then you can recursively call the function with that directory. Alternatively you can use a stack for providing the same effect of a recursive call but avoiding stack overflow for very long path trees. #include <windows.h> #include <string> #include <vector> #include <stack> #include <iostream> using namespace std; bool ListFiles(wstring path, wstring mask, vector<wstring>& files) { HANDLE hFind = INVALID_HANDLE_VALUE; WIN32_FIND_DATA ffd; wstring spec; stack<wstring> directories; directories.push(path); files.clear(); while (!directories.empty()) { path = directories.top(); spec = path + L\"\\\\\" + mask; directories.pop(); hFind = FindFirstFile(spec.c_str(), &ffd); if (hFind == INVALID_HANDLE_VALUE) { return false; } do { if (wcscmp(ffd.cFileName, L\".\") != 0 && wcscmp(ffd.cFileName, L\"..\") != 0) { if (ffd.dwFileAttributes & FILE_ATTRIBUTE_DIRECTORY) { directories.push(path + L\"\\\\\" + ffd.cFileName); } else { files.push_back(path + L\"\\\\\" + ffd.cFileName); } } } while (FindNextFile(hFind, &ffd) != 0); if (GetLastError() != ERROR_NO_MORE_FILES) { FindClose(hFind); return false; } FindClose(hFind); hFind = INVALID_HANDLE_VALUE; } return true; } int main(int argc, char* argv[]) { vector<wstring> files; if (ListFiles(L\"F:\\\\cvsrepos\", L\"*\", files)) { for (vector<wstring>::iterator it = files.begin(); it != files.end(); ++it) { wcout << it->c_str() << endl; } } return 0; }\n\nIn addition to the above mentioned boost::filesystem you may want to examine wxWidgets::wxDir and Qt::QDir. Both wxWidgets and Qt are open source, cross platform C++ frameworks. provides a flexible way to traverse files recursively using or a simpler function. As well you can implement the traversal with and functions (I assume that Traverse() and GetAllFiles() are wrappers that eventually use GetFirst() and GetNext() functions). provides access to directory structures and their contents. There are several ways to traverse directories with QDir. You can iterate over the directory contents (including sub-directories) with QDirIterator that was instantiated with QDirIterator::Subdirectories flag. Another way is to use QDir's GetEntryList() function and implement a recursive traversal. Here is sample code (taken from here # Example 8-5) that shows how to iterate over all sub directories.\n\nWe are in 2019. We have filesystem standard library in . The provides facilities for performing operations on file systems and their components, such as paths, regular files, and directories. There is an important note on this link if you are considering portability issues. It says: The filesystem library facilities may be unavailable if a hierarchical file system is not accessible to the implementation, or if it does not provide the necessary capabilities. Some features may not be available if they are not supported by the underlying file system (e.g. the FAT filesystem lacks symbolic links and forbids multiple hardlinks). In those cases, errors must be reported. The filesystem library was originally developed as , was published as the technical specification ISO/IEC TS 18822:2015, and finally merged to ISO C++ as of C++17. The boost implementation is currently available on more compilers and platforms than the C++17 library. @adi-shavit has answered this question when it was part of std::experimental and he has updated this answer in 2017. I want to give more details about the library and show more detailed example. std::filesystem::recursive_directory_iterator is an that iterates over the directory_entry elements of a directory, and, recursively, over the entries of all subdirectories. The iteration order is unspecified, except that each directory entry is visited only once. If you don't want to recursively iterate over the entries of subdirectories, then directory_iterator should be used. Both iterators returns an object of directory_entry. has various useful member functions like , , , etc. The member function returns an object of std::filesystem::path and it can be used to get , , . Consider the example below. I have been using and compiled it over the terminal using\n\nYou would probably be best with either boost or c++14's experimental filesystem stuff. IF you are parsing an internal directory (ie. used for your program to store data after the program was closed), then make an index file that has an index of the file contents. By the way, you probably would need to use boost in the future, so if you don't have it installed, install it! Second of all, you could use a conditional compilation, e.g.: #ifdef WINDOWS //define WINDOWS in your code to compile for windows #endif The code for each case is taken from https://stackoverflow.com/a/67336/7077165 #ifdef POSIX //unix, linux, etc. #include <stdio.h> #include <dirent.h> int listdir(const char *path) { struct dirent *entry; DIR *dp; dp = opendir(path); if (dp == NULL) { perror(\"opendir: Path does not exist or could not be read.\"); return -1; } while ((entry = readdir(dp))) puts(entry->d_name); closedir(dp); return 0; } #endif #ifdef WINDOWS #include <windows.h> #include <string> #include <vector> #include <stack> #include <iostream> using namespace std; bool ListFiles(wstring path, wstring mask, vector<wstring>& files) { HANDLE hFind = INVALID_HANDLE_VALUE; WIN32_FIND_DATA ffd; wstring spec; stack<wstring> directories; directories.push(path); files.clear(); while (!directories.empty()) { path = directories.top(); spec = path + L\"\\\\\" + mask; directories.pop(); hFind = FindFirstFile(spec.c_str(), &ffd); if (hFind == INVALID_HANDLE_VALUE) { return false; } do { if (wcscmp(ffd.cFileName, L\".\") != 0 && wcscmp(ffd.cFileName, L\"..\") != 0) { if (ffd.dwFileAttributes & FILE_ATTRIBUTE_DIRECTORY) { directories.push(path + L\"\\\\\" + ffd.cFileName); } else { files.push_back(path + L\"\\\\\" + ffd.cFileName); } } } while (FindNextFile(hFind, &ffd) != 0); if (GetLastError() != ERROR_NO_MORE_FILES) { FindClose(hFind); return false; } FindClose(hFind); hFind = INVALID_HANDLE_VALUE; } return true; } #endif //so on and so forth.\n\nYou don't. The C++ standard has no concept of directories. It is up to the implementation to turn a string into a file handle. The contents of that string and what it maps to is OS dependent. Keep in mind that C++ can be used to write that OS, so it gets used at a level where asking how to iterate through a directory is not yet defined (because you are writing the directory management code). Look at your OS API documentation for how to do this. If you need to be portable, you will have to have a bunch of #ifdefs for various OSes.\n\nFile tree walk is a recursive way to wall the whole directory tree in the path. More details are here. NOTE : You can also use that can skip hidden files like or or #include <ftw.h> #include <stdio.h> #include <sys/stat.h> #include <string.h> int list(const char *name, const struct stat *status, int type) { if (type == FTW_NS) { return 0; } if (type == FTW_F) { printf(\"0%3o\\t%s\n\n\", status->st_mode&0777, name); } if (type == FTW_D && strcmp(\".\", name) != 0) { printf(\"0%3o\\t%s/\n\n\", status->st_mode&0777, name); } return 0; } int main(int argc, char *argv[]) { if(argc == 1) { ftw(\".\", list, 1); } else { ftw(argv[1], list, 1); } return 0; } output looks like following: Let us say if you want to match a filename (example: searching for all the files.) for a specific needs, use . #include <ftw.h> #include <stdio.h> #include <sys/stat.h> #include <iostream> #include <fnmatch.h> static const char *filters[] = { \"*.jpg\", \"*.jpeg\", \"*.png\" }; int list(const char *name, const struct stat *status, int type) { if (type == FTW_NS) { return 0; } if (type == FTW_F) { int i; for (i = 0; i < sizeof(filters) / sizeof(filters[0]); i++) { /* if the filename matches the filter, */ if (fnmatch(filters[i], name, FNM_CASEFOLD) == 0) { printf(\"0%3o\\t%s\n\n\", status->st_mode&0777, name); break; } } } if (type == FTW_D && strcmp(\".\", name) != 0) { //printf(\"0%3o\\t%s/\n\n\", status->st_mode&0777, name); } return 0; } int main(int argc, char *argv[]) { if(argc == 1) { ftw(\".\", list, 1); } else { ftw(argv[1], list, 1); } return 0; }"
    },
    {
        "link": "https://cppstories.com/2019/01/cpp17indetail-update5",
        "document": "On Friday 18th January I’ve pushed another update for the book. This time I rewrote the whole chapter about . Please have a look at what changed and what are the plans.\n\nThe book got more than 25 new pages!\n\nThe book had a filesystem chapter from the start, but it was concise and didn’t contain much information. From the beginning, the plan was to rewrite it, similarly as with the parallel algorithms chapter.\n\nI hoped to do the work in just a few weeks… maybe even before December. But as in other software development related projects, it’s better to multiply the first estimate by 2…3 :)\n\nThat’s why I released a new chapter - CSV reader - before this one. By working on a real project, I could learn more and experiment. I needed that to be able to deliver better content.\n\nThe refreshed chapter is now 5x larger than the first version! The whole book contains now 306 pages (56 more than I initially planned :))\n\nTo sum up, with this refreshed chapter you’ll see:\n• How got into the Standard\n• What the basic types and operations are\n• How you can work with the paths\n• How to handle errors in\n• How to iterate over a directory\n• How to create new directories and files\n\nHere’s the link to the book:\n\nHere’s an example, where you can pass a path and then use filter and match the file names.\n\nAnd to hold the info about the files the code uses the following helper structure:\n\nThe code iterates over a directory with and then filters out only regular files. Later, the function transforms that vector of paths into a vector of objects.\n\nWhen all files are collected the function uses to do the matching.\n\nAs a possible optimisation, we can also create a vector of directory_entries rather than paths. This would allow us to fetch the files size faster, as is usually cached and needs another file access.\n\nI’m happy to announce that thanks to the collaboration with the team @Educative we published C++17 in Detail as an interactive course!\n\n You can see it… and even preview it for free here:\n\n >> C++17 in Detail: A Deep Dive\n\nIt consists of 200 lessons, many quizzes, code snippets… and what’s best is that it has more than 120 playgrounds! That means you can compile and edit code sample directly in the browser… so there’s no need for you to switch back and forth to some compiler/IDE.\n\n I think that such approach increases your learning experience.\n\nAnd for those of you who are interesting in that form of learning, you can use this coupon:\n\nUse this coupon to buy the course at a much lower price!\n\nSpecial thanks to JFT, Jacek Galowicz, Michał Czaja, and other reviewers who contributed to the chapter!\n\nSo far the book was mentioned in several places.\n• The book is listed in one of the articles from the Visual C++ Team:\n\n Books on C++17 | Visual C++ Team Blog\n• There’s a review at CppDepend blog:\n\n C++ 17 In Detail Book Review – CppDepend Blog (including a little discount)\n• And there’s also a GoodReads page:\n\n C++17 in Detail @GoodReads\n\nThe remaining parts are related mostly to book polishing and smaller fixes!\n\nFor example this week I plan to release a small update for the chapter (adding notes about handling file permissions).\n\nI appreciate your initial feedback and support! The book has now more than 860 readers (and only six refunds)! That’s not too bad I think :)\n\nLet me know what’s your experience with the book. What would you like to change? What would you like to see more?\n\nYou can use this comment site:\n\n https://leanpub.com/cpp17indetail/feedback"
    },
    {
        "link": "https://github.com/gulrak/filesystem",
        "document": "\n• Filesystem\n• Usage\n• Using it as Single-File-Header\n• Using it as Forwarding-/Implementation-Header\n• Differences\n• Not Implemented on C++ before C++17\n\nThis is a header-only single-file compatible helper library, based on the C++17 and C++20 specs, but implemented for C++11, C++14, C++17 or C++20 (tightly following the C++17 standard with very few documented exceptions). It is currently tested on macOS 10.12/10.14/10.15/11.6, Windows 10, Ubuntu 18.04, Ubuntu 20.04, CentOS 7, CentOS 8, FreeBSD 12, Alpine ARM/ARM64 Linux and Solaris 10 but should work on other systems too, as long as you have at least a C++11 compatible compiler. It should work with Android NDK, Emscripten and I even had reports of it being used on iOS (within sandboxing constraints) and with v1.5.6 there is experimental support for QNX. The support of Android NDK, Emscripten, QNX, and since 1.5.14 GNU/Hurd and Haiku is not backed up by automated testing but PRs and bug reports are welcome for those too and they are reported to work. It is of course in its own namespace to not interfere with a regular should you use it in a mixed C++17 environment (which is possible).\n\nTest coverage is well above 90%, and starting with v1.3.6 and in v1.5.0 more time was invested in benchmarking and optimizing parts of the library. I'll try to continue to optimize some parts and refactor others, striving to improve it as long as it doesn't introduce additional C++17/C++20 compatibility issues. Feedback is always welcome. Simply open an issue if you see something missing or wrong or not behaving as expected and I'll comment.\n\nI'm often in need of filesystem functionality, mostly , but directory access too, and when beginning to use C++11, I used that language update to try to reduce my third-party dependencies. I could drop most of what I used, but still missed some stuff that I started implementing for the fun of it. Originally I based these helpers on my own coding- and naming conventions. When C++17 was finalized, I wanted to use that interface, but it took a while, to push myself to convert my classes.\n\nThe implementation is closely based on chapter 30.10 from the C++17 standard and a draft close to that version is Working Draft N4687. It is from after the standardization of C++17 but it contains the latest filesystem interface changes compared to the Working Draft N4659. Staring with v1.4.0, when compiled using C++20, it adapts to the changes according to path sorting order and handling from Working Draft N4860.\n\nI want to thank the people working on improving C++, I really liked how the language evolved with C++11 and the following standards. Keep on the good work!\n\nIf you ask yourself, what is standing for, it is simply , yeah, I know, not very imaginative, but I wanted a short namespace and I use it in some of my private classes (so it has nothing to do with Haskell, sorry for the name clash).\n\nis developed on macOS but CI tested on macOS, Windows, various Linux Distributions, FreeBSD and starting with v1.5.12 on Solaris. It should work on any of these with a C++11-capable compiler. Also there are some checks to hopefully better work on Android, but as I currently don't test with the Android NDK, I wouldn't call it a supported platform yet, same is valid for using it with Emscripten. It is now part of the detected platforms, I fixed the obvious issues and ran some tests with it, so it should be fine. All in all, I don't see it replacing where full C++17 or C++20 is available, it doesn't try to be a \"better\" , just an almost drop-in if you can't use it (with the exception of the UTF-8 preference).\n\nℹ️ Important: This implementation is following the \"UTF-8 Everywhere\" philosophy in that all instances will be interpreted the same as encoding wise and as being in UTF-8. The will be seen as UTF-16. See Differences in API for more information.\n\nUnit tests are currently run with:\n\nThe header comes with a set of unit-tests and uses CMake as a build tool and Catch2 as test framework. All tests are registered with in CMake, so the ctest commando can be used to run the tests.\n\nAll tests against this implementation should succeed, depending on your environment it might be that there are some warnings, e.g. if you have no rights to create Symlinks on Windows or at least the test thinks so, but these are just informative.\n\nTo build the tests from inside the project directory under macOS or Linux just:\n\nThis generates the test binaries that run the tests and the last command executes them.\n\nIf the default compiler is a GCC 8 or newer, or Clang 7 or newer, it additionally tries to build a version of the test binary compiled against GCCs/Clangs implementation, named as an additional test of conformance. Ideally all tests should compile and succeed with all filesystem implementations, but in reality, there are some differences in behavior, sometimes due to room for interpretation in in the standard, and there might be issues in these implementations too.\n\nThe latest release version is v1.5.14 and source archives can be found here.\n\nThe latest pre-native-backend version is v1.4.0 and source archives can be found here.\n\nThe latest pre-C++20-support release version is v1.3.10 and source archives can be found here.\n\nCurrently only the latest minor release version receives bugfixes, so if possible, you should use the latest release.\n\nAs is at first a header-only library, it should be enough to copy the header or the directory into your project folder or point your include path to this place and simply include the header (or if you use the subdirectory).\n\nEverything is in the namespace , so one way to use it only as a fallback could be:\n\nIf you want to also use the wrapper with support as fallback, you might use:\n\nNow you have e.g. and it is either the wrapper or the C++17 .\n\nℹ️ Be aware, as a header-only library, it is not hiding the fact, that it uses system includes, so they \"pollute\" your global namespace. Use the forwarding-/implementation-header based approach (see below) to avoid this. For Windows it needs and it might be a good idea to define or prior to including or headers to reduce pollution of your global namespace and compile time. They are not defined by to allow combination with contexts where the full is needed, e.g. for UI elements.\n\nℹ️ Hint: There is an additional header named that implements this dynamic selection of a filesystem implementation, that you can include instead of when you want where available and where not.\n\nAlternatively, starting from v1.1.0 can also be used by including one of two additional wrapper headers. These allow to include a forwarded version in most places ( ) while hiding the implementation details in a single cpp file that includes to implement the needed code. Using this way makes sure system includes are only visible from inside the cpp file, all other places are clean.\n\nBe aware, that it is currently not supported to hide the implementation into a Windows-DLL, as a DLL interface with C++ standard templates in interfaces is a different beast. If someone is willing to give it a try, I might integrate a PR but currently working on that myself is not a priority.\n\nIf you use the forwarding/implementation approach, you can still use the dynamic switching like this:\n\nand in the implementation hiding cpp, you might use (before any include that includes to take precedence:\n\nℹ️ Hint: There are additional helper headers, named and that use this technique, so you can simply include them if you want to dynamically select the filesystem implementation.\n\nStarting from v1.1.0, it is possible to add as a git submodule, add the directory to your with and then simply use to ensure correct include path that allow to work.\n\nThe offers a few options to customize its behavior:\n• - Compile tests, default is when used as a submodule, else .\n• - Compile the examples, default is when used as a submodule, else .\n• - Add install target to build, default is when used as a submodule, else .\n• - Compile , the variant of the test suite running against , defaulting to . This is only done if the compiler is detected as being able to do it.\n• can be set to a list of features to override when the detection of C++17 or C++20 for additional tests is not working (e.g. to enforce building a with C++20).\n\nPlease use hedronvision/bazel-cc-filesystem-backport, which will automatically set everything up for you.\n\nThere is a version macro defined in case future changes might make it needed to react on the version, but I don't plan to break anything. It's the version as decimal number .\n\nℹ️ Note: Only even patch versions will be used for releases and odd patch version will only be used for in between commits while working on the next version.\n\nThere is almost no documentation in this release, as any documentation would work, besides the few differences explained in the next section. So you might head over to https://en.cppreference.com/w/cpp/filesystem for a description of the components of this library.\n\nWhen compiling with C++11, C++14 or C++17, the API is following the C++17 standard, where possible, with the exception that parameters are only supported on C++17. When Compiling with C++20, defaults to the C++20 API, with the and interfaces and the deprecated factory method.\n\nℹ️ Note: If the C++17 API should be enforced even in C++20 mode, use the define . Even then it is possible to create from but and return normal UTF-8 encoded instances, so code written for C++17 could still work with when compiled with C++20.\n\nThe only additions to the standard are documented here:\n\nThese are simple wrappers around , and . They simply add an method and a constructor with an argument as the variants in C++17 have them.\n\nThis is a helper class that currently checks for UTF-8 encoding on non-Windows platforms but on Windows it fetches the command line arguments as Unicode strings from the OS with\n\nand then converts them to UTF-8, and replaces and . It is a guard-like class that reverts its changes when going out of scope.\n\nThat way is UTF-8 encoded as long as the scope from is valid.\n\nNote: On macOS, while debugging under Xcode the code currently will return as Xcode starts the application with as encoding, no matter what encoding is actually used and even setting in the product scheme doesn't change anything. I still need to investigate this.\n\nAs this implementation is based on existing code from my private helper classes, it derived some constraints of it. Starting from v1.5.0 most of the differences between this and the standard C++17/C++20 API where removed.\n\nThis implementation has switchable behavior for the LWG defects #2682, #2935, #2936 and #2937. The currently selected behavior (starting from v1.4.0) is following #2682, #2936, #2937 but not following #2935, as I feel it is a bug to report no error on a or where a regular file of the same name prohibits the creation of a directory and forces the user of those functions to double-check via if it really worked. The more intuitive approach to directory creation of treating a file with that name as an error is also advocated by the newer paper WG21 P1164R0, the revision P1161R1 was agreed upon on Kona 2019 meeting see merge and GCC by now switched to following its proposal (GCC #86910).\n\nThese are not implemented under C++11 and C++14, as there is no available and I did want to keep this implementation self-contained and not write a full C++17-upgrade for C++11/14. Starting with v1.1.0 these are supported when compiling under C++17 of C++20.\n\nStarting with v1.5.2 will try to allow the use of where it detects is availability. Additionally if you have a compatible c++11 implementation it can be used instead of by defining and importing the implementation into the namespace with:\n\nTo not depend on any external third party libraries and still stay portable and compact, this implementation is following the \"UTF-8 Everywhere\" philosophy in that all instances will be interpreted the same as encoding wise and as being in UTF-8. The will be seen as UTF-16 and will be seen as Unicode codepoints. Depending on the size of characters, it will handle as being UTF-16 (e.g. Windows) or Unicode codepoints (currently all other platforms).\n\nStarting with v1.5.0 is following the C++17 standard in using and on Windows as the types internally used for path representation. It is still possible to get the old behavior by defining and get as and as .\n\nIf you need to call some Windows API, with v1.5.0 and above, simply use the W-variant of the Windows-API call (e.g. ).\n\nℹ️ Note: When using the old behavior by defining , use the member (e.g. ). This gives you the Unicode variant independent of the macro and makes sharing code between Windows, Linux and macOS easier and works with and .\n\nThe return type of these two methods is depending on the used C++ standard and if is defined. On C++11, C++14 and C++17 or when is defined, the return type is , and on C++20 without the define it is .\n\nI created a wiki entry about quite a lot of behavioral differences between different implementations that could result in a mention here, but this readme only tries to address the design choice differences between and those. I try to update the wiki page from time to time.\n\nAny additional observations are welcome!\n\nSince v1.5.0 the complete inner mechanics of this implementations where changed to the native format as the internal representation. Creating any mixed slash object under Windows (e.g. with ) will lead clean path with via and via API. On all platforms redundant additional separators are removed, even if this is not enforced by the standard and other implementations mostly not do this.\n\nAdditionally this implementation follows the standards suggestion to handle posix paths of the form and USC path on windows also as having a root-name (e.g. ). The GCC implementation didn't choose to do that while testing on Ubuntu 18.04 and macOS with GCC 8.1.0 or Clang 7.0.0. This difference will show as warnings under . This leads to a change in the algorithm described in the standard for where any path with will degrade to an assignment, while this implementation has the exception where and a normal append will be done, to allow:\n\nFor all non-host-leading paths the behavior will match the one described by the standard.\n\nAs symbolic links on Windows, while being supported more or less since Windows Vista (with some strict security constraints) and fully since some earlier build of Windows 10, when \"Developer Mode\" is activated, are at time of writing (2018) rarely used, still they are supported wiit th this implementation.\n\nThe Windows ACL permission feature translates badly to the POSIX permission bit mask used in the interface of C++17 filesystem. The permissions returned in the are therefore currently synthesized for the -level and copied to the - and -level. There is still some potential for more interaction with the Windows permission system, but currently setting or reading permissions with this implementation will most certainly not lead to the expected behavior.\n• Fix for #166, did return non empty result for the directory name\n• Pull request #162, fix for directory iterator treating all files subsequent to a symlink as symlink on Windows\n• Pull request #161, the CMake alias is now set unconditionally\n• Fix for #160, the cmake config now only sets install targets by default if the project is no subproject, as documented\n• Fix for #157, suppress C4191 warning on MSVC for GetProcAddress casts\n• Fix for #156, on POSIX , and of would return wrong result if a colon was in the filename\n• Fix for #151, now, consistently with will not throw on symlinks to non-existing targets, but make the entry have as the type\n• Pull request #149, add version to CMake project and export it\n• Fix for #146, handle on POSIX directory iteration and file copy to avoid errors on network filesystems\n• Pull request #145, fix for Y2038 bug in timeToFILETIME on Windows\n• Pull request #144, now also copies the permissions\n• Fix for #142, removed need for on systems that don't support and fixed build configuration and tests to support Solaris as new platform.\n• Pull request #138, if the platform uses the POSIX backend and has no , one is defined.\n• Pull request #136, the Windows implementation used some unnecessary expensive shared pointer for resource management and these where replaced by a dedicated code.\n• Fix for #132, pull request #135, now just deletes symbolic links instead of following them.\n• Pull request #133, fix for where a numerical overflow could happen in a multiplication.\n• Replaced travis-ci.org with GitHub Workflow for the configurations: Ubuntu 20.04: GCC 9.3, Ubuntu 18.04: GCC 7.5, GCC 8.4, macOS 10.15: Xcode 12.4, Windows 10: Visual Studio 2019\n• Fix for #125, where on Windows no longer breaks on long filenames.\n• Fix for #124, treated mounted folder/volumes erroneously as symlinks, leading to fail on paths containing those.\n• Fix for #122, incrementing the will not try to enter dead symlinks.\n• Fix for #121, on Windows backend the failed when the path pointed to a read-only entry, see also (microsoft/STL#1511) for the corresponding issue in on windows.\n• Fix for #119, added missing support for char16_t and char32_t and on C++20 char8_t literals.\n• Pull request #118, when running tests as root, disable tests that would not work.\n• Pull request #117, added checks to tests to detect the clang/libstdc++ combination.\n• Fix for #116, internal macro allows os detection to support systems without the member, experimental first QNX compile support as initial use case, fixed issue with filesystems returning DT_UNKNOWN (e.g. reiserfs).\n• Pull request #115, added support when clang with libstdc++ is detected.\n• Fix for #114, for macOS the pre-Catalina deployment target detection worked only if was included before or / .\n• Fix for #113, the use of standard chapter numbers was misleading since C++17 and C++20 features are supported, and was replaced by the tag-like chapter names that stay (mostly) consistent over the versions.\n• Pull request #112, lots of cleanup work on the readme, thanks!\n• Enhancement for #111, further optimization of directory iteration, performance for over large trees now somewhere between libc++ and libstdc++.\n• Enhancement for #110, now has preliminary support for Cygwin. Changes where made to allow the tests to compile and run successfully (tested with GCC 10.2.0), feedback and additional PRs welcome as it is currently not part of the CI configuration.\n• Pull request #109, various spelling errors in error messages and comments fixed.\n• Fix for #107, the error handling for status calls was suppressing errors on symlink targets.\n• Pull request #105, added option to override additional build of versions of the tests for comparison and the possibility to use to prefill the used compile features defaulting to when not given.\n• Enhancement #104, on POSIX backend: optimized reuse of status information and reduced creation leads to about 20%-25% in tests with over a larger directory tree.\n• Pull request #103, was not in the list of supported char types on non-Windows backends.\n• Pull request #102, improved support makes use of or when available, and allows use of custom implementation when defining and importing the string view into the namespace before including filesystem header.\n• Pull request #101, fix for #100, append and concat type of operations on path called redundant conversions.\n• Pull request #98, on older linux variants (GCC 7/8), the comparison tests now link with to avoid issues.\n• Fix for #97, on BTRFS the test case for failed due to the filesystems behavior, the test case was adapted to take that into account.\n• Pull request #96, the export attribute defines and are now honored when when set from outside to allow override of behavior.\n• Fix for #95, the syntax for disabling the deprecated warning in tests in MSVC was wrong.\n• Pull request #93, now the CMake configuration file is configured and part of the files.\n• Fix for #91, the way the CMake build options , and where implemented, prohibited setting them from a parent project when using this via , this fix allows to set them again.\n• Major refactoring for #90, the way, the Windows version of was originally created from the POSIX based implementation was, by adaption of the incoming and outgoing strings. This resulted in a mutable cache inside on Windows, that was inherently not thread-safe, even for methods. To not add additional patches to a suboptimal solution, this time I reworked the code to now store native path-representation. This changed a lot of code, but when combined with as helped to avoid lots of conversion for calls to Win-API.\n\n As interfaces where changed, it had to be released in a new minor version. The set of refactorings resulted in the following changes:\n• and can now be as the standard mandates\n• On Windows is now the default for and is the default for .\n• This allows the implementation to call Win-API without allocating conversions\n• Thread-safety on methods of is no longer an issue\n• Some code could be simplified during this refactoring\n• Automatic prefixing of long path on Windows can now be disabled with defining , for all other types of prefixes or namespaces the behavior follows that of MSVC\n• In case the old / based approach for Windows is still needed, it can be activated with\n• Enhancement for #89, now supports introduced in with C++20.\n• Refactoring for #88, had a performance issue, as it was still using a loop based approach to recreate the parent from elements. This created lots of temporaries and was too slow especially on long paths.\n• Enhancements for #71, when compiled with C++20:\n• and are supported where is the parameter type\n• The spaceship operator is now supported for\n• With the define will fall back to the old and API if preferred\n• Bugfix for where the internal call to was not using the variant, throwing possible exceptions instead of setting .\n• Enhancement is now on by default.\n• Some cleanup work to reduce preprocessor directives for better readability and remove unneeded template specializations.\n• Fix for #81, fixed issues with handling parameters that are string views.\n• Fix for #79, the bit operations for filesystem bitmasks that should be are now .\n• Refactoring for #78, the dynamic switching helper includes are now using to ensure that is only selected on macOS if the deployment target is at least Catalina.\n• Bugfix for #77, the and the had an issue with the option, that leads to the inability to skip SIP protected folders on macOS.\n• Enhancement for #76, is now used when available, additionally to , in the helping headers to allow them to work even when is not used.\n• Bugfix for #75, NTFS reparse points to mapped volumes where handled incorrect, leading to on or not-found-errors on . Namespaced paths are not filtered anymore.\n• Pull request #74, on Windows symlink evaluation used the wrong reparse struct information and was not handling the case of relative paths well, thanks for the contribution.\n• Refactoring for #73, enhanced performance in path handling. the changes lead to much fewer path/string creations or copies, speeding up large directory iteration or operations on many path instances.\n• Bugfix for #72, the in was completed to fulfill the requirements to build on CentOS 7 with . CentOS 7 and CentOS 8 are now part of the CI builds.\n• Bugfix for #70, root names are now case insensitive on Windows. This fix also adds the new behavior switch that allows to enable post C++17 behavior, where the comparison is as if it was an element wise path comparison as described in LWG 2936 and C++20 . It is default off in v1.3.6 and will be default starting from v1.4.0 as it changes ordering.\n• Pull request #69, use versions of from wrappers on Windows if using GCC with libc++.\n• Bugfix for #68, better handling of permission issues for directory iterators when using and initial support for compilation with emscripten.\n• Refactoring for #66, unneeded shared_ptr guards where removed and the file handles closed where needed to avoid unnecessary allocations.\n• Bugfix for #63, fixed issues on Windows with clang++ and C++17.\n• Pull request #62, various fixes for better Android support, thanks for the PR\n• Pull request #61, now supports use in projects with disabled exceptions. API signatures using exceptions for error handling are not available in this mode, thanks for the PR (this resolves #60 and #43)\n• Bugfix for #58, on MinGW the compilation could fail with an error about an undefined constant.\n• Bugfix for #56, didn't ignore trailing slash on the base parameter, thanks for PR #57.\n• Bugfix for #55, returned when nothing needed to be created, because the directory already existed.\n• Bugfix for #54, was not reset, if cached result was returned.\n• Pull request #53, fix for wrong handling of leading whitespace when reading from a stream.\n• Pull request #52, an ARM Linux target is now part of the CI infrastructure with the service of Drone CI.\n• Pull request #51, FreeBSD is now part of the CI infrastructure with the service of Cirrus CI.\n• Important: is re-licensed from BSD-3-Clause to MIT license. (see #47)\n• Bugfix for #44, fixes for warnings from newer Xcode versions.\n• The Visual Studio 2019 compiler, GCC 9.2 and Clang 9.0 where added to the CI configuration.\n• Bugfix for #41, on Windows didn't replace an existing regular file as required by the standard, but gave an error. New tests and a fix as provided in the issue was implemented.\n• Bugfix for #39, for the forwarding use via or there was a use of in the forwarding part leading to an error if was not included before the header. The tests were changed to give an error in that case too and the useage of was removed.\n• Bugfix for #38, casting the return value of gave a warning with on MSYS2 and MinGW GCC 9 builds.\n• Pull request #30, the will automatically exclude building examples and tests when used as submodule, the configuration options now use a prefixed name to reduce risk of conflicts.\n• Pull request #24, install target now creates a in for that exports a target as .\n• Pull request #34, fixes TOCTOU situation on , thanks for the PR!\n• Feature #35, new CMake option to add an install target that is defaulted to OFF if is used via .\n• Bugfix for #33, fixes an issue with that leaves a trailing separator in case of a resulting path ending with as last element.\n• Bugfix for #36, warnings on Xcode 11.2 due to unhelpful references in path element iteration.\n• Pull request #23, tests and examples can now be disabled in CMake via setting and to , or .\n• Pull request #25, missing specialization for construction from when available was added.\n• Additional test case when is available.\n• Bugfix for #27, the declaration was not compiling on pre C++17 compilers and no test accessed it, to show the problem. Fixed it to an construction C++11 compiler should accept and added a test that is successful on all combinations tested.\n• Bugfix for #29, stricter warning settings where chosen and resulting warnings where fixed.\n• Enabled stronger warning switches and resulting fixed issues on GCC and MinGW\n• Bugfix for #22, the where not forwarded from to in one of the cases.\n• Fix for (#21), when compiling on Alpine Linux with musl instead of glibc, the wrong signature was expected. The complex preprocessor define mix was dropped in favor of the usual dispatch by overloading a unifying wrapper.\n• Added MinGW 32/64 and Visual Studio 2015 builds to the CI configuration.\n• Pull request (#13), set minimum required CMake version to 3.7.2 (as in Debian 8).\n• Bugfix for (#15), the forward/impl way of using missed a include in the windows case.\n• Bugfix for (#16), VS2019 didn't like the old size dispatching in the utf8 decoder, so it was changed to a sfinae based approach.\n• New feature (#17), optional support for standard conforming interface when compiling on Windows with defined , this is default when using the header, to enhance compatibility.\n• New feature (#18), optional filesystem exceptions/errors on Unicode errors with defined (instead of replacing invalid code points or UTF-8 encoding errors with the replacement character ).\n• Additional Bugfix for (#12), error in old unified code of ; as is now deprecated, I decided to drop it and the resulting code is much easier, shorter and due to more refactoring faster\n• Travis-CI now additionally test with Xcode 10.2 on macOS\n• Bugfix for (#11), had some issues with -sequences.\n• Bugfix for (#12), could run into endless loops, the methods depth() and pop() had issues and the copy behavior and conformance was broken, added tests\n• Restructured some CMake code into a macro to ease the support for C++17 builds of tests and examples for interoperability checks.\n• Some fixes on Windows tests to ease interoperability test runs.\n• Added simple example showing the used to add the sizes of files in a directory tree.\n• now conforms LWG #2682, disallowing the use of `copy_option::create_symlinks' to be used on directories\n• Restructuring of the project directory. The header files are now using as extension to be marked as c++ and they where moved to to be able to include by as the former include name might have been to generic and conflict with other files.\n• Better CMake support: now can be used as a submodul and added with and will export itself as target. To use it, only is needed and the include directories will be set so will be a valid directive. Still you can simply only add the header file to you project and include it from there.\n• Enhancement (#10), support for separation of implementation and forwarded api: Two additional simple includes are added, that can be used to forward declarations ( ) and to wrap the implementation into a single cpp ( )\n• The variants of the api are now supported when compiling with C++17.\n• Added checks to hopefully better compile against Android NDK. There where no tests run yet, so feedback is needed to actually call this supported.\n• was renamed to better reflect that it is a c++ language header.\n• Bugfix for (#6), where and both are now able to remove a single file and both will not raise an error if the path doesn't exist.\n• Merged pull request (#7), a typo leading to setting error code instead of comparing it in under Windows.\n• Bugfix for ((#8), the Windows version of now releases resources when reaching like the POSIX one does.\n• Bugfix for (#4), missing error_code propagation in and fixed.\n• Bugfix for (#3), fixed missing inlines and added test to ensure including into multiple implementation files works as expected.\n• Refactored test to work with all tested implementations (gcc, clang, msvc++).\n• Added helper class as converter, to help follow the UTF-8 path on windows. Simply instantiate it with and and it will fetch the Unicode version of the command line and convert it to UTF-8. The destructor reverts the change.\n• Added folder with hopefully some usefull example usage. Examples are tested (and build) with and C++17 when available.\n• Starting with this version, only even patch level versions will be tagged and odd patch levels mark in-between non-stable wip states.\n• Tests can now also be run against MS version of for comparison.\n• Fixed some integer type mismatches that could lead to warnings.\n• Fixed conversion issues in test and example on clang 7.0.0.\n• Bugfix: now sees empty path as non-existant and reports an error. Due to this now returns relative paths for non-existant argument paths. (#1)\n• Bugfix: tests didn't respect equality domain issues and dereferencapable constraints, leading to fails on tests.\n• Bugfix: Some tagged methods and functions could indirectly throw exceptions due to UFT-8 decoding issues.\n• is now also generated if LLVM/clang 7.0.0 is found.\n\nThis was the first public release version. It implements the full range of C++17 , as far as possible without other C++17 dependencies."
    }
]