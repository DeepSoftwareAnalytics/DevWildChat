[
    {
        "link": "https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html",
        "document": "\n• Learn to capture video from a camera and display it.\n• You will learn these functions : cv.VideoCapture(), cv.VideoWriter()\n\nOften, we have to capture live stream with a camera. OpenCV provides a very simple interface to do this. Let's capture a video from the camera (I am using the built-in webcam on my laptop), convert it into grayscale video and display it. Just a simple task to get started.\n\nTo capture a video, you need to create a VideoCapture object. Its argument can be either the device index or the name of a video file. A device index is just the number to specify which camera. Normally one camera will be connected (as in my case). So I simply pass 0 (or -1). You can select the second camera by passing 1 and so on. After that, you can capture frame-by-frame. But at the end, don't forget to release the capture.\n\nreturns a bool ( / ). If the frame is read correctly, it will be . So you can check for the end of the video by checking this returned value.\n\nSometimes, cap may not have initialized the capture. In that case, this code shows an error. You can check whether it is initialized or not by the method cap.isOpened(). If it is , OK. Otherwise open it using cap.open().\n\nYou can also access some of the features of this video using cap.get(propId) method where propId is a number from 0 to 18. Each number denotes a property of the video (if it is applicable to that video). Full details can be seen here: cv::VideoCapture::get(). Some of these values can be modified using cap.set(propId, value). Value is the new value you want.\n\nFor example, I can check the frame width and height by and . It gives me 640x480 by default. But I want to modify it to 320x240. Just use and .\n\nPlaying video from file is the same as capturing it from camera, just change the camera index to a video file name. Also while displaying the frame, use appropriate time for . If it is too less, video will be very fast and if it is too high, video will be slow (Well, that is how you can display videos in slow motion). 25 milliseconds will be OK in normal cases.\n\nSo we capture a video and process it frame-by-frame, and we want to save that video. For images, it is very simple: just use . Here, a little more work is required.\n\nThis time we create a VideoWriter object. We should specify the output file name (eg: output.avi). Then we should specify the FourCC code (details in next paragraph). Then number of frames per second (fps) and frame size should be passed. And the last one is the isColor flag. If it is , the encoder expect color frame, otherwise it works with grayscale frame.\n\nFourCC is a 4-byte code used to specify the video codec. The list of available codes can be found in fourcc.org. It is platform dependent. The following codecs work fine for me.\n• In Fedora: DIVX, XVID, MJPG, X264, WMV1, WMV2. (XVID is more preferable. MJPG results in high size video. X264 gives very small size video)\n• In Windows: DIVX (More to be tested and added)\n\nFourCC code is passed as ‘cv.VideoWriter_fourcc('M’,'J','P','G') cv.VideoWriter_fourcc(*'MJPG')` for MJPG.\n\nThe below code captures from a camera, flips every frame in the vertical direction, and saves the video."
    },
    {
        "link": "https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html",
        "document": ""
    },
    {
        "link": "https://docs.opencv.org/4.x/index.html",
        "document": ""
    },
    {
        "link": "https://docs.opencv.org/4.x/d8/dfe/classcv_1_1VideoCapture.html",
        "document": "Class for video capturing from video files, image sequences or cameras. More...\n\nClass for video capturing from video files, image sequences or cameras. The class provides C++ API for capturing video from cameras or for reading video files and image sequences. Here is how the class can be used: Returns true if the array has no elements. Class for video capturing from video files, image sequences or cameras. Grabs, decodes and returns the next video frame. Opens a video file or a capturing device or an IP video stream for video capturing. Returns true if video capturing has been initialized already. In C API the black-box structure is used instead of VideoCapture.\n• (C++) A basic sample on using the VideoCapture interface can be found at\n• (Python) A basic sample on using the VideoCapture interface can be found at\n• (Python) A multi threaded video processing sample can be found at\n• (Python) VideoCapture sample showcasing some features of the Video4Linux2 backend\n\nOpens a video file or a capturing device or an IP video stream for video capturing with API Preference and parameters. This is an overloaded member function, provided for convenience. It differs from the above function only in what argument(s) it accepts. The parameter allows to specify extra parameters encoded as pairs . See cv::VideoCaptureProperties\n\nGrabs the next frame from video file or capturing device. (non-zero) in the case of success. The method/function grabs the next frame from video file or camera and returns true (non-zero) in the case of success. The primary use of the function is in multi-camera environments, especially when the cameras do not have hardware synchronization. That is, you call VideoCapture::grab() for each camera and after that call the slower method VideoCapture::retrieve() to decode and get frame from each camera. This way the overhead on demosaicing or motion jpeg decompression etc. is eliminated and the retrieved frames from different cameras will be closer in time. Also, when a connected camera is multi-head (for example, a stereo camera or a Kinect device), the correct way of retrieving data from it is to call VideoCapture::grab() first and then call VideoCapture::retrieve() one or more times with different values of the channel parameter. Using Kinect and other OpenNI compatible depth sensors\n\nOpens a video file or a capturing device or an IP video stream for video capturing with API Preference and parameters. This is an overloaded member function, provided for convenience. It differs from the above function only in what argument(s) it accepts. The parameter allows to specify extra parameters encoded as pairs . See cv::VideoCaptureProperties if the file has been successfully opened The method first calls VideoCapture::release to close the already opened file or camera.\n\nGrabs, decodes and returns the next video frame. the video frame is returned here. If no frames has been grabbed the image will be empty. if no frames has been grabbed The method/function combines VideoCapture::grab() and VideoCapture::retrieve() in one call. This is the most convenient method for reading video files or capturing data from decode and returns the just grabbed frame. If no frames has been grabbed (camera has been disconnected, or there are no more frames in video file), the method returns false and the function returns empty image (with cv::Mat, test it with Mat::empty()). In C API, functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video capturing structure. It is not allowed to modify or release the image! You can copy the frame using cvCloneImage and then do whatever you want with the copy.\n\nThe documentation for this class was generated from the following file:"
    },
    {
        "link": "https://docs.opencv.org/4.x/d1/dfb/intro.html",
        "document": "OpenCV (Open Source Computer Vision Library: http://opencv.org) is an open-source library that includes several hundreds of computer vision algorithms. The document describes the so-called OpenCV 2.x API, which is essentially a C++ API, as opposed to the C-based OpenCV 1.x API (C API is deprecated and not tested with \"C\" compiler since OpenCV 2.4 releases)\n\nOpenCV has a modular structure, which means that the package includes several shared or static libraries. The following modules are available:\n• Core functionality (core) - a compact module defining basic data structures, including the dense multi-dimensional array Mat and basic functions used by all other modules.\n• Image Processing (imgproc) - an image processing module that includes linear and non-linear image filtering, geometrical image transformations (resize, affine and perspective warping, generic table-based remapping), color space conversion, histograms, and so on.\n• Image file reading and writing (imgcodecs) - includes functions for reading and writing image files in various formats.\n• Video I/O (videoio) - an easy-to-use interface to video capturing and video codecs.\n• Camera Calibration and 3D Reconstruction (calib3d) - basic multiple-view geometry algorithms, single and stereo camera calibration, object pose estimation, stereo correspondence algorithms, and elements of 3D reconstruction.\n• Object Detection (objdetect) - detection of objects and instances of the predefined classes (for example, faces, eyes, mugs, people, cars, and so on).\n• Machine Learning (ml) - The Machine Learning module includes a set of classes and functions for statistical classification, regression, and clustering of data.\n• ... some other helper modules, such as FLANN and Google test wrappers, Python bindings, and others.\n\nThe further chapters of the document describe functionality of each module. But first, make sure to get familiar with the common API concepts used thoroughly in the library.\n\nAll the OpenCV classes and functions are placed into the namespace. Therefore, to access this functionality from your code, use the specifier or directive:\n\nSome of the current or future OpenCV external names may conflict with STL or other libraries. In this case, use explicit namespace specifiers to resolve the name conflicts:\n\nFirst of all, std::vector, cv::Mat, and other data structures used by the functions and methods have destructors that deallocate the underlying memory buffers when needed. This means that the destructors do not always deallocate the buffers as in case of Mat. They take into account possible data sharing. A destructor decrements the reference counter associated with the matrix data buffer. The buffer is deallocated if and only if the reference counter reaches zero, that is, when no other structures refer to the same buffer. Similarly, when a Mat instance is copied, no actual data is really copied. Instead, the reference counter is incremented to memorize that there is another owner of the same data. There is also the cv::Mat::clone method that creates a full copy of the matrix data. See the example below:\n\nYou see that the use of Mat and other basic structures is simple. But what about high-level classes or even user data types created without taking automatic memory management into account? For them, OpenCV offers the cv::Ptr template class that is similar to std::shared_ptr from C++11. So, instead of using plain pointers:\n\nyou can use:\n\nencapsulates a pointer to a T instance and a reference counter associated with the pointer. See the cv::Ptr description for details.\n\nOpenCV deallocates the memory automatically, as well as automatically allocates the memory for output function parameters most of the time. So, if a function has one or more input arrays (cv::Mat instances) and some output arrays, the output arrays are automatically allocated or reallocated. The size and type of the output arrays are determined from the size and type of input arrays. If needed, the functions take extra parameters that help to figure out the output array properties.\n\nThe array frame is automatically allocated by the operator since the video frame resolution and the bit-depth is known to the video capturing module. The array edges is automatically allocated by the cvtColor function. It has the same size and the bit-depth as the input array. The number of channels is 1 because the color conversion code cv::COLOR_BGR2GRAY is passed, which means a color to grayscale conversion. Note that frame and edges are allocated only once during the first execution of the loop body since all the next video frames have the same resolution. If you somehow change the video resolution, the arrays are automatically reallocated.\n\nThe key component of this technology is the cv::Mat::create method. It takes the desired array size and type. If the array already has the specified size and type, the method does nothing. Otherwise, it releases the previously allocated data, if any (this part involves decrementing the reference counter and comparing it with zero), and then allocates a new buffer of the required size. Most functions call the cv::Mat::create method for each output array, and so the automatic output data allocation is implemented.\n\nSome notable exceptions from this scheme are cv::mixChannels, cv::RNG::fill, and a few other functions and methods. They are not able to allocate the output array, so you have to do this in advance.\n\nAs a computer vision library, OpenCV deals a lot with image pixels that are often encoded in a compact, 8- or 16-bit per channel, form and thus have a limited value range. Furthermore, certain operations on images, like color space conversions, brightness/contrast adjustments, sharpening, complex interpolation (bi-cubic, Lanczos) can produce values out of the available range. If you just store the lowest 8 (16) bits of the result, this results in visual artifacts and may affect a further image analysis. To solve this problem, the so-called saturation arithmetics is used. For example, to store r, the result of an operation, to an 8-bit image, you find the nearest value within the 0..255 range:\n\nSimilar rules are applied to 8-bit signed, 16-bit signed and unsigned types. This semantics is used everywhere in the library. In C++ code, it is done using the functions that resemble standard C++ cast operations. See below the implementation of the formula provided above:\n\nwhere cv::uchar is an OpenCV 8-bit unsigned integer type. In the optimized SIMD code, such SSE2 instructions as paddusb, packuswb, and so on are used. They help achieve exactly the same behavior as in C++ code.\n\nTemplates is a great feature of C++ that enables implementation of very powerful, efficient and yet safe data structures and algorithms. However, the extensive use of templates may dramatically increase compilation time and code size. Besides, it is difficult to separate an interface and implementation when templates are used exclusively. This could be fine for basic algorithms but not good for computer vision libraries where a single algorithm may span thousands lines of code. Because of this and also to simplify development of bindings for other languages, like Python, Java, Matlab that do not have templates at all or have limited template capabilities, the current OpenCV implementation is based on polymorphism and runtime dispatching over templates. In those places where runtime dispatching would be too slow (like pixel access operators), impossible (generic implementation), or just very inconvenient ( ) the current implementation introduces small template classes, methods, and functions. Anywhere else in the current OpenCV version the use of templates is limited.\n\nConsequently, there is a limited fixed set of primitive data types the library can operate on. That is, array elements should have one of the following types:\n• a tuple of several elements where all elements have the same type (one of the above). An array whose elements are such tuples, are called multi-channel arrays, as opposite to the single-channel arrays, whose elements are scalar values. The maximum possible number of channels is defined by the CV_CN_MAX constant, which is currently set to 512.\n\nFor these basic types, the following enumeration is applied:\n\nMulti-channel (n-channel) types can be specified using the following options:\n• CV_8UC1 ... CV_64FC4 constants (for a number of channels from 1 to 4)\n• CV_8UC(n) ... CV_64FC(n) or CV_MAKETYPE(CV_8U, n) ... CV_MAKETYPE(CV_64F, n) macros when the number of channels is more than 4 or unknown at the compilation time.\n\nArrays with more complex elements cannot be constructed or processed using OpenCV. Furthermore, each function or method can handle only a subset of all possible array types. Usually, the more complex the algorithm is, the smaller the supported subset of formats is. See below typical examples of such limitations:\n• The face detection algorithm only works with 8-bit grayscale or color images.\n• Linear algebra functions and most of the machine learning algorithms work with floating-point arrays only.\n• Basic functions, such as cv::add, support all types.\n\nThe subset of supported types for each function has been defined from practical needs and could be extended in future based on user requests.\n\nMany OpenCV functions process dense 2-dimensional or multi-dimensional numerical arrays. Usually, such functions take as parameters, but in some cases it's more convenient to use (for a point set, for example) or (for 3x3 homography matrix and such). To avoid many duplicates in the API, special \"proxy\" classes have been introduced. The base \"proxy\" class is cv::InputArray. It is used for passing read-only arrays on a function input. The derived from InputArray class cv::OutputArray is used to specify an output array for a function. Normally, you should not care of those intermediate types (and you should not declare variables of those types explicitly) - it will all just work automatically. You can assume that instead of InputArray/OutputArray you can always use , , , or . When a function has an optional input or output array, and you do not have or do not want one, pass cv::noArray().\n\nOpenCV uses exceptions to signal critical errors. When the input data has a correct format and belongs to the specified value range, but the algorithm cannot succeed for some reason (for example, the optimization algorithm did not converge), it returns a special error code (typically, just a boolean variable).\n\nThe exceptions can be instances of the cv::Exception class or its derivatives. In its turn, cv::Exception is a derivative of . So it can be gracefully handled in the code using other standard C++ library components.\n\nThe exception is typically thrown either using the macro, or its printf-like variant, or using the CV_Assert(condition) macro that checks the condition and throws an exception when it is not satisfied. For performance-critical code, there is CV_DbgAssert(condition) that is only retained in the Debug configuration. Due to the automatic memory management, all the intermediate buffers are automatically deallocated in case of a sudden error. You only need to add a try statement to catch exceptions, if needed:\n\nThe current OpenCV implementation is fully re-enterable. That is, the same function or the same methods of different class instances can be called from different threads. Also, the same Mat can be used in different threads because the reference-counting operations use the architecture-specific atomic instructions."
    },
    {
        "link": "https://face-recognition.readthedocs.io",
        "document": ""
    },
    {
        "link": "https://realpython.com/face-recognition-with-python",
        "document": "Do you have a phone that you can unlock with your face? Have you ever wondered how that works? Have you ever wanted to build your own face recognizer? With Python, some data, and a few helper packages, you can create your very own. In this project, you’ll use face detection and face recognition to identify faces in a given image.\n\nIn this tutorial, you’ll build your own face recognition tool using:\n• Face detection to find faces in an image\n• Machine learning to power face recognition for given images\n• Command-line arguments to direct your application with\n• Bounding boxes to label faces with the help of Pillow\n\nWith these techniques, you’ll gain a solid foundation in computer vision. After implementing your knowledge in this project, you’ll be ready to apply these techniques in solving real-world problems beyond face recognition.\n\nClick the link below to download the complete source code for this project:\n\nYour program will be a typical command-line application, but it’ll offer some impressive capabilities. To accomplish this feat, you’ll first use face detection, or the ability to find faces in an image. Then, you’ll implement face recognition, which is the ability to identify detected faces in an image. To that end, your program will do three primary tasks: When training, your face recognizer will need to open and read many image files. It’ll also need to know who appears in each one. To accomplish this, you’ll set up a directory structure to give your program information about the data. Specifically, your project directory will contain three data directories: You can put images directly into . For , you should have images separated by subject into directories with the subject’s name. Setting your training directory up this way will allow you to give your face recognizer the information that it needs to associate a label—the person pictured—with the underlying image data. Note: This strategy works well for training on images that contain a single face. If you want to train on images with multiple identifiable faces, then you’ll have to investigate an alternative strategy for marking the faces in the training images. You’ll walk through this project step by step, starting with preparing your environment and data. After that, you’ll be ready to load your training data and get to work on training your model to recognize unlabeled faces. Once your app is able to do that, you’ll need a way to display your results. You’ll build a command-line interface so that users can interact with your app. Finally, you’ll run the app through all of its paces. This is of vital importance because it’ll help you see your application through the eyes of a user. That way, you can better understand how your application works in practice, a process that’s key to finding bugs.\n\nIn this step, you’ll create a project environment, install necessary dependencies, and set the stage for your application. First, create your project and data directories: Running these commands creates a directory called , moves to it, then creates the folders , , and , which you’ll use throughout the project. Now you can create a virtual environment using the tool of your choice. Before you start installing this project’s dependencies with , you’ll need to ensure that you have CMake and a C compiler like gcc installed on your system. If your system doesn’t already have them installed, then follow these instructions to get started: To install CMake on Windows, visit the CMake downloads page and install the appropriate installer for your system. You can’t get gcc as a stand-alone download for Windows, but you can install it as a part of the MinGW runtime environment through the Chocolatey package manager with the following command: To install CMake on Linux, visit the CMake downloads page and install the appropriate installer for your system. Alternatively, CMake binaries may also be available through your favorite package manager. If you use package management, for example, then you can install CMake with this: You’ll also install gcc through your package manager. To install gcc with , you’ll install the metapackage: To verify that you’ve successfully installed gcc, you can check the version: If this returns a version number, then you’re good to go! To install CMake on macOS, visit the CMake downloads page and install the appropriate installer for your system. If you have Homebrew installed, then you can install both CMake and gcc that way: After following these steps for your operating system, you’ll have Cmake and gcc installed and ready to assist you in building your project. Now open your favorite text editor to create your file: This tells which dependencies your project will be using and pins them to these specific versions. This is important because future versions could have changes to their APIs that break your code. When you specify the versions needed, you have full control over what versions are compatible with your project. Note: This project was built on Python 3.9 and also tested on 3.10. Because some of the packages used in this tutorial still use the legacy installation method, you may run into issues if you use 3.11. After creating the requirements file and activating your virtual environment, you can install all of your dependencies at once: This command calls and tells it to install the dependencies in the file that you just created. Next, you’ll need to find a dataset for training and validating your data. Celebrity images are a popular choice for testing face recognition because so many celebrity headshots are widely available. That’s the approach that you’ll take in this tutorial. If you haven’t already, you can download everything you need for data training and validation by clicking the link below: Free Bonus: Click here to download the full source code to build your own face recognition app with Python. As an alternative, it can be great practice to set up your own dataset and folder structure. If you’d like to give that a try, then you can use this dataset or pictures of your own. If your dataset isn’t already split into training and validation sets, then you should go ahead and make that split now. In the directory, you should create a separate folder for each person who appears in your training images. Then you can put all the images into their appropriate folders: You can place the validation images directly into the directory. Your validation images need to be images that you don’t train with, but you can identify the people who appear in them. In this step, you’ve prepared your environment. First, you created a directory and several subdirectories to house your project and its data. Then you created a virtual environment, installed some dependencies manually, and then created a file with your project dependencies pinned to a specific version. With that, you used to install your project dependencies. Then, you downloaded a dataset and split it into training and validation sets. Next, you’ll write the code to load the data and train your model.\n\nIn this step, you’ll start writing code. This code will load your training data and start training your model. By the end of this step, you’ll have loaded your training data, detected faces in each image, and saved them as encodings. First, you’ll need to load images from and train your model on them. To do that, open your favorite editor, create a file called , and start writing some code: You start your script by importing from Python’s standard library, along with , a third-party library that you installed in the previous step. Then, you define a constant for the default encoding path. Keeping this path as a constant toward the top of your script will help you down the line if you want to change that path. Next, you add three calls to and set to . You may not need these lines of code if you already created the three directories in the previous step. However, for convenience, this code automatically creates all the directories that you’ll use if they don’t already exist. Finally, you define . This function uses a loop to go through each directory within , saves the label from each directory into , then uses the function from to load each image. As input, will require a model type and a location to save the encodings that you’ll generate for each image. Note: You’re not using the required arguments yet, but you’ll add more code to that relies on these arguments in just a moment. The model determines what you’ll use to locate faces in the input images. Valid model type choices are and , which refer to the respective algorithms used:\n• HOG (histogram of oriented gradients) is a common technique for object detection. For this tutorial, you only need to remember that it works best with a CPU.\n• CNN (convolutional neural network) is another technique for object detection. In contrast to a HOG, a CNN works better on a GPU, otherwise known as a video card. These algorithms don’t rely on deep learning. If you’d like to learn more about how algorithms like these work under the hood, then Traditional Face Detection With Python is your guide. Next, you’ll use to detect the face in each image and get its encoding. This is an array of numbers describing the features of the face, and it’s used with the main model underlying to reduce training time while improving the accuracy of a large model. This is known as transfer learning. Then, you’ll add all the names and encodings to the lists and , respectively: After updating your project with this code, your function is ready to collect names and encodings from all the files in your directory:\n• Line 15 uses to detect the locations of faces in each image. The function returns a list of four-element tuples, one tuple for each detected face. The four elements per tuple provide the four coordinates of a box that could surround the detected face. Such a box is also known as a bounding box.\n• Line 16 uses to generate encodings for the detected faces in an image. Remember that an encoding is a numeric representation of facial features that’s used to match similar faces by their features.\n• Lines 18 to 20 add the names and their encodings to separate lists. Now you’ve generated encodings and added them, along with the label for each image, to a list. Next, you’ll combine them into a single dictionary and save that dictionary to disk. Note: You’re saving your encodings to disk because generating them can be time-consuming, especially if you don’t have a dedicated GPU. Once they’re generated, saving them allows you to reuse the encodings in other parts of your code without re-creating them every time. Import from the standard library and use it to save the name-encoding dictionary: With this addition to , you create a dictionary that puts the and lists together and denotes which list is which. Then, you use to save the encodings to disk. Finally, you add a call to at the end so that you can test whether it works. You can now run your script to confirm that it creates your encodings: After some time, your script should finish execution, having created a file called in your directory. Well done, you’ve completed this step! In this section, you created the function, which loads your training images, finds the faces within the images, and then creates a dictionary containing the two lists that you created with each image. You then saved that dictionary to disk so that you could reuse the encodings. Now you’re ready to deal with unlabeled faces!\n\nIn this step, you’ll build the function, which recognizes faces in images that don’t have a label. First, you’ll open the encodings that you saved in the previous step and load the unlabeled image with : After adding this code, your function will now be able to open and load the saved face encodings using and then load the image in which you want to recognize faces. This is also known as your test image. You’ll pass the location of the unlabeled image, the model you want to use for face detection, and the location of the saved encodings from the previous step to this function. Then you’ll open the encodings file and load the data with . You’ll also load the image with and assign the output to . You’ll use to find the face in and get its encoding: Your function has just gotten more interesting. With these lines of code, you can detect faces in your input image and get their encodings, which will aid your code in identifying the faces. Now you’ll use the encoding of the detected face to make a comparison with all of the encodings that you found in the previous step. This will happen within a loop so that you can detect and recognize multiple faces in your unknown image: In this additional code, you iterate through and in parallel using . Then, you call the non-public function , passing the encodings for the unknown image and the loaded encodings. This function doesn’t yet exist, but you’ll build it in just a moment. Note: Looping over two iterables at the same time using Python’s function is called parallel iteration. This comes in handy when you need to do an operation on all the elements of two lists at the same time. You also add a conditional statement that assigns to if doesn’t find a match. Finally, you print and the coordinates of the identified face that are saved in . Before you can run , you’ll need to implement . This helper function will take the unknown and loaded encodings. It’ll make a comparison between the unknown encoding and each of the loaded encodings using from . Ultimately, will return the most likely match, or it’ll implicitly return if the function exits without reaching a return statement: You’ve now created , which does the hard work of identifying each face in the given image. In this function, you call to compare each unknown encoding in your test image with the encodings that you loaded previously. The function returns a list of and values for each loaded encoding. The indices of this list are equal to those of the loaded encodings, so the next thing you do is keep track of for each possible match. You do this with , which you imported from at the top of your script. Using allows you to track how many votes each potential match has by counting the values for each loaded encoding by the associated name. You then return the name that has the most votes in its favor. But what’s a vote, and who’s voting? Think back to the first function that you wrote in this tutorial, where you generated encodings for a bunch of training images of celebrities’ faces. When you call , your unknown face is compared to every known face that you have encodings for. Each match acts as a vote for the person with the known face. Since you should have multiple images of each known face, a closer match will have more votes than one that isn’t as close a match. Finally, outside of the function definition, you add a call to to test that it’s working as expected. Note: Remember to remove the call to that you previously used to create the encodings. Unless you change the training data, you won’t have to run this function again, and it would unnecessarily use computing time. In its current state, fetches the encodings that you created in step two and compares them to the encodings that it generates on an input image. It does that for all the faces that it can find in an image. For example, if you download the example code for step three, then you’ll find an image called that shows two characters from the American sitcom Seinfeld: Free Bonus: Click here to download the full source code to build your own face recognition app with Python. Recall that at the end of the last snippet, you added a test call to with the parameter . If you use that image, then running should give you output like this: Your script will recognize only one of the two people shown in the image because you only included one of the two characters’ faces in the training data. Python will label any face that the script locates but can’t identify from the encoding that you generated in the previous step as . Try it out with some other images! Now that you’ve gotten the prediction for your image, you’ll extend this function to show it to the user. One way to do this is to display the results on the input image itself. This has the bonus of being clear for the user and requiring little extra work on their part.\n\nNow comes the time to draw on your input image! This will help the user see which face is being identified and what it’s being identified as. A popular technique is to draw a bounding box around the face and give it a label. To do this, you’ll use Pillow, a high-powered image processing library for Python. Note: Are you interested in image processing with Python? If so, check out the Real Python podcast interview with Mike Driscoll. For now, just load the image into Pillow and create an object in the function: Here, you start by adding three lines of code that set up the ability to draw on an existing image:\n• Line 3 at the top of your script imports the and modules from .\n• Line 16 creates an object, which will help you draw a bounding box around detected faces. Next, within the loop in , you remove the call from step three, and in line 25, you make a call to another new helper function, this one named . Finally, you add some housekeeping that Pillow requires. You manually remove the object from the current scope with the statement in line 27. Then you show the image by calling in line 28. Next, you’ll implement the function, which will draw a bounding box on the recognized face and add a caption to that bounding box with the name of the identified face, or Unknown if it doesn’t match any known face. To do this, will need to take as parameters the object, the tuple of points that define a square area around a recognized face, and the name that you got from : You start by creating two constants near the top of your script and assigning them to two common HTML color names, and . You then use these constants multiple times in . Defining them as constants means that you’ll have less maintenance effort if you want to change the colors later on. Then, in the first line of your new helper function, you unpack the tuple into its four parts: , , , and . You use these coordinates in the next line to draw a rectangle around the recognized face using the method in . The next step is to determine the bounding box for the text caption. You do this with , which takes a pair of anchor coordinates and the caption text as parameters and returns the four coordinates of a bounding box that fits the caption. The anchor is a coordinate tuple of where you want the box to start. Because you read English left to right, and captions are typically on the bottom, you use the left and bottom coordinates of the face’s bounding box as the anchor for your caption box. Next, you draw another rectangle, but for this one, you define the rectangle with the bounding box coordinates that you got in the previous line. You also color in the rectangle by using the parameter. This second rectangle serves as the caption area directly under the bounding box that surrounds the recognized face. And last, you call on the object to write the name in the caption box that you just drew. You use the parameter again, but in this case, it determines the color of the text. After you define , your function is complete. You just wrote the backbone of your project, which takes an image with an unknown face, gets its encoding, checks that against all the encodings made during the training process, and then returns the most likely match for it. You can now use this function when you want to recognize an unknown face. If you run your script at the end of this step, then Python will display the image for you with the predictions of who’s in the image baked right into the image: The next step is to validate your model to ensure that your model isn’t overfitted or tuned too specifically to the training data.\n\nModel validation is a technique that tests your trained model by providing data that it hasn’t seen before but that you have. Knowing the correct label for each image allows you to get an idea of your model’s performance on new data. At the most basic level, you’re just running your function on images that already contain a known face. In step one, you created a validation directory that contains images with faces that you can recognize. The function that you’ll build next will use to open each of the validation images and then call on them: In line 6, you open the directory with and then use to get all the files in that directory. You confirm that the resource is a file in line 7. Then, in lines 8 to 10, you call the function from step three on the current image file. Finally, in line 13, you add a call to so that you can test your script. If you run now, then Python will make all the images from within pop up with predictions baked right into the images: A more robust validation could include accuracy measures and visualizations, such as a confusion matrix showing the true positives, true negatives, false positives, and false negatives from your validation run. How else could you extend this? In addition to the traditional confusion matrix, you could calculate model evaluation measures such as overall accuracy and true positive rate, also known as recall. Once you’ve built your validation function, it’s time to tie your app together and make it user-friendly.\n\nNow that you’ve built your project, it’s time to actually perform face recognition. You might have saved and played with your program already, but it’s always worthwhile to take it for another spin. That way, you can diagnose bugs, uncover different uses, and more. Watch the video below for a short guided tour through your new project: This video shows you all of the options that you can use to interact with your face recognizer. These are:\n• will show you a list of options, a description of what each of them does, and any arguments that they take.\n• will start the training process. You can optionally specify whether to use the CPU-based HOG method or a GPU-based CNN.\n• will run the validation process, where the model takes images with known faces and tries to identify them correctly.\n• is the option that you’ll probably use the most. Use this along with the option to specify the location of an image with unknown faces that you want to identify. Under the hood, this works the same as validation except that you specify the image location yourself. Before your first use, you’ll want to train the model with your training images. This will allow your model to be especially good at identifying those particular faces. You can check the accuracy of your model by running the validation process with new images of the same people in your training data and seeing if the labels match the faces. If you’re not satisfied with the results, then try adding more images to your training data, retraining the model, and attempting validation again. If you got the desired results, then you can start using the option with images that you choose."
    },
    {
        "link": "https://pypi.org/project/face-recognition",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://buildmedia.readthedocs.org/media/pdf/face-recognition/latest/face-recognition.pdf",
        "document": ""
    },
    {
        "link": "https://data-flair.training/blogs/python-face-recognition",
        "document": "Python can detect and recognize your face from an image or video\n\nFace Detection and Recognition is one of the areas of computer vision where the research actively happens.\n\nThe applications of Face Recognition include Face Unlock, Security and Defense, etc. Doctors and healthcare officials use face recognition to access the medical records and history of patients and better diagnose diseases.\n\nIn this python project, we are going to build a machine learning model that recognizes the persons from an image. We use the face_recognition API and OpenCV in our project.\n\nTo install the above packages, use the following command.\n\nTo install the face_recognition, install the dlib package first.\n\nNow, install face_recognition module using the below command\n\nPlease download the source code of python face recognition project: Face Recognition Project Code\n\nWe can do this face recognition project using our own dataset. For this project, let’s take the cast of the popular American web series “Friends” as the dataset. The dataset is included with face recognition project code, which you downloaded in the previous section.\n\nBefore moving on, let’s know what face recognition and detection are.\n\nFace recognition is the process of identifying or verifying a person’s face from photos and video frames.\n\nFace detection is defined as the process of locating and extracting faces (location and size) in an image for use by a face detection algorithm.\n\nFace recognition method is used to locate features in the image that are uniquely specified. The facial picture has already been removed, cropped, scaled, and converted to grayscale in most cases. Face recognition involves 3 steps: face detection, feature extraction, face recognition.\n\nOpenCV is an open-source library written in C++. It contains the implementation of various algorithms and deep neural networks used for computer vision tasks.\n\nCreate 2 directories, train and test. Pick an image for each of the cast from the internet and download it onto our “train” directory. Make sure that the images you’ve selected show the features of the face well enough for the classifier.\n\nFor testing the model, let’s take a picture containing all of the cast and place it onto our “test” directory.\n\nFor your comfort, we have added training and testing data with the project code.\n\nFirst import the necessary modules.\n\nThe face_recognition library contains the implementation of the various utilities that help in the process of face recognition.\n\nNow, create 2 lists that store the names of the images (persons) and their respective face encodings.\n\nFace encoding is a vector of values representing the important measurements between distinguishing features of a face like the distance between the eyes, the width of the forehead, etc.\n\nWe loop through each of the images in our train directory, extract the name of the person in the image, calculate its face encoding vector and store the information in the respective lists.\n\n3. Test the model on the test dataset\n\nAs mentioned above, our test dataset only contains 1 image with all of the persons in it.\n\nRead the test image using the cv2 imread() method.\n\nThe face_recognition library provides a useful method called face_locations() which locates the coordinates (left, bottom, right, top) of every face detected in the image. Using those location values we can easily find the face encodings.\n\nWe loop through each of the face locations and its encoding found in the image. Then we compare this encoding with the encodings of the faces from the “train” dataset.\n\nThen calculate the facial distance meaning that we calculate the similarity between the encoding of the test image and that of the train images. Now, we pick the minimum valued distance from it indicating that this face of the test image is one of the persons from the training dataset.\n\nNow, draw a rectangle with the face location coordinates using the methods from the cv2 module.\n\nDisplay the image using the imshow() method of the cv2 module.\n\nSave the image to our current working directory using the imwrite() method.\n\nRelease the resources that weren’t deallocated(if any).\n\nLet’s see the output of the model.\n\nIn this machine learning project, we developed a face recognition model in python and opencv using our own custom dataset."
    }
]